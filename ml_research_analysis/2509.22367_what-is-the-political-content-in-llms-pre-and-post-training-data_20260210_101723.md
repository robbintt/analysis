---
ver: rpa2
title: What Is The Political Content in LLMs' Pre- and Post-Training Data?
arxiv_id: '2509.22367'
source_url: https://arxiv.org/abs/2509.22367
tags:
- documents
- political
- policy
- right
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the origins of political bias in large
  language models (LLMs) by analyzing the political content of their pre- and post-training
  data. Using the OLMO2 model family and its fully open-source training datasets,
  the authors sample and classify documents for political orientation (left, right,
  or neutral) using a robust zero-shot classifier.
---

# What Is The Political Content in LLMs' Pre- and Post-Training Data?

## Quick Facts
- **arXiv ID:** 2509.22367
- **Source URL:** https://arxiv.org/abs/2509.22367
- **Reference count:** 40
- **Primary result:** Left-leaning documents outnumber right-leaning ones by 3-12x in OLMO2 training data, with strong correlation (r=0.90) between training data political stances and model outputs

## Executive Summary
This study analyzes political bias in large language models by examining the political content of their pre- and post-training data using the OLMO2 model family and its fully open-source training datasets. The authors find a consistent left-leaning bias across all datasets, with left-leaning documents outnumbering right-leaning ones by factors of 3 to 12. Source domain analysis reveals systematic differences in framing, with right-leaning content coming primarily from blogs and left-leaning content from established news outlets. The study establishes a strong correlation (r = 0.90) between the political stances in training data and the biases exhibited by models when probed on eight policy issues, suggesting that political biases are largely encoded during the pre-training stage.

## Method Summary
The authors sample documents from OLMO2 training datasets (DOLMA, DOLMINO, SFT-MIX, DPO-MIX) using reservoir sampling, then classify them for political orientation (left, right, neutral) using a zero-shot classifier based on LLAMA3.3-70B-4BIT. They analyze source domains by parsing URLs and use BERTopic with ALL-MPNET-BASE-V2 for topic clustering, with GPT-4.1-NANO labeling topics. Policy stances are classified using a CoT prompt with META-LLAMA-3.1-70B-BNB-4BIT. Model behavior is probed using ProbVAA with 72 prompt variants Ã— 30 samples per policy issue, and correlations are computed between data stances and model outputs across OLMO2 checkpoints (Base, SFT, DPO, Instruct).

## Key Results
- Left-leaning documents outnumber right-leaning ones by 3-12x across all training datasets
- Pre-training corpora contain ~4x more politically engaged content than post-training data
- Strong correlation (r = 0.90) between training data stances and model output stances on policy issues
- Source domain analysis shows right-leaning content from blogs vs. left-leaning from news outlets
- Topic modeling reveals systematic framing differences on issues like climate change and animal rights

## Why This Works (Mechanism)
The study works by systematically sampling and classifying documents for political orientation, then correlating these classifications with model behavior. The zero-shot classifier provides scalable annotation across massive datasets, while topic modeling and source analysis reveal how political content is framed differently across domains. The correlation between data stances and model outputs suggests that models encode political biases present in their training data, particularly during the pre-training phase where most content resides.

## Foundational Learning

**Concept: Pre-training vs. Post-training Data Composition**
- **Why needed here:** This is the central analytical unit. You must understand that "bias" doesn't come from a single source but from distinct phases with different data characteristics (scale, curation, purpose).
- **Quick check question:** Can you name two key differences in political content between DOLMA (pre-training) and SFT-MIX (post-training) described in the paper?

**Concept: Stance Classification and Correlation Metrics**
- **Why needed here:** The entire argument rests on measuring "political leaning" in text and connecting it to model behavior. You need to understand the validity of the automated classifier and the meaning of the Pearson correlation coefficient.
- **Quick check question:** What does a Pearson correlation of r = 0.90 between training data stance and model output stance suggest about the relationship between data and model bias?

**Concept: Framing and Source Domain Analysis**
- **Why needed here:** Bias is not just "left/right" but is embedded in *how* topics are discussed. Analyzing sources (blogs vs. news) and the resulting framing is key to understanding the nuance of the model's political worldview.
- **Quick check question:** According to the paper, how does the primary source domain differ between left-leaning and right-leaning political documents in the pre-training corpus?

## Architecture Onboarding

**Component map:**
DOLMA/DOLMINO (Pre-training, ~3.4x more political content) -> SFT-MIX/DPO-MIX (Post-training, less political content) -> LLAMA3.3-70B-4BIT (Zero-shot classifier) -> Assigns "Left", "Right", "Neutral" labels -> Source Domain Extractor (URL parsing) -> BERTopic (Topic clustering) -> GPT-4.1-NANO (Topic labeling) -> META-LLAMA-3.1-70B-BNB-4BIT (Policy stance classification) -> OLMO2 Model Checkpoints (Base, SFT, DPO, Instruct) -> Probed on policy statements (ProbVAA) -> Output stances correlated with data stances

**Critical path:** 1. Sample from OLMO2 training datasets. 2. Classify documents for political leaning. 3. Analyze domains and topics. 4. Probe model for its stance. 5. Correlate data stance with model stance.

**Design tradeoffs:**
- **Sampling vs. Full Corpus:** The authors sample (e.g., 200k documents from DOLMA) due to scale. This is efficient but introduces sampling error. The representativeness is validated, but a full corpus analysis could reveal rarer signals.
- **Automated Classification vs. Human Annotation:** Reliance on LLM-based classifiers (LLAMA3.3-70B) for large-scale annotation is scalable but inherits the biases of the classifier model itself. This is a critical validity assumption.
- **General "Left/Right" Labels vs. Fine-grained Policy Stances:** The initial classification is broad, while the core analysis uses specific policy issues (e.g., "Expanded Environmental Protection"). This trade-off allows for broad characterization followed by a more targeted, politically relevant analysis.

**Failure signatures:**
- **Classifier Bias:** The core classifier may itself be biased, mislabeling documents and thus creating a false picture of the data's imbalance.
- **Probing Brittleness:** The model's responses to the ProbVAA policy statements might be sensitive to prompt phrasing (though the authors try to mitigate this with multiple paraphrases). An "unstable" model (like a base model) might give inconsistent answers, making stance calculation noisy.
- **Spurious Correlation:** The high Pearson correlation might not imply causation. Both the data and the model could be reflecting a third factor (e.g., general discourse on the internet) without a direct link.

**First 3 experiments:**
1. **Validate the Classifier:** Human-annotate a sample of documents classified as left/right/neutral by the pipeline to calculate the classifier's precision, recall, and F1 score against ground truth.
2. **Analyze Base vs. Instruct Model Discrepancy:** Compare the political stances of the `OLMO2-13B-BASE` model against the `OLMO2-13B-INSTRUCT` model on the same set of policy probes to quantify the direct effect of the full post-training pipeline.
3. **Investigate a Specific Topic Cluster:** Manually analyze a high-importance topic like "Climate Change." Inspect documents from both left and right clusters to verify the identified framing differences (e.g., economic stability vs. urgency) and how the model responds to related prompts.

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** Would removing heavily politicized content from pre-training corpora effectively reduce political bias in LLMs?
- **Basis in paper:** [explicit] The authors state: "Future research could explore strategies to mitigate political bias in LLMs... This might provide an actionable solution for removing political bias in LLMs going forward: pre-filter pre-training data and remove heavily politicized text."
- **Why unresolved:** The study establishes correlation between training data and model bias (r=0.90) but does not test whether causal interventions like filtering would reduce bias.
- **What evidence would resolve it:** Train models on filtered corpora with reduced political content and compare resulting model biases to unfiltered baselines.

**Open Question 2**
- **Question:** Do these findings generalize beyond OLMO2 to other model families, closed-source models, and non-English training data?
- **Basis in paper:** [explicit] The ethics statement notes: "Our study focuses exclusively on the English documents from the OLMO2 family... it limits generalizability."
- **Why unresolved:** OLMO2 is the only fully open-source model with complete training data, preventing comparable analysis of other major LLMs.
- **What evidence would resolve it:** Apply the same methodology to other models with disclosed training data, or require/advocate for data transparency from other providers.

**Open Question 3**
- **Question:** What causal mechanisms drive the relationship between pre-training political content and model bias?
- **Basis in paper:** [inferred] The authors find strong correlation but explicitly state: "we cannot disentangle stance reinforcement from improved consistency with the current evaluation setup" and "our analysis reveals correlations rather than causal mechanisms."
- **Why unresolved:** Correlational analysis cannot establish whether the political content causes bias, whether both arise from confounding factors, or whether filtering would have unintended effects.
- **What evidence would resolve it:** Controlled experiments varying specific aspects of political content while holding other factors constant; mechanistic interpretability of how political concepts are encoded.

**Open Question 4**
- **Question:** Can post-training interventions successfully reverse or substantially shift political biases encoded during pre-training?
- **Basis in paper:** [inferred] The authors note prior work shows "only limited success in aligning models to different political leanings through post-training interventions" and suggest "when strong imbalances are already encoded in pre-training data... alignment can only partially steer models' political biases, rather than fully reverse them."
- **Why unresolved:** The relative contributions of pre-training versus post-training to final bias remain unclear; post-training may primarily improve consistency rather than change stance.
- **What evidence would resolve it:** Systematic comparison of bias magnitude before/after post-training with matched political content interventions.

## Limitations
- The study relies on automated classifiers for political orientation, which may introduce bias from the classification model itself
- The high correlation (r = 0.90) between training data stance and model output could reflect spurious correlation rather than direct causation
- The sampling approach, while validated, may miss rare but politically significant content
- Classifier performance on domain-specific political language remains untested

## Confidence

- **High confidence:** The systematic left-leaning imbalance across all datasets (3-12x ratio) is robust and consistent across multiple validation sets. The source domain analysis (blogs vs. news outlets) and topic clustering differences are methodologically sound.
- **Medium confidence:** The correlation between training data stance and model bias, while strong, could be influenced by classifier artifacts or shared third factors in the data.
- **Low confidence:** The exact causal mechanism linking pre-training data composition to final model bias, particularly given the smaller post-training datasets, requires further investigation.

## Next Checks
1. **Classifier validation:** Human-annotate a stratified sample of 500 documents from each dataset to measure precision/recall of the automated classifier against ground truth political orientation.
2. **Base vs. Instruct comparison:** Directly compare political stances between OLMO2-13B-BASE and OLMO2-13B-INSTRUCT models on identical policy probes to isolate post-training effects.
3. **Topic cluster audit:** Manually examine 50 documents from a key topic cluster (e.g., climate change) to verify the framing differences identified by the automated analysis and assess their political significance.