---
ver: rpa2
title: Symbolic Regression with Multimodal Large Language Models and Kolmogorov Arnold
  Networks
arxiv_id: '2505.07956'
source_url: https://arxiv.org/abs/2505.07956
tags:
- function
- functions
- symbolic
- regression
- llm-lex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel symbolic regression approach using
  vision-capable large language models (LLMs) combined with Kolmogorov-Arnold Networks
  (KANs). The method generates plots of univariate functions, prompts LLMs to propose
  ansatzes, and optimizes parameters through genetic algorithms.
---

# Symbolic Regression with Multimodal Large Language Models and Kolmogorov Arnold Networks

## Quick Facts
- arXiv ID: 2505.07956
- Source URL: https://arxiv.org/abs/2505.07956
- Reference count: 19
- Authors propose combining vision-capable LLMs with KANs for symbolic regression, achieving exact function identification for many test cases

## Executive Summary
This paper introduces a novel symbolic regression approach that leverages vision-capable large language models (LLMs) combined with Kolmogorov-Arnold Networks (KANs). The method generates plots of univariate functions, prompts LLMs to propose functional ansatzes, and optimizes parameters through genetic algorithms. For multivariate functions, it trains KANs and applies symbolic regression to each edge function. The combined expressions are then simplified using additional LLM processing. The approach successfully identifies exact expressions for many test functions, often outperforming traditional methods like Mathematica's FindFormula.

## Method Summary
The symbolic regression approach works by first generating visual representations of univariate functions, then using vision-capable LLMs to interpret these plots and propose candidate functional forms (ansatzes). These proposals are refined through genetic algorithm optimization of parameters. For multivariate cases, the method trains KANs to learn the underlying function, then applies symbolic regression to each individual edge function within the KAN architecture. The resulting component expressions are combined and simplified using additional LLM processing to produce interpretable symbolic forms. The implementation is available as the LLM-LEx package for univariate functions and KAN-LEx for multivariate functions.

## Key Results
- Successfully identifies exact expressions for many test functions, outperforming Mathematica's FindFormula
- Demonstrates effectiveness for both univariate (LLM-LEx) and multivariate (KAN-LEx) function classes
- Shows particular success in proposing interpretable functional forms that capture underlying data structure
- Publicly available Python packages implement the methodology for practical use

## Why This Works (Mechanism)
The approach works by combining the pattern recognition capabilities of vision-capable LLMs with the flexible function approximation power of KANs. LLMs excel at identifying structural patterns from visual representations, proposing reasonable functional forms that capture the essential characteristics of plotted data. KANs provide a flexible framework for learning complex multivariate relationships through compositions of univariate functions. The genetic algorithm optimization refines parameter estimates within the proposed ansatzes, while LLM-based simplification produces clean, interpretable final expressions. This multi-stage approach leverages the strengths of each component while compensating for individual weaknesses.

## Foundational Learning

**Multimodal Large Language Models**: Why needed - To interpret visual plots and propose functional forms from graphical data representations; Quick check - Model can correctly identify basic function types from plotted examples.

**Kolmogorov-Arnold Networks**: Why needed - To learn complex multivariate relationships through compositions of univariate functions; Quick check - Network can accurately approximate known multivariate functions.

**Genetic Algorithms**: Why needed - To optimize parameters within proposed ansatzes through evolutionary search; Quick check - Algorithm converges to correct parameters for simple test functions.

**Symbolic Regression**: Why needed - To convert numerical approximations into interpretable mathematical expressions; Quick check - Method produces correct symbolic form for known functions.

## Architecture Onboarding

**Component Map**: Data → Plot Generation → LLM Prompt → Ansatz Proposal → Genetic Optimization → Final Expression (univariate); Data → KAN Training → Edge Function Regression → Expression Combination → LLM Simplification (multivariate)

**Critical Path**: The most sensitive stages are the LLM prompt formulation and the genetic algorithm optimization, as poor prompts or premature convergence can prevent discovery of correct functional forms.

**Design Tradeoffs**: High computational cost from iterative LLM queries versus improved interpretability and accuracy compared to purely numerical methods. The decomposition strategy for multivariate functions sacrifices some global structural information for tractability.

**Failure Signatures**: Inability to propose correct ansatzes from plots, genetic algorithm convergence to local minima, or failure to simplify combined multivariate expressions into coherent forms.

**First Experiments**: 1) Test on simple univariate functions with known exact forms to verify basic functionality. 2) Compare performance against Mathematica's FindFormula on benchmark datasets. 3) Evaluate robustness by testing on noisy versions of clean functions.

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Performance highly dependent on prompt engineering quality with significant variation across different formulations
- Computationally expensive due to iterative LLM queries and genetic algorithm optimization
- Multivariate decomposition strategy may lose global structural information and complicate expression assembly
- Limited evaluation on noisy data and restricted to specific visually representable function classes

## Confidence

**High**: Basic feasibility of combining LLMs with KANs for symbolic regression is demonstrated through successful function identification in test cases

**Medium**: Performance claims relative to traditional methods like Mathematica's FindFormula are supported by examples but may not generalize across all function classes

**Low**: Assertion that the approach "works effectively" for multivariate functions requires more rigorous validation due to decomposition complexity and potential failure modes

## Next Checks
1. Conduct systematic testing on noisy datasets with varying noise levels to evaluate robustness compared to established symbolic regression tools
2. Benchmark performance across diverse function families (polynomial, transcendental, periodic, etc.) to identify strengths and weaknesses relative to traditional methods
3. Evaluate the impact of different prompt engineering strategies through ablation studies to determine optimal formulations for various function classes