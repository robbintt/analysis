---
ver: rpa2
title: 'AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs'
arxiv_id: '2505.21389'
source_url: https://arxiv.org/abs/2505.21389
tags:
- question
- difficulty
- questions
- arxiv
- autojudger
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoJudger is an agent-driven framework that reduces the cost of
  benchmarking multimodal large language models (MLLMs). It combines Item Response
  Theory (IRT) to estimate question difficulty with an MLLM-based agent to adaptively
  select questions during evaluation.
---

# AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs

## Quick Facts
- **arXiv ID**: 2505.21389
- **Source URL**: https://arxiv.org/abs/2505.21389
- **Reference count**: 40
- **Primary result**: Achieves over 90% ranking accuracy using only 4% of the data

## Executive Summary
AutoJudger is an agent-driven framework that significantly reduces the cost of benchmarking multimodal large language models (MLLMs) while maintaining high ranking accuracy. The framework combines Item Response Theory (IRT) to estimate question difficulty with an MLLM-based agent that adaptively selects questions during evaluation. By using semantic-aware retrieval and dynamic memory to guide question selection, AutoJudger achieves over 90% ranking consistency while using only 4% of the full benchmark data, representing a 25x reduction in evaluation costs.

## Method Summary
AutoJudger operates in two phases: offline difficulty estimation and online adaptive evaluation. First, it uses Item Response Theory (Rasch model) to estimate question difficulty parameters from responses of 60 offline MLLMs. During evaluation, it initializes with a diverse question set using CLIP embeddings and K-means clustering. An MLLM-based agent (Qwen2.5-VL-7B-Instruct) then iteratively selects questions based on difficulty matching, semantic diversity, and dynamic memory that tracks category-level statistics. The agent's selection is guided by probability bounds (0.2-0.8 success rate) to maximize information gain, while maintaining coherence through memory of previously evaluated questions.

## Key Results
- Achieves over 90% ranking accuracy compared to full benchmark evaluation
- Uses only 4% of the original benchmark data (25x reduction in evaluation costs)
- Maintains consistent performance across four different benchmarks (MMMU-Dev Val, SEEDBench-Image, MMT-Bench-Val, AI2D-Test) with 17 models
- Outperforms random selection, difficulty-only, and semantic-only baselines in both ranking accuracy and semantic distance metrics

## Why This Works (Mechanism)

### Mechanism 1: IRT-based difficulty estimation
Pre-estimated question difficulty enables efficient adaptive evaluation without exhaustively testing all questions. Item Response Theory (Rasch model) estimates difficulty parameters from offline model responses before evaluation. During evaluation, the model's latent ability is estimated in real-time from responses to selected questions, enabling probability-based filtering of candidate questions.

### Mechanism 2: Combined semantic diversity and difficulty filtering
Combining semantic diversity with difficulty-based filtering produces more informative question subsets than either alone. CLIP embeddings represent questions semantically. K-means clustering ensures initial diversity. During iteration, questions are first filtered by IRT probability bounds (p ∈ [0.2, 0.8]), then max-min retrieval selects candidates maximally distant from previously asked questions.

### Mechanism 3: Dynamic memory for global coherence
Dynamic memory enables globally coherent question selection by maintaining category-level statistics across evaluation. Memory stores per-category statistics (count, difficulty range, accuracy) in a markdown table. The agent reasons over this history to identify under-explored categories and balance selection, rather than greedily matching difficulty to ability.

## Foundational Learning

- **Item Response Theory (Rasch/1PL model)**: Core to difficulty estimation and real-time ability tracking. Understanding p(correct) = 1/(1 + exp(-(ability - difficulty))) is essential.
  - Quick check: Given ability = 0.5 and difficulty = -0.3, what's the predicted probability of a correct answer? (Answer: ~0.45)

- **Max-min retrieval for diversity**: Drives candidate selection. Understanding that max-min prioritizes points furthest from any existing point helps debug retrieval behavior.
  - Quick check: If you have 3 points forming a triangle and must add a 4th, where would max-min place it? (Answer: Near the center, maximizing minimum distance to all existing points)

- **Agent-based decision making with structured memory**: The interviewer agent synthesizes memory tables, ability estimates, and candidate questions. Understanding prompt engineering and structured context is critical.
  - Quick check: Why store category-level statistics rather than raw question-response pairs? (Answer: Compression for context limits, enables pattern recognition)

## Architecture Onboarding

- **Component map**: Offline models → responses → IRT fitting → difficulty scores → CLIP embeddings → K-means clustering → initial questions → adaptive loop → agent selection → memory update

- **Critical path**: Difficulty estimation accuracy → retrieval quality → agent selection quality → ranking consistency. The IRT priors and semantic embeddings are fixed infrastructure; agent prompts are the tunable surface.

- **Design tradeoffs**:
  - Candidate pool size |C*ₖ|: Paper finds 5 optimal; larger pools add noise
  - Compression ratio δ: 5% is default; lower ratios favor AutoJudger over baselines
  - Encoding model: CLIP text embeddings outperform image or concat for diversity
  - Memory granularity: Category-level vs. question-level; paper chooses category for interpretability

- **Failure signatures**:
  - Difficulty drift: If new models are much stronger/weaker than offline set, p(correct) bounds may filter inappropriately
  - Semantic collapse: If retrieval returns semantically similar candidates, check embedding quality and cluster distribution
  - Memory staleness: If category statistics don't update, check category assignment logic
  - Agent inconsistency: If selected questions don't match agent reasoning, inspect prompt formatting

- **First 3 experiments**:
  1. Validate IRT difficulty estimates: Hold out 20% of offline models, estimate difficulties from remaining 80%, correlate with held-out accuracy patterns
  2. Ablate retrieval components: Run (a) random, (b) difficulty-only, (c) semantic-only, (d) full pipeline on MMT-Bench at 3% compression
  3. Stress test generalization: Apply difficulties estimated from one benchmark to another, analyzing transfer limits

## Open Questions the Paper Calls Out
- How frequently must difficulty estimates be updated to maintain evaluation reliability as model capabilities advance?
- Does the scaling performance of the judging agent follow a predictable power law regarding evaluation accuracy?
- How robust is AutoJudger when evaluating model architectures that are fundamentally different from the offline models used to estimate difficulty?

## Limitations
- IRT difficulty estimates may not generalize to models with fundamentally different architectures or training regimes
- Semantic embedding space (CLIP) may not fully capture task-relevant diversity in multimodal contexts
- Category-based memory system assumes clean, consistent categorization of questions which may not hold for complex benchmarks

## Confidence
- **High confidence**: Core mechanism of using IRT for difficulty estimation and adaptive selection shows robust empirical support with consistent ranking accuracy >90%
- **Medium confidence**: Semantic retrieval mechanism's effectiveness depends on CLIP embeddings adequately representing question diversity
- **Medium confidence**: Dynamic memory's contribution to coherent selection is supported by ablation studies, but category-level abstraction may oversimplify

## Next Checks
1. Cross-benchmark transferability test: Apply difficulty estimates from one benchmark to evaluate models on another benchmark, measuring ranking accuracy degradation
2. Ablation of retrieval components: Systematically disable individual components of the retrieval pipeline and measure their marginal contributions to ranking accuracy at various compression ratios
3. Out-of-distribution model evaluation: Evaluate AutoJudger's performance when applied to models that are architecturally or functionally distinct from the 60 offline models used for difficulty estimation