---
ver: rpa2
title: "Bi-Level Policy Optimization with Nystr\xF6m Hypergradients"
arxiv_id: '2505.11714'
source_url: https://arxiv.org/abs/2505.11714
tags:
- gradient
- function
- critic
- nystr
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper reformulates actor-critic reinforcement learning as\
  \ a bilevel optimization problem, where the actor is the leader and the critic is\
  \ the follower. The key innovation is computing the actor's hypergradient using\
  \ the Nystr\xF6m method to approximate the inverse Hessian-vector product, avoiding\
  \ the numerical instability of conjugate gradient."
---

# Bi-Level Policy Optimization with Nyström Hypergradients

## Quick Facts
- **arXiv ID:** 2505.11714
- **Source URL:** https://arxiv.org/abs/2505.11714
- **Reference count:** 40
- **Primary result:** BLPO reformulates actor-critic RL as bilevel optimization, using Nyström method for stable hypergradient computation, matching or outperforming PPO on 10 standard control tasks.

## Executive Summary
This paper introduces Bi-Level Policy Optimization (BLPO), a new approach to actor-critic reinforcement learning that reformulates the problem as a bilevel optimization (Stackelberg game) where the actor is the leader and the critic is the follower. The key innovation is computing the actor's hypergradient using the Nyström method to approximate the inverse Hessian-vector product, which provides more numerical stability than traditional conjugate gradient methods. Under linear critic parameterization, BLPO provably converges to a local strong Stackelberg equilibrium in polynomial time. Empirically, BLPO matches or outperforms PPO on 10 standard discrete and continuous control tasks, with the Nyström method providing more stable and faster convergence than conjugate gradient.

## Method Summary
BLPO treats actor-critic RL as a bilevel optimization problem where the actor (leader) optimizes its policy given the critic's best-response value function, and the critic (follower) minimizes squared error to approximate the true value function. The method uses nested critic updates—fully optimizing the critic to best-response before each actor step—combined with the Nyström method to compute the inverse Hessian-vector product for the hypergradient. The Nyström approach samples columns of the Hessian proportional to diagonal magnitude and uses Woodbury identity to compute the approximation in O(pq²) time without materializing the full Hessian, avoiding the numerical instability of conjugate gradient methods.

## Key Results
- BLPO matches or outperforms PPO on 10 standard discrete and continuous control tasks
- Nyström-based hypergradient computation provides more stable and faster convergence than conjugate gradient
- Under linear critic parameterization, BLPO provably converges to a local strong Stackelberg equilibrium in polynomial time with high probability
- Nested critic updates combined with Nyström hypergradient are essential for performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reformulating actor-critic as a bilevel optimization (Stackelberg game) captures the asymmetric dependency where the actor depends on the critic, enabling more principled updates.
- Mechanism: The actor (leader) optimizes its policy given the critic's best-response value function; the critic (follower) minimizes squared error to approximate the true value function for the current policy. This yields a hierarchical optimization structure rather than simultaneous gradient descent.
- Core assumption: The critic's objective is strongly convex (guaranteed under linear value function parameterization with full-rank features; see Theorem 5.2).
- Evidence anchors:
  - [abstract] "The dependency of the actor on the critic in actor-critic (AC) reinforcement learning means that AC can be characterized as a bilevel optimization (BLO) problem, also called a Stackelberg game."
  - [section 5, Eq. 4] Formal BLO formulation with actor as outer and critic as inner optimization.
  - [corpus] Related work on bilevel RL confirms this framing but notes prior instability with conjugate gradient methods (Zheng et al., 2022; Chakraborty et al., 2024).

### Mechanism 2
- Claim: Nested critic updates—fully optimizing the critic to best-response before each actor step—provide a stable value function estimate that reflects the current policy.
- Mechanism: For each outer iteration, run multiple inner gradient steps (warm-started from the previous solution) to approximately solve the critic's objective, then compute the actor's hypergradient. This contrasts with standard PPO's simultaneous single-step updates for both actor and critic.
- Core assumption: Sufficient inner iterations (O(κ) where κ is the condition number) to achieve an accurate best-response; warm-starts reduce the required iterations.
- Evidence anchors:
  - [abstract] "First, the critic's update should be nested to learn a best response to the actor's policy."
  - [Algorithm 1] Inner loop runs Kω iterations before outer update; Lemma 4.5 bounds the distance between successive inner solutions.
  - [corpus] Similar nesting was attempted in prior work (Zheng et al., 2022) but failed due to IHVP instability.

### Mechanism 3
- Claim: The Nyström method approximates the inverse Hessian-vector product (IHVP) more stably than conjugate gradient by computing a low-rank approximation, avoiding ill-conditioning issues.
- Mechanism: Sample q columns of the Hessian proportional to diagonal magnitude; construct a low-rank approximation H_q = H[:,Q]H[Q,Q]^†H[:,Q]^⊤; apply Woodbury identity to compute (H_q + αI)^{-1}v in O(pq²) time without materializing the full Hessian. This bypasses iterative refinement that amplifies numerical errors under poor conditioning.
- Core assumption: The Hessian is approximately low-rank locally; regularization α is chosen to balance stability and curvature fidelity.
- Evidence anchors:
  - [abstract] "Computing this hypergradient involves finding an inverse Hessian vector product, a process that can be numerically unstable. We thus propose... BLPO, which... leverages the Nyström method to compute the hypergradient."
  - [section 3, Eq. 6-7] IHVP definition and Woodbury derivation.
  - [section 6, Figure 5, Appendix A.2] BLPO-CG underperforms and is less stable than BLPO-Nyström across tasks.
  - [corpus] Hataya & Yamada (2023) introduced Nyström for IHVP in bilevel optimization; corpus confirms low-rank Hessians are empirically observed in neural networks (Ghorbani et al., 2019).

## Foundational Learning

- **Bilevel Optimization & Stackelberg Games**
  - Why needed here: BLPO frames actor-critic as a leader-follower game; understanding this hierarchy clarifies why nested updates and hypergradients are natural.
  - Quick check question: In a Stackelberg game, does the follower observe the leader's action before optimizing? (Yes—this is why the critic is nested.)

- **Hypergradients & Implicit Function Theorem**
  - Why needed here: The actor's update requires differentiating through the critic's optimal solution; the IFT yields the implicit gradient without unrolling.
  - Quick check question: Why does the implicit gradient involve the inverse Hessian of the inner objective? (Because ∂y*/∂x = -(∇²_yy g)^{-1} ∇²_xy g.)

- **Nyström Low-Rank Approximation**
  - Why needed here: Direct Hessian inversion is infeasible; Nyström provides a tractable approximation with controlled error.
  - Quick check question: If you sample q columns uniformly at random, what happens to the approximation if the Hessian's eigenvalues decay slowly? (Error increases; importance sampling by diagonal magnitude mitigates this.)

## Architecture Onboarding

- **Component map**: Actor network π_θ (policy) -> Critic network V_ω (value) -> Nyström IHVP module -> Advantage estimator (GAE) -> Rollout buffer

- **Critical path**:
  1. Collect rollouts with current policy π_θ
  2. Compute advantages using V_ω and GAE
  3. For each mini-batch: run K_ω inner critic updates (warm-started)
  4. Compute IHVP via Nyström (sample q columns, regularize with α)
  5. Compute hypergradient: ∇_θ J - (∇²_θω L) · v_Nyström
  6. Update θ via gradient ascent on hypergradient

- **Design tradeoffs**:
  - Inner iterations (K_ω) vs. wall-clock time: more iterations improve critic accuracy but slow training; paper finds K_ω ∈ {3,10} sufficient
  - Nyström rank (q) vs. approximation error: q=5 worked empirically; larger q improves accuracy but increases overhead
  - Regularization α: smaller α preserves curvature but risks instability; paper uses α=50, lower than prior CG-based methods (500–10,000)
  - Linear vs. nonlinear critic: theory assumes linear for strong convexity; practice uses MLPs—guarantees become approximate

- **Failure signatures**:
  - Exploding gradients / NaN loss: α too small or q too low for the Hessian rank; increase α or q
  - Critic loss diverges: inner learning rate η_ω too high or insufficient warm-start; reduce η_ω or increase K_ω
  - Actor underperforms PPO: hypergradient clipping or bounding too aggressive; check CLIP_F and IHVP_BOUND hyperparameters
  - Slow convergence: CG-based variant stalls on ill-conditioned Hessians; confirm Nyström is enabled

- **First 3 experiments**:
  1. **Ablation on inner iterations**: Run BLPO with K_ω ∈ {1, 3, 10} on Walker2d; plot episodic return and critic loss. Expect K_ω=1 to underperform, K_ω≥3 to match or exceed PPO.
  2. **Nyström vs. CG comparison**: On Hopper and Pusher, compare BLPO-Nyström vs. BLPO-CG (same hyperparameters); log IHVP computation time and gradient variance. Expect Nyström to be faster and more stable.
  3. **Rank sensitivity**: Vary q ∈ {3, 5, 10, 20} on a continuous control task; measure approximation error ‖H^{-1}v - H_q^{-1}v‖ and final return. Expect diminishing returns beyond q=5 if Hessian is low-rank.

## Open Questions the Paper Calls Out
- Can BLPO scale effectively to larger neural network architectures and more complex environments (e.g., Atari games with visual inputs)?
- Does BLPO's convergence guarantee hold in practice when the critic is parameterized by deep neural networks rather than linear function approximation?
- Can the Nyström-based hypergradient approach improve performance in other bilevel optimization problems such as RLHF, meta-learning, and hyperparameter optimization?

## Limitations
- Theoretical convergence guarantees rely on linear critic parameterization, but experiments use MLPs
- Claims about generalization to Atari or larger networks are untested
- Performance comparisons rest solely on the paper's experiments as the corpus lacks independent benchmarks of BLPO

## Confidence
- **High** for core mechanism (bilevel formulation with nested critic updates and Nyström IHVP) and empirical gains over PPO on tested tasks
- **Medium** for theoretical convergence proof (restricted to linear critics) and stability claim relative to CG (based on single paper's ablation)
- **Low** for claims about generalization to Atari or larger networks, as these were not tested

## Next Checks
1. **Inner-iteration sensitivity**: Run BLPO with K_ω ∈ {1, 3, 10} on Walker2d; confirm K_ω=1 underperforms and K_ω≥3 matches or exceeds PPO.
2. **Nyström vs. CG stability**: On Hopper and Pusher, compare BLPO-Nyström vs. BLPO-CG; log IHVP computation time and gradient variance to confirm Nyström is faster and more stable.
3. **Rank sensitivity**: Vary q ∈ {3, 5, 10, 20} on a continuous control task; measure approximation error ‖H⁻¹v - H_q⁻¹v‖ and final return to confirm diminishing returns beyond q=5.