---
ver: rpa2
title: Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition
arxiv_id: '2506.17792'
source_url: https://arxiv.org/abs/2506.17792
tags:
- sharp
- prism
- state
- refinement
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SHARP, a hierarchical adaptive refinement approach
  for accelerating policy synthesis in large Markov decision processes (MDPs). The
  method addresses the state-space explosion problem by dynamically partitioning the
  state space and iteratively refining only the most critical regions based on value
  spread criteria.
---

# Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition

## Quick Facts
- arXiv ID: 2506.17792
- Source URL: https://arxiv.org/abs/2506.17792
- Authors: Alexandros Evangelidis; Gricel Vázquez; Simos Gerasimou
- Reference count: 40
- Primary result: Hierarchical adaptive refinement achieves up to 2× speedup over PRISM's standard solvers for MDP policy synthesis

## Executive Summary
This paper introduces SHARP, a hierarchical adaptive refinement approach for accelerating policy synthesis in large Markov decision processes. The method addresses state-space explosion by dynamically partitioning the state space and iteratively refining only critical regions based on value spread criteria. Implemented as a PRISM extension, SHARP demonstrates significant performance improvements while maintaining solution accuracy on warehouse robotics scenarios with up to 1 million states.

## Method Summary
SHARP constructs a hierarchy tree where each node represents a partition of the state space. The approach solves local sub-MDPs with boundary conditions and propagates refined values up the hierarchy. The method uses value spread criteria to identify which regions require further refinement, focusing computational effort on the most critical areas. This hierarchical decomposition allows for efficient handling of large MDPs by avoiding uniform refinement across the entire state space.

## Key Results
- Achieves up to 2× speedup over PRISM's standard solvers
- Maintains solution accuracy while improving efficiency
- Demonstrates near-linear scaling behavior as problem size increases
- Shows particularly significant improvements for reward-based properties

## Why This Works (Mechanism)
The hierarchical decomposition reduces computational complexity by focusing refinement efforts only on critical regions identified through value spread analysis. By solving smaller sub-MDPs with boundary conditions rather than the full problem at once, SHARP exploits the structure of the state space to avoid unnecessary computations in regions where precise solutions are not required for overall accuracy.

## Foundational Learning
- Markov Decision Processes (MDPs): Why needed - fundamental model for stochastic decision-making; Quick check - can represent both states and actions with probabilistic transitions
- Value iteration methods: Why needed - core technique for solving MDPs; Quick check - iteratively updates value estimates until convergence
- Hierarchical decomposition: Why needed - enables scalable solution of large MDPs; Quick check - partitions problem into smaller, manageable subproblems
- Boundary conditions: Why needed - ensure consistency between sub-MDPs; Quick check - values at partition boundaries must match
- Value spread analysis: Why needed - identifies regions requiring refinement; Quick check - measures variation in value estimates within partitions

## Architecture Onboarding
Component map: State space -> Hierarchical partition tree -> Sub-MDP solvers -> Value propagation -> Refined solution

Critical path: Initial partition → Local sub-MDP solving → Value spread calculation → Refinement decision → Boundary value propagation

Design tradeoffs: Granularity vs. computational cost, accuracy vs. speedup, partition strategy vs. problem structure compatibility

Failure signatures: Poor partitioning leading to excessive boundary conditions, inappropriate value spread thresholds causing over/under-refinement, scalability issues with irregular state spaces

First experiments:
1. Verify SHARP produces identical solutions to PRISM on small test cases
2. Test convergence behavior on problems with known optimal policies
3. Measure speedup and accuracy trade-offs on progressively larger MDPs

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions, but several areas warrant investigation including performance on non-grid-based problems, behavior on industrial-scale MDPs, and comparison with alternative decomposition methods.

## Limitations
- Evaluation focused primarily on warehouse robotics domain
- Limited benchmarking against other hierarchical or decomposition approaches
- Arbitrary value spread threshold (0.01) may not generalize across problem instances
- No analysis of initial partitioning strategy impact on overall efficiency

## Confidence
- Performance improvements (2× speedup): Medium - based on limited experimental scope
- Accuracy preservation: High - demonstrated through solution comparison with baseline
- Near-linear scaling: Medium - requires validation on larger problem instances
- Hierarchical partitioning effectiveness: Medium - depends on domain characteristics

## Next Checks
1. Test SHARP on diverse MDP domains beyond warehouse robotics, including non-grid-based problems and larger state spaces
2. Compare against other decomposition methods (e.g., approximate aggregation, bisimulation) to establish relative performance
3. Conduct sensitivity analysis on the value spread threshold and partitioning strategies to understand their impact on solution quality and efficiency