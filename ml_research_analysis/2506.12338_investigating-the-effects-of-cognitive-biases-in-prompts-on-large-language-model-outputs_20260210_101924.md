---
ver: rpa2
title: Investigating the Effects of Cognitive Biases in Prompts on Large Language
  Model Outputs
arxiv_id: '2506.12338'
source_url: https://arxiv.org/abs/2506.12338
tags:
- biases
- cognitive
- bias
- prompts
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study examines how cognitive biases in user prompts affect\
  \ LLM outputs, introducing a framework that systematically injects confirmation\
  \ and availability biases into prompts and measures their impact on model accuracy.\
  \ Using datasets like BIG-Bench Hard and FinQA, experiments with GPT-3.5, GPT-4,\
  \ Vicuna, and Mistral show that biased prompts significantly reduce LLM accuracy\u2014\
  sometimes by over 20%\u2014compared to unbiased baselines."
---

# Investigating the Effects of Cognitive Biases in Prompts on Large Language Model Outputs

## Quick Facts
- **arXiv ID**: 2506.12338
- **Source URL**: https://arxiv.org/abs/2506.12338
- **Reference count**: 0
- **Primary result**: Biased prompts significantly reduce LLM accuracy by over 20% compared to unbiased baselines

## Executive Summary
This study examines how cognitive biases in user prompts affect LLM outputs, introducing a framework that systematically injects confirmation and availability biases into prompts and measures their impact on model accuracy. Using datasets like BIG-Bench Hard and FinQA, experiments with GPT-3.5, GPT-4, Vicuna, and Mistral show that biased prompts significantly reduce LLM accuracy—sometimes by over 20%—compared to unbiased baselines. Attention weight analysis reveals that biases shift model focus toward incorrect answer options, indicating a mechanism for unreliable outputs. The research highlights the need for bias-aware prompt design and mitigation strategies to enhance the robustness and reliability of LLM applications.

## Method Summary
The research introduces a systematic framework for injecting cognitive biases into prompts and measuring their effects on LLM outputs. The methodology involves creating biased and unbiased prompt pairs, using datasets like BIG-Bench Hard and FinQA for evaluation, and analyzing attention weights to understand bias mechanisms. Experiments were conducted across multiple LLMs including GPT-3.5, GPT-4, Vicuna, and Mistral. The framework specifically targets confirmation and availability biases, comparing model performance on identical tasks with and without bias injection to quantify the impact on accuracy.

## Key Results
- Biased prompts reduce LLM accuracy by over 20% compared to unbiased baselines
- Attention weight analysis shows biased prompts shift model focus toward incorrect answer options
- Effects observed across multiple models (GPT-3.5, GPT-4, Vicuna, Mistral) and datasets (BIG-Bench Hard, FinQA)

## Why This Works (Mechanism)
The mechanism behind bias effects operates through the attention mechanism in transformer-based LLMs. When cognitive biases are present in prompts, they create stronger activation patterns that compete with and override correct reasoning pathways. The attention weights show that biased information receives disproportionate focus, effectively hijacking the model's reasoning process. This occurs because LLMs are trained to predict the next token based on patterns in the prompt, including emotional and cognitive cues that may not be explicitly rational but strongly influence token selection probabilities.

## Foundational Learning
- **Cognitive bias injection methodology**: Understanding how to systematically introduce biases into prompts for controlled experiments
  - Why needed: Enables reproducible testing of bias effects across different models and tasks
  - Quick check: Can you create biased and unbiased versions of the same prompt that differ only in bias content?

- **Attention weight analysis**: Using attention mechanisms to understand how models process biased information
  - Why needed: Provides insight into the internal reasoning process and identifies where biases interfere
  - Quick check: Do attention weights show increased focus on incorrect options when biases are present?

- **Dataset selection for bias testing**: Choosing appropriate benchmarks (BIG-Bench Hard, FinQA) that allow controlled bias injection
  - Why needed: Ensures experiments measure specific bias effects rather than general model limitations
  - Quick check: Does the dataset contain clear correct/incorrect answers that can be manipulated by bias?

## Architecture Onboarding

**Component Map**: Prompt Generation -> Bias Injection -> LLM Processing -> Output Evaluation -> Attention Analysis

**Critical Path**: The core workflow involves generating task prompts, systematically injecting biases, running through LLMs, evaluating accuracy against ground truth, and analyzing attention patterns to identify bias mechanisms.

**Design Tradeoffs**: The study prioritizes controlled experimental conditions over real-world complexity, sacrificing ecological validity for clear causal inference. This tradeoff enables precise measurement of bias effects but may limit generalizability to naturally occurring biased prompts.

**Failure Signatures**: Significant accuracy drops (>20%) when comparing biased vs. unbiased prompts, attention weight shifts toward incorrect options, and systematic degradation across multiple model types and datasets indicate successful bias injection rather than random noise.

**3 First Experiments**:
1. Replicate the BIG-Bench Hard accuracy comparison between biased and unbiased prompts
2. Conduct attention weight analysis to verify shifted focus toward incorrect options
3. Test bias effects across different prompting strategies (few-shot vs. zero-shot)

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize to open-domain or real-world user interactions where prompt complexity varies significantly
- Findings are limited to structured decision-making and mathematical reasoning tasks, not tested on open-ended generation
- Mechanistic explanation through attention weights is correlative rather than definitively causal

## Confidence
- **High confidence**: Bias injection methodology and controlled experiments showing accuracy reduction (>20%) are well-established and reproducible
- **Medium confidence**: Generalization of findings to real-world scenarios and diverse prompt types requires additional validation
- **Medium confidence**: Attention weight analysis provides suggestive but not definitive mechanistic evidence

## Next Checks
1. Replicate experiments across additional diverse datasets including open-ended generation tasks to assess generalizability beyond structured reasoning
2. Test bias effects using different prompting strategies (few-shot, chain-of-thought, system prompts) to understand interaction effects
3. Conduct user studies with real-world prompts to validate laboratory findings in practical deployment scenarios and measure actual bias manifestation in natural usage patterns