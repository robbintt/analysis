---
ver: rpa2
title: RDF-Based Structured Quality Assessment Representation of Multilingual LLM
  Evaluations
arxiv_id: '2504.21605'
source_url: https://arxiv.org/abs/2504.21605
tags:
- context
- knowledge
- multilingual
- llms
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an RDF-based framework for systematically
  evaluating multilingual LLM quality across four context conditions (complete, incomplete,
  conflicting, and no context) in German and English. The approach uses a structured
  vocabulary to capture responses and assess knowledge leakage, where models favor
  training data over provided context.
---

# RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations

## Quick Facts
- arXiv ID: 2504.21605
- Source URL: https://arxiv.org/abs/2504.21605
- Reference count: 5
- Introduces RDF-based framework for systematic multilingual LLM evaluation across four context conditions

## Executive Summary
This paper presents an RDF-based framework for systematically evaluating multilingual LLM quality by detecting knowledge leakage and context prioritization behaviors. The approach uses a structured vocabulary to capture responses across four experimental conditions (complete, incomplete, conflicting, and no context) in German and English. Through experiments with GPT-4o-mini and Gemini-2.0-Flash in the fire safety domain, the framework demonstrates that models predominantly adhere to provided context (89-93% replication rate) even when incorrect, with language-specific performance differences emerging in handling incomplete information and baseline knowledge.

## Method Summary
The framework defines an RDF vocabulary at http://purl.org/sqare# with 14 classes and 57 properties to represent questions, answers, context materials, and validation results. The experimental design uses four context conditions per question: complete (correct context), incomplete (missing information), conflicting (incorrect context), and no context. Two models (GPT-4o-mini, Gemini-2.0-Flash) were evaluated in German and English across 28 fire safety domain questions using zero-shot, system-first prompting. Responses were stored as RDF triples and validated against fire safety standards, with paired statistical analysis using McNemar's exact test, Δ-accuracy with 95% Newcombe CI, and Cohen's κ for agreement.

## Key Results
- Models predominantly adhere to given context (89-93% replication rate) even when incorrect
- English models handle incomplete information better; German models show stronger baseline knowledge without context
- Paired statistical comparisons showed no significant differences between models in most conditions
- Cohen's κ indicated high agreement in error replication across conditions
- RDF vocabulary demonstrated sufficient expressiveness to capture all assessment facets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured RDF representation enables systematic detection of context adherence versus training knowledge leakage in LLMs.
- Mechanism: The vocabulary links :Question → :Answer → :Material → :ValidationResult through explicit relationships (:hasGivenFor, :hasUsedMaterial, :hasValidationResult), allowing SPARQL queries to identify when models replicate provided context (correct or incorrect) versus drawing from parametric training knowledge.
- Core assumption: Assumption: Knowledge conflicts manifest as detectable deviations between provided context and model responses that can be captured through binary validity labels and relationship tracing.
- Evidence anchors:
  - [abstract] "This structured representation enables the comprehensive analysis of knowledge leakage-where models favor training data over provided context"
  - [section 3] "Relationships such as :hasGivenFor (:Question to :Answer), :hasUsedMaterial (:Answer to :Material), and :hasValidationResult (:Answer to :ValidationResult) enable SPARQL queries for analyzing knowledge leakage"
  - [corpus] Related work on structured relevance assessment (arxiv 2507.21287) supports structured evaluation frameworks for RALMs but does not specifically validate this RDF approach.
- Break condition: If models produce responses that cannot be traced to either context or known training patterns, or if validation criteria cannot be formalized as boolean predicates.

### Mechanism 2
- Claim: Four-context experimental design (complete, incomplete, conflicting, no-context) isolates context prioritization behavior from baseline knowledge.
- Mechanism: By systematically varying context completeness and correctness while holding questions constant, the framework creates controlled conditions where model responses reveal whether they defer to provided context or training data. The conflicting condition specifically exposes error replication rates.
- Core assumption: Assumption: Models behave consistently enough within each condition that response patterns generalize beyond individual questions.
- Evidence anchors:
  - [abstract] "revealing critical patterns in context prioritization... models predominantly adhere to given context (89-93% replication rate) even when incorrect"
  - [section 4] "These conditions were chosen to assess how LLMs prioritize context versus training knowledge, revealing behaviors like context adherence or knowledge leakage"
  - [corpus] Limited corpus validation; neighbor papers focus on RAG improvements rather than this specific four-context isolation methodology.
- Break condition: If models exhibit high variance across questions within the same condition, making aggregate patterns unreliable for inference.

### Mechanism 3
- Claim: Language-tagged literals combined with paired statistical comparison expose multilingual performance asymmetries.
- Mechanism: The RDF vocabulary supports language-specific literals (:hasText with language tags), enabling separate analysis pipelines per language. Paired McNemar tests and Cohen's κ across languages then quantify agreement patterns and accuracy differences between models.
- Core assumption: Assumption: Observed language differences reflect underlying model training distributions rather than translation artifacts in the test set.
- Evidence anchors:
  - [abstract] "English models handling incomplete information better and German models showing stronger baseline knowledge without context"
  - [section 4.1] "In English, only the no_context condition shows a significant discordance (p = 0.0039), with GPT-4o-mini outperforming Gemini by 32.1 pp"
  - [corpus] Neighbor paper on multilingual RAG (arxiv 2504.04771) confirms multilingual retrieval heterogeneity challenges but does not validate this specific finding.
- Break condition: If language effects are confounded by prompt translation quality or domain terminology differences across languages.

## Foundational Learning

- Concept: RDF triples and SPARQL query patterns
  - Why needed here: The entire framework is built on subject-predicate-object triples; understanding how to traverse :Question → :Answer → :ValidationResult chains is essential for analyzing results.
  - Quick check question: Can you write a SPARQL query that returns all answers where :isValid is false and the associated :Material was marked as conflicting?

- Concept: McNemar's test for paired nominal data
  - Why needed here: The statistical comparison between models relies on McNemar's exact test to detect significant discordance in paired correctness labels.
  - Quick check question: Given a 2×2 contingency table with cells (a=27, b=1, c=0, d=0), why would McNemar's test not be meaningful?

- Concept: Knowledge conflict in LLMs (parametric vs. contextual knowledge)
  - Why needed here: The framework's core purpose is detecting when models prioritize training-based knowledge over provided context, which requires understanding this distinction.
  - Quick check question: If a model correctly answers a question in the "no context" condition but incorrectly answers the same question in the "conflicting context" condition by following the incorrect context, what behavior does this demonstrate?

## Architecture Onboarding

- Component map:
  - T-Box schema (14 classes, 57 properties) defines the evaluation vocabulary at http://purl.org/sqare#
  - Language-tagged literals enable multilingual response storage
  - OWL/SHACL constraints enforce data integrity
  - SPARQL endpoint enables analytical queries over collected responses
  - Validation module assigns :isValid based on domain standards and context expectations

- Critical path:
  1. Define questions and materials in RDF with language tags
  2. Execute model inference across four context conditions
  3. Store responses as :Answer instances linked to :Question and :Material
  4. Apply validation criteria to generate :ValidationResult
  5. Run SPARQL queries to compute leakage rates, agreement metrics, and cross-lingual patterns

- Design tradeoffs:
  - RDF expressiveness vs. query complexity: SPARQL enables rich analysis but requires expertise vs. simple CSV tables
  - Binary validity labels vs. nuanced scoring: Simplicity of analysis vs. granularity of quality assessment
  - 28-question pilot vs. larger scale: Demonstrates vocabulary sufficiency but limits statistical power (wide CIs noted in paper)

- Failure signatures:
  - Undefined Cohen's κ in conditions with zero variance (e.g., all models correct or all wrong)
  - McNemar test inapplicable when b + c < 5 discordant pairs
  - Wide confidence intervals (e.g., ±25 pp in German incomplete condition) indicating insufficient sample size

- First 3 experiments:
  1. Replicate the fire safety experiment with a different domain (e.g., medical guidelines) to validate vocabulary generalizability—confirm that all assessment facets remain expressible.
  2. Scale question set from 28 to 200+ to reduce confidence interval widths and enable meaningful McNemar comparisons across all conditions.
  3. Add a low-resource language (e.g., a language with limited training data presence) to test whether the vocabulary captures performance degradation patterns distinct from German/English differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the RDF-based evaluation framework perform when applied to low-resource languages beyond German and English?
- Basis in paper: [explicit] Future work section states intention to "add evaluations for low-resource languages"
- Why unresolved: Current study only tested high-resource languages with established model support; low-resource languages may exhibit different context prioritization patterns due to reduced training data availability.
- What evidence would resolve it: Application of the framework to at least three low-resource languages with systematic comparison of context adherence and knowledge leakage rates against German/English baselines.

### Open Question 2
- Question: What metrics beyond binary correctness can better quantify the severity and nuance of knowledge leakage in LLM responses?
- Basis in paper: [explicit] Authors state future work will "refine leakage metrics" and "define reliability metrics"
- Why unresolved: Current validation uses binary is_valid labels; McNemar tests showed wide confidence intervals (±25pp) and Cohen's κ was undefined in some conditions, suggesting current measures lack granularity for nuanced leakage assessment.
- What evidence would resolve it: Development and validation of continuous leakage scores against human expert judgments across multiple domains.

### Open Question 3
- Question: Does the observed 89-93% context adherence rate (replicating incorrect information) generalize beyond the fire safety domain?
- Basis in paper: [inferred] Study limited to single domain with "well-defined knowledge base"; authors mention extending to "new domains" but cross-domain validation absent
- Why unresolved: Fire safety has authoritative, settled knowledge; domains with contested or rapidly-changing knowledge (e.g., law, medicine) may produce different conflict resolution patterns.
- What evidence would resolve it: Replication across diverse domains with varying knowledge characteristics (settled vs. contested, stable vs. rapidly-evolving).

## Limitations

- Statistical power limitations due to small sample size (28 questions) resulted in wide confidence intervals (±25pp) and undefined Cohen's κ in some conditions
- Framework generalizability untested beyond fire safety domain with its well-defined, authoritative knowledge base
- Potential prompt translation artifacts not controlled for, which could confound observed language differences

## Confidence

- High confidence: RDF vocabulary expressiveness and systematic detection of context adherence vs. training knowledge
- Medium confidence: Multilingual performance patterns due to limited sample size and unknown translation quality
- Low confidence: Statistical significance claims where McNemar's test is undefined or shows insufficient discordant pairs

## Next Checks

1. **Vocabulary Generalizability**: Apply the RDF framework to a medical or legal domain with different terminology density and complexity to verify that all assessment facets remain expressible
2. **Statistical Power Validation**: Scale the experiment to 200+ questions and re-run paired comparisons to verify whether current medium-confidence findings persist with tighter confidence intervals
3. **Translation Artifact Control**: Implement back-translation validation for German/English prompts and verify that language differences persist when prompt quality is equalized