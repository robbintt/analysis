---
ver: rpa2
title: 'PiCME: Pipeline for Contrastive Modality Evaluation and Encoding in the MIMIC
  Dataset'
arxiv_id: '2507.03165'
source_url: https://arxiv.org/abs/2507.03165
tags:
- contrastive
- modalities
- modality
- learning
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PiCME, a systematic pipeline to evaluate\
  \ and encode multimodal clinical data from MIMIC. It compares five clinical modalities\u2014\
  text, images, time-series, and structured data\u2014across all combinations in both\
  \ contrastive and supervised learning settings."
---

# PiCME: Pipeline for Contrastive Modality Evaluation and Encoding in the MIMIC Dataset

## Quick Facts
- arXiv ID: 2507.03165
- Source URL: https://arxiv.org/abs/2507.03165
- Reference count: 40
- Primary result: Contrastive learning across clinical modalities matches or exceeds supervised baselines; Modality-Gated LSTM improves performance on 5-modality tasks.

## Executive Summary
PiCME introduces a systematic pipeline to evaluate and encode multimodal clinical data from MIMIC, comparing five clinical modalities—text, images, time-series, and structured data—across all combinations in both contrastive and supervised learning settings. The authors show that three modalities (discharge summaries, chest X-rays, demographics) yield peak performance, with contrastive learning often matching or outperforming supervised baselines. Beyond three modalities, performance plateaus due to integration noise. To address this, they propose the Modality-Gated LSTM, which uses contrastively learned modality weights to dynamically reweight inputs during supervised fine-tuning, improving AUROC from 73.19% to 76.93% and AUPRC from 51.27% to 62.26% in five-modality settings.

## Method Summary
PiCME evaluates multimodal clinical data using five modalities from MIMIC-IV and MIMIC-CXR: discharge summaries, radiology reports, chest X-rays, clinical time-series, and demographics. Each modality uses appropriate encoders (ClinicalBERT with LoRA for text, ResNet for images, LSTM for time-series, MLP for demographics). The pipeline employs contrastive pre-training with InfoNCE (2 modalities) or weighted OvO loss (3+ modalities) using learnable λ weights, followed by three fine-tuning approaches: frozen encoders with MLP, fully supervised from scratch, or Modality-Gated LSTM that uses contrastive λ weights to dynamically reweight modality contributions. Evaluation uses AUROC/AUPRC on in-hospital mortality and phenotype prediction tasks with 80/10/10 splits, averaged over 10 seeds.

## Key Results
- Three modalities (discharge summaries, chest X-rays, demographics) achieve peak performance; adding more modalities yields diminishing returns.
- Contrastive learning matches or exceeds supervised baselines across most modality combinations.
- Modality-Gated LSTM improves 5-modality performance: AUROC increases from 73.19% to 76.93%, AUPRC from 51.27% to 62.26%.
- Text-text pairs show highest alignment (top-5 cosine similarity), while text-time-series pairs show lowest.

## Why This Works (Mechanism)
The pipeline works by first learning robust cross-modal representations through contrastive pre-training, which captures complementary information across modalities without requiring labeled data. The weighted OvO loss with learnable λ parameters allows the model to discover which modality pairs are most informative for downstream tasks. The Modality-Gated LSTM then leverages these pre-trained modality importance weights during fine-tuning, dynamically adjusting how much each modality contributes to the final prediction based on its learned relevance. This approach addresses the noise and redundancy that arise when integrating too many heterogeneous modalities, particularly time-series data which can degrade alignment.

## Foundational Learning

1. **Contrastive Learning with InfoNCE** - why needed: learns representations that bring similar samples closer while pushing dissimilar ones apart across modalities without labels; quick check: verify top-5 cosine similarity between positive pairs exceeds random pairs by >0.2.

2. **Weighted OvO Loss for Multiple Modalities** - why needed: extends InfoNCE to more than two modalities by learning importance weights λ for each modality combination; quick check: ensure λ weights sum to 1 after softmax normalization.

3. **Modality-Gated LSTM Architecture** - why needed: dynamically reweights modality contributions during supervised fine-tuning based on pre-trained importance; quick check: verify λ is correctly applied to cell state update (Cₜ = Fₜ⊙Cₜ₋₁ + (Iₜ⊙C̃ₜ)⊙λ⃗ ).

## Architecture Onboarding

**Component Map**: MIMIC data → Modality Encoders (BERT, ResNet, LSTM, MLP) → Contrastive Pre-training (InfoNCE/OvO) → Fine-tuning (Frozen MLP, Fully Supervised, Modality-Gated LSTM) → Prediction

**Critical Path**: Data preprocessing → Encoder initialization → Contrastive training with weighted OvO loss → Fine-tuning with Modality-Gated LSTM → Evaluation

**Design Tradeoffs**: Using contrastive learning avoids label requirements but requires careful hyperparameter tuning; freezing encoders preserves learned representations but limits task-specific adaptation; the Modality-Gated LSTM adds complexity but enables dynamic modality weighting.

**Failure Signatures**: Poor alignment with 4-5 modalities (top-5 accuracy drops below 0.6); mLSTM underperforming concatenation (AUROC < supervised baseline); high variance across seeds (>3% std).

**First Experiments**: 1) Train 2-modality contrastive model (text-image) and verify alignment improves over epochs; 2) Implement Modality-Gated LSTM and confirm λ weights are correctly normalized; 3) Test 3-modality combination (text-image-demographics) and compare contrastive vs supervised performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Exact sample counts for each modality combination are unspecified, making it difficult to verify reported performance gains.
- Critical implementation details like ResNet variant, ClinicalBERT LoRA rank/α, and time-series preprocessing steps are missing.
- Performance plateaus beyond three modalities suggest fundamental limitations in multimodal integration, particularly with noisy time-series data.

## Confidence
**High confidence**: General pipeline design and observation that three modalities perform best are well-supported.
**Medium confidence**: Specific performance metrics are reported with statistical rigor but exact reproduction is uncertain due to missing details.
**Low confidence**: Exact preprocessing steps and model hyperparameters cannot be verified, making precise reproduction impossible.

## Next Checks
1. Verify time-series preprocessing matches the paper by reproducing exact feature selection, imputation strategy, and normalization for the 48-hour window.
2. Confirm Modality-Gated LSTM correctly applies contrastively learned λ weights to cell state update with proper normalization.
3. Test weighted OvO contrastive loss produces expected alignment patterns (text-text > text-image > text-time-series) across all modality combinations.