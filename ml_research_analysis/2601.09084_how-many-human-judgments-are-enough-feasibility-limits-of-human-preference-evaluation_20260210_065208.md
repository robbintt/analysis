---
ver: rpa2
title: How Many Human Judgments Are Enough? Feasibility Limits of Human Preference
  Evaluation
arxiv_id: '2601.09084'
source_url: https://arxiv.org/abs/2601.09084
tags:
- evaluation
- preference
- human
- judgments
- budget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses a fundamental question in human preference
  evaluation: how many judgments are required to reliably detect small improvements
  between generative models. The core method analyzes statistical feasibility limits
  under heterogeneous prompts, showing that detectability is governed by total KL
  divergence accumulated across judgments.'
---

# How Many Human Judgments Are Enough? Feasibility Limits of Human Preference Evaluation

## Quick Facts
- arXiv ID: 2601.09084
- Source URL: https://arxiv.org/abs/2601.09084
- Authors: Wilson Y. Lee
- Reference count: 40
- Primary result: Proportional allocation is minimax-optimal for diffuse preference signals across heterogeneous prompts

## Executive Summary
This paper addresses the fundamental question of how many human judgments are needed to reliably detect small improvements between generative models. Through rigorous statistical analysis, it establishes feasibility limits for human preference evaluation under heterogeneous prompts, showing that detection power is governed by total KL divergence accumulated across judgments. The work demonstrates that when preference signals are diffuse across prompts - typical of open-ended evaluations like Chatbot Arena - proportional allocation strategies are optimal and cannot be improved by sophisticated allocation methods.

The empirical analysis reveals that a significant portion of well-sampled model comparisons fall into low-signal regimes requiring hundreds of judgments for reliable detection. Specifically, 17.6% of Chatbot Arena model pairs with ≥200 judgments have preference margins |δ| ≤ 0.10, requiring ≥500 judgments for 90% detection power. Similar patterns appear across modalities including image and code generation. The results emphasize that inconclusive or negative human evaluation outcomes often reflect underpowered evaluation rather than true model equivalence, highlighting the critical need to account for effect size, budget, and protocol design in evaluation planning.

## Method Summary
The paper develops a statistical framework for analyzing human preference evaluation power under heterogeneous prompts. It models preference judgments as independent binary outcomes with heterogeneous prompt-level variances, then derives detection power as a function of total KL divergence accumulated across judgments. The theoretical analysis establishes that when preference signals are uniformly distributed across prompts (diffuse signal regime), proportional allocation of judgments across prompts is minimax-optimal. This means no allocation strategy can outperform proportional allocation when the signal is truly diffuse across prompts.

The empirical component validates these theoretical findings using data from existing evaluation platforms. The analysis examines aggregate statistics from Chatbot Arena, MT-Bench, and other evaluation protocols to estimate detection power across different judgment budgets and effect sizes. The work introduces novel metrics for characterizing evaluation feasibility, including the fraction of comparisons falling into low-signal regimes and the judgment requirements for achieving specified detection power levels.

## Key Results
- 17.6% of well-sampled Chatbot Arena model pairs with ≥200 judgments have preference margins |δ| ≤ 0.10, requiring ≥500 judgments for 90% detection power
- MT-Bench achieves 1.5× better detectability through reduced prompt-level variance compared to Chatbot Arena
- 14.2% of image comparisons and 41% of code comparisons with execution feedback fall into low-signal regimes requiring high judgment budgets
- Proportional allocation is minimax-optimal when preference signals are diffuse across prompts

## Why This Works (Mechanism)
The mechanism underlying the theoretical results relies on the concentration of KL divergence under heterogeneous prompts. When preferences are diffuse across prompts, the variance in signal strength across prompts creates a fundamental limit on achievable power gains through sophisticated allocation. The minimax-optimal result emerges because proportional allocation ensures each prompt contributes equally to the total information gain, preventing wasteful over-sampling of low-signal prompts while maintaining adequate coverage of high-signal prompts.

The empirical validation works by analyzing aggregate statistics from existing evaluation platforms to estimate the distribution of preference signals across prompts. By characterizing the variance structure of preferences under different protocols, the analysis can predict detection power requirements for various effect sizes and judgment budgets. The cross-modal comparisons demonstrate that protocol design choices (like prompt selection criteria) significantly impact detection feasibility through their effects on prompt-level variance.

## Foundational Learning

KL divergence and information theory:
Why needed: KL divergence quantifies the information gain from each judgment, directly determining detection power
Quick check: Can you derive the relationship between KL divergence and binary hypothesis testing error rates?

Minimax optimality and game theory:
Why needed: Establishes fundamental limits on achievable performance regardless of allocation strategy
Quick check: Can you explain why proportional allocation minimizes worst-case detection error under diffuse signals?

Heterogeneous variance models:
Why needed: Real-world evaluations exhibit varying signal-to-noise ratios across prompts
Quick check: How does variance heterogeneity affect the power gain achievable through allocation optimization?

Statistical power analysis:
Why needed: Connects judgment budgets to detection probabilities for specific effect sizes
Quick check: Can you compute required sample sizes for achieving 90% power at various effect sizes?

Protocol design trade-offs:
Why needed: Different evaluation protocols create different variance structures affecting feasibility
Quick check: What protocol modifications would reduce prompt-level variance in human evaluations?

## Architecture Onboarding

Component map:
Statistical framework -> Power analysis -> Protocol characterization -> Empirical validation -> Feasibility assessment

Critical path:
1. Model preferences as binary outcomes with prompt-level heterogeneity
2. Compute KL divergence accumulation across judgments
3. Derive detection power as function of budget and effect size
4. Validate predictions using platform aggregate statistics
5. Characterize feasibility across protocols and modalities

Design tradeoffs:
The paper trades detailed per-prompt analysis for aggregate statistical characterization, enabling broader applicability but sacrificing granularity. This approach assumes independence across prompts and constant preference strength, which may not hold in practice but provides tractable analysis. The focus on feasibility limits rather than optimal allocation reflects the practical reality that sophisticated allocation provides limited benefit when signals are diffuse.

Failure signatures:
Underestimation of detection power if prompt-level correlations are ignored; overestimation if effect sizes are assumed constant across prompts; incorrect feasibility conclusions if protocol-specific variance structures differ significantly from analyzed cases.

Three first experiments:
1. Test the predicted power curves by conducting controlled evaluations with systematically varied judgment budgets across different effect sizes
2. Compare proportional allocation against alternative strategies under varying degrees of prompt heterogeneity to identify when sophisticated methods provide advantages
3. Analyze the impact of prompt correlation structures on detection power by modeling and simulating correlated preference outcomes

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes independent binary judgments and constant preference strength across prompts, which may not hold in practice where prompts exhibit correlated preferences and varying signal-to-noise ratios
- Empirical validation relies on aggregate statistics from existing evaluation platforms rather than controlled experiments that could isolate protocol effects from model differences
- Limited sample sizes across modalities make broad cross-modal generalizability claims premature

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Minimax-optimality result for proportional allocation under diffuse signals | High |
| Empirical detection power calculations | Medium |
| Cross-modal generalizability claims | Low |

## Next Checks

1. Conduct controlled experiments varying judgment budgets systematically to validate the predicted power curves across different preference effect sizes
2. Test alternative allocation strategies beyond proportional allocation under different prompt heterogeneity regimes to identify when sophisticated methods provide advantages
3. Perform sensitivity analysis on the independence assumptions by modeling prompt-level correlation structures and their impact on detection power calculations