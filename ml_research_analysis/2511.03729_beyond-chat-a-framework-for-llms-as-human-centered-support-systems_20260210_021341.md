---
ver: rpa2
title: 'Beyond Chat: a Framework for LLMs as Human-Centered Support Systems'
arxiv_id: '2511.03729'
source_url: https://arxiv.org/abs/2511.03729
tags:
- systems
- llms
- human
- support
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a role-based framework for human-centered
  LLM support systems, categorizing them into four roles: companions, coaches, mediators,
  and curators. It compares real-world deployments across domains, identifies cross-cutting
  design principles (transparency, personalization, guardrails, memory with privacy,
  empathy-reliability balance), and outlines evaluation metrics extending beyond accuracy
  to trust, engagement, and longitudinal outcomes.'
---

# Beyond Chat: a Framework for LLMs as Human-Centered Support Systems

## Quick Facts
- **arXiv ID**: 2511.03729
- **Source URL**: https://arxiv.org/abs/2511.03729
- **Reference count**: 17
- **Primary result**: Proposes role-based framework (Companion, Coach, Mediator, Curator) with cross-cutting design principles and evaluation metrics for human-centered LLM support systems.

## Executive Summary
This paper introduces a role-based framework for conceptualizing and designing LLMs as human-centered support systems. Rather than treating LLMs as generic assistants, it categorizes them into four distinct roles—companions for emotional support, coaches for skill development, mediators for conflict resolution, and curators for information synthesis—each with specific design principles and evaluation criteria. The framework emphasizes that effective LLM deployment in sensitive domains requires balancing empathy with reliability, maintaining privacy-preserving memory for continuity, and implementing robust guardrails against risks like hallucination and bias.

The paper provides a comprehensive analysis of real-world deployments across domains, identifies cross-cutting design principles, and proposes evaluation metrics that extend beyond accuracy to include trust, engagement, and longitudinal outcomes. It addresses critical risks including over-reliance, hallucination, bias, privacy exposure, and unequal access, while outlining future directions for unified evaluation, hybrid human-AI models, and cross-domain benchmarking. The goal is to support responsible integration of LLMs in settings where people need accompaniment and guidance, not just answers.

## Method Summary
The paper employs literature synthesis from 17 references and case study analysis of existing systems (Replika, Character.AI, ElliQ, Duolingo Max, Khanmigo, DoNotPay, Perplexity, Consensus, Elicit) to develop a conceptual framework. The methodology is explicitly theoretical rather than empirical, comparing real-world deployments across domains to identify patterns and principles. Evaluation metrics are proposed across three levels: functional (accuracy, latency, stability), human-centered (trust, interpretability, emotional resonance), and longitudinal (engagement, skill progression, well-being, dependency avoidance). Domain-specific instruments like the UCLA Loneliness Scale are mentioned but not operationalized.

## Key Results
- Categorizes LLMs into four distinct roles (Companion, Coach, Mediator, Curator) with context-specific design constraints
- Identifies five core design principles: transparency, personalization, guardrails, memory with privacy, empathy-reliability balance
- Proposes evaluation metrics extending beyond accuracy to include trust, engagement, and longitudinal outcomes
- Analyzes risks including over-reliance, hallucination, bias, privacy exposure, and unequal access
- Outlines future directions spanning unified evaluation, hybrid human-AI models, and cross-domain benchmarking

## Why This Works (Mechanism)

### Mechanism 1: Role-Based Constraint Alignment
The framework maps user intent to specific role profiles that trigger distinct design constraints, optimizing for context-specific outcomes rather than generic helpfulness. This restricts the optimization space, reducing conflicts between goals like empathy and reliability. For example, "emotional presence" is prioritized for Companions while "citation accuracy" is essential for Curators. The core assumption is that users seek fundamentally different types of support that cannot be simultaneously maximized. This mechanism breaks if user intent shifts fluidly within a single session, requiring complex role-switching logic.

### Mechanism 2: Longitudinal Scaffolding via Memory
Systems maintain long-term memory of user interactions to provide scaffolding that supports incremental skill development or emotional continuity. This transforms interactions from discrete transactions into developmental relationships. The mechanism assumes users value and consent to long-term data retention in exchange for personalized support. It breaks if memory retrieval introduces noise or fails to forget outdated preferences, causing the system to appear "clingy" or unresponsive to change.

### Mechanism 3: Uncertainty Mitigation via Hybrid Guardrails
In high-stakes roles, reliability is achieved by combining LLM generation with external verification and explicit uncertainty signaling. Guardrails and transparency act as corrective layers, forcing the system to cite sources or admit limitations. This assumes users can correctly interpret uncertainty disclaimers without losing trust entirely. The mechanism fails if the verification layer is too slow, degrading perceived utility.

## Foundational Learning

- **Concept**: **Zone of Proximal Development (ZPD)**
  - **Why needed here**: Provides theoretical basis for how LLM "Coach" differs from answer engine through scaffolding—providing assistance just beyond current independent capability
  - **Quick check question**: Can you distinguish between a system giving a user the answer vs. providing the "next step" in their learning process?

- **Concept**: **Therapeutic Alliance**
  - **Why needed here**: From psychotherapy, applied to "Companion" role to explain why relationship (trust, bond) is primary mechanism of change, justifying heavy weighting of empathy
  - **Quick check question**: If a user feels "heard" by an AI but receives factually average advice, does the paper suggest this interaction could still be successful? (Yes).

- **Concept**: **Retrieval-Augmented Generation (RAG)**
  - **Why needed here**: Practically necessary for implementing "Curator" and "Mediator" roles to fulfill design principles of citation accuracy and verifiability
  - **Quick check question**: How would you architect a system that must provide citations for every claim it makes, rather than relying on the model's parametric memory?

## Architecture Onboarding

- **Component map**: Role Profiles -> Memory Store -> Guardrail Layer -> Grounding Interface
- **Critical path**: 1) Define the Role (determines empathy vs. reliability balance), 2) Configure Memory/Privacy settings (determines continuity vs. risk), 3) Implement Evaluation (switch from accuracy-only to longitudinal metrics)
- **Design tradeoffs**: 
  - Empathy vs. Accuracy: Companion mode prioritizes sympathetic language over fact-checking; Curator mode does opposite
  - Simplification vs. Nuance: Mediator mode simplifies legal jargon but risks losing critical nuance
  - Memory vs. Privacy: Long-term memory improves Coach outcomes but increases data liability
- **Failure signatures**:
  - "Sycophant" Loop: Companion mode validates harmful delusions to maximize empathy metrics
  - "Amnesiac" Gap: Coach mode forgets user's skill level from previous day, breaking scaffolding
  - "Hallucinated Citation": Curator mode invents DOI or law, bypassing verification checks
- **First 3 experiments**:
  1. Role A/B Testing: Deploy same model as Coach vs. Curator to measure divergence in empathy vs. citation scores
  2. Memory Ablation: Run Coach scenario with memory enabled vs. disabled to measure impact on progress metrics
  3. Hallucination Stress Test: Force Mediator role to answer legal queries with and without RAG to quantify risk reduction

## Open Questions the Paper Calls Out
None

## Limitations
- Framework remains largely conceptual with limited empirical validation
- Evaluation metrics lack operational definitions, thresholds, or standardized protocols
- Access limitations to proprietary systems constrain empirical verification
- Claims about longitudinal outcomes are speculative without concrete measurement protocols

## Confidence

- **High Confidence**: Role identification (Companion, Coach, Mediator, Curator) and core design principles (transparency, personalization, guardrails, memory with privacy, empathy-reliability balance) are well-supported by literature and case studies
- **Medium Confidence**: Theoretical mechanisms (role-based constraint alignment, longitudinal scaffolding, uncertainty mitigation) are logically sound but lack direct empirical validation
- **Low Confidence**: Specific evaluation metrics and implementation remain undefined; longitudinal outcome claims lack validation data

## Next Checks
1. Role A/B Testing Protocol: Deploy same underlying LLM model configured as different roles (Coach vs. Curator) to same user query set, measuring divergence in empathy vs. citation scores
2. Memory Ablation Study: Run Coach scenario with memory enabled vs. disabled, tracking user progress metrics over 4-8 weeks
3. Grounding Interface Efficacy Test: Force Mediator role to answer legal queries with and without retrieval-augmented generation, measuring hallucination rates and citation accuracy