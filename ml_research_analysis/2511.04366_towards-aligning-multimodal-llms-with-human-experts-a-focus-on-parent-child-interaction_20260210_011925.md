---
ver: rpa2
title: 'Towards Aligning Multimodal LLMs with Human Experts: A Focus on Parent-Child
  Interaction'
arxiv_id: '2511.04366'
source_url: https://arxiv.org/abs/2511.04366
tags:
- child
- parent
- interaction
- slps
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores aligning multimodal large language models (MLLMs)
  with speech-language pathologists (SLPs) in analyzing joint attention in parent-child
  interactions. Through interviews and video annotations with three SLPs, the researchers
  identified key behavioral cues (gaze, action, vocalisation) used to assess interaction
  quality.
---

# Towards Aligning Multimodal LLMs with Human Experts: A Focus on Parent-Child Interaction

## Quick Facts
- **arXiv ID**: 2511.04366
- **Source URL**: https://arxiv.org/abs/2511.04366
- **Authors**: Weiyan Shi; Kenny Tsu Wei Choo
- **Reference count**: 40
- **Primary result**: MLLM system achieves 85% accuracy in behavioral description and over 64% accuracy in joint attention evaluation compared to expert labels

## Executive Summary
This study investigates the alignment of multimodal large language models (MLLMs) with speech-language pathologists (SLPs) for analyzing joint attention in parent-child interactions. Through interviews and video annotations with three SLPs, researchers identified key behavioral cues (gaze, action, vocalization) used to assess interaction quality. A two-stage MLLM system was developed: first extracting behavioral descriptions from videos using structured prompting, then evaluating interaction quality using few-shot examples. The MLLM achieved up to 85% accuracy in behavioral description and over 64% accuracy in joint attention evaluation compared to expert labels. The findings demonstrate that alignment is more robust at the observation level where experts share common descriptors than at the judgment level where interpretive criteria diverge.

## Method Summary
The research employed a human-centered approach to develop and evaluate an MLLM system for joint attention analysis. Three SLPs participated in semi-structured interviews to identify key behavioral cues and establish evaluation criteria for parent-child interactions. The team then collected 20 video clips of parent-child interactions and conducted two annotation rounds with the SLPs to establish ground truth labels. A two-stage MLLM system was implemented: Stage 1 used structured prompting to extract behavioral descriptions (gaze, action, vocalization) from videos, while Stage 2 evaluated interaction quality using few-shot examples based on expert-defined criteria. The system's outputs were compared against expert annotations to assess alignment accuracy.

## Key Results
- MLLM achieved up to 85% accuracy in extracting behavioral descriptions from videos
- MLLM demonstrated over 64% accuracy in evaluating joint attention compared to expert labels
- Alignment was more robust at the observation level (behavioral description) than at the judgment level (interaction evaluation)

## Why This Works (Mechanism)
The alignment framework succeeds because it mirrors the expert analysis process: first decomposing complex social interactions into observable behavioral components, then synthesizing these observations into evaluative judgments. The structured prompting approach enables the MLLM to systematically identify and describe key behavioral cues that SLPs consistently recognize. The two-stage architecture allows the model to separate feature extraction from judgment formation, reducing cognitive load and improving accuracy. By grounding the evaluation in few-shot examples from actual experts, the system learns to map behavioral descriptions to clinical assessments in a way that reflects expert reasoning patterns.

## Foundational Learning
**Multimodal Large Language Models (MLLMs)**: AI systems that process and integrate multiple data modalities (text, image, audio) to generate comprehensive outputs. Needed because parent-child interactions involve complex visual, auditory, and temporal dynamics that single-modality models cannot capture. Quick check: Can the model process video frames alongside speech transcripts simultaneously?

**Joint Attention Analysis**: The assessment of how effectively communication partners share focus on objects or events through coordinated gaze, gestures, and vocalizations. Critical for evaluating developmental communication skills. Quick check: Does the model identify triadic attention patterns (parent-child-object) accurately?

**Structured Prompting**: A technique using carefully designed templates and instructions to guide LLM outputs into specific formats or content areas. Essential for ensuring consistent behavioral description extraction. Quick check: Are behavioral descriptions consistently formatted across different video clips?

**Few-shot Learning**: A machine learning approach where models learn new tasks from a small number of examples rather than requiring large training datasets. Important for adapting MLLMs to expert-specific evaluation criteria without extensive retraining. Quick check: Can the model generalize evaluation criteria from 5-10 expert examples to new interactions?

## Architecture Onboarding

**Component Map**: Video Input -> Stage 1 (Behavioral Description Extraction) -> Structured Prompting -> Stage 2 (Interaction Quality Evaluation) -> Few-shot Examples -> Expert Alignment Assessment

**Critical Path**: The most time-sensitive path is Stage 1 to Stage 2 pipeline, as errors in behavioral description directly propagate to evaluation accuracy. The quality of structured prompts and few-shot examples determines overall system performance.

**Design Tradeoffs**: The system trades computational efficiency for accuracy by using two separate stages rather than a single end-to-end model. This increases interpretability and allows for targeted error correction but requires more processing time. The few-shot approach reduces data requirements but may limit generalizability to unseen interaction types.

**Failure Signatures**: 
- Inaccurate gaze tracking leading to incorrect behavioral descriptions
- Misinterpretation of vocal cues due to audio quality issues
- Over-reliance on few-shot examples causing rigid evaluation patterns
- Context misunderstanding when behavioral cues are ambiguous

**First 3 Experiments**:
1. Test behavioral description accuracy across different interaction contexts (play, reading, mealtime)
2. Evaluate model performance with varying numbers of few-shot examples (1-10 examples)
3. Compare structured prompting approaches (zero-shot vs. few-shot prompting)

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of SLPs (n=3) and video clips (n=20) may limit generalizability
- Performance gap between observation stage (85%) and evaluation stage (64%) accuracy
- Focus on single interaction metric (joint attention) may not capture full communication complexity

## Confidence
- **High confidence**: Behavioral description extraction capability (85% accuracy)
- **Medium confidence**: Overall alignment framework and two-stage approach
- **Medium confidence**: Finding that observation-level alignment is more robust than judgment-level alignment

## Next Checks
1. Test alignment framework with larger and more diverse sample of SLPs and parent-child interaction videos across different clinical settings
2. Conduct head-to-head comparison between MLLM-generated descriptions and multiple human annotators to quantify inter-rater reliability
3. Evaluate system performance across different types of parent-child interactions (play, reading, daily routines) to assess robustness across varied contexts