---
ver: rpa2
title: Uncertainty Prioritized Experience Replay
arxiv_id: '2506.09270'
source_url: https://arxiv.org/abs/2506.09270
tags:
- uncertainty
- learning
- epistemic
- replay
- ensemble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel method to prioritize experience replay
  in reinforcement learning by using epistemic uncertainty rather than TD-error. This
  addresses the problem of noisy TV, where PER over-samples high-variance transitions.
---

# Uncertainty Prioritized Experience Replay

## Quick Facts
- arXiv ID: 2506.09270
- Source URL: https://arxiv.org/abs/2506.09270
- Reference count: 40
- The paper introduces a novel method to prioritize experience replay in reinforcement learning by using epistemic uncertainty rather than TD-error.

## Executive Summary
This paper proposes Uncertainty Prioritized Experience Replay (UPER), a novel approach that replaces traditional TD-error prioritization in experience replay with an information gain criterion based on epistemic and aleatoric uncertainty estimates. The method addresses the "noisy TV" problem where standard PER over-samples high-variance transitions, wasting capacity on inherently unpredictable experiences. By using an ensemble of distributional RL agents, UPER can estimate both reducible (epistemic) and irreducible (aleatoric) uncertainty, prioritizing transitions where learning can actually reduce uncertainty rather than sampling noise.

## Method Summary
UPER modifies the standard Prioritized Experience Replay framework by replacing TD-error prioritization with information gain. The method uses an ensemble of N=10 QR-DQN distributional agents with shared CNN trunks and independent quantile heads. For each transition, it computes target epistemic uncertainty (·∫ºŒ¥) and aleatoric uncertainty (√Ç) using variance decomposition across ensemble members. The priority variable is defined as p_i = ¬Ω log(1 + ·∫ºŒ¥/√Ç), which trades off epistemic uncertainty against aleatoric uncertainty. The method maintains computational efficiency through shared trunks and parallel GPU processing, with only a ~2s/iteration overhead compared to standard QR-DQN.

## Key Results
- UPER outperforms standard PER and QR-DQN baselines on the Atari-57 benchmark, particularly on games where PER shows negative impact
- In a modified conal bandit task with varying mean rewards, ·∫ºŒ¥ prioritization outperforms ensemble disagreement alone (·∫º)
- UPER demonstrates improved performance on a noisy gridworld environment compared to TD-error prioritization
- The method effectively avoids over-sampling high-variance transitions that waste learning capacity

## Why This Works (Mechanism)

### Mechanism 1
The information gain prioritization outperforms TD-error prioritization by explicitly separating learnable uncertainty from irreducible noise. The priority variable p_i = ¬Ω log(1 + ·∫ºŒ¥/√Ç) trades off epistemic uncertainty (reducible through learning) against aleatoric uncertainty (inherent noise). This encourages sampling transitions where learning can actually reduce uncertainty, while avoiding noisy transitions that waste capacity. The mechanism relies on a Bayesian Gaussian model approximating the relationship between prior uncertainty, data noise, and posterior information gain.

### Mechanism 2
Target epistemic uncertainty (·∫ºŒ¥) captures model bias that ensemble disagreement alone misses. Standard ensemble variance (·∫º from Clements et al.) measures disagreement but ignores systematic offset from targets. Adding Œ¥¬≤Œò = (Œò - Eœà,œÑ[Œ∏œÑ])¬≤ ensures prioritization accounts for actual distance to ground truth, not just ensemble consistency. This guards against pathological cases where all ensemble members converge to the same wrong estimate.

### Mechanism 3
Distributional ensembles provide tractable estimates of both uncertainty types without separate models. QR-DQN learns quantile representations Œ∏œÑ of return distributions. Across ensemble members œà, variance decomposition gives: √Ç(s,a) = VœÑ[Eœà(Œ∏œÑ)] (aleatoric, from average distribution spread) and ·∫º(s,a) = EœÑ[Vœà(Œ∏œÑ)] (epistemic, from ensemble disagreement). Shared trunk with K heads maintains computational efficiency.

## Foundational Learning

- **Concept**: Epistemic vs. Aleatoric Uncertainty Decomposition
  - **Why needed here**: The entire method hinges on correctly separating reducible (epistemic) from irreducible (aleatoric) uncertainty. Without this distinction, you cannot construct the information gain criterion.
  - **Quick check question**: Given a noisy reward signal with variance œÉ¬≤, can learning reduce that variance? If you said yes, review: aleatoric uncertainty (œÉ¬≤) is inherent to the environment; epistemic uncertainty is about your estimate of the mean, which learning can refine.

- **Concept**: Distributional RL and Quantile Regression
  - **Why needed here**: UPER builds on QR-DQN, which models full return distributions via quantiles. Understanding quantile regression loss œÅœÑ(u) = u(œÑ - ùüô_{u<0}) is necessary to implement the base architecture before adding ensembles.
  - **Quick check question**: Why use quantiles instead of direct variance estimation? If you're unsure, review: quantiles capture full distribution shape, enabling richer uncertainty estimates beyond just variance.

- **Concept**: Prioritized Experience Replay (PER) and Importance Sampling
  - **Why needed here**: UPER modifies only the priority variable pi in the standard PER framework. You must understand the sampling probability P(i) = p_i^Œ± / Œ£p_k^Œ± and the importance weight correction w_i ‚àù (N¬∑P(i))^{-Œ≤} to implement correctly.
  - **Quick check question**: What happens if Œ≤ = 0 throughout training? If you don't know, review: Œ≤ corrects the bias from non-uniform sampling; Œ≤=0 means no correction, leading to biased value estimates.

## Architecture Onboarding

- **Component map**: Input (s, a, r, s', mask m) ‚Üí Shared CNN trunk ‚Üí Action-distributed quantile heads [N ensemble √ó M quantiles] ‚Üí Compute Œ∏œÑ(s,a;œà) for each head ‚Üí Target epistemic uncertainty: ·∫ºŒ¥ = ·∫º + Œ¥¬≤Œò ‚Üí Aleatoric uncertainty: √Ç = VœÑ[Eœà(Œ∏œÑ)] ‚Üí Priority: pi = ¬Ω log(1 + ·∫ºŒ¥/√Ç) ‚Üí Sum-tree prioritized sampling ‚Üí Gradient update with IS weights

- **Critical path**: 
  1. Implement base QR-DQN (single head, M quantiles) and verify on simple environment
  2. Extend to N-head ensemble with random mask storage per transition
  3. Implement uncertainty decomposition (Equations 8, 13)
  4. Replace TD-error priority with information gain in sum-tree
  5. Add IS weight correction with Œ≤ annealing

- **Design tradeoffs**:
  - Ensemble size N: Larger N improves uncertainty estimates but increases memory/compute. Paper uses N=10; likely diminishing returns beyond 5-10.
  - Quantile count M: More quantiles better approximate distributions but increase output dimensionality. Paper uses M=32 (implied from QR-DQN standard).
  - Priority exponent Œ±: Controls prioritization strength. Paper sweeps 0.6-1.0; higher values more aggressively focus on high-uncertainty transitions.
  - Assumption: Shared trunk with separate heads is efficient; full independent networks would be prohibitive.

- **Failure signatures**:
  - Performance worse than uniform replay: ·∫ºŒ¥ estimates near-zero (ensemble collapse) or √Ç near-zero (division instability). Fix: Add small Œµ to denominator; verify masking creates diversity.
  - Slow convergence with high variance: Œ≤ annealing too fast or Œ± too high, causing severe distribution shift. Fix: Slow Œ≤ annealing; reduce Œ±.
  - No improvement over PER in low-noise environments: Expected behavior; UPER advantage comes from avoiding noise.

- **First 3 experiments**:
  1. Conal bandit validation: Reproduce Figure 1b/c with na=5 arms, œÉ_max=2. Confirm UPER samples low-variance arms more than PER.
  2. Ablation on ensemble size: Run Pong with N ‚àà {2, 5, 10, 20} heads. Plot training curves and wall-clock time.
  3. Per-game comparison on Atari-57 subset: Select 5 games where PER ablation shows negative impact. Run UPER vs. PER vs. uniform.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the Uncertainty Prioritized Experience Replay (UPER) framework retain its performance advantages when utilizing non-ensemble uncertainty estimators, such as pseudo-counts, particularly in sparse reward settings?
  - **Basis in paper**: The Discussion section states that "exploring other forms of uncertainty estimation in RL such as pseudo-counts (Lobel et al., 2023)... is a promising research path."
  - **Why unresolved**: The paper relies exclusively on an ensemble of distributional agents to estimate uncertainties; it does not test if the method works with other estimation techniques like pseudo-counts which may behave differently in sparse scenarios.
  - **What evidence would resolve it**: Benchmark results comparing UPER's performance on sparse reward tasks when using pseudo-counts versus the default ensemble distributional estimator.

- **Open Question 2**: Can the information gain prioritization criterion derived for RL be effectively extrapolated to improve data selection in other learning paradigms like supervised or continual learning?
  - **Basis in paper**: The Conclusion claims that "In principle, these concepts can be extrapolated to other learning systems" such as supervised or continual learning.
  - **Why unresolved**: The theoretical derivation and experiments are confined to the RL framework (MDPs), leaving the applicability to non-sequential or stationary data distributions untested.
  - **What evidence would resolve it**: Experiments applying the information gain prioritization (ŒîH_Œ¥) to active learning loops or continual learning benchmarks to observe if it improves sample efficiency or reduces catastrophic forgetting.

- **Open Question 3**: What are the formal trade-offs between different functional forms of the prioritization variable (e.g., E/U vs. E¬≤/U vs. ŒîH) regarding robustness to model bias?
  - **Basis in paper**: Supplementary Material 3.6 discusses the impact of model bias acting as a temperature and notes that "future work could be dedicated to understanding these trade-offs more formally in the context of prioritized replay."
  - **Why unresolved**: While the paper argues for Information Gain (ŒîH), the supplementary material shows that higher-order ratios may handle bias differently, but a formal theoretical comparison is lacking.
  - **What evidence would resolve it**: A theoretical analysis or ablation study comparing the stability and convergence of various ratio forms under controlled conditions of model mis-specification.

## Limitations

- The method relies on ensemble disagreement which may not capture all forms of epistemic uncertainty, particularly in cases of systematic model bias
- The Gaussian approximation used to derive the information gain criterion may not hold in complex RL environments with non-Gaussian return distributions
- Performance improvements are primarily observed in noisy environments, with limited advantage in deterministic settings

## Confidence

- **High**: The mathematical derivation of the information gain criterion from Bayesian principles (Section 3.2)
- **Medium**: The claim that ensemble distributional RL provides tractable epistemic/aleatoric uncertainty estimates without separate models (Section 5 implementation)
- **Low**: The assumption that Œ¥¬≤Œò meaningfully corrects for systematic ensemble bias in all cases (Appendix A)

## Next Checks

1. **Ablation on ensemble diversity**: Run Atari experiments with N=2, 5, 10, 20 heads while monitoring ensemble disagreement (·∫º) and performance. Verify that performance plateaus beyond a certain N, indicating diminishing returns and confirming sufficient diversity.

2. **Ablation on aleatoric uncertainty calibration**: In the conal bandit task, manipulate aleatoric uncertainty estimates (e.g., artificially inflate √Ç) and observe whether prioritization correctly downweights high-noise arms. This validates that √Ç is functioning as intended rather than being ignored.

3. **Domain shift validation**: Test UPER on a low-noise deterministic environment (e.g., Pong) versus a high-noise stochastic environment (e.g., Montezuma's Revenge with added observation noise). Confirm that UPER's advantage appears primarily in the noisy setting, as expected from the theoretical motivation.