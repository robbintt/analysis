---
ver: rpa2
title: 'Utility Boundary of Dataset Distillation: Scaling and Configuration-Coverage
  Laws'
arxiv_id: '2512.05817'
source_url: https://arxiv.org/abs/2512.05817
tags:
- bound
- step
- configuration
- dataset
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes a unified theoretical framework for dataset\
  \ distillation, showing that gradient, distribution, and trajectory matching methods\
  \ all reduce the same generalization error through a bi-level optimization mechanism.\
  \ The key contributions are two scaling laws: (1) a single-configuration law showing\
  \ error decreases as 1/\u221Ak until reaching an irreducible bound, explaining IPC\
  \ saturation; and (2) a coverage law showing that distilled sample size must scale\
  \ linearly with configuration diversity Hcov to maintain accuracy across configurations."
---

# Utility Boundary of Dataset Distillation: Scaling and Configuration-Coverage Laws

## Quick Facts
- **arXiv ID**: 2512.05817
- **Source URL**: https://arxiv.org/abs/2512.05817
- **Reference count**: 40
- **Primary result**: Establishes unified theoretical framework showing dataset distillation methods reduce generalization error through bi-level optimization, with two scaling laws: 1/√k error decay until irreducible bound and linear scaling between distilled sample size and configuration diversity

## Executive Summary
This paper establishes a unified theoretical framework for dataset distillation that demonstrates how gradient matching, distribution matching, and trajectory matching methods all reduce the same generalization error through a bi-level optimization mechanism. The authors derive two key scaling laws: a single-configuration law showing error decreases as 1/√k until reaching an irreducible bound, and a coverage law showing that distilled sample size must scale linearly with configuration diversity to maintain accuracy across configurations. Extensive experiments across MNIST, CIFAR-10/100, and ImageNette datasets validate both laws across multiple distillation methods with high statistical significance.

## Method Summary
The authors develop a theoretical framework that unifies three families of dataset distillation methods (gradient matching, distribution matching, and trajectory matching) by showing they all optimize the same generalization error bound through bi-level optimization. They introduce configuration diversity Hcov to quantify the variety of model architectures and training settings, then derive two scaling laws: the single-configuration law describing how distillation error decreases with distilled sample size k, and the coverage law describing how required distilled sample size scales with configuration diversity. The framework provides theoretical justification for previously observed empirical behaviors like IPC saturation and guides the design of configuration-robust distilled datasets.

## Key Results
- Single-configuration scaling law shows distillation error decreases as 1/√k until reaching an irreducible bound, explaining IPC saturation phenomenon
- Coverage law demonstrates distilled sample size must scale linearly with configuration diversity Hcov to maintain accuracy across diverse configurations
- Empirical validation across multiple methods (DC, DSA, DM, MTT, MGD 3) and datasets (MNIST, CIFAR-10/100, ImageNette) confirms both laws with R² values consistently above 0.85

## Why This Works (Mechanism)
The framework works by establishing that all three major families of dataset distillation methods (gradient matching, distribution matching, and trajectory matching) can be viewed as optimizing the same generalization error bound through bi-level optimization. This unification reveals that the fundamental mechanism behind dataset distillation is reducing the expected generalization error over the true data distribution. The theoretical analysis shows that as distilled sample size increases, the error decreases according to the 1/√k law until hitting an irreducible bound determined by the information loss inherent in the distillation process. For multiple configurations, the coverage law emerges because maintaining performance across diverse model architectures and training settings requires proportionally more distilled samples to adequately represent the configuration space.

## Foundational Learning
- **Bi-level optimization**: Optimization problems with an outer loop optimizing over model parameters and an inner loop optimizing over distilled data; needed to understand how distilled datasets are optimized for downstream tasks, quick check: verify understanding of nested optimization structure
- **Configuration diversity (Hcov)**: A metric quantifying the variety of model architectures and training configurations; needed to measure how many different settings a distilled dataset must support, quick check: calculate Hcov for a given set of architectures
- **Generalization error bounds**: Theoretical limits on how well models trained on distilled data perform on true data; needed to establish the fundamental limits of dataset distillation, quick check: derive error bound for simple linear case
- **Trajectory matching**: A family of methods that match optimization trajectories during training; needed to understand how temporal information in training dynamics can be distilled, quick check: compare trajectory matching vs gradient matching objectives
- **Irreducible bound**: The minimum achievable error in dataset distillation due to information loss; needed to explain why performance plateaus at high distilled sample sizes, quick check: identify factors contributing to irreducible bound
- **IPC saturation**: The phenomenon where increasing distilled sample size yields diminishing returns in performance; needed to explain practical observations in distillation experiments, quick check: plot performance vs distilled sample size to identify saturation point

## Architecture Onboarding

**Component Map**
Theoretical Framework -> Scaling Laws Derivation -> Experimental Validation -> Method Comparison

**Critical Path**
The critical path flows from theoretical analysis of generalization error through derivation of scaling laws to experimental validation. The framework establishes the unified view of distillation methods, derives the 1/√k and linear coverage laws, then validates these predictions across multiple datasets and methods. The most time-consuming step is the comprehensive experimental validation across diverse methods and datasets.

**Design Tradeoffs**
The framework trades theoretical generality for practical applicability by focusing on linear models while empirically validating on deep networks. This approach provides clean theoretical insights but requires additional empirical assumptions for deep learning extension. The configuration diversity metric Hcov provides a unified measure but may not capture all aspects of model complexity. The framework assumes convex losses for theoretical analysis while demonstrating practical utility with non-convex deep learning objectives.

**Failure Signatures**
Violations of the 1/√k scaling law would indicate either non-convexity effects dominating the behavior or implementation issues in the distillation process. Failure of the linear coverage law would suggest the configuration diversity metric Hcov is inadequate for capturing the true complexity of the configuration space. Poor empirical validation across methods would indicate the theoretical unification may not capture all practical nuances of different distillation approaches.

**First 3 Experiments**
1. Verify 1/√k scaling law on MNIST with varying distilled sample sizes (k=10, 50, 100, 200, 500) using DC method
2. Test coverage law by training distilled datasets on diverse architectures (ResNet18, VGG16, MobileNet) and measuring performance degradation
3. Compare different distillation methods (DC, DSA, DM) on CIFAR-10 to validate unified theoretical framework predictions

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Theoretical framework assumes convex loss functions and linear models, which may not fully capture deep neural network behavior
- Limited validation to supervised image classification tasks, leaving applicability to other domains (NLP, RL) unclear
- Configuration diversity metric Hcov may not capture all relevant aspects of model architecture or task complexity variations

## Confidence

**High Confidence:**
- Empirical validation of scaling laws across multiple methods and datasets with R² > 0.85
- Theoretical unification of gradient, distribution, and trajectory matching methods under bi-level optimization
- Linear relationship between distilled sample size and configuration diversity

**Medium Confidence:**
- Theoretical generalization bounds derived for linear models
- Specific form of 1/√k scaling law
- Interpretation of IPC saturation as reaching irreducible bound

**Low Confidence:**
- Complete theoretical extension to deep neural networks without empirical validation
- Universality of configuration diversity metric Hcov across all model variations
- Exact practical implementation details for optimal coverage

## Next Checks
1. Validate scaling laws hold for deeper architectures (ResNet, Vision Transformers) and different training paradigms (self-supervised pretraining)
2. Test framework on non-image datasets including NLP text classification and tabular data to assess cross-domain generalizability
3. Compare proposed configuration diversity metric Hcov with alternative measures of model or task complexity to evaluate robustness and identify improvements