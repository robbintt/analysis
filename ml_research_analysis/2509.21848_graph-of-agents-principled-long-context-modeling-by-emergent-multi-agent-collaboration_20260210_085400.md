---
ver: rpa2
title: 'Graph of Agents: Principled Long Context Modeling by Emergent Multi-Agent
  Collaboration'
arxiv_id: '2509.21848'
source_url: https://arxiv.org/abs/2509.21848
tags:
- context
- answer
- information
- long
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of long context modeling in large
  language models, where inputs often exceed the model's context window. Existing
  approaches like retrieval-augmented generation (RAG) and multi-agent systems (e.g.,
  Chain-of-Agents) rely on heuristic designs that limit generalizability.
---

# Graph of Agents: Principled Long Context Modeling by Emergent Multi-Agent Collaboration

## Quick Facts
- arXiv ID: 2509.21848
- Source URL: https://arxiv.org/abs/2509.21848
- Reference count: 22
- Primary result: GoA improves F1 score by 5.7% over RAG and 16.35% over Chain-of-Agents on long-context QA benchmarks

## Executive Summary
The paper addresses the challenge of long context modeling in large language models, where inputs often exceed the model's context window. Existing approaches like retrieval-augmented generation (RAG) and multi-agent systems (e.g., Chain-of-Agents) rely on heuristic designs that limit generalizability. The authors propose Graph of Agents (GoA), a principled framework that formalizes long context modeling as a compression problem, yielding an information-theoretic objective. GoA dynamically constructs an input-dependent collaboration structure by clustering semantically similar text chunks and building communication paths within each cluster to maximize query relevance.

Experiments on six document question-answering benchmarks show that GoA significantly outperforms both RAG and Chain-of-Agents, improving the average F1 score by 5.7% and 16.35% respectively. Notably, GoA with a 2K context window surpasses a 128K context window Llama 3.1 8B on LongBench, demonstrating a dramatic increase in effective context length. The method's superiority is particularly evident in complex multi-hop reasoning tasks, where flexible collaboration is crucial. Ablation studies confirm that semantic, contextual search is vital for GoA's performance, and the approach is robust to different embedding and clustering methods.

## Method Summary
GoA formalizes long context modeling as a compression problem with an information-theoretic objective. The method chunks input text, embeds chunks using BGE-M3, clusters them via k-medoids (k=4), and builds communication paths within clusters. Within each cluster, a greedy algorithm selects the next chunk based on its similarity to the query conditioned on the current summary. LLM workers sequentially summarize each path, and a manager LLM synthesizes the final answer. The framework uses a linear forest topology (disjoint paths) to balance parallelization and local coherence, avoiding the "forgetting" problem of long chains while maintaining context integration.

## Key Results
- GoA improves average F1 score by 5.7% over RAG and 16.35% over Chain-of-Agents on six LongBench QA benchmarks
- GoA with 2K context window outperforms Llama 3.1 8B with 128K context window on LongBench
- Performance gains are particularly strong on multi-hop reasoning tasks like HotpotQA and 2WikiM
- Ablation studies confirm semantic contextual search is critical for GoA's effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Surrogate Compression Objective
The paper theoretically links maximizing mutual information between the compressed context and answer to maximizing pointwise mutual information (PMI) between the query and compressed context. Since PMI correlates with embedding similarity via InfoNCE training, the system uses cosine similarity in embedding space as a greedy heuristic to guide compression. This assumes embedding similarity is a valid proxy for information gain relevant to the query.

### Mechanism 2: Dynamic Linear Forest Topology
GoA structures agents as a "linear forest" (disjoint paths) rather than a single chain, allowing parallel processing while maintaining local coherence. This reduces the "forgetting" phenomenon observed in long serial chains while avoiding the context-free isolation of fully parallel workers. The approach assumes semantically related chunks can be effectively grouped by clustering algorithms.

### Mechanism 3: Contextual Greedy Selection
Unlike standard RAG which retrieves based on query alone, GoA updates the "search query" implicitly via the evolving summary. The algorithm selects the next chunk that maximizes similarity between the query and the concatenation of current summary with candidate chunk. This ensures new information is relevant given what has already been summarized, assuming a greedy approach suffices to navigate the combinatorial space of chunk orderings.

## Foundational Learning

- **Mutual Information (MI) & Pointwise Mutual Information (PMI)**: The theoretical justification for GoA rests on maximizing MI between the compressed text and the answer. Quick check: How does maximizing the PMI between a query and a compressed context relate to the probability of correctly predicting the answer?

- **InfoNCE and Contrastive Learning**: The paper relies on the property that embeddings trained with InfoNCE approximate PMI. This bridges the gap between a theoretical information objective and a practical vector similarity search. Quick check: Why does the dot product of two embeddings trained with InfoNCE approximate the log-ratio of co-occurrence probabilities?

- **Graph Inductive Biases (Linear Forests)**: GoA constrains the communication graph to be a linear forest to balance latency (parallelism) and reasoning depth (serial context). Quick check: Why is a "linear forest" (a set of disjoint paths) preferable to a single long chain or a fully connected graph for this specific task?

## Architecture Onboarding

- **Component map**: Chunker -> Embedder -> Clusterer -> Path Builder -> Workers (parallel paths) -> Manager
- **Critical path**: The Path Builder is the novel critical component. If this fails to order chunks logically, Workers summarize out-of-order or redundant text, wasting context window.
- **Design tradeoffs**: Number of Subgraphs (k): Low k approaches Chain-of-Agents (slow, high "forgetting"). High k approaches parallel RAG (fast, low context integration). Paper finds k=4 optimal for 8B models. Chunk Size must balance embedding granularity with worker context limits.
- **Failure signatures**: Repetitive Summaries (clustering failed), Missing Info (relevant chunks filtered during clustering), Context Drift (summaries become generic).
- **First 3 experiments**: 1) Run GoA vs. Random-Order GoA to validate greedy selection improves F1. 2) Sweep k∈{1,2,4,8} on HotpotQA to observe reasoning depth vs breadth trade-off. 3) Swap BGE-M3 embedder with GloVe to verify theoretical link between embedding quality and compression objective.

## Open Questions the Paper Calls Out

### Open Question 1
Can the performance of Graph of Agents be improved by relaxing the linear forest constraint to allow for Directed Acyclic Graphs (DAGs) or overlapping clusters? The current implementation restricts agents to a linear forest topology to ensure parallelization, preventing agents in different subgraphs from sharing contextual information during summarization. A comparative study benchmarking the current linear forest against a DAG-based variant on complex multi-hop reasoning tasks would be needed.

### Open Question 2
Can the principled information-theoretic objective be utilized to train a policy network that constructs the collaboration graph more effectively than the current greedy approach? The current framework is training-free, relying on a greedy heuristic to construct the graph. It is unknown if a learned optimization strategy could better approximate the global maximum of the mutual information objective. Experimental results showing the performance of a reinforcement learning agent trained to construct the graph topology against the greedy baseline would be needed.

### Open Question 3
Does dynamically adapting the number of subgraphs (k) based on input semantic density yield better performance than the fixed values used in the study? While the paper identifies a trade-off in the number of subgraphs, it leaves the exploration of an adaptive mechanism for determining k as an unstated implication of the sensitivity analysis. An ablation study where k is determined dynamically per document compared against fixed k values on datasets with high length variance would be needed.

## Limitations
- The theoretical link between the compression objective and practical performance is not rigorously validated with empirical evidence
- The assumption that semantic clustering adequately groups interdependent chunks for multi-hop reasoning remains questionable
- The reliance on greedy selection introduces potential error propagation, but sensitivity to suboptimal early choices is not fully characterized

## Confidence
- **High Confidence**: Empirical results showing GoA outperforms RAG and Chain-of-Agents on tested benchmarks with specific F1 score improvements
- **Medium Confidence**: The claim that semantic contextual search is critical for performance, supported by ablation studies
- **Low Confidence**: The theoretical justification that embedding similarity via InfoNCE approximates PMI and thus the mutual information objective

## Next Checks
1. Compare GoA against a version where chunks are ordered randomly to isolate whether the greedy selection strategy actually improves F1 scores
2. Systematically sweep the number of subgraphs k∈{1,2,4,8} on HotpotQA to quantify the trade-off between reasoning depth and breadth
3. Replace the InfoNCE-trained BGE-M3 embedder with a static embedding method (e.g., GloVe) to test whether the theoretical connection between embedding quality and compression objective is necessary for GoA's performance gains