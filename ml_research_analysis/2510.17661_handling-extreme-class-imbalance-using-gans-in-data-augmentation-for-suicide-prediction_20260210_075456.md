---
ver: rpa2
title: 'Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide
  Prediction'
arxiv_id: '2510.17661'
source_url: https://arxiv.org/abs/2510.17661
tags:
- data
- suicide
- positive
- samples
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses extreme class imbalance in suicide prediction
  by using Generative Adversarial Networks (GANs) to generate synthetic data. Starting
  with a dataset containing only four positive cases out of 656 samples, the researchers
  trained Logistic Regression (LR), Support Vector Machine (SVM), and Random Forest
  (RF) models on both real and GAN-augmented data.
---

# Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction

## Quick Facts
- arXiv ID: 2510.17661
- Source URL: https://arxiv.org/abs/2510.17661
- Reference count: 29
- Using GANs to generate synthetic minority samples enables suicide prediction models to detect previously undetectable cases in extremely imbalanced datasets

## Executive Summary
This study addresses extreme class imbalance in suicide prediction by using Generative Adversarial Networks (GANs) to generate synthetic data. Starting with a dataset containing only four positive cases out of 656 samples, the researchers trained Logistic Regression (LR), Support Vector Machine (SVM), and Random Forest (RF) models on both real and GAN-augmented data. SVM consistently outperformed other models, achieving the best balance between detecting the rare positive cases and minimizing false positives.

## Method Summary
The authors used Conditional GANs to generate synthetic minority samples for suicide prediction, starting with an ABCD dataset containing only 4 positive cases out of 656 samples. Raw survey responses were converted to logit measures using Rasch modeling before GAN generation. Three models (LR, SVM, RF) were trained on both real and GAN-augmented data, with performance evaluated on a test set containing 1 positive and 130 negative cases.

## Key Results
- SVM trained on real data correctly identified one suicide attempt case with 31 false positives
- SVM trained on GAN-generated data achieved the same recall with 33 false positives
- Random Forest failed to detect any positive cases when trained on real data but succeeded when trained on synthetic data, albeit with 85 false positives

## Why This Works (Mechanism)

### Mechanism 1
Conditional GANs (CGAN) can alleviate "zero-recall" failure modes in tree-based models by creating a learnable decision boundary for the minority class. In extreme imbalance (4 positive vs 652 negative), Random Forest (RF) splits optimize for the majority class, effectively ignoring the minority signals. By training a CGAN to generate synthetic minority samples (126 positive instances), the feature space is populated sufficiently for tree-based algorithms to construct meaningful splits that recognize the positive class, rather than defaulting to a majority-vote bias.

### Mechanism 2
Support Vector Machines (SVM) exhibit superior transfer stability from synthetic to real domains compared to Logistic Regression (LR) and RF under extreme imbalance. SVM optimizes a margin-maximizing hyperplane. The paper suggests this margin-based approach is less sensitive to the specific distributional noise introduced by the GAN (e.g., the male-sex bias seen in synthetic data) than the probability-thresholding of LR or the greedy splitting of RF. This allows SVM to maintain high recall (detection) without the extreme explosion of false positives seen in other models.

### Mechanism 3
Rasch modeling transforms sparse survey data into unidimensional logit measures that are more compatible with GAN generation than raw ordinal values. Raw survey responses (ordinal 1-4) are noisy. By converting these into "logit measures" via Rasch modeling, the authors create continuous, normally-distributed-like features (Panic, Suicidal Ideation). This smoothness likely aids the GAN in learning the probability density function more effectively than it would with sparse, discrete integer inputs.

## Foundational Learning

- **Class Imbalance & Metric Selection**: With 652 negative and 4 positive samples, "Accuracy" is a dangerous trap (a model predicting "No Suicide" 100% of the time has 99.4% accuracy). Why does the paper prioritize "Macro Recall" over "Weighted F1" for evaluating the GAN's success?
- **Conditional GANs (CGAN)**: A standard GAN generates random samples mimicking the dataset. A *Conditional* GAN allows the authors to force the generator to create specifically *positive* (minority) cases, which is the only way to balance the dataset for training. What additional input is fed to the Generator and Discriminator in a CGAN compared to a standard GAN?
- **Overfitting vs. Generalization in Synthetic Data**: Models trained on GAN data (RF_G) showed high false positives on real data. This indicates the model learned the synthetic distribution but struggled to generalize that "learned risk" perfectly to real humans. If a model achieves 100% Recall on the real test set but has 85 False Positives, is it clinically useful for suicide prevention?

## Architecture Onboarding

- **Component map**: ABCD Dataset (Raw Survey) -> Rasch Modeling (Logits) -> Train/Test Split -> Conditional GAN (Generator learns Minority Distribution) -> Synthetic Set (Balanced) -> Grid-Search Pipelines (LR, SVM, RF) -> Trained Models (Real vs. Synthetic versions) -> Real Test Set (1 Positive, 130 Negative) -> Metrics (Macro Recall, Confusion Matrix)
- **Critical path**: The validation of GAN output is the highest risk. The paper notes the GAN failed to match the 'sex' feature distribution (male bias). An engineer must monitor feature drift here; if the synthetic data diverges too far from real data, downstream models (like RF_G) will produce excessive False Positives.
- **Design tradeoffs**: The architecture favors finding the "one in a million" case (Suicide Attempt) at the cost of flagging many false alarms. RF on Real data = 0 False Positives but misses the suicide case (Useless). RF on GAN data = Detects case but 85 False Positives (Costly). SVM offers the middle ground (Detects case, 33 False Positives).
- **Failure signatures**: Silent Failure: RF trained on Real Data (Sensitivity 0.0, Specificity 1.0). The model looks "accurate" but fails the primary objective. Noise Amplification: RF_G high false positive rate (>60%), indicating the model learned artifacts of the GAN generation process rather than true clinical signals. Feature Collapse: The GAN's inability to replicate the 50/50 sex ratio, potentially biasing models toward male-specific patterns.
- **First 3 experiments**: 1) Baseline Audit: Train LR/RF/SVM on the raw, imbalanced data (no GAN). Verify that RF fails to predict the positive class (Confirming the problem). 2) GAN Fidelity Check: Generate synthetic samples and plot Kernel Density Estimation (KDE) curves for 'Panic' and 'Suicidal' features against real data (Replicating Fig 1/2/3). Check for the male-sex bias noted in the paper. 3) Recall Stress Test: Train SVM on the GAN-augmented set and test *only* on the single positive real case. Confirm if the model flags it (Recall = 1.0) before measuring the False Positive cost.

## Open Questions the Paper Calls Out
None

## Limitations
- Extreme data scarcity with only 4 positive cases in the training set severely constrains generalizability
- GAN's inability to replicate the sex distribution (50% male in real data vs. 94% male in synthetic data) suggests potential bias in synthetic sample generation
- Single positive case in the test set provides insufficient evidence for robust performance claims

## Confidence
- **High Confidence**: GAN-based augmentation can generate minority class samples that enable models to detect previously undetectable cases (RF failure on real data vs. success on GAN data)
- **Medium Confidence**: SVM's margin-based optimization provides superior stability when trained on synthetic data compared to other models
- **Low Confidence**: The specific recall/false positive tradeoff achieved (1 positive with 33 false positives) represents an optimal solution, given the single test case provides insufficient evidence

## Next Checks
1. Multi-site validation: Test the trained models on independent suicide prediction datasets from different institutions to assess generalizability beyond the ABCD sample
2. Calibration analysis: Evaluate the probability calibration of SVM predictions to determine if confidence scores meaningfully stratify risk rather than requiring binary cutoffs
3. Cost-benefit simulation: Model the clinical impact of 33-85 false positives per true positive detection in terms of clinician workload and patient distress versus lives saved