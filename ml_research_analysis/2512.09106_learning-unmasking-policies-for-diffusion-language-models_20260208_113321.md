---
ver: rpa2
title: Learning Unmasking Policies for Diffusion Language Models
arxiv_id: '2512.09106'
source_url: https://arxiv.org/abs/2512.09106
tags:
- arxiv
- sampling
- diffusion
- policy
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a reinforcement learning approach for learning
  unmasking strategies in diffusion language models (dLLMs). The authors formalize
  dLLM sampling as a Markov decision process, propose a lightweight policy architecture
  based on a single-layer transformer that maps dLLM token confidences to unmasking
  decisions, and train the policy using group relative policy optimization.
---

# Learning Unmasking Policies for Diffusion Language Models

## Quick Facts
- arXiv ID: 2512.09106
- Source URL: https://arxiv.org/abs/2512.09106
- Authors: Metod Jazbec; Theo X. Olausson; Louis Béthune; Pierre Ablin; Michael Kirchhof; João Monteiro; Victor Turrisi; Jason Ramapuram; Marco Cuturi
- Reference count: 40
- Primary result: Learned RL policies for dLLM unmasking match heuristic samplers on standard tasks and outperform them in full diffusion settings

## Executive Summary
This paper introduces a reinforcement learning approach for learning unmasking strategies in diffusion language models (dLLMs). The authors formalize dLLM sampling as a Markov decision process, propose a lightweight policy architecture based on a single-layer transformer that maps dLLM token confidences to unmasking decisions, and train the policy using group relative policy optimization. Experiments demonstrate that the learned policies match the performance of state-of-the-art heuristic samplers in standard generation settings while outperforming them in the full diffusion setting without semi-autoregressive decoding. The policies also transfer across different models and sequence lengths, though performance degrades on out-of-domain data.

## Method Summary
The approach formulates dLLM sampling as a Markov decision process where the environment is the frozen dLLM and the agent is a lightweight policy network. The policy observes token-level confidence scores (maximum softmax probability per position), binary mask vectors indicating masked positions, and diffusion timestep embeddings. It outputs Bernoulli unmasking decisions for each masked token. Training uses Group Relative Policy Optimization (GRPO) with multiplicative rewards that balance correctness and efficiency, avoiding reward hacking where policies optimize speed at the expense of quality. The policy architecture is a single-layer transformer with adaptive layer normalization, operating solely on confidence signals rather than full hidden states.

## Key Results
- Learned policies match Dream-specific policies when trained on LLaDA and evaluated on Dream
- Policies transfer from L=256 to L=512 with minimal performance degradation
- In full diffusion setting (BL=256), learned policies outperform heuristic samplers while maintaining correctness
- Confidence-only input outperforms hidden-state input with 1000× fewer parameters

## Why This Works (Mechanism)

### Mechanism 1: Confidence Signals as Sufficient State Representation
Token-level confidence scores from the dLLM provide sufficient information for learning effective unmasking decisions without requiring access to hidden states or embeddings. The policy network maps confidence vectors, binary mask, and time index to unmasking logits. Since confidences are already computed during standard dLLM forward passes, this adds negligible overhead (<0.01% of base model size). The single-layer transformer with adaptive layer normalization learns spatial and temporal patterns in confidence distributions.

### Mechanism 2: Multiplicative Reward Prevents Efficiency-Correctness Trade Collapse
Multiplicative reward formulation prevents reward hacking where policies optimize for speed at the expense of correctness. Under additive rewards, incorrect but fast samples can receive positive advantage when all group samples are wrong (early training). Multiplicative formulation assigns zero reward to incorrect samples regardless of speed, ensuring gradient signal only propagates through correct generations. The α hyperparameter then controls efficiency optimization among correct samples.

### Mechanism 3: Policy Transfer Through Confidence-Only Interface
Policies trained on one dLLM can transfer to other dLLMs because the confidence-based interface abstracts away model-specific representations. Since the policy operates solely on confidence scalar sequences without access to token embeddings or model internals, it learns generalizable unmasking patterns (e.g., "unmask high-confidence tokens first," "preserve spatial coherence"). RoPE positional embeddings enable sequence-length extrapolation.

## Foundational Learning

- **Concept: Masked Diffusion Models (MDMs)**
  - Why needed here: The entire approach assumes understanding of iterative denoising from fully-masked sequences, where the unmasking order affects both quality and efficiency.
  - Quick check question: Can you explain why dLLMs can theoretically generate faster than AR models, and what prevents naive parallel unmasking from working well?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: The training pipeline uses GRPO's group-based advantage normalization as a variance reduction technique without requiring a separate value network.
  - Quick check question: Given G=8 samples per prompt, how does GRPO compute advantages differently from vanilla REINFORCE, and why does this help stabilize training?

- **Concept: Semi-Autoregressive vs. Full Diffusion Generation**
  - Why needed here: The key empirical finding is that learned policies outperform heuristics specifically in the full-diffusion (non-semi-AR) regime where heuristics degrade.
  - Quick check question: What is the difference between generating with block length BL=32 (semi-AR) versus BL=256 (full diffusion), and why might this affect sampling strategy performance?

## Architecture Onboarding

- **Component map**: 
  - dLLM forward pass → confidence vector c_t → Policy network (f_ϕ) → unmasking logits b_t → Bernoulli sampling → updated mask m_{t+1} → next dLLM step

- **Critical path**:
  1. dLLM forward pass → produces confidence vector c_t for all masked positions
  2. Policy network processes (c_t, m_t, t) → outputs logits b_t
  3. Bernoulli sampling: u_k^t ~ Bernoulli(σ(b_k^t/τ_π)) → unmasking decisions
  4. Fallback: if no positions selected, unmask highest-confidence position
  5. Repeat until all positions unmasked → compute reward → GRPO update

- **Design tradeoffs**:
  - **Bernoulli vs. Plackett-Luce**: Bernoulli is simpler and parallelizable but risks empty action sets; DPLS guarantees unmasking but is sequential. Paper shows comparable performance.
  - **Confidence-only vs. Hidden states**: Hidden states (300M params, 1000× larger) perform worse and are less stable, despite more semantic information.
  - **α tuning**: Higher α → faster but less controllable. α≥4.0 bifurcates into either α≈3.0 or α≈10.0 behavior, limiting fine-grained control.

- **Failure signatures**:
  - **Reward hacking** (additive reward): NFEs collapse to minimum while correctness stays near zero. Switch to multiplicative reward.
  - **Training instability** (high α): Only 1/2 seeds converge for α=10.0. Reduce α or use expert steering.
  - **Domain transfer failure**: Math-trained policies underperform on code by ~10-20%. Retrain on domain-specific data or mixed datasets.
  - **Empty action sequences**: During training, enforce non-empty unmasking to prevent policy from learning to predict all zeros.

- **First 3 experiments**:
  1. **Sanity check**: Train policy with α=1.0 on GSM8k subset (1000 samples), verify training reward increases and NFEs decrease over 500 steps. Compare against random unmasking baseline.
  2. **Architecture validation**: Compare confidence-only input vs. hidden-state input on same data. Expect confidence-only to match or exceed hidden-state performance with 1000× fewer parameters.
  3. **Transfer probe**: Take trained policy from experiment 1, evaluate on HumanEval without retraining. Expect performance between random and high-confidence baselines, confirming partial transfer with domain gap.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Why do learned unmasking policies show diminished performance on Dream-7B compared to LLaDA-8B, and can this gap be closed?
- **Basis in paper**: The authors state: "Understanding which characteristics of Dream (e.g., it being initialized from an AR model) contribute to the diminished performance of RL policies is an important open question."
- **Why unresolved**: Dream is initialized from an autoregressive model (Qwen 2.5) while LLaDA is trained from scratch as a masked diffusion model, but the causal relationship to policy performance remains unclear.
- **What evidence would resolve it**: Training policies on model mixtures or conducting ablations comparing AR-initialized vs. from-scratch dLLMs could isolate the contributing factors.

### Open Question 2
- **Question**: Can the accuracy-efficiency trade-off be controlled through a mechanism that does not require manually setting the α hyperparameter during training?
- **Basis in paper**: The authors note: "fine-grained tuning of the accuracy-efficiency trade-off can be challenging with our approach" and propose "investigate whether the accuracy-speed tradeoff could be controlled through some alternative mechanism."
- **Why unresolved**: Varying α produces discontinuous jumps in policy behavior rather than smooth interpolation, limiting practical controllability.
- **What evidence would resolve it**: Developing methods to learn α jointly with the policy, or post-hoc controllable mechanisms (e.g., through policy temperature or conditioning), would address this.

### Open Question 3
- **Question**: How can expert steering training be stabilized to improve policy discovery in the full-diffusion (non-semi-AR) setting?
- **Basis in paper**: The authors state: "We leave further investigations into stabilizing expert steering training for future work" after noting it "introduces significant instability during training, further reducing the controllability through α."
- **Why unresolved**: While expert steering helps discover better policies, multiple α values collapse to near-identical policies, suggesting optimization instability.
- **What evidence would resolve it**: Alternative training strategies (e.g., curriculum learning, different exploration bonuses, or regularized expert mixing) could improve stability.

## Limitations

- **Limited validation beyond short-sequence tasks**: Evaluation focuses on GSM8k (60 tokens avg) and HumanEval (70 tokens avg), leaving unclear how policies perform on long-form generation tasks where context and coherence matter more.
- **Narrow scope of dLLM architectures**: Experiments validate transfer between LLaDA and Dream (both decoder-only, Transformer-based), but not across architectural variants like encoder-decoder or recurrent models.
- **Sensitivity to reward design and hyperparameters**: Performance is highly dependent on α (speed-accuracy tradeoff) and reward correctness definitions. Math-trained policies fail on code (10-20% degradation), and high α values cause training instability (50% convergence rate).

## Confidence

**High Confidence**: Claims about policy architecture efficiency (300K params vs 1000× larger hidden-state alternatives), basic GRPO training mechanics, and empirical results on GSM8k/HumanEval benchmarks are well-supported with detailed experimental procedures and reproducible results.

**Medium Confidence**: Transfer claims between dLLM models (LLaDA→Dream) are demonstrated but limited to two specific architectures. Claims about confidence-only interfaces being sufficient rely on controlled ablations but may not generalize to more complex tasks or different dLLM designs.

**Low Confidence**: Claims about fundamental limitations of hidden-state inputs and generalizability of multiplicative reward formulation across continuous task spaces are inferred from specific experimental conditions and may not hold universally.

## Next Checks

1. **Long-sequence generalization test**: Evaluate learned policies on long-form generation tasks (e.g., 1024+ token stories or extended dialogues) to assess whether confidence-based unmasking scales to contexts where token dependencies span hundreds of positions.

2. **Architectural transfer validation**: Test policy transfer from decoder-only dLLMs to encoder-decoder variants or other diffusion architectures to verify whether the confidence-only interface truly abstracts away model-specific representations.

3. **Cross-domain robustness analysis**: Systematically evaluate policies trained on one domain (math) across multiple target domains (code, creative writing, reasoning) with varying token distributions and uncertainty patterns to quantify transfer limits and identify failure modes.