---
ver: rpa2
title: Actor-Critic without Actor
arxiv_id: '2509.21022'
source_url: https://arxiv.org/abs/2509.21022
tags:
- step
- diffusion
- critic
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Actor-Critic without Actor (ACA), a lightweight
  reinforcement learning framework that eliminates the explicit actor network and
  generates actions directly from the gradient field of a noise-level critic. By removing
  the actor, ACA avoids policy lag, reduces computational overhead, and ensures actions
  remain immediately aligned with up-to-date value estimates.
---

# Actor-Critic without Actor
## Quick Facts
- arXiv ID: 2509.21022
- Source URL: https://arxiv.org/abs/2509.21022
- Reference count: 34
- Key result: Achieves 10.2% higher returns than SAC on average while using fewer parameters

## Executive Summary
Actor-Critic without Actor (ACA) introduces a novel lightweight reinforcement learning framework that eliminates the explicit actor network by generating actions directly from the gradient field of a noise-level critic. This approach removes policy lag issues and reduces computational overhead while maintaining competitive performance on MuJoCo benchmarks. The method uses a noise-level critic that conditions on both the noised action and diffusion timestep, enabling stable gradients across noise levels and preserving multi-modal action distributions.

## Method Summary
ACA replaces the traditional actor-critic architecture by removing the explicit actor network entirely. Instead, actions are generated directly from the gradient field of a noise-level critic, which conditions on both the noised action and diffusion timestep. This design ensures actions remain immediately aligned with up-to-date value estimates, avoiding policy lag. The noise-level critic enables stable gradients across different noise levels while preserving multi-modal action distributions that are critical for certain tasks.

## Key Results
- ACA achieves 10.2% higher returns than SAC on average across benchmarks
- Outperforms diffusion-based baselines in early-stage learning
- Maintains 99.3% sample coverage across high-value modes in a 2D bandit task

## Why This Works (Mechanism)
The core innovation eliminates the actor network and generates actions directly from the gradient field of the critic. By removing the actor, ACA avoids policy lag where the actor's policy becomes misaligned with the current value estimates during training. The noise-level critic conditions on both the noised action and diffusion timestep, which enables stable gradients across different noise levels. This architecture preserves multi-modal action distributions by maintaining the ability to represent multiple optimal actions simultaneously, which is crucial for tasks with multiple valid solutions.

## Foundational Learning
- Actor-Critic Architecture: Why needed - traditional baseline for RL; Quick check - understand policy/value separation
- Policy Lag: Why needed - explains motivation for removing actor; Quick check - can you identify when actor and critic become misaligned?
- Diffusion Models in RL: Why needed - provides theoretical foundation for noise-level conditioning; Quick check - understand how noise levels affect action generation
- Multi-modal Distributions: Why needed - critical for tasks with multiple optimal actions; Quick check - can you identify scenarios where multiple actions yield similar rewards?
- Gradient Field Navigation: Why needed - explains how actions are generated without explicit policy; Quick check - understand how following gradients produces actions

## Architecture Onboarding
- Component Map: Observation -> Noise-level Critic (conditions on noised action + timestep) -> Action Generation (via gradient field)
- Critical Path: State encoding → Noised action sampling → Noise-level critic evaluation → Gradient field computation → Action output
- Design Tradeoffs: Eliminates actor network (fewer parameters, no policy lag) vs. requires more sophisticated critic that handles noise conditioning
- Failure Signatures: Poor performance on tasks requiring precise action selection, instability when critic gradients become noisy, loss of multi-modality in complex action spaces
- First Experiments: 1) Run ACA on simple continuous control task to verify basic functionality, 2) Compare learning curves against SAC on same task, 3) Test multi-modality preservation on 2D bandit task

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of ablation studies isolating the impact of the noise-level critic versus actor removal
- Statistical significance of performance improvements not quantified across multiple random seeds
- Multi-modality preservation claims based on simple 2D bandit task may not generalize to high-dimensional control tasks

## Confidence
- Performance claims: Medium (benchmark results show improvements but lack statistical rigor)
- Multi-modality preservation: Medium (demonstrated in simple task but uncertain in complex environments)
- Computational efficiency: Low (logical but not empirically validated)

## Next Checks
1. Run statistical significance tests (e.g., Mann-Whitney U) on learning curves across at least 5 random seeds for each benchmark task
2. Implement ablation studies comparing ACA with and without the noise-level critic while keeping actor removal constant
3. Test ACA on more complex multi-modal tasks with higher-dimensional action spaces to verify the 99.3% coverage claim generalizes beyond 2D bandit scenarios