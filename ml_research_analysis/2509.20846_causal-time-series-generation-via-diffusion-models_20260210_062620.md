---
ver: rpa2
title: Causal Time Series Generation via Diffusion Models
arxiv_id: '2509.20846'
source_url: https://arxiv.org/abs/2509.20846
tags:
- time
- environment
- generation
- counterfactual
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CaTSG, a diffusion-based framework for causal
  time series generation. It addresses the problem that existing conditional time
  series models learn spurious correlations by not accounting for unobserved confounders.
---

# Causal Time Series Generation via Diffusion Models

## Quick Facts
- **arXiv ID:** 2509.20846
- **Source URL:** https://arxiv.org/abs/2509.20846
- **Reference count:** 40
- **Primary result:** Proposes CaTSG, a diffusion framework that achieves 2.4%-98.7% improvement over baselines while supporting interventional and counterfactual generation

## Executive Summary
This paper introduces CaTSG, a diffusion-based framework for causal time series generation that addresses the problem of unobserved confounders in conditional models. Traditional conditional time series models learn spurious correlations because they don't account for hidden variables that affect both conditions and outputs. CaTSG incorporates a learnable environment bank and backdoor-adjusted guidance to support all three levels of Pearl's causal ladder: observational, interventional, and counterfactual generation. The method derives causal score functions via backdoor adjustment and abduction-action-prediction, enabling principled causal generation that existing models cannot handle.

## Method Summary
CaTSG extends conditional time series generation to causal settings by modeling latent environments as a learnable embedding bank. The framework consists of three components: EnvInfer (a TCN-based encoder that infers environment weights), a learnable Environment Bank (K discrete embeddings), and a 1D U-Net denoiser. Training uses three losses: noise prediction (ε), SwAV-style swapped prediction (ℓsw), and orthogonality regularization (ℓorth). Inference employs backdoor-adjusted guidance for interventional generation and abduction-action-prediction for counterfactuals, requiring K forward passes per denoising step for environment marginalization.

## Key Results
- Achieves 2.4% - 98.7% improvement over baselines across four metrics (MDD, KL, MMD, J-FTSD)
- Successfully generates interventional samples that break spurious correlations
- Demonstrates counterfactual generation capability on both synthetic (Harmonic-VM/VP) and real-world datasets (Air Quality, Traffic)

## Why This Works (Mechanism)

### Mechanism 1: Discretized Latent Confounder Approximation
The model mitigates unobserved confounding by approximating continuous latent environments with a finite, learnable embedding bank rather than ignoring them. This converts the intractable problem of "finding the unobserved cause" into a classification/clustering problem over a learned codebook.

### Mechanism 2: Backdoor-Adjusted Score Guidance
Standard conditional diffusion learns spurious correlations (P(X|C)); CaTSG approximates the interventional distribution (P(X|do(C))) by modifying the diffusion score function using Pearl's backdoor adjustment. The interventional score is derived as an expectation over the confounder E, approximated by aggregating noise predictions from the denoiser conditioned on the top-K environment embeddings.

### Mechanism 3: Abduction-Action-Prediction (AAP)
The framework supports counterfactual generation (Level 3 of Pearl's ladder) by fixing the inferred environment (E) while altering the context (C). Unlike interventional generation (which averages over E), counterfactual generation requires "Abduction" - inferring the specific E responsible for the observed factual (X, C).

## Foundational Learning

- **Concept:** Pearl's Causal Ladder (Do-Calculus)
  - **Why needed here:** To understand the difference between the paper's three tasks: Observational (association), Interventional (action), and Counterfactual (imagination). Without this, the "Backdoor Adjustment" is just a mathematical trick rather than a causal necessity.
  - **Quick check question:** If we intervene on C, why must we cut edges incoming to C in the causal graph?

- **Concept:** Score-Based Generative Models (Diffusion)
  - **Why needed here:** CaTSG does not use a standard likelihood objective; it uses a "score function" (∇x log p(x)). Understanding that the denoiser predicts the gradient of the log-density (or noise ε) is vital to grasping how "guidance" steers generation.
  - **Quick check question:** How does "Classifier-Free Guidance" (CFG) allow us to trade off between conditional and unconditional generation?

- **Concept:** Self-Supervised Learning (SwAV)
  - **Why needed here:** The EnvInfer module is trained without ground-truth environment labels. It uses a SwAV-style loss (clustering with Sinkhorn-Knopp) to force different augmentations of the same sample to agree on an environment code.
  - **Quick check question:** Why does the Sinkhorn-Knopp algorithm prevent all samples from collapsing into a single environment cluster?

## Architecture Onboarding

- **Component map:** EnvInfer (TCN + Projection) -> Environment Bank (K × H) -> Denoiser (1D U-Net)
- **Critical path:**
  1. Training: Joint optimization of Denoiser (CFG-style), EnvInfer (SwAV-style), and Env Bank
  2. Inference (Interventional): EnvInfer calculates weights w; Denoiser runs K forward passes; outputs are aggregated via weighted sum using w
  3. Inference (Counterfactual): EnvInfer calculates weights w using factual (X, C); these weights are frozen; Denoiser generates using counterfactual C' and the frozen w
- **Design tradeoffs:** K (Bank Size) - Small K forces coarse environment binning; large K increases inference cost. Discretization vs Continuous - The paper discretizes E for tractable backdoor integration.
- **Failure signatures:**
  - Bank Collapse: All inputs map to one environment ID, reverting to standard conditional diffusion
  - Low Diversity Interventions: Guidance scale too low (looks like observation) or too high (unrealistic artifacts)
  - Identifiability Failure: Counterfactuals look like random samples rather than specific alternatives
- **First 3 experiments:**
  1. Sanity Check (Synthetic): Train on Harmonic-VM. Plot t-SNE of EnvInfer embeddings. Do clusters correspond to ground-truth physics parameters (m, γ, k)?
  2. Ablation (Guidance): Generate with Backdoor Guidance disabled. Check if spurious correlations reappear.
  3. Counterfactual Consistency: Take real sample (X, C), generate counterfactual X' with C' ≈ C. Verify X' ≈ X.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the framework be extended to learn the causal structure (SCM) rather than relying on a predefined graph?
- **Basis in paper:** [explicit] The conclusion lists "relaxing the reliance on predefined SCMs" as a primary challenge, and Appendix H.3 warns that if true mechanisms violate assumed edges, the backdoor adjustment may fail.
- **Why unresolved:** The current instantiation assumes a fixed causal graph (E → C, E → X, C → X), which reduces robustness when transferring to domains with different or unknown mechanism invariances.
- **What evidence would resolve it:** An extension that dynamically infers or adapts the causal structure from observational data while maintaining validity of interventional and counterfactual generation.

### Open Question 2
- **Question:** How can counterfactual time series generation be rigorously evaluated on real-world data where ground-truth counterfactuals are unobservable?
- **Basis in paper:** [explicit] The introduction highlights "evaluation difficulty" as a key challenge, and the conclusion explicitly calls for "developing systematic evaluation protocols for counterfactuals in real-world data."
- **Why unresolved:** The paper relies on synthetic datasets (Harmonic-VM/VP) for quantitative counterfactual evaluation because real-world ground truth is unavailable, limiting validation to visual case studies.
- **What evidence would resolve it:** A standardized evaluation framework or benchmark that validates counterfactual fidelity in real-world scenarios using domain-specific physical laws, simulators, or proxy consistency metrics.

### Open Question 3
- **Question:** Can the CaTSG framework be adapted to handle multi-modal conditions (e.g., textual or visual guidance) alongside time-series covariates?
- **Basis in paper:** [explicit] Section 6 and Appendix H.7 state that the method currently "restricts conditions c to time series inputs" and identify "multi-modal conditioning" as a significant direction for enabling richer interventional generation.
- **Why unresolved:** The architecture currently processes vector-based time-series inputs; it is unclear how the environment bank and backdoor-adjusted guidance would integrate with unstructured data modalities like text or images.
- **What evidence would resolve it:** A successful extension that incorporates multi-modal encoders and demonstrates improved control or diversity in generation compared to the baseline.

## Limitations
- Finite environment bank approximation may fail when confounders are truly continuous or high-dimensional, limiting generalizability beyond synthetic domains
- SwAV-based environment inference lacks explicit supervision, making it unclear whether inferred environments correspond to true causal factors or spurious correlations
- Real-world SCM validation is challenging - the paper assumes correct causal graph structure without testing sensitivity to misspecification

## Confidence
- **High:** Observational generation performance metrics and superiority over baselines
- **Medium:** Interventional generation claims - methodology is sound but real-world validation is limited
- **Medium:** Counterfactual generation capability - theoretical framework is complete but empirical demonstration is constrained by data availability

## Next Checks
1. Test bank size sensitivity by systematically varying K and measuring environment clustering quality vs. generation fidelity trade-offs
2. Conduct sensitivity analysis on SCM misspecification by deliberately altering assumed causal graphs and measuring performance degradation
3. Implement cross-dataset transfer learning to evaluate whether environment bank generalizes across different domains with shared confounding structures