---
ver: rpa2
title: Quantifying Compositionality of Classic and State-of-the-Art Embeddings
arxiv_id: '2509.19332'
source_url: https://arxiv.org/abs/2509.19332
tags:
- embeddings
- embedding
- compositionality
- each
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel two-step evaluation framework to
  quantify the compositionality of embeddings across different modalities (words,
  sentences, knowledge graphs) and models (Word2Vec, SBERT, GPT, Llama, graph embeddings).
  The first step uses Canonical Correlation Analysis (CCA) to measure linearity between
  entity attributes and embeddings.
---

# Quantifying Compositionality of Classic and State-of-the-Art Embeddings

## Quick Facts
- arXiv ID: 2509.19332
- Source URL: https://arxiv.org/abs/2509.19332
- Authors: Zhijin Guo; Chenhao Xue; Zhaozhen Xu; Hongbo Bo; Yuxuan Ye; Janet B. Pierrehumbert; Martha Lewis
- Reference count: 40
- Primary result: Novel two-step framework quantifies additive compositionality across modalities (words, sentences, KGs) using CCA and leave-one-out reconstruction

## Executive Summary
This paper introduces a framework to quantify the compositionality of embeddings across different modalities and models. The approach measures linearity between interpretable entity attributes and embeddings using Canonical Correlation Analysis (CCA), then evaluates additive generalization through leave-one-out reconstruction experiments. The framework reveals that embeddings exhibit increasing additive compositionality during training, with stronger compositional signals in later training stages and deeper layers before a decline at the top layer.

## Method Summary
The framework operates in two steps: First, CCA measures the linear relationship between entity attributes (binary matrix A) and embeddings (continuous matrix U). Second, additive generalization is evaluated by reconstructing embeddings for unseen attribute combinations using leave-one-out experiments—solving AX=U via pseudo-inverse on training data, then predicting held-out embeddings. The method is applied across three modalities: sentences with concept labels, knowledge graphs with user demographics, and words with morphological features. Evaluation metrics include CCA correlation, L2 loss, cosine similarity, and retrieval accuracy, with permutation-based significance testing.

## Key Results
- CCA successfully detects linear compositionality in SBERT sentence embeddings (cosine similarity 0.78 real vs 0.49 permuted)
- Additive reconstruction works for word2vec embeddings (0.44 cosine similarity vs random baseline)
- Compositionality increases during training (PCC rises 1.5× from early to late stages in graph models)
- Stronger compositional signals observed in deeper transformer layers before declining at the top layer

## Why This Works (Mechanism)

### Mechanism 1
CCA detects linear alignment between interpretable attribute structures and continuous embedding spaces by learning projection matrices that maximize Pearson correlation between projected representations. This indicates semantically meaningful attributes are linearly encoded in the embedding geometry.

### Mechanism 2
Additive compositionality enables zero-shot generalization to unseen attribute combinations. By solving AX=U via pseudo-inverse and predicting held-out embeddings, successful reconstruction implies novel combinations can be synthesized from known components.

### Mechanism 3
Compositionality is a dynamic property that emerges through training and varies across network depth. Tracking CCA correlation and reconstruction metrics across training checkpoints and transformer layers reveals compositionality increases during training and peaks in mid-to-late layers before declining at the final layer.

## Foundational Learning

- **Concept: Canonical Correlation Analysis (CCA)**
  - Why needed here: Core statistical tool for measuring cross-space linear relationships between discrete attributes and continuous embeddings
  - Quick check question: Given two zero-mean random vectors x ∈ ℝ^n and y ∈ ℝ^m, can you write the CCA objective and explain why it finds shared subspaces rather than just correlation of individual dimensions?

- **Concept: Leave-one-out cross-validation for reconstruction**
  - Why needed here: The paper's generalization test hinges on predicting embeddings for held-out attribute combinations using only the learned linear decomposition
  - Quick check question: If you have 100 entities each with 5 binary attributes, and you hold out entity #42, what is the shape of A_{-42} and what does the pseudo-inverse solution X = (A_{-42})^+ U_{-42} represent?

- **Concept: Additive vs. non-additive compositionality**
  - Why needed here: The paper explicitly tests only linear additive composition; understanding what it excludes (idioms, non-linear semantic interactions) is critical for interpreting failure cases
  - Quick check question: Why would "hot dog" or "give up" be expected to violate additive reconstruction, and what does a high reconstruction residual imply about such phrases?

## Architecture Onboarding

- **Component map:** Attribute Matrix A (binary) → Embedding Matrix U (continuous) → CCA Module → Canonical Correlations → Reconstruction Module → Predicted Embeddings → Evaluation Metrics
- **Critical path:** 1) Construct A and U for target modality; 2) Run CCA to confirm linear correlation (real vs. permuted PCC); 3) If correlation exists, proceed to leave-one-out reconstruction; 4) Compute û_i = a_i X for each held-out entity, compare to u_i; 5) Perform Monte Carlo permutation tests
- **Design tradeoffs:** Binary vs. continuous attribute encoding (binary simplifies interpretation but may lose graded semantic information); pooling strategy choice affects layer-wise dynamics; number of canonical components k (early capture strongest correlations, later may be noisy)
- **Failure signatures:** High reconstruction L2 loss with low cosine similarity → non-additive structure; high CCA correlation but poor retrieval accuracy → linear structure exists but insufficient for discrimination; final-layer performance drop → task-specific specialization or pooling artifacts
- **First 3 experiments:** 1) Replicate SBERT sentence-concept experiment on held-out SGD slice (100 sentences, 10 concepts) to verify CCA correlation > 0.6; 2) Run leave-one-out reconstruction on word2vec + MorphoLex for words with exactly 2 suffixes; 3) Extract embeddings from layers 0-6 of small BERT model on 50 sentences; plot cosine similarity by layer

## Open Questions the Paper Calls Out
- How can evaluation frameworks be adapted to capture non-linear compositionality in embeddings?
- Do reconstruction failures stem primarily from inherent non-linearity or from incomplete conceptual attribute definitions?
- Is the observed additive compositionality preserved in high-dimensional Large Language Model (LLM) embeddings?

## Limitations
- Framework concentrates on detecting linearity-based compositionality; more methods are needed to handle non-linear cases
- Curse of dimensionality prevented CCA evaluation on Llama embeddings due to computational cost
- Failures may stem from various factors including inherent non-linearity, missing components, or incomplete representations

## Confidence
- **High confidence:** Established statistical methods (CCA, leave-one-out cross-validation) are correctly applied
- **Medium confidence:** Interpreting residuals as evidence of non-linear structure—residuals may also arise from incomplete attribute specifications or pooling artifacts
- **Medium confidence:** Training-stage dynamics claim given limited temporal snapshots and no ablation of pooling strategies

## Next Checks
1. Verify CCA configuration and pseudo-inverse settings by reproducing SBERT sentence-concept experiments on a held-out SGD slice
2. Test word2vec + MorphoLex reconstruction on 2-suffix words to confirm cosine similarity > 0.5
3. Profile BERT layer-wise compositionality on 50 sentences to confirm mid-layer peak and final-layer decline