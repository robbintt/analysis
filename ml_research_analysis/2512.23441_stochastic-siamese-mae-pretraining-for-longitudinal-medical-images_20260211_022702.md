---
ver: rpa2
title: Stochastic Siamese MAE Pretraining for Longitudinal Medical Images
arxiv_id: '2512.23441'
source_url: https://arxiv.org/abs/2512.23441
tags:
- temporal
- time
- learning
- future
- disease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STAMP introduces a self-supervised learning framework for modeling
  temporal dynamics in longitudinal medical images through time-conditioned stochasticity.
  It extends Siamese MAE pretraining with a learned prior distribution conditioned
  on time intervals and past image embeddings, enabling the model to capture multiple
  possible future outcomes rather than deterministic predictions.
---

# Stochastic Siamese MAE Pretraining for Longitudinal Medical Images

## Quick Facts
- arXiv ID: 2512.23441
- Source URL: https://arxiv.org/abs/2512.23441
- Reference count: 40
- Primary result: STAMP achieves state-of-the-art performance in disease progression prediction across OCT and MRI datasets using time-conditioned stochastic pretraining

## Executive Summary
STAMP introduces a self-supervised learning framework for modeling temporal dynamics in longitudinal medical images through time-conditioned stochasticity. It extends Siamese MAE pretraining with a learned prior distribution conditioned on time intervals and past image embeddings, enabling the model to capture multiple possible future outcomes rather than deterministic predictions. The approach reformulates reconstruction as conditional variational inference, allowing inference from single scans with variable time windows. Evaluated on two OCT datasets for Age-Related Macular Degeneration progression prediction and one MRI dataset for Alzheimer's Disease onset, STAMP outperformed existing MAE-based methods, foundation models, and temporal contrastive approaches across multiple metrics. The method demonstrated robust generalization under domain shifts and irregular visit schedules while maintaining computational efficiency comparable to SiamMAE. Clinically, this enables personalized monitoring and earlier intervention by forecasting individual disease trajectories from baseline scans, addressing key challenges in progressive disease management.

## Method Summary
STAMP extends the Siamese MAE architecture to incorporate time-conditioned stochasticity for modeling disease progression in longitudinal medical images. The model processes two 3D volumes (past and future) with 75% masking applied only to the future volume. A learnable temporal encoding (TE) generated from the time interval between scans is added to the CLS token. Two separate MLPs produce prior and posterior distributions over a 32-dimensional categorical latent space, with the posterior conditioned on both past and future embeddings while the prior uses only past embeddings and TE. The decoder performs cross-attention using the stochastic sample and past embeddings as Key/Value against masked future patches as Query. Training minimizes a combined loss of MSE reconstruction plus asymmetric KL divergence (0.2·KL(q||SG(p)) + 0.8·KL(SG(q)||p)). The model is trained on 800 epochs with horizontal flip augmentation being critical for performance.

## Key Results
- Outperformed MAE, foundation models (NTC, MAE), and temporal contrastive approaches on three datasets
- Demonstrated robust generalization across domain shifts and irregular visit schedules
- Maintained computational efficiency comparable to SiamMAE while adding stochastic modeling capabilities
- Enabled inference from single scans with arbitrary time windows through learned time-conditioned prior

## Why This Works (Mechanism)

### Mechanism 1: Time-Conditioned Stochastic Prior
Encoding the time interval between scans enables the model to learn progression dynamics at varying temporal scales, rather than treating all temporal gaps uniformly. A learnable MLP generates temporal encodings (TE) from discretized time differences using sin-cos waves, which are added to the CLS token. This conditions the prior distribution pψ(zt+Δt|ht, Δt) on both the past embedding and the time gap, allowing inference-time prompting with arbitrary prediction horizons.

### Mechanism 2: Variational Inference for Multi-Outcome Modeling
Reformulating reconstruction as conditional variational inference allows the model to capture a distribution over plausible futures rather than a single deterministic trajectory. A posterior qφ is learned from both past and partially visible future CLS tokens, while the prior pψ is learned from only the past and TE. KL-divergence aligns them, enabling sampling zt+Δt ~ qφ during training and ~ pψ at inference.

### Mechanism 3: Cross-Attention Decoder with Stochastic Token Injection
Injecting a sampled stochastic token alongside past embeddings into the decoder enables reconstruction to be guided by both concrete history and sampled uncertainty. The decoder receives [ẑt+Δt, ht] as Key/Value (past + stochastic sample) and [ht+Δt visible + MASK tokens] as Query. Cross-attention queries the combined representation to reconstruct the masked future patches.

## Foundational Learning

- **Concept: Variational Autoencoders (VAEs) and ELBO**
  - Why needed here: STAMP extends VAE formulation to temporal, conditional setting. Understanding ELBO terms (reconstruction + KL) is essential to grasp the loss.
  - Quick check question: Can you explain why minimizing KL(q||p) alone would collapse the posterior?

- **Concept: Vision Transformers (ViT) and Patch Embedding**
  - Why needed here: STAMP uses ViT backbone with 3D patchification. Understanding tokenization and positional encodings is prerequisite.
  - Quick check question: How does masking 75% of patches affect encoder computation compared to processing all patches?

- **Concept: Siamese Networks and Weight Sharing**
  - Why needed here: STAMP uses shared encoder weights for both temporal branches. Understanding why weight sharing enables comparison is key.
  - Quick check question: Why would separate encoders for t and t+Δt fail to learn temporal relationships?

## Architecture Onboarding

- **Component map:** Input volumes → 3D patchification → 75% masking on future → Shared ViT encoder → Temporal encoding addition → Prior/posterior MLPs → Stochastic sampling → Cross-attention decoder → MSE + KL loss

- **Critical path:**
  1. Patchification and masking (must handle 3D volumes correctly)
  2. Temporal encoding injection (must be added before encoder AND decoder)
  3. Prior/posterior MLP outputs → categorical logits → sampling with straight-through estimator
  4. KL loss with stop-gradient (0.2/0.8 asymmetry is critical for stability)

- **Design tradeoffs:**
  - 2-visit pretraining limits modeling of complex multi-step dynamics but reduces computational cost vs. full sequence modeling
  - Partially masking future (vs. fully visible in RSP) prevents posterior collapse but may lose some future information
  - Discretized TE limits precision for irregular intervals; fractional interpolation helps but not explicitly trained

- **Failure signatures:**
  - Posterior collapse: KL→0, all sampled tokens identical (mitigate by checking KL magnitude)
  - Time-insensitivity: Performance same regardless of TE prompting (prior not learning time dependence)
  - Reconstruction dominance: MSE→0 but downstream task fails (encoder not learning useful representations)
  - Domain shift failure: Pretraining on HARBOR → poor PINNACLE performance (scanner differences overwhelm learned features)

- **First 3 experiments:**
  1. Sanity check: Pretrain on small subset; verify reconstruction loss decreases and KL stabilizes at non-zero value. Check that TE-added vs. TE-ablated models show different attention maps.
  2. Ablation: Train SiamMAE + TE only (no stochasticity), SiamMAE + stochasticity only (no TE), and full STAMP. Compare downstream AUROC to isolate contribution of each component.
  3. Generalization test: Pretrain on dataset A, evaluate linear probe on dataset B with different scanner/protocol. Compare to MAE and foundation model baselines to assess transfer.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does extending the pretraining framework to incorporate more than two sequential visits improve the modeling of complex, long-term disease trajectories? [explicit] The authors state in the Discussion that "reliance on only two visit volumes during pretraining limits its ability to capture complex dynamic relationships."

- **Open Question 2:** How can the interpretability of the learned prior distribution be improved to allow for clinical validation of the predicted stochastic outcomes? [explicit] The authors acknowledge that "the prior's large sampling space makes post hoc explainability analysis of the sampled tokens challenging."

- **Open Question 3:** Can the model's utility be expanded to handle multi-modal inputs (e.g., combining genetic data or clinical demographics with imaging) to better resolve the uncertainty in disease progression? [inferred] The paper notes "wide inter-subject variability in disease progression rates due to unknown latent factors (e.g. genetics, dietary habits)" as a motivation for stochasticity, but currently relies solely on imaging.

## Limitations

- Data Diversity and Generalizability: Performance under domain shift to different scanners, acquisition protocols, or disease types remains untested
- Latent Space Interpretability: The 32-bin categorical representation provides no inherent semantic meaning, making clinical explanation difficult
- Computational Overhead: Two-encoder forward pass and additional MLP parameters increase memory requirements for deployment

## Confidence

- **High Confidence:** Core mechanism of combining time-conditioned stochasticity with Siamese MAE architecture is well-supported by ablation studies
- **Medium Confidence:** Clinical utility claims are supported by improved predictive metrics but lack direct clinical validation
- **Low Confidence:** Assertion that learned prior captures "multiple plausible futures" is technically correct but practically limited by 2-timepoint pretraining

## Next Checks

1. **Domain Shift Robustness:** Pretrain STAMP on one OCT dataset (HARBOR), then evaluate on a completely different OCT dataset with different scanner parameters and patient demographics

2. **Temporal Resolution Sensitivity:** Systematically vary the discretization granularity of time intervals during pretraining (e.g., 1-month vs 3-month bins) and evaluate impact on downstream prediction accuracy

3. **Latent Space Clinical Alignment:** Extract learned stochastic tokens for patients with known progression patterns, apply clustering or visualization techniques, and assess whether clusters correspond to clinically meaningful progression stages