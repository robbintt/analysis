---
ver: rpa2
title: 'FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations
  in Language Models'
arxiv_id: '2507.20930'
source_url: https://arxiv.org/abs/2507.20930
tags:
- error
- detection
- errors
- arxiv
- editing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a fine-tuned approach for detecting and editing
  hallucinations in language models, focusing on financial RAG systems. A domain-specific
  error taxonomy is defined and used to generate synthetic training data by inserting
  controlled factual errors into financial QA corpora.
---

# FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models

## Quick Facts
- arXiv ID: 2507.20930
- Source URL: https://arxiv.org/abs/2507.20930
- Reference count: 24
- Small fine-tuned models achieve comparable hallucination detection to o3 with only 4B parameters

## Executive Summary
This work introduces FRED, a framework for detecting and editing hallucinations in language models, specifically targeting financial retrieval-augmented generation (RAG) systems. The authors define a domain-specific error taxonomy and generate synthetic training data by inserting controlled factual errors into financial QA corpora. Four small language models—Phi-4, Phi-4-mini, Qwen3-4B, and Qwen3-14B—are fine-tuned using LoRA to perform span-level hallucination detection and correction. The best-performing model, fine-tuned Phi-4, achieves an 8% improvement in binary F1 score and a 30% gain in overall detection performance compared to OpenAI-o3, demonstrating that small, fine-tuned models can effectively detect and correct factual errors in high-stakes financial applications.

## Method Summary
The method involves defining a financial-specific error taxonomy and generating synthetic training data by inserting controlled factual errors into FinQA and TAT-QA datasets. Four small language models are fine-tuned using LoRA (rank=16, alpha=16) on tagged error spans, targeting all attention and MLP projection layers. The fine-tuning uses bf16 precision, gradient checkpointing, and a 2-epoch schedule with LR=2e-5. Evaluation is performed on FAVA and FinQA+TATQA test sets using binary F1, per-category F1 scores, and FActScoreLite for editing quality assessment. The approach demonstrates that small models can achieve competitive hallucination detection performance compared to larger frontier models.

## Key Results
- Phi-4 achieves 8% improvement in binary F1 score and 30% overall detection gain over OpenAI-o3
- Phi-4-mini maintains competitive performance with only 2% drop in binary detection and 0.1% decline in overall detection
- 4-billion-parameter models show degraded numerical error detection (F1 ~48-51) compared to Phi-4-36k (F1 ~86)

## Why This Works (Mechanism)
The approach works by combining domain-specific error definitions with synthetic data generation that closely mimics real-world hallucination patterns in financial contexts. The fine-tuning process enables models to learn precise error detection and correction patterns, while the 8K parameter LoRA adapters provide efficient adaptation without full model retraining. The use of tagged error spans allows the model to learn both detection and correction simultaneously, creating a unified hallucination editing capability.

## Foundational Learning

**LoRA fine-tuning** (why needed: enables efficient model adaptation with small trainable parameters; quick check: verify rank=16, alpha=16 configuration matches paper)
**Error taxonomy design** (why needed: provides structured framework for hallucination detection; quick check: confirm all six error types are represented in synthetic data)
**Synthetic data generation** (why needed: creates controlled training examples with known error patterns; quick check: validate error insertion follows specified stochastic algorithm)
**FActScoreLite evaluation** (why needed: measures quality of both detection and correction; quick check: confirm multiple backend configurations are used)
**4-bit quantization** (why needed: reduces memory footprint for deployment; quick check: verify Unsloth quantization is applied to Phi-4/Qwen models)

## Architecture Onboarding

**Component map**: Synthetic data generator -> LoRA fine-tuning pipeline -> Detection/Correction model -> FActScoreLite evaluation
**Critical path**: Error insertion → Tagged passage creation → LoRA fine-tuning → Binary and per-category F1 evaluation → FActScoreLite editing quality assessment
**Design tradeoffs**: Synthetic data vs. real annotations (controllable but may not capture all real patterns); small models vs. large frontier models (efficient but may miss complex reasoning); LoRA vs. full fine-tuning (parameter-efficient but potentially limited capacity)
**Failure signatures**: Invalid tag nesting in synthetic data; poor numerical error detection in smaller models; over-editing with high recall but low precision
**First experiments**: 1) Generate synthetic data with exact error insertion parameters; 2) Train Phi-4 with specified LoRA configuration; 3) Evaluate binary F1 on FAVA test set

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic data generation may not capture all real-world hallucination patterns despite lower unfixable error rates
- Financial error taxonomy may not generalize to other domains without retraining
- Evaluation focuses on detection metrics but doesn't assess end-to-end task performance improvements

## Confidence

**High confidence**: Phi-4's 8% binary F1 improvement and 30% overall detection gain over o3 are well-supported by reported metrics
**Medium confidence**: Synthetic data generation methodology is reproducible but exact prompts are unspecified
**Medium confidence**: FActScoreLite evaluation with multiple backends is sound but test sets may not represent all real-world patterns

## Next Checks

1. Reproduce synthetic error insertion pipeline and compare error distribution against paper-reported values
2. Evaluate fine-tuned Phi-4 model on non-financial QA datasets to assess domain generalization
3. Conduct ablation studies removing specific error types from synthetic training data to quantify their contribution to performance