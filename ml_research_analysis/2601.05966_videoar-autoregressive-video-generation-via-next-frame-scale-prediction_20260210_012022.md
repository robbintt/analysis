---
ver: rpa2
title: 'VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction'
arxiv_id: '2601.05966'
source_url: https://arxiv.org/abs/2601.05966
tags:
- video
- generation
- temporal
- autoregressive
- frame
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VideoAR, the first large-scale autoregressive
  framework for video generation that integrates multi-scale next-frame prediction
  with visual autoregressive (VAR) modeling. It addresses the challenges of spatial-temporal
  misalignment and error propagation in autoregressive video generation by disentangling
  spatial and temporal dependencies through intra-frame VAR modeling combined with
  causal next-frame prediction.
---

# VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction

## Quick Facts
- **arXiv ID:** 2601.05966
- **Source URL:** https://arxiv.org/abs/2601.05966
- **Reference count:** 38
- **Primary result:** First large-scale autoregressive framework for video generation integrating multi-scale next-frame prediction with visual autoregressive modeling, achieving competitive performance with diffusion models

## Executive Summary
This paper introduces VideoAR, a pioneering large-scale autoregressive framework for video generation that addresses spatial-temporal misalignment and error propagation through a novel combination of intra-frame visual autoregressive (VAR) modeling and causal next-frame prediction. The approach employs a 3D multi-scale tokenizer to encode spatio-temporal dynamics and introduces several technical innovations including Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask to enhance long-term consistency. Trained via a progressive multi-stage pipeline, VideoAR achieves state-of-the-art performance among autoregressive models with a VBench score of 81.74 and FVD of 88.6 on UCF-101, demonstrating competitive performance with diffusion models while requiring over 10× fewer inference steps.

## Method Summary
VideoAR introduces a unified autoregressive framework that disentangles spatial and temporal dependencies in video generation. The method combines visual autoregressive (VAR) modeling with causal next-frame prediction, using a 3D multi-scale tokenizer to encode spatio-temporal dynamics. The architecture integrates Multi-scale Temporal RoPE for enhanced temporal modeling, Cross-Frame Error Correction to mitigate error propagation, and Random Frame Mask during training to improve robustness. A progressive multi-stage training pipeline gradually increases spatial and temporal resolution, aligning learning across scales. The approach predicts both next-frame and scale information, enabling efficient autoregressive generation while maintaining temporal consistency and spatial fidelity across multiple resolutions.

## Key Results
- Achieves state-of-the-art FVD of 88.6 on UCF-101 among autoregressive models, improving from 99.5
- Reaches VBench score of 81.74, competitive with diffusion models an order of magnitude larger
- Reduces inference steps by over 10× compared to baseline autoregressive approaches
- Demonstrates effective handling of spatial-temporal misalignment and error propagation

## Why This Works (Mechanism)
The framework works by disentangling spatial and temporal dependencies through a dual approach: VAR modeling captures intra-frame spatial relationships while causal next-frame prediction handles temporal progression. The 3D multi-scale tokenizer enables efficient encoding of spatio-temporal dynamics across different resolutions, while Multi-scale Temporal RoPE extends the effective context window for long-range temporal dependencies. Cross-Frame Error Correction actively compensates for accumulated errors during generation, and Random Frame Mask during training improves generalization by forcing the model to handle incomplete sequences. The progressive multi-stage training pipeline allows the model to learn coarse-to-fine representations, gradually aligning spatial and temporal learning across increasing resolutions and durations.

## Foundational Learning
- **Visual Autoregressive (VAR) Modeling** - Why needed: Captures intra-frame spatial dependencies within individual frames; Quick check: Verify frame-level reconstruction quality before temporal prediction
- **Causal Next-Frame Prediction** - Why needed: Enables sequential generation of frames while maintaining temporal coherence; Quick check: Measure temporal consistency across generated sequences
- **3D Multi-scale Tokenization** - Why needed: Efficiently encodes spatio-temporal information across different resolutions; Quick check: Validate tokenization preserves both spatial detail and temporal dynamics
- **Temporal Positional Encoding** - Why needed: Extends context window for long-range temporal dependencies; Quick check: Test generation quality with varying sequence lengths
- **Progressive Multi-stage Training** - Why needed: Gradually aligns spatial and temporal learning from coarse to fine scales; Quick check: Monitor performance improvements across training stages
- **Error Correction Mechanisms** - Why needed: Mitigates error propagation in autoregressive generation; Quick check: Measure error accumulation across generated sequences

## Architecture Onboarding

Component map:
Input Video -> 3D Multi-scale Tokenizer -> Visual Autoregressive Model -> Next-Frame Predictor -> Multi-scale Temporal RoPE -> Cross-Frame Error Correction -> Generated Output

Critical path:
3D Multi-scale Tokenizer -> Visual Autoregressive Model -> Next-Frame Predictor -> Multi-scale Temporal RoPE -> Output

Design tradeoffs:
The framework balances computational efficiency with generation quality through its multi-scale approach. Using a 3D tokenizer instead of 2D allows better spatio-temporal encoding but increases computational cost. The progressive training pipeline trades longer training time for better alignment of spatial and temporal learning. Cross-Frame Error Correction adds computational overhead during inference but significantly improves generation quality by mitigating error propagation.

Failure signatures:
- Blurry or inconsistent frames indicate issues with the 3D multi-scale tokenizer or VAR modeling
- Temporal discontinuities suggest problems with causal next-frame prediction or temporal positional encoding
- Error accumulation over long sequences points to insufficient Cross-Frame Error Correction
- Poor quality at higher resolutions indicates inadequate progressive training alignment

First experiments:
1. Validate 3D multi-scale tokenization preserves spatial detail and temporal dynamics on short sequences
2. Test VAR modeling effectiveness by measuring frame reconstruction quality before temporal prediction
3. Evaluate Cross-Frame Error Correction by measuring error propagation across generated sequences of varying lengths

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions for future research.

## Limitations
- Potential computational cost trade-offs with the 3D multi-scale tokenizer for long-range dependencies
- Claims of competitive performance with much larger diffusion models require careful validation
- Asserted novelty as "first" large-scale framework needs verification against concurrent work
- Generalizability across diverse video datasets and out-of-distribution content remains unclear

## Confidence
- **High confidence:** Core technical contributions (3D multi-scale tokenizer, Multi-scale Temporal RoPE, Cross-Frame Error Correction, Random Frame Mask) are well-defined and implementable
- **Medium confidence:** Reported quantitative improvements (FVD on UCF-101, VBench score) appear robust but require independent replication
- **Low confidence:** Claims about being "first" in the field and achieving competitive performance with much larger diffusion models need more thorough comparative analysis

## Next Checks
1. Independent replication of VideoAR's performance on UCF-101 and VBench benchmarks using the same evaluation protocols to verify the claimed FVD improvement from 99.5 to 88.6 and VBench score of 81.74
2. Systematic ablation studies comparing VideoAR against recent concurrent autoregressive approaches (e.g., Scale-Wise VAR, SAMPO) to confirm the novelty and effectiveness of the proposed multi-scale next-frame prediction framework
3. Cross-dataset evaluation on diverse video domains (e.g., KITTI, HMDB51, or Ego4D) to assess the model's generalizability and robustness beyond the primary training dataset