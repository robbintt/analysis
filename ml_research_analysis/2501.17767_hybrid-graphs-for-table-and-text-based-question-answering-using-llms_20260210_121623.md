---
ver: rpa2
title: Hybrid Graphs for Table-and-Text based Question Answering using LLMs
arxiv_id: '2501.17767'
source_url: https://arxiv.org/abs/2501.17767
tags:
- table
- question
- graph
- hybrid
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ODYSSEY, a zero-shot, fine-tuning-free method
  for Table-Text Question Answering (QA) that constructs and prunes a Hybrid Graph
  from structured and unstructured data sources to efficiently provide relevant context
  to large language models (LLMs). The approach addresses the challenge of multi-hop
  reasoning across tables and text without requiring expensive human-curated training
  data.
---

# Hybrid Graphs for Table-and-Text based Question Answering using LLMs
## Quick Facts
- arXiv ID: 2501.17767
- Source URL: https://arxiv.org/abs/2501.17767
- Reference count: 23
- Key outcome: Zero-shot method achieving up to 10% EM improvement and 53% token reduction

## Executive Summary
This paper introduces ODYSSEY, a zero-shot Table-Text Question Answering (QA) method that constructs and prunes Hybrid Graphs from structured and unstructured data sources. The approach addresses multi-hop reasoning challenges across tables and text without requiring expensive human-curated training data. By efficiently providing relevant context to large language models, ODYSSEY achieves state-of-the-art performance while significantly reducing input token usage.

## Method Summary
ODYSSEY builds a Hybrid Graph by first extracting entities from questions and connecting them to relevant table and text nodes. The method then prunes irrelevant nodes using a score-based approach to minimize input context for LLMs. The pruned graph is traversed to generate relevant context, which is fed to the LLM for final answer generation. The approach uses various LLMs including GPT-3.5, GPT-4, and LLaMA-3 for evaluation, demonstrating effectiveness across different model architectures.

## Key Results
- Achieves up to 10% improvement in Exact Match scores on Hybrid-QA dataset
- Shows 5.4% EM improvement on OTT-QA dataset
- Reduces input token usage by up to 53% compared to baseline methods

## Why This Works (Mechanism)
ODYSSEY works by constructing a comprehensive graph structure that captures relationships between entities, tables, and text passages. The pruning mechanism intelligently removes irrelevant information while preserving critical paths needed for multi-hop reasoning. This selective context provision allows LLMs to focus on relevant information, improving both accuracy and efficiency. The zero-shot nature eliminates dependency on task-specific training data, making the approach more adaptable across domains.

## Foundational Learning
- Hybrid Graph Construction: Why needed - To capture relationships between tables and text; Quick check - Verify graph connectivity between question entities and answer nodes
- Node Pruning Strategy: Why needed - To reduce input token usage while maintaining accuracy; Quick check - Compare performance with and without pruning
- Multi-hop Reasoning: Why needed - To handle complex questions requiring information from multiple sources; Quick check - Test on questions requiring at least 2 reasoning hops

## Architecture Onboarding
Component map: Question -> Entity Extraction -> Hybrid Graph Construction -> Node Pruning -> Context Generation -> LLM -> Answer

Critical path: The core workflow follows Entity Extraction → Graph Construction → Pruning → Context Generation → LLM inference. Each stage must complete successfully for the next to function.

Design tradeoffs: The method balances between graph completeness and efficiency. More comprehensive graphs improve accuracy but increase token usage. The pruning strategy represents a key optimization point that affects both performance and efficiency.

Failure signatures: Common failure modes include over-pruning (removing relevant nodes), under-pruning (retaining too much irrelevant context), and graph construction errors that fail to capture necessary relationships. These manifest as either reduced accuracy or unnecessary token consumption.

First experiments:
1. Run ODYSSEY on a simple Hybrid-QA example with one table and one text passage
2. Test the pruning mechanism with varying thresholds on a small dataset
3. Evaluate context generation quality by manually inspecting the LLM input context

## Open Questions the Paper Calls Out
None

## Limitations
- Performance may be dependent on specific LLM capabilities and may not generalize to all models
- The pruning strategy might remove relevant information in complex scenarios
- Limited error analysis for understanding failure cases
- Unclear performance on diverse datasets beyond current benchmarks

## Confidence
- High confidence: Zero-shot performance improvements and token efficiency gains
- Medium confidence: General applicability of Hybrid Graph construction approach
- Low confidence: Scalability and robustness across different domain contexts and table structures

## Next Checks
1. Evaluate ODYSSEY's performance across multiple diverse Table-Text QA datasets beyond the current benchmarks to assess generalizability.
2. Conduct ablation studies to quantify the impact of individual components (graph construction, pruning, node linking) on overall performance.
3. Test the method with different LLM architectures and sizes to determine dependency on specific model capabilities and establish model-agnostic performance metrics.