---
ver: rpa2
title: Off-Policy Evaluation and Learning for Matching Markets
arxiv_id: '2507.13608'
source_url: https://arxiv.org/abs/2507.13608
tags:
- policy
- dips
- variance
- reward
- matching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiPS and DPR, novel off-policy evaluation
  estimators for matching markets. These methods leverage intermediate first-stage
  rewards (e.g., sending scouting messages) to reduce variance compared to traditional
  estimators like IPS and DR, which only use sparse match labels.
---

# Off-Policy Evaluation and Learning for Matching Markets
## Quick Facts
- arXiv ID: 2507.13608
- Source URL: https://arxiv.org/abs/2507.13608
- Authors: Yudai Hayashi, Shuhei Goda, Yuta Saito
- Reference count: 40
- Key outcome: Introduces DiPS and DPR estimators that leverage intermediate first-stage rewards to reduce variance in off-policy evaluation for matching markets, outperforming traditional IPS and DR methods.

## Executive Summary
This paper addresses off-policy evaluation in matching markets where decisions occur in multiple stages (e.g., first sending a message, then forming a match). Traditional estimators like IPS and DR only use sparse final rewards (matches), leading to high variance. The authors propose DiPS and DPR, which exploit intermediate rewards from earlier stages to achieve better bias-variance trade-offs. Theoretical analysis proves these new estimators have lower variance than existing methods, and experiments on synthetic data and real job-matching platform logs demonstrate superior performance in policy evaluation and learning tasks.

## Method Summary
The paper introduces DiPS (Doubly Importance-Sampled Policy Selection) and DPR (Doubly Penalized Reward) estimators for off-policy evaluation in two-stage matching markets. DiPS extends DR by applying importance weighting to first-stage rewards in addition to second-stage rewards, reducing variance when first-stage rewards are informative but sparse. DPR goes further by replacing the importance-weighted second-stage term with a reward regression model, offering additional variance reduction when the model is accurate. Both methods assume knowledge of the first-stage logging policy and leverage the staged structure to improve estimation quality compared to single-stage estimators.

## Key Results
- DiPS and DPR achieve lower mean squared error than IPS and DR in both synthetic experiments and real A/B test data from a job-matching platform
- Theoretical analysis proves DiPS and DPR have strictly lower variance than their single-stage counterparts under standard assumptions
- DPR provides additional variance reduction over DiPS when second-stage reward models are accurate
- The methods enable more reliable offline policy learning, selecting better-performing policies in evaluation tasks

## Why This Works (Mechanism)
The variance reduction comes from exploiting the staged nature of matching markets where intermediate actions (like sending messages) provide partial information about eventual outcomes. By incorporating first-stage rewards through importance weighting, DiPS reduces variance compared to estimators that only consider final matches. DPR achieves further improvement by replacing the high-variance importance-weighted second-stage term with a regression model, trading potential bias for reduced variance when the model is reasonably accurate.

## Foundational Learning
- **Off-policy evaluation**: Estimating performance of a policy using data collected by a different policy; needed because real-world policy testing is expensive/costly
- **Importance sampling**: Weighting samples by the ratio of target to behavior policy probabilities; quick check: verify logging policy is known
- **Doubly robust estimation**: Combining importance weighting with reward modeling for bias-variance trade-off; quick check: ensure both components are well-specified
- **Two-stage matching markets**: Markets where decisions happen sequentially (e.g., message then match); quick check: identify intermediate rewards
- **Variance decomposition**: Breaking down estimator variance into components to analyze improvement sources; quick check: verify theoretical variance bounds

## Architecture Onboarding
**Component Map**: Logging Policy -> Behavior Data -> Importance Weights -> DiPS/DPR Estimators -> Policy Performance Estimates

**Critical Path**: Logging policy π_b → collect first-stage rewards r₁ and second-stage rewards r₂ → estimate importance weights w = π_e/π_b → apply DiPS/DPR formula → obtain policy value estimate

**Design Tradeoffs**: DiPS trades implementation simplicity for moderate variance reduction; DPR trades potential model bias for maximum variance reduction when accurate models are available

**Failure Signatures**: High importance weights indicate distribution shift; poor reward model fit indicates DPR bias; sparse second-stage rewards indicate high-variance baseline methods

**First Experiments**: 1) Verify importance weight stability in test data; 2) Compare variance of DiPS vs IPS on synthetic data; 3) Test DPR sensitivity to reward model accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical validation limited to synthetic data and single proprietary job-matching platform, limiting generalizability
- Performance depends on quality of second-stage reward models in DPR, with no extensive sensitivity analysis provided
- Theoretical results assume known logging policies and may not hold under policy uncertainty or model misspecification

## Confidence
Theoretical claims: High
Empirical claims: Medium

## Next Checks
1. Test DiPS and DPR on public datasets from other matching market domains (e.g., ride-sharing or housing allocation) to assess generalizability
2. Conduct sensitivity analysis varying the accuracy of second-stage reward models to understand DPR's robustness to model misspecification
3. Evaluate performance when first-stage and second-stage rewards are correlated or when the staged structure is less pronounced