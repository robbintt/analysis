---
ver: rpa2
title: Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA
arxiv_id: '2511.04384'
source_url: https://arxiv.org/abs/2511.04384
tags:
- visual
- arxiv
- multi-task
- medical
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of explainable AI for gastrointestinal
  imaging by proposing a multi-task learning framework that integrates visual question
  answering, explanation generation, and visual grounding. The core method involves
  fine-tuning the Florence-2 model with LoRA on three curated datasets: Kvasir-VQA-x1
  for question-answer pairs, a synthetically enriched explanation dataset for medical
  reasoning, and a text-to-region dataset for visual grounding using pseudo-masks.'
---

# Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA

## Quick Facts
- arXiv ID: 2511.04384
- Source URL: https://arxiv.org/abs/2511.04384
- Reference count: 15
- Primary result: BLEU=0.4539, ROUGE-L=0.6531, BERTScore F1=0.9479 on MediaEval Medico 2025 challenge

## Executive Summary
This paper addresses the challenge of explainable AI for gastrointestinal imaging by proposing a multi-task learning framework that integrates visual question answering, explanation generation, and visual grounding. The core method involves fine-tuning the Florence-2 model with LoRA on three curated datasets: Kvasir-VQA-x1 for question-answer pairs, a synthetically enriched explanation dataset for medical reasoning, and a text-to-region dataset for visual grounding using pseudo-masks. The multi-task approach enables the model to jointly learn visual grounding, reasoning, and interpretation. The primary results show that the proposed method substantially improves over single-task baselines, with the best model achieving BLEU of 0.4539, ROUGE-L of 0.6531, and BERTScore F1 of 0.9479 on the official private dataset. The model demonstrates robust generalization and improved performance in both answer accuracy and visual localization.

## Method Summary
The authors fine-tune Florence-2 with LoRA (rank=128, alpha=256) on three datasets: Kvasir-VQA-x1 for QA pairs, synthetic explanations generated by Gemma-27B from metadata, and text-to-region pairs with ClipSeg pseudo-masks. Three task-specific tokens (<MedVQA>, <MedVQA_EXPLAIN>, <REFERRING_EXPRESSION_SEGMENTATION>) enable unified task switching. Training uses LR=5e-5, warmup_ratio=0.1, FP16, and effective_batch_size=12 for 1 epoch on 2×T4 GPUs. At inference, the model chains VQA prediction with localization and explanation generation.

## Key Results
- Multi-task training substantially improves performance over single-task baselines
- Best model achieves BLEU=0.4539, ROUGE-L=0.6531, BERTScore F1=0.9479 on private test set
- Visual grounding performance improves with multi-task training, avoiding overfitting to linguistic patterns
- Model demonstrates robust generalization across question types and image conditions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Joint training on visual grounding acts as a regularizer that preserves visual understanding while improving language quality.
- **Mechanism:** When trained on VQA alone with increasing LoRA capacity, the model overfits to linguistic patterns and visual grounding degrades (Seg-IoU Instrument: 0.4911 → 0.2961). Adding the segmentation task forces the model to maintain visual-feature representations, yielding simultaneous improvements in both Seg-IoU (0.7098 → 0.7403) and BLEU (0.4432 → 0.4726).
- **Core assumption:** The segmentation supervision signal is sufficiently aligned with VQA-relevant regions to serve as useful inductive bias.
- **Evidence anchors:**
  - [abstract] "multi-task setup enables the model to jointly learn visual grounding, reasoning, and interpretation"
  - [Section 3] "This suggests the model overfits to linguistic cues... segmentation task acts as an effective regularizer"
  - [corpus] S-Chain paper addresses visual-text alignment in medical VLMs but does not validate this specific regularization effect
- **Break condition:** If pseudo-masks are too noisy or misaligned with answer-relevant regions, grounding supervision may introduce conflicting gradients.

### Mechanism 2
- **Claim:** Task-specific tokens enable a single unified model to switch between VQA, explanation, and grounding modes without architectural changes.
- **Mechanism:** Florence-2's prompt-based task design is extended with three special tokens—<MedVQA>, <MedVQA_EXPLAIN>, and <REFERRING_EXPRESSION_SEGMENTATION>—that condition the decoder on the desired output modality. This allows shared visual representations while task tokens route to appropriate output heads.
- **Core assumption:** The model has sufficient capacity to learn distinct task distributions without catastrophic interference.
- **Evidence anchors:**
  - [Section 2.2] "we used three task-specific tokens: <MedVQA> for VQA, <MedVQA_EXPLAIN> for explanations, and <REFERRING_EXPRESSION_SEGMENTATION> for region grounding"
  - [Section 2.2] Masks were converted to Florence-2 location tokens for compatibility
  - [corpus] Molmo (cited in paper) demonstrated similar unified task handling with pointing and captioning
- **Break condition:** If task distributions diverge significantly (e.g., medical explanations vs. simple binary answers), interference may degrade per-task performance.

### Mechanism 3
- **Claim:** Synthetic explanation generation from metadata and visual descriptions provides structured reasoning supervision that improves answer coherence.
- **Mechanism:** The authors use Gemma-27B to synthesize explanations from ground-truth metadata (abnormality type, location) and visual descriptions, explicitly discouraging medical jargon to improve generalization. This creates 3,344 explanation samples that teach the model to articulate observable features.
- **Core assumption:** LLM-generated explanations sufficiently approximate expert reasoning patterns for transfer learning.
- **Evidence anchors:**
  - [Section 2.1] "used the Gemma-27B model with few-shot prompting to synthesize... into complete, well-structured textual explanations"
  - [Section 4] "Synthetic explanation data generation using Gemma-27B produced less diverse descriptions, hindering effective training"
  - [corpus] No direct validation in corpus; ProtoVQA addresses explainability but through prototypical learning, not synthetic data
- **Break condition:** If synthetic explanations lack diversity or contain systematic errors, the model may learn brittle reasoning patterns.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**
  - **Why needed here:** Full fine-tuning of large vision-language models is computationally prohibitive; LoRA enables efficient adaptation by injecting trainable low-rank matrices into attention modules while freezing base weights.
  - **Quick check question:** Can you explain why increasing LoRA rank from 32 to 128 improved language metrics but degraded visual grounding in single-task training?

- **Multi-Task Learning with Shared Representations**
  - **Why needed here:** The framework relies on a shared visual encoder learning representations useful for VQA, explanation, and segmentation simultaneously; understanding gradient interactions across tasks is essential for debugging performance tradeoffs.
  - **Quick check question:** What happens to task-specific performance if one task's loss dominates the combined objective?

- **Pseudo-Label Supervision**
  - **Why needed here:** The text-to-region dataset uses ClipSeg-generated pseudo-masks rather than expert annotations; understanding the tradeoffs of approximate supervision is critical for interpreting IoU results and planning improvements.
  - **Quick check question:** Why might pseudo-masks be useful for grounding even if they lack medical precision?

## Architecture Onboarding

- **Component map:** Florence-2 backbone (vision encoder + BERT decoder) -> LoRA adaptation layer -> Three task tokens (<MedVQA>, <MedVQA_EXPLAIN>, <REFERRING_EXPRESSION_SEGMENTATION>) -> Combined objective with VQA, explanation, and grounding losses

- **Critical path:**
  1. Curate and merge three datasets, ensuring 80/20 split with no image overlap
  2. Convert segmentation masks to Florence-2 location token format
  3. Fine-tune with LoRA on combined objective for 1 epoch
  4. At inference, route inputs through appropriate task tokens
  5. For Subtask 2: predict answer first, then localize with referring expression

- **Design tradeoffs:**
  - **LoRA capacity vs. grounding preservation:** Higher rank improves language but risks overfitting without grounding regularization
  - **Synthetic data scale vs. diversity:** Gemma-27B explanations provided scale but lacked diversity (noted as limitation)
  - **Pseudo-mask quality vs. annotation cost:** ClipSeg masks enable grounding supervision at scale but introduce noise

- **Failure signatures:**
  - **High BLEU, low Seg-IoU:** Model is overfitting to linguistic cues; add or strengthen grounding task
  - **Low explanation completeness scores (0.3–0.7):** Model struggles with descriptive attributes; may need richer explanation training data
  - **Poor performance on rare landmarks:** Class imbalance in grounding data; polyps overshadow other classes

- **First 3 experiments:**
  1. **Ablate grounding task:** Train VQA-only with same LoRA config to confirm regularization effect; expect Seg-IoU drop and language gain
  2. **Vary LoRA rank:** Test rank=32, 64, 128 on multi-task setup to find optimal capacity before interference
  3. **Analyze per-class grounding:** Compute Seg-IoU by abnormality type to identify which classes suffer from pseudo-mask noise vs. class imbalance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can pseudo-masks from the trained model be used as prompts for SAM to generate more precise segmentation masks that improve visual grounding performance?
- **Basis in paper:** [explicit] "Future work could leverage the model's pseudo-masking to propose regions for refinement, using them as prompts for the Segment Anything Model (SAM), which can generate more precise masks that may be used to train an auxiliary segmentation head alongside VQA training."
- **Why unresolved:** The authors used ClipSeg for pseudo-mask generation, which lacks medical precision, but did not test whether SAM-refined masks would improve the text-to-region task.
- **What evidence would resolve it:** A comparative study measuring Seg-IoU improvements on instruments, polyps, and pseudo-mask categories when using SAM-refined masks versus ClipSeg-only masks for multi-task training.

### Open Question 2
- **Question:** Does confidence scoring based on decoding stability reliably predict explanation quality across different question types?
- **Basis in paper:** [explicit] "We found that conflicting explanations scored lower, while correct ones scored higher, though further validation is needed."
- **Why unresolved:** The authors proposed confidence scores from average top-k probability mass but only anecdotally observed correlation with correctness; systematic validation was not conducted.
- **What evidence would resolve it:** Correlation analysis between confidence scores and human expert ratings of explanation correctness, completeness, and faithfulness across all question categories.

### Open Question 3
- **Question:** How does incorporating negative examples into the augmented datasets affect model bias and visual grounding performance?
- **Basis in paper:** [explicit] "the lack of negative examples in the augmented datasets may have also biased the model training."
- **Why unresolved:** The authors identified this limitation but did not experiment with adding negative samples to assess impact on bias reduction.
- **What evidence would resolve it:** Ablation experiments comparing model performance with and without negative examples, measuring changes in false positive rates and Seg-IoU across rare and common classes.

## Limitations

- Synthetic explanation generation produced less diverse descriptions, potentially limiting reasoning robustness
- Pseudo-mask quality from ClipSeg introduces noise in visual grounding supervision, particularly for subtle abnormalities
- Class imbalance in grounding data (polyps overrepresented) may bias the model toward common abnormalities

## Confidence

- **High confidence**: Multi-task framework's effectiveness in improving both visual grounding and language quality simultaneously
- **Medium confidence**: Task-specific tokens enable unified modality switching without interference
- **Low confidence**: Synthetic explanation data quality and its contribution to reasoning improvements

## Next Checks

1. **Ablation study with human-annotated explanations**: Replace synthetic explanation dataset with expert-annotated explanations (100 samples) and retrain to quantify impact on explanation completeness and reasoning coherence.

2. **Cross-validation of pseudo-mask quality**: Conduct expert review of stratified sample (5%) of ClipSeg-generated masks to establish ground truth accuracy. Correlate mask quality scores with Seg-IoU to quantify noise penalty.

3. **Zero-shot generalization test**: Evaluate fine-tuned model on external GI VQA dataset (if available) or Kvasir-VQA-x1 test set with novel abnormality combinations to assess true reasoning vs. memorization.