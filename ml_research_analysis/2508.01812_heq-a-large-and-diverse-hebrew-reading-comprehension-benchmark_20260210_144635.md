---
ver: rpa2
title: 'HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark'
arxiv_id: '2508.01812'
source_url: https://arxiv.org/abs/2508.01812
tags:
- hebrew
- dataset
- evaluation
- question
- wikipedia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces HeQ, a Hebrew reading comprehension benchmark\
  \ of 30,147 diverse QA pairs drawn from Wikipedia and Israeli tech news. Hebrew\u2019\
  s morphological richness poses challenges for standard span evaluation due to affixation-induced\
  \ boundary shifts, so the authors design a novel Token-Level Normalized Levenshtein\
  \ Similarity (TLNLS) metric that is less sensitive to morphological variations than\
  \ F1 or EM."
---

# HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark
## Quick Facts
- arXiv ID: 2508.01812
- Source URL: https://arxiv.org/abs/2508.01812
- Reference count: 13
- This work introduces HeQ, a Hebrew reading comprehension benchmark of 30,147 diverse QA pairs drawn from Wikipedia and Israeli tech news.

## Executive Summary
This work introduces HeQ, a Hebrew reading comprehension benchmark of 30,147 diverse QA pairs drawn from Wikipedia and Israeli tech news. Hebrew's morphological richness poses challenges for standard span evaluation due to affixation-induced boundary shifts, so the authors design a novel Token-Level Normalized Levenshtein Similarity (TLNLS) metric that is less sensitive to morphological variations than F1 or EM. They also refine annotation guidelines and a controlled crowdsourcing protocol to improve data quality and diversity. Experiments show mBERT outperforms Hebrew-specific models despite less Hebrew pretraining, highlighting the benefit of multilingual pretraining. Models trained on HeQ significantly outperform those trained on the earlier ParaShoot dataset, and data quality matters as much as size. The work advances Hebrew NLP evaluation and highlights the need for MRL-tailored metrics and diverse, high-quality data.

## Method Summary
The authors constructed HeQ by collecting Hebrew text passages from Wikipedia and Israeli tech news, then annotating them with diverse question-answer pairs through a controlled crowdsourcing protocol. They developed a novel Token-Level Normalized Levenshtein Similarity (TLNLS) metric to address morphological boundary issues in Hebrew MRC evaluation. The annotation guidelines were refined to improve data quality and diversity, and the resulting dataset of 30,147 QA pairs was used to benchmark various language models, including mBERT and Hebrew-specific models.

## Key Results
- mBERT outperforms Hebrew-specific models despite less Hebrew pretraining
- Models trained on HeQ significantly outperform those trained on the earlier ParaShoot dataset
- Data quality matters as much as size for Hebrew MRC performance

## Why This Works (Mechanism)
The Hebrew language's morphological richness creates challenges for standard span evaluation metrics because affixation can cause correct answers to be marked wrong due to minor boundary shifts. The TLNLS metric addresses this by measuring token-level similarity rather than exact span matching, making it more robust to morphological variations. The controlled crowdsourcing protocol ensures diverse and high-quality annotations, while the dataset's coverage of Wikipedia and tech news provides broad topical diversity. The multilingual pretraining of mBERT appears to provide advantages over Hebrew-specific models, possibly due to exposure to diverse linguistic patterns.

## Foundational Learning
1. **Morphologically Rich Languages (MRLs)**: Languages where words can have many forms through affixation, inflection, and derivation. Why needed: Understanding why Hebrew poses unique challenges for MRC evaluation. Quick check: Identify examples of Hebrew words with different affixes but related meanings.
2. **Reading Comprehension Benchmark Construction**: The process of creating datasets with passages and question-answer pairs for evaluating NLP models. Why needed: Understanding how HeQ was built and validated. Quick check: Compare HeQ's annotation guidelines with those of SQuAD.
3. **Token-Level Normalized Levenshtein Similarity (TLNLS)**: A metric that measures similarity at the token level rather than exact span matching. Why needed: Understanding the novel evaluation approach for Hebrew MRC. Quick check: Calculate TLNLS scores for sample answer pairs with morphological variations.

## Architecture Onboarding
Component Map: Passages -> Annotation Protocol -> QA Pairs -> TLNLS Evaluation -> Model Benchmarking
Critical Path: Text Collection → Annotation Guidelines Refinement → Crowdsourcing Protocol → Quality Control → Model Evaluation
Design Tradeoffs: Standard F1/EM metrics are simple but fail on morphological variations; TLNLS is more robust but requires token-level processing
Failure Signatures: Poor performance on morphologically complex answers, over-reliance on exact span matching, limited generalization to unseen domains
First Experiments: 1) Compare TLNLS vs F1 on a small Hebrew MRC dataset, 2) Test mBERT on Hebrew passages with varying morphological complexity, 3) Evaluate model performance across Wikipedia vs tech news domains

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does scaling the HeQ dataset to over 100,000 samples yield diminishing returns or distinct performance gains for Hebrew MRC models?
- Basis in paper: [explicit] In Section 5.2, the authors hypothesize that "to improve future models a 100K+ dataset is needed," based on the observation that increasing size from 15K to 30K yielded only a modest 3% TLNLS increase.
- Why unresolved: The current study only extrapolates the need for larger data; it does not experimentally validate performance at the 100K scale.
- What evidence would resolve it: Training models on an expanded HeQ dataset containing 100K+ high-quality samples and measuring the delta in TLNLS scores.

### Open Question 2
- Question: To what extent does the absence of "natural questions" in HeQ affect the generalization of models to real-world Hebrew information-seeking scenarios?
- Basis in paper: [explicit] Section 8 identifies the "Lack of Natural Questions" as a key limitation, noting that without authentic user queries, the dataset "may not fully capture the breadth... of real-world questions."
- Why unresolved: HeQ uses annotator-generated questions (similar to SQuAD) rather than organic search queries, potentially limiting its ecological validity.
- What evidence would resolve it: A comparative study evaluating models trained on HeQ against a dataset of organic Hebrew queries (e.g., search logs).

### Open Question 3
- Question: Is the Token-Level Normalized Levenshtein Similarity (TLNLS) metric robust and transferable to other Morphologically Rich Languages (MRLs) outside of Hebrew?
- Basis in paper: [inferred] While Section 4.1 lists "language independence" as a desired quality for the new metric, the empirical evaluation in Section 4.3 is restricted to Hebrew.
- Why unresolved: The authors aim for a general MRL metric but only demonstrate its efficacy in compensating for Hebrew's specific affixation patterns.
- What evidence would resolve it: Applying TLNLS to MRC datasets in other MRLs (e.g., Arabic, Turkish) and correlating scores with human evaluation of answer correctness.

## Limitations
- The dataset size (30,147 QA pairs) is moderate compared to large-scale benchmarks like SQuAD
- Focus on Wikipedia and Israeli tech news may limit generalizability to other domains
- The TLNLS metric, while addressing morphological boundary issues, is novel and lacks extensive validation against human judgments

## Confidence
- **High**: The benchmark construction methodology, including annotation guidelines and crowdsourcing protocol, is clearly described and reproducible.
- **Medium**: The empirical results showing mBERT's superior performance are robust but based on a limited model comparison.
- **Medium**: The effectiveness of TLNLS in addressing morphological challenges is demonstrated but requires further validation.

## Next Checks
1. Evaluate TLNLS against human judgments to confirm its superiority over F1/EM in Hebrew MRC.
2. Expand model comparisons to include more Hebrew-specific and multilingual models to strengthen the mBERT findings.
3. Test the dataset's generalizability by evaluating models on out-of-domain Hebrew texts.