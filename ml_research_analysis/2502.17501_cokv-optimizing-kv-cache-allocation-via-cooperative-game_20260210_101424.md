---
ver: rpa2
title: 'CoKV: Optimizing KV Cache Allocation via Cooperative Game'
arxiv_id: '2502.17501'
source_url: https://arxiv.org/abs/2502.17501
tags:
- cokv
- cache
- heads
- value
- shapley
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models require substantial memory for key-value
  cache storage, limiting their efficiency in long-text processing. To address this,
  we propose CoKV, a novel method that evaluates attention head importance through
  cooperative game theory, specifically using the Sliced Shapley value to assess collaborative
  contributions rather than independent importance.
---

# CoKV: Optimizing KV Cache Allocation via Cooperative Game

## Quick Facts
- **arXiv ID**: 2502.17501
- **Source URL**: https://arxiv.org/abs/2502.17501
- **Reference count**: 29
- **Primary result**: Achieves 97.29% model performance retention with only 1.6% of full KV cache budget

## Executive Summary
CoKV addresses the memory bottleneck of long-context language model inference by dynamically allocating KV cache budgets to attention heads based on their collaborative importance scores derived from cooperative game theory. The method uses Sliced Shapley Value to evaluate how heads contribute when working together rather than independently, enabling selective eviction that maintains performance while reducing memory usage by 64% and decoding latency by 50%. Experiments on LongBench show CoKV outperforms existing KV cache compression methods and even achieves better performance than full cache when retaining 512 tokens on average.

## Method Summary
CoKV employs cooperative game theory to evaluate attention head importance through Sliced Shapley Value (SSV), measuring each head's collaborative contribution across sampled coalition sizes. The method treats each GQA group as a player in a cooperative game and computes the expected complementary contribution of each head to model accuracy when included in different coalitions. After computing SSV scores on a validation set, heads are normalized and the lowest-scoring α heads receive zero additional cache budget. During inference, cache budgets are allocated proportionally to normalized SSV scores plus a local window, with token eviction within each head performed using SnapKV-style attention pooling. The approach integrates seamlessly with FlashAttention and GQA architectures.

## Key Results
- Achieves 97.29% of full KV cache performance using only 1.6% of the cache budget (average 128 tokens per group)
- Reduces decoding latency by 50% and peak memory usage by 64% compared to full cache
- Outperforms full KV cache when retaining 512 tokens on average across LongBench tasks
- Maintains task performance with minimal degradation even under aggressive compression (10.3% degradation at 64-token budget)

## Why This Works (Mechanism)

### Mechanism 1: Collaborative Head Importance via Sliced Shapley Value
The method models attention heads as players in a cooperative game, computing their Sliced Shapley Value to measure collaborative contributions rather than independent importance. By sampling coalition sizes H={32,64,96,128} and computing complementary contributions U(S) - U(N\S), CoKV captures how heads work together to improve model performance. The assumption that complementary contribution distributions correlate with true Shapley values across coalition sizes enables efficient approximation without sampling all possible coalition sizes.

### Mechanism 2: Complementary Contribution for Computational Efficiency
Using complementary contributions U(S) - U(N\S) instead of marginal contributions enables efficient multi-player Shapley estimation. This approach allows updating all players in coalition S simultaneously, reducing required model forward passes from O(n²) to O(|H|). The utility function U(S), defined as model accuracy when heads in N\S are masked, provides the foundation for computing importance scores that reflect actual task performance.

### Mechanism 3: Negative-Contribution Head Identification and Selective Eviction
CoKV identifies heads that contribute negatively to model performance by analyzing SSV scores and applying min-max normalization with threshold α. Heads with the α-lowest SSV scores receive zero additional budget, removing cache from heads that may introduce noise or irrelevant activations. This selective eviction not only optimizes storage and decoding speed but can also enhance overall model performance by eliminating detrimental contributions.

## Foundational Learning

- **Shapley Value in Cooperative Game Theory**: Core mathematical framework for attributing contribution to players in coalitions; understanding marginal vs. complementary contributions is essential for grasping SSV efficiency. *Quick check: Can you explain why complementary contribution U(S) - U(N\S) can update multiple players' values while marginal contribution cannot?*

- **KV Cache Mechanics in Transformer Decoding**: Understanding how K, V tensors grow linearly with sequence length and why eviction is necessary for long-context efficiency. *Quick check: In autoregressive decoding, why must KV cache be retained across token generations but can be compressed within a single forward pass?*

- **Grouped Query Attention (GQA)**: CoKV treats GQA groups (not individual heads) as players; GQA shares KV across head groups, changing the granularity of importance evaluation. *Quick check: How does treating a GQA group as a single player differ from evaluating each attention head independently in MHA?*

## Architecture Onboarding

- **Component map**: Validation dataset (15% split) → SSV precomputation (Algorithm 1) → Normalized importance scores → Cache budget allocation (Equation 3) → Token eviction (Algorithm 2) → Compressed KV cache
- **Critical path**: 1) SSV precomputation must complete before inference (20.93 hours for Llama-3-8B with 250 samples per coalition size on 4× RTX 3090) 2) Budget allocation per head depends on normalized SSV scores 3) Token eviction within each head uses local-window attention pooling (SnapKV integration) 4) GQA compatibility: each group of 4 heads shares one cache allocation decision
- **Design tradeoffs**: Precomputation cost vs. inference efficiency (higher M improves accuracy but increases offline compute); α selection controls aggressiveness of negative-head suppression; coalition size selection H={32,64,96,128} is empirically chosen
- **Failure signatures**: SSV instability (MAE between runs exceeds 1/n → increase M); task mismatch (applying scores across task types degrades performance → verify cross-task generalization); severe compression artifacts (check α is not too aggressive or task requires more retention)
- **First 3 experiments**: 1) SSV convergence validation: Compute SSV twice with different seeds on HotpotQA; verify MAE < 1/256 for Llama-3-8B-Instruct 2) Ablation on coalition sizes: Compare H={64,128} vs H={32,64,96,128} to measure sensitivity 3) Masking sanity check: Mask top-k vs bottom-k heads by SSV; confirm top-k masking degrades performance significantly more than bottom-k

## Open Questions the Paper Calls Out

- **Can computational complexity be further reduced through parallelization or approximation?** The authors acknowledge SSV precomputation requires significant GPU resources (20+ hours) and plan to develop efficient approximation algorithms and parallel computing strategies.

- **How does CoKV perform when combined with KV cache quantization techniques?** The paper notes that combining eviction and quantization is an interesting direction for future research, as it's unclear if CoKV-identified important heads retain utility when compressed to low-bit representations.

- **Can a unified importance profile be developed to handle diverse tasks without per-task precomputation?** The authors recognize the task-specific constraint requiring separate profiling for different task types and suggest meta-learning approaches that predict head importance dynamically from input prompts.

## Limitations
- High computational overhead during SSV precomputation phase (20.93 hours for Llama-3-8B on 4× RTX 3090)
- Dependence on accurate utility function U(S) that correlates with downstream task performance
- Current implementation assumes GQA architectures, limiting direct applicability to standard multi-head attention
- Empirical selection of coalition sizes H={32,64,96,128} lacks theoretical justification

## Confidence
- **High Confidence**: The core mechanism of using cooperative game theory to evaluate head importance through Sliced Shapley Values is mathematically sound and well-supported by empirical evidence
- **Medium Confidence**: Performance claims rely on specific task distributions in LongBench and may not generalize to all domains; cross-task generalization shows within-category transfer works but between-category degrades significantly
- **Low Confidence**: Exact utility function U(S) and its correlation with downstream metrics remains underspecified; negative-contribution mechanism effectiveness depends heavily on α selection without full characterization

## Next Checks
1. **SSV Stability Analysis**: Reproduce two-independent-run validation with MAE computation on HotpotQA to verify reported MAE < 1/256 threshold is achievable and that increasing M samples improves stability

2. **Cross-Task Utility Transfer**: Apply SSV scores computed on general knowledge QA to mathematical reasoning tasks to quantify performance degradation and validate within-category generalization claims

3. **Negative Head Impact Verification**: Conduct systematic ablation studies masking top-k, bottom-k, and random-k heads by SSV score across multiple datasets to confirm asymmetric impact pattern (severe degradation for top-k, potential improvement for bottom-k)