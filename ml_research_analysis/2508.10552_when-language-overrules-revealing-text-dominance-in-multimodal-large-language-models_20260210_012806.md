---
ver: rpa2
title: 'When Language Overrules: Revealing Text Dominance in Multimodal Large Language
  Models'
arxiv_id: '2508.10552'
source_url: https://arxiv.org/abs/2508.10552
tags:
- modality
- text
- attention
- dominance
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of text dominance in multimodal\
  \ large language models, where models over-rely on textual inputs while underutilizing\
  \ other modalities like images, video, audio, time-series, and graphs. The authors\
  \ propose two metrics\u2014Modality Dominance Index (MDI) and Attention Efficiency\
  \ Index (AEI)\u2014to quantify this imbalance across different modalities and model\
  \ architectures."
---

# When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2508.10552
- Source URL: https://arxiv.org/abs/2508.10552
- Reference count: 6
- Primary result: This paper addresses the problem of text dominance in multimodal large language models, where models over-rely on textual inputs while underutilizing other modalities like images, video, audio, time-series, and graphs. The authors propose two metrics—Modality Dominance Index (MDI) and Attention Efficiency Index (AEI)—to quantify this imbalance across different modalities and model architectures. Through systematic analysis, they identify three causes: token redundancy in non-textual modalities, fusion architecture design, and task formulations favoring text. To address this, they introduce a token compression method guided by CLS token attention, which reduces redundant tokens in non-textual modalities. Applying this to LLaVA-7B reduces MDI from 10.23 to 0.86, demonstrating significant rebalancing of cross-modal attention and more equitable multimodal integration.

## Executive Summary
This paper investigates the pervasive problem of text dominance in multimodal large language models, where textual inputs disproportionately influence model decisions at the expense of other modalities. The authors systematically analyze why this occurs and propose both metrics to quantify the imbalance and a novel compression method to address it. Their work reveals that multimodal models often fail to truly integrate information across modalities, instead relying heavily on text while underutilizing visual, audio, or other non-textual inputs.

The study demonstrates that this text dominance stems from three primary sources: excessive token redundancy in non-textual modalities, fusion architecture design choices, and task formulations that inherently favor textual reasoning. Through their proposed token compression approach, the authors show significant improvements in cross-modal attention balance, reducing the Modality Dominance Index from 10.23 to 0.86 in LLaVA-7B models. This work provides both diagnostic tools and practical solutions for developing more genuinely multimodal systems.

## Method Summary
The authors develop a comprehensive framework for diagnosing and addressing text dominance in multimodal models. They introduce two key metrics: the Modality Dominance Index (MDI) to quantify the imbalance between text and non-text modalities, and the Attention Efficiency Index (AEI) to measure how effectively attention mechanisms utilize different modalities. The core technical contribution is a token compression method that identifies and removes redundant tokens in non-textual modalities based on their attention patterns relative to the CLS token. This compression is guided by analyzing cross-modal attention distributions and aims to reduce token redundancy while preserving semantic information. The method is applied to LLaVA-7B models, demonstrating significant reductions in text dominance while maintaining or improving task performance.

## Key Results
- MDI scores in LLaVA-7B models decreased from 10.23 to 0.86 after applying token compression
- The compression method reduced redundant tokens in non-textual modalities by 60-80% while preserving performance
- Cross-modal attention became more balanced, with visual tokens receiving proportionally more attention
- The proposed metrics (MDI and AEI) successfully identified and quantified text dominance across different model architectures

## Why This Works (Mechanism)
The token compression method works by identifying and removing redundant tokens in non-textual modalities that receive disproportionately low attention relative to the CLS token. By compressing these redundant tokens, the model can allocate more attention capacity to the remaining informative tokens across all modalities, thereby reducing the inherent bias toward text tokens that typically have higher semantic density and more diverse attention patterns.

## Foundational Learning
- **Modality Dominance Index (MDI)**: A metric quantifying the imbalance between text and non-text modality influence in multimodal models. Why needed: To objectively measure and compare text dominance across different models and architectures. Quick check: Calculate MDI for a simple bimodal model and verify it increases when text tokens dominate attention patterns.

- **Attention Efficiency Index (AEI)**: Measures how effectively attention mechanisms utilize different modalities. Why needed: To distinguish between models that use modalities efficiently versus those that are biased toward certain modalities. Quick check: Compare AEI values for models with different fusion strategies to validate it captures efficiency differences.

- **Cross-modal attention analysis**: The study of how attention mechanisms distribute focus across different input modalities. Why needed: To identify patterns of modality preference and redundancy. Quick check: Visualize attention heatmaps to confirm that compressed tokens show reduced attention weights post-processing.

## Architecture Onboarding
- **Component map**: Input modalities (text, image, audio, etc.) → Tokenizer modules → Fusion architecture (Q-former, cross-attention) → Multimodal LLM backbone → Output generation
- **Critical path**: Tokenization → Redundancy identification (CLS-based attention analysis) → Token compression → Cross-modal fusion → Multimodal reasoning
- **Design tradeoffs**: The compression method trades some representational capacity in non-textual modalities for more balanced attention distribution. The approach must balance token reduction against information preservation.
- **Failure signatures**: Over-compression leading to loss of critical information, under-compression failing to address text dominance, or compression that disproportionately affects certain types of visual features.
- **First experiments**: 1) Apply compression to a unimodal text model and verify no MDI change occurs. 2) Test compression on a simple bimodal toy model to validate the mechanism. 3) Apply varying compression ratios to observe the relationship between compression level and MDI reduction.

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses primarily on image-text models (LLaVA series) with limited evaluation on other multimodal combinations (audio, video, time-series)
- The CLS-token-based compression approach may not be optimal for all fusion architectures
- The analysis does not fully explore how different task formulations beyond visual question answering contribute to text dominance

## Confidence
- **High confidence**: The identification of three main causes of text dominance (token redundancy, fusion architecture, task formulation)
- **Medium confidence**: The effectiveness of the proposed compression method in reducing MDI scores
- **Medium confidence**: The generalizability of findings across different multimodal model families

## Next Checks
1. Test the compression method on non-vision modalities (audio, time-series) to assess cross-modal applicability
2. Evaluate the proposed metrics on alternative fusion architectures beyond Q-former designs
3. Conduct ablation studies varying task formulations while keeping model architecture constant to isolate contribution of each cause