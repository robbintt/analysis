---
ver: rpa2
title: Can Large Language Models Grasp Concepts in Visual Content? A Case Study on
  YouTube Shorts about Depression
arxiv_id: '2503.05109'
source_url: https://arxiv.org/abs/2503.05109
tags:
- mllm
- image
- concepts
- visual
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores how multimodal large language models (MLLMs)
  can assist in analyzing video content related to depression by interpreting abstract
  visual concepts like presenting style, interacting style, arousal, and diversity.
  Using LLaVA-1.6 Mistral 7B to analyze 725 keyframes from 142 YouTube Shorts, the
  research compares AI interpretations with human understanding across four prompt
  configurations varying in detail and operationalization.
---

# Can Large Language Models Grasp Concepts in Visual Content? A Case Study on YouTube Shorts about Depression

## Quick Facts
- arXiv ID: 2503.05109
- Source URL: https://arxiv.org/abs/2503.05109
- Reference count: 40
- Primary result: MLLMs perform well on straightforward visual concepts but struggle with complex performative concepts, especially when prompts are overly specific

## Executive Summary
This study investigates whether multimodal large language models can accurately interpret abstract visual concepts in video content related to depression. Using LLaVA-1.6 Mistral 7B to analyze 725 keyframes from 142 YouTube Shorts, researchers compared AI interpretations with human understanding across four prompt configurations varying in detail and operationalization. The findings reveal that while MLLMs perform well on straightforward concepts like arousal and diversity, they struggle significantly with more complex performative concepts like presenting and interacting styles, particularly when video genres include non-human elements or conflicting visual-textual information.

## Method Summary
The researchers collected 142 YouTube Shorts videos related to "depression" and extracted 725 keyframes using FFmpeg with an SSIM threshold of >0.3 to ensure visual distinctness. LLaVA-1.6 Mistral 7B was used to analyze each keyframe using four different prompt configurations (Naive, Simple, Detailed, Open-minded) for four abstract concepts: presenting style, interacting style, arousal, and diversity. A separate Llama-3.1-8B-Instruct model parsed the MLLM's textual explanations into structured labels. Human-AI alignment was measured through bootstrapped consistency scores, supplemented by qualitative thematic analysis of the model's explanations.

## Key Results
- MLLMs show high alignment with humans on straightforward visual concepts like arousal and diversity, but significantly lower alignment on complex performative concepts like presenting and interacting styles
- Increased prompt detail does not consistently improve alignment, with detailed prompts sometimes causing the model to reject valid instances that don't match specific examples provided
- MLLMs prioritize textual overlays over visual context when modalities conflict, leading to systematic misinterpretations of visual intent

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Induced Constraint (Over-Operationalization)
When prompts provide detailed definitions with specific examples, MLLMs appear to retrieve these narrow prototypes during inference, causing them to reject valid instances that lack the specific visual markers listed in the prompt. This over-reliance on surface-level lexical-visual matching leads to poor generalization for informal or non-standard contexts.

### Mechanism 2: Modal Hierarchy and Conflict Resolution
MLLMs prioritize textual overlays over visual context when synthesizing multimodal inputs, particularly in user-generated content where text acts as a narrative anchor. This creates systematic misalignment when visual and textual signals conflict, as the model uses text to "explain" the image rather than interpreting visual modality independently.

### Mechanism 3: Concept Complexity Threshold
MLLM alignment is conditional on the "objectivity" of the concept—models align well with low-context visual features (color, motion) but struggle with high-context performative intent that requires Theory of Mind. Pre-training objectives favor statistical correlation over causal or intentional reasoning.

## Foundational Learning

- **Keyframe Extraction vs. Temporal Understanding**: The paper uses static keyframes instead of video sequences, discarding temporal dynamics and audio information that could be critical for interpreting concepts like arousal or narrative flow.
  - *Quick check*: If a video has a static image but high-arousal audio (screaming), would the keyframe approach correctly classify the arousal level?

- **Operationalization in Prompt Engineering**: The study demonstrates that "prompt detail" is not monotonic—defining concepts abstractly vs. giving specific examples determines whether the model generalizes or overfits to the prompt.
  - *Quick check*: For detecting "leadership," should you define it abstractly or list examples (e.g., "suit and tie," "standing at podium")? What's the risk of the latter?

- **Human-AI Alignment (vs. Accuracy)**: In social science, there's no objective "ground truth"—the goal is to measure how closely AI matches human subjectivity (Inter-coder reliability), not accuracy.
  - *Quick check*: If the MLLM correctly identifies "diversity" based on pixel count but disagrees with a human who interprets "diversity" as cultural representation, is this a model failure or operationalization failure?

## Architecture Onboarding

- **Component map**: YouTube Data API → YoutubeDownloader → FFmpeg (SSIM filter) → Static Keyframes (725 frames) → LLaVA-1.6 Mistral 7B (Prompt + Keyframe → Annotation + Explanation) → Llama-3.1-8B-Instruct (Parses output) → Bootstrap Alignment Score + Thematic Analysis

- **Critical path**: The interaction between Prompting Strategy and Parsing Logic is crucial—if the MLLM generates explanations that contradict its own annotation (hallucination), the parser fails or misclassifies the result.

- **Design tradeoffs**: Detailed prompts reduce false positives for standard genres but increase false negatives for creative/short-form genres; using image-LLMs on keyframes is computationally cheaper but loses audio/temporal context.

- **Failure signatures**:
  - "Literalism" Trap: Model claims "No presenting style" because it literally sees no PowerPoint slides, missing informal presentation context
  - Text-Visual Confusion: Model outputs "No interaction" based on visual staticness but contradicts itself by noting text overlay "implies a narrative"

- **First 3 experiments**:
  1. Genre Sensitivity Test: Run pipeline on 50 formal news clips vs. 50 informal vlogs to quantify drop in "Presenting" alignment
  2. Noise Injection: Add "Open-minded" tags to "Detailed" prompts to see if explicitly instructing the model to ignore specific examples recovers alignment for informal genres
  3. Text-Ablation: Blur text overlays in keyframes to force model to rely purely on visual cues, measuring how much alignment changes for "Interacting" vs. "Arousal"

## Open Questions the Paper Calls Out

### Open Question 1
Does incorporating video temporality improve MLLM alignment for abstract concepts like emotional valence? The authors note that "concepts like emotional valence and genre often depend on a holistic understanding of the video's overall narrative" and suggest exploring MLLMs that interpret video sequences. This remains unresolved because the current study relied on isolated keyframes due to computational constraints and limited context windows.

### Open Question 2
How does integrating audio and transcripts with visual data affect MLLM alignment when modalities conflict? The paper identifies "Synthesizing multimodal inputs" as a future direction, noting that "conflicting information across different modalities can complicate interpretations." This is unresolved because the current workflow decoded videos into keyframes only, excluding audio and transcript data.

### Open Question 3
What human-centered auditing frameworks effectively mitigate systematic biases in MLLM video analysis? The authors call for future work to "incorporate human-centered evaluation as a standard step in MLLM-assisted content analysis workflows." This remains unresolved as the study identified systematic biases (e.g., with non-human genres) but did not propose or validate a specific auditing intervention.

## Limitations
- Reliance on static keyframes rather than full video sequences discards temporal dynamics and audio information critical for interpreting concepts like arousal
- Human-AI alignment metric lacks external validation benchmark—there is no "ground truth" beyond human consensus
- Selection of 142 videos from an initial pool of 3,892 using unspecified criteria introduces potential sampling bias

## Confidence

**High Confidence:** The core finding that MLLMs struggle with complex performative concepts (Presenting, Interacting) compared to straightforward visual features (Arousal, Diversity) is well-supported by both quantitative alignment scores and qualitative analysis of explanations.

**Medium Confidence:** The observation that increased prompt detail does not consistently improve alignment requires cautious interpretation, as the study only tested four discrete prompt configurations without exploring intermediate variations.

**Medium Confidence:** The mechanism suggesting MLLMs prioritize textual overlays over visual context is plausible but based primarily on qualitative analysis rather than controlled experiments that would definitively prove this modality hierarchy.

## Next Checks
1. **Genre-Sensitivity Validation:** Run the pipeline on a balanced set of 100 formal news clips versus 100 informal vlogs to quantify the exact magnitude of alignment drop for "Presenting" style across different content genres.

2. **Text-Ablation Experiment:** Systematically blur or remove text overlays from a stratified sample of keyframes to measure how much alignment for "Interacting" and "Presenting" changes when the model cannot rely on textual cues.

3. **Prompt-Continuum Exploration:** Design and test five intermediate prompt configurations between "Naive" and "Detailed" (e.g., "Simple+Examples," "Abstract+Context") to map the relationship between prompt specificity and alignment more precisely.