---
ver: rpa2
title: A Comprehensive Dataset for Human vs. AI Generated Image Detection
arxiv_id: '2601.00553'
source_url: https://arxiv.org/abs/2601.00553
tags:
- image
- dataset
- images
- detection
- ai-generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MS COCOAI, a comprehensive dataset of 96,000
  real and AI-generated images for detecting synthetic media. Built on MS COCO captions,
  the dataset features images generated by five leading models (Stable Diffusion 3,
  SDXL, SD 2.1, DALL-E 3, MidJourney v6), with each caption used to produce aligned
  real and synthetic image pairs.
---

# A Comprehensive Dataset for Human vs. AI Generated Image Detection

## Quick Facts
- **arXiv ID:** 2601.00553
- **Source URL:** https://arxiv.org/abs/2601.00553
- **Reference count:** 21
- **Primary result:** 96k-image dataset with binary (0.801 accuracy) and 6-class (0.449 accuracy) detection baselines

## Executive Summary
This paper introduces MS COCOAI, a large-scale dataset for detecting AI-generated images from real photographs. The dataset contains 96,000 image-caption pairs, with 16k real MS COCO images and 16k generated images from each of five leading models (Stable Diffusion 3, SDXL, SD 2.1, DALL-E 3, MidJourney v6). The dataset supports two tasks: binary classification of real vs. AI-generated images, and model attribution to identify the specific generator. A baseline ResNet-50 model using frequency-domain features achieves 80.1% accuracy for binary detection but only 44.9% for model attribution, demonstrating the challenging nature of distinguishing between different AI generators.

## Method Summary
The dataset construction leverages MS COCO captions to create aligned real and synthetic image pairs across five leading generative models. For the baseline evaluation, a ResNet-50 classifier is used with frequency-domain representations obtained through 2D Fourier Transform preprocessing. The method converts images to grayscale, applies FFT, and uses the magnitude spectrum as input features. The dataset is split into 42k training, 9k validation, and 45k test images, with perturbations available for robustness studies but not used in the baseline evaluation.

## Key Results
- Baseline ResNet-50 with frequency-domain features achieves 0.801 accuracy for binary real vs. AI detection
- Model attribution task shows significantly lower performance at 0.449 accuracy
- Dataset contains 96,000 images across 6 classes (real + 5 generators)
- Binary detection performs substantially better than fine-grained model attribution

## Why This Works (Mechanism)
The frequency-domain approach captures global statistical patterns and high-frequency artifacts characteristic of generative models. By converting images to the Fourier domain before classification, the method focuses on structural and noise patterns rather than pixel-level details. This representation appears particularly effective for the binary detection task but struggles with distinguishing between specific generators due to their similar high-level characteristics.

## Foundational Learning
- **Frequency-domain analysis**: Understanding FFT and spectral features for image representation
  - *Why needed*: Captures global patterns and artifacts missed by spatial domain
  - *Quick check*: Verify magnitude spectrum preserves discriminative information
- **Multi-class attribution**: Distinguishing between 5 generative models requires fine-grained feature extraction
  - *Why needed*: Goes beyond binary detection to identify specific model fingerprints
  - *Quick check*: Ensure label encoding supports 6-class classification
- **Dataset alignment**: Using same captions for real and synthetic pairs enables controlled comparison
  - *Why needed*: Removes semantic variability as confounding factor
  - *Quick check*: Confirm caption-image pairing consistency across splits

## Architecture Onboarding
- **Component map**: COCO captions -> 5 generators -> image pairs -> preprocessing -> ResNet-50 -> classification
- **Critical path**: Image preprocessing (grayscale + FFT) -> ResNet-50 feature extraction -> softmax classification
- **Design tradeoffs**: Frequency-domain features trade spatial detail for robustness to common perturbations
- **Failure signatures**: Low Task B accuracy suggests insufficient model-specific feature extraction; large train-val gap indicates overfitting
- **First experiments**: 1) Train ResNet-50 with grayscale FFT input, 2) Compare pretrained vs from-scratch initialization, 3) Test different learning rates (1e-4 vs 1e-3)

## Open Questions the Paper Calls Out
1. **Can sophisticated model fingerprinting techniques significantly improve the low baseline accuracy (44.9%) observed for model attribution (Task B)?**
   - The baseline ResNet-50 model failed to distinguish effectively between the five specific generators, suggesting the visual features used were insufficient for fine-grained attribution.

2. **Does utilizing the semantic alignment between captions and images via cross-modal learning improve detection performance over unimodal baselines?**
   - The baseline relies solely on frequency-domain visual features, ignoring the textual modality despite the dataset being specifically constructed with caption-aligned pairs.

3. **How robust are detection methods against the specific perturbations included in the dataset?**
   - The paper details perturbations but does not present specific performance metrics on the perturbed subsets, leaving the stability of the 0.801 Task A accuracy under these degradations unknown.

## Limitations
- Model attribution remains significantly more challenging than binary detection
- Baseline method uses only visual features, ignoring available caption information
- Limited evaluation of detector robustness against included perturbations

## Confidence
- **Dataset construction:** High - well-defined and publicly available
- **Experimental framework:** High - clear task definitions and metrics
- **Baseline reproduction:** Low - preprocessing details and training hyperparameters unspecified
- **Exact accuracy matching:** Low - depends on ambiguous preprocessing and training configurations

## Next Checks
1. Verify the exact preprocessing pipeline: confirm grayscale conversion before FFT, apply log(1+magnitude) scaling, and replicate to 3 channels for ResNet-50 input
2. Test multiple training configurations: compare from-scratch vs ImageNet-pretrained ResNet-50, test Adam with lr=1e-4 and lr=1e-3, batch sizes 32 and 64
3. Validate label encoding: ensure 6-class outputs (real + 5 generators) with proper mapping from label_2 to class indices