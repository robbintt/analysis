---
ver: rpa2
title: 'Towards Spectroscopy: Susceptibility Clusters in Language Models'
arxiv_id: '2601.12703'
source_url: https://arxiv.org/abs/2601.12703
tags:
- after
- math
- clusters
- token
- grammar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces susceptibility clustering as a method to\
  \ discover interpretable structure in language models by treating the model's response\
  \ to data perturbations as a spectrum to be analyzed. The approach measures susceptibilities\u2014\
  covariances between component-level observables and data distribution perturbations\u2014\
  using localized Gibbs posteriors via SGLD."
---

# Towards Spectroscopy: Susceptibility Clusters in Language Models

## Quick Facts
- arXiv ID: 2601.12703
- Source URL: https://arxiv.org/abs/2601.12703
- Reference count: 40
- Key outcome: Susceptibility clustering identifies 510 interpretable clusters in Pythia-14M, with 50.8% matching SAE features from Pythia-70M

## Executive Summary
This paper introduces susceptibility clustering as a method to discover interpretable structure in language models by treating the model's response to data perturbations as a spectrum to be analyzed. The approach measures susceptibilities—covariances between component-level observables and data distribution perturbations—using localized Gibbs posteriors via SGLD. The authors find that tokens following contexts "for similar reasons" cluster together, with clusters ranging from grammatical patterns to code structure to mathematical notation. The methodology is validated by matching half of the discovered clusters to SAE features from a larger model, suggesting both methods recover genuine patterns in the data distribution.

## Method Summary
The method computes susceptibilities as covariances between component observables (specific attention head weight combinations) and data distribution perturbations, using pSGLD sampling from localized Gibbs posteriors centered at trained model checkpoints. For each context-continuation pair, the algorithm runs pSGLD to generate samples from the posterior, computes per-component susceptibilities, and aggregates these into a vector representation. The susceptibility matrix is preprocessed by standardizing columns then zero-meaning rows, then converted into a symmetrized k-nearest neighbor graph with self-tuning RBF weights. Iterative ACL local clustering finds minimum-conductance clusters by computing personalized PageRank from random seeds, rejecting clusters that are too large or too small, and removing discovered nodes until fewer than 0.1% remain.

## Key Results
- Identified 510 interpretable clusters in Pythia-14M ranging from grammatical patterns to code structure to mathematical notation
- Half of these clusters (50.8%) match SAE features from Pythia-70M, validating the approach
- Clusters persist across model scales, suggesting they reflect genuine data distribution patterns
- Using Gaussian posterior (nβ=0) instead of true posterior (nβ=3) yields inferior results: fewer clusters (288 vs 510) and loss of functional separation

## Why This Works (Mechanism)
The method works because susceptibilities capture how model components respond to data perturbations, revealing underlying structure in the data distribution. By measuring covariances between component observables and perturbations via localized Gibbs sampling, the approach effectively probes the geometry of the loss landscape around the trained model. Tokens that cluster together share similar susceptibility patterns because they follow contexts for similar underlying reasons—whether grammatical, semantic, or structural. The localized posterior ensures sampling focuses on relevant perturbations near the data distribution, while the conductance-based clustering algorithm naturally identifies well-separated groups with coherent internal structure.

## Foundational Learning
- **Susceptibility**: Covariance between model component observables and data perturbations; needed to measure component-level responses to perturbations; quick check: verify χ^C_xy = −Cov[ϕ_C(w), ℓ_xy(w) − L(w)] implementation
- **Localized Gibbs posterior**: Posterior distribution p(w|D, γ) ∝ p(w)exp{−γ/2||w−w*||²−γL(w)}; needed to focus sampling on relevant perturbations near trained model; quick check: confirm γ=300 creates appropriate localization
- **Conductance clustering**: Graph partitioning method that minimizes edge cut relative to cluster volume; needed to find well-separated interpretable clusters; quick check: verify minimum-conductance prefix computation in PageRank
- **SGLD sampling**: Stochastic gradient Langevin dynamics for approximate posterior sampling; needed to generate samples from localized posterior; quick check: confirm RMSProp preconditioner parameters (ε=1e-5)
- **Self-tuning RBF weights**: Edge weights w(x,y)=exp(−||x−y||²/(σ_x·σ_y)) where σ is distance to k-th neighbor; needed to create adaptive similarity graph; quick check: verify k=45 neighbor selection
- **Personalized PageRank**: Random walk with teleportation used for local clustering; needed to rank nodes by proximity to seed; quick check: confirm α=0.001 teleportation rate

## Architecture Onboarding

**Component Map**: pSGLD sampling -> Susceptibility computation -> Matrix preprocessing -> Graph construction -> ACL clustering -> Cluster output

**Critical Path**: The most time-consuming step is pSGLD sampling (100 draws, 55 steps between draws, batch size 16), which dominates computational cost. This must complete successfully before any subsequent steps can proceed.

**Design Tradeoffs**: The choice between Gaussian posterior (nβ=0) and true posterior (nβ=3) represents a fundamental tradeoff between computational efficiency and clustering quality. The paper demonstrates that nβ=3 yields substantially better results (510 vs 288 clusters, better functional separation) but requires more computation. Similarly, the conductance-based clustering algorithm trades off between finding small, pure clusters versus larger, more comprehensive groupings.

**Failure Signatures**: Algorithm finds only main body, no peripheral clusters (indicates preprocessing order error or incorrect rejection threshold). Clusters lack interpretability (suggests improper susceptibility computation or poor component observable definition). Excessive computation time with poor results (indicates pSGLD sampling issues or inappropriate hyperparameter choices).

**First Experiments**:
1. Run pSGLD with nβ=0 versus nβ=3 on identical data to verify the paper's claim that nβ=0 yields fewer clusters (288 vs 510) and loss of functional separation
2. Vary k-nearest neighbors parameter (k=30, 45, 60) in graph construction to assess robustness of cluster structure
3. Apply susceptibility clustering to a different model architecture (GPT-2 Small) to test generalizability beyond Pythia models

## Open Questions the Paper Calls Out
None specified.

## Limitations
- Implementation details for component observables and localized prior sampling are underspecified, particularly how delta constraints are enforced during SGLD and how the prior integrates with RMSProp preconditioning
- Only partial validation through SAE feature matching (50.8% overlap) raises questions about whether susceptibility clustering captures the same or complementary structure
- Limited exploration of hyperparameter sensitivity, particularly for Gibbs sampling and clustering parameters

## Confidence
- High confidence: Theoretical decomposition of susceptibilities into data distribution modes is mathematically sound; clustering methodology using conductance is well-established; clusters persisting across scales is robustly demonstrated
- Medium confidence: Claim of discovering "interpretable structure" is supported but could benefit from more systematic functional validation; 50.8% SAE match rate validates approach but reveals substantial differences
- Low confidence: Exact relationship between susceptibility clusters and SAE features remains unclear given only partial overlap; sensitivity to hyperparameter choices is not explored

## Next Checks
1. Implement controlled ablation comparing susceptibility clustering with Gaussian posterior (nβ=0) versus true posterior (nβ=3) on identical dataset to verify claimed differences in cluster count and functional separation
2. Systematically vary k-nearest neighbors parameter in graph construction (k=30, 45, 60) and measure effects on cluster counts, conductance values, and interpretability
3. Apply susceptibility clustering to different model architecture (GPT-2 Small) using same procedure to test generalizability beyond Pythia models