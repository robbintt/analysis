---
ver: rpa2
title: 'ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models
  using Pareto High-quality Data'
arxiv_id: '2504.16628'
source_url: https://arxiv.org/abs/2504.16628
tags:
- preference
- training
- data
- paretohqd
- assistant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ParetoHqD, an offline multiobjective alignment
  method for large language models that addresses issues of preference representation
  and data imbalance in existing approaches. The method represents human preferences
  as directions in the objective space and uses Pareto-optimal high-quality data for
  training.
---

# ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data

## Quick Facts
- **arXiv ID:** 2504.16628
- **Source URL:** https://arxiv.org/abs/2504.16628
- **Reference count:** 40
- **Primary result:** Achieves 7.03% collapse rate vs 13.99-49.71% for baselines with 55.87 GPU hours runtime vs 132.47-2272.84 hours for others

## Executive Summary
This paper introduces ParetoHqD, an offline multiobjective alignment method for large language models that addresses issues of preference representation and data imbalance in existing approaches. The method represents human preferences as directions in the objective space and uses Pareto-optimal high-quality data for training. It employs a two-stage supervised fine-tuning process where each stage uses a small Pareto high-quality training set that best matches a given preference direction. Experimental results on two alignment tasks show ParetoHqD achieves superior performance with lower collapse rates, faster runtime, and better hypervolume indicator scores compared to five baselines including MORLHF, MODPO, and RiC.

## Method Summary
ParetoHqD is a two-stage supervised fine-tuning approach for multiobjective alignment of LLMs. It first identifies Pareto-optimal data points from the offline dataset by iteratively extracting non-dominated solutions. For each preference direction (represented as a ray in objective space), it selects the $k=100$ data points closest to this ray. Stage 1 performs SFT on these points. Stage 2 generates new responses using Stage 1 models, filters them for Pareto quality, and performs a second SFT with $k/2=50$ points. The method uses LoRA adapters (r=64, alpha=128, dropout=0.05) with Adam optimizer (LR=1.41e-4, batch size 8) on Llama-2 7B using HH-RLHF dataset and pre-trained reward models.

## Key Results
- Achieves 7.03% collapse rate compared to 13.99-49.71% for baselines
- Requires only 55.87 GPU hours versus 132.47-2272.84 hours for other methods
- Achieves hypervolume indicator of 0.7526 showing better convergence and diversity
- Effectively handles both convex and concave Pareto fronts while baselines fail on concave regions

## Why This Works (Mechanism)

### Mechanism 1: Geometric Preference Resolution
Linear scalarization assigns identical values to heterogeneous data points on the same contour line, causing the model to learn conflicting patterns. By defining preferences as a ray originating from the ideal reward vector toward a compromise point, the system selects training data with a consistent reward ratio, explicitly modeling the trade-off rather than just the scalar sum.

### Mechanism 2: Pareto Filtering for Noise Reduction
Large offline datasets have overrepresentation of medium-score combinations. By filtering for Pareto optimal data (non-dominated solutions), the method removes the "mediocre majority," allowing the SFT process to focus capacity on high-value trade-offs. This reduces gradient noise from low-reward samples that would otherwise dominate the loss landscape.

### Mechanism 3: Iterative Data Augmentation (Stage 2)
Stage 1 uses static offline data which may be scarce for specific preference directions. Stage 2 uses Stage 1 models to generate new responses, filters these for Pareto quality, and re-trains. This creates synthetic, high-quality curriculum tailored to the model's current state, smoothing the decision boundary and alleviating overfitting from small training sets.

## Foundational Learning

- **Concept: Pareto Optimality & Dominance**
  - **Why needed here:** The entire data selection logic relies on filtering dominated solutions. You must understand that a solution A dominates B if A is better in all objectives and strictly better in at least one.
  - **Quick check question:** Given two responses with scores (Harmless: 5, Helpful: 3) and (Harmless: 4, Helpful: 4), does either dominate the other?

- **Concept: Linear Scalarization Limits**
  - **Why needed here:** You must grasp why simple weighted sums fail to find solutions in concave regions of the Pareto front to understand the motivation for directional preference.
  - **Quick check question:** If a Pareto front is concave (shaped like a crescent moon), can a straight line (contour of a weighted sum) touch every point on the front?

- **Concept: Overfitting in SFT**
  - **Why needed here:** The paper uses a two-stage architecture specifically to combat overfitting due to small data sizes ($k=100$).
  - **Quick check question:** If validation loss rises while training loss drops during Stage 1, what is likely occurring and how does Stage 2 aim to address it?

## Architecture Onboarding

- **Component map:** Reward Models -> Pareto Filter -> Directional Selector -> Stage 1 SFT -> Stage 2 Generator -> Stage 2 Filter & SFT

- **Critical path:** The Pareto Filtering step. If the reward models are noisy or the initial dataset has a poor distribution, the "High-Quality" dataset ($D_{Pareto}$) will be empty or mislabeled, causing the rest of the pipeline to fail.

- **Design tradeoffs:**
  - Dataset Size ($N_p$) vs. Quality: Setting the threshold $N_p$ too low yields too little data for SFT; setting it too high includes "dominated" noise.
  - Simplicity vs. Complexity in Stage 2: The paper uses a heuristic selection for Stage 2 data rather than a full re-ranking, optimizing for speed over theoretical perfection.

- **Failure signatures:**
  - Language Collapse: High "Collapse Rate" indicates the model learned conflicting patterns from scalarized data or overfitted to short phrases.
  - Empty Pareto Front: If resulting LLMs cluster in one area of the objective space, the Preference Direction selection may be failing to distinguish between different user weights.

- **First 3 experiments:**
  1. Sanity Check Reward Correlation: Scatter plot the rewards of the raw dataset. Verify that a Pareto front actually exists and isn't just a blob of uncorrelated noise.
  2. Ablation on Data Size ($k$): Run Stage 1 with $k=50, 100, 200$ to find the minimum data required to prevent underfitting before enabling Stage 2.
  3. Collapse Rate Monitoring: Generate 100 samples for a neutral preference and check for repetitive loops to verify that linear scalarization baselines are indeed failing as expected.

## Open Questions the Paper Calls Out

1. How can ParetoHqD be adapted to efficiently scale to a large number of alignment objectives ($M > 3$) without incurring prohibitive computational overhead? (Section 5 mentions developing "lightweight adaptation strategies" as future work)

2. How robust is the method when the offline dataset lacks high-quality data near the true Pareto front for specific preference directions? (The paper relies on HH-RLHF dataset which has a specific distribution but does not test scenarios where available data is heavily biased away from certain trade-off regions)

3. Does the efficiency of using a small Pareto high-quality dataset ($k=100$) transfer to significantly larger LLM architectures (e.g., 70B+ parameters)? (Experiments are conducted exclusively on Llama-2 7B; larger models may require more than $k=100$ samples to effectively shift behavior)

## Limitations

- The two-stage approach relies heavily on Stage 1 models generating high-quality synthetic data, with limited analysis of quality distribution of generated responses
- The preference direction formulation assumes linear trade-offs in reward space, which may not capture complex human preference structures
- The study uses only two objectives and a single dataset, limiting generalizability to problems with more objectives or different data distributions

## Confidence

- **High Confidence:** Experimental methodology for measuring collapse rate and hypervolume is sound; geometric interpretation of preferences as directional rays is mathematically well-defined
- **Medium Confidence:** Pareto filtering approach and its effectiveness in reducing data imbalance are supported by results, but theoretical guarantees for convergence are not established
- **Low Confidence:** Claims about Stage 2 augmentation being essential for preventing overfitting are primarily based on performance differences rather than diagnostic evidence

## Next Checks

1. Run ParetoHqD without Stage 2 augmentation using the same $k=100$ points throughout and compare collapse rate and hypervolume to determine if Stage 2's contribution is additive or essential.

2. Test the method with non-uniform preference distributions (heavy clustering around certain trade-offs) and evaluate whether the Euclidean distance selection maintains diversity across the Pareto front or collapses to popular regions.

3. Apply the method to a three-objective alignment problem (adding "relevance" as a third objective) and measure whether the computational advantages scale as claimed or if geometric complexity becomes a bottleneck.