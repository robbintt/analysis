---
ver: rpa2
title: Mitigating Social Bias in English and Urdu Language Models Using PRM-Guided
  Candidate Selection and Sequential Refinement
arxiv_id: '2512.09854'
source_url: https://arxiv.org/abs/2512.09854
tags:
- urdu
- bias
- english
- language
- utility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses social bias in large language models, focusing
  on single-word generation in English and Urdu. It proposes a preference-ranking
  model (PRM)-guided inference-time mitigation framework with three methods: baseline
  generation, PRM-Select (best-of-N sampling), and PRM-Sequential (iterative refinement).'
---

# Mitigating Social Bias in English and Urdu Language Models Using PRM-Guided Candidate Selection and Sequential Refinement

## Quick Facts
- arXiv ID: 2512.09854
- Source URL: https://arxiv.org/abs/2512.09854
- Authors: Muneeb Ur Raheem Khan
- Reference count: 4
- Demonstrates PRM-guided debiasing effectively reduces bias in both English and Urdu language models, with Urdu showing higher baseline bias

## Executive Summary
This paper introduces a lightweight, inference-time framework for mitigating social bias in large language models (LLMs) during single-word generation tasks. The approach leverages a Preference Ranking Model (PRM) to guide candidate selection and sequential refinement, aiming to balance bias reduction with utility preservation across English and Urdu. The study reveals that Urdu exhibits significantly higher baseline bias than English, highlighting structural inequities in multilingual LLM training. PRM-guided methods—particularly PRM-Select (best-of-N sampling)—effectively reduce bias and nearly eliminate cross-lingual gaps, while PRM-Sequential achieves maximal fairness at a slight cost to Urdu utility due to overcorrection. The work demonstrates that lightweight, inference-time debiasing methods can substantially improve fairness, especially for underrepresented languages.

## Method Summary
The proposed framework employs a Preference Ranking Model (PRM) to guide inference-time mitigation of social bias in LLMs. Three methods are evaluated: baseline generation, PRM-Select (selecting the highest-scoring candidate from multiple samples), and PRM-Sequential (iterative refinement using PRM feedback). GPT-3.5 generates candidates, while GPT-4o-mini scores them for bias and utility. The study tests 200 English and 200 Urdu prompts, assessing both bias reduction and utility preservation. Urdu prompts are drawn from culturally relevant domains, while English prompts are adapted from prior bias benchmarks. The PRM-guided approaches dynamically adjust candidate selection and refinement to minimize bias without compromising utility, with PRM-Select showing strong performance in reducing cross-lingual bias gaps.

## Key Results
- Urdu exhibits significantly higher baseline bias than English, reflecting structural inequities in multilingual LLM training.
- PRM-Select effectively reduces bias and nearly eliminates cross-lingual bias gaps between English and Urdu.
- PRM-Sequential achieves maximal fairness but slightly reduces Urdu utility due to overcorrection.

## Why This Works (Mechanism)
The PRM-guided framework works by leveraging a learned preference model to dynamically score and select or refine LLM-generated candidates for bias and utility. By introducing a lightweight, inference-time scoring step, the system can filter out biased outputs or iteratively improve them without retraining the base model. The PRM acts as a learned bias detector and utility assessor, enabling targeted mitigation. The best-of-N sampling (PRM-Select) directly suppresses biased candidates, while sequential refinement (PRM-Sequential) progressively nudges outputs toward fairness. The approach is especially effective for underrepresented languages like Urdu, where baseline bias is high and traditional fine-tuning is resource-intensive.

## Foundational Learning
- **Preference Ranking Models (PRMs)**: Why needed: To provide a learned, adaptable scoring mechanism for bias and utility. Quick check: Verify PRM scores align with human judgments on a validation set.
- **Best-of-N Sampling**: Why needed: To reduce the likelihood of biased outputs by selecting from multiple candidates. Quick check: Compare bias metrics across different N values.
- **Sequential Refinement**: Why needed: To iteratively improve outputs by incorporating feedback, especially useful for persistent biases. Quick check: Track bias reduction per refinement step.
- **Cross-Lingual Bias Analysis**: Why needed: To understand and address disparities in bias across languages, crucial for fairness in multilingual models. Quick check: Compare baseline bias scores across languages.
- **Utility Preservation**: Why needed: To ensure bias mitigation does not degrade the linguistic quality or relevance of outputs. Quick check: Correlate bias reduction with utility score changes.
- **Inference-Time Mitigation**: Why needed: To enable bias reduction without costly retraining, making the approach scalable and adaptable. Quick check: Measure computational overhead of PRM scoring.

## Architecture Onboarding

**Component Map**
PRM -> GPT-3.5 (generator) -> Candidate Pool -> PRM-Select/Refine -> Output

**Critical Path**
1. PRM scores candidate outputs for bias and utility.
2. GPT-3.5 generates multiple candidates for each prompt.
3. PRM-Select chooses the best candidate; PRM-Sequential iteratively refines until utility threshold is met.

**Design Tradeoffs**
- PRM-Select is computationally efficient but may miss nuanced biases.
- PRM-Sequential is more thorough but risks overcorrection and higher computational cost.
- Using GPT-4o-mini as PRM introduces dependency on proprietary models; open-source alternatives may reduce accessibility.

**Failure Signatures**
- Overcorrection: Excessive bias reduction at the expense of utility (observed in Urdu with PRM-Sequential).
- Inconsistent PRM scoring: PRM may embed its own biases or misjudge nuanced cases.
- Language-specific limitations: Framework may underperform in languages with limited training data or different grammatical structures.

**3 First Experiments**
1. Test PRM-Select with varying N (e.g., 5, 10, 20) to find optimal bias-utility trade-off.
2. Apply PRM-Sequential to a subset of high-bias Urdu prompts to quantify overcorrection risk.
3. Replace GPT-4o-mini PRM with a fine-tuned open-source model to assess generalizability and computational efficiency.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's effectiveness may not generalize to languages with different grammatical structures or cultural contexts.
- Results are tightly coupled with GPT-3.5 and GPT-4o-mini; performance with other models or open-source PRMs is untested.
- The study focuses on single-word generation; scalability to longer text or complex contexts is uncertain.
- Bias scoring relies on GPT-4o-mini, which may embed its own biases; binary utility scoring oversimplifies nuanced judgments.

## Confidence
- High: PRM-Select effectively reduces bias in both English and Urdu; Urdu shows higher baseline bias than English; PRM-Sequential achieves maximal fairness with a clear utility trade-off.
- Medium: Generalizability to other languages and models; interpretation of overcorrection as a universal limitation.
- Low: Effectiveness in real-world deployment beyond controlled single-word tasks; long-term stability under diverse user interactions.

## Next Checks
1. Test the PRM-guided framework on morphologically rich languages (e.g., Finnish, Turkish) and low-resource languages to evaluate bias mitigation patterns across linguistic families.
2. Extend the study to sentence-level or document-level generation tasks to assess whether bias patterns observed in single words persist or evolve in longer contexts.
3. Replace GPT-4o-mini with smaller, task-specific PRMs (e.g., fine-tuned BERT variants) to determine if bias scoring accuracy is model-dependent and to improve computational efficiency.