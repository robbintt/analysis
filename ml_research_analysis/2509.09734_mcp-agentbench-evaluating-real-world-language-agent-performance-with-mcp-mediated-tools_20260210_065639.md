---
ver: rpa2
title: 'MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated
  Tools'
arxiv_id: '2509.09734'
source_url: https://arxiv.org/abs/2509.09734
tags:
- tool
- tools
- user
- server
- servers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the critical evaluation gap for language agents
  operating within the Model Context Protocol (MCP) ecosystem. It introduces MCP-AgentBench,
  a comprehensive benchmark with 600 queries across 6 categories of varying interaction
  complexity, built on a testbed of 33 operational MCP servers providing 188 distinct
  tools.
---

# MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools

## Quick Facts
- arXiv ID: 2509.09734
- Source URL: https://arxiv.org/abs/2509.09734
- Reference count: 40
- Primary result: Open-source models (e.g., Qwen3-235B-A22B) achieve performance on par with or exceeding proprietary systems in MCP-mediated tool use

## Executive Summary
This paper addresses the critical evaluation gap for language agents operating within the Model Context Protocol (MCP) ecosystem. It introduces MCP-AgentBench, a comprehensive benchmark with 600 queries across 6 categories of varying interaction complexity, built on a testbed of 33 operational MCP servers providing 188 distinct tools. The benchmark employs MCP-Eval, an outcome-oriented LLM-as-a-judge methodology prioritizing real-world task success over rigid execution paths. Empirical evaluation of 10 leading LLMs reveals that top open-source models, notably Qwen3-235B-A22B, achieve performance on par with or exceeding proprietary systems, challenging prevailing assumptions about model dominance. The work provides a standardized framework and empirical insights to accelerate development of capable, interoperable AI systems fully leveraging MCP's benefits.

## Method Summary
The paper introduces MCP-AgentBench, a benchmark for evaluating language agents on MCP-mediated tool interactions. The methodology involves deploying 33 MCP servers (188 tools) unified via mcprouter, generating 600 test queries across 6 complexity categories (single/multi-server Ã— single/parallel/sequential calls), and evaluating agent performance using MCP-Eval, an outcome-oriented LLM-as-a-judge approach. Agents are tested in both ReAct and Native Tool Calling frameworks with a maximum of 30 actions per query. The evaluation prioritizes task success over procedural adherence, comparing final answers against reference answers using o3-mini-high as the judge.

## Key Results
- Qwen3-235B-A22B achieved 64.7% pass rate with ReAct framework but collapsed to 40.2% with Native Tool Calling
- Top open-source models matched or exceeded proprietary systems' performance, challenging assumptions about model dominance
- MCP-Eval achieved 91.67% agreement with human majority vote (Cohen's Kappa = 0.734)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MCP standardization reduces agent-tool integration complexity by providing a universal communication layer.
- Mechanism: MCP servers expose capabilities in a standardized format with consistent feedback mechanisms. Instead of M agents requiring N bespoke integrations, agents interact with a unified protocol. The paper uses `mcprouter` to abstract server-specific operational idiosyncrasies under a unified invocation interface.
- Core assumption: Agents can reliably discover and parse MCP-exposed tool schemas without manual pre-definition.
- Evidence anchors:
  - [abstract] "MCP is rapidly emerging as a pivotal open standard, designed to enhance agent-tool integration and interoperability"
  - [section 1, page 1] "MCP standardizes the feedback mechanism from the environment, ensuring agents receive richer, more actionable, and consistent responses"
  - [corpus] MCP-RADAR, MCP-Universe, and 5+ related benchmarks similarly treat MCP as the integration standard, confirming convergent adoption but offering limited independent validation of integration-efficiency claims.
- Break condition: If tool schemas are incomplete, inconsistent across servers, or exceed model context windows (paper notes 128-tool limit), the standardization benefit degrades.

### Mechanism 2
- Claim: Outcome-oriented evaluation (MCP-Eval) better captures real-world agent capability than trajectory-based metrics.
- Mechanism: An LLM judge (o3-mini-high) compares the model's final answer against the reference answer and user query, prioritizing task success over procedural adherence. Pass rate is computed as the fraction of queries successfully resolved.
- Core assumption: Multiple valid solution paths exist; the judge can reliably distinguish tool-derived answers from parametric knowledge.
- Evidence anchors:
  - [abstract] "novel outcome-oriented evaluation methodology (MCP-Eval) prioritizes real-world task success over procedural adherence"
  - [section 2.3, page 5] "MCP-Eval deliberately prioritizes the correctness of the final outcome over the intermediate execution trajectory"
  - [section 3.3, page 7] MCP-Eval achieved 91.67% agreement with human majority vote (Cohen's Kappa = 0.734)
  - [corpus] No corpus papers validate MCP-Eval specifically; agreement metrics are self-reported.
- Break condition: If the judge cannot detect hallucination or fabrication, or if reference answers embed task-specific biases, validity degrades.

### Mechanism 3
- Claim: Interaction framework choice (ReAct vs. Native Tool Calling) significantly impacts model performance, with no universally superior method.
- Mechanism: ReAct uses explicit Thought-Action-Observation cycles; Native TC embeds tool calls directly in model output. Paper shows Qwen3-235B-A22B achieves 64.7% with ReAct but collapses to 40.2% with TC due to premature termination.
- Core assumption: Models have been trained or fine-tuned for specific interaction paradigms; mismatch causes capability collapse.
- Evidence anchors:
  - [section 3.2, page 6] "The most striking instance is Qwen3-235B-A22B, which leads the benchmark with ReAct but suffers a drastic performance collapse in TC mode"
  - [section 3.2, page 7] "Claude 4 Sonnet shows a marked improvement with TC over ReAct"
  - [corpus] Related MCP benchmarks do not systematically compare ReAct vs. TC, limiting external validation.
- Break condition: If deployment constraints force a mismatched framework (e.g., requiring TC for a model optimized for ReAct), expect significant performance degradation.

## Foundational Learning

- Concept: **Model Context Protocol (MCP)**
  - Why needed here: MCP is the core integration layer. Without understanding its server-tool schema, dynamic discovery, and stateless constraints, you cannot interpret benchmark design or failure modes.
  - Quick check question: Explain how MCP differs from traditional function calling in terms of feedback richness and tool discovery.

- Concept: **ReAct vs. Native Tool Calling**
  - Why needed here: Method choice directly determines whether a model succeeds or fails on identical tasks. Understanding the trade-offs is critical for model selection.
  - Quick check question: For a model that generates multiple concurrent tool calls per step, which framework is likely more suitable?

- Concept: **LLM-as-a-Judge Evaluation**
  - Why needed here: MCP-Eval relies on this paradigm. Understanding its limitations (judge bias, agreement thresholds) is necessary to interpret reported pass rates.
  - Quick check question: What are two failure modes where an LLM judge might incorrectly pass or fail an answer?

## Architecture Onboarding

- Component map:
  - MCP Server Testbed (33 servers, 188 tools) -> mcprouter (unified interface) -> Agent Evaluation (ReAct/TC) -> MCP-Eval (LLM judge)

- Critical path:
  1. Deploy MCP servers with text-based, stateless, executable constraints
  2. Route all servers through `mcprouter` for unified agent interface
  3. Generate queries via LLM + human verification (category-aligned, deterministic outcomes)
  4. Run agent with max 30 actions per query (ReAct or TC mode)
  5. Evaluate final answers via MCP-Eval (outcome-oriented, not trajectory-based)

- Design tradeoffs:
  - **Stateless servers only**: Ensures reproducibility but excludes real-world stateful services (e.g., databases with persistent sessions)
  - **Text-based interaction**: Simplifies evaluation but omits multimodal tool outputs
  - **Outcome over trajectory**: Allows self-correction but may reward lucky guesses
  - **128-tool limit**: Matches LLM constraints but restricts multi-server complexity

- Failure signatures:
  - **Refusal to Use Tool**: Model defaults to parametric knowledge when tool invocation is required
  - **Premature Termination**: Model generates final answer without calling required tools (observed in Qwen3 TC mode)
  - **Hallucination**: Fabricated details contradicting tool outputs
  - **Omission of Key Information**: Incomplete synthesis of multi-step results

- First 3 experiments:
  1. Replicate the Qwen3-235B-A22B ReAct vs. TC comparison on a subset of 50 queries to confirm framework sensitivity.
  2. Ablate the MCP-Eval judge by substituting o3-mini-high with a smaller model; measure agreement degradation.
  3. Extend to a stateful server (e.g., session-aware database) and document reproducibility failures to quantify the stateless constraint's scope.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can benchmarks effectively evaluate agents on stateful MCP servers?
- Basis in paper: [inferred] The methodology explicitly excluded stateful servers due to the complexity of managing initial states and resets.
- Why unresolved: The current framework relies on statelessness for reproducibility, which limits assessment to static interactions.
- What evidence would resolve it: A reliable mechanism for state management that allows reproducible scoring across persistent sessions.

### Open Question 2
- Question: What architectural features determine whether a model performs better with ReAct or Native Tool Calling?
- Basis in paper: [inferred] Results show Qwen3-235B-A22B excels with ReAct but fails drastically with Tool Calling.
- Why unresolved: The paper documents the performance gap but does not isolate the underlying cause of this framework dependency.
- What evidence would resolve it: A diagnostic study correlating specific model architectures or fine-tuning data with framework success rates.

### Open Question 3
- Question: Can the MCP-Eval methodology be extended to non-textual modalities?
- Basis in paper: [inferred] The testbed was restricted to text-based input/output due to evaluation complexities of other modalities.
- Why unresolved: Real-world tool use often involves images or audio, which current judging prompts cannot assess.
- What evidence would resolve it: A multi-modal benchmark subset where LLM-as-a-judge scores align with human evaluation of non-text outputs.

## Limitations

- The benchmark exclusively uses stateless servers, excluding real-world applications requiring persistent sessions
- Evaluation is limited to text-based interactions, missing multimodal tool outputs
- Without access to the exact 33-server testbed and 600-query dataset, reproducibility is severely constrained

## Confidence

- **High confidence**: MCP's emergence as an integration standard (supported by corpus convergence across multiple benchmarks)
- **Medium confidence**: Outcome-oriented evaluation superiority (MCP-Eval agreement metrics are self-reported; no external validation)
- **Medium confidence**: Open-source model performance parity (benchmarked only within MCP ecosystem; real-world generalization untested)
- **Low confidence**: Framework choice impact (single dramatic example; broader framework-model compatibility patterns unverified)

## Next Checks

1. Replicate the Qwen3-235B-A22B ReAct vs. TC comparison on a subset of 50 queries to confirm framework sensitivity and identify conditions triggering performance collapse.
2. Test MCP-Eval's reliability by substituting the o3-mini-high judge with a smaller model (e.g., GPT-3.5) and measuring agreement degradation across the same query set.
3. Extend the benchmark to include a stateful MCP server (e.g., session-aware database) and document reproducibility failures to quantify the impact of the stateless constraint on real-world applicability.