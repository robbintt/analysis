---
ver: rpa2
title: Enhancing compact convolutional transformers with super attention
arxiv_id: '2508.18960'
source_url: https://arxiv.org/abs/2508.18960
tags:
- attention
- accuracy
- zhang
- wang
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a compact convolutional transformer variant
  that improves upon existing architectures by adopting token mixing, sequence-pooling,
  and convolutional tokenizers. The authors introduce super attention, which reduces
  the number of parameters in the attention layer by 25% and total parameters by 40%,
  while maintaining or improving performance.
---

# Enhancing compact convolutional transformers with super attention

## Quick Facts
- arXiv ID: 2508.18960
- Source URL: https://arxiv.org/abs/2508.18960
- Reference count: 6
- Proposed compact convolutional transformer variant achieves 46.29% top-1% validation accuracy and 76.31% top-5% validation accuracy on CIFAR100

## Executive Summary
This paper proposes a compact convolutional transformer variant that improves upon existing architectures by adopting token mixing, sequence-pooling, and convolutional tokenizers. The authors introduce super attention, which reduces the number of parameters in the attention layer by 25% and total parameters by 40%, while maintaining or improving performance. Experiments on CIFAR100 show that the proposed model achieves 46.29% top-1% validation accuracy and 76.31% top-5% validation accuracy, outperforming the baseline compact convolutional transformer with standard attention (36.50% and 66.33% respectively). The model demonstrates better training stability, faster convergence, and parameter efficiency without requiring data augmentation, positional embeddings, or learning rate scheduling. The architecture is 60% smaller than scaled dot product attention transformers while being more efficient for inference when context length is less than the embedding dimension.

## Method Summary
The paper introduces a compact convolutional transformer variant with super attention that replaces standard value projections with token mixing via a square matrix W_A ∈ ℝ^(ℓ×ℓ) applied to values V. The architecture uses a 3x3 convolutional tokenizer followed by sequence pooling for the head, with orthogonal weight initialization and AdamW optimizer (lr=0.01, weight_decay=0.01). The model is trained for 75 epochs with batch size 1024 on CIFAR100 without data augmentation or positional embeddings. The super attention mechanism reduces parameters by 40% compared to baseline CCT while improving top-1 accuracy by 9.79% and top-5 accuracy by 10.98%.

## Key Results
- Achieves 46.29% top-1% validation accuracy on CIFAR100 (vs 36.50% baseline)
- Achieves 76.31% top-5% validation accuracy on CIFAR100 (vs 66.33% baseline)
- Reduces total parameters by 40% and attention parameters by 25% compared to standard attention

## Why This Works (Mechanism)
The super attention mechanism works by replacing the standard linear value projection with a token mixing matrix W_A that operates directly on the values V. This token mixing approach reduces the number of parameters in the attention layer by 25% and total parameters by 40%, while maintaining or improving performance. The architecture demonstrates better training stability, faster convergence, and parameter efficiency without requiring data augmentation, positional embeddings, or learning rate scheduling. The 3x3 convolutional tokenizer and sequence pooling head contribute to the model's efficiency and effectiveness for image classification tasks.

## Foundational Learning

**Convolutional tokenizers** - Use 3x3 convolutions to convert image patches into token sequences; needed to reduce spatial dimensions while preserving local features; quick check: verify output sequence length is reasonable (32-128 tokens)

**Token mixing** - Replace standard W_V projection with W_A ∈ ℝ^(ℓ×ℓ) matrix applied to values; needed to reduce parameters while maintaining attention functionality; quick check: confirm W_A dimensions match sequence length

**Sequence pooling** - Pool tokens into fixed-size representations for classification; needed to aggregate spatial information; quick check: verify pooling dimensions match embedding size

**Orthogonal initialization** - Initialize weights using orthogonal matrices; needed for stable training and gradient flow; quick check: verify all weight matrices have orthogonal initialization

**Constant learning rate** - Use fixed learning rate of 0.01 without warmup; needed for training stability claim; quick check: monitor first 10 iterations for loss spikes or NaN

## Architecture Onboarding

**Component map**: Input Image → 3x3 Conv Tokenizer → Super Attention Layers (6) → Sequence Pooling → Output

**Critical path**: The super attention mechanism is the critical innovation - replacing W_V with W_A token mixing matrix applied to V, which reduces parameters while maintaining performance

**Design tradeoffs**: Quadratic complexity of W_A matrix requires careful tokenizer configuration to keep sequence length small; constant learning rate eliminates warmup but may cause instability; no data augmentation simplifies training but may limit generalization

**Failure signatures**: Loss spiking or NaN values in first 10 iterations (indicates learning rate too high); OOM errors during training (indicates sequence length too large for W_A matrix); poor convergence (indicates improper W_A initialization)

**First experiments**: 1) Test tokenizer with different strides to find optimal sequence length, 2) Compare Identity vs orthogonal initialization for W_A matrix, 3) Train with reduced learning rate (1e-3) to verify constant rate stability

## Open Questions the Paper Calls Out
None

## Limitations
- Tokenizer stride and padding configuration unspecified, creating uncertainty about sequence length and memory requirements
- W_A matrix initialization method not stated, affecting reproducibility
- MLP output projection details missing, including hidden dimensions and activation functions
- Constant learning rate of 0.01 without warmup may cause training instability

## Confidence
- **High**: Architectural innovation and parameter efficiency claims are well-specified mathematically
- **Medium**: Performance improvements depend on missing implementation details that could significantly affect results
- **Low**: Training stability claim given the aggressive hyperparameters specified

## Next Checks
1. Verify tokenizer implementation produces reasonable sequence length (target: 32-128 tokens) to ensure W_A matrix fits within GPU memory constraints

2. Test training stability with reduced learning rate (1e-3) or 5-epoch warmup to confirm the 0.01 constant rate is feasible

3. Implement and test both Identity and orthogonal initialization schemes for W_A to determine which yields better performance