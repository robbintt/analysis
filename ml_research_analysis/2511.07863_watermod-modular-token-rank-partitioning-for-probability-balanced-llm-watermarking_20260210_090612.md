---
ver: rpa2
title: 'WaterMod: Modular Token-Rank Partitioning for Probability-Balanced LLM Watermarking'
arxiv_id: '2511.07863'
source_url: https://arxiv.org/abs/2511.07863
tags:
- watermarking
- watermod
- entropy
- token
- watermark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WaterMod, a modular token-rank partitioning
  method for probability-balanced LLM watermarking. The approach addresses limitations
  in existing watermarking techniques that rely on random or semantically clustered
  token splits, which can degrade fluency by excluding high-probability tokens.
---

# WaterMod: Modular Token-Rank Partitioning for Probability-Balanced LLM Watermarking

## Quick Facts
- arXiv ID: 2511.07863
- Source URL: https://arxiv.org/abs/2511.07863
- Authors: Shinwoo Park; Hyejin Park; Hyeseon Ahn; Yo-Sub Han
- Reference count: 16
- Primary result: WaterMod achieves up to 100% AUROC watermark detection while maintaining low perplexity (12.58) and accuracy across NLP, reasoning, and code tasks

## Executive Summary
This paper introduces WaterMod, a modular token-rank partitioning method for probability-balanced LLM watermarking that addresses limitations in existing techniques. Traditional watermarking methods that split vocabulary randomly or by semantic clusters can degrade fluency by excluding high-probability tokens. WaterMod solves this by sorting tokens by model probability and partitioning via modular arithmetic (rank mod k), ensuring high-probability tokens are distributed across classes. This preserves generation quality while enabling both zero-bit attribution and multi-bit payload embedding through a unified framework.

## Method Summary
WaterMod operates by sorting the vocabulary by model probability at each decoding step, then partitioning token ranks using modular arithmetic (rank mod k). For zero-bit watermarking (k=2), an entropy-adaptive gate selects which parity class receives the bias based on Shannon entropy. For multi-bit watermarking (k>2), the payload digit selects the target class. The method adds a bias δ to logits of tokens in the selected class before sampling. Detection involves recomputing logits, recomputing the green list/class, and computing a z-score from green-token counts against a binomial null hypothesis.

## Key Results
- Achieves 100% AUROC watermark detection on GSM8K reasoning tasks
- Maintains low perplexity of 12.58 on MBPP+ code generation tasks
- Outperforms state-of-the-art baselines in both zero-bit and multi-bit settings across natural language, mathematical reasoning, and code generation

## Why This Works (Mechanism)

### Mechanism 1: Probability-Ranked Partitioning Preserves High-Quality Tokens
Sorting vocabulary by model probability and partitioning via `rank mod k` distributes semantically similar tokens across different classes, ensuring at least one high-probability candidate remains in each class. Adjacent probability ranks represent near-synonyms from the model's perspective, and modular arithmetic guarantees consecutive ranks fall into different classes.

### Mechanism 2: Entropy-Adaptive Green List Selection Balances Signal Strength Against Fluency
The Shannon entropy gate dynamically selects which class receives the bias, allowing stronger watermark insertion in high-entropy contexts while protecting fluency in deterministic (low-entropy) generation. Low-entropy contexts (math/code) get the top-1 token protected, while high-entropy contexts (open-ended text) can use the signal-carrying class.

### Mechanism 3: Unified Modular Framework Enables Zero-Bit and Multi-Bit via Single Parameter
The same `rank mod k` operation supports both binary attribution (k=2) and payload embedding (k>2) without architectural changes. For k=2, there are 2 residue classes for binary detection; for k>2, k classes enable payload embedding with `log_2(k)` bits per token.

## Foundational Learning

- **Logit-based watermarking fundamentals**: WaterMod modifies logits (pre-softmax scores) before sampling; understanding how bias δ shifts sampling probabilities is essential.
  - Quick check: If token A has logit 5.0 and token B has logit 4.0, and we add bias δ=1.0 to token B, what are the new relative probabilities after softmax?

- **Shannon entropy vs. spike entropy in LLM outputs**: The entropy gate controls watermark strength adaptively; the paper compares Shannon entropy (default) against spike entropy in Appendix D.
  - Quick check: Why would spike entropy yield better detection while Shannon entropy yields better task performance?

- **Modular arithmetic residue classes**: The core partitioning rule `r mod k = d` is pure modular arithmetic; understanding that consecutive integers have different residues is key.
  - Quick check: For k=4, what are the residue classes for ranks 0, 1, 2, 3, 4, 5? Which ranks share the same class?

- **Binomial hypothesis testing for detection**: Detection uses z-scores computed from green-token counts under a binomial null hypothesis; understanding false-positive rate control is necessary.
  - Quick check: Under the null hypothesis with k=2, if you observe 60 green tokens in 100 positions, is this statistically significant at z>1.96 (p<0.025 one-sided)?

## Architecture Onboarding

- **Component map**: Logits → Probability sort → Modular partition (rank mod k) → Class selection (entropy gate or payload digit) → Logit bias δ → Sample

- **Critical path**: 1) Probability sorting at each decoding step (O(V log V) bottleneck), 2) PRF-seeded pseudorandomness from previous token and secret key, 3) Detection requires model access to recompute logits

- **Design tradeoffs**: k=2 vs k>2 (signal strength vs payload capacity), δ magnitude (detection vs perplexity), H_scale (entropy gate aggressiveness)

- **Failure signatures**: High perplexity despite low δ (misconfigured entropy gate), low AUROC in multi-bit (insufficient bias δ), detection fails after paraphrasing (expected degradation), payload recovery errors (insufficient sequence length)

- **First 3 experiments**:
  1. Reproduce zero-bit AUROC on GSM8K with k=2, δ=1.0, H_scale=1.2; target AUROC should approach 100%
  2. Ablate entropy gate by comparing WaterMod with fixed parity vs entropy-adaptive selection on C4; quantify perplexity increase and AUROC change
  3. Stress-test multi-bit on code (MBPP+) with k=4, δ=2.5, 16-bit payload; if pass@1 drops below 25% or AUROC below 95%, iterate on δ and k

## Open Questions the Paper Calls Out
None

## Limitations
- Model dependence: Effectiveness tied to underlying model's probability distributions and may not generalize uniformly across architectures
- Detection overhead: Requires full model access during both generation and detection, creating significant computational overhead
- Zero-bit reliability: Detection reliability for very short sequences remains untested despite strong results on long sequences

## Confidence
- **Probability-Ranked Partitioning Preserves Fluency**: High confidence (mathematically sound, directly testable)
- **Entropy-Adaptive Gate Optimizes Tradeoff**: Medium confidence (mechanism clear but entropy assumption needs broader validation)
- **Unified Framework Enables Both Settings**: High confidence (internally consistent, experimental results demonstrate success)

## Next Checks
1. **Short-Sequence Detection Power Analysis**: Evaluate detection reliability on progressively shorter text sequences (10 tokens to full outputs) across all domains; measure minimum sequence length for AUROC > 0.95

2. **Cross-Model Compatibility Testing**: Implement WaterMod across multiple LLM architectures and evaluate whether probability-ranked partitioning consistently preserves fluency compared to model-specific baselines

3. **Real-Time Performance Benchmarking**: Measure end-to-end latency overhead of WaterMod's probability sorting and entropy computation during generation; profile computational cost at different vocabulary sizes for practical deployment scenarios