---
ver: rpa2
title: 'Mark My Words: A Robust Multilingual Model for Punctuation in Text and Speech
  Transcripts'
arxiv_id: '2506.03793'
source_url: https://arxiv.org/abs/2506.03793
tags:
- punctuation
- language
- languages
- text
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cadence, a robust multilingual punctuation
  restoration model for 22 Indian languages and English. The model is built by adapting
  Gemma 3-1B to a bidirectional transformer and training it using a Masked Next Token
  Prediction objective on a large, diverse multilingual corpus.
---

# Mark My Words: A Robust Multilingual Model for Punctuation in Text and Speech Transcripts
## Quick Facts
- **arXiv ID**: 2506.03793
- **Source URL**: https://arxiv.org/abs/2506.03793
- **Reference count**: 30
- **Primary result**: Cadence achieves state-of-the-art F1 of 0.76 on IndicCorp-v2 and 0.63 on BPCC, supporting 30 punctuation classes across 22 Indian languages and English.

## Executive Summary
This paper introduces Cadence, a robust multilingual punctuation restoration model for 22 Indian languages and English. The model is built by adapting Gemma 3-1B to a bidirectional transformer and training it using a Masked Next Token Prediction objective on a large, diverse multilingual corpus. Cadence achieves state-of-the-art performance on formal and spontaneous speech transcripts, with an overall F1 score of 0.76 on IndicCorp-v2 and 0.63 on the BPCC dataset for English. It supports 30 punctuation classes, including Indic-specific marks, and shows strong generalization to low-resource and unseen languages like Bhojpuri. The model is released to support downstream NLP tasks and empower low-resource language pipelines.

## Method Summary
Cadence is a robust multilingual punctuation restoration model developed by adapting Gemma 3-1B to a bidirectional transformer architecture. The model is trained using a Masked Next Token Prediction objective on a large, diverse multilingual corpus covering 22 Indian languages and English. The bidirectional modification allows the model to better capture context from both directions, which is critical for accurate punctuation prediction in speech transcripts. The training process leverages a wide range of punctuation classes, including those specific to Indian languages, and is evaluated on both formal and spontaneous speech datasets. The model demonstrates strong generalization to low-resource and unseen languages, with performance validated on Bhojpuri as an unseen language example.

## Key Results
- Achieves state-of-the-art F1 score of 0.76 on IndicCorp-v2 and 0.63 on BPCC dataset for English.
- Supports 30 punctuation classes, including Indic-specific marks, across 22 Indian languages and English.
- Demonstrates strong generalization to low-resource and unseen languages, validated on Bhojpuri.

## Why This Works (Mechanism)
The bidirectional transformer architecture allows Cadence to capture richer contextual information from both left and right contexts, which is essential for accurate punctuation prediction in natural language. The Masked Next Token Prediction objective enables the model to learn robust representations for punctuation restoration by focusing on predicting the next token in a masked context. The large, diverse multilingual corpus ensures that the model is exposed to a wide variety of linguistic patterns and punctuation usage across languages, enhancing its generalization capabilities. The inclusion of Indic-specific punctuation marks ensures that the model is tailored to the needs of Indian language users, while the adaptation of Gemma 3-1B provides a strong foundation for multilingual understanding.

## Foundational Learning
- **Bidirectional Transformers**: Allow the model to process context from both directions, improving punctuation prediction accuracy. *Why needed*: Punctuation often depends on surrounding context. *Quick check*: Compare performance with unidirectional models.
- **Masked Next Token Prediction**: Trains the model to predict the next token in a masked context, enhancing robustness. *Why needed*: Encourages the model to learn meaningful representations for punctuation. *Quick check*: Evaluate performance on masked vs. non-masked objectives.
- **Multilingual Corpus**: Provides diverse linguistic input across 22 languages, improving generalization. *Why needed*: Ensures the model can handle varied punctuation patterns. *Quick check*: Test on low-resource and unseen languages.
- **Indic-Specific Punctuation**: Includes marks unique to Indian languages, ensuring cultural and linguistic relevance. *Why needed*: Standard punctuation models may miss language-specific nuances. *Quick check*: Validate performance on Indic-specific tasks.
- **Adaptation of Gemma 3-1B**: Leverages a strong multilingual foundation for fine-tuning. *Why needed*: Provides a robust starting point for multilingual tasks. *Quick check*: Compare with training from scratch.
- **Large-Scale Training**: Enables the model to learn complex patterns across diverse data. *Why needed*: Essential for capturing subtle punctuation cues. *Quick check*: Analyze training data diversity.

## Architecture Onboarding
- **Component Map**: Input Text -> Bidirectional Transformer (Cadence) -> Punctuation Prediction -> Output Text
- **Critical Path**: The bidirectional transformer is the core component, enabling context-aware punctuation restoration.
- **Design Tradeoffs**: Bidirectional vs. unidirectional architectures balance context capture and computational efficiency.
- **Failure Signatures**: Poor performance on unseen languages or noisy transcripts may indicate overfitting to training data.
- **First Experiments**:
  1. Evaluate Cadence on spontaneous, out-of-domain speech transcripts for robustness.
  2. Conduct cross-lingual transfer experiments for multiple unseen languages.
  3. Perform architectural ablations comparing bidirectional and unidirectional models.

## Open Questions the Paper Calls Out
None

## Limitations
- True robustness across all claimed low-resource languages is uncertain due to limited validation examples.
- The necessity and optimality of the bidirectional architecture are not fully established.
- Training corpus composition is not detailed, raising potential concerns about domain or topical biases.

## Confidence
- **High**: Core performance claims for high-resource languages and datasets (IndicCorp-v2, BPCC).
- **Medium**: Low-resource and unseen language performance, due to limited validation examples.
- **Low**: Necessity and optimality of the bidirectional architecture, given lack of ablations.

## Next Checks
1. Systematically evaluate Cadence on spontaneous, out-of-domain speech transcripts (e.g., interviews, meetings) for both high-resource and low-resource languages to measure true robustness.
2. Conduct cross-lingual transfer experiments for multiple unseen languages (beyond Bhojpuri) to quantify generalization and identify potential failure modes.
3. Perform architectural ablations comparing the bidirectional transformer to standard causal decoders with alternative training objectives (e.g., causal LM, seq2seq) to isolate the contribution of the bidirectional modification.