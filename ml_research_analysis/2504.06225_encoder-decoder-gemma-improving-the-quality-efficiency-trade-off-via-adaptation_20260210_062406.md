---
ver: rpa2
title: 'Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation'
arxiv_id: '2504.06225'
source_url: https://arxiv.org/abs/2504.06225
tags:
- encoder-decoder
- gemma
- arxiv
- llms
- decoder-only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to adapt pretrained decoder-only
  language models (LLMs) into encoder-decoder models. The authors argue that encoder-decoder
  architectures offer a better quality-efficiency trade-off due to their separate
  encoder and decoder modules, which allow for richer contextual representations and
  more flexible model sizing.
---

# Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation

## Quick Facts
- arXiv ID: 2504.06225
- Source URL: https://arxiv.org/abs/2504.06225
- Authors: Biao Zhang, Fedor Moiseev, Joshua Ainslie, Paul Suganthan, Min Ma, Surya Bhupatiriraju, Fede Lebron, Orhan Firat, Armand Joulin, Zhe Dong
- Reference count: 22
- Primary result: Encoder-decoder adaptation of Gemma models achieves 7% improvement over decoder-only models in instruction tuning while enabling better quality-efficiency trade-offs

## Executive Summary
This paper introduces a method to adapt pretrained decoder-only language models into encoder-decoder architectures, arguing that this provides a superior quality-efficiency trade-off. The approach reuses parameters from decoder-only models by initializing the encoder from the decoder and cross-attention from self-attention weights (for balanced models) or random initialization with warmup (for unbalanced models). Two pretraining objectives are compared: prefix language modeling with knowledge distillation and UL2. The adapted models are evaluated on multiple benchmarks including pretraining tasks, instruction tuning, and SuperGLUE, showing that encoder-decoder models achieve comparable or slightly better pretraining performance but significantly better instruction-tuning performance than their decoder-only counterparts. For example, the 2B-2B model outperforms Gemma 2 2B by about 7% after instruction tuning. The adaptation method also allows for pairing large encoders with small decoders (e.g., 9B-2B), providing better quality with similar inference latency.

## Method Summary
The authors adapt pretrained decoder-only language models (specifically Gemma 2 and smaller models) into encoder-decoder architectures by reusing parameters from the original models. The encoder is initialized from the decoder weights, while cross-attention is initialized from self-attention weights for balanced models or randomly with a warmup phase for unbalanced models. Two pretraining objectives are employed: prefix language modeling with knowledge distillation and UL2. The adapted models are then evaluated on multiple benchmarks including pretraining tasks, instruction tuning, and SuperGLUE. The approach allows for flexible model sizing, enabling large encoders to be paired with small decoders for better quality with similar inference latency.

## Key Results
- Encoder-decoder models achieve comparable or slightly better pretraining performance than decoder-only counterparts
- Instruction tuning performance improves by approximately 7% for 2B-2B model versus Gemma 2 2B
- Encoder-decoder models consistently outperform decoder-only models on SuperGLUE, indicating higher-quality contextual representations
- The adaptation approach enables pairing large encoders (9B) with small decoders (2B) for improved quality with similar inference latency

## Why This Works (Mechanism)
The encoder-decoder architecture offers a better quality-efficiency trade-off by separating encoding and decoding functions. The encoder provides richer contextual representations that can be reused across multiple decoding steps, while the decoder focuses solely on generating output sequences. This separation allows for more flexible model sizing - large encoders can be paired with small decoders to maintain inference efficiency while improving quality. The adaptation method leverages pretrained decoder-only weights, avoiding the need to train from scratch and thus reducing computational costs. Knowledge distillation during pretraining helps transfer the capabilities of larger models to smaller ones, while the UL2 objective provides a more comprehensive training signal by combining multiple objectives.

## Foundational Learning

**Encoder-Decoder Architecture**: A neural network structure with separate encoding and decoding components, where the encoder processes input into contextual representations that the decoder uses to generate outputs. *Why needed*: Enables more efficient computation and better contextual understanding compared to decoder-only models. *Quick check*: Verify that encoder produces meaningful contextual representations by examining attention patterns and intermediate activations.

**Knowledge Distillation**: A training technique where a smaller "student" model learns from a larger "teacher" model by mimicking its outputs. *Why needed*: Allows smaller models to achieve performance closer to larger models without requiring the full computational cost. *Quick check*: Compare student and teacher outputs on validation set to ensure alignment.

**Prefix Language Modeling**: A pretraining objective where the model predicts tokens within a context window, similar to standard language modeling but with a defined prefix. *Why needed*: Provides a natural pretraining objective for encoder-decoder models that leverages both encoding and decoding capabilities. *Quick check*: Monitor perplexity on held-out prefix prediction tasks during training.

## Architecture Onboarding

**Component Map**: Input -> Encoder -> Cross-Attention -> Decoder -> Output
The encoder processes the input sequence, cross-attention layers allow the decoder to attend to encoder representations, and the decoder generates the output sequence token by token.

**Critical Path**: Input sequence → Encoder processing → Cross-attention computation → Decoder generation → Output prediction
The bottleneck typically occurs in cross-attention computation, which scales quadratically with input length.

**Design Tradeoffs**: 
- Balanced vs unbalanced encoder-decoder sizing: Balanced models (equal encoder/decoder sizes) provide more uniform performance, while unbalanced models (large encoder, small decoder) optimize for efficiency
- Pretraining objective selection: Prefix language modeling with distillation transfers knowledge effectively but requires teacher models, while UL2 provides comprehensive training without additional models
- Parameter reuse strategy: Initializing encoder from decoder weights leverages pretrained capabilities but may limit architectural innovation

**Failure Signatures**: 
- Degraded performance on long sequences may indicate insufficient cross-attention capacity
- Training instability often results from improper initialization of cross-attention layers in unbalanced models
- Loss of generation diversity suggests over-reliance on encoder representations without sufficient decoder autonomy

**First Experiments**:
1. Compare pretraining convergence rates between prefix language modeling with distillation versus UL2 objective
2. Evaluate instruction-following performance across different encoder-decoder size ratios (1:1, 2:1, 4:1)
3. Measure inference latency and memory usage for various encoder-decoder configurations under realistic deployment scenarios

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation is limited to English-only tasks, with no assessment of multilingual capabilities despite adapting a multilingual model family
- Training procedures rely on synthetic data for instruction tuning, raising questions about generalization to real-world prompts
- The adaptation methodology appears computationally intensive, requiring substantial compute for both pretraining objectives and knowledge distillation

## Confidence

**High confidence**: Quality-efficiency trade-off improvements for instruction tuning (7% gain for 2B-2B vs Gemma 2 2B) are well-supported by direct comparisons on standardized benchmarks. The assertion that encoder-decoder architectures enable better parameter efficiency through flexible sizing (e.g., 9B-2B pairing) is clearly demonstrated.

**Medium confidence**: Claims about pretraining performance improvements are less definitive, as results show "comparable or slightly better" outcomes rather than consistent superiority. The assertion that encoder-decoder models produce "higher-quality contextual representations" based on SuperGLUE performance requires additional validation.

**Low confidence**: The claim that adaptation is "more compute-efficient than training from scratch" lacks direct comparison data showing training costs for encoder-decoder models built from the ground up.

## Next Checks

1. Evaluate multilingual performance on benchmarks like XTREME or mMLU to verify that adaptation benefits extend beyond English-language tasks, particularly given Gemma's multilingual pretraining.

2. Conduct ablation studies isolating the impact of each adaptation component (encoder initialization method, cross-attention initialization, pretraining objective) to identify which elements drive the observed improvements.

3. Measure actual inference latency and memory usage for various encoder-decoder size combinations under realistic deployment scenarios to validate the claimed efficiency benefits across different hardware configurations.