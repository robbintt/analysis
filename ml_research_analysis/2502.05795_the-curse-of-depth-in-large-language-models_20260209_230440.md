---
ver: rpa2
title: The Curse of Depth in Large Language Models
arxiv_id: '2502.05795'
source_url: https://arxiv.org/abs/2502.05795
tags:
- layer
- layers
- scaling
- pre-ln
- variance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and addresses the Curse of Depth, a phenomenon
  where nearly half of the deep layers in modern LLMs contribute significantly less
  to learning compared to earlier layers. The authors theoretically and empirically
  trace this to Pre-Layer Normalization (Pre-LN), which causes exponential growth
  in output variance with depth, making deep layers behave nearly as identity mappings.
---

# The Curse of Depth in Large Language Models

## Quick Facts
- arXiv ID: 2502.05795
- Source URL: https://arxiv.org/abs/2502.05795
- Reference count: 40
- Primary result: LayerNorm Scaling (LNS) mitigates the Curse of Depth by enabling deeper layers in LLMs to contribute more effectively to learning

## Executive Summary
This paper identifies the "Curse of Depth" phenomenon in large language models, where nearly half of deep layers contribute significantly less to learning compared to earlier layers. The authors trace this issue to Pre-Layer Normalization (Pre-LN), which causes exponential growth in output variance with depth, making deep layers behave nearly as identity mappings. They propose LayerNorm Scaling (LNS), a simple modification that scales layer normalization outputs inversely by the square root of depth. Across model sizes from 130M to 7B parameters, LNS consistently outperforms previous normalization techniques in both pre-training and fine-tuning tasks.

## Method Summary
The authors propose LayerNorm Scaling (LNS) as a solution to the Curse of Depth problem in LLMs. LNS modifies the layer normalization operation by scaling its outputs inversely by the square root of the current layer depth. This simple adjustment counteracts the exponential variance growth that occurs in Pre-LN architectures, allowing deeper layers to participate more effectively in the learning process. The method is hyperparameter-free, requires no additional parameters, and can be easily implemented. The authors validate LNS across a wide range of model scales (130M to 7B parameters) and demonstrate its effectiveness in both autoregressive language modeling and vision transformer applications.

## Key Results
- Nearly half of deep layers in modern LLMs contribute significantly less to learning than earlier layers
- LNS enables deeper layers to contribute more effectively to model performance
- LNS outperforms previous normalization techniques across all tested model scales (130M to 7B parameters)

## Why This Works (Mechanism)
The Curse of Depth occurs because Pre-LN causes exponential variance growth with depth, making deep layers behave as identity mappings. The layer normalization output variance grows as O(2^L) where L is the layer depth. LNS scales the layer normalization outputs by 1/sqrt(L), which stabilizes the variance growth and allows gradients to flow more effectively through deep layers. This simple scaling modification ensures that deeper layers can learn meaningful representations rather than becoming redundant identity mappings.

## Foundational Learning
- **Layer Normalization**: Normalizes across features within a layer; needed to understand how variance scales with depth
- **Pre-Layer Normalization**: Applies normalization before the sub-layer; quick check: verify it's the dominant architecture in modern LLMs
- **Residual Connections**: Skip connections that add the input to the output; why needed to understand how gradients flow through deep networks
- **Variance Propagation**: How signal variance changes through network layers; quick check: trace variance growth through Pre-LN architecture
- **Attention Mechanisms**: Self-attention blocks in transformers; why needed to understand their interaction with normalization
- **MLP Blocks**: Multi-layer perceptron layers; quick check: identify where they appear in transformer architecture

## Architecture Onboarding
**Component Map**: Input -> Embedding -> Attention Block -> LN -> MLP Block -> Residual Add -> LN -> ... -> Output
**Critical Path**: Input flows through stacked transformer blocks, each containing attention, LN, MLP, and residual connections
**Design Tradeoffs**: Pre-LN vs Post-LN normalization; depth vs width scaling; complexity vs performance
**Failure Signatures**: Deep layers becoming identity mappings; vanishing gradients; training instability in later stages
**First Experiments**: 1) Measure variance growth across layers in Pre-LN models; 2) Apply LNS and measure variance stabilization; 3) Compare training curves with and without LNS across different depths

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes idealized conditions that may not capture complex real-world interactions
- Long-term stability during extended training or production use remains unverified
- Study focuses primarily on autoregressive language modeling, with limited exploration of other modalities

## Confidence
**High confidence**: Identification of depth-dependent variance growth problem and mathematical derivation of scaling solution
**Medium confidence**: Empirical performance improvements across model scales, though relative gains may vary
**Medium confidence**: Claim that LNS is hyperparameter-free and universally beneficial, as edge cases are not explored

## Next Checks
1. Conduct long-term stability analysis by training models with LNS for extended periods (2-3x standard duration) to verify persistent benefits
2. Test LNS in diverse model architectures beyond standard transformers, including encoder-only, decoder-only, and hybrid architectures
3. Perform ablation studies to isolate LNS contribution by comparing against alternative normalization strategies and analyzing interactions with other components