---
ver: rpa2
title: 'Towards Smarter Hiring: Are Zero-Shot and Few-Shot Pre-trained LLMs Ready
  for HR Spoken Interview Transcript Analysis?'
arxiv_id: '2504.05683'
source_url: https://arxiv.org/abs/2504.05683
tags:
- response
- turbo
- human
- suggestions
- candidate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study evaluates the performance of prominent pre-trained LLMs
  in HR interview transcript analysis compared to human evaluators. Using the HURIT
  dataset of 3,890 transcripts, the research assessed GPT-4 Turbo, GPT-3.5 Turbo,
  and other models on six communication criteria: fluency, coherence, tone/politeness,
  relevance, conciseness, and grammaticality.'
---

# Towards Smarter Hiring: Are Zero-Shot and Few-Shot Pre-trained LLMs Ready for HR Spoken Interview Transcript Analysis?

## Quick Facts
- arXiv ID: 2504.05683
- Source URL: https://arxiv.org/abs/2504.05683
- Reference count: 40
- Primary result: GPT-4 Turbo and GPT-3.5 Turbo produce interview scores comparable to human experts for certain criteria (Kendall's τ up to 0.44), but struggle with error identification and specific feedback.

## Executive Summary
This study evaluates whether pre-trained LLMs can effectively analyze HR spoken interview transcripts without task-specific fine-tuning. Using the HURIT dataset of 3,890 transcripts from L2 English speakers in Asia, the research tests prominent models (GPT-4 Turbo, GPT-3.5 Turbo, and others) on six communication criteria: fluency, coherence, tone/politeness, relevance, conciseness, and grammaticality. Results show that while LLMs can produce scores aligned with human expert judgments for certain criteria, they consistently underperform humans in identifying specific errors and providing actionable feedback. The study concludes that LLMs show promise for scoring automation but require human oversight for comprehensive interview assessment.

## Method Summary
The study uses the HURIT dataset containing 3,890 spoken English interview transcripts from non-native speakers in Asia. Each transcript was evaluated by three certified HR managers independently across six criteria using a 0-5 scale. The same transcripts were then processed through pre-trained LLMs (GPT-4 Turbo, GPT-3.5 Turbo, and other models) using zero-shot and few-shot prompting approaches. The LLM prompts instructed models to adopt an evaluator persona and score responses across the six criteria. Kendall's correlation coefficients were computed to measure alignment between LLM and human scores, while qualitative analysis compared error identification and feedback quality.

## Key Results
- GPT-4 Turbo and GPT-3.5 Turbo achieved Kendall's correlation coefficients up to 0.44 for tone/politeness, comparable to human evaluators
- Smaller models (text-ada-001, text-babbage-001) consistently scored candidates 1-2 points higher than human evaluators across all criteria
- LLMs struggled significantly with error identification and providing specific, actionable feedback compared to human evaluators
- Few-shot prompting (2-shot, 4-shot, 8-shot) provided no substantial improvements across evaluated criteria

## Why This Works (Mechanism)

### Mechanism 1: Zero-Shot Scoring Alignment via Pre-trained Language Understanding
Pre-trained LLMs leverage their training on diverse text corpora to recognize patterns of effective communication and social appropriateness. The zero-shot prompt instructs models to adopt an evaluator persona and apply a 0-5 rating scale. However, correlation strength varies significantly across criteria—GPT-4 Turbo achieves τ=0.44 for tone/politeness but only τ=0.07 for coherence, indicating that some communication aspects are better encoded in pre-training than others.

### Mechanism 2: Error Identification via Surface Pattern Recognition vs. Deep Comprehension
Models identify errors through surface-level linguistic patterns (e.g., grammatical mistakes, obvious repetitions) but fail to capture nuanced communication failures that require contextual understanding. They miss subtler problems like abrupt topic transitions or missing logical connections because these require understanding speaker intent and interview context beyond the transcript text.

### Mechanism 3: Feedback Generation via Template-Based Response Patterns
When prompted for feedback, LLMs produce generic improvement suggestions by following learned patterns for "constructive criticism" rather than deeply analyzing specific transcript content. They generate phrases like "practice speaking more slowly" without referencing exact transcript locations or providing concrete alternatives, unlike human evaluators who connect suggestions to specific moments in responses.

## Foundational Learning

- **Zero-Shot vs. Few-Shot Prompting**: The study relies on zero-shot prompting to test whether pre-trained models can perform HR assessment without task-specific training. Understanding why few-shot approaches failed is critical for system design. *Quick check: Why might adding few-shot examples not improve performance on a subjective evaluation task like interview scoring?*

- **Kendall's Correlation Coefficient (τ)**: The primary metric for comparing LLM to human evaluations. Understanding the range (0.07 = very weak, 0.44 = strong) determines which criteria are suitable for automation vs. require human oversight. *Quick check: If GPT-4 Turbo achieves τ=0.44 for tone/politeness but τ=0.07 for coherence, what does this imply about where human-in-the-loop oversight should be prioritized?*

- **Inter-Annotator Agreement (Fleiss's Kappa)**: Even human experts show only "moderate" agreement (κ=0.41-0.60 across criteria). This establishes the upper bound for what LLMs can reasonably achieve and calibrates expectations for scoring systems. *Quick check: If human evaluators disagree 30-40% of the time, should we expect LLMs to achieve higher agreement with any single human evaluator?*

## Architecture Onboarding

- **Component map**: Audio Transcription Layer (Whisper) → LLM Evaluation Layer (GPT-4 Turbo/GPT-3.5 Turbo) → Human Evaluation Layer (HR managers) → Aggregation Layer (Kendall's τ computation) → Feedback Synthesis Layer (Human-in-the-loop review)

- **Critical path**: Transcription quality → Prompt design → Model selection → Score generation → Human review → Feedback refinement. Transcription errors propagate through the pipeline, affecting both LLM and human evaluation.

- **Design tradeoffs**: 
  1. Separate vs. unified prompts (unified approach selected despite no clear winner)
  2. Model size vs. cost (GPT-4 Turbo outperforms GPT-3.5 Turbo but at higher API cost)
  3. Automation vs. human oversight (full automation viable for scoring only; error identification and feedback require human augmentation)

- **Failure signatures**: 
  1. Over-generous scoring by smaller models (1-2 points higher than humans)
  2. Generic feedback using phrases like "practice more" without citing specific transcript locations
  3. Missed pragmatic errors (inappropriate formality, missing transitions, cultural inappropriateness)

- **First 3 experiments**: 
  1. Baseline correlation test: Run GPT-4 Turbo on 50 transcripts; compute Kendall's τ against human scores to establish minimum correlation thresholds
  2. Error detection audit: Compare LLM vs. human error identification on 20 transcripts; categorize misses by error type (syntactic vs. pragmatic vs. structural)
  3. Feedback specificity scoring: Rate LLM vs. human feedback on specificity, actionability, and correctness to quantify human-in-the-loop workload

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the observed alignment between LLM and human evaluator scores be generalized to a broader range of HR interview question types, such as behavioral or technical queries?
- **Basis in paper**: [explicit] The "Limitations" section states that the study explores only four types of questions and that "future research should consider including a more diverse set of question types to enhance the comprehensiveness of the analysis."
- **Why unresolved**: The current HURIT dataset and experiments are restricted to four specific question types ("Tell me about yourself," "Strengths/Weaknesses," "Hobbies," and "CEO discussion"), limiting the external validity of the findings across the full spectrum of interview scenarios.
- **What evidence would resolve it**: Evaluation results (e.g., Kendall's correlation coefficients) from LLMs tested on a new dataset containing diverse behavioral (e.g., STAR method) and technical interview transcripts compared against human expert baselines.

### Open Question 2
- **Question**: Do pre-trained LLMs maintain consistent evaluation performance when applied to L2 English speakers from diverse cultural and regional backgrounds outside of Asia?
- **Basis in paper**: [explicit] The "Limitations" section notes that "HR spoken transcripts used in this study are exclusively from the Asian region" and explicitly calls for "future research [to] aim to collect data from a more geographically diverse sample of candidates to improve the external validity of the study."
- **Why unresolved**: The homogeneity of the dataset (Asian region only) means the models' ability to handle varied accents, cultural communication norms, and specific L2 error patterns found in other regions remains untested.
- **What evidence would resolve it**: A replication of the study using transcripts sourced from non-Asian L2 English speakers (e.g., European or South American speakers), demonstrating similar correlation scores and error detection capabilities.

### Open Question 3
- **Question**: Can fine-tuning or specialized prompting strategies improve the ability of LLMs to identify complex errors (such as disjointed transitions) and provide specific, actionable feedback?
- **Basis in paper**: [inferred] The abstract and conclusion state that models "frequently fail to identify errors and offer specific actionable advice" and that "LLMs have not yet been able to reach the level of human performance to find flaws." The study relied solely on zero-shot and few-shot pre-trained models.
- **Why unresolved**: The paper identifies a specific performance gap in Task B (error identification) and Task C (actionable feedback) compared to human evaluators, but does not test whether training interventions could close this gap.
- **What evidence would resolve it**: A comparative study showing that fine-tuning LLMs on an expert-annotated corpus of HR feedback results in a statistically significant increase in the specificity and accuracy of error detection compared to the zero-shot baseline.

## Limitations
- Dataset contains exclusively L2 English speakers from Asian regions, limiting generalizability to other linguistic and cultural contexts
- Evaluation criteria represent a specific subset of communication skills, potentially missing other critical dimensions like problem-solving or emotional intelligence
- Only tested zero-shot and few-shot prompting approaches, leaving open the possibility that fine-tuning or retrieval-augmented methods might yield better performance

## Confidence
- **High Confidence**: The finding that GPT-4 Turbo and GPT-3.5 Turbo produce scores comparable to human experts for certain criteria (τ=0.44 for tone/politeness) is well-supported by the data and methodology.
- **Medium Confidence**: The claim that models struggle with error identification and actionable feedback is supported, but the specific mechanisms (surface vs. deep comprehension) remain somewhat speculative without direct probing of model behavior.
- **Low Confidence**: The assertion that few-shot prompting yields no substantial improvements (Appendix A.2) is based on limited testing (2-shot, 4-shot, 8-shot) without exploring alternative few-shot strategies or prompt engineering techniques.

## Next Checks
1. **Cross-Cultural Validation**: Test the same LLM evaluation pipeline on interview transcripts from native English speakers and different cultural regions to assess performance degradation and identify culturally-specific scoring biases.
2. **Error Type Decomposition**: Conduct a fine-grained analysis categorizing LLM error identification failures into syntactic, pragmatic, and structural types, then test whether targeted prompt engineering or multi-turn dialogue can address specific blind spots.
3. **Human-in-the-Loop Efficiency Study**: Measure the actual time and effort required for human reviewers to correct LLM feedback, quantify the reduction in workload compared to full human evaluation, and identify which types of feedback corrections are most frequent.