---
ver: rpa2
title: 'Patient-Centered Summarization Framework for AI Clinical Summarization: A
  Mixed-Methods Design'
arxiv_id: '2510.27535'
source_url: https://arxiv.org/abs/2510.27535
tags:
- patient
- patients
- clinical
- summaries
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a patient-centered clinical summarization
  framework using mixed-methods involving patient and clinician input, and evaluated
  open-source LLMs against this standard. Patients emphasized lifestyle, social support,
  stressors, and values, while clinicians prioritized concise functional and psychosocial
  context.
---

# Patient-Centered Summarization Framework for AI Clinical Summarization: A Mixed-Methods Design

## Quick Facts
- arXiv ID: 2510.27535
- Source URL: https://arxiv.org/abs/2510.27535
- Reference count: 32
- Primary result: Open-source LLMs capture only 0.8 patient-centered domains per summary vs. 3.6 in transcripts

## Executive Summary
This study develops a patient-centered clinical summarization framework using mixed-methods involving patient and clinician input, then evaluates open-source LLMs against this standard. Patients emphasized lifestyle routines, social support, recent stressors, and care values, while clinicians sought concise functional, psychosocial, and emotional context. Five LLMs (Llama-3.1-8B, Mistral-8B, etc.) were tested on 72 atrial fibrillation consultations using zero- and few-shot prompting, achieving best ROUGE-L of 0.206 and BERTScore of 0.683. Human evaluation showed models matched experts in completeness and fluency but lagged in correctness and patient-centeredness, with AI-generated summaries capturing only 0.8 patient-centered domains on average versus 3.6 in transcripts.

## Method Summary
The study used a mixed-methods design involving Patient and Public Involvement and Engagement (PPIE) interviews with 18 participants (patients and clinicians) to develop a patient-centered summarization framework. Semi-structured interviews identified five patient-centered domains that were translated into annotation guidelines. Eight clinician annotators applied these guidelines to create gold-standard Patient-Centered Summaries (PCS) from 88 transcribed atrial fibrillation consultations. Five open-source LLMs (Llama-3.2-3B, Llama-3.1-8B, Mistral-8B, Gemma-3-4B, Qwen3-8B) were then tested on 72 transcripts using zero-shot and few-shot prompting (1-3 examples). Performance was evaluated using ROUGE-L, BERTScore, and human qualitative assessment across five domains (correctness, completeness, conciseness, patient-centeredness, fluency).

## Key Results
- Best zero-shot performance: Mistral-8B (ROUGE-L 0.189) and Llama-3.1-8B (BERTScore 0.673)
- Best few-shot performance: Llama-3.1-8B (ROUGE-L 0.206, BERTScore 0.683)
- AI-generated summaries captured only 0.8 patient-centered domains on average vs. 3.6 in transcripts
- Models matched human experts in completeness and fluency but lagged in correctness and patient-centeredness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PPIE-derived annotation guidelines create domain-aligned evaluation standards that expose model blind spots in patient-centered content.
- Mechanism: Semi-structured interviews with patients and clinicians surface implicit preferences (lifestyle routines, support systems, emotional health) not represented in biomedical training corpora. Translating these into structured annotation guidelines produces gold-standard summaries that operationalize "patient-centeredness" as measurable domains. When models are benchmarked against this standard, the gap between lexical/semantic overlap (BERTScore 0.683) and domain capture (0.8 vs 3.6 domains) becomes quantifiable.
- Core assumption: Patient and clinician preferences expressed in PPIE sessions generalize to broader populations and clinical contexts.
- Evidence anchors: [abstract] "Patients emphasized lifestyle routines, social support, recent stressors, and care values. Clinicians sought concise functional, psychosocial, and emotional context." [section] "On average, transcripts contained (3.6 ± 2.5) patient-centered domains, whereas AI-generated summaries captured only (0.8 ± 0.9), and nearly half (35/72, 49%) contained none."

### Mechanism 2
- Claim: Few-shot prompting provides marginal gains in lexical/semantic metrics but does not close the patient-centeredness gap.
- Mechanism: Adding 1-3 example summaries in the prompt gives the model in-context demonstrations of desired output structure. This improves ROUGE-L (0.189→0.206) and BERTScore (0.673→0.683) by providing format and style signals. However, without explicit domain labels in examples, models do not learn to prioritize patient-centered content over biomedical content.
- Core assumption: The prompt examples are representative of the target output distribution and do not introduce systematic biases.
- Evidence anchors: [abstract] "The best zero-shot performance was achieved by Mistral-8B (ROUGE-L 0.189) and Llama-3.1-8B (BERTScore 0.673); the best few-shot by Llama-3.1-8B (ROUGE-L 0.206, BERTScore 0.683)." [section] "As few-shot attempts were introduced, performance improved across all models. With one example, Llama-3.1-8B outperformed all others..."

### Mechanism 3
- Claim: Training data bias toward biomedical documentation causes models to systematically undervalue psychosocial and emotional content.
- Mechanism: LLMs are pretrained on clinical notes optimized for billing and biomedical documentation. This creates an inductive bias where "important" content is equated with pathophysiological details. When tasked with summarizing patient-clinician dialogues, models reproduce this bias, deprioritizing patient preferences, values, and concerns even when present in the input.
- Core assumption: The training distribution of clinical notes is the primary driver of model behavior, and instruction-following alone cannot override this bias.
- Evidence anchors: [abstract] "summaries often focus on patients' biology rather than their preferences, values, wishes, and concerns" [section] "most are trained in general-purpose data and are not familiar with the type of personal, emotional, and value-based content essential for patient-centered care"

## Foundational Learning

- Concept: **Patient and Public Involvement and Engagement (PPIE) methodology**
  - Why needed here: The framework relies on structured qualitative input from patients and clinicians to define evaluation criteria. Without understanding PPIE, you cannot replicate or adapt the annotation guideline creation process.
  - Quick check question: Can you explain how PPIE differs from standard user research, and why semi-structured interviews were chosen over surveys for this study?

- Concept: **ROUGE-L vs BERTScore evaluation metrics**
  - Why needed here: The study uses these metrics to quantify model performance. ROUGE-L measures lexical overlap (longest common subsequence), while BERTScore captures semantic similarity via embeddings. Understanding their limitations is critical for interpreting the gap between quantitative and qualitative results.
  - Quick check question: Why might a model achieve moderate BERTScore (0.683) while scoring poorly on patient-centeredness (-4.0 mean rating)?

- Concept: **Zero-shot vs few-shot prompting**
  - Why needed here: The study benchmarks models across both settings. Understanding how in-context examples influence output structure vs content prioritization is essential for prompt engineering in this domain.
  - Quick check question: What type of information would need to be included in few-shot examples to improve patient-centered domain capture, not just format adherence?

## Architecture Onboarding

- Component map: PPIE interview module -> Thematic analysis module -> Annotation guideline generator -> Gold-standard PCS creator -> Prompt engineering module -> Model inference layer -> Evaluation pipeline

- Critical path: PPIE interviews → Thematic analysis → Annotation guidelines → Gold-standard PCS → Prompt refinement → Model inference → Evaluation. The annotation guidelines are the keystone; errors here propagate to gold-standard and all downstream evaluation.

- Design tradeoffs:
  - Open-source vs proprietary models: Chose open-source for privacy (sensitive clinical conversations) but limited model capability
  - UK PPIE / US data split: Pragmatic (data availability) but introduces cultural/systemic gaps
  - No fine-tuning: Isolates prompting effects but may understate achievable performance
  - Manual qualitative evaluation: High validity but low scalability (72 summaries, 2 raters)

- Failure signatures:
  - Hallucination: Model invents medications/procedures not in transcript (rated -3 to -5 on correctness)
  - Patient-centered domain collapse: Model captures <1 of 3.6 available domains on average
  - Format compliance without content: Model produces structured Background/Issues/Plan but omits psychosocial details

- First 3 experiments:
  1. **Domain-tagged few-shot prompting**: Include explicit patient-centered domain labels in few-shot examples (e.g., "[Lifestyle: patient manages BP daily, volunteers]") to test whether in-context domain signals improve capture rate.
  2. **Fine-tuning on PCS gold-standard**: Fine-tune Llama-3.1-8B on the 88 gold-standard PCS to measure whether supervised learning closes the patient-centeredness gap beyond prompting alone.
  3. **Cross-context validation**: Apply the UK-derived framework to a non-US, non-UK clinical dataset (e.g., different healthcare system) to test generalization and identify necessary adaptations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would fine-tuning open-source LLMs on patient-centered clinical summaries significantly improve their ability to capture patient values and preferences compared to zero- and few-shot prompting?
- Basis in paper: [inferred] The study excluded fine-tuning as a limitation, and results showed poor patient-centeredness scores (AI captured only 0.8 domains vs. 3.6 in transcripts).
- Why unresolved: Only prompting strategies were tested; the effect of domain-specific fine-tuning on PCS performance remains unexplored.
- What evidence would resolve it: A controlled experiment comparing few-shot baselines against models fine-tuned on the gold-standard PCS dataset, evaluated using the same qualitative metrics.

### Open Question 2
- Question: Does the patient-centered summarization framework require adaptation when applied to healthcare systems with different cultural and funding contexts (e.g., US insurance-based vs. UK NHS)?
- Basis in paper: [explicit] "Future research should focus on validating and adapting this framework across diverse settings."
- Why unresolved: The framework was developed with UK PPIE participants but tested on US consultations; whether priorities differ by healthcare context is unknown.
- What evidence would resolve it: Replication of the PPIE methodology in other countries or healthcare systems, comparing identified domains to the original five-domain framework.

## Limitations

- The framework's patient-centered criteria depend on UK PPIE participants, raising generalizability concerns to other healthcare systems and cultures
- The gold-standard PCS creation and annotation guidelines are not publicly available, preventing independent validation of the patient-centeredness framework
- Evaluation uses a single disease cohort (atrial fibrillation), limiting applicability to other clinical contexts

## Confidence

- **High Confidence**: The quantitative performance gap between models (ROUGE-L 0.189-0.206, BERTScore 0.673-0.683) and qualitative evaluation showing models match experts in completeness/fluency but lag in correctness and patient-centeredness
- **Medium Confidence**: The attribution of poor patient-centeredness to training data bias, as the mechanism is plausible but not directly tested through intervention studies
- **Medium Confidence**: The mechanism that few-shot prompting improves lexical/semantic metrics without addressing patient-centered content gaps, as this is observed but the prompt examples are not fully specified

## Next Checks

1. Apply the UK-derived patient-centered framework to clinical conversations from different healthcare systems (e.g., Germany, Japan) to test cultural generalizability and identify necessary adaptations
2. Conduct controlled experiments comparing model outputs with and without explicit patient-centered domain labels in few-shot examples to isolate the impact of in-context learning on patient-centeredness capture
3. Perform ablation studies on the annotation guidelines by having independent annotators apply them to the same transcripts to measure inter-rater reliability and validate the framework's construct validity