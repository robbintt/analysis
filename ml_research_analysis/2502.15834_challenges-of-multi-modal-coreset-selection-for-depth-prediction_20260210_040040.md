---
ver: rpa2
title: Challenges of Multi-Modal Coreset Selection for Depth Prediction
arxiv_id: '2502.15834'
source_url: https://arxiv.org/abs/2502.15834
tags:
- coreset
- selection
- multimodal
- dataset
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the challenge of adapting coreset selection
  methods, which are effective for reducing computational costs in training, to multimodal
  settings, specifically focusing on depth prediction tasks using both RGB images
  and semantic masks as inputs. The authors extend a state-of-the-art coreset selection
  approach to multimodal data by employing embedding aggregation and dimensionality
  reduction techniques, such as PCA and UMAP, on concatenated or averaged embeddings
  from the MultiMAE transformer model.
---

# Challenges of Multi-Modal Coreset Selection for Depth Prediction

## Quick Facts
- arXiv ID: 2502.15834
- Source URL: https://arxiv.org/abs/2502.15834
- Reference count: 6
- Primary result: Multimodal coreset selection achieves only ~50% of full dataset performance for depth prediction, showing minimal improvement over random selection

## Executive Summary
This paper investigates the adaptation of coreset selection methods from unimodal to multimodal settings, specifically for depth prediction tasks using RGB images and semantic masks. The authors extend a state-of-the-art coreset selection approach by employing embedding aggregation and dimensionality reduction techniques on concatenated or averaged embeddings from the MultiMAE transformer model. Experiments on the CLEVR dataset reveal that multimodal coreset selection achieves only around 50% of the performance of training on the full dataset, showing minimal improvement over random selection. The best performance was observed with PCA dimensionality reduction using 1024 features, but the improvement was incremental.

## Method Summary
The method involves extracting embeddings from a MultiMAE transformer model for both RGB and semantic mask inputs, then aggregating these embeddings through concatenation, mean pooling, or sum pooling. The aggregated embeddings are optionally reduced in dimensionality using PCA or UMAP before applying a submodular gain function to select a 20% coreset. This coreset is then used to train a DPT output adapter for depth prediction on the CLEVR dataset, with performance compared to training on the full dataset and random 20% subsets.

## Key Results
- Multimodal coreset selection achieves only ~50% of full dataset performance (Val RMSE retention)
- All aggregation methods (concat, mean, sum) cluster near random baseline performance
- PCA at 1024 dimensions shows modest improvement over random selection (55.93% Val RMSE retention)
- UMAP dimensionality reduction shows no consistent gain over PCA
- Training loss retention is higher than validation metrics, suggesting overfitting to coreset

## Why This Works (Mechanism)

### Mechanism 1: Submodular Gain Function for Diversity-Preserving Selection
Selecting samples that maximize distance to already-selected samples while minimizing distance to unselected samples may preserve dataset diversity in a compressed subset. The gain function P(xᵢ) = Σ||f(p) - f(xᵢ)||² (selected) - Σ||f(p) - f(xᵢ)||² (remaining) creates a greedy optimization that partitions data into bins. Uniform sampling from bins ensures late-selected samples contribute equally. Core assumption: Embedding space distances meaningfully capture sample informativeness and diversity for the downstream task.

### Mechanism 2: Embedding Aggregation for Multimodal Fusion
Concatenating or averaging transformer token embeddings from multiple modalities may create unified representations for coreset selection. Extract embeddings f(x) from MultiMAE transformer for RGB and semantic mask inputs. Aggregate via: (1) concatenation [307.824 features reported as typo, likely 768×2], (2) mean pooling across tokens, (3) sum pooling. Core assumption: Simple aggregation operations preserve sufficient cross-modal information for sample importance ranking.

### Mechanism 3: Dimensionality Reduction for Tractable Feature Space
PCA dimensionality reduction may marginally improve coreset selection by focusing on principal variance directions. Apply PCA or UMAP to concatenated embeddings before coreset selection. Tested dimensions: 512, 1024, 2048, 4096. Core assumption: Principal components align with features relevant for downstream depth prediction task.

## Foundational Learning

- **Submodular Optimization**
  - Why needed here: The coreset selection uses a submodular gain function that requires understanding how greedy selection with diminishing returns properties can approximate optimal subset selection.
  - Quick check question: Can you explain why maximizing marginal gain iteratively provides theoretical guarantees for submodular functions?

- **MultiMAE Architecture and Token Embeddings**
  - Why needed here: Embeddings are extracted from the MultiMAE transformer; understanding how masked autoencoders represent multimodal inputs (RGB + semantic masks) is critical for interpreting why aggregation fails.
  - Quick check question: How does MultiMAE handle multiple input modalities through shared vs. modality-specific encoders?

- **Dimensionality Reduction Trade-offs (PCA vs. UMAP)**
  - Why needed here: The paper tests both linear (PCA) and manifold (UMAP) reduction, with different results. Understanding when global vs. local structure preservation matters for downstream tasks informs method selection.
  - Quick check question: Why might UMAP's local structure preservation underperform PCA for coreset selection where global diversity matters?

## Architecture Onboarding

- Component map: [RGB Image] + [Semantic Mask] -> [MultiMAE Transformer] -> Token Embeddings (768-dim per modality) -> [Aggregation Layer] -> Concat / Mean / Sum -> [Dimensionality Reduction] -> PCA / UMAP (optional, 512-4096 dim) -> [Submodular Gain Calculation] -> Distance-based binning -> [Uniform Sampling from Bins] -> 20% Coreset -> [DPT Output Adapter] -> Depth Prediction

- Critical path: Embedding extraction -> Aggregation -> Gain computation -> Binning. The paper identifies the embedding aggregation stage as the likely failure point for multimodal scenarios.

- Design tradeoffs:
  - Concatenation preserves modality-specific features but doubles dimensionality
  - Mean pooling reduces dimensionality but may erase cross-modal interactions
  - PCA at 1024 dims shows modest gains; higher dims add noise, lower dims lose information
  - UMAP nonlinear projection shows no consistent benefit—possibly disrupts distance semantics needed for gain function

- Failure signatures:
  - Val RMSE retention stuck at ~50% (random baseline: 50.23%)
  - Training loss retention higher than validation metrics (51-55% vs 43-50%), suggesting overfitting to coreset
  - Dimensionality sweet spot at 1024; performance degrades on both sides
  - Linear adapter for bottleneck embeddings failed entirely

- First 3 experiments:
  1. **Baseline replication**: Run random coreset (20%) vs. full dataset on CLEVR depth prediction to establish your own 50% benchmark before attempting improvements.
  2. **Ablation on modality contribution**: Select coreset using only RGB embeddings vs. only semantic mask embeddings vs. combined. This diagnoses whether one modality dominates or if fusion is the bottleneck.
  3. **Alternative gain functions**: Test whether gain functions that explicitly model cross-modal alignment (e.g., measuring consistency between modalities for same sample) outperform distance-based diversity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specialized coreset selection techniques be developed to explicitly capture inter-modal relationships?
- Basis in paper: [explicit] The paper concludes by "highlighting the need for specialized methods to better capture inter-modal relationships" as current adaptations fail.
- Why unresolved: The simple aggregation strategies (concatenation, mean, PCA) tested in the study failed to model the complex dependencies between RGB and semantic masks effectively.
- What evidence would resolve it: A novel selection metric that accounts for cross-modal correlation, demonstrating significant performance gains over the random baseline.

### Open Question 2
- Question: Do the observed failures in multimodal coreset selection persist when applied to complex, real-world datasets?
- Basis in paper: [inferred] The experiments are restricted to the synthetic CLEVR dataset, limiting the generalizability of the negative results to real-world autonomous driving scenarios.
- Why unresolved: The "50% performance drop" may be specific to the simplicity of CLEVR; real-world data noise might either exacerbate these issues or provide more distinguishable features for selection.
- What evidence would resolve it: Replicating the PCA and aggregation protocols on diverse, real-world benchmarks (e.g., NYU-Depth-v2) to see if retention rates improve.

### Open Question 3
- Question: Does utilizing embeddings from cross-modal attention layers improve coreset representativeness?
- Basis in paper: [inferred] The authors extract embeddings from the transformer backbone, but the resulting coresets exhibited "worse convergence... likely due to reduced data representativeness."
- Why unresolved: Backbone embeddings may lack the fused, task-specific features required to identify the most informative samples for depth prediction.
- What evidence would resolve it: An ablation study comparing backbone embeddings to those from the DPT adapter or fusion layers, showing higher retention of validation RMSE.

## Limitations
- The 50% performance retention is close to random baseline, suggesting multimodal coreset selection may not provide meaningful benefit over simpler sampling strategies for depth prediction tasks
- Aggregation methods (concat/mean/sum) show minimal performance differences, indicating the choice of fusion strategy may not be the primary bottleneck
- UMAP's consistent underperformance suggests current dimensionality reduction techniques may not preserve task-relevant structure for coreset selection

## Confidence
- **High Confidence**: The experimental methodology and baseline comparisons are sound; the results showing multimodal coreset performance clustering near random baseline are reliable
- **Medium Confidence**: The conclusion that multimodal coreset selection is challenging for depth prediction tasks is supported, but the reasons (aggregation failures vs. fundamental task difficulty) remain unclear
- **Low Confidence**: The claim that this is a "novel" approach is questionable since the paper doesn't provide comprehensive comparison to alternative multimodal selection strategies or task-specific adaptations

## Next Checks
1. **Cross-task validation**: Test the same coreset selection approach on multimodal classification tasks where performance drops might be more severe to determine if depth prediction is uniquely amenable to random sampling
2. **Inter-modal consistency analysis**: Measure correlation between RGB and semantic mask embeddings within samples to quantify whether cross-modal alignment is sufficient for current selection methods to work
3. **Adaptive dimensionality selection**: Implement a validation-based approach to select optimal embedding dimensionality rather than testing fixed values, to determine if the 1024 sweet spot is task-specific or methodological