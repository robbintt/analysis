---
ver: rpa2
title: 'In Context Learning with Vision Transformers: Case Study'
arxiv_id: '2505.20872'
source_url: https://arxiv.org/abs/2505.20872
tags:
- arxiv
- learning
- image
- transformer
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether transformer models can perform in-context
  learning (ICL) on image data. Specifically, it explores whether transformers can
  learn to approximate image transformation functions (linear, convolutional, or ViT-based)
  using only a small number of input-output examples provided in context, without
  direct training on those examples.
---

# In Context Learning with Vision Transformers: Case Study

## Quick Facts
- arXiv ID: 2505.20872
- Source URL: https://arxiv.org/abs/2505.20872
- Authors: Antony Zhao; Alex Proshkin; Fergal Hennessy; Francesco Crivelli
- Reference count: 16
- One-line primary result: Transformers can in-context learn simple convolutional mappings on small grayscale images, achieving low error with only a handful of examples.

## Executive Summary
This paper investigates whether transformer models can perform in-context learning (ICL) on image data, specifically learning to approximate image transformation functions (linear, convolutional, or ViT-based) using only input-output examples provided in context. The core method uses a decoder-only transformer that takes sequences of (image, transformed_image) pairs as context, with images processed through either a CNN or ViT encoder to obtain embeddings. The model is trained to predict transformed images using masked mean squared error loss, with a curriculum that gradually increases image complexity and context size. The primary results show that the transformer can successfully learn to approximate both linear and non-linear image transformations in-context, achieving MSE performance comparable to or better than models trained from scratch on the same limited examples.

## Method Summary
The method constructs a decoder-only transformer model that performs in-context learning on image transformation tasks. Images are downsampled to 8×8 grayscale from CIFAR-10, normalized, and processed through either a CNN or ViT encoder to obtain 256-dimensional embeddings. These embeddings are combined with function outputs to form a sequence passed to a GPT-2 style decoder (12 layers, 8 heads, 256 embedding size). The model is trained using masked mean squared error loss with a quadratic weighting scheme that emphasizes later sequence positions. A curriculum approach is employed where training starts with small d×d masked image blocks and gradually increases dimensionality while also increasing context length according to n = kd + 1 samples.

## Key Results
- Transformer achieves MSE performance comparable to least-squares baselines for linear image transformations
- For non-linear targets (random 2-layer CNN and random ViT functions), ICL model matches or exceeds models trained from scratch on limited examples
- Successfully demonstrates in-context learning of simple convolutional mappings on small grayscale images with only handful of examples
- Sample complexity increases with input size, requiring more context as dimensionality grows

## Why This Works (Mechanism)

### Mechanism 1: In-Context Function Class Approximation via Sequential Prediction
Decoder-only transformers can approximate function classes by conditioning on input-output example pairs without parameter updates at test time. The model learns a general optimization-like procedure during training that transfers to unseen functions from the same class, implementing gradient-based function fitting through attention layers.

### Mechanism 2: Curriculum Training for Progressive Dimension Scaling
Gradually increasing image dimensionality and context length during training enables stable convergence on higher-complexity ICL tasks. Starting with low-dimensional function approximation allows the model to first learn basic skills before scaling up, avoiding optimization difficulties from high-dimensional inputs at initialization.

### Mechanism 3: Query-Focused Loss Weighting
Emphasizing loss on later sequence positions improves final query prediction accuracy by forcing the model to prioritize context aggregation over early-sequence fitting. The quadratic weighting scheme (2k/n)² for k=1,2,...,n incentivizes the model to learn effective context aggregation since later predictions have more context available.

## Foundational Learning

- **In-Context Learning (ICL) in Transformers**: Why needed: The entire paper assumes familiarity with ICL—the ability of transformers to learn from demonstration examples in the prompt without gradient updates. Quick check: Can you explain why ICL differs from few-shot fine-tuning, and why the model sees no gradient updates during the ICL evaluation phase?

- **Vision Transformer (ViT) Architecture**: Why needed: The paper uses ViT encoders and ViT-based target functions. Understanding patch embeddings, positional encodings, and self-attention on image patches is essential to interpret the experimental setups. Quick check: Given an 8×8 image with patch size 4, how many patch tokens does a ViT encoder produce (excluding CLS)?

- **Decoder-Only Transformer with Causal Masking**: Why needed: The GPT-2 style decoder uses causal masking to ensure predictions only attend to preceding context. Understanding autoregressive prediction and masking is necessary to follow the training procedure. Quick check: In a sequence of (x₁, f(x₁), x₂, f(x₂), x₃, ?), which positions can the final prediction attend to?

## Architecture Onboarding

- **Component map**: CIFAR-10 images (downsampled to 8×8 grayscale) -> CNN/ViT encoder -> 256-dim embeddings -> sequence construction (embedding, transformed_embedding pairs) -> GPT-2 decoder (12 layers, 8 heads) -> predictions -> weighted MSE loss

- **Critical path**: 1. Sample image x from CIFAR-10, normalize, downsample to 8×8; 2. Apply frozen target function f to obtain f(x); 3. Encode x via CNN/ViT encoder → embedding e_x; 4. Encode f(x) similarly or via learned projection → embedding e_fx; 5. Construct sequence: [e_x₁, e_fx₁, ..., e_xₙ, e_fxₙ]; 6. Apply causal mask; predict each e_fxᵢ from preceding context; 7. Compute weighted MSE; backprop through encoder + decoder

- **Design tradeoffs**: CNN vs. ViT encoder: CNN converges faster; ViT may generalize better to structured targets but requires more training. Context length n: Longer context improves approximation but increases memory and may slow convergence. Curriculum speed: Aggressive dimension increments reduce training time but risk instability.

- **Failure signatures**: Loss plateaus early with high error: Curriculum may be too aggressive or model capacity insufficient. Strong performance on early positions but weak on query: Loss weighting may be insufficient. Poor generalization to shifted distributions: Encoder may be overfitting to training distribution.

- **First 3 experiments**:
  1. Replicate E1 (linear targets, CNN encoder) with n=5d+1 context pairs; verify MSE approaches least-squares baseline on held-out linear functions
  2. Ablate curriculum by training directly on d=8; compare loss curves and final MSE to curriculum-trained model to quantify curriculum benefit
  3. Test out-of-distribution generalization: Train on random linear functions, evaluate on f(x) = w^T(k * x) with random 3×3 kernel k; compare to in-distribution performance

## Open Questions the Paper Calls Out

- Can in-context learning for vision transformers scale effectively to full-resolution natural images (e.g., 224×224 RGB) without prohibitive increases in sample complexity?
- Would in-context learning on pretrained vision backbones (rather than randomly initialized encoders) improve sample efficiency and generalization to natural image transformations?
- Can alternative fusion mechanisms such as cross-attention reduce the number of required context examples while maintaining or improving approximation accuracy?
- What is the precise relationship between input dimensionality and minimum context length required for effective in-context learning of image transformations?

## Limitations

- Experiments limited to 8×8 grayscale images downsampled from CIFAR-10 due to compute constraints
- Exact architectural details for combining image embeddings with transformed outputs in the sequence remain unspecified
- Curriculum training parameters (step counts, learning rate scheduling) not fully specified
- No ablation studies on the loss weighting mechanism to verify optimal weighting scheme

## Confidence

- **High confidence** in the core finding that transformers can perform ICL on image transformations, supported by MSE results matching or exceeding baselines across multiple function classes
- **Medium confidence** in the curriculum training mechanism, as the approach is described but not extensively validated against alternatives
- **Medium confidence** in the query-weighted loss mechanism, as the improvement is reported but not thoroughly ablated

## Next Checks

1. Implement and test the exact sequence construction method for combining image and transformed-image embeddings, comparing different architectural approaches to verify the intended design
2. Run ablation studies on the loss weighting mechanism (linear vs. quadratic vs. uniform) to determine if the reported improvement is robust or sensitive to the specific weighting scheme
3. Test curriculum schedule sensitivity by comparing fixed-dimension training (d=8 from start) against the proposed curriculum to quantify the exact contribution of progressive dimension scaling