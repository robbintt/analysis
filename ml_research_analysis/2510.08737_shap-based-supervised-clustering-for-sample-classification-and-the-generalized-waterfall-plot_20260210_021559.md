---
ver: rpa2
title: SHAP-Based Supervised Clustering for Sample Classification and the Generalized
  Waterfall Plot
arxiv_id: '2510.08737'
source_url: https://arxiv.org/abs/2510.08737
tags:
- shap
- values
- data
- each
- clustering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHAP-based supervised clustering provides a novel approach to analyzing
  machine learning model predictions by grouping samples not just by their predicted
  class, but by the specific features driving those predictions. The method clusters
  SHAP values (which quantify feature contributions to predictions) rather than raw
  data, revealing subgroups of samples that arrive at similar predictions through
  distinct pathways.
---

# SHAP-Based Supervised Clustering for Sample Classification and the Generalized Waterfall Plot

## Quick Facts
- arXiv ID: 2510.08737
- Source URL: https://arxiv.org/abs/2510.08737
- Authors: Justin Lin; Julia Fukuyama
- Reference count: 13
- Primary result: Novel SHAP-based supervised clustering reveals meaningful subgroups within predicted classes and generalizes waterfall plots for multi-classification

## Executive Summary
This paper introduces SHAP-based supervised clustering, a method that groups samples based on their feature contributions to predictions rather than just predicted class labels. By clustering SHAP values (which quantify how much each feature contributes to a prediction), the approach reveals subgroups of samples that arrive at similar predictions through distinct pathways. The method was validated on both simulated data and Alzheimer's disease data from 2,422 patients, demonstrating its ability to uncover clinically meaningful patterns that standard classification would miss.

The researchers also developed a generalization of waterfall plots for multi-classification problems, enabling visualization of high-dimensional SHAP paths. XGBoost models achieved 90% accuracy on simulated data and 93% on ADNI data. The clustering revealed nuanced subgroups within each clinical status that could inform more personalized treatment approaches, with features like CDRSB, LDELTOTAL, mPACCdigit, and MMSE scores showing particular importance in distinguishing subgroups.

## Method Summary
The method combines supervised machine learning with SHAP value analysis and hierarchical clustering. First, a supervised classification model (XGBoost) is trained to predict class labels from the data. Then, SHAP values are computed for each sample, representing the contribution of each feature to the prediction. Instead of clustering raw data points, the method clusters these SHAP values using hierarchical clustering, revealing groups of samples that arrive at similar predictions through different feature pathways. The generalized waterfall plot extends traditional visualization techniques to multi-class problems, allowing researchers to visualize the high-dimensional SHAP paths that characterize each cluster. This approach provides a deeper understanding of model predictions by showing not just what the model predicts, but how it arrives at those predictions differently across samples.

## Key Results
- In a simulated experiment with 1,500 samples across 10 features, the approach successfully identified three target classes and discovered that Class 2 actually contained two distinct subgroups based on different feature combinations
- Applied to Alzheimer's disease data from 2,422 patients, the method revealed nuanced subgroups within each clinical status (cognitively normal, mild cognitive impairment, Alzheimer's/dementia)
- The researchers developed a generalization of waterfall plots for multi-classification problems, enabling visualization of high-dimensional SHAP paths

## Why This Works (Mechanism)
SHAP-based supervised clustering works by leveraging the interpretability of SHAP values to reveal the decision-making process of classification models. While traditional clustering groups samples based on similarity in their raw feature space, this method groups samples based on similarity in how their features contribute to predictions. This is particularly powerful because it can identify samples that reach the same predicted class through different mechanisms - for example, two patients with Alzheimer's disease might have similar diagnoses but arrive there through different combinations of cognitive deficits and biomarkers. The hierarchical clustering of SHAP values naturally groups samples with similar feature contribution patterns, revealing the underlying structure in how the model makes decisions. This approach is especially valuable when the model's decision process itself contains meaningful clinical or scientific insights, as it allows researchers to identify subgroups that share not just outcomes but also the pathways leading to those outcomes.

## Foundational Learning
- **SHAP values**: Quantify the contribution of each feature to a model's prediction for a specific sample. Why needed: Provide interpretable measures of feature importance for individual predictions. Quick check: Verify SHAP values sum to the difference between the model's prediction and the expected value across all training samples.
- **Hierarchical clustering**: Builds a tree of clusters by merging or splitting existing clusters based on distance metrics. Why needed: Enables visualization of the hierarchical structure in how samples arrive at predictions. Quick check: Compare dendrograms using different linkage methods to assess stability of clusters.
- **Waterfall plots**: Visualize the cumulative contribution of features to a prediction, typically used for SHAP values. Why needed: Provide intuitive visualization of feature contributions for individual samples. Quick check: Ensure all bars in a waterfall plot sum to the total prediction change from baseline.

## Architecture Onboarding

Component map: Data -> XGBoost Model -> SHAP Values -> Distance Matrix -> Hierarchical Clustering -> Generalized Waterfall Plots

Critical path: The essential workflow involves training the XGBoost model on labeled data, computing SHAP values for all samples, calculating pairwise distances between SHAP vectors, performing hierarchical clustering on these distances, and finally visualizing the results using generalized waterfall plots. Each step builds directly on the previous one, with the clustering results being directly determined by the quality of the SHAP value computation.

Design tradeoffs: The method trades computational efficiency for interpretability. Calculating SHAP values for all samples can be computationally expensive, especially for large datasets or complex models. Additionally, the method assumes that SHAP values are stable and meaningful across samples, which may not hold if the underlying model is unstable or if features are highly correlated. The choice of distance metric and linkage method for clustering also significantly affects results but lacks clear optimal choices.

Failure signatures: Poor clustering results may indicate issues with the base model's stability, highly correlated features that make individual contributions hard to distinguish, or insufficient variation in feature contributions across samples. If SHAP values show little variation between samples, the clustering will fail to identify meaningful subgroups. Similarly, if the base model is overfitting or underfitting, the SHAP values may not capture genuine patterns in the data.

First experiments:
1. Run the full pipeline on a small, well-understood dataset to verify each component works correctly
2. Compare clustering results using different distance metrics (Euclidean, Manhattan, cosine) to identify the most stable configuration
3. Visualize SHAP values for individual samples before clustering to understand the range and distribution of feature contributions

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The method relies heavily on SHAP values being both stable and meaningful across samples, but SHAP's consistency depends on the underlying model's robustness and the stability of feature importance rankings
- The clustering results are sensitive to the choice of linkage method and distance metrics, which were not extensively explored in the paper
- The computational cost scales poorly with large numbers of samples and features, as calculating pairwise distances between high-dimensional SHAP vectors can be prohibitive

## Confidence
- SHAP-based supervised clustering reveals meaningful subgroups: Medium - Validated on simulated and ADNI data but needs broader testing
- Clustering SHAP values rather than raw data provides novel insights: High - Theoretically sound and demonstrated in examples
- Generalized waterfall plots enable visualization of high-dimensional SHAP paths: High - Methodologically clear and demonstrated

## Next Checks
1. Test the method on a dataset with known but subtle subgroups (e.g., cancer subtypes with overlapping features) to assess sensitivity to real-world complexity
2. Compare clustering results using different distance metrics and linkage methods to evaluate stability and optimal configurations
3. Apply the approach to time-series data to determine how temporal dependencies affect SHAP-based clustering interpretability