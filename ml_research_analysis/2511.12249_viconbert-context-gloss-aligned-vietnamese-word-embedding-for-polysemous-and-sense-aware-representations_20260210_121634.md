---
ver: rpa2
title: 'ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous
  and Sense-Aware Representations'
arxiv_id: '2511.12249'
source_url: https://arxiv.org/abs/2511.12249
tags:
- semantic
- word
- gloss
- vietnamese
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ViConBERT, a contextualized Vietnamese word
  embedding model designed for fine-grained semantic understanding. The model uses
  contrastive learning and gloss-based distillation to align contextual representations
  with sense definitions, improving word sense disambiguation and semantic similarity.
---

# ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations

## Quick Facts
- arXiv ID: 2511.12249
- Source URL: https://arxiv.org/abs/2511.12249
- Reference count: 26
- Outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on Vietnamese semantic similarity tasks

## Executive Summary
This paper introduces ViConBERT, a contextualized Vietnamese word embedding model designed for fine-grained semantic understanding. The model uses contrastive learning and gloss-based distillation to align contextual representations with sense definitions, improving word sense disambiguation and semantic similarity. A new synthetic dataset, ViConWSD, is constructed from Vietnamese WordNet and large language models to evaluate WSD and contextual similarity tasks. Experiments show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's ρ = 0.60). Visualizations demonstrate the model's ability to distinguish homonyms, polysemous words, and generalize to unseen terms. The approach addresses the lack of robust Vietnamese semantic resources and models.

## Method Summary
ViConBERT combines contrastive learning with gloss-based distillation to create sense-aware Vietnamese word embeddings. The model uses PhoBERT as a context encoder, extracts target word representations via multi-head attention, and aligns them with gloss embeddings from a frozen Vietnamese embedding model using InfoNCE loss. A semantic structure loss preserves relative semantic relationships between context and gloss embedding spaces. The approach is trained on a synthetic dataset (ViConWSD) generated from Vietnamese WordNet using LLMs, containing 33,471 synsets and 100,160 words. The model is trained for 100 epochs with batch size 768, dual learning rates, and evaluated on WSD, contextual similarity, and semantic similarity tasks.

## Key Results
- Achieves state-of-the-art WSD performance with F1 = 0.87 on ViConWSD test set
- Competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's ρ = 0.60) semantic similarity benchmarks
- Successfully distinguishes homonyms, polysemous words, and generalizes to unseen terms
- Ablation studies show the importance of contrastive learning and semantic structure preservation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning contextualized word embeddings with gloss (definition) embeddings via contrastive learning improves sense discrimination for polysemous words.
- Mechanism: The InfoNCE loss pulls the context embedding of a target word closer to its correct gloss embedding while pushing it away from mismatched glosses in the batch. This creates a learned embedding space where distance reflects sense similarity rather than just surface form.
- Core assumption: Gloss definitions from Vietnamese WordNet provide stable, semantically meaningful anchors that reliably correspond to word senses in context.
- Evidence anchors:
  - [abstract] "integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning"
  - [section 3.2] "LInfoNCE encourages context embeddings to align with corresponding gloss embeddings"
  - [corpus] CALE (arxiv:2508.04494) similarly uses concept-aligned embeddings for sense differentiation, suggesting cross-lingual validity of the alignment approach.
- Break condition: If gloss definitions are noisy, circular, or fail to capture usage distinctions, the model learns to align with misleading semantic anchors.

### Mechanism 2
- Claim: Preserving relative semantic structure between context and gloss embedding spaces improves graded similarity detection beyond discrete classification.
- Mechanism: The Semantic Structure Loss (L_SS) computes dissimilarity matrices for both context and gloss embeddings in a batch, then minimizes their MSE. This forces the model to preserve the "topology" of semantic relationships—words that are similar in gloss space should remain similar in context space.
- Core assumption: The pretrained gloss encoder (dangvantuan/vietnamese-embedding) encodes a semantically meaningful structure worth distilling.
- Evidence anchors:
  - [section 3.2] "encourages the relative distances (i.e., semantic topology) among context embeddings to mirror those among their corresponding gloss embeddings"
  - [section 4.3] ViConBERT achieves competitive performance on ViSim-400 (ρ = 0.60), a graded similarity benchmark.
  - [corpus] No direct corpus evidence for this specific loss formulation; it appears novel to this work.
- Break condition: If the gloss encoder's structure is arbitrary or poorly calibrated for Vietnamese, distilling its topology may propagate systematic distortions.

### Mechanism 3
- Claim: Using multi-head attention to extract target-word representations from context improves handling of variable-length word spans.
- Mechanism: After mean-pooling over the target word's token indices to form a query vector, multi-head attention attends over the full contextualized sequence. This allows the model to dynamically weight relevant context tokens rather than relying solely on the target's pooled representation.
- Core assumption: Sense-relevant information is distributed across the sentence, not localized to the target word tokens.
- Evidence anchors:
  - [section 3.1] "Using multi-head attention with Q as query and HC as key-value"
  - [figure 1] Architecture diagram shows attention mechanism between context encoder output and target representation.
  - [corpus] PolyBERT (arxiv:2506.00968) also uses BERT for context+gloss but does not specify attention-based target extraction; comparison is not directly available.
- Break condition: If attention heads collapse or overfit to positional patterns, the model may fail to capture sense-distinguishing context.

## Foundational Learning

- **Concept: Contrastive Learning (InfoNCE)**
  - Why needed here: The core training signal comes from distinguishing correct gloss-context pairs from incorrect ones in a batch.
  - Quick check question: Can you explain why increasing batch size generally improves contrastive learning, and what the temperature parameter (τ = 0.3) controls?

- **Concept: Knowledge Distillation**
  - Why needed here: ViConBERT distills semantic structure from a frozen gloss encoder rather than training both components from scratch.
  - Quick check question: What are the trade-offs between freezing vs. fine-tuning the gloss encoder in this architecture?

- **Concept: Word Sense Disambiguation (WSD) Evaluation**
  - Why needed here: The paper evaluates on WSD using F1@k and NDCG@k; understanding these metrics is essential for interpreting results.
  - Quick check question: Why might a model achieve high F1@10 but lower F1@1, and what does this reveal about its ranking quality?

## Architecture Onboarding

- **Component map:**
  - Input sentence + target word indices -> Context Encoder (PhoBERT-base/large) -> Contextualized hidden states
  - Mean pool over target span -> Query vector -> Multi-head attention (over hidden states) -> Projected context embedding EC
  - Gloss text -> Gloss Encoder (dangvantuan/vietnamese-embedding) -> Gloss embedding EG
  - EC and EG -> Compute LInfoNCE (alignment loss)
  - Dissimilarity matrices of EC and EG -> Compute L_SS (semantic structure loss)
  - Backprop through context encoder and projection layers only

- **Critical path:**
  1. Input sentence + target word indices → Context Encoder → contextualized hidden states
  2. Mean pool over target span → query vector
  3. Multi-head attention (query over hidden states) → projected context embedding EC
  4. Gloss text → Gloss Encoder → gloss embedding EG
  5. Compute LInfoNCE (EC vs. EG against batch negatives)
  6. Compute LSS (MSE between dissimilarity matrices)
  7. Backprop through context encoder and projection layers only

- **Design tradeoffs:**
  - Freezing the gloss encoder reduces training cost and provides stable anchors, but prevents adaptation to domain-specific gloss formulations.
  - Using PhoBERT over XLM-R or ViDeBERTa: Paper shows PhoBERT-base achieves best F1@10 (83.18) vs. ViDeBERTa-base (80.53), likely due to Vietnamese-specific pretraining.
  - Synthetic dataset (ViConWSD) enables large-scale training but relies on LLM-generated glosses; 90% quality on 200 sampled pairs does not guarantee full dataset cleanliness.

- **Failure signatures:**
  - **Sense collapse**: All context embeddings cluster near a single gloss prototype (check by visualizing EC distributions per sense).
  - **Overfitting to frequent senses**: High F1@10 on common words but poor generalization to rare senses (check per-sense F1 breakdown).
  - **Attention degradation**: Attention weights uniform or concentrated on [CLS]/punctuation (inspect attention patterns on held-out examples).

- **First 3 experiments:**
  1. **Ablate LSS**: Train with only InfoNCE loss (λ = 0) and compare WSD F1 and ViSim-400 correlation to full model to isolate the contribution of structure preservation.
  2. **Swap gloss encoder**: Replace dangvantuan/vietnamese-embedding with VoVanPhuc/sup-SimCSE-VietNamese-phobert-base and measure performance gap; paper shows minimal difference, but confirm on your data split.
  3. **Error analysis on homonyms**: Extract all "khoan" examples from ViConWSD test set, compute per-sense precision/recall, and manually inspect failure cases to identify whether errors stem from gloss ambiguity or context encoding failures.

## Open Questions the Paper Calls Out

None

## Limitations

- The model relies on synthetic data generated by LLMs, which may introduce noise and domain shift despite reported 90% quality on sampled pairs
- Freezing the gloss encoder prevents adaptation to domain-specific semantic nuances and may propagate systematic biases
- Evaluation is limited to Vietnamese-specific benchmarks without cross-lingual comparisons or multilingual transfer experiments

## Confidence

- **High confidence**: The core contrastive learning mechanism (InfoNCE loss) and its implementation are well-grounded in established literature. The reported WSD F1 scores and their superiority over baselines are reliable given the clear evaluation protocol.
- **Medium confidence**: The contribution of the Semantic Structure Loss and its interaction with InfoNCE is plausible but requires further ablation studies to quantify its independent impact. The synthetic dataset quality claims are based on sampling rather than comprehensive validation.
- **Low confidence**: The generalization claims to unseen words and the robustness of attention-based target extraction lack empirical validation through targeted experiments or error analysis.

## Next Checks

1. **Ablation study of loss components**: Train three variants—InfoNCE only (λ=0), L_SS only, and full ViConBERT—on the same ViConWSD training split. Compare WSD F1@k and ViSim-400 Spearman's ρ to isolate the marginal contribution of semantic structure preservation. This addresses whether L_SS provides benefits beyond alignment alone.

2. **Error analysis on homonyms and polysemes**: Extract all test instances containing the word "khoan" (drill/scope/account) from ViConWSD. Compute per-sense precision, recall, and F1. Manually inspect the top 20 errors to determine whether failures stem from gloss ambiguity, context encoding limitations, or attention mechanism shortcomings. This validates the model's ability to distinguish fine-grained senses.

3. **Synthetic data quality audit**: Randomly sample 500 ViConWSD pairs stratified by sense frequency. Use human annotation (native Vietnamese speakers) to rate gloss-context semantic alignment on a 3-point scale. Compare error rates between frequent and rare senses to assess whether the LLM generation pipeline introduces systematic biases.