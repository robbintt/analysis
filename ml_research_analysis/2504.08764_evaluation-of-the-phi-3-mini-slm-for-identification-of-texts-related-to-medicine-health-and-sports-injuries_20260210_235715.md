---
ver: rpa2
title: Evaluation of the phi-3-mini SLM for identification of texts related to medicine,
  health, and sports injuries
arxiv_id: '2504.08764'
source_url: https://arxiv.org/abs/2504.08764
tags:
- texts
- sports
- health
- filtering
- injury
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the phi-3-mini small language model (SLM)
  for identifying texts related to medicine/health and sports injuries. The SLM assigned
  topic-relatedness scores to 9+ million news headlines, and a subset was compared
  to human evaluators.
---

# Evaluation of the phi-3-mini SLM for identification of texts related to medicine, health, and sports injuries

## Quick Facts
- arXiv ID: 2504.08764
- Source URL: https://arxiv.org/abs/2504.08764
- Reference count: 9
- Primary result: Phi-3-mini shows poor reliability for sports injury text identification (binary agreement 6%), with better performance for medicine/health when using higher filtering thresholds (binary agreement 74%)

## Executive Summary
This study evaluates the phi-3-mini small language model (SLM) for identifying texts related to medicine/health and sports injuries. The SLM assigned topic-relatedness scores to 9+ million news headlines, with a subset compared to human evaluators. Results show significant domain-dependent performance differences: the model demonstrated good agreement (74%) with human ratings for medicine/health texts under high filtering conditions, but poor performance for sports injury texts (6% binary agreement). The study highlights the importance of filtering strategies and domain-specific training data for effective SLM deployment in specialized text classification tasks.

## Method Summary
The study processed 9,353,430 Canadian news headlines through phi-3-mini-4k-instruct using a standardized prompt asking for 1-10 relevance scores for seven topics. Low filtering selected texts with scores >7 for a topic, while high filtering applied additional Boolean conditions (e.g., for medicine/health: score >7 AND cannabis/opioids/sports injuries scores = 1). A subset of 2,261 texts (1,144 medicine/health, 1,117 sports injuries) underwent human evaluation, with scores compared to SLM outputs using Spearman correlation and binary agreement metrics. The analysis was conducted across multiple filtering thresholds to assess performance optimization.

## Key Results
- For medicine/health texts: Low filtering showed low correlation (ρ=0.23, p<0.001), high filtering showed low-moderate correlation (ρ=0.39, p<0.001) with human ratings
- For sports injury texts: Low filtering showed low-moderate correlation (ρ=0.34, p<0.001), but high filtering showed negligible correlation (ρ=0.03, p=0.45)
- Binary agreement was 54% for medicine/health (low filter) and only 6% for sports injuries (low filter)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Phi-3-mini's medical domain knowledge emerges from benchmark pre-exposure rather than task-specific fine-tuning
- Mechanism: The model was benchmarked on MedQA during development, encoding clinical/medical knowledge that transfers to medicine/health text classification without additional training
- Core assumption: Benchmark exposure during training creates reusable domain representations for downstream classification tasks
- Evidence anchors:
  - [abstract] "SLMs often are benchmarked on health/medicine-related tasks, such as MedQA"
  - [section IV] "it was benchmarked on MedQA as mentioned in the introduction, which might explain some of the better performance for medical/health"
  - [corpus] Neighbor paper "Medicine on the Edge" evaluates on-device LLMs for clinical reasoning, showing similar domain transfer patterns in edge-deployed models
- Break condition: If MedQA-style questions don't appear in training data, medical topic classification should degrade to sports injury performance levels

### Mechanism 2
- Claim: Boolean filtering on multiple topic scores reduces false positives by enforcing mutual exclusivity constraints
- Mechanism: When the SLM scores a text high on medicine/health AND low on unrelated topics (cannabis=1, opioids=1), specificity increases because true positives tend to cluster differently than false positives across topic dimensions
- Core assumption: True positives for one topic should not simultaneously score high on unrelated topics; multi-topic high scores indicate model confusion
- Evidence anchors:
  - [abstract] "Our sample was selected (filtered) based on 1 (low filtering) or more (high filtering) Boolean conditions"
  - [section II.C] "for health/medicine additional conditions include relatedness scores for cannabis, sports injuries, and opioids being equal to 1"
  - [corpus] No direct corpus evidence for this specific filtering mechanism; related papers focus on domain adaptation rather than multi-condition filtering
- Break condition: If topics are semantically overlapping (e.g., "sports medicine"), high filtering should reduce recall without improving precision

### Mechanism 3
- Claim: SLM assigns systematically higher scores than humans, compressing meaningful variation into a narrow high range
- Mechanism: The model defaults to high confidence (mode=10, low standard deviation 0.48-0.88) even when humans show distributed ratings (mean 1.87-6.32), suggesting calibration failure rather than ranking failure
- Core assumption: The 1-10 scale elicits different interpretation patterns from SLMs vs. humans; SLMs treat it as binary (related/unrelated) while humans use gradations
- Evidence anchors:
  - [section IV] "For medical/health texts, depending on filtering, there was mixed agreement from the human evaluators (low filtering, 54%), to good agreement (high filtering, 74%)"
  - [Table 7] SLM mode consistently 10.0 across all conditions; human modes range 1.0-7.0
  - [corpus] Weak corpus evidence; neighbor papers don't address score calibration in SLMs
- Break condition: If score distributions between SLM and humans converge (similar variance, similar central tendency), correlation should improve without filtering changes

## Foundational Learning

- **Spearman rank correlation (ρ)**
  - Why needed here: Measures monotonic relationship between SLM and human ordinal ratings; robust to non-linear scaling differences
  - Quick check question: If SLM scores are [9,9,10] and human scores are [3,5,8] for the same texts, what would ρ approximately be?

- **Inter-rater reliability (ICC, Fleiss's Kappa)**
  - Why needed here: Establishes human evaluation quality baseline before comparing to SLM; low human agreement would invalidate SLM comparison
  - Quick check question: If 7 raters' ICC is 0.64 for sports injuries vs. 0.76 for medicine, which domain's human ground truth is more trustworthy?

- **Boolean multi-condition filtering**
  - Why needed here: Core technique for improving SLM precision by rejecting predictions that fail auxiliary constraints
  - Quick check question: Given scores (medicine=8, cannabis=3, opioids=1), would this pass "high filtering" for medicine/health?

## Architecture Onboarding

- **Component map**: 
  - Prompt template -> Phi-3-mini-4k-instruct inference -> Score extraction -> Boolean filtering -> Human evaluation comparison

- **Critical path**:
  1. Define topic list and prompt template → 2. Process 9M headlines through SLM (4 weeks on 3 GPUs) → 3. Apply filtering conditions → 4. Sample filtered texts → 5. Human evaluation (binary + numeric) → 6. Compute correlation/agreement metrics

- **Design tradeoffs**:
  - **Filtering threshold (7 vs. higher)**: Higher threshold improves precision but reduces recall; study found 74% binary agreement at high filtering vs. 54% at low for medicine
  - **Throughput vs. accuracy**: SLM processes 475K-950K texts/week per GPU; human evaluation infeasible at scale
  - **Single-topic vs. multi-topic inference**: Running 7 topics per text increases compute 7x but enables Boolean filtering

- **Failure signatures**:
  - **Sports injury classification collapse**: Binary agreement 6%, ρ=0.03 with high filtering—model lacks domain-specific training
  - **Score compression**: SLM outputs clustered at 9-10 regardless of text relevance, inflating false positive rate
  - **Named entity confusion**: SLM rates journalist bylines as topic-related based on reporter specialty (e.g., sports reporter name → high sports injury score)

- **First 3 experiments**:
  1. **Threshold sweep**: Test filtering thresholds 5, 6, 7, 8, 9 on a held-out validation set to find precision-recall optimal point
  2. **Few-shot prompting**: Add 3-5 labeled examples to prompt to improve score calibration and reduce compression
  3. **Domain-adaptive filtering**: For sports injuries, add sport-specific exclusion conditions (e.g., exclude if medicine/health>7 AND no injury keywords detected)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does phi-3-mini utilize named entity recognition (specifically author names) to infer topic relevance?
- Basis in paper: [explicit] The authors observed the SLM rated reporter bylines as highly related and suspect the model recognized the journalist's beat, stating "further analysis is needed to confirm."
- Why unresolved: The study design focused on headline text ratings, not ablation studies on specific text features like author names.
- What evidence would resolve it: An ablation study removing or randomizing author names in the dataset while keeping content constant.

### Open Question 2
- Question: Can alternative filtering conditions significantly improve the correlation between phi-3-mini and human evaluators for sports injury texts?
- Basis in paper: [explicit] Authors state "it is possible that there needed to be stricter filters on sports injury than what ended up being used in this study" to address the negligible correlation found.
- Why unresolved: The "high filtering" condition used actually reduced correlation to negligible levels (ρ=0.03), suggesting the specific logical conditions applied were counterproductive.
- What evidence would resolve it: Iterative testing of various threshold combinations and boolean logic on a validation set of sports injury headlines.

### Open Question 3
- Question: Does incorporating domain-specific training data improve the model's ability to identify niche topics like sports injuries compared to general instruction tuning?
- Basis in paper: [explicit] Authors speculate better performance in medicine/health occurred because the model was benchmarked on MedQA, whereas it likely lacked "specific sports injury training data."
- Why unresolved: This study evaluated an off-the-shelf model; it did not involve retraining or fine-tuning the model to test this hypothesis.
- What evidence would resolve it: A comparative evaluation of a phi-3-mini variant fine-tuned on a sports injury corpus against the baseline model.

## Limitations
- Poor sports injury classification reliability with only 6% binary agreement and negligible correlation with human ratings
- Score calibration issues with SLM consistently outputting high scores (mode=10) regardless of text relevance
- Limited domain-specific training data for niche topics like sports injuries compared to medicine/health

## Confidence

- **High confidence**: Phi-3-mini's poor performance on sports injury classification is reliably established through multiple metrics (ρ=0.03, p=0.45 for high filtering; 6% binary agreement)
- **Medium confidence**: Better performance on medicine/health topics is observed but heavily dependent on filtering thresholds, with high filtering improving binary agreement from 54% to 74%
- **Low confidence**: The proposed mechanism linking MedQA benchmark exposure to medical domain knowledge transfer lacks direct empirical validation within this study

## Next Checks

1. Conduct a controlled ablation study comparing phi-3-mini performance on MedQA-style questions versus sports injury classification to quantify the domain knowledge transfer effect
2. Implement a score calibration pipeline using a small labeled validation set to map SLM outputs to human rating distributions before full-scale deployment
3. Test alternative filtering strategies, such as threshold calibration per topic or using percentile-based rather than absolute score thresholds, to optimize precision-recall tradeoffs