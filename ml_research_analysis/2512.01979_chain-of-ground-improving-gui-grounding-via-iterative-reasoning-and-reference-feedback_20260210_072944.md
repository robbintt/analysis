---
ver: rpa2
title: 'Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference
  Feedback'
arxiv_id: '2512.01979'
source_url: https://arxiv.org/abs/2512.01979
tags:
- grounding
- wang
- visual
- chen
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurately grounding natural
  language instructions to precise UI elements in complex interfaces. The authors
  propose Chain-of-Ground (CoG), a training-free multi-step grounding framework that
  leverages multimodal large language models (MLLMs) for iterative visual reasoning
  and refinement.
---

# Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback

## Quick Facts
- arXiv ID: 2512.01979
- Source URL: https://arxiv.org/abs/2512.01979
- Reference count: 40
- Primary result: Chain-of-Ground achieves 68.4% grounding accuracy on ScreenSpot-Pro, surpassing state-of-the-art by 4.8%

## Executive Summary
Chain-of-Ground (CoG) addresses the challenge of accurately grounding natural language instructions to precise UI elements in complex interfaces. The authors propose a training-free multi-step grounding framework that leverages multimodal large language models (MLLMs) for iterative visual reasoning and refinement. Instead of direct prediction, CoG enables the model to progressively reflect and adjust its hypotheses through reference feedback, achieving more accurate and interpretable localization. On the ScreenSpot-Pro benchmark, CoG achieves 68.4% accuracy, surpassing the previous state-of-the-art by 4.8%. The method also shows strong performance on a new real-world industrial control panel dataset (TPanel-UI), improving over the baseline Qwen3-VL-235B by 6.9%.

## Method Summary
Chain-of-Ground is a training-free multi-step grounding framework that treats grounding as a stateful search rather than a single regression task. The method uses iterative "reflect-and-refine" loops where an initial anchor prediction is visually encoded back onto the input image as a marker. Subsequent steps refine this anchor through iterative updates, allowing the model to self-correct. The framework can use homogeneous model chains (same model for all steps) or heterogeneous chains (different models for different steps), with the latter showing superior performance. Visual markers provide spatial cues that are more effective than textual coordinate descriptions, leveraging the MLLM's visual-semantic alignment.

## Key Results
- CoG achieves 68.4% grounding accuracy on ScreenSpot-Pro, surpassing state-of-the-art by 4.8%
- On TPanel-UI, CoG improves over baseline Qwen3-VL-235B by 6.9%
- Triple-step CoG outperforms single-step by 4.1% on ScreenSpot-Pro
- Visual feedback (65.8%) outperforms text-based feedback (64.3%) on ScreenSpot-Pro

## Why This Works (Mechanism)

### Mechanism 1
Iterative "reflect-and-refine" loops recover grounding accuracy lost in single-pass predictions by treating grounding as a stateful search. The framework generates an initial anchor prediction, visually encodes it back onto the image, and allows the MLLM to evaluate and correct its own hypothesis in subsequent steps. This self-correction capability leverages the base MLLM's spatial reasoning to interpret visual markers as feedback signals.

### Mechanism 2
Heterogeneous model stacking outperforms single-model self-correction by leveraging different MLLMs' distinct inductive biases and error modes. A smaller, specialized model generates a coarse anchor, while a larger, high-capacity model refines the precise location. This creates an ensemble effect where the refinement model compensates for the anchor model's blind spots, producing superior results compared to homogeneous loops.

### Mechanism 3
Visual markers provide superior spatial cues compared to textual coordinate descriptions by allowing the MLLM to process its own hypothesis using its native visual encoder. Converting predicted coordinates into visual overlays (e.g., colored circles or squares) bypasses the difficulty MLLMs have with pure numerical coordinate text, leveraging the model's visual-semantic alignment to judge distance and relation to the target.

## Foundational Learning

- **Visual Grounding in MLLMs**: Understanding that MLLMs struggle with precise coordinate regression vs. semantic classification is essential to grasp why iterative refinement is needed. Quick check: Can you explain why telling an MLLM to "click at [100, 200]" is harder for it than saying "click the red button on the right"?

- **Chain-of-Thought (CoT) & Reflection**: CoG extends the textual "Chain-of-Thought" paradigm into the visual-spatial domain. You must understand that "reasoning" here implies intermediate processing steps rather than direct input-output mapping. Quick check: How does adding a "reflection" step change the computational graph of a standard MLLM inference call?

- **Test-Time Scaling / Compute Optimal Inference**: CoG is a "training-free" method that buys accuracy with compute (multiple inference passes). Understanding this trade-off is critical for real-world deployment where latency matters. Quick check: If you have a strict latency budget of 500ms, is CoG (Triple-Step) a viable strategy for a 7B parameter model?

## Architecture Onboarding

- **Component map**: Anchor MLLM (Image, Instruction) -> Initial Coordinates → Feedback Encoder (Image, Coordinates) -> Marked Image → Refinement MLLM (Marked Image, Instruction) -> Refined Coordinates → Iteration Controller (manages loop)

- **Critical path**: The Feedback Encoder is the linchpin. If the visual marker is rendered incorrectly (wrong color, size, or opacity), the Refinement MLLM cannot ground its correction. The paper specifically tests Small vs. Large marks (r=10 vs r=100), indicating sensitivity here.

- **Design tradeoffs**:
  - Marker Size: Small markers may be ignored; large markers risk occluding adjacent UI elements
  - Model Selection: Homogeneous loops (same model) are easier to deploy but hit accuracy ceilings; Heterogeneous loops offer SOTA but increase infrastructure complexity
  - Inference Cost: Triple-step inference takes ~3x the latency and cost of single-step

- **Failure signatures**:
  - Oscillation: The predicted coordinate jumps back and forth between two wrong locations in successive steps
  - Hallucinated Anchor: The first model marks a region that doesn't exist, and the second model hallucinates context around it
  - Occlusion Blindness: The refinement model fails to predict a button because the large marker completely covered it

- **First 3 experiments**:
  1. Baseline Integrity: Run a single-step evaluation on your target MLLM (e.g., Qwen-VL) to establish a raw accuracy floor on ScreenSpot-Pro
  2. Feedback Ablation: Implement the Feedback Encoder. Compare "Text-based" (appending coordinates to prompt) vs. "Image-based" (drawing a circle) feedback to verify visual feedback is superior
  3. Iteration Stability: Run a Dual-Step vs. Triple-Step comparison. Check if the third step actually improves accuracy or introduces noise

## Open Questions the Paper Calls Out
- Generalization to visual grounding domains outside GUI (e.g., natural images, document understanding) remains untested and requires application to standard benchmarks
- Error propagation mitigation when intermediate reasoning steps are incorrect needs mechanisms to filter low-quality intermediate feedback
- Optimal trade-off between accuracy gains and computational latency is unexplored; dynamic halting strategies could save resources

## Limitations
- Higher inference time and computational cost compared to single-pass approaches
- CoG does not guarantee correct reasoning paths; both correct and incorrect answers can arise from flawed intermediate steps
- Generalization to other domains (beyond GUI) remains an open question

## Confidence
- High confidence: Core mechanism of iterative refinement with visual feedback is clearly described and validated on ScreenSpot-Pro
- Medium confidence: Heterogeneous model stacking strategy is supported by ablation studies, but optimal model ordering depends on specific configurations
- Low confidence: Claims about robustness to real-world degradations on TPanel-UI are based on a dataset that may not be accessible

## Next Checks
1. Test multiple prompt formats (text-based vs. image-based feedback) on a held-out validation set to confirm visual markers outperform textual coordinates
2. Verify that coordinate outputs from the anchor model are correctly interpreted by the refinement model, especially in heterogeneous chains
3. Simulate blur, noise, and masking on ScreenSpot-Pro to test robustness claims and identify failure modes under real-world conditions