---
ver: rpa2
title: Fine-tuning LLMs with variational Bayesian last layer for high-dimensional
  Bayesian optimization
arxiv_id: '2510.01471'
source_url: https://arxiv.org/abs/2510.01471
tags:
- optimization
- bayesian
- variables
- lora
- proc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of high-dimensional Bayesian
  optimization (BO) with irregular variables (categorical, ordinal) by leveraging
  pre-trained large language models (LLMs) as surrogate models. The proposed LoRA-VBLL
  method combines parameter-efficient fine-tuning using Low-Rank Adaptation (LoRA)
  with variational Bayesian last layer (VBLL) framework to efficiently estimate uncertainty.
---

# Fine-tuning LLMs with variational Bayesian last layer for high-dimensional Bayesian optimization

## Quick Facts
- arXiv ID: 2510.01471
- Source URL: https://arxiv.org/abs/2510.01471
- Authors: Haotian Xiang; Jinwen Xu; Qin Lu
- Reference count: 40
- Primary result: Achieves 78% less memory than BLoB while maintaining top performance on high-dimensional BO benchmarks

## Executive Summary
This paper addresses the challenge of high-dimensional Bayesian optimization with irregular variables by leveraging pre-trained large language models as surrogate models. The proposed LoRA-VBLL method combines parameter-efficient fine-tuning using Low-Rank Adaptation (LoRA) with a variational Bayesian last layer framework to efficiently estimate uncertainty. An ensemble approach automatically selects optimal LoRA ranks through data-adaptive weighting, demonstrating superior performance across various high-dimensional benchmarks and real-world molecular optimization tasks.

## Method Summary
The method trains an ensemble of LoRA-VBLL surrogates with different ranks, using a two-phase training procedure: initial MSE pre-training followed by ELBO optimization. The VBLL framework maintains a posterior distribution over the last layer weights rather than point estimates, enabling uncertainty quantification. Ensemble members are weighted by their marginal likelihood (approximated by ELBO), and recursive updates allow efficient posterior refinement between expensive LoRA fine-tuning events. Features can be cached for discrete candidate pools to further reduce computational overhead.

## Key Results
- ENS-LoRA-VBLL achieves superior performance on Pest Control (25D categorical), MAXSAT60 (60D binary), and Redoxmer (SMILES) benchmarks
- Requires 78% less memory than BLoB while maintaining competitive optimization performance
- Automated rank selection through ensemble weighting eliminates need for manual hyperparameter tuning
- Demonstrates significant improvements over baselines including GP, I-BNN, LLLA, BLoB, and DKL

## Why This Works (Mechanism)

### Mechanism 1
Separating feature extraction (deterministic LoRA-finetuned LLM) from the prediction head (Variational Bayesian Last Layer) enables scalable uncertainty quantification required for Bayesian Optimization acquisition functions. The LLM backbone provides high-dimensional feature representations $\phi(x)$, while a variational distribution $q(\beta)$ over last layer weights allows outputting predictive distributions rather than point estimates, facilitating Thompson Sampling without full Bayesian Neural Network costs.

### Mechanism 2
Ensembling multiple LoRA-VBLL models with distinct ranks automates hyperparameter selection and improves robustness via data-adaptive weighting. The method trains $J$ independent models with different LoRA ranks and uses marginal likelihood (approximated by ELBO) to dynamically weight models that better explain observed data, mitigating risk of choosing sub-optimal rank a priori.

### Mechanism 3
Computational efficiency is achieved by decoupling frequency of LLM fine-tuning from Bayesian posterior updates of final layer. LoRA parameters update infrequently using event-triggers when predictive likelihood drops, while posterior of final Bayesian layer updates analytically via recursive Bayes between these expensive fine-tuning steps. LLM features can be cached if search space is discrete candidate pool, avoiding redundant forward passes.

## Foundational Learning

- **Concept: Bayesian Optimization (BO) Loop**
  - Why needed here: Core application where LLM functions as surrogate model to approximate black-box function through cycle: Surrogate Update → Acquisition Function (Thompson Sampling) → Query → Observation
  - Quick check question: Can you explain why Thompson Sampling relies on drawing samples from posterior distribution rather than just using mean prediction?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: Fine-tuning all parameters of LLM for every optimization step is computationally prohibitive; LoRA allows model to adapt by training only small rank-decomposition matrices ($A$ and $B$)
  - Quick check question: How does freezing pre-trained weights $W_0$ and only updating $A$ and $B$ affect model's capacity to learn new tasks versus adapting to numerical optimization tasks?

- **Concept: Evidence Lower Bound (ELBO)**
  - Why needed here: Paper uses ELBO not just for training but as metric to rank ensemble members; understanding trade-off between likelihood term (fitting data) and KL divergence (regularization) crucial for diagnosing model performance
  - Quick check question: In context of ensemble weighting, what does higher ELBO value imply about specific LoRA rank's suitability for current data?

## Architecture Onboarding

- **Component map:** Structured data → Text prompts (LIFT) → Pre-trained LLM with LoRA adapters ($A, B$) → Variational Bayesian Last Layer (VBLL) → Thompson Sampling acquisition → New candidates

- **Critical path:**
  1. Implement prompt converter (LIFT) to translate optimization variables to text
  2. Implement VBLL head with custom ELBO loss function - most likely integration point for bugs
  3. Implement Recursive Update logic to ensure posterior updates efficiently without full re-training
  4. Verify Ensemble weighting logic updates weights correctly based on predictive likelihood

- **Design tradeoffs:**
  - Rank Selection vs. Compute: Higher ranks allow more expressivity but require more memory; paper automates via ensembling but multiplies memory usage linearly by number of ensemble members
  - Fine-tuning Frequency: Event-trigger balances accuracy vs. speed; threshold too tight triggers expensive fine-tuning too often, too loose lets model drift
  - Feature Caching: Only works if search space is discrete/pool-based; invalid for continuous variables

- **Failure signatures:**
  - Posterior Collapse: Covariance $\Sigma$ in VBLL collapses to near-zero, resulting in over-confident predictions and lack of exploration
  - Ensemble Domination: One rank immediately dominates (weights $\approx 1.0$), effectively turning ensemble into single model and losing robustness
  - NaN Loss: Instability in ELBO calculation, specifically KL divergence or matrix inversion in recursive update, often due to poorly conditioned features from LLM

- **First 3 experiments:**
  1. Sanity Check (Synthetic): Run LoRA-VBLL on low-dimensional synthetic function (e.g., Branin) with known optimum; verify Thompson Sampling reduces regret compared to random search
  2. Ablation (Recursive vs. Full): Compare performance and wall-clock time of "Recursive Updates Only" vs. "Full Fine-tuning Every Step" on small dataset to validate efficiency mechanism
  3. Rank Sensitivity: Run sweep of single LoRA-VBLL models with ranks $\{4, 8, 16, 32\}$ and compare against ENS-LoRA-VBLL to confirm no single rank consistently wins and ensemble is necessary

## Open Questions the Paper Calls Out

- **Question:** How can computational efficiency of acquisition function optimization be improved for high-dimensional continuous variables where feature caching is infeasible?
  - Basis in paper: Authors state in Limitations that "further improvement is due for the general case that entails gradient evaluation for continuous variables," noting caching only helps when $x$ is chosen from pool of candidates
  - Why unresolved: Current efficiency relies on caching features for discrete candidate pools; continuous optimization requires gradient evaluation through LLM feature extractor at new points, remaining time-consuming bottleneck
  - What evidence would resolve it: Method that accelerates gradient-based acquisition optimization for continuous variables, achieving comparable wall-clock efficiency to pool-based methods without relying on pre-computed feature caches

- **Question:** Can specialized acquisition functions be designed to better leverage specific uncertainty quantification of Variational Bayesian Last Layer (VBLL)?
  - Basis in paper: Limitations note that "The AF design follows the current practise," implying standard Thompson Sampling may not fully exploit properties of LoRA-VBLL surrogate
  - Why unresolved: Interaction between variational posterior of last layer and exploration-exploitation strategy has not been co-designed; standard acquisition functions may be suboptimal for specific covariance structure provided by VBLL
  - What evidence would resolve it: Derivation and testing of novel acquisition function tailored to VBLL posterior distribution that demonstrates statistically significant improvements in regret minimization over standard Thompson Sampling

- **Question:** Is there critical threshold for LLM backbone scaling in HDBO, or do returns diminish rapidly relative to computational cost?
  - Basis in paper: Authors mention testing Llama-3.1-8B but found "performance gain is marginal, while it is much more time-consuming," leaving trade-off between model scale and optimization efficiency unclear
  - Why unresolved: While larger models offer better representation, marginal gain observed suggests simple Bayesian last layer might limit utility of deeper LLM features, or benchmarks may be insufficiently complex to require larger models
  - What evidence would resolve it: Systematic scaling law study plotting optimization performance against inference latency and parameter count across wider range of LLM sizes on complex, non-synthetic tasks

## Limitations
- Long-term stability of "stale" feature assumption in rapidly shifting optimization landscapes not experimentally validated, particularly for continuous variable spaces where feature caching is invalid
- Memory efficiency quantified but computational efficiency (wall-clock time) not comprehensively compared against full fine-tuning baselines
- Event-trigger mechanism for LoRA updates lacks specified threshold, which could significantly impact both performance and efficiency

## Confidence
- **High Confidence:** Memory efficiency claims (78% reduction vs. BLoB) and core mathematical formulation of VBLL (posterior updates, ELBO optimization) are well-defined and reproducible
- **Medium Confidence:** Ensemble weighting scheme and automated rank selection show strong empirical support but rely on ELBO as proxy for model evidence, which may not always be optimal
- **Low Confidence:** Long-term stability of "stale" feature assumption in rapidly shifting optimization landscapes is not experimentally validated, particularly for continuous variable spaces where feature caching is invalid

## Next Checks
1. **Dynamic Landscape Test:** Apply LoRA-VBLL to synthetic function with abrupt local shifts (e.g., multi-modal Rastrigin) to quantify degradation in performance when relying on cached features vs. real-time fine-tuning
2. **Rank Collapse Analysis:** Monitor ensemble weight distributions during optimization to verify no single rank dominates prematurely, undermining robustness claim
3. **Memory-Wall Clock Trade-off:** Benchmark against full fine-tuning baselines on high-dimensional molecular dataset, measuring both peak memory usage and total optimization time to validate efficiency argument comprehensively