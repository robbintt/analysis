---
ver: rpa2
title: 'Learning Regularizers: Learning Optimizers that can Regularize'
arxiv_id: '2510.08968'
source_url: https://arxiv.org/abs/2510.08968
tags:
- loss
- regularization
- training
- optimizer
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether learned optimizers can internalize
  regularization techniques without explicit application during optimization. The
  authors train an LSTM-based learned optimizer on tasks like MNIST classification,
  incorporating regularization objectives such as SAM, GSAM, and GAM.
---

# Learning Regularizers: Learning Optimizers that can Regularize

## Quick Facts
- **arXiv ID:** 2510.08968
- **Source URL:** https://arxiv.org/abs/2510.08968
- **Authors:** Suraj Kumar Sahoo; Narayanan C Krishnan
- **Reference count:** 40
- **Primary result:** Learned optimizers can internalize regularization effects (SAM, GSAM, GAM) during meta-training and transfer these properties to new architectures and datasets without explicit regularization.

## Executive Summary
This paper demonstrates that learned optimizers (LOs) can be trained to internalize traditional regularization techniques, eliminating the need for explicit regularization during optimization. The authors train LSTM-based LOs on MNIST classification tasks with regularization objectives like SAM, GSAM, and GAM. During meta-testing on FMNIST and CIFAR-10 with different architectures (MLP and CNN), the regularized LOs consistently achieve higher test accuracies than unregularized counterparts while seeking flatter minima. The key insight is that the LO learns to modify its update rule to prefer stable, flat regions of the loss landscape through meta-learning, rather than relying on explicit penalty terms.

## Method Summary
The authors develop a coordinate-wise LSTM-based learned optimizer trained via meta-learning with a bi-level optimization framework. During meta-training on MNIST, the LO is updated using a meta-loss that combines task performance with regularization terms (SAM/GSAM/GAM for flatness, and perturbation-based smoothing for robustness). The optimizer processes gradients and parameters independently per coordinate while maintaining hidden state memory. After meta-training, the LO is evaluated on held-out datasets (FMNIST, CIFAR-10) and architectures (MLP, CNN) without applying explicit regularization. The effectiveness is measured through test accuracy and PGA-based sharpness evaluation of the final optima.

## Key Results
- Regularized LOs achieve higher test accuracies than unregularized LOs across all meta-test scenarios (e.g., 0.9172±0.0269 vs 0.9125±0.0212 for MNIST with MLP)
- LOs trained with SAM regularization demonstrate ability to seek flatter minima, verified through PGA-based neighborhood analysis
- Regularized LOs generalize regularization properties across different architectures (MLP to CNN) and datasets (MNIST to CIFAR-10)
- Coordinate-wise LSTM architecture enables transfer to variable model sizes while maintaining regularization effects

## Why This Works (Mechanism)

### Mechanism 1: Internalization of Geometry-Aware Objectives
If an optimizer's meta-objective penalizes sharp loss landscapes during training, the resulting Learned Optimizer (LO) parameterizes an update rule that implicitly seeks flat minima without requiring explicit regularization penalties at test time. The LO parameters $\phi$ are updated via gradient descent on a meta-loss function $L_{meta, reg}$ incorporating sharpness terms (SAM or GAM). Through Truncated Backpropagation Through Time, the LO learns to associate certain gradient/parameter trajectories with "stable" regions, modifying its update rule to favor these trajectories even when the sharpness term is removed during meta-testing.

### Mechanism 2: Implicit Trajectory Smoothing via Perturbation Stability
A smoothing regularization term $L_{smooth}$ minimizes the discrepancy between parameter updates for original states $s_t$ and perturbed states $s'_t$. This forces the LO to ignore high-frequency noise in the gradient signal, mimicking the denoising effect of methods like SAM but via the optimizer's internal weights. Gradient noise correlates with sharpness or instability; ignoring it leads to flatter minima.

### Mechanism 3: Coordinate-wise Recurrence for Global Geometry
A coordinate-wise LSTM can aggregate local parameter dynamics over time to infer global properties of the loss landscape, such as curvature. The LO processes parameters $\theta_i$ independently but shares weights across coordinates. The hidden state $h_t$ retains a memory of past gradients. By observing how individual coordinates evolve, the LSTM effectively approximates higher-order information without explicit computation.

## Foundational Learning

- **Concept: Meta-Learning (Bi-Level Optimization)**
  - **Why needed here:** The entire methodology relies on distinguishing the "inner loop" (optimizee training on a task) from the "outer loop" (training the optimizer itself).
  - **Quick check question:** In Eq. 3, what does the weight $\phi$ represent versus the parameter $\theta$?

- **Concept: Sharpness-Aware Minimization (SAM)**
  - **Why needed here:** SAM is the target property the LO tries to learn. Understanding that SAM seeks parameters where loss remains low within a neighborhood is crucial to interpreting the "Regularized LO" results.
  - **Quick check question:** Why does minimizing the "worst-case loss in a neighborhood" (Eq. 5) lead to better generalization?

- **Concept: Truncated Backpropagation Through Time (TBPTT)**
  - **Why needed here:** This is the computational mechanism allowing gradients to flow from the final optimizee performance back through the optimizer's history to update its weights.
  - **Quick check question:** Why is "truncation" necessary when differentiating through the optimization trajectory (Eq. 4)?

## Architecture Onboarding

- **Component map:** Optimizee parameters $\theta$ -> LSTM-based LO -> Parameter updates $\Delta\theta$ -> New parameters $\theta'$ -> Hidden state $h_t$

- **Critical path:**
  1. Initialize optimizee and LO
  2. **Inner Loop:** Run LO for $N$ steps (unroll) on a task (e.g., MNIST)
  3. **Regularization Check:** At specific steps, compute the sharpness penalty (SAM/GSAM) of the optimizee's current state
  4. **Meta-Loss:** Combine task performance (cumulative loss) with the sharpness penalty and smoothing penalty
  5. **Outer Update:** Backpropagate through the $N$-step trajectory to update the LO's LSTM weights $\phi$

- **Design tradeoffs:**
  - **Unrolling Length ($T$):** Longer $T$ captures more convergence dynamics but explodes memory usage and gradient variance
  - **Coordinate-wise vs. Global:** Coordinate-wise scaling allows the LO to handle variable model sizes but theoretically limits its ability to model cross-parameter interactions
  - **Explicit vs. Implicit Regularization:** Trades off high runtime cost of explicit SAM for higher memory cost during LO meta-training

- **Failure signatures:**
  - **Gradient Instability:** LO outputs diverging updates (NaNs). Check preprocessing (log-scaling) and smoothing loss
  - **Over-regularization:** Optimizee loss stalls or fails to decrease. The LO learned to prioritize flatness over low training loss
  - **Catastrophic Forgetting:** LO performs well on meta-training tasks but fails on meta-testing. The LO learned dataset-specific curvature priors

- **First 3 experiments:**
  1. **Sanity Check (Regression):** Replicate the "Early Evidence" experiment. Train an LO on polynomial regression with L2 regularization. Verify that the meta-tested LO produces parameters with a lower L2 norm than a vanilla LO
  2. **Ablation on Curvature:** Train two LOs on MNIST: one with SAM and one without. Meta-test both on FMNIST. Plot the "Maximum Loss in Neighborhood" (via PGA) for both to verify the SAM-LO finds flatter minima
  3. **Architecture Transfer:** Take the best MNIST-trained LO and apply it to a CNN on CIFAR-10. Monitor if the accuracy drop is significant compared to standard Adam, assessing the "coordinate-wise" generalization capability

## Open Questions the Paper Calls Out

### Open Question 1
Can learned regularizers maintain their effectiveness when meta-tested on architectures and datasets that differ significantly from the meta-training distribution, specifically in high-capacity models? The authors note that for tasks substantially differing from the original training objective, such as "CIFAR-10 classification using a CNN, the validity of our hypothesis remains inconclusive." The study was restricted to small datasets (MNIST, FMNIST) and small models (MLPs, shallow CNNs) due to computational constraints; the transferability to complex "dissimilar surfaces" remains unproven.

### Open Question 2
Can a single Learned Optimizer internalize multiple regularization properties simultaneously, such as combining the benefits of SAM and GAM? The conclusion suggests that "incorporating multiple types of loss surfaces could significantly enhance the optimizer's effectiveness in navigating complex landscapes." The current experiments train and evaluate separate LOs for SAM, GSAM, and GAM independently, without exploring the interaction or fusion of these distinct regularization objectives.

### Open Question 3
How can the neighborhood radius for evaluating regularization properties be optimally selected or adapted, given its high sensitivity to the specific loss landscape? The authors state that the effectiveness of regularization evaluation is "highly dependent on the choice of neighborhood radius," noting that fixed radii risk capturing multiple minima or missing meaningful variations. The paper relies on a heuristic suggestion of 0.01 for the radius, acknowledging that this parameter requires balancing distinct behaviors that are currently manually tuned.

## Limitations
- Exact meta-learning hyperparameters (λ_smooth, λ_reg, meta-learning rate, LSTM architecture details) are not specified, limiting reproducibility
- The coordinate-wise LSTM assumption may be insufficient for capturing complex inter-parameter curvature dependencies
- Computational constraints restricted experiments to small datasets and models, leaving scalability to high-capacity models unverified
- Curriculum learning schedule and batch sizes for meta-training remain unspecified

## Confidence

- **High Confidence**: Claim that learned optimizers achieve better generalization than unregularized counterparts during meta-testing
- **Medium Confidence**: Claim that learned optimizers can internalize specific regularization techniques (SAM, GSAM, GAM)
- **Medium Confidence**: Claim of cross-architecture generalization demonstrated but mechanism has theoretical limitations

## Next Checks

1. **Parameter Norm Validation**: Replicate the polynomial regression experiment to verify that regularized LO produces lower-norm parameters compared to vanilla LO, confirming basic regularization internalization

2. **Curvature Measurement Validation**: Perform PGA-based sharpness measurements on meta-tested models to empirically verify that SAM-trained LOs find flatter minima than vanilla LOs on held-out datasets

3. **Architecture Transfer Validation**: Apply the best MNIST-trained LO to CIFAR-10 CNN to measure accuracy degradation and PGA-based sharpness, testing the coordinate-wise generalization claim across substantially different architectures