---
ver: rpa2
title: 'Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion
  Large Language Models'
arxiv_id: '2510.05090'
source_url: https://arxiv.org/abs/2510.05090
tags:
- diffusion
- language
- decoding
- refinement
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge in diffusion large language
  models (dLLMs) where once a token is accepted during decoding, it cannot be revised,
  leading to persistent early mistakes and degraded output quality. The authors propose
  TOLERATOR, a training-free decoding strategy that explicitly separates decoding
  into a fill-up stage and a refinement stage using token-level cross-validation.
---

# Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models

## Quick Facts
- arXiv ID: 2510.05090
- Source URL: https://arxiv.org/abs/2510.05090
- Authors: Runchu Tian; Junxia Cui; Xueqiang Xu; Feng Yao; Jingbo Shang
- Reference count: 35
- Key outcome: Training-free decoding strategy improves dLLM output quality by 15-18% through token-level cross-validation during refinement stage

## Executive Summary
This paper addresses the fundamental limitation in discrete diffusion large language models (dLLMs) where once a token is accepted during decoding, it cannot be revised, leading to persistent early mistakes. The authors propose TOLERATOR, a two-stage training-free decoding strategy that first generates a complete draft using standard dLLM decoding with early termination prevention, then iteratively refines it through token-level cross-validation. In the refinement stage, subsets of tokens are remasked and re-predicted while using the remaining tokens as context, allowing previously accepted tokens to be revisited and corrected. Evaluated on five benchmarks across two dLLMs, TOLERATOR consistently outperforms vanilla decoding, ReMDM, and RCR under equal computational budgets.

## Method Summary
TOLERATOR is a training-free decoding strategy that separates decoding into fill-up and refinement stages. The fill-up stage uses standard dLLM decoding with an early termination (EoT) penalty to generate complete drafts. The refinement stage then iteratively remasks random subsets of tokens and re-predicts them conditioned on the remaining tokens, enabling cross-validation where tokens alternately act as context and targets. This approach aligns inference with dLLM training objectives where all masked positions contribute to loss. The method uses cosine-annealed remasking rates and allocates half the computational budget to each stage, achieving consistent improvements across accuracy, code generation, and mathematical reasoning benchmarks.

## Key Results
- Under 16 forward steps, average performance improves from 29.0% to 34.6% (+17.9% relatively) on Dream-v0-Instruct-7B
- Under 16 forward steps, average performance improves from 21.3% to 24.5% (+15.3% relatively) on LLaDA-8B-Instruct
- TOLERATOR shows largest gains in parallel decoding scenarios (T < L) where tokens generated in the same step can't attend to each other
- Particularly effective for accuracy-focused tasks (TriviaQA, GPQA, GSM8K) with more modest gains on code generation (HumanEval, MBPP)

## Why This Works (Mechanism)

### Mechanism 1: Training-Inference Distribution Alignment
The refinement stage matches dLLM training objectives by reconstructing randomly masked tokens given visible context with uniform loss over all masked positions. Vanilla decoding deviates by only evaluating high-confidence positions while ignoring other masked tokens that contributed to training loss. This alignment allows the model to leverage its learned conditional distribution for iterative self-correction.

### Mechanism 2: Parallel Decoding Coherence Repair
In parallel decoding, tokens generated in the same forward pass cannot attend to each other, causing local inconsistencies. Cross-validation resolves this by allowing same-step tokens to interact directly: when token A is remasked, token B (originally invisible) serves as context. Iterating progressively reduces inconsistencies and improves coherence.

### Mechanism 3: Draft Preservation Through EoT Penalty
Penalizing early termination (scaling down EoT token logit by λ_eot > 1) produces longer drafts that preserve more information. Even if drafts contain errors, they maintain useful signal that refinement can correct while amplifying correct content. This creates richer material for the refinement stage to work with.

## Foundational Learning

- **Concept:** Discrete diffusion language models (dLLMs)
  - **Why needed here:** TOLERATOR is specifically designed for discrete token-space diffusion, not continuous embeddings. Understanding the masking/unmasking process is essential.
  - **Quick check question:** How does dLLM decoding differ from autoregressive decoding in terms of token dependency and parallelism?

- **Concept:** Bidirectional attention and visibility constraints
  - **Why needed here:** The core limitation TOLERATOR addresses stems from how tokens can attend to each other during parallel generation versus refinement.
  - **Quick check question:** Why can't tokens generated in the same forward step attend to each other, and how does cross-validation circumvent this?

- **Concept:** Training-inference distribution mismatch
  - **Why needed here:** TOLERATOR's key insight is that vanilla decoding creates a mismatch with training; understanding this clarifies why the method is training-free.
  - **Quick check question:** During dLLM training, which positions contribute to the loss, and how does this differ from vanilla inference evaluation?

## Architecture Onboarding

- **Component map:** Masked sequence → Fill-up stage (EoT penalty) → Complete draft → Refinement stage (K iterations with random subset remasking) → Final sequence

- **Critical path:**
  1. Initialize: masked sequence of length L
  2. Fill-up: T × ρ forward steps with EoT penalty
  3. Refinement: T × (1-ρ) forward steps with annealed remasking
  4. Return final sequence after K refinement iterations

- **Design tradeoffs:**
  - Higher γ_k → broader correction but less stable context
  - Higher λ_eot → longer drafts but potential verbosity
  - More refinement steps → better quality but no natural convergence (oscillates)

- **Failure signatures:**
  - **Code tasks (HumanEval, MBPP):** Smaller gains due to format sensitivity — refinement may disrupt syntax
  - **Non-parallel decoding:** Limited benefit when T = L (fully sequential)
  - **Convergence:** No fixed point reached; token edits decrease but never reach zero

- **First 3 experiments:**
  1. **Baseline comparison:** Run vanilla decoding vs. TOLERATOR on GSM8K with fixed budget T=16, measuring accuracy and checking if early arithmetic errors get corrected during refinement.
  2. **Parallelism sweep:** Vary T from 4 to 256 on GPQA to confirm the U-shaped benefit curve (largest gains at moderate parallelism) and identify where TOLERATOR's advantage diminishes.
  3. **Ablation order:** Test (a) no EoT penalty (λ_eot=1.0), (b) fixed refinement rate (no annealing), (c) single-stage refinement (no fill-up separation) to isolate each component's contribution on TriviaQA.

## Open Questions the Paper Calls Out

**Open Question 1:** How can the iterative refinement process be modified to achieve natural convergence to a fixed point rather than oscillating indefinitely? The paper notes the process "continues to oscillate" and "never converges to zero" in terms of token edits, even after many steps.

**Open Question 2:** Can the cross-validation strategy be adapted to maintain syntactic integrity for format-sensitive tasks like code generation? The method shows smaller gains on code benchmarks because it "can occasionally disrupt the formatting of well-formed code."

**Open Question 3:** Does an adaptive allocation of compute between the fill-up and refinement stages outperform the fixed 0.5 ratio used in experiments? The allocation ratio was set to 0.5 "solely to ensure a fair comparison," implying the optimal split is unknown.

## Limitations
- Sampling process ambiguity: Random subset selection details (uniform vs weighted, reproducibility) remain unspecified
- Acceptance criterion vagueness: Token acceptance during fill-up based on "confidence or entropy" without specific thresholds or formulas
- Convergence behavior: No natural stopping criterion; refinement continues oscillating indefinitely without reaching fixed point

## Confidence

**High Confidence:**
- TOLERATOR consistently outperforms baselines across all five benchmarks under equal computational budgets
- The method shows particular effectiveness in parallel decoding scenarios (T < L)
- Performance gains are robust across different dLLM architectures (Dream and LLaDA)

**Medium Confidence:**
- The training-inference distribution alignment mechanism fully explains the effectiveness
- The draft preservation hypothesis (longer drafts enable better refinement) is the primary driver of improvements
- The specific parameter choices (γ_min=0.4, γ_max=0.8, ρ=0.5) are optimal

**Low Confidence:**
- The method would generalize equally well to autoregressive LLMs with appropriate modifications
- The cosine annealing schedule is superior to other potential refinement rate schedules
- The performance gap would remain significant with larger dLLM models (beyond 8B parameters)

## Next Checks

**Check 1: Acceptance Criterion Sensitivity Analysis**
Replicate the main GSM8K experiment while varying the token acceptance threshold (confidence > 0.8, 0.9, 0.95) and entropy-based criteria. Measure how fill-up quality affects refinement effectiveness and identify the sensitivity of overall performance to this critical parameter.

**Check 2: Refinement Dynamics Visualization**
Run TOLERATOR on TriviaQA with detailed logging of: (a) edit frequency per refinement iteration, (b) accuracy trajectory across refinement steps, and (c) token-level correction patterns. This would reveal whether improvements come from consistent corrections or selective lucky fixes, and whether oscillations indicate diminishing returns.

**Check 3: Cross-Architecture Generalization Test**
Evaluate TOLERATOR on a third dLLM architecture (e.g., DeepSeekMath or InternLM) with the same parameter settings. This would validate whether the claimed training-inference alignment mechanism is architecture-agnostic or whether the method's effectiveness depends on specific model training details.