---
ver: rpa2
title: Cost-Sensitive Unbiased Risk Estimation for Multi-Class Positive-Unlabeled
  Learning
arxiv_id: '2510.25226'
source_url: https://arxiv.org/abs/2510.25226
tags:
- learning
- unlabeled
- class
- risk
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of learning from multi-class
  positive and unlabeled (MPU) data, where only a subset of classes is labeled and
  the unlabeled data contains a mixture of all classes. The proposed method, CSMPU,
  addresses two key limitations of existing MPU approaches: the lack of unbiased risk
  estimation and instability due to class imbalance and unlabeled data.'
---

# Cost-Sensitive Unbiased Risk Estimation for Multi-Class Positive-Unlabeled Learning

## Quick Facts
- arXiv ID: 2510.25226
- Source URL: https://arxiv.org/abs/2510.25226
- Reference count: 40
- Key outcome: Introduces CSMPU, a cost-sensitive unbiased risk estimator for MPU learning that achieves consistent improvements in accuracy and stability over strong baselines across eight public datasets.

## Executive Summary
This paper addresses the challenge of learning from multi-class positive and unlabeled (MPU) data, where only a subset of classes is labeled and the unlabeled data contains a mixture of all classes. The proposed method, CSMPU, introduces a cost-sensitive unbiased risk estimator based on adaptive loss weighting, assigning distinct weights to positive and inferred-negative loss components to ensure an unbiased estimate of the target risk. The method also incorporates a non-negativity correction to stabilize training and provides a theoretical generalization error bound. Extensive experiments demonstrate consistent improvements in both accuracy and stability over strong baselines, making CSMPU a practical and effective solution for multi-class PU learning.

## Method Summary
CSMPU tackles the limitations of existing MPU approaches by introducing a cost-sensitive unbiased risk estimator. The method assigns distinct weights to positive and inferred-negative loss components, ensuring an unbiased estimate of the target risk. A non-negativity correction is incorporated to stabilize training, and theoretical analysis provides a generalization error bound for the proposed estimator. The framework is designed to handle class imbalance and unlabeled data, achieving robust one-vs-rest detection for each observed class.

## Key Results
- CSMPU achieves consistent improvements in both accuracy and stability over strong baselines across eight public datasets.
- The method demonstrates robust one-vs-rest detection for each observed class, even with varying class priors and numbers of classes.
- Theoretical generalization error bounds are provided, validating the effectiveness of the cost-sensitive unbiased risk estimator.

## Why This Works (Mechanism)
The mechanism behind CSMPU's effectiveness lies in its ability to provide an unbiased estimate of the target risk through cost-sensitive weighting. By assigning distinct weights to positive and inferred-negative loss components, the method ensures that the risk estimator remains unbiased, even in the presence of class imbalance and unlabeled data. The non-negativity correction further stabilizes training, preventing the loss from becoming negative and ensuring convergence.

## Foundational Learning
1. **Multi-class Positive-Unlabeled (MPU) Learning**
   - *Why needed:* MPU learning addresses scenarios where only a subset of classes is labeled, and the unlabeled data contains a mixture of all classes.
   - *Quick check:* Can you explain the difference between binary PU learning and MPU learning?

2. **Unbiased Risk Estimation**
   - *Why needed:* Ensures that the estimated risk is an unbiased estimate of the target risk, even when some classes are unobserved.
   - *Quick check:* How does the constant-sum identity of the loss function contribute to unbiased risk estimation?

3. **Cost-Sensitive Learning**
   - *Why needed:* Allows for the assignment of distinct weights to different loss components, improving the robustness of the risk estimator.
   - *Quick check:* What are the benefits of using cost-sensitive learning in the context of MPU data?

## Architecture Onboarding

### Component Map
CSMPU -> Cost-Sensitive Unbiased Risk Estimator -> Non-Negativity Correction -> Generalization Error Bound

### Critical Path
1. Input MPU data (labeled positive and unlabeled mixture)
2. Apply cost-sensitive unbiased risk estimator
3. Incorporate non-negativity correction
4. Compute generalization error bound
5. Output trained model with robust one-vs-rest detection

### Design Tradeoffs
- **Bias vs. Variance:** The cost-sensitive weighting reduces bias but may increase variance in the risk estimator.
- **Complexity vs. Stability:** The non-negativity correction adds complexity but improves training stability.
- **Theoretical Rigor vs. Practical Applicability:** The method provides theoretical guarantees but may require careful tuning for extreme class imbalance.

### Failure Signatures
- Poor performance on datasets with extreme class imbalance
- Instability in training when class priors are highly skewed
- Overestimation of generalization error bounds in practice

### First 3 Experiments to Run
1. Test CSMPU on a dataset with extreme class imbalance (e.g., one class with <1% prevalence) to evaluate stability and performance.
2. Validate the generalization bound empirically by comparing predicted vs. actual error rates across multiple runs and datasets.
3. Conduct ablation studies to quantify the impact of the cost-sensitive weighting and non-negativity correction separately on overall performance.

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** Can the framework be extended to disentangle the individual unobserved classes within the unlabeled mixture, rather than treating them as a single negative meta-class?
- **Basis in paper:** [explicit] The authors state that "individual unobserved classes cannot be uniquely identified" and explicitly define the goal as separating observed classes from the remainder without partitioning it.
- **Why unresolved:** The current mathematical formulation aggregates all unobserved data into a single index $k$ (the negative meta-class) within the one-vs-rest structure.
- **What evidence would resolve it:** A theoretical extension incorporating regularization or constraints to separate the unlabeled mixture into distinct clusters.

### Open Question 2
- **Question:** Can the method be adapted for standard losses like logistic log-loss without requiring the strict constant-sum identity or ad-hoc symmetrization?
- **Basis in paper:** [explicit] Section III.B states the framework requires the base loss to satisfy the constant-sum identity and notes that "logistic log-loss... violates the property."
- **Why unresolved:** The derivation of the unbiased risk estimator relies on the algebraic cancellation of unobservable terms, which depends on this symmetry.
- **What evidence would resolve it:** A modified risk decomposition that corrects for the asymmetry of standard losses while maintaining consistency.

### Open Question 3
- **Question:** How does the generalization performance and convergence stability change when class priors are estimated dynamically (end-to-end) rather than pre-calculated?
- **Basis in paper:** [inferred] While a pipeline for prior estimation is proposed (Section III.C), the theoretical bounds (Theorem 3) and primary experiments assume fixed or ground-truth priors.
- **Why unresolved:** Theoretical guarantees rely on fixed weights ($\pi_i$); simultaneous optimization introduces time-varying bias that could destabilize training.
- **What evidence would resolve it:** Convergence analysis incorporating the variance of the prior estimator into the generalization error bound.

## Limitations
- The method assumes that inferred negatives from PU data are reliable, which may break down in noisy or ambiguous datasets.
- Limited discussion on computational complexity and scalability when the number of classes grows large.
- Theoretical generalization bounds lack empirical validation in terms of how tight or predictive they are in real-world datasets.

## Confidence
- **High confidence** in the experimental results showing consistent improvements over baselines on the tested datasets.
- **Medium confidence** in the theoretical claims regarding unbiased risk estimation and generalization bounds, as they are derived but not extensively validated empirically.
- **Low confidence** in the robustness of the method to extreme class imbalance or highly skewed class priors without further tuning or modification.

## Next Checks
1. Test CSMPU on datasets with extreme class imbalance (e.g., one class with <1% prevalence) to evaluate stability and performance.
2. Validate the generalization bound empirically by comparing predicted vs. actual error rates across multiple runs and datasets.
3. Conduct ablation studies to quantify the impact of the cost-sensitive weighting and non-negativity correction separately on overall performance.