---
ver: rpa2
title: 'MRO: Enhancing Reasoning in Diffusion Language Models via Multi-Reward Optimization'
arxiv_id: '2510.21473'
source_url: https://arxiv.org/abs/2510.21473
tags:
- reward
- reasoning
- denoising
- learning
- correlation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes Multi-Reward Optimization (MRO) to address\
  \ the limited reasoning performance of diffusion language models (DLMs), which stems\
  \ from their failure to capture token correlations during denoising. MRO enhances\
  \ intra-sequence and inter-sequence token correlations by designing and optimizing\
  \ multiple rewards\u2014token verification, perplexity, and task quality\u2014using\
  \ test-time scaling, rejection sampling, and reinforcement learning."
---

# MRO: Enhancing Reasoning in Diffusion Language Models via Multi-Reward Optimization

## Quick Facts
- **arXiv ID:** 2510.21473
- **Source URL:** https://arxiv.org/abs/2510.21473
- **Reference count:** 40
- **Primary result:** Multi-Reward Optimization (MRO) improves DLM reasoning accuracy by up to 3 points on average by optimizing token correlations across denoising steps.

## Executive Summary
This paper addresses the reasoning limitations of diffusion language models (DLMs), which struggle to capture token correlations during parallel masked generation. The proposed Multi-Reward Optimization (MRO) framework enhances both intra-sequence (within-step) and inter-sequence (between-step) token dependencies through three reward signals: token verification, perplexity, and task quality. The approach is theoretically grounded with Step-wise Group Reward Optimization (SGRO) to reduce reward variance, and empirically validated across multiple reasoning benchmarks, demonstrating performance approaching strong autoregressive models while requiring fewer denoising steps.

## Method Summary
MRO optimizes DLMs by maximizing three rewards: Token Verification Reward (R_tv) for intra-sequence correlation, Perplexity Reward (R_ppl) for fluency, and Quality Reward (R_q) for final answer accuracy. The method can be applied during test-time scaling with beam search, rejection sampling fine-tuning, or REINFORCE training. SGRO groups denoising steps to reduce reward variance during optimization. The framework is implemented on LLaDA-8B, trained on DeepScaleR plus Countdown and Sudoku datasets, and evaluated on GSM8K, MATH500, GPQA, Countdown, and Sudoku benchmarks.

## Key Results
- MRO improves accuracy by up to 3 points on average across reasoning benchmarks
- Test-time scaling with MRO enables DLMs to approach Qwen2.5-7B autoregressive performance
- SGRO reduces reward variance compared to step-wise potential-based shaping
- MRO achieves high-quality reasoning paths with fewer denoising steps

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimizing for *token correlation*—specifically intra-sequence (within-step) and inter-sequence (between-step) dependencies—improves reasoning coherence in Diffusion Language Models (DLMs).
- **Mechanism:** Standard DLMs generate masked tokens independently, causing reasoning inconsistencies. MRO introduces two reward signals: a **Token Verification Reward** ($R_{tv}$) that re-scores tokens based on their joint probability (proxying mutual information) and a **Perplexity Reward** ($R_{ppl}$) to ensure fluency. By maximizing these, the model is forced to generate tokens that are contextually dependent on one another.
- **Core assumption:** The bottleneck in DLM reasoning is the independence assumption of the denoising process, not just a lack of model capacity or data.
- **Evidence anchors:**
  - [abstract] Defines the two correlation types and states independent generation fails to capture them.
  - [section 4.2.1] Details $R_{tv}$ calculation using leave-one-out probability and $R_{ppl}$ using an AR model.
  - [corpus] Related work like *d2* (2512.25014) also targets reasoning in DLMs via RL, suggesting optimizing generation policies is a key trend, though MRO specifically targets correlation structure.
- **Break condition:** If reasoning errors persist despite high correlation scores, the issue may stem from foundational knowledge gaps rather than the generation structure.

### Mechanism 2
- **Claim:** **Step-wise Group Reward Optimization (SGRO)** reduces variance in the reward signal compared to step-wise potential-based shaping.
- **Mechanism:** Potential-based reward shaping (adding $R_{tv}/R_{ppl}$ at every step) theoretically preserves optimal policy but introduces high variance in practice. SGRO groups $w$ denoising steps together, applying the shaping reward once per group. This reduces the frequency of noisy potential function computations (Property 2 proof).
- **Core assumption:** The correlation dynamics across a small group of steps remain stable enough that a group-level reward provides a sufficient learning signal.
- **Evidence anchors:**
  - [section 4.3] Introduces SGRO to mitigate variance and explicitly states $Var(R(s,a)) > Var(R^{(w)}(s,a))$.
  - [figure 7] Shows performance stability relative to group size, supporting the variance reduction hypothesis.
- **Break condition:** If the group size $w$ is set too large (e.g., >32 as implied by instability in Appendix D.2), the gradient signal may become too sparse to guide the model effectively.

### Mechanism 3
- **Claim:** Modeling the denoising process as a **Markov Decision Process (MDP)** allows for test-time scaling and targeted optimization.
- **Mechanism:** By framing state $s_t$ as the current partially denoised sequence and action $a_t$ as the prediction of masked tokens, MRO applies RL techniques. This allows the use of **Rejection Sampling** (RS) and **Test-Time Scaling** (TTS) to filter for high-reward trajectories, effectively searching the "reasoning space" of the DLM.
- **Core assumption:** The distribution of high-quality reasoning paths is reachable via the model's initial sampling distribution and can be identified by the defined rewards.
- **Evidence anchors:**
  - [section 4.2.3] Defines the MDP formulation ($s_t \triangleq (p_0, r_t, t)$).
  - [table 1 & 4] Shows TTS and RS significantly boost performance (e.g., GSM8K jumps from 79.4 to 82.6).
  - [corpus] *Improving Diffusion Language Model Decoding...* (2601.20339) supports the efficacy of search strategies in DLM trajectory space.
- **Break condition:** If the base model's probability mass contains virtually no correct reasoning paths for a specific problem type, rejection sampling will fail (reward hacking or empty sampling buffer).

## Foundational Learning

- **Concept: Masked Diffusion Models (e.g., LLaDA)**
  - **Why needed here:** Unlike Autoregressive (AR) models that predict strictly left-to-right, DLMs predict multiple masked tokens in parallel. MRO is designed specifically to fix the "independence" flaw in this parallel generation.
  - **Quick check question:** How does the loss function in a masked DLM differ from standard next-token prediction, and why does this encourage parallel generation?

- **Concept: Potential-Based Reward Shaping**
  - **Why needed here:** MRO uses this technique to distribute the sparse final reward (correct answer) across the dense intermediate steps (coherence) without altering the optimal policy.
  - **Quick check question:** If you add a shaping reward $F(s, a, s') = \gamma \Phi(s') - \Phi(s)$, why does the optimal policy $\pi^*$ remain unchanged?

- **Concept: REINFORCE / Policy Gradient**
  - **Why needed here:** The paper uses REINFORCE to fine-tune the DLM parameters $\theta$ based on the multi-reward signal.
  - **Quick check question:** In the REINFORCE update $ \nabla_\theta J(\theta) \approx \sum \nabla_\theta \log \pi_\theta(a_t|s_t) R_t$, what does the reward $R_t$ represent in the context of MRO's SGRO?

## Architecture Onboarding

- **Component map:**
  1. **Backbone:** Pre-trained DLM (e.g., LLaDA-8B) acting as the policy $\pi_\theta$.
  2. **Reward Evaluators:**
     - *Token Verifier:* A copy of the DLM used to score joint probabilities of generated tokens ($R_{tv}$).
     - *Perplexity Model:* An external AR model (e.g., GPT-2/Llama) scoring fluency ($R_{ppl}$).
     - *Outcome Verifier:* A symbolic checker or heuristic for the final answer ($R_q$).
  3. **Optimizer:** SGRO module (groups rewards) + REINFORCE/Rejection Sampling loop.

- **Critical path:** The calculation of the **Token Verification Reward ($R_{tv}$)**. This requires re-forwarding the current batch through the model to compute leave-one-out probabilities. This is computationally expensive (Section 4.2.1 mentions optimization via GPU parallelism).

- **Design tradeoffs:**
  - **Group Size ($w$):** High $w$ reduces variance (better stability) but lowers update frequency. The paper finds $w=32$ or smaller is effective (Figure 7).
  - **Beam Size ($k$):** Higher $k$ improves search quality during Test-Time Scaling but linearly increases inference cost.

- **Failure signatures:**
  - **Reward Hacking:** The model generates sequences with low perplexity but incorrect logic to maximize $R_{ppl}$ at the expense of $R_q$.
  - **Mode Collapse:** If $R_{tv}$ is too dominant, the model might generate repetitive, highly correlated but meaningless tokens.
  - **Variance Explosion:** Without SGRO (or with $w=1$), training loss may oscillate violently despite high reward averages.

- **First 3 experiments:**
  1. **Correlation Baseline:** Replicate Figure 1. Plot correlation metrics vs. accuracy on a small validation set (e.g., 50 samples) to verify the core assumption before training.
  2. **SGRO Ablation:** Run MRO with group sizes $w \in \{1, 8, 32\}$ on a toy dataset (e.g., Countdown). Monitor training loss variance to confirm Property 2.
  3. **Inference Scaling Test:** Implement beam search with MRO rewards (no training) on GSM8K. Compare "Vanilla" vs. "MRO-Guided" decoding to isolate the impact of the reward signal on pre-trained models.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the optimal group size $w$ for Step-wise Group Reward Optimization (SGRO) be determined adaptively or automatically rather than through manual ablation?
- **Basis:** [explicit] Section A.1 (Limitations) states that the selection of hyperparameters requires careful consideration and future work should "design techniques to determine these hyperparameters automatically."
- **Why unresolved:** The current work relies on fixed group sizes ($w=32$ or $w=2$) established via grid search (Figure 7), which may not be optimal across varying reasoning tasks or model scales.
- **What evidence would resolve it:** An adaptive scheduling algorithm for $w$ that achieves superior or equivalent performance without requiring manual tuning for each dataset.

### Open Question 2
- **Question:** Would implementing advanced reinforcement learning algorithms (e.g., PPO, GRPO) yield significant performance gains over the basic REINFORCE method used in this study?
- **Basis:** [inferred] Section C.2 notes that the RL approach is "relatively basic" and "does not show a substantial advantage" over rejection sampling, suggesting the specific optimization algorithm remains a variable.
- **Why unresolved:** The authors demonstrated the validity of the *reward* design but did not extensively explore the *optimization* algorithm, leaving the potential of more sophisticated RL methods untested.
- **What evidence would resolve it:** Experiments applying Proximal Policy Optimization (PPO) or Group Relative Policy Optimization (GRPO) to the MRO framework, showing improved sample efficiency or final accuracy compared to the REINFORCE baseline.

### Open Question 3
- **Question:** Is the Multi-Reward Optimization framework transferable to continuous diffusion language models, or is it restricted to discrete masked diffusion architectures?
- **Basis:** [inferred] The paper focuses exclusively on discrete masked diffusion (LLaDA), yet Section 2.1 acknowledges the existence of continuous diffusion models (e.g., Diffusion-LM).
- **Why unresolved:** The proposed Token Verification Reward relies on re-masking and predicting discrete tokens, an operation that lacks a direct equivalent in continuous latent space.
- **What evidence would resolve it:** Adapting the intra-sequence reward mechanism for continuous distributions and demonstrating reasoning improvements on a continuous diffusion model benchmark.

## Limitations

- The method requires multiple forward passes through the model to compute the Token Verification Reward, making it computationally expensive
- The framework relies on external autoregressive models for perplexity scoring, adding complexity and potential brittleness
- The study doesn't fully explore the impact of different group sizes ($w$) on final performance across all reasoning tasks

## Confidence

- **High:** Token correlation optimization improves reasoning coherence; SGRO reduces reward variance compared to step-wise potential-based shaping
- **Medium:** MRO enables DLMs to approach AR model performance on reasoning benchmarks; the specific rewards (R_tv, R_ppl, R_q) are the most effective combination
- **Low:** The approach will scale effectively to models 10x larger without architectural modifications; the method is universally applicable across all DLM architectures without tuning

## Next Checks

1. **Correlation vs. Accuracy Validation:** Replicate the correlation analysis from Figure 1 on a held-out validation set (e.g., 50 samples from GSM8K) to confirm that MRO consistently increases both intra-sequence and inter-sequence token correlations before full training.

2. **SGRO Variance Analysis:** Implement MRO with varying group sizes ($w \in \{1, 8, 32\}$) on a smaller reasoning task (e.g., Countdown). Log the variance of $R_{tv}$, $R_{ppl}$, and $R_q$ separately during training to empirically verify that SGRO reduces variance as claimed in Property 2.

3. **Pre-trained Model Decoding Test:** Implement MRO-guided beam search (with $k=4$, temperature=0.25) on a pre-trained LLaDA-8B model without any fine-tuning. Compare the accuracy on GSM8K against "Vanilla" beam search to isolate the impact of the reward signal on the base model's reasoning capability.