---
ver: rpa2
title: LLM or Human? Perceptions of Trust and Information Quality in Research Summaries
arxiv_id: '2601.15556'
source_url: https://arxiv.org/abs/2601.15556
tags:
- abstract
- abstracts
- participants
- trust
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how readers perceive LLM-generated research
  abstracts and how such perceptions affect trust and quality evaluations. Using a
  mixed-methods survey experiment with ML experts, it compares three abstract types:
  human-written, LLM-generated, and LLM-edited.'
---

# LLM or Human? Perceptions of Trust and Information Quality in Research Summaries

## Quick Facts
- arXiv ID: 2601.15556
- Source URL: https://arxiv.org/abs/2601.15556
- Reference count: 40
- Primary result: Readers struggle to reliably identify LLM-generated research abstracts, but disclosure of LLM use significantly increases trust and positive evaluations.

## Executive Summary
This study investigates how readers perceive LLM-generated research abstracts and how such perceptions affect trust and quality evaluations. Using a mixed-methods survey experiment with ML experts, it compares three abstract types: human-written, LLM-generated, and LLM-edited. Participants struggled to reliably identify LLM involvement, often defaulting to human authorship assumptions. However, disclosure of LLM use significantly increased trust and positive evaluations. LLM-edited abstracts received the highest ratings and preference, valued for combining human substance with AI-enhanced clarity. Three reader orientations emerged: Disclosure Advocates, Pragmatic Skeptics, and Optimists. The findings suggest transparency is key to trust, while ambiguity triggers detection heuristics rather than merit-based evaluation.

## Method Summary
The study sampled 150 arXiv papers from cs.LG (Oct 2019–Oct 2022) and generated three abstract versions per paper: original human-written, LLM-generated (Llama 3.1 8B with full-text prompt), and LLM-edited (original rewritten). ML experts (N=69) completed a 60-minute survey evaluating three papers each under two conditions: guessing authorship (blind) or knowing authorship disclosure. Participants rated Trust, Clarity, Comprehensiveness, Engagement, and Conciseness using Likert scales, selected preferred abstracts, and provided free-text justifications. Analysis used linear mixed-effects models with random participant effects and PCA to identify participant orientations.

## Key Results
- Participants could not reliably identify LLM involvement, performing near chance level
- Disclosure of LLM use significantly increased trust and positive quality evaluations
- LLM-edited abstracts received highest ratings and were most frequently selected when authorship was disclosed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transparency about LLM involvement increases trust, contrary to the "disclosure penalty" observed in some prior work.
- Mechanism: Disclosure preemptively satisfies the "hermeneutics of suspicion"—readers' tendency to scrutinize text for authorship signals. When authorship is known, cognitive resources shift from authenticity detection to quality assessment.
- Core assumption: Readers in expert communities (ML researchers) view disclosure as a professional norm signal rather than a quality warning.
- Evidence anchors:
  - [abstract] "Disclosure of LLM use significantly increased trust and positive evaluations"
  - [section 5.3] "participants who were told that an abstract involved LLM assistance expressed higher trust, whereas nondisclosure often triggered suspicion, even when the text was written entirely by humans"
  - [corpus] Related work shows mixed effects: Li et al. (EMNLP 2024) report disclosure decreases quality ratings but increases variability; corpus signals are weak on this specific trust-disclosure relationship.
- Break condition: Communities where AI use is stigmatized rather than normalized; non-expert populations with lower LLM familiarity.

### Mechanism 2
- Claim: Hybrid human-LLM content (LLM-edited abstracts) receives higher quality ratings than either fully human or fully LLM-generated content.
- Mechanism: LLM editing enhances surface clarity and structure while preserving human-authored substance and technical depth. Readers value the combination of "linguistic clarity" with authentic research content.
- Core assumption: Readers can perceive clarity improvements independently from detecting authorship.
- Evidence anchors:
  - [abstract] "LLM-edited abstracts received the highest ratings and preference, valued for combining human substance with AI-enhanced clarity"
  - [section 4.2.2] "over half of participants (55%) selected the LLM-edited abstract" when authorship disclosed
  - [corpus] SemCSE (arXiv 2507.13105) leverages LLM-generated summaries for scientific embeddings, suggesting hybrid approaches have utility beyond perception.
- Break condition: Domains where surface polish is penalized (e.g., creative writing valuing idiosyncratic voice); content requiring high-stakes factual precision.

### Mechanism 3
- Claim: Detection heuristics for LLM authorship are systematically unreliable and often backfire.
- Mechanism: Readers use intuitive cues (fluency, pronouns, technical density, grammatical "errors") as authenticity signals. These heuristics are bidirectional—both human and LLM text can trigger false positives.
- Core assumption: Readers implicitly expect lower quality from AI text, so high-quality text is attributed to humans.
- Evidence anchors:
  - [section 4.1] "participants were not able to attribute abstract authorship better than chance"
  - [section 4.1, Table 4] "P31 praised LLM-generated content as written by 'a well educated researcher'... P12 criticized a human-written abstract for 'unnecessarily complicated phrases (most likely generated by an LLM)'"
  - [corpus] Fiedler & Döpke (2025) confirm experts and detectors perform only slightly above chance; corpus consensus supports this mechanism.
- Break condition: Highly stereotyped AI outputs without human review; readers with specialized detection training.

## Foundational Learning

- Concept: Trust calibration vs. trustworthiness
  - Why needed here: The paper measures perceived trust, which is an attitude distinct from actual system trustworthiness or reliance behavior.
  - Quick check question: If users report high trust in an LLM summarizer but make no behavioral changes, is trust calibrated?

- Concept: Mixed experimental design (within-subjects × between-subjects)
  - Why needed here: The study uses Latin square rotation (within) plus information/guess conditions (between) to isolate disclosure effects from authorship effects.
  - Quick check question: Why does this design require linear mixed-effects models rather than standard ANOVA?

- Concept: Exploratory Factor Analysis (EFA) for construct validation
  - Why needed here: The five quality dimensions (Trust, Clarity, Comprehensiveness, Engagement, Conciseness) emerged from EFA on 18 survey items.
  - Quick check question: If Cronbach's α were 0.45 instead of >0.8, what would that indicate about the composite scores?

## Architecture Onboarding

- Component map: Paper corpus (arXiv ML papers, 2019-2022) -> Abstract generation (Llama 3.1 8B + ACM guidelines prompt) -> Survey experiment (N=69 ML experts) -> Measures: Likert ratings + behavioral selection + free-text -> Analysis: Mixed-effects models + LLM-assisted thematic coding

- Critical path: The between-subjects disclosure manipulation is the key causal lever. Without it, you cannot separate actual LLM involvement effects from perceived involvement effects.

- Design tradeoffs:
  - Single model (Llama 3.1 8B) limits generalizability but enables reproducibility on commodity hardware
  - ML expert participants appropriate for ML abstracts but limits generalization to broader populations
  - 60-minute survey duration constrains task count (3 papers per participant)

- Failure signatures:
  - Participants recognizing papers from prior exposure (2 excluded tasks)
  - Over-scrutiny penalty: first-shown abstract selected less than chance (22% vs 33%)
  - Non-native English writing misattributed to LLM generation

- First 3 experiments:
  1. Replicate with non-expert readers (students, policymakers) to test if disclosure-trust effect reverses in populations with lower AI familiarity
  2. Vary the disclosure framing (neutral vs. apologetic vs. proud) to isolate the effect of disclosure tone from disclosure itself
  3. Longitudinal design measuring trust changes after repeated exposure to disclosed LLM content over weeks

## Open Questions the Paper Calls Out

- Question: Do trust and quality perceptions of LLM-assisted abstracts generalize to non-expert populations, such as students or lay readers?
  - Basis in paper: [explicit] Section 6 (Limitations) notes that the participant pool was restricted to ML experts, which limits generalizability to audiences with less favorable attitudes toward AI.
  - Why unresolved: The study design relied exclusively on participants with high technical literacy and familiarity with LLMs.
  - Evidence: Replication studies with diverse populations, including early-career researchers or the general public.

- Question: Does repeated exposure to LLM-assisted writing lead to trust calibration, disclosure fatigue, or habituation over time?
  - Basis in paper: [explicit] Section 6 states the survey captures a snapshot and does not examine how trust evolves or whether evaluations translate into real-world reliance behaviors.
  - Why unresolved: The experiment consisted of a single session, preventing the observation of longitudinal effects.
  - Evidence: Longitudinal field studies tracking researchers' reliance on and perception of AI tools over months.

- Question: Do the effects of disclosure and LLM authorship differ in high-stakes domains (e.g., medicine) or those with narrative styles (e.g., humanities)?
  - Basis in paper: [explicit] Section 6 highlights that the study focused on ML abstracts with standardized conventions, potentially differing from domains with higher stakes or less rigid structures.
  - Why unresolved: The corpus was limited to machine learning papers from arXiv.
  - Evidence: Comparative experiments using abstracts from medical journals or humanities publications.

## Limitations
- Generalizability constraints: Findings based on ML experts evaluating ML abstracts limit applicability to non-technical domains or general audiences
- Single model dependency: All LLM outputs generated using Llama 3.1 8B may not represent other model families or architectures
- Self-report measurement: Trust and quality ratings rely on participant self-assessment rather than behavioral measures of actual reliance or comprehension

## Confidence
- Transparency increases trust (High): Multiple data sources converge on this finding—statistical significance in mixed-effects models, consistent behavioral preferences, and qualitative justifications
- Hybrid content receives highest ratings (Medium): While statistically supported, this claim depends on the specific editing protocol and may not generalize across different human-AI collaboration workflows
- Detection heuristics are unreliable (High): Both quantitative accuracy rates (near-chance) and qualitative misattribution patterns provide convergent evidence

## Next Checks
1. External validity test: Replicate the disclosure manipulation with humanities scholars evaluating philosophy abstracts to test if transparency-trust relationships hold across disciplinary cultures
2. Longitudinal trust dynamics: Track trust ratings over 4-6 weeks of repeated exposure to disclosed LLM content to determine if initial disclosure benefits diminish with familiarity
3. Behavioral validation: Replace self-report trust measures with eye-tracking or comprehension testing to verify that stated trust correlates with actual engagement and information extraction