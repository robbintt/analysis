---
ver: rpa2
title: 'Myanmar XNLI: Building a Dataset and Exploring Low-resource Approaches to
  Natural Language Inference with Myanmar'
arxiv_id: '2504.09645'
source_url: https://arxiv.org/abs/2504.09645
tags:
- myanmar
- english
- language
- translation
- xnli
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Myanmar XNLI (myXNLI) dataset to address
  the lack of low-resource language benchmarks in cross-lingual natural language inference
  (XNLI). The authors construct myXNLI by translating English XNLI data into Myanmar
  using community crowdsourcing followed by expert verification, demonstrating the
  value of expert review in improving data quality.
---

# Myanmar XNLI: Building a Dataset and Exploring Low-resource Approaches to Natural Language Inference with Myanmar

## Quick Facts
- **arXiv ID**: 2504.09645
- **Source URL**: https://arxiv.org/abs/2504.09645
- **Reference count**: 40
- **Primary result**: Expert-verified translations and cross-lingual data augmentation improve Myanmar NLI performance by up to 2 percentage points.

## Executive Summary
This paper introduces the Myanmar XNLI (myXNLI) dataset to address the lack of low-resource language benchmarks in cross-lingual natural language inference (XNLI). The authors construct myXNLI by translating English XNLI data into Myanmar using community crowdsourcing followed by expert verification, demonstrating the value of expert review in improving data quality. They evaluate state-of-the-art multilingual language models on myXNLI, establishing initial performance baselines for Myanmar. Data augmentation techniques—including multilingual training, cross-matched language pairs, and genre-based side inputs—improve model accuracy by up to 2 percentage points for Myanmar while also benefiting other low-resource languages. The study confirms that these strategies generalize across low-resource languages, highlighting the importance of high-quality data and targeted training methods in advancing NLP for underrepresented languages.

## Method Summary
The authors created myXNLI by translating MultiNLI/XNLI English data into Myanmar through machine translation for training data and human translation (crowd-sourced + expert verified) for dev/test sets. They fine-tuned mDeBERTa-v3-base for exactly 1 epoch on this data, exploring augmentation strategies including cross-matched language pairs (en-en, en-my, my-en, my-my) and genre metadata as special token prefixes. The model achieved 79.46% baseline accuracy, with improvements to 80.99% using cross-matched augmentation and 81.45% with genre metadata.

## Key Results
- Expert verification improved translation quality from 51.68% "perfect" translations to higher quality, yielding ~1.5-2 point accuracy gains
- Cross-matched language pair training (en-en, en-my, my-en, my-my) improved Myanmar accuracy from 79.46% to 80.99%
- Genre metadata as special token prefixes improved accuracy from 79.46% to 79.76% (modest but consistent across languages)
- mDeBERTa outperformed XLM-R and monolingual models across all experimental conditions

## Why This Works (Mechanism)

### Mechanism 1
Expert verification of community-translated data improves downstream model performance by correcting systematic translation errors. Expert reviewers catch mistranslations of polysemous words, inconsistent transliterations, and cultural context errors that crowd-workers miss. The paper categorizes these errors: "mistranslations of polysemous English words" (e.g., "reach" as destination vs. contact), "arbitrary transliterations of English named entities," and "inadequate cultural or background knowledge" (e.g., translating "Indians" as South Asian rather than Native American).

### Mechanism 2
Training multilingual models on cross-matched language pairs (en-en, en-my, en-my, my-my) improves low-resource language NLI performance. Cross-matched pairs create quadruple training data that aligns representations across languages at the sentence-pair level. The model learns that semantic relationships (entailment, contradiction, neutral) are language-invariant.

### Mechanism 3
Genre metadata as special token prefixes provides contextual signals that improve NLI classification. Genre tokens (e.g., "Telephone," "Fiction") help the model recognize register-specific patterns. For telephone conversations, repeated short utterances like "Yes/No" should be weighted differently than in written genres.

## Foundational Learning

- **Natural Language Inference (NLI)**
  - Why needed here: The core task—classifying premise-hypothesis pairs as entailment, contradiction, or neutral. Understanding this is essential for interpreting all results.
  - Quick check question: Given "You don't have to stay there" as premise, would "You can leave" be entailment, contradiction, or neutral?

- **Cross-Lingual Transfer**
  - Why needed here: The paper's central strategy is leveraging English (high-resource) to improve Myanmar (low-resource). Understanding zero-shot transfer explains why English-only fine-tuning works on Myanmar.
  - Quick check question: If a model is fine-tuned only on English XNLI, why can it classify Myanmar sentence pairs without seeing Myanmar training data?

- **Multilingual Transformer Architectures (XLM-R, mDeBERTa)**
  - Why needed here: These are the evaluated models. mDeBERTa uses disentangled attention (separate content/position vectors) and Replace Token Detection pre-training.
  - Quick check question: What is the key architectural difference between mDeBERTa and XLM-R that might explain mDeBERTa's superior XNLI performance?

## Architecture Onboarding

- **Component map**: MultiNLI/XNLI English source → Machine translation (Google API) for training / Human translation for dev-test → Expert revision → myXNLI dataset → mDeBERTa-base fine-tuning → Evaluation on test set

- **Critical path**: Dataset quality (expert revision) provides ~2 point gain—the same magnitude as all augmentation methods combined. Multilingual model selection: mDeBERTa outperforms XLM-R across all scenarios. Training strategy: Translate-train (fine-tune on target language) > Cross-lingual transfer > Translate-test (translate to English, use English model).

- **Design tradeoffs**: Monolingual (MyanBERTa) vs. Multilingual (mDeBERTa): Multilingual performs ~22 points better (57.40% vs. 79.46%) despite Myanmar-specific pre-training. Machine translation vs. Human translation: Training data uses MT (scalable, BLEU=51.73); dev/test uses human translation (higher quality, labor-intensive). Single-language vs. Cross-matched training: Cross-matched adds 4× data but requires storing parallel corpora.

- **Failure signatures**: Urdu anomaly: Fine-tuning on Urdu alone degrades all performance (70.01% → 66.70% on Urdu itself)—suggests model capacity dilution or poor translation quality. Genre over-generalization: Spoken-style genre training caused incorrect predictions when repeated utterances appeared in test data (Figure A3). Space character sensitivity: Removing optional Myanmar spaces from test data (while training had spaces) caused 32 net additional errors.

- **First 3 experiments**:
  1. Reproduce cross-lingual transfer baseline: Fine-tune mDeBERTa-base on English XNLI only (1 epoch), evaluate on myXNLI test set. Expected: ~75-76% accuracy.
  2. Ablate expert revision: Compare model performance on initial vs. revised myXNLI test sets to quantify data quality impact (expected ~1.5-2 point difference).
  3. Test cross-matched augmentation on a different low-resource language: Apply en-sw cross-matched training (if you have Swahili data) to verify generalization beyond Myanmar.

## Open Questions the Paper Calls Out

- Can prompt-based data augmentation techniques like PromDA effectively improve NLI performance for Myanmar? The current study did not generate synthetic training data using language models.

- How do recent multilingual architectures like mT5 and Aya perform on the myXNLI benchmark compared to mDeBERTa? The paper restricted its evaluation to XLM-R, mDeBERTa, and their monolingual counterparts.

- How does the model's NLI performance degrade when provided with incorrect genre metadata or out-of-domain text? The authors explored using genre as a helpful side-input but did not evaluate the model's robustness to adversarial or mismatched genre conditions.

## Limitations

- Data Quality Uncertainty: Expert verification improved quality but lacks inter-annotator agreement metrics, and 0.8% of sentences remained incomprehensible even after revision.
- Generalization Concerns: Success on Myanmar may not extend to all low-resource languages, particularly those with different linguistic structures.
- Evaluation Constraints: All experiments use accuracy only, without reporting F1 scores, precision-recall trade-offs, or per-class performance.

## Confidence

**High Confidence** (Multiple direct evidences):
- Expert verification improves data quality with direct metrics (51.68% perfect translations) and downstream performance gains
- Baseline mDeBERTa performance on myXNLI (79.46%) is well-documented through multiple experimental conditions
- 2 percentage point improvement from genre metadata is consistent across languages

**Medium Confidence** (Indirect or single-source evidence):
- Cross-matched pairs improve performance relies on translation quality assumptions with only partial verification (8% label mismatch)
- Multilingual models outperform monolingual models assumes mDeBERTa's pre-training was more effective without detailed architectural comparisons
- Urdu performance degradation suggests model capacity issues but lacks capacity analysis

**Low Confidence** (Weak or no supporting evidence):
- Specific impact of individual error categories on model performance is not quantified separately
- Assumption that genre metadata improves NLI lacks corpus-level evidence beyond observed accuracy gain

## Next Checks

1. **Cross-Lingual Generalization Test**: Apply cross-matched augmentation to a different low-resource language pair (e.g., Swahili-English) to verify the 2 percentage point improvement generalizes beyond Myanmar.

2. **Expert Revision Inter-Annotator Agreement**: Conduct reliability study with two independent expert reviewers to calculate Cohen's kappa and assess consistency of corrections.

3. **Ablation of Translation Quality Factors**: Create controlled experiments correcting only one type of translation error at a time to measure individual contributions to model performance.