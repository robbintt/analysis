---
ver: rpa2
title: 'QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large Language
  Model Adaptation'
arxiv_id: '2506.02295'
source_url: https://arxiv.org/abs/2506.02295
tags:
- arabic
- qari
- text
- qari-ocr
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Qari-OCR is a fine-tuned vision-language model derived from Qwen2-VL-2B-Instruct,
  designed for high-fidelity Arabic text recognition. It was trained iteratively on
  progressively enhanced synthetic datasets, with QARI v0.2 achieving state-of-the-art
  open-source performance: WER 0.160, CER 0.061, and BLEU 0.737 on diacritically-rich
  Arabic texts.'
---

# QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large Language Model Adaptation

## Quick Facts
- **arXiv ID:** 2506.02295
- **Source URL:** https://arxiv.org/abs/2506.02295
- **Reference count:** 5
- **Primary result:** QARI v0.2 achieves state-of-the-art WER 0.160, CER 0.061, and BLEU 0.737 on diacritically-rich Arabic text.

## Executive Summary
QARI-OCR is a fine-tuned vision-language model derived from Qwen2-VL-2B-Instruct, designed for high-fidelity Arabic text recognition. It was trained iteratively on progressively enhanced synthetic datasets, with QARI v0.2 achieving state-of-the-art open-source performance: WER 0.160, CER 0.061, and BLEU 0.737 on diacritically-rich Arabic texts. The model demonstrates superior handling of Arabic tashkeel, diverse fonts, complex layouts, and low-resolution images, and includes exploration of document structure understanding and handwritten text recognition in QARI v0.3. All models and datasets are publicly released to support further research.

## Method Summary
QARI-OCR fine-tunes Qwen2-VL-2B-Instruct using parameter-efficient LoRA adapters on progressively complex synthetic Arabic datasets. The training pipeline uses Unsloth with 4-bit QLoRA, optimized for an NVIDIA A6000 GPU. Three model variants were developed: v0.1 (basic text), v0.2 (diacritics, 10 fonts), and v0.3 (complex layouts). The approach leverages synthetic data generation from Arabic corpora to create training images that simulate real-world document complexities.

## Key Results
- QARI v0.2 achieves state-of-the-art WER 0.160, CER 0.061, and BLEU 0.737 on diacritically-rich Arabic text
- Progressive synthetic data curriculum enables handling of Arabic script nuances, particularly diacritics and varied typography
- Model outperforms existing open-source Arabic OCR solutions on standard benchmark datasets
- QARI v0.3 explores document structure understanding and handwritten text recognition capabilities

## Why This Works (Mechanism)

### Mechanism 1: Progressive Synthetic Data Curriculum
Iteratively training on synthetic datasets of increasing complexity (v0.1 → v0.2 → v0.3) enables the model to handle Arabic script nuances, particularly diacritics (tashkeel) and varied typography. A base MLLM is fine-tuned in stages: first on clean, simple data to establish a baseline, then on data enriched with diacritics and more fonts for textual fidelity, and finally on data with complex layouts for structural understanding.

### Mechanism 2: MLLM as a Specialized OCR Engine
Adapting a general-purpose Multimodal Large Language Model via parameter-efficient fine-tuning can yield state-of-the-art performance on specialized vision tasks like Arabic OCR. The pre-trained MLLM already possesses strong general vision-language alignment, and fine-tuning with LoRA adapters on a targeted domain specializes this general capability without full retraining.

### Mechanism 3: Precision Trade-offs with Quantization
Aggressive model quantization (e.g., 4-bit) severely degrades performance on fine-grained tasks like Arabic OCR, making 8-bit a more practical minimum for high fidelity. Arabic OCR relies on subtle model weights to distinguish small features (dots, diacritics), and reducing precision to 4-bit introduces too much noise/information loss into these weights.

## Foundational Learning

**Concept: Vision-Language Models (VLMs/MLLMs)**
- Why needed here: The core architecture is not a traditional OCR pipeline but an MLLM that processes an image input and produces a text output, guided by a language model.
- Quick check question: What is the role of the vision encoder vs. the language model in this system?

**Concept: Parameter-Efficient Fine-Tuning (PEFT / LoRA)**
- Why needed here: The model is adapted using LoRA adapters, not full fine-tuning. This is critical for understanding how the model was trained and how to potentially retrain it.
- Quick check question: Why would the authors use LoRA (rank 16) instead of full fine-tuning? What part of the model is modified?

**Concept: Synthetic Data for Training**
- Why needed here: The entire approach hinges on a synthetic data pipeline. The quality, diversity, and specific construction of this data are the primary levers for performance.
- Quick check question: What are the key dimensions of variation introduced in the synthetic datasets (v0.1, v0.2, v0.3)?

## Architecture Onboarding

**Component map:** Base Model (Qwen2-VL-2B-Instruct) -> Adapter Layers (LoRA rank 16) -> Training Data (Synthetic image-text pairs) -> Inference (Image + prompt → Arabic transcription)

**Critical path:** Synthetic Data Quality → Fine-tuning with LoRA (on an A6000 GPU) → 8-bit Quantized Inference

**Design tradeoffs:**
- v0.2 (Plain Text Accuracy) vs. v0.3 (Structure/Layout Awareness): Choose based on whether you need raw text extraction or HTML-like structural reconstruction
- 8-bit vs. 4-bit Quantization: Paper shows 4-bit is unusable for this task; 8-bit is the minimum viable for high accuracy
- Dataset Size (50k for v0.2) vs. Efficiency (10k for v0.3): Larger dataset yields better general text accuracy; smaller, specialized dataset yields better structure and lower training cost

**Failure signatures:**
- Confusing similar-looking Arabic letters (e.g., 'ب' 'ت' 'ث') due to dot misplacement, especially on degraded images
- Dropping or hallucinating diacritics (tashkeel)
- Misinterpreting layout in dense text or skipping peripheral elements (page numbers, marginalia)
- Complete performance collapse if 4-bit quantization is used

**First 3 experiments:**
1. Benchmark QARI v0.2 (8-bit) on your specific Arabic document set against the paper's reported metrics to establish a baseline
2. Compare QARI v0.2 and QARI v0.3 on samples requiring structure extraction to quantify the trade-off between textual fidelity and structural understanding
3. Test the model's robustness limits by systematically degrading input images to see where performance drops below an acceptable threshold

## Open Questions the Paper Calls Out

**Open Question 1:** Can Qari-OCR be extended to robust, quantitative Arabic handwriting recognition? The authors state future work includes extending capabilities to Arabic handwriting recognition, but current v0.3 results are qualitative explorations lacking rigorous benchmarking.

**Open Question 2:** How can text recognition accuracy be maintained within complex graphical elements like figures and charts? Section 6 lists struggles to accurately recognize text embedded within figures as a limitation, with the model currently prioritizing main body text.

**Open Question 3:** What modifications are required to effectively capture peripheral text elements like marginalia and page numbers? Section 7 calls for advancing layout analysis for peripheral text as a key future direction, as the model tends to skip edge-bound elements during full-page transcription.

## Limitations

- The model struggles to accurately recognize text embedded within figures and charts, prioritizing main body text over graphical elements
- Performance on Arabic handwriting recognition remains qualitative rather than quantitatively validated
- The model tends to skip peripheral text elements like headers, footers, and marginal notes during full-page transcription

## Confidence

- **High Confidence:** QARI v0.2 achieves state-of-the-art WER 0.160, CER 0.061, and BLEU 0.737 on diacritically-rich Arabic text
- **Medium Confidence:** The progressive synthetic data curriculum is the primary driver of performance gains, though not definitively proven through ablation studies
- **Low Confidence:** 4-bit quantization is universally unusable for this task, based on testing only the specific Qwen2-VL-2B-Instruct base model and reported synthetic datasets

## Next Checks

1. **Ablation Study on Synthetic Data Stages:** Retrain QARI v0.2 by skipping specific stages of the synthetic data curriculum to isolate and quantify the contribution of each complexity level to final performance.

2. **Domain Transfer Robustness Test:** Evaluate pre-trained QARI v0.2 on held-out test sets from domains not represented in training data (e.g., modern Arabic social media posts, scientific abstracts, historical manuscripts).

3. **Quantization Resilience Analysis:** Systematically test model performance with 6-bit and 5-bit quantization in addition to reported 8-bit and 4-bit, plotting performance degradation to identify true minimum precision threshold.