---
ver: rpa2
title: 'Chain-of-Experts: Unlocking the Communication Power of Mixture-of-Experts
  Models'
arxiv_id: '2506.18945'
source_url: https://arxiv.org/abs/2506.18945
tags:
- expert
- experts
- routing
- arxiv
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Chain-of-Experts introduces iterative expert communication within
  Mixture-of-Experts layers, allowing tokens to be routed sequentially through different
  experts across multiple iterations rather than being assigned to a static subset.
  This design uses independent routers at each iteration, enabling tokens to re-evaluate
  and select new experts based on intermediate representations, thus increasing expert
  diversity and compositional depth.
---

# Chain-of-Experts: Unlocking the Communication Power of Mixture-of-Experts Models
## Quick Facts
- **arXiv ID**: 2506.18945
- **Source URL**: https://arxiv.org/abs/2506.18945
- **Reference count**: 40
- **Primary result**: Iterative expert communication reduces math reasoning loss from 1.20 to 1.12 under fixed compute

## Executive Summary
Chain-of-Experts introduces iterative expert communication within Mixture-of-Experts layers, allowing tokens to be routed sequentially through different experts across multiple iterations rather than being assigned to a static subset. This design uses independent routers at each iteration, enabling tokens to re-evaluate and select new experts based on intermediate representations, thus increasing expert diversity and compositional depth. Experiments show that under fixed compute, Chain-of-Experts reduces validation loss from 1.20 to 1.12 on math reasoning tasks compared to standard MoE. It also offers more efficient scaling: 2× iterations match the performance of 3× expert selections in width while reducing memory usage by 17.6-42%. The benefits stem from enhanced expert specialization and richer expert combinations enabled by iterative routing, with ablation studies confirming the importance of per-iteration gating and inner residual connections. This approach unlocks a new scaling dimension—depth through expert iteration—complementing traditional width and depth scaling in sparse models.

## Method Summary
Chain-of-Experts introduces iterative expert communication within Mixture-of-Experts layers by routing tokens through different experts across multiple iterations rather than assigning them statically. Each iteration uses an independent router to select experts based on the current token representation, allowing tokens to visit different experts and form diverse expert combinations. The architecture maintains the standard MoE gating mechanism but applies it repeatedly within each layer, with inner residual connections preserving information flow between iterations. Under fixed computational budget, this approach trades width (number of experts) for depth (number of iterations), enabling more efficient scaling. The key insight is that iterative routing increases expert diversity and compositional depth, leading to better performance than traditional static routing while potentially reducing memory usage by requiring fewer experts per layer.

## Key Results
- Reduces math reasoning validation loss from 1.20 to 1.12 under fixed compute budget
- Achieves 2× iterations performance equivalent to 3× expert selections in width, reducing memory usage by 17.6-42%
- Ablation studies confirm importance of per-iteration gating and inner residual connections for performance

## Why This Works (Mechanism)
The iterative routing mechanism works by allowing tokens to dynamically re-evaluate their expert needs at each iteration, rather than being locked into static expert assignments. This creates a form of conditional computation where tokens can explore different expert combinations based on their evolving representations. The independent routers at each iteration enable tokens to correct initial routing decisions and access specialized knowledge that might not be available in their first expert selection. The inner residual connections preserve information across iterations while allowing the model to build increasingly refined representations through expert composition. This mechanism effectively increases the model's capacity to learn complex, hierarchical representations without proportionally increasing the number of experts or parameters.

## Foundational Learning
- **Mixture-of-Experts (MoE)**: Sparse activation where only a subset of experts are used per token; needed for efficient scaling of large models by activating specialized sub-networks conditionally
- **Expert Diversity**: The variety of different expert representations available to tokens; critical for Chain-of-Experts as iterative routing increases access to diverse expert knowledge
- **Conditional Computation**: Models that compute different amounts of work for different inputs; foundational concept enabling MoE and Chain-of-Experts' dynamic routing decisions
- **Router Stability**: Consistency of routing decisions during training; important for Chain-of-Experts as independent per-iteration routers could introduce routing instability without proper regularization
- **Residual Connections**: Skip connections that preserve information flow; essential for Chain-of-Experts to maintain representation quality across multiple routing iterations
- **Expert Specialization**: The degree to which different experts learn distinct functions; enhanced by iterative routing as tokens can access multiple specialized experts

## Architecture Onboarding
**Component Map**: Input tokens -> Router 1 -> Expert 1 subset -> Router 2 -> Expert 2 subset -> ... -> Router N -> Expert N subset -> Output aggregation
**Critical Path**: Token embedding → Router evaluation → Expert selection → Expert computation → Residual addition → Next iteration or output
**Design Tradeoffs**: Width vs depth (more experts vs more iterations), router complexity vs routing stability, memory efficiency vs computational overhead
**Failure Signatures**: Routing instability (chaotic expert selection patterns), gradient vanishing (poor residual connections), expert collapse (insufficient diversity)
**First Experiments**: 1) Baseline MoE vs Chain-of-Experts on math reasoning task, 2) Ablation of per-iteration gating, 3) Memory usage comparison across different iteration counts

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Experimental validation limited to math reasoning tasks with narrow model size range (12B-70B parameters) and expert counts (4-16)
- Memory savings estimates depend on specific architectural assumptions and may not generalize across implementations
- Computational overhead from multiple router evaluations per token could create bottlenecks in practical deployments

## Confidence
- **High confidence**: Performance improvements on math reasoning benchmarks under fixed compute budgets are empirically demonstrated and reproducible
- **Medium confidence**: Memory efficiency gains are supported by controlled experiments but may vary with implementation details and hardware
- **Low confidence**: Claims about unlocking new scaling dimensions and generalization to other domains/tasks require further validation

## Next Checks
1. Evaluate Chain-of-Experts performance across diverse task domains (code generation, general QA, multilingual tasks) to assess domain generalization limits
2. Conduct scaling experiments beyond 70B parameters and 16 experts to verify claimed efficiency benefits at production scales
3. Implement and measure router stability metrics during training to quantify potential routing instability and evaluate regularization strategies