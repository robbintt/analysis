---
ver: rpa2
title: 'Optimising ChatGPT for creativity in literary translation: A case study from
  English into Dutch, Chinese, Catalan and Spanish'
arxiv_id: '2504.18221'
source_url: https://arxiv.org/abs/2504.18221
tags:
- translation
- chatgpt
- language
- text
- errors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the optimal settings for ChatGPT to generate
  creative literary translations from English into Dutch, Chinese, Catalan, and Spanish.
  Researchers tested six configurations combining text granularity (paragraph vs.
---

# Optimising ChatGPT for creativity in literary translation: A case study from English into Dutch, Chinese, Catalan and Spanish

## Quick Facts
- arXiv ID: 2504.18221
- Source URL: https://arxiv.org/abs/2504.18221
- Reference count: 22
- Primary result: Minimal prompt "Translate the following text into [TG] creatively" at temperature 1.0 yielded best creative translations, but ChatGPT underperformed human translation in all languages.

## Executive Summary
This study investigated optimal ChatGPT configurations for creative literary translation from English into Dutch, Chinese, Catalan, and Spanish. Researchers tested six configurations combining text granularity (paragraph vs. document level), temperature settings (0.0 vs. 1.0), and prompting strategies (minimal instruction vs. genre/author context vs. direct creativity request). Using a Creativity Score formula balancing creative shifts against translation errors, the analysis found that minimal prompting at temperature 1.0 produced the most creative translations, outperforming other configurations and DeepL in three of four languages. However, ChatGPT consistently underperformed compared to human translation, and optimal settings varied significantly by language.

## Method Summary
The study used Kurt Vonnegut's short story "2BR02B" (2548 words, 123 paragraphs) containing 54 pre-annotated Units of Creative Potential. Researchers tested six ChatGPT configurations across three phases: granularity (paragraph vs. document), temperature (0.0 vs. 1.0), and prompting strategy (minimal vs. genre/author context vs. direct creativity request). Manual annotation classified creative shifts and errors using DQF-MMQ error taxonomy and Creative Shift classification. Automatic metrics (BLEU, chrF, TER, COMET, COMET-Kiwi) provided additional evaluation. Results were compared against DeepL and human translation baselines where available.

## Key Results
- Minimal prompt "Translate the following text into [TG] creatively" at temperature 1.0 yielded the best creative translations for Spanish, Dutch, and Chinese
- ChatGPT consistently underperformed human translation across all languages
- Less prompting information generally produced better creative outputs, though with higher error rates
- Document-level translation degraded quality for Chinese due to attention limitations and error accumulation

## Why This Works (Mechanism)

### Mechanism 1: Temperature-Mediated Creative Tradeoff
Higher temperature settings (1.0) increase creative shifts in translation outputs but simultaneously elevate error rates, creating a net-gain only when the creativity score formula weights novelty sufficiently. Temperature controls the probability distribution over next-token predictions—higher values flatten the distribution, allowing lower-probability tokens (more novel/creative choices) to be selected. However, this same mechanism permits grammatical errors, hallucinations, and incoherent collocations. The relationship between temperature and creativity is non-linear and language-dependent; what works for ES, NL, and ZH at 1.0 fails for CA.

### Mechanism 2: Minimal-Prompt Inversion Effect
Reduced prompt information paradoxically yields superior creative translation quality compared to information-rich prompts containing genre, author, or domain context. Detailed prompts may constrain the model's probability space toward expected, "safe" translations aligned with the stated domain, reducing exploratory token selection. Minimal prompts leave the solution space less constrained. The model's internal representations of "creative" already encode sufficient stylistic awareness without explicit genre/author framing.

### Mechanism 3: Context-Length Performance Degradation
Document-level translation can underperform paragraph-level translation due to cumulative error propagation and attention degradation over longer sequences. LLMs exhibit attention dilution and "forgetting" effects over long contexts; translation quality degrades toward document end, particularly for error-sensitive languages. The degradation is model-architecture specific rather than language-specific.

## Foundational Learning

- **Concept: Creativity Index (CI) Formula**
  - Why needed here: The study's core metric combines novelty (creative shifts) and acceptability (errors) into a single comparable score across configurations.
  - Quick check question: If a translation has 10 creative shifts and 50 error points across 500 source words with 54 UCPs, what is the CI? (Answer: (10/54 - 50/500) × 100 = 8.52)

- **Concept: Creative Shifts Taxonomy (Bayer-Hohenwarter)**
  - Why needed here: Annotators classified deviations into abstraction, concretization, and modification—understanding this is essential for interpreting the CS counts.
  - Quick check question: If "business" (meaning "concern") is translated literally as "commercial activity," is this a CS or an error? (Answer: Error—Major, because it confuses the reader about meaning)

- **Concept: Temperature as Sampling Parameter**
  - Why needed here: Temperature 0.0 ≠ deterministic output in ChatGPT API; understanding this prevents false expectations of reproducibility.
  - Quick check question: Why did the study choose 1.0 instead of the maximum 2.0? (Answer: Higher values produced "word vomit"—incoherent, unreadable text)

## Architecture Onboarding

- **Component map:**
  Input Layer: Prompt template + source text segment (paragraph or document)
  Generation Layer: GPT-4o API with configurable temperature (0.0-2.0)
  Evaluation Layer: Human annotation (CSs + errors) → Creativity Index; AEMs (BLEU, chrF, TER, COMET, COMET-Kiwi) for automatic comparison
  Comparison Layer: ChatGPT configurations vs. DeepL/GT vs. Human Translation baseline

- **Critical path:**
  1. Select optimal text granularity per language (Phase 1 results)
  2. Select optimal temperature per language (Phase 2 results)
  3. Select optimal prompt strategy (Phase 3 results)
  4. Note: Optimal settings are language-dependent—no universal configuration exists

- **Design tradeoffs:**
  - Temperature 1.0 vs. 0.0: Creativity vs. reliability (language-dependent break-even)
  - Paragraph vs. document: Local accuracy vs. global coherence
  - Minimal vs. detailed prompt: Creative freedom vs. domain-specific accuracy

- **Failure signatures:**
  - "Word vomit" at temperature >1.0: Incoherent text, unusable output
  - Sobriquet non-translation: Culturally-specific nicknames left in source language
  - End-of-document degradation: Errors accumulate, critical meaning failures emerge
  - chrF comparison reveals non-determinism: Even temperature 0.0 produces ~88% similarity, not 100%

- **First 3 experiments:**
  1. Replicate the Phase 3 comparison (Prompts 2 vs. 3) with your target language using the same 48-sentence subset to establish baseline comparability with published results.
  2. Test temperature values at 0.25 intervals between 0.0-1.0 to identify language-specific optimal points (the study only tested 0.0 and 1.0).
  3. Measure reproducibility by running identical prompts 5 times at both temperature settings to quantify variance; the study notes "there is a level of randomization... that requires many iterations."

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the "less is more" prompting strategy (minimal instruction) generalize to other literary genres and authors, or is it specific to the science fiction text used?
- **Open Question 2:** Why does adding specific domain and author information (Prompt 2) result in lower creativity scores compared to a simple instruction to "translate creatively"?
- **Open Question 3:** Can automatic evaluation metrics (AEMs) be developed to reliably correlate with human assessments of creativity and error severity in literary translation?

## Limitations

- Language-specificity of findings: Optimal settings vary significantly by language, with Catalan requiring different temperature settings than other languages
- Single-annotator methodology: All manual evaluations performed by single annotator per language, though inter-annotator reliability testing was conducted
- Limited temperature exploration: Only two temperature values (0.0 and 1.0) tested, with 1.0 being the practical upper bound before output degradation

## Confidence

**High Confidence:**
- ChatGPT configurations consistently underperform human translation across all languages
- Minimal prompting strategy outperforms information-rich prompts for creative translation generation
- Temperature 1.0 increases creative shifts but also increases error rates, creating a tradeoff that varies by language

**Medium Confidence:**
- Document-level translation degrades quality for Chinese due to attention limitations and error accumulation
- The minimal prompt "Translate the following text into [TG] creatively" at temperature 1.0 yields optimal creative translations for ES, NL, and ZH

**Low Confidence:**
- Generalizability of optimal settings to other literary texts or genres beyond sci-fi short stories
- Automatic metric correlation with human creativity scores (only NL showed significant correlation; other languages did not)

## Next Checks

1. **Temperature optimization validation:** Test temperature values at 0.25 intervals between 0.0-1.0 for each target language to identify precise optimal points beyond the binary comparison used in the study.

2. **Inter-annotator reliability verification:** Replicate the manual annotation process with at least three annotators per language to verify the single-annotator findings and establish more robust error/shift classification standards.

3. **Genre transferability test:** Apply the optimal configurations (minimal prompt + temperature 1.0 for ES/NL/ZH; minimal prompt + temperature 0.0 for CA) to literary texts from different genres (poetry, drama, non-fiction) to assess generalizability beyond sci-fi short stories.