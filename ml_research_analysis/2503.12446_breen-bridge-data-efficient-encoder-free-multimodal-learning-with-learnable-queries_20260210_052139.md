---
ver: rpa2
title: 'BREEN: Bridge Data-Efficient Encoder-Free Multimodal Learning with Learnable
  Queries'
arxiv_id: '2503.12446'
source_url: https://arxiv.org/abs/2503.12446
tags:
- learnable
- image
- queries
- wang
- breen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BREEN introduces a data-efficient encoder-free multimodal model
  that addresses the performance gap between encoder-based and encoder-free MLLMs.
  The core innovation is a learnable query mechanism that bridges visual and textual
  modalities by distilling knowledge from a pretrained CLIP model.
---

# BREEN: Bridge Data-Efficient Encoder-Free Multimodal Learning with Learnable Queries

## Quick Facts
- **arXiv ID**: 2503.12446
- **Source URL**: https://arxiv.org/abs/2503.12446
- **Reference count**: 40
- **Primary result**: Achieves competitive performance with only 13 million text-image pairs, outperforming prior encoder-free models by 2% on average across multiple benchmarks

## Executive Summary
BREEN introduces a data-efficient encoder-free multimodal model that addresses the performance gap between encoder-based and encoder-free MLLMs. The core innovation is a learnable query mechanism that bridges visual and textual modalities by distilling knowledge from a pretrained CLIP model. The learnable query, positioned between image and text tokens, is supervised by CLIP-derived visual representations to ensure effective knowledge transfer. Additionally, an image expert processes image tokens and learnable queries independently to minimize interference with the LLM's textual reasoning capabilities. BREEN achieves competitive performance with only 13 million text-image pairs—about one percent of the data required by existing methods—outperforming prior encoder-free models like Mono-InternVL by 2% on average across multiple benchmarks including MMMU, MME, and MMVet. The model also demonstrates the importance of multi-granular learnable queries for balancing performance across diverse tasks, from fine-grained OCR to high-level perceptual reasoning.

## Method Summary
BREEN is an encoder-free multimodal model that uses learnable queries to bridge visual and textual modalities without requiring a separate visual encoder. The model leverages a pretrained CLIP model to distill visual knowledge into learnable queries that are inserted between image and text tokens. An image expert module processes image tokens and learnable queries independently to minimize interference with the LLM's textual reasoning capabilities. The model is trained on only 13 million text-image pairs, making it significantly more data-efficient than existing encoder-free approaches. The key innovation is the learnable query mechanism that allows effective knowledge transfer from CLIP to the multimodal model while maintaining the efficiency benefits of encoder-free architectures.

## Key Results
- Achieves competitive performance with only 13 million text-image pairs (approximately 1% of data used by existing methods)
- Outperforms prior encoder-free models like Mono-InternVL by 2% on average across multiple benchmarks
- Demonstrates strong performance across diverse tasks including MMMU, MME, and MMVet benchmarks
- Shows the importance of multi-granular learnable queries for balancing performance across different task types

## Why This Works (Mechanism)
The model works by using learnable queries as intermediaries between visual and textual modalities. These queries are trained to capture visual information distilled from a pretrained CLIP model, allowing the LLM to reason about visual content without requiring a separate visual encoder. The image expert module processes visual information independently, preventing interference with the LLM's textual reasoning capabilities. This design enables effective knowledge transfer while maintaining the efficiency benefits of encoder-free architectures.

## Foundational Learning
**CLIP Distillation**: Why needed - to transfer visual knowledge to learnable queries; Quick check - verify that CLIP representations capture relevant visual features
**Learnable Queries**: Why needed - to bridge visual and textual modalities without separate encoders; Quick check - ensure queries effectively capture and represent visual information
**Image Expert Module**: Why needed - to process visual information independently and prevent interference with LLM reasoning; Quick check - validate that visual processing doesn't degrade textual reasoning performance
**Multi-Granular Queries**: Why needed - to handle different task requirements from fine-grained OCR to high-level reasoning; Quick check - test performance across diverse task types
**Encoder-Free Architecture**: Why needed - to maintain efficiency while achieving competitive performance; Quick check - compare data efficiency with encoder-based alternatives

## Architecture Onboarding

**Component Map**: CLIP model -> Learnable Queries -> Image Expert -> LLM Tokens

**Critical Path**: Visual input → CLIP distillation → Learnable queries → Image expert processing → LLM reasoning → Output

**Design Tradeoffs**: The model trades off some potential performance gains from dedicated visual encoders for significant improvements in data efficiency and training speed. The learnable query mechanism requires careful supervision from CLIP to ensure effective knowledge transfer.

**Failure Signatures**: Potential failures include poor visual understanding if CLIP distillation is ineffective, interference between visual and textual processing if the image expert module is not properly isolated, and reduced performance on tasks requiring tight visual-textual integration.

**First 3 Experiments**: 1) Validate CLIP distillation quality by comparing learned query representations to CLIP features; 2) Test image expert isolation by measuring performance degradation when visual processing is enabled/disabled; 3) Evaluate multi-granular query effectiveness across different task types (OCR vs perceptual reasoning).

## Open Questions the Paper Calls Out
None

## Limitations
- Relies heavily on CLIP distillation, potentially inheriting biases and limitations from the pretrained CLIP model
- Limited empirical evidence about trade-offs in handling complex multimodal reasoning tasks requiring tight visual-textual integration
- Effectiveness depends on the quality of CLIP-derived visual representations, with unclear performance degradation when using different CLIP variants

## Confidence
- **High Confidence**: Data efficiency claims (13M pairs vs ~1% of competitors) and benchmark performance improvements (2% average gain over Mono-InternVL)
- **Medium Confidence**: The effectiveness of the learnable query mechanism for bridging modalities, as validation primarily comes from downstream benchmarks rather than detailed ablation studies on the query mechanism itself
- **Medium Confidence**: Claims about multi-granular learnable queries being crucial for balancing performance across tasks, as the paper demonstrates correlation but not causation

## Next Checks
1. Ablation study isolating the contribution of learnable queries versus image expert module to determine which component drives the majority of performance gains
2. Testing BREEN's performance using different CLIP variants (e.g., CLIP-ViT, CLIP-ResNet) to assess dependency on specific visual backbones
3. Evaluation on out-of-distribution datasets to measure robustness and generalization beyond the reported benchmarks