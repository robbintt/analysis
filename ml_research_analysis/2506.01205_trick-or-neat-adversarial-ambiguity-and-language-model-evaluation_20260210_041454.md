---
ver: rpa2
title: 'Trick or Neat: Adversarial Ambiguity and Language Model Evaluation'
arxiv_id: '2506.01205'
source_url: https://arxiv.org/abs/2506.01205
tags:
- telescope
- random
- woman
- synonym
- subj
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AmbAdv, the first adversarial ambiguity dataset
  designed to evaluate how well language models (LMs) detect ambiguity under adversarial
  variations. The dataset includes syntactically, lexically, and phonologically ambiguous
  sentences with perturbations such as synonym replacements, random word substitutions,
  and rhyme-based alterations.
---

# Trick or Neat: Adversarial Ambiguity and Language Model Evaluation

## Quick Facts
- arXiv ID: 2506.01205
- Source URL: https://arxiv.org/abs/2506.01205
- Reference count: 40
- Primary result: Language models encode ambiguity information in their representations but struggle to use it in outputs

## Executive Summary
This paper introduces AmbAdv, the first adversarial ambiguity dataset designed to evaluate how well language models detect ambiguity under adversarial variations. The dataset includes syntactically, lexically, and phonologically ambiguous sentences with perturbations such as synonym replacements, random word substitutions, and rhyme-based alterations. Experiments with four open-access 7B-parameter LMs show that direct prompting fails to reliably identify ambiguity, often exhibiting a "yes bias." However, linear probes trained on model representations can decode ambiguity with high accuracy, sometimes exceeding 90%, especially for last-noun and end-of-prompt tokens. The findings reveal that while LMs encode ambiguity-related information in their representations, they struggle to leverage it in their outputs, suggesting partial reliance on memorization rather than ambiguity understanding.

## Method Summary
The paper creates AmbAdv, an adversarial ambiguity dataset featuring syntactically, lexically, and phonologically ambiguous sentences with perturbations like synonym replacements, random word substitutions, and rhyme-based alterations. The dataset is used to evaluate four open-access 7B-parameter LMs (Qwen-2.5, Mistral-v0.3, Llama-3, and Gemma) through two approaches: direct prompting for ambiguity detection and linear probes trained on model representations. The linear probes are designed to decode ambiguity information from specific token positions, particularly focusing on last-noun and end-of-prompt tokens. The experiments compare model performance across different ambiguity types and perturbation strategies.

## Key Results
- Direct prompting fails to reliably identify ambiguity, showing a "yes bias" in responses
- Linear probes achieve high accuracy (sometimes exceeding 90%) in decoding ambiguity from model representations
- Ambiguity information is most accessible from last-noun and end-of-prompt tokens
- LMs encode ambiguity-related information in representations but struggle to leverage it in outputs

## Why This Works (Mechanism)
The paper's approach works because it exploits the distinction between what language models encode in their representations versus what they produce in their outputs. By using adversarial perturbations and linear probes, the researchers can isolate and measure the latent ambiguity information that models store but fail to utilize during generation. The methodology leverages the fact that model representations contain richer semantic and syntactic information than their surface-level outputs suggest, particularly when examining specific token positions that carry ambiguity cues.

## Foundational Learning
- Adversarial examples and perturbations - Needed to create challenging test cases that expose model limitations; Quick check: Can the model handle systematically altered ambiguous sentences?
- Linear probe methodology - Required to extract information from model representations without fine-tuning; Quick check: Does probe accuracy exceed chance level?
- Ambiguity types (syntactic, lexical, phonological) - Essential for comprehensive evaluation across different ambiguity dimensions; Quick check: Are all ambiguity types equally challenging for models?
- Token position significance - Critical for understanding where ambiguity information is most accessible; Quick check: Which token positions yield highest probe accuracy?

## Architecture Onboarding

**Component Map**
AmbAdv Dataset -> 7B LMs (Qwen-2.5, Mistral, Llama-3, Gemma) -> Linear Probes/Direct Prompts -> Ambiguity Detection Results

**Critical Path**
Dataset creation → Model evaluation → Linear probe training → Result analysis → Interpretation of representation vs output performance

**Design Tradeoffs**
The choice of 7B-parameter models balances computational feasibility with model capability, though this limits generalizability to larger models. Direct prompting is simple but shows bias, while linear probes are more accurate but require supervised training.

**Failure Signatures**
"Yes bias" in direct prompting indicates models default to affirmative ambiguity responses. Low linear probe accuracy suggests insufficient ambiguity encoding in representations. Poor performance on specific ambiguity types reveals model weaknesses in particular linguistic phenomena.

**First Experiments**
1. Test direct prompting on unperturbed ambiguous sentences to establish baseline performance
2. Evaluate linear probe accuracy on each token position to identify most informative locations
3. Compare performance across different ambiguity types to determine which are most challenging

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize to larger or differently architected models
- Adversarial perturbations represent a narrow set of ambiguity types, missing broader natural language complexity
- Linear probe methodology relies on supervised learning, potentially not reflecting genuine LM understanding

## Confidence
- Core claims about representation encoding: Medium
- Direct prompting limitations: Medium
- Generalizability to other models: Low
- Distinction between memorization and understanding: Medium

## Next Checks
1. Test the same methodology on larger and more diverse LM architectures
2. Evaluate models on naturally occurring ambiguous sentences from real-world corpora
3. Conduct ablation studies to isolate specific features in model representations that correlate with ambiguity detection