---
ver: rpa2
title: 'Attackers Can Do Better: Over- and Understated Factors of Model Stealing Attacks'
arxiv_id: '2503.06188'
source_url: https://arxiv.org/abs/2503.06188
tags:
- data
- target
- substitute
- attack
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work shows that target model performance is the key limiting
  factor for stealing success, not target architecture complexity. Attacks succeed
  better on high-performing targets because substitutes learn correct predictions
  far more easily than mistakes.
---

# Attackers Can Do Better: Over- and Understated Factors of Model Stealing Attacks

## Quick Facts
- **arXiv ID**: 2503.06188
- **Source URL**: https://arxiv.org/abs/2503.06188
- **Reference count**: 40
- **Primary result**: Target model performance is the key limiting factor for stealing success, not target architecture complexity.

## Executive Summary
This work challenges conventional wisdom about model stealing attacks by demonstrating that target model performance, not complexity, is the primary limiting factor for attack success. The authors show that substitutes learn correct predictions far more easily than mistakes, making high-performing targets more vulnerable to stealing. The research reveals that attackers can achieve high-fidelity model stealing with less effort than previously assumed, particularly when using active learning strategies and understanding the relationship between data complexity and attack effectiveness.

## Method Summary
The study investigates model stealing attacks on CIFAR-10 image classifiers with black-box access (labels only). Three target models are used: SimpleNet scratch (~5M params, 91.76% accuracy), ResNet-34 scratch (~21M, 93.61%), and ResNet-34 pretrained (97.14%). Attackers use three substitute architectures (SimpleNet scratch, ResNet-18 pretrained, ResNet-34 pretrained) and three datasets (original CIFAR-10, CINIC-10 problem-domain, and 50k artificial images generated via Stable Diffusion 2.1). Five query budgets (1k, 5k, 10k, 20k, 45k) are tested with four strategies: baseline random sampling, Active Learning (DFAL + κ-center), Adversarial Augmentation (DeepFool), and combination. 180 total configurations are evaluated on fidelity (agreement between target/substitute predictions) and accuracy metrics.

## Key Results
- Target model performance is the primary limiting factor for stealing success, not target architecture complexity
- Attackers with more complex data underestimate performance while simpler data leads to overestimation
- Data-free attacks are feasible with query budgets smaller than the target training set
- Active learning can significantly boost attack efficiency, particularly at larger query budgets

## Why This Works (Mechanism)
The core mechanism behind successful model stealing is that substitutes learn correct predictions more easily than mistakes. When target models have high accuracy, the correct decision boundaries are more consistent and easier to approximate. The study shows that architectural transfer learning impacts attacks: substitutes trained from scratch learn better from scratch-trained targets, while pretrained substitutes perform better on pretrained targets. Data complexity plays a crucial role - attackers with complex datasets tend to underestimate their attack performance, while those with simpler data overestimate it.

## Foundational Learning
- **CIFAR-10 classification task**: Standard benchmark dataset for image classification (why needed: provides consistent evaluation baseline; quick check: verify 10 classes and 50k training/10k test split)
- **Model fidelity metrics**: Agreement between target and substitute predictions (why needed: primary success measure; quick check: calculate both fidelity and accuracy on same test set)
- **Active learning strategies**: DFAL and κ-center sampling methods (why needed: improves query efficiency; quick check: compare random vs AL performance at small budgets)
- **Transfer learning impact**: How pretraining affects substitute learning (why needed: explains architecture choice effects; quick check: compare scratch vs pretrained substitutes on same target)
- **Data complexity assessment**: Qualitative evaluation of dataset difficulty (why needed: explains performance variation; quick check: compare validation vs test accuracy gaps)
- **Query budget optimization**: Relationship between queries used and attack success (why needed: practical constraint for real attacks; quick check: verify 45k queries < target training set size)

## Architecture Onboarding

**Component Map**: Target Models (SimpleNet/ResNet-34) -> Attack Strategies (Random/AL/Adv/Combo) -> Substitute Training -> Evaluation (Fidelity/Accuracy)

**Critical Path**: Dataset preparation → Target model training → Query optimization pipeline setup → Substitute training with attack strategy → Performance evaluation on CIFAR-10 test set

**Design Tradeoffs**: 
- Scratch vs pretrained substitutes: scratch learns better from scratch-trained targets but requires more queries
- Data source choice: original data provides best results, artificial data overestimates performance, problem-domain offers middle ground
- Query budget allocation: larger budgets enable active learning benefits but increase attack cost

**Failure Signatures**: 
- Low fidelity on pretrained targets with scratch substitutes (should be ~17% worse per Table VII)
- Overestimated performance with artificial data (validation >90% but test <75%)
- Active learning degrades performance at small budgets (1k-5k queries)

**First Experiments**:
1. Train target models to verify CIFAR-10 performance matches reported values
2. Generate artificial dataset using Stable Diffusion 2.1 with exact prompts from Table XVI
3. Run single configuration attack (ResNet-34 target, SimpleNet substitute, CIFAR-10 data, 10k queries) to establish baseline performance

## Open Questions the Paper Calls Out
None

## Limitations
- Missing critical hyperparameters for model training (learning rate, batch size, optimizer, epochs, early stopping criteria)
- Active learning hyperparameters (seed size, number of rounds) per query budget not reported
- Transfer learning specifics including layer freezing and fine-tuning details unspecified
- Artificial data generation process lacks details on Stable Diffusion settings
- No statistical significance testing or confidence intervals reported for performance differences

## Confidence
- **High confidence**: Target model performance as primary limiting factor (consistent patterns across experiments)
- **Medium confidence**: Architectural transfer learning impact (limited to three specific target/substitute pairs)
- **Medium confidence**: Data complexity effects on attack performance (three dataset types with varying characteristics)
- **Low confidence**: Specific quantitative performance thresholds and exact query budget efficiency gains (dependent on unreported hyperparameters)

## Next Checks
1. Verify target model training pipeline achieves reported CIFAR-10 performance before proceeding with attacks
2. Generate validation set using exact Stable Diffusion prompts and verify artificial data validation/test accuracy gap pattern
3. Run minimal active learning experiment comparing random sampling vs DFAL+κ-center at 1k query budget