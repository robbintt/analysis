---
ver: rpa2
title: Benchmarking Concept-Spilling Across Languages in LLMs
arxiv_id: '2601.12549'
source_url: https://arxiv.org/abs/2601.12549
tags:
- language
- languages
- english
- spilling
- meanings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel comparative framework for evaluating
  multilingual semantic robustness in large language models (LLMs), focusing on "language
  spilling" - the phenomenon where models default to semantic representations from
  dominant languages (typically English) when generating content in other languages.
  The authors develop a structured methodology using 100 high-polysemy English words,
  translated into nine target languages, and prompt models to generate five distinct
  meanings with examples for each translated word.
---

# Benchmarking Concept-Spilling Across Languages in LLMs

## Quick Facts
- arXiv ID: 2601.12549
- Source URL: https://arxiv.org/abs/2601.12549
- Reference count: 40
- Primary result: Evaluates multilingual semantic robustness in LLMs using "language spilling" phenomenon across 16 models and 9 languages

## Executive Summary
This paper introduces a novel comparative framework for evaluating multilingual semantic robustness in large language models (LLMs), focusing on "language spilling" - the phenomenon where models default to semantic representations from dominant languages (typically English) when generating content in other languages. The authors develop a structured methodology using 100 high-polysemy English words, translated into nine target languages, and prompt models to generate five distinct meanings with examples for each translated word. These outputs are validated against authoritative dictionary definitions using a judge model, with spilling rates calculated as the percentage of invalid meanings. The study evaluates 16 diverse models and finds significant variation in semantic robustness across both models and languages. The strongest model (Apertus-70B-2509) achieved an average spilling rate of 20%, while the weakest (llama-3.1-8b-instruct) reached 41%. The methodology demonstrates high agreement with human evaluators (77.43%) and provides principled rankings for model comparison without requiring definitive attribution of error sources.

## Method Summary
The methodology uses 100 high-polysemy English words selected from Wiktionary, translated into nine target languages (Spanish, French, German, Russian, Chinese, Japanese, Korean, Arabic, and Hindi). For each translated word, models are prompted to generate five distinct meanings with examples. Outputs are validated against dictionary definitions from authoritative sources (Cambridge Dictionary for English, Cambridge Dictionary for Spanish, Larousse for French, Duden for German, Wiktionary for Russian, Pleco for Chinese, Jisho for Japanese, Naver for Korean, Al Maany for Arabic, and Oxford for Hindi). A judge model (llama-3.1-70b-instruct) evaluates whether generated meanings match dictionary definitions, with spilling rates calculated as the percentage of invalid meanings. The study compares 16 diverse models including GPT-4, Claude-3, Gemini, Llama, Mistral, and various open-source alternatives.

## Key Results
- Strong variation in semantic robustness across models: Apertus-70B-2509 achieved 20% average spilling rate, while llama-3.1-8b-instruct reached 41%
- Significant language-specific differences in model performance, with no single model dominating across all languages
- High agreement between judge model and human evaluators (77.43% for exact match, 90.96% for partial match)
- Methodology successfully ranks models from strongest to weakest based on semantic robustness metrics

## Why This Works (Mechanism)
The methodology works by creating controlled test conditions that isolate semantic understanding from generation quality. By using high-polysemy words and requiring distinct meanings with examples, the framework forces models to demonstrate genuine understanding of multiple semantic concepts rather than defaulting to dominant language patterns. The validation against authoritative dictionary definitions provides an objective standard for measuring semantic accuracy, while the judge model approach enables scalable evaluation across many languages and models. The multi-language design captures cross-linguistic semantic relationships and potential spilling patterns that single-language benchmarks miss.

## Foundational Learning
- **Polysemy**: Multiple meanings associated with a single word form; essential for testing semantic depth and distinguishing between genuine understanding versus surface-level pattern matching
- **Language spilling**: Defaulting to semantic representations from dominant languages; critical concept for understanding cross-linguistic transfer and potential biases in multilingual models
- **Judge model validation**: Using one model to evaluate outputs of another; enables scalable evaluation while maintaining consistency across languages and models
- **Semantic robustness**: Ability to maintain accurate meaning representations across different linguistic contexts; key metric for multilingual model quality
- **Cross-linguistic semantic transfer**: How meaning representations transfer between languages; fundamental to understanding multilingual model behavior
- **Dictionary-based ground truth**: Using authoritative linguistic resources as evaluation standards; provides objective benchmarks for semantic accuracy

## Architecture Onboarding
Component map: Dictionary definitions -> Translated words -> Model prompts -> Generated meanings -> Judge model validation -> Spilling rate calculation
Critical path: Word selection and translation → Prompt engineering → Model generation → Judge validation → Spilling calculation
Design tradeoffs: Dictionary-based validation provides objective standards but may miss contextual nuances; judge model approach enables scalability but introduces potential systematic biases
Failure signatures: High spilling rates indicate either semantic misunderstanding, generation failures, or prompt adherence issues; language-specific failures suggest cross-linguistic transfer problems
Three first experiments: 1) Compare judge model validation against human evaluation on subset of outputs, 2) Test same methodology with low-resource languages, 3) Conduct error analysis to categorize types of spilling failures

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions, but the methodology raises several important considerations about the nature of semantic understanding in multilingual models, the relationship between dictionary definitions and natural language usage, and the scalability of judge-based evaluation approaches.

## Limitations
- Relies entirely on dictionary definitions as ground truth, which may not capture natural language complexity
- Cannot definitively attribute errors to semantic misunderstanding versus generation failures or prompt adherence issues
- Limited to nine major world languages, potentially missing patterns in low-resource language scenarios
- Single judge model used for validation may introduce systematic biases

## Confidence
- High confidence in comparative rankings between models due to consistent evaluation conditions
- Medium confidence in absolute spilling rate values due to uncertainty about error attribution
- Medium confidence in cross-language comparisons given potential dictionary quality variations
- Low confidence in generalizability to real-world usage scenarios due to controlled evaluation conditions

## Next Checks
1. Replicate the study using human evaluators for validation across all language pairs to establish ground truth rates and assess judge model reliability
2. Test the same methodology with a broader set of low-resource and morphologically complex languages to evaluate scalability of findings
3. Conduct error analysis to distinguish between semantic misunderstanding, generation failures, and prompt adherence issues in the model outputs