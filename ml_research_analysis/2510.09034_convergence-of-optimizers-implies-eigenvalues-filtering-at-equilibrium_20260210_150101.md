---
ver: rpa2
title: Convergence of optimizers implies eigenvalues filtering at equilibrium
arxiv_id: '2510.09034'
source_url: https://arxiv.org/abs/2510.09034
tags:
- theorem
- usam
- semi-algebraic
- gradient
- eigenvalues
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the implicit bias of optimization algorithms
  by studying what fixed points they converge to under the assumption that convergence
  occurs. The authors develop a generalized Hadamard-Perron theorem showing that for
  generic step sizes and initializations, if an iterative method converges to a point,
  the spectral radius of the Jacobian at that point must be at most 1.
---

# Convergence of optimizers implies eigenvalues filtering at equilibrium
## Quick Facts
- arXiv ID: 2510.09034
- Source URL: https://arxiv.org/abs/2510.09034
- Authors: Jerome Bolte; Quoc-Tung Le; Edouard Pauwels
- Reference count: 35
- Key outcome: Analyzes implicit bias of optimizers through eigenvalue filtering at convergence points using generalized Hadamard-Perron theorem

## Executive Summary
This paper provides a theoretical framework for understanding the implicit bias of optimization algorithms by analyzing what fixed points they converge to. The authors develop a generalized Hadamard-Perron theorem that shows for generic step sizes and initializations, if an iterative method converges to a point, the spectral radius of the Jacobian at that point must be at most 1. This framework reveals that different optimizers act as eigenvalue filters determined by their hyperparameters, with USAM variants showing stricter eigenvalue constraints favoring flatter minima.

The paper introduces two novel algorithms (Two-step USAM and Hessian USAM) that provide enhanced eigenvalue filtering capabilities. Experiments on MNIST, Fashion-MNIST, and CIFAR10 datasets demonstrate that SAM variants consistently find flatter minimizers compared to standard gradient descent. The theoretical predictions align well with empirical observations, showing that the implicit bias of optimization algorithms can be understood through stability analysis at convergence. The results provide insights into the edge-of-stability phenomenon and the connection between algorithmic stability and generalization.

## Method Summary
The authors analyze the implicit bias of optimization algorithms by studying their fixed points under the assumption that convergence occurs. They develop a generalized Hadamard-Perron theorem showing that for generic step sizes and initializations, if an iterative method converges to a point, the spectral radius of the Jacobian at that point must be at most 1. This framework reveals that different optimizers act as eigenvalue filters determined by their hyperparameters. For various algorithms including gradient descent, heavy ball, Nesterov, and USAM, the authors derive constraints on the Hessian eigenvalues at convergence points. USAM variants are shown to enforce stricter eigenvalue constraints, favoring flatter minima. The paper introduces two novel algorithms (Two-step USAM and Hessian USAM) that provide enhanced eigenvalue filtering. Experiments on MNIST, Fashion-MNIST, and CIFAR10 datasets demonstrate that SAM variants consistently find flatter minimizers compared to standard gradient descent, validating the theoretical predictions.

## Key Results
- Generalized Hadamard-Perron theorem establishes that convergent optimizers must have spectral radius â‰¤ 1 at fixed points
- Different optimizers (GD, heavy ball, Nesterov, USAM variants) impose specific eigenvalue constraints at convergence
- USAM variants find significantly flatter minima than standard gradient descent across multiple datasets
- Two novel algorithms (Two-step USAM and Hessian USAM) achieve enhanced eigenvalue filtering
- Theoretical predictions align well with empirical observations on MNIST, Fashion-MNIST, and CIFAR10

## Why This Works (Mechanism)
The mechanism works by analyzing the stability of fixed points through the spectral radius of the Jacobian. When an optimizer converges to a point, the spectral radius constraint ensures that the Jacobian at that point has eigenvalues with magnitudes at most 1. This creates eigenvalue filtering where only certain Hessian eigenvalue configurations are compatible with convergence. The paper shows that USAM variants impose stricter spectral radius constraints, which translates to stricter bounds on Hessian eigenvalues, effectively favoring flatter minima. The mechanism connects algorithmic stability at convergence to the implicit bias of finding specific types of solutions.

## Foundational Learning
- **Hadamard-Perron theorem**: Fundamental result in dynamical systems about stable and unstable manifolds; needed to understand the stability analysis framework
  - Quick check: Verify spectral radius < 1 implies local asymptotic stability
- **Spectral radius**: Maximum magnitude of eigenvalues of a matrix; needed to characterize convergence constraints
  - Quick check: Compute spectral radius of Jacobian at fixed points for simple optimizers
- **Implicit bias**: The tendency of optimization algorithms to converge to certain types of solutions; needed to understand generalization properties
  - Quick check: Compare convergence points of different optimizers on simple quadratic problems
- **Eigenvalue filtering**: The property that optimizers select solutions with specific Hessian eigenvalue distributions; needed to connect stability to solution properties
  - Quick check: Measure Hessian eigenvalue distribution at convergence points for different optimizers
- **Edge-of-stability phenomenon**: Behavior of optimizers near the boundary of convergence; needed to understand practical limitations
  - Quick check: Monitor training loss oscillations and step size effects near convergence
- **USAM variants**: Sharpness-Aware Minimization variants; needed to understand modern optimization approaches
  - Quick check: Compare convergence behavior of USAM vs standard gradient descent on test functions

## Architecture Onboarding
Component map: Loss function -> Optimizer (GD/Heavy Ball/Nesterov/USAM) -> Update rule -> Convergence point -> Jacobian analysis -> Eigenvalue filtering constraints

Critical path: The analysis starts with the loss function, applies the optimizer update rule, examines convergence to fixed points, analyzes the Jacobian at those points, and derives eigenvalue filtering constraints that determine which solutions are reachable through convergence.

Design tradeoffs: The main tradeoff is between convergence speed and the strictness of eigenvalue filtering. USAM variants provide better eigenvalue filtering (flatter minima) but may converge more slowly than standard gradient descent. The choice of optimizer involves balancing these competing objectives.

Failure signatures: Optimizers fail to converge when the spectral radius exceeds 1, leading to divergence or oscillation. Near the edge of stability, step sizes may need to be reduced to maintain convergence. The analysis assumes convergence occurs, so non-convergent regimes require separate treatment.

First experiments:
1. Test eigenvalue filtering predictions on simple quadratic functions with known Hessian structure
2. Compare convergence points and eigenvalue distributions across GD, heavy ball, and Nesterov on standard test functions
3. Validate USAM eigenvalue filtering predictions on small neural networks with synthetic data

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on the assumption that convergence actually occurs - doesn't address non-convergent regimes
- The "generic" assumption in the generalized Hadamard-Perron theorem may have edge cases
- Connection between eigenvalue filtering and generalization is shown through correlation rather than established causal links
- Edge-of-stability phenomenon interpretation remains qualitative rather than mathematically precise

## Confidence
High: The eigenvalue filtering predictions for specific optimizers (GD, heavy ball, Nesterov, USAM variants) based on the spectral radius analysis. The mathematical framework appears sound and the predictions are verifiable.

Medium: The practical significance of eigenvalue filtering for generalization. While experiments show flatter minima are found, the direct impact on generalization error is not rigorously established.

Low: The edge-of-stability phenomenon interpretation - while the paper connects this to their framework, the relationship is described qualitatively rather than through precise mathematical conditions.

## Next Checks
1. Test whether the eigenvalue filtering predictions hold for non-convergent regimes and what happens at the edge of stability
2. Conduct systematic experiments varying step sizes and initializations to verify the "generic" assumption predictions
3. Measure generalization error explicitly and test whether the predicted eigenvalue filtering correlates with improved generalization across different datasets and architectures