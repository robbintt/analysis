---
ver: rpa2
title: 'Modernizing Facebook Scoped Search: Keyword and Embedding Hybrid Retrieval
  with LLM Evaluation'
arxiv_id: '2509.13603'
source_url: https://arxiv.org/abs/2509.13603
tags:
- search
- retrieval
- query
- system
- meta
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a modernized Facebook Group Scoped Search
  system that integrates traditional keyword-based retrieval with embedding-based
  retrieval (EBR) to enhance search relevance and diversity. The approach combines
  semantic understanding from EBR with the precision of keyword matching, addressing
  the limitations of keyword-only search in capturing natural language queries and
  reducing sparse result sets in group contexts.
---

# Modernizing Facebook Scoped Search: Keyword and Embedding Hybrid Retrieval with LLM Evaluation

## Quick Facts
- arXiv ID: 2509.13603
- Source URL: https://arxiv.org/abs/2509.13603
- Authors: Yongye Su; Zeya Zhang; Jane Kou; Cheng Ju; Shubhojeet Sarkar; Yamin Wang; Ji Liu; Shengbo Guo
- Reference count: 15
- One-line primary result: Hybrid retrieval improves scoped search relevance rates to 85.2% (top-5) and 94.7% (top-5 somewhat relevant)

## Executive Summary
This paper presents a modernized Facebook Group Scoped Search system that integrates semantic retrieval with traditional keyword search to enhance result relevance and diversity. The hybrid approach combines the precision of keyword matching with the semantic understanding of embedding-based retrieval (EBR), addressing limitations in capturing natural language queries and reducing sparse result sets in group contexts. To support rapid iteration, the authors developed an LLM-based evaluation framework using Llama 3 as an automated judge, enabling scalable offline relevance assessment without human labeling bottlenecks.

## Method Summary
The system implements parallel retrieval from Unicorn inverted index (keyword-based) and Faiss vector index (embedding-based using SSR 12-layer 200M parameter model). Query preprocessing includes tokenization, normalization, and optional rewriting before simultaneous dispatch to both retrieval pathways. The combined candidates are ranked by an L2 multi-stage ranking model with Multi-Task Multi-Label (MTML) architecture optimizing for click, share, and comment engagement signals. LLM evaluation uses Llama 3 with multimodal capability to assess relevance through structured prompts across multiple rounds, producing aggregate metrics including top-k relevant rates and error/skip rates.

## Key Results
- Top-5 relevance rate improved from 84.7% to 85.2% with hybrid retrieval
- Top-5 somewhat relevant rate increased from 94.1% to 94.7%
- Error rates remained stable (10.8% → 10.4%) and skip rates low (~2%)
- Hybrid approach shows consistent improvements over keyword-only and EBR-only baselines

## Why This Works (Mechanism)

### Mechanism 1
Parallel retrieval from keyword and embedding indices improves result diversity and relevance in scoped search contexts where keyword-only retrieval produces sparse results. User queries are preprocessed and dispatched simultaneously to Facebook's Unicorn inverted index (lexical matching) and a Faiss vector index (semantic matching via SSR 12-layer 200M parameter model). Candidates from both pathways are merged at the ranking stage, with lexical features (TF-IDF, BM25) and semantic features (cosine similarity) passed to the L2 ranker. Keyword and embedding retrieval provide complementary coverage—keyword excels at precise term matching while EBR captures semantic relationships (e.g., "small individual cakes with frosting" matching "cupcakes").

### Mechanism 2
LLM-based evaluation (Llama 3 as judge) provides scalable, consistent offline relevance assessment that accelerates iteration cycles without human labeling bottlenecks. Real queries are replayed through the search backend to collect query-result pairs. Llama 3 with multimodal capability evaluates relevance using structured prompts across multiple rounds, producing relevance ratings. Metrics include top-k relevant rate, somewhat relevant rate, error rate, and skip rate. LLM relevance judgments correlate sufficiently with human judgments to serve as a reliable proxy for search quality.

### Mechanism 3
Fine-tuning L2 ranking models with Multi-Task Multi-Label (MTML) architecture to handle hybrid retrieval signals improves engagement optimization across multiple objectives. The combined candidate set (keyword + EBR results) with mixed feature types feeds into a multi-stage ranking pipeline. The L2 supermodel uses sub-models targeting click, share, and comment signals with weighted importance, allowing joint optimization. Engagement signals can be jointly optimized without significant objective conflicts.

## Foundational Learning

- **Approximate Nearest Neighbor (ANN) Search**: EBR uses Faiss vector index for semantic retrieval. Understanding ANN is essential for debugging retrieval quality and latency tradeoffs. Quick check: Can you explain why ANN algorithms (e.g., HNSW) trade exactness for speed, and how this affects retrieval recall at scale?

- **Score Fusion / Result Merging**: Hybrid retrieval requires combining BM25 scores with cosine similarity scores. Different fusion strategies (weighted sum, reciprocal rank fusion) affect final ranking. Quick check: How would you normalize scores from keyword (BM25) and embedding (cosine similarity) retrieval before combining them?

- **Multi-Stage Ranking (L1/L2)**: The architecture separates retrieval (L1) from ranking (L2). Understanding this separation is critical for debugging where relevance issues originate. Quick check: If top-5 relevance improves but engagement doesn't, which stage would you investigate first and why?

## Architecture Onboarding

- **Component map**: Query preprocessing → parallel dispatch to (Unicorn inverted index + SSR/Faiss vector index) → candidate merging → L2 multi-stage ranking → results → LLM evaluation pipeline

- **Critical path**: 1. Query enters preprocessing (tokenization, normalization, rewriting) 2. Parallel retrieval: keyword path (Unicorn) and embedding path (SSR encoder + Faiss ANN) 3. Candidate fusion with feature extraction (TF-IDF, BM25, cosine similarity) 4. L2 ranking with MTML supermodel (click/share/comment optimization) 5. Results returned; evaluation pipeline runs offline

- **Design tradeoffs**: Latency vs. relevance (EBR adds milliseconds); precision vs. diversity (keyword excels at precision, EBR adds semantic diversity); evaluation cost vs. iteration speed (LLM evaluation reduces human labeling but introduces model-dependent bias)

- **Failure signatures**: High skip rate (>5%): Indicates query-result pair formatting issues or LLM prompt failures; High error rate (>15%): Suggests prompt design problems or model limitations; Relevance improves but engagement flat: Ranking model may not be optimizing right signals; Latency spike after EBR enablement: Check ANN index size, caching efficiency, GPU inference bottlenecks

- **First 3 experiments**: 1. A/B test EBR-only vs. keyword-only vs. hybrid: Measure top-5 relevance rate, engagement metrics (click/share/comment), and p95 latency to validate hybrid advantage. 2. Vary fusion weights: Systematically adjust keyword vs. embedding score weights (e.g., 70/30, 50/50, 30/70) to find optimal balance for group-scoped queries. 3. LLM vs. human judgment correlation: Sample 200 query-result pairs, have human raters judge relevance, compare with LLM ratings to validate LLM-as-judge reliability before full deployment.

## Open Questions the Paper Calls Out

### Open Question 1
Can applying large language models directly in the ranking stage further increase result relevance? The authors explicitly identify "applying LLMs in the ranking stage to further increase the result relevance" as a key direction for future work. The current system utilizes LLMs strictly for offline evaluation (LLM-as-a-Judge) rather than for real-time ranking logic or candidate scoring. Deployment of an LLM-integrated ranking model showing improved relevance metrics compared to the current fine-tuned L2 model would resolve this.

### Open Question 2
What efficiency and relevance gains are achievable through LLM-driven adaptive retrieval strategies? The conclusion lists "using LLM-driven adaptive retrieval strategies" as a promising method to further enhance the system. The presented architecture relies on a static parallel retrieval of keywords and embeddings, lacking dynamic, query-dependent selection mechanisms. Comparative analysis of a dynamic system that adjusts retrieval pathways based on query intent versus the static blended baseline would provide evidence.

### Open Question 3
What specific benefits does the anticipated Multi-Task Multi-Label (MTML) supermodel provide over the current L2 architecture? Section 2.2 describes an "anticipated supermodel design" utilizing MTML components, while Figure 1 and the evaluation appear to rely on a "New fine-tuned L2 model" without explicitly confirming the supermodel's deployment. The paper does not quantify the incremental performance or maintenance improvements of the modular MTML architecture compared to the tested single-structure model. Ablation studies isolating the MTML architecture's performance and iteration speed against the current production L2 model would resolve this.

## Limitations
- Lack of direct online A/B testing validation - all improvements demonstrated through offline LLM evaluation rather than live user engagement metrics
- Sparse details on exact SSR model architecture (only 12-layer, 200M parameter specification provided), making exact reproduction challenging
- Focus on Facebook Group Scoped Search specifically, limiting generalizability to other search contexts or platforms

## Confidence
- **High Confidence**: Hybrid retrieval mechanism combining keyword and embedding retrieval is well-established in literature, and reported offline improvements (85.2% vs 84.7% top-5 relevance) are consistent with expected gains from such approaches
- **Medium Confidence**: LLM-as-judge evaluation framework shows promise with low error/skip rates, but without human judgment correlation data, reliability of these automated metrics remains partially unverified
- **Low Confidence**: Claims about multi-task optimization effectiveness are weakly supported, as neighbor corpus shows limited evidence for MTML in search ranking specifically, and engagement metrics beyond relevance rates are not reported

## Next Checks
1. **Human Judgment Correlation Study**: Sample 200 query-result pairs from hybrid system and have human annotators rate relevance using same criteria as LLM judge. Calculate Cohen's kappa or Pearson correlation to validate LLM-as-judge reliability before full deployment.

2. **Online A/B Testing**: Deploy hybrid retrieval system to 5% of Facebook Group search traffic with proper instrumentation. Measure not just relevance metrics but also click-through rate, dwell time, and engagement signals (shares/comments) to validate offline improvements translate to user behavior.

3. **Component Ablation Testing**: Run controlled experiments isolating keyword-only, embedding-only, and hybrid modes while holding all other variables constant. Measure marginal contribution of each component to both relevance metrics and latency to optimize the trade-off.