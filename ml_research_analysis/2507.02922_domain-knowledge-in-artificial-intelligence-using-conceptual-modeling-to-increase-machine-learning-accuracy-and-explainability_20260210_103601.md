---
ver: rpa2
title: 'Domain Knowledge in Artificial Intelligence: Using Conceptual Modeling to
  Increase Machine Learning Accuracy and Explainability'
arxiv_id: '2507.02922'
source_url: https://arxiv.org/abs/2507.02922
tags:
- data
- learning
- machine
- knowledge
- conceptual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using domain knowledge from conceptual models
  to improve machine learning (ML) performance and transparency. The authors developed
  the Conceptual Modeling for Machine Learning (CMML) method, which uses Extended
  Entity-Relationship (EER) constructs to guide data preparation for ML.
---

# Domain Knowledge in Artificial Intelligence: Using Conceptual Modeling to Increase Machine Learning Accuracy and Explainability

## Quick Facts
- arXiv ID: 2507.02922
- Source URL: https://arxiv.org/abs/2507.02922
- Reference count: 40
- Primary result: CMML improves ML performance (up to 24% RMSE reduction, 23.8% variance increase) and transparency

## Executive Summary
This paper proposes the Conceptual Modeling for Machine Learning (CMML) method, which leverages Extended Entity-Relationship (EER) conceptual models to improve machine learning model performance and transparency. The method provides five guidelines for data preparation that incorporate domain knowledge: feature labeling, deriving features, imputing features, summarizing entities, and creating multiple training datasets. Tested on two real-world foster care case management scenarios, CMML demonstrated significant improvements in model accuracy (23.8% increase in explained variance, 24-point reduction in RMSE) and classification performance (recall increased from 59.28% to 84%). A focus group with 15 data scientists validated the method's utility for improving transparency and preventing misrepresentation of domain knowledge.

## Method Summary
The CMML method uses Extended Entity-Relationship (EER) constructs to guide data preparation for machine learning. The five guidelines are: (1) Feature Labeling - prefix features with their originating entity names, (2) Deriving Features - create new features through computations based on domain knowledge, (3) Imputing Features - handle missing values with domain-aware logic, (4) Summarizing Entities - aggregate features from "many" side of 1:N relationships to prevent duplication, and (5) Creating Multiple Datasets - split data by entity subtypes. The method was evaluated using case studies on foster care data (predicting length of stay and psychotropic medication prescriptions) with various ML algorithms including Random Forest, GBM, LightGBM, Deep Learning, and AutoML.

## Key Results
- Length of stay prediction: 23.8% increase in explained variance and 24-point reduction in RMSE compared to baseline
- Psychotropic medication classification: Recall improved from 59.28% to 84%, with statistically significant increase in F-measure
- Focus group validation: 15 data scientists confirmed CMML provides structured approach to data preparation and improves transparency
- Domain experts could better interpret feature importance when labeled with entity origins

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entity summarization in one-to-many relationships improves ML model accuracy by eliminating training case duplication.
- Mechanism: When the target-bearing entity sits on the "one" side of a 1:N relationship, raw data flattening causes over-representation of repeated entity instances. By aggregating features from the "many" side (counts for categorical, mean/sum/min/max for numerical), each target instance contributes exactly once to training, reducing bias toward over-represented cases.
- Core assumption: The predictive signal lies at the granularity of the target-bearing entity, not the transactional level.
- Evidence anchors:
  - [Section 4.1.1] TDS1 (applying Guidelines 2 and 4) showed "average increase of 23.8% in explained variance across the various models and an average reduction in RMSE of 24 (7.4%)" compared to DS0.
  - [Section 3.4.2] Figure 5-7 demonstrate how Customer 101 with four orders is reduced from four rows to one with summarized features like ORDER_Count and Order_Average.
  - [Corpus] Weak direct corpus support; neighbor papers focus on physics-informed ML and neurosymbolic AI, not tabular data aggregation.
- Break condition: If predictive signal exists at transaction level (e.g., predicting individual order outcomes), summarization destroys relevant variance.

### Mechanism 2
- Claim: Domain-aware imputation prevents semantic errors that degrade model performance.
- Mechanism: Conceptual models encode whether an attribute is mandatory or optional for an entity subtype. Missing values for optional attributes (e.g., "surcharge" for LARGE_ORDER subtype) should not be imputed, as the null means "not applicable" not "unknown." Imputing would inject noise.
- Core assumption: The conceptual model accurately reflects real-world domain constraints.
- Evidence anchors:
  - [Section 3.4.4] "If the missing values are not imputed, this reduces the training sample size... However, if missing values are imputed, this creates noise."
  - [Section 4.1.1] Guideline 5 created TDS2-Younger and TDS3-Older datasets, with TDS2 showing improved performance across all models.
  - [Corpus] No direct corpus validation; this is domain-knowledge-specific.
- Break condition: If conceptual model is outdated or the data reflects undocumented exceptions to stated rules.

### Mechanism 3
- Claim: Feature labeling with entity origin improves process transparency without degrading performance.
- Mechanism: Prefixing features with entity names (e.g., PLACEMENT_OutcomeTrialHomeVisit) preserves data lineage through transformations. This enables auditors and practitioners to trace feature importance back to domain concepts.
- Core assumption: Transparency is valued and entity-level aggregation is meaningful to stakeholders.
- Evidence anchors:
  - [Section 4.1.1, Figure 10] Shows top features labeled with entity origin, revealing PLACEMENT as key predictor entity.
  - [Section 4.2] Focus group participants stated: "a regulator or auditor can ask for the data lineage, so traceability is important."
  - [Corpus] "Mutual Understanding between People and Systems via Neurosymbolic AI" paper discusses explicit symbolic representations aiding interpretability, loosely supportive.
- Break condition: When features are heavily transformed through PCA/embeddings such that entity origin becomes semantically ambiguous.

## Foundational Learning

- Concept: Extended Entity-Relationship (EER) cardinality
  - Why needed here: CMML's Guidelines 4 and 5 depend on correctly identifying 1:1, 1:N, N:M relationships and specialization hierarchies.
  - Quick check question: Given an ORDER and ORDER_LINE table where each order has multiple line items, which entity is on the "many" side?

- Concept: Target-bearing vs. predictor entities
  - Why needed here: Determines the unit of analysis for your training data and which entity requires summarization.
  - Quick check question: If predicting customer lifetime value from order history, which is the target-bearing entity?

- Concept: Derived vs. stored attributes
  - Why needed here: Guideline 2 requires identifying features that must be computed (not extracted directly) from available data.
  - Quick check question: Age is not stored but DOB is. Is "age" a derived attribute?

## Architecture Onboarding

- Component map: Raw data + EER model -> Apply CMML guidelines -> Training datasets with entity semantics
- Critical path:
  1. Identify target-bearing entity and target attribute
  2. Apply Guideline 1 (labeling) universally
  3. Check for 1:N relationships -> Guideline 4 (summarization)
  4. Check for derived attributes -> Guideline 2 (feature derivation)
  5. Check for missing values -> Guideline 3 (imputation logic)
  6. Check for specialization -> Guideline 5 (multiple datasets)

- Design tradeoffs:
  - Multiple TDSn for subtypes vs. single model: improves precision for homogeneous subgroups but increases model maintenance overhead
  - Aggressive summarization vs. granular features: reduces duplication but may discard predictive transaction-level patterns
  - Transparency vs. automation: CMML requires manual model review; AutoML tools may override guidelines

- Failure signatures:
  - RMSE increases after summarization -> signal may exist at transaction level, not entity level
  - Feature importance shows counterintuitive entities -> check labeling consistency
  - Model performance degrades on subtype-specific datasets -> insufficient samples in subtypes

- First 3 experiments:
  1. Replicate the paper's TDS1 transformation on your own 1:N relational data; compare RMSE/r² against flattened baseline.
  2. Apply Guideline 3 to a dataset with subtype-specific optional attributes; measure performance difference between domain-aware imputation vs. mean imputation.
  3. Run feature importance analysis with and without entity-label prefixes; validate that domain experts can interpret the labeled version faster.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can frameworks be developed where conceptual models both inform machine learning processes and evolve based on patterns discovered through ML?
- Basis in paper: [explicit] Section 5.2 (Future Work) states the need to "develop frameworks where conceptual models both inform machine learning processes and evolve based on patterns discovered through ML."
- Why unresolved: The current CMML method is unidirectional, using static conceptual models to prepare data, but does not address how insights from the trained models could update the original domain knowledge.
- What evidence would resolve it: A working prototype or methodology demonstrating a bidirectional flow where model outputs successfully trigger updates to the conceptual schema.

### Open Question 2
- Question: Can automated techniques be developed to extract conceptual models from existing databases to enable organizations without formal models to benefit from CMML?
- Basis in paper: [explicit] Section 5.2 (Future Work) calls for research to "develop automated techniques for extracting conceptual models from existing databases, enabling organizations without formal models to benefit from the performance improvements."
- Why unresolved: The current method relies on the assumption that an EER diagram is available, which excludes organizations with legacy systems or undocumented data structures.
- What evidence would resolve it: Algorithms that successfully reverse-engineer accurate EER diagrams from raw databases and subsequently improve ML performance when applied via CMML.

### Open Question 3
- Question: How should the specific CMML guidelines be prioritized or selected for different machine learning contexts?
- Basis in paper: [explicit] Section 5 (Discussion) notes that "since not all guidelines apply to a given situation, optimal guideline selection must be specific to each context... more work is required to consider how to prioritize the guidelines."
- Why unresolved: The paper provides a general set of five guidelines but lacks a decision matrix or heuristic for data scientists to determine which subset (e.g., summarizing vs. deriving features) to apply for a given dataset.
- What evidence would resolve it: A comparative study or heuristic model that correlates specific data characteristics (e.g., cardinality, missingness) with the optimal subset of CMML guidelines to maximize performance.

## Limitations
- Proprietary datasets prevent independent verification of claimed performance gains
- Method effectiveness depends on accuracy and completeness of conceptual model
- Subtype-specific dataset creation may not generalize well to domains with many overlapping subtypes
- Transparency benefits are domain-expert dependent and not empirically validated beyond focus group reports

## Confidence
- **High confidence**: Entity summarization in 1:N relationships improves performance when target-bearing entity sits on "one" side
- **Medium confidence**: Domain-aware imputation improves performance
- **Low confidence**: Transparency benefits are domain-expert dependent

## Next Checks
1. **Ablation study**: Apply CMML guidelines incrementally (G1→G2→G3→G4→G5) to quantify individual contribution of each guideline to performance improvements.
2. **Model-agnostic validation**: Test CMML on non-tabular data (time series, text embeddings) to assess generalizability beyond relational datasets.
3. **Conceptual model quality impact**: Intentionally introduce errors in the EER model and measure degradation in CMML-derived model performance to establish sensitivity thresholds.