---
ver: rpa2
title: 'ChessQA: Evaluating Large Language Models for Chess Understanding'
arxiv_id: '2510.23948'
source_url: https://arxiv.org/abs/2510.23948
tags:
- chess
- answer
- piece
- move
- position
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ChessQA introduces a comprehensive benchmark to evaluate large
  language models'' chess understanding across five progressively complex categories:
  Structural, Motifs, Short Tactics, Position Judgment, and Semantic. These categories
  mirror the ascending abstractions players master as they accumulate chess knowledge.'
---

# ChessQA: Evaluating Large Language Models for Chess Understanding

## Quick Facts
- **arXiv ID**: 2510.23948
- **Source URL**: https://arxiv.org/abs/2510.23948
- **Reference count**: 40
- **Primary result**: ChessQA benchmark reveals persistent LLM weaknesses in chess understanding across 5 categories, with explicit reasoning improving performance by +14.7 percentage points on average.

## Executive Summary
ChessQA introduces a comprehensive benchmark to evaluate large language models' chess understanding across five progressively complex categories: Structural, Motifs, Short Tactics, Position Judgment, and Semantic. These categories mirror the ascending abstractions players master as they accumulate chess knowledge. The benchmark provides a controlled, consistent setting for diagnosis and comparison, with parameterized difficulty and versioned datasets to evolve alongside improving models. Experiments with 15 contemporary LLMs reveal persistent weaknesses across all categories, with Short Tactics and Position Judgment being particularly challenging. Enabling explicit reasoning consistently improves performance by an average of +14.7 percentage points, and higher token usage correlates with better accuracy. Error analysis identifies four recurrent failure modes: board-state hallucination, legality mistakes, correct analysis with wrong final action, and false "no answer" conclusions.

## Method Summary
ChessQA evaluates LLMs on 50 chess understanding tasks spanning five categories, using FEN strings as inputs and exact-match or multiple-choice formats for outputs. The benchmark draws from public Lichess puzzles, evaluations, and ChessBase 17 commentary, with tasks generated through algorithms for each category. Models are evaluated via OpenRouter API with thinking-enabled (32K tokens) and non-thinking (8K tokens) configurations. Performance is measured by exact-match accuracy per category and overall, with token usage and cost tracked as secondary metrics. The benchmark's design emphasizes determinism (using UCI move format) and progressive difficulty to enable fine-grained capability assessment.

## Key Results
- LLMs show persistent weaknesses across all ChessQA categories, with Short Tactics (mean 17.4%) and Position Judgment being most challenging
- Explicit reasoning improves accuracy by +14.7 percentage points on average, with GPT-5 improving from 44.0% to 79.3%
- Higher token usage correlates positively with accuracy improvements across nearly all categories
- Board-state hallucination is identified as a primary upstream failure mode that propagates into downstream errors

## Why This Works (Mechanism)

### Mechanism 1: Explicit reasoning improves accuracy through extended computation
Additional tokens enable intermediate state tracking, self-correction, and multi-step simulation before committing to an answer. Evidence shows GPT-5 improved from 44.0% to 79.3% with thinking mode enabled, and higher token usage correlates positively with accuracy across categories.

### Mechanism 2: Board-state hallucination is a primary upstream failure
Models misread FEN strings, leading to faulty legal move generation or tactic calculation. Error analysis shows GPT-5* occasionally confuses piece positions or invents phantom pieces, and adding explicit piece-arrangement context improves accuracy significantly.

### Mechanism 3: Category performance follows a skill hierarchy
Tasks are not independent; solving Position Judgment correctly likely requires passing through accurate state tracking and tactical calculation. Even top models show structural correctness (97% for GPT-5*) but struggle with higher-level tasks like Position Judgment (40%).

## Foundational Learning

- **Concept**: FEN (Forsyth–Edwards Notation)
  - Why needed here: All benchmark inputs are FEN strings; models must parse piece placement, side to move, castling rights, en passant, and move counters to reason correctly.
  - Quick check question: Given FEN `rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1`, which side just moved?

- **Concept**: UCI (Universal Chess Interface) move format
  - Why needed here: Answers require deterministic move strings (e.g., `e2e4`, `e7e8q`) to avoid SAN ambiguity; exact-match scoring depends on correct formatting.
  - Quick check question: What UCI string represents a knight move from g1 to f3?

- **Concept**: Tactical motifs (pin, fork, skewer, battery)
  - Why needed here: The Motifs category tests recognition of recurring patterns without deep search; Structural correctness is prerequisite.
  - Quick check question: In a pin, which piece cannot move because it would expose the king?

## Architecture Onboarding

- **Component map**: FEN string -> Prompt constructor (adds exemplars, task instructions) -> LLM (via OpenRouter) -> Output extraction (regex for "FINAL ANSWER:") -> Exact-match comparison to gold -> Accuracy logging
- **Critical path**: 1) Generate task item (FEN + question + options + gold answer) 2) Format prompt with exemplars and "FINAL ANSWER:" requirement 3) Query model with appropriate token budget 4) Extract terminal line, canonicalize, compare to gold 5) Log accuracy, token usage, and error type
- **Design tradeoffs**: UCI vs SAN chosen for determinism at cost of human readability; difficulty calibration via puzzle rating and centipawn buckets; token budget balances accuracy vs cost
- **Failure signatures**: 1) Board-state hallucination: Wrong piece positions or phantom pieces 2) Legality mistakes: Proposing illegal moves 3) Correct analysis, wrong action: Sound intermediate steps but incorrect final move 4) False "no answer": Asserting no valid solution when one exists
- **First 3 experiments**: 1) Run GPT-5 with/without thinking mode on Structural subset to confirm reasoning scaling 2) Ablate prompt by adding explicit piece-arrangement text to quantify hallucination reduction 3) Evaluate Qwen3 8B on Short Tactics across difficulty levels to identify capability ceiling

## Open Questions the Paper Calls Out

### Open Question 1: Can LLMs achieve human-level token efficiency in chess reasoning?
The paper questions whether models need ~11,668 tokens per problem when humans can verbalize solutions much more efficiently. This remains unresolved as the paper shows accuracy improves with token usage but doesn't explore whether similar accuracy can be achieved with fewer tokens.

### Open Question 2: What interventions can reduce board-state hallucination errors?
While the paper documents board-state hallucination as a primary failure mode and shows explicit context helps, it offers no solution for improving intrinsic board-state parsing from FEN strings. This is unresolved as no architectural or training solutions are proposed.

### Open Question 3: How can LLMs improve at Position Judgment tasks requiring long-horizon planning?
Position Judgment accuracy remains stubbornly limited even among top models, demonstrating deficiency in implicit search and long-term planning. The paper establishes the weakness but doesn't propose methods to enhance multi-step lookahead or strategic evaluation capabilities.

## Limitations

- **Data provenance and replicability**: Code repository and dataset URLs not provided, requiring substantial engineering effort for faithful reproduction
- **Model and environment variability**: Model IDs, sampling parameters, and exact prompt templates not fully specified, affecting reproducibility
- **Ground-truth construction noise**: Position Judgment uses Stockfish evaluations with unclear bucketing criteria; Semantic questions rely on potentially subjective PGN comments

## Confidence

- **High confidence**: Category design hierarchy and relative difficulty ordering (Structural easiest, Short Tactics hardest)
- **Medium confidence**: Explicit reasoning token scaling effect and correlation with accuracy
- **Medium confidence**: Board-state hallucination as primary failure mode
- **Low confidence**: Hierarchy of skill dependency (Structural → Motifs → Short Tactics → Position Judgment)

## Next Checks

1. **Reasoning token scaling validation**: Run controlled experiment on 100-item Structural subset varying token budget (8K, 16K, 32K) to confirm accuracy gains plateau or continue scaling

2. **Board-state hallucination ablation**: Implement prompt variant explicitly listing all piece positions alongside FEN string for subset of tasks to quantify grounding impact

3. **Open-source model capability ceiling**: Evaluate Qwen3 8B on Short Tactics across three difficulty tiers to determine whether performance degrades gracefully or hits hard ceiling