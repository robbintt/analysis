---
ver: rpa2
title: Coordinated Humanoid Manipulation with Choice Policies
arxiv_id: '2512.25072'
source_url: https://arxiv.org/abs/2512.25072
tags:
- policy
- humanoid
- control
- choice
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a modular teleoperation system and a novel
  learning framework for coordinated humanoid manipulation. The teleoperation interface
  decomposes control into intuitive submodules (arm tracking, hand control, hand-eye
  coordination, and locomotion), enabling efficient data collection.
---

# Coordinated Humanoid Manipulation with Choice Policies

## Quick Facts
- arXiv ID: 2512.25072
- Source URL: https://arxiv.org/abs/2512.25072
- Authors: Haozhi Qi; Yen-Jen Wang; Toru Lin; Brent Yi; Yi Ma; Koushil Sreenath; Jitendra Malik
- Reference count: 40
- Primary result: Modular teleoperation and Choice Policy outperform diffusion policies and behavior cloning on dishwasher loading and whole-body loco-manipulation tasks.

## Executive Summary
This paper introduces a modular teleoperation system and a novel learning framework for coordinated humanoid manipulation. The teleoperation interface decomposes control into intuitive submodules (arm tracking, hand control, hand-eye coordination, and locomotion), enabling efficient data collection. The proposed Choice Policy generates multiple candidate actions and learns to score them, effectively capturing multimodal behaviors while maintaining fast inference. Experiments on dishwasher loading and whole-body loco-manipulation show that Choice Policy outperforms diffusion policies and behavior cloning, with hand-eye coordination proving critical for success in long-horizon tasks.

## Method Summary
The method combines a modular VR-based teleoperation interface with a Choice Policy learning framework. The teleoperation decomposes whole-body control into four submodules: arm tracking (on-demand IK via trigger), hand primitives (grouped finger control), hand-eye coordination (head tracking to maintain visual feedback), and locomotion (joystick-triggered RL policy). The Choice Policy architecture uses a frozen DINOv3 encoder for RGB, ResNet-18 for depth, and MLP for proprioception. It generates K action proposals (K×T×|A| output) and learns to score them via MSE regression. During training, only the lowest-error proposal receives gradients (winner-takes-all). At inference, the lowest-scoring proposal is executed.

## Key Results
- Choice Policy achieves 7/10 success rate on full dishwasher loading vs 2/10 for diffusion policy and 1/10 for behavior cloning
- Hand-eye coordination improves insertion success from 1/10 to 5/10 for diffusion policy and 2/10 to 7/10 for Choice Policy
- The approach maintains fast inference while capturing multimodal behaviors through K proposal generation
- Whole-body loco-manipulation shows Choice Policy outperforms baselines but still faces challenges in coupled locomotion-manipulation tasks

## Why This Works (Mechanism)

### Mechanism 1: Modular Teleoperation Decomposition
Decomposing whole-body humanoid control into discrete submodules (arm tracking, hand primitives, hand-eye coordination, locomotion) reduces operator cognitive load and enables higher-quality demonstration collection. A single VR controller is multiplexed across modes—trigger for arm end-effector delta pose, grip button for grouped finger actuation, joystick toggling between thumb control and locomotion commands. On-demand activation (arm control only when trigger pressed) prevents drift during sequential bimanual tasks.

### Mechanism 2: Choice Policy Winner-Takes-All Training
Generating K action proposals and training only the lowest-error proposal (winner-takes-all) prevents mode averaging while maintaining single-forward-pass inference. The action proposal network outputs K×T×|A| trajectories. During training, MSE is computed per proposal; only the best receives gradients. A parallel score network learns to predict this MSE. At inference, argmin over scores selects execution.

### Mechanism 3: Hand-Eye Coordination for Visual Feedback
Active head tracking of the manipulating hand maintains task-relevant visual feedback, which is critical for long-horizon manipulation where occlusions otherwise prevent successful completion. Button-triggered mode computes head yaw/pitch from displacement vector r = p_hand − p_head, pointing camera at active hand. This ensures the manipulation region stays in view during handover and insertion phases.

## Foundational Learning
- **Behavior Cloning with MSE Loss**: Choice Policy builds on BC architecture but modifies training to handle multimodality; understanding mode collapse from MSE averaging is prerequisite. Quick check: Given two equally valid action vectors [1, 0] and [-1, 0] for the same observation, what does MSE-minimizing BC predict?
- **Diffusion Policy Basics**: Primary baseline; understanding its denoising sampling clarifies why inference is slow and how Choice Policy trades off expressiveness for speed. Quick check: How many forward passes does Diffusion Policy require per action prediction, and what does Choice Policy use instead?
- **Multi-Choice Learning / Winner-Takes-All**: Core training paradigm; understanding why only the best proposal receives gradients is essential for implementing the loss correctly. Quick check: In winner-takes-all training with K=5 proposals, if proposal 2 has lowest MSE, which proposals contribute to the action loss gradient?

## Architecture Onboarding

- **Component map**: VR Controller → Mode Selector → Arm IK / Hand Mapper / Head Tracker / Locomotion Policy → Observation Encoder (DINOv3 frozen + ResNet-18 depth + MLP proprioception) → Feature Vector → Action Proposal Network (MLP → K×T×|A|) + Score Network (MLP → K) → argmin(score) → selected action sequence → Low-level controller (100 Hz RL policy)

- **Critical path**: Implement teleoperation interface (VR controller → robot commands via ROS2) → Collect demonstrations with hand-eye coordination enabled → Train Choice Policy with K≥5 proposals, winner-takes-all loss → Deploy with score-based selection; monitor which proposal is selected per phase

- **Design tradeoffs**: Higher K provides better multimodal coverage but increases compute/memory and potential overfitting; hand-eye heuristics vs. learned head policy trade simplicity for adaptability; action chunking horizon T balances motion smoothness against reactivity

- **Failure signatures**: All proposals have similar scores → score network not learning; check score loss convergence; single proposal always selected → insufficient mode diversity; increase K or check data multimodality; handover/insertion fails despite pickup success → likely hand-eye coordination issue; verify head camera visibility; locomotion instability during manipulation → check low-level RL policy tuning and mode-switch timing

- **First 3 experiments**: Replicate dishwasher pickup-only task with K=1 (equivalent to BC) vs K=5 to verify winner-takes-all benefit on a short subtask; ablate hand-eye coordination: run full task with fixed head orientation, quantify insertion success drop; visualize proposal selection across rollout phases to confirm head specialization emerges without explicit supervision

## Open Questions the Paper Calls Out

### Open Question 1
Can a learned hand-eye coordination mechanism outperform the current heuristic algorithm in unstructured environments? The authors identify that their "current hand-eye coordination relies on a heuristic algorithm" and suggest "developing a more adaptive or learned mechanism could further enhance performance." This remains unresolved because heuristic tracking may fail when visual occlusion is severe or when predictive gaze shifts are required before hand movement. Comparative experiments showing a learned policy maintaining better visual tracking and task success rates than the fixed geometric approach would resolve this.

### Open Question 2
How can the visual perception system be augmented to improve generalization to substantially different scenes and objects? The paper notes the "visual perception component has limited generalization to substantially different scenes and objects" and suggests exploring "diverse training data or pre-training." This remains unresolved because the current model, trained on limited demonstrations, struggles with OOD variations (e.g., Color OOD), limiting deployment versatility. Demonstrations of successful manipulation on unseen object geometries and textures without collecting new task-specific demonstrations would resolve this.

### Open Question 3
How can the coupling of locomotion and manipulation be stabilized to reduce the failure rates observed in long-horizon loco-manipulation tasks? Table IV shows low success rates (2/5) for the whiteboard wiping task, and the text concludes that "achieving robust end-to-end success... remains an open challenge." This remains unresolved because errors in locomotion (e.g., drifting from the target) propagate to the manipulation phase, causing failures that the policy cannot correct. A study showing increased success rates in whole-body tasks through tight integration of lower-body stability control with upper-body action planning would resolve this.

## Limitations
- Hyperparameters including action horizon T, MLP layer sizes, learning rates, batch sizes, and training epochs are not specified
- Action execution details such as low-level controller specifics (PD gains, joint target tracking) are not fully described
- Task-specific assumptions limit hand-eye coordination to simple geometric heuristics that may not generalize to complex scenarios

## Confidence
- **High confidence**: Modular teleoperation design effectively reduces operator cognitive load, as evidenced by smooth performance after minimal practice and successful task completion with hand-eye coordination
- **Medium confidence**: Choice Policy's winner-takes-all training successfully addresses mode averaging in BC, but reliance on score network accuracy under distribution shift remains uncertain
- **Medium confidence**: Hand-eye coordination is critical for long-horizon tasks, but simple heuristic may fail in more complex scenarios requiring adaptive gaze control

## Next Checks
1. Replicate with minimal proposals: Run dishwasher pickup-only task with K=1 (BC equivalent) vs. K=5 to empirically verify winner-takes-all improves performance on a short subtask
2. Ablate hand-eye coordination: Execute full dishwasher task with fixed head orientation and quantify insertion success rate drop compared to coordinated head tracking
3. Analyze proposal selection: Visualize which of the K proposals is selected across different rollout phases to confirm that head specialization emerges without explicit supervision, as suggested by Figure 6