---
ver: rpa2
title: Investigating Batch Inference in a Sequential Monte Carlo Framework for Neural
  Networks
arxiv_id: '2601.21983'
source_url: https://arxiv.org/abs/2601.21983
tags:
- data
- monte
- carlo
- gradient
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes data annealing (DA) schemes to reduce the computational
  cost of Sequential Monte Carlo (SMC) samplers when inferring posterior distributions
  over neural network weights. Traditional SMC uses full-batch likelihood evaluations,
  which become expensive for large datasets and high-dimensional models.
---

# Investigating Batch Inference in a Sequential Monte Carlo Framework for Neural Networks

## Quick Facts
- arXiv ID: 2601.21983
- Source URL: https://arxiv.org/abs/2601.21983
- Authors: Andrew Millard; Joshua Murphy; Peter Green; Simon Maskell
- Reference count: 0
- Primary result: DA schemes achieve up to 6× faster training while maintaining similar accuracy to full-batch methods

## Executive Summary
This paper introduces data annealing (DA) schemes to reduce the computational cost of Sequential Monte Carlo (SMC) samplers for neural network inference. Traditional SMC requires full-batch likelihood evaluations, which become prohibitively expensive for large datasets. The authors propose five naive DA schedules and one smooth DA method that incrementally increase data subset sizes during inference. Experiments demonstrate significant runtime improvements while maintaining comparable accuracy to full-batch approaches, with the constant-to-refine scheme showing particularly strong performance.

## Method Summary
The paper proposes data annealing schemes that modify the traditional SMC framework by using stochastic gradient estimates from progressively larger data subsets rather than full-batch evaluations. The SMC sampler maintains a population of particles representing weight samples, with each iteration involving proposal generation and weight updates based on likelihood ratios. DA schemes vary the data subset size across iterations: constant DA uses fixed-size subsets throughout, linear DA increases subset size linearly, and smooth DA (SDA) provides a principled framework for incremental data subset growth. The automated DA (ADA) scheme adapts step sizes based on acceptance rates. The framework is tested with both Hamiltonian Monte Carlo (HMC) and Langevin dynamics proposals.

## Key Results
- DA schemes achieve up to 6× faster training compared to full-batch SMC
- Constant-to-refine (CTR) scheme matches full-batch accuracy with significantly reduced runtime
- HMC proposals outperform Langevin dynamics across all DA schemes
- Linear and smooth DA methods provide consistent performance improvements

## Why This Works (Mechanism)
Data annealing works by leveraging the fact that early SMC iterations can use smaller data subsets for proposal generation and weight updates without sacrificing convergence quality. The stochastic gradients from smaller batches provide sufficient information for exploration in the early stages when particles are far from the posterior mode. As iterations progress, larger subsets provide more accurate likelihood estimates, refining the particle population closer to the true posterior. The smooth DA framework provides a principled way to schedule this progression, ensuring that computational savings are achieved without compromising inference quality.

## Foundational Learning

1. Sequential Monte Carlo basics
   - Why needed: Understanding SMC particle propagation and weight updates is crucial for grasping how DA modifies the inference process
   - Quick check: Can explain how particles are resampled and propagated through iterations

2. Stochastic gradient MCMC
   - Why needed: DA schemes rely on gradient estimates from mini-batches rather than full datasets
   - Quick check: Can contrast full-batch vs mini-batch gradient computation

3. Annealing schedules in MCMC
   - Why needed: DA extends traditional tempering approaches by annealing data subset size instead of temperature
   - Quick check: Can describe how annealing affects exploration vs exploitation trade-off

## Architecture Onboarding

Component map: Data → DA scheduler → SMC sampler → Particles → Proposals (HMC/Langevin) → Weight updates

Critical path: Data subset selection → Stochastic gradient computation → Proposal generation → Weight calculation → Particle resampling

Design tradeoffs: Computational efficiency vs inference accuracy, exploration vs exploitation in early iterations, complexity of scheduling vs performance gains

Failure signatures: Poor convergence with aggressive DA schedules, high variance in weight estimates with too-small subsets, computational overhead from frequent resampling

First experiments:
1. Implement CTR scheme on MNIST with LeNet-5 architecture
2. Compare linear vs smooth DA on FashionMNIST with CNN
3. Test ADA scheme with different initial step sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments limited to MNIST and FashionMNIST with relatively simple architectures
- Evaluation focuses primarily on accuracy rather than uncertainty calibration
- ADA scheme relies on heuristic step size adaptation without theoretical guarantees
- Limited comparison with other scalable Bayesian inference methods

## Confidence
- High confidence in computational efficiency improvements and runtime comparisons
- Medium confidence in relative performance rankings between different DA schemes
- Medium confidence in generalizability to larger-scale problems
- Low confidence in theoretical justification for scheme performance differences

## Next Checks
1. Test DA schemes on larger-scale image datasets (CIFAR-10/100) and deeper architectures (ResNets)
2. Conduct systematic ablation studies comparing different proposal distributions beyond HMC and Langevin dynamics
3. Evaluate posterior predictive performance and uncertainty calibration metrics (expected calibration error, log-likelihood) in addition to accuracy