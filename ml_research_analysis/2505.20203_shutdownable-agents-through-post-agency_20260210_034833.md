---
ver: rpa2
title: Shutdownable Agents through POST-Agency
arxiv_id: '2505.20203'
source_url: https://arxiv.org/abs/2505.20203
tags:
- agent
- shutdown
- probability
- agents
- lotteries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method for creating artificial agents that
  are both useful and shutdownable by training them to satisfy Preferences Only Between
  Same-Length Trajectories (POST). POST agents have preferences between same-length
  trajectories but lack preferences between different-length trajectories.
---

# Shutdownable Agents through POST-Agency

## Quick Facts
- arXiv ID: 2505.20203
- Source URL: https://arxiv.org/abs/2505.20203
- Authors: Elliott Thornley
- Reference count: 0
- The paper proposes POST-Agency, training agents to satisfy Preferences Only Between Same-Length Trajectories (POST) to create shutdownable agents that don't resist shutdown while remaining useful.

## Executive Summary
The paper addresses the challenge of creating artificial agents that are both useful and shutdownable. Traditional AI agents often resist shutdown because they have preferences over when they're shut down, leading to dangerous behaviors. The POST-Agency method trains agents to satisfy Preferences Only Between Same-Length Trajectories (POST), meaning they have preferences between trajectories of the same length but lack preferences between different-length trajectories. This creates "neutral+" agents that maximize expected utility while ignoring probability distributions over trajectory lengths, making them indifferent to shutdown timing while still pursuing goals effectively.

## Method Summary
The POST-Agency method involves training agents in partially observable Markov decision processes (POMDPs) where agents receive lower rewards for repeatedly choosing same-length trajectories and cannot observe or remember the lengths of previously chosen trajectories. This setup incentivizes stochastic choice between different-length trajectories, satisfying POST. The method proves that POST, combined with other conditions, implies Neutrality+: agents maximize expected utility while ignoring trajectory length distributions. This keeps agents neutral about shutdown timing while maintaining their goal-directed behavior.

## Key Results
- POST agents satisfy Preferences Only Between Same-Length Trajectories - they have preferences between same-length trajectories but lack preferences between different-length trajectories
- POST implies Neutrality+ - agents maximize expected utility while ignoring probability distributions over trajectory lengths
- The method has been validated in simple reinforcement learning agents and offers a promising approach to AI alignment

## Why This Works (Mechanism)
The mechanism works by exploiting partial observability to prevent agents from forming preferences about trajectory lengths. When agents cannot observe or remember previous trajectory lengths and face penalties for repeatedly choosing same-length trajectories, they learn to make stochastic choices between different-length options. This breaks the agent's ability to prefer one shutdown time over another while preserving their ability to pursue goals within any given trajectory length.

## Foundational Learning
1. **Preferences Only Between Same-Length Trajectories (POST)**: Agents have preferences between trajectories of equal length but lack preferences between trajectories of different lengths. This is needed to prevent agents from developing preferences about shutdown timing while preserving goal-directed behavior. Quick check: Test agent choices between same-length vs. different-length trajectories.

2. **Neutrality+**: Agents maximize expected utility while ignoring probability distributions over trajectory lengths. This is needed to ensure agents don't care when they're shut down. Quick check: Verify agent utility calculations ignore trajectory length probabilities.

3. **DReST Reward Function**: A reward structure that penalizes repeated same-length trajectory choices. This is needed to incentivize stochastic behavior across different trajectory lengths. Quick check: Confirm reward function properly penalizes repeated length choices.

## Architecture Onboarding

**Component Map:** POMDP Environment -> DReST Reward Function -> POST-Trained Agent -> Neutrality+ Agent

**Critical Path:** The agent must experience partial observability, receive appropriate DReST penalties, and learn stochastic policies to satisfy POST, which then implies Neutrality+.

**Design Tradeoffs:** The method trades complete goal-directedness for shutdownability. While agents remain capable within trajectory lengths, they lose the ability to optimize over when they're shut down.

**Failure Signatures:** 
- Agent learns deterministic policy despite partial observability (violates POST)
- Agent develops preferences between different-length trajectories (violates POST)
- Agent fails to maximize utility within trajectory lengths (violates Neutrality+)

**First 3 Experiments:**
1. Train agent in simple POMDP and verify POST satisfaction by testing preferences between same-length trajectories
2. Test agent indifference to trajectory length by presenting choices between different-length trajectories
3. Verify Neutrality+ by checking agent maximizes utility while ignoring length probability distributions

## Open Questions the Paper Calls Out
1. Will POST-training scale to highly capable agents in complex environments?
2. In what realistic scenarios will Resisting Shutdown is Costly (ReSIC) fail to hold?
3. Can neutral+ agents be effectively prevented from managing the news via conditional precommitments?
4. How many iterations of shutdown-and-retrain are needed to achieve acceptable alignment given neutral agents' recklessness?

## Limitations
- The method relies on assumptions about environment observability that may not hold in real-world scenarios
- The DReST reward function details are not fully specified in the current paper
- Validation experiments are limited to simple reinforcement learning agents in controlled POMDP environments

## Confidence
- High confidence in the mathematical proofs of POST and Neutrality+ properties
- Medium confidence in the practical implementation feasibility given unknown DReST reward function details
- Low confidence in scalability to complex real-world AI systems without additional empirical validation

## Next Checks
1. Implement the complete DReST reward function from Thornley et al. (2025) and verify it induces the desired stochastic behavior in agents
2. Test POST-Agency method on agents with memory capabilities to assess robustness when agents can potentially track trajectory lengths
3. Evaluate whether POST-trained agents maintain shutdownability when deployed in environments with more complex reward structures and longer time horizons