---
ver: rpa2
title: Can Emotion Fool Anti-spoofing?
arxiv_id: '2505.23962'
source_url: https://arxiv.org/abs/2505.23962
tags:
- speech
- anti-spoofing
- emotional
- emotion
- emospoof-tts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the vulnerability of anti-spoofing systems
  to emotionally expressive synthetic speech. It introduces EmoSpoof-TTS, a dataset
  of over 29 hours of emotional TTS samples across four emotions and three modern
  TTS models, and shows that existing anti-spoofing models perform poorly on emotional
  speech compared to neutral speech, with inconsistent performance across emotions.
---

# Can Emotion Fool Anti-spoofing?

## Quick Facts
- **arXiv ID**: 2505.23962
- **Source URL**: https://arxiv.org/abs/2505.23962
- **Reference count**: 0
- **Key result**: Introduces EmoSpoof-TTS dataset and GEM gated ensemble method, significantly improving anti-spoofing accuracy on emotional synthetic speech (e.g., EER drops from 44.03% to 10.36% for HAS on StyleTTS2).

## Executive Summary
This work addresses a critical vulnerability in anti-spoofing systems: their susceptibility to emotionally expressive synthetic speech. The authors introduce EmoSpoof-TTS, a dataset of over 29 hours of emotional TTS samples across four emotions and three modern TTS models, revealing that existing anti-spoofing models perform significantly worse on emotional speech compared to neutral speech. To address this, they propose GEM (Gated Ensemble Method), which combines emotion-specialized anti-spoofing models with a speech emotion recognition gating network, substantially improving detection accuracy on emotional speech while maintaining performance on neutral speech and reducing emotion-specific disparities.

## Method Summary
The study introduces EmoSpoof-TTS, a dataset containing 36K spoofed and 12K bona-fide samples across four emotions, ten speakers, and three TTS models (StyleTTS2, F5-TTS, CosyVoice). The baseline model uses pre-trained RawNet2, which is fine-tuned on EmoSpoof-TTS to create Emo-RawNet2. Four emotion-specialized models are then created by further fine-tuning Emo-RawNet2 on emotion-specific subsets. The proposed GEM method combines these emotion-specialized models using a gating mechanism based on speech emotion recognition probabilities. The ensemble produces final predictions through weighted averaging, where weights are determined by the confidence of emotion classification.

## Key Results
- Standard anti-spoofing models show significantly degraded performance on emotional speech compared to neutral speech
- GEM reduces Equal Error Rate (EER) substantially, e.g., from 44.03% to 10.36% for HAS on StyleTTS2
- The ensemble method maintains strong performance on neutral speech while improving emotional speech detection
- GEM reduces emotion-specific performance disparities across different emotional states

## Why This Works (Mechanism)
Anti-spoofing systems are typically trained on neutral speech, making them sensitive to the prosodic and spectral variations introduced by emotional expression in synthetic speech. By creating emotion-specialized models and using a gating mechanism to route inputs based on detected emotion, GEM can leverage models that are particularly attuned to the acoustic characteristics of each emotional state, thereby improving overall detection accuracy.

## Foundational Learning
- **Anti-spoofing**: Detection of synthetic or spoofed speech to protect biometric systems. Why needed: Core security application being addressed. Quick check: Model correctly classifies known synthetic vs. real samples.
- **Speech Emotion Recognition (SER)**: Classification of emotional content in speech. Why needed: Provides gating mechanism for emotion-specialized models. Quick check: SER achieves >80% accuracy on emotional test set.
- **Gated Ensemble**: Method combining multiple models using a gating network to route inputs. Why needed: Enables dynamic selection of appropriate emotion-specific anti-spoofing models. Quick check: Ensemble outperforms individual emotion models.
- **Equal Error Rate (EER)**: Metric where false acceptance rate equals false rejection rate. Why needed: Standard evaluation metric for anti-spoofing systems. Quick check: EER decreases after implementing GEM.
- **Fine-tuning**: Adapting pre-trained models to new domains or tasks. Why needed: Creates emotion-specialized models from baseline anti-spoofing architecture. Quick check: Fine-tuned models show improved performance on target emotion subset.

## Architecture Onboarding

**Component Map**: EmoSpoof-TTS → RawNet2 (baseline) → Emo-RawNet2 (fine-tuned) → 4 × Emotion-Specialized Models → GEM (gating + weighted averaging)

**Critical Path**: Input speech → Speech Emotion Recognition → Emotion probability vector → Weighted combination of emotion-specialized anti-spoofing scores → Final anti-spoofing decision

**Design Tradeoffs**: The method trades increased model complexity and inference time (multiple models + SER) for significantly improved accuracy on emotional speech. The gating mechanism adds robustness but introduces dependency on SER accuracy.

**Failure Signatures**: Poor performance when SER misclassifies emotions, leading to incorrect model weighting. Degraded performance on emotions with insufficient training data or underrepresented in the SER training set.

**First Experiments**:
1. Evaluate baseline RawNet2 on EmoSpoof-TTS test set to establish performance degradation on emotional speech
2. Train and evaluate SER model on MSP-Podcast and test on EmoSpoof-TTS to verify emotion detection capability
3. Implement GEM with pre-trained emotion-specialized models and compare EER against baseline and individual emotion models

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does GEM maintain performance advantages when applied to other state-of-the-art anti-spoofing architectures besides RawNet2?
- Basis in paper: The study relies exclusively on RawNet2, leaving efficacy on other architectures unverified.
- Why unresolved: Different architectures utilize distinct feature extraction methods which may interact differently with emotional variations.
- What evidence would resolve it: Replicating experiments using alternative anti-spoofing backbones and comparing relative performance gains.

### Open Question 2
- Question: How robust is the system when the SER gating module misclassifies emotion?
- Basis in paper: The framework relies on SER probabilities for weighting, but doesn't analyze SER inaccuracy impact.
- Why unresolved: SER errors could degrade performance below that of a standard single model.
- What evidence would resolve it: Ablation study measuring EER on samples where SER prediction disagrees with ground truth.

### Open Question 3
- Question: Can resilience to emotion-targeted attacks be maintained across wider variety of emotional states and unseen TTS architectures?
- Basis in paper: The authors explicitly state this as future work, noting current limitations to four emotions and three TTS models.
- Why unresolved: Current study is limited to discrete emotions and specific synthesis algorithms.
- What evidence would resolve it: Evaluating GEM on extended dataset with broader emotion taxonomy and newly released synthesis algorithms.

## Limitations
- The EmoSpoof-TTS dataset was not publicly available at time of review, preventing independent verification
- Critical SER model architecture and training details are only referenced through citation [26]
- Fine-tuning procedures lack specific optimizer details, though Adam is assumed based on standard practice
- Performance improvements are based on specific train/test splits and may not generalize across different partitions

## Confidence
- **High Confidence**: Core observation that emotional synthetic speech degrades anti-spoofing performance; general framework of using emotion-specialized models with gating
- **Medium Confidence**: Specific EER improvements reported for GEM based on described methodology and problem setup
- **Low Confidence**: Claims about relative performance across all emotion-TTS combinations and complete elimination of emotion-specific disparities

## Next Checks
1. **Dataset Availability and Integrity Check**: Obtain and verify EmoSpoof-TTS dataset, confirming exact composition of train/test splits, speaker assignments, and TTS model distributions
2. **SER Model Implementation and Accuracy**: Implement SER model referenced in [26], train on MSP-Podcast, evaluate emotion classification accuracy on EmoSpoof-TTS test set
3. **GEM Ablation on Train/Test Split**: Compare standard Emo-RawNet2 against GEM using exact train+val to test split, verifying reported EER improvements are reproducible