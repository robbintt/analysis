---
ver: rpa2
title: 'LLM Agents at the Roundtable: A Multi-Perspective and Dialectical Reasoning
  Framework for Essay Scoring'
arxiv_id: '2509.14834'
source_url: https://arxiv.org/abs/2509.14834
tags:
- essay
- score
- evaluation
- prompt
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes RES, a multi-agent framework for zero-shot automated
  essay scoring. RES simulates a roundtable discussion where LLM-based evaluator agents,
  each with distinct perspectives, independently generate trait-based rubrics and
  evaluate essays.
---

# LLM Agents at the Roundtable: A Multi-Perspective and Dialectical Reasoning Framework for Essay Scoring

## Quick Facts
- **arXiv ID:** 2509.14834
- **Source URL:** https://arxiv.org/abs/2509.14834
- **Authors:** Jinhee Jang; Ayoung Moon; Minkyoung Jung; YoungBin Kim; Seung Jin Lee
- **Reference count:** 29
- **Primary result:** RES achieves up to 34.86% improvement in average QWK over straightforward prompting methods on ASAP dataset

## Executive Summary
This paper introduces RES, a zero-shot automated essay scoring framework that simulates a roundtable discussion among specialized LLM-based evaluator agents. Each agent adopts a distinct persona (e.g., grammar expert, content expert) to generate trait-based rubrics and evaluate essays independently. Through dialectical reasoning, agents collaboratively synthesize their evaluations into a final holistic score. Experiments on the ASAP dataset using ChatGPT and Claude demonstrate superior alignment with human scoring compared to straightforward prompting methods, achieving up to 34.86% improvement in average QWK.

## Method Summary
RES implements a two-stage zero-shot AES framework using 4 LLM-based evaluator agents with 12 total traits (3 per agent). First, evaluator personas are generated based on essay context and student proficiency, then each agent independently constructs trait-based rubrics and performs rationale-first scoring. Second, a dialectical reasoning process simulates roundtable discussion among personas, with a moderator agent synthesizing the deliberations into a final holistic score. The framework uses GPT-4.1-mini and Claude-3.5-haiku without requiring training data, operating entirely through prompt engineering.

## Key Results
- RES achieves 34.86% improvement in average QWK over Vanilla baseline when using Claude-3.5-haiku
- Dialectical reasoning contributes a 10% relative improvement in QWK compared to simple averaging of scores
- Agent count shows diminishing returns: 11.8% improvement from 1→3 agents vs 8.7% for 3→5 agents

## Why This Works (Mechanism)

### Mechanism 1: Specialized Persona-Based Evaluation
Assigning distinct evaluator personas to LLM agents improves scoring alignment with human judgment by preventing evaluation criteria from becoming too lenient, strict, or misaligned with student proficiency. Each persona independently constructs trait-based rubrics calibrated to the essay's context, maintaining consistent persona-specific evaluation criteria throughout the process.

### Mechanism 2: Rationale-First Scoring
Requiring evaluators to generate explicit justifications before assigning trait scores improves score quality and enables more productive dialectical reasoning. Rationale generation forces the model to externalize its evaluation criteria and reasoning, which becomes the foundation for subsequent dialectical discussion and makes disagreements tractable.

### Mechanism 3: Dialectical Synthesis
Simulated roundtable discussion among evaluator personas, followed by moderator synthesis, produces more human-aligned holistic scores than aggregating independent evaluations. The dialectical process surfaces conflicts between perspectives, enabling agents to critique each other's blind spots, while the moderator integrates these deliberations into a consensus score that balances competing considerations.

## Foundational Learning

- **Concept: Quadratic Weighted Kappa (QWK)**
  - Why needed here: QWK is the evaluation metric used throughout the paper to measure inter-rater agreement on ordinal scales, penalizing large disagreements more heavily than small ones.
  - Quick check question: If a model predicts scores [1, 2, 3] and human scores are [2, 3, 4], would QWK be higher or lower than if predictions were [1, 1, 3]?

- **Concept: Zero-Shot Evaluation**
  - Why needed here: The entire RES framework operates without fine-tuning or labeled training data, requiring understanding of how LLMs can be prompted to perform tasks without gradient updates.
  - Quick check question: How does zero-shot AES differ from supervised AES in terms of data requirements and generalization to new prompts?

- **Concept: Multi-Agent LLM Architectures**
  - Why needed here: RES implements a specific multi-agent pattern (parallel evaluation → dialectical synthesis), requiring understanding of the broader space of multi-agent designs.
  - Quick check question: What's the difference between multi-agent voting (aggregate predictions) and multi-agent dialectical reasoning (synthesize reasoning)?

## Architecture Onboarding

- **Component map:**
  Input -> Persona Creator -> Rubric Constructor (N parallel) -> Rationale-First Evaluator (N parallel) -> Dialogue Simulator -> Moderator -> Output

- **Critical path:**
  1. Persona creation must generate context-appropriate personas (wrong grade level → misaligned rubrics)
  2. Rubric traits must be non-redundant across personas (redundancy → wasted computation)
  3. Rationale quality determines dialectical reasoning quality (weak rationales → superficial discussion)
  4. Moderator prompt must enforce genuine synthesis, not averaging (averaging → loses dialectical benefit)

- **Design tradeoffs:**
  - Agent count vs. efficiency: 3→5 agents yields diminishing returns (8.7% vs 11.8% improvement). Paper uses 4 agents as default. Cost increases linearly.
  - Trait count vs. noise: 4→12 traits: +22.9% improvement; 12→20 traits: +2.5% improvement. Paper uses 3 traits per agent (12 total).
  - API models vs. open-source: Paper uses GPT-4.1-mini and Claude-3.5-haiku. Open-source alternatives showed weaker instruction-following.

- **Failure signatures:**
  - Persona collapse: All agents generate similar rubrics → no diversity benefit
  - Rationale-score misalignment: Rationales don't justify assigned scores → dialectical reasoning degrades
  - Moderator shortcut: Moderator averages scores instead of synthesizing reasoning → loses dialectical benefit
  - Format parsing failures: JSON output parsing errors → pipeline breaks

- **First 3 experiments:**
  1. Reproduce ablation on agent count (N=1,3,5) on 1-2 ASAP prompts to validate diminishing-returns curve
  2. Test dialectical reasoning removal: compare RES with DR vs. RES without DR (simple averaging)
  3. Stress test on edge cases: run RES on prompts with unusual score ranges or genres

## Open Questions the Paper Calls Out
- Can RES maintain comparable scoring performance when deployed on open-weight LLMs with weaker instruction-following capabilities?
- How can the multi-agent dialectical reasoning framework be extended to generate pedagogically valuable feedback rather than only holistic scores?
- Does RES generalize to essay scoring datasets beyond ASAP, particularly across different languages, educational levels, and scoring rubrics?
- What is the optimal configuration of evaluator agent count and trait granularity that maximizes scoring accuracy relative to computational cost?

## Limitations
- Performance with open-source LLMs remains untested, with preliminary results showing weaker instruction-following capabilities
- Framework focuses exclusively on score prediction rather than generating pedagogically valuable feedback
- Evaluation limited to ASAP dataset (US grades 7-10, English-only) without cross-dataset validation

## Confidence
- **High confidence:** Ablation studies on agent count and trait count effects are methodologically sound
- **Medium confidence:** Core mechanism claims rely on assumptions about LLM behavior that aren't directly validated
- **Low confidence:** Generalization claims beyond ASAP dataset are not supported by experimental evidence

## Next Checks
1. **Persona consistency validation:** Run RES on 10 essays with 3 different agent configurations (same personas, shuffled assignments). Check if scores vary significantly (>0.2 QWK).
2. **Dialectical reasoning depth test:** Manually inspect 5 dialectical discussion outputs to verify genuine multi-perspective deliberation rather than single agent dominance.
3. **Cross-dataset generalization:** Test RES on a different AES dataset (e.g., Kaggle's Kaggle-Graded-Essay) to validate 34.86% improvement generalizes beyond ASAP.