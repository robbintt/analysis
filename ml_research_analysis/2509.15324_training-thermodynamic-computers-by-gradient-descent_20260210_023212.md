---
ver: rpa2
title: Training thermodynamic computers by gradient descent
arxiv_id: '2509.15324'
source_url: https://arxiv.org/abs/2509.15324
tags:
- computer
- thermodynamic
- neural
- network
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates how to train a thermodynamic computer using
  gradient descent to perform computations at specified observation times, enabling
  it to mimic the behavior of a trained neural network. The method involves adjusting
  the parameters of a thermodynamic computer to maximize the probability of generating
  idealized dynamical trajectories that reproduce the activations of a reference neural
  network.
---

# Training thermodynamic computers by gradient descent

## Quick Facts
- arXiv ID: 2509.15324
- Source URL: https://arxiv.org/abs/2509.15324
- Authors: Stephen Whitelam
- Reference count: 0
- Primary result: Gradient descent successfully trains thermodynamic computers to perform computations at specified times, achieving 92.0% MNIST accuracy with theoretical 10^7 energy advantage

## Executive Summary
This paper introduces a method to train thermodynamic computers using gradient descent to perform computations at specified observation times. The approach treats the thermodynamic computer as a dynamical system whose parameters (couplings, biases, and input signals) can be optimized to maximize the probability of generating trajectories that reproduce the activations of a trained neural network. Applied to an MNIST image classification task with 64 hidden nodes and 10 output nodes, the method achieves 92.0% test-set accuracy while estimating a thermodynamic advantage of over 10^7 compared to digital implementations.

The training framework uses a teacher-student approach where a neural network serves as a reference model, and the thermodynamic computer is trained to replicate its behavior through gradient descent optimization of the Onsager-Machlup functional. This establishes gradient descent as a practical training method for thermodynamic computing, potentially enabling energy-efficient machine learning in thermodynamic hardware that operates out of equilibrium.

## Method Summary
The method trains thermodynamic computers by optimizing their parameters to maximize the probability of generating idealized dynamical trajectories that match neural network activations. The optimization uses gradient descent on the Onsager-Machlup functional, which measures the probability of trajectories in non-equilibrium systems. The training assumes access to idealized trajectories that exactly reproduce neural network activations, and computes gradients through these trajectories to update system parameters including couplings, biases, and input signals. The approach is demonstrated on an MNIST classification task using a thermodynamic computer with 64 hidden nodes and 10 output nodes.

## Key Results
- Achieves 92.0% test-set accuracy on MNIST image classification
- Demonstrates successful gradient descent training of thermodynamic computers
- Estimates thermodynamic advantage of over 10^7 compared to digital implementations
- Shows feasibility of training out-of-equilibrium thermodynamic systems to mimic neural network behavior

## Why This Works (Mechanism)
The method works by treating thermodynamic computers as dynamical systems whose parameters can be optimized to maximize the probability of generating specific trajectories. By using gradient descent on the Onsager-Machlup functional, which quantifies the likelihood of trajectories in non-equilibrium systems, the parameters are adjusted to make the system more likely to follow paths that reproduce neural network activations. The teacher-student framework provides a reference model (the neural network) against which the thermodynamic computer's behavior can be compared and optimized. The gradient computation through the Onsager-Machlup functional allows systematic parameter updates that improve the system's ability to generate the desired computational trajectories at specified observation times.

## Foundational Learning
**Gradient descent optimization**: Why needed: Core algorithm for updating thermodynamic computer parameters to improve performance. Quick check: Verify convergence of parameter updates during training.

**Onsager-Machlup functional**: Why needed: Provides mathematical framework for quantifying probability of dynamical trajectories in non-equilibrium systems. Quick check: Confirm functional correctly measures trajectory likelihoods.

**Teacher-student framework**: Why needed: Enables use of trained neural networks as reference models for training thermodynamic computers. Quick check: Validate that thermodynamic computer trajectories match neural network activations.

**Non-equilibrium thermodynamics**: Why needed: Underlies the physical behavior of thermodynamic computers operating away from equilibrium. Quick check: Ensure system remains in appropriate non-equilibrium regime during operation.

**Dynamical systems theory**: Why needed: Provides mathematical foundation for analyzing and controlling time-dependent behavior of thermodynamic computers. Quick check: Verify stability of system dynamics during training and operation.

## Architecture Onboarding

**Component map**: Input signals -> Thermodynamic computer (couplings + biases) -> Dynamical trajectories -> Gradient computation (Onsager-Machlup) -> Parameter updates -> Improved trajectories

**Critical path**: Parameter initialization → Trajectory generation → Gradient computation → Parameter update → Performance evaluation → (loop)

**Design tradeoffs**: Accuracy vs. energy efficiency (theoretical advantage of 10^7 suggests significant benefit), complexity of gradient computation vs. training effectiveness, system size vs. practical implementability, idealized trajectory requirements vs. experimental feasibility.

**Failure signatures**: Poor convergence during training, inability to generate required trajectories, parameter updates that increase rather than decrease error, divergence from neural network reference behavior, failure to maintain non-equilibrium conditions.

**3 first experiments**: 1) Verify gradient descent convergence on simplified test problems before full MNIST training. 2) Compare trajectory generation accuracy with and without parameter optimization. 3) Test sensitivity of results to different observation time specifications.

## Open Questions the Paper Calls Out
None

## Limitations
- Thermodynamic advantage estimate relies on rough digital implementation comparisons and theoretical minimums rather than measured hardware performance
- Training assumes idealized trajectories that may not be physically realizable in experimental implementations
- MNIST task represents a relatively simple classification problem, limiting generalizability to more complex tasks

## Confidence

**High**: MNIST classification results, gradient descent training method validity

**Medium**: Practical implementation feasibility

**Low**: Theoretical thermodynamic advantage, scalability to complex tasks

## Next Checks
1. Implement the trained model on actual thermodynamic hardware to measure real energy consumption and verify the theoretical advantage claims
2. Test the training approach on more complex datasets (e.g., CIFAR-10) and larger network architectures
3. Conduct a sensitivity analysis to determine how robust the training is to parameter noise and measurement uncertainty in realistic hardware conditions