---
ver: rpa2
title: 'Investigating Bias: A Multilingual Pipeline for Generating, Solving, and Evaluating
  Math Problems with LLMs'
arxiv_id: '2509.17701'
source_url: https://arxiv.org/abs/2509.17701
tags:
- language
- llms
- were
- english
- arabic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces an automated multilingual pipeline for evaluating
  LLM-generated math solutions across English, German, and Arabic. Using curriculum-aligned
  problems and LLM judges, it found that English solutions were consistently rated
  highest, while Arabic was often ranked lowest.
---

# Investigating Bias: A Multilingual Pipeline for Generating, Solving, and Evaluating Math Problems with LLMs

## Quick Facts
- arXiv ID: 2509.17701
- Source URL: https://arxiv.org/abs/2509.17701
- Reference count: 24
- Primary result: English LLM math solutions consistently ranked highest across all solver models; Arabic ranked lowest

## Executive Summary
This study introduces an automated multilingual pipeline for evaluating LLM-generated math solutions across English, German, and Arabic. Using curriculum-aligned problems and LLM judges, it found that English solutions were consistently rated highest, while Arabic was often ranked lowest. German solutions typically fell in the middle. The results reveal persistent linguistic bias in educational AI outputs, highlighting the need for more equitable, multilingual model development to ensure fair access and support for all learners.

## Method Summary
The study built a pipeline to generate, solve, and evaluate K-10 math problems across three languages. It used 628 exercises aligned with the German curriculum spanning grades 2-10 and five topic areas. Three solver LLMs (GPT-4o-mini, Gemini 2.5 Flash, Qwen-plus) generated step-by-step solutions in each language. A held-out judging panel of LLMs evaluated solutions with randomized presentation order to reduce position bias, using Claude 3.5 Haiku as neutral judge when evaluating its own outputs. Majority voting determined top-performing language per exercise, categorized as Best/Mid/Worst/TIE based on criteria including clarity, accuracy, and pedagogical effectiveness.

## Key Results
- English solutions consistently received the highest rankings across all solver models
- Arabic solutions were most frequently ranked lowest, while German typically fell in the middle
- The linguistic bias pattern persisted regardless of which solver model generated the solutions

## Why This Works (Mechanism)
The pipeline leverages LLM capabilities for both generation and evaluation, using curriculum-aligned problems to ensure educational relevance. By implementing a held-out judging strategy with randomized presentation order, the method reduces both position bias and self-evaluation contamination. The multilingual approach allows direct comparison of solution quality across languages while maintaining consistent mathematical content and difficulty levels.

## Foundational Learning
- Multilingual curriculum alignment: Ensures problems are educationally appropriate across languages; quick check: verify grade-level correspondence between original and translated problems
- LLM judge reliability: Critical for automated evaluation at scale; quick check: test judge consistency on identical solution sets
- Position bias mitigation: Prevents order effects from influencing rankings; quick check: analyze ranking distribution by presentation position
- Technical term extraction: Standardizes evaluation criteria across languages; quick check: verify term extraction consistency across translations

## Architecture Onboarding
**Component map:** Exercise generation -> Translation -> Solution generation -> Judge evaluation -> Result aggregation
**Critical path:** Translation → Solution generation → Judge evaluation
**Design tradeoffs:** LLM judges provide scalability but may lack pedagogical expertise compared to human experts
**Failure signatures:** Position bias if solutions presented in fixed order; self-evaluation bias if solver model judges its own outputs
**First experiments:**
1. Test judge consistency by evaluating identical solutions with different randomized orders
2. Verify held-out strategy correctly excludes solver model when judging its own outputs
3. Validate translation quality by comparing technical term consistency across languages

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize beyond German K-10 curriculum to other educational contexts
- Reliance on LLM judges rather than human experts raises questions about pedagogical assessment validity
- Translation artifacts and cultural differences in mathematical notation could confound observed bias patterns

## Confidence
- High confidence: Consistent ranking pattern across multiple solver models and documented bias prevention methodology
- Medium confidence: Magnitude of differences between languages due to reliance on LLM judges for pedagogical assessment
- Medium confidence: Claim that linguistic bias affects educational outcomes without measuring actual learning impacts

## Next Checks
1. Replicate the study with human expert judges evaluating a subset of solutions to validate LLM judge rankings
2. Test the pipeline with additional curricula and recent LLM models to assess generalizability
3. Conduct cross-linguistic analysis of technical term consistency and mathematical notation to isolate language effects from content quality differences