---
ver: rpa2
title: 'Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity
  Identification'
arxiv_id: '2507.11086'
source_url: https://arxiv.org/abs/2507.11086
tags:
- entity
- legal
- data
- llms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated traditional string-matching algorithms and\
  \ Large Language Models (LLMs) for cross-border entity identification in financial\
  \ risk management. Traditional methods like Levenshtein, cosine, and Jaccard distances\
  \ achieved high accuracy (92%) but suffered from high false positive rates (20\u2013\
  40%)."
---

# Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification

## Quick Facts
- **arXiv ID**: 2507.11086
- **Source URL**: https://arxiv.org/abs/2507.11086
- **Reference count**: 15
- **Primary result**: LLMs outperformed traditional string-matching algorithms for cross-border entity identification, with interface-based models achieving >93% accuracy and lower false positive rates than traditional methods

## Executive Summary
This study evaluates Large Language Models (LLMs) versus traditional string-matching algorithms for cross-border entity identification in financial risk management. Traditional methods like Levenshtein, cosine, and Jaccard distances achieved high accuracy (>92%) but suffered from high false positive rates (20–40%). Interface-based LLMs including Microsoft Copilot and Alibaba's Qwen2.5 demonstrated superior performance with accuracies above 93%, F1 scores exceeding 96%, and lower false positive rates (40–80%). The results highlight LLMs' potential to improve entity matching by leveraging contextual understanding and semantic relationships, reducing manual intervention in compliance workflows.

## Method Summary
The study compared traditional distance-based methods (Levenshtein, cosine, Jaccard) against three LLM categories: HuggingFace models (deBERTa-v3-base, BERT-base-uncased, BART-large-mnli), and interface-based models (Microsoft Copilot, Qwen2.5, Mistral). The workflow involved data retrieval from RIAD databases and web scraping (Informa), followed by LLM classification using zero-shot learning with fine-tuning on 433 fictitious entity examples. Evaluation used 65 Portuguese company cases with labels: Accepted, Rejected, or Doubtful. Models were assessed on accuracy, F1 score, ROC AUC, and critically, False Positive Rate (FPR) for financial compliance contexts.

## Key Results
- Interface-based LLMs (Qwen2.5, Copilot) achieved 93-95% accuracy and F1 scores above 96%
- Traditional methods reached 92-94% accuracy but with FPR ranging from 20-80%
- deBERTa-v3-base showed 100% FPR despite high accuracy, indicating fundamental matching failures
- Qwen2.5 achieved best overall performance: 95.24% accuracy, 97.44% F1, 40% FPR

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs improve entity matching by leveraging contextual understanding to handle abbreviations, semantic relationships, and legal form transitions that string-distance methods cannot capture.
- Mechanism: Pre-trained LLMs encode linguistic patterns through exposure to diverse text corpora. When presented with entity pairs like "PASTIGEST IND E COM" vs. "PASTIGEST - INDÚSTRIA E COMÉRCIO," the model recognizes "IND" as an abbreviation for "INDÚSTRIA" via learned contextual patterns rather than character-level matching.
- Core assumption: The pre-training corpus contains sufficient examples of business entity naming conventions, abbreviations, and Portuguese linguistic patterns to transfer to this task.
- Evidence anchors:
  - [abstract]: "LLMs leverage extensive training to interpret context, handle abbreviations, and adapt to legal transitions."
  - [section 3.3.4]: "In zero-shot learning (ZSL), the model tackles a task without being exposed to any labeled data specific to that task...leveraging the knowledge it has acquired during its training."
  - [corpus]: Related paper "Financial Named Entity Recognition: How Far Can LLM Go?" (FMR=0.68) supports LLM utility for financial entity extraction, though it does not confirm the specific cross-border matching mechanism.
- Break condition: Performance degrades significantly when encountering entity naming conventions, legal forms, or linguistic patterns poorly represented in pre-training data.

### Mechanism 2
- Claim: Zero-shot classification with fine-tuning enables domain adaptation without requiring large labeled datasets.
- Mechanism: The workflow fine-tunes LLMs on a JSON file of 433 fictitious entity examples, teaching the model the specific "Accepted," "Rejected," and "Doubtful" classification schema. The model then generalizes from these examples to real Portuguese entity pairs via in-context learning and pattern recognition.
- Core assumption: 433 synthetic/fictitious training examples provide sufficient signal for the model to learn the domain-specific matching criteria without overfitting.
- Evidence anchors:
  - [section 3.3.6]: "We created a JSON file with some examples, so the model understands what I need him to do. This file is composed of 433 observations."
  - [section 3.1]: Workflow describes "LLM has been previously fine-tuned with data regarding some fictitious entities."
  - [corpus]: Weak direct evidence—related papers do not address fine-tuning scale requirements for entity matching specifically.
- Break condition: Fine-tuning on synthetic data may not generalize when real-world entity variations diverge significantly from synthetic patterns; insufficient fine-tuning examples lead to high false positive rates.

### Mechanism 3
- Claim: Interface-based LLMs (Copilot, Qwen2.5) outperform local HuggingFace models due to larger parameter counts and broader training data, enabling better semantic matching.
- Mechanism: Larger proprietary models accessed via API have been trained on more extensive, more diverse corpora with additional post-training alignment, improving their ability to distinguish between genuinely different entities versus surface-level string variations.
- Core assumption: The performance difference stems from model scale and training data breadth rather than from evaluation methodology differences between local inference and API-based inference.
- Evidence anchors:
  - [section 5]: "Interface-based LLMs like Microsoft/Copilot and Alibaba's Qwen2.5 showcased exceptional performance...likely due to their extensive training on diverse datasets and advanced architectures."
  - [table p.19]: Qwen2.5 achieves 95.24% accuracy, 40% FPR; deBERTa-v3-base achieves 92.06% accuracy, 100% FPR.
  - [corpus]: Insufficient comparative evidence on interface vs. local model performance for entity matching specifically.
- Break condition: Performance advantage may diminish or invert in domains requiring strict data privacy (preventing API use), or when API models' training data lacks domain-specific financial/legal terminology.

## Foundational Learning

- **Concept: Entity Matching vs. Named Entity Recognition (NER)**
  - Why needed here: The paper addresses entity *matching* (determining if two records refer to the same entity), which differs from NER (extracting entity mentions from text). Confusing these leads to misapplied methods.
  - Quick check question: Given two company records with slightly different names, would you use a sequence labeling model or a binary classification approach?

- **Concept: False Positive Rate in High-Stakes Contexts**
  - Why needed here: The paper emphasizes FPR as the critical metric for financial compliance—incorrectly accepting a mismatched entity poses regulatory risk. A model with 95% accuracy may still be unacceptable if its false positive rate is too high.
  - Quick check question: If Model A has 94% accuracy and 20% FPR, while Model B has 96% accuracy and 80% FPR, which is preferable for financial compliance screening?

- **Concept: Zero-Shot Classification**
  - Why needed here: The paper applies zero-shot learning to entity matching, requiring understanding of how pre-trained models generalize to unseen tasks without task-specific labeled data.
  - Quick check question: Can a model trained only on English product reviews classify Portuguese company entity matches without any Portuguese training examples? What conditions must hold?

## Architecture Onboarding

- **Component map:**
  ```
  [Data Retrieval] → [RIAD Database Check] → [Web Scraping (Informa, etc.)]
         ↓
  [Entity Record (Name, Legal Form, ID, Country)]
         ↓
  [Preprocessing: Name Normalization, Legal Form Extraction]
         ↓
  [LLM Classification] ← [Fine-tuned Model (433 examples)]
         ↓
  [Prediction: Accepted / Rejected / Doubtful]
         ↓
  [Human Review (Doubtful cases only)]
  ```

- **Critical path:** The fine-tuning quality and the prompt design for zero-shot classification are the highest-leverage points. Poor fine-tuning data directly causes the 100% FPR observed with deBERTa.

- **Design tradeoffs:**
  - **Local models (HuggingFace)** vs. **Interface-based (Copilot, Qwen2.5)**: Local models offer data privacy and cost predictability but show higher FPR in this study; interface models perform better but require sending data to external APIs (problematic for confidential financial data).
  - **Threshold tuning**: The paper does not report threshold optimization; adjusting classification thresholds could reduce FPR at the cost of recall.

- **Failure signatures:**
  - **deBERTa 100% FPR**: Model defaults to "Accepted" for all pairs—indicates fine-tuning failed to teach rejection criteria.
  - **High FPR with abbreviations**: Models struggle when declared names contain abbreviations not present in fine-tuning examples (e.g., "IND" for "INDÚSTRIA").
  - **ROC AUC ≈ 50%**: Model is no better than random—fundamental failure in learned representations.

- **First 3 experiments:**
  1. **Baseline replication**: Implement Levenshtein, Cosine, and Jaccard on the 65-case Portuguese dataset to reproduce reported FPR values (20-80%); verify your ground-truth labels match the paper's "Accepted/Rejected/Doubtful" schema.
  2. **Fine-tuning data audit**: Before training any LLM, manually inspect the 433-example JSON fine-tuning file for class balance and coverage of edge cases (abbreviations, legal form variations). An imbalanced or narrow fine-tuning set likely caused deBERTa's 100% FPR.
  3. **Interface model API test with held-out data**: Run Qwen2.5 or Copilot on a separate validation set of 20+ Portuguese entity pairs not in the original study, tracking FPR specifically. The paper's 40-80% FPR range for interface models requires independent verification before production deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the entity-matching framework be effectively generalized to countries with distinct linguistic characteristics and less accessible data registries than Portugal?
- **Basis in paper:** [Explicit] The authors state, "Our main goal is to extrapolate and adapt these methodologies to all the possible countries," noting that "linguistic diversity" and changes in data description may complicate model functions.
- **Why unresolved:** The current study relies specifically on Portuguese data, chosen for its linguistic similarity to Spanish and the availability of open-source verification registries (e.g., Informa), which may not exist for other target nations.
- **What evidence would resolve it:** Successful deployment and evaluation of the proposed workflow using a diverse dataset from countries with different alphabets, legal structures, and limited public record accessibility.

### Open Question 2
- **Question:** What specific fine-tuning or architectural modifications are required to reduce the False Positive Rate (FPR) to near-zero for critical financial applications?
- **Basis in paper:** [Explicit] The paper concludes that "challenges remain in minimizing false positives" and emphasizes that "further refinements are necessary to reduce FPR and ensure consistent performance," given the requirement for zero tolerance in financial contexts.
- **Why unresolved:** While interface-based LLMs outperformed traditional methods, the study highlights that false positives (e.g., accepting an incorrect entity) remain a critical risk that current models have not fully eliminated.
- **What evidence would resolve it:** A modified model evaluation showing a significantly reduced FPR (approaching 0%) on a hold-out set of ambiguous entities without a substantial loss in recall.

### Open Question 3
- **Question:** Does domain-specific fine-tuning on financial legal forms and abbreviations significantly outperform zero-shot classification in handling lexical variations?
- **Basis in paper:** [Inferred] The authors note that "models struggle due to insufficient fine-tuning on domain-specific patterns" regarding abbreviations and legal transitions, and list "developing stronger fine-tuning" as a primary goal for future research.
- **Why unresolved:** The current implementation relies heavily on zero-shot classification (ZSC) and general pre-training, which the authors identify as a potential source of error for complex lexical variations.
- **What evidence would resolve it:** A comparative ablation study measuring performance delta between the current zero-shot approach and a version fine-tuned specifically on financial abbreviations and historical legal form changes.

## Limitations

- The evaluation set is limited to 65 Portuguese company cases, potentially limiting generalizability to other languages and legal systems
- Fine-tuning on 433 fictitious examples may not capture real-world entity variation complexity, as evidenced by deBERTa's 100% FPR
- Interface-based models show superior performance but raise data privacy concerns for financial compliance applications

## Confidence

- **High Confidence**: The comparative advantage of LLMs over traditional string-distance methods in handling abbreviations and semantic variations
- **Medium Confidence**: The specific superiority of interface-based models (Copilot, Qwen2.5) over local HuggingFace models, given potential confounding factors in evaluation setup
- **Low Confidence**: The generalizability of results to other languages, legal systems, and entity types beyond the Portuguese dataset studied

## Next Checks

1. **Independent Dataset Validation**: Evaluate the best-performing models (Qwen2.5, Copilot) on a held-out validation set of 20+ Portuguese entity pairs from a different source than the original study, with careful FPR tracking to verify the 40-80% FPR range

2. **Cross-Lingual Transfer Test**: Apply the fine-tuned models to entity matching tasks in other languages (Spanish, French) with similar legal entity naming conventions to assess generalization beyond Portuguese

3. **Threshold Sensitivity Analysis**: Systematically vary classification thresholds for both distance-based methods and LLM confidence scores, plotting precision-recall curves specifically for the high-stakes false positive rate to identify optimal operating points for financial compliance