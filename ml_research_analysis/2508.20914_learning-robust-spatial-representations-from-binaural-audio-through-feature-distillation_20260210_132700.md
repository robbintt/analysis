---
ver: rpa2
title: Learning Robust Spatial Representations from Binaural Audio through Feature
  Distillation
arxiv_id: '2508.20914'
source_url: https://arxiv.org/abs/2508.20914
tags:
- spatial
- speech
- features
- learning
- signal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a self-supervised pretraining framework for
  learning robust spatial representations from binaural audio using feature distillation.
  The method predicts spatial features (GCC, GCC-PHAT, IPD+ILD) computed from clean
  speech from noisy reverberant input, without requiring direction-of-arrival (DoA)
  labels.
---

# Learning Robust Spatial Representations from Binaural Audio through Feature Distillation

## Quick Facts
- arXiv ID: 2508.20914
- Source URL: https://arxiv.org/abs/2508.20914
- Reference count: 40
- Primary result: Pretraining with spatial feature distillation achieves up to 46.44% reduction in mean angular error for DoA estimation compared to fully supervised baselines, especially in low-data regimes.

## Executive Summary
This paper proposes a self-supervised pretraining framework for learning robust spatial representations from binaural audio using feature distillation. The method predicts spatial features (GCC, GCC-PHAT, IPD+ILD) computed from clean speech from noisy reverberant input, without requiring direction-of-arrival (DoA) labels. After pretraining, the encoder is fine-tuned for DoA estimation. Experiments on a simulated binaural dataset show that models pretrained with spatial features significantly outperform fully supervised and classic model-based approaches, achieving up to 46.44% reduction in mean angular error, especially in low-data regimes.

## Method Summary
The framework uses a 2-layer causal Conformer encoder to predict spatial features extracted from clean speech given noisy reverberant input. Spatial features (GCC, GCC-PHAT, CPS-PHAT phase, or IPD+ILD) serve as distillation targets. After pretraining with MSE loss, the predictor is discarded and the encoder is fine-tuned for DoA estimation with a linear classifier. The binaural dataset is constructed by convolving speech with room impulse responses and applying head-related transfer functions at sampled azimuths, with diffuse noise augmentation.

## Key Results
- Pretrained models achieve 46.44% reduction in mean angular error compared to fully supervised baselines
- Phase-only spatial features (GCC-PHAT, CPS-PHAT) outperform magnitude-inclusive features (GCC, IPD+ILD)
- Performance gains are most pronounced in low-data regimes (10h vs 1h vs 10min splits)
- The approach outperforms both classic model-based methods and fully supervised DNNs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Predicting clean spatial features from noisy/reverberant inputs forces the encoder to learn noise-robust spatial representations.
- Mechanism: The pretraining task acts as a learnable spatial feature denoiser. By computing spatial features from clean binaural speech and training the model to predict these from corrupted inputs, the encoder must learn to extract signal-dependent spatial cues while suppressing noise and reverberation artifacts.
- Core assumption: Spatial features computed from clean speech contain sufficient information about source direction, and recovering them from noisy inputs requires learning a noise-invariant representation of spatial cues.

### Mechanism 2
- Claim: Discarding the feature predictor and retaining only the encoder transfers learned spatial knowledge to downstream tasks without task-specific bias.
- Mechanism: The feature predictor acts as a decoder for spatial features. By removing it after pretraining, only the encoder's representation—which has been shaped to capture spatial information in a generalizable form—is transferred.
- Core assumption: The encoder's internal representation, not the predictor's output, contains the spatial information needed for DoA estimation.

### Mechanism 3
- Claim: Phase-based spatial features (GCC-PHAT, CPS-PHAT phase) are more effective pretraining targets than magnitude-inclusive features (GCC, ILD+IPD).
- Mechanism: GCC-PHAT and CPS-PHAT phase whiten the spectrum, normalizing magnitude and retaining only phase information. This forces the encoder to focus on phase relationships (ITD/IPD) which are directly related to DoA, rather than learning to predict magnitude patterns that may not generalize across environments.
- Core assumption: Phase information is the primary spatial cue for DoA estimation in binaural setups, and normalizing magnitude removes spurious correlations.

## Foundational Learning

- Concept: **Interaural Phase Difference (IPD) and Interaural Time Difference (ITD)**
  - Why needed here: The paper assumes understanding that IPD is the frequency-domain representation of ITD and is the primary cue for horizontal sound localization in binaural hearing.
  - Quick check question: Given a binaural STFT frame, how would you compute IPD, and why does it relate to sound source direction?

- Concept: **Generalized Cross-Correlation with Phase Transform (GCC-PHAT)**
  - Why needed here: GCC-PHAT is a core pretraining target. Understanding its computation and why PHAT weighting improves robustness to noise is essential.
  - Quick check question: Why does PHAT weighting (magnitude normalization) make GCC more robust to reverberation compared to unweighted GCC?

- Concept: **Self-Supervised Pretraining with Distillation Targets**
  - Why needed here: The method uses spatial features as distillation targets rather than contrastive or masked prediction objectives common in other SSL approaches.
  - Quick check question: What is the difference between using hand-crafted features as distillation targets vs. learning them end-to-end from raw audio with contrastive learning?

## Architecture Onboarding

- Component map: STFT input (real/imag concatenated) -> 2-layer causal Conformer encoder -> Linear feature predictor -> MSE loss to spatial feature targets
- Critical path: 1) Generate binaural dataset with clean/noisy pairs using HRTF convolution and RIR augmentation; 2) Pretrain encoder+predictor on spatial feature prediction (50k steps); 3) Save encoder weights, discard predictor; 4) Initialize DoA model with pretrained encoder, add linear classifier; 5) Fine-tune on labeled DoA data with noise augmentation
- Design tradeoffs: Phase-only targets outperform magnitude-inclusive ones; causal Conformer enables real-time processing; pretraining data scale is substantial (960h LibriSpeech); clean speech required for target feature computation
- Failure signatures: High MAE in low SNR (-20dB) suggests limits of learned robustness; including ILD in targets degrades performance; supervised baselines with GCC-PHAT input outperform STFT-DNN
- First 3 experiments: 1) Reproduce baseline comparison: Train GCCPHAT-DNN and STFT-DNN on 10h labeled data, verify Table 2 MAE values; 2) Ablate feature targets: Pretrain three SFD models with GCC-PHAT, CPS-PHAT phase, and IPD+ILD; fine-tune on 1h data; confirm phase targets outperform magnitude-inclusive targets; 3) Low-data regime test: Fine-tune SFD-GCC-PHAT and STFT-DNN on 10min, 1h, and 10h splits; plot MAE vs. data size

## Open Questions the Paper Calls Out
- How can the pretraining framework be adapted for scenarios where no clean speech reference is available to compute the spatial feature targets?
- How does the model perform in dynamic environments involving moving sound sources or multiple concurrent speakers?
- Do the performance gains observed in simulated environments transfer to real-world binaural recordings?

## Limitations
- The method requires clean speech to compute spatial feature targets, limiting real-world applicability
- Performance in dynamic environments with moving sources or multiple speakers remains unexplored
- Simulated data may not fully capture real-world acoustic complexities and generalization is uncertain

## Confidence
- **High Confidence**: The pretraining framework design and overall methodology are clearly specified
- **Medium Confidence**: The claim that this approach significantly outperforms supervised baselines is supported by the data
- **Low Confidence**: The scalability analysis beyond tested data regimes and computational efficiency claims lack benchmarking

## Next Checks
1. Test pretrained models on HRTF sets not seen during pretraining to verify spatial representation robustness across head geometries
2. Systematically vary the SNR range used for target feature computation to determine sensitivity to clean feature quality
3. Apply the pretrained encoder to a real-world binaural dataset without fine-tuning to assess practical deployment viability