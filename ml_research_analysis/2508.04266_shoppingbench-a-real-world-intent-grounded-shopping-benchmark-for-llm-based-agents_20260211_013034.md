---
ver: rpa2
title: 'ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based
  Agents'
arxiv_id: '2508.04266'
source_url: https://arxiv.org/abs/2508.04266
tags:
- product
- tool
- products
- user
- assistant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ShoppingBench is a large-scale benchmark for evaluating language\
  \ agents in realistic e-commerce tasks. It simulates complex user intents\u2014\
  such as applying vouchers, managing budgets, and finding multi-product sellers\u2014\
  through 3,310 grounded instructions and a sandbox with 2.5 million products."
---

# ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based Agents

## Quick Facts
- arXiv ID: 2508.04266
- Source URL: https://arxiv.org/abs/2508.04266
- Reference count: 40
- Primary result: Even top models like GPT-4.1 achieve under 50% success, highlighting the difficulty of real-world e-commerce tasks.

## Executive Summary
ShoppingBench introduces a large-scale benchmark for evaluating language agents in realistic e-commerce tasks. It features 3,310 grounded instructions across four intent types and a sandbox with 2.5 million products. The benchmark uses intent-aware metrics (Absolute Success Rate and Cumulative Average of Product Relevance) to assess performance. A trajectory distillation strategy—using GPT-4.1-generated, filtered synthetic data—and fine-tuning with SFT and RL boosts a smaller Qwen3-4B model to surpass GPT-4.1, demonstrating effective capability transfer. Analysis of failure modes and ablation studies further reveal the importance of reasoning, tool usage, and external knowledge access for agent success.

## Method Summary
ShoppingBench evaluates LLM agents on complex e-commerce tasks across four intent types: Products Finder, Knowledge, Multi-products Seller, and Coupon & Budget. The benchmark uses 3,310 user instructions and a sandbox with 2.5 million products from Lazada. Instructions are generated via a 3-stage process: product sampling, field extraction, and GPT-4.1 query simulation. The agent backbone is Qwen3-4B with SFT and GRPO fine-tuning. Tools include find_product, view_product_information, python_execute, web_search, recommend_product, and terminate. Evaluation uses Absolute Success Rate (ASR) per intent and Cumulative Average of Product Relevance (CAR) measuring title similarity, price range, and feature overlap. Training involves trajectory distillation with GPT-4.1 → rejection sampling → SFT on Qwen3-4B (5 epochs, lr=5e-5, seq_len=20,480, ~2 hrs) → GRPO RL (lr=1e-6, batch=128, 5 epochs, ~9 hrs). Hardware: 8× A100-80G.

## Key Results
- Even top models like GPT-4.1 achieve under 50% success, highlighting the difficulty of real-world e-commerce tasks.
- A trajectory distillation strategy—using GPT-4.1-generated, filtered synthetic data—and fine-tuning with SFT and RL boosts a smaller Qwen3-4B model to surpass GPT-4.1.
- Analysis of failure modes and ablation studies further reveal the importance of reasoning, tool usage, and external knowledge access for agent success.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rejection sampling on synthetic trajectories can effectively transfer capabilities from a large model to a smaller one.
- Mechanism: GPT-4.1 generates tool-calling trajectories from 2,410 instructions. Trajectories that fail the Absolute Success Rate metric are filtered out. The surviving 5,552 steps train Qwen3-4B via SFT, then GRPO-based RL refines tool-calling with format and tool rewards. This distills reasoning patterns without inheriting the teacher's scale.
- Core assumption: High-quality trajectories from GPT-4.1 encode transferable reasoning patterns; success metric correctly identifies them.
- Evidence anchors:
  - [abstract] "we propose a trajectory distillation strategy and leverage supervised fine-tuning, along with reinforcement learning on synthetic trajectories, to distill the capabilities of a large language agent into a smaller one"
  - [section] "Shopping Agent Training" describes rejection sampling, SFT dataset of 5,552 steps, and GRPO with tool reward
  - [corpus] ShopSimulator (arxiv 2601.18225) also explores RL-driven LLM shopping agents, but ShoppingBench uniquely combines rejection sampling with GRPO for capability distillation
- Break condition: If rejection sampling filters too aggressively (e.g., <10% retention), training diversity collapses. If metric doesn't correlate with actual task success, distilled model learns wrong patterns.

### Mechanism 2
- Claim: Explicit reasoning traces before action improve performance on constraint-heavy tasks but hurt simple retrieval tasks.
- Mechanism: The agent generates a thought text before tool calls. For Coupon & Budget tasks, reasoning helps decompose voucher rules, budget arithmetic, and product combinations. For Products Finder, adding reasoning introduces unnecessary steps and error accumulation. GPT-4.1 with "No Think" achieves 61.6% vs 59.6% ASR on Products Finder, but 30.4% vs 23.2% on Coupon & Budget (think > no-think).
- Core assumption: Reasoning traces are beneficial when task requires multi-step constraint satisfaction, harmful when task is near-deterministic.
- Evidence anchors:
  - [abstract] "Even top models like GPT-4.1 achieve under 50% success, highlighting the difficulty of real-world e-commerce tasks"
  - [section] "Effect of Thinking" and Table 5 show intent-specific reasoning effects
  - [corpus] DeepShop (arxiv 2506.02839) notes similar task-complexity effects but doesn't quantify reasoning ablations
- Break condition: If reasoning traces become too long (>500 tokens), latency and cost outweigh accuracy gains. If reasoning is shallow or generic, it provides no benefit.

### Mechanism 3
- Claim: External web search is critical for knowledge-intensive intents; product detail inspection correlates with success across all intents.
- Mechanism: Knowledge intent requires answering questions like "What major did Kunihiko Kodaira study?" then finding related products. Without web_search, GPT-4.1 drops from 62.0% to 50.0% ASR. Across all intents, viewing product information strongly correlates with success (Pearson correlation analysis). Agents that skip product inspection fail due to attribute mismatches.
- Core assumption: LLMs lack long-tail e-commerce knowledge; external retrieval compensates. Detailed inspection reduces attribute mismatch failures.
- Evidence anchors:
  - [abstract] "Analysis of failure modes and ablation studies further reveal the importance of reasoning, tool usage, and external knowledge access"
  - [section] Table 2 shows web_search ablation; Figure 5 shows correlation analysis; failure breakdown shows attribute mismatch (#AM) as largest error category
  - [corpus] ShoppingComp (arxiv 2511.22978) emphasizes safety-critical decision making but doesn't isolate web search effects
- Break condition: If web search returns irrelevant results, agent wastes steps. If product detail pages lack structured attributes, inspection doesn't help.

## Foundational Learning

- Concept: **Partially Observable Markov Decision Process (POMDP)**
  - Why needed here: Each agent trajectory is formalized as (U, S, A, T, O, R). Understanding this framing is essential for reasoning about state, observation, action, and reward in the sandbox.
  - Quick check question: Given a user instruction and current product list, what is the observation space versus the hidden state?

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: RL phase uses GRPO with tool reward. This is a PPO variant optimized for tool-calling with group-based advantage estimation.
  - Quick check question: How does GRPO differ from standard PPO in how it computes advantages?

- Concept: **BM25 Sparse Retrieval**
  - Why needed here: Product search engine uses Pyserini with BM25. Understanding term frequency and inverse document frequency helps debug retrieval failures.
  - Quick check question: Why might BM25 fail to retrieve a product with rare attributes that aren't in the query?

## Architecture Onboarding

- Component map:
  Shopping Sandbox -> API Tool Pool -> Agent Backbone -> Evaluation Engine -> Web Search Wrapper

- Critical path:
  1. User instruction parsed into intent type (Products Finder / Knowledge / Multi-products seller / Coupon & Budget)
  2. Agent generates reasoning + tool call
  3. Tool executes against sandbox or web search
  4. Observation returned to agent
  5. Loop until terminate called
  6. Evaluation engine checks all constraints, computes ASR/CAR

- Design tradeoffs:
  - **Rejection sampling threshold**: Too strict → insufficient training data; too loose → noise in distillation
  - **Reasoning enabled/disabled**: Intent-specific; must match task complexity
  - **Web search budget**: More calls improve Knowledge tasks but add latency and cost
  - **BM25 vs dense retrieval**: BM25 is fast but may miss semantic matches; dense retrieval adds latency

- Failure signatures:
  - **Attribute mismatch (#AM)**: Agent recommends product missing required attribute (e.g., "twist color" → "black")
  - **Product missing (#PM)**: Agent terminates early, doesn't find all requested products
  - **Constraint not satisfied (#CNS)**: Budget or voucher conditions violated
  - **Knowledge error (#KE)**: Agent hallucinates answer instead of using web_search
  - **Metric issue (#MI)**: Prediction correct but metric too strict (e.g., "high waist" vs "high")

- First 3 experiments:
  1. **Baseline assessment**: Run GPT-4.1 and Qwen3-4B on all four intent types with reasoning enabled. Log ASR, CAR, and failure mode distribution.
  2. **Ablate web_search**: Disable web_search tool, re-evaluate Knowledge intent. Confirm ASR drop matches paper's ~12 point decline for GPT-4.1.
  3. **Rejection sampling sweep**: Vary ASR threshold for trajectory filtering (e.g., 0.5, 0.7, 0.9), train SFT-only model, measure ASR on held-out test set to find optimal retention rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can agent architectures be improved to better handle multi-constraint reasoning tasks involving vouchers, budgets, and multi-product seller requirements?
- Basis in paper: [explicit] The paper states "findings underscore the need for advances in agent architecture, tool usage, problem decomposition, and web information integration" given that GPT-4.1 achieves under 50% success.
- Why unresolved: The paper demonstrates the problem but does not propose architectural solutions beyond trajectory distillation, which transfers existing capabilities rather than fundamentally improving reasoning.
- What evidence would resolve it: Novel agent architectures tested on ShoppingBench showing improved ASR on Coupon & Budget and Multi-products seller intents.

### Open Question 2
- Question: Does trajectory distillation create capabilities that generalize beyond the teacher model's limitations, or does it merely replicate existing performance bounds?
- Basis in paper: [inferred] The SFT+RL-Qwen3-4B matches GPT-4.1 (48.7% vs 48.2% ASR), suggesting the smaller model reaches but does not exceed the teacher's performance ceiling.
- Why unresolved: The paper shows successful capability transfer but does not test whether distilled models can surpass their teachers on held-out intent types or novel constraint combinations.
- What evidence would resolve it: Experiments showing distilled agents outperforming teachers on out-of-distribution tasks or novel constraint configurations not present in training trajectories.

### Open Question 3
- Question: How should automatic evaluation metrics handle near-miss attribute matches (e.g., "high waist" vs. "high") without penalizing semantically correct predictions?
- Basis in paper: [inferred] The case study explicitly identifies "Metric Issue" failures where predictions like "waist type: high waist" are marked incorrect against ground truth "waist type: high."
- Why unresolved: Current metrics use exact string matching for attributes, creating false negatives that may misrepresent actual agent performance.
- What evidence would resolve it: A revised metric incorporating semantic similarity or normalized attribute matching, validated against human judgment of prediction correctness.

## Limitations
- The product corpus (2.5M items from Lazada) is not confirmed as publicly released, creating a significant barrier to exact reproduction.
- ASR/CAR metrics may be overly strict—failure analysis shows attribute mismatches and metric strictness as distinct error categories.
- External web search via Serper API introduces latency, cost, and potential variability across regions or time.

## Confidence
- **High confidence**: The core mechanism of trajectory distillation (rejection sampling + SFT + GRPO) is well-specified and demonstrably effective; ASR improvements from 18.4% to 49.2% for Qwen3-4B are measurable and robust.
- **Medium confidence**: Intent-specific reasoning effects are supported by ablation data, but task complexity thresholds for "when to think" are not fully quantified.
- **Low confidence**: External knowledge retrieval's contribution is inferred from correlation analysis; the causal link between web_search usage and Knowledge intent success is indirect.

## Next Checks
1. **Replicate the ASR gap**: Disable web_search for GPT-4.1 and confirm ASR drops by ~12 points on Knowledge intent, matching paper's 62.0% → 50.0% decline.
2. **Sweep rejection sampling threshold**: Vary ASR threshold for trajectory filtering (0.5, 0.7, 0.9), train SFT-only models, measure ASR on held-out set to find optimal retention rate.
3. **Diagnose attribute mismatch failures**: For Coupon & Budget intent, trace view_product_information call frequency and correlate with CAR vs ASR gap to confirm insufficient detail-checking as root cause.