---
ver: rpa2
title: Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification
arxiv_id: '2505.19776'
source_url: https://arxiv.org/abs/2505.19776
tags:
- political
- bias
- sentiment
- entities
- biases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new method to analyze political bias in
  large language models (LLMs) by exploiting inconsistencies in target-oriented sentiment
  classification. The authors quantify bias using an entropy-based inconsistency metric,
  inserting 1,319 politically and demographically diverse politician names into 450
  political sentences across six languages and seven LLM models.
---

# Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification

## Quick Facts
- arXiv ID: 2505.19776
- Source URL: https://arxiv.org/abs/2505.19776
- Reference count: 40
- Primary result: Entropy-based inconsistency metric reveals systematic political bias in sentiment classification across seven LLM models and six languages

## Executive Summary
This study introduces a novel framework for detecting political bias in large language models by exploiting inconsistencies in target-oriented sentiment classification. The authors insert 1,319 politically and demographically diverse politician names into 450 political sentences across six languages, then measure sentiment prediction consistency when names are swapped. Their entropy-based metric reveals systematic positive and negative biases toward left and far-right politicians respectively, with Western languages showing higher bias intensity. The findings demonstrate that larger models exhibit stronger and more consistent biases, challenging assumptions about scale reducing bias. The study offers both a robust measurement framework and preliminary mitigation strategy through fictional name substitution.

## Method Summary
The authors develop an entropy-based inconsistency metric to quantify political bias in LLMs by analyzing target-oriented sentiment classification. They create a dataset of 450 politically charged sentences, each containing a politician's name, and systematically replace these names with 1,319 diverse alternatives from different political spectrums and demographic backgrounds across six languages. Sentiment predictions are generated for each sentence-name combination using seven different LLM models. The inconsistency metric measures entropy variation in sentiment predictions when names are swapped within the same sentence context, assuming sentiment should remain stable if the model is unbiased. The approach also tests bias mitigation by substituting real politician names with fictional but plausible alternatives, measuring both bias reduction and classification accuracy changes.

## Key Results
- All seven tested LLM models show significant sentiment prediction inconsistencies when politician names are swapped, revealing systematic political bias
- Left-wing politicians receive systematically more positive sentiment predictions, while far-right politicians receive more negative predictions
- Larger models exhibit stronger and more consistent biases, with higher bias intensity observed for Western languages compared to non-Western languages

## Why This Works (Mechanism)
The method exploits the fundamental assumption in target-oriented sentiment classification that sentiment polarity should remain stable when only the target entity changes, provided the sentence context remains constant. When LLMs produce different sentiment predictions for the same sentence with different politician names, this inconsistency reveals underlying associations between specific names and sentiment directions in the model's learned representations. The entropy-based metric quantifies these variations, capturing both the magnitude and direction of bias across political spectrums. This approach works because it bypasses the need for labeled bias data by using the logical inconsistency itself as the signal, making it applicable across languages and model architectures without requiring retraining or fine-tuning.

## Foundational Learning
**Entropy-based inconsistency measurement**: Quantifying prediction variation using Shannon entropy when inputs are systematically modified - needed to provide a statistical framework for detecting bias patterns that are not easily observable through simple accuracy metrics. Quick check: Calculate entropy for a binary sentiment classification task where predictions shift from 50/50 to 90/10 distributions.

**Cross-lingual bias detection**: Applying the same bias measurement framework across multiple languages to identify systematic patterns versus language-specific artifacts - needed to distinguish universal model biases from culturally or linguistically specific training data effects. Quick check: Compare bias scores across languages with varying training data availability and cultural contexts.

**Politician name embedding analysis**: Understanding how proper names are represented in LLM embeddings and how these representations interact with sentiment classification layers - needed to explain why specific politicians evoke consistent sentiment patterns across different sentence contexts. Quick check: Visualize name embeddings using dimensionality reduction to identify clustering patterns by political affiliation.

## Architecture Onboarding

**Component map**: Sentence templates -> Politician name insertion -> LLM sentiment prediction -> Entropy calculation -> Bias quantification -> Mitigation testing

**Critical path**: The entropy calculation from sentiment predictions represents the critical path, as this transforms raw model outputs into actionable bias metrics. Without this step, the method cannot distinguish between random variation and systematic bias patterns.

**Design tradeoffs**: The authors trade model coverage for depth of analysis by focusing on seven representative models rather than exhaustive testing. This allows for more detailed analysis per model but may miss model-specific bias patterns. The choice of 450 sentences balances coverage of political contexts with computational feasibility.

**Failure signatures**: The method may fail when politician names genuinely evoke different sentiments due to their unique political histories or actions, conflating legitimate sentiment variation with bias. Additionally, languages with limited political vocabulary in training data may show artificially high inconsistency scores that don't reflect true bias.

**Three first experiments**:
1. Measure baseline sentiment consistency using fictional names to establish the expected entropy range for unbiased predictions
2. Test the method's sensitivity by introducing controlled bias through prompting variations and measuring detection accuracy
3. Compare entropy scores across different sentiment classification tasks (e.g., product reviews vs. political statements) to validate the specificity of the political bias detection

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- The fixed dataset of 450 sentences and 1,319 politician names may not capture the full spectrum of political discourse across languages and contexts
- The assumption that sentiment should remain stable when names are swapped may not hold for politicians with unique historical actions or reputations
- The study does not fully disentangle whether stronger biases in larger models reflect architectural effects or training data characteristics

## Confidence
- **High**: Systematic political bias exists across all tested models and languages, supported by consistent entropy variation patterns
- **Medium**: Claims about bias intensity differences across model sizes and language families, given controlled experimental design but limited sample size per category
- **Low to Medium**: Effectiveness of fictional name substitution as a mitigation strategy, as it shows promise but leaves significant inconsistency unresolved

## Next Checks
1. Expand the politician name set and political sentences to include non-Western political contexts and historical figures to test cross-cultural generalizability
2. Conduct ablation studies varying model size while controlling for training data composition to isolate architectural versus data-driven bias sources
3. Test the fictional name substitution approach across additional bias types (e.g., demographic, ideological) to assess whether the mitigation strategy generalizes beyond political sentiment classification