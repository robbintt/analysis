---
ver: rpa2
title: Evaluating the Creativity of LLMs in Persian Literary Text Generation
arxiv_id: '2509.18401'
source_url: https://arxiv.org/abs/2509.18401
tags:
- creativity
- literary
- texts
- persian
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a culturally grounded framework for evaluating
  LLM creativity in Persian literary text generation. Adapting the Torrance Tests
  of Creative Thinking, the authors assess originality, fluency, flexibility, and
  elaboration in Persian sentences, addressing a gap in creativity evaluation for
  non-English languages.
---

# Evaluating the Creativity of LLMs in Persian Literary Text Generation

## Quick Facts
- **arXiv ID**: 2509.18401
- **Source URL**: https://arxiv.org/abs/2509.18401
- **Reference count**: 10
- **Primary result**: Introduces culturally grounded framework for evaluating LLM creativity in Persian literary text generation

## Executive Summary
This paper presents a novel framework for evaluating the creativity of large language models in Persian literary text generation. The authors adapt the Torrance Tests of Creative Thinking to assess originality, fluency, flexibility, and elaboration in Persian sentences. They construct CPers, a dataset of 4,371 Persian literary texts spanning 20 themes, and annotate 200 texts for creativity and literary devices. Using Claude 3.7 Sonnet as an LLM judge, they evaluate six state-of-the-art models and find that DeepSeek-R1 achieves the highest overall creativity score (4.44), excelling in elaboration and flexibility.

## Method Summary
The authors developed a culturally grounded framework for evaluating LLM creativity in Persian literary text generation by adapting the Torrance Tests of Creative Thinking. They constructed CPers, a novel dataset containing 4,371 Persian literary texts across 20 themes, and annotated 200 texts for creativity and literary devices. Six state-of-the-art LLMs were evaluated using Claude 3.7 Sonnet as an LLM judge, with assessments focusing on originality, fluency, flexibility, and elaboration. The framework addresses the gap in creativity evaluation for non-English languages by incorporating Persian literary traditions and cultural context into the evaluation metrics.

## Key Results
- DeepSeek-R1 achieves the highest overall creativity score (4.44) among evaluated models
- Qwen2.5-VL-32B-Instruct demonstrates strong originality but low fluency
- The culturally grounded framework successfully captures Persian literary creativity dimensions
- Model performance varies significantly across creativity dimensions

## Why This Works (Mechanism)
The framework's effectiveness stems from adapting established creativity metrics to Persian cultural context, using culturally relevant texts and evaluation criteria. The Torrance Tests provide a structured approach to measuring creative dimensions that align with literary quality assessment. Using an LLM judge (Claude 3.7 Sonnet) enables systematic, scalable evaluation across multiple models while maintaining consistency in assessment criteria.

## Foundational Learning
- **Torrance Tests of Creative Thinking**: Standardized metrics for measuring creativity; needed to provide structured evaluation framework; quick check: verify test dimensions align with literary creativity assessment
- **Cultural grounding in evaluation**: Adapting metrics to specific linguistic/cultural contexts; needed because creativity manifests differently across cultures; quick check: validate cultural relevance with domain experts
- **LLM-based evaluation**: Using language models as judges for creative output; needed for scalable, consistent assessment; quick check: compare LLM judge scores with human evaluations
- **Multi-dimensional creativity assessment**: Evaluating multiple aspects (originality, fluency, flexibility, elaboration); needed to capture comprehensive creative quality; quick check: ensure dimensions cover all relevant creative aspects

## Architecture Onboarding

**Component Map**: CPers Dataset Construction -> Text Annotation -> LLM Judge Configuration -> Model Evaluation -> Results Analysis

**Critical Path**: The evaluation pipeline flows from dataset construction through annotation to LLM judging, with each stage building upon previous ones to ensure culturally relevant, comprehensive assessment.

**Design Tradeoffs**: Using a single LLM judge (Claude 3.7 Sonnet) provides consistency but may introduce bias; relying on Torrance Tests ensures structured evaluation but may not capture all aspects of Persian literary creativity.

**Failure Signatures**: Inconsistent creativity scores across models may indicate judge bias; low inter-annotator agreement suggests annotation challenges; poor correlation with human evaluations indicates framework misalignment.

**3 First Experiments**:
1. Test framework with different LLM judges to assess score stability
2. Validate annotation process with multiple Persian literature experts
3. Evaluate framework transferability to another non-English language

## Open Questions the Paper Calls Out
None

## Limitations
- Cultural specificity may limit framework transferability to other languages
- Single LLM judge introduces potential bias in creativity assessment
- Dataset construction may not represent full diversity of Persian literature
- Four creativity dimensions may overlook other relevant Persian literary aspects

## Confidence

**High Confidence**: CPers dataset construction (4,371 texts, 20 themes) and annotation process for 200 texts demonstrate robust methodology. Comparative performance rankings of different LLMs are supported by systematic evaluation.

**Medium Confidence**: Claim that culturally grounded evaluation frameworks are necessary for non-English languages is well-supported by Persian case study, but generalizability requires further validation.

**Low Confidence**: Assertion that DeepSeek-R1 represents best overall model is tentative due to single judge model and lack of human validation.

## Next Checks
1. Conduct human evaluation studies with Persian literature experts to validate LLM judge's assessments
2. Test framework's transferability by adapting it for another non-English language with distinct literary traditions
3. Perform ablation studies using different LLM judges to assess stability of creativity rankings