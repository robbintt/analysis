---
ver: rpa2
title: Joint Learning of Depth and Appearance for Portrait Image Animation
arxiv_id: '2501.08649'
source_url: https://arxiv.org/abs/2501.08649
tags:
- depth
- image
- diffusion
- arxiv
- portrait
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel diffusion-based architecture for jointly
  learning depth and appearance in portrait image animation. The method extends a
  pre-trained latent diffusion model to simultaneously generate RGB and depth outputs
  by expanding the network to handle 6-channel inputs (3 for RGB, 3 for depth) and
  using a reference network to maintain identity consistency.
---

# Joint Learning of Depth and Appearance for Portrait Image Animation

## Quick Facts
- arXiv ID: 2501.08649
- Source URL: https://arxiv.org/abs/2501.08649
- Reference count: 40
- Primary result: State-of-the-art monocular depth estimation (AbsRel: 0.162, δ1: 0.765, RMSE: 0.047) on studio test set

## Executive Summary
This paper proposes a novel diffusion-based architecture for jointly learning depth and appearance in portrait image animation. The method extends a pre-trained latent diffusion model to simultaneously generate RGB and depth outputs by expanding the network to handle 6-channel inputs (3 for RGB, 3 for depth) and using a reference network to maintain identity consistency. The model is trained on a combination of studio-captured facial images with ground truth 3D geometry and in-the-wild facial videos with pseudo-ground truth depth.

The primary contributions include achieving state-of-the-art monocular depth estimation (0.162 AbsRel, 0.765 δ1, 0.047 RMSE on studio test set), enabling depth-based image editing and relighting, and extending the model for audio-driven talking head animation with consistent 3D output. The joint learning approach ensures correlation between appearance and depth channels, enabling applications like depth-conditioned image generation and facial animation with synchronized 3D geometry.

## Method Summary
The proposed method extends a pre-trained latent diffusion model to jointly generate RGB and depth outputs. The architecture expands the standard Stable Diffusion UNet to handle 6-channel inputs by concatenating independently noised RGB and depth latents. A reference network (copy of the denoising UNet) extracts identity features from a reference image, which are injected into the main generation UNet via spatial attention layers. The model is trained on a hybrid dataset combining studio-captured facial images with ground truth 3D geometry and in-the-wild facial videos with pseudo-ground truth depth estimated by 3DMM fitting.

## Key Results
- Achieves state-of-the-art monocular depth estimation (AbsRel: 0.162, δ1: 0.765, RMSE: 0.047) on studio test set
- Enables depth-based image editing and relighting applications
- Extends to audio-driven talking head animation with synchronized 3D geometry
- Demonstrates improved depth accuracy compared to standard monocular depth estimation methods

## Why This Works (Mechanism)

### Mechanism 1: Latent Channel Concatenation for Joint Distribution
The authors expand the standard 4-channel Stable Diffusion UNet to handle a 6-channel input (3 for RGB latent, 3 for Depth latent). By adding noise independently to the RGB and Depth latents but processing them through a shared denoising backbone, the cross-attention layers within the UNet implicitly learn the structural relationship between texture and geometry.

### Mechanism 2: ReferenceNet for Identity Preservation
The architecture employs a "ReferenceNet," which is a copy of the denoising UNet. It processes a reference image to extract spatial features. These features are injected into the main generation UNet via spatial attention layers, allowing the main network to "query" the identity features of the reference image while generating new frames or poses.

### Mechanism 3: Hybrid Data Curriculum for Generalization
The model is trained on "ground truth" depth from studio scans and "pseudo-ground truth" depth estimated by a 3D morphable model on in-the-wild videos. This exposes the diffusion model to the variance of real-world faces while anchoring depth accuracy to scan data.

## Foundational Learning

- **Concept**: Latent Diffusion Models (LDMs) & VAEs
  - **Why needed here**: The paper operates in a compressed "latent space" rather than pixel space to save memory.
  - **Quick check question**: If the VAE has a reconstruction error on teeth, will the final output have perfect teeth? (Answer: Likely no).

- **Concept**: Attention Mechanisms (Spatial vs. Cross-Attention)
  - **Why needed here**: The ReferenceNet injects features via attention. Distinguishing between self-attention (context within the image) and cross-attention (context from external inputs like the reference image) is key to debugging identity transfer.
  - **Quick check question**: In the ReferenceNet injection, which acts as the Key/Value and which acts as the Query?

- **Concept**: Inpainting (Channel-wise)
  - **Why needed here**: The paper uses "channel-wise inpainting" for tasks like Depth-to-Image. This involves masking specific channels while keeping others visible to the diffusion process.
  - **Quick check question**: If you mask the depth channel during inference but provide a depth map, how does the model interpret this?

## Architecture Onboarding

- **Component map**: Reference Image → ReferenceNet (Parallel UNet); Target Noise → Main UNet (6-channel); Audio/Pose → Motion Modules (Cross-Attention)
- **Critical path**: 1) Encode Reference Image using ReferenceNet to cache intermediate feature maps. 2) Sample random noise (6 channels). 3) Denoise step-by-step using Main UNet, attending to ReferenceNet features. 4) Decode final 6-channel latent to pixel space.
- **Design tradeoffs**: Channel Expansion vs. Separate Models - the authors choose to concatenate RGB and Depth to ensure correlation but requires modifying the base model architecture.
- **Failure signatures**: "Janus" Problem - if the depth estimation creates multiple face fronts, the 3DMM pseudo-data may be misaligned with the RGB data.
- **First 3 experiments**: 1) Ablate ReferenceNet to verify identity preservation capability. 2) Run inference on a standard face dataset using the "Image-to-Depth" inpainting mode to verify AbsRel metrics. 3) Input a single reference image and ask for unconditional generation to check depth-RGB alignment.

## Open Questions the Paper Calls Out

1. Can the model be extended to predict accurate geometry for non-skin regions, such as hair, accessories, and the upper body? The authors state the model currently "can only predict depth maps only for the skin region" because it relies on depth maps derived from registered 3D geometry which typically excludes these areas.

2. To what extent does scaling training data and duration improve performance relative to state-of-the-art 2D-only audio-driven methods? The authors note they were limited by "modest computational resources," preventing them from scaling training datasets and times to levels comparable with existing state-of-the-art methods.

3. Does the reliance on 3DMM-fitted pseudo-ground truth for in-the-wild data limit the model's ability to learn fine-grained, out-of-distribution geometric details? The training pipeline still relies on monocular tracking for in-the-wild depth, which may constrain the joint learning when pseudo-ground truth is poor.

## Limitations
- Limited to skin regions - cannot predict accurate geometry for hair, accessories, or upper body
- Dependent on 3DMM pseudo-ground truth - may inherit limitations of monocular tracking for extreme poses
- Computational constraints prevented scaling to larger datasets comparable to state-of-the-art methods

## Confidence
- Depth estimation claims (AbsRel 0.162): **High** - supported by standard metrics and comparison to baselines
- Identity preservation through ReferenceNet: **Medium** - ablation shows improvement, but qualitative evaluation dominates
- Generalization to in-the-wild data: **Medium** - hybrid training helps, but test set composition unclear
- Audio-driven animation quality: **Low** - qualitative results shown, but quantitative metrics limited

## Next Checks
1. **Ablation replication**: Train without ReferenceNet on same data split to verify identity drift claims
2. **Cross-dataset depth evaluation**: Test depth estimation on a publicly available face dataset (e.g., FaceScape) to assess generalization beyond studio data
3. **Depth-RGB alignment audit**: Perform systematic visual inspection of 100 generated samples to quantify RGB-depth misalignment frequency and severity