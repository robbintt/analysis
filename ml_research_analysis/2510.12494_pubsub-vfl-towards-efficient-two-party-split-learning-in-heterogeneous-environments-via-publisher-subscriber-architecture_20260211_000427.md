---
ver: rpa2
title: 'PubSub-VFL: Towards Efficient Two-Party Split Learning in Heterogeneous Environments
  via Publisher/Subscriber Architecture'
arxiv_id: '2510.12494'
source_url: https://arxiv.org/abs/2510.12494
tags:
- data
- learning
- should
- pubsub-vfl
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PubSub-VFL addresses computational inefficiency in two-party split
  learning by combining Publisher/Subscriber decoupling with hierarchical asynchronous
  mechanisms. It introduces embedding and gradient channels with buffer and deadline
  mechanisms to enable asynchronous communication while preserving data alignment.
---

# PubSub-VFL: Towards Efficient Two-Party Split Learning in Heterogeneous Environments via Publisher/Subscriber Architecture

## Quick Facts
- arXiv ID: 2510.12494
- Source URL: https://arxiv.org/abs/2510.12494
- Reference count: 40
- Primary result: 2-7× faster training with up to 91.07% CPU utilization without accuracy loss

## Executive Summary
PubSub-VFL introduces a Publisher/Subscriber architecture for efficient two-party vertical federated learning that addresses computational inefficiency through hierarchical asynchrony. The system decouples data alignment from model training using embedding and gradient channels with buffer and deadline mechanisms, enabling asynchronous communication while preserving data alignment. A privacy-preserving dynamic programming optimizer determines optimal worker allocation and batch size based on system profiles to handle resource and data heterogeneity. Theoretical analysis proves stable convergence under differential privacy, while experiments on five datasets demonstrate significant improvements in computation and communication efficiency without sacrificing accuracy.

## Method Summary
PubSub-VFL implements a two-party split learning framework where the Active Party holds labels and the top model while the Passive Party holds features and the bottom model. The method replaces synchronous communication with a Publisher/Subscriber architecture using dedicated embedding and gradient channels indexed by batch IDs. Each party maintains a Parameter Server and multiple workers, with a one-time profiling phase to measure computation and communication costs. A dynamic programming optimizer then determines the optimal number of workers and batch size to balance load between parties. The training process employs FIFO buffers with capacity limits and waiting deadlines to manage asynchrony, while a semi-asynchronous update strategy adapts synchronization intervals based on training progress. Gaussian Differential Privacy is applied to embeddings to ensure privacy preservation.

## Key Results
- Achieves 2-7× faster training compared to state-of-the-art baselines
- Reaches up to 91.07% CPU utilization in heterogeneous environments
- Maintains model accuracy while significantly reducing waiting time per epoch
- Successfully handles resource and data heterogeneity through profile-driven optimization

## Why This Works (Mechanism)

### Mechanism 1: Publisher/Subscriber Decoupling for ID Alignment
The system replaces direct peer-to-peer blocking communication with a Publisher/Subscriber model where unique batch IDs are assigned to training batches. Passive Party workers publish embeddings to specific "embedding channels" indexed by Batch ID, while Active Party workers subscribe to these channels. This decouples the arrival times of intermediate results, allowing faster workers to proceed without waiting for slower peers, provided the correct Batch ID is maintained.

### Mechanism 2: Hierarchical Asynchrony via Buffers and Deadlines
The system employs a two-level asynchrony approach: inter-party channel buffers use FIFO queues where old data is dropped when full, and a "Waiting Deadline" forces workers to skip batches if data isn't received in time. Intra-party semi-asynchronous updates use a dynamic synchronization interval that adapts via a tanh function based on training progress, tightening synchronization as accuracy increases.

### Mechanism 3: Profile-Driven Resource Optimization
Before training, the system profiles computation and communication costs to generate delay model constants. It then solves a minimization problem using Dynamic Programming to find the optimal configuration (number of workers and batch size) that balances total time between the two parties. This approach mitigates computational imbalance caused by resource heterogeneity.

## Foundational Learning

- **Concept: Vertical Federated Learning (VFL)** - Needed because PubSub-VFL is designed for VFL where data is partitioned by features requiring strict ID alignment. Quick check: Can you distinguish between the "Active Party" (holding labels) and the "Passive Party" (holding only features) in a split learning architecture?

- **Concept: Publisher/Subscriber (Pub/Sub) Pattern** - Needed as the core architectural shift from tightly coupled protocols to a loosely coupled channel system. Quick check: How does a Pub/Sub system ensure that a message published to "Batch ID 101" is consumed only by the subscriber listening for "Batch ID 101"?

- **Concept: Differential Privacy (DP) in Embeddings** - Needed because the paper claims compatibility with DP applied to embeddings to prevent inference attacks, which adds noise and affects convergence. Quick check: If Gaussian noise is added to the embeddings before they enter the Pub/Sub channels, which party typically adds this noise, and which party receives the noisy data?

## Architecture Onboarding

- **Component map:** Active Party (PS_a, Workers w_a) -> Embedding Channels C_e[Batch ID] <- Passive Party (PS_p, Workers w_p) -> Gradient Channels C_g[Batch ID]

- **Critical path:**
  1. Profiling: Measure T_calc and T_comm to generate cost model constants
  2. Planning: Solve optimization equation to determine optimal worker counts and batch size
  3. Publishing: Passive Party workers compute embeddings + DP noise and publish to C_e[Batch ID]
  4. Subscribing: Active Party workers pull from C_e, compute top model, and publish gradients to C_g[Batch ID]
  5. Updating: Passive Party workers pull gradients from C_g; local PS aggregates updates every ΔT_t steps

- **Design tradeoffs:** Stale gradients vs. speed (larger buffers increase parallelism but risk using stale data), privacy vs. convergence (higher DP noise improves security but lowers accuracy), complexity (Batch ID consistency adds implementation overhead)

- **Failure signatures:** Channel congestion (buffer overflow causing FIFO drops), sync collapse (accuracy oscillation from excessive asynchrony), ID misalignment (model failure from mismatched Batch IDs)

- **First 3 experiments:**
  1. Latency micro-benchmark: Measure "Waiting Time" per epoch for PubSub-VFL vs. baseline VFL-PS
  2. Heterogeneity stress test: Run with skewed CPU core ratios to confirm optimizer maintains high CPU utilization
  3. Convergence under DP: Train with increasing privacy noise to verify theoretical convergence claims

## Open Questions the Paper Calls Out

- **Multi-party extension:** How can the PubSub-VFL architecture be extended to support multi-party vertical federated learning without incurring prohibitive complexity in ID alignment or communication overhead? The current design is optimized for two-party interaction, while multi-party settings introduce complex dependencies.

- **DP error floor reduction:** Can the convergence error floor induced by Gaussian Differential Privacy be reduced without compromising privacy guarantees? The theoretical analysis shows an asymptotic error floor proportional to DP noise variance, indicating a strict privacy-accuracy trade-off.

- **Dynamic resource adaptation:** How robust is the dynamic programming optimizer when system resource profiles fluctuate dynamically during execution? The methodology assumes stable hardware conditions, but real-world resource contention could invalidate the pre-computed optimal configuration.

## Limitations

- The architecture only supports two-party learning and has not been extended to multi-party scenarios
- The dynamic programming optimizer assumes static system profiles, which may not hold in dynamic environments
- The FIFO buffer size and waiting deadline parameters appear empirically chosen without comprehensive sensitivity analysis

## Confidence

- **High confidence:** The Pub/Sub decoupling mechanism fundamentally works as described - separating ID alignment from training via channel-based communication is architecturally sound
- **Medium confidence:** The theoretical convergence proof under differential privacy is valid, but real-world validation requires testing across diverse network conditions
- **Low confidence:** The optimality claims of the dynamic programming planner depend heavily on accurate delay model constants that are difficult to estimate precisely

## Next Checks

1. **Staleness sensitivity test:** Systematically vary FIFO buffer sizes and deadlines to measure the tradeoff between throughput and accuracy degradation

2. **Dynamic resource perturbation:** Introduce artificial CPU/memory contention during training to test whether the static configuration breaks down

3. **Cross-dataset generalization:** Validate performance consistency on datasets with different feature dimensionality ratios and label distributions