---
ver: rpa2
title: Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings
arxiv_id: '2602.00574'
source_url: https://arxiv.org/abs/2602.00574
tags:
- reasoning
- latent
- visual
- arxiv
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces modal-mixed chain-of-thought (CoT) reasoning,
  extending CoT beyond language to better handle multimodal reasoning. It proposes
  a method that interleaves textual tokens with compact visual sketches represented
  as latent embeddings, using the vision-language model (VLM) itself as an encoder
  and training it to reconstruct its own intermediate vision embeddings.
---

# Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings

## Quick Facts
- arXiv ID: 2602.00574
- Source URL: https://arxiv.org/abs/2602.00574
- Authors: Yifei Shao; Kun Zhou; Ziming Xu; Mohammad Atif Quamar; Shibo Hao; Zhen Wang; Zhiting Hu; Biwei Huang
- Reference count: 19
- Primary result: Introduces modal-mixed CoT reasoning that interleaves text tokens with compact visual sketches via latent embeddings, achieving 25.7% average accuracy on vision-intensive reasoning tasks

## Executive Summary
This paper extends chain-of-thought (CoT) reasoning from language-only to multimodal contexts by introducing modal-mixed CoT reasoning. The approach interleaves textual tokens with compact visual sketches represented as latent embeddings, allowing the vision-language model to handle both symbolic and perceptual reasoning in a unified framework. A diffusion-based latent decoder generates visual sketches conditioned on the VLM's hidden states, enabling the model to offload fine-grained visual processing while focusing on high-level reasoning. The method is trained through supervised fine-tuning on annotated traces followed by reinforcement learning to optimize modality switching and chain composition.

## Method Summary
The method introduces modal-mixed chain-of-thought reasoning by representing visual sketches as compact latent embeddings rather than raw pixels or tokens. The vision-language model serves as both encoder and reconstructor of intermediate vision embeddings, while a diffusion-based latent decoder generates the visual sketches from these latents conditioned on the VLM's hidden states. Training occurs in two stages: first supervised fine-tuning on annotated reasoning traces that interleave text and latents, then reinforcement learning to optimize when to switch modalities and how to compose long reasoning chains. This approach allows the VLM to handle high-level reasoning while delegating fine-grained visual processing to the latent decoder.

## Key Results
- Achieves 25.7% average accuracy on vision-intensive reasoning tasks, outperforming Qwen2.5-VL-7B-Instruct (24.3%) and Janus-Pro-7B (23.4%)
- Demonstrates consistent gains across 11 diverse multimodal reasoning tasks
- Shows effective modality switching learned through reinforcement learning
- Validates the approach's effectiveness in handling both symbolic and perceptual reasoning components

## Why This Works (Mechanism)
The method works by decomposing multimodal reasoning into alternating symbolic and perceptual steps, where the VLM handles symbolic reasoning while latent embeddings capture perceptual information. By using the VLM itself as an encoder for intermediate vision embeddings, the approach creates a self-consistent representation that aligns with the model's internal reasoning process. The diffusion-based latent decoder allows generation of compact visual sketches that preserve essential visual information while being computationally efficient. The two-stage training pipeline enables the model to first learn from human-annotated traces and then optimize its reasoning strategy through reinforcement learning, improving both the quality of individual steps and the overall chain composition.

## Foundational Learning
- **Latent embeddings for visual representation**: Compact, compressed representations of visual information that preserve essential features while reducing dimensionality - needed to efficiently interleave visual sketches with text tokens in the same reasoning chain; quick check: verify latent dimensions maintain sufficient visual information for downstream reasoning tasks
- **Diffusion-based generative modeling**: Probabilistic models that learn to denoise data step-by-step to generate samples - needed to reconstruct visual sketches from latent embeddings in a controlled, high-quality manner; quick check: assess visual sketch quality and fidelity to original inputs
- **Vision-language model encoding**: Multi-modal architectures that process both visual and textual inputs through shared or aligned representations - needed as the backbone for multimodal reasoning and as the encoder for vision embeddings; quick check: evaluate VLM's ability to accurately reconstruct its own intermediate vision embeddings
- **Reinforcement learning for reasoning optimization**: Training paradigm where agents learn through reward-based feedback rather than direct supervision - needed to optimize modality switching decisions and chain composition beyond what's possible with supervised fine-tuning alone; quick check: measure improvement in reasoning chain quality and task performance after RL fine-tuning
- **Chain-of-thought reasoning**: Explicit intermediate reasoning steps that break down complex problems into simpler sub-problems - needed as the foundational framework that the modal-mixed approach extends from language-only to multimodal contexts; quick check: verify that intermediate steps meaningfully contribute to final task performance
- **Modality switching**: Dynamic decision-making about when to use text versus visual representations in reasoning - needed to balance computational efficiency with reasoning effectiveness; quick check: analyze patterns of modality switching across different task types

## Architecture Onboarding

Component map: Visual input → VLM Encoder → Latent Embeddings ↔ Diffusion Decoder → Visual Sketches → VLM Decoder → Text Output

Critical path: The core processing path where visual inputs are encoded into latent embeddings, which are then decoded into visual sketches when needed for reasoning steps, with the VLM processing both text tokens and latent embeddings to produce intermediate reasoning steps and final outputs.

Design tradeoffs: The approach trades off computational efficiency and model complexity for improved multimodal reasoning capabilities. Using latent embeddings rather than raw visual tokens reduces the interleaving overhead but may lose fine-grained visual details. The self-encoding approach leverages the VLM's existing capabilities but introduces potential instability through compounding errors. The two-stage training pipeline provides better optimization but requires substantial annotated data.

Failure signatures: Performance degradation may occur when visual sketches lose critical details necessary for reasoning, when the VLM fails to accurately reconstruct intermediate vision embeddings, or when reinforcement learning fails to learn effective modality switching strategies. Computational bottlenecks may arise from repeated latent encoding/decoding during inference.

First experiments:
1. Ablation study removing the diffusion decoder to test if raw latent embeddings alone suffice for reasoning
2. Training stability analysis comparing single-stage versus two-stage training approaches
3. Cross-task generalization evaluation on out-of-distribution multimodal reasoning problems

## Open Questions the Paper Calls Out
None

## Limitations
- The self-encoding approach introduces potential instability and compounding errors, particularly as model scale increases
- Substantial annotated reasoning traces are required for the two-stage training pipeline, raising concerns about data efficiency
- Visual sketch representations via latent embeddings may lose critical perceptual details necessary for complex visual reasoning
- The relatively low absolute accuracy (25.7%) on vision-intensive tasks indicates the approach is still far from solving complex multimodal reasoning problems
- Computational overhead during inference, particularly the cost of generating visual sketches through the diffusion decoder at each reasoning step, is not adequately addressed

## Confidence
- **High Confidence**: The core methodology of interleaving text tokens with latent visual sketches is technically sound and represents a novel extension of CoT reasoning to multimodal contexts
- **Medium Confidence**: The two-stage training approach (supervised fine-tuning + RL) is likely effective but requires validation across more diverse model architectures and task domains
- **Medium Confidence**: The claimed performance improvements over baselines are supported by experimental results, though the relatively modest absolute accuracy gains warrant cautious interpretation

## Next Checks
1. Evaluate model performance and training stability when scaling to larger VLM architectures (e.g., 34B+ parameters) to assess whether the self-encoding approach remains viable
2. Conduct ablation studies systematically removing either the supervised fine-tuning stage or the RL stage to quantify their individual contributions to performance gains
3. Test the approach on out-of-distribution visual reasoning tasks not seen during training to evaluate true generalization capabilities beyond the 11 benchmark tasks used in the paper