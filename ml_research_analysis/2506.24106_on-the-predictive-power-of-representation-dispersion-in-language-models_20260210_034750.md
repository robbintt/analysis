---
ver: rpa2
title: On the Predictive Power of Representation Dispersion in Language Models
arxiv_id: '2506.24106'
source_url: https://arxiv.org/abs/2506.24106
tags:
- perplexity
- dispersion
- distance
- llama-3
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Representation dispersion\u2014the average pairwise cosine distance\
  \ among a language model\u2019s contextual embeddings\u2014strongly correlates with\
  \ model perplexity across multiple model families (LLaMA, Qwen, Mistral, Phi, Gemma)\
  \ and domains (Wikipedia, news, medical abstracts). Lower perplexity aligns with\
  \ more dispersed embeddings, even within semantically similar clusters, suggesting\
  \ broader embedding geometry enables sharper next-token predictions."
---

# On the Predictive Power of Representation Dispersion in Language Models

## Quick Facts
- **arXiv ID**: 2506.24106
- **Source URL**: https://arxiv.org/abs/2506.24106
- **Reference count**: 40
- **Primary result**: Representation dispersion strongly correlates with model perplexity and can predict downstream task accuracy without labels.

## Executive Summary
This paper introduces representation dispersion—the average pairwise cosine distance among contextual embeddings—as a powerful predictor of language model performance. The authors demonstrate that lower perplexity correlates strongly with more dispersed embeddings across multiple model families including LLaMA, Qwen, Mistral, Phi, and Gemma. This relationship holds across diverse domains such as Wikipedia, news, and medical abstracts. The study reveals that broader embedding geometry enables sharper next-token predictions and provides practical applications for model selection and training optimization.

The findings establish representation dispersion as both a diagnostic tool and a training signal. By measuring dispersion on unlabeled text, practitioners can predict downstream task accuracy without requiring labeled data. Additionally, models with larger dispersion gaps between domain-specific and general tokens achieve superior performance in specialized tasks like math and code generation. The paper also shows that incorporating a push-away objective during training directly reduces perplexity, offering a geometry-based approach to more effective and interpretable language models.

## Method Summary
The authors measured representation dispersion by calculating the average pairwise cosine distance among contextual embeddings generated by various language models across multiple domains. They analyzed several model families (LLaMA, Qwen, Mistral, Phi, Gemma) and evaluated performance on Wikipedia, news, and medical abstracts. The study employed correlation analysis between dispersion metrics and perplexity scores, examined downstream task performance predictions using dispersion measurements, and tested the effectiveness of dispersion-based layer selection for kNN-LM retrieval-augmented methods. Additionally, they implemented a push-away objective during training to increase dispersion and measured its impact on perplexity reduction in both single and cross-domain scenarios.

## Key Results
- Lower perplexity strongly correlates with more dispersed embeddings across multiple model families
- Dispersion measured on unlabeled text predicts downstream task accuracy without requiring labels
- Incorporating a push-away objective during training directly reduces perplexity in both single- and cross-domain scenarios

## Why This Works (Mechanism)
Representation dispersion captures the geometric spread of contextual embeddings in the embedding space. When embeddings are more dispersed, they occupy distinct regions of the space, reducing overlap and ambiguity in representing different semantic concepts. This geometric separation enables the model to make sharper distinctions between similar tokens and contexts, leading to more confident predictions and lower perplexity. The mechanism works because language models rely on the relative positions of embeddings to determine token probabilities, and greater dispersion provides better discrimination capacity within the learned representation space.

## Foundational Learning
- **Representation dispersion**: The average pairwise cosine distance among contextual embeddings. Why needed: Serves as the core metric linking embedding geometry to model performance. Quick check: Calculate dispersion for sample embeddings and verify it increases with semantic diversity.
- **Cosine distance**: A measure of similarity between vectors based on the angle between them. Why needed: Provides a scale-invariant way to measure embedding relationships. Quick check: Compute cosine distance between orthogonal vectors (should be 1.0).
- **Perplexity**: A measurement of how well a probability model predicts a sample. Why needed: Standard metric for evaluating language model quality. Quick check: Verify perplexity decreases as prediction accuracy increases.
- **kNN-LM**: A retrieval-augmented method that uses k-nearest neighbors from a datastore to improve language model predictions. Why needed: Demonstrates practical application of dispersion-based layer selection. Quick check: Compare perplexity with and without kNN augmentation.
- **Push-away objective**: A training objective that explicitly encourages greater dispersion among embeddings. Why needed: Provides causal evidence for the dispersion-perplexity relationship. Quick check: Train with and without push-away objective and measure dispersion differences.
- **Contextual embeddings**: Vector representations of tokens that capture their meaning in context. Why needed: The fundamental unit being analyzed for dispersion. Quick check: Generate embeddings for the same word in different contexts and observe variations.

## Architecture Onboarding

**Component Map**
Token input -> Embedding layer -> Transformer layers -> Output layer -> Dispersion calculation module -> Performance metrics

**Critical Path**
Token sequence flows through transformer layers, generating contextual embeddings. These embeddings are then analyzed for dispersion, which correlates with the model's next-token prediction quality measured by perplexity.

**Design Tradeoffs**
- Computational overhead: Dispersion calculation adds minimal overhead compared to inference cost
- Interpretability vs. performance: More dispersed embeddings may trade some semantic clustering for better discrimination
- Training complexity: Push-away objective requires careful tuning to avoid destabilizing training

**Failure Signatures**
- Over-dispersed embeddings may lose semantic coherence
- Insufficient dispersion may indicate underfitting or poor representation learning
- Domain mismatch between training and evaluation data may weaken dispersion-perplexity correlation

**First Experiments**
1. Measure dispersion-accuracy correlation on a held-out validation set
2. Compare perplexity before and after applying push-away objective
3. Test layer selection for kNN-LM using different dispersion thresholds

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Causality between dispersion and perplexity remains unproven despite strong correlations
- Domain-specific findings may not generalize to specialized domains like legal or financial documents
- Layer selection methodology for kNN-LM lacks systematic evaluation across diverse tasks

## Confidence
- High Confidence: Correlation between representation dispersion and perplexity across multiple model families; relationship between dispersion and downstream task performance; effectiveness of dispersion-based layer selection for kNN-LM
- Medium Confidence: Causality established through push-away objective; cross-domain generalization of findings; practical applicability of dispersion as a diagnostic tool
- Low Confidence: Universal applicability across all NLP tasks; optimal dispersion thresholds for different applications; impact on model robustness and generalization

## Next Checks
1. Conduct controlled experiments varying only dispersion (through different training objectives) while keeping other factors constant to establish stronger causal evidence of dispersion's impact on perplexity and downstream performance.

2. Test the dispersion-perplexity relationship across diverse domain-specific datasets (legal documents, financial reports, scientific literature) to validate generalizability beyond the current domain coverage.

3. Implement systematic layer selection experiments for kNN-LM across multiple tasks and model architectures to determine optimal dispersion-based selection criteria and quantify performance gains compared to other selection methods.