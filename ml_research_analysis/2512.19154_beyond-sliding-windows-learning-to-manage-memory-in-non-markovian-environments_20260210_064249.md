---
ver: rpa2
title: 'Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments'
arxiv_id: '2512.19154'
source_url: https://arxiv.org/abs/2512.19154
tags:
- memory
- learning
- stacking
- length
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adaptive Stacking, a memory management technique
  for reinforcement learning agents in partially observable environments. The method
  allows agents to learn which observations to retain in a bounded memory stack, rather
  than passively keeping the most recent observations.
---

# Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments

## Quick Facts
- arXiv ID: 2512.19154
- Source URL: https://arxiv.org/abs/2512.19154
- Reference count: 40
- Primary result: Agents learn to selectively retain reward-predictive observations in bounded memory, matching oracle performance while using far less memory than passive Frame Stacking.

## Executive Summary
This paper introduces Adaptive Stacking, a memory management technique for reinforcement learning agents in partially observable environments. The method allows agents to learn which observations to retain in a bounded memory stack, rather than passively keeping the most recent observations. The authors prove convergence guarantees for agents using Adaptive Stacking under both unbiased optimization and TD-based learning, even with significantly smaller memory than required for full observability. Experiments on memory-intensive tasks demonstrate that Adaptive Stacking matches the performance of oracle memory agents while using far less memory, and substantially outperforms naive baselines under tight memory budgets. The approach offers a promising path toward scalable, memory-efficient RL in large, partially observable environments.

## Method Summary
Adaptive Stacking replaces passive Frame Stacking (FIFO buffer) with an active decision process where the agent learns to selectively retain observations in a bounded stack. At each step, the agent outputs a memory action selecting which observation to pop from the stack before pushing the new observation. This allows selective retention of critical information (like distant goals) while continuously updating transient state information. The method is theoretically grounded with convergence guarantees under value-consistency assumptions and is tested across multiple RL algorithms (Q-learning, PPO) and architectures (MLP, LSTM, Transformer).

## Key Results
- Adaptive Stacking matches oracle Frame Stacking performance using stack size k=2 versus k*=L+2 on T-Maze tasks of length up to 10^6
- Achieves 95% performance with only 12% of the memory budget on MiniGrid-Memory
- Maintains sample efficiency and generalization across architecture types (MLP, LSTM, Transformer)
- Reduces Transformer attention complexity from O(k^2) to O(κ^2) where κ≪k

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** An agent with bounded memory can act optimally in highly non-Markovian environments if it learns to selectively retain sparse, reward-predictive observations.
- **Mechanism:** Adaptive Stacking replaces the passive FIFO buffer of Frame Stacking with an active decision process. At each step $t$, the agent outputs a memory action $i_t \in \{1, \dots, k\}$ selecting which observation to pop from the stack $s_t$ before pushing the new observation $x_{t+1}$. This allows the agent to "hold" a critical cue (e.g., a distant goal color) indefinitely while continuously updating transient state information (e.g., current position), effectively compressing the history $x_{t:t-k^*}$ into a stack $s_t$ of size $k \ll k^*$.
- **Core assumption:** The environment's optimal policy depends causally on only a small subset of past observations $\kappa$ (the "minimal sufficient memory length"), even if they are separated by long time lags $k^*$.
- **Evidence anchors:**
  - [abstract] "...environment only causally depends on a relatively small number of observations... learn the removal of memories not predictive of future rewards."
  - [section 4] Equation 1 ($s_{t+1} = \text{push}(\text{pop}(s_t, i_t), x_{t+1})$) and Figure 2 illustrating the retention of the goal cue over grey corridor steps.
  - [corpus] Paper 64786 (ELMUR) supports the need for external memory layers in long-horizon RL; Paper 7131 supports the efficiency of short context windows over long contexts.
- **Break condition:** Fails if $\kappa > k$ (stack is physically too small to hold necessary information) or if the reward depends on complex interactions between *all* recent observations (dense dependency).

### Mechanism 2
- **Claim:** Convergence to an optimal policy is theoretically guaranteed under Temporal Difference (TD) learning if the memory abstraction is "value-consistent."
- **Mechanism:** Adaptive Stacking functions as a form of state abstraction where multiple latent histories $x_{t:t-k^*}$ map to the same stack state $s_t$. Theorem 2 proves that as long as this mapping preserves the partial ordering of values (Assumption 4.1: Value-Consistency), standard Q-learning converges to the optimal policy. This allows the agent to safely discard "irrelevant" history without losing decision-making capacity.
- **Core assumption:** "Value-consistency" holds, meaning all latent histories compatible with a given memory state have the same expected return (e.g., in goal-reaching tasks, the path taken doesn't matter, only the current position and goal memory).
- **Evidence anchors:**
  - [section 4.3] Definition of Value-Consistency and Theorem 2 (Partial-order Preserving).
  - [appendix d] Table 2 lists benchmarks (e.g., T-Maze, MiniGrid) where this assumption holds vs. fails (e.g., Reward Machines).
- **Break condition:** Fails in environments where the reward depends on the specific trajectory history (e.g., "do not revisit states") if that history is compressed away, violating value-consistency.

### Mechanism 3
- **Claim:** Reducing the context length via Adaptive Stacking improves sample efficiency and generalization by exponentially reducing the policy search space.
- **Mechanism:** Frame Stacking with window $k^*$ induces a search space of size $O(|\mathcal{X}|^{k^*})$. Adaptive Stacking reduces this to $O(|\mathcal{X}|^k)$ where $k \ge \kappa$. By filtering out non-predictive observations (distractors), the agent avoids overfitting to spurious correlations in the full history. This enables an agent trained on short mazes (e.g., length 16) to generalize to mazes of length $10^6$ because the relevant "state" (position + goal) remains structurally identical.
- **Core assumption:** Distractor observations are indeed irrelevant to value prediction.
- **Evidence anchors:**
  - [section 1] "...exponential increase in the dimensionality... severe increase in compute... poor sample efficiency."
  - [section 5] Figure 24 shows AS maintains high optimality while FS degrades on out-of-distribution maze lengths.
- **Break condition:** If observations discarded as "distractors" actually contain latent information useful for credit assignment in more complex tasks not captured by $\kappa$.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Processes (POMDPs) vs. $k$-order MDPs**
  - **Why needed here:** The paper frames the problem as a $k^*$-order NMDP where the history $x_{t:t-k^*}$ is required for Markov property. You must understand why simply seeing the current frame $x_t$ is insufficient (non-Markovian) and why standard Frame Stacking is a brute-force solution to this.
  - **Quick check question:** In a Passive T-Maze of length 100, what is the $k^*$ required for Frame Stacking versus the $\kappa$ required for Adaptive Stacking?

- **Concept: Policy Gradient / Actor-Critic Methods**
  - **Why needed here:** The Adaptive Stacking "pop" action is learned via RL (specifically PPO in experiments). The agent has two heads: an environment action policy $\pi(a_t|s_t)$ and a memory management policy $\pi(i_t|s_t)$. You need to grasp how gradients flow to the memory policy based on the reward.
  - **Quick check question:** Does the "pop" action $i_t$ receive an immediate intrinsic reward, or is it shaped solely by the environment reward $r_{t+1}$?

- **Concept: Transformers and Attention Complexity**
  - **Why needed here:** The paper explicitly calculates compute savings for Transformers, noting attention is $O(k^2)$. Understanding this quadratic cost is essential to see why reducing context length $k^*$ to $\kappa$ is a massive efficiency gain for sequence models.
  - **Quick check question:** If a Transformer agent uses Frame Stacking with $k=1000$ vs Adaptive Stacking with $k=4$, what is the approximate ratio of reduction in FLOPs for the attention mechanism?

## Architecture Onboarding

- **Component map:**
  1. Input: Observation $x_t$
  2. State/Stack: $s_t = [x_{i_1}, \dots, x_{i_k}]$ (buffer of selected observations)
  3. Encoder: Shared backbone (MLP/LSTM/Transformer) processing $s_t$
  4. Heads:
     - Env-Head: Outputs $a_t \in \mathcal{A}$
     - Mem-Head: Outputs $i_t \in \{1, \dots, k\}$ (index to pop)
  5. Transition: `Stack_{t+1} = PUSH( POP(Stack_t, i_t), x_{t+1} )`

- **Critical path:** The gradient flow from the Environment Reward to the **Mem-Head**. The system fails if the Mem-Head learns a trivial policy (e.g., always pop index 0, reducing to Frame Stacking). It must learn the correlation between retaining specific observations and delayed rewards.

- **Design tradeoffs:**
  - **Stack Size ($k$):** Must be $\ge \kappa$ (minimal sufficient memory). Setting $k$ too small drops critical info; setting $k$ too large increases compute and introduces noise/distraction.
  - **Architecture:** Transformers offer exponential compute savings ($k^2 \to \kappa^2$) but may struggle with credit assignment compared to LSTMs in some benchmarks.

- **Failure signatures:**
  - **Passive Memory Regret:** High value on this metric (Figure 4) indicates the agent is discarding the goal cue.
  - **FIFO Mimicry:** If analysis of $i_t$ distribution shows uniform preference for the oldest index, the agent has failed to learn adaptive retention and is behaving like inefficient Frame Stacking.

- **First 3 experiments:**
  1. **T-Maze Sanity Check:** Implement Passive T-Maze (Figure 1). Train AS with $k=2$ against FS with $k=L+2$. Verify AS solves length $10^6$ maze after training on length 16.
  2. **Ablation on $k$:** Run AS on T-Maze with $k=\{2, 3, 4, 5\}$. Plot success rate vs. maze length to verify performance is independent of stack size once $k \ge \kappa=2$.
  3. **Architecture Comparison:** Implement AS with an MLP vs. Transformer backbone on MiniGrid-Memory (Figure 6d). Compare sample efficiency to verify the paper's claim of architecture-agnostic performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can Adaptive Stacking be integrated with automaton inference to handle environments where the Value-Consistency assumption is violated?
- **Basis in paper:** [explicit] Appendix D.1 explicitly states this distinction is useful for "future work aiming to blend Adaptive Stacking with automaton inference" to delineate boundaries of abstraction.
- **Why unresolved:** The current theoretical guarantees rely on Assumption 4.1, which fails in history-sensitive tasks like those with unobservable reward machines.
- **What evidence would resolve it:** A converged policy in a domain with unobservable reward logic (e.g., "Deliver Coffee" tasks) that maintains optimality despite the violation of Value-Consistency.

### Open Question 2
- **Question:** To what extent does Adaptive Stacking mitigate "attention dilution" and "recency bias" in Transformer-based agents compared to standard sliding windows?
- **Basis in paper:** [inferred] Appendix A suggests Adaptive Stacking can resolve attention dilution in LLMs, but the main experiments focus on RL returns rather than analyzing attention weights or context utilization.
- **Why unresolved:** While the method implicitly truncates context, the specific interaction between the adaptive stack and the softmax normalization in attention mechanisms remains unanalyzed.
- **What evidence would resolve it:** Comparative analysis of attention weight distributions and retrieval accuracy on long-context reasoning benchmarks using Transformer agents.

### Open Question 3
- **Question:** Can convergence guarantees be established for TD-learning when the "Value-Consistency" assumption does not hold?
- **Basis in paper:** [inferred] Theorem 2 strictly relies on Assumption 4.1 (Value-Consistency). The paper notes that while more compressed abstractions exist, they generally cannot be shown to converge with TD-learning.
- **Why unresolved:** The paper leaves open the theoretical robustness of the algorithm in the presence of the history-dependent value conflicts described in the counter-examples section.
- **What evidence would resolve it:** A proof of convergence for a policy class that learns optimally even when distinct latent histories map to the same memory state but possess different values.

## Limitations
- Theoretical guarantees rely critically on the "Value-Consistency" assumption, which may not hold in more complex environments where reward depends on specific trajectory histories
- Experimental validation is limited to discrete, synthetic environments; real-world applicability to continuous control or natural visual inputs remains unproven
- Memory management policy learns through delayed reward signals alone, which may struggle in environments with sparse rewards

## Confidence

- **High confidence:** The mechanism that Adaptive Stacking can match oracle Frame Stacking performance with dramatically smaller memory (k vs k*) in the tested environments, and the architectural feasibility of learning a memory selection policy
- **Medium confidence:** The sample efficiency claims and out-of-distribution generalization, as these depend heavily on the specific structure of the synthetic benchmarks and may not transfer to more complex tasks
- **Medium confidence:** The theoretical convergence guarantees, as they assume value-consistency which is verified empirically but may not be easily checked in advance for arbitrary environments

## Next Checks

1. **Break Assumption Test:** Design a variant of the T-Maze where the optimal policy requires remembering the *order* of previously visited colored cells (e.g., "red then blue" vs "blue then red" lead to different rewards). Verify that Adaptive Stacking fails when value-consistency is violated.

2. **Continuous Control Transfer:** Implement Adaptive Stacking on a modified Continuous Control benchmark (e.g., a partially observable navigation task with visual input from a first-person camera). Compare performance and memory usage against standard LSTM/Transformer baselines with sliding windows.

3. **Attention Efficiency Validation:** For a sequence modeling task (e.g., language modeling on a long-context dataset), implement Adaptive Stacking with a Transformer to verify the claimed O(k² → κ²) reduction in FLOPs translates to measurable training time and memory savings on real hardware.