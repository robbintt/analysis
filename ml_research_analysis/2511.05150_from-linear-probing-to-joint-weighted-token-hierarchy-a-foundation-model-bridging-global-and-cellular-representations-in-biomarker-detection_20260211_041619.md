---
ver: rpa2
title: 'From Linear Probing to Joint-Weighted Token Hierarchy: A Foundation Model
  Bridging Global and Cellular Representations in Biomarker Detection'
arxiv_id: '2511.05150'
source_url: https://arxiv.org/abs/2511.05150
tags:
- pathology
- foundation
- arxiv
- global
- biomarker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JWTH, a pathology foundation model designed
  to bridge the gap between global tissue-level embeddings and fine-grained cellular
  representations for biomarker detection in H&E-stained whole-slide images. The model
  combines large-scale self-supervised pretraining with cell-centric post-tuning and
  an attention pooling mechanism that fuses class and local tokens, addressing the
  limitation of prior models that discard cell-level cues.
---

# From Linear Probing to Joint-Weighted Token Hierarchy: A Foundation Model Bridging Global and Cellular Representations in Biomarker Detection

## Quick Facts
- arXiv ID: 2511.05150
- Source URL: https://arxiv.org/abs/2511.05150
- Reference count: 34
- This paper introduces JWTH, a pathology foundation model designed to bridge the gap between global tissue-level embeddings and fine-grained cellular representations for biomarker detection in H&E-stained whole-slide images.

## Executive Summary
JWTH addresses the limitation of existing pathology foundation models that discard fine-grained cellular cues by introducing an attention pooling mechanism that fuses class and local tokens. The model combines large-scale self-supervised pretraining with cell-centric post-tuning, achieving up to 8.3% higher balanced accuracy over state-of-the-art pathology foundation models across four biomarkers and eight cohorts. By preserving cellular-level information through hierarchical token processing, JWTH enables more interpretable and robust AI-based biomarker prediction in computational pathology.

## Method Summary
JWTH uses a Vision Transformer pretrained on 84M pathology patches with DINO, iBOT, and Koleo self-supervised objectives, followed by Gram-anchored post-training to stabilize local tokens. During inference, an attention pooling mechanism replaces linear probing by having the class token attend over all patch tokens, producing a weighted representation that combines global context with salient local morphology. The frozen encoder approach with attention pooling balances computational efficiency with performance, avoiding the instability of full fine-tuning while capturing cellular-level information critical for biomarker detection.

## Key Results
- JWTH achieves up to 8.3% higher balanced accuracy compared to state-of-the-art pathology foundation models
- The model shows an average improvement of 1.2% across four biomarkers and eight cohorts
- Attention pooling yields +3.3% average BACC over linear probing baseline

## Why This Works (Mechanism)

### Mechanism 1: Attention Pooling Replaces Linear Probing
The class token serves as a query attending over all patch tokens as keys and values, producing a weighted representation that blends global context with salient local morphology. This allows the model to selectively amplify cell-level features relevant to the biomarker task, preserving fine-grained cellular cues that linear probing discards.

### Mechanism 2: Gram-Anchored Post-Training Stabilizes Local Tokens
An auxiliary Gram-matrix regularization term during post-training prevents dimensional collapse of patch token embeddings and preserves diversity in fine-grained representations, addressing inconsistency or collapse in local tokens that can occur after large-scale self-supervised pretraining.

### Mechanism 3: Staining Augmentation Improves Cross-Domain Generalization
Random perturbations to LAB and HSV color channels during pretraining induce robustness to scanner and protocol variations across pathology centers, reducing domain gap by making the encoder learn features invariant to color distribution shifts.

## Foundational Learning

- **Vision Transformer (ViT) Token Structure**
  - Why needed here: JWTH operates on ViT outputs; understanding the distinction between class token (global) and patch tokens (local) is essential for grasping attention pooling.
  - Quick check question: Can you explain why the class token alone might miss fine-grained spatial information?

- **Self-Supervised Learning Objectives (DINO, iBOT)**
  - Why needed here: The pretraining combines image-level (DINO) and patch-level reconstruction (iBOT) losses; understanding these clarifies what the tokens encode.
  - Quick check question: What does iBOT reconstruct, and how does it differ from DINO's objective?

- **Attention Mechanism (Query-Key-Value)**
  - Why needed here: The attention pooling module uses the class token as query; understanding Q-K-V dynamics explains how local tokens are selectively weighted.
  - Quick check question: If the class token has no semantic relationship to a patch token, what attention score would you expect?

## Architecture Onboarding

- **Component map:** Input patch (256×256) → ViT encoder → tokens [cls, patch₁,..., patchₙ] → Attention pooling (Q=cls, K=V=patches) → Aggregated representation → Linear classifier → Biomarker prediction

- **Critical path:** 1) Pretrain ViT with DINO+iBOT+Koleo on 84M patches; 2) Post-train with added Gram regularization; 3) Freeze encoder; apply attention pooling + linear classifier for downstream tasks

- **Design tradeoffs:** Frozen encoder vs. fine-tuning: Freezing is computationally efficient but may leave task-irrelevant features frozen; fine-tuning is unstable for large transformers. Attention pooling vs. simple concatenation: Attention provides learned weighting but adds parameters and computation; concatenation is simpler but loses selectivity.

- **Failure signatures:** Performance no better than linear probing → local tokens likely collapsed or noisy; visualize token diversity. Large gap between normalized and non-normalized test sets → staining augmentation insufficient; test with domain-specific normalization.

- **First 3 experiments:** 1) Reproduce Table 1 ablation: compare linear probing vs. attention pooling on a held-out biomarker task. 2) Visualize attention weights: overlay top-attended patch tokens on original patches to verify cell-level focus. 3) Cross-cohort transfer test: train on one institution's data, test on another to assess staining augmentation effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
Does the JWTH architecture improve performance on slide-level biomarker prediction tasks (e.g., mutation status) when integrated into a Multiple Instance Learning (MIL) framework? The evaluation is restricted to patch-level classification tasks rather than slide-level molecular biomarkers, leaving unclear if the improved patch-level embeddings effectively aggregate to improve slide-level predictions.

### Open Question 2
Do the local tokens stabilized by Gram-anchoring explicitly correlate with cellular instances or nuclear morphology? While the paper claims post-tuning reinforces biologically meaningful cues such as nuclear morphology, the results only report classification accuracy, not the semantic alignment of tokens to cells.

### Open Question 3
How does the computational overhead of the attention pooling mechanism during inference compare to standard linear probing, particularly when processing whole-slide images? The mechanism adds computational steps that may hinder clinical deployment speed, but latency and memory impact are not discussed.

### Open Question 4
To what extent does the proposed attention pooling mechanism outperform end-to-end fine-tuning of the transformer backbone? The paper implies the method is a surrogate for fine-tuning, but a direct performance comparison is absent, leaving uncertain if the architectural change is superior to modern fine-tuning techniques.

## Limitations
- Gram-anchored post-training mechanism lacks direct empirical validation in the pathology domain, relying on inference from DINOv3 literature
- Exact hyperparameters for pretraining and post-training are unspecified, creating potential variability in reproducibility
- Clinical utility and interpretability of attention-weighted patch tokens for biomarker detection remain unproven

## Confidence

- **High Confidence**: Attention pooling mechanism improves BACC over linear probing (supported by Table 1 ablation)
- **Medium Confidence**: Gram-anchored post-training stabilizes local tokens (inferred from DINOv3; lacks direct pathology validation)
- **Medium Confidence**: Staining augmentation improves cross-domain generalization (supported by BACC gains but lacks ablation on augmentation strength)

## Next Checks

1. **Ablation Study**: Compare JWTH with and without Gram-anchored post-training on a held-out biomarker task to validate its contribution beyond standard pretraining.

2. **Attention Visualization**: Generate heatmaps of attention weights on test patches to verify that the model attends to biologically relevant cellular regions, not noise.

3. **Domain Shift Test**: Evaluate JWTH on external datasets with known staining differences (e.g., TCGA vs. local cohorts) to assess robustness beyond CRC-Norm/Unnorm.