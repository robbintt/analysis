---
ver: rpa2
title: Space Explanations of Neural Network Classification
arxiv_id: '2511.22498'
source_url: https://arxiv.org/abs/2511.22498
tags:
- https
- explanations
- space
- figure
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Space Explanations, a novel logic-based approach
  for generating interpretable and provably correct explanations for neural network
  classifications. The key idea is to compute space explanations that capture relationships
  among features and approximate decision boundaries, rather than constraining only
  individual features.
---

# Space Explanations of Neural Network Classification

## Quick Facts
- **arXiv ID:** 2511.22498
- **Source URL:** https://arxiv.org/abs/2511.22498
- **Reference count:** 40
- **Primary result:** Logic-based approach generating interpretable, provably correct explanations for NN classifications by capturing feature relationships and approximating decision boundaries

## Executive Summary
This paper introduces Space Explanations, a novel approach for generating interpretable explanations of neural network classifications. The method leverages Craig interpolation algorithms to compute logical formulas that capture regions of feature space guaranteed to produce the same classification, rather than constraining individual features. By combining interpolation with unsatisfiable core generation, the approach creates explanations that are both mathematically sound and human-readable. The experimental evaluation demonstrates that Space Explanations provide more meaningful and expressive interpretations compared to state-of-the-art methods, particularly in capturing non-trivial feature relationships and approximating decision boundaries.

## Method Summary
The method encodes neural networks and classification queries into Quantifier-Free Linear Real Arithmetic (QF_LRA) logic, then uses Craig interpolation to extract space explanations. Given an input sample and its classification, the system constructs an unsatisfiable formula combining the sample, network behavior, and negated classification. An interpolant is derived that represents a region in feature space guaranteed to yield the same classification. The approach includes strategies for generalization (selecting interpolation algorithms), reduction (simplifying explanations via unsatisfiable cores), and capture (focusing on specific feature relationships). The framework is implemented in SpEXplAIn, which interfaces with the OpenSMT2 solver.

## Key Results
- Space Explanations capture non-trivial feature relationships (e.g., linear combinations like x₁ - x₂ > C) rather than axis-aligned rectangles
- Generated explanations approximate decision boundaries more accurately than interval-based methods
- The approach scales better with input dimension than abductive methods like VeriX, though depth remains a challenge
- Unsatisfiable core reduction significantly improves formula readability while maintaining correctness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Craig Interpolation extracts sufficient conditions covering regions rather than single points
- **Mechanism:** Constructs unsatisfiable formula φₛ ∧ ψ, derives interpolant I where φₛ → I and I ∧ ψ is unsatisfiable
- **Core assumption:** NN can be accurately encoded into QF_LRA and solver finds proofs efficiently
- **Evidence:** Abstract mentions "Craig interpolation algorithms" for "provable guarantees... in continuous areas"

### Mechanism 2
- **Claim:** Linear arithmetic interpolation generates non-axis-aligned decision boundary approximations
- **Mechanism:** Uses linear combinations of features (e.g., x₁ - x₂ > C) instead of independent feature constraints
- **Core assumption:** Decision boundaries are locally approximable by linear constraints or convex polytopes
- **Evidence:** Section 3 defines Capture strategy for isolating feature relationships; Figure 5 visualizes non-trivial boundaries

### Mechanism 3
- **Claim:** Unsatisfiable cores simplify explanations without losing guarantees
- **Mechanism:** Reduces interpolant size by identifying and retaining only essential constraints for the proof
- **Core assumption:** SMT solver can extract minimal unsatisfiable cores efficiently
- **Evidence:** Section 3 defines Reduce strategy; Table 2 shows significant term count reduction with R_min

## Foundational Learning

- **Concept: Craig Interpolation**
  - **Why needed:** Engine of the paper; interpolant bridges local properties and global constraints
  - **Quick check:** Given formulas A and B where A ∧ B is UNSAT, does the interpolant I contain variables not present in A or B?

- **Concept: Satisfiability Modulo Theories (SMT) & QF_LRA**
  - **Why needed:** Relies on SMT solvers to reason over Real Arithmetic; QF_LRA limits expressiveness to linear regions
  - **Quick check:** Can QF_LRA express "If x > 5 then y < 10" natively without encoding tricks?

- **Concept: Neural Network Verification Encoding**
  - **Why needed:** Explains NNs logically by encoding ReLU activations and linear layers
  - **Quick check:** How is ReLU (max(0, x)) handled in linear logic solvers (piecewise linear encodings or linear over-approximations)?

## Architecture Onboarding

- **Component map:** Frontend -> Encoder -> SpEXplAIn -> OpenSMT2 -> Output
- **Critical path:** Choice of Interpolation Algorithm (Itp_F vs Itp_DF vs Itp_f) dictates specificity vs coverage balance
- **Design tradeoffs:**
  - Complexity vs Readability: Reduce strategy yields concise formulas but scales poorly
  - Completeness vs Scalability: Better input dimension scaling than VeriX but worse depth scaling
- **Failure signatures:**
  - Stagnation: Explanation returns only sample point; Fix: Switch to weaker interpolation algorithms
  - Timeouts on Deep Networks: Runtime spikes on >3 hidden layers; Fix: Disable expensive reduction or use Generalize only
  - NC Results: Explanations not subsets; Diagnosis: Expected for different interpolation strategies
- **First 3 experiments:**
  1. Replicate Figure 1 with Heart Attack 2D slice using Capture on (Age, Cholesterol)
  2. Replicate Figure 6a runtime scalability test comparing against abductive explainer
  3. Compare Itp_F (Strong) vs Itp_F' (Weak) on same sample point to quantify generality gap

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can specialized solver optimizations be developed to improve scalability for deep neural networks?
- **Basis:** Authors state runtime increases significantly with depth due to lack of "NN-specific optimizations"
- **Evidence needed:** Implementation incorporating NN-tailored optimizations demonstrating stable runtime growth with depth

### Open Question 2
- **Question:** How can Space Explanations be adapted to handle Convolutional Neural Networks (CNNs)?
- **Basis:** Conclusion lists goal to "handle other NN structures, such as convolutional NNs"
- **Evidence needed:** Modified encoding strategy successfully generating space explanations for CNN-based image classification

### Open Question 3
- **Question:** Can a specific algorithm be developed to directly approximate decision boundaries?
- **Basis:** Authors list "develop an algorithm to approximate decision boundaries" as future work
- **Evidence needed:** Algorithm outputting formal approximation of geometric boundary separating classes

### Open Question 4
- **Question:** Can Space Explanations be applied to analyze classification evolution across hidden layers?
- **Basis:** Final sentence states intent to "apply our method to analyze how decisions evolve across the hidden layers"
- **Evidence needed:** Demonstration of explanation generation applied to intermediate layer activations

## Limitations
- Scalability constrained by SMT solver performance on deep networks (exponential depth scaling)
- QF_LRA encoding limits explanations to linear regions, preventing curved boundary capture
- Reduce strategy introduces significant computational overhead and may timeout on complex datasets

## Confidence

| Claim | Confidence |
|-------|------------|
| Core interpolation mechanism | High |
| Scalability claims | Medium |
| Better decision boundary approximation | High |
| Interpretability for non-experts | Low |

## Next Checks

1. **Decision Boundary Approximation:** Measure exact volume of space explanation relative to true decision boundary region for Heart Attack 2D slice

2. **Algorithm Sensitivity:** Systematically test all three interpolation algorithms (Itp_F, Itp_DF, Itp'_F) on identical samples to measure trade-off between generality and computational cost

3. **Solver Optimization Impact:** Profile OpenSMT2 runtime to identify primary bottleneck (proof search, interpolant generation, or unsatisfiable core computation) and assess potential gains from NN-specific optimizations