---
ver: rpa2
title: 'Synthesizing Public Opinions with LLMs: Role Creation, Impacts, and the Future
  to eDemorcacy'
arxiv_id: '2504.00241'
source_url: https://arxiv.org/abs/2504.00241
tags:
- llms
- public
- data
- opinion
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses declining response rates and non-response bias
  in traditional survey methods by exploring Large Language Models (LLMs) to synthesize
  public opinion data. It introduces a novel role creation framework using knowledge
  injection via retrieval-augmented generation (RAG) and HEXACO personality profiles
  to dynamically generate survey responses.
---

# Synthesizing Public Opinions with LLMs: Role Creation, Impacts, and the Future to eDemorcacy

## Quick Facts
- arXiv ID: 2504.00241
- Source URL: https://arxiv.org/abs/2504.00241
- Reference count: 9
- One-line primary result: Role-creation with RAG improves LLM survey adherence from ~70% to ~84% on CES questions.

## Executive Summary
The paper addresses declining response rates and non-response bias in traditional surveys by using Large Language Models (LLMs) to synthesize public opinion data. It introduces a role-creation framework that combines HEXACO personality traits, political leanings, and demographic information to generate survey responses via retrieval-augmented generation (RAG). Compared to standard few-shot prompting, this method significantly increases answer adherence to real human survey responses. The approach shows promise for reducing costs and improving coverage, particularly in under-resourced or restrictive environments.

## Method Summary
The method generates synthetic survey responses by constructing persona profiles from HEXACO personality dimensions, political leanings (Conservative/Liberal/Populist), and demographics. These roles are stored in a vector database. For each survey question, RAG retrieves relevant role profile segments to assemble a prompt for the LLM. The LLM then generates a response conditioned on the retrieved role context. Answer adherence is measured by comparing synthetic responses to human responses from the 2021 Cooperative Election Study (CES) dataset.

## Key Results
- Role-creation method increases adherence from ~70% to ~84% compared to standard few-shot prompting.
- Adherence improvements are consistent across multiple open-weight LLMs tested.
- The approach reduces reliance on large-scale human survey data and mitigates non-response bias.

## Why This Works (Mechanism)
The framework improves alignment by injecting structured persona knowledge into the LLM prompt via RAG, guiding generation to reflect realistic human traits and political orientations. The HEXACO model provides a stable, quantifiable trait framework, while RAG ensures context relevance for each question. Dynamic persona creation enables scalable and customizable survey simulation.

## Foundational Learning
- Concept: HEXACO personality model
  - Why needed here: Provides structured trait dimensions used to define synthetic personas with politically relevant orientations.
  - Quick check question: Can you map two political leanings (e.g., conservative vs liberal) onto HEXACO traits for a concise role prompt?
- Concept: In-context learning
  - Why needed here: The role-creation method is a form of in-context learning where persona information and retrieved knowledge are injected into the prompt without modifying model weights.
  - Quick check question: If you double the number of in-context exemplars while keeping persona prompts fixed, would you expect adherence to increase monotonically?
- Concept: RAG for persona-conditioned generation
  - Why needed here: Enables dynamic retrieval of role profile segments relevant to each survey question to guide the LLM’s response in character.
  - Quick check question: What could happen to response diversity if the retrieval step always returns the same role segment across all questions?

## Architecture Onboarding
- Component map: Attribute pool (demographics + HEXACO + political leaning) → Role Generator → Vector DB (role embeddings) → Query → RAG Retriever (role segments) → LLM Prompt Assembler (role context + question) → LLM → Synthetic response → Evaluation layer (compare to human CES responses; compute adherence metrics).
- Critical path: Design role schema (attributes + prompts) → Generate and embed roles → On query, retrieve top-k role segments → Assemble prompt → Generate with temperature=0 → Log and evaluate.
- Design tradeoffs:
  - Role granularity vs scalability: more nuanced roles improve fidelity but increase vector store size and retrieval complexity.
  - RAG vs fine-tuning: RAG is cheaper and updatable; fine-tuning may better internalize stable preference patterns but costs more.
  - Model size vs inference cost: larger models show bigger adherence gains but require more compute.
- Failure signatures:
  - Persistent inconsistency across questions for the same persona (role coherence breaks).
  - Adherence drops below few-shot baseline (retrieval mismatch or noisy role profiles).
  - Systematic bias toward majority opinions regardless of persona (prompt dominance or model priors override role context).
- First 3 experiments:
  1. Baseline adherence measurement with few-shot prompting only (no roles) across multiple open-weight LLMs.
  2. Role-creation run: build HEXACO-demography roles, store in vector DB, and evaluate adherence on the same CES questions.
  3. Ablation: swap RAG retrieval for random role segments to isolate the effect of relevance in role-context selection.

## Open Questions the Paper Calls Out
- Can embedding preferences through fine-tuning and trigger-based generation yield more consistent generalization than the current dynamic role-creation framework?
- Does the role-creation framework maintain high fidelity when simulating opinions in low-resource, non-English languages and non-Western cultures?
- Are high adherence scores a result of genuine predictive capability or mere memorization of associations from the models' pre-training data?
- To what extent does the role-creation technique mitigate LLM sensitivity to subtle prompt variations compared to standard few-shot prompting?

## Limitations
- The evaluation uses only a single US-based survey; external validity to other cultures, languages, and political systems remains untested.
- The framework may reflect model biases or amplify skewed views if persona profiles are not representative.
- Key implementation details (RAG configuration, exact prompts, question set) are underspecified, limiting reproducibility.

## Confidence
- High confidence: The role-creation framework improves LLM survey adherence over standard few-shot prompting on the CES dataset.
- Medium confidence: RAG-enhanced role prompts can improve response alignment compared to no-retrieval baselines.
- Low confidence: Claims about scalability to under-resourced regions or compliance with survey laws are speculative without empirical backing.

## Next Checks
1. Reproduce the role-creation pipeline using the exact 30 CES questions and publicly available human responses, varying only the LLM model size to confirm adherence gains persist across scales.
2. Conduct a sensitivity analysis by paraphrasing role prompts and questions to measure stability of adherence scores under linguistic variation.
3. Perform an ablation study where RAG retrieval is replaced with random role segments to quantify the impact of relevance on response alignment.