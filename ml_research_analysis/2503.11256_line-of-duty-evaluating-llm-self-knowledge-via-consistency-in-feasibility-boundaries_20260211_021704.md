---
ver: rpa2
title: 'Line of Duty: Evaluating LLM Self-Knowledge via Consistency in Feasibility
  Boundaries'
arxiv_id: '2503.11256'
source_url: https://arxiv.org/abs/2503.11256
tags:
- self-knowledge
- tasks
- llms
- task
- boundaries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces a novel methodology to evaluate LLM self-knowledge
  by allowing models to set their own feasibility boundaries and analyzing consistency
  when generating and attempting tasks. The approach measures accuracy, foresight,
  and insight across five self-knowledge types: functional ceiling, contextual awareness,
  ambiguity identification, ethical integrity, and temporal perception.'
---

# Line of Duty: Evaluating LLM Self-Knowledge via Consistency in Feasibility Boundaries
## Quick Facts
- arXiv ID: 2503.11256
- Source URL: https://arxiv.org/abs/2503.11256
- Reference count: 13
- Key outcome: Frontier LLMs fail to maintain consistent self-knowledge boundaries over 80% of the time, with critical weaknesses in temporal awareness and contextual understanding.

## Executive Summary
This study introduces a novel methodology to evaluate LLM self-knowledge by allowing models to set their own feasibility boundaries and analyzing consistency when generating and attempting tasks. The approach measures accuracy, foresight, and insight across five self-knowledge types: functional ceiling, contextual awareness, ambiguity identification, ethical integrity, and temporal perception. Results show that even frontier models like GPT-4o and Mistral Large demonstrate significant self-knowledge gaps that could impact real-world AI deployment and trustworthiness.

## Method Summary
The methodology involves task decomposition where models first assess whether they can complete a task, then either execute it or generate an explanation for why it cannot be done. This creates a consistency check between the model's self-assessment and actual performance. The evaluation spans five self-knowledge types measured through ten distinct tasks, with consistency defined as agreement between feasibility assessment and actual execution. The framework provides a universal approach to assessing self-knowledge beyond human-defined limits, focusing on the model's own perception of its capabilities.

## Key Results
- Frontier models fail to maintain consistent self-knowledge boundaries over 80% of the time
- Greatest weaknesses observed in temporal awareness and contextual understanding
- Models exhibit overconfidence in functional capabilities but conservatism in ethical and ambiguous tasks

## Why This Works (Mechanism)
The methodology works by creating a closed-loop evaluation system where models must both assess and execute tasks, revealing discrepancies between self-perception and actual capability. This approach captures the temporal and contextual dimensions of self-knowledge that traditional evaluation methods miss. The consistency check between initial feasibility assessment and final execution provides a direct measure of self-knowledge reliability.

## Foundational Learning
1. Self-knowledge assessment mechanisms - needed to understand how models evaluate their own capabilities; quick check: identify the three main types of self-assessment used in the methodology
2. Consistency metrics in AI evaluation - needed to measure reliability of model self-perception; quick check: define the consistency formula used in the study
3. Feasibility boundary setting - needed to understand how models establish capability limits; quick check: list the five self-knowledge types evaluated

## Architecture Onboarding
Component Map: Task Decomposition -> Feasibility Assessment -> Execution/Explanation -> Consistency Check
Critical Path: The core workflow follows task decomposition through to consistency validation, with the feasibility assessment serving as the critical decision point that determines whether execution or explanation generation occurs.
Design Tradeoffs: The binary classification approach provides clear evaluation criteria but may oversimplify complex capability assessments. The universal framework applies across self-knowledge types but may miss nuanced capability variations.
Failure Signatures: Major failures occur when models incorrectly assess task feasibility, particularly in temporal awareness and contextual understanding domains. Overconfidence in functional capabilities versus conservatism in ethical tasks represents a consistent failure pattern.
First Experiments:
1. Test binary versus continuous scale evaluation to assess impact on consistency measurement
2. Evaluate cross-linguistic performance to determine if self-knowledge gaps are language-dependent
3. Implement temporal tracking to measure how self-knowledge consistency evolves with model updates

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation methodology relies on binary task classification that may not capture nuanced model capabilities
- Five self-knowledge types represent predefined categories that might not encompass all relevant dimensions
- Temporal aspects of evaluation may be confounded by model training cutoffs and knowledge updating mechanisms

## Confidence
High Confidence: Core finding that frontier models fail to maintain consistent self-knowledge boundaries over 80% of the time is well-supported
Medium Confidence: Comparative analysis of different self-knowledge types shows consistent patterns but may be influenced by task selection
Low Confidence: Specific percentage breakdowns for different self-knowledge types should be interpreted cautiously due to potential task-specific biases

## Next Checks
1. Conduct cross-linguistic validation by testing methodology with non-English prompts
2. Implement continuous scale evaluation instead of binary classification
3. Design longitudinal study to track how self-knowledge consistency evolves with model updates