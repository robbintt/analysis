---
ver: rpa2
title: 'ELAB: Extensive LLM Alignment Benchmark in Persian Language'
arxiv_id: '2504.12553'
source_url: https://arxiv.org/abs/2504.12553
tags:
- persian
- language
- social
- safety
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ELAB, a comprehensive benchmark for evaluating
  Persian large language models (LLMs) across safety, fairness, and social norms.
  The framework combines translated versions of existing English benchmarks with newly
  created Persian datasets, including ProhibiBench-fa, SafeBench-fa, FairBench-fa,
  SocialBench-fa, and GuardBench-fa.
---

# ELAB: Extensive LLM Alignment Benchmark in Persian Language

## Quick Facts
- **arXiv ID**: 2504.12553
- **Source URL**: https://arxiv.org/abs/2504.12553
- **Reference count**: 9
- **Primary result**: Introduces ELAB, a comprehensive benchmark for evaluating Persian LLMs across safety, fairness, and social norms using translated benchmarks and culturally grounded datasets

## Executive Summary
This paper introduces ELAB, a comprehensive benchmark for evaluating Persian large language models (LLMs) across safety, fairness, and social norms. The framework combines translated versions of existing English benchmarks with newly created Persian datasets, including ProhibiBench-fa, SafeBench-fa, FairBench-fa, SocialBench-fa, and GuardBench-fa. These datasets address culturally specific ethical issues and harmful content in the Persian context. The evaluation employs an LLM-as-a-judge approach using GPT-4o-mini to score model responses. Results show significant variation in model performance, with Gemma-2-9B-it achieving the highest overall scores, while Qwen2.5 models generally lag behind.

## Method Summary
The ELAB benchmark evaluates Persian LLMs using an LLM-as-a-judge approach with GPT-4o-mini scoring model responses on a 0-10 scale. The framework combines translated English benchmarks (Anthropic-fa, AdvBench-fa, HarmBench-fa, DecodingTrust-fa) with culturally-generated Persian datasets created through synthetic generation and social media collection. Models are deployed via vLLM on A100 GPUs with a standard "helpful assistant" system prompt. Four evaluation prompt templates assess safety, fairness, social norms, and cultural dimensions separately. The approach aims to capture both translated alignment challenges and Persian-specific cultural considerations.

## Key Results
- Gemma-2-9B-it achieved the highest overall scores, averaging above 90 in many categories
- Qwen2.5 models generally performed below other evaluated models
- Translation alone proved insufficient for reliable evaluation, as shown by distributional differences between translated and culturally-generated datasets
- GuardBench-fa (real offensive content) showed lower scores than synthetic datasets, suggesting real-world content is harder

## Why This Works (Mechanism)

### Mechanism 1: LLM-as-a-Judge for Cross-Dimensional Scoring
- Claim: GPT-4o-mini can reliably score Persian LLM responses across safety, fairness, and social norms using structured evaluation prompts.
- Mechanism: Judge receives (system prompt + user question + model response), outputs 0-10 score with reasoning. Four distinct prompt templates encode evaluation criteria per dimension.
- Core assumption: GPT-4o-mini possesses sufficient Persian language understanding and cultural knowledge to evaluate alignment accurately.
- Evidence anchors:
  - [abstract]: "The evaluation employs an LLM-as-a-judge approach using GPT-4o-mini to score model responses."
  - [section]: "The GPT-4o mini model was used as the judge to evaluate the responses generated by the models. For each question, the model's response was scored on a scale of 0 to 10."
  - [corpus]: EPT Benchmark and MELAC papers similarly use automated evaluation for Persian LLMs, suggesting this paradigm is gaining traction, though none validate judge accuracy against human annotators.
- Break condition: Judge systematically favors certain response styles or lacks cultural competency for nuanced Persian norms like *taarof*.

### Mechanism 2: Multi-Source Dataset Construction
- Claim: Combining translated, synthetically generated, and naturally collected data captures alignment challenges that any single source misses.
- Mechanism: (1) Translate English benchmarks via GPT-4o-mini with back-translation verification; (2) Generate adversarial questions via Command-R Plus with jailbreaking; (3) Collect real offensive content from Persian social media with manual filtering.
- Core assumption: Each source provides non-overlapping coverage of Persian alignment failure modes.
- Evidence anchors:
  - [abstract]: "The framework combines translated versions of existing English benchmarks with newly created Persian datasets... translation alone is insufficient for reliable evaluation."
  - [section]: "Figure 2, visualized using SNE on the embedding space, reveals clear distributional differences between translated and culturally-generated datasets."
  - [corpus]: PBBQ and PerHalluEval also create native Persian benchmarks, reinforcing that translation-only approaches are inadequate.
- Break condition: Synthetic jailbreaking data doesn't reflect real-world Persian harmful content patterns.

### Mechanism 3: Culturally-Grounded Taxonomy with Interdependency Awareness
- Claim: Safety, fairness, and social norms are interdependent; unified evaluation reveals trade-offs (e.g., over-refusal exacerbating bias).
- Mechanism: Datasets span all three dimensions with granular subcategories. ProhibiBench-fa adds 11 fine-grained harmful content categories; GuardBench-fa captures real cultural violations.
- Core assumption: Three dimensions capture the most culturally salient concerns; Persian-specific norms (e.g., *aberoo*, *taarof*) map onto these categories.
- Evidence anchors:
  - [section]: "addressing interdependencies (e.g., overly strict safety filters exacerbating dialect bias)"
  - [section]: "These three aspects constitute culturally salient dimensions in Persian linguistic and cultural frameworks."
  - [corpus]: Taarof paper explicitly models Persian politeness as a distinct alignment challenge not captured by Western frameworks.
- Break condition: Important dimensions missing from taxonomy; cultural norms incorrectly collapsed into generic safety categories.

## Foundational Learning

- Concept: LLM-as-a-Judge limitations
  - Why needed here: Entire evaluation relies on GPT-4o-mini's judgment; understanding its failure modes is critical for interpreting results.
  - Quick check question: What systematic biases might the judge have when scoring Persian cultural sensitivity vs Western norms?

- Concept: Translation-artifacts in benchmark transfer
  - Why needed here: Four core datasets are translated; SNE visualization shows distributional divergence from native data.
  - Quick check question: How would you detect whether a translated prompt lost its adversarial edge in Persian?

- Concept: Jailbreaking for adversarial data generation
  - Why needed here: ProhibiBench-fa uses DAN jailbreaking on Command-R to generate harmful queries.
  - Quick check question: What types of Persian-specific harmful content might jailbreaking fail to surface?

## Architecture Onboarding

- Component map:
  - Translated datasets: Anthropic-fa, AdvBench-fa, HarmBench-fa, DecodingTrust-fa → baseline alignment coverage
  - Generated datasets: SafeBench-fa, FairBench-fa, SocialBench-fa, ProhibiBench-fa → Persian-specific adversarial testing
  - Collected dataset: GuardBench-fa (6,651 samples) → real-world offensive language from social media
  - Judge: GPT-4o-mini with 4 prompt templates (safety, fairness, social norms, cultural)
  - Aggregation: Mean scores per dimension → public leaderboard

- Critical path:
  1. Dataset preparation → 2. Model deployment via vLLM on A100 → 3. Response generation with "You are a helpful assistant" system prompt → 4. Judge scoring → 5. Leaderboard ranking

- Design tradeoffs:
  - Judge model: GPT-4o-mini (fast, cheap) vs human annotators (accurate but expensive) — paper acknowledges this limitation
  - Model constraint: <10B parameters enables single A100 deployment but excludes larger Persian-capable models
  - Translation vs native: Enables cross-lingual comparison but SNE shows embeddings diverge significantly

- Failure signatures:
  - Score = 0 (model responded in non-Persian language)
  - Large performance gaps between translated vs culturally-generated datasets for same model
  - GuardBench-fa scores significantly lower than synthetic datasets (suggests real-world content is harder)

- First 3 experiments:
  1. Reproduce Gemma-2-9B-it scores on ProhibiBench-fa subset (N=50) to validate pipeline integrity
  2. Measure judge consistency: re-score 30 responses twice, compute score variance
  3. Compare model rankings on translated vs native datasets to quantify translation artifact effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the GPT-4o-mini judge exhibit high inter-rater reliability with native Persian human annotators when evaluating the specific cultural dimensions of ELAB (e.g., *taarof* or *aberoo*)?
- Basis in paper: [explicit] The authors state in the Limitations section that "scores derived from this method may not always reflect absolute accuracy," despite being standard practice.
- Why unresolved: The validity of the entire leaderboard depends on the assumption that the automated judge aligns with human cultural understanding, which is not verified in the study.
- What evidence would resolve it: A correlation analysis comparing GPT-4o-mini scores against human expert evaluations on a subset of culturally sensitive model responses.

### Open Question 2
- Question: Do larger models (exceeding 10 billion parameters) demonstrate superior alignment on ELAB, or do they encounter different failure modes compared to the smaller open-source models evaluated?
- Basis in paper: [explicit] The evaluation is "constrained to open-source models with fewer than 10 billion parameters due to computational resource limitations."
- Why unresolved: It is unclear if the performance trends observed (e.g., Gemma-2-9b-it leading) hold true for state-of-the-art larger models or proprietary systems.
- What evidence would resolve it: Inclusion of larger parameter models (e.g., Llama-3-70B) in the evaluation framework to compare alignment scores.

### Open Question 3
- Question: Does optimizing for higher safety scores in Persian LLMs inadvertently exacerbate fairness issues, such as increased dialect bias or over-refusal?
- Basis in paper: [inferred] The Introduction claims the framework addresses "interdependencies (e.g., overly strict safety filters exacerbating dialect bias)," but the results report Safety and Fairness independently without analyzing this trade-off.
- Why unresolved: Table 3 presents independent scores; it is unknown if high safety ratings (e.g., refusals) negatively impact fairness metrics.
- What evidence would resolve it: A cross-category error analysis to determine if safety-aligned refusals are disproportionately triggered by specific Persian dialects or cultural contexts.

## Limitations

- The entire evaluation relies on GPT-4o-mini as judge without validation against human annotators for Persian cultural norms
- Core datasets are not publicly linked, creating reproducibility barriers
- Generation hyperparameters and GuardBench-fa collection methodology lack sufficient detail

## Confidence

- **High confidence**: ELAB framework architecture, dataset taxonomy (safety/fairness/social norms), and model deployment via vLLM on A100 are well-specified and reproducible
- **Medium confidence**: Translation artifacts and cultural specificity claims are supported by SNE visualization showing distributional differences, but quantitative validation is limited
- **Low confidence**: Judge accuracy for Persian cultural evaluation, synthetic vs real harmful content equivalence, and cross-lingual benchmark validity lack empirical grounding

## Next Checks

1. **Judge accuracy validation**: Re-score 50 ProhibiBench-fa responses using both GPT-4o-mini and 2-3 Persian-speaking human annotators. Compute inter-annotator agreement (Cohen's kappa) to establish judge reliability baseline.

2. **Synthetic content authenticity**: Generate 100 harmful queries using Command-R jailbreaking, then collect 100 naturally occurring harmful examples from Persian social media. Conduct pairwise comparison with native Persian speakers to measure semantic/contextual alignment.

3. **Translation artifact quantification**: For each translated dataset, measure performance correlation between original English and Persian versions across all evaluated models. Compute statistical significance to determine whether translation preserves benchmark difficulty and discriminability.