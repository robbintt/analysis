---
ver: rpa2
title: Training Language Models for Social Deduction with Multi-Agent Reinforcement
  Learning
arxiv_id: '2502.06060'
source_url: https://arxiv.org/abs/2502.06060
tags:
- player
- crewmates
- imposter
- language
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to train language model agents to
  communicate effectively in the social deduction game Among Us without requiring
  human demonstrations. The key idea is to decompose communication into listening
  and speaking skills, and to use dense rewards derived from agents' instrumental
  goals (e.g., predicting the imposter's identity) to guide learning.
---

# Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning
## Quick Facts
- arXiv ID: 2502.06060
- Source URL: https://arxiv.org/abs/2502.06060
- Reference count: 40
- Trains language model agents for social deduction games without human demonstrations

## Executive Summary
This paper presents a method to train language model agents to communicate effectively in social deduction games like Among Us using multi-agent reinforcement learning. The key innovation is decomposing communication into distinct listening and speaking components, with dense rewards derived from agents' instrumental goals rather than sparse task completion signals. By training agents to predict the imposter's identity from discussions and rewarding messages that influence other agents' beliefs, the approach achieves significant performance improvements over standard RL methods.

The method addresses the challenge of training communication in multi-agent settings where traditional RL struggles due to sparse rewards and the complexity of natural language interactions. Through careful reward design and skill decomposition, the approach enables emergent behaviors like evidence provision and strategic accusations while maintaining robustness against adversarial imposters and generalizing to different game configurations.

## Method Summary
The approach trains language model agents in social deduction games by decomposing communication into listening and speaking skills. Listening is improved by training agents to predict the imposter's identity from discussion content, using this prediction as a dense reward signal. Speaking is enhanced by rewarding messages that successfully influence other agents' beliefs about the imposter's identity. The method employs multi-agent reinforcement learning with carefully designed reward structures that focus on instrumental goals rather than just final game outcomes. This decomposition allows each communication skill to be optimized independently while maintaining their interdependence in the overall task.

## Key Results
- Doubles win rates compared to standard reinforcement learning baselines
- Agents exhibit emergent behaviors including accusing suspects and providing evidence
- Method shows robustness to adversarially trained imposters
- Generalizes successfully to different environment configurations

## Why This Works (Mechanism)
The method works by addressing the fundamental challenge of sparse rewards in multi-agent communication tasks. By decomposing communication into listening and speaking components, each can be trained with appropriate dense reward signals derived from instrumental goals. The listening component is trained to predict the imposter's identity, providing immediate feedback on information extraction from discussions. The speaking component is rewarded for messages that successfully influence other agents' beliefs, creating a direct link between communication actions and their intended effects. This decomposition allows for more efficient credit assignment and skill development compared to end-to-end training with only sparse task completion rewards.

## Foundational Learning
**Social Deduction Games** - Multiplayer games where players must deduce hidden roles or identities through discussion and observation. Needed to understand the communication dynamics and strategic elements of the target task. Quick check: Can identify core mechanics of Among Us (imposter identification through discussion).

**Multi-Agent Reinforcement Learning** - Framework for training multiple agents that interact within a shared environment. Required for coordinating learning across multiple agents with competing or aligned objectives. Quick check: Can explain credit assignment challenges in multi-agent settings.

**Natural Language Processing for Agents** - Techniques for processing and generating human-like text in agent communication. Essential for enabling meaningful dialogue between agents. Quick check: Can describe tokenization and embedding approaches for agent messages.

**Reward Shaping and Dense Rewards** - Methods for designing reward signals that provide frequent feedback during learning. Critical for overcoming sparse reward problems in complex tasks. Quick check: Can identify situations where dense rewards accelerate learning.

**Skill Decomposition in Learning** - Breaking complex tasks into manageable sub-skills for more efficient training. Enables targeted optimization of specific capabilities. Quick check: Can explain benefits of separating listening and speaking skills.

## Architecture Onboarding
Component Map: Environment -> Agent Observation -> Language Model -> Action Selection -> Communication -> Other Agents -> Reward Calculation

Critical Path: The agent receives observations including discussion history and game state, processes them through the language model to generate responses, sends messages to other agents, receives their responses, and receives rewards based on instrumental goal achievement (imposter prediction accuracy and belief influence).

Design Tradeoffs: The method trades off between pure end-to-end learning and modular skill development. While decomposition enables more efficient training and interpretable behaviors, it may miss emergent communication patterns that arise from holistic learning. The choice of instrumental goals as reward sources provides dense feedback but may not capture all aspects of effective communication.

Failure Signatures: Potential failures include agents developing degenerate communication patterns that maximize rewards without genuine information exchange, overfitting to specific game configurations, or learning to manipulate reward signals rather than achieving true strategic objectives. Agents might also fail to generalize communication styles across different player behaviors.

First Experiments:
1. Train agents with only listening skill (no speaking ability) to verify if prediction accuracy improves with discussion exposure
2. Train agents with only speaking skill (fixed listening) to test if messages influence other agents' predictions as intended
3. Test transfer performance when moving from small to large player counts to evaluate generalization

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Evaluation limited to single game environment (Among Us) with specific dynamics
- Dense reward formulation may not generalize to other social deduction games with different success metrics
- Adversarial training results tested against limited strategy sets, may not capture full human deception complexity

## Confidence
High: Core methodology of decomposing communication into listening and speaking components
Medium: Dense reward formulation based on instrumental goals
Medium: Claimed improvements over baseline RL approaches

## Next Checks
1. Test the method across multiple social deduction game environments with varying communication structures and success criteria to evaluate generalization
2. Conduct human evaluation studies comparing agent performance against human players in controlled Among Us matches
3. Analyze the emergence of unintended communication patterns or exploits in long-term training runs to assess safety and robustness