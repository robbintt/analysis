---
ver: rpa2
title: Evaluation of LLMs on Long-tail Entity Linking in Historical Documents
arxiv_id: '2505.03473'
source_url: https://arxiv.org/abs/2505.03473
tags:
- entity
- entities
- linking
- llms
- long-tail
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models (LLMs) for long-tail
  entity linking in historical documents using the MHERCL v0.1 benchmark. Traditional
  methods struggle with rare entities, so the researchers compared GPT 3.5 and Llama
  3 (8B and 70B) against ReLiK, a state-of-the-art system.
---

# Evaluation of LLMs on Long-tail Entity Linking in Historical Documents

## Quick Facts
- **arXiv ID**: 2505.03473
- **Source URL**: https://arxiv.org/abs/2505.03473
- **Reference count**: 23
- **Primary result**: LLMs significantly outperform traditional methods on rare entity linking in historical documents, with recall up to 60.3% vs 45.7%

## Executive Summary
This study evaluates large language models for long-tail entity linking in historical documents, where traditional methods struggle with rare entities. The researchers compare GPT 3.5 and Llama 3 (8B and 70B) against ReLiK, a state-of-the-art system, using the MHERCL v0.1 benchmark. The LLMs were prompted to jointly detect and link entities to Wikipedia pages using JSON output format. Results demonstrate that LLMs significantly outperform traditional methods in recall, particularly for rare entities, though precision is lower due to over-generation. The study suggests LLMs can complement traditional approaches for long-tail entity linking tasks.

## Method Summary
The researchers created a benchmark dataset (MHERCL v0.1) with 100 English historical documents and annotated 9,242 entity mentions. They evaluated three LLMs (GPT 3.5, Llama 3-8B, and Llama 3-70B) against ReLiK using recall, precision, and F1-score metrics. The LLMs were prompted to jointly perform entity detection and linking, outputting results in JSON format. The evaluation included comparative analysis of overall performance and performance on rare entities using different frequency thresholds. Qualitative analysis was also conducted to understand LLM behavior and error patterns.

## Key Results
- LLMs achieved significantly higher recall than ReLiK, with Llama 3-70B reaching 60.3% vs ReLiK's 45.7%
- Precision was lower for LLMs due to over-generation issues, but F1-scores were competitive on very rare entities (threshold = 20)
- Performance degradation was observed with OCR errors and insufficient context, highlighting challenges in real-world historical document processing

## Why This Works (Mechanism)
LLMs demonstrate superior performance on long-tail entity linking because they can leverage their broad knowledge base and contextual understanding to recognize rare entities that traditional methods miss. Unlike rule-based or statistical approaches that rely heavily on entity frequency, LLMs can infer connections between entities and their contexts even when entities are poorly represented in training data. The ability to process full document context rather than isolated mentions enables better disambiguation and linking decisions for rare entities.

## Foundational Learning
- **Entity Linking**: The task of connecting entity mentions in text to entries in a knowledge base; needed because it enables semantic understanding and knowledge integration
- **Long-tail Entities**: Rare entities with few occurrences in training data; quick check: entities appearing less than 20 times in the dataset
- **Joint Detection and Linking**: Performing entity recognition and linking in a single step; needed to capture contextual relationships that separate steps might miss
- **Knowledge Base Population**: Adding new information to structured knowledge repositories; quick check: whether linked entities exist in Wikipedia
- **OCR Error Impact**: How optical character recognition mistakes affect entity linking performance; quick check: correlation between OCR confidence scores and linking accuracy
- **Recall-Precision Tradeoff**: The balance between finding all relevant entities versus avoiding false positives; quick check: F1-score as harmonic mean of recall and precision

## Architecture Onboarding

**Component Map**: Historical Document -> OCR Processing -> LLM Prompt -> JSON Output -> Wikipedia Knowledge Base -> Evaluation Metrics

**Critical Path**: Document preprocessing → LLM inference → Entity linking → Knowledge base validation → Performance evaluation

**Design Tradeoffs**: The study chose to use general-purpose LLMs rather than fine-tuned models to maintain flexibility, accepting lower precision for higher recall. This represents a recall-focused design prioritizing coverage over accuracy, suitable for exploratory analysis but requiring post-processing for production deployment.

**Failure Signatures**: Performance degrades significantly when entities contain OCR errors or when insufficient context is provided for disambiguation. The LLMs also tend to over-generate entities, producing false positives that don't exist in the knowledge base.

**3 First Experiments**:
1. Test the same LLM prompts on a different historical document collection with varying OCR quality to assess robustness
2. Implement a post-processing step that filters LLM-generated entities against a stricter knowledge base to improve precision
3. Compare performance when providing additional context windows versus using the full document for linking decisions

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Small benchmark dataset (100 documents) may not represent the diversity of historical document collections
- Focus exclusively on English-language historical texts limits generalizability to other languages and time periods
- Lower precision scores due to over-generation raise concerns about practical deployment and error propagation

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| LLMs significantly outperform traditional methods on rare entity linking | Medium |
| Results generalize across different historical document types | Low |
| The recall advantage persists with better prompting or hybrid approaches | Medium |

## Next Checks

1. **Cross-dataset validation**: Test the same LLM approaches on multiple historical document collections with varying OCR qualities, time periods, and subject domains to assess robustness beyond the MHERCL v0.1 benchmark.

2. **Hybrid system evaluation**: Implement and evaluate hybrid approaches that combine LLM entity detection with traditional entity linking methods, measuring whether this improves precision while maintaining recall advantages.

3. **Knowledge base expansion**: Evaluate entity linking performance using multiple knowledge bases (beyond Wikipedia) to determine if the recall advantage persists when matching against specialized historical knowledge repositories.