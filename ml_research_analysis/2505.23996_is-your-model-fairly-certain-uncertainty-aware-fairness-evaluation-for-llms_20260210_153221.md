---
ver: rpa2
title: Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs
arxiv_id: '2505.23996'
source_url: https://arxiv.org/abs/2505.23996
tags:
- fairness
- uncertainty
- llms
- ucerf
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UCerF, an uncertainty-aware fairness metric
  for large language models (LLMs) that jointly evaluates prediction correctness and
  model confidence across demographic groups. Unlike conventional accuracy-based fairness
  metrics, UCerF captures bias in LLM decisions by analyzing both prediction disparities
  and uncertainty differences between groups, revealing hidden biases that traditional
  metrics overlook.
---

# Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs

## Quick Facts
- **arXiv ID**: 2505.23996
- **Source URL**: https://arxiv.org/abs/2505.23996
- **Reference count**: 40
- **Key outcome**: UCerF metric jointly evaluates correctness and confidence to reveal hidden LLM fairness biases that accuracy-only metrics miss

## Executive Summary
This paper introduces UCerF, an uncertainty-aware fairness metric that evaluates both prediction correctness and model confidence across demographic groups. Unlike conventional accuracy-based fairness metrics, UCerF captures bias by analyzing both prediction disparities and uncertainty differences between groups. The authors also create SynthBias, a large-scale (31,756 samples) synthetic dataset for gender-occupation fairness evaluation that addresses limitations in existing benchmarks. Experimental results across ten open-source models show that UCerF provides more nuanced fairness assessments by revealing hidden biases that traditional metrics overlook.

## Method Summary
The UCerF framework evaluates LLM fairness by jointly considering prediction correctness and model uncertainty. It uses perplexity as an uncertainty estimator, computing certainty from token probabilities and combining it with correctness to produce a desirability score. Fairness is then computed as the expected difference in desirability between demographic groups. The method is evaluated on the SynthBias dataset, which uses GPT-4o to generate sentences with clear ambiguity definitions, followed by human annotation. The framework supports both intrinsic prediction tasks and MCQ-style evaluation.

## Key Results
- UCerF reveals hidden biases that Equalized Odds misses - Mistral-7B appears less fair due to high confidence in incorrect predictions despite similar EO scores
- SynthBias dataset provides 10x more samples and 2.6x larger vocabulary than WinoBias, with higher embedding distance variance
- Models ranking changes significantly between UCerF and accuracy-based metrics, demonstrating UCerF's complementary value
- The method is modular - different uncertainty estimators (perplexity, Rényi divergence, Fisher-Rao distance) produce similar rankings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Jointly evaluating correctness and confidence reveals fairness disparities that accuracy-only metrics miss
- **Mechanism**: UCerF defines a "desirability" score D(x) ∈ [-1, 1] that combines prediction correctness with model certainty. Fairness is then computed as the expected difference in desirability between demographic groups.
- **Core assumption**: Uncertainty differences between groups reflect meaningful internal bias in model decision-making that affects real-world behavior, particularly under stochastic sampling
- **Evidence anchors**: Case study shows Falcon-40B has identical TPR for both groups under Equalized Odds, but UCerF reveals lower desirability for anti-stereotypical cases due to uncertainty differences

### Mechanism 2
- **Claim**: Perplexity provides a simple, interpretable proxy for model uncertainty that suffices for fairness evaluation
- **Mechanism**: The paper uses perplexity f_perplexity(x; G) = 2^H(G(x)) where H is entropy over class probabilities. This is converted to normalized certainty c(x) ∈ [0,1].
- **Core assumption**: Token-level perplexity on occupation tokens captures meaningful uncertainty about the coreference resolution task
- **Evidence anchors**: Recent studies show logit-based uncertainty estimators remain competitive with more complex metrics; UCerF with Rényi divergence and Fisher-Rao distance produces similar rankings

### Mechanism 3
- **Claim**: Human-validated synthetic data with clear ambiguity definitions enables more reliable fairness benchmarking of modern LLMs
- **Mechanism**: SynthBias uses GPT-4o to generate sentences, then applies rule-based filters and human annotation with 75% consensus threshold for coherence and ambiguity classification
- **Core assumption**: Human annotators can reliably distinguish ambiguous from unambiguous coreference, and GPT-4o can generate diverse enough samples to avoid memorization
- **Evidence anchors**: SynthBias has 10x more samples, 2.6x larger vocabulary, and higher embedding distance variance than WinoBias; t-SNE/PCA shows broader semantic coverage

## Foundational Learning

- **Concept: Equalized Odds (EO)**
  - **Why needed here**: The primary baseline metric that UCerF is compared against; understanding its limitations motivates the paper
  - **Quick check question**: If a model has TPR=1.0 for both pro-stereotypical and anti-stereotypical groups, what is its EO score? Is it guaranteed to be fair under UCerF?

- **Concept: Perplexity and Entropy**
  - **Why needed here**: Core uncertainty estimator used throughout the paper; needed to interpret equations and results
  - **Quick check question**: If a model assigns probability 0.8 to one occupation and 0.2 to another, how would you compute the perplexity? What does lower perplexity indicate?

- **Concept: Coreference Resolution**
  - **Why needed here**: The task underlying both WinoBias and SynthBias datasets; understanding the task is essential for interpreting examples
  - **Quick check question**: In "The doctor gave the nurse a gift because it was her birthday," what information determines whether "her" refers to doctor or nurse?

## Architecture Onboarding

- **Component map**:
  Input Sentence + Pronoun → LLM → Token Probabilities for Occupations → Perplexity Calculation → Desirability Score D(x) → Group-wise Comparison → UCerF Score

- **Critical path**:
  1. Implement token probability extraction for target occupations (may require model internals access)
  2. Implement perplexity → certainty normalization (Equation 1)
  3. Implement desirability scoring with correctness check (Equation 2)
  4. Implement pairwise UCerF computation (Equation 3-4)

- **Design tradeoffs**:
  - Perplexity vs. complex uncertainty estimators: Paper shows perplexity is sufficient and more interpretable, but may not capture all uncertainty types for generative tasks
  - Sample-wise vs. group-wise UCerF: Sample-wise requires minimal pairs; group-wise works without pairs but may be less sensitive
  - Type1 vs. Type2 evaluation: Type1 has no ground truth (evaluates uncertainty parity only); Type2 evaluates both correctness and uncertainty

- **Failure signatures**:
  - Models with very low confidence on all samples may appear artificially fair (high UCerF via Figure 3d pattern)
  - MCQ format constrains answer space, potentially inflating fairness scores compared to open generation
  - Models not instruction-tuned may have poor MCQ accuracy regardless of fairness

- **First 3 experiments**:
  1. Reproduce Figure 5 case study: Evaluate a single model on nurse-physician pairs from SynthBias Type2. Compute both EO and UCerF
  2. Validate uncertainty estimator choice: Implement UCerF with perplexity, Rényi divergence, and Fisher-Rao distance on a subset of SynthBias
  3. Test dataset difficulty hypothesis: Evaluate the same model on both WinoBias and SynthBias Type2. Check if SynthBias yields lower accuracy and reveals additional fairness disparities

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does extending UCerF evaluation to non-binary pronouns (e.g., they/them, ze/hir) reveal additional fairness disparities that binary-gendered evaluation misses, and how should desirability be computed when pronoun ambiguity differs by pronoun type?
- **Basis in paper**: The authors explicitly state: "we acknowledge the importance of evaluating fairness on the full spectrum of pronouns in future studies" while noting that WinoGender's treatment of "they/them" was ambiguous between singular and plural references
- **Why unresolved**: The paper confines analysis to binary pronouns for clarity, and the metric formulation in Eq. 3-4 assumes minimal pairs between two groups; non-binary extensions require handling non-paired or multi-group comparisons
- **What evidence would resolve it**: An expanded SynthBias dataset with validated non-binary pronoun samples and comparative UCerF scores

### Open Question 2
- **Question**: How does the choice of uncertainty estimator (perplexity vs. semantic entropy vs. conformal prediction) affect UCerF rankings across models, and is there a single estimator that best aligns with human fairness judgments?
- **Basis in paper**: The authors state: "It is worth exploring and adopting other appropriate uncertainty estimators for more targeted fairness evaluation" and demonstrate this is modular in Appendix J
- **Why unresolved**: Perplexity was chosen for simplicity and interpretability, but the paper shows preliminary evidence that Rényi divergence and Fisher-Rao distance produce different model rankings
- **What evidence would resolve it**: Systematic comparison of UCerF computed with 5+ uncertainty estimators on the same model/dataset combinations, correlated with human annotator judgments of fairness

### Open Question 3
- **Question**: Can UCerF serve as an optimization signal for fairness-aware training, and would minimizing UCerF during fine-tuning improve fairness without degrading task performance?
- **Basis in paper**: The paper establishes UCerF as an evaluation metric but does not explore whether it can be used as a training objective
- **Why unresolved**: UCerF depends on group-level statistics and is non-differentiable in its current form; converting it to a loss function suitable for gradient-based training requires reformulation
- **What evidence would resolve it**: Experimental results showing models fine-tuned with a UCerF-derived loss achieve improved UCerF scores compared to baseline, with controlled experiments measuring any accuracy trade-offs

## Limitations

- Perplexity-based uncertainty estimator may not capture all forms of model uncertainty, particularly for open-ended generation tasks with unbounded answer spaces
- Framework assumes differences in uncertainty between demographic groups reflect meaningful internal bias, which may not hold across all domains or tasks
- MCQ format constrains the answer space, potentially inflating fairness scores compared to open generation scenarios where the model must generate responses

## Confidence

- **UCerF's ability to reveal hidden biases**: Medium - Theoretically sound with supporting case studies, but needs broader validation across diverse tasks
- **Perplexity as a sufficient uncertainty estimator**: High - Strong theoretical justification and empirical validation showing similar rankings to more complex estimators
- **SynthBias dataset validity**: Medium - Addresses known limitations of WinoBias through size and diversity, but validation is limited to this specific task and framework

## Next Checks

1. **Domain transfer test**: Evaluate UCerF on a different fairness-sensitive task (e.g., sentiment analysis with demographic attributes) to test whether perplexity-based uncertainty remains a reliable proxy for bias detection outside coreference resolution

2. **Uncertainty estimator robustness**: Implement UCerF using Monte Carlo dropout and ensemble methods alongside perplexity on the same task and models. Compare whether UCerF rankings change significantly with different uncertainty estimators

3. **Open generation comparison**: Modify the evaluation framework to allow open-ended responses rather than MCQ format. Compare UCerF scores and whether the metric still captures meaningful fairness disparities when the model must generate rather than select from options