---
ver: rpa2
title: 'NuRisk: A Visual Question Answering Dataset for Agent-Level Risk Assessment
  in Autonomous Driving'
arxiv_id: '2509.25944'
source_url: https://arxiv.org/abs/2509.25944
tags:
- risk
- reasoning
- baseline
- driving
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NuRisk addresses the challenge of agent-level quantitative risk
  assessment in autonomous driving by introducing a Visual Question Answering dataset
  with 2.9K scenarios and 1.1M agent-level samples, combining real-world data from
  nuScenes and Waymo with safety-critical scenarios from CommonRoad simulation. The
  dataset provides Bird-Eye-View sequential images with quantitative risk annotations,
  enabling spatio-temporal reasoning.
---

# NuRisk: A Visual Question Answering Dataset for Agent-Level Risk Assessment in Autonomous Driving

## Quick Facts
- arXiv ID: 2509.25944
- Source URL: https://arxiv.org/abs/2509.25944
- Authors: Yuan Gao; Mattia Piccinini; Roberto Brusnicki; Yuchen Zhang; Johannes Betz
- Reference count: 32
- 2.9K scenarios and 1.1M agent-level samples with quantitative risk annotations

## Executive Summary
NuRisk introduces a novel Visual Question Answering dataset designed specifically for agent-level risk assessment in autonomous driving. The dataset combines real-world data from nuScenes and Waymo with safety-critical scenarios from CommonRoad simulation, providing Bird-Eye-View sequential images with quantitative risk annotations that enable spatio-temporal reasoning. This addresses a critical gap in autonomous driving datasets by focusing on explicit risk assessment at the agent level rather than just object detection or behavior prediction.

The dataset's unique contribution lies in its comprehensive risk annotation pipeline that provides quantitative risk scores for individual agents across temporal sequences. By integrating real-world driving scenarios with synthetically generated safety-critical situations, NuRisk enables training and evaluation of models on both common and rare high-risk events. The dataset is specifically designed to test explicit spatio-temporal reasoning capabilities, which are essential for accurate risk assessment in dynamic driving environments.

## Method Summary
NuRisk is constructed by combining real-world driving data from established autonomous driving datasets (nuScenes and Waymo) with synthetically generated safety-critical scenarios from CommonRoad simulation. The dataset consists of Bird-Eye-View sequential images capturing agent behaviors over time, with each agent annotated with quantitative risk scores. The risk annotation pipeline assigns explicit numerical risk values based on the potential for collision or other safety violations, enabling models to learn quantitative rather than just binary risk assessments.

The dataset includes 2.9K unique driving scenarios containing 1.1M individual agent-level samples, providing sufficient diversity for training robust risk assessment models. The scenarios span various driving contexts including urban environments, highway driving, and complex intersection scenarios. Each scenario provides multi-frame sequences that capture the temporal evolution of risk, allowing models to learn from both spatial configurations and temporal dynamics of agent behaviors.

## Key Results
- Pre-trained VLMs achieve only 33% accuracy on agent-level risk assessment tasks
- Fine-tuning a 7B VLM with LoRA adaptation improves accuracy to 41%
- Fine-tuned models demonstrate 75% latency reduction compared to baseline approaches
- Proprietary models lack explicit spatio-temporal reasoning capabilities that the fine-tuned approach demonstrates

## Why This Works (Mechanism)
NuRisk works by providing a structured framework for learning quantitative risk assessment through explicit spatio-temporal reasoning. The dataset's combination of real-world and synthetic safety-critical scenarios exposes models to both common driving situations and rare high-risk events that are essential for comprehensive risk understanding. The quantitative risk annotations enable models to learn continuous risk values rather than binary classifications, which is more aligned with how human drivers assess risk in real-time.

The Bird-Eye-View representation simplifies the visual complexity while preserving the essential spatial relationships between agents, making it easier for models to focus on risk-relevant features. The temporal sequences capture the dynamic nature of risk evolution, allowing models to learn how risk changes over time based on agent behaviors and interactions. This spatio-temporal approach is crucial because risk in autonomous driving is inherently dynamic and context-dependent.

## Foundational Learning

**Visual Question Answering (VQA) in Autonomous Driving**
- Why needed: Enables models to answer specific risk-related questions about driving scenarios
- Quick check: Can the model answer "What is the risk level of the pedestrian crossing?" with quantitative scores

**Spatio-Temporal Reasoning**
- Why needed: Risk assessment requires understanding both spatial configurations and temporal dynamics
- Quick check: Can the model predict how risk changes as agents move over time

**Quantitative Risk Scoring**
- Why needed: Binary classifications are insufficient for nuanced risk assessment in real-world driving
- Quick check: Does the model output continuous risk values that correlate with actual collision probability

**Multi-Agent Interaction Modeling**
- Why needed: Risk often emerges from interactions between multiple agents, not individual behaviors
- Quick check: Can the model assess risk when multiple agents are in close proximity or conflicting trajectories

## Architecture Onboarding

**Component Map:**
Real-world Data (nuScenes/Waymo) -> Synthetic Safety-Critical Scenarios (CommonRoad) -> Risk Annotation Pipeline -> Dataset Construction -> VLM Training -> Risk Assessment Model

**Critical Path:**
The critical path involves processing sequential BEV images through the VLM backbone, extracting spatio-temporal features, and mapping these to quantitative risk scores. The LoRA fine-tuning adaptation layer is crucial for achieving the performance improvements while maintaining efficiency.

**Design Tradeoffs:**
The use of BEV representation simplifies visual processing but may lose some perspective information. The combination of real and synthetic data provides diversity but may introduce domain gaps. The quantitative risk annotation provides granularity but requires complex labeling processes.

**Failure Signatures:**
Models may fail on scenarios with unusual agent configurations not present in training data, struggle with long-term risk prediction beyond the temporal sequence window, or produce overconfident risk assessments for low-probability events.

**First 3 Experiments:**
1. Baseline evaluation of pre-trained VLMs on the NuRisk test set to establish performance metrics
2. Fine-tuning experiments with different VLM sizes and LoRA configurations to optimize accuracy-latency tradeoff
3. Ablation studies removing either real-world or synthetic scenarios to assess their relative contribution to model performance

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Pre-trained VLMs achieve only 33% accuracy, indicating the fundamental difficulty of the task
- The dataset relies heavily on synthetic safety-critical scenarios that may not fully capture real-world complexity
- Quantitative risk annotations may not encompass all relevant safety considerations in autonomous driving

## Confidence
**High confidence** in dataset construction methodology and technical validity of risk annotation pipeline
**Medium confidence** in performance claims based on specific VLM architectures and fine-tuning approaches
**Low confidence** in generalizability to real-world autonomous driving deployment

## Next Checks
1. Test dataset performance across multiple VLM architectures beyond the reported 7B model
2. Conduct ablation studies to determine relative importance of dataset components
3. Implement real-world pilot study to validate model improvements in naturalistic driving scenarios