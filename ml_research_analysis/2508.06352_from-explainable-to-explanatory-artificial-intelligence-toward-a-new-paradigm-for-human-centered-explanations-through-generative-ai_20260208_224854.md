---
ver: rpa2
title: 'From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm
  for Human-Centered Explanations through Generative AI'
arxiv_id: '2508.06352'
source_url: https://arxiv.org/abs/2508.06352
tags:
- explanatory
- systems
- explanations
- explanation
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Explanatory AI as a complementary paradigm
  to explainable AI, using generative AI capabilities to serve as explanatory partners
  for human understanding rather than providers of algorithmic transparency. Through
  a multidisciplinary synthesis and systematic eight-dimensional conceptual model,
  the authors distinguish Explanatory AI through narrative communication, adaptive
  personalization, and progressive disclosure principles.
---

# From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI

## Quick Facts
- arXiv ID: 2508.06352
- Source URL: https://arxiv.org/abs/2508.06352
- Reference count: 0
- This paper introduces Explanatory AI as a complementary paradigm to explainable AI, using generative AI capabilities to serve as explanatory partners for human understanding rather than providers of algorithmic transparency.

## Executive Summary
This paper proposes Explanatory AI as a complementary paradigm to traditional explainable AI, shifting focus from algorithmic transparency to narrative communication that serves as an explanatory partner for human understanding. Through a multidisciplinary synthesis and systematic eight-dimensional conceptual model, the authors distinguish Explanatory AI through principles of narrative communication, adaptive personalization, and progressive disclosure. Empirical validation through Rapid Contextual Design methodology with healthcare professionals demonstrates that users consistently prefer context-sensitive, multimodal explanations over technical transparency, providing strong evidence that users seek interpretive understanding rather than algorithmic introspection.

## Method Summary
The research employed Rapid Contextual Design methodology involving six contextual inquiries with nursing staff across German healthcare facilities in July 2024. Participants were selected for varying qualification levels and language proficiency (native vs. non-native German speakers). The method combined semi-structured dialogue with direct observation, followed by joint interpretation sessions and affinity diagram clustering to map themes to the eight-dimensional conceptual model. The study aimed to validate whether users prefer context-sensitive, multimodal explanations over technical transparency.

## Key Results
- Users consistently prefer context-sensitive, multimodal explanations over technical transparency
- Narrative, context-embedded explanations improve user comprehension and decision-making compared to statistical feature attributions
- Adaptive personalization and progressive disclosure principles reduce cognitive overload while maintaining explanatory depth on demand

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Narrative, context-embedded explanations improve user comprehension and decision-making compared to statistical feature attributions.
- Mechanism: Generative AI translates algorithmic outputs into human-aligned explanatory patterns—causal chains, analogies, and contrastive framing—that match how people naturally exchange understanding in social contexts.
- Core assumption: Users seek interpretive sensemaking ("Why does this make sense?") rather than algorithmic introspection ("How did the model decide?").
- Evidence anchors:
  - [abstract] "users consistently prefer context-sensitive, multimodal explanations over technical transparency"
  - [section 5] "caregivers are not primarily interested in technical fidelity, but in practical, interpretive support"
  - [corpus] Related work on LLM-enhanced XAI (arXiv:2506.05887) supports natural language translation of technical explanations
- Break condition: If domain requires strict audit trails where narrative simplification introduces legal/regulatory risk, this mechanism degrades.

### Mechanism 2
- Claim: Adaptive personalization increases explanation effectiveness across diverse user populations.
- Mechanism: System dynamically adjusts vocabulary complexity, explanation depth, and modality based on real-time signals (user role, language proficiency, cognitive load, time pressure).
- Core assumption: Personalization parameters can be inferred or explicitly provided without excessive user burden.
- Evidence anchors:
  - [abstract] "adaptive personalization" named as distinguishing principle
  - [section 5] "It would be helpful if the system could adapt to the skill level of the caregiver" (U01-6)
  - [corpus] Human-centered XAI evaluation frameworks (arXiv:2502.09849) emphasize stakeholder diversity
- Break condition: If personalization data is unavailable, inaccurate, or if adaptation latency exceeds real-time constraints, effectiveness drops.

### Mechanism 3
- Claim: Progressive disclosure reduces cognitive overload while maintaining explanatory depth on demand.
- Mechanism: Layered information structure presents minimal viable explanation first, then reveals deeper mechanistic detail through user-initiated follow-up, aligning with Gricean quantity maxim.
- Core assumption: Users can accurately assess their own comprehension gaps and will seek deeper explanations when needed.
- Evidence anchors:
  - [abstract] "progressive disclosure principles" included in conceptual model
  - [section 5] "users prefer being guided than being shown everything at once" (U01, U05); "too much explanation leads to more confusion, not less" (U04)
  - [corpus] Weak direct corpus evidence on progressive disclosure in XAI; this remains underexplored
- Break condition: If users under-request depth due to time pressure or overconfidence, critical information may remain undiscovered.

## Foundational Learning

- Concept: **Contrastive explanation** (explaining "Why P rather than Q?")
  - Why needed here: The paper emphasizes that human explanations are inherently contrastive; Explanatory AI must frame outputs relative to alternatives, not just state factors.
  - Quick check question: Can you articulate why the AI recommended X instead of the most likely alternative Y?

- Concept: **Gricean maxims** (quality, quantity, relevance, manner)
  - Why needed here: These pragmatics principles underpin the paper's argument that effective explanations provide "just enough" detail with relevance to user goals.
  - Quick check question: Does your explanation give the right amount of information—not too little, not too much—for this specific user and context?

- Concept: **Cognitive load theory**
  - Why needed here: Progressive disclosure and multimodal formatting are justified by the need to avoid overwhelming users, especially under time pressure or stress.
  - Quick check question: Can a user under high workload extract the actionable insight within 5–10 seconds?

## Architecture Onboarding

- Component map: Raw model output -> Contextual enrichment (domain knowledge, user context) -> Narrative generation -> Personalization adjustment -> User delivery -> Follow-up handling
- Critical path: Raw model output → Contextual enrichment (domain knowledge, user context) → Narrative generation → Personalization adjustment → User delivery → Follow-up handling
- Design tradeoffs:
  - **Fidelity vs. comprehensibility**: Strict algorithmic correspondence limits narrative flexibility; looser alignment improves understanding but risks oversimplification or hallucination
  - **Personalization depth vs. privacy**: Rich user profiles improve adaptation but raise data collection concerns
  - **Generation latency vs. interactivity**: Real-time dialogue requires fast inference; complex explanations may need precomputation or caching
- Failure signatures:
  - Users repeatedly ask follow-up questions indicating initial explanation was insufficient
  - Explanation contradicts known domain constraints (hallucination signal)
  - High variance in user satisfaction scores across different user subgroups (personalization gap)
  - Users bypass explanations entirely, suggesting perceived low value
- First 3 experiments:
  1. A/B test narrative vs. statistical explanations with target users; measure comprehension accuracy, decision quality, and subjective satisfaction.
  2. Progressive disclosure pilot: present minimal explanation first, track follow-up query depth and frequency to calibrate default information level.
  3. Cross-user personalization test: vary explanation complexity by stated expertise level; assess whether adaptation improves outcomes for low-expertise users without frustrating experts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific evaluation frameworks and metrics are required to effectively measure Explanatory AI quality regarding comprehensibility and narrative coherence?
- Basis in paper: [explicit] The authors explicitly call for "robust evaluation frameworks specifically designed for Explanatory AI" in Section 6.3, noting that current metrics focus on fidelity rather than communicative effectiveness.
- Why unresolved: Existing metrics (e.g., faithfulness) do not capture user-centric dimensions like narrative coherence or actionable insight.
- What evidence would resolve it: Validated instruments and standardized benchmarks that correlate automated measures with human comprehension and satisfaction.

### Open Question 2
- Question: What is the optimal balance between algorithmic fidelity and narrative comprehensibility across different high-stakes domains?
- Basis in paper: [explicit] Section 6.3 identifies the "critical research direction" of investigating the trade-off between accuracy and clarity, which varies by context (e.g., emergency medicine vs. financial planning).
- Why unresolved: It is unclear when technical precision should be prioritized over comprehensible approximation to support sound decision-making.
- What evidence would resolve it: Empirical data establishing boundary conditions where specific explanation strategies maximize decision quality.

### Open Question 3
- Question: How do user trust and critical thinking skills evolve during extended interactions with Explanatory AI systems?
- Basis in paper: [explicit] The research agenda in Section 6.3 highlights the need for longitudinal studies to assess potential negative effects like over-reliance or skill degradation.
- Why unresolved: Current empirical validation is cross-sectional (Rapid Contextual Design), capturing only initial preferences rather than long-term dynamics.
- What evidence would resolve it: Longitudinal user studies tracking trust calibration and decision-making autonomy over time.

## Limitations
- Empirical validation relies on a small sample (6 healthcare professionals) from a single geographic region, limiting generalizability across domains and cultures
- The 8-dimensional conceptual model was developed through synthesis rather than systematic literature review, introducing potential selection bias
- Technical implementation details for hallucination detection and progressive disclosure control are not specified

## Confidence
- **High confidence**: User preference for narrative over technical explanations (supported by multiple direct quotes and consistent across participants)
- **Medium confidence**: The 8-dimensional conceptual model's comprehensiveness (synthesized from literature but not empirically validated beyond the case study)
- **Low confidence**: Scalability of adaptive personalization across diverse user populations (limited sample size prevents robust subgroup analysis)

## Next Checks
1. Replicate the contextual inquiry methodology with 30+ participants across at least two different domains (healthcare and finance) to test generalizability
2. Conduct A/B testing comparing Explanatory AI outputs against traditional XAI explanations using standardized comprehension and decision quality metrics
3. Implement the hallucination detection guardrail and measure false positive/negative rates across 1,000 generated explanations