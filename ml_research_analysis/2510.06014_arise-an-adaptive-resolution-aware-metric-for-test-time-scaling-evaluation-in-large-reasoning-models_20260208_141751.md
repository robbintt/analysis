---
ver: rpa2
title: 'ARISE: An Adaptive Resolution-Aware Metric for Test-Time Scaling Evaluation
  in Large Reasoning Models'
arxiv_id: '2510.06014'
source_url: https://arxiv.org/abs/2510.06014
tags:
- scaling
- reasoning
- test-time
- arise
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ARISE, a novel evaluation metric for assessing
  test-time scaling in large reasoning models. The key innovation is a dual-awareness
  approach that combines sample-level evaluation to detect negative scaling behaviors
  (where more computation degrades performance) with a dynamic sampling mechanism
  that accounts for accuracy fluctuations and token count instability during scaling.
---

# ARISE: An Adaptive Resolution-Aware Metric for Test-Time Scaling Evaluation in Large Reasoning Models

## Quick Facts
- arXiv ID: 2510.06014
- Source URL: https://arxiv.org/abs/2510.06014
- Reference count: 0
- Primary result: Introduces ARISE metric that detects negative scaling behaviors and uses dynamic sampling to evaluate test-time scaling efficiency across reasoning domains

## Executive Summary
This paper introduces ARISE, a novel evaluation metric designed to assess test-time scaling capabilities in large reasoning models. The key innovation is a dual-awareness approach that combines sample-level evaluation to detect negative scaling behaviors (where increased computation degrades performance) with a dynamic sampling mechanism that accounts for accuracy fluctuations and token count instability during scaling. The metric was evaluated across multiple reasoning domains including mathematical problem-solving, code generation, and agentic tasks. Experimental results demonstrate that ARISE provides reliable, fine-grained measurement of scaling efficiency, revealing substantial performance differences among state-of-the-art models. Notably, Claude Opus achieved the highest test-time scaling score among evaluated models, establishing it as a leader in scaling efficiency compared to other contemporary reasoning models.

## Method Summary
ARISE evaluates test-time scaling by combining sample-level awareness with dynamic sampling mechanisms. The metric assesses model performance across multiple compute budgets (token/sample counts) while identifying instances where additional computation degrades accuracy (negative scaling). A penalty system discourages this behavior, while adaptive sampling reduces measurement noise from variance in accuracy and token usage. The method produces a unified scaling efficiency score that can be broken down by domain, enabling comparison of how effectively models convert additional compute into accuracy gains across mathematical reasoning, code generation, and agentic tasks.

## Key Results
- ARISE successfully identifies and penalizes negative scaling behaviors where increased computation leads to performance degradation
- The metric provides stable, reproducible measurements by mitigating accuracy fluctuations and token count instability through dynamic sampling
- Claude Opus demonstrated superior scaling efficiency among evaluated models, achieving the highest test-time scaling score
- ARISE enables fine-grained comparison of scaling efficiency across diverse reasoning domains while maintaining interpretability

## Why This Works (Mechanism)

### Mechanism 1: Sample-Level Negative Scaling Detection
- Claim: ARISE identifies and penalizes instances where additional test-time compute degrades model performance, rather than rewarding aggregate improvements that mask per-sample failures.
- Mechanism: The metric evaluates accuracy trajectories at the individual sample level across multiple compute budgets. When a model's accuracy decreases as token allocation increases (negative scaling), this behavior incurs a penalty proportional to the degradation magnitude. This prevents models from achieving high scores through strong performance on easy problems while degrading on harder ones that should benefit from additional compute.
- Core assumption: Monotonic or improving accuracy with increased compute represents desirable scaling behavior; degradation indicates flawed reasoning accumulation rather than problem difficulty.
- Evidence anchors:
  - [abstract] "sample-level awareness that effectively penalizes negative scaling behaviors where increased computation leads to performance degradation"
  - [corpus] "Does Thinking More always Help? Mirage of Test-Time Scaling in Reasoning Models" directly validates this concern, demonstrating that extended thinking traces do not reliably improve outcomes
  - [corpus] "What If We Allocate Test-Time Compute Adaptively?" suggests uniform allocation is suboptimal, supporting sample-level differentiation
- Break condition: If problem difficulty distributions are highly skewed, sample-level penalties may conflate genuine problem intractability with model scaling inefficiency.

### Mechanism 2: Dynamic Sampling for Variance Mitigation
- Claim: ARISE employs adaptive sampling to reduce measurement noise from accuracy fluctuations and token count instability during scaling evaluation.
- Mechanism: Test-time scaling measurements exhibit variance—accuracy may spike or dip at different compute levels due to sampling randomness or verification inconsistency. The dynamic sampling mechanism adjusts sample density or weighting based on observed variance at each compute level, reducing outlier influence on aggregate scores. This produces more stable, reproducible scaling efficiency measurements.
- Core assumption: Accuracy and token count fluctuations during scaling represent measurement noise rather than meaningful signal about model behavior.
- Evidence anchors:
  - [abstract] "dynamic sampling mechanism that mitigates the impact of accuracy fluctuations and token count instability on the final assessment"
  - [corpus] "Seer Self-Consistency: Advance Budget Estimation for Adaptive Test-Time Scaling" addresses related token consumption and latency variability concerns
  - [corpus] "Adaptive Termination for Multi-round Parallel Reasoning" demonstrates semantic entropy-guided approaches for handling reasoning variance
- Break condition: If fluctuations encode genuine behavioral signal (e.g., mode-switching in agentic tasks), dynamic sampling may obscure meaningful performance patterns.

### Mechanism 3: Cross-Domain Unified Scaling Efficiency Scoring
- Claim: ARISE produces comparable scaling efficiency scores across diverse reasoning domains, enabling standardized model comparison beyond fixed-compute accuracy.
- Mechanism: By combining sample-level penalties with variance-adjusted sampling, ARISE generates a single interpretable efficiency metric reflecting how effectively a model converts additional compute into accuracy gains. This allows ranking models on scaling quality—Claude Opus scored highest among evaluated models—rather than conflating scaling capability with baseline performance.
- Core assumption: Scaling efficiency is a measurable, partially domain-transferable capability; aggregate scoring meaningfully summarizes multi-domain behavior.
- Evidence anchors:
  - [abstract] "comprehensive experiments evaluating state-of-the-art reasoning models across diverse domains including mathematical reasoning, code generation, and agentic tasks"
  - [abstract] "Claude Opus as exhibiting superior scaling characteristics compared to other contemporary reasoning models"
  - [corpus] "m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning" suggests domain-specific scaling behavior exists, which ARISE must aggregate meaningfully
- Break condition: If scaling efficiency is highly domain-specific without transfer, single-score aggregation may mask critical specializations or weaknesses.

## Foundational Learning

- Concept: Test-Time Scaling (Inference-Time Compute)
  - Why needed here: ARISE evaluates this paradigm where models dynamically allocate computation during inference—longer chains-of-thought, multiple samples, iterative refinement—rather than during training. Without understanding that "more thinking" can degrade results, ARISE's motivation is unclear.
  - Quick check question: Name two reasons why increasing test-time compute might reduce a model's accuracy on certain problems.

- Concept: Negative/Inverse Scaling
  - Why needed here: ARISE's core innovation is detecting and penalizing this behavior. Understanding that some problems exhibit U-shaped or monotonically decreasing performance with more compute is essential.
  - Quick check question: If accuracy drops from 75% to 55% when token budget increases from 200 to 800, should the evaluation score increase, decrease, or stay the same? What does ARISE do?

- Concept: Evaluation Robustness to Variance
  - Why needed here: The dynamic sampling mechanism addresses measurement instability. Understanding why raw accuracy at fixed compute is insufficient—due to variance, non-monotonicity, and sampling noise—frames ARISE's contribution.
  - Quick check question: Why might averaging accuracy across compute levels be misleading if variance differs substantially between levels?

## Architecture Onboarding

- Component map:
  - Input layer -> Sample-level evaluator -> Dynamic sampler -> Penalty module -> Aggregation layer -> Output
  - Input: Model outputs at multiple compute budgets across evaluation domains
  - Sample-level evaluator: Per-problem accuracy trajectory computation; detects negative scaling instances
  - Dynamic sampler: Variance-aware weighting; adjusts sample density based on observed instability
  - Penalty module: Applies degradation penalties for inverse scaling behavior
  - Aggregation layer: Combines weighted sample scores into domain and overall ARISE scores
  - Output: Single scaling efficiency score per model with optional domain breakdowns

- Critical path:
  1. Collect model responses at defined compute budgets (e.g., 100, 500, 1000, 2000 tokens) for each test sample
  2. Compute accuracy at each budget point per sample
  3. Identify negative scaling instances and apply proportional penalties
  4. Calculate variance at each budget level; apply dynamic sampling weights
  5. Aggregate across samples within domains, then across domains for final score

- Design tradeoffs:
  - Granularity vs. interpretability: Sample-level precision enables fine-grained detection but may over-penalize genuinely difficult problems
  - Domain weighting: Equal domain weighting vs. task-importance weighting affects model rankings
  - Variance threshold: Aggressive noise reduction increases evaluation cost; lenient thresholds risk unstable scores

- Failure signatures:
  - High-variance models score poorly despite strong average performance (exploratory reasoning penalized)
  - Domain specialists show misleading aggregate scores if weak domains dominate weighting
  - Budget boundary effects: poorly chosen compute levels miss critical scaling inflection points

- First 3 experiments:
  1. Baseline validation: Run ARISE on 2-3 models across 100 MATH problems at 4 token budgets. Verify negative scaling detection triggers correctly and scores differentiate models as expected.
  2. Component ablation: Disable the penalty module and compare scores to full ARISE. Quantify score changes when ignoring negative scaling—this reveals how much inverse scaling affects rankings.
  3. Cross-domain consistency: Evaluate one model on math, code generation, and a simple agentic task. Confirm domain-specific scaling patterns appear in breakdowns rather than being averaged into opacity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ARISE perform when applied to multi-modal reasoning tasks or domains requiring subjective evaluation?
- Basis in paper: [inferred] The experimental scope is explicitly limited to "mathematical reasoning, code generation, and agentic tasks," leaving its effectiveness in other domains unverified.
- Why unresolved: Visual reasoning and open-ended generation exhibit different error distributions and token usage patterns than the logical tasks tested.
- What evidence would resolve it: Successful application of ARISE to multi-modal benchmarks (e.g., visual QA) and subjective tasks while maintaining stable rankings.

### Open Question 2
- Question: Is the penalization weight for negative scaling behaviors mathematically optimal across diverse model architectures?
- Basis in paper: [inferred] The paper introduces a mechanism to "effectively penalize negative scaling behaviors," but the specific sensitivity of this penalty to different reasoning strategies (e.g., chain-of-thought vs. brute-force) is not discussed.
- Why unresolved: An aggressive penalty might unfairly disadvantage smaller models that require more exploration to reach correct answers.
- What evidence would resolve it: A sensitivity analysis demonstrating that the metric's model rankings remain stable even when the negative-scaling penalty hyperparameter is adjusted.

### Open Question 3
- Question: To what extent does a high ARISE score predict success on out-of-distribution (OOD) reasoning tasks?
- Basis in paper: [inferred] While the paper claims ARISE measures "scaling efficiency," it does not validate whether this efficiency is a generalizable trait or merely reflective of in-distribution performance.
- Why unresolved: A model might scale efficiently on known benchmarks (math/code) but fail to apply those resources effectively when facing novel problem structures.
- What evidence would resolve it: Correlation analysis between ARISE scores derived from standard benchmarks and performance gains on unseen, complex reasoning datasets.

## Limitations

- The paper's abstract does not specify the exact mathematical formulation of ARISE, making independent implementation challenging without access to the full methodology
- Domain transfer validity remains unclear - while ARISE claims cross-domain comparability, scaling efficiency may be inherently domain-specific
- The relationship between dynamic sampling parameters and variance thresholds is not specified, potentially affecting reproducibility

## Confidence

- **High confidence**: The conceptual framework of detecting negative scaling through sample-level evaluation is well-supported by cited literature on inverse scaling phenomena
- **Medium confidence**: The claim that Claude Opus exhibits superior scaling characteristics is plausible given its known architecture, but comparative evaluation details are missing from the abstract
- **Low confidence**: The dynamic sampling mechanism's effectiveness in practice cannot be verified without implementation details and empirical validation data

## Next Checks

1. **Negative scaling detection validation**: Create synthetic datasets with known inverse scaling patterns (e.g., accuracy dropping at higher compute levels) and verify ARISE correctly identifies and penalizes these cases
2. **Variance sensitivity analysis**: Test ARISE's robustness by running identical models with different random seeds at each compute budget, measuring how dynamic sampling affects score stability
3. **Domain transferability test**: Evaluate a single model across domains with varying difficulty distributions to verify ARISE scores meaningfully compare scaling efficiency rather than conflating problem difficulty with scaling capability