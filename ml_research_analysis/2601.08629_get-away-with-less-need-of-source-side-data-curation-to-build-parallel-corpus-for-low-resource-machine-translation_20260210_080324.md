---
ver: rpa2
title: 'Get away with less: Need of source side data curation to build parallel corpus
  for low resource Machine Translation'
arxiv_id: '2601.08629'
source_url: https://arxiv.org/abs/2601.08629
tags:
- data
- sentences
- sentence
- lalita
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research addresses the challenge of developing effective Machine
  Translation (MT) systems for low-resource languages, where acquiring sufficient
  high-quality parallel data is prohibitively expensive. The study introduces LALITA
  (LexicalAndLinguisticallyInformedTextAnalysis), a framework that uses linguistic
  and lexical features to assess and select structurally complex source sentences
  for optimal MT training.
---

# Get away with less: Need of source side data curation to build parallel corpus for low resource Machine Translation

## Quick Facts
- **arXiv ID**: 2601.08629
- **Source URL**: https://arxiv.org/abs/2601.08629
- **Reference count**: 40
- **Primary result**: LALITA framework reduces MT training data needs by over 50% while maintaining or improving translation quality across multiple languages

## Executive Summary
This research introduces LALITA, a novel framework for source-side data curation in low-resource machine translation. The framework addresses the critical challenge of building effective MT systems when parallel data is scarce and expensive to obtain. LALITA uses linguistic and lexical features to identify structurally complex sentences that are optimal for training, enabling significant data reduction while maintaining or improving translation quality. The approach demonstrates effectiveness across multiple languages including Hindi, Odia, Nepali, Norwegian Nynorsk, and German, achieving comparable results with less than half the training data.

## Method Summary
LALITA employs a two-stage process: first, it extracts lexical and linguistic features from source sentences using SpaCy's dependency parser and morphological analyzers. Second, it applies Principal Component Analysis (PCA) to these feature vectors to identify the most informative dimensions and calculate a LALITA score that quantifies sentence complexity. This score guides the selection of sentences for training data curation. The framework prioritizes structurally complex sentences that provide maximum learning value for MT systems, based on the hypothesis that such sentences help models learn generalizable patterns rather than simple phrase mappings.

## Key Results
- Reduces training data requirements by over 50% across multiple language pairs while maintaining or improving translation quality
- Achieves BLEU scores comparable to or better than models trained on full datasets
- Demonstrates effectiveness for both low-resource and high-resource scenarios
- Shows consistent performance across diverse languages including morphologically rich languages like Hindi and Odia

## Why This Works (Mechanism)
LALITA works by identifying and prioritizing structurally complex sentences that provide maximum learning value for MT systems. The framework leverages linguistic features extracted through dependency parsing and morphological analysis, combined with lexical complexity measures. By focusing on complex sentences rather than simple ones, the model learns to handle diverse syntactic structures and morphological variations, leading to more robust translation capabilities. The PCA-based feature reduction ensures that only the most informative aspects of sentence complexity are considered, making the curation process both effective and computationally efficient.

## Foundational Learning

1. **Lexical Complexity Analysis**: Measures word-level complexity using metrics like syllable count and character length
   - *Why needed*: To identify sentences with diverse vocabulary that challenge the model to learn richer representations
   - *Quick check*: Compare word frequency distributions between selected and non-selected sentences

2. **Syntactic Dependency Parsing**: Analyzes grammatical relationships between words in sentences
   - *Why needed*: To capture structural complexity that influences translation difficulty
   - *Quick check*: Verify dependency tree depth and branching patterns vary appropriately

3. **Morphological Analysis**: Examines word forms and their grammatical properties
   - *Why needed*: Essential for languages with rich morphology where word forms carry significant meaning
   - *Quick check*: Ensure selected sentences contain diverse morphological variants

## Architecture Onboarding

**Component Map**: Raw Text -> SpaCy Dependency Parser -> Morphological Analyzer -> Feature Vector Extraction -> PCA Reduction -> LALITA Score Calculation -> Data Selection

**Critical Path**: The core pipeline flows from text preprocessing through linguistic feature extraction to PCA-based complexity scoring, with each stage building on the previous to identify optimal training sentences.

**Design Tradeoffs**: Prioritizes data efficiency over computational simplicity by using complex linguistic analysis, but reduces overall training costs through smaller datasets. The PCA step trades some feature detail for computational tractability.

**Failure Signatures**: Poor performance on highly divergent language pairs, over-reliance on BLEU scores for evaluation, potential bias in PCA feature selection affecting certain language families.

**First Experiments**:
1. Test LALITA on a typologically distant language pair (e.g., Chinese-Japanese) to assess cross-family applicability
2. Conduct human evaluation studies alongside automated metrics to verify translation quality improvements
3. Evaluate performance on truly zero-resource scenarios using only monolingual data sources

## Open Questions the Paper Calls Out

The paper acknowledges several open questions regarding the framework's applicability to truly zero-resource scenarios where no parallel data exists, the potential biases introduced by PCA-based feature selection, and how the approach might perform on language pairs with very different typological characteristics. The authors also note the need for more comprehensive human evaluation studies to complement automated metrics.

## Limitations

- Primarily tested on Indo-European and Indo-Aryan languages, limiting generalizability to typologically diverse language pairs
- Heavy reliance on BLEU scores may not fully capture translation quality nuances, especially for morphologically rich languages
- Computational efficiency claims based on single GPU setup, limiting hardware configuration generalizability
- Does not address potential biases introduced by PCA-based feature selection across different language families

## Confidence

- **High confidence** in the core finding that curated data can achieve comparable results with reduced training data
- **Medium confidence** in the generalizability of LALITA across diverse language pairs and resource scenarios
- **Low confidence** in the framework's effectiveness for highly divergent language families or in zero-resource scenarios

## Next Checks

1. Evaluate LALITA on typologically diverse language pairs (e.g., Chinese-Japanese, Arabic-Finnish) to test cross-family applicability
2. Conduct human evaluation studies alongside automated metrics to verify translation quality improvements
3. Test the framework's performance on truly zero-resource scenarios where no parallel data exists, using only monolingual data sources