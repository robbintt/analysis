---
ver: rpa2
title: 'AToken: A Unified Tokenizer for Vision'
arxiv_id: '2509.14476'
source_url: https://arxiv.org/abs/2509.14476
tags:
- video
- reconstruction
- understanding
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AToken, the first unified visual tokenizer
  achieving both high-fidelity reconstruction and semantic understanding across images,
  videos, and 3D assets. The key innovation is a shared 4D latent space processed
  by a pure transformer architecture with 4D rotary position embeddings, enabling
  unified handling of all modalities.
---

# AToken: A Unified Tokenizer for Vision

## Quick Facts
- arXiv ID: 2509.14476
- Source URL: https://arxiv.org/abs/2509.14476
- Authors: Jiasen Lu, Liangchen Song, Mingze Xu, Byeongjoo Ahn, Yanjun Wang, Chen Chen, Afshin Dehghan, Yinfei Yang
- Reference count: 34
- Primary result: First unified visual tokenizer achieving high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets using a shared 4D latent space and transformer architecture.

## Executive Summary
AToken introduces the first unified visual tokenizer capable of handling images, videos, and 3D assets within a single framework. The key innovation is a shared 4D latent space processed by a pure transformer architecture with 4D rotary position embeddings, enabling unified handling of all modalities. To overcome training instabilities common in transformer-based tokenizers, the authors introduce an adversarial-free objective combining perceptual and Gram matrix losses. A progressive curriculum gradually extends capabilities from images to videos and 3D, supporting both continuous and discrete representations. The resulting model achieves state-of-the-art reconstruction (0.21 rFID, 82.2% ImageNet accuracy for images; 3.01 rFVD, 40.2% MSRVTT retrieval for videos; 28.28 PSNR, 90.9% accuracy for 3D) while enabling diverse downstream applications from multimodal LLMs to image-to-3D synthesis.

## Method Summary
AToken encodes diverse visual inputs into a shared 4D latent space using a sparse transformer encoder initialized from SigLIP2-SO400M-patch16-naflex. The architecture processes space-time patches (4×16×16) with 4D rotary position embeddings, supporting native resolution handling across modalities. Training follows a progressive curriculum: Stage 1 establishes image reconstruction and understanding, Stage 2 adds video with expanded latent dimensions, and Stage 3 incorporates 3D via multi-view aggregation. The adversarial-free objective combines L1 reconstruction, LPIPS perceptual loss, Gram matrix loss for texture covariance, and CLIP distillation. A round-robin sampling strategy with gradient accumulation balances task objectives across modalities while preventing catastrophic forgetting.

## Key Results
- Achieves state-of-the-art reconstruction: 0.21 rFID for images (vs 0.22 baseline), 3.01 rFVD for videos, 28.28 PSNR for 3D
- Maintains strong semantic understanding: 82.2% zero-shot ImageNet accuracy, 40.2% MSRVTT retrieval, 90.9% 3D classification accuracy
- Progressive curriculum improves single-modality performance: Image rFID improves from 0.258 to 0.209 through multimodal expansion
- Supports both continuous latents (VAE-style) and discrete tokens via FSQ quantization for diverse downstream applications

## Why This Works (Mechanism)

### Mechanism 1: Sparse 4D Latent Space Unification
A single shared 4D representation can encode images, videos, and 3D assets without modality-specific architectural modifications. Each modality activates different subspaces within a unified 4D coordinate system (t, x, y, z): images occupy the (x, y) plane at t = z = 0; videos extend temporally with z = 0; 3D assets populate (x, y, z) voxels at t = 0. The sparse representation stores feature-position pairs rather than dense grids, enabling native resolution processing and efficient cross-modal learning through shared attention layers with 4D rotary position embeddings. This works because visual modalities share sufficient structural regularities (spatial coherence, local texture patterns) that a unified positional encoding can transfer learning across modalities without catastrophic interference.

### Mechanism 2: Adversarial-Free Training via Gram Matrix Loss
Combining perceptual loss with Gram matrix terms achieves state-of-the-art reconstruction without the instabilities of GAN-based training. The authors decompose reconstruction error (rFID) into mean and covariance components, finding ~86.6% stems from covariance (texture/style). Gram matrix loss directly optimizes feature covariances by minimizing ||G(Φ(x)) - G(Φ(x̂))||²_F across network layers, capturing second-order statistics that perceptual losses (LPIPS) miss. This eliminates discriminator-generator dynamics that cause mode collapse in transformer-based tokeners. The approach works because covariance matching is sufficient for texture fidelity while pixel-level L1 + perceptual features handle structure and semantics.

### Mechanism 3: Progressive Multimodal Curriculum
Training in stages (image → video → 3D) enables cross-modal transfer without catastrophic forgetting, and can even improve single-modality performance. Stage 1 establishes image reconstruction + understanding from SigLIP2 initialization. Stage 2 adds video with expanded latent dimensions (32→48). Stage 3 incorporates 3D via multi-view aggregation. Round-robin sampling with gradient accumulation balances task objectives. The sparse transformer separates features from positions, allowing each modality to train at native resolution without padding interference. This works because early-stage visual features (edges, textures) transfer across modalities; temporal and geometric refinements build on this foundation without overriding learned representations.

## Foundational Learning

- **Visual Tokenization (VAE/VQ-VAE)**: AToken outputs both continuous latents (VAE-style) and discrete tokens (via FSQ quantization). Understanding the compression-reconstruction tradeoff is essential for interpreting latent dimension choices (32→48 channels) and downstream generation quality.
  - Quick check question: Can you explain why VAE latents are continuous while VQ-VAE produces discrete codes, and when each is preferred for downstream tasks?

- **Rotary Position Embeddings (RoPE)**: AToken extends 2D RoPE to 4D (t, x, y, z) for unified positional encoding across modalities. This enables relative position awareness and native resolution handling without learned absolute embeddings.
  - Quick check question: How does RoPE encode relative positions differently from learned absolute position embeddings, and why does this support variable-length sequences?

- **Perceptual Losses (LPIPS, Gram Matrix)**: The adversarial-free training relies on LPIPS for perceptual similarity and Gram loss for texture/covariance matching. Understanding what each captures guides loss coefficient tuning.
  - Quick check question: What does LPIPS measure that pixel-level L1 loss misses, and what additional information does Gram matrix loss provide over LPIPS?

## Architecture Onboarding

- **Component map**: Input → space-time patchify → sparse encoder (4D RoPE attention) → dual projections → (a) decoder for reconstruction, (b) attention pool + text alignment for understanding
- **Critical path**: The sparse transformer encoder processes feature-position pairs with 4D rotary position embeddings, producing latents that feed both reconstruction and understanding heads. The decoder mirrors encoder architecture for image/video output or Gaussian splatting parameters for 3D.
- **Design tradeoffs**: Transformer vs. Convolutional (native resolution vs. reconstruction quality), latent dimensions (32 vs 48 channels for capacity), sparse vs. dense representation (arbitrary resolution vs. simplicity), continuous vs. discrete tokens (reconstruction quality vs. LLM compatibility).
- **Failure signatures**: GAN instability (diverging discriminator logits), cross-modal interference (degraded image quality), 3D reconstruction artifacts (color shifts), temporal incoherence in video (flickering frames).
- **First 3 experiments**: 1) Reproduce single-modality baseline with L1 + LPIPS + Gram + CLIP losses, verify rFID < 0.3 and >80% accuracy. 2) Ablate Gram loss, compare rFID trajectory against full objective. 3) Test cross-modal transfer after Stage 2, confirm image quality preserved/improved (0.258 → 0.209 pattern).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a single "omnimodel" trained on AToken's unified latent space effectively perform generation and understanding tasks across images, videos, and 3D simultaneously?
- Basis in paper: Section 7 states, "Due to limited computational resources, we could only test AToken on separate downstream tasks. Building the comprehensive omnimodel that would demonstrate AToken's full potential remains as future work."
- Why unresolved: The authors evaluated downstream applications using separate models rather than a single, unified generative and understanding model built on top of AToken.
- What evidence would resolve it: Training a single transformer-based model on AToken latents to perform all tasks concurrently and benchmarking it against modality-specific state-of-the-art models.

### Open Question 2
- Question: How can diffusion models be optimized to handle AToken's high-dimensional (48-channel) latent space to match the fidelity of specialized 3D synthesis?
- Basis in paper: Section 5.5 notes a performance gap in image-to-3D tasks and hypothesizes that "A diffusion model operating in this higher-dimensional space likely requires further optimization... We leave the exploration of these optimizations as a promising direction for future work."
- Why unresolved: The authors' generative model struggled to maintain color and style consistency compared to baselines like Trellis-SLAT, likely due to the jump from 8 latent channels (baseline) to 48 channels (AToken).
- What evidence would resolve it: Developing and evaluating diffusion schedules or conditioning mechanisms specifically tuned for high-dimensional latents to close the quality gap with specialized 3D tokenizers.

### Open Question 3
- Question: Is explicit semantic clustering in low-dimensional latent spaces necessary for strong performance, or can larger models effectively leverage seemingly intermixed representations?
- Basis in paper: Section 4.5 observes that while dense features show semantic clustering, projected 48-dim latents appear "intermixed" in T-SNE visualizations. The authors explicitly state they "leave detailed investigation of semantic preservation through aggressive dimensionality reduction for future work."
- Why unresolved: There is a discrepancy between the T-SNE visualizations (suggesting mixed class distributions) and the high performance on downstream tasks, leaving the necessity of latent space structure unclear.
- What evidence would resolve it: A quantitative analysis comparing the geometric properties of the latent space against downstream accuracy, specifically testing if enforcing semantic clustering improves or degrades reconstruction and understanding trade-offs.

## Limitations
- 4D RoPE design lacks ablation studies against modality-specific positional encodings or simpler fusion strategies
- 3D reconstruction pipeline relies on synthetic multiview rendering, may not generalize to real-world capture scenarios with occlusions and non-Lambertian surfaces
- Adversarial-free training stability claims lack direct head-to-head comparison with lightweight GAN alternatives

## Confidence
- **High confidence**: Reconstruction quality metrics (rFID, rFVD, PSNR) and zero-shot understanding accuracy across all three modalities
- **Medium confidence**: Claims about cross-modal transfer benefits and progressive curriculum effectiveness
- **Low confidence**: Claims about downstream applications (multimodal LLM integration, image-to-3D synthesis) are demonstrated but not rigorously evaluated

## Next Checks
1. **Ablate 4D RoPE**: Train AToken with modality-specific 2D RoPE (images/videos) and 3D positional encoding, keeping all else equal. Compare cross-modal learning speed and final reconstruction quality to validate whether 4D RoPE's complexity is justified by measurable gains.
2. **Evaluate 3D robustness**: Test AToken's 3D reconstruction on synthetic-to-real transfer benchmarks (e.g., synthetic CAD models → real indoor scenes). Measure PSNR degradation and artifact frequency to assess generalization beyond controlled multiview rendering conditions.
3. **Compare training stability**: Run parallel training experiments using the Gram loss objective versus a lightweight GAN (e.g., PatchGAN discriminator) on the same AToken architecture. Track FID trajectories, mode collapse incidents, and final distribution coverage to provide direct evidence for adversarial-free stability claims.