---
ver: rpa2
title: 'A tale of two goals: leveraging sequentiality in multi-goal scenarios'
arxiv_id: '2503.21677'
source_url: https://arxiv.org/abs/2503.21677
tags:
- goal
- goals
- agent
- next
- final
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses a failure mode in goal-conditioned reinforcement
  learning when following sequences of intermediate goals: standard approaches can
  get stuck in goal configurations that prevent completing subsequent goals. To solve
  this, the authors introduce two new Markov Decision Process (MDP) formulations where
  the agent conditions on both the current and either the next or final goal in the
  sequence.'
---

# A tale of two goals: leveraging sequentiality in multi-goal scenarios

## Quick Facts
- **arXiv ID**: 2503.21677
- **Source URL**: https://arxiv.org/abs/2503.21677
- **Reference count**: 6
- **Primary result**: Two new MDP formulations for multi-goal RL (M2g: next two goals; Mgseq: current + final goal) outperform standard single-goal conditioning in navigation and pole-balancing tasks.

## Executive Summary
This paper addresses a fundamental failure mode in goal-conditioned reinforcement learning when following sequences of intermediate goals. Standard approaches that condition only on the current goal can get stuck in goal configurations that prevent completing subsequent goals. The authors introduce two new MDP formulations—Mgseq (current + final goal) and M2g (next two goals)—that incorporate sequential information into the policy conditioning. Experiments on three environments (Dubins Hallway, GC-Cartpole, PointMaze Serp3) show that M2g-TD3 achieves higher success rates and faster learning compared to Mgseq-TD3 and standard TD3, with the method also showing robustness to sparse reward settings through dual-goal hindsight relabeling.

## Method Summary
The paper proposes two MDP formulations for multi-goal RL: Mgseq conditions the policy on both the current behavioral goal and the final goal in the sequence, while M2g conditions on the current behavioral goal and the next goal. Both use TD3 with modified HER relabeling that samples two achieved goals from the same trajectory to relabel both the behavioral and future goals. The method includes a fixed expert planner that provides intermediate goal sequences, and the policy transitions to the next goal once the current one is reached. The approach is evaluated across three environments with different state spaces and goal thresholds.

## Key Results
- M2g-TD3 outperforms both Mgseq-TD3 and standard TD3 in sample efficiency and final performance across all three environments
- Mgseq-TD3 suffers from value propagation issues, with rewards failing to propagate beyond 2-3 steps in the sequence
- The dual-goal hindsight relabeling significantly improves performance in sparse reward settings
- M2g avoids the "stop-at-every-goal" behavior seen in myopic agents while maintaining better value propagation than Mgseq

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Conditioning a policy on the next two sequential goals (M2g) enforces state-compatibility that myopic single-goal conditioning misses.
- **Mechanism:** The optimization objective incorporates the achievability of the subsequent goal while reaching the current one. This forces the value function to assign low scores to states where the current goal is reached in a configuration that blocks progress toward the second goal (e.g., facing a wall).
- **Core assumption:** The planning horizon of two goals is sufficient to resolve local incompatibilities, and the state space allows for "preparation" actions before reaching the current goal.
- **Evidence anchors:**
  - [abstract]: "The low-level policy is typically conditioned on the current goal... this approach can fail when an intermediate goal can be reached in multiple ways."
  - [section 3.2]: "The agent must learn how to always be able to reach a first goal in a configuration compatible with reaching the second one."
  - [corpus]: Corpus evidence is weak; related papers like "Multi-Goal Dexterous Hand Manipulation" address multi-goal efficiency but do not explicitly validate the "next-two-goals" state-compatibility mechanism.
- **Break condition:** Fails if the critical state configuration requires looking ahead 3+ goals (e.g., a complex maneuver requiring setup two steps in advance), which M2g explicitly ignores.

### Mechanism 2
- **Claim:** Reducing input diversity and temporal horizon via the M2g formulation accelerates value propagation relative to full-sequence conditioning (Mgseq).
- **Mechanism:** In M2g, the "final" goal input is actually just the next immediate goal in the planner's sequence. This reduces the variance of the input distribution compared to Mgseq (where the final goal can be anywhere), allowing the critic to generalize faster across a narrower temporal horizon.
- **Core assumption:** The underlying TD3 architecture struggles with high input variance and long-horizon credit assignment.
- **Evidence anchors:**
  - [section 4.6]: "Mgseq-TD3 faces much higher diversity because its final goal can be any goal in the goal space... M2g-TD3 is defined on a shorter temporal horizon, facilitating value propagation."
  - [section 4.3]: Mentions that Mgseq-TD3 value updates "have only propagated from the next two goals or, at best, the next three goals."
  - [corpus]: Not explicitly compared in the provided corpus.
- **Break condition:** Breaks if the environment requires global planning information (knowing the final destination is strictly necessary to choose the correct local branch), as M2g only sees the immediate next step.

### Mechanism 3
- **Claim:** Dual-goal hindsight relabeling stabilizes learning in sparse reward settings.
- **Mechanism:** The method extends Hindsight Experience Replay (HER) to relabel both the behavioral goal and the future goal. By sampling achieved goals from the same trajectory, the agent generates dense supervision for the sequential transition.
- **Core assumption:** Achieved goals in a random trajectory contain useful signal about sequential reachability.
- **Evidence anchors:**
  - [section 3.2]: "Given a sample... we can relabel bgt → ag_{k1} and fgt → ag_{k2}."
  - [section 4.5]: Ablation study shows that omitting current goal relabeling significantly affects performance in sparse environments.
  - [corpus]: "Goal-based Self-Adaptive Generative Adversarial Imitation Learning" mentions HER for multi-goal tasks but does not confirm the dual-relabeling specific mechanism.
- **Break condition:** Fails if the achieved goals in random trajectories rarely form valid sequential transitions (i.e., the data is too noisy to extract structure).

## Foundational Learning

- **Concept:** Markov Property in Goal-Conditioned MDPs
  - **Why needed here:** The paper argues that transitioning between goals based on an unobserved final goal violates the Markov property. Understanding why we must include the final/next goal in the observation (s, bg, fg) is key to grasping the architecture.
  - **Quick check question:** If the policy observes only (s, bg) but the transition to bg_next depends on the hidden final goal fg, is the process Markovian?

- **Concept:** Hindsight Experience Replay (HER)
  - **Why needed here:** The proposed M2g/Mgseq architectures rely heavily on a modified HER strategy for sample efficiency.
  - **Quick check question:** How does replacing the intended goal with an actually achieved goal solve the sparse reward problem?

- **Concept:** Value Propagation Horizon
  - **Why needed here:** The paper attributes the superior performance of M2g over Mgseq to "shorter temporal horizons" and better value propagation.
  - **Quick check question:** Why is it harder for a TD(0) critic to learn the value of a state if the reward is 10 steps away versus 2 steps away?

## Architecture Onboarding

- **Component map:**
  Planner (expert system) -> Controller (TD3 Actor-Critic) -> State, Behavioral Goal, Future Goal inputs -> Replay buffer with dual-goal relabeling

- **Critical path:**
  1. Agent acts using π(s, bg, fg)
  2. On reaching bg, the system queries the Planner to update bg ← bg_next and fg ← fg_new
  3. During training, Dual Relabeling samples a transition, selects two future achieved states ag_{k1}, ag_{k2}, and updates the goal inputs and rewards accordingly

- **Design tradeoffs:**
  - M2g vs. Mgseq: Choose M2g for stability and speed in complex local navigation (shorter horizon). Choose Mgseq if the path requires strict global consistency (though this risks unstable learning)
  - Planner dependency: The system requires a fixed, expert planner during training; a learned/evolving planner would break the Markov assumption used in the MDP formulation

- **Failure signatures:**
  - Mgseq stalling: Success rate plateaus or learns slowly; value function visualizations show rewards failing to propagate beyond 2-3 steps
  - M2g local optima: Agent successfully weaves through early goals but gets stuck in a dead-end that was not visible with a 2-goal horizon
  - Relabeling noise: High variance in learning curves if the relabeling strategy samples invalid goal pairs

- **First 3 experiments:**
  1. Maze Navigation (Dubins/Hallway): Validate that the agent turns to face the second goal before reaching the first goal (checking state compatibility)
  2. GC-Cartpole: Test time-to-goal performance. Verify that M2g does not slow down unnecessarily at intermediate goals (unlike the "stop-at-every-goal" behavior of myopic agents)
  3. Ablation on Relabeling: Disable the relabeling of the second goal (fg) to confirm that dual conditioning without dual relabeling degrades performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance and stability of the proposed M2g and Mgseq formulations degrade when using a non-stationary, learned high-level planner instead of the fixed expert planner used in this study?
- **Basis in paper:** [explicit] The Conclusion identifies the use of a fixed planner as a "main limitation," noting that because goal transitions are integrated into the MDP, an evolving planner would change the MDP dynamics and make learning more challenging.
- **Why unresolved:** The current methodology relies on a pre-computed graph of intermediate goals, decoupling high-level plan stability from low-level policy training. It is unknown if the low-level agents can adapt to a shifting goal distribution or non-stationary transition dynamics.
- **What evidence would resolve it:** Experiments integrating the proposed MDPs into a full HRL loop where the planner is updated concurrently, measuring the success rate and sample efficiency against the current fixed-planner baselines.

### Open Question 2
- **Question:** Does the M2g formulation fail in environments where optimal behavior requires satisfying state constraints (compatibility) for goals beyond the immediate next one?
- **Basis in paper:** [explicit] The General Discussion states that M2g "benefits from a less global information" and that its performance "may degrade in more complex environments" where achieving the current goal in a configuration compatible with the next goal is insufficient for future steps.
- **Why unresolved:** The evaluated tasks (navigation, pole-balancing) appear to be solvable with a horizon of two goals. It is unclear if the "short-sightedness" of M2g introduces a ceiling on task complexity that Mgseq might theoretically overcome.
- **What evidence would resolve it:** Evaluation in "long-horizon" environments specifically designed to require specific configurations three or more steps ahead (e.g., manipulation tasks where joint angles must be set early to allow reaching much later).

### Open Question 3
- **Question:** Can the value propagation issues observed in the Mgseq formulation be mitigated by increasing network capacity or altering the critic architecture to better handle high input diversity?
- **Basis in paper:** [inferred] The authors note in the General Discussion that Mgseq struggles because it forces the agent to "generalize over a wider set of inputs" (current + any final goal) compared to M2g (current + next goal), leading to suboptimal value estimates.
- **Why unresolved:** The paper attributes the performance gap to the fundamental formulation (horizon length/diversity), but it is possible that the neural network approximator simply lacked the capacity to model the larger goal space effectively.
- **What evidence would resolve it:** An ablation study scaling the critic network size for Mgseq specifically, or analyzing the TD-error distribution to see if the network is failing to fit the value function due to capacity constraints.

## Limitations

- Relies on a fixed, handcrafted expert planner rather than learning the high-level plan concurrently
- Only tested on three environments with relatively simple sequential structures
- Assumes two-goal lookahead is sufficient, without exploring alternative horizons
- The dual-relabeling mechanism may fail in environments where achieved goals rarely form valid sequential transitions

## Confidence

- **High confidence**: The empirical superiority of M2g over Mgseq in sample efficiency and stability (supported by consistent results across all three environments)
- **Medium confidence**: The mechanism explanations for why M2g outperforms Mgseq (based on theoretical arguments and ablation studies, but lacking direct causal evidence)
- **Low confidence**: The generality of the two-goal lookahead assumption across different environment types and planning complexities

## Next Checks

1. Test the method with a learned planner instead of a handcrafted expert to assess robustness to planner uncertainty
2. Evaluate performance with different lookahead horizons (1, 3, 4 goals) to determine if two goals is optimal
3. Apply the method to a non-navigation task where sequential goal compatibility involves different state dimensions (e.g., object manipulation requiring specific grasp configurations)