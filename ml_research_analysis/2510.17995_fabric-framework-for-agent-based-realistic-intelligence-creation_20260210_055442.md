---
ver: rpa2
title: 'FABRIC: Framework for Agent-Based Realistic Intelligence Creation'
arxiv_id: '2510.17995'
source_url: https://arxiv.org/abs/2510.17995
tags:
- approver
- request
- data
- user
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FABRIC, a framework for generating synthetic
  agentic data using only large language models. The approach addresses the challenge
  of acquiring structured, executable, and validated tool-use data by introducing
  four modular pipelines that span from full task trajectories to atomic function
  calls.
---

# FABRIC: Framework for Agent-Based Realistic Intelligence Creation

## Quick Facts
- arXiv ID: 2510.17995
- Source URL: https://arxiv.org/abs/2510.17995
- Reference count: 40
- Generates synthetic agentic data using only LLMs without human supervision

## Executive Summary
FABRIC presents a modular framework for generating synthetic agentic data using only large language models, addressing the challenge of acquiring structured, executable tool-use data at scale. The framework introduces four specialized pipelines spanning from full task trajectories to atomic function calls, employing DAG-based planning, multi-step instruction synthesis, and schema-aligned validation. Key innovations include DAG templates for structural scaffolding, judge-based quality filtering, and strict JSON schema validation to ensure machine-parseability without human-in-the-loop supervision.

## Method Summary
The framework chains eight LLM calls to generate complete agentic records: starting with enterprise domain labels and use cases, it produces function definitions, pseudo-code workflows, function descriptions, user scenarios, and final agent execution traces in JSON format. Each pipeline stage validates outputs against JSON schemas before proceeding, with judge-based filtering removing low-quality samples. The approach enables flexible generation of BFCL-style function-calling benchmarks, multi-turn dialogues, and SFT-ready serialized datasets using only LLM inference.

## Key Results
- Framework produces high-fidelity synthetic datasets supporting training and evaluation of tool-augmented agents
- Modular design enables flexible generation from multi-turn dialogues to atomic function-calling benchmarks
- Schema-aligned validation ensures outputs conform to strict syntactic and semantic constraints

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DAG-anchored planning provides structural scaffolding for coherent tool-call sequencing, reducing hallucination of invalid dependencies
- **Mechanism:** The framework enumerates DAG templates (linear chain, fan-out/fan-in, diamond, loop-like) that explicitly define which tool outputs feed into subsequent tool inputs. Each node is instantiated with ground-truth function signatures, constraining generation space to executable workflows
- **Core assumption:** LLMs generate more coherent multi-step plans when constrained by explicit dependency graphs than when reasoning without structural scaffolding
- **Evidence anchors:** Abstract mentions "DAG-based agentic planning"; section 3.2 describes DAG-Level Decomposition with atomic supervision units
- **Break condition:** If tool dependencies are genuinely unknown at generation time or require runtime discovery, pre-specified DAGs may over-constrain and exclude valid solution paths

### Mechanism 2
- **Claim:** Schema-constrained generation with multi-stage validation produces machine-parseable outputs without human verification
- **Mechanism:** Each pipeline stage emits outputs in declared formats (Python lists, JSON objects, executable pseudocode) that are programmatically validated via JSON-schema checks, type verification, and positional alignment between inputs/outputs
- **Core assumption:** LLMs can reliably produce syntactically valid structured outputs when prompted with explicit format constraints and examples
- **Evidence anchors:** Abstract states outputs "conform to strict syntactic and semantic constraints"; section 3.1.1 specifies output schemas including JSON structures
- **Break condition:** If edge cases require outputs outside predefined schemas, rigid validation may filter legitimate samples

### Mechanism 3
- **Claim:** Judge-based filtering with multi-criteria scoring improves data quality by removing underspecified or misaligned samples
- **Mechanism:** An LLM-as-judge evaluates each generated (utterance, tool call) pair on instruction clarity, argument sufficiency, grounding quality, and naturalness. Samples scoring below threshold are rejected
- **Core assumption:** LLM judges can reliably assess alignment between user intent and tool-call correctness using rubric-based scoring
- **Evidence anchors:** Abstract references "judge-based filtering"; section 3.2 Stage 6 describes judge evaluation scoring on multiple criteria
- **Break condition:** If judge rubrics don't capture domain-specific correctness criteria, filtering may pass invalid samples or reject valid ones

## Foundational Learning

- **Concept: Directed Acyclic Graphs (DAGs) for workflow modeling**
  - **Why needed here:** The framework uses DAG templates to represent tool dependencies; understanding topological ordering, parallel vs. sequential execution, and fan-out/fan-in patterns is essential for designing valid agentic workflows
  - **Quick check question:** Given tools A, B, C where A produces input for both B and C, and B's output is needed by C, can you draw the DAG and identify which steps can execute in parallel?

- **Concept: JSON Schema validation and constrained decoding**
  - **Why needed here:** Each pipeline outputs structured data validated against schemas; knowing how to define required fields, type constraints, and nested object structures is necessary for debugging validation failures and extending schemas
  - **Quick check question:** Write a JSON schema requiring a `tool_calls` array where each item has a `name` (string) and `arguments` (object with at least one required key)

- **Concept: LLM-as-judge evaluation paradigms**
  - **Why needed here:** Quality filtering relies on judge-based scoring; understanding rubric design, score calibration, and judge prompt engineering helps diagnose filtering quality and adjust thresholds
  - **Quick check question:** If a judge consistently scores "indirect" argument styles lower than "direct" styles regardless of actual quality, what prompt modification might address this bias?

## Architecture Onboarding

- **Component map:**
  RecordSynth (Pipeline 1) → DAGFirstGeneration (Pipeline 2) ←→ MultiTurnDialogueSynth (Pipeline 3) → AgenticRecordRollout (Pipeline 4)

- **Critical path:** For BFCL-style function-calling data: RecordSynth → DAGFirstGeneration → Rollout. For multi-turn training: RecordSynth → MultiTurnDialogueSynth → Rollout. Each path requires successful validation at every stage.

- **Design tradeoffs:**
  - **Modularity vs. coherence:** Pipelines can run independently but may produce inconsistent tool definitions if seeded differently; shared `functions_list` from RecordSynth mitigates this
  - **Strict validation vs. yield:** Aggressive schema/judge filtering improves quality but reduces data yield; thresholds must be tuned per use case
  - **Mock responses vs. real execution:** MultiTurnDialogueSynth uses mocked tool outputs which increases scalability but may miss real API failure modes

- **Failure signatures:**
  - `json.loads()` failure → malformed JSON output; check prompt formatting and constrained generation settings
  - Judge rejection rate >80% → utterance generation prompts may be underspecified or judge rubric is too strict
  - DAG executability validation failure → user utterance missing required inputs; utterance generation prompt needs enrichment
  - Parallel step misclassification → `is_parallel` flag logic errors in Agentic Execution step generation

- **First 3 experiments:**
  1. **End-to-end single-domain test:** Run RecordSynth with a known domain, validate all 8 stages produce parseable outputs, and trace where validation failures occur
  2. **DAG decomposition validation:** Feed RecordSynth output to DAGFirstGeneration, manually verify that atomic triples correctly isolate single-node tool invocations and that judge scoring correlates with manual quality assessment on 50 samples
  3. **Judge threshold calibration:** Generate 200 samples across direct/indirect argument styles, run judge evaluation at multiple thresholds, and measure precision/recall against human-labeled ground truth to identify optimal filtering threshold

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the generated synthetic data induce model collapse or reasoning degradation when used iteratively for self-training? The paper warns of "Degeneration through self-distillation" but presents no empirical data on performance stability across multiple generations.
- **Open Question 2:** To what extent do schema validation and judge-based filtering guarantee that synthetic records are as effective as human-collected data for training? The framework asserts strict constraints but does not provide comparative study proving automated checks create data of equal utility to human-annotated gold standards.
- **Open Question 3:** How do errors in the initial LLM-generated DAG structures or pseudocode propagate to affect the fidelity of final agent execution traces? The framework relies on LLM's internal consistency without external verification of logical soundness of generated plans.

## Limitations
- LLM-as-judge reliability lacks empirical validation for rubric effectiveness across domains
- Scalability bottlenecks likely due to 8-stage pipeline with strict validation and judge filtering; no yield statistics reported
- Schema generalizability limited to fixed JSON schemas; flexibility for novel tool signatures untested

## Confidence

- **High confidence:** Modular pipeline architecture and explicit DAG-based planning mechanism are well-specified and technically sound. Validation approach using JSON schemas and structured output constraints is standard practice.
- **Medium confidence:** Judge-based filtering mechanism is reasonable but lacks empirical validation. Effectiveness of LLM judges for this specific task remains unproven without benchmark comparison.
- **Low confidence:** Claims about data quality and scalability are not substantiated with yield rates, rejection statistics, or comparative benchmarks against human-generated data.

## Next Checks

1. **Judge rubric validation:** Generate 200 samples with varying quality levels, have human annotators score the same samples using the judge rubric, and compute inter-annotator agreement and judge-human correlation to assess rubric reliability.

2. **Pipeline yield measurement:** Run the complete RecordSynth pipeline for three different domains and measure: (a) overall valid sample yield percentage, (b) average validation failures per stage, and (c) judge rejection rate at different threshold settings.

3. **DAG executability test:** Take 50 generated DAG-first samples and attempt to execute them in a sandboxed environment with mock APIs to verify that the pseudo_code execution actually produces outputs matching the expected results, catching cases where DAG scaffolding creates invalid workflows.