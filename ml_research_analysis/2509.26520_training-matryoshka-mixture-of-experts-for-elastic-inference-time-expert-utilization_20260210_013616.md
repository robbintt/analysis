---
ver: rpa2
title: Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert Utilization
arxiv_id: '2509.26520'
source_url: https://arxiv.org/abs/2509.26520
tags:
- experts
- training
- expert
- m-moe
- matryoshka
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Matryoshka Mixture-of-Experts (M-MoE) is proposed to address the
  brittleness of standard fixed-k MoE models during inference-time expert count changes.
  M-MoE introduces a coarse-to-fine structure by varying the number of activated experts
  during training, compelling the model to learn a meaningful ranking of experts.
---

# Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert Utilization

## Quick Facts
- arXiv ID: 2509.26520
- Source URL: https://arxiv.org/abs/2509.26520
- Reference count: 40
- Key outcome: M-MoE achieves multi-expert-count performance with single-model