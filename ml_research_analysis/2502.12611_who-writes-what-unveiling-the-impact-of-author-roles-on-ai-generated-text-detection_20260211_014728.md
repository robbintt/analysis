---
ver: rpa2
title: 'Who Writes What: Unveiling the Impact of Author Roles on AI-generated Text
  Detection'
arxiv_id: '2502.12611'
source_url: https://arxiv.org/abs/2502.12611
tags:
- b-instruct0
- qwen2
- significant
- text
- cefr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how author attributes\u2014including gender,\
  \ CEFR proficiency, academic field, and language environment\u2014affect AI-generated\
  \ text detection. Using the ICNALE corpus and texts from 12 modern LLMs, we conducted\
  \ a rigorous evaluation with multi-factor ANOVA and weighted least squares to assess\
  \ bias across eight state-of-the-art detectors."
---

# Who Writes What: Unveiling the Impact of Author Roles on AI-generated Text Detection

## Quick Facts
- **arXiv ID:** 2502.12611
- **Source URL:** https://arxiv.org/abs/2502.12611
- **Reference count:** 40
- **Primary result:** CEFR proficiency and language environment consistently impact AI-generated text detection accuracy; gender and academic field effects are detector-dependent.

## Executive Summary
This study systematically evaluates how author attributes—gender, CEFR proficiency, academic field, and language environment—affect the accuracy of AI-generated text detectors. Using the ICNALE corpus and 12 modern LLMs to generate 66,794 texts, the authors assess eight state-of-the-art detectors with multi-factor ANOVA and weighted least squares regression. Results show consistent detection biases for non-native speakers, with CEFR proficiency and language environment producing the most robust effects. The findings highlight significant fairness issues in current detectors and call for more inclusive benchmarks and training data.

## Method Summary
The authors combine 5,138 human essays from the ICNALE corpus with AI-generated texts from 12 LLMs (61,656 samples) to create a dataset of 66,794 texts with persona metadata. Eight off-the-shelf detectors are applied to all texts, with thresholds calibrated to achieve 5% false positive rate on human texts. Detection accuracy is aggregated by author attribute combinations, and weighted least squares regression with Type II ANOVA quantifies the significance of each factor. Post-hoc LSMeans tests with Holm correction identify pairwise differences for significant factors.

## Key Results
- CEFR proficiency and language environment consistently affect detection accuracy across most detectors.
- Gender and academic field effects are detector-dependent and often confounded by other factors.
- Non-native speakers and lower-proficiency writers face higher false positive rates due to distributional mismatch with detector training data.
- Language environment (EFL vs. ESL vs. NS) produces distinct detection accuracy patterns, likely due to differing lexical and syntactic norms.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** AI text detectors systematically misclassify non-native speakers at higher rates due to distributional mismatch between detector training data and learner corpora.
- **Mechanism:** Detectors trained primarily on native-speaker or high-proficiency corpora treat non-native linguistic patterns (e.g., unidiomatic expressions, simplified syntax) as anomalous, leading to inflated false positive rates for EFL/ESL writers. The detector's decision boundary implicitly encodes native-like fluency as a proxy for human authorship.
- **Core assumption:** Detector training corpora underrepresent learner English and overrepresent polished, native-level prose.
- **Evidence anchors:**
  - [abstract]: "CEFR proficiency and language environment consistently affected detection accuracy."
  - [section 1]: "non-native speakers or lower-proficiency writers might be wrongly flagged as AI-generated due to errors or unidiomatic expressions that differ from native-level training data."
  - [corpus]: Weak—no corpus neighbor directly studies detector training composition; related authorship analysis review (arXiv:2505.15422) notes distributional challenges but not specific to detection bias.
- **Break condition:** If detectors are retrained or fine-tuned on balanced learner corpora and CEFR effects disappear, the mechanism is confounded by training data rather than inherent detector architecture.

### Mechanism 2
- **Claim:** Language environment (EFL vs. ESL vs. NS) produces distinct lexical and syntactic patterns that detectors weight unevenly.
- **Mechanism:** EFL learners (e.g., China, Japan, Korea) often exhibit formulaic, textbook-influenced writing that may resemble AI outputs in regularity, while ESL learners (e.g., Philippines, Singapore) incorporate more idiomatic variety. Detectors relying on perplexity or token-probability metrics may assign lower anomaly scores to EFL patterns, causing differential accuracy.
- **Core assumption:** Perplexity-based detectors correlate AI-likeness with statistical regularity; regularity varies by language environment.
- **Evidence anchors:**
  - [section 5.3]: "Language environment is significant for most detectors... for 'binoculars', all pairwise comparisons (EFL vs. ESL, EFL vs. NS, ESL vs. NS) are significant, with EFL and ESL generally showing higher accuracy than NS."
  - [section 3.2]: Draws on World Englishes theory (Kachru, Jenkins) to justify environment categories.
  - [corpus]: DNA-DetectLLM (arXiv:2509.15550) discusses mutation-repair detection paradigms but does not address language environment bias—evidence is weak.
- **Break condition:** If perplexity-independent detectors (e.g., GLTR binning) show no language environment effect while probability-based ones do, the mechanism isolates to probability metrics.

### Mechanism 3
- **Claim:** Gender effects on detection are detector-dependent rather than universal, suggesting architectural or training-specific biases rather than intrinsic linguistic differences.
- **Mechanism:** Some detectors (e.g., classifier-based RoBERTa) may have training data skewed by gendered writing samples, leading to differential calibration. However, multi-factor ANOVA shows gender is non-significant after controlling for CEFR, academic field, and environment, implying apparent gender effects are confounded.
- **Core assumption:** Observed gender differences in single-factor analyses reflect omitted variable bias rather than true gender-based detection differences.
- **Evidence anchors:**
  - [abstract]: "gender and academic field effects were detector-dependent."
  - [section 5.3, Table 9]: Sex p-values >0.05 for all detectors in multi-factor WLS; single-factor t-tests show significance for some detectors but these are confounded.
  - [corpus]: AuthorMist (arXiv:2503.08716) addresses detector evasion, not demographic bias—no direct corpus support.
- **Break condition:** If randomized controlled experiments with balanced gender samples still show detector-specific effects, confounding is ruled out and architectural bias is implicated.

## Foundational Learning

- **Concept: CEFR Proficiency Levels (A2–C2)**
  - Why needed here: The paper uses CEFR as the primary proficiency axis; understanding this scale is essential to interpret why B1 vs. B2 vs. A2 subgroups show different detection accuracies.
  - Quick check question: Would a B1_2 learner typically produce more syntactically complex sentences than an A2_0 learner? (Yes—B1_2 corresponds to upper-intermediate, A2_0 to elementary.)

- **Concept: Weighted Least Squares (WLS) Regression**
  - Why needed here: The bias analysis relies on multi-factor WLS to handle unbalanced subgroup sizes; understanding weighting by group size (wi) is critical to reading Table 7 LSMeans.
  - Quick check question: If Group A has 1000 samples and Group B has 100, how does WLS weight their contributions compared to OLS? (WLS weights each group's squared error by sample count; OLS weights equally per observation, not per group.)

- **Concept: Type II ANOVA**
  - Why needed here: The paper uses Type II sums of squares to assess each factor's unique contribution after accounting for others; this differs from Type I (sequential) and Type III (marginal).
  - Quick check question: In an unbalanced design with correlated predictors, which ANOVA type tests each factor's effect after all other factors? (Type II.)

## Architecture Onboarding

- **Component map:** ICNALE corpus (5,138 human essays) + 12 LLM generators (Qwen2.5, LLaMA3.x, Mistral) producing 61,656 AI texts; total 66,794 samples with persona metadata (sex, CEFR, academic genre, language environment) -> 8 off-the-shelf detectors (classifier-based: RoBERTa-Base/Large GPT2, RoBERTa ChatGPT, RADAR; metric-based: GLTR, Binoculars, Fast DetectGPT, DetectGPT, LLMDet) -> FPR calibration at 5% threshold; accuracy computed per subgroup; WLS regression + Type II ANOVA for bias quantification -> Post-hoc LSMeans pairwise comparisons with Holm correction; bootstrap sensitivity for parameter stability.

- **Critical path:**
  1. Acquire ICNALE essays and extract metadata.
  2. Generate parallel AI texts using system prompts encoding persona attributes.
  3. Run all 8 detectors on all texts, calibrate thresholds to 5% FPR.
  4. Aggregate accuracy by attribute combinations; fit WLS model.
  5. Perform Type II ANOVA; extract significant factors.
  6. Conduct post-hoc LSMeans tests for significant factors only.
  7. Bootstrap coefficients to validate robustness.

- **Design tradeoffs:**
  - **Off-the-shelf vs. fine-tuned detectors:** Using pre-trained detectors maximizes real-world generalization assessment but limits domain adaptation; fine-tuning on ICNALE would improve accuracy but obscure out-of-domain performance gaps.
  - **FPR vs. accuracy thresholding:** Fixing FPR at 5% prioritizes not penalizing human authors (critical for fairness), but may reduce overall accuracy if detector scores are poorly calibrated.
  - **Multi-factor WLS vs. single-factor tests:** WLS controls confounders but requires sufficient sample sizes per attribute combination; single-factor tests are simpler but yield confounded results.

- **Failure signatures:**
  - **Collapsed subgroup variance:** If a CEFR level has very few samples (e.g., XX_0 with 73 NS), LSMeans may be unstable—check bootstrap CI width.
  - **Detector failure on specific models:** If a detector achieves ~50% accuracy on all Qwen models, it may be overfitted to GPT-family distributions—inspect Tables 15–20 for patterns.
  - **Non-significant ANOVA despite visible gaps:** If visual inspection shows accuracy differences but ANOVA is non-significant, check subgroup sample sizes and weighting; small groups may be underpowered.

- **First 3 experiments:**
  1. **Reproduce FPR calibration:** Re-run Binoculars and FastDetectGPT on the full corpus, calibrate thresholds, and verify reported FPRs in Table 5 match within ±0.5%.
  2. **Single-attribute ablation:** Fit single-factor WLS models for each detector (CEFR only, language environment only, etc.) and compare p-values to multi-factor results to quantify confounding magnitude.
  3. **Bootstrap stability check:** For one detector (e.g., gpt2-base), bootstrap WLS coefficients 1000 times; verify all original estimates fall within 95% CIs per Tables 37–46; if not, investigate outlier subgroups.

## Open Questions the Paper Calls Out

- **Question:** Can AI text detectors be effectively debiased with respect to author attributes like CEFR proficiency and language environment while maintaining detection accuracy?
  - **Basis in paper:** [explicit] The abstract and conclusion state the work "paves the way for future research on bias mitigation" and underscores "the need for socially aware benchmarks, debiasing strategies, and more inclusive training data."
  - **Why unresolved:** This study quantifies biases but does not propose or evaluate specific debiasing interventions.
  - **What evidence would resolve it:** Experiments showing that modified training protocols or adversarial debiasing techniques reduce accuracy gaps between subgroups (e.g., native vs. non-native speakers) without substantial drops in overall detection performance.

- **Question:** Do the observed biases generalize to non-Asian English learner populations, other languages, and commercial or proprietary detectors?
  - **Basis in paper:** [explicit] The Limitations section states: "our evaluation is primarily based on open-source detectors and the ICNALE corpus, which predominantly comprises texts from Asian English learners. Future work should extend this analysis to additional systems and more diverse datasets to further validate the generalizability of our findings."
  - **Why unresolved:** The demographic and linguistic scope is narrow; results may not apply to other contexts.
  - **What evidence would resolve it:** Replication studies using corpora from diverse geographic regions, additional languages, and both open-source and commercial detectors showing consistent bias patterns (or lack thereof).

- **Question:** Which specific linguistic features drive the differential detection accuracy across CEFR proficiency levels and language environments?
  - **Basis in paper:** [inferred] The paper establishes that CEFR and language environment consistently affect detection accuracy, and mentions that sociolinguistic research shows "differences manifest in vocabulary choices, syntactic complexity, and rhetorical conventions," but does not isolate which features cause the bias.
  - **Why unresolved:** The statistical framework measures bias existence and magnitude but does not explain the underlying linguistic mechanisms.
  - **What evidence would resolve it:** Feature attribution analyses (e.g., SHAP values on detector outputs) or controlled experiments manipulating specific linguistic features to identify which drive differential detection rates across proficiency levels.

## Limitations
- The ICNALE corpus provides broad geographic coverage but limited text lengths (250-500 words) and predefined persona categories, constraining generalizability to longer-form or domain-specific writing.
- The use of off-the-shelf detectors without fine-tuning ensures real-world relevance but also means that observed biases reflect both detector architecture and training data composition, which are not separately disentangled.
- Post-hoc pairwise comparisons involve many statistical tests (n=96 per detector), increasing the risk of false positives despite Holm correction.

## Confidence
- **High Confidence:** CEFR proficiency and language environment effects on detection accuracy (p < 0.001 in multi-factor WLS for most detectors).
- **Medium Confidence:** Detector-specific gender and academic field effects (p < 0.05 in single-factor tests but non-significant in multi-factor WLS, suggesting confounding).
- **Low Confidence:** Extrapolation to other writing domains or detector types (e.g., fine-tuned models, code generation) not tested.

## Next Checks
1. **Prompt Stability:** Re-run LLM generation with varying temperature (0.1, 0.7, 1.0) for a subset of essays to assess whether detected bias patterns persist across stochastic variations.
2. **Detector Architecture Isolation:** Repeat analysis using only perplexity-independent detectors (e.g., GLTR) versus probability-based ones to test whether language environment effects are tied to specific scoring mechanisms.
3. **Training Corpus Audit:** For one or two detectors (e.g., RoBERTa-Base, Binoculars), attempt to identify the presence/absence of learner corpora in their training data and correlate with bias magnitude.