---
ver: rpa2
title: 'rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy
  Injection'
arxiv_id: '2512.08300'
source_url: https://arxiv.org/abs/2512.08300
tags:
- planner
- reasoning
- qwen2
- strategies
- strategy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enhancing reasoning capabilities
  in large language models (LLMs), particularly small ones, by introducing a reinforced
  strategy injection mechanism (rSIM). The core idea is to employ a small planner
  LLM to guide the reasoning process of another LLM by adaptively injecting predefined
  reasoning strategies (e.g., self-reflection, decomposition) at each reasoning step.
---

# rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection

## Quick Facts
- arXiv ID: 2512.08300
- Source URL: https://arxiv.org/abs/2512.08300
- Authors: Sijia Chen; Baochun Li; Di Niu
- Reference count: 40
- One-line primary result: rSIM enables small LLMs (e.g., Qwen2.5-0.5B) to achieve reasoning performance on par with much larger models (e.g., Qwen2.5-14B) on tasks like MATH.

## Executive Summary
This paper addresses the challenge of enhancing reasoning capabilities in small language models by introducing a reinforced strategy injection mechanism (rSIM). The core innovation is decoupling strategy selection from reasoning execution: a small "planner" LLM selects from a discrete set of human-defined reasoning strategies (e.g., self-reflection, decomposition), which are then injected into the prompt of a "reasoner" LLM at each reasoning step. Both agents are jointly trained as a multi-agent reinforcement learning system using a leader-follower framework. Results demonstrate that rSIM enables small models to achieve reasoning performance comparable to much larger models, and the trained planner can be used as a pluggable component to improve other LLMs without further training.

## Method Summary
rSIM employs a two-agent system where a planner LLM selects reasoning strategies and a reasoner LLM executes the reasoning steps. The planner has a custom action head outputting probabilities over 9 discrete strategies. During inference, the planner selects a strategy, which is injected into the reasoner's prompt. The two agents are trained jointly via multi-agent reinforcement learning using a two-stage leader-follower optimization: first prioritizing the planner's learning (λ=0.7), then focusing on the reasoner's ability to follow strategies (λ=0.3). The reasoner is pre-finetuned on 1,000 incorrectly-solved MATH samples formatted with step-by-step separation. Training uses group-relative advantage estimation with reward functions combining accuracy, format compliance, and strategy-following metrics.

## Key Results
- Small Qwen2.5-0.5B with rSIM achieves 51.6% accuracy on MATH, comparable to Qwen2.5-14B at 51.2%
- The trained planner can be used as a plug-in to improve reasoning performance of other LLMs without additional training
- rSIM shows continual learning capability, with planners able to be fine-tuned on new tasks while maintaining performance on previous ones
- GRPO training fails to improve small models (reward collapse to zero), while rSIM succeeds by injecting strategies

## Why This Works (Mechanism)

### Mechanism 1: Externalizing Reasoning Strategy Selection via a Planner Agent
The primary bottleneck for small LLMs is their inability to select appropriate reasoning strategies, not their ability to execute steps given a strategy. By decoupling strategy selection from reasoning execution, a small planner can guide a reasoner's CoT process through adaptive injection of human-defined strategies.

### Mechanism 2: Two-Stage Leader-Follower Policy Optimization
Alternating optimization focus between planner and reasoner stabilizes training in the multi-agent setting. The two-stage approach (first λ=0.7 for planner, then λ=0.3 for reasoner) prevents conflicting policy updates and ensures both agents learn effectively.

### Mechanism 3: Pluggable, Continually Trained Strategy Policy
The planner's learned policy for injecting strategies generalizes across different LLMs and tasks without retraining the base model. This pluggable architecture enables continual learning, where the planner can be fine-tuned on new tasks while maintaining capabilities on previous ones.

## Foundational Learning

- **Multi-Agent Reinforcement Learning (MARL) - Leader-Follower**: Why needed: This is the core training paradigm where the planner (leader) commits to a strategy and the reasoner (follower) responds. Quick check: In a leader-follower game, does the leader observe the follower's action before choosing its own?
- **Generalized Advantage Estimation (GAE)**: Why needed: The training objective relies on GAE to balance bias and variance when estimating the value of actions (strategy selections). Quick check: What is the primary role of the λ parameter in GAE?
- **Chain-of-Thought (CoT) Reasoning**: Why needed: The entire rSIM framework operates by injecting strategies into the CoT process, with the step-wise structure allowing the planner to intervene at discrete points. Quick check: What is the primary benefit of breaking down a problem into a chain of thoughts?

## Architecture Onboarding

- **Component map**: Planner Agent -> Interactive Sampling Loop -> Reasoner Agent -> Final Answer
- **Critical path**:
  1. Pre-train reasoner on 1,000 incorrectly-solved MATH samples with `\n\n` step separation
  2. Stage 1 Training (Planner Focus): Set λ=0.7, train on MATH, planner learns strategy-reward correlations
  3. Stage 2 Training (Reasoner Focus): Set λ=0.3, continue training, reasoner learns to follow injected strategies
  4. Deployment/Plugin: Freeze trained planner, use with any compatible LLM via interactive sampling loop
- **Design tradeoffs**: Fixed human-defined strategy set limits exploration but ensures interpretability; joint training yields tighter coupling but is computationally expensive; explicit strategy injection increases token overhead
- **Failure signatures**: Reward collapse (drops to zero early), strategy imbalance (disproportionate favoring of one strategy), plugin incompatibility (planner fails with different model families)
- **First 3 experiments**:
  1. Sanity Check - Single Agent: Train base LLM with GRPO on MATH, verify failure mode
  2. Core Experiment - Joint rSIM: Train planner and reasoner jointly using two-stage scheme on MATH, compare against GRPO baseline
  3. Ablation - Plugin Transfer: Use planner trained on one model to guide different, untrained LLM on different dataset

## Open Questions the Paper Calls Out

### Open Question 1
How can the planner's discrete, human-defined action space be replaced or augmented with a dynamically expandable or continuous space to accommodate strategies not present in the initial expert priors? The authors currently lack a mechanism to dynamically update or expand the planner's action space, which limits generalizability.

### Open Question 2
To what extent does the planner's observed bias toward specific strategies (e.g., over-reliance on Self-Reflection) limit its ability to solve tasks that require distinct reasoning approaches like Verification or Prioritization? The reward mechanism may be insufficient to enforce strategy diversity across different domains.

### Open Question 3
Can the textual descriptions of reasoning strategies be optimized automatically (e.g., via gradient descent or evolutionary algorithms) rather than relying on static, human-crafted prompts? Misleading, ambiguous, or overly narrow prompts can harm the reasoning process.

## Limitations
- The pluggable planner mechanism's generalizability is uncertain beyond the Qwen model family and MATH-like tasks
- The discrete strategy set (9 options) may constrain reasoning flexibility for highly complex or domain-specific problems
- The two-stage training procedure's sensitivity to hyperparameter choices (particularly λ values) is not fully explored

## Confidence

| Claim Cluster | Confidence |
|---|---|
| Core rSIM Framework Performance Claims | High |
| Pluggable Planner Generalization Claims | Medium |
| Two-Stage Training Necessity Claims | Medium |

## Next Checks

1. Cross-Architecture Transfer Test: Deploy a planner trained on Qwen2.5-0.5B to guide Llama-3.1-8B or Mistral-7B on MATH and GSM8K to validate true pluggability beyond model family

2. Strategy Space Expansion Analysis: Systematically evaluate performance when increasing the strategy set from 9 to 15-20 options, measuring both accuracy gains and planner strategy diversity

3. Continuous Training Stability: Implement a curriculum where the planner is first trained on MATH, then continuously trained on GSM8K, then CodeAlpaca-20k, monitoring for catastrophic forgetting and measuring relative performance gains compared to separate training runs