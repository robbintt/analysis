---
ver: rpa2
title: 'Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency
  Through Vocabulary Adaptation'
arxiv_id: '2504.17025'
source_url: https://arxiv.org/abs/2504.17025
tags:
- training
- language
- vocabulary
- italian
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the adaptation of large language models
  (LLMs) to the Italian language, addressing the inefficiencies of using English-centric
  models on non-English languages. The primary challenge lies in the mismatch between
  the tokenizer and vocabulary designed for English and the linguistic characteristics
  of Italian, leading to higher token "fertility" (average number of tokens per word)
  and slower inference speeds.
---

# Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation

## Quick Facts
- arXiv ID: 2504.17025
- Source URL: https://arxiv.org/abs/2504.17025
- Reference count: 23
- Primary result: Vocabulary adaptation reduces token fertility by 25% for Mistral-7B-v0.1 and 16% for Llama-3.1-8B while maintaining performance

## Executive Summary
This work addresses the inefficiency of using English-centric LLMs for Italian by replacing their tokenizers and vocabularies with Italian-optimized alternatives. The authors introduce SAVA (Semantic Alignment Vocabulary Adaptation), a neural mapping approach that leverages a helper model's embeddings to initialize new vocabulary tokens. Through continual training on mixed language data, the adapted models achieve significant token fertility reduction (25% for Mistral, 16% for Llama) while maintaining competitive performance on Italian benchmarks. The study demonstrates that vocabulary adaptation enables efficient language-specific tokenization without sacrificing model quality.

## Method Summary
The method replaces the source LLM's tokenizer and vocabulary with an Italian-optimized alternative (Minerva tokenizer) while preserving the model architecture. Four initialization strategies are compared for new vocabulary tokens: Random, Frequency-based Average (FVT), Cosine-based Local Proximity (CLP), and the proposed SAVA approach. SAVA trains a linear mapping between helper and source embedding spaces on the intersection vocabulary, then uses this mapping to initialize out-of-vocabulary tokens. The adapted models undergo continual training on 75% Italian / 25% English data for 2-12 billion tokens, recovering performance lost during vocabulary substitution.

## Key Results
- Token fertility reduced by 25% for Mistral-7B-v0.1 and 16% for Llama-3.1-8B
- Llama-3.1-8B vocabulary reduced from 128k to 32k, achieving 10% parameter reduction
- SAVA achieves comparable performance to source models on Italian benchmarks with fewer computational resources
- SAVA shows fastest convergence, reaching in 400 batches what Random achieves in 2000 batches

## Why This Works (Mechanism)

### Mechanism 1: Token Fertility Reduction via Language-Specific Vocabulary
Replacing English-centric tokenizers with Italian-optimized alternatives reduces fragmentation of Italian words into subword units. The Minerva tokenizer provides better morphological coverage of Italian, resulting in fewer tokens per word and more efficient inference.

### Mechanism 2: Semantic Alignment Vocabulary Adaptation (SAVA)
SAVA trains a linear mapping between helper and source embedding spaces on shared vocabulary tokens. This learned transformation preserves semantic relationships while anchoring to the source model's representation geometry, enabling meaningful initialization of new tokens.

### Mechanism 3: Performance Recovery via Short Continual Training
After vocabulary substitution disrupts learned representations, limited continual training on mixed language data allows the model to adapt to the new tokenization scheme while retaining source language capabilities. The 75/25 target/source split balances adaptation with preservation.

## Foundational Learning

- **Token Fertility**: Average tokens per word. Critical metric for measuring tokenizer efficiency. Quick check: If "informazione" splits into 4 tokens vs 1, which has higher fertility? Higher fertility means slower inference.

- **Embedding Space Alignment**: Different models' embedding spaces can be related by linear transformations. Quick check: If two models represent "casa" with related embeddings, does a rotation + scaling require linear mapping?

- **Tied vs Untied Embeddings**: Tied weights share embedding and LM head matrices. When adapting vocabulary, both must be replaced together. Quick check: For 32k vocab and 4096 dimensions, what's parameter count for tied vs untied? What changes with 16k vocab?

## Architecture Onboarding

**Component map**:
Source LLM → Replace tokenizer with Minerva → Replace embedding matrix (intersection tokens copied, new tokens initialized via SAVA) → Replace LM head (if tied) → Transformer layers unchanged → Continual training on mixed data

**Critical path**:
1. Obtain Minerva tokenizer and helper model weights
2. Identify vocabulary intersection (~16-20k tokens)
3. Train SAVA mapping on intersection embeddings
4. Initialize new embeddings using SAVA transformation
5. Replace embedding matrix and LM head in source model
6. Run continual training on 75% target / 25% source data

**Design tradeoffs**:
| Heuristic | Pros | Cons |
|-----------|------|------|
| Random | Simplest, no dependencies | Slowest convergence |
| FVT | Fast, no helper needed | May blur semantics |
| CLP | Uses helper geometry | Similarity-based only |
| SAVA | Best alignment, fastest | Requires helper model |

| Vocabulary Size | Effect |
|----------------|--------|
| Keep original | Same parameters, fertility reduced |
| Prune (Llama: 128k→32k) | 10% parameter reduction, may lose rare tokens |

**Failure signatures**:
- Exploding loss at start: Check initialization variance matches source distribution
- Slow convergence: Verify ≥10k shared tokens in intersection
- Source language forgetting: Increase English data from 25% to 40-50%
- Poor target generation: Verify Minerva tokenizer loading correctly

**First 3 experiments**:
1. Measure fertility reduction on held-out Italian corpus comparing source vs Minerva tokenizer
2. Validate SAVA mapping by measuring reconstruction error on held-out intersection tokens
3. Run SAVA-adapted model for 400 batches and compare to Random baseline at 2000 batches

## Open Questions the Paper Calls Out
- How does SAVA scale across languages, particularly in mid- and low-resource settings where high-quality helper models are scarce?
- What is the impact of utilizing a larger helper model (e.g., 7B parameters) on vocabulary adaptation quality?
- How does reduction in token fertility specifically influence performance on generative tasks compared to multiple-choice benchmarks?

## Limitations
- Results demonstrated only for Italian, a high-resource language; generalizability to morphologically different or low-resource languages unknown
- Cross-lingual SAVA effectiveness unverified; embedding space alignment may not generalize across typologically diverse languages
- Evaluation limited to multiple-choice and generative tasks without comprehensive Italian-specific benchmarks
- Computational overhead of SAVA training and continual training not fully quantified against alternatives

## Confidence
**High Confidence**:
- Fertility reduction measurements are direct and methodologically sound
- SAVA convergence speed demonstrated through ablation studies
- Llama parameter reduction is straightforward calculation

**Medium Confidence**:
- SAVA semantic alignment shown through cosine similarity but lacks qualitative validation
- Performance parity with source models demonstrated but limited to few-shot settings

**Low Confidence**:
- Cross-lingual generalization claims are extrapolations from single-language study
- Net efficiency gains not fully quantified including all computational costs

## Next Checks
1. Conduct tokenizer ablation study using alternative Italian tokenizers to isolate Minerva's contribution to fertility reduction
2. Apply SAVA to adapt an English LLM to a typologically different language (Finnish/Turkish) to test cross-lingual generalization
3. Perform comprehensive efficiency cost-benefit analysis comparing SAVA + continual training against alternatives including wall-clock time and compute costs