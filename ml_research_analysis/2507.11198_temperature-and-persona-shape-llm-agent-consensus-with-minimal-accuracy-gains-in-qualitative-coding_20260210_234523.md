---
ver: rpa2
title: Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains
  in Qualitative Coding
arxiv_id: '2507.11198'
source_url: https://arxiv.org/abs/2507.11198
tags:
- coding
- consensus
- temperature
- agent
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluated how large language model (LLM)
  agent personas and temperature settings affect consensus-building and coding accuracy
  in qualitative coding. A multi-agent system (MAS) was designed to simulate human
  deductive coding workflows, with agents discussing and resolving coding decisions
  for 3,538 dialog segments from online math tutoring transcripts.
---

# Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding

## Quick Facts
- **arXiv ID**: 2507.11198
- **Source URL**: https://arxiv.org/abs/2507.11198
- **Reference count**: 19
- **Primary result**: Temperature and persona congruency significantly affect consensus formation in LLM multi-agent systems, but rarely improve coding accuracy compared to single agents.

## Executive Summary
This study systematically evaluated how large language model (LLM) agent personas and temperature settings affect consensus-building and coding accuracy in qualitative coding. A multi-agent system (MAS) was designed to simulate human deductive coding workflows, with agents discussing and resolving coding decisions for 3,538 dialog segments from online math tutoring transcripts. Six open-source LLMs were tested across 18 experimental conditions varying temperature (0.0, 0.5, 1.0) and personality congruency (matched vs. mismatched personas). Temperature consistently reduced consensus likelihood across all models, while incongruent personas delayed consensus in most models. Surprisingly, MAS rarely improved coding accuracy compared to single-agent coding, with only one model (OpenHermesV2:7B) showing reliable gains for a single code category under specific conditions. Qualitative analysis revealed that MAS could help refine ambiguous codes but often lacked transparency and internal consistency in reasoning. Overall, MAS did not reliably enhance qualitative coding accuracy, suggesting that LLMs may be better suited as collaborators for surfacing ambiguities and improving codebooks rather than as consensus-seeking accuracy maximizers.

## Method Summary
The study used a multi-agent LLM system to perform deductive coding of 3,538 tutoring dialog segments into 8 predefined qualitative codes. Six open-source LLMs (3B-32B parameters) were tested across 18 conditions varying temperature (0.0, 0.5, 1.0) and persona congruency (matched vs. mismatched personas). The MAS workflow involved single-agent coding (neutral persona), dual-agent discussion with assigned personas (Bold, Empathetic, or Balanced), and a consensus agent to resolve persistent disagreements. Agent outputs were extracted using regex and evaluated against human gold standard codes for accuracy and consensus formation (First, Delayed, No Consensus).

## Key Results
- Temperature significantly reduced consensus likelihood across all six LLMs (OR = 0.07 to OR = 0.59)
- MAS with incongruent personas delayed consensus in four out of six LLMs
- Single agents matched or outperformed MAS consensus in most conditions
- Only one model (OpenHermesV2:7B) showed above-chance accuracy gains from MAS deliberation for a single code category

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Higher decoding temperature reduces consensus likelihood by increasing output stochasticity.
- Mechanism: Temperature controls the randomness of token selection during generation. Higher temperature (e.g., 1.0 vs 0.0) produces more diverse outputs, which increases the probability that agents generate different codes for the same input, making consensus harder.
- Core assumption: Agent outputs are primarily driven by the underlying token probability distribution, and temperature-induced diversity directly translates to coding disagreement.
- Evidence anchors:
  - [abstract] "Temperature significantly impacted whether and when consensus was reached across all six LLMs."
  - [Section 5.1] "Higher temperatures significantly reduced the likelihood of reaching consensus in the first round (p's < .001), with odds ratios ranging from OR = 0.07 to OR = 0.59."
  - [corpus] Weak direct evidence; corpus focuses on MAS creativity and bias rather than temperature effects on consensus.
- Break condition: If temperature effects are confounded by model architecture or prompt design, the mechanism may not generalize.

### Mechanism 2
- Claim: Incongruent personas delay consensus by introducing behavioral heterogeneity without improving accuracy.
- Mechanism: Personas (bold, empathetic, neutral) influence agent communication style and decision-making tendencies. Mismatched personas (e.g., bold-empathetic) create more discussion but do not systematically lead to more accurate outcomes.
- Core assumption: Persona-induced behavioral differences are stable enough to affect group dynamics, but not substantial enough to improve task performance.
- Evidence anchors:
  - [abstract] "MAS with multiple personas significantly delayed consensus in four out of six LLMs compared to uniform personas."
  - [Section 5.1] "Incongruent personas were significantly associated with increased delayed agreement in LLaMA-3B, WizardLM-2:7B, Phi-3:14B, and Mistral-24B."
  - [corpus] Corpus papers (e.g., "Scaffolding Creativity") suggest divergent personas can enhance creativity, but this study shows no accuracy benefit in coding tasks.
- Break condition: If personas are inconsistently expressed or model-specific, the mechanism may not replicate across different LLM families.

### Mechanism 3
- Claim: Multi-agent deliberation does not reliably improve coding accuracy because same-model agents share fundamental reasoning limitations.
- Mechanism: MAS using the same base model are effectively interacting with probabilistic variants of the same system. Deliberation amplifies existing biases rather than correcting them, especially when the codebook is already well-defined.
- Core assumption: Accuracy gains require genuinely diverse reasoning capabilities, not just behavioral variation within a single model.
- Evidence anchors:
  - [abstract] "Single agents matched or outperformed MAS consensus in most conditions."
  - [Section 5.2] "Only one model (OpenHermesV2:7B) and code category showed above-chance gains from MAS deliberation."
  - [corpus] "MAS-ZERO" paper suggests manual agent design often fails to align with LLM strengths, supporting the limitation of same-model MAS.
- Break condition: If models with different architectures or training data are combined, accuracy improvements may emerge.

## Foundational Learning

- Concept: Deductive coding with codebooks
  - Why needed here: Understanding that qualitative coding can follow predefined categories (codes) is essential to grasp the experimental task and why single agents might already perform well.
  - Quick check question: Can you explain the difference between deductive and inductive coding, and why a well-defined codebook might reduce the benefit of multi-agent deliberation?

- Concept: Temperature parameter in LLMs
  - Why needed here: Temperature is a core hyperparameter that controls output randomness, and this study shows its critical role in consensus dynamics.
  - Quick check question: If you set temperature to 0.0, what behavior would you expect from an LLM agent, and how might that affect consensus-building in a multi-agent system?

- Concept: Persona prompting
  - Why needed here: Personas are used to induce behavioral diversity in agents, but this study questions their effectiveness for accuracy tasks.
  - Quick check question: How might you design a persona prompt to encourage an agent to be "bold" versus "empathetic," and what tradeoffs might this introduce?

## Architecture Onboarding

- Component map:
  - SingleAgentCoding -> DualAgentDiscussion -> ConsensusAgent -> Post-processor

- Critical path:
  1. Initialize agents with codebook, data segment, and assigned persona/temperature.
  2. Agents generate initial codes independently.
  3. If codes disagree, agents discuss and revise.
  4. If consensus not reached, ConsensusAgent arbitrates.
  5. Compare final code to human gold standard.

- Design tradeoffs:
  - Same-model vs multi-model agents: Same-model agents are computationally efficient but may lack genuine reasoning diversity.
  - Temperature setting: Low temperature (0.0-0.5) increases consensus likelihood but may reduce exploration of ambiguous cases.
  - Persona congruency: Incongruent personas add discussion overhead without clear accuracy benefits.

- Failure signatures:
  - Agents fail to output structured codes (requires reprompting or fallback).
  - Agents hallucinate data points or code categories not in the input.
  - Agents change codes without providing justification (transparency issue).
  - ConsensusAgent overrides without clear rationale.

- First 3 experiments:
  1. Run single-agent coding with temperature = 0.0 across all models to establish baseline accuracy.
  2. Run dual-agent discussion with congruent personas (e.g., bold-bold) at temperature = 0.5 to test consensus dynamics.
  3. Run dual-agent discussion with incongruent personas (e.g., bold-empathetic) at temperature = 1.0 to evaluate worst-case consensus and accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does combining agents from different model families (e.g., GPT-4, Claude, LLaMA) yield more robust consensus than using multiple instances of the same model?
- Basis in paper: [explicit] The authors state it "remains an open question whether multi-agent diversity derived from different model families... would lead to even richer or more robust consensus processes."
- Why unresolved: The study only tested agents instantiated from the same base LLM with varied personas, not heterogeneous model architectures.
- What evidence would resolve it: Comparative experiments measuring consensus quality and accuracy between homogeneous and heterogeneous multi-LLM agent groups.

### Open Question 2
- Question: Do systematic prompt engineering techniques, such as chain-of-thought or few-shot prompting, enhance the reliability of multi-agent qualitative coding?
- Basis in paper: [explicit] The paper highlights the "need for more systematic prompt engineering techniques, such as few-shot prompting and chain-of-thought prompting, which are beyond the scope of the present study."
- Why unresolved: The study relied on simple prompt designs due to computational constraints and scope.
- What evidence would resolve it: Experiments comparing the current simple prompting strategy against chain-of-thought and few-shot methods on coding accuracy and agent reasoning transparency.

### Open Question 3
- Question: Does multi-agent consensus provide greater benefits in inductive coding or codebook creation tasks than in the deductive coding tasks evaluated here?
- Basis in paper: [explicit] The authors suggest "future studies might examine inductive coding or mixed-methods settings where codebook refinement and creation are explicitly part of the analytic process."
- Why unresolved: The current study focused exclusively on deductive coding using a fixed, pre-validated codebook.
- What evidence would resolve it: Evaluation of MAS performance on tasks requiring the generation and refinement of codes rather than the application of existing ones.

## Limitations

- **Prompt Construction Uncertainty**: The study relies on defined persona traits (bold, empathetic, neutral) but does not provide exact prompt text, creating uncertainty about whether personality cues were sufficiently strong to produce consistent behavioral differences.
- **Model Family Effects**: All agents within a given model family share the same underlying architecture and training data. The lack of accuracy gains from MAS may reflect fundamental limitations of single-model deliberation rather than broader insights about multi-agent systems.
- **Gold Standard Validation**: The evaluation uses human gold standard codes, but the paper does not describe the inter-rater reliability or the number of human coders involved.

## Confidence

**High Confidence**:
- Temperature consistently reduces consensus likelihood across all models (p's < .001)
- MAS rarely improves coding accuracy compared to single-agent coding
- Incongruent personas delay consensus in most models without accuracy benefits

**Medium Confidence**:
- The finding that MAS is better suited for surfacing ambiguities than maximizing accuracy
- The specific effectiveness of OpenHermesV2:7B under particular conditions
- The claim that MAS lacks transparency and internal consistency in reasoning

**Low Confidence**:
- Generalizability of MAS limitations to other qualitative coding tasks or domains
- Whether different LLM families or cross-model MAS would show different patterns
- The extent to which observed consensus delays reflect meaningful deliberation versus superficial discussion

## Next Checks

1. **Cross-Model MAS Experiment**: Implement MAS using different LLM families (e.g., combine Llama with DeepSeek) to test whether accuracy improvements emerge when agents have genuinely different reasoning capabilities.

2. **Prompt Specification Replication**: Request and implement the exact persona prompt text used in the study to verify whether the observed effects replicate with precise prompt replication.

3. **Gold Standard Reliability Assessment**: Obtain documentation of inter-rater reliability metrics and number of human coders for the gold standard to validate whether accuracy measurements are robust to potential validation artifacts.