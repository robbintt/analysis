---
ver: rpa2
title: 'BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices'
arxiv_id: '2510.10560'
source_url: https://arxiv.org/abs/2510.10560
tags:
- memory
- multimodal
- episodic
- vision
- bitmar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BitMar addresses the challenge of deploying large multimodal vision-language
  models on edge devices by introducing a quantized architecture that combines 1.58-bit
  text and vision encoders with an episodic memory system. The approach integrates
  a BitNet-style text encoder and a DiNOv2-based vision encoder to generate compact
  embeddings, which are fused and used to query a fixed-size episodic memory module.
---

# BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices
## Quick Facts
- arXiv ID: 2510.10560
- Source URL: https://arxiv.org/abs/2510.10560
- Reference count: 6
- Primary result: BitMar achieves 42.8% accuracy on BoolQ and 54.6% on WinoGrande with 1.58-bit text/vision encoders and episodic memory, using only 14M parameters.

## Executive Summary
BitMar introduces a quantized multimodal architecture designed for efficient deployment on edge devices. By combining 1.58-bit text and vision encoders with an episodic memory module, the system generates compact embeddings and retrieves relevant context for long-context generation. Experiments demonstrate competitive performance on language understanding tasks while maintaining low latency and memory usage suitable for resource-constrained environments.

## Method Summary
BitMar uses a BitNet-style 1.58-bit text encoder and a DiNOv2-based 1.58-bit vision encoder to generate compact embeddings. These embeddings are fused and used to query a fixed-size episodic memory module. A BitNet decoder with per-layer conditioning and sliding-window attention processes the fused and retrieved context. The architecture integrates aggressive quantization with episodic memory augmentation to enable efficient multimodal reasoning on edge devices.

## Key Results
- Achieves 42.8% accuracy on BoolQ and 54.6% on WinoGrande with only 14M parameters.
- Episodic memory improves task accuracy by 3â€“4 percentage points on entity/property reasoning and multimodal QA.
- Demonstrates low latency and memory usage suitable for edge deployment.

## Why This Works (Mechanism)
The system leverages aggressive quantization (1.58-bit) to drastically reduce model size and computational requirements, while episodic memory allows retrieval of relevant context for improved reasoning. Fusion of multimodal embeddings and sliding-window attention in the decoder enable efficient long-context generation. The per-layer conditioning in the decoder further enhances adaptability to varying input contexts.

## Foundational Learning
- **Quantization (1.58-bit)**: Reduces model size and computation; needed for edge deployment; quick check: verify model size and latency reduction.
- **Episodic Memory**: Stores and retrieves relevant context; needed for improved reasoning; quick check: test memory retrieval accuracy.
- **Multimodal Fusion**: Combines text and vision embeddings; needed for unified reasoning; quick check: validate fusion effectiveness.
- **Sliding-Window Attention**: Enables efficient long-context processing; needed for scalability; quick check: measure attention efficiency.
- **Per-Layer Conditioning**: Adapts decoder to varying contexts; needed for flexibility; quick check: assess conditioning impact.
- **BitNet Architecture**: Supports low-bit operations; needed for quantization; quick check: confirm BitNet compatibility.

## Architecture Onboarding
- **Component Map**: Text Encoder (1.58-bit BitNet) -> Vision Encoder (1.58-bit DiNOv2) -> Fusion Module -> Episodic Memory -> BitNet Decoder (per-layer conditioning + sliding-window attention)
- **Critical Path**: Input (text/vision) -> Encoders -> Fusion -> Episodic Memory Query -> Decoder (with attention) -> Output
- **Design Tradeoffs**: Aggressive quantization reduces model size and latency but may impact accuracy; episodic memory improves reasoning but increases complexity and slight overhead.
- **Failure Signatures**: Degraded performance on morphological productivity and psycholinguistic alignment; limited scalability for large context windows.
- **3 First Experiments**: 1) Test quantization impact on accuracy vs. model size. 2) Evaluate episodic memory retrieval effectiveness. 3) Measure latency and memory usage on edge devices.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to a narrow set of tasks (BoolQ, WinoGrande, entity/property reasoning, multimodal QA).
- Episodic memory introduces trade-offs, slightly degrading performance on morphological productivity and psycholinguistic alignment.
- Benefits demonstrated under controlled conditions; impact of variable input lengths, dynamic workloads, and hardware heterogeneity on edge devices not explored.
- No ablation studies isolating contributions of quantization, episodic memory, and fusion.

## Confidence
- **High**: Competitive accuracy on tested benchmarks (BoolQ, WinoGrande) with aggressive quantization and episodic memory.
- **Medium**: Generalizability to broader multimodal tasks and real-world edge scenarios.
- **Medium**: Trade-offs introduced by episodic memory on linguistic tasks.
- **Low**: Scalability and robustness under varying edge device constraints and input complexities.

## Next Checks
1. Evaluate BitMar on a wider range of multimodal tasks (e.g., visual question answering, image captioning, document understanding) to assess generalizability.
2. Conduct ablation studies to isolate the contributions of quantization, episodic memory, and fusion to overall performance.
3. Test BitMar's performance under realistic edge device conditions, including variable input lengths, dynamic workloads, and hardware heterogeneity.