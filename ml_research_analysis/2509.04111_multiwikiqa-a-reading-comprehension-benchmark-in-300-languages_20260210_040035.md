---
ver: rpa2
title: 'MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages'
arxiv_id: '2509.04111'
source_url: https://arxiv.org/abs/2509.04111
tags:
- languages
- language
- question
- dataset
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultiWikiQA, a multilingual reading comprehension
  dataset covering 306 languages, generated using Wikipedia articles and LLM-generated
  questions. The authors conducted crowdsourced human evaluations of question fluency
  across 30 languages, showing high quality with mean ratings above 2.0 (mostly natural).
---

# MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages

## Quick Facts
- **arXiv ID:** 2509.04111
- **Source URL:** https://arxiv.org/abs/2509.04111
- **Reference count:** 0
- **Primary result:** LLM-generated reading comprehension dataset covering 306 languages, with crowdsourced fluency evaluation in 30 languages and benchmark results across 261 languages.

## Executive Summary
MultiWikiQA addresses the severe lack of reading comprehension benchmarks for low-resource languages by providing extractive QA data across 306 languages. The dataset was generated using Wikipedia articles and LLM-generated questions, with a two-stage process including question rephrasing to prevent word-matching shortcuts. Human evaluations across 30 languages showed high question fluency (mean ratings above 2.0). The benchmark evaluates both decoder and encoder language models, revealing significant performance variation across languages and demonstrating the dataset's difficulty. The authors provide the dataset and evaluation data publicly to support multilingual NLP research.

## Method Summary
The benchmark was created using the 20231101 Wikipedia dump for 315 languages (split into 306 after processing Mandarin and Portuguese variants). Questions were generated using Gemini-1.5-pro in two stages: first generating tentative QA pairs (2-10 per article) with structured JSON output, then rephrasing questions to avoid word-matching shortcuts. The dataset was filtered to require valid JSON keys and verbatim answer matches in context. Each language was sampled until reaching 5,000 entries or exhausting articles. Models were evaluated using the EuroEval framework with decoder models using 2-shot learning and encoder models trained on 1,024+ samples with early stopping on validation data.

## Key Results
- High question fluency achieved across 30 evaluated languages with mean ratings above 2.0
- Significant performance variation observed across 261 evaluated languages
- Encoder models outperform decoder models in few-shot settings for low-resource languages
- Dataset successfully demonstrates difficulty through rephrasing step that prevents word-matching shortcuts

## Why This Works (Mechanism)
The benchmark works by creating a standardized extractive QA task across hundreds of languages using Wikipedia as a consistent knowledge source. The two-stage LLM generation with verbatim filtering ensures questions are answerable from the provided context. The rephrasing step prevents models from exploiting lexical overlap between questions and answers, forcing genuine comprehension. By evaluating both encoder and decoder architectures with different training paradigms (few-shot vs. fine-tuning), the benchmark reveals how model architecture and training strategy interact with language resource availability.

## Foundational Learning
- **Concept: Extractive Question Answering (QA)**
  - **Why needed here:** The core task requires models to locate answers within context, not generate them from memory
  - **Quick check question:** If a model answers correctly but uses different words than source text, does it succeed on MultiWikiQA?

- **Concept: Multilingual Transfer & Resource Levels**
  - **Why needed here:** Performance varies dramatically across high- and low-resource languages due to training data availability
  - **Quick check question:** Why might an encoder model fine-tuned on 1,024 samples outperform a large decoder in few-shot setting for very low-resource language?

- **Concept: Synthetic Data Validation**
  - **Why needed here:** Entire dataset relies on LLM generation, requiring validation methods to ensure quality
  - **Quick check question:** What are two potential failure modes in LLM-generated QA dataset that verbatim-answer check would not catch?

## Architecture Onboarding
- **Component map:** Wikipedia Text -> Gemini-1.5-pro (Generate QA) -> Verbatim Filter -> Gemini-1.5-pro (Rephrase Question) -> Final QA Pair
- **Critical path:** The path from raw article to final benchmark entry is: `Wikipedia Text → Gemini-1.5-pro (Generate QA) → Verbatim Filter → Gemini-1.5-pro (Rephrase Question) → Final QA Pair`. Failure at the verbatim filter is a primary bottleneck.
- **Design tradeoffs:** Rephrasing increases difficulty but may introduce semantic drift; sample threshold of 1,024 excludes very low-resource languages to ensure evaluation reliability.
- **Failure signatures:** Question-Context Overlap occurs if rephrasing fails; LLM Hallucination produces unanswerable questions despite verbatim filter.
- **First 3 experiments:** 1) Baseline & Re-phrasing Ablation: Run baseline model with and without rephrased questions to quantify difficulty increase. 2) Error Analysis by Language Resource Level: Analyze 50 errors each from high- and low-performing low-resource languages to distinguish model limitations from dataset noise. 3) Encoder vs. Decoder Efficiency: Compare training compute and performance of encoder fine-tuned on 1,024 samples versus decoder in 2-shot setting.

## Open Questions the Paper Calls Out
- The authors explicitly state they cannot guarantee that quality conclusions from the 30 surveyed languages generalize to the remaining 276 languages due to resource constraints in extending crowdsourced evaluation.
- The paper acknowledges the complete reliance on a single LLM (Gemini-1.5-pro) for generation, which may introduce systematic artifacts that could favor specific model architectures, though this is not empirically tested.

## Limitations
- Complete reliance on LLM-generated content creates risk of synthetic data artifacts despite verbatim filtering
- Human evaluation covers only 30 languages (10% of total), limiting generalizability of quality assessments
- 1,024 sample minimum for encoder training creates significant coverage gap for very low-resource languages

## Confidence
**High Confidence:** Dataset creation methodology is clearly specified with concrete parameters; evaluation framework using EuroEval is reproducible; encoder vs. decoder performance findings align with established transfer learning principles.

**Medium Confidence:** Question quality claims supported by human evaluation but limited by 30-language sample; benchmark difficulty assertion reasonable but not comprehensively calibrated; decoder few-shot performance supported but needs additional experimentation.

**Low Confidence:** Claims about comprehensive multilingual coverage overstated due to sample threshold exclusions; assertions about driving progress in low-resource modeling are speculative without longitudinal studies.

## Next Checks
1. **Inter-Annotator Agreement Analysis:** Conduct new human evaluations with multiple annotators per question across 10 additional languages to establish reliability scores and verify original fluency ratings.

2. **Synthetic Data Artifact Detection:** Implement automated semantic consistency check comparing original and rephrased questions, then manually analyze 100 random pairs to identify rephrasing failure modes.

3. **Coverage Gap Analysis:** For the 45 languages below 1,024 sample threshold, analyze Wikipedia article counts and language family characteristics, then create supplemental 100-sample evaluation protocol to compare performance patterns.