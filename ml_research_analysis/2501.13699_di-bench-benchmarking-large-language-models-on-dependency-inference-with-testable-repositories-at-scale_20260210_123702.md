---
ver: rpa2
title: 'DI-BENCH: Benchmarking Large Language Models on Dependency Inference with
  Testable Repositories at Scale'
arxiv_id: '2501.13699'
source_url: https://arxiv.org/abs/2501.13699
tags:
- dependency
- code
- python
- dependencies
- file
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DI-BENCH, the first large-scale benchmark
  and evaluation framework for dependency inference in repository-level software development.
  The benchmark consists of 581 real-world repositories across Python, C, Rust, and
  JavaScript, with an automated CI-based curation pipeline using GitHub Actions for
  scalable validation.
---

# DI-BENCH: Benchmarking Large Language Models on Dependency Inference with Testable Repositories at Scale

## Quick Facts
- arXiv ID: 2501.13699
- Source URL: https://arxiv.org/abs/2501.13699
- Reference count: 40
- Primary result: First large-scale benchmark for repository-level dependency inference across 581 repos spanning Python, C#, Rust, JavaScript

## Executive Summary
This paper introduces DI-BENCH, the first comprehensive benchmark for evaluating large language models on dependency inference tasks at the repository level. The benchmark addresses a critical gap in LLM evaluation by focusing on their ability to generate correct dependency specifications that enable successful compilation and testing. With 581 curated repositories spanning four programming languages, DI-BENCH provides a scalable, execution-based evaluation framework that goes beyond textual metrics to measure real-world executability.

The benchmark reveals significant challenges in LLM dependency reasoning, with even the best-performing model achieving only 42.9% execution pass rate on Python repositories. Analysis identifies hallucination and dependency metadata issues as key bottlenecks, with repository size and dependency count negatively impacting performance. The automated CI-based curation pipeline using GitHub Actions enables scalable validation without manual environment setup, making DI-BENCH a valuable tool for advancing LLM capabilities in software development.

## Method Summary
DI-BENCH presents a benchmark and evaluation framework for dependency inference in repository-level software development. The approach involves curating 581 real-world repositories across Python, C#, Rust, and JavaScript, with dependency sections masked in build files. An automated CI-based pipeline uses GitHub Actions workflows to validate executability - repositories must have passing CI before masking, and predicted dependencies must enable successful builds and test execution. The framework supports three prompting strategies: All-In-One (full context), File-Iterate (per-file then merge), and Imports-Only (tree-sitter extracted imports). Evaluation combines textual metrics (precision, recall, F1) with execution-based executability rates and hallucination detection through fake rate measurement.

## Key Results
- Best-performing model achieves only 42.9% execution pass rate on Python repositories, revealing significant dependency inference challenges
- Repository size and dependency count negatively impact performance, with large repositories (>120k tokens) showing substantially lower executability
- Metadata errors (wrong versions, missing features) are a larger bottleneck than package name errors, with oracle metadata improving executability by up to 246.4% for Rust
- Hallucinated (non-existent) dependencies account for significant failure proportions, particularly affecting Rust and JavaScript performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CI-based evaluation pipelines enable scalable, objective dependency inference assessment without manual environment setup.
- **Mechanism:** The benchmark reuses existing GitHub Actions workflows as test harnesses. By locating test jobs via LLM-assisted identification and executing them locally with `act`, the system validates whether inferred dependencies allow successful builds and test execution. This transforms each repository's own CI configuration into an automated oracle.
- **Core assumption:** Repositories with valid CI workflows provide reliable ground truth for dependency correctness; test coverage is sufficient to catch dependency errors.
- **Evidence anchors:**
  - [abstract] "an automated CI-based curation pipeline using GitHub Actions for scalable validation"
  - [section 4.2] "we leverage GitHub Actions... By reusing these developer-written CI workflows... we propose an automated curation pipeline that eliminates human engagement"
  - [corpus] CSR-Bench similarly uses execution-based validation for repository deployment, suggesting this pattern generalizes.
- **Break condition:** If repositories have incomplete test coverage or CI configurations that don't exercise all dependency paths, false positives may occur.

### Mechanism 2
- **Claim:** Full-context aggregation (All-In-One) outperforms modular processing for regular-sized repositories, but context-window limits force trade-offs for larger codebases.
- **Mechanism:** When the entire codebase fits within the model's context window, providing all source files simultaneously enables cross-file dependency reasoning. However, for repositories exceeding token limits (the "large" subset with >120k tokens), this approach fails entirely, forcing fallback to File-Iterate or Imports-Only methods which show significantly degraded performance.
- **Core assumption:** Models can reason across files when given complete context; modular processing loses inter-file dependency relationships.
- **Evidence anchors:**
  - [section 6.1] "the All-In-One approach... consistently outperformed other methods on executability. However, this approach does not work for larger repositories"
  - [table 3] Regular subset: All-In-One achieves 42.9% (Python) vs File-Iterate at 29.6%; Large subset: All-In-One impossible, File-Iterate drops to 8.0%
  - [corpus] DependEval and RepoGenesis similarly highlight repository-level reasoning challenges requiring cross-file understanding.
- **Break condition:** If models develop better long-context reasoning or if retrieval-augmented approaches can efficiently identify relevant files, this trade-off may shift.

### Mechanism 3
- **Claim:** Hallucinated (non-existent) dependencies and incorrect metadata (wrong versions, missing features) directly reduce executability, with metadata errors being a larger bottleneck than package name errors.
- **Mechanism:** Models sometimes generate dependencies that don't exist in package ecosystems (hallucination) or specify incompatible versions. When oracle metadata replaces predicted versions while keeping package names, executability improves substantially (Python: 42.9% → 55.1%, +28.4% relative). Removing hallucinated dependencies also improves rates modestly.
- **Core assumption:** Version and feature specification requires different reasoning than package identification; current training data may lack sufficient version compatibility signals.
- **Evidence anchors:**
  - [section 6.3] "the Python executability rate improved from 42.9% to 55.1%, representing a relative increase of 28.4%"
  - [table 6] Removing fake dependencies improves executability 4.8% (Python), 18.8% (Rust)
  - [figure 4] "Invalid Dependency Version in Build" and "Dependency Not Found" account for significant failure proportions
  - [corpus] Weak corpus signal—related benchmarks focus on code generation errors, not specifically dependency metadata; this appears underexplored.
- **Break condition:** If models are fine-tuned on version compatibility data or if package ecosystems provide better structured metadata, this bottleneck may diminish.

## Foundational Learning

- **Concept: Build system and dependency specification formats**
  - **Why needed here:** Each language uses different configuration files (Python: `pyproject.toml`, Rust: `Cargo.toml`, C#: `.csproj`, JavaScript: `package.json`) with distinct syntax and semantics for specifying dependencies, versions, and features. Understanding these formats is prerequisite to both constructing the benchmark and interpreting model outputs.
  - **Quick check question:** Given a Python project using `scikit-learn` with the `SCORERS` attribute removed in version 1.2, what version constraint should be specified in `pyproject.toml` to ensure compatibility?

- **Concept: Continuous Integration (CI) workflows and job isolation**
  - **Why needed here:** The benchmark relies on identifying test-specific jobs within multi-job workflows (distinguishing testing from linting/publishing) and executing them in isolation. Misidentification leads to validation failures or false positives.
  - **Quick check question:** In a GitHub Actions workflow with jobs named `lint`, `test`, and `publish`, which job(s) should be selected for dependency validation, and what information in the YAML helps distinguish them?

- **Concept: Long-context reasoning limitations in transformers**
  - **Why needed here:** The regular/large split (120k token threshold) exists because current LLMs struggle with extended contexts. Performance degrades with both repository size and dependency count, suggesting attention mechanisms lose relevant signal as context grows.
  - **Quick check question:** Why might a model correctly identify dependencies in a 20-file repository but fail on an 80-file repository with identical per-file complexity?

## Architecture Onboarding

- **Component map:**
  - Curation Pipeline: Repository Crawler (filters by stars, size, CI presence) -> Test Job Locator (LLM-assisted YAML parsing) -> Execution Validator (runs CI locally with `act`) -> Dependency Masker (removes dependency sections from config files)
  - Evaluation Framework: Prompt Constructor (generates All-In-One, File-Iterate, or Imports-Only inputs) -> Model Inference -> Output Parser (extracts updated config files) -> CI Executor (runs masked repos with predicted deps) -> Metric Computer (Precision, Recall, F1, Executability, Fake Rate)
  - Storage: 581 repositories split into Regular (<120k tokens, 387 repos) and Large (>120k tokens, 194 repos), spanning Python, Rust, C#, JavaScript

- **Critical path:**
  1. Repository must have valid, passing CI before masking (validated by Execution Validator)
  2. Masked repository + model-predicted dependencies must successfully install packages during CI execution
  3. Tests must pass after installation for executability=1

- **Design tradeoffs:**
  - **Regular vs. Large split:** Regular enables All-In-One (best performance) but doesn't test long-context capabilities; Large forces modular approaches but reflects realistic enterprise-scale challenges
  - **Textual vs. execution metrics:** Textual (P/R/F1) is fast but doesn't catch version incompatibilities; execution is ground-truth but expensive (5+ minutes per repo)
  - **Imports-Only efficiency:** Fastest inference (minimal context) but loses implementation context that reveals transitive dependencies

- **Failure signatures:**
  - **Missing Dependency in Test (29.8% of Python failures):** Model omits packages used only in test files
  - **Dependency Not Found in Build (22.8%):** Hallucinated package names that don't exist in ecosystem
  - **Invalid Dependency Version (17.5%):** Package exists but specified version doesn't or incompatible with code usage
  - **High Fake Rate with Imports-Only on Large repos (23.1% Python):** Suggests context loss leads to more hallucination

- **First 3 experiments:**
  1. **Reproduce baseline performance gap:** Run All-In-One on Regular Python subset with GPT-4o, verify ~42.9% executability. Compare against File-Iterate to quantify context-completeness benefit.
  2. **Isolate metadata vs. package-name errors:** For failing predictions, manually replace version constraints with ground-truth versions while keeping model-predicted package names. Measure executability improvement to confirm metadata is primary bottleneck.
  3. **Test hallucination impact:** Filter out predicted dependencies with fake rate >0 (non-existent packages), re-run execution. Quantify modest improvement (paper shows 4.8% Python, 18.8% Rust) to assess whether hallucination mitigation is worth investment vs. metadata accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM architectures be optimized to better reason about dependency metadata, such as version constraints, to bridge the gap between textual accuracy and execution success?
- Basis in paper: [explicit] Section 6.3 explicitly identifies "Reasoning the dependency metadata is a bottleneck," showing that replacing predicted metadata with oracle metadata yielded a relative execution improvement of 28.4% (Python) to 246.4% (Rust).
- Why unresolved: Current models achieve moderate textual precision/recall but fail to generate the exact metadata required for successful builds, suggesting a lack of deep understanding of version compatibility and build system logic.
- What evidence would resolve it: The development of a model that achieves high executability rates on DI-BENCH without requiring oracle metadata intervention, specifically demonstrating accurate version constraint reasoning.

### Open Question 2
- Question: Can agentic frameworks or retrieval-augmented generation (RAG) strategies overcome the performance degradation observed in large repositories where single-context methods fail?
- Basis in paper: [explicit] Section 4.1 states that the "Large" dataset presents "additional challenges of model context limits, specifically motivating the exploration of novel methodologies," and Section 5 notes the intentional exclusion of complex agent-based techniques in baseline experiments.
- Why unresolved: The "All-In-One" method, which performs best on regular repositories, is inapplicable to large repositories due to context length limits, while modular methods (File-Iterate) suffer significant performance drops on larger codebases.
- What evidence would resolve it: An agentic system or RAG-based approach that maintains stable performance (comparable to "All-In-One" on regular sets) when applied to the "Large" subset of DI-BENCH.

### Open Question 3
- Question: What specific mechanisms can effectively reduce the hallucination rate of non-existent packages or versions in repository-level dependency inference?
- Basis in paper: [explicit] Section 6.3 concludes that "Hallucination issues remain one of the primary obstacles," noting that removing hallucinated dependencies improved executability, particularly for Rust (18.8% relative improvement).
- Why unresolved: LLMs frequently generate dependencies that do not exist in the package ecosystem or the local repository, as indicated by the "Fake Rate" metric, directly causing build failures.
- What evidence would resolve it: A model fine-tuned or prompted with mechanisms (e.g., tool use to query package registries) that achieves a near-zero Fake Rate across all four programming languages in the benchmark.

## Limitations

- Benchmark performance evaluation relies heavily on CI test execution, which may not capture all runtime dependency scenarios
- Performance degradation with repository size may be partially attributable to model-specific context window limitations rather than fundamental dependency reasoning challenges
- The 120k token threshold for regular/large split was chosen empirically but may not optimally balance context completeness against model limitations

## Confidence

- **High Confidence:** CI-based evaluation pipeline design, relative performance ordering across prompting strategies, metadata error impact quantification
- **Medium Confidence:** Repository size correlation with performance, hallucination vs metadata error contributions, cross-language generalization
- **Low Confidence:** Optimal context window thresholds, long-context reasoning limitations, scalability of evaluation framework to larger enterprise repositories

## Next Checks

1. Test DI-BENCH methodology on repositories with known complex dependency chains (e.g., transitive dependencies spanning multiple files) to validate that execution-based evaluation captures subtle dependency errors
2. Replicate the experiment using models with extended context windows (e.g., Gemini-1.5-Pro with 1M token context) to isolate whether performance degradation is truly a dependency reasoning problem versus a context limitation
3. Conduct ablation study removing the CI execution requirement while maintaining ground-truth dependency annotations to quantify the overhead versus benefit of the full evaluation pipeline