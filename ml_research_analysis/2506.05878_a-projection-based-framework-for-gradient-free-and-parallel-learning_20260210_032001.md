---
ver: rpa2
title: A projection-based framework for gradient-free and parallel learning
arxiv_id: '2506.05878'
source_url: https://arxiv.org/abs/2506.05878
tags:
- projection
- learning
- graph
- training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a projection-based framework for neural\
  \ network training that reformulates the learning problem as a large-scale feasibility\
  \ problem instead of traditional loss minimization. The method uses projection operators\
  \ and iterative projection algorithms to find network parameters and states satisfying\
  \ local constraints derived from the network\u2019s elementary operations."
---

# A projection-based framework for gradient-free and parallel learning

## Quick Facts
- **arXiv ID**: 2506.05878
- **Source URL**: https://arxiv.org/abs/2506.05878
- **Reference count**: 40
- **One-line primary result**: Projection methods achieve competitive accuracy on MLPs and CNNs while enabling parallel training and handling non-differentiable operations.

## Executive Summary
This paper introduces a projection-based framework for neural network training that reformulates the learning problem as a large-scale feasibility problem rather than traditional loss minimization. The method uses projection operators and iterative projection algorithms to find network parameters and states satisfying local constraints derived from the network's elementary operations. A key contribution is PJAX, a JAX-based software framework that automatically composes projection operators for elementary operations, enabling parallel and GPU/TPU-accelerated training. The approach is evaluated on diverse architectures across standard benchmarks, showing it as a viable alternative to gradient-based training with clear advantages in parallelism and handling non-differentiable operations.

## Method Summary
The framework reformulates neural network training as finding a state vector that satisfies local constraints from each operation in the network. Each primitive operation (dot product, ReLU, etc.) has an associated constraint set representing valid input-output pairs. Training becomes an iterative process of projecting a state vector containing all edge values and parameters onto these local constraint sets. The PJAX library automatically composes these projection operators for standard primitives. The computation graph is bipartite, enabling parallel execution where projections on one set of nodes can be computed simultaneously before synchronizing with the other set. The method is evaluated using iterative projection algorithms like Douglas-Rachford and Alternating Projections on standard architectures including MLPs, CNNs, and RNNs across benchmarks like MNIST, CIFAR-10, HIGGS, and Shakespeare.

## Key Results
- Projection methods (particularly Douglas-Rachford) approach SGD's accuracy on MLPs while being faster per step
- Adam consistently achieves the highest accuracy and fastest convergence across all architectures
- The method handles non-differentiable operations inherently, enabling training of networks with quantization
- Memory requirements are higher due to shared parameters, but parallelization provides significant speed benefits for deeper networks
- Skip connections are crucial for effective training in deep architectures using projection methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reformulating neural network training as a feasibility problem allows finding valid network parameters without gradient descent.
- **Mechanism:** The method decomposes a network into primitive operations and defines constraint sets for each operation's valid input-output pairs. Training iteratively projects a state vector onto these local constraint sets. If projections converge, the state lies in the intersection of all constraints, representing a valid network computation.
- **Core assumption:** The intersection of constraint sets derived from training data and network architecture is non-empty or sufficiently close, and iterative projection algorithms can find this intersection despite non-convexity.
- **Evidence anchors:**
  - [abstract] "reformulates the learning problem as a large-scale feasibility problem... finding network parameters and states that satisfy local constraints."
  - [section 3] "The overall feasibility problem involves finding a state vector $z$ that lies in the intersection of all such individual node constraints."
  - [corpus] Related work "Projection-Based Correction for Enhancing Deep Inverse Networks" supports projection methods for enforcing constraints.
- **Break condition:** If constraints are mutually exclusive or the algorithm gets stuck in a limit cycle, convergence fails.

### Mechanism 2
- **Claim:** Structuring updates via a bipartite computation graph enables parallel execution of projections.
- **Mechanism:** The computation graph is split into two disjoint sets of nodes ($A$ and $B$) where edges only connect nodes across sets. This allows all projections in set $A$ to be computed simultaneously (in parallel), followed by set $B$.
- **Core assumption:** Hardware (GPU/TPU) can effectively parallelize these fine-grained, independent operations, and memory overhead doesn't bottleneck the process.
- **Evidence anchors:**
  - [abstract] "Training then involves projecting onto these constraints, a local operation that can be parallelized across the network."
  - [section 3, Theorem 2] "The projection $P_{C_A}(z)$... can be computed by independently (and in parallel) applying the projection operators."
  - [corpus] Limited evidence for this specific bipartite parallelization in provided corpus.
- **Break condition:** If the graph cannot be efficiently bipartitioned or synchronization overhead dominates, theoretical speedup is lost.

### Mechanism 3
- **Claim:** Using projection operators allows the framework to handle non-differentiable components inherently.
- **Mechanism:** Standard backpropagation requires gradients, but this framework only needs the projection onto the graph of each function. Even discrete functions like quantization can be handled by defining geometric sets of valid points and projecting onto them without calculating slopes.
- **Core assumption:** A closed-form or computationally efficient projection operator exists for the desired non-differentiable primitive.
- **Evidence anchors:**
  - [abstract] "...clear advantages in parallelism and the ability to handle non-differentiable operations."
  - [appendix c.1, theorem 9] Explicit derivation of the projection operator for a quantization function.
  - [corpus] "Federated Learning on Riemannian Manifolds" uses projection-based approaches for manifold constraints.
- **Break condition:** If projection onto the non-convex set is ambiguous or computationally expensive, training becomes slow or unstable.

## Foundational Learning

- **Concept:** **Feasibility vs. Optimization Problems**
  - **Why needed here:** This paper shifts from minimizing a scalar loss function to finding a point that satisfies a system of constraints. Understanding this distinction is crucial for interpreting loss behavior and convergence criteria.
  - **Quick check question:** Can you explain why finding a point in the intersection of sets $A \cap B$ is different from minimizing a function $f(x)$?

- **Concept:** **Projection Operators**
  - **Why needed here:** This is the atomic operation replacing the gradient step. A projection $P_C(x)$ maps a point $x$ to the closest point in set $C$.
  - **Quick check question:** If you have a point $x$ and a constraint set $C$ (e.g., the unit ball), what does the projection $P_C(x)$ do if $x$ is already inside $C$? What if it is outside?

- **Concept:** **Bipartite Graphs**
  - **Why needed here:** The paper exploits the bipartite nature of computation graphs to achieve parallelism. Nodes can be colored into two groups where edges only cross between groups, enabling the update schedule.
  - **Quick check question:** In a standard feed-forward neural network graph, how would you assign nodes to sets $A$ and $B$ to ensure no two nodes in the same set are connected?

## Architecture Onboarding

- **Component map:** Parameter nodes -> Elementary operation nodes (dot, ReLU, etc.) -> Projection operators -> Bipartite computation graph -> Iterative solver (DR/AP)

- **Critical path:** To implement a new model:
  1. Implement the forward pass using PJAX primitives (this builds the graph)
  2. Define the loss constraint (e.g., cross-entropy proximal operator)
  3. Select a projection algorithm (e.g., DR) and run the `update` loop, iteratively projecting edge states until consensus is reached

- **Design tradeoffs:**
  - **Parallelism vs. Memory:** The method parallelizes well but requires storing edge variables for every connection ($O(|E|)$), which is significantly more memory-intensive than standard backprop ($O(|V|)$), especially for architectures with weight sharing (CNNs/RNNs)
  - **Locality vs. Depth:** Local updates can struggle to propagate error signals through deep stacks. **Skip connections** are cited as a critical architectural requirement to facilitate information flow in this paradigm

- **Failure signatures:**
  - **Memory OOM:** High memory usage on CNNs/RNNs due to edge state replication
  - **Stagnation in Deep Nets:** Deep MLPs without skip connections may fail to converge or achieve low accuracy as local projections fail to propagate gradients globally
  - **Slower Wall-Clock Time:** Despite parallelism, if the number of projection steps $K$ per batch is high (e.g., 50), total training time may exceed Adam unless the per-step speedup is massive

- **First 3 experiments:**
  1. **MLP on MNIST:** Train a shallow MLP using PJAX with Douglas-Rachford. Compare accuracy and step-time against a standard JAX/Adam baseline to verify the "viability" claim
  2. **Ablation on Depth & Skip Connections:** Train a 4-layer MLP with and without skip connections. Observe if performance degrades without skips, validating the "local update" limitation
  3. **Non-differentiable Test:** Implement a quantization layer (using Theorem 9) in a network. Attempt to train it, verifying that the method handles discrete operations where backpropagation would fail

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive step sizes, preconditioning, or acceleration techniques be integrated into projection algorithms to close the accuracy gap with adaptive gradient methods like Adam?
- Basis in paper: [explicit] The Discussion section states that "opportunities exist to design more advanced projection algorithms that focus on improving approximation, potentially through adaptive step sizes, preconditioning, or acceleration techniques" to address the current performance deficit compared to Adam.
- Why unresolved: While the paper demonstrates that Douglas-Rachford and Alternating Projections are viable, they consistently achieve lower final accuracy than Adam, suggesting the current iterative schemes are not fully optimizing the feasibility problem.
- What evidence would resolve it: The development and empirical validation of a modified projection algorithm (e.g., Adaptive Douglas-Rachford) that matches or exceeds Adam's convergence speed and final accuracy on the benchmarks used (e.g., CIFAR-10, HIGGS).

### Open Question 2
- Question: What implementation strategies can effectively mitigate the high memory footprint associated with parameter sharing in architectures like CNNs and RNNs?
- Basis in paper: [explicit] The Discussion identifies "developing strategies to reduce the memory footprint of these methods, especially in architectures with significant parameter sharing" as a "crucial direction."
- Why unresolved: The method stores edge variables ($O(|E|)$) rather than node gradients ($O(|V|)$), causing memory issues when unrolling RNNs or applying convolutional kernels across spatial locations, as detailed in Section 4.
- What evidence would resolve it: A modified framework or algorithm that successfully trains deep CNNs or long-sequence RNNs with memory consumption comparable to backpropagation-based methods.

### Open Question 3
- Question: Do projection-based methods require fundamentally different network architectures or connectivity patterns beyond simple skip connections to propagate information effectively in deep networks?
- Basis in paper: [explicit] The Discussion suggests that "the iterative nature of information propagation via local updates may necessitate tailored network architectures beyond simple skip connections" to enhance robustness and performance.
- Why unresolved: The paper shows that standard deep MLPs struggle without skip connections, and while skip connections help, they may not be the optimal structural solution for local constraint satisfaction.
- What evidence would resolve it: Designing and testing novel architectural blocks specifically optimized for local constraint satisfaction that outperform standard architectures with skip connections.

### Open Question 4
- Question: What are the theoretical convergence guarantees and generalization properties of using iterative projection algorithms for the non-convex feasibility problems arising in neural network training?
- Basis in paper: [explicit] The Discussion lists "deepening the theoretical understanding of convergence and generalization" as a necessary area of future work.
- Why unresolved: While the paper references convergence in convex settings, neural networks represent non-convex feasibility problems where the intersection of constraint sets may be empty or difficult to characterize, leaving the theoretical soundness of the empirical success undefined.
- What evidence would resolve it: Rigorous mathematical proofs establishing conditions under which the algorithm converges to a stationary point or feasible solution in non-convex settings, and how this relates to generalization error.

## Limitations
- **Memory overhead:** High memory usage for architectures with parameter sharing (CNNs/RNNs) due to storing edge variables for every connection
- **Deep network scalability:** Struggles to converge on deep networks without skip connections due to local updates failing to propagate error signals globally
- **Accuracy gap:** Consistently achieves lower final accuracy than adaptive gradient methods like Adam, suggesting current projection algorithms are not fully optimizing the feasibility problem

## Confidence
- **High confidence:** The method's ability to handle non-differentiable operations and its parallel execution on standard benchmarks (MNIST, CIFAR-10)
- **Medium confidence:** The claim that projection methods approach SGD's accuracy on MLPs, as results show a consistent gap with Adam
- **Low confidence:** The scalability of the method to very deep networks or architectures with complex parameter sharing without significant architectural modifications

## Next Checks
1. **Test on a non-differentiable primitive:** Implement a quantization layer and train it using PJAX to verify the method's claimed advantage over backpropagation
2. **Ablate skip connections in a deep MLP:** Train a 4-layer MLP on MNIST with and without skip connections to confirm the performance degradation and the critical role of architectural modifications
3. **Profile memory and step-time scaling:** Measure memory usage and wall-clock time per step for CNNs and RNNs as depth and width increase, comparing against standard gradient-based methods to validate the memory and speed claims