---
ver: rpa2
title: Efficient Multi-Source Knowledge Transfer by Model Merging
arxiv_id: '2508.19353'
source_url: https://arxiv.org/abs/2508.19353
tags:
- components
- task
- axis
- atlas
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AXIS, a scalable framework for multi-source
  knowledge transfer that leverages Singular Value Decomposition (SVD) to decompose
  source models into elementary components and aggregate only the most salient ones.
  Unlike prior approaches, AXIS maintains constant memory and runtime costs regardless
  of the number of source models by decoupling knowledge aggregation from adaptation.
---

# Efficient Multi-Source Knowledge Transfer by Model Merging

## Quick Facts
- arXiv ID: 2508.19353
- Source URL: https://arxiv.org/abs/2508.19353
- Authors: Marcin Osial; Bartosz Wójcik; Bartosz Zieliński; Sebastian Cygert
- Reference count: 40
- Key outcome: AXIS outperforms state-of-the-art aTLAS by up to 2.54 percentage points on 21 target tasks while maintaining constant memory and runtime costs regardless of source model count.

## Executive Summary
This paper introduces AXIS, a scalable framework for multi-source knowledge transfer that leverages Singular Value Decomposition (SVD) to decompose source models into elementary components and aggregate only the most salient ones. Unlike prior approaches, AXIS maintains constant memory and runtime costs regardless of the number of source models by decoupling knowledge aggregation from adaptation. The method demonstrates superior performance over the state-of-the-art aTLAS across 21 target tasks, achieving up to 2.54 percentage points higher accuracy on ViT-B-32 architecture and maintaining advantages on larger ViT-L-14 models. AXIS also exhibits robustness to noisy or pruned source parameters and input degradations, making it a practical solution for efficient multi-source transfer learning.

## Method Summary
AXIS is a two-stage framework for merging T-1 fine-tuned source models into a single transferable representation. Stage 1 performs SVD decomposition on each source model's task matrix (weight differences from pretrained checkpoint), collects all rank-one components, and selects the top-K components globally by singular value magnitude. These components are summed and re-orthogonalized via final SVD to create a merged matrix ∆ₘ. Stage 2 fine-tunes only the top-N singular values of this merged matrix on the target task while keeping all vectors frozen. This decoupling ensures constant memory usage regardless of source count, as sources are discarded after aggregation.

## Key Results
- AXIS achieves up to 2.54 percentage points higher accuracy than aTLAS on ViT-B-32 across 21 target tasks
- Memory usage remains constant (~35GB) while aTLAS scales linearly (30→50GB) as sources increase from 5 to 20
- Performance remains robust when source parameters are pruned (up to 70% removal) or contain noise
- Final SVD re-orthogonalization is critical—removing it causes catastrophic performance drops (~8 p.p.)
- The method maintains advantages on larger ViT-L-14 architecture while scaling efficiently

## Why This Works (Mechanism)

### Mechanism 1: SVD-Based Granular Component Extraction
Decomposing task vectors into rank-one components enables selective knowledge extraction that outperforms coarse-grained averaging. Each task matrix ∆ᵢ is decomposed via SVD into triplets (u, σ, v⊤). A global Top-K ranking across all sources selects only high-magnitude components, filtering noise while preserving dominant learned patterns. The core assumption is that transferable knowledge resides in principal singular components with high singular values.

### Mechanism 2: Decoupled Aggregation-Adaptation Pipeline
Separating knowledge aggregation (Stage 1) from target adaptation (Stage 2) yields constant O(1) memory/runtime regardless of source count. Stage 1 produces a fixed-size merged matrix ∆ₘ via one-time SVD operations. Stage 2 fine-tunes only top-N singular values of ∆ₘ. Unlike aTLAS (which keeps all source vectors in memory), AXIS discards sources after aggregation, preserving sufficient knowledge for target adaptation.

### Mechanism 3: Re-Orthogonalization Stabilizes Adaptation
A final SVD re-parameterization of ∆ₘ creates a decorrelated basis that prevents destructive interference during fine-tuning. Aggregated components from different tasks are non-orthogonal. Final SVD (∆ₘ → UₜΣₜVₜ⊤) produces orthogonal vectors and recalibrated singular values, allowing gradient updates to singular values without destabilizing frozen components.

## Foundational Learning

- **Singular Value Decomposition (SVD):**
  - Why needed here: AXIS relies on SVD to decompose weight matrices into rank-one components (u, σ, v⊤) and extract principal directions.
  - Quick check question: Given a 100×50 matrix, can you identify the dimensions of U, Σ, and V⊤ and explain what the diagonal entries of Σ represent?

- **Task Vectors (Weight Differences):**
  - Why needed here: The fundamental unit AXIS manipulates is τ = θ_finetuned - θ_pretrained. Understanding that fine-tuning modifies weights in structured directions is essential.
  - Quick check question: If you fine-tune a model on task A and another on task B from the same pretrained checkpoint, what operation gives you each task vector?

- **Parameter-Efficient Fine-Tuning (PEFT) Principles:**
  - Why needed here: AXIS trains only N% of singular values while freezing vectors. This follows PEFT philosophy: small trainable parameter sets can adapt large models.
  - Quick check question: Why might fine-tuning only singular values (not vectors) reduce overfitting risk compared to full fine-tuning?

## Architecture Onboarding

- **Component map:** Source models (T-1) → SVD each → Global rank by σ → Top-K selection → Sum components → Re-SVD → ∆ₘ (fixed size) → Fine-tune top-N σ → Target model

- **Critical path:**
  1. Load pretrained CLIP (ViT-B-32 or ViT-L-14)
  2. Load T-1 task vectors from fine-tuned checkpoints
  3. Per-layer: SVD each ∆ᵢ → collect all (u,σ,v⊤) triplets
  4. Global sort by σ, select top-K (default K≈10% of layer rank)
  5. Sum selected: ∆ₘ = Σₖ uₖσₖvₖ⊤
  6. Re-SVD: ∆ₘ = UₜΣₜVₜ⊤
  7. Freeze Uₜ, Vₜ⊤; make top-N values in Σₜ trainable
  8. Fine-tune on target dataset (10 epochs, lr=10⁻¹)

- **Design tradeoffs:**
  - Higher K → more knowledge preserved, but more noise/interference
  - Higher N → more adaptability, but higher overfitting risk and parameter count
  - Paper finds K=76 (~10% rank) and N=10% are robust defaults (Figure 9)

- **Failure signatures:**
  - Accuracy drops to ~20%: You skipped the re-SVD step and directly used non-orthogonal ∆ₘ
  - No improvement over pretrained: You're training singular vectors instead of just values, or K is too small
  - Memory scales with source count: You're keeping source vectors in memory during Stage 2 (should discard after aggregation)

- **First 3 experiments:**
  1. **Baseline validation:** Run AXIS on a single target task (e.g., Flowers102) with K=76, N=10%. Compare accuracy to aTLAS baseline. Expected: AXIS should match or exceed aTLAS by 1-3 p.p.
  2. **Ablate final SVD:** Remove the re-orthogonalization step (line 14-15 in Algorithm 1). Plot accuracy vs. number of sources. You should see catastrophic degradation around 5-8 sources (Figure 11 pattern).
  3. **Stress test K selection:** Sweep K from 15 to 560 components. Plot accuracy. You should see peak near K=60-100 with <1.5% degradation at extreme K values (Figure 9 pattern).

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Method relies on shared architecture and pre-trained initialization, limiting applicability to models with different structures
- Fixed rank budget (K) may become bottleneck as source diversity increases, potentially discarding critical but lower-magnitude information
- SVD-based component selection effectiveness not rigorously proven—could work due to general rank-based PEFT benefits rather than specific decomposition approach

## Confidence
- **High Confidence**: Empirical results showing AXIS outperforming aTLAS by 1-2.5 percentage points across 21 tasks, and memory/runtime efficiency claims (constant O(1) scaling vs linear scaling)
- **Medium Confidence**: Theoretical justification for why SVD decomposition and selective component aggregation preserves most transferable knowledge, and claim that re-orthogonalization prevents destructive interference
- **Low Confidence**: Robustness claims to noisy/perturbed source parameters and input degradations, as these were tested but not deeply analyzed for underlying mechanisms

## Next Checks
1. **Ablation of component selection strategy**: Systematically test whether selecting bottom-ranked or randomly-selected components (instead of top-K by singular value) degrades performance, to confirm that principal components contain the most transferable knowledge rather than just being a convenient selection criterion.

2. **Analysis of knowledge distribution**: Examine the singular value spectra of task vectors to determine whether transferable knowledge is actually concentrated in top components, or if useful information is distributed across multiple magnitude ranges that AXIS might discard.

3. **Cross-dataset generalization stress test**: Evaluate AXIS on target tasks from domains completely absent in source tasks (e.g., medical imaging if sources are natural images) to test whether the method truly extracts general-purpose knowledge or simply memorizes source-specific patterns.