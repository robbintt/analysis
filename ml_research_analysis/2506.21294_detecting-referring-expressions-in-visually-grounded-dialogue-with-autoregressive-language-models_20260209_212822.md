---
ver: rpa2
title: Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive
  Language Models
arxiv_id: '2506.21294'
source_url: https://arxiv.org/abs/2506.21294
tags:
- context
- agos
- mention
- dialogue
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores the use of autoregressive language models\
  \ for detecting referring expressions in visually grounded dialogue, focusing on\
  \ the linguistic context\u2019s ability to inform detection. The proposed method\
  \ adapts a pretrained LLM to annotate dialogue utterances by inserting mention span\
  \ boundary tokens during next-token prediction, optionally conditioned on preceding\
  \ dialogue history."
---

# Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models

## Quick Facts
- arXiv ID: 2506.21294
- Source URL: https://arxiv.org/abs/2506.21294
- Reference count: 10
- This paper explores the use of autoregressive language models for detecting referring expressions in visually grounded dialogue, focusing on the linguistic context's ability to inform detection.

## Executive Summary
This paper presents a text-only approach for detecting referring expressions (mentions) in visually grounded dialogue using autoregressive language models. The method adapts a pretrained LLM to annotate dialogue utterances by inserting mention span boundary tokens during next-token prediction, optionally conditioned on preceding dialogue history. Experiments with LLaMA 3.1 8B fine-tuned using parameter-efficient methods on two dialogue datasets show that linguistic context alone is relatively informative for mention detection, achieving F1 scores around 0.86-0.94. The approach demonstrates diminishing returns with larger context windows and confirms that cross-modal approaches remain necessary for robust performance in visually grounded dialogue.

## Method Summary
The method uses autoregressive language modeling to detect mentions by generating annotated reproductions of utterances with inserted boundary tokens (>> for start, << for end). The approach fine-tunes LLaMA 3.1 8B using QLoRA parameter-efficient methods on two visually grounded dialogue datasets (AGOS and PB-GOLD). During inference, constrained decoding restricts the vocabulary at each step to the next input token or valid special tokens, forcing the model to reproduce the input utterance while inserting mention boundaries. Context windows of 0, 3, 7, and 19 preceding messages are tested to evaluate the impact of dialogue history on detection performance.

## Key Results
- F1 scores range from 0.86-0.94 across different context window sizes on cross-validation
- Larger context windows generally improve performance, though with diminishing returns (0→3 messages: +0.029 F1 on AGOS; 7→19: +0.002)
- Cross-dataset transfer shows performance drops of 0.12-0.16 F1 compared to within-dataset cross-validation
- Text-only approach validates linguistic context's informativeness but confirms cross-modal approaches remain necessary for robust performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dialogue history provides disambiguating context for referential intent.
- Mechanism: Preceding utterances establish antecedents and conversational focus, enabling the model to distinguish referential from non-referential token usage (e.g., "that" as exclamation vs. reference).
- Core assumption: Speakers maintain coherent referential chains; linguistic patterns correlate with visual grounding intent.
- Evidence anchors:
  - [abstract] "investigate the extent to which the linguistic context alone can inform the detection of mentions"
  - [section 1] "whether the use of 'that' in the exclaimed utterance 'Take that!' is referential or instead part of a non-referential interjection cannot be known without additional context"
  - [corpus] Related work on dialogue grounding (MapTask perspectivist annotation, GREC dialogue-based comprehension) supports linguistic context importance, though cross-modal validation remains standard.
- Break condition: Conversations with sparse visual references or ambiguous pronouns lacking antecedents in available history.

### Mechanism 2
- Claim: Constrained autoregressive decoding produces structured span annotations.
- Mechanism: At each generation step, vocabulary is restricted to the next input token or valid special tokens (>> and <<), forcing grammatical reproduction with inserted boundaries.
- Core assumption: The fine-tuned model learns to predict boundary placement as part of the language modeling objective.
- Evidence anchors:
  - [abstract] "demarcating mention span boundaries in text via next-token prediction"
  - [section 4.2] "at every time step we dynamically restrict the vocabulary, where the allowed tokens include the next token from the input utterance and any valid special tokens"
  - [corpus] Generative IE surveys (Zhang et al., 2025) show autoregressive framing effective for structured prediction tasks.
- Break condition: Nested mentions exceeding single-span IOB encoding capacity (rare in these datasets but noted as limitation).

### Mechanism 3
- Claim: Parameter-efficient fine-tuning enables adaptation with limited annotated dialogue data.
- Mechanism: QLoRA updates low-rank adapters while keeping base model frozen, preserving pretrained knowledge while learning task-specific boundary patterns.
- Core assumption: Pretrained LLMs encode sufficient linguistic understanding of reference; only light adaptation needed.
- Evidence anchors:
  - [abstract] "relatively small datasets, and parameter-efficient fine-tuning"
  - [section 4.2] "fine-tune LLAMA 3.1 8B using QLoRA... on a single 24GB NVIDIA GeForce RTX 3090"
  - [corpus] Limited direct corpus evidence on QLoRA for this specific task; assumption based on general PEFT literature.
- Break condition: Significantly different dialogue domains or languages not represented in base model pretraining.

## Foundational Learning

- Concept: **Referring expressions vs. mentions**
  - Why needed here: The paper treats these interchangeably; understanding that REs are expressions with referential intent toward visual entities is essential for interpreting what the model detects.
  - Quick check question: In "I see the red one," which span is the referring expression?

- Concept: **Autoregressive language modeling for structured prediction**
  - Why needed here: Framing span detection as next-token prediction with boundary insertion differs fundamentally from sequence labeling; understanding this enables proper implementation.
  - Quick check question: How does constrained decoding differ from unconstrained generation?

- Concept: **Visually grounded dialogue**
  - Why needed here: The task definition hinges on expressions having "visually perceivable referents"; distinguishing this from general coreference is critical for dataset creation and evaluation.
  - Quick check question: Does "you" in a dialogue about images count as a mention under this task definition?

## Architecture Onboarding

- Component map:
  - Input: Speaker-prefixed utterance + optional dialogue history (H = preceding messages)
  - Encoder-Decoder: Decoder-only LLaMA 3.1 8B (transformer with causal attention)
  - Adaptation: QLoRA adapters on projection layers and lm_head
  - Output: Annotated reproduction with >> << boundary tokens
  - Constraint: Dynamic vocabulary restriction per timestep

- Critical path:
  1. Format input: context messages separated by \n, target message after \n\n, inference token ->
  2. Forward pass through fine-tuned LLaMA with LoRA adapters active
  3. Apply constrained decoding: mask logits to permit only valid tokens
  4. Generate until full utterance reproduced
  5. Extract spans between boundary markers

- Design tradeoffs:
  - Larger context windows improve F1 but with diminishing returns (0→3 messages: +0.029 F1 on AGOS; 7→19: +0.002)
  - Text-only approach sacrifices visual validation for simplicity; cross-dataset transfer drops ~0.12-0.16 F1
  - Constrained decoding prevents hallucination but cannot correct input errors

- Failure signatures:
  - High false positives on pronouns without clear antecedents ("let's do it" vs. "let's go for it")
  - Partial spans from structural ambiguity (excluded relative clauses, split mentions)
  - Complete misses on ungrammatical phrasing or noise in input
  - Cross-domain transfer: AGOS→PB-GOLD outperforms reverse by ~0.08-0.13 F1 (richer linguistic context in AGOS generalizes better)

- First 3 experiments:
  1. Reproduce cross-validation on AGOS with context windows 0, 3, 7, 19; target F1 range 0.86-0.90
  2. Ablate constrained decoding (unconstrained generation) to measure precision impact on boundary placement
  3. Evaluate on held-out utterances with artificially truncated history to quantify context dependency per mention type (pronouns vs. full NPs)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the text-only mention detection approach perform on organic, non-task-oriented conversations where mentions of visually perceivable referents occur more sparsely?
- Basis in paper: [explicit] The authors state: "The extent to which conversations with comparatively sparse mention occurrences, and that take place outside of task-oriented settings, still exhibit such actionable patterns is, as of yet, unclear."
- Why unresolved: Both AGOS and PB-GOLD are task-oriented dialogues where images are the focal point, resulting in artificially high mention incidence rates. The approach relies on exploitable regularities in linguistic context that may not hold in natural conversation.
- What evidence would resolve it: Evaluation on corpora of unrestricted spoken dialogue or casual conversation with annotated visually grounded mentions.

### Open Question 2
- Question: To what extent does the autoregressive approach generalize to languages other than English?
- Basis in paper: [explicit] The authors explicitly note: "the language used in the dialogues from both datasets is exclusively English, meaning the experiments reported in this paper do not provide explicit insight into the extent to which the approach generalizes to other languages."
- Why unresolved: No multilingual evaluation was conducted; different languages may exhibit different patterns of reference, discourse structure, and mention density that affect model performance.
- What evidence would resolve it: Cross-lingual experiments on visually grounded dialogue datasets in other languages with comparable annotation protocols.

### Open Question 3
- Question: What is the optimal method for incorporating visual context to validate candidate mentions identified from linguistic context alone?
- Basis in paper: [explicit] The authors conclude: "Inevitably, a general solution to the problem will require a cross-modal approach... the visual information must somehow be incorporated to validate whether candidate mentions indeed have a referent in the visual context."
- Why unresolved: The paper deliberately uses a text-only approach to isolate linguistic contributions; cross-modal integration is identified as necessary but beyond the current scope.
- What evidence would resolve it: Comparative experiments combining the text-based mention detector with various visual encoding methods (e.g., CLIP-based image encoders, visual grounding models) to measure improvements in precision and recall.

### Open Question 4
- Question: How does performance degrade as visual contexts become more complex, dynamic, and less constrained?
- Basis in paper: [inferred] The paper notes that PB images with more complex scenes require more frequent consideration of bridging relationships, and states: "Discerning, from the linguistic context alone, whether an RE has such a referent becomes far more challenging, if not impossible, when the configuration of the visual context... is less constrained, more dynamic, and cannot be anticipated ahead of time."
- Why unresolved: Both datasets use static image sets known to participants in advance; real-world visually grounded dialogue involves dynamic, unpredictable visual environments.
- What evidence would resolve it: Evaluation on dialogue corpora collected in dynamic visual environments (e.g., embodied agents, video-grounded dialogue) with varying visual complexity.

## Limitations

- The text-only approach cannot handle nested mentions, limiting applicability to complex referential structures where one mention contains another.
- Cross-dataset transfer shows significant performance drops (0.12-0.16 F1) compared to within-dataset cross-validation, demonstrating limited generalization.
- The method cannot correct ungrammatical input or noise, potentially propagating errors from imperfect source material.

## Confidence

**High Confidence**: The core claim that linguistic context improves mention detection is well-supported by experimental results showing consistent F1 improvements with larger context windows (0→3 messages: +0.029 F1 on AGOS; 7→19: +0.002). The autoregressive constrained decoding mechanism is clearly specified and reproducible, with the vocabulary restriction approach directly documented in the method section. The parameter-efficient fine-tuning approach using QLoRA is standard practice and appropriately justified given dataset size constraints.

**Medium Confidence**: The claim that text-only approaches can achieve competitive performance relative to cross-modal methods is partially supported but limited by the evaluation scope. While the paper shows strong results on held-out data within datasets, the cross-dataset transfer experiments reveal significant domain sensitivity. The assumption that pretrained LLMs encode sufficient linguistic understanding of reference for this task is reasonable but not empirically validated beyond the specific datasets used.

**Low Confidence**: The generalizability of these findings to significantly different dialogue domains or languages not represented in base model pretraining remains untested. The paper does not provide evidence on how the approach would perform with substantially different utterance lengths, conversational styles, or non-English dialogue data.

## Next Checks

1. **Nested Mention Handling**: Implement a post-processing step to detect and correctly annotate nested mentions in AGOS dataset samples where this occurs, then measure impact on overall F1 scores. This would validate whether the single-span limitation is a practical constraint.

2. **Context Window Sensitivity Analysis**: Systematically vary context window sizes beyond the tested range (0, 3, 7, 19) to identify optimal window length and quantify the point of diminishing returns for different mention types (pronouns vs. full noun phrases).

3. **Cross-Domain Transfer Experiment**: Fine-tune the model on a subset of AGOS, then evaluate on PB-GOLD and a third, structurally different visually grounded dialogue dataset (if available) to quantify domain transfer limitations and identify specific failure patterns.