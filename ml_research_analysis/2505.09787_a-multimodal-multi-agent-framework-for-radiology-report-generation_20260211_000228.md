---
ver: rpa2
title: A Multimodal Multi-Agent Framework for Radiology Report Generation
arxiv_id: '2505.09787'
source_url: https://arxiv.org/abs/2505.09787
tags:
- agent
- arxiv
- report
- multi-agent
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a multimodal multi-agent framework for radiology\
  \ report generation (RRG) to address limitations in factual consistency, hallucination,\
  \ and cross-modal misalignment in existing MLLM-based approaches. The proposed framework\
  \ decomposes RRG into five specialized agents\u2014Retrieval, Draft, Refiner, Vision,\
  \ and Synthesis\u2014following stepwise clinical reasoning."
---

# A Multimodal Multi-Agent Framework for Radiology Report Generation

## Quick Facts
- arXiv ID: 2505.09787
- Source URL: https://arxiv.org/abs/2505.09787
- Reference count: 40
- Key result: Multi-agent framework achieves BLEU 0.0466, ROUGE-1 0.3652, and improved LLM evaluation scores over baseline

## Executive Summary
This paper introduces a multimodal multi-agent framework for radiology report generation (RRG) to address limitations in factual consistency, hallucination, and cross-modal misalignment in existing MLLM-based approaches. The proposed framework decomposes RRG into five specialized agents—Retrieval, Draft, Refiner, Vision, and Synthesis—following stepwise clinical reasoning. Evaluated on MIMIC-CXR and IU X-ray datasets, the framework achieves significant improvements over baseline in standard text metrics and LLM-based evaluation of diagnostic accuracy and stylistic alignment.

## Method Summary
The framework uses a five-agent pipeline: a Retrieval Agent fine-tunes CLIP on MIMIC-CXR to select similar prior reports, a Draft Agent generates an initial report from these references, a Refiner Agent distills key findings, a Vision Agent provides visual descriptions of chest X-rays using LLaVA-Med 1.5, and a Synthesis Agent integrates all outputs into a final structured report. The pipeline processes each chest X-ray through sequential agent calls with retrieval grounding enforced at multiple stages. Training uses MIMIC-CXR for retrieval fine-tuning and follows established dataset splits for evaluation.

## Key Results
- BLEU score: 0.0466 (vs. 0.0036 baseline)
- ROUGE-1: 0.3652 (vs. 0.2398 baseline)
- ROUGE-2: 0.1292 (vs. 0.0278 baseline)
- LLM evaluation shows improved diagnostic accuracy (8.26 vs. 7.78) and stylistic alignment (8.16 vs. 7.98)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing radiology report generation into specialized agents improves factual consistency and reduces hallucination compared to monolithic MLLM approaches.
- Mechanism: Each agent handles a bounded subtask (retrieval, drafting, refinement, vision, synthesis), constraining the output space and enabling retrieval-grounded verification at multiple stages. The Refiner Agent explicitly enforces that every sentence must be supported by input context, reducing speculation.
- Core assumption: Radiologist reasoning is approximately decomposable into distinct cognitive phases that can be mapped to modular agents without catastrophic information loss.
- Evidence anchors:
  - [abstract] "task-specific agents handle retrieval, draft generation, visual analysis, refinement, and synthesis"
  - [Section 3.3] "To ensure factality, the agent enforces retrieval-grounded constraints: every sentence must be clearly supported by the input, with no speculation"
  - [corpus] Related work (Zeng et al., 2024) reports similar findings that multi-agent systems enhance impression generation in radiology reports
- Break condition: If agent boundaries don't align with actual clinical reasoning structure, handoffs between agents may introduce information fragmentation or propagation of early errors.

### Mechanism 2
- Claim: CLIP-based cross-modal retrieval provides clinically relevant prior reports that serve as style and content templates, improving both lexical metrics and diagnostic accuracy.
- Mechanism: The Retrieval Agent encodes input X-ray images into visual embeddings and retrieves top-k semantically similar reports via cosine similarity. These reports provide: (1) domain-specific terminology, (2) report structure patterns, and (3) diagnostic context for similar presentations.
- Core assumption: Similar visual presentations in chest X-rays correlate with similar clinical findings and report structures.
- Evidence anchors:
  - [abstract] "Retrieval Agent uses a CLIP-based model to select similar prior reports"
  - [Section 3.1] "These reports provide relevant diagnostic context, such as clinical findings and report style"
  - [corpus] Multiple related papers leverage retrieval-augmented approaches for RRG, suggesting this is a broadly validated strategy
- Break condition: If retrieval fails to surface relevant cases (e.g., rare pathologies underrepresented in database), downstream agents lack useful priors, potentially degrading output quality below baseline.

### Mechanism 3
- Claim: Parallel vision-language processing with late synthesis combines image-grounded observations with text-derived findings more effectively than single-model joint encoding.
- Mechanism: The Vision Agent independently generates visual descriptions from the X-ray while textual agents process retrieved reports. The Synthesis Agent then integrates these complementary signals, explicitly filtering for observations supported by either visual or textual inputs while maintaining stylistic coherence.
- Core assumption: Independent processing of modalities followed by structured integration preserves more information than early fusion, and LLMs can reliably resolve conflicts between visual and textual signals.
- Evidence anchors:
  - [abstract] "Synthesis Agent integrates all outputs into a final structured report"
  - [Section 3.5] "The final report includes only observations explicitly supported by the textual or visual inputs"
  - [corpus] Limited direct corpus evidence for this specific integration pattern; assumption remains unvalidated across diverse pathologies
- Break condition: If visual and textual signals contain genuine contradictions (e.g., prior report describes resolved condition), synthesis may produce inconsistent outputs or suppress valid findings.

## Foundational Learning

- Concept: **Contrastive Language-Image Pre-training (CLIP)**
  - Why needed here: Understanding how the Retrieval Agent encodes images and text into shared embedding space for semantic similarity matching.
  - Quick check question: Can you explain why CLIP enables cross-modal retrieval without paired training data for every query?

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core pattern underlying how retrieved prior reports inform generation, and the trade-offs between retrieval coverage and noise.
  - Quick check question: What happens to output quality if you retrieve 1 report vs. 50 reports, and why?

- Concept: **Multi-Agent Orchestration**
  - Why needed here: Understanding agent communication patterns, prompt engineering for specialized roles, and failure propagation across agents.
  - Quick check question: If the Draft Agent produces a hallucinated finding, which downstream agents could catch vs. amplify this error?

## Architecture Onboarding

- Component map: Input X-ray → Retrieval Agent (CLIP encoder) → top-k reports → Draft Agent (GPT-4o) → Refiner Agent (GPT-4o) → Vision Agent (LLaVA-Med 1.5) → Synthesis Agent (GPT-4o) → final report

- Critical path: Retrieval → Draft → Refiner → Synthesis (Vision runs parallel, feeds Synthesis). Latency dominated by sequential LLM calls; retrieval is O(n) embedding lookup.

- Design tradeoffs:
  - **k=5 retrieval**: Balances context richness vs. noise. Paper doesn't ablate this.
  - **GPT-4o for text agents vs. LLaVA-Med for vision**: Commercial API dependency for text; open-weight for vision. Cost-latency tradeoff not quantified.
  - **Sequential vs. parallel agent execution**: Refiner depends on Draft output; Vision could run earlier but currently doesn't feed back to textual agents.

- Failure signatures:
  - Retrieved reports contain outdated or contradictory findings → Synthesis produces inconsistent reports (consistency score drops, as observed in Table 2: 6.74 vs. 6.94 baseline)
  - Vision Agent generates vague descriptions ("unchanged from prior" without prior access) → final report lacks specificity
  - Rare pathology with no similar cases in retrieval database → system defaults to generic template, may miss critical findings

- First 3 experiments:
  1. **Ablate retrieval (k=0)**: Run full pipeline without retrieved reports to isolate contribution of RAG vs. multi-agent decomposition.
  2. **Swap Synthesis Agent model**: Replace GPT-4o with smaller open model (e.g., Llama-3-8B) to measure quality degradation and establish cost-quality frontier.
  3. **Error propagation analysis**: Manually inject a false finding into Draft Agent output; trace which downstream agents catch vs. propagate the error. This validates the claimed interpretability benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the specific contribution of each specialized agent (Draft, Refiner, Vision, Synthesis) to the overall performance?
- Basis in paper: [explicit] The authors state in the Conclusion that "Future work includes a more systematic investigation, particularly through agent-level ablation."
- Why unresolved: The paper evaluates the framework as a whole against a baseline but does not isolate the performance impact of removing or altering individual agents.
- What evidence would resolve it: Results from experiments where agents are removed one by one (ablation study) to quantify their individual necessity and effectiveness.

### Open Question 2
- Question: Can the framework maintain high diagnostic accuracy while simultaneously improving textual consistency?
- Basis in paper: [explicit] The Discussion notes a "slight drop in consistency compared to the baseline" (6.74 vs. 6.94) despite gains in other metrics, attributing this to RAG complexity.
- Why unresolved: The current Synthesis Agent struggles to fully integrate retrieved texts and visual descriptions without introducing redundancy or irrelevant content that lowers consistency scores.
- What evidence would resolve it: A modified synthesis mechanism that achieves consistency scores statistically significantly higher than the baseline without reducing diagnostic accuracy.

### Open Question 3
- Question: What is the optimal number of retrieved reports ($k$) to maximize clinical grounding while minimizing noise?
- Basis in paper: [inferred] The method sets a default $k=5$, but the Introduction acknowledges that retrieving "too many can introduce noise and redundancy."
- Why unresolved: The paper does not perform a sensitivity analysis on the retrieval parameter $k$, leaving the trade-off between context coverage and information overload unexplored.
- What evidence would resolve it: Performance curves across metrics (e.g., BERTScore, Consistency) when varying $k$ (e.g., $k=1$ to $k=10$).

## Limitations

- Prompt engineering dependency creates significant uncertainty about reproducibility and sensitivity to small changes in agent instructions.
- Retrieval quality assumptions may fail for rare pathologies where database coverage is limited.
- Model selection constraints create API dependency and don't establish cost-quality tradeoffs.

## Confidence

- High confidence: The modular architecture approach (five specialized agents) is sound and the BLEU/ROUGE improvements over baseline are statistically robust given the large metric gaps (0.0466 vs 0.0036 BLEU, 0.3652 vs. 0.2398 ROUGE-1).
- Medium confidence: Claims about improved factual consistency and reduced hallucination are supported by LLM evaluation metrics but would benefit from radiologist validation on actual clinical accuracy.
- Low confidence: The assertion that independent vision-language processing followed by late synthesis is superior to joint encoding lacks direct comparative evidence within this paper.

## Next Checks

1. **Retrieval ablation study**: Systematically vary k=1,3,5,10 retrieved reports and measure impact on final output quality. This will reveal whether 5 reports is optimal or if performance plateaus/declines with more context.

2. **Error propagation analysis**: Inject controlled hallucinations into Draft Agent output and trace which downstream agents catch vs. propagate errors. This validates the interpretability and factuality claims.

3. **Clinical expert evaluation**: Have radiologists rate a subset of generated reports for diagnostic accuracy and clinical utility, comparing against both baseline models and human-written reports. This addresses whether LLM evaluation correlates with actual clinical value.