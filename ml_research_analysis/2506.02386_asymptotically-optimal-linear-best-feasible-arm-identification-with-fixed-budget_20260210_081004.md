---
ver: rpa2
title: Asymptotically Optimal Linear Best Feasible Arm Identification with Fixed Budget
arxiv_id: '2506.02386'
source_url: https://arxiv.org/abs/2506.02386
tags:
- lemma
- algorithm
- then
- best
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the best feasible arm identification problem
  under fixed budget in linear bandits, where arms are constrained by linear inequalities
  and the goal is to identify the optimal arm within the feasible set. The authors
  propose BLFAIPS, a posterior sampling-based algorithm that integrates a min-learner
  (posterior sampling with constraint-aware sampling) and a max-learner (AdaHedge
  with a novel loss function) in a game-theoretic framework.
---

# Asymptotically Optimal Linear Best Feasible Arm Identification with Fixed Budget

## Quick Facts
- **arXiv ID:** 2506.02386
- **Source URL:** https://arxiv.org/abs/2506.02386
- **Authors:** Jie Bian; Vincent Y. F. Tan
- **Reference count:** 40
- **Primary result:** First algorithm achieving asymptotic optimality in linear best feasible arm identification under fixed budget

## Executive Summary
This paper addresses the best feasible arm identification problem in linear bandits, where the goal is to identify the optimal arm within a feasible set defined by linear inequality constraints. The authors propose BLFAIPS, a posterior sampling-based algorithm that uses a game-theoretic framework involving a min-learner and a max-learner. The algorithm achieves asymptotic optimality by matching the theoretical lower bound on the exponential decay rate of error probability, making it the first to do so in this setting.

## Method Summary
BLFAIPS operates by framing the sampling process as a zero-sum game between a "learner" (max-player) and an "adversary" (min-player). The max-learner (AdaHedge) selects a sampling distribution over arms to maximize distinguishability between true parameters and alternatives, while the min-learner (Posterior Sampling) selects the hardest alternative parameters that minimize this distinguishability. This interactive dynamic drives sampling allocation toward the optimal theoretical limit. The algorithm uses AdaHedge to adaptively tune the exploration-exploitation balance without manual hyperparameter tuning, and employs posterior sampling to efficiently target competitor parameters that are hardest to distinguish from the truth.

## Key Results
- BLFAIPS is the first algorithm to achieve asymptotic optimality in linear best feasible arm identification
- The algorithm matches the theoretical lower bound on the exponential decay rate of error probability
- Empirical results on synthetic and real-world datasets demonstrate superior performance compared to baselines
- The algorithm converges faster and achieves higher accuracy under varying dimensions and budget constraints

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Achieves asymptotic optimality through a zero-sum game between max-learner and min-learner
- **Mechanism:** The max-learner (AdaHedge) maximizes distinguishability between true parameters and alternatives, while the min-learner (Posterior Sampling) selects hardest alternative parameters that minimize distinguishability
- **Core assumption:** Problem hardness can be expressed as max-min optimization and noise is Gaussian
- **Evidence anchors:** Abstract mentions "game-based sampling rule involving a min-learner and a max-learner"; Section 3 states algorithm is "guided by a game-theoretic interpretation"

### Mechanism 2
- **Claim:** Posterior sampling targets hardest-to-distinguish competitor parameters
- **Mechanism:** Min-learner samples parameters from posterior conditioned on set where current empirical best arm is not optimal, focusing pulls on arms that best rule out alternatives
- **Core assumption:** Posterior distribution is well-calibrated enough that samples effectively represent plausible "confusing" realities
- **Evidence anchors:** Section 3 specifies sampling from "alternative" set; Algorithm 1 Line 12 defines constrained sampling step

### Mechanism 3
- **Claim:** AdaHedge enables adaptive tuning without manual hyperparameter tuning
- **Mechanism:** Max-learner uses AdaHedge to update weights based on novel loss function, adapting learning rate based on mixability gap for stability and convergence
- **Core assumption:** Loss function accurately captures negative information gain and mixability gap ensures stable learning rate
- **Evidence anchors:** Section 1.4 mentions replacing exponential weights with AdaHedge to eliminate doubling trick; Algorithm 1 Lines 21-24 detail AdaHedge update logic

## Foundational Learning

- **Concept:** Linear Bandits & Ridge Regression
  - **Why needed here:** Entire framework assumes rewards and costs are linear functions of arm features; must understand how parameters are estimated from history using Ridge Regression to interpret posterior and exploration mechanism
  - **Quick check question:** Given history of arm vectors X and rewards Y, how do you compute Ridge estimate θ̂ and covariance matrix V⁻¹?

- **Concept:** Constrained Optimization & Feasibility
  - **Why needed here:** Core problem is identifying best feasible arm; need to understand linear inequality constraints and how they partition arm space into feasible and infeasible sets
  - **Quick check question:** If arm z has high expected reward but expected cost > τ, is it the "best feasible arm"?

- **Concept:** Minimax Theorem (Game Theory)
  - **Why needed here:** Algorithm's theoretical guarantee relies on equivalence between max-min and min-max formulations, justifying dual-learner architecture
  - **Quick check question:** In zero-sum game, if max-player finds strategy with value V, what is value for min-player using best response?

## Architecture Onboarding

- **Component map:** Environment -> Estimator -> Min-Learner -> Loss Calculator -> Max-Learner -> Sampler
- **Critical path:**
  1. Constraint Set Construction: Correctly defining set where current best arm is not optimal (Lines 6-11)
  2. Constrained Sampling: Efficiently sampling from truncated Gaussian posterior (Line 12)
  3. Loss Update: Calculating specific quadratic loss for AdaHedge (Line 18)

- **Design tradeoffs:**
  - AdaHedge vs. Standard Exponential Weights: AdaHedge removes need for doubling trick, improving anytime performance but adding computational overhead
  - Posterior Variance η: Setting η based on noise bounds is critical; incorrect values break theoretical guarantees

- **Failure signatures:**
  - Stalling (0% accuracy): Empirical feasible set becomes empty early; check fallback to uniform sampling and initialization
  - Oscillation: Best empirical arm flips rapidly between two arms; indicates hard problem instance with small gap
  - Loss Explosion: AdaHedge weights become deterministic too quickly before estimates are accurate

- **First 3 experiments:**
  1. Toy Linear System (d=2): Verify "End of Optimism" instance; check if algorithm correctly identifies feasible arm among superoptimal and suboptimal arms
  2. Ablation on Max-Learner: Replace AdaHedge with fixed learning rate Hedge; confirm performance drops without tuning
  3. Constraint Tightness: Vary cost threshold τ; observe behavior as best feasible arm moves closer to boundary

## Open Questions the Paper Calls Out

- **Open Question 1:** Can asymptotic optimality be extended to non-convex or high-dimensional parameter spaces?
  - Basis: Conclusion states "extending these results to more general settings, such as non-convex or high-dimensional parameter spaces, remains valuable"
  - Why unresolved: Current proofs rely on bounded and closed parameter spaces and specific Laplace approximations
  - What evidence would resolve it: Proving matching upper and lower bounds for error exponent in non-convex or high-dimensional regimes

- **Open Question 2:** Can theoretical guarantees be maintained under weaker noise assumptions like sub-Gaussian or heavy-tailed distributions?
  - Basis: Conclusion suggests "refining guarantees under weaker assumptions, especially regarding noise... could further broaden applicability"
  - Why unresolved: Current analysis relies on Gaussian likelihoods and priors to derive posterior concentration
  - What evidence would resolve it: Derivation of lower bound and algorithm convergence using only moment bounds rather than exact distributional forms

- **Open Question 3:** Is it possible to identify best feasible arm optimally when feasibility threshold τ is unknown or when multiple optimal arms exist?
  - Basis: Problem setup assumes τ is known and enforces unique best arm; lifting these constraints is standard theoretical extension
  - Why unresolved: Algorithm constructs set based on known threshold τ, and proof relies on uniqueness of optimum
  - What evidence would resolve it: Algorithm that simultaneously estimates threshold or identifies set of optimal arms while preserving exponential decay rate

## Limitations
- Truncated Gaussian sampling step lacks implementation details, potentially requiring specialized MCMC or rejection sampling methods
- AdaHedge loss function's numerical stability with large initial parameter uncertainties is not addressed
- Theoretical guarantees assume Gaussian noise, but real-world applications may have heavy-tailed or bounded noise

## Confidence

- **High confidence:** Game-theoretic formulation and connection to lower bound (Sections 3-4)
- **Medium confidence:** AdaHedge adaptation and theoretical guarantees for specific loss function
- **Medium confidence:** Empirical results, though limited to synthetic and one real-world dataset

## Next Checks

1. Reproduce algorithm on "End of Optimism" instance (d=2) to verify core mechanism works in simplest non-trivial case
2. Implement truncated posterior sampling and measure acceptance rates across different problem instances to validate computational feasibility
3. Conduct ablation studies comparing BLFAIPS with standard TS with constraint checking, BLFAIPS without AdaHedge, and theoretically optimal allocation when gap Γ is known