---
ver: rpa2
title: Emerging Semantic Segmentation from Positive and Negative Coarse Label Learning
arxiv_id: '2508.18186'
source_url: https://arxiv.org/abs/2508.18186
tags:
- coarse
- annotations
- segmentation
- label
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training segmentation models
  using coarse annotations, which are faster and cheaper to produce than pixel-level
  labels but often contain noise. The authors propose a method that uses both positive
  (target) and negative (background) coarse annotations, even when they contain noisy
  pixels, to train a convolutional neural network (CNN) for semantic segmentation.
---

# Emerging Semantic Segmentation from Positive and Negative Coarse Label Learning

## Quick Facts
- arXiv ID: 2508.18186
- Source URL: https://arxiv.org/abs/2508.18186
- Reference count: 24
- Primary result: Method achieves 68.3% mIoU on Cityscapes and 71.6% on LES-AV, outperforming WSL and SSL baselines

## Executive Summary
This paper introduces a novel approach for semantic segmentation using noisy coarse annotations that combine both positive (target) and negative (background) labels. The method employs two coupled convolutional neural networks: one estimates true segmentation probabilities while the other models the characteristics of noisy annotations through pixel-wise confusion matrices. The approach also incorporates complementary label learning to encourage estimating the negative label distribution. Experimental results demonstrate significant improvements over state-of-the-art weakly-supervised and semi-supervised methods across MNIST, Cityscapes, and LES-AV retinal image datasets, particularly when the ratio of coarse annotations is small compared to dense annotations.

## Method Summary
The proposed method uses two coupled CNNs trained jointly: a segmentation network that predicts true label probabilities and a coarse annotation network that estimates pixel-wise confusion matrices. The framework processes both positive and negative coarse annotations through a combined loss function that includes cross-entropy terms and trace regularization. The trace minimization encourages the estimated confusion matrices to represent maximally unreliable annotators while maintaining fidelity with observed noisy annotations. The method also employs a transition matrix with zeros on the diagonal to incorporate complementary label learning, effectively doubling the supervisory signal and providing constraints on what true labels cannot be. Training is performed via stochastic gradient descent with synthetic coarse annotations generated through morphological operations on MNIST images.

## Key Results
- On MNIST, achieves 82.5% mIoU compared to 77.2% for positive annotations only
- On Cityscapes, achieves 68.3% mIoU, outperforming state-of-the-art WSL and SSL methods
- On LES-AV retinal dataset, achieves 71.6% mIoU comparable to strongly-supervised approaches
- Performance improvements are most pronounced when coarse annotation ratios are small

## Why This Works (Mechanism)

### Mechanism 1: Confusion Matrix Regularization for Noise-True Label Disentanglement
The method estimates pixel-wise confusion matrices with trace minimization to separate systematic annotation noise from true segmentation labels. The trace regularization creates optimization tension that encourages the estimated confusion matrices to represent maximally unreliable annotators while the cross-entropy term ensures fidelity to observed annotations. This constrained optimization forces the model to explain discrepancies through learned noise patterns rather than simply memorizing them, enabling disentanglement of true labels from annotation artifacts.

### Mechanism 2: Complementary Label Learning via Transition Matrix Transformation
The method incorporates negative/background coarse annotations through a structured transition matrix with zeros on its diagonal, encoding that a pixel marked as class i in the negative annotation cannot belong to class i. By transforming predictions through M^⊤, the model learns to infer positive labels from complementary annotations, effectively doubling the supervisory signal and providing constraints on what true labels cannot be.

### Mechanism 3: Coupled Network Joint Optimization
The two CNNs are coupled through the loss function, where segmentation predictions are transformed by confusion matrices before comparison with noisy annotations. This creates a feedback loop: improved noise modeling enables better segmentation signal extraction, while better segmentation provides cleaner targets for noise pattern estimation. The joint optimization landscape contains reachable solutions where both networks converge to useful states without collapsing to trivial solutions.

## Foundational Learning

- **Confusion Matrix in Multi-Class Settings**
  - Why needed here: The method's core innovation relies on modeling annotation noise through pixel-wise confusion matrices; understanding how CM entries represent class-wise error rates is essential for interpreting the regularization.
  - Quick check question: In a confusion matrix A^(o)_φ(x) for a 3-class segmentation problem, what does entry A_{ij} represent, and why would minimizing the trace encourage "unreliable annotator" behavior?

- **Transition Matrices and Complementary Label Learning**
  - Why needed here: The negative annotation pathway uses transition matrix M with structural zeros on the diagonal; understanding this constraint is critical for debugging the complementary learning channel.
  - Quick check question: Why must the transition matrix have m_ii = 0 for all classes, and how does multiplying by M^⊤ transform a complementary label prediction into a positive label estimate?

- **Weakly-Supervised vs. Semi-Supervised Segmentation Paradigms**
  - Why needed here: The paper positions itself against both WSL and SSL baselines; distinguishing these paradigms helps contextualize when this method applies versus alternatives.
  - Quick check question: Given a dataset with 16 pixel-level masks and 4 coarse annotations (Table 2), would you classify the "semi" configuration as weakly-supervised, semi-supervised, or both? Why?

## Architecture Onboarding

- **Component map:**
Input Image (x)
    │
    ├──► Segmentation Network (θ) ──► ˆp_θ(x) [true label probabilities]
    │                                       │
    │                                       ├──► × ˆA^(o)_φ(x) ──► L_obj (positive pathway)
    │                                       │
    │                                       └──► × ˆA^(c)_φ(x) ──► × M^⊤ ──► L_comp (negative pathway)
    │                                               ▲
    └──► Coarse Annotation Network (φ) ─────────────┘
         Outputs: {ˆA^(o)_φ(x), ˆA^(c)_φ(x)} [per-image confusion matrices]

- **Critical path:**
  1. Forward pass through segmentation network produces ˆp_θ(x)
  2. Coarse annotation network generates image-specific confusion matrices
  3. Matrix multiplication transforms predictions to annotation space
  4. Loss computation against coarse labels with trace regularization
  5. Joint backpropagation updates both θ and φ

- **Design tradeoffs:**
  - **Network capacity split:** More parameters in coarse annotation network may improve noise modeling but reduce segmentation capacity
  - **Trace regularization weight (λ):** Higher values push toward noise attribution; lower values risk overfitting to noisy annotations
  - **Annotation type balance:** Relying solely on positive annotations (ablation) reduces supervision but simplifies training

- **Failure signatures:**
  - **Identity confusion matrix collapse:** ˆA_φ(x) → I means model memorizes noise directly (check trace values during training)
  - **Degenerate predictions:** All predictions converge to single class (may indicate transition matrix instability)
  - **Negative pathway divergence:** L_comp increases while L_obj decreases suggests conflicting gradients (verify M^⊤ computation)

- **First 3 experiments:**
  1. **Ablation validation:** Train without negative annotation pathway (L_obj only) to isolate complementary learning contribution; expect performance drop per Table 1 results (77.2% → 82.5% on MNIST)
  2. **Trace sensitivity analysis:** Sweep λ values {0.01, 0.1, 1.0, 10.0} on validation set to identify regularization sweet spot; monitor confusion matrix trace values for collapse
  3. **Annotation quality stress test:** Systematically reduce coarse annotation coverage (level-1 to level-5 as in Fig. 3a) to establish minimum viable annotation density for stable training

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can imposing structural priors on the estimated confusion matrices (CMs) and transition matrices (TMs) effectively extend this framework to extremely sparse annotations, such as scribbles or spots?
- **Basis in paper:** [explicit] The conclusion states that future work "shall consider imposing structures on the CMs and TMs to broaden the applicability to scribble or spot annotations."
- **Why unresolved:** The current method is validated on "coarse drawings" which contain significantly more pixel information than sparse scribbles; without structural constraints, the matrix estimation may be under-constrained or unstable with sparser inputs.
- **What evidence would resolve it:** Successful convergence and competitive mIoU results in experiments where the training data is restricted to sparse scribble or point-click inputs rather than coarse masks.

### Open Question 2
- **Question:** Can promptable foundation models be integrated into this framework to enhance flexibility and reduce annotation burdens?
- **Basis in paper:** [explicit] The conclusion identifies the exploration of "promptable foundation models" as a promising direction to handle various input forms and improve upon the current CNN-based architecture.
- **Why unresolved:** It is currently unknown how the method's specific mechanism for modeling annotation noise (via coupled CNNs) would interact with the pre-trained embeddings or prompting mechanisms of large foundation models.
- **What evidence would resolve it:** A study implementing the noise-modeling head on top of a foundation model backbone, comparing performance and annotation efficiency against the current "two coupled CNNs" approach.

### Open Question 3
- **Question:** Is the regularization of confusion matrices effective against systematic annotation bias, or is it limited to the random morphological noise simulated in the paper?
- **Basis in paper:** [inferred] The paper validates the method using Morpho-MNIST (simulating random fractures/thinning) and minimizes the trace of the CM to encourage "maximally unreliable" annotators.
- **Why unresolved:** Real-world coarse annotations often suffer from systematic bias (e.g., consistent boundary shifts) rather than random pixel noise; the current regularization might struggle to disentangle a consistent bias from the true signal.
- **What evidence would resolve it:** Ablation studies using synthetically biased training sets (e.g., consistently dilated or shifted masks) to verify if the estimated confusion matrices successfully correct for systematic errors.

## Limitations

- The method assumes annotation noise follows systematic, learnable patterns rather than random corruption, which may not hold for crowdsourced annotations or adversarial noise
- The transition matrix formulation depends heavily on annotator reliability and labeling protocols, as it assumes complementary labels provide consistent negative evidence
- LES-AV dataset claims have low statistical reliability due to the extremely small test set of only 4 images

## Confidence

- **High confidence**: MNIST experimental results (controlled synthetic noise environment with clear performance improvements)
- **Medium confidence**: Cityscapes results (complex real-world data but limited comparison against SSL methods)
- **Low confidence**: LES-AV dataset claims (extremely small test set of 4 images makes mIoU comparisons statistically unreliable)

## Next Checks

1. **Noise pattern analysis**: Verify that learned confusion matrices capture meaningful systematic noise patterns rather than overfitting to dataset-specific artifacts by testing on held-out annotation styles
2. **Scalability assessment**: Evaluate memory and computational requirements for per-pixel confusion matrix estimation on high-resolution images beyond the LES-AV scale
3. **Annotation coverage sensitivity**: Systematically quantify the minimum percentage of image area requiring positive and negative annotations for stable training across different noise levels