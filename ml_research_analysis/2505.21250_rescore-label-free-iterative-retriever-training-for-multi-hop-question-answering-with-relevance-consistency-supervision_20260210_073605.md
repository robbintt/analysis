---
ver: rpa2
title: 'ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering
  with Relevance-Consistency Supervision'
arxiv_id: '2505.21250'
source_url: https://arxiv.org/abs/2505.21250
tags:
- question
- documents
- answer
- rescore
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReSCORE, a novel method for training dense
  retrievers for multi-hop question answering without labeled documents. ReSCORE leverages
  large language models to generate pseudo-ground-truth labels by measuring the joint
  probability of question relevance and answer consistency, then uses these labels
  to train the retriever via KL divergence loss within an iterative retrieval-augmented
  generation framework.
---

# ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision

## Quick Facts
- **arXiv ID:** 2505.21250
- **Source URL:** https://arxiv.org/abs/2505.21250
- **Reference count:** 11
- **Primary result:** ReSCORE achieves state-of-the-art results on MuSiQue, HotpotQA, and 2WikiMHQA by training retrievers without labeled documents using LLM-generated pseudo-ground-truth labels.

## Executive Summary
ReSCORE introduces a novel method for training dense retrievers in multi-hop question answering without requiring labeled documents. The approach leverages large language models to generate pseudo-ground-truth labels by measuring the joint probability of question relevance and answer consistency. These pseudo-labels are then used to train the retriever via KL divergence loss within an iterative retrieval-augmented generation framework. Experiments demonstrate that ReSCORE significantly improves both retrieval quality and end-to-end QA performance compared to existing iterative RAG systems, while also showing broad applicability across different iterative RAG frameworks.

## Method Summary
ReSCORE trains dense retrievers for multi-hop question answering by generating pseudo-ground-truth labels through LLM probability computations. The method computes P_LM(q,a|d) = P_LM(q|d) · P_LM(a|q,d) over sampled documents, capturing both relevance and consistency. The retriever is trained to match this distribution using KL divergence loss while iteratively reformulating queries and retrieving documents. The approach uses a Contriever base model with frozen document embeddings and trained query embeddings, operating within an iterative RAG framework where the LLM generates answers, thoughts, and reformulated queries until a non-"unknown" answer is produced or maximum iterations are reached.

## Key Results
- ReSCORE achieves MHR@8 scores of 51.2→81.2→88.0 across iterations on 2WikiMHQA, surpassing GT-label training which plateaus at 61.7
- QA performance improvements: EM/F1 gains of 1.9/3.2 on MuSiQue, 1.2/2.5 on HotpotQA, and 2.8/5.2 on 2WikiMHQA
- Joint relevance-consistency scoring improves retrieval by 14.4% vs. individual components alone
- Method generalizes across multiple iterative RAG frameworks (FLARE, Adaptive-Note, Self-RAG)

## Why This Works (Mechanism)

### Mechanism 1: Joint Relevance-Consistency Scoring Filters False Positives
The pseudo-ground-truth distribution Q_LM(d|q) ∝ P_LM(q,a|d) factorizes into relevance (P_LM(q|d)) and consistency (P_LM(a|q,d)). The relevance term down-weights documents with superficial answer overlap; the consistency term filters out topically relevant but answer-uninformative documents. Evidence shows that answer probability alone degrades recall by 23.8% on average, while joint probability improves by 14.4%.

### Mechanism 2: Iterative Training Enables Progressive Document Discovery
Training retrievers iteratively allows progressive retrieval of complementary evidence rather than forcing alignment with all ground-truth documents simultaneously. At each iteration i, the retriever is trained only on documents relevant to the current reformulated query q^(i), avoiding embedding conflicts. Results show MHR@8 scores increase with iterations (51.2→81.2→88.0) while GT-label training plateaus.

### Mechanism 3: KL Divergence Distills LLM Retrieval Preferences into Dense Retriever
Minimizing KL divergence between retriever distribution P_R and LLM-derived distribution Q_LM transfers retrieval preferences without requiring labeled data. The LLM computes Q_LM over a sampled subset of M documents per iteration, and the retriever's softmax distribution is trained to match via D_KL(Q_LM || P_R). This effectively distills the LLM's relevance-consistency judgments into the embedding space.

## Foundational Learning

- **Dense Retrieval with Dual Encoders:** Essential for understanding how ReSCORE trains the query encoder while keeping document embeddings frozen; quick check: what does a high dot product indicate between query and document embeddings?
- **KL Divergence as Distribution Matching Loss:** Critical for understanding the core training objective; quick check: what happens when P assigns low probability to events where Q assigns high probability in D_KL(Q||P)?
- **Multi-Hop Question Answering and Reasoning Chains:** Necessary for understanding how the iterative RAG framework decomposes complex questions; quick check: for "Which city was the director of the film Parasite born?", what are the two hops and required documents?

## Architecture Onboarding

- **Component map:** Question -> Retriever (Contriever) -> Top-k Documents -> LLM -> Answer/Thought/Reformulated Query -> Pseudo-GT Generator -> KL Loss -> Query Encoder Update
- **Critical path:** Question enters as q^(1), retriever returns D^(1), LLM predicts answer or generates thought t^(1) and q^(2), pseudo-GT probabilities computed for top-M documents, KL loss accumulated, repeat until answer or max iterations, backpropagate cumulative loss
- **Design tradeoffs:** M=32 sampling reduces LLM compute cost but may miss relevant documents; freezing document encoder reduces memory but limits adaptation; thought-concat vs LLM-rewrite affects query reformulation quality
- **Failure signatures:** Plateauing MHR@k indicates retriever not learning complementary discovery; degraded performance vs BM25 suggests Contriever issues; high false-positive retrieval indicates relevance term underweighted
- **First 3 experiments:** 1) Ablate pseudo-GT components (relevance-only, consistency-only, joint); 2) Compare GT vs pseudo-GT labels on validation subset; 3) Test query reformulation strategies (None, LLM-rewrite, Thought-concat) on held-out data

## Open Questions the Paper Calls Out

- **Generalization to new domains:** The model is tuned to specific datasets and its ability to generalize to other datasets with different reasoning patterns remains limited, highlighting an OOD generalization challenge.
- **Computational cost reduction:** The iterative retrieval process increases computational costs and latency, particularly for questions with high hop requirements, which may be prohibitive in practical applications.
- **Hybrid training strategy:** Section 4.2.3 shows GT labels achieve higher initial retrieval rates while ReSCORE is better at cumulative retrieval, suggesting potential for combining both approaches.

## Limitations

- The method's performance depends heavily on the quality of LLM-generated pseudo-labels, which may not perfectly align with human judgments
- Sampling only M=32 documents per iteration may miss relevant documents, limiting the retriever's exposure to diverse evidence
- Freezing the document encoder while training the query encoder could create misalignment between query and document embeddings over time

## Confidence

- **High Confidence:** Empirical improvements in retrieval quality (MHR@8 increases) and QA performance (EM/F1 gains) are well-supported by experimental results
- **Medium Confidence:** The claim that iterative training enables progressive document discovery is supported by trends but needs more rigorous validation of the underlying mechanism
- **Low Confidence:** The assertion that KL divergence optimally distills LLM preferences lacks ablation studies comparing alternative distillation methods

## Next Checks

1. **Probe LLM pseudo-label quality:** Conduct human evaluation of LLM's relevance and consistency judgments on a random sample of documents to quantify alignment with human ratings
2. **Analyze embedding space evolution:** Visualize query and document embeddings before and after ReSCORE training using t-SNE or UMAP to verify learned separation
3. **Test retriever generalization to new domains:** Evaluate ReSCORE-trained retrievers on out-of-domain MHQA datasets to validate generalization beyond training distribution