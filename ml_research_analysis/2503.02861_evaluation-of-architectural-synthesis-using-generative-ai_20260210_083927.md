---
ver: rpa2
title: Evaluation of Architectural Synthesis Using Generative AI
arxiv_id: '2503.02861'
source_url: https://arxiv.org/abs/2503.02861
tags:
- architectural
- design
- parts
- step
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comparative evaluation of two multimodal
  generative AI systems, GPT-4o and Claude 3.5 Sonnet, for architectural 3D synthesis.
  Using two Palladian buildings as case studies, the authors assess the systems' ability
  to interpret architectural drawings and generate CAD scripts in OpenSCAD.
---

# Evaluation of Architectural Synthesis Using Generative AI

## Quick Facts
- arXiv ID: 2503.02861
- Source URL: https://arxiv.org/abs/2503.02861
- Authors: Jingfei Huang; Alexandros Haridis
- Reference count: 4
- Primary result: Claude 3.5 Sonnet outperformed GPT-4o in generating accurate 3D architectural models from drawings

## Executive Summary
This paper presents a comparative evaluation of two multimodal generative AI systems, GPT-4o and Claude 3.5 Sonnet, for architectural 3D synthesis. Using two Palladian buildings as case studies, the authors assess the systems' ability to interpret architectural drawings and generate CAD scripts in OpenSCAD. The evaluation methodology involves sequential text and image prompting, with a binary scoring matrix assessing part generation, positioning, proportion, orientation, and spatial relationships. Results show Claude 3.5 Sonnet achieved 73.12% average performance for Villa Rotonda and 73.43% for Palazzo Porto, outperforming GPT-4o's 61.56% and 65% respectively.

While both systems successfully generated individual parts with correct positions and orientations, they struggled to accurately assemble parts into desired spatial relationships, highlighting limitations in spatial reasoning. Claude 3.5 demonstrated better self-correction capabilities. The study contributes to understanding AI capabilities in architectural design tasks and suggests potential for AI as a collaborative technical assistant in early-stage 3D modeling.

## Method Summary
The authors evaluated two multimodal generative AI systems using a binary scoring matrix across five categories: part generation, part positioning, proportion, orientation, and spatial relationships. They used sequential text and image prompting to generate OpenSCAD scripts for two Palladian buildings. The evaluation was performed through three trials per system, with the best result selected for final scoring. Claude 3.5 Sonnet was tested with and without self-correction capabilities enabled.

## Key Results
- Claude 3.5 Sonnet achieved 73.12% average performance for Villa Rotonda and 73.43% for Palazzo Porto
- GPT-4o achieved 61.56% and 65% respectively for the same buildings
- Both systems successfully generated individual parts with correct positions and orientations
- Both systems struggled with assembling parts into correct spatial relationships

## Why This Works (Mechanism)
The evaluation methodology leverages the sequential prompting approach to progressively refine AI-generated architectural models. By combining text and image inputs, the systems can interpret architectural drawings and translate them into parametric CAD scripts. The binary scoring matrix provides objective metrics for comparing performance across multiple dimensions of architectural synthesis.

## Foundational Learning
- Multimodal generative AI capabilities - why needed: To understand AI systems' ability to process both text and visual inputs for architectural tasks; quick check: Compare performance across different input modalities
- Spatial reasoning in AI systems - why needed: To identify fundamental limitations in how AI models understand 3D relationships; quick check: Test with progressively complex spatial arrangements
- OpenSCAD parametric modeling - why needed: To provide a standardized format for evaluating architectural synthesis; quick check: Verify generated scripts produce intended geometries

## Architecture Onboarding
- Component map: Input drawings → Multimodal AI processing → CAD script generation → OpenSCAD rendering → Binary scoring
- Critical path: Drawing interpretation → Part generation → Spatial assembly → Proportion/orientation accuracy
- Design tradeoffs: Binary scoring simplicity vs. nuanced architectural quality assessment
- Failure signatures: Correct individual parts but incorrect spatial relationships; orientation issues with complex geometries
- First experiments: 1) Test with single-part architectural elements, 2) Evaluate self-correction capability differences, 3) Compare performance across different architectural styles

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of only two Palladian buildings limits generalizability
- Binary scoring matrix may oversimplify complex architectural synthesis tasks
- Both systems showed fundamental limitations in spatial reasoning capabilities

## Confidence
- Comparative performance results: Medium confidence
- Findings regarding Claude 3.5's self-correction capabilities: High confidence within evaluated context

## Next Checks
1. Cross-style validation: Test both systems with non-Palladian architectural styles to assess whether the performance differential holds across different architectural vocabularies
2. Human expert evaluation: Supplement binary scoring with qualitative assessments from professional architects
3. Progressive complexity testing: Evaluate performance on increasingly complex multi-part architectural assemblies to identify specific thresholds where spatial reasoning failures become more pronounced