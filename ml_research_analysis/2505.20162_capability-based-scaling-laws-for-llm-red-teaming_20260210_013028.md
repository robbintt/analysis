---
ver: rpa2
title: Capability-Based Scaling Laws for LLM Red-Teaming
arxiv_id: '2505.20162'
source_url: https://arxiv.org/abs/2505.20162
tags:
- uni00000014
- uni00000012
- uni00000016
- uni00000018
- uni0000001c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) are becoming increasingly capable
  and agentic, raising the importance of red-teaming for safe deployment. Traditional
  prompt-engineering approaches may become ineffective as models surpass red-teamers
  in capabilities, turning red-teaming into a weak-to-strong problem.
---

# Capability-Based Scaling Laws for LLM Red-Teaming

## Quick Facts
- **arXiv ID:** 2505.20162
- **Source URL:** https://arxiv.org/abs/2505.20162
- **Reference count:** 40
- **Primary result:** Attack success rates follow sigmoid scaling laws based on attacker-target capability gap, with models excelling at social-science reasoning making better attackers.

## Executive Summary
Large language models are becoming increasingly capable and agentic, making red-teaming essential for safe deployment. This paper frames red-teaming as a capability gap problem between attacker and target models. Through evaluation of over 600 attacker-target pairs using LLM-based jailbreak attacks, the authors demonstrate that attack success follows predictable scaling laws: more capable models are better attackers, success drops sharply when targets exceed attackers in capability, and attack effectiveness correlates with social-science benchmark performance. These findings suggest fixed-capability attackers (like humans) may become ineffective against future models, and model providers must accurately measure and control models' persuasive abilities.

## Method Summary
The authors evaluated 600+ attacker-target model pairs using four jailbreak attack methods (PAIR, TAP, PAP, Crescendo) on HarmBench behaviors. Attacker models from Llama2, Llama3, Vicuna, Mistral, and Qwen2.5 families were "unlocked" via LoRA fine-tuning on harmful datasets to remove safety refusals. Attack success was measured as ASR@25 (best-of-25) with judge evaluation. Capability was proxied by MMLU-Pro scores. The key analysis involved fitting a linear model in logit space to predict attack success from the capability gap between attacker and target.

## Key Results
- Attack success rates follow a consistent sigmoid-like curve across model families, declining predictably as capability gap increases
- Attack success correlates most strongly with attacker performance on social-science splits of MMLU-Pro, not technical STEM splits
- Fixed-capability attackers (humans or smaller models) are predicted to become increasingly ineffective against future, more capable models

## Why This Works (Mechanism)

### Mechanism 1
Attack success rates are governed by relative capability gap, not absolute attacker capability. The paper models ASR as a sigmoid function where success declines sharply once target capability exceeds attacker capability, suggesting targets' reasoning ability allows them to detect and refuse manipulative intent.

Core assumption: MMLU-Pro scores reliably proxy both general reasoning and attack/defense sub-capabilities. Evidence: attack success drops sharply once target's capability exceeds attacker's. Break condition: If targets use specialized safety mechanisms that decouple general capability from robustness, the proxy fails.

### Mechanism 2
Jailbreak capability correlates more with social-science reasoning than technical knowledge. Effective attacks mimic human social-engineering, so models scoring higher on social-science benchmarks may possess better models of human cognition and social dynamics.

Core assumption: Social-science performance reflects latent persuasive capability. Evidence: attacker ASR correlates most strongly with social-science splits of MMLU-Pro. Break condition: If jailbreaks shift to logic/exploit-based attacks, correlation with social-science scores may diminish.

### Mechanism 3
Model unlocking via LoRA is required to utilize full generative capability for red-teaming. Standard safety-tuned models refuse harmful prompts, but LoRA fine-tuning on harmful examples removes this "shallow" refusal behavior without degrading reasoning.

Core assumption: Safety alignment is "shallow" and reversible via fine-tuning. Evidence: authors exploit the fact that safety alignment can be easily undone via LoRA. Break condition: If future safety training employs "deep" alignment, simple LoRA may fail or destroy reasoning capabilities.

## Foundational Learning

**Sigmoid Function in Logit Space:** The paper models ASR using a sigmoid curve, fitting linear regression in logit space. Quick check: Why map probabilities to logit space before fitting linear trends to capability gaps?

**Weak-to-Strong Generalization (Inversion):** The core insight inverts the common "strong-to-weak" attack assumption. Quick check: Why does the paper predict human red-teaming will become ineffective against future models?

**LoRA (Low-Rank Adaptation):** The method relies on LoRA to efficiently modify model behavior without full retraining. Quick check: Why is LoRA preferred over full fine-tuning for "unlocking" attacker models?

## Architecture Onboarding

**Component map:** Unlocked Attacker -> Jailbreak Prompt Generation -> Target Model -> Response -> Judge Evaluation -> Attack Refinement (iterative)

**Critical path:**
1. Unlock Attacker: Fine-tune open-source model on harmful dataset to remove refusals
2. Configure Benchmark: Select harmful behaviors and target model
3. Execute Loop: Attacker generates prompt → Target responds → Judge scores → Attacker refines
4. Analyze Gap: Plot ASR vs. (Attacker MMLU-Pro - Target MMLU-Pro) to verify sigmoid trend

**Design tradeoffs:**
- Judge Strength: Smarter judges improve early-stop efficiency (ASR@1) but don't increase best-case success rate (ASR@25)
- Proxy Selection: MMLU-Pro is a noisy proxy; outliers exist due to safety-specific training
- Attack Method: Different methods (PAIR/TAP/PAP/Crescendo) trade complexity for effectiveness

**Failure signatures:**
- Attacker Refusal: Model still refuses to generate attacks despite "unlocking"
- Stagnant ASR: Attack success plateaus below scaling curve prediction
- Proxy Divergence: Target has high MMLU-Pro but low robustness (or vice versa)

**First 3 experiments:**
1. **Unlocking Validation:** Apply LoRA unlock to base model and verify direct HarmBench ASR increases significantly
2. **Gap Correlation:** Run 3 attackers (low/med/high MMLU-Pro) against single target, plot ASR vs. Capability Gap to check sigmoid trend
3. **Judge Ablation:** Run PAIR using strong judge vs. weak judge, compare ASR@25 to confirm judge affects selection not generation quality

## Open Questions the Paper Calls Out
None

## Limitations
- Proxy validity: MMLU-Pro scores may not perfectly capture attack/defense sub-capabilities, as shown by Llama outliers
- Generalizability: Scaling law derived from specific attack methods may not apply to all jailbreak techniques
- Model unlocking: LoRA-based unlocking may fail for future models with deeper safety integration

## Confidence
**High Confidence:** Attack success drops sharply when target capability exceeds attacker capability (sigmoid curve in capability gap space)
**Medium Confidence:** Social-science MMLU-Pro performance predicts jailbreak capability, though correlation could be dataset-specific
**Medium Confidence:** Prediction that human red-teamers will become ineffective against future models follows logically but assumes human capabilities won't advance
**Low Confidence:** Universal applicability across all attack strategies and model architectures

## Next Checks
**Check 1:** Replicate capability-gap experiment with adversarial attacks relying on logical manipulation rather than social persuasion to test whether social-science correlation holds
**Check 2:** Validate model unlocking procedure across broader range of model sizes and families, measuring whether unlocked models maintain reasoning capabilities
**Check 3:** Test scaling law's predictive power on models with known specialized safety training to evaluate whether sigmoid trend holds with adversarial training