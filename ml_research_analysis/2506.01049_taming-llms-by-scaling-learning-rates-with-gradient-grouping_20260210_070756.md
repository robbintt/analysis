---
ver: rpa2
title: Taming LLMs by Scaling Learning Rates with Gradient Grouping
arxiv_id: '2506.01049'
source_url: https://arxiv.org/abs/2506.01049
tags:
- learning
- arxiv
- training
- lora
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Scaling with Gradient Grouping (SGG) addresses training instability
  and slow convergence in large language models (LLMs) by dynamically clustering gradient
  statistics within each layer and applying cluster-specific scaling factors to modulate
  parameter-wise learning rates. This approach maintains parameter-wise adaptation
  while imposing group-wise constraints to promote learning homogeneity across layers
  and clusters.
---

# Taming LLMs by Scaling Learning Rates with Gradient Grouping

## Quick Facts
- arXiv ID: 2506.01049
- Source URL: https://arxiv.org/abs/2506.01049
- Reference count: 39
- Primary result: SGG improves validation perplexity and accelerates convergence across diverse LLM benchmarks while maintaining stability under extreme learning rate and batch size scaling.

## Executive Summary
Scaling with Gradient Grouping (SGG) addresses training instability and slow convergence in large language models by dynamically clustering gradient statistics within each layer and applying cluster-specific scaling factors to modulate parameter-wise learning rates. This approach maintains parameter-wise adaptation while imposing group-wise constraints to promote learning homogeneity across layers and clusters. Experiments demonstrate SGG consistently improves validation perplexity and accelerates convergence across diverse LLM and MLLM benchmarks, including C4 pre-training (e.g., 14.30 PPL vs 15.56 for 1B models), GLUE fine-tuning, commonsense reasoning tasks, and multimodal benchmarks. Notably, SGG enables low-rank pre-training to match full-rank performance for the first time and exhibits exceptional robustness to learning rate and batch size scaling, maintaining stable training across extreme configurations.

## Method Summary
SGG acts as a wrapper for adaptive optimizers like AdamW, clustering momentum vectors within each layer using mini-batch K-means into K groups. It computes cluster-specific scaling factors based on the Median of Deviation to Average (MDA) metric, which compares local cluster deviation to global model statistics. These scaling factors modulate the base optimizer's parameter-wise learning rates, with more stable clusters receiving larger scaling factors. The method dynamically re-clusters every T iterations (typically 500) and stores cluster indices and scaling factors on CPU to minimize GPU memory overhead. SGG integrates seamlessly with existing optimizers and PEFT techniques without requiring architectural modifications.

## Key Results
- C4 pre-training: 14.30 PPL vs 15.56 for baseline on 1B models
- Enables low-rank pre-training to match full-rank performance for the first time
- Exceptional robustness to learning rate and batch size scaling, maintaining stable training across extreme configurations
- Consistently improves validation perplexity and accelerates convergence across diverse LLM and MLLM benchmarks

## Why This Works (Mechanism)

### Mechanism 1
SGG stabilizes training by homogenizing learning dynamics across layers and clusters, dampening divergent updates while amplifying stable ones. The algorithm calculates the Median of Deviation to Average (MDA) for each cluster relative to the global model statistics, generating cluster-specific scaling factors where clusters with lower local deviation receive larger scaling factors.

### Mechanism 2
SGG achieves better optimization landscapes by retaining fine-grained, parameter-wise adaptation while imposing soft, dynamic group constraints. Unlike methods that replace adaptive learning rates with single group values, SGG multiplies the base optimizer's pre-computed parameter-wise learning rate by cluster-specific scaling factors.

### Mechanism 3
Dynamic, online re-clustering captures the shifting optimization behaviors of parameters that static partitioning schemes miss. SGG periodically performs mini-batch K-means clustering on momentum vectors at interval T, allowing group boundaries to shift as the model moves from initial random weights to feature formation.

## Foundational Learning

- **Concept: Adaptive Gradient Methods (Adam/AdamW)**
  - Why needed: SGG is explicitly designed as a "wrapper" for these optimizers; you cannot understand the "scaling" operation without understanding the base adaptive learning rate.
  - Quick check: If Adam computes a unique learning rate for every parameter, what statistic does SGG use to group these parameters together?

- **Concept: Momentum (β₁) vs. Gradient**
  - Why needed: SGG clusters momentum vectors, not raw gradients. Understanding momentum's smoothing effect is crucial to realizing why SGG clusters are stable enough to be useful.
  - Quick check: Why would clustering the exponentially smoothed momentum yield better groupings than clustering the raw, noisy gradient?

- **Concept: K-Means Clustering**
  - Why needed: This is the core algorithmic primitive for the "Grouping" part of SGG. The paper selects Mini-batch K-means for efficiency.
  - Quick check: In the context of a layer with 1 million parameters, what does "K=3" actually represent in terms of optimization behavior?

## Architecture Onboarding

- **Component map:** Base Optimizer -> Momentum Extraction -> Mini-batch K-Means (CPU) -> MDA Calculator -> Factor Generator (EMA) -> Modified Learning Rate -> Weight Update

- **Critical path:** The Re-clustering Interval (T). This is the primary lever for balancing computational overhead vs. adaptation freshness. If T is too small, training slows down significantly due to K-means overhead.

- **Design tradeoffs:**
  - GPU vs. CPU: Storing indices/scalars on CPU saves ~4GB GPU memory for 1B models with minimal time cost. GPU implementation is faster but memory-heavy.
  - Cluster Count (K): Empirically finds K=2 or K=3 sufficient. Higher K fragments the group constraint, potentially reducing the homogenization benefit.

- **Failure signatures:**
  - Surge in Loss: May occur if MDA calculation becomes unstable (e.g., division by near-zero local deviation).
  - Stagnation: If scaling factor decay β₃ is too high, scaling factors might not adapt to changing gradient landscape.
  - OOM: Attempting to store full cluster assignment matrices on GPU for large models.

- **First 3 experiments:**
  1. **Sanity Check (Toy Task):** Run LLaMA-60M on C4 with Adam vs. Adam+SGG. Verify validation PPL improves (e.g., 34.06 → 30.31).
  2. **Hyperparameter Sensitivity (T):** Sweep re-clustering interval T on 130M model to find efficiency frontier where PPL gains plateau but training time hasn't exploded.
  3. **Robustness Test:** Train Qwen2.5-0.5B with large learning rate and batch size. Compare Adam (likely spike/diverge) vs. SGG (should remain stable).

## Open Questions the Paper Calls Out

### Open Question 1
Can learned grouping functions or heuristic-based static partitioning achieve better performance-efficiency trade-offs than the current online clustering method? The paper suggests this as an alternative to online clustering but hasn't explored these options.

### Open Question 2
Can lightweight approximation operations replicate the benefits of explicit gradient clustering without the associated computational costs? The authors identify significant costs of online clustering as a limitation and call for approximation methods.

### Open Question 3
Does SGG generalize effectively to diverse architectures like Mixture-of-Experts (MoE) and generative image models? The current experiments focus on dense Transformer architectures, leaving MoE sparsity and generative dynamics untested.

## Limitations
- Performance claims rely heavily on controlled experimental settings with modest improvement magnitudes
- Computational overhead from dynamic re-clustering, though claimed minimal, is not quantified in absolute terms across different model scales
- MDA metric lacks theoretical grounding for why it specifically captures training instability

## Confidence
- **High confidence:** SGG's ability to maintain stable training under extreme LR and batch size configurations
- **Medium confidence:** Convergence acceleration claims and the enabling of low-rank pre-training to match full-rank performance
- **Medium confidence:** Overall performance improvements across benchmarks, though modest in magnitude

## Next Checks
1. **MDA metric validation:** Create synthetic gradient sequences with artificially introduced instability to verify SGG's MDA metric correctly identifies and dampens unstable clusters while amplifying stable ones.

2. **Dynamic vs static clustering comparison:** Implement a variant of SGG using fixed, pre-defined clusters (e.g., based on layer position) and compare its performance against dynamic SGG to quantify the value of online re-clustering.

3. **Scaling with model size:** Test SGG on models beyond 1B parameters (e.g., 3B-7B LLaMA variants) to validate whether claimed GPU memory savings and computational efficiency scale proportionally with parameter count.