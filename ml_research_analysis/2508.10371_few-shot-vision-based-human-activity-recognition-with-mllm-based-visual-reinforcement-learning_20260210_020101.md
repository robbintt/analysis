---
ver: rpa2
title: Few-shot Vision-based Human Activity Recognition with MLLM-based Visual Reinforcement
  Learning
arxiv_id: '2508.10371'
source_url: https://arxiv.org/abs/2508.10371
tags:
- learning
- human
- recognition
- activity
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FA VOR, a few-shot human activity recognition
  method that leverages visual reinforcement learning with multimodal large language
  models (MLLMs). The approach generates multiple candidate responses with reasoning
  traces and final answers, then optimizes the model using Group Relative Policy Optimization
  (GRPO) with rule-based verifiable rewards.
---

# Few-shot Vision-based Human Activity Recognition with MLLM-based Visual Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2508.10371
- **Source URL**: https://arxiv.org/abs/2508.10371
- **Authors**: Wenqi Zheng; Yutaka Arakawa
- **Reference count**: 22
- **Primary result**: FA VOR achieves 55.78% average accuracy in 1-shot settings, improving by +14.78% over baseline and +0.04% over SFT

## Executive Summary
This paper introduces FA VOR, a novel approach for few-shot human activity recognition that leverages multimodal large language models (MLLMs) with visual reinforcement learning. The method generates multiple candidate responses with reasoning traces and final answers, then optimizes the model using Group Relative Policy Optimization (GRPO) with rule-based verifiable rewards. Extensive experiments demonstrate FA VOR's superiority over supervised fine-tuning across multiple challenging datasets, with particular success in extremely low-shot scenarios.

## Method Summary
FA VOR employs a two-stage approach: first generating multiple candidate responses with reasoning traces using MLLMs, then applying visual reinforcement learning through GRPO with rule-based verifiable rewards. The system processes visual inputs through frozen vision modules while the reasoning component explores different answer candidates. The reward function evaluates both the reasoning trace quality and final answer accuracy, enabling the model to learn optimal reasoning strategies even with minimal training examples.

## Key Results
- In 1-shot settings, FA VOR achieves 55.78% average accuracy across four datasets
- FA VOR shows substantial improvements on UCF-101 (+29.62%) and maintains consistent gains across 2-shot and 4-shot settings
- In 16-shot scenarios, FA VOR reaches 63.55% average accuracy, outperforming baselines by +22.55%
- Even with frozen vision modules, FA VOR delivers significant performance gains, highlighting the effectiveness of reward-driven reasoning exploration

## Why This Works (Mechanism)
FA VOR succeeds by combining the reasoning capabilities of MLLMs with reinforcement learning's exploration benefits. The approach generates multiple reasoning traces, allowing the model to discover effective reasoning strategies that may not be present in the training data. GRPO optimizes these strategies based on verifiable rewards, creating a self-improving loop. The rule-based reward system provides clear feedback while maintaining interpretability, enabling the model to learn which reasoning patterns lead to correct answers even with minimal examples.

## Foundational Learning
- **Multimodal Large Language Models**: Neural architectures that process both visual and textual information, essential for understanding activity context from video frames and descriptions
- **Group Relative Policy Optimization (GRPO)**: A reinforcement learning algorithm that optimizes policies by comparing performance within groups rather than absolute metrics, crucial for stable learning with sparse rewards
- **Visual Reinforcement Learning**: Training paradigm where agents learn through environmental feedback, necessary for developing reasoning strategies without extensive supervision
- **Rule-based Verifiable Rewards**: Evaluation metrics that provide clear, interpretable feedback on model outputs, important for debugging and understanding learning progress
- **Few-shot Learning**: Machine learning paradigm where models learn from very limited examples, critical for real-world scenarios where labeled data is scarce
- **Reasoning Traces**: Step-by-step logical processes generated by models, valuable for understanding decision-making and improving interpretability

## Architecture Onboarding
**Component Map**: Input Video -> Vision Module -> MLLM Reasoning -> Candidate Generation -> GRPO Optimizer -> Updated Policy
**Critical Path**: The core pipeline involves frozen vision modules extracting visual features, MLLM generating multiple reasoning candidates, and GRPO optimizing the reasoning policy based on reward feedback
**Design Tradeoffs**: The approach trades computational efficiency for improved accuracy in low-shot scenarios. While GRPO requires significant training resources, it enables learning effective reasoning strategies without extensive labeled data
**Failure Signatures**: Poor performance may indicate inadequate reward function design, insufficient exploration of reasoning candidates, or limitations in the vision module's feature extraction capabilities
**3 First Experiments**:
1. Test FA VOR on a simplified dataset with 1-2 activity classes to verify basic functionality
2. Compare GRPO optimization against standard supervised fine-tuning on the same candidate generation approach
3. Evaluate the impact of different reward function designs on final performance

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The computational requirements for training with MLLM-based visual reinforcement learning may limit practical deployment in resource-constrained environments
- Rule-based verifiable rewards may not capture all nuances of activity recognition quality, potentially leading to suboptimal exploration
- The true contribution of visual processing versus reasoning capabilities remains unclear based on ablation study results
- Generalizability across different domain shifts or more diverse activity types remains untested

## Confidence
- **High confidence**: Methodology framework and experimental design are sound with clear implementation details and reproducible results on established benchmarks
- **Medium confidence**: Relative performance improvements are likely real but may be somewhat inflated by specific experimental conditions or choice of baselines
- **Low confidence**: Scalability claims beyond 16-shot settings and the true contribution of visual component versus reasoning capabilities

## Next Checks
1. Conduct domain adaptation experiments testing FA VOR on activities significantly different from training distributions to assess true generalization capability
2. Implement and compare against state-of-the-art few-shot learning baselines (e.g., prototypical networks, meta-learning approaches) to establish stronger comparative performance metrics
3. Perform computational efficiency analysis measuring training time, memory requirements, and inference latency across different hardware configurations to assess practical deployment viability