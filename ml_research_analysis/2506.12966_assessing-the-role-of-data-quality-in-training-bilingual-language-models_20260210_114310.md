---
ver: rpa2
title: Assessing the Role of Data Quality in Training Bilingual Language Models
arxiv_id: '2506.12966'
source_url: https://arxiv.org/abs/2506.12966
tags:
- data
- quality
- performance
- language
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work shows that performance gaps in bilingual language models
  are primarily caused by unequal data quality rather than language differences. The
  authors introduce a simple filtering method that selects high-quality bilingual
  training data using only high-quality English data.
---

# Assessing the Role of Data Quality in Training Bilingual Language Models

## Quick Facts
- arXiv ID: 2506.12966
- Source URL: https://arxiv.org/abs/2506.12966
- Authors: Skyler Seto; Maartje ter Hoeve; Maureen de Seyssel; David Grangier
- Reference count: 40
- Performance gaps in bilingual language models are primarily caused by unequal data quality rather than language differences

## Executive Summary
This work addresses performance gaps in bilingual language models by demonstrating that data quality disparities, not language differences, are the primary cause. The authors introduce a simple yet effective filtering method that uses only high-quality English data to identify and retain high-quality bilingual training data. Applied to French, German, and Chinese, this approach improves monolingual performance by 2-4% and reduces bilingual model performance gaps to under 1%. The method proves effective across different model sizes and requires only small amounts of high-quality English seed data, making it a practical solution for closing performance gaps in multilingual pretraining.

## Method Summary
The authors train a logistic regression classifier on English data quality labels using multilingual SBERT embeddings. This classifier is then applied to target-language documents to identify high-quality examples, retaining the top 10% based on predicted quality scores. The filtered data is combined with high-quality English data (FineWebEDU) to pretrain bilingual models. The approach is evaluated across three language pairs (EN-FR, EN-DE, EN-ZH) using a 1.3B decoder-only transformer architecture trained for 200K steps with standard language modeling objectives.

## Key Results
- Monolingual performance improves by 2-4% after quality filtering
- Bilingual performance gaps reduced to under 1% compared to monolingual baselines
- Method works across different model sizes and language pairs
- Small amounts of high-quality English data are sufficient for effective filtering

## Why This Works (Mechanism)
The approach works because data quality, not language-specific factors, primarily drives performance gaps in bilingual models. By using high-quality English data as a reference, the classifier can identify structurally similar high-quality documents in target languages, even without explicit cross-lingual quality labels. This creates more balanced training distributions across languages, eliminating the performance degradation typically seen when training bilingual versus monolingual models.

## Foundational Learning
- Multilingual SBERT embeddings - Why needed: Enable cross-lingual representation of document quality; Quick check: Verify embeddings produce similar distributions across languages for documents of comparable quality
- Quality classification with logistic regression - Why needed: Simple, effective binary classifier for high-quality vs low-quality documents; Quick check: Confirm classifier achieves >90% accuracy on English validation set
- Percentile-based filtering - Why needed: Robust thresholding method that adapts to varying data distributions; Quick check: Validate that filtered datasets maintain sufficient volume (>200B tokens for 1.3B model)

## Architecture Onboarding

**Component map:** DCLM data -> SBERT embeddings -> Logistic regression classifier -> Target corpus filtering -> Bilingual pretraining -> Evaluation

**Critical path:** The quality classifier trained on English data directly determines which target-language documents are retained for training. The classifier's ability to generalize across languages via multilingual embeddings is therefore the most critical component.

**Design tradeoffs:** The method trades some data quantity (only 10% retained) for significant quality improvements. Alternative approaches could use more sophisticated quality models or different filtering thresholds, but the simplicity and effectiveness of the percentile-based logistic regression approach is a key strength.

**Failure signatures:** Over-filtering leading to data scarcity and repetition artifacts; poor cross-lingual embedding alignment for distant languages causing ineffective filtering; insufficient total tokens after filtering preventing adequate training.

**3 first experiments:** 1) Train quality classifier on English data and evaluate cross-lingual transfer to target languages using held-out validation sets; 2) Compare performance using 90% vs 95% filtering thresholds to identify optimal balance; 3) Evaluate model performance on translated test sets before and after quality filtering to measure impact.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on proprietary translation systems, limiting reproducibility of baseline comparisons
- Quality classifier assumes cross-lingual embedding alignment which may not hold equally well for all language pairs
- Study focuses on three language pairs and one model scale, limiting generalizability

## Confidence
- High confidence: Data quality filtering improves monolingual performance and reduces bilingual gaps
- Medium confidence: Data quality is the primary driver of performance gaps rather than language-specific factors
- Medium confidence: Small amounts of high-quality English data are sufficient for effective filtering

## Next Checks
1. Replicate the filtering pipeline on an additional language pair using open-source translation for evaluation to verify generalizability
2. Conduct ablation studies varying the amount of English seed data to quantify the minimum effective threshold
3. Evaluate filtered datasets using additional quality metrics beyond perplexity to better characterize quality improvements