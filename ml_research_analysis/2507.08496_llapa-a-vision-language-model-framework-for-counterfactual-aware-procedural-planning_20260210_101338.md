---
ver: rpa2
title: 'LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural
  Planning'
arxiv_id: '2507.08496'
source_url: https://arxiv.org/abs/2507.08496
tags:
- counterfactual
- planning
- arxiv
- visual
- llapa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LLaPa is a vision-language model framework for procedural planning
  that addresses the dual challenges of multimodal perception and counterfactual reasoning.
  It introduces two specialized modules: a Task-Environment Reranker that uses task-oriented
  segmentation to focus on task-relevant objects, and a Counterfactual Activities
  Retriever that detects and emphasizes counterfactual conditions through visual token
  selection.'
---

# LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning

## Quick Facts
- arXiv ID: 2507.08496
- Source URL: https://arxiv.org/abs/2507.08496
- Reference count: 40
- Authors: Shibo Sun; Xue Li; Donglin Di; Mingjie Wei; Lanshun Nie; Wei-Nan Zhang; Dechen Zhan; Yang Song; Lei Fan
- Primary result: Achieves state-of-the-art performance on procedural planning benchmarks, with 53.2% executability and 36.1% correctness on counterfactual tasks.

## Executive Summary
LLaPa addresses the challenges of multimodal perception and counterfactual reasoning in procedural planning by introducing two specialized modules: a Task-Environment Reranker (TER) that uses task-oriented segmentation to focus on task-relevant objects, and a Counterfactual Activities Retriever (CAR) that detects and emphasizes counterfactual conditions through visual token selection. The framework generates executable action sequences from textual task descriptions and environmental images, demonstrating strong performance on ActPlan-1K and ALFRED benchmarks. The modules show promising transferability when integrated with other vision-language models.

## Method Summary
LLaPa processes image and text inputs through a two-stage training framework. First, it trains a counterfactual classifier to identify counterfactual conditions in text. Then, it fine-tunes the TER module, which uses Grounded-SAM to generate task-relevant segmentation masks and integrates them into the vision encoder's self-attention via log-transformed additive bias. The CAR module extracts visual tokens corresponding to counterfactual conditions using conditional pooling. These processed features are concatenated with the original text and fed to an LLM to generate action sequences. The framework uses InternVL2-8B with InternViT encoder, processes images at 16×16 patches, and extracts 16 counterfactual tokens using 4×4 sub-grids.

## Key Results
- Achieves 53.2% executability and 36.1% correctness on counterfactual tasks in ActPlan-1K benchmark
- Outperforms advanced models on both ActPlan-1K and ALFRED benchmarks
- Demonstrates 6-9% correctness improvement when modules are transferred to Qwen2-VL

## Why This Works (Mechanism)

### Mechanism 1: Task-Oriented Segmentation Reranking
The Task-Environment Reranker (TER) employs Grounded-SAM to generate binary masks for objects mentioned in task descriptions, transforms these masks into patch-level attention weights via adaptive max pooling, and integrates them into the self-attention mechanism through a log-transformed additive bias: Attn(v,W) = softmax(QK^T/√d + log(W))V. This reranks visual features to emphasize task-relevant regions while suppressing irrelevant visual elements.

### Mechanism 2: Counterfactual Clause Detection and Visual Token Extraction
The Counterfactual Activities Retriever (CAR) segments text embeddings into clauses, classifies each using an MLP with sigmoid activation to identify counterfactual conditions, and retrieves corresponding visual tokens. For identified counterfactual clauses, it aggregates patch-level masks into counterfactual weight matrices, applies element-wise masking to reranked features, and performs conditional pooling over K×K sub-grids—averaging only from masked regions to produce compact counterfactual visual tokens.

### Mechanism 3: Two-Stage Training Decoupling
Stage 1 trains only the CAR classifier (binary cross-entropy loss) on COPLAN and ActPlan-1K training data with all other parameters frozen. Stage 2 fine-tunes TER's self-attention layers, MLP projector, and LLM using language modeling loss on EgoCOT and ActPlan-1K. A special prompt token guides the LLM to associate counterfactual tokens with planning adjustments.

## Foundational Learning

- **Concept: Task-Oriented Segmentation (TOS)**
  - Why needed: TER builds on TOS principles—identifying objects relevant to a specific goal rather than generic scene parsing. Understanding TOS clarifies why TER uses Grounded-SAM rather than standard semantic segmentation.
  - Quick check: Given an image of a kitchen and the task "make coffee," which objects should TOS highlight? (Expected: coffee machine, mug, water source—not toaster or blender)

- **Concept: Attention Biasing with Mask Matrices**
  - Why needed: TER's integration of segmentation masks into self-attention via log(W) additive bias is non-standard. This mechanism differs from attention masking (multiplicative) and requires understanding how additive bias shifts attention distributions.
  - Quick check: If a mask patch W_ij = 0, what happens to attention weight at that position? (Expected: log(0) is undefined—implementation likely uses small epsilon; attention strongly suppressed but not zeroed)

- **Concept: Conditional Pooling**
  - Why needed: CAR's conditional pooling averages features only from masked sub-grid regions, differing from standard pooling. This preserves task-relevant features while suppressing noise, critical for compact counterfactual token generation.
  - Quick check: If a sub-grid g has no masked positions (Ω_g = ∅), what is V^cf_{i,g}? (Expected: Zero vector, per equation 4)

## Architecture Onboarding

- **Component map:**
  Input: Images I, Task Text S → [Vision Encoder (ViT)] → raw image embeddings v → [TER] → reranked embeddings v_rerank → [CAR] → counterfactual tokens v^cf → [MLP Projector] → aligned features → [Concatenation] → [LLM] → action sequence A

- **Critical path:** Text parsing → TER mask generation → self-attention reranking → CAR classification → conditional pooling → token concatenation → LLM decoding. Errors in segmentation or classification cascade through the pipeline.

- **Design tradeoffs:**
  - K parameter (conditional pooling sub-grids): K=4 optimal in ablation; K<4 loses granularity, K>4 introduces noise. Tune based on image resolution and object density.
  - Frozen vs. fine-tuned segmentation: TER uses frozen Grounded-SAM for efficiency but risks domain mismatch; consider fine-tuning if target environments differ significantly from SAM's training distribution.
  - Single-pass vs. iterative: CAR enables single-pass counterfactual handling but cannot refine plans dynamically like Plasma's replanning; hybrid approaches may be needed for complex multi-condition scenarios.

- **Failure signatures:**
  - Low executability with high LCS: TER likely masking relevant objects—check segmentation quality on failed examples.
  - High executability but low correctness on counterfactual tasks: CAR classifier missing conditions—review classifier precision/recall on validation set.
  - Hallucination of extra tools: TER insufficiently suppressing irrelevant regions—verify OR aggregation vs. sum normalization tradeoff.
  - Degraded transfer to new VLMs: MLP projector misaligned—re-train Stage 2 with target VLM's projector.

- **First 3 experiments:**
  1. TER isolation test: Replace TER's Grounded-SAM with ground-truth object bounding boxes (if available) or zero masks. Measure executability delta on ActPlan-1K normal activities to quantify TER's contribution vs. baseline VLM.
  2. CAR classifier calibration: Evaluate classifier precision/recall on held-out counterfactual clauses. Plot ROC curve and determine optimal classification threshold beyond default 0.5—paper does not specify threshold tuning.
  3. Transfer validation: Apply TER+CAR modules to Qwen2-VL following paper's protocol (Table 4). If gains < reported 6-9% correctness improvement, investigate patch size alignment between Qwen2-VL's ViT and Grounded-SAM output.

## Open Questions the Paper Calls Out

- How can LLaPa be extended to incorporate dynamic video contexts to enhance adaptability in real-world scenarios?
- How does LLaPa's performance vary when task descriptions are sparse, ambiguous, or incomplete?
- Can the Counterfactual Activities Retriever (CAR) detect and adapt to visual anomalies that are not explicitly described in the text?

## Limitations
- Relies heavily on comprehensive textual task descriptions, limiting effectiveness with sparse or ambiguous inputs
- Current approach does not support dynamic video context integration
- Performance depends on segmentation mask quality from Grounded-SAM, which may fail on occlusions or atypical object appearances

## Confidence
- **High confidence**: The mechanism of integrating segmentation masks into self-attention via log(W) additive bias is technically sound and well-supported by the described implementation.
- **Medium confidence**: The claim of state-of-the-art performance is supported by reported metrics but lacks detailed statistical significance testing.
- **Low confidence**: The transferability results are presented without ablation on VLM-specific factors like patch size alignment or projector dimensionality mismatch.

## Next Checks
1. **Classifier Calibration and Threshold Tuning**: Evaluate the CAR classifier's precision/recall on a held-out validation set of counterfactual clauses. Plot the ROC curve and determine the optimal classification threshold beyond the default 0.5.
2. **TER Isolation Test with Ground-Truth Segmentation**: Replace Grounded-SAM with ground-truth object bounding boxes (if available) or synthetic masks for task-relevant objects. Measure the executability delta on ActPlan-1K normal activities.
3. **Transfer Validation with Ablation**: Apply TER+CAR modules to Qwen2-VL following the paper's protocol, but include ablations: (a) TER only, (b) CAR only, (c) baseline Qwen2-VL. Compare correctness improvements to validate that gains are additive.