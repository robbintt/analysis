---
ver: rpa2
title: 'Classification is a RAG problem: A case study on hate speech detection'
arxiv_id: '2508.06204'
source_url: https://arxiv.org/abs/2508.06204
tags:
- hate
- policy
- speech
- content
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that retrieval-augmented generation (RAG)
  can transform classification from a model-based task to a context-based evaluation
  process, enabling more flexible and adaptable content moderation. The Contextual
  Policy Engine (CPE) applies this approach to hate speech detection, achieving strong
  performance with F1 scores up to 0.996 on standard benchmarks while providing inherent
  explainability through policy-grounded reasoning.
---

# Classification is a RAG problem: A case study on hate speech detection

## Quick Facts
- arXiv ID: 2508.06204
- Source URL: https://arxiv.org/abs/2508.06204
- Authors: Richard Willats; Josh Pennington; Aravind Mohan; Bertie Vidgen
- Reference count: 13
- Primary result: RAG-based hate speech detection achieves 0.996 F1 score while enabling dynamic policy updates without retraining

## Executive Summary
This paper demonstrates that retrieval-augmented generation (RAG) can transform hate speech classification from a parametric model task to a context-based evaluation process. The Contextual Policy Engine (CPE) achieves state-of-the-art performance on hate speech detection benchmarks while providing inherent explainability through policy-grounded reasoning. By externalizing classification boundaries into retrievable policy documents, the system enables zero-shot policy updates and demonstrates superior adaptability to new identity groups compared to traditional supervised learning approaches.

## Method Summary
The CPE uses a RAG architecture where content is classified by retrieving relevant policy chunks and generating structured outputs conditioned on this context. The system uses an orchestrator for prompt routing, an embedding-based retrieval system with reranking, and a grounded language model (preference-optimized from Llama 3.3) that adheres to retrieved documents over parametric knowledge. Classification decisions are framed as policy violation evaluations rather than categorical predictions, enabling dynamic policy updates without model retraining.

## Key Results
- Achieves 0.996 F1 score on standard hate speech detection benchmarks
- Maintains 97-99% accuracy in correctly exempting or protecting specific identity groups based on policy changes
- Shows only 1.6% F1 drop when extending coverage to non-traditional targets vs. 52-83% drops for parametric systems
- Enables dynamic policy updates without model retraining, achieving 100% policy adherence

## Why This Works (Mechanism)

### Mechanism 1: Externalized Classification Boundaries
Shifting category definitions from model parameters to retrieved documents enables zero-shot policy updates without retraining. The system retrieves relevant policy chunks at inference time and conditions generation on this context rather than relying on parametric knowledge. When policy changes, document updates immediately propagate to classifications. Core assumption: The generator can correctly interpret and apply policy text to novel inputs without task-specific training.

### Mechanism 2: Task Reformulation from Categorical to Relational
Reframing classification as policy violation detection improves generalization to unseen content and identity groups. Rather than learning statistical patterns associated with labels, the model evaluates whether content satisfies explicit violation criteria. This decouples detection from training distribution. Core assumption: Policy documents capture the true decision boundary sufficiently that relational evaluation generalizes better than pattern matching.

### Mechanism 3: Retrieval-Grounded Explainability
Exposing retrieved policy segments provides ante-hoc explanations tied directly to decision evidence. The generator produces structured outputs including classification, policy category, target identification, and free-text explanation. Because generation is conditioned on retrieved chunks, explanations trace to specific policy language. Core assumption: The retrieval system surfaces the correct policy sections for each input, and the generator faithfully reflects this context in its reasoning.

## Foundational Learning

**Concept: Retrieval-Augmented Generation (RAG)**
Why needed here: This is the core architecture. Understanding how retrieval and generation compose is prerequisite to debugging either component.
Quick check question: Can you explain why RAG reduces hallucination compared to pure generation, and under what conditions it might increase errors?

**Concept: Preference Optimization / Instruction Following**
Why needed here: The paper uses a "Grounded Language Model" that is "preference-optimized from Llama 3.3" to adhere to documents over parametric knowledge.
Quick check question: What is the difference between fine-tuning and preference optimization, and why might the latter be preferred for adherence tasks?

**Concept: Reranking**
Why needed here: The retrieval pipeline includes a reranker after embedding search. This is a critical quality bottleneck.
Quick check question: Why add a reranker after embedding search? What failure mode does this address?

## Architecture Onboarding

**Component map:**
[Input Content] → [Orchestrator: prompt routing + formatting] → [Retrieval System: embedding search → reranker] → [Policy Datastore: chunked documents] → [Generator: Grounded LM] → [Structured Output: rating, category, target, explanation]

**Critical path:**
1. Policy document quality (definitions, edge cases, examples) — if this is incomplete, no retrieval can compensate
2. Retrieval relevance — if wrong chunks are fetched, generator lacks correct evidence
3. Generator grounding — if model ignores retrieved context, all prior work is wasted

**Design tradeoffs:**
- Chunk size vs. context completeness: Smaller chunks improve retrieval precision but may fragment policy logic; larger chunks provide context but dilute relevance signals
- Precision vs. recall optimization: Orchestrator can be calibrated; high-recall settings increase false positives, high-precision settings miss edge cases
- Latency vs. quality: RAG adds retrieval and reasoning overhead vs. pure parametric classification

**Failure signatures:**
- Policy gaps: Consistent misclassifications on specific content types → check if policy defines that case
- Retrieval failures: Explanations reference irrelevant policy sections → examine chunk boundaries and embedding quality
- Grounding failures: Explanations don't match retrieved text → generator may be relying on parametric knowledge; verify grounding optimization

**First 3 experiments:**
1. Policy ablation test: Remove specific policy sections and measure classification degradation on affected categories to validate retrieval-dependence
2. Retrieval quality audit: Manually review top-k retrieved chunks for 50-100 inputs to identify systematic retrieval failures
3. Boundary stress test: Test on edge cases near policy boundaries (reclaimed slurs, counter-speech) to assess whether relational evaluation handles nuance better than baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Policy Grounding Dependency: Performance fundamentally depends on completeness and quality of policy documents, creating potential for silent failures on uncovered scenarios
- Retrieval Quality Assumptions: No systematic evaluation of retrieval failure modes or analysis of cases where generator produces incorrect outputs based on partial/ misleading context
- Generalization Boundaries: Claims about superior adaptability exceed demonstrated evidence, based on limited extension of known categories

## Confidence
- High Confidence: CPE achieves stated F1 scores on established benchmarks and successfully demonstrates dynamic policy updates without retraining
- Medium Confidence: Superior adaptability for non-traditional identity groups is supported but based on limited set of extensions
- Low Confidence: Broader claims about universal advantages over traditional supervised learning extend beyond demonstrated evidence

## Next Checks
1. Policy Completeness Audit: Systematically remove specific policy sections and measure classification performance degradation across all categories
2. Retrieval Failure Analysis: Conduct blind review of top-5 retrieved chunks for 100 random inputs to evaluate topical relevance and correlation with classification accuracy
3. Novel Category Stress Test: Evaluate CPE on 5-10 completely new protection scenarios to measure true limits of zero-shot adaptability