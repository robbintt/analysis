---
ver: rpa2
title: Autoregressive Speech Enhancement via Acoustic Tokens
arxiv_id: '2507.12825'
source_url: https://arxiv.org/abs/2507.12825
tags:
- speech
- tokens
- enhancement
- audio
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel autoregressive transducer-based architecture
  for speech enhancement using discrete acoustic tokens. The approach addresses limitations
  of prior work that used non-autoregressive models and semantic tokens, which lose
  speaker identity information.
---

# Autoregressive Speech Enhancement via Acoustic Tokens

## Quick Facts
- **arXiv ID:** 2507.12825
- **Source URL:** https://arxiv.org/abs/2507.12825
- **Reference count:** 40
- **Primary result:** Acoustic tokens preserve speaker identity better than semantic tokens in autoregressive speech enhancement models.

## Executive Summary
This paper introduces a novel autoregressive transducer-based architecture for speech enhancement using discrete acoustic tokens. The approach addresses limitations of prior work that used non-autoregressive models and semantic tokens, which lose speaker identity information. The proposed Speech Enhancement Transducer (SET) employs an encoder-decoder framework with autoregressive prediction, outperforming semantic token methods in preserving speaker identity and improving intelligibility. Experiments on VoiceBank and Libri1Mix datasets show that acoustic tokens from EnCodec and DAC outperform semantic tokens from WavLM, and the SET architecture further enhances performance. However, discrete representations still lag behind continuous ones, particularly in intelligibility metrics, highlighting the need for further research in this area.

## Method Summary
The paper formulates speech enhancement as a discrete token-to-token translation problem. A pre-trained audio codec (EnCodec or DAC) converts raw waveforms into discrete acoustic tokens using residual vector quantization. The SET architecture processes these tokens autoregressively: a conformer encoder processes the noisy input tokens, a causal conformer predictor processes the clean target tokens (shifted right), and a joiner combines these representations to predict the next token. The model is trained with cross-entropy loss for 50 epochs, with teacher forcing disabled during the final 5 epochs to mitigate exposure bias. At inference, beam search generates the enhanced token sequence, which is then converted back to audio using the pre-trained vocoder.

## Key Results
- Acoustic tokens (EnCodec, DAC) outperform semantic tokens (WavLM) in preserving speaker identity (Cosine Similarity)
- The autoregressive SET architecture improves intelligibility over non-autoregressive approaches (lower dWER)
- Discrete token methods still lag behind continuous baselines in intelligibility metrics despite matching quality and speaker identity

## Why This Works (Mechanism)

### Mechanism 1
Acoustic tokens preserve speaker identity better than semantic tokens for speech enhancement. Acoustic tokenizers use residual vector quantization to reconstruct audio waveforms, retaining high-fidelity details like timbre and prosody. In contrast, semantic tokenizers apply k-means clustering to self-supervised features, optimized for linguistic content that tends to discard acoustic characteristics like speaker-specific intonation. The quantization bottleneck of the acoustic codec is sufficiently wide to retain speaker-specific spectral details that k-means clustering filters out.

### Mechanism 2
Autoregressive modeling improves intelligibility over non-autoregressive approaches by capturing temporal dependencies. Standard NAR models predict output tokens independently based on the input mixture. The SET architecture predicts the current output token conditioned on previous output tokens, allowing the model to enforce temporal consistency and correct local errors based on the evolving context of the enhanced speech stream. The output tokens possess sequential dependencies that provide predictive power beyond the noisy input alone.

### Mechanism 3
The Speech Enhancement Transducer (SET) architecture aligns input and output streams efficiently for many-to-many mapping. Unlike sequence-to-sequence models that must learn alignment, speech enhancement is a synchronous many-to-many problem where input time steps roughly correspond to output time steps. SET uses a transducer joiner to combine encoder (noisy context) and predictor (clean history) features, avoiding the computational overhead of learning alignment while retaining AR properties. The temporal alignment between the noisy input and the enhanced output is relatively rigid, justifying the transducer structure over flexible attention.

## Foundational Learning

- **Concept: Residual Vector Quantization (RVQ)**
  - **Why needed here:** The paper relies on acoustic tokens (EnCodec/DAC) which use RVQ. Understanding that audio is compressed into multiple parallel streams of discrete codes (codebooks) is essential for configuring the model input embeddings.
  - **Quick check question:** How does increasing the number of codebooks (bitrate) affect the model's parameter count and the information density it must process?

- **Concept: Exposure Bias**
  - **Why needed here:** The paper identifies this as the critical failure mode of AR models. You must understand that training uses ground truth history (Teacher Forcing), while inference uses potentially error-prone predictions, leading to drift.
  - **Quick check question:** Why does disabling teacher forcing for the final few epochs (refinement) help mitigate the drop in performance seen during beam search?

- **Concept: Transducer Loss (RNN-T / Transducer)**
  - **Why needed here:** The SET architecture is based on this. You need to know how the "Joiner" combines the encoder (acoustics) and predictor (language) logit streams to produce a final probability distribution.
  - **Quick check question:** In a transducer, does the Joiner typically sum the log-probabilities or concatenate the hidden states before the final classification layer?

## Architecture Onboarding

- **Component map:** Tokenizer -> Encoder (Conformer) -> Predictor (Causal Conformer) -> Joiner (Linear) -> Detokenizer
- **Critical path:** The Refinement training phase. Without the final 5 epochs of training without Teacher Forcing, the AR model performance collapses during inference due to exposure bias.
- **Design tradeoffs:**
  - Acoustic vs. Semantic: Acoustic preserves speaker (high CosSim) but struggles with intelligibility (high dWER) compared to continuous baselines; Semantic is the reverse.
  - AR vs. NAR: AR improves intelligibility but is slower and prone to exposure bias; NAR is faster but ignores output dependencies.
  - Data Scale: Discrete models lag behind continuous ones, hypothesizing that discrete token LMs require significantly more data than the 100 hours used.
- **Failure signatures:**
  - Exposure Bias: Massive drop in metrics (e.g., dWER jumping from 18% to 37%) when switching from Teacher Forcing evaluation to standard Beam Search.
  - Speaker Drift: Using semantic tokens resulting in high intelligibility but low Cosine Similarity (speaker identity loss).
  - High-Bitrate Saturation: Performance degrading for EnCodec at bitrates > 3.0 kbps because the sequence length/information density becomes unmanageable for the language model.
- **First 3 experiments:**
  1. Tokenizer Ablation: Train the NAR model on identical data using WavLM discrete tokens vs. EnCodec tokens. Compare Cosine Similarity to verify that acoustic tokens are strictly necessary for speaker preservation.
  2. Architecture Ablation: Train the SET (AR) model and compare it against the NAR baseline using dWER to confirm the benefit of autoregressive context.
  3. Inference Mode Analysis: Evaluate the trained AR model using Teacher Forcing vs. Beam Search vs. Beam Search with Refinement (BSR). Quantify the "gap" to determine if the refinement strategy is successfully closing the exposure bias divide.

## Open Questions the Paper Calls Out

### Open Question 1
Can novel or hybrid discrete audio representations be developed to close the performance gap with continuous methods, particularly regarding speech intelligibility (dWER)? The paper concludes that next steps involve developing more powerful discrete representations to reduce this gap.

### Open Question 2
Does scaling training data to thousands of hours effectively mitigate the performance deficit of discrete autoregressive models compared to continuous baselines? The paper hypothesizes that the intelligibility gap is due to insufficient data (only 100 hours vs. the thousands needed for discrete LMs).

### Open Question 3
Can the SET architecture be adapted for streaming inference or improved via hypothesis rescoring with external language models? The paper notes the architecture is well-suited for streaming inference and hypothesis rescoring using external speech language models.

## Limitations

- Exposure bias remains a critical failure mode requiring mitigation strategies like disabling teacher forcing for final training epochs
- Discrete token methods significantly underperform continuous baselines on intelligibility metrics despite matching quality and speaker identity
- Joiner architecture dimension (120 channels) appears inconsistent with expected output requirements, creating ambiguity in implementation

## Confidence

- **High Confidence**: Acoustic tokens outperform semantic tokens in preserving speaker identity, supported by experiments and literature on universal speech token learning
- **Medium Confidence**: Autoregressive modeling improves intelligibility over non-autoregressive approaches, though the performance gain could be partially due to model capacity rather than AR property alone
- **Medium Confidence**: Transducer architecture is efficient for many-to-many mapping, but limited direct empirical comparison against full encoder-decoder baseline

## Next Checks

1. **Data Scaling Experiment**: Train the SET model on a significantly larger dataset (500-1000 hours) using acoustic tokens to empirically test whether the intelligibility gap is primarily due to data limitations.

2. **Exposure Bias Ablation**: Systematically compare the proposed "final-epochs-without-teacher-forcing" mitigation strategy against scheduled sampling and beam search with teacher forcing during inference.

3. **Joiner Architecture Clarification**: Implement and test two interpretations of the joiner (standard linear layer vs. multi-head design) to determine the correct architecture for optimal intelligibility results.