---
ver: rpa2
title: Generating Risky Samples with Conformity Constraints via Diffusion Models
arxiv_id: '2512.18722'
source_url: https://arxiv.org/abs/2512.18722
tags:
- samples
- generated
- risky
- conformity
- category
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating risky samples
  that deceive a target model while still conforming to the conditioned category.
  Previous methods using diffusion models for this task often struggle to maintain
  category conformity, introducing label noise and limiting effectiveness.
---

# Generating Risky Samples with Conformity Constraints via Diffusion Models

## Quick Facts
- **arXiv ID:** 2512.18722
- **Source URL:** https://arxiv.org/abs/2512.18722
- **Authors:** Han Yu; Hao Zou; Xingxuan Zhang; Zhengyi Wang; Yue He; Kehan Li; Peng Cui
- **Reference count:** 7
- **Primary result:** Proposes RiskyDiff to generate risky samples that fool target models while maintaining category conformity, outperforming existing methods in risk level, quality, and conformity.

## Executive Summary
This paper tackles the challenge of generating "risky" samples that deceive a target classifier while maintaining semantic conformity to the conditioned category. Existing diffusion-based approaches often introduce label noise by generating samples that semantically drift away from the target class. The authors propose RiskyDiff, which uses both text and screened image embeddings as implicit constraints, along with an explicit conformity score, to maintain category identity while maximizing risk. The method shows significant improvements in risk level, generation quality (FID), and category conformity compared to baselines. Additionally, the generated high-conformity risky samples can enhance the generalization ability of target models when used for data augmentation.

## Method Summary
RiskyDiff is a diffusion-based method for generating risky samples that fool a target model while maintaining category conformity. It incorporates text and screened image embeddings as implicit constraints and adds an explicit conformity score. The process involves estimating Gaussian distributions of CLIP embeddings for each class, training an error predictor to screen high-risk embeddings, and guiding the diffusion sampling process with combined risk and conformity gradients. The method ensures generated samples are difficult for the target model to classify correctly while remaining semantically valid members of the conditioned class.

## Key Results
- RiskyDiff achieves higher error rates on target models while maintaining better category conformity than existing methods
- Generated samples show superior generation quality with lower FID scores
- Using high-conformity risky samples for data augmentation improves model generalization and out-of-distribution accuracy
- The method effectively closes "error slices" without introducing label noise that confuses retraining

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injecting both text and screened image embeddings acts as an "anchor" to prevent semantic drift during adversarial generation.
- **Mechanism:** Standard diffusion relies primarily on text conditioning. When guided toward "risky" regions, text alone is often insufficient to maintain class identity. By sampling initial image embeddings from the validation distribution of the target class and filtering them with an error predictor, the system initializes generation in a high-risk yet semantically valid region of the latent space.
- **Core assumption:** The CLIP embedding space is structured such that a Gaussian approximation around class means contains semantically valid but "risky" variants of the class.
- **Evidence anchors:**
  - [abstract] "incorporates the embeddings of both texts and images as implicit constraints"
  - [section 3.2] "We expect [the image embedding] to correspond to an image of the category y so that it serves as another implicit constraint."
- **Break condition:** If the validation dataset is too small or biased, the estimated Gaussian will not cover the valid risky manifold, leading to low diversity or invalid screening.

### Mechanism 2
- **Claim:** Simultaneous gradient optimization of "risk" and "conformity" allows traversing the decision boundary without crossing semantic boundaries.
- **Mechanism:** The denoising step is modified by a guidance score that combines the target model's loss with the CLIP similarity score. This creates a "tunnel" where the image is optimized to fool the specific model while maintaining a minimum similarity threshold to the class description.
- **Core assumption:** The gradients of the target model and the CLIP encoder offer a direction that maximizes one while preserving the other.
- **Evidence anchors:**
  - [abstract] "design a conformity score to further explicitly strengthen the category conformity"
  - [section 3.2] "This could guide the sampling process... towards the direction where [the score] is larger, i.e. [the model] is less likely to be [correct]."
- **Break condition:** If the gradient scale is set too high relative to the conformity coefficient, the adversarial force overpowers the semantic anchor, resulting in artifacts or label noise.

### Mechanism 3
- **Claim:** High-conformity risky samples close "error slices" more effectively than standard adversarial examples.
- **Mechanism:** Previous methods introduced label noise, which confused the model during retraining. By strictly enforcing conformity, RiskyDiff generates samples that are difficult for the model but correctly labeled. When added to the training set, these samples act as hard negatives that refine the decision boundary without introducing false positives.
- **Core assumption:** The target model's failure on these samples is due to a lack of exposure to specific difficult features, rather than an inherent impossibility of the task.
- **Evidence anchors:**
  - [abstract] "generalization ability of the models can be enhanced by augmenting training data with generated samples of high conformity."
  - [section 4.2] "images generated by [baselines] could decrease the performance... This serves as additional evidence that [their] nonconformity... brings stronger label noise."
- **Break condition:** If the generated samples are too "risky," they may be treated as outliers by the model during retraining and ignored.

## Foundational Learning

- **Concept:** **Classifier-Free Guidance (CFG) & Gradient Guidance**
  - **Why needed here:** The core of RiskyDiff is manipulating the sampling trajectory of a diffusion model. You must understand how external gradients are injected into the ε-prediction to steer the generation.
  - **Quick check question:** How does the gradient scale s in Eq. 2 affect the trade-off between generation quality and adherence to the guidance condition?

- **Concept:** **CLIP Latent Space Geometry**
  - **Why needed here:** The method relies on CLIP image embeddings and text embeddings aligning semantically.
  - **Quick check question:** Why is the inner product h(x̂) · y_text used as the conformity score, and what does a low score imply about the generated image?

- **Concept:** **Domain Generalization & Error Slices**
  - **Why needed here:** The ultimate goal is not just to attack, but to fix the model.
  - **Quick check question:** Why would adding "risky" samples that conform to the class improve Out-of-Distribution (OOD) accuracy, as shown in Table 5?

## Architecture Onboarding

- **Component map:**
  - Gaussian Estimator -> Error Predictor MLP -> Stable-unCLIP Backbone -> Guidance Engine

- **Critical path:**
  1. **Pre-processing:** Train error predictor on validation embeddings/errors; compute class Gaussian statistics.
  2. **Initialization:** Sample z_T ~ N(0, I).
  3. **Conditional Embedding:** Sample c ~ N(μ, σ²); reject if error predictor(c) ≠ 1.
  4. **Denoising Loop:** At step t, predict x̂, calculate risk/conformity, compute gradient ∇S, update z_t → z_{t-1}.

- **Design tradeoffs:**
  - **Gradient Scale (s) vs. Conformity (λ):** High s yields high error rates but degrades FID and semantic coherence. λ is the stabilizer; increasing it lowers risk but ensures the sample remains a valid member of the class.
  - **Screening Threshold:** Aggressive screening ensures high risk but may slow down generation if the rejection rate is high.

- **Failure signatures:**
  - **Semantic Drift:** Generated "dog" looks like a "cat" (Conformity constraint λ too low or s too high).
  - **Low Risk:** Target model predicts correctly (Screening ineffective or gradient guidance scale s too low).
  - **Mode Collapse:** Generated images look identical (Variance σ² estimated too low).

- **First 3 experiments:**
  1. **Hyperparameter Sweep (Ablation):** Reproduce Figure 5. Fix s, vary λ (and vice versa) on a single class (e.g., "airplane") to visualize the Risk vs. Conformity frontier.
  2. **Transferability Test:** Generate samples for ResNet-50 and test against ViT-B/16 (Table 2) to verify if the "risk" is specific to the architecture or the semantic boundary.
  3. **Retraining Loop:** Generate 100 risky samples for a failing class, augment the training set, retrain, and measure accuracy delta. Compare against adding standard "unCLIP" samples to verify the signal-to-noise ratio of the labels.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the Gaussian assumption for embedding sampling limit the discovery of risky samples in categories with complex, multi-modal feature distributions?
- **Basis in paper:** [inferred] The method samples image embeddings from a Gaussian distribution N(μ_y, diag(σ²_y)) estimated from validation data.
- **Why unresolved:** Real-world categories often exhibit intricate, multi-modal structures that a single diagonal covariance matrix cannot capture, potentially restricting the diversity of generated risky samples.
- **Evidence would resolve it:** Ablation studies comparing the current sampling method against multi-modal distribution estimators to see if risk and diversity improve.

### Open Question 2
- **Question:** To what extent does the accuracy of the error predictor g_φ used in embedding screening impact the computational efficiency and final transferability of the generated samples?
- **Basis in paper:** [inferred] The authors introduce an MLP error predictor for screening but state, "We do not require this error predictor to be very precise."
- **Why unresolved:** A weak predictor might fail to filter non-risky embeddings, reducing efficiency, while an overfitted predictor might introduce bias towards the target model, reducing sample transferability to other architectures.
- **Evidence would resolve it:** Analysis correlating the AUC of the error predictor with the final error rate of generated samples across various target and transfer models.

### Open Question 3
- **Question:** Can the proposed conformity constraints be effectively adapted for dense prediction tasks like semantic segmentation?
- **Basis in paper:** [inferred] The paper restricts the evaluation to image classification datasets.
- **Why unresolved:** Defining "category conformity" via global CLIP embeddings may not suffice for dense tasks where preserving spatial-semantic alignment and object boundaries is critical for the label to remain valid.
- **Evidence would resolve it:** Applying RiskyDiff to a segmentation benchmark to evaluate if the conformity guidance can maintain mIoU while maximizing pixel-wise prediction error.

## Limitations
- The Gaussian approximation of CLIP embeddings may not capture complex, multi-modal class distributions
- The effectiveness of the error predictor depends heavily on the size and quality of the validation dataset
- Assumes target model weaknesses can be addressed through synthetic examples rather than requiring architectural changes

## Confidence
- **High Confidence:** The core mechanism of combining text and image embeddings as implicit constraints, and demonstration that conformity-enforced risky samples outperform nonconforming alternatives in augmentation experiments
- **Medium Confidence:** The specific formulation of the conformity score and its gradient-based implementation, as the paper doesn't fully explore alternative formulations or their impact on sample diversity
- **Low Confidence:** The scalability of the screening process to larger datasets or more complex image domains, as the paper primarily validates on relatively constrained datasets like CIFAR-100 and NICO++

## Next Checks
1. **Ablation Study on Embedding Screening:** Remove the error predictor screening step and compare risk levels to validate whether screening genuinely selects higher-risk samples or if gradient guidance alone is sufficient.

2. **Cross-Domain Generalization Test:** Generate risky samples for classes in CIFAR-100 and evaluate their effectiveness on a completely different dataset (e.g., ImageNet subsets) to test whether generated samples capture universal "risky" features or are dataset-specific.

3. **Real-World Deployment Scenario:** Simulate a practical use case where the target model is continuously updated with newly generated risky samples. Measure the convergence rate and ultimate robustness compared to standard adversarial training or data augmentation methods.