---
ver: rpa2
title: Instability in Downstream Task Performance During LLM Pretraining
arxiv_id: '2510.04848'
source_url: https://arxiv.org/abs/2510.04848
tags:
- score
- training
- example
- task
- checkpoint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates instability in downstream task performance
  during LLM pretraining, where evaluation scores frequently fluctuate despite long-term
  improvement. The authors observe this instability across multiple task categories
  and model sizes, occurring both at aggregate and individual example levels.
---

# Instability in Downstream Task Performance During LLM Pretraining

## Quick Facts
- arXiv ID: 2510.04848
- Source URL: https://arxiv.org/abs/2510.04848
- Authors: Yuto Nishida; Masaru Isonuma; Yusuke Oda
- Reference count: 22
- Primary result: Checkpoint averaging and ensemble methods reduce instability in downstream task scores during LLM pretraining without modifying training

## Executive Summary
This study investigates frequent fluctuations in downstream task performance during LLM pretraining, where evaluation scores oscillate despite long-term improvement. The authors observe this instability across multiple task categories and model sizes, occurring both at aggregate and individual example levels. To address this issue, they propose checkpoint integration methods—checkpoint averaging and ensemble—that aggregate neighboring checkpoints during inference. These methods reduce score variance and improve mean performance without modifying the training process.

## Method Summary
The study pretrains LLMs (150M to 13B parameters) on the LLM-jp Corpus v3 using frequent checkpoint saving. Downstream task evaluation is performed on 9 task categories using 4-shot inference with llm-jp-eval v1.4.1. Two integration methods are implemented: checkpoint averaging (arithmetic mean of parameters over a window) and checkpoint ensemble (majority vote on predictions). Stability is measured using Mean Total Variation (MTV) and Instability Score (IS) on the final 20% of checkpoints, comparing raw checkpoint scores against integrated versions.

## Key Results
- Downstream task scores fluctuate frequently during pretraining, even in later stages, across multiple task categories and model sizes
- Checkpoint averaging reduces score variance and improves mean performance, with MTV reduction guaranteed mathematically under small parameter change assumptions
- Checkpoint ensemble stabilizes individual example predictions through majority voting, particularly effective for multiple-choice and NLI tasks
- Scaling model size does not consistently reduce evaluation instability, suggesting the phenomenon is intrinsic to pretraining dynamics

## Why This Works (Mechanism)

### Mechanism 1: Parameter Averaging Reduces Score Variance
Checkpoint averaging smooths parameter trajectories in weight space, reducing mean total variation. The authors show theoretically that if parameter changes between adjacent steps are small, the score of the averaged model approximates the average of individual checkpoint scores, guaranteeing lower variance through the triangle inequality.

### Mechanism 2: Ensemble Voting Filters Transient Errors
Individual examples oscillate between correct and incorrect predictions across checkpoints. Averaging predictions or using majority voting cancels out these transient errors by prioritizing signals that persist across the window.

### Mechanism 3: Instability Independent of Model Scale
The study finds no consistent decrease in instability with larger model sizes, suggesting the noise causing fluctuations is intrinsic to optimization dynamics or data ordering rather than model capacity.

## Foundational Learning

- **Stochastic Weight Averaging (SWA)**: Understanding how averaging weights moves models toward flat loss landscape basins helps explain why checkpoint averaging improves generalization and stability. Quick check: Does averaging weights from two distinct training runs yield the same benefits as averaging checkpoints from a single run?

- **Optimization Noise vs. Signal**: Distinguishing between long-term improvement (signal) and short-term fluctuations (noise) is essential for accepting that averaging is valid rather than blurring distinct learning phases. Quick check: If a downstream metric drops monotonically for 1000 steps, is that "instability" or "catastrophic forgetting"?

- **Generalization Gap / Evaluation Variance**: The study focuses on downstream metrics (zero/few-shot) which have higher and different variance behavior than training loss. Quick check: Why might training loss decrease smoothly while downstream task accuracy fluctuates jaggedly?

## Architecture Onboarding

- **Component map**: Checkpoints Θ = {θ₁, ..., θₘ} → Integration Module (Path A: Averaging, Path B: Ensemble) → Integrated model or aggregated predictions

- **Critical path**: Frequent checkpoint saving availability is the bottleneck. Methods require a window of n neighbors (e.g., n=20). If checkpoint intervals are too large, the first-order approximation assumption breaks.

- **Design tradeoffs**:
  - Averaging: Lower inference cost (1 forward pass), improves mean performance more, requires loading/storing separate weight files
  - Ensemble: n× inference cost, no weight manipulation required, better stabilizes individual example predictions

- **Failure signatures**:
  - Mode Collapse in Averaging: Checkpoints too far apart may land in low-density parameter regions
  - Delayed Detection: Large window sizes may smooth over genuine rapid capability gains
  - Insufficient Checkpoints: Too few checkpoints to observe meaningful fluctuations

- **First 3 experiments**:
  1. Plot score trajectory of a specific downstream task for last 20% of checkpoints to confirm jagged instability
  2. Sweep window sizes (n ∈ {2, 5, 10, 20}) for averaging and measure variance reduction
  3. Compare inference latency and stability improvement of averaging vs. ensemble on high-fluctuation examples

## Open Questions the Paper Calls Out

- Whether the observed instability persists or transforms after instruction tuning or alignment-based fine-tuning
- Whether adaptive checkpoint integration strategies incorporating model confidence could provide additional stabilization benefits
- Why machine translation exhibits qualitatively different (more stable) behavior compared to other downstream task categories
- How output instability relates to prediction probability dynamics and internal representations during pretraining

## Limitations

- Study focuses exclusively on pretraining dynamics without instruction-tuned or adapted models
- Ensemble method only evaluated on three task categories (MC, NLI, HE)
- Results based on Japanese-English mixed corpora, raising generalizability questions
- Theoretical bounds assume small parameter changes that may not hold in early training phases

## Confidence

- **High Confidence**: Empirical observation of downstream score instability across multiple model sizes and task categories; checkpoint averaging reduces MTV and improves mean scores
- **Medium Confidence**: Theoretical MTV reduction derivation relies on first-order approximations; ensemble effectiveness demonstrated but limited to specific task types
- **Low Confidence**: Claim that scaling doesn't resolve instability based on limited model sizes; assumption that instability represents noise lacks validation

## Next Checks

1. Measure L2 norm of parameter differences between consecutive checkpoints during final 20% of training to empirically verify first-order approximation assumptions

2. Replicate instability analysis on monolingual English corpus to determine if fluctuations are dataset-specific or intrinsic to pretraining

3. Compare instability metrics across entire training trajectory to determine whether proposed methods are beneficial throughout training or only in later stages