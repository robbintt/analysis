---
ver: rpa2
title: On Evaluating Performance of LLM Inference Serving Systems
arxiv_id: '2507.09019'
source_url: https://arxiv.org/abs/2507.09019
tags:
- latency
- evaluation
- performance
- inference
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Current LLM inference evaluation suffers from fundamental anti-patterns
  that obscure true performance characteristics and impede scientific progress. These
  anti-patterns manifest across baseline fairness, evaluation setup, and metric design,
  uniquely problematic for LLM inference due to its dual-phase nature, heterogeneous
  workloads, and strict temporal requirements.
---

# On Evaluating Performance of LLM Inference Serving Systems

## Quick Facts
- arXiv ID: 2507.09019
- Source URL: https://arxiv.org/abs/2507.09019
- Authors: Amey Agrawal; Nitin Kedia; Anmol Agarwal; Jayashree Mohan; Nipun Kwatra; Souvik Kundu; Ramachandran Ramjee; Alexey Tumanov
- Reference count: 14
- Primary result: Current LLM inference evaluation suffers from fundamental anti-patterns that obscure true performance characteristics and impede scientific progress

## Executive Summary
The paper identifies critical anti-patterns in LLM inference serving system evaluation that hinder meaningful performance comparisons and scientific progress. These anti-patterns span baseline fairness, evaluation setup, and metric design, uniquely problematic due to LLM inference's dual-phase nature, heterogeneous workloads, and strict temporal requirements. The authors propose a comprehensive checklist framework to identify and avoid these anti-patterns, demonstrated through a case study on speculative decoding evaluation where conventional metrics mislead.

## Method Summary
The authors conducted a systematic analysis of evaluation practices in LLM inference serving literature, identifying common anti-patterns through literature review and empirical case studies. They developed a comprehensive checklist framework covering eight evaluation dimensions: implementation fairness, parameter tuning, model selection, workload diversity, practical latency bounds, appropriate metric selection, performance distribution analysis, and avoiding obscuring normalization. The speculative decoding case study demonstrates how conventional metrics (Time Between Tokens, TPOT, TTFT) can mislead while consistency metrics like Fluidity Index reveal critical trade-offs between speed and generation consistency.

## Key Results
- Current evaluation suffers from anti-patterns including unfair baselines, outdated models, non-representative workloads, and misleading metrics
- A comprehensive checklist framework identifies eight evaluation dimensions to ensure rigorous assessment
- Speculative decoding evaluation exemplifies how conventional metrics mislead, requiring consistency metrics like Fluidity Index
- The methodology enables meaningful comparisons, reproducible results, and accelerates genuine progress

## Why This Works (Mechanism)
The framework works by systematically identifying and addressing evaluation anti-patterns through a structured checklist approach. By ensuring fair baseline comparisons, proper parameter tuning, representative workloads, and appropriate metrics that capture both average and tail performance, the methodology reveals true performance characteristics that conventional evaluation obscures.

## Foundational Learning
1. **Dual-phase nature of LLM inference** - Prefill (prompt processing) and decode (token generation) phases have fundamentally different performance characteristics and requirements
   - Why needed: Different optimization strategies affect these phases differently, making fair comparison complex
   - Quick check: Does your evaluation separate or account for prefill and decode phase performance?

2. **Temporal requirements in LLM serving** - Strict latency bounds and real-time generation constraints create unique evaluation challenges
   - Why needed: Traditional throughput metrics miss the real-time constraints critical for user experience
   - Quick check: Are your metrics capturing tail latencies and generation stalls?

3. **Heterogeneous workloads** - Varying sequence lengths, model sizes, and usage patterns affect performance differently
   - Why needed: Single workload evaluation doesn't represent real-world deployment scenarios
   - Quick check: Does your evaluation include diverse sequence lengths and batch sizes?

4. **Performance distribution analysis** - Median metrics alone obscure critical tail behavior and generation consistency
   - Why needed: User experience depends heavily on worst-case performance, not just averages
   - Quick check: Are you reporting percentile distributions and consistency metrics?

5. **Consistency vs. speed trade-offs** - Techniques improving average speed may degrade generation consistency
   - Why needed: Users notice generation stalls and inconsistencies more than small speed improvements
   - Quick check: Do your metrics capture generation fluidity and stall frequency?

## Architecture Onboarding

**Component Map:** Evaluation Setup -> Metrics Selection -> Performance Measurement -> Result Analysis -> Validation

**Critical Path:** The evaluation pipeline must ensure baseline fairness (comparable implementations, parameter tuning) before measuring performance, then use appropriate metrics that capture both average and tail behavior, followed by distribution analysis to validate results.

**Design Tradeoffs:** Comprehensive evaluation vs. practical adoption burden - the exhaustive checklist may be challenging for researchers to implement fully, requiring prioritization of critical criteria.

**Failure Signatures:** Misleading results occur when evaluation uses outdated models, non-representative workloads, unfair baselines, or metrics that obscure performance distributions (e.g., reporting only median latency).

**First Experiments:**
1. Apply the checklist to 3 existing LLM inference systems and identify which anti-patterns they violate
2. Compare speculative decoding performance using conventional metrics vs. Fluidity Index to demonstrate the difference
3. Evaluate the same system across multiple workload types to show performance variation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What unified decision framework can resolve conflicting metric trends when evaluating techniques like speculative decoding that improve median latency while degrading tail latency?
- Basis in paper: [explicit] The speculative decoding case study shows "TPOT shows conflicting trends between median and tail" and the authors state "Which metric should we trust? A more holistic evaluation, avoiding these anti-patterns, is necessary."
- Why unresolved: The paper demonstrates the problem and proposes using additional metrics like Fluidity Index, but does not establish a principled framework for weighting or trading off conflicting metric outcomes.
- What evidence would resolve it: A systematic study correlating different metric trade-off profiles with user satisfaction across application types, yielding decision rules for metric prioritization.

### Open Question 2
- Question: How do proposed consistency metrics (e.g., Fluidity Index) empirically correlate with actual user-perceived quality in interactive applications?
- Basis in paper: [explicit] The paper introduces Fluidity Index as revealing "critical trade-offs between average speed and generation consistency" but notes this metric is required for accurate assessment without validating it against user experience data.
- Why unresolved: While the metric captures generation consistency theoretically, the paper does not include user studies or production A/B tests validating that Fluidity Index improvements translate to measurable user experience gains.
- What evidence would resolve it: Controlled user studies measuring perceived responsiveness, task completion satisfaction, or conversation quality ratings across systems with varying Fluidity Index scores but similar throughput.

### Open Question 3
- Question: What are the minimal yet sufficient evaluation reporting requirements that balance rigorous assessment against practical adoption burden for researchers?
- Basis in paper: [inferred] The comprehensive checklist spans eight dimensions with multiple sub-criteria; the appendix assessment shows even well-regarded systems fail multiple criteria, suggesting potential barriers to adoption.
- Why unresolved: The paper provides an exhaustive framework but does not investigate whether a smaller subset of critical criteria could capture most evaluation quality issues, or how to make compliance feasible for typical publication constraints.
- What evidence would resolve it: Analysis correlating subset compliance scores with evaluation validity outcomes, identifying minimum criteria that predict robust conclusions.

### Open Question 4
- Question: How should evaluation methodologies systematically evolve to remain relevant as model architectures rapidly change (e.g., GQA, MoE, new attention variants)?
- Basis in paper: [explicit] The paper states "emerging applications continually introduce novel performance requirements and usage patterns, demanding that evaluation methodologies adapt continuously to remain meaningful" but provides no concrete mechanism for this adaptation.
- Why unresolved: While the paper identifies outdated model evaluation as Anti-Pattern 3, it does not propose how evaluation standards should keep pace with architectural innovation or whether forward-looking evaluation protocols are possible.
- What evidence would resolve it: Longitudinal study tracking how evaluation effectiveness degrades as architectures evolve, or development of architecture-agnostic evaluation primitives that remain valid across architectural changes.

## Limitations
- Checklist framework lacks specific quantitative thresholds for many recommendations
- Case study focuses on a single technique rather than broader applicability
- Does not fully address how evaluation anti-patterns differentially affect prefill vs decode phases

## Confidence

**High confidence:** Claims about anti-patterns related to outdated models and non-representative workloads are straightforward empirical observations supported by literature review.

**Medium confidence:** Speculative decoding case study findings are internally consistent but may not generalize to all LLM inference serving scenarios.

**Medium confidence:** Overall framework's practical utility requires further validation across diverse real-world deployments.

## Next Checks
1. Implement the proposed checklist framework across 3-5 diverse LLM inference systems and measure whether it identifies previously undetected anti-patterns that explain performance discrepancies between reported and actual user experiences.

2. Conduct a controlled experiment varying a single parameter (e.g., sequence length or model size) while holding all others constant to quantify how commonly violated checklist items affect measured performance metrics.

3. Perform a longitudinal study tracking how adoption of these evaluation standards affects the rate of genuine algorithmic improvements versus implementation optimizations in published LLM inference research over a 12-month period.