---
ver: rpa2
title: 'CASTILLO: Characterizing Response Length Distributions of Large Language Models'
arxiv_id: '2505.16881'
source_url: https://arxiv.org/abs/2505.16881
tags:
- dataset
- response
- prompt
- length
- wally
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CASTILLO is a large-scale dataset characterizing response length
  distributions of 13 open-source LLMs across 7 instruction-following datasets. For
  each prompt-model pair, 10 independent completions were generated under fixed decoding
  parameters, recording token lengths and statistics (mean, std-dev, percentiles).
---

# CASTILLO: Characterizing Response Length Distributions of Large Language Models

## Quick Facts
- arXiv ID: 2505.16881
- Source URL: https://arxiv.org/abs/2505.16881
- Reference count: 40
- Key outcome: CASTILLO characterizes response length distributions of 13 LLMs across 7 datasets, revealing significant inter- and intra-model variability with CV ranging from 7% to 45%, and identifies 956 text degeneration cases.

## Executive Summary
CASTILLO provides a comprehensive characterization of response length distributions across 13 open-source LLMs and 7 instruction-following datasets. For each prompt-model pair, the dataset includes 10 independent completions with computed statistics (mean, std-dev, percentiles) and flags text degeneration cases. The analysis reveals substantial variability in response lengths, with coefficients of variation ranging from 7% to 45%, and identifies patterns where smaller models and code-oriented datasets show higher degeneration rates. This dataset enables proactive scheduling research by providing a foundation for predicting response lengths and benchmarking model-specific generation behaviors.

## Method Summary
CASTILLO characterizes response length distributions by generating 10 independent completions per prompt-model pair using HuggingFace Transformers. The method employs standardized decoding parameters with do_sample=True, caps input at 2,500 tokens and output at 15,000 tokens, and computes per-sample statistics including mean, std-dev, percentiles, and coefficient of variation. The dataset construction involves 7 instruction-following datasets downsampled to ≤2,000 samples each, 13 LLMs ranging from 1B to 70B parameters, and systematic generation of length statistics while flagging degeneration cases based on output length thresholds and variance patterns.

## Key Results
- Response length distributions show significant inter-model variability, with mean response lengths varying by hundreds of tokens across models for identical prompts
- Intra-model variability is substantial, with coefficient of variation ranging from 7% to 45% across different prompt-model combinations
- Text degeneration occurs in 956 cases, clustering in smaller models (particularly llama-1B) and code-oriented datasets (ShareGPT, Apps)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Response length distributions vary significantly across models even under identical prompts and decoding settings.
- Mechanism: Model-specific behaviors emerge from architectural differences, training data, and instruction-tuning procedures, collectively shaping how a model allocates tokens to different task types.
- Core assumption: The 13 models and 7 datasets sampled are representative enough to capture meaningful model-specific patterns.
- Evidence anchors:
  - [abstract] "Our analysis reveals significant inter- and intra-model variability in response lengths"
  - [section 4.1] Figure 3 heatmap shows mean response lengths varying by hundreds of tokens across models
  - [corpus] Related work on length control (LIFEBench, Progress Ratio Embeddings) confirms length regulation is model-dependent

### Mechanism 2
- Claim: Substantial intra-model variability exists even when decoding parameters are fixed.
- Mechanism: Autoregressive sampling with temperature > 0 introduces stochasticity, allowing the same prompt to produce responses differing by hundreds of tokens across independent runs.
- Core assumption: The 10 samples per prompt-model pair sufficiently characterize the response distribution.
- Evidence anchors:
  - [abstract] "coefficient of variation ranging from 7% to 45%"
  - [section 4.1] Figure 5 shows CV values across all model-dataset combinations

### Mechanism 3
- Claim: Partial text degeneration occurs in subsets of responses and is more frequent in smaller models and code-oriented datasets.
- Mechanism: Degeneration is flagged when outputs hit the 15,000-token cap or show high variance with at least one output exceeding 8,500 tokens, suggesting localized instability rather than systematic failure.
- Core assumption: The two-stage filtering heuristic correctly identifies degeneration without over-flagging benign verbosity.
- Evidence anchors:
  - [abstract] "identifies 956 text degeneration cases, often occurring in smaller models and code-oriented datasets"
  - [section 4.2] Figure 6 shows llama-1B accounts for 40% of degeneration; ShareGPT and Apps contribute most cases

## Foundational Learning

- Concept: **Autoregressive generation with sampling**
  - Why needed here: Understanding why fixed prompts produce variable-length outputs requires grasping how token-by-token sampling with temperature induces stochasticity.
  - Quick check question: If temperature = 0, would the 10 completions for the same prompt be identical?

- Concept: **Prefill vs. decode phases in LLM inference**
  - Why needed here: Proactive scheduling relies on predicting decode-phase length (memory-intensive, sequential) before it starts; prefill is fast and compute-intensive.
  - Quick check question: Which phase dominates latency for long responses, and why does this matter for resource allocation?

- Concept: **Coefficient of variation (CV) as a normalized variability measure**
  - Why needed here: CV allows comparison of variability across models with different baseline response lengths.
  - Quick check question: If model A has mean response 100 tokens with std-dev 30, and model B has mean 500 tokens with std-dev 100, which has higher CV?

## Architecture Onboarding

- Component map: Source datasets with prompts -> 13 LLMs with standardized generation settings -> 10 independent completions per prompt-model pair -> computed statistics (mean, std-dev, percentiles) -> degeneration filtering -> dataset partitioning (train/val/test)

- Critical path: Prompt selection -> tokenization -> batch generation (10 samples) -> length extraction -> statistics computation -> degeneration filtering -> dataset partitioning

- Design tradeoffs:
  - 10 samples per pair balances characterization quality vs. compute cost
  - 2,500-token input cap preserves coverage while avoiding OOM errors
  - 15,000-token output cap prevents pathological completions but may truncate valid long-form responses
  - Default decoding parameters (model-specific) prioritize realism over controlled ablation

- Failure signatures:
  - High CV (>30%) indicates unpredictable outputs for certain prompt-model combinations
  - Degeneration cases cluster in smaller models (llama-1B) and verbose datasets (ShareGPT, Apps)
  - If using the sanitized subset, degeneration patterns are excluded; if using full dataset, expect ~956 flagged cases

- First 3 experiments:
  1. Build a baseline length predictor (regression or bucketed classification) using train/validation splits and evaluate on test split
  2. Compare model-specific predictors vs. a single universal predictor to quantify the value of model-aware features
  3. Use the degeneration-only subset to train a classifier that flags high-risk prompts before generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do response length variability patterns generalize to non-transformer architectures such as state-space models (e.g., Mamba) or extended LSTM-based systems?
- Basis in paper: [explicit] Authors state in Section 6 that findings "may not generalize to alternative architectures such as state-space models or LSTM-based systems" and suggest "future iterations of CASTILLO could incorporate such models."
- Why unresolved: CASTILLO exclusively evaluated transformer-based LLMs due to their current dominance, leaving other architectures uncharacterized.
- What evidence would resolve it: Extending CASTILLO's methodology to include state-space models and comparing coefficient of variation ranges and degeneration rates against transformer baselines.

### Open Question 2
- Question: For chain-of-thought reasoning models, how can response lengths be partitioned and predicted between reasoning traces and final answers?
- Basis in paper: [explicit] Section 6 proposes "partitioning the response into two measurable segments: (1) the reasoning trace delimited by <think:> tokens and (2) the final answer" and hypothesizes "reasoning lengths may also reflect distinct architectural or model 'personalities'."
- Why unresolved: CASTILLO's current framework treats responses as monolithic units without distinguishing reasoning phases.
- What evidence would resolve it: Extending the dataset schema to segment chain-of-thought outputs and analyzing whether reasoning trace lengths exhibit distinct prediction patterns from final answer lengths.

### Open Question 3
- Question: What is the marginal effect of each decoding parameter (temperature, top-k, top-p) on response length distributions and degeneration likelihood?
- Basis in paper: [explicit] Section 6 notes that "generation configuration remains a powerful lever that modulates output variability" and calls for "model-specific controlled ablation studies to quantify the marginal effect of each decoding parameter."
- Why unresolved: CASTILLO used default or fixed decoding settings per model without systematic variation across parameters.
- What evidence would resolve it: Controlled experiments varying individual decoding parameters while holding others constant, measuring changes in mean length, coefficient of variation, and degeneration rates.

## Limitations

- The dataset lacks explicitly documented decoding hyperparameters, making exact reproduction difficult despite the paper stating these were recorded during generation
- The 15,000-token output cap may truncate potentially valid responses and bias length statistics downward for tasks requiring extended explanations
- The study's focus on autoregressive LLMs limits generalizability to other architectures like state-space models or diffusion-based text generators

## Confidence

**High Confidence**: The existence of significant inter-model variability in response lengths is strongly supported by heatmap evidence showing hundreds-of-tokens differences across models for identical prompts.

**Medium Confidence**: The claim about intra-model variability is supported by CV statistics, but the interpretation that this variability is "substantial" depends on subjective thresholds. The degeneration detection methodology relies on heuristic cutoffs that may not generalize to all use cases.

**Low Confidence**: The characterization of degeneration patterns (more frequent in smaller models and code datasets) is based on limited direct evidence. The paper provides descriptive statistics but lacks rigorous causal analysis to confirm these associations hold beyond the specific models and datasets tested.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Reproduce the generation process using CASTILLO's methodology but systematically vary temperature (0.0, 0.7, 1.5) and top-k/top-p values to quantify how decoding parameters influence length variability. Compare the resulting CV distributions to determine if the reported 7%-45% range is parameter-dependent or model-inherent.

2. **Sample Size Adequacy Test**: For a subset of high-CV prompt-model pairs, generate 50-100 completions instead of 10 and perform Kolmogorov-Smirnov tests to determine whether the additional samples significantly alter the estimated mean, std-dev, or percentile values. This validates whether 10 samples provide sufficient characterization for scheduling decisions.

3. **Degeneration Pattern Validation**: Manually examine 100 randomly selected cases flagged as degeneration and 100 cases just below the degeneration threshold (std-dev < 2×mean but >8,000 tokens) to compute precision and recall of the heuristic. Additionally, test whether the degeneration pattern persists when generating with temperature=0 to isolate sampling-induced degeneration from model capability limitations.