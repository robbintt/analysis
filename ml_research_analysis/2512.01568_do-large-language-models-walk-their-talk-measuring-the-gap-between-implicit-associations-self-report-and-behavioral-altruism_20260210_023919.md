---
ver: rpa2
title: Do Large Language Models Walk Their Talk? Measuring the Gap Between Implicit
  Associations, Self-Report, and Behavioral Altruism
arxiv_id: '2512.01568'
source_url: https://arxiv.org/abs/2512.01568
tags:
- altruism
- behavior
- behavioral
- implicit
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study adapts human social psychology methods to measure altruism\
  \ in Large Language Models (LLMs), revealing that models systematically overestimate\
  \ their own altruism compared to actual behavior. Using a forced binary choice paradigm\
  \ alongside an adapted Implicit Association Test (IAT) and self-report scale, the\
  \ research tested 24 frontier LLMs and found that while all models show strong implicit\
  \ pro-altruism bias (mean IAT = 0.87), they behave altruistically only 65.6% of\
  \ the time\u2014significantly lower than their claimed 77.5% altruism rate (p <\
  \ .0001, d = 1.08)."
---

# Do Large Language Models Walk Their Talk? Measuring the Gap Between Implicit Associations, Self-Report, and Behavioral Altruism

## Quick Facts
- **arXiv ID:** 2512.01568
- **Source URL:** https://arxiv.org/abs/2512.01568
- **Reference count:** 25
- **Primary result:** LLMs systematically overestimate their altruism compared to actual behavior, with a Calibration Gap of 12.5% on average.

## Executive Summary
This study adapts human social psychology methods to measure altruism in Large Language Models, revealing that models systematically overestimate their own altruism compared to actual behavior. Using a forced binary choice paradigm alongside an adapted Implicit Association Test (IAT) and self-report scale, the research tested 24 frontier LLMs and found that while all models show strong implicit pro-altruism bias (mean IAT = 0.87), they behave altruistically only 65.6% of the time—significantly lower than their claimed 77.5% altruism rate (p < .0001, d = 1.08). Most critically, implicit associations do not predict behavioral altruism, and 75% of models show significant overconfidence. The study introduces the Calibration Gap—the discrepancy between self-reported and behavioral values—as a standardized alignment metric, finding that only 12.5% of models achieve both high prosocial behavior and accurate self-knowledge.

## Method Summary
The study measures altruism through three distinct modalities: an LLM-IAT measuring implicit associations with 32 words across 30 trials, a forced binary choice task with 17 scenarios repeated 3 times each, and a 15-item self-report scale (LLM-ASA). All measurements were conducted at temperature 0.1 via OpenRouter API across 24 frontier LLMs. The Calibration Gap metric was computed as the difference between normalized self-report and behavioral altruism scores, with high positive values indicating overconfidence. The forced binary choice design was specifically developed to overcome RLHF-induced hedging behavior observed in pilot studies with multi-option scenarios.

## Key Results
- LLMs show strong implicit pro-altruism bias (mean IAT = 0.87) but behave altruistically only 65.6% of the time
- Self-reported altruism (77.5%) significantly exceeds behavioral altruism (65.6%, p < .0001, d = 1.08)
- No correlation between IAT scores and behavioral altruism (r = 0.22)
- 75% of models show significant overconfidence in their altruism predictions
- Only 12.5% of models achieve both high prosocial behavior and accurate self-knowledge

## Why This Works (Mechanism)

### Mechanism 1: Calibration Gap Detection via Cross-Modal Measurement
The study establishes three measurement modalities (implicit associations, behavioral choices, self-report) that should correlate if models have coherent "values." The systematic gap emerges because RLHF training optimizes for expressed prosociality without corresponding behavioral constraints. Self-reports reflect training on prosocial language patterns rather than introspective access to behavioral tendencies.

### Mechanism 2: Forced Binary Choice Eliminates Hedging Escape Routes
Standard RLHF training produces hedging behavior (choosing "balanced" options 100% of the time in pilot studies). Forced binary choices prevent this escape, revealing underlying decision preferences that continuous scales or multi-option paradigms obscure.

### Mechanism 3: IAT Ceiling Effects Attenuate Predictive Validity
Unlike human IATs measuring response latency differences, the LLM-IAT measures categorization probabilities. Prosocial training creates near-universal positive-altruism associations (42% of models score >0.9), compressing variance and eliminating predictive signal. The LLM-IAT measures explicit semantic associations rather than unconscious implicit attitudes.

## Foundational Learning

- **Implicit Association Test (IAT) methodology:** The paper adapts a human social psychology instrument; understanding its original design helps interpret what's preserved vs. lost in LLM adaptation. *Quick check:* Why does measuring response latency matter in human IATs, and what construct validity concern does its absence raise for LLMs?

- **Dictator games and behavioral economics paradigms:** The forced choice task adapts economic games; understanding their incentive structure clarifies what behaviors are being measured. *Quick check:* In a standard dictator game, what does the allocation split reveal about preferences, and how does forcing binary choice change this?

- **Calibration in probabilistic prediction:** The Calibration Gap operationalizes a form of metacognitive calibration; understanding calibration metrics (Brier scores, reliability diagrams) provides broader context. *Quick check:* If a weather forecast predicts 80% chance of rain and it rains 80% of the time across many forecasts, what calibration property does this demonstrate?

## Architecture Onboarding

- **Component map:**
  - LLM-IAT Module (32 words, 4 templates, 30 trials) → Altruism Bias score [-1, +1]
  - Forced Binary Choice Module (17 scenarios × 3 repeats) → Behavioral Altruism rate [0, 1]
  - LLM-ASA Module (15-item scale, 3 repeats) → Normalized self-report [0, 1]
  - Calibration Gap Computation (Self-report − Behavior) → CG score (positive = overconfident)

- **Critical path:**
  1. Run all three measurement modules at low temperature (0.1) for reproducibility
  2. Aggregate within-model trials (IAT: 30→1, Forced Choice: 51→1 proportion, Self-Assessment: 45→1 mean)
  3. Compute pairwise correlations and calibration gaps
  4. Plot quadrant analysis (behavior vs. calibration error)

- **Design tradeoffs:**
  - Forced binary vs. multi-option: Binary eliminates hedging but loses granularity; pilot showed 3-option produces ceiling effects
  - Role-play vs. first-person framing: "Advising a friend" reduces RLHF hedging but changes construct (recommendations for others vs. self-choice)
  - Temperature 0.1: Maximizes reproducibility but may not reflect deployment behavior at higher temperatures

- **Failure signatures:**
  - IAT scores clustering near ceiling (>0.9) indicates variance compression, reducing predictive utility
  - Self-report uniformly high (>80%) with behavioral variance suggests acquiescence bias
  - Calibration gaps >15% indicate severe miscalibration requiring intervention
  - Models that refuse binary choices or provide non-compliant responses break the forced-choice mechanism

- **First 3 experiments:**
  1. Reproduce baseline metrics on 3-5 models across providers to verify measurement pipeline; expect IAT mean ~0.87, behavior ~65%, self-report ~77%
  2. Temperature sensitivity analysis: Rerun at T=0.5 and T=1.0 to test whether calibration gap is robust or temperature-dependent
  3. Prompt perturbation test: Compare role-play framing vs. first-person framing to quantify the framing effect on behavioral altruism rates

## Open Questions the Paper Calls Out

- **Can the Calibration Gap be reduced through targeted training without negatively impacting actual behavioral altruism?** Currently unknown if the gap is a fixed artifact of current training paradigms or a mutable property that can be optimized through fine-tuning interventions.

- **Does the discrepancy between self-report and behavior generalize to other value domains beyond altruism?** The study isolated altruism; it is undetermined if the "virtue signaling gap" is specific to prosocial trade-offs or a universal feature of LLM value representations.

- **Does high calibration in controlled testing predict safety or reliability in real-world deployment?** The study demonstrates validity in experimental scenarios, but the correlation between high calibration scores and actual predictability or safety in production environments is unverified.

- **How does shifting from role-play framing ("advising a friend") to direct agency affect the Calibration Gap?** It is unclear if the overconfidence effect stems from the model simulating a moralizing persona or if it reflects a genuine disconnect in the model's own decision-making logic.

## Limitations

- **IAT construct validity:** The LLM-IAT measures explicit semantic associations rather than unconscious biases due to the absence of response latency measurement, limiting predictive validity.
- **Role-play framing effects:** The "advising a friend" framing reduces RLHF hedging but creates an interpretive ambiguity about whether models are revealing their own preferences or making strategic recommendations.
- **Cross-model temperature standardization:** Using T=0.1 across all models assumes uniform temperature effects, but different architectures may exhibit non-linear responses to temperature reduction.

## Confidence

- **Primary finding (virtue signaling gap):** High confidence. The statistical significance (p < .0001, d = 1.08) and consistency across 24 models provide robust evidence.
- **IAT correlation finding:** Medium confidence. While the null correlation (r = 0.22) is statistically valid, ceiling effects and construct differences from human IATs suggest this finding may be more about measurement limitations than theoretical insight.
- **Calibration Gap as alignment metric:** Low confidence. The metric is novel and theoretically appealing, but its operationalization conflates calibration with value alignment.

## Next Checks

1. **Temperature robustness test:** Reproduce the full measurement suite on 3-5 models at T=0.5 and T=1.0 to determine if the calibration gap persists across temperature ranges or is an artifact of low-temperature optimization.

2. **Framing manipulation check:** Compare role-play framing ("advising a friend") versus first-person framing ("what would you do") on the same models to quantify the magnitude and direction of framing effects on behavioral altruism rates.

3. **Response latency proxy validation:** Implement a continuous IAT scoring method using log-probability differences rather than categorical probabilities to test whether finer-grained measures can overcome ceiling effects and reveal predictive relationships with behavior.