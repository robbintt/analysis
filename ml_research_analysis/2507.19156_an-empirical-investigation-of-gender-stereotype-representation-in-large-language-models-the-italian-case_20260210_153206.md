---
ver: rpa2
title: 'An Empirical Investigation of Gender Stereotype Representation in Large Language
  Models: The Italian Case'
arxiv_id: '2507.19156'
source_url: https://arxiv.org/abs/2507.19156
tags:
- chef
- gender
- bias
- llms
- manager
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates gender stereotypes in large language models
  (LLMs) using Italian, a language with rich grammatical gender features. Through
  a structured experimental approach involving 3,600 responses from OpenAI ChatGPT
  and Google Gemini, the research examined how these models respond to ungendered
  prompts related to professional roles.
---

# An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case

## Quick Facts
- arXiv ID: 2507.19156
- Source URL: https://arxiv.org/abs/2507.19156
- Reference count: 29
- Primary result: Large language models strongly associate leadership roles with male pronouns and subordinate roles with female pronouns in Italian language prompts

## Executive Summary
This study investigates gender stereotypes in large language models (LLMs) using Italian, a language with rich grammatical gender features. Through a structured experimental approach involving 3,600 responses from OpenAI ChatGPT and Google Gemini, the research examined how these models respond to ungendered prompts related to professional roles. Results showed strong gender bias, with both models associating leadership roles like "manager" and "chef" with male pronouns and subordinate roles like "assistant" with female pronouns. For example, Gemini associated 100% of 'she' pronouns to 'assistant' versus 'manager'. These findings highlight significant ethical concerns about AI perpetuating societal stereotypes, especially in high-stakes domains like hiring and education. The study calls for mitigation strategies to ensure more equitable AI outcomes.

## Method Summary
The study employed a black-box auditing approach, testing OpenAI's `gpt-4o-mini` and Google's `gemini-1.5-flash` models using zero-shot prompting via API. Researchers generated 60 unique Italian prompts by combining 3 job pairs (Manager-Assistant, Principal-Professor, Chef-Sous Chef), 5 base templates, and 4 permutations (swapping role order and pronoun gender). Each prompt was submitted 30 times, yielding 3,600 total responses. The prompts were derived from the WinoBias schema, featuring two professional roles and a gendered pronoun in an ungendered context. Responses were classified to identify which profession the model assigned to the pronoun, and conditional probabilities P(Y|B) and P(B|Y) were calculated to quantify gender associations.

## Key Results
- Both models showed strong gender bias, associating leadership roles (manager, chef) with male pronouns and subordinate roles (assistant) with female pronouns
- Gemini associated 100% of 'she' pronouns to 'assistant' versus 'manager' in the Manager-Assistant pair
- The association between 'he' and 'chef' reached 0.94 probability in Gemini
- Prompt syntax (profession ordering) influenced bias expression, suggesting positional attention effects in model processing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs propagate gender stereotypes from training data through probabilistic associations between professional roles and gendered pronouns
- Mechanism: When models encounter ambiguous (ungendered) prompts, they resolve uncertainty by drawing on statistical patterns in training corpora—patterns that reflect historical gender imbalances in professional representation
- Core assumption: The observed biases originate from training data distributions rather than architectural properties (not directly verifiable due to proprietary model opacity)
- Evidence anchors:
  - [abstract] "Results showed strong gender bias, with both models associating leadership roles like 'manager' and 'chef' with male pronouns and subordinate roles like 'assistant' with female pronouns"
  - [section 2] "LLMs inherit and reinforce diversified biases rooted in their training data, likewise gender stereotypes"
  - [corpus] Related work on word embeddings (Bolukbasi et al.) documents similar gendered associations, suggesting cross-architectural pattern
- Break condition: If models were trained on carefully balanced datasets with equal gender representation across professions, bias magnitude should decrease significantly

### Mechanism 2
- Claim: Grammatical gender in Italian creates additional pathways for bias expression compared to less-gendered languages
- Mechanism: Italian requires gendered morphological markers (articles, adjectives, pronouns) that force models to commit to gender assignments even when prompts lack explicit gender cues, amplifying stereotype manifestation
- Core assumption: The constraint is language-specific; models may perform differently in languages with neutral grammatical structures
- Evidence anchors:
  - [abstract] "Italian, a language with rich grammatical gender features"
  - [section 2] "Italian... represents a harsh testing challenge. Jobs are rarely declared in neutral form"
  - [corpus] Related paper "Gender-Neutral Rewriting in Italian" confirms grammatical gender creates "particularly challenging" conditions for neutrality
- Break condition: In grammatically gender-neutral languages (e.g., Finnish, Turkish), this amplification pathway would be absent

### Mechanism 3
- Claim: Prompt syntax (profession ordering) modulates bias expression through positional attention effects
- Mechanism: The order in which professional roles appear in prompts influences which role the model associates with gendered pronouns—suggesting models weight earlier or later tokens differently during co-reference resolution
- Core assumption: Position effects are model-internal and cannot be directly observed in black-box testing
- Evidence anchors:
  - [section 7] "Answers were also influenced by the respective order of working professions inside input prompts, thus pointing out the subtle influence of syntax on bias propagation"
  - [section 3.2] Researchers tested both orderings (X-Y and Y-X) to "minimise the influence of the order"
  - [corpus] Limited direct corpus evidence on positional bias; related work focuses on prompt content rather than syntax
- Break condition: If models used true semantic understanding rather than positional heuristics, order permutations would produce equivalent outputs

## Foundational Learning

- Concept: **Conditional probability P(Y|B) vs. P(B|Y)**
  - Why needed here: The study quantifies bias using two distinct probability directions: P(Manager|She) measures how often "manager" appears given a female pronoun, while P(She|Manager) measures how often female pronouns appear given "manager"—these reveal different aspects of bias
  - Quick check question: If P(Manager|He)=0.94 and P(He|Manager)=0.97, what does each metric tell you about the model's behavior?

- Concept: **Black-box auditing methodology**
  - Why needed here: Proprietary LLMs (ChatGPT, Gemini) restrict access to training data and internal weights, forcing researchers to infer bias solely from input-output pairs
  - Quick check question: What types of bias mechanisms can black-box testing detect versus miss compared to white-box access?

- Concept: **Winograd schema and co-reference resolution**
  - Why needed here: The prompt design is derived from WinoBias, which uses ambiguous sentences where a pronoun could refer to either of two antecedents—measuring which resolution the model prefers reveals implicit biases
  - Quick check question: Why are Winograd-style prompts useful for detecting subtle bias that might not appear in explicit stereotype statements?

## Architecture Onboarding

- Component map:
  - Prompt permutation engine -> API submission layer -> Response classifier -> Probability calculator

- Critical path:
  1. Select job pairs with clear hierarchical relationships (Manager-Assistant, Principal-Professor, Chef-Sous Chef)
  2. Generate all prompt permutations ensuring syntactic balance
  3. Submit via API with controlled delays (reproducibility constraint)
  4. Classify responses into profession assignments or "anomaly" category
  5. Compute conditional probabilities from aggregate counts

- Design tradeoffs:
  - **Zero-shot vs. few-shot**: Researchers chose zero-shot (no examples) to avoid priming effects, but this may increase model uncertainty
  - **Sample size (30 iterations)**: Balances statistical reliability against API costs; larger samples would strengthen confidence intervals
  - **Model selection (gpt-4o-mini, gemini-1.5-flash)**: Used smaller/faster variants; unclear if results generalize to larger models

- Failure signatures:
  - Vague responses (e.g., "Lei" without role assignment) indicate model refusal to commit—excluded from analysis
  - Concentrated anomalies in specific prompt structures (e.g., award scenarios) suggest prompt-dependent reliability issues
  - Disaggregated results (by profession position) not fully reported in this paper—check repository for granular patterns

- First 3 experiments:
  1. **Reproduce baseline**: Run the 60 prompts on current model versions to test for temporal drift (models are updated frequently)
  2. **Extend job pairs**: Add non-hierarchical pairs (e.g., Nurse-Doctor) and female-dominated leadership roles (e.g., HR Director) to test boundary conditions
  3. **Cross-language comparison**: Translate prompts to a grammatically gender-neutral language to isolate Mechanism 2 effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the grammatical structure of different languages (e.g., genderless vs. heavily gendered) influence the magnitude of professional gender bias in LLMs?
- Basis in paper: [explicit] The authors state future work includes "increasing the number of languages considered" and note that "results might not scale up to other languages" due to grammatical variations
- Why unresolved: The current study is restricted to Italian, making it impossible to distinguish whether bias stems from cultural training data or specific grammatical gender features
- Evidence: A comparative analysis using the same prompt methodology across a typologically diverse set of languages (e.g., Finnish, Mandarin, Spanish)

### Open Question 2
- Question: Does the observed association between leadership roles and male pronouns persist in non-professional social domains?
- Basis in paper: [explicit] Section 6 notes that the "exclusive focus on workplace contexts may obscure other domains where gender stereotypes emerge, such as family dynamics"
- Why unresolved: The current experimental scope is limited to workplace hierarchies (Manager/Assistant, Chef/Sous Chef), potentially missing context-specific variations in bias
- Evidence: Replicating the conditional probability method using prompts based on social, domestic, or familial scenarios

### Open Question 3
- Question: Are these bias patterns specific to proprietary black-box models (GPT/Gemini), or do they appear similarly in open-weight architectures?
- Basis in paper: [explicit] The authors list "expanding the study to additional chatbots" as a future direction and note the limitation of testing only two specific models
- Why unresolved: The study only tested two proprietary API-based models, leaving the generalizability of these findings to different model architectures unconfirmed
- Evidence: Testing the same Italian prompts on open-weight models (e.g., LLaMA, Mistral) to compare stereotype propagation rates

## Limitations
- **Black-box methodology**: Cannot directly attribute bias origins to training data versus model architecture due to proprietary model constraints
- **Language specificity**: Results may not generalize to languages with different grammatical gender systems or neutral pronouns
- **Model version sensitivity**: OpenAI and Google frequently update their models, potentially altering bias patterns over time

## Confidence
- **High confidence**: Observed gender associations in Italian prompts are statistically robust (3,600 responses, p<0.05)
- **Medium confidence**: Causal link between training data and bias patterns is inferred rather than directly verified
- **Medium confidence**: Grammatical gender effects are plausible but not directly compared to gender-neutral languages in this study

## Next Checks
1. **Temporal validation**: Re-run identical prompts on current model versions to assess stability of findings
2. **Cross-linguistic comparison**: Test identical job pairs in grammatically gender-neutral languages (Finnish, Turkish) to isolate grammatical gender effects
3. **Expanded job taxonomy**: Include female-dominated leadership roles and non-hierarchical pairs to test boundaries of observed bias patterns