---
ver: rpa2
title: Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in Children's
  Literature Translation
arxiv_id: '2506.22038'
source_url: https://arxiv.org/abs/2506.22038
tags:
- features
- translation
- llms
- ratio
- nmts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether machine translations (MTs) can
  match human translations (HTs) in English-to-Chinese children's literature, focusing
  on J.M. Barrie's Peter Pan.
---

# Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in Children's Literature Translation

## Quick Facts
- arXiv ID: 2506.22038
- Source URL: https://arxiv.org/abs/2506.22038
- Reference count: 9
- One-line primary result: HTs achieve 0.9376 accuracy in distinguishing themselves from MTs, while LLMs better preserve stylistic elements like "-er suffix" than NMTs.

## Executive Summary
This study investigates whether machine translations (MTs) can match human translations (HTs) in English-to-Chinese children's literature, focusing on J.M. Barrie's *Peter Pan*. A corpus of 21 translations (7 HTs, 7 NMTs, 7 LLMs) is analyzed using 447 linguistic features across generic and creative text translation (CTT)-specific dimensions. Classification experiments reveal that HTs achieve high accuracy (0.9376) in distinguishing themselves from MTs, while intra-group classification within NMTs (0.5965) and LLMs (0.5917) is more challenging. Clustering shows HTs and NMTs are well-separated, but LLMs display heterogeneous patterns, with some aligning closer to HTs. Key differences include MTs overusing conjunctions and explicit similes ("像...一样"), while LLMs better preserve stylistic elements like "-er suffix" and repetitive expressions.

## Method Summary
The study constructs a corpus of 21 Chinese translations of *Peter and Wendy* (47,978 English tokens), including 7 HTs spanning 1929-2020, 7 NMTs generated via major APIs (DeepL, GoogleTrans, MicrosoftTrans, AmazonTrans, BaiduTrans, YoudaoTrans, NiuTrans), and 7 LLMs (ChatGPT 4o, Claude 3.5-sonnet, Gemini 1.5-flash, Kimi v1, DeepSeek v3, TowerInstruct 7b-v0.2, LaraTrans Creative) using a transcreation prompt. Text is preprocessed with LTP Base2 for segmentation, PoS, and dependency parsing. 447 features are extracted across lexical, syntactic, readability, n-gram, and CTT-specific dimensions. Chi-square ranking selects top 30 features for classification using an ensemble of five classifiers (Naïve Bayes, Logistic Regression, SVM, Decision Tree, Random Forest), with accuracy as the metric. Clustering employs k-means and hierarchical methods with ARI evaluation.

## Key Results
- HTs achieve 0.9376 accuracy in distinguishing themselves from MTs using all features
- NMTs overuse conjunctions and explicit similes ("像...一样") compared to HTs
- LLMs, particularly ChatGPT and Claude, better preserve stylistic elements like "-er suffix" than NMTs

## Why This Works (Mechanism)

### Mechanism 1: Stylometric Feature-Based Classification
Classification using linguistic features can distinguish HTs from MTs with high accuracy (0.9376). Feature engineering extracts 447 features across lexical, syntactic, readability, n-gram, and CTT-specific dimensions. Chi-square ranking selects the top 30 features, which are fed to an ensemble of five classifiers. The averaged accuracy quantifies how separable the groups are. Core assumption: Translation types leave quantifiable linguistic fingerprints that persist across translations of the same source text. Evidence: HTs-NMTs reaching the highest accuracy (0.9376) using all features. Break condition: Different source texts, genres, or language pairs may produce different discriminative features; intra-group classification for NMTs and LLMs remains near chance (~0.59).

### Mechanism 2: CTT-Specific Feature Preservation in LLMs
Prompt-tuned LLMs better preserve creative text features than NMTs. Instructional prompts (CRISPE-framed) guide LLMs toward transcreation strategies—preserving repetition, "-er suffix," and varied figurative language. This shifts behavior from source-constrained literal mapping to target-oriented stylistic adaptation. Core assumption: Prompt engineering can modulate LLM attention to stylistic elements, reducing normalization effects common in NMT. Evidence: LLMs exhibit a higher ratio of -er suffix compared to NMTs and outperform NMTs in preserving this pattern. Break condition: MT-tailored LLMs (LLT, LTI) showed no stylistic advantage; open-source models like DeepSeek performed similarly to NMTs on CTT features.

### Mechanism 3: MT Homogenization via Explicitation
NMTs overuse explicit conjunctions and fixed simile structures due to source-text fidelity bias. NMT systems optimize for surface-level alignment metrics, resulting in literal translation of logical connectors (然后) and comparative structures (像...一样). This produces "translationese" with reduced lexical variation. Core assumption: Training objectives for NMT prioritize adequacy over stylistic diversity, leading to homogenization. Evidence: MTs tend to rely more on explicit logical connectors, exhibiting a certain tendency toward "explicitation." Break condition: NMTs fine-tuned on literary corpora may reduce this effect; LLMs with creative prompts already show reduced explicitation.

## Foundational Learning

- **Stylometry/Quantitative Stylistics**: Core framework for measuring translation differences through statistical feature analysis. Quick check: Can you distinguish generic textual features from CTT-specific features?
- **Translationese**: The underlying phenomenon—systematic linguistic patterns that characterize translated vs. original texts. Quick check: What two features most strongly separated HTs from MTs in this study?
- **Feature Selection via Chi-Square Ranking**: Practical technique to reduce 447 features to 30 for efficient classification without noise. Quick check: Why might keeping all features degrade classifier performance?

## Architecture Onboarding

- **Component map**: Source text → LTP preprocessing (segmentation, PoS, dependency) → Feature extraction (447 features: lexical, syntactic, readability, n-gram, CTT-specific) → Chi-square selection (top 30) → Ensemble classification (5 classifiers) / K-means + hierarchical clustering.
- **Critical path**: Corpus construction (21 translations, same source) → Chinese NLP preprocessing via LTP → Feature engineering across both generic and CTT dimensions → Feature selection → Classification (ACC metric) and clustering (ARI metric).
- **Design tradeoffs**: Single literary work controls source variation but limits generalizability; LLM training data may contain Peter Pan translations; feature redundancy not fully addressed; quantitative-only approach lacks qualitative validation.
- **Failure signatures**: Intra-group classification near chance for NMTs (0.5965) and LLMs (0.5917); MT-tailored LLMs (LLT, LTI) underperformed on CTT features; Claude (LCL) used English quotation marks in Chinese output.
- **First 3 experiments**:
  1. Replicate with a different children's work to test feature generalizability.
  2. Add human evaluation to validate whether LLM stylistic alignment improves reader experience.
  3. Run prompt ablation tests to isolate which instructions drive CTT feature preservation.

## Open Questions the Paper Calls Out

### Open Question 1
Do LLMs maintain their stylistic alignment with human translations in children's literature genres other than fantasy, and to what extent does training data contamination influence current results? Basis: The authors note that "training data of LLMs may contain human translations of *Peter Pan*, potentially influencing the results" and suggest expanding datasets to "varied genres and styles." Why unresolved: The study relies on a single, famous source text (*Peter Pan*), making it impossible to distinguish between genuine stylistic capability and memorization of existing human translations in the training data. What evidence would resolve it: Replicating the stylometric analysis using less canonical or contemporary children's books with a lower probability of being in LLM training sets.

### Open Question 2
How do the identified stylistic deviations—such as the lack of "-er suffixes" or repetitive structures—impact the reading comprehension and engagement of the target child audience? Basis: The paper states the lack of "in-depth qualitative analysis to explain why certain stylistic deviations occur" and suggests future work incorporate "human evaluations to better understand how MT outputs impact the reading experience of child audiences." Why unresolved: The current study relies solely on quantitative stylometric measurements (distant reading) and does not include empirical data on how children perceive or process the translated texts. What evidence would resolve it: Conducting user studies with child participants to measure readability, engagement, and emotional response to HT vs. MT outputs.

### Open Question 3
Can specific prompt engineering or fine-tuning techniques effectively mitigate the "homogenisation" and lack of figurative diversity observed in NMT and lower-performing LLM outputs? Basis: The authors ask in the limitations section "whether stylistic deficiencies could be mitigated through prompt engineering or fine-tuning techniques," specifically regarding the "rigid" nature of NMT outputs and hallucinations in some LLMs. Why unresolved: The study evaluated models using a specific prompt strategy but did not systematically compare different prompt configurations or fine-tuning approaches to correct specific errors like the overuse of "像...一样". What evidence would resolve it: Experiments varying prompt constraints or utilizing domain-specific fine-tuning to see if they increase the diversity of similes and creative expressions to HT levels.

## Limitations

- Single-source focus on *Peter Pan* limits generalizability across genres or works
- Intra-group classification performance near chance (0.59-0.60) suggests heterogeneity within MT groups
- Feature extraction methodology incompletely specified, making exact replication difficult
- LLM training data may contain Peter Pan translations, introducing potential contamination
- No human evaluation to validate whether detected stylistic differences correspond to perceptible quality differences

## Confidence

**High Confidence**: HTs can be distinguished from MTs using stylometric features (0.9376 accuracy); MTs overuse conjunctions and explicit similes compared to HTs; LLMs outperform NMTs on "-er suffix" preservation.

**Medium Confidence**: LLMs preserve creative text features better than NMTs when prompted appropriately; classification and clustering methods effectively capture translation style differences.

**Low Confidence**: Generalizability across different source texts or genres; claims about LLM superiority in CTT tasks beyond this single work; extent to which detected features reflect true stylistic quality rather than measurable artifacts.

## Next Checks

1. **Cross-text validation**: Replicate the full study pipeline with a different children's literary work to test whether the same features (conjunction usage, "-er suffix", etc.) remain discriminative across texts.

2. **Human evaluation integration**: Conduct a blind reading study where native Chinese speakers rate translation quality focusing on stylistic naturalness, then correlate these ratings with the quantitative stylometric scores.

3. **Prompt engineering ablation**: Systematically remove individual instructions from the CRISPE prompt (e.g., "preserve rhythm," "use varied similes") to identify which specific guidance drives improvements in CTT feature preservation.