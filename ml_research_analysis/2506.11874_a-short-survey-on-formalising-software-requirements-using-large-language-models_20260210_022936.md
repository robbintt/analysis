---
ver: rpa2
title: A Short Survey on Formalising Software Requirements using Large Language Models
arxiv_id: '2506.11874'
source_url: https://arxiv.org/abs/2506.11874
tags:
- language
- requirements
- https
- formal
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys literature on using large language models (LLMs)
  to formalise software requirements. It identifies 35 key papers covering tools and
  techniques for translating natural language requirements into formal specifications
  across domains like Dafny, C, Java, and hardware verification.
---

# A Short Survey on Formalising Software Requirements using Large Language Models

## Quick Facts
- arXiv ID: 2506.11874
- Source URL: https://arxiv.org/abs/2506.11874
- Reference count: 40
- Primary result: Survey identifies 35 papers on using LLMs for formal software requirements, finding assertion generation more reliable (50-89% accuracy) than full contract synthesis

## Executive Summary
This paper surveys the emerging field of using large language models to translate natural language software requirements into formal specifications. The authors identify 35 key papers across multiple domains including Dafny, C, Java, and hardware verification, categorizing approaches by methodology (prompt-only vs. fine-tuning) and application (assertions vs. full contracts). The survey reveals that while LLMs show promise for assertion generation with success rates above 50%, full contract synthesis remains challenging. Neuro-symbolic integration with theorem provers and SMT solvers emerges as a key research direction for improving reliability.

## Method Summary
The authors conducted a comprehensive literature review of papers using LLMs for formal requirements specification, categorizing them by target formal language (Dafny, JML, LTL, ACSL, VeriFast), methodology (prompt-only vs. fine-tuning), and application (assertion generation vs. full contract synthesis). They analyzed tools like Laurel for Dafny assertion generation, SpecGen for JML contracts, and AssertLLM for hardware verification, noting that most works use zero-shot prompting while some employ fine-tuning or neuro-symbolic integration with theorem provers.

## Key Results
- LLM-based assertion generation achieves higher reliability (50-89% accuracy) than full contract synthesis due to smaller, well-scoped tasks
- Neuro-symbolic frameworks combining LLMs with SMT solvers/theorem provers show improved accuracy over pure LLM approaches
- Chain-of-thought prompting enables handling of complex specification tasks that single-pass prompting cannot solve
- Most surveyed work uses prompt-only approaches, with fine-tuning showing limited but measurable improvements (e.g., 21% for SpecSyn)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM-based assertion generation achieves higher reliability (50-89%) than full contract synthesis because assertions represent smaller, well-scoped units with limited context.
- **Mechanism:** Assertions focus on specific program points or behaviors (e.g., helper assertions in Dafny proofs), reducing the abstraction and contextual understanding required compared to system-wide formal specifications.
- **Core assumption:** The verification task can be decomposed into localized properties that don't require global system understanding.
- **Evidence anchors:** [abstract] "LLM-based assertion generation is more reliable than full contract synthesis, with success rates above 50% and 89% for helper assertions and hardware verification, respectively." [section 4.8] "Tasks involving smaller, well-scoped units like assertions yield more accurate results from LLMs... due to the limited context and reduced complexity compared to full contract generation."

### Mechanism 2
- **Claim:** Neuro-symbolic frameworks (LLM + SMT solver/theorem prover) improve reliability by providing structured error correction and logical validation that pure LLMs cannot guarantee.
- **Mechanism:** The LLM handles natural language interpretation and initial formalization; the symbolic verifier provides deterministic correctness checking and generates actionable feedback for refinement iterations.
- **Core assumption:** The theorem prover or SMT solver can produce human-interpretable error messages that the LLM can incorporate into subsequent iterations.
- **Evidence anchors:** [section 4.1] "Theorem prover also provides feedback for further improvements in NLI model. Error correction mechanisms can also be deployed by using the tool Explanation-Refiner." [section 4.6] "SAT-LLM performed better than ChatGPT alone, identifying 80% of conflicts with Precision of 1.00, Recall of 0.83, and F1 score of 0.91."

### Mechanism 3
- **Claim:** Iterative refinement with chain-of-thought (CoT) prompting enables LLMs to handle complex specification tasks that single-pass prompting cannot solve.
- **Mechanism:** CoT decomposes larger tasks into multi-step reasoning stages; each intermediate result drives subsequent prompting, allowing cumulative error correction and progressive refinement.
- **Core assumption:** Intermediate reasoning steps are correctly generated and don't propagate errors downstream.
- **Evidence anchors:** [section 5.1] "Chain of Thought (CoT) prompting involves a sequence of prompts producing intermediate results... These orchestrated interactions can improve LLM performance on tasks requiring logic, calculation and decision-making." [section 4.2] "nl2spec... allows users to iteratively refine translations, making formalization easier."

## Foundational Learning

- **Concept: Dafny and SMT-based verification**
  - **Why needed here:** Dafny is repeatedly referenced (Laurel framework, helper assertions) as a verification-aware language where LLM-generated assertions must satisfy the underlying SMT solver's reasoning capabilities.
  - **Quick check question:** Can you explain why a helper assertion like `assert s1 + "." + s2 == [s1[0]] + (s1[1..] + "." + s2)` helps an SMT solver prove a postcondition about string parsing?

- **Concept: Temporal logic (LTL) and formal specification languages**
  - **Why needed here:** The survey covers translation from natural language to LTL, JML, and other formal notations; understanding what makes a valid specification is prerequisite to evaluating LLM outputs.
  - **Quick check question:** What is the semantic difference between a safety property ("something bad never happens") and a liveness property ("something good eventually happens") in LTL?

- **Concept: Prompt engineering paradigms (zero-shot, few-shot, CoT, RAG)**
  - **Why needed here:** Table 1 classifies surveyed work by methodology; most use "prompt-only" approaches, with emerging CoT and RAG techniques showing promise for complex reasoning.
  - **Quick check question:** When would retrieval-augmented generation (RAG) be preferred over few-shot prompting for generating formal specifications from domain-specific requirements?

## Architecture Onboarding

- **Component map:** NL Input Layer → LLM Translation Layer (prompt engineering + optional fine-tuning) → Formal Output Layer (target specification language) → Verification Layer (SMT solver/theorem prover) → Feedback Loop (error messages → refined prompts)
- **Critical path:** NL requirements → prompt construction → LLM generation → syntax validation → verifier execution → error feedback → iterative refinement
- **Design tradeoffs:**
  - Prompt-only vs. fine-tuning: Prompt-only is faster to deploy but domain adaptation requires fine-tuning (e.g., SpecSyn's 21% improvement)
  - Assertion generation vs. full contracts: Assertions are more reliable immediately; contracts need iteration but provide stronger guarantees
  - Verifier-in-loop latency: Adding verification improves accuracy but increases per-query time significantly
- **Failure signatures:**
  - Syntactically valid but semantically wrong specifications (GPT-4o/VeriFast: "functionally reasonable" but fails verification)
  - Redundant or over-constrained specifications that prevent valid implementations
  - "Lost-in-the-middle" errors in long CoT chains (U-shaped attention bias)
  - Verifier timeout on complex specifications requiring extensive proof search
- **First 3 experiments:**
  1. **Assertion generation baseline:** Implement Laurel-style assertion generation for Dafny using zero-shot prompting on a held-out benchmark; measure syntactic correctness rate and verifier pass rate separately.
  2. **Verifier-in-loop ablation:** Compare assertion generation accuracy with and without verifier error messages in the prompt; quantify feedback loop contribution to success rate.
  3. **CoT vs. direct prompting for LTL translation:** On NL-to-LTL pairs (using datasets from Xu et al., 94.4% accuracy baseline), compare direct prompting vs. structured CoT prompting; measure accuracy degradation on multi-sentence requirements vs. single-sentence.

## Open Questions the Paper Calls Out
None

## Limitations
- The survey relies heavily on aggregated literature review without primary experimental data
- Key limitations include unknown prompt templates, model hyperparameters, and training datasets for many cited tools
- Reported accuracy metrics combine syntactic correctness and semantic validity without clear separation
- The field's rapid evolution means some cited approaches may already be superseded by newer methods

## Confidence
- **High confidence:** LLM-based assertion generation shows higher reliability than full contract synthesis; neuro-symbolic integration improves accuracy over pure LLM approaches; prompt engineering is critical for successful formalization.
- **Medium confidence:** Iterative refinement with chain-of-thought reasoning enables complex specification tasks; fine-tuning provides measurable but limited improvements over prompt-only approaches.
- **Low confidence:** Specific accuracy figures (89% for AssertLLM, 94.4% for NL-to-LTL) due to lack of methodological details and potential cherry-picking in original papers.

## Next Checks
1. **Prompt template replication test:** Implement the exact prompt engineering approach from Laurel or nl2spec using publicly available codebases; measure assertion generation accuracy on held-out benchmarks.
2. **Verification loop ablation study:** Compare assertion generation accuracy with and without verifier feedback; quantify the contribution of error messages to success rates.
3. **CoT reasoning evaluation:** Test chain-of-thought prompting on multi-sentence natural language requirements; measure accuracy degradation patterns to identify lost-in-the-middle effects.