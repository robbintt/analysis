---
ver: rpa2
title: Emergent Riemannian geometry over learning discrete computations on continuous
  manifolds
arxiv_id: '2512.00196'
source_url: https://arxiv.org/abs/2512.00196
tags:
- networks
- input
- network
- neural
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses Riemannian geometry to study how neural networks
  perform discrete computations on continuous manifolds. The authors analyze the pullback
  metric tensor across network layers, revealing that computation decomposes into
  discretizing continuous inputs and performing logical operations on discretized
  variables.
---

# Emergent Riemannian geometry over learning discrete computations on continuous manifolds

## Quick Facts
- **arXiv ID:** 2512.00196
- **Source URL:** https://arxiv.org/abs/2512.00196
- **Reference count:** 40
- **One-line primary result:** Small initial weights yield structured geometry with better generalization, while large weights produce random metrics; input noise smooths curvature corresponding to flatter posterior distributions.

## Executive Summary
This paper uses Riemannian geometry to study how neural networks perform discrete computations on continuous manifolds. The authors analyze the pullback metric tensor across network layers, revealing that computation decomposes into discretizing continuous inputs and performing logical operations on discretized variables. They show rich learning regimes (small initial weights) yield structured geometry with low-dimensional representations and better generalization, while lazy regimes produce random metrics. Input noise during training decreases curvature and smooths the geometry, corresponding to flatter posterior distributions. The approach provides a geometric framework linking continuous input manifolds to discrete task outputs, demonstrating that specific Riemannian structures emerge during learning.

## Method Summary
The paper studies MLPs learning Boolean functions (XOR, AND, OR) on continuous input manifolds like tori or planes, mapping continuous angular inputs to discrete class labels. Networks are trained with either small initial weights (rich regime) or large initial weights (lazy regime), with optional input noise. The key analysis involves computing the Jacobian of hidden layer activations with respect to input task variables, constructing the pullback metric tensor $G = J^T J$, and analyzing its trace and Gaussian curvature using the Brioschi formula. The study compares generalization performance, metric dimensionality via participation ratio, and curvature patterns across different learning regimes.

## Key Results
- Rich learning regimes with small initial weights produce structured, low-dimensional geometry that generalizes well to held-out manifold regions.
- Lazy regimes with large initial weights learn random, high-dimensional metrics that fail to generalize beyond training data.
- Input noise during training decreases Gaussian curvature near class centers and smooths the geometry, corresponding to flatter posterior distributions.
- The pullback metric tensor localizes near class boundaries to effectively discretize continuous inputs through geometric warping.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The pullback metric tensor localizes near class boundaries to effectively discretize continuous inputs.
- **Mechanism:** As the network learns, the metric tensor $G$ evolves to stretch the representational space near decision boundaries (high trace values) and compress it elsewhere. This "warping" creates steep transitions in the hidden layer, mimicking a discrete step function on a continuous manifold. Early layers effectively binarize the input angles by collapsing irrelevant dimensions (e.g., $\cos(\theta)$).
- **Core assumption:** The manifold hypothesis holds, and the input data topology is preserved through the network's embedding layers.
- **Evidence anchors:**
  - [abstract] "metric tensor becomes highly localised close to class boundaries to reflect the discretisation"
  - [section 3.2] "space near decision boundaries is stretched, while space far from them is compressed... effectively discretises the inputs"
  - [corpus] "RNNs perform task computations by dynamically warping neural representations" supports the general link between warping and computation.
- **Break condition:** If the network is strictly linear, it cannot induce the necessary curvature to fully discretize inputs or solve non-linearly separable tasks (like XOR), leading to degenerate metrics.

### Mechanism 2
- **Claim:** Small initial weights (rich regime) force the network to learn structured, low-dimensional geometric representations that generalize.
- **Mechanism:** In the rich regime ($\sigma^2 \ll 1$), the component of the metric orthogonal to the task ($G^\perp$) is small. The network learns by growing the task-relevant mode $a(t)^2 G_{task}$, resulting in a low-rank, structured metric. In the lazy regime, the random initialization dominates ($G \approx G^\perp$), resulting in a high-dimensional, unstructured metric that fails to generalize to unseen manifold regions.
- **Core assumption:** Generalization requires the internal geometry to structurally mirror the input manifold's task-relevant features rather than memorizing training points via random projections.
- **Evidence anchors:**
  - [abstract] "rich learning regimes (small initial weights) yield structured geometry... while lazy regimes produce random metrics"
  - [section 3.3] "rich learning led to lower-dimensional representations... lazy network learned random, near-orthogonal projections"
  - [corpus] "Learning Beyond Euclid" suggests generalization bounds depend on intrinsic manifold structure, aligning with the rich regime's behavior.
- **Break condition:** If the held-out test data requires extrapolation beyond the topological structure learned during training (e.g., entirely new logical constraints), even the rich geometry may fail to generalize.

### Mechanism 3
- **Claim:** Input noise during training smooths the representational geometry, reducing curvature and approximating Bayesian inference.
- **Mechanism:** Noise injection causes the network to learn a "flatter" posterior distribution. Geometrically, this flattens the manifold (reducing Gaussian curvature $K$) near class centers and reduces the magnitude of the metric tensor near boundaries, preventing the sharp "folding" associated with overconfident discrete classification.
- **Core assumption:** The network's output confidence can be directly mapped to the geometric curvature and metric gradients of the hidden layer manifold.
- **Evidence anchors:**
  - [abstract] "Input noise during training decreases curvature and smooths the geometry, corresponding to flatter posterior distributions."
  - [section 3.4] "curvature near the class centres decreased, even below zero... correlated with the model output learning the flatter posterior distribution"
  - [corpus] Weak direct corpus evidence for the specific noise-curvature link; anchored primarily in the paper's Bayesian model analysis.
- **Break condition:** If noise levels are excessively high, the curvature may invert (become negative) excessively or the metric may degrade, potentially losing the discriminative structure required for the task.

## Foundational Learning

- **Concept: Riemannian Pullback Metric**
  - **Why needed here:** This is the primary tool used to measure how the network warps the input space. It quantifies how distances on the input manifold (e.g., changes in angle) translate to distances in the hidden layer activation space.
  - **Quick check question:** If the metric tensor $G$ has a high value in a specific region of the input manifold, what does that imply about the network's sensitivity to inputs in that region?

- **Concept: Rich vs. Lazy Learning Regimes**
  - **Why needed here:** The paper demonstrates that the *same* architecture can develop fundamentally different geometries based solely on initialization scale. Understanding this distinction is critical for controlling whether the network learns structural features (Rich) or acts as a random projector (Lazy).
  - **Quick check question:** Which regime would you select if your goal was to interpret the learned features rather than just minimize training loss?

- **Concept: Gaussian Curvature on Manifolds**
  - **Why needed here:** Curvature serves as a proxy for the "discreteness" of the computation. The paper links diverging positive curvature to "folding" (discretization) and negative/flat curvature to smoothed (Bayesian) decision boundaries.
  - **Quick check question:** In the context of this paper, does high positive curvature at a point indicate a stable, flat representation or a sharp transition/fold?

## Architecture Onboarding

- **Component map:** Input Manifold ($M$) -> Embedding ($\psi$) -> Network ($\phi$) -> Analytic Core
- **Critical path:**
  1. Compute the Jacobian of the hidden layer with respect to input task variables ($\theta$), not the Euclidean inputs ($x$).
  2. Construct the Metric Tensor $G = J^T J$.
  3. Analyze the trace of $G$ (sensitivity) and the Brioschi formula for Curvature $K$ to verify discretization vs. smoothing.
- **Design tradeoffs:**
  - **Rich (Small Weights):** Prioritizes structured geometry and generalization. Risk: Slower convergence, potential for getting stuck in local minima if the task is complex.
  - **Lazy (Large Weights):** Prioritizes speed and kernel-like fitting. Risk: No feature learning, random geometry, failure to generalize to unseen manifold regions.
  - **Noise Injection:** Prioritizes uncertainty quantification (Bayesian smoothing). Risk: Reduces the sharpness of discretization, potentially lowering accuracy on clean test data.
- **Failure signatures:**
  - **Lazy Mode:** High participation ratio (dimensionality) in hidden layers; Gram matrix $W^T W$ looks like an identity matrix (random projection); Curvature map looks random/flat.
  - **Oversmoothing:** Curvature becomes universally negative or zero; metric trace is uniformly low, resulting in "uncertain" outputs (approx. 0.5) across the whole manifold.
- **First 3 experiments:**
  1. **Verify the Discretization Mechanism:** Train a small MLP (Rich regime) on the XOR-on-Torus task. Plot the trace of the hidden layer metric tensor. Confirm it peaks at the decision boundaries ($\theta = \pi/2, 3\pi/2$).
  2. **Compare Regimes:** Train two networks on the same task, one with $\sigma_{init} = 0.01$ (Rich) and one with $\sigma_{init} = 1.0$ (Lazy). Hold out a quadrant of the torus during training. Compare their generalization accuracy and the rank of their weight Gram matrices.
  3. **Test Noise Smoothing:** Retrain the Rich network with Gaussian noise on the input angles. Plot the curvature $K$ against the noise level $\sigma$. Verify that $K$ decreases (smooths) as noise increases.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the Riemannian geometric signatures of discretization and logical operation identified in MLPs emerge in more complex architectures (e.g., CNNs, Transformers) trained on real-world data?
- **Basis in paper:** [explicit] The Conclusion states that while the study focused on simple networks and synthetic manifolds, "future work could extend these analyses to more complex architectures trained on real-world data."
- **Why unresolved:** The current findings are derived exclusively from Multi-Layer Perceptrons (MLPs) with specific activation functions on synthetic manifolds (tori/planes).
- **What evidence would resolve it:** Analyzing the pullback metric and curvature in large-scale vision or language models to verify if the distinct discretization and logical operation phases appear across layers.

### Open Question 2
- **Question:** Can enforcing specific constraints on the pullback metric during training successfully impose geometric invariances or biases (e.g., rotation invariance) in neural networks?
- **Basis in paper:** [explicit] The Conclusion suggests "future work could take a prescriptive approach by manipulating representational geometry via constraints on the metric during training."
- **Why unresolved:** The current work is descriptive, characterizing geometry that emerges from standard gradient descent rather than testing the causal effects of actively shaping the geometry.
- **What evidence would resolve it:** Experiments implementing metric-based regularization terms that penalize deviations from a target geometry, followed by an evaluation of the network's invariance properties.

### Open Question 3
- **Question:** Do biological neural networks exhibit similar curvature signatures near class boundaries when learning to map continuous inputs to discrete outputs?
- **Basis in paper:** [explicit] The Conclusion proposes combining these analyses with connectivity inference tools "to study changes in representational geometry over learning in biological networks."
- **Why unresolved:** The current study is theoretical and computational, limiting its claims to artificial neural networks without biological validation.
- **What evidence would resolve it:** Applying Riemannian geometry analysis to neural recording data from biological systems performing classification tasks to check for curvature peaks at decision boundaries.

## Limitations

- **Initialization sensitivity:** The rich vs. lazy regime distinction depends critically on specific initialization scales that are not precisely specified.
- **Numerical instability:** Curvature computation near singularities (where cos(θ) → 0) may produce numerical instability and unreliable values.
- **Limited external validation:** The corpus evidence for the specific noise-curvature link is weak, relying primarily on the paper's internal Bayesian analysis.

## Confidence

- **High Confidence:** The discretization mechanism via pullback metric localization - strongly supported by both abstract claims and detailed section analysis.
- **Medium Confidence:** The rich vs. lazy learning regime distinction - well-supported by the paper but relies on specific initialization assumptions.
- **Low Confidence:** The input noise smoothing effect - primarily anchored in the paper's internal Bayesian analysis with limited external corpus support.

## Next Checks

1. **Boundary Localization Verification:** Train the XOR-on-Torus task with rich initialization (σ = 0.01) and verify the pullback metric trace peaks precisely at decision boundaries (θ = π/2, 3π/2) rather than being uniformly distributed.

2. **Generalization Regime Comparison:** Train two networks on the same task with σ = 0.01 (rich) and σ = 1.0 (lazy), hold out [−π/2, 3π/2] × [−π/2, 3π/2], and compare both generalization accuracy and the rank of their weight Gram matrices (G^T G) to confirm structured vs. random geometry.

3. **Noise-Curvature Relationship:** Retrain the rich network with increasing input noise levels (σ_noise = 0.0, 0.1, 0.5) and plot Gaussian curvature K across the torus. Verify that curvature decreases (smooths) and approaches zero/negative values as noise increases, confirming the Bayesian smoothing mechanism.