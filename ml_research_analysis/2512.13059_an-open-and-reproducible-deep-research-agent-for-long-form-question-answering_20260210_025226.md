---
ver: rpa2
title: An Open and Reproducible Deep Research Agent for Long-Form Question Answering
arxiv_id: '2512.13059'
source_url: https://arxiv.org/abs/2512.13059
tags:
- search
- answer
- research
- question
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents an open and reproducible deep research system
  for long-form question answering, which won the text-to-text track of the MMU-RAG
  competition at NeurIPS 2025. The system integrates an open-source large language
  model with an open web search API to perform iterative retrieval, reasoning, and
  synthesis in real-world open-domain settings.
---

# An Open and Reproducible Deep Research Agent for Long-Form Question Answering

## Quick Facts
- arXiv ID: 2512.13059
- Source URL: https://arxiv.org/abs/2512.13059
- Reference count: 29
- Won text-to-text track of MMU-RAG competition at NeurIPS 2025

## Executive Summary
This work presents an open-source deep research system that won the text-to-text track of the MMU-RAG competition at NeurIPS 2025. The system integrates iterative retrieval, reasoning, and synthesis using an open web search API and large language models, with preference tuning based on LLM-as-a-judge feedback to improve answer quality. The approach demonstrates significant improvements in clarity and insightfulness metrics while maintaining competitive factuality scores.

## Method Summary
The system combines an open-source large language model with an open web search API to perform iterative retrieval, reasoning, and synthesis for long-form question answering. Key components include a reranker that improves retrieval precision, a summarizer that extracts citation-tagged passages, and a research agent fine-tuned with Direct Preference Optimization (DPO) using LLM-as-a-judge feedback. The agent iteratively searches up to 5 times, integrating new information into final answers with inline citations. Preference pairs are constructed from 20 sampling runs per question, scored on clarity, insightfulness, and factuality, then filtered by minimum score-gap thresholds.

## Key Results
- Preference tuning improved clarity from 6.71 to 8.18 and insightfulness from 6.52 to 7.50
- The tuned model performs more searches (1.03→1.20) while maintaining competitive factuality scores
- Achieved highest overall score in MMU-RAG competition text-to-text track

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage retrieval with reranking and summarization improves information relevance and reduces noise for the generator.
- Mechanism: The search tool retrieves K documents, reranks them with a cross-encoder style model (Qwen3-Reranker-0.6B), and summarizes the top-N into concise, citation-tagged passages before feeding them to the research agent.
- Core assumption: Summarization preserves task-relevant information while filtering redundancy, and reranking improves precision over initial retrieval.
- Evidence anchors: [abstract] "enhanced search components including reranking and summarization modules"; [Page 2, Section 2.1] "The reranker reorders the top-K documents... selects the most relevant ones... The summarizer extracts information relevant to the given query and previous reasoning steps"; [corpus] Related work RAG-BioQA (arXiv:2510.01612) shows retrieval-augmented frameworks benefit long-form QA, though in biomedical domain—limited direct evidence for general open-domain.

### Mechanism 2
- Claim: Direct Preference Optimization (DPO) using LLM-as-a-judge feedback shifts model outputs toward higher clarity and insightfulness.
- Mechanism: Generate multiple answers per question (20 samples), score them with o3-mini on clarity/insightfulness/factuality, construct preference pairs (chosen vs. rejected), and fine-tune the research agent with DPO on reasoning traces and final answers.
- Core assumption: The judge model's preferences align with human judgments of answer quality, and the score gap threshold θ filters noisy pairs.
- Evidence anchors: [abstract] "preference tuning based on LLM-as-a-judge feedback evaluating clarity, insightfulness, and factuality"; [Page 3, Section 3.1-3.2] "We construct the DPO data using the preference scores... The answer with the highest preference score is selected as the chosen response"; [Page 4, Table 1] Clarity improved from 6.71 to 8.18, insightfulness from 6.52 to 7.50; [corpus] No direct corpus evidence for DPO on long-form QA specifically; related work (Search-R1, DeepResearcher) uses RL rather than DPO for search-augmented reasoning.

### Mechanism 3
- Claim: Iterative multi-turn search with explicit reasoning improves coverage for complex questions requiring multi-hop synthesis.
- Mechanism: The research agent receives initial search summaries, reasons about gaps, issues follow-up queries (up to 5), and integrates results into a final answer with inline citations.
- Core assumption: The agent can accurately identify when information is insufficient and formulate effective follow-up queries.
- Evidence anchors: [abstract] "iterative retrieval, reasoning, and synthesis"; [Page 2, Section 2.2] "It takes the question and the previously generated summaries as input, determines whether additional searches are needed through reasoning, and issues a new query when necessary"; [Page 4, Table 1] Search count increased from 1.03 to 1.20 after tuning, suggesting the trained agent learns to search more; [corpus] Video Deep Research Benchmark (arXiv:2601.06943) supports iterative retrieval for multi-hop reasoning in video QA—indirect support for open-domain text.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: Core training method; requires understanding how preference pairs are constructed from judge scores and how the loss optimizes policy without explicit reward modeling.
  - Quick check question: Given two candidate answers with clarity scores 8.0 and 5.0, which becomes the "chosen" sample, and what does the DPO loss optimize?

- Concept: LLM-as-a-judge evaluation
  - Why needed here: The preference tuning pipeline depends on o3-mini scoring; understanding judge prompts and scoring scales is critical for reproducing or modifying the training data.
  - Quick check question: If the judge prompt asks for 0-10 ratings but systematically avoids scores above 8, how would this affect preference pair quality?

- Concept: Retrieval-Augmented Generation (RAG) with tool use
  - Why needed here: The system is fundamentally a tool-augmented agent; understanding how search tools are invoked and how retrieved context is integrated is essential for debugging.
  - Quick check question: What happens if the search tool returns empty results for a follow-up query—how should the agent's prompt handle this gracefully?

## Architecture Onboarding

- Component map: Search Tool (ClueWeb22 API → Qwen3-Reranker-0.6B → Qwen3-Next-80B-A3B-Thinking summarizer) -> Research Agent (Qwen3-Next-80B-A3B-Thinking fine-tuned with DPO) -> iterative search calls -> final answer with inline citations

- Critical path:
  1. Question received → initial search via Search Tool → summarized results
  2. Research Agent reasons, identifies gaps, issues follow-up query (up to 5 iterations)
  3. Search Tool returns new summaries → agent integrates → final answer with citations
  4. Citation format: `(#alphanumeric)` mapping to document IDs

- Design tradeoffs:
  - More searches increase coverage but risk citation errors (0.06 → 0.09 error rate)
  - Lower θ threshold yields more training pairs (983 vs. 341) but noisier preferences
  - Factuality metric relies on noisy user-click signals (Researchy Questions only), limiting training data

- Failure signatures:
  - Citation errors: malformed IDs, missing citations for claims, or hallucinated document references
  - Premature termination: agent outputs final answer without sufficient search iterations
  - Repetitive queries: follow-up searches that duplicate initial results

- First 3 experiments:
  1. Ablate reranker: Compare Qwen3-Reranker vs. no reranking on clarity/insightfulness scores to isolate reranker contribution.
  2. Vary DPO threshold θ: Test θ ∈ {0.3, 0.5, 0.7} on held-out questions; measure quality vs. training data size tradeoff (Table 2/3 data exists—replicate with new seed).
  3. Cap search iterations: Limit to 1, 3, 5 searches and measure citation error rate vs. insightfulness to find optimal search budget.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does factuality show only marginal improvements compared to clarity and insightfulness in preference-tuned research agents?
- Basis in paper: [explicit] The authors note "factuality shows a marginal improvement (+0.41)" versus clarity (+1.47) and insightfulness (+0.98), and state they "believe this is partly because training for factuality is more difficult... as it relies on noisy user-click–based training signals and is available only for a subset of the training data."
- Why unresolved: The paper identifies the problem but does not systematically disentangle whether the bottleneck is noisy training signals, limited data coverage, or fundamental limitations of preference-based optimization for factual accuracy.
- What evidence would resolve it: Ablation studies comparing factuality gains using cleaner ground-truth signals (e.g., expert annotations) versus user-click data, and experiments varying the proportion of factuality-labeled training examples.

### Open Question 2
- Question: Does increased search activity inherently trade off against citation accuracy in deep research systems?
- Basis in paper: [explicit] Results show the tuned model performs more searches (+0.17) while exhibiting a slight increase in citation_error_rate (+0.03). The paper reports this but does not explain or address the mechanism.
- Why unresolved: It is unclear whether this correlation is causal (e.g., more sources create more citation opportunities for error) or coincidental, and whether mitigation strategies exist.
- What evidence would resolve it: Controlled experiments varying search count while holding other factors constant, plus analysis of error types in citations to determine if they stem from source multiplicity, summarization, or agent reasoning.

### Open Question 3
- Question: How does the minimum score-gap threshold (θ) for preference pair construction affect generalization versus overfitting?
- Basis in paper: [inferred] The authors construct datasets with θ ∈ {0.3, 0.5, 0.7} yielding 983, 828, and 341 pairs respectively, noting "only marginal differences among them" in evaluation. However, they do not analyze whether different thresholds affect robustness, out-of-distribution performance, or sample efficiency.
- Why unresolved: Smaller θ yields more training data but potentially noisier preferences; larger θ yields cleaner pairs but fewer samples. The tradeoff remains unexplored beyond the reported competition metric.
- What evidence would resolve it: Systematic evaluation across different question types, domains, and difficulty levels for each θ value, plus analysis of the preference score distributions in held-out data.

### Open Question 4
- Question: Can the approach scale effectively to longer research horizons beyond the current 5-search limit?
- Basis in paper: [inferred] The system limits agents to "up to 5 searches," and the training data averages ~1.2-1.4 searches per question. The paper does not address whether the preference tuning generalizes to scenarios requiring deeper, multi-step investigation.
- Why unresolved: Real-world research often requires significantly more iterative searching; it is unknown if the current training methodology produces agents that can handle extended reasoning chains without degradation.
- What evidence would resolve it: Experiments evaluating tuned models on tasks designed to require 10+ searches, measuring both answer quality and reasoning coherence as search depth increases.

## Limitations

- The factuality metric relies on noisy user-click data available only for Researchy Questions subset
- Citation accuracy degrades slightly with increased search activity (0.06→0.09 error rate)
- Access to ClueWeb22-A corpus via DeepResearchGym search API may require institutional registration

## Confidence

- High confidence: Experimental results showing improved clarity (6.71→8.18) and insightfulness (6.52→7.50) scores after preference tuning
- Medium confidence: The mechanism linking iterative search (1.03→1.20 searches) to improved answer quality
- Low confidence: The assumption that reranking with Qwen3-Reranker-0.6B meaningfully improves over initial retrieval

## Next Checks

1. Reproduce the core quality improvement: Run the exact 100-held-out Researchy Questions evaluation comparing VANILLA vs tuned models on clarity, insightfulness, and factuality using the provided evaluation prompts (Figures 6-8) and o3-mini scoring.

2. Ablate the reranker contribution: Implement the suggested experiment removing Qwen3-Reranker and comparing the resulting clarity/insightfulness scores to the full system, isolating the reranker's specific contribution to answer quality.

3. Test judge alignment bias: Generate 50 diverse answers and score them with both o3-mini (using training prompts) and human annotators on the same clarity/insightfulness scales to measure agreement and identify systematic judge biases.