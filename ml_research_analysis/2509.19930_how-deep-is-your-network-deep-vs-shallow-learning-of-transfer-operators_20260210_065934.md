---
ver: rpa2
title: How deep is your network? Deep vs. shallow learning of transfer operators
arxiv_id: '2509.19930'
source_url: https://arxiv.org/abs/2509.19930
tags:
- operator
- functions
- operators
- eigenfunctions
- ranndy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces RaNNDy, a randomized neural network approach
  for learning transfer operators and their spectral decompositions from data. The
  method randomly selects weights for hidden layers while training only the output
  layer, significantly reducing training time and resources.
---

# How deep is your network? Deep vs. shallow learning of transfer operators

## Quick Facts
- arXiv ID: 2509.19930
- Source URL: https://arxiv.org/abs/2509.19930
- Authors: Mohammad Tabish; Benedict Leimkuhler; Stefan Klus
- Reference count: 40
- The paper introduces RaNNDy, a randomized neural network approach for learning transfer operators and their spectral decompositions from data with significantly reduced computational time.

## Executive Summary
This paper introduces RaNNDy, a randomized neural network framework that learns transfer operators (Koopman, Perron-Frobenius, Schrödinger) from data by randomly selecting hidden layer weights while training only the output layer. This approach dramatically reduces training time and computational resources compared to traditional deep learning methods while avoiding hyperparameter sensitivity and slow convergence issues. The method leverages variational principles to formulate loss functions and can compute eigenfunctions directly via a closed-form solution for the output layer. RaNNDy also enables uncertainty quantification through ensemble learning over multiple random weight initializations.

## Method Summary
RaNNDy uses randomized neural networks where hidden layer weights are randomly selected and fixed, with only the output layer trained. The method formulates loss functions based on variational principles and can compute eigenfunctions directly through a closed-form solution for the output layer. For self-adjoint operators, it solves a generalized eigenvalue problem involving empirical covariance matrices. The framework supports ensemble learning for uncertainty quantification by running multiple models with different random weight initializations.

## Key Results
- RaNNDy achieves comparable accuracy to VAMPnets on benchmark systems but with significantly reduced computational time (e.g., 0.32 seconds vs 121.35 seconds for Ornstein-Uhlenbeck process)
- The method successfully identifies coherent structures in fluid dynamics and folded/unfolded states in protein molecules
- Ensemble learning provides meaningful uncertainty quantification, with higher variance in data-sparse regions of the phase space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Randomizing hidden layer weights eliminates backpropagation overhead while preserving approximation quality.
- Mechanism: Fixed random weights transform inputs into a high-dimensional feature space via the Random Feature Map (RFM): R(x) = σ(Wx + b). Only the output layer W_o is optimized, converting the problem from iterative gradient descent over all parameters to solving a linear system or eigenvalue problem for W_o alone.
- Core assumption: The random feature space spans a sufficiently rich basis for the target eigenfunctions—this depends on network width and the distribution from which W, b are drawn.
- Evidence anchors:
  - [abstract] "the weights of the hidden layers of the neural network are randomly selected and only the output layer is trained"
  - [Section 3.1] "the optimal output weight matrix W_o in (7) can either be obtained iteratively... or can be solved directly"
  - [corpus] The kooplearn library paper contextualizes operator learning but does not validate RaNNDy's specific randomization claim.
- Break condition: If the random basis is poorly conditioned or fails to capture relevant features of the operator's eigenfunctions, accuracy degrades. The paper notes this as an open problem (Section 5).

### Mechanism 2
- Claim: Variational principles enable a closed-form solution for eigenfunctions when using randomized bases.
- Mechanism: For self-adjoint operators, the Rayleigh quotient maximization leads to a generalized eigenvalue problem: C_01 W_o = C_00 W_o Λ. The matrices C_00 = Ψ_0 Ψ_0^⊤ and C_01 = Ψ_0 Ψ_1^⊤ are computed from data, and W_o is obtained via matrix inversion rather than iterative optimization.
- Core assumption: The operator is compact and self-adjoint (or can be transformed to an equivalent self-adjoint problem via forward-backward operators for non-reversible systems).
- Evidence anchors:
  - [abstract] "it is possible to compute a closed-form solution for the output layer which directly represents the eigenfunctions"
  - [Section 3.5] "we finally obtain the eigenvalue problem Ĉ_00⁺Ĉ_01 W_o = W_o Λ"
  - [corpus] Weak direct validation; spectral pollution paper discusses related eigenvalue approximation challenges.
- Break condition: For non-self-adjoint operators, singular value decomposition is required instead, using Ĉ_00⁻¹ Ĉ_01 Ĉ_11⁻¹ Ĉ_10 W_o = W_o Λ². Conditioning of C_00 and C_11 matrices is critical.

### Mechanism 3
- Claim: Ensemble learning over multiple random weight initializations provides uncertainty quantification.
- Mechanism: Multiple base learners are created by sampling different (W, b) pairs. Each produces eigenfunction estimates; the ensemble mean gives the prediction, and variance across models quantifies uncertainty. Low data density regions show higher variance.
- Core assumption: Model variance correlates with epistemic uncertainty in data-sparse regions.
- Evidence anchors:
  - [abstract] "it is possible to estimate uncertainties associated with the computed spectral properties via ensemble learning"
  - [Section 4.1.2, Figures 3e-3f] "regions with more data points have less uncertainty"
  - [corpus] No corpus papers validate ensemble uncertainty for this specific architecture.
- Break condition: If all random bases share systematic blind spots (e.g., all miss a particular frequency), ensemble variance may underestimate true uncertainty.

## Foundational Learning

- **Transfer Operators (Koopman and Perron-Frobenius)**
  - Why needed here: RaNNDy approximates spectral properties of these operators, which describe how probability densities (Perron-Frobenius) or observables (Koopman) evolve in dynamical systems.
  - Quick check question: Given a trajectory x_t → x_{t+τ}, which operator propagates densities forward and which propagates observables backward?

- **Variational Principles (Rayleigh Quotient)**
  - Why needed here: The loss functions are derived from variational principles that relate eigenvalue optimization to Rayleigh quotient maximization/minization.
  - Quick check question: For eigenvalue λ_i, what constraint ensures orthogonality to previous eigenfunctions φ_1, ..., φ_{i-1}?

- **Generalized Eigenvalue Problems**
  - Why needed here: The closed-form solution for W_o requires solving C_01 w = λ C_00 w, where C_00 and C_01 are empirical covariance matrices.
  - Quick check question: Why must C_00 be invertible (or regularized) for the eigenvalue problem to be well-posed?

## Architecture Onboarding

- **Component map:**
  - Input layer: Receives state x ∈ R^d
  - Hidden layers (fixed): Random weights W, biases b → Random Feature Map R(x) = σ(Wx + b)
  - Output layer (trainable): W_o ∈ R^{N×n} where n = number of desired eigenfunctions
  - Two modes: (1) Optimal basis approximator trains W_o iteratively via VAMP-2 score; (2) Eigenfunction approximator solves closed-form generalized eigenvalue problem

- **Critical path:**
  1. Sample (W, b) from chosen distribution (paper uses standard normal)
  2. Compute transformed data matrices Ψ_0 = R(X), Ψ_1 = R(Y) or (A∘R)(X) depending on operator
  3. Build empirical matrices Ĉ_00, Ĉ_01 (and Ĉ_11 for non-self-adjoint)
  4. Solve eigenvalue problem; extract top n eigenvectors as columns of W_o
  5. Eigenfunctions are φ_i(x) = (W_o[:,i])^⊤ R(x)

- **Design tradeoffs:**
  - Network width N: Larger N → richer basis but larger matrix operations (O(N²m) for matrix products)
  - Activation σ: Must be nonlinear; paper uses sigmoid/tanh. ReLU may produce sparse features affecting conditioning.
  - Regularization: Tikhonov regularization (γ > 0 in Eq. 7) stabilizes matrix inversion but may bias solutions.

- **Failure signatures:**
  - Ill-conditioned Ĉ_00: Eigenvalues become unstable; remedy with regularization or pseudoinverse thresholding
  - Eigenfunction accuracy degrades near domain boundaries: Caused by data sparsity (Figure 2b)
  - Uncertainty spikes in high-variance regions of phase space: Expected behavior, indicates data gaps

- **First 3 experiments:**
  1. **Ornstein-Uhlenbeck process**: 1D system with known analytic eigenfunctions (Hermite polynomials). Verify eigenvalue accuracy and compare training time vs VAMPnets baseline.
  2. **Triple-well 2D potential**: Test ensemble uncertainty quantification by running 100 models with different random seeds; confirm variance correlates with data density.
  3. **Protein folding (Chignolin)**: Apply to real molecular dynamics trajectory; compare identified folded/unfolded state counts against VAMPnets (Table 3 shows ~8282 vs 8296 folded frames).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should the distribution of weights and biases for the randomized hidden layers be optimally selected to prevent ill-conditioned matrix representations?
- Basis in paper: [explicit] The conclusion states that selecting the distribution is an "open problem" and poor choices negatively impact approximation quality.
- Why unresolved: While the method works with standard random initialization, the sensitivity of the matrix conditioning to these distributions remains unanalyzed.
- Evidence: A theoretical or empirical analysis linking specific weight distributions to the condition number of the estimated operator matrices.

### Open Question 2
- Question: What is the formal relationship between the proposed RaNNDy framework and kernel-based techniques, specifically random Fourier features?
- Basis in paper: [explicit] Section 5 identifies exploring the "relationships with kernel-based techniques and random Fourier features" as a crucial future direction.
- Why unresolved: The paper utilizes random features but does not derive the implicit kernel or compare theoretical guarantees against kernel methods.
- Evidence: A formal proof establishing RaNNDy as an approximation to a specific kernel method or Reproducing Kernel Hilbert Space (RKHS).

### Open Question 3
- Question: In what specific problem regimes does the flexibility of fully trainable networks (like VAMPnets) provide significant accuracy gains over RaNNDy's fixed random basis?
- Basis in paper: [inferred] The authors acknowledge that "VAMPnets might in general be more flexible" and RaNNDy may underperform "where a randomized basis is not optimal."
- Why unresolved: Numerical results show comparable accuracy on benchmarks, but the boundaries of the trade-off between training speed and basis flexibility are not mapped.
- Evidence: Comparative benchmarks on highly complex, non-linear dynamical systems where random projections fail to capture the necessary manifold structure.

## Limitations

- The optimal distribution for sampling random weights and biases remains an open problem, with poor choices potentially leading to ill-conditioned matrix representations
- The method may underperform in problem regimes where random feature projections cannot capture the necessary manifold structure, requiring fully trainable networks like VAMPnets
- For non-self-adjoint operators, the method requires singular value decomposition instead of the simpler generalized eigenvalue problem, potentially reducing computational advantages

## Confidence

- Core randomization mechanism (fixed hidden weights): **High confidence** - The approach of fixing random weights while training only the output layer is well-established in random feature literature and directly validated
- Closed-form eigenfunction solution: **Medium confidence** - Works under self-adjoint assumptions with variational principles, but numerical stability depends on matrix conditioning
- Ensemble uncertainty quantification: **Medium confidence** - Visualizations show expected behavior but lack external validation; ensemble variance may underestimate uncertainty if all models share systematic blind spots

## Next Checks

1. Test RaNNDy's robustness across different activation functions (ReLU, tanh, sigmoid) and random weight distributions (Gaussian, uniform) on benchmark systems with known eigenfunctions

2. Evaluate performance on non-self-adjoint operators where singular value decomposition is required, comparing accuracy and computational cost against iterative methods

3. Conduct systematic ablation studies varying network width and depth to establish scaling relationships and identify break points where random features become insufficient