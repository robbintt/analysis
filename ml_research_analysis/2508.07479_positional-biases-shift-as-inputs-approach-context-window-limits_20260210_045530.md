---
ver: rpa2
title: Positional Biases Shift as Inputs Approach Context Window Limits
arxiv_id: '2508.07479'
source_url: https://arxiv.org/abs/2508.07479
tags:
- retrieval
- reasoning
- input
- middle
- first
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how positional biases in Large Language\
  \ Models (LLMs) shift as inputs approach the context window limits. The authors\
  \ define a relative input length (Lrel) as the ratio of an input\u2019s length to\
  \ the model\u2019s context window size, and systematically vary the position of\
  \ relevant information within distractor text across seven Lrel values (0.06 to\
  \ 1.0)."
---

# Positional Biases Shift as Inputs Approach Context Window Limits

## Quick Facts
- arXiv ID: 2508.07479
- Source URL: https://arxiv.org/abs/2508.07479
- Reference count: 40
- Primary result: Positional biases in LLMs shift as inputs approach context window limits, with Lost in the Middle effect strongest at low Lrel values

## Executive Summary
This paper investigates how positional biases in Large Language Models (LLMs) change as input lengths approach context window limits. The authors introduce relative input length (Lrel) as a metric and systematically vary information positions across seven Lrel values from 0.06 to 1.0. They construct retrieval-reasoning minimal pairs to isolate positional effects from other factors. The key finding is that the Lost in the Middle effect is strongest when Lrel ≤ 0.5, but diminishes beyond this threshold as primacy bias weakens while recency bias remains stable, creating a distance-based bias favoring information closer to the end.

## Method Summary
The authors define relative input length (Lrel) as the ratio of input length to context window size and systematically vary this across seven values (0.06 to 1.0). They construct retrieval-reasoning minimal pairs where each retrieval question targets specific information needed for the corresponding reasoning question. By varying the position of relevant information within distractor text and controlling for other factors, they can isolate positional biases. The experimental design enables comparison of accuracy across different positions (first, middle, last) at varying Lrel values.

## Key Results
- Lost in the Middle (LiM) effect is strongest when Lrel ≤ 0.5, with middle-positioned information showing lower accuracy than first and last positions
- Beyond Lrel = 0.5, LiM effect diminishes as primacy bias weakens while recency bias remains stable
- Successful retrieval is prerequisite for effective reasoning, with reasoning accuracy increasing 25% to 373% when retrieval succeeds versus fails
- Positional biases in reasoning are largely inherited from retrieval, as reasoning becomes more robust to position when retrieval succeeds

## Why This Works (Mechanism)
The paper identifies that positional encoding mechanisms in transformers interact differently with input length relative to context window size. As Lrel increases beyond 0.5, the attention mechanism's ability to resolve primacy bias weakens while recency bias remains stable, creating a distance-based preference. This suggests that the model's attention patterns are sensitive to absolute position information in ways that become more pronounced as inputs fill more of the available context space.

## Foundational Learning
- Relative input length (Lrel): Ratio of input length to context window size; needed to quantify how much of available context is utilized
- Lost in the Middle (LiM) effect: Performance degradation for middle-positioned information; check by comparing accuracy across positions
- Minimal pair methodology: Constructing closely related test cases to isolate specific variables; needed to separate positional effects from other factors
- Primacy bias: Preference for initial positions; check by measuring accuracy drop at beginning vs middle
- Recency bias: Preference for final positions; check by measuring accuracy difference at end vs middle
- Distance-based bias: Preference based on proximity to sequence end; check by correlating position distance with accuracy

## Architecture Onboarding
Component map: Input sequence -> Positional encodings -> Self-attention layers -> Output representations -> Retrieval module -> Reasoning module
Critical path: Input position encoding → Attention mechanism → Information retrieval → Reasoning computation
Design tradeoffs: Fixed positional encodings vs learned position embeddings; global attention vs local attention patterns
Failure signatures: Accuracy drops in middle positions at low Lrel; weakened primacy bias at high Lrel; strong correlation between retrieval and reasoning performance
First experiments: 1) Vary Lrel systematically while measuring position-specific accuracy; 2) Compare performance across different attention mechanisms; 3) Test retrieval-only vs reasoning-only tasks to isolate effects

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Findings may not generalize across different LLM architectures or training paradigms
- Minimal pair construction may not capture all real-world retrieval-reasoning scenarios
- Analysis of bias mechanisms remains descriptive rather than providing explanatory models
- Focus on a specific set of LLMs limits broader applicability

## Confidence
- High: Systematic variation of Lrel values and observed LiM effect patterns
- Medium: Interpretation of primacy vs recency bias shifts and their relationship to Lrel
- Medium: Inheritance of positional biases from retrieval to reasoning

## Next Checks
1. Replicate findings across multiple LLM families (GPT, Claude, open-source models) to test generalizability
2. Conduct ablation studies isolating effects of attention mechanisms vs positional encodings on observed biases
3. Test whether fine-tuning on position-balanced data can mitigate identified biases across different Lrel ranges