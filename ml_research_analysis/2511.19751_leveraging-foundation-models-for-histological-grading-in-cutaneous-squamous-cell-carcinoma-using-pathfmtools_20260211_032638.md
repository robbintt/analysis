---
ver: rpa2
title: Leveraging Foundation Models for Histological Grading in Cutaneous Squamous
  Cell Carcinoma using PathFMTools
arxiv_id: '2511.19751'
source_url: https://arxiv.org/abs/2511.19751
tags:
- cscc
- foundation
- grading
- each
- slide
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce PathFMTools, a lightweight Python package
  enabling efficient exploration, analysis, and visualization of pathology foundation
  models. Using this tool, they evaluate two state-of-the-art vision-language foundation
  models (CONCH and MUSK) for histological grading in cutaneous squamous cell carcinoma
  (cSCC) across 440 H&E WSIs.
---

# Leveraging Foundation Models for Histological Grading in Cutaneous Squamous Cell Carcinoma using PathFMTools

## Quick Facts
- arXiv ID: 2511.19751
- Source URL: https://arxiv.org/abs/2511.19751
- Reference count: 31
- Primary result: Foundation models achieve AUROC up to 0.81 for cSCC grading using attention-based multiple instance learning

## Executive Summary
This paper introduces PathFMTools, a Python package for analyzing pathology foundation models, and evaluates its application to histological grading of cutaneous squamous cell carcinoma (cSCC). The authors benchmark three adaptation strategies—zero-shot classification, clustering, and attention-based multiple instance learning (ABMIL)—using two state-of-the-art vision-language models (CONCH and MUSK) across 440 H&E whole slide images. ABMIL outperforms other methods with AUROC of 0.81, demonstrating strong data efficiency and interpretability. The study highlights the potential of foundation model embeddings for clinical tumor grading while identifying key challenges around dataset diversity and clinical validation.

## Method Summary
The authors evaluate three adaptation strategies for foundation models in cSCC grading: zero-shot classification via text-image similarity, K-means clustering with logistic regression on cluster histograms, and attention-based multiple instance learning (ABMIL) that learns to weight informative patches. They use PathFMTools to preprocess WSIs (Otsu segmentation, 448×448 patches at 40× downsampled to 224×224) and generate embeddings from CONCH and MUSK models. ABMIL employs gated attention with MLP layers, trained with cyclic learning rates and evaluated via 5-fold patient-level stratified cross-validation.

## Key Results
- Zero-shot approaches achieved AUROC of 0.63 (CONCH) and 0.60 (MUSK) for cSCC grading
- K-means clustering with 25 clusters reached AUROC up to 0.75 for individual clusters and 0.76 for multivariate logistic regression
- ABMIL outperformed all methods with AUROC of 0.81 (CONCH) and 0.78 (MUSK), requiring fewer than 100 training samples to surpass logistic regression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot classification can predict tumor grade by mapping image patches to language descriptions without task-specific training.
- Mechanism: Vision-language models project images and text into shared embedding space. Cosine similarity between patch embeddings and text prompts ("poorly differentiated" vs. "well differentiated") determines grade. Slide-level score uses max-pooling of patch-level similarities.
- Core assumption: Morphological features of cSCC differentiation align semantically with text prompts in model's latent space.
- Evidence anchors: Abstract shows zero-shot AUROC of 0.63/0.60; Methods section describes cosine similarity computation against text phrases.
- Break condition: If visual features of "differentiation" are ambiguous or outside pre-training data distribution, text-to-image alignment fails.

### Mechanism 2
- Claim: Unsupervised clustering of embeddings reveals morphological patterns predictive of tumor grade.
- Mechanism: K-means clustering discretizes embedding space into 25 groups. Slides represented as histograms of cluster counts. Logistic regression on these histograms finds correlations between morphological clusters and grade labels.
- Core assumption: Foundation model's feature space preserves biologically relevant semantic similarity, such that high-grade morphologies cluster distinctly from low-grade ones.
- Evidence anchors: Results show individual clusters achieve AUROC of ~0.75; inspection reveals intuitive findings like keratin fragments in low-grade clusters.
- Break condition: If model captures confounding artifacts (e.g., sweat glands) in distinct clusters, predictive performance may be spurious.

### Mechanism 3
- Claim: Attention-based Multiple Instance Learning (ABMIL) aggregates patch-level signals into robust slide-level predictions.
- Mechanism: ABMIL treats slide as "bag" of patch embeddings. Gated attention mechanism learns to weight patches based on relevance to slide-level label (grade), focusing on most discriminative regions while ignoring background.
- Core assumption: Diagnostic signals are localized in specific patches, and pre-trained embeddings provide sufficient initialization for attention network to learn spatial relationships.
- Evidence anchors: Results show ABMIL achieves AUROC of 0.81 with attention heatmaps highlighting influential regions; data efficiency analysis confirms performance with <100 training samples.
- Break condition: If attention mechanism overfits to staining artifacts or if discriminative features are diffuse rather than localized.

## Foundational Learning

- Concept: **Multiple Instance Learning (MIL)**
  - Why needed here: WSIs are gigapixel images with only slide-level labels. MIL allows training without patch-level annotations.
  - Quick check question: Can you explain why standard supervised classification fails when you have image-level labels but patch-level data?

- Concept: **Vision-Language Contrastive Learning (CLIP-style)**
  - Why needed here: Explains zero-shot capability. Model knows what "well differentiated" looks like relative to that text phrase without explicit training.
  - Quick check question: How does maximizing cosine similarity between image and correct text caption enable zero-shot classification on unseen classes?

- Concept: **Embedding Space Geometry**
  - Why needed here: Mechanisms 2 & 3 rely on distances and directions in feature space. Clustering assumes similar patches are close; linear classifiers assume classes are linearly separable.
  - Quick check question: Why does K-means clustering on raw pixel values usually fail for histology, but works on foundation model embeddings?

## Architecture Onboarding

- Component map: Input (H&E WSI) -> PathFMTools (Slide class: Otsu segmentation → Tiling → embed_patches) -> Heads (Zero-shot: Cosine similarity vs Text Prompts; Clustering: K-Means → Histogram → Logistic Regression; ABMIL: Gated Attention → MLP → Sigmoid)
- Critical path: CPU preprocessing (segmentation/tiling) to GPU embedding generation handoff is bottleneck. Package separates these for efficient resource usage.
- Design tradeoffs:
  - Zero-shot vs. ABMIL: Zero-shot requires zero labels but performs poorly (AUROC ~0.60). ABMIL requires ~100 labeled slides but achieves higher performance (AUROC ~0.81).
  - Cluster count (K=25): Chosen via silhouette analysis; higher K offers granularity but risks overfitting to noise.
- Failure signatures:
  - Cluster Confounding: MUSK associated "sweat glands" with high-grade tumors.
  - Attention Drift: Attention heatmaps focusing on slide edges or artifacts.
- First 3 experiments:
  1. Zero-shot Validation: Run 2-stage zero-shot pipeline on hold-out set of 10 slides to verify text-prompt alignment.
  2. Data Efficiency Curve: Train ABMIL on subsets (10, 50, 100 slides) to confirm rapid convergence (<100 samples).
  3. Cluster Inspection: Visualize top patches for 5 most predictive clusters to ensure biological relevance.

## Open Questions the Paper Calls Out

- Is an AUROC of ~0.80 sufficient for clinical utility in cSCC grading given the high inter-rater variability of the task?
- How does AI assistance based on foundation model embeddings affect clinician performance and workflow integration?
- Does the grading performance of CONCH and MUSK generalize to external cohorts with different staining protocols and scanner hardware?

## Limitations
- Single institutional dataset limits external validity and generalization to diverse staining protocols
- Clinical significance threshold of 0.80 for diagnostic tools remains challenging despite AUROC improvements
- Interpretability through attention heatmaps requires further validation to ensure biological relevance

## Confidence
- High confidence: ABMIL architecture performance claims and PathFMTools implementation details
- Medium confidence: Zero-shot classification validity across diverse cSCC presentations
- Medium confidence: Cluster-based grading correlations with clinical outcomes

## Next Checks
1. External validation: Test complete pipeline (zero-shot, clustering, ABMIL) on independent cSCC dataset from different institution
2. Ablation study: Systematically vary cluster counts (K=10, 15, 20, 25, 30) and attention head dimensions to confirm optimal hyperparameters
3. Clinical correlation: Compare model predictions with pathologist concordance rates and patient survival outcomes to establish clinical utility beyond AUROC metrics