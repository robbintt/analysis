---
ver: rpa2
title: 'Pruning as a Defense: Reducing Memorization in Large Language Models'
arxiv_id: '2502.15796'
source_url: https://arxiv.org/abs/2502.15796
tags:
- pruning
- memorization
- layers
- performance
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how pruning techniques can reduce memorization
  in large language models, a critical issue that enables membership inference attacks.
  The authors evaluate layer-wise and global pruning strategies on Pythia models (160M
  to 12B parameters), pruning different percentages of weights based on their L1 norm.
---

# Pruning as a Defense: Reducing Memorization in Large Language Models

## Quick Facts
- **arXiv ID:** 2502.15796
- **Source URL:** https://arxiv.org/abs/2502.15796
- **Reference count:** 40
- **Primary result:** Pruning significantly reduces memorization in LLMs while maintaining reasonable performance, with global pruning being most effective.

## Executive Summary
This work investigates how pruning techniques can reduce memorization in large language models, a critical issue that enables membership inference attacks. The authors evaluate layer-wise and global pruning strategies on Pythia models (160M to 12B parameters), pruning different percentages of weights based on their L1 norm. They assess memorization using the extractability framework, measuring how well models can reproduce training sequences when prompted. Results show that pruning significantly reduces memorization across all tested context lengths, with global pruning being more effective than layer-wise approaches.

## Method Summary
The paper evaluates pruning as a defense against memorization in LLMs by applying magnitude pruning to Pythia models of varying sizes. The approach involves computing L1 norms for weight matrices, sorting and removing the lowest-magnitude weights according to different strategies (layer-wise, global, attention-only, selective layer pruning), and evaluating the resulting models on memorization and perplexity metrics. Memorization is quantified by measuring exact suffix match probabilities given k-token prefixes from the training data, while perplexity is computed on held-out validation data.

## Key Results
- Pruning significantly reduces memorization across all tested context lengths (k=50,100,200,500)
- Global pruning achieves the largest reductions in memorization compared to layer-wise approaches
- Attention-only pruning provides the most substantial decrease in memorization but at the cost of highest perplexity increases
- Deeper layer pruning (last 25%) reduces memorization while maintaining better performance than attention-only pruning
- Pythia-12B shows memorization dropping from 2.2% to 1.4% with global pruning while maintaining relatively low perplexity increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention layers store disproportionate memorization-relevant weights compared to MLP layers.
- Mechanism: Pruning attention-only layers removes the lowest L1-norm weights specifically from attention mechanisms, disrupting the model's capacity to reconstruct training sequences verbatim while leaving feedforward pathways intact.
- Core assumption: Memorization is more localized in attention parameters than in MLP parameters (Assumption: inferred from relative effectiveness, not directly measured).
- Evidence anchors:
  - [abstract] "attention-only pruning providing the most substantial decrease in memorization"
  - [section] Table 2-3: Attention-only achieves lowest memorization fractions (e.g., Pythia-12B: 0.008 vs 0.016 baseline) but highest perplexity (10.79 vs 7.42 baseline)
  - [corpus] "Finding Dori: Memorization in Text-to-Image Diffusion Models Is Not Local" suggests memorization localization is architecture-dependent and not universally concentrated

### Mechanism 2
- Claim: Deeper layers are more redundant for general performance but contribute significantly to memorization.
- Mechanism: Pruning the last 25% of layers removes parameters that have been shown to be more aggressively prunable without harming task performance, while simultaneously reducing memorization capacity.
- Core assumption: Later layers encode more instance-specific patterns rather than generalizable representations.
- Evidence anchors:
  - [abstract] "Deeper layer pruning also reduces memorization while maintaining better performance than attention-only pruning"
  - [section] Table 3: Last-25% pruning on Pythia-12B yields perplexity 8.33 vs attention-only's 10.79; references Gromov et al. (2024) on deeper layer redundancy

### Mechanism 3
- Claim: Global magnitude pruning creates heterogeneous sparsity patterns that disrupt memorization more effectively than uniform layer-wise pruning.
- Mechanism: By selecting lowest-magnitude weights across the entire model rather than enforcing per-layer quotas, global pruning naturally removes more memorization-critical weights that happen to have low magnitude, regardless of their layer position.
- Core assumption: Memorization-relevant weights are not uniformly distributed across layers and may cluster in low-magnitude regions.
- Evidence anchors:
  - [section] Table 2: Global pruning consistently outperforms layer-wise (e.g., Pythia-2.8B: 0.0048 vs 0.0069)
  - [abstract] "global pruning achieving the largest reductions"

## Foundational Learning

- Concept: **Extractability-based memorization quantification**
  - Why needed here: The paper's entire methodology depends on defining memorization as verbatim reproduction from k-token prefixes using greedy decoding. Without understanding this operationalization, you cannot interpret the results or reproduce experiments.
  - Quick check question: Given a training sequence "The capital of Germany is Berlin" and prefix "The capital of Germany is", would a model outputting "Berlin" constitute memorization at k=5?

- Concept: **L1 magnitude pruning**
  - Why needed here: All pruning strategies in this paper use L1 norm as the importance criterion. Understanding why L1 (vs. L2 or gradient-based) matters for interpreting which weights are removed.
  - Quick check question: If a weight matrix has values [0.01, -0.5, 0.3, 0.001], which weight would be pruned first under L1 magnitude pruning?

- Concept: **Perplexity-memorization trade-off frontier**
  - Why needed here: The paper's practical contribution is characterizing acceptable operating points. You need to understand why perplexity increases as memorization decreases to make informed deployment decisions.
  - Quick check question: If Pythia-12B baseline perplexity is 7.42 and attention-only pruning yields 10.79, what does this 45% increase suggest about the attention mechanism's dual role?

## Architecture Onboarding

- Component map:
```
Pythia Model Architecture
├── Embedding Layer (not pruned)
├── Transformer Layers [N layers]
│   ├── Attention Block
│   │   ├── Q, K, V projections (linear) ← prunable
│   │   └── Output projection (linear) ← prunable
│   └── MLP Block
│       ├── Up projection (linear) ← prunable
│       └── Down projection (linear) ← prunable
└── Final LayerNorm (not pruned)

Pruning Strategies:
├── Layer-wise: Remove n% lowest-L1 weights per layer
├── Global: Remove n% lowest-L1 weights across all layers
├── Attention-only: Global pruning on attention blocks only
└── Selective: Global pruning on first/last 25% of layers
```

- Critical path: Load pretrained Pythia checkpoint → Compute L1 norms for target weight matrices → Sort and mask lowest n% → Evaluate memorization (5,000 samples, k∈{50,100,200,500}) → Compute perplexity on validation set.

- Design tradeoffs:
  - Attention-only vs. Last-25%: Attention-only maximizes memorization reduction (~63% for Pythia-12B) but at 45% perplexity increase; Last-25% achieves ~55% memorization reduction with only 12% perplexity increase.
  - Smaller models more sensitive: Pythia-160M uses 10-15% pruning; Pythia-12B tolerates 35-45% pruning (Table 1).
  - No retraining: Paper uses post-hoc pruning without fine-tuning, trading maximum performance for implementation simplicity.

- Failure signatures:
  - Perplexity spike (>50% increase): Indicates over-aggressive pruning for model size; reduce pruning level.
  - Memorization plateau: If pruning level increases but memorization doesn't decrease further, suggests architecture-specific saturation point.
  - Coherent generation loss: If outputs become syntactically broken (not just lower quality), attention layers may be over-pruned.

- First 3 experiments:
  1. Reproduce baseline: Load Pythia-410M, measure memorization at k=100 on 500 samples, confirm ~0.01 baseline from Table 2.
  2. Single-strategy ablation: Apply 15% layer-wise pruning, measure memorization reduction and perplexity increase; verify against Table 6-7 values.
  3. Attention vs. MLP contribution test: Prune attention-only at 15% and MLP-only at 15% separately; compare memorization reduction ratios to isolate which component type contributes more to memorization in your target model size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can in-training pruning achieve better performance-memorization trade-offs than post-hoc magnitude pruning?
- Basis in paper: [explicit] "Future work could explore other pruning methods, such as in-training pruning (Roy et al., 2020), to enhance performance stability while reducing memorization."
- Why unresolved: The study only evaluates post-hoc pruning strategies; in-training approaches may allow models to adapt during training, potentially preserving task performance while achieving similar memorization reduction.
- What evidence would resolve it: Experiments comparing post-hoc magnitude pruning against in-training pruning across the same model family, measuring both memorization rates and perplexity at equivalent sparsity levels.

### Open Question 2
- Question: Do alternative sparsity techniques like quantization or low-rank approximation provide comparable or superior memorization reduction with less performance degradation?
- Basis in paper: [explicit] "Investigating alternative sparsity techniques, such as quantization and low-rank approximations, could provide further improvements."
- Why unresolved: Pruning is only one approach to model compression; quantization reduces precision while low-rank approximation reduces parameter rank—these may affect memorization differently than weight removal.
- What evidence would resolve it: Systematic comparison of pruning, quantization (e.g., INT8/INT4), and low-rank factorization on memorization metrics, perplexity, and downstream task performance.

### Open Question 3
- Question: How well does reduced perplexity correlate with actual downstream task utility and text generation quality after pruning?
- Basis in paper: [explicit] "Future work could incorporate more comprehensive evaluation metrics, such as ROUGE or BLEU scores, to better capture the quality of the generated text."
- Why unresolved: Perplexity measures predictive uncertainty but does not directly assess semantic coherence, factual accuracy, or task-specific performance—models may maintain low perplexity while degrading in practical utility.
- What evidence would resolve it: Evaluation of pruned models on standardized benchmarks (e.g., MMLU, HellaSwag) and generation quality metrics (BLEU, ROUGE, human evaluation) alongside memorization and perplexity measurements.

### Open Question 4
- Question: Why do attention layers contribute disproportionately more to memorization than MLP layers?
- Basis in paper: [inferred] Results show attention-only pruning achieves the largest memorization reduction (e.g., Pythia-12B: 0.008 vs. 0.014 global pruning) but also causes the highest perplexity increase (10.79 vs. 8.20), yet the paper does not explain this mechanistic relationship.
- Why unresolved: The empirical finding is reported but not explained—understanding whether attention heads encode specific training sequences, positional patterns, or contextual bindings would inform targeted mitigation strategies.
- What evidence would resolve it: Ablation studies on individual attention heads, analysis of attention patterns during memorized sequence recall, and probing experiments to identify what information is stored in attention versus MLP weights.

## Limitations
- The paper uses extractability framework based on greedy decoding from k-token prefixes, which may not capture all forms of memorization
- Results are limited to Pythia models trained on The Pile, restricting generalizability to other architectures and datasets
- The pruning approach is one-shot without retraining, potentially missing optimal performance-memorization trade-offs
- The paper does not explore computational cost implications of different pruning strategies

## Confidence
- **High Confidence:** The core finding that pruning reduces memorization is well-supported by consistent experimental results across multiple model sizes and pruning strategies.
- **Medium Confidence:** The relative effectiveness of different pruning strategies (attention-only vs. layer-wise vs. global) is demonstrated, but optimal strategy depends on specific deployment constraints.
- **Low Confidence:** The assertion that attention layers disproportionately store memorization-relevant weights is inferred from relative pruning effectiveness rather than directly measured.

## Next Checks
1. **Ablation on weight type contribution:** Implement separate pruning of attention Q/K/V projections versus MLP projections to quantify their individual contributions to memorization reduction.
2. **Retraining validation:** Apply fine-tuning after pruning to determine if the memorization-performance trade-off improves with retraining.
3. **Generalization across architectures:** Apply the same pruning methodology to other transformer-based models (e.g., Llama, GPT-2) trained on different datasets to validate whether observed patterns generalize beyond Pythia models.