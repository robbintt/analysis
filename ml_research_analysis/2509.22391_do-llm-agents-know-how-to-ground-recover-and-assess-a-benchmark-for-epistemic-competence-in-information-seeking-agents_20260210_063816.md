---
ver: rpa2
title: Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic
  Competence in Information-Seeking Agents
arxiv_id: '2509.22391'
source_url: https://arxiv.org/abs/2509.22391
tags:
- evidence
- reasoning
- agents
- agent
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SeekBench introduces a process-level evaluation framework for\
  \ LLM search agents, revealing that RL training improves answer accuracy but degrades\
  \ evidence-grounded reasoning. The benchmark identifies three epistemic competencies\u2014\
  groundedness, recovery, and calibration\u2014through expert-annotated traces of\
  \ 28,493 agent interactions."
---

# Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents

## Quick Facts
- arXiv ID: 2509.22391
- Source URL: https://arxiv.org/abs/2509.22391
- Reference count: 40
- Agents trained via RL achieve 35% overconfident answering vs 63% for few-shot, but exhibit lower reasoning quality (RQI=0.08 vs 0.27)

## Executive Summary
SeekBench introduces a process-level evaluation framework for LLM search agents that measures epistemic competence across three dimensions: groundedness, recovery, and calibration. Through expert-annotated traces of 28,493 agent interactions, the benchmark reveals that RL training improves answer accuracy but degrades evidence-grounded reasoning quality. The framework identifies that specialized agents can complement each other's strengths when combined strategically—Search-R1 excels at information synthesis while base models collect higher-quality evidence.

## Method Summary
The framework evaluates LLM search agents through expert-annotated traces across three competencies: groundedness (reasoning justified by evidence), recovery (adaptive search reformulation), and calibration (answering aligned with evidence sufficiency). Evidence state E is computed as the sum of clarity (C) and sufficiency (Q) indicators, gating competency evaluation. The system uses GPT-4.1-mini as an LLM judge for large-scale annotation (κ=0.731 vs human), measuring Reasoning Quality Index (RQI), Evidence Recovery Function (ERF), and Calibration Error (CE) across 28,493 traces from 7 QA benchmarks.

## Key Results
- RL training reduces overconfident answering from 63% to 35% but degrades RQI from 0.27 to 0.08
- Search-R1 achieves highest RQI (0.63) for Information Synthesis while base models collect better evidence
- Agent synthesis combining base model evidence with Search-R1 synthesis improves F1 by +3.50
- Calibration error is lowest for RL agents (0.309) but few-shot achieves highest RQI (0.27)

## Why This Works (Mechanism)

### Mechanism 1: Evidence State Conditioning
Claim: Reasoning quality and answer timing should be evaluated conditional on evidence quality. The framework defines evidence state E ∈ {0,1,2} as sum of clarity and sufficiency binary indicators, gating competency evaluation.

### Mechanism 2: RL-Induced Calibration-Quality Tradeoff
Claim: RL training improves answer calibration but degrades evidence-grounded reasoning quality. RL optimizes for correct final answers, teaching when to answer but not justified reasoning chains.

### Mechanism 3: Agent Synthesis via Complementary Competencies
Claim: Specialized agents can be combined such that one agent's evidence collection feeds another's synthesis capability. Search-R1 excels at synthesis while base models collect higher-quality evidence.

## Foundational Learning

- Concept: **Epistemic Competence**
  - Why needed here: Distinguishes agents producing correct answers through justified reasoning from those getting lucky or gaming metrics.
  - Quick check question: Can you explain why an agent with 90% answer accuracy might still have poor epistemic competence?

- Concept: **Process-Level vs Outcome-Level Evaluation**
  - Why needed here: SeekBench's core innovation is evaluating traces step-by-step rather than final answers only.
  - Quick check question: Given a trace with correct final answer but ungrounded intermediate reasoning, would outcome-level evaluation flag any issue?

- Concept: **Construct Validity via Latent Constructs**
  - Why needed here: Grounds competencies (unobservable) in annotated features (observable) using psychometric principles.
  - Quick check question: What evidence would validate that RQI actually measures "groundedness" rather than some artifact?

## Architecture Onboarding

- Component map: Raw trace -> Step segmentation -> Annotation (type + quality) -> Evidence state computation -> Metric aggregation -> Competency profile
- Critical path: Annotation schema with 8 fields capturing functional type and quality attributes feeds into evidence state calculator and metrics layer
- Design tradeoffs: LLM judges enable scale (28,493 traces) but introduce κ≈0.73 agreement ceiling vs human κ≈0.81
- Failure signatures: RQI near 0 with high F1 suggests answer guessing without reasoning; high overcautious rate indicates underconfidence from over-regularization
- First 3 experiments: 1) Validate annotation schema with human gold labels (κ>0.7); 2) Measure calibration error and overconfident rate on existing agent; 3) Provide Base evidence to Search-R1 and confirm ΔF1>0

## Open Questions the Paper Calls Out

### Open Question 1
How can RL objectives be redesigned to improve evidence-grounded reasoning rather than sacrificing it for accuracy? The paper identifies an inverse relationship between F1 and RQI under current RL rewards.

### Open Question 2
What modular architectures can effectively combine complementary strengths of distinct agent types? The paper shows synthesis benefits but doesn't propose cohesive live integration.

### Open Question 3
Can training interventions target specific weaknesses in Plan Formation and State Assessment competencies? These currently represent weakest reasoning capabilities across all models.

## Limitations
- Evidence state decomposition assumes independently annotatable clarity and sufficiency indicators, vulnerable to annotation noise
- RL-induced tradeoff may not generalize beyond answer-focused reward structures
- Agent synthesis benefits untested for end-to-end efficiency vs monolithic agents

## Confidence
- Evidence-state conditioning: High
- RL tradeoffs (mechanism-specific): Medium
- Scalability of expert annotation: Low

## Next Checks
1. Validate that all 8 annotation features are explicitly defined and consistently applied
2. Test whether reasoning-quality rewards eliminate calibration-quality tradeoff without sacrificing accuracy
3. Benchmark synthesis pipeline against integrated agents on runtime and memory efficiency