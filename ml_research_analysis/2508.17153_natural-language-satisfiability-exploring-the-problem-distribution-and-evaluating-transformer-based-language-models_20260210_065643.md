---
ver: rpa2
title: 'Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating
  Transformer-based Language Models'
arxiv_id: '2508.17153'
source_url: https://arxiv.org/abs/2508.17153
tags:
- language
- satisfiability
- problem
- tlms
- fragments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates transformer-based language models' ability
  to solve natural language satisfiability problems across fragments of English with
  varying computational complexity. Five language fragments are defined, corresponding
  to fragments of first-order logic with complexity ranging from NLOGSPACE-complete
  to NEXPTIME-complete.
---

# Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Models

## Quick Facts
- **arXiv ID:** 2508.17153
- **Source URL:** https://arxiv.org/abs/2508.17153
- **Authors:** Tharindu Madusanka; Ian Pratt-Hartmann; Riza Batista-Navarro
- **Reference count:** 21
- **Primary result:** Transformer models show declining accuracy (88.3% to 70.1%) on natural language satisfiability problems as logical complexity increases

## Executive Summary
This paper investigates transformer-based language models' ability to solve natural language satisfiability problems across fragments of English with varying computational complexity. Five language fragments are defined, corresponding to fragments of first-order logic with complexity ranging from NLOGSPACE-complete to NEXPTIME-complete. The authors empirically determine the phase-change region for each fragment where satisfiability problems are most challenging and construct datasets accordingly. They then fine-tune two transformer models (T5-large and DeBERTa-v3-large) on these datasets and evaluate their performance.

The empirical results demonstrate that model accuracy decreases as the underlying computational complexity increases, from 88.3% for the simplest fragment to 70.1% for the most complex. The models fail to generalize to problems with more predicates than seen during training, suggesting they do not learn general rules of inference. In zero-shot settings, even large models like GPT-4 achieve only modest performance, with accuracy declining as the number of variables increases. The authors conclude that despite their impressive capabilities, transformer models are still far from reliably learning the rules of inference needed for logical reasoning tasks.

## Method Summary
The authors define five language fragments of English corresponding to fragments of first-order logic with increasing computational complexity. For each fragment, they empirically determine the phase-change region where satisfiability problems become most challenging. They construct datasets of natural language satisfiability problems for each fragment and fine-tune two transformer models (T5-large and DeBERTa-v3-large) on these datasets. The models are evaluated on their ability to determine whether given natural language statements are satisfiable, with performance measured across different complexity levels and generalization tests involving unseen predicate counts.

## Key Results
- Model accuracy decreases from 88.3% to 70.1% as computational complexity increases from NLOGSPACE to NEXPTIME
- Models fail to generalize to problems with more predicates than seen during training
- Zero-shot performance of GPT-4 is modest and declines with increasing variable count
- Performance degradation suggests transformers do not learn general inference rules

## Why This Works (Mechanism)
The paper's approach works by systematically mapping natural language satisfiability problems to computational complexity classes and evaluating transformer performance across this spectrum. By identifying phase-change regions where problems become most challenging, the authors create targeted datasets that stress-test model capabilities. The controlled variation in logical complexity allows for clear observation of performance degradation patterns, revealing fundamental limitations in how transformers handle logical reasoning tasks.

## Foundational Learning
**First-Order Logic Fragments** - why needed: To create a rigorous framework for analyzing natural language satisfiability problems; quick check: Verify the five defined fragments correctly map to their claimed complexity classes.
**Computational Complexity Classes** - why needed: To quantify and compare the difficulty of different problem types; quick check: Confirm the complexity bounds for each fragment are properly established.
**Phase-Change Regions** - why needed: To identify where satisfiability problems become most challenging; quick check: Validate the empirical determination method for these regions.
**Transformer Architecture** - why needed: To understand the model's capacity for logical reasoning; quick check: Review the specific transformer variants used and their parameter counts.
**Generalization Testing** - why needed: To assess whether models learn abstract rules or memorize patterns; quick check: Examine the predicate count variation methodology.

## Architecture Onboarding

**Component Map:**
Natural Language Input -> Tokenization -> Transformer Encoder-Decoder -> Output Generation -> Satisfiability Classification

**Critical Path:**
Input sentences are tokenized and processed through the transformer architecture, which generates predictions about satisfiability. The critical path involves attention mechanisms, feed-forward layers, and output layers that must handle varying logical complexity while maintaining semantic understanding.

**Design Tradeoffs:**
The study uses fine-tuning versus zero-shot evaluation to balance between leveraging pre-trained knowledge and testing inherent reasoning capabilities. Fine-tuning improves performance but may encourage pattern matching over rule learning. The choice of T5 and DeBERTa models represents a tradeoff between model size and computational efficiency.

**Failure Signatures:**
Models show consistent accuracy degradation as complexity increases, with complete failure to generalize beyond training predicate counts. The performance drop is monotonic across complexity fragments, suggesting systematic rather than random failure modes.

**Three First Experiments:**
1. Evaluate model performance on cross-fragment generalization tasks
2. Test sensitivity to different tokenization strategies for logical expressions
3. Compare performance across different transformer variants with varying attention mechanisms

## Open Questions the Paper Calls Out
None

## Limitations
- The five-fragment taxonomy may not capture the full spectrum of natural language logical complexity
- Generalization failure may conflate syntactic generalization with semantic understanding
- Performance gap between fine-tuned and zero-shot settings raises questions about genuine reasoning versus pattern matching
- The study may underestimate abstract reasoning capabilities suggested by non-trivial zero-shot performance

## Confidence

**High Confidence:** The observed inverse relationship between computational complexity and model accuracy (88.3% to 70.1%) is well-supported by experimental design and results.

**Medium Confidence:** The conclusion that transformers fail to learn general rules of inference is plausible but relies heavily on generalization failure as diagnostic evidence.

**Low Confidence:** The assertion that transformer models are "far from reliably learning the rules of inference" overstates the case based on available evidence, particularly given GPT-4's modest but non-trivial zero-shot performance.

## Next Checks
1. Construct additional language fragments between the five defined categories to map exact complexity thresholds where performance degrades.
2. Design experiments testing for inference rule learning through controlled perturbations within known predicate sets, rather than solely increasing predicate count.
3. Evaluate models with explicit symbolic reasoning components on the same benchmark to determine whether limitations are inherent to transformer architectures or represent broader challenges in neural approaches to logical reasoning.