---
ver: rpa2
title: 'MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars'
arxiv_id: '2510.12785'
source_url: https://arxiv.org/abs/2510.12785
tags:
- video
- multi-view
- reference
- image
- frames
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MVP4D introduces a morphable multi-view video diffusion model (MMVDM)
  that generates 360-degree animatable portrait videos from a single reference image.
  The model synthesizes up to 392 frames across multiple viewpoints simultaneously
  using a state-of-the-art video diffusion transformer.
---

# MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars

## Quick Facts
- arXiv ID: 2510.12785
- Source URL: https://arxiv.org/abs/2510.12785
- Reference count: 40
- Key outcome: Generates 360° animatable portrait videos from single image with PSNR 23.39 vs 22.17 baseline

## Executive Summary
MVP4D introduces a morphable multi-view video diffusion model (MMVDM) that generates 360-degree animatable portrait videos from a single reference image. The model synthesizes up to 392 frames across multiple viewpoints simultaneously using a state-of-the-art video diffusion transformer. To overcome data limitations, a multi-modal training curriculum combines monocular, multi-view image, and multi-view video datasets. MVP4D significantly improves temporal and 3D consistency compared to previous methods, achieving higher PSNR (23.39 vs 22.17), lower LPIPS (0.294 vs 0.280), and better JOD (5.88 vs 5.30) on Nersemble self-reenactment. The generated videos can be distilled into real-time 4D avatars with superior realism and expression fidelity.

## Method Summary
MVP4D extends the CogVideoX video diffusion transformer to handle multi-view generation by flattening and concatenating tokens across spatial, temporal, and viewpoint dimensions. The model uses a three-stage curriculum: stage 1 trains at 256×256 resolution on monocular videos, stage 2 increases resolution to 512×512 and introduces multi-view data, and stage 3 generates full-resolution multi-view videos. A novel multi-view classifier-free guidance computes view-specific unconditional predictions to avoid cross-view token confusion. The generated videos are distilled into 4D Gaussian splatting avatars using FLAME mesh deformation and temporal UV mapping.

## Key Results
- PSNR improvement: 23.39 vs 22.17 on Nersemble self-reenactment
- LPIPS reduction: 0.294 vs 0.280 compared to baseline
- JOD improvement: 5.88 vs 5.30, indicating better 3D consistency
- Novel view quality: 5.58 JOD at 8 views vs 5.44 with conventional CFG

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint attention across all spatio-temporal-view tokens enables coherent multi-view video synthesis without explicit 3D supervision.
- Mechanism: The diffusion transformer flattens and concatenates tokens from the reference latent (1 × H/16 × W/16) and each view's video latents ((1 + F̃) × H/16 × W/16), then processes them through 30 transformer layers that attend jointly across spatial, frame, and viewpoint dimensions. This allows the model to learn implicit multi-view geometry and temporal dynamics through self-attention rather than explicit 3D constraints.
- Core assumption: The pre-trained CogVideoX video prior can be extended to multi-view generation with appropriate conditioning and positional encodings.
- Evidence anchors: [section 3.2] and weak corpus validation.
- Break condition: Token count exceeds memory capacity or view/frame count exceeds generalization range.

### Mechanism 2
- Claim: A progressive multi-modal training curriculum enables 360-degree video generation without requiring large-scale multi-view video data.
- Mechanism: Three-stage curriculum progressively increases resolution (256→512), frame count, and generation mode complexity. Stage 1 uses mode 1 only (reference→all views/frames). Stages 2-3 introduce modes 2-4 (partial conditioning). Each stage samples from all datasets (VFHQ monocular, Nersemble multi-view video, RenderMe-360/Ava-256 360° images) with equal probability.
- Core assumption: Features learned from static multi-view images (360° coverage) and monocular videos (temporal dynamics) will compose to produce temporally coherent multi-view videos.
- Evidence anchors: [section 3.3] and Table S13 ablation showing PSNR 21.6 vs. 20.5.
- Break condition: Curriculum stages skipped or dataset imbalance toward one modality.

### Mechanism 3
- Claim: View-specific unconditional predictions resolve CFG ambiguity in multi-view settings.
- Mechanism: Standard CFG computes ε_guided = ε_uncond + s·(ε_cond − ε_uncond). For multi-view, naive ε_uncond gives all views identical spatio-temporal embeddings without view-specific information, causing the transformer to confuse cross-view token relationships. MVP4D runs independent forward passes per view to generate {ε_uncond,v}^V_v=1, preserving view-specific context even in unconditional predictions. Guidance scale increases linearly from 1.1 (0° deviation) to 1.6 (180° deviation).
- Core assumption: View-specific unconditional predictions provide sufficient anchor for CFG without requiring paired conditional/unconditional training.
- Evidence anchors: [section 5] and Table 7 showing JOD 5.58 vs. 5.44.
- Break condition: Applied with V > training view count without view-specific unconditional computation.

## Foundational Learning

- Concept: **Diffusion Transformers with Spatio-Temporal Autoencoders**
  - Why needed here: The architecture builds on CogVideoX, which uses a 3D VAE (8× spatial, 4× temporal compression) and transformer denoising. Without understanding latent video representation and transformer-based diffusion, the joint multi-view extension is opaque.
  - Quick check question: Given a video of shape [1 + 4F̃, H, W], what is the shape after CogVideoX's spatio-temporal autoencoder encoding?

- Concept: **3D Morphable Models (FLAME)**
  - Why needed here: All conditioning signals (pose maps, expression deformation maps) derive from FLAME fitting. The 4D reconstruction attaches Gaussians to FLAME mesh triangles and deforms via FLAME blendshapes.
  - Quick check question: What are the three primary parameter groups in FLAME, and which one drives expression-dependent wrinkles in MVP4D?

- Concept: **Classifier-Free Guidance (CFG)**
  - Why needed here: The paper's multi-view CFG modification is the key inference technique that makes multi-view generation stable. Standard CFG assumes a single unconditional prediction; understanding why this breaks for multi-view is essential to appreciating the fix.
  - Quick check question: Why does computing a single unconditional prediction ε_uncond for all views cause the transformer to confuse cross-view token relationships?

## Architecture Onboarding

- Component map: Reference Image → FlowFace 3DMM Tracker → Conditioning Signals → VAE Encoder → z_ref → Noisy Latents + Conditioning → Diffusion Transformer, 30 layers → Denoised Z_gen → VAE Decoder → Multi-View Video → DISK+LightGlue Keypoints + FLAME Mesh → 4D Gaussian Splatting Avatar

- Critical path:
  1. Conditioning extraction: FlowFace tracker must successfully fit FLAME to reference image
  2. Diffusion sampling: DDIM with 100 steps for key views, 50 for others
  3. Iterative generation: Mode 1 → key views → Mode 2 → fill views → Mode 3 → extend frames → Mode 4 → fill extended views
  4. 4D fitting: 1.5 hours optimization of Gaussian primitives with velocity regularization

- Design tradeoffs:
  - Token count vs. memory: 8×49×512 requires 35GB VRAM
  - View count vs. 3D consistency: Training on V=4 generalizes to V=8 but V=16 likely degrades
  - CFG scale vs. fidelity: Higher scale improves novel views but may over-saturate

- Failure signatures:
  - Flickering in multi-view output: Indicates conventional CFG was used instead of multi-view CFG
  - Geometry collapse at novel views: Insufficient multi-view training data or skipped curriculum stages
  - Teeth appearing/disappearing rapidly: Autoencoder's temporal compression drops frames
  - Quality degradation in long sequences: Mode 3 autoregressive extension accumulates errors

- First 3 experiments:
  1. Validate single-view video generation: Run inference with V=1, F=49 on VFHQ-style monocular input
  2. Ablate multi-view CFG: Generate 8-view video with no CFG, conventional CFG, and multi-view CFG
  3. Test curriculum generalization: Train a reduced model skipping stage 1 and evaluate on Nersemble

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can high-frequency, rapid temporal details (such as transient teeth visibility) be preserved during generation without reducing the compression factor of the spatio-temporal auto-encoder?
- Basis in paper: [explicit] The authors state the method "struggles to capture fine-grained temporal details, such as rapidly appearing and disappearing teeth" specifically due to the "auto-encoder’s high spatiotemporal compression."
- Why unresolved: The current architecture relies on aggressive downsampling (factor of 16 spatial, 4 temporal) to manage the token count for the diffusion transformer, creating a bottleneck for high-frequency details.
- What evidence would resolve it: A modification to the latent representation or an auxiliary loss term that recovers sub-frame details.

### Open Question 2
- Question: Can the quality degradation observed in long video sequences be mitigated to prevent the accumulation of errors during iterative generation?
- Basis in paper: [explicit] The paper notes that "When generating long sequences using the first-frame conditioning strategy (generation mode 3), quality degrades with each iteration, limiting the practical sequence length."
- Why unresolved: The current autoregressive-like extension (Mode 3) conditions on previously generated (and potentially imperfect) frames, causing artifacts to compound over time.
- What evidence would resolve it: A new sampling strategy or error-correction mechanism that maintains consistent PSNR and LPIPS scores across hundreds of generated frames.

### Open Question 3
- Question: How can the model be adapted to handle extreme or diverse lighting conditions which are currently absent from the training data?
- Basis in paper: [explicit] The authors identify that the "method cannot accurately model extreme lighting conditions, as the training data lacks significant lighting variation in the multi-view setting."
- Why unresolved: The model generalizes poorly to lighting distributions not represented in the multi-view datasets.
- What evidence would resolve it: Successful integration of a physically-based lighting model or an intrinsic image decomposition step.

## Limitations

- Data dependency: Multi-modal curriculum relies on balanced sampling from multiple datasets with unknown exact splits
- Architectural transparency: Key implementation details like attention biasing and positional encoding are underspecified
- 4D reconstruction fidelity: Limited quantitative metrics for avatar stage beyond qualitative comparisons

## Confidence

**High confidence** in joint spatio-temporal-view attention mechanism (JOD improvements 5.88 vs 5.30)
**Medium confidence** in curriculum effectiveness (clear PSNR improvements but sensitivity unknown)
**Medium confidence** in multi-view CFG innovation (logical explanation and empirical improvements but requires V forward passes)

## Next Checks

1. **Curriculum ablation experiment**: Train MVP4D starting directly at 512×512 resolution (skipping stage 1) and compare PSNR degradation against full curriculum on Nersemble self-reenactment.

2. **CFG scaling study**: Systematically vary the multi-view CFG guidance scale from 1.0 to 2.0 in 0.1 increments and measure the trade-off between JOD and LPIPS.

3. **Temporal chunk degradation analysis**: Generate 196-frame videos (4 chunks of 49 frames) and measure PSNR/LPIPS degradation per chunk to quantify autoregressive error accumulation.