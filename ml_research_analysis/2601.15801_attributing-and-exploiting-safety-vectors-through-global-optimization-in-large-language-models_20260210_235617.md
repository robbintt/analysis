---
ver: rpa2
title: Attributing and Exploiting Safety Vectors through Global Optimization in Large
  Language Models
arxiv_id: '2601.15801'
source_url: https://arxiv.org/abs/2601.15801
tags:
- safety
- heads
- attention
- harmful
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces GOSV, a global optimization framework that
  identifies safety-critical attention heads in large language models through two
  complementary activation repatching strategies. The approach reveals that aligned
  models maintain two spatially distinct safety pathways: Malicious Injection Vectors
  (from harmful patching) and Safety Suppression Vectors (from zero ablation), with
  only ~24% overlap across models.'
---

# Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models

## Quick Facts
- arXiv ID: 2601.15801
- Source URL: https://arxiv.org/abs/2601.15801
- Authors: Fengheng Chu; Jiahao Chen; Yuhong Wang; Jun Wang; Zhihui Fu; Shouling Ji; Songze Li
- Reference count: 40
- Primary result: Introduces GOSV framework that achieves 92.5-100% ASR by exploiting two distinct safety pathways with only 24% overlap

## Executive Summary
This paper introduces GOSV, a global optimization framework that identifies safety-critical attention heads in large language models through two complementary activation repatching strategies. The approach reveals that aligned models maintain two spatially distinct safety pathways: Malicious Injection Vectors (from harmful patching) and Safety Suppression Vectors (from zero ablation), with only ~24% overlap across models. Systematic analysis shows that safety mechanisms are encoded in approximately 30% of attention heads, with complete safety breakdown occurring when this threshold is reached. The method enables an inference-time white-box jailbreak attack that achieves 92.5-100% attack success rate across four open-source models, substantially outperforming existing white-box attacks by exploiting these identified safety vectors.

## Method Summary
GOSV identifies safety-critical attention heads by jointly optimizing Bernoulli parameters for all heads simultaneously using REINFORCE-based policy gradient. The framework uses two repatching strategies: Harmful Patching (replacing head outputs with mean activations from malicious instruction processing) and Zero Ablation (suppressing information flow with zero activations). For each strategy, GOSV collects N=1000 activation samples from 100 malicious training examples, then optimizes head selection parameters θ(l,h) over 500 epochs with K=32 samples per iteration. The method ranks heads by their optimized probabilities and exploits the top ~30% at inference time to achieve complete safety breakdown while maintaining coherent outputs.

## Key Results
- GOSV achieves 92.5-100% attack success rate across four 7-8B parameter models (Llama-2, Vicuna, ChatGLM-6B)
- Only ~24% overlap between Malicious Injection Vectors and Safety Suppression Vectors indicates two spatially distinct safety pathways
- Complete safety breakdown occurs at ~30% of attention heads; exceeding this threshold causes general capability degradation
- GOSV outperforms local greedy attribution methods (Ships) by >50% ASR improvement
- Inference-time attack maintains perplexity close to original model while achieving near-complete safety bypass

## Why This Works (Mechanism)

### Mechanism 1: Dual Safety Pathway Discovery
Aligned LLMs maintain two spatially distinct functional pathways for safety that can be independently compromised. Harmful Patching injects mean activations from malicious instruction processing, capturing "malicious injection vectors"; Zero Ablation suppresses information flow, identifying "safety suppression vectors." The low overlap (~24%) indicates separate pathways. Core assumption: Safety information can be represented as vectors transported by specific attention heads, analogous to function vectors.

### Mechanism 2: Global Optimization Captures Cooperative Interdependencies
Joint optimization over all attention heads identifies safety-critical heads that local greedy attribution misses. REINFORCE-based policy gradient optimizes Bernoulli parameters θ(l,h) for each head simultaneously, sampling configurations and minimizing semantic distance to target harmful responses. Core assumption: LLM components are highly interdependent; safety behaviors emerge from distributed interactions rather than independent component contributions.

### Mechanism 3: Safety Threshold Concentration at ~30%
Complete safety breakdown requires repatching approximately 30% of total attention heads; exceeding this degrades general capabilities. Safety is distributed across a specific subset of heads. Below ~30%, partial bypass occurs; at ~30%, safety fully collapses; above ~30%, perplexity spikes and outputs become incoherent. Core assumption: Safety mechanisms require coordinated activity across a distributed subset; disrupting this subset is sufficient but over-intervention harms language modeling.

## Foundational Learning

- Concept: **Activation Patching**
  - Why needed here: Core intervention technique; GOSV replaces head outputs z_l,h at the last token position to alter downstream generation
  - Quick check question: During a forward pass, if you replace head (l, h)'s activation with μ_l,h, which layers receive the modified signal?

- Concept: **REINFORCE / Policy Gradient**
  - Why needed here: GOSV uses REINFORCE to optimize discrete head selection via Bernoulli sampling without differentiable selection
  - Quick check question: In REINFORCE, how does the gradient of expected reward reach the policy parameters when sampling is non-differentiable?

- Concept: **Multi-Head Attention Architecture**
  - Why needed here: Understanding Q, K, V projections and head output concatenation is essential for mapping intervention points
  - Quick check question: For a model with L=32 layers and H=32 heads per layer, what is the total number of intervention locations?

## Architecture Onboarding

- Component map:
  Input: Malicious query Q → Training data: (Q_s, R_s) pairs → Repatching values: {μ_l,h} → Optimized parameters: θ(l,h) → Loss: (1 - cosine_similarity) → Output: I_safety = set of safety-critical head locations

- Critical path:
  1. Collect 100 malicious instructions from AdvBench
  2. Generate target harmful responses using unaligned base model
  3. Compute {μ_l,h} from N=1000 activation samples (Harmful Patching) or set μ_l,h=0 (Zero Ablation)
  4. Run GOSV: 500 epochs, K=32 samples/iteration, Adam optimizer, lr=0.1
  5. Rank heads by σ(θ(l,h)); select top heads at ~30% threshold
  6. At inference: repatch selected heads at last token position

- Design tradeoffs:
  - Harmful Patching vs Zero Ablation: Both effective; Zero Ablation is simpler (no mean computation). Paper shows each independently achieves full breakdown
  - 100 training examples sufficient (Figure 7 plateaus); 500 epochs balances convergence vs compute (Figure 6)

- Failure signatures:
  - Low ASR (<50%): Check Bernoulli convergence; verify target responses are from unaligned model, not aligned refusals
  - Incoherent outputs: Over-intervention (>30% heads) increases perplexity—reduce head count
  - Poor generalization to new datasets: Ensure training examples cover diverse harm categories

- First 3 experiments:
  1. **Sanity check**: Implement Zero Ablation GOSV on Llama-2-7b-chat with 20 training examples; verify ASR increases over epochs
  2. **Overlap analysis**: Run both strategies on same model; compute overlap of top-200 heads; expect ~20-25%
  3. **Threshold validation**: Progressively repatch 10%, 20%, 30%, 40% of ranked heads; plot ASR and perplexity; verify plateau and PPL spike patterns

## Open Questions the Paper Calls Out

### Open Question 1
Can defense mechanisms be developed that simultaneously protect both the Malicious Injection Vectors and Safety Suppression Vectors pathways without degrading model capabilities? Basis in paper: "First, we have not developed defense strategies to mitigate the identified vulnerabilities. Future work should focus on developing robust safeguards based on these insights." Why unresolved: The paper demonstrates two distinct attack surfaces with only ~24% overlap, but provides no defensive countermeasures; protecting one pathway may not protect the other. What evidence would resolve it: A defense method that maintains low ASR under both attack strategies while preserving perplexity and helpfulness metrics.

### Open Question 2
Do the two distinct safety pathways and ~30% safety-encoding threshold generalize to larger proprietary models (e.g., GPT-4, Claude)? Basis in paper: "The generalizability to larger proprietary models... requires further investigation." Why unresolved: Experiments were limited to four 7-8B parameter open-source models; scaling properties and architectural variations in larger models remain unexplored. What evidence would resolve it: Replication of the GOSV framework on models with ≥70B parameters showing consistent pathway separation and threshold behavior.

### Open Question 3
What mechanistic explanation accounts for why aligned LLMs maintain two spatially distinct functional safety pathways rather than a unified mechanism? Basis in paper: The low overlap (~24%) between Malicious Injection Vectors and Safety Suppression Vectors is observed but not causally explained; the authors note this "demonstrates that aligned LLMs maintain separate functional pathways" without explaining why. Why unresolved: The paper establishes the empirical existence of two pathways but does not investigate whether this arises from training dynamics, architectural constraints, or emergent properties. What evidence would resolve it: Probing experiments or training interventions that reveal whether pathway separation is necessary, incidental, or an artifact of specific alignment procedures.

## Limitations
- The dual safety pathway hypothesis lacks mechanistic proof of distinct functional roles beyond statistical independence
- The 30% threshold may be architecture-dependent rather than universal across diverse model families
- Inference-time attack effectiveness depends on training distribution generalization not thoroughly validated

## Confidence

- **High Confidence**: Basic mechanism of global optimization for head selection works as described, with reproducible ASR improvements over local methods. The 30% threshold observation is consistent across models and provides a clear operational guideline.
- **Medium Confidence**: Dual safety pathway hypothesis is supported by low overlap statistics but lacks mechanistic proof of distinct functional roles. Effectiveness of inference-time attack is well-demonstrated but may have limited generalizability to prompts outside training distribution.
- **Low Confidence**: Universality of 30% threshold across all LLM architectures remains speculative without broader validation. Semantic interpretation of Malicious Injection vs Safety Suppression vectors as representing different safety mechanisms is plausible but not definitively proven.

## Next Checks

1. **Cross-Architecture Threshold Validation**: Test the 30% safety threshold on models with different architectures (different layer counts, attention patterns, or model families like Mistral vs Llama) to determine if the threshold is universal or architecture-dependent.

2. **Functional Pathway Dissection**: Design experiments to test whether heads identified by Harmful Patching and Zero Ablation actually serve different safety functions by examining model behavior when only one pathway is compromised versus both.

3. **Adversarial Prompt Generalization**: Evaluate the inference-time attack success rate on novel malicious prompts that differ substantially from the training set in terms of harm categories, linguistic style, or prompt structure to assess true generalization capability.