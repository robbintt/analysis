---
ver: rpa2
title: How Far Are We from Genuinely Useful Deep Research Agents?
arxiv_id: '2512.01948'
source_url: https://arxiv.org/abs/2512.01948
tags:
- research
- uni00000048
- uni00000011
- deep
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FINDER and DEFT as the first unified framework
  for evaluating and diagnosing deep research agents at both task and process levels.
  FINDER is a fine-grained benchmark with 100 expert-curated research tasks and 419
  structured checklist items, while DEFT is a 14-category failure taxonomy covering
  reasoning, retrieval, and generation stages.
---

# How Far Are We from Genuinely Useful Deep Research Agents?

## Quick Facts
- arXiv ID: 2512.01948
- Source URL: https://arxiv.org/abs/2512.01948
- Reference count: 40
- Introduces FINDER and DEFT as the first unified framework for evaluating and diagnosing deep research agents at both task and process levels

## Executive Summary
This paper addresses the critical gap in evaluating and diagnosing Deep Research Agents (DRAs) by introducing FINDER and DEFT - a unified framework for fine-grained benchmarking and failure taxonomy. FINDER provides 100 expert-curated research tasks with 419 structured checklist items, while DEFT offers a 14-category failure taxonomy covering reasoning, retrieval, and generation stages. Experiments on 9 mainstream models reveal that current DRAs struggle more with evidence integration, verification, and reasoning resilience than with task comprehension, with generation errors accounting for 39% of failures and retrieval issues for over 32%.

## Method Summary
The framework consists of FINDER, a benchmark with 100 expert-curated research tasks and 419 structured checklist items, and DEFT, a grounded-theory-based failure taxonomy with 14 categories validated through human-LLM co-annotation. Nine DRA systems were evaluated using Gemini-2.5-Pro as judge LLM for RACE scoring and Gemini-2.5-Flash for FACT verification. The methodology employs grounded theory with open, axial, and selective coding phases, human-LLM collaboration, and Krippendorff's alpha for inter-coder reliability validation.

## Key Results
- Current DRAs fail more on evidence integration and verification (32%+) than task comprehension (10.55% FUR)
- Generation errors account for 39% of failures, with Strategic Content Fabrication (SCF) at 18.95% being the highest-frequency failure mode
- Reasoning resilience, rather than reasoning intensity, is identified as the key factor for consistent high-quality outcomes
- Checklist adherence improves instruction-following and comprehensiveness in generated reports

## Why This Works (Mechanism)

### Mechanism 1: Fine-Grained Checklist Constraint for Reproducible Evaluation
- Expert-curated checklists (419 items across 100 tasks) constrain evaluation space by specifying required report structure, analytical depth, and factual grounding
- Transforms open-ended generation into verifiable sub-tasks, enabling attribution of failures to specific capability dimensions
- Assumes checklist items accurately reflect user demands and research rigor standards

### Mechanism 2: Grounded-Theory Failure Taxonomy (DEFT) for Process-Level Diagnosis
- DEFT built via open, axial, and selective coding with human-LLM collaboration and Krippendorff's alpha validation
- Yields 14 fine-grained failure modes across reasoning (28.14%), retrieval (33.10%), and generation (38.76%) stages
- Assumes failures observed across ~1,000 reports generalize to broader DRA behaviors

### Mechanism 3: Reasoning Resilience over Reasoning Intensity
- Identifies reasoning resilience - ability to detect deviations, recalibrate, and adapt strategies - as key differentiator
- Explains why models with strong task comprehension still fail at evidence integration and verification
- Assumes dynamic, noisy research environments reliably expose resilience gaps

## Foundational Learning

- **Concept: Grounded Theory Methodology**
  - Why needed here: DEFT's taxonomy construction relies on open → axial → selective coding with inter-coder reliability, which is unfamiliar to many ML practitioners
  - Quick check question: Can you explain the difference between open coding (concept generation) and axial coding (relationship exploration)?

- **Concept: Agentic Search and Synthesis Pipeline**
  - Why needed here: DRAs differ from standard RAG by requiring iterative retrieval, multi-stage reasoning, and structured report generation
  - Quick check question: What distinguishes a DRA from a single-turn RAG system in terms of feedback loops?

- **Concept: Inter-Coder Reliability (Krippendorff's α)**
  - Why needed here: The paper uses α to validate that human and LLM coders agree on failure categories
  - Quick check question: Why is α preferred over simple percent agreement for categorical coding?

## Architecture Onboarding

- **Component map:** FINDER (Task prompts + checklists) → DEFT (14-category failure taxonomy) → RACE (Reference-based quality scoring) → FACT (Citation accuracy scoring)
- **Critical path:** Receive task prompt → Plan retrieval strategy → Execute iterative web exploration → Integrate and verify information → Generate structured report → Evaluate via RACE + FACT → Diagnose failures via DEFT
- **Design tradeoffs:** Checklist granularity vs. annotation cost (419 items is labor-intensive); LLM-as-judge vs. human evaluation (faster but risks bias); taxonomy breadth vs. specificity (14 categories balance coverage and usability)
- **Failure signatures:** Strategic Content Fabrication (18.95%), Insufficient External Information Acquisition (16.30%), Lack of Analytical Depth (11.09%), Verification Mechanism Failure (8.72%)
- **First 3 experiments:**
  1. Run your DRA on 10 FINDER tasks; manually annotate failures using DEFT categories; compare your distribution to the paper's
  2. Remove explicit cross-checking logic from your retrieval stage; measure SCF and VMF rate changes
  3. Generate reports with and without explicit checklist prompting; score both with RACE; quantify delta in instruction-following and comprehensiveness

## Open Questions the Paper Calls Out

### Open Question 1
What specific architectural constraints or verification mechanisms can effectively mitigate Strategic Content Fabrication (SCF) without stifling necessary synthesis capabilities?
- Basis: Paper identifies SCF as highest-frequency failure mode (18.95%) and states strengthening constraints and verifications is key
- Unresolved: Paper diagnoses SCF but does not propose or validate specific technical solutions
- Evidence needed: Comparative study measuring SCF rates in agents with citation-required-before-generation modules versus baseline

### Open Question 2
How can agent frameworks be evolved to enhance "reasoning resilience" specifically to overcome the "Rigid Planning Strategy" (RPS) failure mode?
- Basis: Paper introduces reasoning resilience as key factor and identifies RPS (5.60%) as failure where agents adhere to fixed plans
- Unresolved: While paper defines lack of resilience, it evaluates existing systems rather than proposing methods to instill dynamic planning
- Evidence needed: Evaluation of modified agent architecture that dynamically re-plans based on intermediate retrieval failures

### Open Question 3
To what extent does reliance on proprietary models (specifically Gemini variants) as "Judge LLM" introduce systematic bias in RACE and FACT evaluations?
- Basis: Methodology explicitly uses Gemini 2.5 Pro and Flash for evaluation while criticizing existing benchmarks for subjective metrics
- Unresolved: Paper does not perform ablations on judge model itself
- Evidence needed: Correlation analysis of rankings generated by different judge models (e.g., Claude, GPT-4) on same reports

## Limitations

- Checklist Generalization: 419-item FINDER checklist may overfit to specific 100 tasks without validation on independently constructed checklists
- Judge LLM Bias: RACE and FACT rely entirely on Gemini models for scoring and fact-checking without human validation for majority of evaluations
- Intervention Validation Gap: Paper identifies reasoning resilience as critical factor but does not test interventions to improve it

## Confidence

- **High Confidence**: DEFT taxonomy construction methodology (grounded theory + inter-coder reliability) is well-documented and reproducible; failure mode distributions across models are empirically derived
- **Medium Confidence**: Checklist-based evaluation approach is theoretically sound but lacks cross-validation on independent task sets; RACE/FACT evaluation pipeline is specified but depends on Gemini-specific prompts
- **Low Confidence**: Claims about reasoning resilience as primary intervention target are speculative without empirical validation of specific remediation strategies

## Next Checks

1. Apply FINDER's 419-item checklist to a new set of 20+ expert-curated tasks outside original dataset; measure inter-rater reliability and sensitivity to capability gaps
2. Select 50 reports and have human experts score RACE and FACT alongside Gemini; compute correlation coefficients to quantify judge LLM bias
3. Implement verification-augmented DRA pipeline (explicit cross-checking at each retrieval step); compare SCF and VMF rates against baseline to test if verification resilience reduces strategic fabrication