---
ver: rpa2
title: Can LLMs Enable Verification in Mainstream Programming?
arxiv_id: '2503.14183'
source_url: https://arxiv.org/abs/2503.14183
tags:
- nums
- code
- verification
- dafny
- mode
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigates whether large language models (LLMs) can
  bridge the gap between mainstream programming and formal verification by generating
  verified code in three verification languages: Dafny, Nagini, and Verus. Using manually
  curated datasets derived from the HumanEval benchmark, the authors explore six different
  modes of code generation, ranging from completing proofs for existing code to generating
  implementations and specifications from natural language descriptions.'
---

# Can LLMs Enable Verification in Mainstream Programming?

## Quick Facts
- arXiv ID: 2503.14183
- Source URL: https://arxiv.org/abs/2503.14183
- Reference count: 34
- LLMs achieve up to 86% verification success in Dafny across six modes, but struggle with full synthesis from natural language

## Executive Summary
This study investigates whether large language models can bridge the gap between mainstream programming and formal verification by generating verified code in three verification languages: Dafny, Nagini, and Verus. Using manually curated datasets derived from the HumanEval benchmark, the authors explore six different modes of code generation, ranging from completing proofs for existing code to generating implementations and specifications from natural language descriptions. The evaluation employs Claude 3.5 Sonnet to generate code across these modes and validation frameworks to ensure correctness.

Key results show that LLMs achieve high success rates in Dafny (up to 86% verification success in certain modes), while performance in Nagini and Verus is lower due to model unfamiliarity and language-specific challenges. Mode 1, which focuses on generating proofs for given code, demonstrates the highest success rates, while Mode 6, requiring complete generation from natural language, achieves the lowest success (below 30% for Dafny and 15% for Nagini and Verus). The study identifies common pitfalls, such as type errors in Verus and timeouts in Nagini, and highlights the potential for future improvements through fine-tuning and specialized error-correction mechanisms.

## Method Summary
The study uses Claude 3.5 Sonnet to generate verified code across six modes using HumanEval-derived benchmarks for Dafny (132 programs), Nagini (106 programs), and Verus (55 programs). The pipeline involves task preparation, LLM generation, verification, iterative error feedback (up to 5 iterations), post-processing, and validation. Validation checks whether generated specifications imply reference specifications using SMT solvers. The study reports unique verified programs across 5 runs per benchmark.

## Key Results
- Mode 1 (proof-only, given code + spec) achieves 86% success in Dafny, 53% in Nagini, and 45% in Verus
- Mode 6 (NL-only) drops to 29% success in Dafny and below 15% in Nagini and Verus
- Dafny outperforms other languages due to greater training data availability and expressiveness
- Common failures include invariant maintenance errors (Dafny), timeouts (Nagini), and type errors (Verus)

## Why This Works (Mechanism)

### Mechanism 1: Iterative Error-Feedback Correction
LLMs can incrementally repair verification failures when provided structured error messages from verifiers. The system attempts verification, collects errors, and feeds them back to the LLM for correction, repeating up to 5 iterations. This allows the model to localize and fix issues rather than regenerating from scratch. Break condition: Timeouts in Nagini provide no actionable information—LLMs cannot diagnose or repair these.

### Mechanism 2: Partial Context Anchoring
Providing partial artifacts (implementation OR specification) dramatically improves verification success compared to full synthesis from natural language alone. Six modes expose progressively less context. Mode 1 (proof-only, given code + spec) achieves 86% in Dafny; Mode 6 (NL-only) drops to 29%. The constrained search space reduces ambiguity. Break condition: When specification functions encode domain-specific logic the model cannot infer from signatures alone.

### Mechanism 3: Training Distribution Alignment
Verification success correlates with the abundance of target-language examples in pre-training data. Dafny (most training data available) achieves 86% Mode 1 success; Verus (newest, least data) achieves only 45%. The model leverages learned syntax patterns and idioms. Break condition: Language features requiring specialized knowledge—Verus's type system distinctions between executable code and specification expressions (int vs usize)—cause systematic failures.

## Foundational Learning

- **Loop Invariants and Inductive Reasoning**
  - Why needed here: The paper explicitly identifies "undecidability of automatic loop invariant synthesis" as the core challenge. Invariant maintenance errors are the most common failure in Dafny (175 occurrences) and Nagini (259).
  - Quick check question: Given a loop that sums array elements, what invariant relates the accumulator to the loop index?

- **SMT Solvers and Verification Conditions**
  - Why needed here: All three systems (Dafny, Nagini, Verus) compile programs to verification conditions discharged by SMT solvers. Timeouts in Nagini (468 occurrences) reflect solver limitations.
  - Quick check question: Why might an SMT solver timeout on a verification condition that is logically valid?

- **Pre/Postcondition Specification Semantics**
  - Why needed here: The validation step checks whether generated specs *imply* reference specs. Generated preconditions can be weaker; postconditions can be stronger. Understanding this asymmetry is essential for interpreting results.
  - Quick check question: If a generated postcondition is stronger than the reference, is this always acceptable? When might it indicate a problem?

## Architecture Onboarding

- **Component map**: Task Preparation -> LLM Interaction Layer -> Verification Engine -> Feedback Controller -> Post-Processor -> Validator
- **Critical path**: Task Preparation → LLM Generation → Verification → (if failure) Error Feedback → Repair Loop (≤5 iterations) → Post-Processing → Validation
- **Design tradeoffs**: Validation uses *implication* rather than *equivalence* to allow flexibility, but may accept specs that don't fully match user intent; Iteration limit of 5 balances cost against diminishing returns; Post-processing limited to Nagini; Dafny/Verus rely entirely on LLM repair
- **Failure signatures**: Dafny: "Invariant (maintain)" (175), "Postcondition not proved" (91)—incomplete proof hints; Nagini: Timeouts (468)—no actionable feedback, LLM cannot diagnose; Verus: Type errors (192)—int/usize confusion between code and specification contexts
- **First 3 experiments**: 1) Run Mode 1 on Dafny with a simple loop-based problem to establish baseline success and observe error patterns; 2) Compare Nagini timeout rates at increased resource limits to determine if failures are solver-capacity or fundamental proof-search issues; 3) Add explicit type-cast hints to Verus prompts and measure reduction in type errors

## Open Questions the Paper Calls Out

### Open Question 1
Can fine-tuning on synthetic datasets significantly improve verification success rates for low-resource languages like Nagini and Verus? The authors state in the Conclusion that they plan to address poor performance in Nagini and Verus via fine-tuning, noting that current failures likely stem from model unfamiliarity.

### Open Question 2
To what extent do specialized error-correction mechanisms (e.g., type fixers or self-debugging) reduce verification errors compared to generic feedback loops? The Conclusion proposes leveraging "specialized error-correction mechanisms" and Section 3.1 identifies specific pitfalls that generic LLM feedback struggles to resolve.

### Open Question 3
Do reasoning-focused models (e.g., OpenAI o1 or DeepSeek-R1) outperform Claude 3.5 Sonnet in generating complete verified implementations from natural language? The authors limited the evaluation to Claude 3.5 Sonnet, explicitly noting they had not yet tested o1 or DeepSeek-R1 due to access limits.

## Limitations
- Mode 6 performance remains below 30% even for Dafny, indicating full synthesis from natural language is still a significant challenge
- Reliance on implication-based validation rather than equivalence checking may mask specification mismatches that don't preserve user intent
- Evaluation focuses on relatively small programs from HumanEval, and performance on larger, real-world codebases remains unknown

## Confidence

- **High confidence**: Mode 1 success rates (86% Dafny, 53% Nagini, 45% Verus) and the general pattern that partial context improves performance
- **Medium confidence**: The diagnosis of training distribution alignment as the primary driver of language-specific performance differences
- **Low confidence**: The scalability of error-feedback mechanisms to larger programs and more complex verification tasks

## Next Checks

1. Evaluate Mode 1 performance on real-world Dafny projects (e.g., from Dafny's own repository) to assess generalization beyond HumanEval benchmarks
2. Implement and test a timeout recovery mechanism for Nagini that attempts to decompose proofs or simplify verification conditions rather than relying on standard error feedback
3. Measure the impact of adding explicit type hints and casting examples to Verus prompts, specifically targeting the int/usize conversion issues that cause 192 type errors