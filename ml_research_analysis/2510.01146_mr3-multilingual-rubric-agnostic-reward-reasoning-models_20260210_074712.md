---
ver: rpa2
title: 'mR3: Multilingual Rubric-Agnostic Reward Reasoning Models'
arxiv_id: '2510.01146'
source_url: https://arxiv.org/abs/2510.01146
tags:
- reasoning
- response
- language
- evaluation
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: mR3 introduces a massively multilingual reward reasoning model
  trained on 72 languages, achieving the broadest language coverage in reward modeling
  to date. The core method involves standardizing input formats and using a dataset
  of 100k high-quality examples to train models that generate reasoning traces, explanations,
  and scores.
---

# mR3: Multilingual Rubric-Agnostic Reward Reasoning Models

## Quick Facts
- arXiv ID: 2510.01146
- Source URL: https://arxiv.org/abs/2510.01146
- Authors: David Anugraha; Shou-Yi Hung; Zilu Tang; Annie En-Shiun Lee; Derry Tanti Wijaya; Genta Indra Winata
- Reference count: 40
- Primary result: Introduces mR3, a massively multilingual reward reasoning model covering 72 languages, achieving the broadest language coverage in reward modeling to date.

## Executive Summary
mR3 introduces a massively multilingual reward reasoning model trained on 72 languages, achieving the broadest language coverage in reward modeling to date. The core method involves standardizing input formats and using a dataset of 100k high-quality examples to train models that generate reasoning traces, explanations, and scores. Key findings show that mR3 outperforms existing reward models and much larger models (e.g., GPT-OSS-120B) by up to 9x while being smaller, with consistent improvements across multilingual settings. Human evaluations confirm the quality of reasoning traces and rubrics, including for extremely low-resource languages unseen during training. The approach demonstrates effectiveness in off-policy preference optimization and provides interpretable, rubric-based evaluation across diverse languages.

## Method Summary
The method centers on training multilingual reward reasoning models using a standardized input format (instruction, input, response(s), rubric) and three structured outputs (reasoning trace, explanation, score). The dataset construction involves generating rubrics with GPT-4.1, distilling reasoning traces using GPT-OSS-120B, and applying alignment filtering to retain only examples where all three reasoning strategies (English reasoning, target-language reasoning, translated reasoning) produce correct outputs. Training uses supervised fine-tuning with cross-entropy loss, employing an easy-to-hard curriculum based on GPT-OSS-20B consistency scores. The approach achieves strong cross-lingual transfer, enabling evaluation in low-resource languages without direct training data.

## Key Results
- mR3 achieves the broadest language coverage in reward modeling to date with 72 languages
- Outperforms existing reward models and much larger models (e.g., GPT-OSS-120B) by up to 9x while being smaller
- Demonstrates consistent improvements across multilingual settings and generalizes to unseen low-resource languages
- Human evaluations confirm quality of reasoning traces and rubrics, including for extremely low-resource languages unseen during training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Rubric-based evaluation with explicit reasoning traces improves evaluation quality and interpretability across languages.
- **Mechanism:** The model receives structured input (instruction, input, response(s), rubric) and is trained to generate three outputs: (1) a reasoning trace, (2) a concise explanation, and (3) a score. The reasoning trace forces the model to work through evaluation criteria step-by-step before committing to a judgment.
- **Core assumption:** Models trained to verbalize their reasoning process will make more consistent and accurate evaluations than those trained to output only scalar scores.
- **Evidence anchors:** [abstract] "our models generate reasoning traces, explanations, and scores"; [Section 3.1.1] "the reasoning model generates a reasoning trace, trace, a concise explanation e justifying the evaluation, and a score s reflecting response quality under r"; [corpus] R3 (Anugraha et al., 2025) demonstrates similar rubric-agnostic reward reasoning for English-only settings

### Mechanism 2
- **Claim:** Difficulty-filtered multilingual data with aligned reasoning variants enables transfer across languages with minimal per-language data.
- **Mechanism:** The dataset construction retains only examples where all three reasoning strategies (English reasoning, target-language reasoning, translated reasoning) produce correct outputs. This alignment filters ambiguous or noisy examples, creating high-quality supervision even for low-resource languages.
- **Core assumption:** Cross-lingual transfer works best when the model sees consistent reasoning patterns across language variants for the same underlying task.
- **Evidence anchors:** [Section 3.1.3] "We retain only those training samples for which all three strategies produce correct outputs, minimizing confounding effects when comparing strategies"; [Section 4.2] "target-language reasoning...exhibits the largest relative gains after fine-tuning, even surpassing the base model's ENG-ENG performance"; [corpus] M-Prometheus (Pombal et al., 2025) shows multilingual judges benefit from translated training data but lacks systematic study of reasoning language strategies

### Mechanism 3
- **Claim:** Easy-to-hard curriculum ordering improves multilingual reward model training stability and final performance.
- **Mechanism:** Training samples are sorted by difficulty (primary: GPT-OSS-20B consistency score, secondary: token length). The model sees easier examples first, building foundational evaluation patterns before tackling edge cases.
- **Core assumption:** Reward model training benefits from graduated difficulty similar to human learning, avoiding early exposure to confusing or ambiguous examples.
- **Evidence anchors:** [Section 3.2.2] "sorting training data from easiest to hardest yields the best performance, where difficulty is defined primarily by consistency of correct predictions"; [Appendix H.2] Easy-to-hard curriculum outperforms random shuffle, hard-to-easy, and English-first variants on validation metrics; [corpus] No direct corpus evidence for curriculum learning in multilingual reward models

## Foundational Learning

- **Concept: Reward Models as Evaluators**
  - **Why needed here:** mR3 is fundamentally a reward model—it must learn to evaluate text quality against rubrics. Without understanding that reward models provide scalar or categorical signals for preference optimization, the motivation is unclear.
  - **Quick check question:** Can you explain how a reward model differs from a classification model and why explicit reasoning traces are added in mR3?

- **Concept: Cross-Lingual Transfer**
  - **Why needed here:** mR3 achieves strong performance on unseen low-resource languages (e.g., Javanese, Albanian, Telugu) despite no training data. Understanding transfer mechanisms explains why this works.
  - **Quick check question:** Why would a model trained on English, Spanish, and Chinese reasoning patterns generalize to Telugu evaluation tasks?

- **Concept: Curriculum Learning**
  - **Why needed here:** The paper's most specific methodological contribution is the easy-to-hard curriculum. Without prior knowledge of curriculum strategies, this appears as arbitrary ordering.
  - **Quick check question:** What signals could indicate whether a training sample is "easy" or "hard" for a language model? How does mR3 measure this?

## Architecture Onboarding

- **Component map:** Dataset curation -> Rubric generation (GPT-4.1 for missing rubrics) -> Reasoning distillation (GPT-OSS-120B) -> Alignment filtering (retain only examples correct across all strategies) -> Difficulty ranking -> SFT training (3 epochs, lr=1e-5) -> Evaluation across task types

- **Critical path:** 1. Dataset curation → 2. Rubric generation (GPT-4.1 for missing rubrics) → 3. Reasoning distillation (GPT-OSS-120B) → 4. Alignment filtering (retain only examples correct across all strategies) → 5. Difficulty ranking → 6. SFT training (3 epochs, lr=1e-5) → 7. Evaluation across task types

- **Design tradeoffs:**
  - **100k vs. larger datasets:** Paper ablates 10k/25k/50k/100k—100k gives best results but the marginal gains suggest diminishing returns
  - **SFT vs. RLVR:** Paper explicitly rejects RLVR (takes 2 days vs. 8 hours, worse performance on rubric-following)
  - **Language forcing vs. translation:** Forcing target-language reasoning with initial tokens gives better results than translating English reasoning

- **Failure signatures:**
  - Model outputs reasoning but incorrect score → rubric misalignment or ambiguous input
  - Model repeats "I should think clearly" without actual reasoning → language forcing fails for very low-resource languages (seen in Telugu examples)
  - Model ignores rubric criteria → insufficient rubric-specific training data or rubric too generic
  - Position bias in pairwise evaluation → evaluate with both orderings and average

- **First 3 experiments:**
  1. **Reproduce filtering threshold:** Start with 50k examples (simpler training), verify easy-to-hard curriculum improves over random shuffle on held-out validation (HelpSteer3 works as proxy)
  2. **Ablate reasoning language:** Train separate models on ENG-ENG only vs. full aligned data, compare performance gap on m-RewardBench—this isolates the contribution of multilingual reasoning alignment
  3. **Test on unseen language:** Take a low-resource language completely absent from training (e.g., Javanese), evaluate both accuracy and human preference of reasoning traces—this validates the generalization claim

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can iterative refinement of evaluation rubrics significantly improve the judgment accuracy or reasoning quality of multilingual reward models?
- **Basis in paper:** [explicit] The authors state in the Limitations section that they "do not explore iterative refinement of the rubrics, which we plan to explore in our future works."
- **Why unresolved:** The current study utilizes static, automatically generated rubrics (using GPT-4.1) without investigating whether dynamic updating or refining these rubrics based on model feedback could enhance performance.
- **What evidence would resolve it:** A comparative study showing performance metrics (accuracy on m-RewardBench or human evaluation scores) of an mR3 variant trained with an iterative rubric refinement loop versus the static baseline.

### Open Question 2
- **Question:** To what extent does increasing the volume of target-language pre-training data close the performance gap between target-language reasoning and English reasoning?
- **Basis in paper:** [explicit] The authors observe that "models fine-tuned on English reasoning still perform better than models fine-tuned in target languages" and explicitly highlight "the need to collect large-scale pre-training data in other languages" to address this disparity.
- **Why unresolved:** While the paper demonstrates that fine-tuning helps, the underlying dominance of English reasoning suggests a fundamental limitation in the base models' capabilities in other languages, which was not fully resolved by the SFT data alone.
- **What evidence would resolve it:** Experiments training mR3 on base models with varying scales of target-language pre-training data, measuring the convergence of "Target Prompt→Target Thinking" performance toward "English Prompt→English Thinking" levels.

### Open Question 3
- **Question:** Can Reinforcement Learning through Verifiable Reward (RLVR) be modified to explicitly enforce rubric adherence during training, rather than optimizing solely for final answer correctness?
- **Basis in paper:** [inferred] In Appendix H.5, the authors note that RLVR (GRPO) failed because it "provides feedback only based on the correctness of the final answer, without evaluating the quality of the reasoning process," leading to models that ignore rubrics. The implication is that a rubric-aware RLVR method is needed.
- **Why unresolved:** The current study concludes SFT is superior to the specific GRPO implementation used, but leaves open the possibility that a different RL formulation could combine the reasoning benefits of SFT with the optimization power of RL.
- **What evidence would resolve it:** Developing an RL reward signal that incorporates rubric compliance (e.g., penalizing valid answers that lack rubric-justified reasoning) and showing it outperforms the SFT baseline.

## Limitations
- The alignment filtering strategy may create a training distribution that is too narrow, potentially limiting robustness to diverse evaluation scenarios
- The 100k training examples, while substantial, may not capture the full complexity of multilingual evaluation across 72 languages, particularly for low-resource language pairs
- The reliance on GPT-4.1 for rubric generation and GPT-OSS-120B for reasoning distillation introduces dependency on proprietary/closed models, raising questions about reproducibility

## Confidence

**High Confidence (90%+):** The core empirical finding that mR3 outperforms existing reward models and much larger models (GPT-OSS-120B) by up to 9x while being smaller is well-supported by the evaluation on m-RewardBench and human preference studies.

**Medium Confidence (70-89%):** The generalization claim to unseen low-resource languages (Javanese, Albanian, Telugu) is supported by human evaluations but relies on a limited sample size.

**Low Confidence (below 70%):** The cross-lingual transfer mechanism's effectiveness for extremely low-resource languages is demonstrated but not extensively validated across diverse task types.

## Next Checks

1. **Evaluate curriculum learning sensitivity:** Systematically vary the difficulty estimation method (beyond GPT-OSS-20B consistency) and test whether easy-to-hard ordering remains superior. Test with different curriculum strategies (exponential, logarithmic) to confirm the reported improvements are not artifacts of the specific ordering method.

2. **Stress-test low-resource generalization:** Evaluate mR3 on a broader set of truly unseen low-resource languages (e.g., languages with <1000 training examples total) across multiple task types. Compare against strong baselines like GPT-4o-mini and GPT-4.1 to quantify the 9x improvement in diverse scenarios, including pairwise evaluation with position bias testing.

3. **Test model robustness to distribution shift:** Evaluate mR3 performance when applied to evaluation tasks that differ significantly from training data (e.g., creative writing evaluation, factual accuracy checking). Measure performance degradation and analyze whether reasoning traces remain coherent when the model encounters out-of-distribution prompts.