---
ver: rpa2
title: Stochastic Linear Bandits with Parameter Noise
arxiv_id: '2601.23164'
source_url: https://arxiv.org/abs/2601.23164
tags:
- regret
- bound
- lemma
- algorithm
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies stochastic linear bandits under the parameter
  noise model, where the reward is a linear function of an action vector and a parameter
  vector drawn from a fixed distribution. The authors provide algorithms with variance-dependent
  regret bounds that can outperform the general minimax bounds for specific action
  sets.
---

# Stochastic Linear Bandits with Parameter Noise

## Quick Facts
- **arXiv ID**: 2601.23164
- **Source URL**: https://arxiv.org/abs/2601.23164
- **Reference count**: 40
- **Key outcome**: This paper studies stochastic linear bandits under the parameter noise model, where the reward is a linear function of an action vector and a parameter vector drawn from a fixed distribution. The authors provide algorithms with variance-dependent regret bounds that can outperform the general minimax bounds for specific action sets.

## Executive Summary
This paper studies stochastic linear bandits under the parameter noise model, where rewards are linear functions of action vectors and parameter vectors drawn from a fixed distribution. The authors develop algorithms with variance-dependent regret bounds that improve upon traditional minimax bounds for specific action sets. They provide an optimal design-based successive elimination algorithm for general finite action sets and an explore-exploit algorithm for ℓp unit balls. The key insight is that parameter noise allows leveraging linear structure to achieve better concentration bounds compared to additive noise models.

## Method Summary
The paper introduces algorithms that exploit the parameter noise structure in stochastic linear bandits. For general finite action sets, they present a successive elimination algorithm based on optimal design theory with regret O(d² + √(dT log(K/δ) σ²max)). For ℓp unit balls with p ≤ 2, they develop a simple explore-exploit algorithm that achieves O(d + √(dT σ²q)) regret when the covariance matrix is known, and O(d^(2/3+2/3q)T^(1/3) + √(dT σ²q)) when unknown. The algorithms leverage the parameter noise structure to obtain variance-dependent bounds that improve upon traditional minimax bounds for specific action set geometries.

## Key Results
- For general finite action sets, achieves regret O(d² + √(dT log(K/δ) σ²max)) using optimal design-based successive elimination
- For ℓp unit balls with p ≤ 2, achieves O(d + √(dT σ²q)) regret when covariance is known, and O(d^(2/3+2/3q)T^(1/3) + √(dT σ²q)) when unknown
- Provides matching lower bounds showing these regret bounds are tight up to logarithmic factors

## Why This Works (Mechanism)
The parameter noise model allows the algorithms to exploit the linear structure more effectively than traditional additive noise models. By leveraging the fact that rewards are linear functions of both action vectors and parameter vectors drawn from a fixed distribution, the algorithms can achieve better concentration bounds. This leads to variance-dependent regret bounds that improve upon general minimax bounds for specific action set geometries, particularly when the covariance structure of the parameter noise is known or can be estimated.

## Foundational Learning
- **Stochastic linear bandits**: Multi-armed bandit problems where rewards are linear functions of action vectors and unknown parameters. Understanding this is needed to grasp the problem setting and compare with traditional approaches.
- **Parameter noise model**: Rewards are linear functions of action vectors and parameter vectors drawn from a fixed distribution. This differs from additive noise and is central to the paper's contributions.
- **Optimal design theory**: Mathematical framework for selecting actions to minimize estimation error. Used in the successive elimination algorithm for general action sets.
- **ℓp unit balls**: Geometric action sets where the ℓp norm is bounded by 1. The explore-exploit algorithm specifically targets these action set geometries.
- **Variance-dependent regret bounds**: Bounds that depend on the variance of the reward distribution rather than just worst-case bounds. The paper shows these can be tighter for specific action sets.
- **Successive elimination algorithms**: Bandit algorithms that iteratively eliminate suboptimal actions based on statistical confidence intervals.

## Architecture Onboarding

**Component Map**: Action selection -> Parameter estimation -> Confidence interval computation -> Regret calculation

**Critical Path**: The algorithm selects actions based on exploration-exploitation trade-off, estimates parameters from observed rewards, computes confidence intervals using the parameter noise structure, and updates regret bounds accordingly.

**Design Tradeoffs**: The algorithms balance between exploration (gathering information about the parameter distribution) and exploitation (maximizing immediate reward). The parameter noise assumption enables tighter concentration bounds but requires accurate knowledge or estimation of the covariance structure.

**Failure Signatures**: If the actual noise structure deviates significantly from the assumed parameter noise model, the algorithms may perform worse than traditional approaches. The benefits are also action-set dependent, so poor performance on certain geometries is expected.

**First Experiments**:
1. Compare the explore-exploit algorithm on ℓ2 unit balls against standard linear bandit algorithms
2. Test the successive elimination algorithm on randomly generated finite action sets
3. Evaluate the algorithms under varying levels of parameter noise variance to verify the variance-dependent bounds

## Open Questions the Paper Calls Out
None

## Limitations
- Improvements are action-set dependent and may not materialize for all problem instances
- The parameter noise assumption represents a departure from standard additive noise models and may not apply universally
- The paper lacks empirical validation to demonstrate practical performance improvements in real-world scenarios

## Confidence
- Theoretical contributions are significant with matching lower bounds: Medium
- Practical significance of improvements depends heavily on problem structure: Medium
- Mathematical proofs appear sound but practical validation is needed: Medium

## Next Checks
1. Implement the proposed algorithms and conduct empirical comparisons against standard linear bandit algorithms across various action set geometries to validate the theoretical improvements
2. Extend the analysis to more general parameter distributions beyond those considered, particularly non-Gaussian distributions
3. Investigate the robustness of the algorithms to violations of the parameter noise assumption, comparing performance when the actual noise structure differs from the assumed model