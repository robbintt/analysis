---
ver: rpa2
title: Cross-Domain Imitation Learning via Optimal Transport
arxiv_id: '2110.03684'
source_url: https://arxiv.org/abs/2110.03684
tags:
- learning
- expert
- agent
- optimal
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Gromov-Wasserstein Imitation Learning (GWIL),
  a method for cross-domain imitation learning that addresses the challenge of transferring
  expert demonstrations from one domain to train an agent in a different domain with
  potentially different state-action spaces and dynamics. The core idea is to use
  the Gromov-Wasserstein distance to align and compare states between different agent
  spaces without requiring proxy tasks or explicit mapping between domains.
---

# Cross-Domain Imitation Learning via Optimal Transport

## Quick Facts
- arXiv ID: 2110.03684
- Source URL: https://arxiv.org/abs/2110.03684
- Authors: Arnaud Fickinger; Samuel Cohen; Stuart Russell; Brandon Amos
- Reference count: 12
- Primary result: Introduces Gromov-Wasserstein Imitation Learning (GWIL) for cross-domain imitation learning without requiring explicit state-action space mappings

## Executive Summary
This paper addresses the challenge of cross-domain imitation learning where expert demonstrations come from a different domain than the agent's environment. The key innovation is using Gromov-Wasserstein distance to align and compare states between different agent spaces without requiring proxy tasks or explicit mapping between domains. The method formulates imitation learning as an optimal transport problem, where the Gromov-Wasserstein distance serves as a proxy reward for training the imitation agent. Theoretical analysis characterizes scenarios where GWIL preserves optimality, showing that optimal policies can be recovered up to isometries.

## Method Summary
GWIL treats trajectories as empirical occupancy measures and computes the Gromov-Wasserstein distance between expert and agent trajectories to derive a pseudo-reward for imitation learning. At each training step, the optimal transport coupling is computed between expert trajectory samples and agent trajectory samples using a conditional gradient method. This coupling is then used to calculate a pseudo-reward based on pairwise distance discrepancies between the two domains. The agent is trained using Soft Actor-Critic to maximize cumulative pseudo-reward, enabling policy learning without external rewards or explicit state-action space mappings.

## Key Results
- Successfully transfers expert behavior across domains with different state-action spaces and morphologies (e.g., cheetah to walker)
- Recovers optimal behaviors without external rewards in non-trivial continuous control domains
- Achieves competitive performance compared to baseline methods while offering good scalability
- Demonstrates effectiveness on rigid transformations, different state-action spaces, and significantly different morphologies

## Why This Works (Mechanism)
GWIL leverages the Gromov-Wasserstein distance to find optimal alignments between states in different domains based on their intrinsic geometry rather than requiring explicit mappings. By treating trajectories as occupancy measures and computing the optimal transport coupling, the method creates a natural reward signal that guides the agent to behave similarly to the expert in a geometrically consistent way. The Gromov-Wasserstein framework is particularly suited for this task because it compares distances within each domain rather than absolute values, making it robust to domain shifts.

## Foundational Learning
- **Gromov-Wasserstein Distance**: A measure of similarity between metric spaces that compares pairwise distances within each space rather than absolute values. Needed because it enables comparison of structurally similar states across different domains without requiring explicit mappings. Quick check: Verify that GW distance is invariant to isometric transformations.
- **Optimal Transport Theory**: Mathematical framework for finding optimal mappings between probability distributions. Needed to compute the coupling between expert and agent trajectories that maximizes similarity. Quick check: Confirm that the coupling matrix satisfies marginal constraints.
- **Soft Actor-Critic (SAC)**: Maximum entropy reinforcement learning algorithm that balances exploration and exploitation. Needed as the underlying policy optimization method that maximizes the pseudo-reward derived from GW distance. Quick check: Ensure entropy coefficient is properly tuned for stable learning.

## Architecture Onboarding
- **Component Map**: Expert trajectory -> GW solver -> Coupling matrix -> Pseudo-reward function -> SAC agent -> Policy update
- **Critical Path**: The bottleneck is computing the GW coupling between expert and agent trajectories, which scales quadratically with trajectory length
- **Design Tradeoffs**: Static GW distance ignores temporal structure vs. computational efficiency; Euclidean distance simplicity vs. potential mismatch with domain-specific geometry
- **Failure Signatures**: Isometry ambiguity (agent learns mirror behavior), computational bottleneck with long trajectories, poor performance with non-isometric domain shifts
- **First Experiments**: 1) Verify GW solver produces valid coupling matrices on simple synthetic data 2) Test SAC training with synthetic pseudo-rewards 3) Validate end-to-end pipeline on PointMass Maze with rigid transformation

## Open Questions the Paper Calls Out
- Can incorporating metrics aware of temporal structure improve alignment performance compared to the static Gromov-Wasserstein approach? The current method ignores temporal information and ordering, which may be critical for tasks requiring strict sequential timing.
- How does the Gromov-Wasserstein distance compare to other optimal transport distances that offer different coupling flexibilities? Other OT distances like co-optimal transport might handle non-isometric morphologies more effectively.
- Can the method scale to high-dimensional demonstrations and complex agents without computational bottlenecks? The current experiments are limited to low-dimensional continuous control tasks.

## Limitations
- Theoretical analysis is limited to deterministic dynamics, while practical implementations use stochastic policies
- Isometry requirements may be too restrictive for real-world applications where perfect geometric alignment is impossible
- Computational complexity of Gromov-Wasserstein solver could limit scalability to complex high-dimensional domains

## Confidence
- Theoretical guarantees (Low): Proofs limited to deterministic dynamics with strong isometry assumptions
- Experimental results (Medium): Demonstrated on 2D gridworld and Mujoco tasks, but with limited hyperparameter disclosure
- Scalability claims (Low): Computational complexity concerns not fully addressed

## Next Checks
1. Test GWIL on stochastic dynamics environments to validate practical applicability beyond deterministic settings
2. Implement timing benchmarks comparing GWIL with and without trajectory subsampling to quantify computational trade-offs
3. Evaluate performance using alternative distance metrics (e.g., learned metrics or domain-specific metrics) to assess robustness beyond Euclidean distance