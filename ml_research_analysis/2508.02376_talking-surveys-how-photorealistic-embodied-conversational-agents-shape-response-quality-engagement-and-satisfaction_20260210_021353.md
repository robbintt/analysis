---
ver: rpa2
title: 'Talking Surveys: How Photorealistic Embodied Conversational Agents Shape Response
  Quality, Engagement, and Satisfaction'
arxiv_id: '2508.02376'
source_url: https://arxiv.org/abs/2508.02376
tags:
- agent
- participants
- agents
- https
- embodied
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether embodied conversational agents
  (ECAs) can enhance the quality and engagement of online survey responses compared
  to text-based chatbots. The researchers developed the Virtual Agent Interviewer
  (VAI), an AI-driven survey tool featuring photorealistic video avatars, speech recognition,
  and LLM-powered conversation logic.
---

# Talking Surveys: How Photorealistic Embodied Conversational Agents Shape Response Quality, Engagement, and Satisfaction

## Quick Facts
- **arXiv ID:** 2508.02376
- **Source URL:** https://arxiv.org/abs/2508.02376
- **Reference count:** 40
- **Primary result:** Embodied conversational agents elicit more informative and longer survey responses than text-based chatbots.

## Executive Summary
This study investigates whether photorealistic embodied conversational agents (ECAs) can enhance the quality and engagement of online survey responses compared to text-based chatbots. Researchers developed the Virtual Agent Interviewer (VAI), an AI-driven survey tool featuring photorealistic video avatars, speech recognition, and LLM-powered conversation logic. In a between-subjects experiment with 80 UK participants, surveys were administered with either an embodied agent or a chatbot, collecting 2,265 responses across two standardized psychometric questionnaires (BFI-2-S and CFQ). Results showed that responses to the ECA were significantly more informative and contained more words and characters, indicating richer and more detailed feedback. The ECA also prompted faster responses per unit of information and encouraged higher variability in self-disclosure, though no significant differences were found in specificity, relevance, or sentiment. User satisfaction ratings were similar between agents, but qualitative feedback revealed that many participants found the ECA interaction more natural and engaging, despite occasional technical issues like delays and Uncanny Valley effects.

## Method Summary
The researchers developed the Virtual Agent Interviewer (VAI), a custom survey tool combining a React frontend and Django backend. Participants were randomly assigned to interact with either a photorealistic embodied agent (using HeyGen API with GPT-4o-mini and Whisper ASR) or a text-based chatbot (GPT-4o-mini). Both agents administered two standardized psychometric surveys (BFI-2-S and CFQ) with follow-up questions. Responses were transcribed, corrected with SymSpell, and analyzed for quality (Informativeness, Specificity, Relevance, Clarity), engagement (word count, time, self-disclosure), and satisfaction (Likert scales). Analysis used Mann-Whitney U tests and GPT-4.1 for labeling.

## Key Results
- Responses to the ECA were significantly more informative and contained more words and characters.
- ECA prompted faster responses per unit of information and higher variability in self-disclosure.
- No significant differences were found in specificity, relevance, or sentiment between agents.

## Why This Works (Mechanism)
The photorealistic embodied conversational agent (ECA) enhances survey response quality and engagement by creating a more natural, conversational interaction compared to text-based chatbots. The ECA leverages speech recognition, real-time speech synthesis, and video avatars to simulate human-like conversation, which encourages participants to provide richer, more detailed feedback. The modular prompt architecture and error-handling mechanisms ensure robust conversation flow, while the use of standardized psychometric surveys provides reliable, comparable data. The findings suggest that the embodied nature and multimodal interaction of ECAs can overcome the limitations of text-based interfaces in eliciting high-quality survey responses.

## Foundational Learning
- **Mann-Whitney U test**: A non-parametric test for comparing two independent groups when data is not normally distributed. *Why needed:* To statistically compare response quality and engagement metrics between ECA and chatbot groups. *Quick check:* Ensure data is ordinal or not normally distributed before applying.
- **GPT-4.1 labeling**: Using a large language model to code qualitative responses for specific attributes (e.g., Informativeness, Clarity). *Why needed:* To objectively quantify subjective response qualities at scale. *Quick check:* Validate a sample of LLM-coded responses manually for accuracy.
- **SymSpell correction**: A fast, approximate spell correction algorithm for noisy text. *Why needed:* To clean ASR-generated transcripts for more accurate downstream analysis. *Quick check:* Inspect a sample of corrected transcripts for common ASR errors.
- **Uncanny Valley effect**: A phenomenon where human-like agents become unsettling as they approach realism but fall short of perfect human likeness. *Why needed:* To understand potential negative impacts of photorealistic avatars on user experience. *Quick check:* Survey participants about their comfort level with the avatar after interaction.
- **Between-subjects design**: An experimental design where different participants are exposed to different conditions. *Why needed:* To isolate the effect of the ECA versus chatbot on response quality without carryover effects. *Quick check:* Verify randomization and equal group sizes.
- **Modular prompt architecture**: Breaking down conversation logic into distinct, reusable prompt modules (Security, Summary, Inquiry, Wrapping). *Why needed:* To ensure robust, context-aware conversation flow and error handling. *Quick check:* Review logs for prompt execution order and error recovery.

## Architecture Onboarding
- **Component map:** React Frontend -> Django Backend -> OpenAI GPT-4o-mini/Whisper -> HeyGen API (ECA) / Text Interface (Chatbot) -> SymSpell Correction -> GPT-4.1 Labeling -> Mann-Whitney U Analysis
- **Critical path:** User speech -> Whisper ASR -> Prompt modules -> LLM response -> HeyGen avatar/text output -> User response -> SymSpell correction -> GPT-4.1 labeling -> Statistical analysis
- **Design tradeoffs:** Photorealistic avatars increase engagement but may trigger Uncanny Valley effects and technical latency; modular prompts improve robustness but add complexity; ASR enables voice input but introduces transcription errors.
- **Failure signatures:** High latency or interruptions during ECA interaction; ASR errors leading to garbled responses; Uncanny Valley discomfort reported by participants; uneven distribution of response quality metrics.
- **Three first experiments:** 1) Test the VAI tool with a small pilot group to identify technical issues (latency, ASR accuracy). 2) Manually inspect a sample of transcripts to validate SymSpell correction and GPT-4.1 labeling accuracy. 3) Run a power analysis to determine if 80 participants is sufficient for detecting effect sizes in response quality metrics.

## Open Questions the Paper Calls Out
None

## Limitations
- The study is limited to a single lab study with 80 participants, which may not generalize to broader populations or longer-term use cases.
- The technical implementation relies on specific APIs (HeyGen, OpenAI) that may not be universally available or may change over time.
- The "Uncanny Valley" effects and technical delays reported by participants suggest the system may not yet be production-ready.

## Confidence
- **High Confidence:** ECAs produced more words and characters per response, and faster responses per unit of information.
- **Medium Confidence:** ECAs increase self-disclosure variability, but this requires replication to confirm mechanism and generalizability.
- **Medium Confidence:** ECAs feel more "natural and engaging" based on subjective user reports, which could be influenced by novelty effects.

## Next Checks
1. **Replication with Larger, Diverse Sample:** Conduct a field study with a larger and more demographically diverse participant pool to assess generalizability and test for cultural or age-related differences in ECA acceptance.
2. **Technical Optimization Validation:** Systematically test and optimize the HeyGen API configuration (voice selection, animation thresholds) and ASR silence detection to reduce latency and improve the naturalness of the interaction, then re-run the experiment to measure impact on engagement and satisfaction.
3. **Longitudinal Use Case Study:** Deploy the ECA system in a real-world, unmoderated user research setting over several weeks to evaluate sustained engagement, response quality decay, and practical usability beyond a single lab session.