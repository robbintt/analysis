---
ver: rpa2
title: Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing
arxiv_id: '2511.12784'
source_url: https://arxiv.org/abs/2511.12784
tags:
- formal
- paraphrased
- lean
- language
- statements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of large language models
  (LLMs) in autoformalization tasks when faced with semantically similar paraphrased
  natural language (NL) inputs. The authors hypothesize that, like in the text-to-SQL
  domain, LLMs may exhibit sensitivity to linguistic variations even when semantic
  meaning is preserved.
---

# Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing

## Quick Facts
- arXiv ID: 2511.12784
- Source URL: https://arxiv.org/abs/2511.12784
- Reference count: 12
- Primary result: Autoformalization models are sensitive to semantically similar paraphrased NL inputs, with performance variability across models and benchmarks

## Executive Summary
This paper investigates whether large language models (LLMs) in autoformalization tasks are sensitive to linguistic variations when semantic meaning is preserved. Using GPT-4o-mini and Claude-3.7-sonnet, the authors generate paraphrased natural language statements from formal proofs in MiniF2F and ProofNet benchmarks, then cross-evaluate these paraphrases for semantic and syntactic accuracy. The results demonstrate that paraphrased inputs lead to significant variability in both BLEU scores and compilation accuracy across models and formal systems. GPT-paraphrased inputs improved BLEU scores but showed mixed effects on compilation, while Claude-paraphrased inputs often improved compilation accuracy but slightly reduced BLEU scores. The findings confirm that autoformalization systems exhibit surface-form brittleness even when semantic fidelity is preserved, highlighting the need for more robust pipelines.

## Method Summary
The study employs a two-stage pipeline: first, formal statements from MiniF2F (Isabelle/HOL) and ProofNet (Lean 4) are paraphrased into natural language using GPT-4o-mini and Claude-3.7-sonnet; second, these paraphrased statements are cross-evaluated by both models to generate formalizations. The authors measure semantic fidelity using BLEU scores with NLTK smoothing and syntactic validity through compilation success rates. For each input, 10 formal candidates are generated with temperature=0.0 to ensure reproducibility. The evaluation includes semantic validation using SBERT embeddings, lexical diversity metrics (TTR), and sentence length analysis to ensure paraphrases preserve meaning.

## Key Results
- Paraphrased inputs produced significant variability in BLEU scores across both benchmarks and models
- Claude-paraphrased inputs achieved highest Pass@10 compilation accuracy (70.9%), while GPT-paraphrased inputs improved BLEU scores but had mixed compilation effects
- Lean 4 compilation accuracy declined more sharply under paraphrasing than Isabelle/HOL, suggesting type-strictness filtering
- Cross-model evaluation revealed systematic performance differences, indicating paraphrase transfer asymmetry effects

## Why This Works (Mechanism)

### Mechanism 1: Semantic-Syntactic Decoupling in Autoformalization
Paraphrased inputs that preserve semantic meaning can produce divergent formalization outputs, indicating LLMs rely on surface-form patterns rather than pure semantic grounding. The autoformalization model maps NL→Formal through learned token-level correlations. When paraphrasing alters lexical patterns (even with high cosine similarity ~62-78%), the model's token-level associations shift, producing formally different outputs.

### Mechanism 2: Cross-Model Paraphrase Transfer Asymmetry
The identity of the paraphrasing model creates systematic biases in downstream formalization performance. Each LLM has implicit "translation dialects"—characteristic lexical/structural patterns. When the same model paraphrases and formalizes (e.g., GPT→GPT), self-consistency improves BLEU; cross-model pairs (Claude paraphrase→GPT formalize) show transfer penalties.

### Mechanism 3: Formal System Type-Strictness Filtering
Lean 4's stricter type system exposes paraphrase sensitivity more sharply than Isabelle/HOL. Lean 4 requires explicit type annotations and has less type inference. Paraphrased NL that omits or ambiguously expresses type information compiles successfully in Isabelle but fails in Lean 4.

## Foundational Learning

- **BLEU Score with Brevity Penalty**: Primary metric for semantic fidelity; understanding n-gram precision + length penalty explains why paraphrases can score higher than ground truth. Quick check: If a candidate formalization is syntactically correct but shorter than reference, will BLEU increase or decrease?

- **Pass@K Evaluation**: Captures whether correct formalization exists in top-K samples; essential for interpreting Table 1's compilation accuracy columns. Quick check: Pass@10 = 70% means at least one correct output appears in how many samples? (Answer: ≥1 of 10, in 70% of problems)

- **Type-Token Ratio (TTR) for Lexical Diversity**: Quantifies paraphrase variation; low TTR with high semantic similarity indicates conservative paraphrasing. Quick check: If paraphrasing adds filler words without changing vocabulary, does TTR increase or decrease?

## Architecture Onboarding

- **Component map**: Formal Proof → [LLM Paraphraser] → Paraphrased NL → [Semantic Validator] → [LLM Formalizer] → Candidate Formal → [Compiler Check]
- **Critical path**: Formal→NL paraphrasing quality determines downstream variability. Poor paraphrases (low cosine similarity) invalidate robustness conclusions.
- **Design tradeoffs**: Temperature=0.0 ensures reproducibility but limits paraphrase diversity; single-pass paraphrasing (no iterative refinement) may miss edge cases; Pass@K with K=10 trades compute cost for coverage
- **Failure signatures**: High BLEU + low compilation = syntactically plausible but invalid logic; Low BLEU + high compilation = non-canonical but correct formalization; Cross-model BLEU drop >10% indicates paraphrase dialect mismatch
- **First 3 experiments**: 1) Reproduce MiniF2F baseline with single model (GPT→GPT), confirming reported BLEU/compilation ranges; 2) Add semantic similarity threshold filter (e.g., cosine < 0.6 → reject paraphrase) to isolate high-fidelity subset; 3) Run error categorization on failed compilations to test type-strictness hypothesis (classify: type error, syntax error, missing import)

## Open Questions the Paper Calls Out

- **Does sensitivity to paraphrased inputs persist across broader LLM architectures?**: Future research should expand to more models to determine if some exhibit greater robustness than GPT-4o-mini and Claude-3.7-sonnet.

- **How does performance degrade under low semantic similarity, ambiguity, or inconsistency?**: The study didn't explore results for low semantic similarity, ambiguous, or inconsistent proof requests.

- **What specific error categories are induced by linguistic variations?**: The paper didn't perform systematic evaluation of semantic or compilation error categories to provide deeper insights.

## Limitations

- SBERT embedding similarity may not adequately capture mathematical semantic equivalence for validation
- Lack of error categorization prevents definitive conclusions about failure modes
- Cross-model paraphrase transfer asymmetry mechanism lacks direct corpus evidence
- Limited to two proprietary models (GPT-4o-mini and Claude-3.7-sonnet) without broader architecture testing

## Confidence

- **High Confidence**: Paraphrased inputs produce variability in both BLEU and compilation performance across models and benchmarks
- **Medium Confidence**: Lean 4's stricter type system exposes paraphrase sensitivity more sharply than Isabelle/HOL
- **Low Confidence**: Cross-model paraphrase transfer asymmetry mechanism (paraphrasing model dialects)

## Next Checks

1. Perform detailed error analysis on compilation failures, categorizing them into type errors, syntax errors, and missing imports to test the type-strictness hypothesis directly

2. Have domain experts evaluate a sample of paraphrased NL statements to validate whether SBERT similarity scores accurately reflect mathematical semantic preservation

3. Expand the experiment to include additional LLM pairs (e.g., GPT-4, Gemini) to determine whether cross-model performance differences persist