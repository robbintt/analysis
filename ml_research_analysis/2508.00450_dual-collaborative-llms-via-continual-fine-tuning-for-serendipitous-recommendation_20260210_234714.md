---
ver: rpa2
title: Dual Collaborative LLMs via Continual Fine-Tuning for Serendipitous Recommendation
arxiv_id: '2508.00450'
source_url: https://arxiv.org/abs/2508.00450
tags:
- uni00000013
- user
- uni00000011
- categories
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CoEA tackles LLM-based recommendation\u2019s bias from missing\
  \ group identity and static dual-model optimization by jointly modeling long-term\
  \ group preferences and short-term individual interests, and using periodic collaborative\
  \ updates to close the optimization loop. It uses Dual-Stable Interest Exploration\
  \ to map user behaviors into group collaborative semantic IDs and short-term category\
  \ sets, then periodically refines the Novelty LLM with Relevance LLM feedback and\
  \ KL regularization to maintain novelty without forgetting."
---

# Dual Collaborative LLMs via Continual Fine-Tuning for Serendipitous Recommendation

## Quick Facts
- arXiv ID: 2508.00450
- Source URL: https://arxiv.org/abs/2508.00450
- Reference count: 40
- Primary result: 0.73% lift in recommendation quality and 19.34% gain in novelty via dual-LLM closed-loop optimization

## Executive Summary
CoEA addresses the challenge of serendipitous recommendation by jointly modeling long-term group preferences and short-term individual interests through a dual-LLM architecture. The system uses a Residual-Quantized VAE to map user behaviors into collaborative semantic IDs, then employs periodic collaborative updates between a novelty generator and relevance verifier LLM to close the optimization loop. Offline experiments show significant improvements in both recommendation quality and novelty metrics, while online A/B tests demonstrate measurable business impact with +1.203% GTV and +2.364% 7D-NIEP improvements.

## Method Summary
The CoEA framework consists of two main components: the Dual-Stable Interest Exploration (DSIE) module and the Periodic Collaborative Optimization (PCO) module. DSIE filters and encodes user sequences, then clusters users into Group CSIDs using RQ-VAE, generating textual profiles for each group. The PCO module trains two separate LLMs (Novelty and Relevance) using supervised fine-tuning, then implements a closed-loop update where the Relevance LLM scores candidate categories from the Novelty LLM, and preference pairs are used to fine-tune the Novelty LLM via DPO with KL regularization to prevent catastrophic forgetting.

## Key Results
- Offline experiments show 0.73% lift in quality metrics (C-H@K, C-N@K) over state-of-the-art methods
- 19.34% gain in novelty metrics (NCP@K, CLTP@K) compared to baselines
- Online A/B tests yield +1.203% GTV and +2.364% 7D-NIEP improvements
- Ablation studies confirm the importance of both the dual-LLM loop and KL regularization for maintaining performance

## Why This Works (Mechanism)

### Mechanism 1: Stable Group Profiling via RQ-VAE
Mapping user long-term behaviors to discrete "Collaborative Semantic IDs" (CSID) reduces noise and grounds novelty exploration in stable group preferences. The RQ-VAE quantizes user embeddings into hierarchical IDs, and a Profile LLM generates textual descriptions for clusters rather than relying on noisy individual history. Core assumption: users with similar long-term interaction patterns share latent interests predictive of novel item acceptance. Break condition: if user interests are extremely ephemeral or cross-domain, long-term history clustering may create "generic" personas that fail to predict specific novel interests.

### Mechanism 2: Dual-LLM Closed-Loop Optimization
Replacing static "one-time alignment" with periodic closed-loop updates between a generator and verifier improves long-term novelty. The Relevance LLM scores candidate categories from the Novelty LLM, and preference pairs are used to fine-tune the Novelty LLM via DPO. Core assumption: the Relevance LLM is a sufficiently accurate proxy for real user preference, allowing alignment without immediate user feedback on every candidate. Break condition: if the Relevance LLM drifts or is biased, it will propagate "preference errors" back into the Novelty LLM, narrowing the exploration space.

### Mechanism 3: KL-Regularized Fine-Tuning
Explicitly constraining updates with KL divergence prevents "catastrophic forgetting" of exploration capabilities during alignment. The DPO loss includes a weighted KL divergence term relative to the reference model, ensuring the model doesn't shift too aggressively toward exploitation of known hits. Core assumption: unconstrained fine-tuning on relevance feedback would rapidly overwrite the pre-trained "novelty" generation capabilities. Break condition: if the coefficient α is too high, the model becomes rigid and fails to adapt to new trends; if too low, it suffers forgetting.

## Foundational Learning

### Concept: Residual-Quantized VAE (RQ-VAE)
**Why needed:** To convert continuous, high-dimensional user vectors into discrete "tokens" (CSID) that can be used to identify groups and prompt the LLM with a semantic ID rather than a vector.
**Quick check:** Can you explain why a discrete codebook is preferred here over clustering (e.g., K-Means) for generating LLM prompts? (Hint: Hierarchical granularity).

### Concept: Direct Preference Optimization (DPO)
**Why needed:** To fine-tune the Novelty LLM using binary preference data (clicked vs. not clicked) without training a separate reward model, which stabilizes the closed-loop update.
**Quick check:** How does DPO differ from Reinforcement Learning from Human Feedback (RLHF) in terms of the loss function used?

### Concept: Catastrophic Forgetting
**Why needed:** A critical risk in the architecture; as the model learns new relevance patterns, it must explicitly be prevented (via KL regularization) from forgetting its base knowledge of diverse item categories.
**Quick check:** In the context of this paper, what metric (Quality or Novelty) drops if catastrophic forgetting occurs during the fine-tuning loop?

## Architecture Onboarding

### Component map:
Input: Long-term sequence + Short-term clicks → DSIE Module (Causal Self-Attention + RQ-VAE) → Group CSID + Profile → Dual-LLM Core (Novelty LLM + Relevance LLM) → Optimization (DPO Loop with KL Constraint) → Storage (Offline Key-Value DB)

### Critical path:
The integrity of the **Group CSID**. If the RQ-VAE clustering is too coarse, the Profile LLM generates generic personas (e.g., "General Shopper"), causing the Novelty LLM to hallucinate irrelevant suggestions.

### Design tradeoffs:
- **Group Granularity ($M_N$):** Too few groups → generic recommendations; too many groups → cold-start problem for group profiles and increased DB latency
- **Update Frequency:** Frequent updates capture trends but risk instability in the offline database

### Failure signatures:
- **Novelty Collapse:** NCP@K drops significantly after 20-30 rounds of fine-tuning (indicates α for KL loss is too low)
- **Relevance Drift:** C-H@K drops while NCP stays stable (indicates Relevance LLM is rejecting valid novel candidates)

### First 3 experiments:
1. **Static Group Validation:** Run RQ-VAE on training set and measure intra-group vs. inter-group cosine similarity to ensure clusters have semantic meaning before connecting LLMs
2. **Ablation on Loop:** Run system with "One-time Alignment" vs. "Periodic Optimization" to replicate quality lift reported in Table 2
3. **KL Sensitivity:** Fine-tune Novelty LLM with α∈{0.1, 0.4, 0.6} to verify "stability window" for preventing forgetting

## Open Questions the Paper Calls Out
1. **Cold-start optimization:** How can the real-time performance of model updates be optimized specifically for cold-start users who lack sufficient interaction history for meaningful group assignment?
2. **Update frequency optimization:** What is the optimal frequency and batch size for Periodic Collaborative Optimization updates to balance adaptation speed against computational cost?
3. **Temporal drift robustness:** How robust is the RQ-VAE group clustering to temporal drift in user interests, and when should group assignments be recomputed?

## Limitations
- Proprietary dataset and model sizes limit direct reproducibility and generalizability
- Underspecified components (RQ-VAE codebook initialization, exact prompt templates, negative sampling strategy) could lead to overfitting or under-clustering
- Online results cannot be verified without access to actual recommendation system and deployment logs

## Confidence
- **High confidence:** The conceptual architecture (DSIE + Dual-LLM + PCO) is clearly specified and logically sound; KL regularization inclusion is well-justified
- **Medium confidence:** Offline results are internally consistent and ablation studies support claimed mechanisms, but proprietary dataset limits generalizability
- **Low confidence:** Online A/B test results (+1.203% GTV, +2.364% 7D-NIEP) are not reproducible without access to actual system

## Next Checks
1. **RQ-VAE Clustering Validity:** Re-run user clustering pipeline on Movielens-1M and measure intra-group vs. inter-group cosine similarity to confirm Group CSIDs are semantically coherent
2. **Closed-Loop Ablation:** Implement and compare "one-time alignment" vs. "periodic optimization" to verify reported quality lift (CoEA vs. CoEA w/o R-LLM)
3. **KL Sensitivity Sweep:** Fine-tune Novelty LLM with α∈{0.1, 0.4, 0.6} and track both NCP@K and C-H@K over 30 rounds to confirm stability window and diagnose catastrophic forgetting