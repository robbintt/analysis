---
ver: rpa2
title: 'Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without
  Explicit Reasoning Supervision'
arxiv_id: '2506.03723'
source_url: https://arxiv.org/abs/2506.03723
tags:
- confidence
- answer
- reasoning
- csft
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a simple supervised fine-tuning method (CSFT)
  that improves verbalized confidence calibration for chain-of-thought reasoning in
  LLMs. Instead of requiring complex RL or reasoning supervision, CSFT fine-tunes
  models to produce a verbalized confidence score derived from self-consistency across
  sampled answers.
---

# Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision

## Quick Facts
- arXiv ID: 2506.03723
- Source URL: https://arxiv.org/abs/2506.03723
- Reference count: 26
- CSFT reduces ECE by over 70% and improves accuracy by 2.7% on GSM8K

## Executive Summary
This paper introduces Confidence-Self-Fine-Tuning (CSFT), a simple supervised fine-tuning method that significantly improves verbalized confidence calibration in LLMs without requiring complex reinforcement learning or explicit reasoning supervision. CSFT works by training models to produce confidence scores derived from self-consistency across sampled answers, leading to emergent self-verification behavior where low-confidence responses become longer and more deliberative, while high-confidence ones remain concise. The method demonstrates substantial improvements in both calibration (reducing Expected Calibration Error by over 70%) and accuracy (improving by 2.7% on GSM8K), with strong transfer to unseen tasks like MATH-500 and ARC-Challenge.

## Method Summary
CSFT fine-tunes LLMs using synthetic data generated from existing reasoning datasets. The method generates multiple answer samples per question using temperature-based sampling, then creates a "verbalized confidence" score based on the fraction of samples that agree on the final answer. Models are fine-tuned to predict this confidence score alongside the final answer, with special handling for cases where no consensus exists among samples. The training data consists of triplets containing the question, reasoning steps, and the confidence score. This approach leverages the observation that confidence derived from answer consistency correlates with reasoning quality, enabling models to learn when to be concise versus when to engage in extended deliberation.

## Key Results
- Reduces Expected Calibration Error (ECE) by over 70% on GSM8K
- Improves accuracy by 2.7% on GSM8K test set
- Achieves 37% accuracy improvement on MATH-500 and 6.3% on ARC-Challenge through transfer learning

## Why This Works (Mechanism)
The mechanism behind CSFT's success lies in the correlation between answer consistency across samples and reasoning quality. When multiple sampled answers converge on the same solution, this consensus indicates robust reasoning paths and reliable intermediate steps. By training models to predict this consensus-derived confidence, they learn to associate high confidence with correct, well-reasoned answers and low confidence with uncertain or error-prone reasoning. This creates a feedback loop where the model learns to adjust its response length and depth based on confidence levels - extending reasoning when uncertain and being concise when confident. The emergent self-verification behavior arises because the model implicitly learns to evaluate its own reasoning quality through the confidence prediction task.

## Foundational Learning

**Self-consistency sampling**: Multiple sampling of answers to measure agreement and derive confidence scores. Needed to create reliable confidence labels without ground truth reasoning supervision. Quick check: Compute agreement rates across samples for benchmark datasets.

**Expected Calibration Error (ECE)**: Metric measuring the discrepancy between predicted confidence and actual accuracy. Needed to quantify calibration quality objectively. Quick check: Calculate ECE before and after CSFT on held-out data.

**Verbalized confidence**: Explicit representation of uncertainty within the model's output. Needed to enable adaptive reasoning behavior based on confidence levels. Quick check: Verify confidence scores correlate with accuracy across confidence bins.

## Architecture Onboarding

**Component map**: Question -> Sampling Engine -> Answer Consensus -> Confidence Score -> Fine-tuning Target -> CSFT Model -> Calibrated Response

**Critical path**: The model must accurately predict confidence scores that correlate with actual reasoning quality, as this determines whether self-verification behavior emerges. The sampling and consensus mechanism is critical for generating reliable training signals.

**Design tradeoffs**: CSFT trades computational overhead during training (multiple sampling) for improved calibration and emergent reasoning capabilities. The method requires more training data and compute but eliminates the need for complex RL or explicit reasoning supervision.

**Failure signatures**: Poor calibration when confidence scores don't correlate with accuracy, failure to transfer to new domains, overfitting to GSM8K-style problems, and breakdown under single-sample inference conditions.

**First experiments**:
1. Measure ECE and accuracy on GSM8K before and after CSFT
2. Test emergent behavior by analyzing response length vs confidence correlation
3. Evaluate transfer performance on MATH-500 and ARC-Challenge

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability to domains requiring different reasoning patterns (scientific reasoning, multi-step logical deduction) remains uncertain
- Potential overfitting to self-consistency sampling methodology, with calibration possibly optimal only for multi-sample inference
- Limited statistical rigor in transfer learning results, lacking multiple random seeds and confidence intervals for performance metrics

## Confidence

**High confidence**: CSFT improves verbalized confidence calibration and reduces ECE on GSM8K
**Medium confidence**: Emergent self-verification behavior is demonstrated but needs validation across more diverse reasoning tasks and model architectures
**Medium confidence**: Transfer learning results show improvements but require more rigorous statistical validation and investigation of failure modes

## Next Checks

1. **Cross-domain generalization test**: Evaluate CSFT-trained models on scientific reasoning datasets (ARC-Easy, OpenBookQA) and logical reasoning tasks (LogiQA, ReClor) to assess whether emergent self-verification transfers to different reasoning paradigms.

2. **Single-sample inference analysis**: Systematically compare CSFT-trained models' calibration and accuracy under single-sample versus multi-sample inference conditions to determine robustness to sampling constraints.

3. **Adversarial confidence manipulation**: Design test cases where correct answers should yield low confidence (deceptive intermediate steps) and incorrect answers should yield high confidence to stress-test genuine uncertainty awareness.