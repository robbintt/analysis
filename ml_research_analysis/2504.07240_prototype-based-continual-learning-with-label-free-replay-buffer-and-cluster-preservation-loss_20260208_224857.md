---
ver: rpa2
title: Prototype-Based Continual Learning with Label-free Replay Buffer and Cluster
  Preservation Loss
arxiv_id: '2504.07240'
source_url: https://arxiv.org/abs/2504.07240
tags:
- learning
- loss
- tasks
- cluster
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses catastrophic forgetting in continual learning
  by proposing a label-free replay-based framework that maintains cluster structures
  in the latent space across tasks. The method introduces three key innovations: a
  cluster preservation loss using MMD to minimize distributional shifts, push-away/pull-toward
  mechanisms for class-incremental and domain-incremental scenarios respectively,
  and a replay buffer that stores prototypes without labels.'
---

# Prototype-Based Continual Learning with Label-free Replay Buffer and Cluster Preservation Loss

## Quick Facts
- arXiv ID: 2504.07240
- Source URL: https://arxiv.org/abs/2504.07240
- Reference count: 38
- One-line primary result: Label-free prototype-based replay method outperforms state-of-the-art continual learning methods on both class-incremental and domain-incremental benchmarks

## Executive Summary
This paper addresses catastrophic forgetting in continual learning by proposing a label-free replay-based framework that maintains cluster structures in the latent space across tasks. The method introduces three key innovations: a cluster preservation loss using MMD to minimize distributional shifts, push-away/pull-toward mechanisms for class-incremental and domain-incremental scenarios respectively, and a replay buffer that stores prototypes without labels. Experiments on multiple benchmarks (SplitCIFAR100, SplitImageNet32, SplitTinyImageNet, SplitCaltech256 for CI; R-MNIST and CORe50 for DI) demonstrate that the proposed iSL-LRCP method outperforms state-of-the-art continual learning methods including ER-AML, iCaRL, and PRD. Notably, it even surpasses offline learning in some cases. The unsupervised variant iUL-LRCP also shows competitive performance, outperforming certain supervised baselines.

## Method Summary
The proposed iSL-LRCP framework maintains class prototypes and support samples in a label-free buffer during continual learning. Using frozen DINOv2 features, it projects to a 512-dim latent space and trains with three losses: supervised contrastive loss (L_sc), cluster preservation loss (L_preserve using MMD), and either push-away (L_push for CI) or pull-toward (L_pull for DI) regularization. After each task, K-means clustering identifies prototypes (nearest to cluster centers) and support samples (selected via σ-band sampling along variance-selected dimensions). Nearest prototype classification replaces parametric classifiers. The unsupervised variant iUL-LRCP uses MiniBatch K-means for pseudo-labels.

## Key Results
- Outperforms state-of-the-art methods including ER-AML, iCaRL, and PRD on multiple benchmarks
- Achieves 83.29% average accuracy on SplitCIFAR100 (vs 20.70% with L_sc only, 75.85% with ER-AML)
- Outperforms offline learning on CORe50 (97.03% vs 95.70%)
- Unsupervised iUL-LRCP variant achieves 96.02% on R-MNIST (vs 90.88% for UFL)
- t-SNE visualizations confirm effective preservation of cluster structures

## Why This Works (Mechanism)

### Mechanism 1: Cluster Preservation Loss
Minimizing distributional shifts in the latent space preserves previously learned cluster structures and mitigates catastrophic forgetting. The Cluster Preservation Loss (L_preserve) uses Maximum Mean Discrepancy (MMD) to measure the distance between latent representations of prototypes and support samples before (Z_old) and after (Z_new) training on new task data. By minimizing MMD²(Z_old, Z_new), the model constrains how much the latent geometry can shift when learning new information.

### Mechanism 2: Push-Away and Pull-Toward Regularization
Explicitly managing the relationship between new samples and old prototypes improves adaptation while reducing interference. For Class-Incremental learning, L_push penalizes similarity between new samples and old prototypes, with stronger penalties for loosely packed clusters (inverse weighting by 1-σ_j). For Domain-Incremental learning, L_pull aligns same-class samples with first-task prototypes via cosine similarity.

### Mechanism 3: Label-Free Prototype and Support Sample Storage
Storing geometric representatives (prototypes at cluster centers, support samples at σ-bands) without labels is sufficient for replay-based retention. K-means clustering (K = number of classes) identifies prototypes as samples nearest to cluster centers. Support samples capture cluster spread via σ-band selection along informative dimensions. Nearest prototype classification replaces softmax heads.

## Foundational Learning

- Concept: **Supervised Contrastive Learning**
  - Why needed here: The base loss L_sc aligns same-class samples and separates different-class samples in the latent space, creating the initial cluster structure that the preservation loss then protects.
  - Quick check question: Can you explain why contrastive loss requires a temperature parameter and how it affects the sharpness of the learned embedding distribution?

- Concept: **Maximum Mean Discrepancy (MMD)**
  - Why needed here: MMD is the core metric for L_preserve; it quantifies distributional distance between old and new latent representations without requiring explicit density estimation.
  - Quick check question: Given two sets of samples, how would you compute MMD using an RBF kernel, and why is it sensitive to moments of the distributions?

- Concept: **K-means Clustering and Prototype Classification**
  - Why needed here: The method relies on K-means to identify cluster centers and uses nearest-prototype assignment for inference, eliminating the need for a parametric classifier.
  - Quick check question: Why does K-means assume spherical clusters, and how might this affect performance on elongated or overlapping class distributions?

## Architecture Onboarding

- Component map:
  1. Feature extractor: Pre-trained DINOv2 ViT-L/14 (frozen, outputs 1024-dim vectors)
  2. Projection head: Trainable linear layer (1024 → 512 dimensions)
  3. Buffer: Stores prototypes (1 per class) + support samples (at σ-bands) per class, without labels
  4. Loss combiner: Aggregates L_sc + L_push (or L_pull) + L_preserve with configurable weights

- Critical path:
  1. Extract DINOv2 features → project to 512-dim latent space
  2. Train on current task batch using combined loss (buffer samples included in each batch)
  3. After task completion, run K-means per class → identify prototypes and σ-band support samples
  4. Store prototypes + support samples in label-free buffer
  5. At inference, classify via nearest prototype distance

- Design tradeoffs:
  - Buffer size (M=31 samples/class) vs. memory constraints: larger M captures more distribution spread but increases replay overhead
  - λ_preserve (0.5 for CI, 0.05 for DI): higher values reduce forgetting but may limit plasticity on new tasks
  - Unsupervised variant (iUL-LRCP) uses pseudo-labels from MiniBatch K-means; trades label accuracy for annotation-free operation

- Failure signatures:
  - Sudden accuracy drop on earlier tasks after training new task → L_preserve weight too low or buffer not being sampled
  - New classes overlapping with old in t-SNE → L_push weight too low or temperature τ_push misconfigured
  - Poor DI performance → L_pull not properly matching samples to first-task prototypes (check δ indicator function)

- First 3 experiments:
  1. Ablation on SplitCIFAR100: Train with L_sc only, then L_sc + L_preserve, then full loss. Confirm that adding L_preserve prevents the accuracy collapse shown in Table 5 (20.70% → 83.29%).
  2. Buffer size sensitivity: Vary M ∈ {5, 15, 31, 50} on SplitTinyImageNet. Monitor both accuracy and BWT to find memory-efficiency frontier.
  3. DI vs CI configuration validation: Run on R-MNIST with both L_push and L_pull configurations. Verify that L_pull outperforms L_push for domain-shift scenarios (Table 4 shows 87.55% with proper DI configuration).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the pull-toward mechanism for domain-incremental learning be reformulated to eliminate dependence on first-task labels while maintaining comparable performance?
- Basis in paper: The authors state that L_pull "requires the labels of the first task's class prototypes. While this approach deviates from the label-free replay paradigm, it helped achieve slightly better results."
- Why unresolved: The current DI formulation partially breaks the core label-free principle, creating inconsistency between CI and DI scenarios.
- What evidence would resolve it: A modified L_pull using unsupervised prototype alignment (e.g., Hungarian matching or optimal transport) achieving comparable DI performance without first-task labels.

### Open Question 2
- Question: How does the framework perform when features are not pre-extracted from a foundation model but learned end-to-end during continual learning?
- Basis in paper: The method relies entirely on frozen DINOv2 ViT-L/14 features, only training a linear projection layer. The 1024-dimensional representations may already contain well-structured semantic information that facilitates clustering.
- Why unresolved: The contribution of the proposed losses versus the quality of pre-extracted features remains confounded.
- What evidence would resolve it: Experiments training the feature extractor incrementally from scratch, or using weaker backbone features (e.g., ResNet-18), showing whether cluster preservation remains effective.

### Open Question 3
- Question: How robust is the K-means clustering with N=K assumption to class imbalance, overlapping distributions, or unknown number of classes per task?
- Basis in paper: The paper assumes N=K (clusters equal classes) and does not address scenarios where class distributions may be multi-modal or where the number of classes per task is unknown.
- Why unresolved: Real-world streaming data may violate these assumptions, and the method's sensitivity to cluster count misspecification is unexplored.
- What evidence would resolve it: Ablation studies varying N relative to true K, and experiments on datasets with imbalanced or hierarchically structured classes.

### Open Question 4
- Question: What are the theoretical conditions under which cluster preservation loss alone (without push-away/pull-toward) is sufficient for catastrophic forgetting mitigation?
- Basis in paper: The ablation study shows L_preserve alone achieves 97.03% on CORe50, surpassing offline baseline, but drops to 20.70% on sC100 without L_push.
- Why unresolved: The dramatic dataset-dependent gap suggests underlying factors (cluster separability, task similarity, feature space geometry) determine when cluster preservation is adequate.
- What evidence would resolve it: Systematic analysis correlating dataset properties (intrinsic dimensionality, inter-class margins, feature space entropy) with the necessity of auxiliary losses.

## Limitations
- Performance relies heavily on quality of pre-extracted DINOv2 features; may degrade with weaker backbones or non-image data
- MMD implementation details (kernel bandwidth, unbiased vs biased estimator) are not specified, which could affect the preservation loss
- Buffer size (M=31 per class) is fixed without sensitivity analysis; optimal M may vary by dataset
- Pseudo-code for the mRMR-like variance-based dimension selection and σ-band sampling is not provided

## Confidence
- High confidence in the core mechanism of cluster preservation via MMD (well-supported by ablation and t-SNE visualizations)
- Medium confidence in the push-away/pull-toward regularization (effective but sensitive to hyperparameter tuning)
- Medium confidence in label-free storage effectiveness (outperforms labeled baselines but relies on strong feature extractor)
- Low confidence in DI performance claims (limited comparison to specialized domain adaptation methods)

## Next Checks
1. Implement and test the exact mRMR-like variance-based dimension selection algorithm for support sample extraction
2. Conduct buffer size sensitivity analysis across datasets to identify memory-efficiency tradeoffs
3. Compare iSL-LRCP against specialized domain adaptation methods on DI benchmarks to validate DI claims