---
ver: rpa2
title: Evaluating The Impact of Stimulus Quality in Investigations of LLM Language
  Performance
arxiv_id: '2510.06018'
source_url: https://arxiv.org/abs/2510.06018
tags:
- stimuli
- filler
- gpt-2
- about
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper re-evaluates Large Language Models' (LLMs) performance
  on parasitic gap (PG) constructions, building on prior work by Lan et al. (2024).
---

# Evaluating The Impact of Stimulus Quality in Investigations of LLM Language Performance

## Quick Facts
- **arXiv ID:** 2510.06018
- **Source URL:** https://arxiv.org/abs/2510.06018
- **Reference count:** 15
- **Primary result:** GPT-2's accuracy on parasitic gap constructions increases from ~6% to ~60% when evaluated on refined stimuli that eliminate lexical ambiguities and structural complexity.

## Executive Summary
This paper demonstrates that stimulus quality significantly impacts surprisal-based evaluations of LLM syntactic competency. By identifying and addressing confounds in prior work (Lan et al., 2024) - specifically lexical ambiguities like "John's" and structural complexity in noun phrases - the authors generate a refined dataset using Gemini 2.5 Pro Preview. When evaluated on these controlled stimuli, GPT-2 shows a dramatic improvement in performance on parasitic gap constructions, suggesting that conclusions about LLM failures to acquire linguistic phenomena may be premature if based on stimuli with significant potential confounds.

## Method Summary
The authors evaluate GPT-2's performance on parasitic gap constructions using surprisal-based metrics. They compare three datasets: the original Lan et al. (2024) stimuli (N=8,064), a filtered subset excluding "NAME's VERBing to" patterns (N=5,760), and a new refined dataset generated with Gemini 2.5 Pro Preview using linguistically-informed templates (N=10). Surprisal is calculated as S(wi|C) = -log₂P(wi|C), with Δ = S(-Gap Continuation) - S(+Gap Continuation) and DiD = (Δ+filler - Δ-filler) > 0 as the primary metrics. The refined stimuli eliminate lexical ambiguities and use simpler noun phrase structures to better isolate the target syntactic variable.

## Key Results
- GPT-2's accuracy on Δ+filler > 0 criterion improves from ~6% to ~60% when evaluated on refined vs. original stimuli
- DiD criterion accuracy increases from ~69% to ~80% with refined stimuli
- The refined dataset successfully eliminates the "John's" ambiguity while maintaining grammaticality and pragmatic plausibility
- Performance on filtered dataset (N=5,760) falls between original and refined, confirming the importance of stimulus quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Removing local lexical ambiguity isolates the target syntactic variable, improving model accuracy on surprisal-based evaluations.
- **Mechanism:** Surprisal is an aggregate measure of predictability. If a stimulus contains local lexical confounds (e.g., *John's* as "John is" vs. possessive), the model allocates predictive probability to resolving this ambiguity rather than processing the global filler-gap dependency. Eliminating this noise allows the $\Delta$ to reflect the structural phenomenon.
- **Core assumption:** The model's prior exposure to ambiguous contractions during pre-training significantly skews its probability estimates for the critical region.
- **Evidence anchors:** [abstract] "...characteristics of the stimuli... including lexical ambiguities... may confound model performance." [section 2.1] Details the "John's" ambiguity where the string fails to form the intended island structure without forced bracketing.

### Mechanism 2
- **Claim:** Reducing structural complexity in the Noun Phrase (NP) host aligns test items closer to the model's training distribution ("canonical" usage).
- **Mechanism:** The original stimuli used complex NP structures (e.g., *Bob's decision to dance*) that add processing depth. The refined stimuli use simpler structures (*the story about*). LLMs perform better on high-probability, canonical syntactic patterns. By reducing the "depth" burden, the model can better demonstrate sensitivity to the specific Parasitic Gap (PG) licensing.
- **Core assumption:** The original "failure" was due to compounding processing load (complexity + rarity) rather than a lack of representation for the specific PG rule.
- **Evidence anchors:** [section 2.2] Notes that the internal complexity of NP_COMPLEX structures "introduces a degree of structural depth that goes beyond the more canonical adjunct PG constructions."

### Mechanism 3
- **Claim:** LLM-in-the-loop generation with strict linguistic templates produces higher-validity test items than pure Context-Free Grammars (CFG).
- **Mechanism:** Pure CFGs can generate grammatically valid but pragmatically unnatural or lexically ambiguous sentences. Using a SOTA LLM (Gemini) guided by constraints leverages the model's "fluency" to filter out unnatural phrasing while the template enforces the structural logic required for the experiment.
- **Core assumption:** The generator model (Gemini) has sufficient linguistic competence to adhere to the constraints and does not introduce new, subtle biases.
- **Evidence anchors:** [abstract] "...generating a new, refined dataset using a state-of-the-art (SOTA) generative LLM... guided by linguistically-informed templates..."

## Foundational Learning

- **Concept: Surprisal Theory ($-\log_2 P$)**
  - **Why needed here:** This is the dependent variable. You cannot interpret the results without understanding that lower surprisal = higher predictability, and that the $\Delta$ metric compares the "shock" of a grammatical vs. ungrammatical continuation.
  - **Quick check question:** If a model assigns a probability of 0.5 to a token, what is the surprisal in bits? (Answer: 1 bit).

- **Concept: Minimal Pairs & Factorial Design ($2 \times 2$)**
  - **Why needed here:** The paper uses a design manipulating $[±Filler]$ and $[±Gap]$. Understanding why we need a *difference* in differences (DiD) is crucial to isolating the PG effect from general sentence probability.
  - **Quick check question:** Why compare $+Filler$ sentences against $-Filler$ controls instead of just looking at the raw probability of the $+Gap$ sentence?

- **Concept: Syntactic Islands**
  - **Why needed here:** Parasitic Gaps are unique because they appear inside "islands" (structures from which extraction is usually banned). You must grasp that the model is being tested on its ability to recognize an *exception* to a movement constraint.
  - **Quick check question:** In the sentence "I know who the story about is likely to amuse," where is the island, and where is the gap?

## Architecture Onboarding

- **Component map:** Text -> Tokenization -> Surprisal Calculator -> Aggregator
- **Critical path:** The transition from **Text -> Tokenization**. The paper highlights that BPE (Byte Pair Encoding) tokenization splits words. If your surprisal extraction doesn't correctly sum the surprisal of *all* sub-word tokens in the critical region, your metrics will be wrong.
- **Design tradeoffs:**
  - **GPT-2 vs. SOTA:** Using GPT-2 allows comparison to prior work (Lan et al.) and tests "Poverty of Stimulus" (less training data), but risks false negatives if the model is simply too weak.
  - **Template Constraints:** Strict templates ensure structural validity but reduce lexical diversity; this paper relaxes this by using an LLM generator to balance diversity with structure.
- **Failure signatures:**
  - **Tokenization misalignment:** Calculating surprisal for the word "soon" but missing the initial space token or splitting it into "s" + "oon" incorrectly.
  - **The "John's" Trap:** Failing to realize that a contraction token might include the space or be split differently depending on the tokenizer version, leading to inaccurate surprisal attribution.
- **First 3 experiments:**
  1. **Metric Replication:** Run the provided code on the *original* Lan et al. dataset to verify you can reproduce the ~6% accuracy baseline before attempting to run new data.
  2. **Ablation on Ambiguity:** Take the "Filtered" dataset (N=5,760) and manually inspect 10 random samples to confirm the regex/filtering logic successfully removed the "NAME's VERBing to" pattern.
  3. **Scale Validation:** Generate a larger refined dataset (N=50) using the prompt in Appendix A and run GPT-2 evaluation to see if the 60% accuracy holds with a larger sample size (checking for variance).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the improvement in GPT-2 performance generalize to a larger dataset size and to adjunct parasitic gap constructions?
- **Basis in paper:** [explicit] The authors explicitly state the need for "Expanding the refined dataset to include more items and other PG structures (e.g., adjunct PGs)" in Section 4.3.
- **Why unresolved:** The current study relies on a "preliminary" set of only 10 items focused solely on subject PGs, which limits the statistical power and structural diversity of the findings.
- **What evidence would resolve it:** Replication of the experiment with a dataset of several hundred items covering both subject and adjunct parasitic gap structures.

### Open Question 2
- **Question:** Do modern, larger-scale LLMs exhibit the same sensitivity to stimulus confounds as GPT-2, or do they handle ambiguous/complex stimuli more robustly?
- **Basis in paper:** [explicit] Section 4.3 lists "Testing a wider range of LLMs, including more recent architectures" as a necessary future direction.
- **Why unresolved:** The study restricted its evaluation to GPT-2 to align with previous APS literature; it is unknown if the "failure" on poor-quality stimuli persists in state-of-the-art models.
- **What evidence would resolve it:** A comparative analysis of model performance (e.g., Llama 3, GPT-4) on both the original Lan et al. stimuli and the refined dataset.

### Open Question 3
- **Question:** Does the use of a generative LLM (Gemini) to create test stimuli introduce systematic biases or circularity into the evaluation of syntactic competence?
- **Basis in paper:** [inferred] The methodology relies on Gemini 2.5 Pro Preview to generate "refined" stimuli, but the paper does not verify if Gemini's own syntactic biases influence the generated sentences in ways that artificially favor the evaluated model.
- **Why unresolved:** The paper establishes that stimulus quality matters but does not disentangle whether the improvement comes from removing confounds or from the generative model's specific structural preferences.
- **What evidence would resolve it:** A comparison of GPT-2 performance on stimuli generated by multiple independent methods (e.g., human linguists vs. different LLMs) to control for generator bias.

## Limitations
- The refined dataset contains only N=10 items, limiting statistical power and generalizability of findings
- Results are based exclusively on GPT-2, leaving open whether other LLM architectures show similar stimulus sensitivity
- The study focuses only on subject parasitic gaps, not testing adjunct parasitic gap constructions

## Confidence
- **High confidence:** The methodological framework for identifying stimulus confounds (lexical ambiguity, structural complexity) is well-grounded in syntactic theory and supported by the performance differential between filtered and refined datasets
- **Medium confidence:** The specific improvement from ~6% to ~60% accuracy on refined stimuli is robust but based on a small N=10 sample, requiring replication with larger datasets
- **Medium confidence:** The claim that stimulus quality significantly influences surprisal-based evaluations is well-supported, though the precise magnitude of this effect across different linguistic phenomena remains to be tested

## Next Checks
1. **Scale validation:** Generate and evaluate a larger refined dataset (N≥50 items) to confirm whether the 60% accuracy baseline holds with increased sample size and to establish confidence intervals around performance estimates
2. **Cross-model generalization:** Repeat the evaluation using a more recent LLM architecture (e.g., GPT-3.5 or GPT-4) to determine if stimulus quality effects persist across model families and training regimes
3. **Phenomenon replication:** Apply the refined stimulus generation methodology to other syntactic phenomena (e.g., island constraints, long-distance dependencies) to test whether stimulus quality systematically affects surprisal-based assessments across different grammatical domains