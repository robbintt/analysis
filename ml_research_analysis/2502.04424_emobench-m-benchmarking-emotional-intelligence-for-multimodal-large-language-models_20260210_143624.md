---
ver: rpa2
title: 'EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language
  Models'
arxiv_id: '2502.04424'
source_url: https://arxiv.org/abs/2502.04424
tags:
- uni00000013
- uni00000011
- uni00000048
- uni00000044
- uni00000055
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces EmoBench-M, a comprehensive multimodal benchmark
  designed to evaluate emotional intelligence (EI) capabilities of multimodal large
  language models (MLLMs). The benchmark includes 13 scenarios across three dimensions:
  foundational emotion recognition, conversational emotion understanding, and socially
  complex emotion analysis.'
---

# EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models

## Quick Facts
- **arXiv ID**: 2502.04424
- **Source URL**: https://arxiv.org/abs/2502.04424
- **Reference count**: 40
- **Primary result**: Evaluates 16 MLLMs across 13 multimodal emotion understanding scenarios; reveals significant performance gaps vs. humans.

## Executive Summary
This paper introduces EmoBench-M, a comprehensive multimodal benchmark designed to evaluate emotional intelligence (EI) capabilities of multimodal large language models (MLLMs). The benchmark includes 13 scenarios across three dimensions: foundational emotion recognition, conversational emotion understanding, and socially complex emotion analysis. The dataset was filtered for quality, ensuring high inter-annotator agreement. Evaluations across various open-source and closed-source MLLMs reveal a significant performance gap compared to humans, highlighting the need for further advancement in MLLMs' EI capabilities.

## Method Summary
EmoBench-M evaluates MLLMs on 13 multimodal emotion understanding scenarios using zero-shot inference with standardized prompts. The benchmark combines 12 public datasets totaling 4,100 samples across discriminative tasks (emotion recognition, sentiment analysis) and generative tasks (humor reasoning, emotional intention captioning). Models are evaluated using accuracy, macro-F1, and weighted-F1 metrics, with generative outputs assessed by an LLM-as-a-judge framework (GPT-4o) on logical coherence and multimodal grounding.

## Key Results
- Proprietary models (GPT-4o, Gemini-2.0-Flash-Thinking) outperform open-source MLLMs by 18-25% absolute accuracy across scenarios
- MLLMs show significant performance gaps compared to human benchmarks, particularly on subtle emotions like disgust and fear
- Most models exhibit strong bias toward 'neutral' predictions, especially for complex emotions requiring fine-grained discrimination

## Why This Works (Mechanism)

### Mechanism 1: Structured Multi-Task Discriminative Evaluation
The benchmark forces MLLMs to demonstrate robust, generalizable emotional understanding across diverse real-world contexts through 13 distinct evaluation scenarios. Each scenario is transformed into standardized question-answering format using carefully designed prompts, with performance measured via standard classification metrics.

### Mechanism 2: Generative Emotion Explanation as a Probe for Deep Reasoning
Beyond simple classification, generative tasks (humor reasoning, emotional intention captioning) evaluate the model's ability to synthesize multimodal evidence into coherent, causal explanations. These are graded using an LLM-as-a-Judge framework on logical judgment and multimodal content association.

### Mechanism 3: Unified Framework for Cross-Model Diagnosis
By presenting all scenarios through a consistent API, EmoBench-M enables direct comparison of diverse MLLMs, revealing systematic strengths, weaknesses, and failure modes (e.g., modality bias, cultural blind spots) through per-scenario and per-emotion analysis.

## Foundational Learning

**Concept: Multimodal Fusion & Alignment**
- Why needed: EmoBench-M tasks require understanding how cues from video, audio, and text interact to create meaning
- Quick check: Can you name one scenario where ignoring audio would likely lead to incorrect classification? (e.g., distinguishing sarcasm from sincerity often requires vocal tone)

**Concept: Emotion Taxonomies & Label Hierarchies**
- Why needed: Different tasks use different label sets (e.g., 6 basic emotions, 3 sentiment classes). Models must map internal representations to external taxonomies flexibly
- Quick check: How might performance differ between 3-class sentiment vs 6-basic-emotion tasks? What does this reveal?

**Concept: Evaluation Metrics Beyond Accuracy**
- Why needed: With class imbalance, Accuracy can be misleading. Macro-F1 gives equal weight to each class, highlighting performance on rare but important emotions
- Quick check: If a model achieves 90% accuracy but has a Macro-F1 of 50%, what is a likely problem?

## Architecture Onboarding

**Component map:** Dataset Layer -> Prompt Layer -> Execution Layer -> Parsing & Evaluation Layer -> Analysis Layer

**Critical path:** The path from Prompt Design to Robust Parsing is most critical. If prompts are ambiguous or parsing fails on valid model outputs, the entire evaluation produces garbage data.

**Design tradeoffs:**
- Comprehensiveness vs. Evaluation Cost: Adding more scenarios increases coverage but also computational cost
- Standardization vs. Model Specificity: Forcing identical prompts ensures fairness but might disadvantage models optimized for different prompting styles
- Automated Judging vs. Human Annotation: Using GPT-4o as a judge is scalable but may not perfectly align with human judgment

**Failure signatures:**
- Low Macro-F1 on Balanced Tasks: Indicates model is biased towards majority classes (often 'neutral')
- High Text-Only Performance, Low Audio/Video Performance: Suggests over-reliance on linguistic modality
- Incoherent or Template-Like Explanations: In generative tasks, this signals lack of genuine reasoning

**First 3 experiments:**
1. Baseline Sweep: Run full benchmark on 2-3 representative MLLMs to establish performance range and identify trends
2. Modality Ablation: For SPER, evaluate models with audio-only, text-only, and full audio-video inputs to quantify each modality's contribution
3. Error Analysis on Worst-Performing Scenario: Deep-dive into the model with lowest overall score's worst-performing scenario using confusion matrices

## Open Questions the Paper Calls Out

**Open Question 1:** To what extent is the strong prediction bias towards 'neutral' caused by models defaulting to 'neutral' when textual and acoustic modalities are in conflict, rather than general lack of sensitivity? The confusion matrices show high false negative rates for emotions like 'fear' or 'disgust', but don't explain the underlying mechanism.

**Open Question 2:** How does explicit inclusion of textual subtitles in prompts alter the model's reliance on acoustic features compared to tasks where no text is provided? The prompts differ explicitly between scenarios, but the paper doesn't compare how this structural difference shifts modality attention.

**Open Question 3:** Does high performance in "Logical Judgment Dimension" necessarily correlate with high scores in "Multimodal Content Association", or can a model generate logically coherent explanations that fail to ground in specific visual/acoustic evidence? The evaluation criteria set these as separate dimensions, but the paper doesn't present correlation data.

## Limitations
- Evaluation relies heavily on zero-shot inference, which may not reflect fine-tuned model capabilities
- Use of GPT-4o as an LLM judge introduces potential bias that may not perfectly align with human evaluation
- Performance gap between MLLMs and humans may be partially attributed to differences in task framing and evaluation protocols

## Confidence

**High Confidence:** The benchmark construction methodology, including dataset selection, prompt standardization, and metric definitions, is well-documented and reproducible

**Medium Confidence:** The comparative results across MLLMs are reliable, but absolute performance levels may be influenced by evaluation setup choices

**Low Confidence:** The human baseline comparison, particularly for generative tasks, lacks detailed methodology for establishing human performance standards

## Next Checks

1. **Human Evaluation Replication:** Conduct independent human evaluations on the SMILE dataset using the same rubric to validate the LLM judge's consistency and identify potential biases

2. **Cross-Evaluation Protocol:** Test the same MLLMs using alternative evaluation frameworks (e.g., different prompting strategies or judge models) to assess result robustness

3. **Fine-Tuning Impact Study:** Evaluate whether fine-tuning select MLLMs on emotion-related datasets significantly improves their EmoBench-M scores compared to zero-shot performance