---
ver: rpa2
title: '$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion'
arxiv_id: '2505.16425'
source_url: https://arxiv.org/abs/2505.16425
tags:
- image
- arxiv
- diffusion
- text
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating instructional
  illustrations from procedural text, a key problem in natural language processing
  where purely textual instructions often fail to convey complex physical actions
  and spatial relationships. The authors propose a language-conditioned diffusion
  framework that decomposes instructional content into goals and sequential steps,
  then conditions visual generation on these linguistic elements.
---

# $I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion

## Quick Facts
- arXiv ID: 2505.16425
- Source URL: https://arxiv.org/abs/2505.16425
- Authors: Jing Bi; Pinxin Liu; Ali Vosoughi; Jiarui Wu; Jinxi He; Chenliang Xu
- Reference count: 19
- Key outcome: Proposes a language-conditioned diffusion framework that significantly outperforms baselines in generating instructional illustrations from procedural text by maintaining visual coherence across sequential steps.

## Executive Summary
This paper introduces $I^2G$, a diffusion-based framework for generating instructional illustrations from procedural text. The core challenge is converting step-by-step instructions into a sequence of coherent images that accurately represent both the physical actions and spatial relationships described. The authors propose a novel approach that decomposes instructional content into goals and sequential steps, conditions visual generation on these linguistic elements, and introduces three key innovations: constituency parser-based text encoding for lengthy instructions, a pairwise discourse coherence model for maintaining consistency across instruction sequences, and a novel evaluation protocol specifically designed for procedural language-to-image alignment.

## Method Summary
$I^2G$ employs a pairwise factorization approach where the joint distribution of image sequences is approximated as a product of pairwise distributions. The system uses Stable Diffusion XL as the backbone, modified with custom attention masks for pairwise latent fusion. Text encoding leverages constituency parsing to segment long step descriptions into coherent clauses, which are separately encoded and concatenated. A frozen LLM samples (goal, step) pairs from a prior distribution, and the diffusion model is fine-tuned using gradient-based feedback through sampling steps, optimizing for alignment between generated images and instructional text. The pairwise coherence mechanism ensures visual consistency across adjacent steps through localized cross-step feature sharing in the latent space.

## Key Results
- Significantly outperforms existing baselines (Stable Diffusion variants) on three instructional datasets (HTStep, CaptainCook4D, WikiAll) in generating visuals that accurately reflect linguistic content and sequential nature
- Achieves better text-image alignment as measured by KL divergence and Chi-square tests, with lower scores indicating superior alignment
- Demonstrates improved procedural correctness and commonsense plausibility compared to baselines through human evaluation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The constituency parser-based text encoding preserves semantic completeness for lengthy instructional texts that exceed standard encoder capacities.
- **Mechanism:** Long step descriptions are segmented into coherent clauses (e.g., verb phrases) via constituency parsing. Each clause is encoded separately by OpenCLIP-ViT/G (for steps) or CLIP-ViT/L (for goals), then concatenated, preventing truncation loss of critical procedural details.
- **Core assumption:** The parser correctly identifies clause boundaries that align with instructionally meaningful semantic units.
- **Evidence anchors:**
  - [abstract]: "a constituency parser-based text encoding mechanism that preserves semantic completeness even with lengthy instructions"
  - [section 4.3]: "To retain critical information, we employ a constituency parser to segment each step into coherent clauses (e.g., verb phrases). Each clause is then separately encoded and concatenated."
  - [corpus]: No direct corpus comparison available; mechanism is novel to this paper.
- **Break condition:** Parser fails to correctly segment malformed or ambiguous instructions (e.g., compound sentences with multiple actions), leading to mis-encoding.

### Mechanism 2
- **Claim:** The pairwise discourse coherence model maintains visual consistency across sequential instructions by enabling localized cross-step feature sharing.
- **Mechanism:** Adjacent image pairs are encoded into latent tensors and concatenated. A custom adjacency mask in the attention mechanism restricts each latent's attention primarily to its own slice and the adjacent slice, allowing visual continuity (e.g., recurring objects) without computational explosion.
- **Core assumption:** Procedural tasks exhibit visual continuity where objects/settings persist across adjacent steps.
- **Evidence anchors:**
  - [abstract]: "a pairwise discourse coherence model for maintaining consistency across instruction sequences"
  - [section 4.1]: "We therefore introduce a pairwise factorization... allowing localized interactions across adjacent steps without incurring a combinatorial explosion."
  - [section 4.2]: "Our modified attention ensures that the generated image for step j inherits relevant context from step i... yielding smoother transitions."
  - [corpus]: CookAnything addresses multi-step consistency but uses a different approach; no direct evidence for this specific mask-based attention mechanism.
- **Break condition:** Non-adjacent but thematically related steps (e.g., step 1 and step 7) fail to share visual context, potentially breaking long-range consistency.

### Mechanism 3
- **Claim:** The reward-based feedback fine-tuning improves image-text alignment beyond standard diffusion training.
- **Mechanism:** A frozen LLM samples (goal, step) text pairs. Generated images are scored by an image-text alignment model acting as a reward. Diffusion model parameters are updated via gradient-based feedback through sampling steps to maximize alignment reward.
- **Core assumption:** The reward model accurately captures human-perceived alignment between procedural text and images.
- **Evidence anchors:**
  - [abstract]: "experiments... demonstrate that our method significantly outperforms existing baselines"
  - [section 4.4]: "We freeze the LLM and update only diffusion parameters ϕ via gradient-based feedback through the sampling steps, improving fidelity without retraining the language backbone."
  - [corpus]: Preference alignment in diffusion is established (DDPO, DPOK), but application to procedural instructional text is novel.
- **Break condition:** Reward model provides noisy or misaligned feedback (e.g., high scores for object matches but incorrect actions), leading to reward hacking.

## Foundational Learning

- **Concept: Latent Diffusion Models (LDMs)**
  - **Why needed here:** The core generative engine is Stable Diffusion XL, an LDM that operates in a compressed latent space for efficiency.
  - **Quick check question:** Can you explain how a VAE compresses an image into latents and how the denoising process works in this latent space?

- **Concept: Classifier-Free Guidance (CFG)**
  - **Why needed here:** CFG is used to balance fidelity and diversity during conditional sampling.
  - **Quick check question:** How does CFG mix conditional and unconditional score estimates to control the trade-off between sample fidelity and diversity?

- **Concept: Self-Attention and Masking in Transformers**
  - **Why needed here:** The cross-image consistency mechanism modifies standard self-attention with a custom mask to enforce step ordering.
  - **Quick check question:** How does a standard self-attention layer compute attention weights, and what effect would a binary mask have on which tokens can attend to each other?

## Architecture Onboarding

- **Component map:** Text Encoder (CLIP-ViT/L for goals, OpenCLIP-ViT/G for steps with constituency parsing) -> LLM Sampler (frozen, samples (goal, step) pairs) -> Diffusion Backbone (SDXL UNet with custom adjacency mask) -> VAE (encodes/decodes image pairs to/from latent space) -> Reward Model (image-text alignment for feedback scoring)

- **Critical path:** Parse text → Encode clauses → Sample from LLM → Encode image pairs to latents → Apply adjacency mask in attention → Denoise conditioned on text → Score with reward model → Backpropagate reward to update diffusion backbone

- **Design tradeoffs:**
  - **Pairwise vs. Full Joint:** Pairwise is efficient but may miss long-range dependencies
  - **Frozen LLM vs. End-to-End:** Freezing simplifies training but prevents joint optimization
  - **Reward Model Choice:** Human eval is gold standard but expensive; automated models may miss nuances (Section 5.2 notes MLLM limitations)

- **Failure signatures:**
  - **Inconsistent Objects:** Same object changes appearance across steps
  - **Hallucinated Objects:** Objects present but in wrong context/action
  - **Truncation Artifacts:** Missing details from long instructions
  - **Reward Hacking:** High scores but misaligned images
  - **Step Confusion:** Image for step i reflects content from adjacent steps

- **First 3 experiments:**
  1. **Ablate Text Encoding:** Compare constituency parser-based encoding vs. truncation. Measure KL divergence/Chi2 and semantic completeness.
  2. **Ablate Pairwise Mask:** Run with standard self-attention vs. custom adjacency mask. Measure cross-image object consistency.
  3. **Vary Reward Model:** Fine-tune with different rewards (CLIPScore, BLIP2-caption-based, human). Measure correlation with human judgment.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can instructional image generation systems be improved to accurately depict abstract or implicit actions that lack clear visual manifestations?
- **Basis in paper:** [explicit] The conclusion states: "our approach struggles with abstract or implicit actions that do not manifest themselves as clear visual features."
- **Why unresolved:** Current diffusion models trained on visual data naturally favor concrete, visually explicit actions over abstract procedural concepts, and no architecture modifications have been proposed to address this.
- **What evidence would resolve it:** A systematic comparison on a curated subset of abstract vs. concrete procedural steps, with human evaluation of semantic correctness and new quantitative metrics capturing action abstractness.

### Open Question 2
- **Question:** How can cross-image consistency be maintained in instructional sequences that extend significantly beyond the 6–7 step range?
- **Basis in paper:** [explicit] The authors note: "uniformity across consecutive images can still be improved, especially for extended multi-step processes"; Figure 1 also references StackDiffusion's limitation of not generating more than 6 steps.
- **Why unresolved:** The proposed pairwise factorization may not propagate global context effectively over longer trajectories, leading to drift in object appearance or scene layout.
- **What evidence would resolve it:** Experiments scaling to procedures with 10+ steps, reporting consistency scores (e.g., object persistence, background stability) alongside the existing alignment metrics.

### Open Question 3
- **Question:** Can evaluation protocols be designed to assess action-level alignment rather than rewarding mere object-name overlap?
- **Basis in paper:** [explicit] The paper observes: "the MLLM tends to assign high scores when the object names in the text match, even if the actions described differ. This indicates potential areas for further improvement."
- **Why unresolved:** Current MLLM-based evaluators and CLIPScore capture coarse semantic overlap but fail to verify whether the depicted action matches the instruction.
- **What evidence would resolve it:** Introduction of a benchmark with paired images that share objects but differ in actions, demonstrating that a new metric correlates with human action-correctness judgments more strongly than existing scores.

### Open Question 4
- **Question:** Would integrating multi-turn LLM reasoning into the diffusion conditioning pipeline substantially improve handling of nuanced or temporally dependent instructions?
- **Basis in paper:** [explicit] The authors identify as future work: "integrating multi-turn LLM reasoning into the generative process is a promising avenue for capturing more nuanced instructions."
- **Why unresolved:** The current architecture uses a frozen LLM for sampling (g, s) pairs but does not allow iterative refinement or dialogue-style clarification within the generation loop.
- **What evidence would resolve it:** Ablation studies comparing single-pass conditioning against multi-turn refinement, measuring gains in complex instruction alignment and reduction in ambiguous generations.

## Limitations
- The constituency parser-based text encoding may struggle with highly complex or ambiguous instructional language that doesn't parse cleanly into coherent clauses.
- The pairwise coherence mechanism could miss long-range visual dependencies beyond adjacent steps, potentially breaking consistency in longer instruction sequences.
- The automated reward model, while showing good correlation with human judgment, may still miss subtle misalignments and cannot fully replace human evaluation.

## Confidence

- **High Confidence:** The core mechanism of using constituency parsing for text encoding and the pairwise coherence approach are well-supported by experimental results and ablation studies.
- **Medium Confidence:** The reward-based fine-tuning significantly improves alignment, though the exact impact could be further validated across more diverse evaluation metrics.
- **Medium Confidence:** The claim that KL divergence and Chi-square tests effectively measure text-image alignment is supported, but these metrics may not capture all aspects of procedural correctness.

## Next Checks

1. **Cross-Domain Testing:** Evaluate the model on instructional domains beyond cooking (e.g., assembly instructions, medical procedures) to assess generalizability of the constituency parsing and coherence mechanisms.

2. **Long-Range Consistency Analysis:** Test the pairwise mechanism's limitations by creating synthetic instruction sequences where non-adjacent steps share critical visual elements, measuring consistency across steps separated by multiple intervals.

3. **Human Evaluation Expansion:** Conduct comprehensive human studies comparing not just alignment scores but also procedural correctness, commonsense plausibility, and instructional clarity across different difficulty levels of instructions.