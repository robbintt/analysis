---
ver: rpa2
title: Revisiting Feedback Models for HyDE
arxiv_id: '2511.19349'
source_url: https://arxiv.org/abs/2511.19349
tags:
- feedback
- hyde
- query
- terms
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper revisits the application of traditional feedback models\
  \ (Rocchio, RM3) to HyDE, a pseudo-relevance feedback method that uses LLM-generated\
  \ hypothetical answer documents to enrich query representations for sparse retrievers\
  \ like BM25. While recent LLM-based methods often use simple string concatenation,\
  \ the authors systematically evaluate whether incorporating established feedback\
  \ models can improve HyDE\u2019s performance."
---

# Revisiting Feedback Models for HyDE

## Quick Facts
- arXiv ID: 2511.19349
- Source URL: https://arxiv.org/abs/2511.19349
- Authors: Nour Jedidi; Jimmy Lin
- Reference count: 20
- Primary result: Traditional feedback models (Rocchio, RM3) improve HyDE effectiveness up to 1.4 points (4.2%) over string concatenation baselines, with larger gains on low-resource tasks.

## Executive Summary
This paper systematically evaluates whether traditional feedback models (Rocchio, RM3) can enhance HyDE, a pseudo-relevance feedback method that uses LLM-generated hypothetical documents to enrich query representations for sparse retrievers like BM25. While recent LLM-based methods often use simple string concatenation, the authors demonstrate that incorporating established feedback models substantially improves effectiveness across 14 retrieval datasets. The study shows that Rocchio, in particular, provides significant improvements through better term selection and weighting, offering a simple way to boost retrieval accuracy without requiring complex model architectures.

## Method Summary
The study compares four feedback methods for HyDE: Average Vector (element-wise averaging), Rocchio (α=1, β=0.75), RM3 (λ=0.5), and Naive Concatenation (simple string concatenation). HyDE generates N=8 hypothetical documents (≤512 tokens) per query using LLMs (Qwen2.5-7B-Instruct, Qwen3-14B, gpt-oss-20b). Term selection filters terms appearing in >10% of corpus documents and retains top-k=128 terms based on summed normalized frequency. The feedback models compute term weights that are used as boosts in Lucene BooleanQuery constructions for BM25 retrieval via Pyserini. Experiments are conducted on 5 MS MARCO datasets (DL19–DL23) and 9 BEIR datasets.

## Key Results
- Rocchio achieves up to 1.4 points (4.2%) improvement in Recall@20 over the best string concatenation baseline
- Gains are particularly pronounced on low-resource BEIR tasks where feedback models demonstrate stronger robustness
- The main benefits come from filtering noisy terms and providing more stable, linear weighting of query terms
- Rocchio outperforms RM3 on HyDE-generated documents despite RM3's effectiveness on traditional PRF

## Why This Works (Mechanism)

### Mechanism 1: Corpus-Statistics Term Selection Filters LLM Hallucination Noise
- Claim: Filtering common corpus terms and selecting only top-k expansion terms improves HyDE effectiveness over naive concatenation.
- Mechanism: For each LLM-generated document, compute term-frequency vectors, filter terms appearing in >10% of corpus documents, rank by summed normalized frequency, retain only top k=128 terms.
- Core assumption: LLM-generated hypothetical documents contain high-frequency/noisy terms that degrade BM25 ranking.
- Evidence anchors: Abstract states "key benefits come from filtering noisy terms"; section 4 shows 3, 2.3, and 3.1 point improvement for Avg Vector over Naive Concat, with main difference being term selection step.

### Mechanism 2: Parameterized Query Emphasis Corrects Under-Weighting
- Claim: Rocchio's α/β weighting scheme improves over equal weighting by preserving original query term importance.
- Mechanism: Compute w_{t,q_new} = α·f(q)[t] + (β/N)·Σf(đ_i)[t] with α=1, β=0.75, rather than averaging equally across query + feedback documents.
- Core assumption: String concatenation and averaging under-emphasize original query terms relative to expansion content.
- Evidence anchors: Section 4 notes "Rocchio algorithm gives more weight to the query terms... the HyDE feedback under-emphasizes query terms"; advantage is that "query weight can be adapted linearly with a single parameter rather than being dependent on other factors such as feedback document length."

### Mechanism 3: Linear Interpolation Provides Stable Cross-Domain Weighting
- Claim: Feedback models with simple linear parameters (α, β, λ) offer more stable weighting than adaptive string repetition schemes.
- Mechanism: Use fixed interpolation rather than length-dependent query repeat factors (e.g., MuGI's γ = len(đ_1...)/[len(q)·φ]).
- Core assumption: Linear weighting is more interpretable and robust across heterogeneous query types than heuristic repeat counts.
- Evidence anchors: Section 4 shows "feedback models... demonstrated stronger robustness on the diverse range of queries present in low-resource BEIR tasks"; section 5 attributes improvement to "more stable and linear weighting of query terms and terms in feedback documents via simple parameters like α and λ."

## Foundational Learning

- **Pseudo-Relevance Feedback (PRF)**
  - Why needed here: HyDE is conceptually a PRF variant where LLM-generated documents replace top-retrieved documents; understanding PRF clarifies the two-phase design (term selection + weighting).
  - Quick check question: In traditional PRF, what is the source of expansion terms, and how does HyDE differ?

- **BM25 Term Weighting**
  - Why needed here: The paper modifies how query term weights are computed before BM25 scoring; understanding BM25's query vector construction (TF-based) explains why boost weights matter.
  - Quick check question: If you double the frequency of a query term in the query string, what happens to its BM25 weight approximation?

- **Rocchio Algorithm (Vector Space)**
  - Why needed here: Core baseline; understanding α (query weight) and β (feedback weight) parameters is essential to interpreting results.
  - Quick check question: What happens to the expanded query representation if β is set to 0? What if α=0?

## Architecture Onboarding

- **Component map:** LLM Generator → Term Selection Module → Feedback Model → Query Builder → BM25 Retriever

- **Critical path:**
  1. Prompt LLM to generate N hypothetical answer documents
  2. Build filtered term-frequency vectors for each document (requires corpus index statistics)
  3. Apply feedback model to compute weight for each expansion term
  4. Construct Lucene BooleanQuery with term boosts
  5. Execute BM25 search

- **Design tradeoffs:**
  - Rocchio vs. RM3: Rocchio simpler (two parameters), outperforms RM3 on HyDE documents; RM3 stronger on traditional PRF (top-BM25 docs)
  - String concatenation vs. feedback models: Concatenation simpler to implement (no index statistics needed), but feedback models more robust on low-resource domains
  - Number of generated documents (N): Paper uses N=8; more documents increase compute cost and may introduce noise

- **Failure signatures:**
  - Naive Concat underperforms BM25 baseline: LLM introducing misleading terms (noise overwhelms signal)
  - Rocchio/Avg Vector underperforms Naive Concat: Term selection too aggressive (k too small or corpus filter too strict)
  - Large gap between MS MARCO and BEIR performance: Concatenation methods failing on diverse query types; switch to feedback models
  - RM3 > Rocchio on traditional PRF but Rocchio > RM3 on HyDE: Expected; feedback source characteristics differ

- **First 3 experiments:**
  1. **Ablate term selection:** Compare Avg Vector (with selection) vs. Naive Concat (no selection) using same LLM/documents—validates noise-filtering hypothesis; expect 2–3 point Recall@20 gain.
  2. **Sweep Rocchio α/β:** Fix β=0.75, vary α ∈ {0.5, 0.75, 1.0, 1.25}; then fix α=1, vary β ∈ {0.25, 0.5, 0.75, 1.0}—determines optimal query emphasis; paper defaults may not transfer across corpora.
  3. **Compare feedback sources:** Run Rocchio with (a) HyDE-generated docs vs. (b) top-8 BM25 docs—validates that LLM-generated feedback provides superior signal; expect 2.5–4.3 point improvement per paper's gpt-oss-20b results.

## Open Questions the Paper Calls Out
The paper identifies two future research directions: investigating how to adaptively reweight Rocchio's α and β parameters using schemes like MuGI's adaptive query repeat, and exploring whether the observed performance differences between Rocchio and RM3 stem from term selection effectiveness versus linear weighting stability.

## Limitations
- Term selection requires corpus-level statistics, limiting scalability to very large corpora
- Fixed hyperparameters (α=1, β=0.75, k=128) may not generalize across all retrieval scenarios
- The study doesn't isolate whether improvements come from term selection alone or from the interaction between selection and Rocchio weighting

## Confidence
- **High confidence**: That Rocchio outperforms string concatenation on BEIR datasets (multiple datasets show consistent gains)
- **Medium confidence**: That filtering common terms improves results (mechanism plausible but not extensively validated across different corpus sizes)
- **Medium confidence**: That linear weighting provides stability (supported by results but not tested across diverse query distributions)

## Next Checks
1. **Parameter sensitivity sweep**: Systematically vary α, β, and k across multiple datasets to identify optimal parameter ranges and test robustness.
2. **Term selection ablation**: Run experiments with and without term selection on the same documents to isolate its contribution versus Rocchio weighting.
3. **Alternative LLM generation**: Test whether improvements generalize to smaller, more practical LLMs or whether they require large models like gpt-oss-20b.