---
ver: rpa2
title: Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic
  Inference
arxiv_id: '2510.21184'
source_url: https://arxiv.org/abs/2510.21184
tags:
- reward
- outputs
- repulse
- samples
- episodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RePULSe, a reinforcement learning method
  for language models that aims to reduce the probability of undesirable outputs by
  explicitly sampling and down-weighting low-reward sequences. The core idea is to
  learn a proposal distribution that focuses on low-reward outputs and then use this
  proposal to guide training on reducing their probability under the model.
---

# Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference

## Quick Facts
- arXiv ID: 2510.21184
- Source URL: https://arxiv.org/abs/2510.21184
- Reference count: 40
- Primary result: Introduces RePULSe, a reinforcement learning method that explicitly samples and down-weights low-reward sequences to reduce undesirable outputs while maintaining expected reward

## Executive Summary
This paper presents RePULSe (REducing the Probability of Undesirable Language Sequences), a reinforcement learning approach that aims to reduce the probability of undesirable outputs in language models. The method learns a proposal distribution that focuses on low-reward sequences and uses this to guide training that reduces their probability under the model. By explicitly targeting undesirable outputs rather than treating them implicitly through reward maximization, RePULSe achieves better tradeoffs between expected reward and the probability of bad outputs compared to standard RL methods. The approach also demonstrates increased robustness to adversarial attacks.

## Method Summary
RePULSe operates by first learning a proposal distribution that can efficiently sample low-reward sequences from the language model. This proposal is trained to up-weight sequences that receive low rewards, creating a focused distribution over undesirable outputs. During training, RePULSe uses importance sampling with this proposal to estimate gradients that directly reduce the probability of these low-reward sequences under the main model. The method combines this with standard reward maximization to maintain high-quality outputs. The key innovation is separating the objective into two components: maximizing expected reward while explicitly minimizing the probability of undesirable sequences, rather than treating both objectives through a single reward signal.

## Key Results
- RePULSe improves the tradeoff between expected reward and probability of undesirable outputs compared to standard RL methods
- The method demonstrates increased robustness to specific adversarial attacks that target undesirable outputs
- Experiments show RePULSe can effectively reduce the probability mass assigned to low-reward sequences while maintaining overall output quality

## Why This Works (Mechanism)
RePULSe works by explicitly addressing the limitation of standard RL approaches that treat undesirable outputs implicitly through negative rewards. When a sequence receives a negative reward, traditional RL only indirectly affects its probability through the gradient signal. RePULSe instead learns to directly identify and sample these undesirable sequences, then applies targeted training to reduce their probability mass under the model. This explicit treatment allows for more precise control over the distribution of undesirable outputs without sacrificing the quality of desirable sequences. The proposal distribution acts as a "lens" that focuses training on the specific regions of sequence space that need correction.

## Foundational Learning

**Importance Sampling**: Used to estimate expectations under the model distribution using samples from a different distribution (the proposal). Why needed: Enables efficient estimation of gradients for low-reward sequences that would be rare under direct sampling. Quick check: Verify that proposal and target distributions have sufficient overlap to avoid high variance estimates.

**Reinforcement Learning with Sequence-Level Rewards**: Standard RL framework where models are trained to maximize cumulative rewards for generated sequences. Why needed: Provides the baseline approach that RePULSe improves upon by addressing its implicit treatment of undesirable outputs. Quick check: Ensure reward function properly distinguishes between desirable and undesirable outputs.

**Variational Inference**: Framework for learning approximate distributions (proposals) that can efficiently represent complex target distributions. Why needed: Provides the theoretical foundation for learning the proposal distribution that focuses on low-reward sequences. Quick check: Monitor KL divergence between proposal and target to ensure proper learning.

## Architecture Onboarding

**Component Map**: Language Model -> Reward Function -> Proposal Distribution Learner -> Importance Weighted Gradient Estimator -> Updated Language Model

**Critical Path**: During training, the language model generates sequences, the reward function scores them, the proposal distribution is used to focus sampling on low-reward regions, importance weights are computed, and gradients are applied to reduce the probability of undesirable outputs while maintaining reward maximization.

**Design Tradeoffs**: The method trades increased computational complexity (learning and using a proposal distribution) for more precise control over undesirable output probabilities. This additional overhead may limit scalability but provides better theoretical guarantees and empirical performance on the target objective.

**Failure Signatures**: High variance in importance weights indicates poor overlap between proposal and target distributions, leading to unstable training. If the proposal fails to learn to focus on truly undesirable outputs, the method will not effectively reduce their probability.

**First Experiments**:
1. Test on a simple synthetic task where undesirable outputs can be perfectly enumerated to verify the method works in ideal conditions
2. Apply to a sentiment-controlled generation task to validate the approach on controlled generation
3. Evaluate on a toxicity detection task to confirm robustness improvements against adversarial attacks

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Computational overhead of learning the proposal distribution may be prohibitive for larger language models
- Robustness to adversarial attacks is demonstrated only on specific attack types, limiting generalizability
- Scalability to production-scale models remains uncertain given the sampling requirements

## Confidence
- High: The core RL framework and proposal distribution approach is technically sound and well-specified
- Medium: Experimental results showing improved reward-undesirability tradeoff on benchmark datasets
- Medium: Demonstration of increased robustness to specific adversarial attack types
- Low: Claims about practical applicability to large-scale production systems

## Next Checks
1. Benchmark RePULSe against established unlearning methods (e.g., UCD) on the same tasks to establish relative performance
2. Test robustness against a broader range of adversarial attacks including semantic and context-aware attacks
3. Measure computational overhead and wall-clock training time compared to baseline RL methods across different model scales