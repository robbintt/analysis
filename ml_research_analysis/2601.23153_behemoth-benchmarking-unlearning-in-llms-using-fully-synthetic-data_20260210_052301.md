---
ver: rpa2
title: 'Behemoth: Benchmarking Unlearning in LLMs Using Fully Synthetic Data'
arxiv_id: '2601.23153'
source_url: https://arxiv.org/abs/2601.23153
tags:
- accuracy
- blocks
- rank
- editing
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Behemoth, a synthetic data generation framework
  designed to benchmark model unlearning in large language models (LLMs). The authors
  address the challenge of studying knowledge editing in LLMs by creating fully synthetic
  data with controlled vocabularies and grammar, allowing precise measurement of model
  editing effects without the complexities of real-world data.
---

# Behemoth: Benchmarking Unlearning in LLMs Using Fully Synthetic Data

## Quick Facts
- **arXiv ID:** 2601.23153
- **Source URL:** https://arxiv.org/abs/2601.23153
- **Reference count:** 40
- **Primary result:** Introduces a synthetic data framework to precisely benchmark LLM unlearning, revealing that selective layer editing can be more effective than full-model updates.

## Executive Summary
This paper introduces Behemoth, a synthetic data generation framework designed to benchmark model unlearning in large language models (LLMs). The authors address the challenge of studying knowledge editing in LLMs by creating fully synthetic data with controlled vocabularies and grammar, allowing precise measurement of model editing effects without the complexities of real-world data. Using a custom tokenizer and modular framework, they generate tabular data in the form of {subject, relationship, object} tuples, which are then transformed into sentences using artificial grammars. The framework is demonstrated by training a 31-million-parameter Pythia transformer model on synthetic data and evaluating various model editing strategies, including full-rank and low-rank fine-tuning, as well as the ROME method. Experiments reveal that model editing can be more effective when only a subset of model layers are updated, and the optimal choice of layers depends on the editing strategy and the scale of the change. Notably, fine-tuning the entire model is not always necessary or desirable, and interpretability techniques like activation patching provide limited guidance for layer selection. The results highlight the importance of understanding the interaction between training data distribution and model editing, offering insights into improving the reliability and effectiveness of knowledge editing in LLMs.

## Method Summary
The paper proposes a synthetic data generation framework to benchmark LLM unlearning/editing methods. The method generates fully synthetic {subject, relationship, object} tuples with controlled tokenization and grammar. Sentences are constructed from templates using filler tokens (e.g., "SS 125 FT1 FT2 RR 3 FT3 OO 48"). A custom tokenizer fully partitions token spaces to prevent collisions. The authors train a 31M parameter Pythia transformer from scratch on this synthetic data until 95%+ accuracy, then evaluate editing strategies including full-rank fine-tuning (1 epoch), LoRA (ranks 32-128), and ROME. Editing success is measured by whether the top-probability token matches the new value, while remaining accuracy on unmodified data is also tracked. Experiments include single tuple overrides, 10 same/different relationship overrides, and relationship forgetting scenarios using ratios of 2:1:4 (edited: same-object:random clean data).

## Key Results
- Selective layer editing (fine-tuning only attention or MLP layers) can be more effective than full-model updates for unlearning
- The optimal layer selection depends on the editing strategy and scale of change being made
- Interpretability techniques like activation patching provide limited guidance for choosing which layers to edit
- Full-rank fine-tuning is not always necessary or desirable for effective unlearning

## Why This Works (Mechanism)
The synthetic data generation framework creates a controlled environment where token usage is fully partitioned and grammar is artificial, eliminating confounding factors present in real-world data. This controlled setup allows precise measurement of editing effects by ensuring each token maps to exactly one meaning and relationships are explicitly defined. The modular generation pipeline separates vocabulary creation, grammar definition, and sentence assembly, enabling systematic variation of data properties. The use of filler tokens creates structured templates that maintain grammatical consistency while isolating the relationships being tested. This controlled environment reveals how different editing strategies interact with model architecture at the layer level.

## Foundational Learning
- **Custom tokenization with partitioned vocabularies** - Ensures each token has a single, unambiguous meaning by preventing collisions; quick check: verify token frequency distribution shows no overlaps
- **Artificial grammar with filler tokens** - Creates structured sentence templates that isolate relationships while maintaining syntactic consistency; quick check: confirm all generated sentences follow template pattern
- **Layer-selective fine-tuning** - Updates only specific transformer layers rather than the entire model; quick check: measure parameter change magnitude per layer after editing
- **Low-rank adaptation (LoRA)** - Applies parameter-efficient updates using low-rank matrices; quick check: verify rank parameter matches intended configuration
- **ROMEResidual Offset Method** - Updates MLP layer parameters to change factual associations; quick check: confirm target MLP layer indices match experimental design
- **Activation patching** - Measures neuron importance by comparing model behavior with/without specific activations; quick check: verify patch application matches intended layer/position

## Architecture Onboarding
- **Component map:** Data Generator -> Tokenizer -> Grammar Template -> Sentence Builder -> Pythia-31M Model -> Editing Strategy (FT/LoRA/ROME) -> Evaluation Metrics
- **Critical path:** Synthetic tuple generation → model training → editing intervention → success measurement → remaining accuracy assessment
- **Design tradeoffs:** Controlled synthetic data enables precise measurement but may not capture real-world complexity; small model size facilitates experimentation but limits generalizability
- **Failure signatures:** Accuracy drops below 95% during training indicate insufficient subject diversity or training duration; severe accuracy loss after "forgetting" edits is expected per paper
- **Three first experiments:** 1) Generate dataset and train baseline model to 95%+ accuracy, 2) Run single tuple override editing with full-rank fine-tuning, 3) Compare layer-selective vs full-model editing effectiveness

## Open Questions the Paper Calls Out
- **Open Question 1:** Can increasing the dataset scale enable models to "grok" and efficiently store nested relationships? The authors state that providing many additional examples from an unlimited distribution of subjects with related objects could enable dependent relationships to be 'grokked', but current experiments with extended training durations did not show efficient storage of implied data.
- **Open Question 2:** How does the intentional introduction of token collisions impact the precision of model editing? The current framework prevents collisions through partitioned tokens, but the authors propose that any such token overuse be created intentionally and leave this investigation to future work.
- **Open Question 3:** Do the observed layer-selection effects for unlearning transfer to models trained on more complex grammars? The current study uses a simple filler-token grammar, and the authors note that expansion to more complex grammars and data relationships is left to future work.

## Limitations
- The synthetic data framework, while enabling precise measurement, may not capture the complexity and noise present in real-world unlearning scenarios
- The 31M parameter model size limits generalizability to larger LLMs where layer-wise editing dynamics may differ significantly
- Key hyperparameters for editing methods (LoRA learning rates, ROME targeting layers, random seeds) are not fully specified, complicating exact reproduction

## Confidence
- **High confidence:** Synthetic data generation framework and implementation are well-documented and reproducible
- **Medium confidence:** Findings regarding layer-selective editing effectiveness are supported but layer choices remain context-dependent
- **Low confidence:** Claims about interpretability techniques providing limited guidance for layer selection need further validation

## Next Checks
1. Reproduce core experiments with multiple random seeds to assess stability of layer-selective editing results
2. Test the synthetic framework with a larger model (1B+ parameters) to evaluate scalability of findings
3. Conduct ablation studies varying grammar template structure and filler token patterns to understand sensitivity to synthetic data design choices