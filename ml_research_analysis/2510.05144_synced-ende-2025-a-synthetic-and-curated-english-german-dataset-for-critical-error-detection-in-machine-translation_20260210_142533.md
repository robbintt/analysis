---
ver: rpa2
title: 'SynCED-EnDe 2025: A Synthetic and Curated English - German Dataset for Critical
  Error Detection in Machine Translation'
arxiv_id: '2510.05144'
source_url: https://arxiv.org/abs/2510.05144
tags:
- error
- synced-ende
- evaluation
- translation
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SynCED-EnDe, a new dataset for Critical Error
  Detection (CED) in English-German machine translation. Unlike prior benchmarks,
  SynCED-EnDe provides 1,000 gold-labeled and 8,000 silver-labeled sentence pairs,
  both balanced 50/50 between error and non-error cases.
---

# SynCED-EnDe 2025: A Synthetic and Curated English - German Dataset for Critical Error Detection in Machine Translation

## Quick Facts
- arXiv ID: 2510.05144
- Source URL: https://arxiv.org/abs/2510.05144
- Reference count: 6
- Introduces SynCED-EnDe: 1,000 gold-labeled and 8,000 silver-labeled English→German sentence pairs for Critical Error Detection (CED), achieving MCC=0.819 on XLM-R

## Executive Summary
This paper introduces SynCED-EnDe, a new dataset for Critical Error Detection (CED) in English-German machine translation. Unlike prior benchmarks, SynCED-EnDe provides 1,000 gold-labeled and 8,000 silver-labeled sentence pairs, both balanced 50/50 between error and non-error cases. The dataset is drawn from 2024-2025 sources (StackExchange, GOV.UK) and includes explicit error subclasses, structured trigger flags, and fine-grained auxiliary judgments across five dimensions (obviousness, severity, localization complexity, contextual dependency, adequacy deviation). These enrichments enable systematic analyses of error risk and intricacy beyond binary detection. The dataset is permanently hosted on GitHub and Hugging Face, accompanied by documentation, annotation guidelines, and baseline scripts. Benchmark experiments with XLM-R and related encoders show substantial performance gains over WMT21 due to balanced labels and refined annotations. The authors envision SynCED-EnDe as a community resource to advance safe deployment of MT in information retrieval and conversational assistants, particularly in emerging contexts such as wearable AI devices.

## Method Summary
SynCED-EnDe uses a pipeline of DeepL translation, GPT-4o error injection, and multi-round LLM re-annotation. The dataset contains 8,000 silver-labeled training pairs and 1,000 gold-labeled evaluation pairs, both balanced 50/50 between error and non-error cases. Error injection covers five subclasses: lexical (NAM), numerical (NUM), negation (SEN), safety (SAF), and toxicity (TOX). Each pair includes trigger flags for named entities, numbers, negations, and safety seeds. Auxiliary judgments rate obviousness, severity, localization complexity, contextual dependency, and adequacy deviation. The evaluation set undergoes three rounds of LLM rechecking plus manual verification, while the training set uses only LLM checks. All data is publicly available on GitHub and Hugging Face.

## Key Results
- XLM-R large achieves MCC=0.819, F1-ERR=0.894, F1-NOT=0.911 on SynCED-EnDe vs MCC=0.46 on WMT21
- Balanced 50/50 label distribution eliminates majority-class bias and improves detection reliability
- Auxiliary judgments enable risk-aware evaluation with strong correlations between adequacy deviation, severity, risk, and intricacy
- Synthetic error injection enables controlled error subclass coverage while maintaining cost efficiency through LLM annotation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Balanced 50/50 label distribution improves model learning by eliminating majority-class bias.
- Mechanism: Equal class weighting forces the classifier to develop discriminative features for both ERR and NOT cases rather than defaulting to the majority class. This enables more reliable error detection in deployment scenarios where false negatives (missed critical errors) carry higher cost than false positives.
- Core assumption: Assumes that real-world error distributions, while imbalanced, benefit from balanced training to ensure minority-class sensitivity.
- Evidence anchors:
  - [abstract] "balanced 50/50 between error and non-error cases"
  - [Section 6] "XLM-R reaches MCC = 0.819... vs 0.46 on WMT21" with balanced labels cited as key factor
  - [corpus] Related work on compact CED models (arXiv:2511.09748) shows similar focus on class-balanced evaluation
- Break condition: If deployment context has extreme class imbalance (>90% NOT), balanced training may produce over-sensitive models with high false-positive rates.

### Mechanism 2
- Claim: Multi-round LLM re-annotation with manual verification reduces label noise while maintaining cost efficiency.
- Mechanism: Three rounds of LLM-based checking act as noise filters, followed by human correction on the evaluation set only. This creates a high-quality gold standard (1,000 pairs) for reliable benchmarking while keeping the larger training set (8,000 pairs) economically viable.
- Core assumption: Assumes LLM-based re-annotation catches obvious inconsistencies and that manual intervention on a subset is sufficient for benchmark validity.
- Evidence anchors:
  - [Section 3.3] "evaluation set... three independent LLM-based rechecks followed by manual verification"
  - [Section 4.2] "training set... validated through three rounds of LLM-based rechecking only, without manual intervention"
  - [corpus] Limited direct evidence on LLM annotation reliability; related work (arXiv:2507.16410) uses LLMs for benchmark creation but doesn't validate annotation quality
- Break condition: If LLM annotators share systematic biases with the MT system under evaluation, label quality degrades; manual verification must explicitly check for such overlap.

### Mechanism 3
- Claim: Auxiliary judgment dimensions enable risk-aware evaluation beyond binary accuracy.
- Mechanism: Five fine-grained ratings (obviousness, severity, localization complexity, contextual dependency, adequacy deviation) support composite Risk and Intricacy scores. These allow downstream systems to calibrate confidence or prioritize high-risk errors rather than treating all errors equally.
- Core assumption: Assumes error characteristics are meaningfully separable along these dimensions and that weighted composites (Risk = 0.6×severity + 0.4×adequacy) capture real-world harm potential.
- Evidence anchors:
  - [Section 5.2] "Risk = 0.6S + 0.4A" and "Intricacy = 0.35(6−O) + 0.20L + 0.20C + 0.25A"
  - [Section 5.4] Correlation analysis shows "adequacy deviation, severity, risk, and intricacy are strongly correlated, while obviousness is largely orthogonal"
  - [corpus] No direct corpus evidence for these specific dimension weightings; weights appear heuristic rather than empirically derived
- Break condition: If composite weights don't reflect actual deployment costs, risk scores may misprioritize errors; domain-specific recalibration is likely needed.

## Foundational Learning

- Concept: **Critical Error Detection (CED) vs. Quality Estimation (QE)**
  - Why needed here: CED is a binary safety decision (safe/unsafe), not a continuous quality score. Conflating the two leads to thresholding problems and misaligned optimization.
  - Quick check question: Can you explain why a translation scoring 0.7 on a QE metric might still be unsafe for deployment?

- Concept: **Label imbalance effects on Matthews Correlation Coefficient (MCC)**
  - Why needed here: The paper uses MCC as the primary metric because it's robust to class imbalance. Understanding why F1 alone is insufficient prevents misinterpretation of results.
  - Quick check question: Why does accuracy remain high on imbalanced WMT21 (~72% NOT) even when the model fails to detect most errors?

- Concept: **Synthetic error injection vs. natural error distributions**
  - Why needed here: SynCED-EnDe uses controlled error injection (lexical, numerical, negation, toxicity). This differs from naturally occurring MT errors, affecting generalization claims.
  - Quick check question: What types of real-world MT errors might be underrepresented in a synthetic injection pipeline?

## Architecture Onboarding

- Component map: Source data -> DeepL translation -> GPT-4o error injection -> LLM re-annotation -> TSV output with auxiliary judgments
- Critical path: Load evaluation split (1,000 gold pairs) for benchmarking -> Train on silver split (8,000 pairs) if fine-tuning -> Report MCC, F1-ERR, F1-NOT (all three required) -> Optional: analyze performance by error subclass and risk/intricacy strata
- Design tradeoffs:
  - **Gold vs. silver**: Evaluation set has manual verification; training set does not. Trade-off is cost vs. scale.
  - **Synthetic vs. natural errors**: Controlled injection enables balanced coverage but may not reflect real MT error distributions.
  - **Binary vs. multi-dimensional**: Auxiliary judgments add richness but increase annotation complexity and are eval-only.
- Failure signatures:
  - High MCC but low F1-ERR → model biased toward NOT (check class balance in training)
  - Strong performance on WMT21 but weak on SynCED-EnDe → possible overfitting to Wikipedia domain
  - High accuracy on obvious errors but failure on subtle errors (high intricacy scores) → model lacks fine-grained sensitivity
  - Inconsistent results across error subclasses → class-specific feature learning failure
- First 3 experiments:
  1. **Baseline replication**: Train XLM-R-base on silver split, evaluate on gold split. Target: MCC ≥ 0.72 (per Table 4). Confirm reproducibility.
  2. **Cross-dataset transfer**: Train on SynCED-EnDe silver, evaluate on WMT21 test. Compare to training on WMT21 train split. Hypothesis: balanced training may help or hurt depending on domain shift.
  3. **Risk-stratified analysis**: Bin evaluation set by Risk score (low/medium/high). Report MCC per bin. Hypothesis: high-risk errors should be easier to detect if severity correlates with detectability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do models trained on SynCED-EnDe generalize to detecting naturally occurring MT errors versus synthetic injected errors?
- Basis in paper: [explicit] "Because error injection relies on GPT-4o, some artifacts may not be fully representative of naturally occurring MT errors."
- Why unresolved: The dataset uses controlled synthetic error injection; no comparison with organic error distributions was conducted.
- What evidence would resolve it: Benchmark models trained on SynCED-EnDe tested on human-annotated corpora with naturally occurring errors, comparing performance gaps.

### Open Question 2
- Question: Can the SynCED framework be effectively extended to other language pairs while maintaining annotation quality?
- Basis in paper: [explicit] "Extending the resource to additional language pairs is an important direction for future work."
- Why unresolved: Only English-German was constructed; language-specific challenges (morphology, script, cultural nuances) may affect error injection and labeling pipelines.
- What evidence would resolve it: Replication of the pipeline for diverse language pairs (e.g., En-Zh, En-Ar) with comparable inter-annotator agreement and model performance metrics.

### Open Question 3
- Question: How does the absence of manual validation in the silver training split affect downstream model robustness?
- Basis in paper: [explicit] "Although the 8000 training set underwent three rounds of automated rechecking, it was not manually validated, unlike the 1000 evaluation split."
- Why unresolved: Potential label noise in training data may impact model generalization, but this was not systematically analyzed.
- What evidence would resolve it: Ablation studies comparing models trained on manually validated subsets versus silver-only data, evaluated on held-out gold benchmarks.

## Limitations

- Generalization uncertainty: Synthetic error patterns may not fully represent naturally occurring MT failures, limiting real-world applicability
- Heuristic weighting: Risk and Intricacy composite scores use arbitrary weightings without empirical validation against deployment costs
- Limited manual verification: Only 1,000 evaluation pairs receive human verification, leaving potential systematic biases in the 8,000 training pairs

## Confidence

- **High confidence**: Balanced label distribution's impact on MCC (0.819 vs 0.46) is well-supported by controlled experiments and MCC's mathematical properties
- **Medium confidence**: Auxiliary judgment dimensions enable risk-aware evaluation, but rely on heuristic weightings without empirical validation
- **Low confidence**: LLM-based annotation pipeline quality control for training data lacks direct evidence and comparison with human annotation

## Next Checks

1. **Cross-domain transfer**: Evaluate whether models trained on SynCED-EnDe generalize to naturally occurring errors from different MT systems (e.g., Google Translate, OpenNMT) and domains (news, patents, social media) beyond the StackExchange and GOV.UK sources.

2. **Error distribution analysis**: Compare the synthetic error patterns in SynCED-EnDe against error analyses from real MT system outputs in the same domains to quantify representational coverage and identify potential blind spots in the synthetic injection approach.

3. **Cost-sensitive recalibration**: Conduct user studies or expert reviews to validate whether the Risk and Intricacy composite scores align with actual deployment costs and harm potential across different use cases (medical translation, legal documents, conversational AI).