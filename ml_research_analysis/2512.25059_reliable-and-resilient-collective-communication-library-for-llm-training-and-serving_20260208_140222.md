---
ver: rpa2
title: Reliable and Resilient Collective Communication Library for LLM Training and
  Serving
arxiv_id: '2512.25059'
source_url: https://arxiv.org/abs/2512.25059
tags:
- failure
- r2ccl
- failures
- training
- collective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: R2CCL introduces a fault-tolerant collective communication library
  for large-scale ML training and inference that automatically recovers from network
  failures without job termination. The key innovation is bilateral failure awareness
  with lightweight probing for rapid fault localization, combined with multi-NIC GPU
  buffer registration and DMA-buffer rollback to enable seamless connection migration
  during ongoing collectives.
---

# Reliable and Resilient Collective Communication Library for LLM Training and Serving

## Quick Facts
- arXiv ID: 2512.25059
- Source URL: https://arxiv.org/abs/2512.25059
- Authors: Wei Wang; Nengneng Yu; Sixian Xiong; Zaoxing Liu
- Reference count: 32
- Primary result: Fault-tolerant collective communication library that recovers from network failures without job termination, maintaining <1% training overhead and <3% inference overhead

## Executive Summary
R2CCL introduces a fault-tolerant collective communication library that enables large-scale ML training and inference to automatically recover from network failures without job termination. The system employs bilateral failure awareness with lightweight probing for rapid fault localization, combined with multi-NIC GPU buffer registration and DMA-buffer rollback to enable seamless connection migration during ongoing collectives. For single failures, R2CCL provides load balancing and bandwidth-aware AllReduce optimization to redistribute traffic across remaining healthy NICs. Physical testbed experiments with 8-GPU H100 servers and large-scale simulations demonstrate that R2CCL maintains up to 93% of baseline throughput during failures while supporting various failure types including NIC/hardware faults, link issues, and partial degradations.

## Method Summary
R2CCL is a 3K-line C++ extension to NCCL that intercepts ncclNet calls and provides fault tolerance through three main components: (1) Bilateral failure awareness using out-of-band bootstrap network notifications plus RDMA probe triangulation for millisecond-scale fault localization; (2) Live migration via multi-NIC GPU buffer registration and DMA-buffer rollback enabling sub-millisecond connection migration; (3) Adaptive collective optimization using R2CCL-Balance for generic collectives and R2CCL-AllReduce for bandwidth-aware AllReduce decomposition. The system supports multi-NIC GPU clusters and automatically detects and recovers from various network failures including NIC faults, link issues, and partial degradations without requiring job termination or checkpoint restart.

## Key Results
- Maintains <1% training overhead and <3% inference overhead under single NIC failure
- Achieves up to 93% of baseline throughput during failures for large message sizes
- Outperforms AdapCC and DéjàVu by 12.18× and 47× respectively in failure recovery scenarios
- Scales to 512 GPUs with <5% overhead per failure in Monte Carlo simulations

## Why This Works (Mechanism)

### Mechanism 1: Bilateral Failure Awareness with Probe-Based Localization
R2CCL reduces failure detection from minutes to milliseconds by using out-of-band (OOB) notification plus RDMA probe triangulation. When a connection error occurs, the detecting endpoint notifies its peer via a separate bootstrap network, then both endpoints issue zero-byte RDMA Write probes to triangulate whether the fault is local NIC, remote NIC, or link. This requires the bootstrap network remains operational when the data-path NIC fails and clusters have ≥3 nodes for three-point triangulation.

### Mechanism 2: Multi-NIC GPU Buffer Registration with DMA-Buffer Rollback
Pre-registering GPU buffers with all NICs at initialization enables sub-millisecond connection migration without data loss. During communicator initialization, each GPU buffer is registered with all NICs on the server (installing mapping entries, not duplicating data). On failure, sender rewinds to the last chunk without a completion; receiver resets to last confirmed chunk. Retransmission proceeds over backup NIC ordered by PCIe proximity, assuming applications tolerate the one-time registration overhead at startup.

### Mechanism 3: Bandwidth-Aware AllReduce Decomposition (R2CCL-AllReduce)
Decomposing AllReduce into concurrent global and partial phases reduces workload on bandwidth-constrained nodes, improving throughput when ≥33% of a node's bandwidth is lost. For a degraded node with bandwidth loss fraction X > 1/3, R2CCL splits data: a global AllReduce includes all nodes (throttled by degraded node), while a partial AllReduce excludes it. A subsequent broadcast delivers partial results to the degraded node, requiring remaining healthy nodes have sufficient bandwidth headroom to absorb partial AllReduce.

## Foundational Learning

- **RDMA Queue Pairs (QPs) and Completion Queues (CQs)**: Why needed: R2CCL's failure detection relies on interpreting RDMA transport errors (retry-exceeded, CQE errors) and using probe QPs isolated from data paths. Quick check: Can you explain why RDMA's one-sided operations delay error propagation to the CPU compared to TCP?

- **NCCL Channel Topology (Ring/Tree)**: Why needed: R2CCL modifies channel-to-NIC bindings and reorders logical rings; understanding NCCL's bootstrap vs. steady-state phases is prerequisite. Quick check: What happens to an ongoing collective if a NIC bound to a channel fails in vanilla NCCL?

- **PCIe NUMA Affinity and PXN (Proxy GPU Forwarding)**: Why needed: R2CCL-Balance selects between direct PCIe forwarding and NVLink-based PXN based on NUMA distance and bandwidth headroom. Quick check: Why might PXN via NVLink be preferred over cross-socket PCIe to a remote-NUMA NIC?

## Architecture Onboarding

- **Component map**: Transport Plugin Layer -> Failure Detection Module -> Live Migration Engine -> Collective Scheduler -> Bootstrap Network
- **Critical path**: 1. Initialization: Multi-register GPU buffers with all NICs; pre-establish backup QPs. 2. Runtime: Monitor CQ for errors → OOB notify peer → probe triangulation → rollback to last acknowledged chunk → select backup NIC → resume collective. 3. Optimization: If X ≥ 1/3, invoke R2CCL-AllReduce; otherwise, use R2CCL-Balance or standard ring.
- **Design tradeoffs**: Pre-registration overhead at startup vs. faster failover latency; R2CCL-AllReduce complexity (two-phase coordination) vs. R2CCL-Balance simplicity (generic, all collectives); PXN detour overhead vs. cross-NUMA PCIe latency.
- **Failure signatures**: Single NIC failure: Detected via probe timeout; migration to backup NIC; <1% training overhead claimed. Rail mismatch (adjacent nodes lose different NICs): Topology-aware re-ranking inserts bridge nodes; recursive AllReduce handles multiple bottlenecks. Complete node failure or NVLink fault: Out of scope; falls back to checkpoint-based recovery.
- **First 3 experiments**: 1. Baseline microbenchmark: Run NCCL-tests AllReduce with 8B–16GB messages on healthy cluster; record baseline throughput. 2. Single-NIC failure injection: Disable one NIC mid-collective; measure R2CCL-HotRepair vs. R2CCL-Balance vs. R2CCL-AllReduce throughput retention (expect ~93% for large messages with AllReduce). 3. Multi-failure scalability: Use SimAI to simulate 512 GPUs with 1–10 random failures; verify overhead stays <5% per Figure 10.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but has several implicit limitations: the system does not address NVLink/NVSwitch failures within nodes (stated as "leave as future work"), does not handle complete node failures, and does not discuss behavior when the bootstrap network itself fails. The authors also do not address how their mechanisms would extend to Mixture-of-Experts training workloads with distinct all-to-all communication patterns, nor do they discuss correlated failure scenarios where failures cluster on adjacent servers sharing the same ToR switch or power domain.

## Limitations

- Scope limited to InfiniBand with ConnectX-7 NICs; behavior on RoCE or other RDMA transports not validated
- Does not address multi-node failures where more than one NIC on a single node fails simultaneously
- Bootstrap network failure scenario not discussed - system assumes OOB channel remains operational
- NVLink/NVSwitch intra-node fabric failures are theoretically possible but explicitly left as future work

## Confidence

- **High Confidence**: Bilateral failure detection mechanism and probe-based localization (well-supported by detailed mechanism description and consistent with RDMA transport behavior)
- **Medium Confidence**: Multi-NIC buffer registration and DMA rollback (mechanism described clearly but lacks detailed performance characterization of registration overhead)
- **Medium Confidence**: R2CCL-AllReduce bandwidth-aware decomposition (analytical formulation provided but performance claims rely heavily on simulation rather than physical testbed validation)
- **Low Confidence**: Scalability claims beyond 512 GPUs (based entirely on SimAI simulations without physical validation at scale)

## Next Checks

1. **Failure Detection Latency Validation**: Instrument the OOB bootstrap network and probe QP paths to measure actual detection time from NIC error to bilateral notification under controlled failure injection, comparing against the claimed millisecond-scale performance versus vanilla NCCL minutes.

2. **Multi-Failure Topology Mapping**: Create a physical testbed with at least 4 nodes where different NICs fail on adjacent nodes (rail mismatch scenario) and verify that topology-aware re-ranking and recursive AllReduce decomposition correctly navigate the bandwidth heterogeneity while maintaining throughput claims.

3. **Registration Overhead Profiling**: Measure the one-time GPU buffer registration cost across all NICs during ncclCommInitRank for different GPU memory sizes (from 8GB to 80GB), and correlate this startup overhead with the claimed sub-millisecond failover latency to validate the pre-registration tradeoff.