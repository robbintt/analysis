---
ver: rpa2
title: 'ESMC: MLLM-Based Embedding Selection for Explainable Multiple Clustering'
arxiv_id: '2512.00725'
source_url: https://arxiv.org/abs/2512.00725
tags:
- clustering
- color
- embeddings
- dataset
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ESMC, a method for user-driven clustering
  that leverages hidden states of text tokens in multimodal large language models
  (MLLMs) as feature-specific embeddings. The approach addresses the limitation of
  typical deep clustering methods by enabling clustering based on arbitrary user-defined
  criteria.
---

# ESMC: MLLM-Based Embedding Selection for Explainable Multiple Clustering

## Quick Facts
- **arXiv ID**: 2512.00725
- **Source URL**: https://arxiv.org/abs/2512.00725
- **Reference count**: 40
- **Primary result**: Introduces ESMC, achieving NMI scores of 0.8138 and 0.8817 on Stanford Cars color and type criteria respectively

## Executive Summary
ESMC introduces a novel method for user-driven clustering that leverages hidden states of text tokens in multimodal large language models (MLLMs) as feature-specific embeddings. The approach addresses the limitation of typical deep clustering methods by enabling clustering based on arbitrary user-defined criteria. By extracting target embeddings from MLLM hidden states corresponding to user prompts and employing a lightweight clustering head with pseudo-label learning, ESMC achieves competitive performance across various clustering tasks. The method demonstrates significant improvements in clustering accuracy metrics while maintaining efficiency through a 2-layer MLP clustering head.

## Method Summary
ESMC works by extracting feature-specific embeddings from MLLM hidden states at positions corresponding to user-defined prompts. The process involves three main steps: (1) Target Embedding Extraction using a Logit Lens approach to project hidden states into vocabulary space and identify positions with high logits for target keywords, (2) Pseudo-label Generation through K-means clustering followed by selection of high-confidence samples, and (3) Clustering Head Training using a 2-layer MLP trained on the pseudo-labeled data. The method leverages the late-fusion properties of LLaVA-style MLLMs where text tokens accumulate visual information through attention layers.

## Key Results
- Achieves NMI scores of 0.8138 and 0.8817 on Stanford Cars color and type criteria respectively
- Demonstrates robust performance across seven diverse datasets including Stanford Cars, Flower, Fruit, Card, CMU face, Fruit360, and CIFAR10
- Shows significant improvements with the MLP clustering head, reaching up to 12% performance gain
- Maintains efficiency with a lightweight 2-layer MLP clustering architecture

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Aligned Feature Localization
The method relies on the MLLM's ability to bind visual features to specific text tokens during inference. When prompt tokens like "color" are fed into an MLLM alongside an image, the hidden states of those text tokens act as dense feature extractors for the specified visual attribute. The Logit Lens projection reveals high probabilities for the correct attribute at these positions, indicating successful feature alignment.

### Mechanism 2: Dimensionality Reduction via Semantic Projection
Projecting high-dimensional MLLM hidden states into vocabulary space ($|V|$ dimensions) helps manage the curse of dimensionality. This semantic projection provides a more meaningful basis for clustering than raw activation spaces. The subsequent 2-layer MLP learns to separate these projected vectors based on pseudo-labels, creating sharper decision boundaries than distance-based metrics alone.

### Mechanism 3: Pseudo-Label Refinement
The method transforms an unsupervised problem into a supervised one for the MLP head. High-confidence samples selected from an initial K-means pass provide supervision that outperforms the original clustering. By selecting the top $\alpha$% of samples closest to centroids as pseudo-labels, the approach filters out boundary noise and enables the network to learn more precise cluster boundaries.

## Foundational Learning

- **Concept**: **Logit Lens / Unembedding**
  - **Why needed here**: The paper relies on projecting intermediate transformer layers into vocabulary probability space to "read" the visual features. You must understand how to multiply a hidden state by the unembedding matrix $W_u$ to interpret what the model "would have" predicted at that layer.
  - **Quick check question**: Given a hidden state vector of dimension $d_{model}$ and an unembedding matrix of size $d_{model} \times |V|$, what does the resulting vector represent?

- **Concept**: **Pseudo-Labeling in Deep Clustering**
  - **Why needed here**: ESMC transforms an unsupervised problem into a supervised one for the MLP head. Understanding how to select high-confidence samples (thresholding distance to centroid) is critical to implementing the training loop.
  - **Quick check question**: Why does using the top $\alpha$% nearest samples to a centroid improve training stability compared to using all samples?

- **Concept**: **Attention Fusion in MLLMs**
  - **Why needed here**: The method extracts embeddings from *text tokens* that have "seen" the image. You need to grasp that in transformers, text tokens accumulate visual information through self-attention layers.
  - **Quick check question**: In a standard LLaVA-style forward pass, at which layer does the text token theoretically acquire information about the image content?

## Architecture Onboarding

- **Component map**: Image + Text Prompt -> LLaVA-1.5-7B Backbone -> Logit Lens Extraction -> Algorithm 1 (Localization) -> 2-Layer MLP Cluster Head -> Pseudo-label Generator (K-means + Alpha selection)

- **Critical path**: The "Embedding Localization" step (Section 3.1 / Algorithm 1) is the most brittle component. You must verify that the GPT-4 generated keywords actually appear in the model's top-k logits for the sample images. If this mapping fails, the selected embeddings will be noise.

- **Design tradeoffs**: 
  - Direct Output vs. Hidden State: Direct text generation is too verbose and biased; hidden states are preferred but require complex selection logic.
  - Universal vs. Specific Embedding: Using the final output layer is robust but generic; using intermediate layers is precise but requires per-dataset sampling.

- **Failure signatures**:
  - Language Bias: The model predicts the object name instead of the attribute at the target position.
  - Hallucination: High logits for features not present in the image.
  - Sample Sensitivity: Algorithm 1 fails if the small sample (10 images) is not representative of the color distribution.

- **First 3 experiments**:
  1. **Sanity Check (Logit Visualization)**: Run the pipeline on a single image (e.g., Red Car). Plot the logits at the "color" token position. Verify that "red" has a significantly higher probability than other colors or unrelated words.
  2. **Ablation (Head vs. No Head)**: Run clustering on the *raw* selected embeddings using K-means vs. the MLP-based approach. Confirm the performance gain shown in Table 3.
  3. **Robustness Test (Prompt Variation)**: Change the prompt from "The color of the car is" to "What is the vehicle's paint?". Run Algorithm 1 to see if the target embedding position shifts or if performance degrades.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the ESMC framework be effectively adapted to MLLM architectures with early-fusion mechanisms, given its current reliance on the late-fusion properties of LLaMA-based models? The Conclusion states, "MLLMs like LLaVA, built on LLaMA, exhibit specific text prompt embedding properties. We found this architecture type fuses text and image features at a relatively late stage," implying the method's dependency on this structural characteristic.

- **Open Question 2**: Can the target embedding localization be automated to remove the dependency on external keyword generation (via GPT-4) and manual image sampling? Section 3.1 describes the "Target Embedding Extraction" process, which involves a heuristic step of sampling images (e.g., 10 examples) and prompting GPT-4 for keywords to locate embeddings.

- **Open Question 3**: Does ESMC performance degrade significantly on domains where the MLLM backbone exhibits high uncertainty or lacks pre-training knowledge, as suggested by the failure cases in the Fruit datasets? Section 4.2 and Appendix A.4 attribute "minor underperformance" to "differences in the internal knowledge of the underlying models," citing specific examples where LLaVA misclassified visual features.

## Limitations

- The core mechanism of extracting target embeddings via keyword logits is novel but lacks direct validation of whether selected hidden states actually contain the intended visual features.
- The method requires careful sampling of 10 images for localization, but the selection criteria and representativeness are not specified.
- Performance improvements are attributed to the MLP head, but without clear ablation on the vocabulary projection's role versus the MLP's learning capacity.

## Confidence

- **High Confidence**: The experimental results showing competitive NMI/RI scores on seven datasets are directly reported and reproducible with the provided code.
- **Medium Confidence**: The pseudo-label refinement mechanism is theoretically sound and shows measurable gains, but the exact impact depends on the K-Means initialization quality and the $\alpha$ threshold selection.
- **Low Confidence**: The core mechanism of extracting target embeddings via keyword logits is novel and plausible, but lacks direct validation.

## Next Checks

1. **Keyword Localization Verification**: Run Algorithm 1 on a diverse set of 10 images for a given criterion (e.g., "color"). For each image, visualize the logits at the target token position. Confirm that the intended keywords (e.g., "red", "blue") consistently appear in the top-5 predictions. If keywords are missing or hallucinated, the method fails.

2. **Projection Ablation**: Reproduce the Stanford Cars experiment with three variants: (a) raw hidden states → K-Means, (b) raw hidden states → MLP head, (c) Logit Lens projection → MLP head. Compare the performance gap to isolate whether the vocabulary projection or the MLP architecture drives the improvement.

3. **Prompt Robustness Test**: Modify the prompt template (e.g., from "The color of the car is" to "What is the vehicle's paint?") and re-run the entire pipeline. Measure if Algorithm 1 can re-localize the correct embedding position and if clustering performance remains stable. A drop would indicate high sensitivity to prompt wording.