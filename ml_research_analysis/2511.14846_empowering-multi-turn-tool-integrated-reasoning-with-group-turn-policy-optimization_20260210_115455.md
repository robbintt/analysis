---
ver: rpa2
title: Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization
arxiv_id: '2511.14846'
source_url: https://arxiv.org/abs/2511.14846
tags:
- gtpo
- reward
- reasoning
- grpo
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GTPO, a novel reinforcement learning algorithm
  designed to improve large language models' multi-turn tool-integrated reasoning
  capabilities. The key innovation is a turn-level reward assignment that provides
  fine-grained feedback for individual reasoning steps, rather than trajectory-level
  rewards used in existing approaches like GRPO.
---

# Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization

## Quick Facts
- arXiv ID: 2511.14846
- Source URL: https://arxiv.org/abs/2511.14846
- Reference count: 26
- Primary result: 3.0% relative improvement on average across five mathematical reasoning benchmarks using turn-level rewards and code similarity shaping

## Executive Summary
This paper introduces GTPO, a novel reinforcement learning algorithm designed to improve large language models' multi-turn tool-integrated reasoning capabilities. The key innovation is a turn-level reward assignment that provides fine-grained feedback for individual reasoning steps, rather than trajectory-level rewards used in existing approaches like GRPO. GTPO incorporates return-based advantage estimation with discounting and self-supervised reward shaping using code similarity to densify sparse binary rewards. Experiments with Qwen2.5-7B-Instruct demonstrate that GTPO achieves 3.0% relative improvement on average across five diverse mathematical reasoning benchmarks compared to GRPO, with notable gains in AIME 2024, MATH 500, and SV AMP.

## Method Summary
GTPO extends GRPO by decomposing trajectory-level rewards into turn-level rewards, applying temporal discounting to future rewards, and incorporating code similarity-based reward shaping. The method uses Qwen2.5-7B-Instruct as the base model, first fine-tuned with cold-start SFT on 1.2K problem-trajectory pairs distilled from DeepSeek-R1 via OpenHands scaffold, then trained with RL on DAPO-17K dataset for 40 steps with batch size 1024, learning rate 1e-6, and γ=0.9. The three innovations are: (1) turn-level rewards measuring individual reward at each turn, (2) discounted return-based advantages, and (3) self-supervised reward shaping using code embedding similarity (Amazon Titan Text Embeddings V2, α=0.5) to densify sparse binary rewards.

## Key Results
- 3.0% average relative improvement across five benchmarks compared to GRPO
- Achieves 82.1% on AIME 2024 (vs 80.1% for GRPO)
- 85.4% on MATH 500 (vs 84.6% for GRPO)
- 66.4% on SV AMP (vs 65.6% for GRPO)
- Training stability improvements with less early stagnation than GRPO
- Code similarity shaping provides denser learning signals than binary rewards alone

## Why This Works (Mechanism)

### Mechanism 1: Turn-Level Reward Decomposition
Shifting from trajectory-level to turn-level rewards reduces noise in credit assignment for multi-turn interactions, provided the turns represent semi-independent reasoning steps. Standard GRPO assigns a single scalar reward to the entire token sequence, treating the multi-turn interaction as a bandit problem. GTPO constructs a new MDP where each turn (text + code) is a distinct action receiving a specific reward, isolating feedback for specific tool invocations from subsequent reflection or correction steps.

### Mechanism 2: Temporal Discounting of Returns
Applying a discount factor γ to future rewards allows the model to prioritize immediate reasoning correctness over distant outcomes, stabilizing the learning dynamics. GTPO replaces the single advantage value of GRPO with a discounted return, respecting the temporal structure of the MDP and systematically lowering the weight of rewards from distant future turns relative to the current action.

### Mechanism 3: Self-Supervised Reward Shaping via Code Similarity
Densifying sparse binary rewards using code similarity allows the model to extract learning signals from partially correct, yet ultimately failed, trajectories. Instead of assigning r=0 to incorrect trajectories, GTPO calculates a partial reward based on the cosine similarity of generated code embeddings between the failed trajectory and a set of correct trajectories, transforming a sparse bandit signal into a dense shaping signal.

## Foundational Learning

- **Concept: Tool-Integrated Reasoning (TIR)**
  - Why needed here: GTPO is designed specifically for the TIR loop (Reason → Code → Execute). You cannot implement the turn-level logic without understanding where one turn ends and the next begins (delimited by execution feedback).
  - Quick check question: Can you distinguish between the action (generating code) and the observation (execution result) in a TIR trace?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: GTPO modifies GRPO. You must understand the baseline GRPO advantage estimation (group-based normalization) to see how GTPO changes the variance reduction mechanism.
  - Quick check question: How does GRPO estimate advantages differently from PPO (hint: does it use a value function)?

- **Concept: Return-Based Advantage & Discounting (γ)**
  - Why needed here: The paper explicitly experiments with γ (Table 4). Understanding the trade-off between immediate vs. future reward weight is essential for tuning the system.
  - Quick check question: If γ = 0, what happens to the influence of future rewards on the current step's advantage?

## Architecture Onboarding

- **Component map:** Prompt → LLM generates Turn 1 → Sandbox executes code → LLM generates Turn 2... → Final Answer → Reward Calculation → Advantage Estimation → Policy Update

- **Critical path:** The policy (LLM) generates actions (text + code), the tool sandbox executes code to generate observations, the reward engine calculates rule-based penalties, outcome rewards, and code similarity, the buffer stores rollouts and computes GTPO loss.

- **Design tradeoffs:** Using code similarity adds computational overhead but stabilizes training; the discount factor γ=0.9 focuses on immediate correctness while preserving long-term strategy; embedding code only is better than embedding the whole trajectory.

- **Failure signatures:** Training stagnation from sparse rewards, format errors in tool calling, reward hacking through code similarity.

- **First 3 experiments:** (1) Baseline GRPO reproduction to confirm stagnation issue, (2) γ ablation sweep (0.5, 0.7, 0.9, 1.0) to verify optimal value, (3) reward shaping ablation comparing code-only vs full trajectory similarity vs no shaping.

## Open Questions the Paper Calls Out
- Can GTPO's turn-level reward and shaping mechanisms generalize to complex multi-turn domains beyond mathematical reasoning, such as software engineering?
- Does the relative performance gain of GTPO over GRPO scale effectively to LLMs significantly larger than 7B parameters?
- Is the optimal discount factor (γ=0.9) dependent on the maximum allowed trajectory length?

## Limitations
- Experiments are restricted to 7B parameter models due to computation budget, leaving scalability to larger models untested
- The method may reinforce superficially similar but logically incorrect code patterns through code similarity shaping
- Turn-level decomposition assumes reasoning steps are sufficiently independent, which may not hold for highly sequential problems

## Confidence
- Turn-level reward decomposition effectiveness: High confidence
- Code similarity reward shaping utility: Medium confidence
- 3.0% average relative improvement claim: High confidence

## Next Checks
1. Design problems where incorrect reasoning generates code syntactically similar to correct solutions to test reward hacking vulnerability
2. Create mathematical problems requiring strong temporal dependencies to compare GTPO's performance against GRPO
3. Replace Amazon Titan Text Embeddings with alternative code similarity metrics to validate generalization beyond specific embedding methods