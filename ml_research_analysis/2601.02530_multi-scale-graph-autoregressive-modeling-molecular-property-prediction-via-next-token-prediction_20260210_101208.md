---
ver: rpa2
title: 'Multi-scale Graph Autoregressive Modeling: Molecular Property Prediction via
  Next Token Prediction'
arxiv_id: '2601.02530'
source_url: https://arxiv.org/abs/2601.02530
tags:
- prediction
- graph
- molecular
- cams
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce Connection-Aware Motif Sequencing (CamS), a graph-to-sequence
  interface that enables standard decoder-only Transformers to learn molecular graphs
  via Next Token Prediction (NTP). CamS bridges the gap between SMILES-based NTP (which
  lacks explicit topology) and graph-native masked modeling (which risks disrupting
  critical chemical details like activity cliffs) by serializing molecular graphs
  into multi-scale, causal sequences.
---

# Multi-scale Graph Autoregressive Modeling: Molecular Property Prediction via Next Token Prediction

## Quick Facts
- **arXiv ID**: 2601.02530
- **Source URL**: https://arxiv.org/abs/2601.02530
- **Reference count**: 40
- **Primary result**: Achieves state-of-the-art performance on MoleculeNet (AUROC 0.845, RMSE 1.172) and MoleculeACE (RMSE 0.624) benchmarks

## Executive Summary
This paper introduces Connection-Aware Motif Sequencing (CamS), a graph-to-sequence interface that enables decoder-only Transformers to learn molecular graphs via Next Token Prediction (NTP). CamS bridges the gap between SMILES-based NTP (which lacks explicit topology) and graph-native masked modeling (which risks disrupting critical chemical details like activity cliffs) by serializing molecular graphs into multi-scale, causal sequences. The method mines data-driven connection-aware motifs, serializes them via scaffold-rooted BFS, and concatenates sequences from fine to coarse scales for hierarchical modeling. When pre-trained on a vanilla LLaMA backbone, CamS-LLaMA achieves state-of-the-art performance on MoleculeNet and MoleculeACE benchmarks.

## Method Summary
The approach begins with mining connection-aware motifs from molecular graphs, then serializes these motifs using scaffold-rooted breadth-first search to establish a stable core-to-periphery order. Critically, sequences are concatenated from fine to coarse scales to enable hierarchical modeling. The authors instantiate this framework by pre-training a vanilla LLaMA backbone on CamS sequences, creating CamS-LLaMA. This architecture outperforms both SMILES-based language models and strong graph baselines on molecular property prediction tasks while maintaining interpretability through attention analysis that reveals focus on cliff-determining structural differences.

## Key Results
- Achieves AUROC of 0.845 on MoleculeNet benchmark
- Achieves RMSE of 1.172 on MoleculeNet benchmark
- Achieves RMSE of 0.624 on MoleculeACE benchmark
- Outperforms both SMILES-based language models and strong graph baselines
- Demonstrates interpretable attention focusing on cliff-determining structural differences

## Why This Works (Mechanism)
The method works by preserving molecular topology through multi-scale serialization while maintaining the causal sequence structure that decoder-only Transformers expect. By mining connection-aware motifs and ordering them from scaffold root outward, the approach captures hierarchical structural relationships. The fine-to-coarse concatenation enables the model to learn both local atomic interactions and global molecular topology simultaneously, addressing the limitations of SMILES (which lacks explicit topology) and masked graph modeling (which can disrupt critical chemical details).

## Foundational Learning
- **Molecular Graph Representation**: Why needed - Molecular properties depend on atomic connectivity and spatial arrangement; quick check - Verify the graph preserves bond types and stereochemistry
- **Connection-Aware Motif Mining**: Why needed - Captures chemically meaningful substructures beyond simple subgraphs; quick check - Ensure motifs reflect functional groups and pharmacophores
- **Scaffold-Rooted BFS Serialization**: Why needed - Establishes stable, interpretable ordering from core to periphery; quick check - Confirm serialization preserves chemical relevance hierarchy
- **Multi-Scale Hierarchical Modeling**: Why needed - Enables simultaneous learning of local and global molecular features; quick check - Verify different scales capture appropriate level of detail
- **Decoder-Only Transformer Adaptation**: Why needed - Leverages existing language model architectures for molecular tasks; quick check - Confirm the model handles causal sequence constraints appropriately

## Architecture Onboarding
**Component Map**: Molecular Graph -> Connection-Aware Motif Mining -> Scaffold-Rooted BFS -> Multi-Scale Concatenation -> LLaMA Backbone -> Property Prediction

**Critical Path**: The most critical components are the motif mining (determines what substructures are learned) and the fine-to-coarse concatenation (enables hierarchical learning). The scaffold-rooted BFS provides stability but is less critical than the other two.

**Design Tradeoffs**: Uses existing LLaMA backbone (simpler than custom architectures) vs. encoder-decoder models (potentially more expressive but require more data); focuses on decoder-only (easier to train) vs. full transformer architectures; employs multi-scale (richer representation) vs. single-scale serialization.

**Failure Signatures**: Poor performance on datasets with unusual scaffolds or non-standard chemical motifs; degradation when connection-aware mining fails to capture relevant substructures; issues with molecules lacking clear scaffold hierarchy.

**3 First Experiments**:
1. Ablation study comparing single-scale vs. multi-scale serialization performance
2. Comparison against SMILES-based NTP using identical LLaMA backbone
3. Visualization of attention patterns on activity cliff examples

## Open Questions the Paper Calls Out
None

## Limitations
- Dependency on connection-aware motif mining which may not capture all chemically relevant substructures
- Serialization order assumes scaffold-rooted hierarchy is universally optimal
- Focus on decoder-only transformers excludes potential benefits from encoder-decoder architectures

## Confidence
- **High Confidence**: Claims about improved performance over SMILES-based models and specific benchmark results (MoleculeNet, MoleculeACE) are well-supported by presented metrics
- **Medium Confidence**: Claims about superiority of multi-scale serialization over single-scale approaches are supported but could benefit from more ablation studies
- **Medium Confidence**: Interpretability analysis showing attention focuses on cliff-determining differences is demonstrated but relies on qualitative observation

## Next Checks
1. Ablation study testing alternative serialization orders (coarse-to-fine, random, or task-specific orderings) to quantify the importance of the fine-to-coarse hierarchy
2. Systematic comparison of connection-aware motif mining against alternative substructure extraction methods (e.g., scaffold-based, functional group-based, or data-agnostic motif sets)
3. Cross-dataset generalization tests to evaluate whether performance gains on MoleculeNet and MoleculeACE translate to other molecular property prediction tasks and domains