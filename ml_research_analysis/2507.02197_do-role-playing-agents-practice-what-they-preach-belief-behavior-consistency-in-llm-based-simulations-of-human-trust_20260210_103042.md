---
ver: rpa2
title: Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency
  in LLM-Based Simulations of Human Trust
arxiv_id: '2507.02197'
source_url: https://arxiv.org/abs/2507.02197
tags:
- game
- trust
- player
- amount
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework to evaluate whether LLM-based
  role-playing agents' stated beliefs align with their actual simulated behavior.
  Using the Trust Game and a synthetic persona bank, the authors elicit models' beliefs
  about how personas will behave, then compare these to observed simulation outcomes.
---

# Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust

## Quick Facts
- arXiv ID: 2507.02197
- Source URL: https://arxiv.org/abs/2507.02197
- Reference count: 40
- One-line primary result: LLM role-playing agents exhibit systematic belief-behavior inconsistencies that can be detected and quantified through belief elicitation and simulation comparison.

## Executive Summary
This paper introduces a framework to evaluate whether LLM-based role-playing agents' stated beliefs align with their actual simulated behavior. Using the Trust Game and a synthetic persona bank, the authors elicit models' beliefs about how personas will behave, then compare these to observed simulation outcomes. They assess consistency at both population (via rank correlations and effect sizes) and individual (via multi-round forecasting accuracy) levels. Results reveal systematic belief-behavior inconsistencies: providing task context does not improve alignment, self-conditioning helps some models while imposed priors generally harm it, and forecasting accuracy degrades with longer horizons. These findings highlight the need for pre-deployment consistency checks before using LLMs for behavioral research.

## Method Summary
The authors evaluate belief-behavior consistency in LLM role-playing agents using the Trust Game with 50 synthetic personas from an augmented GenAgents bank (demographics + Big Five personality traits). They elicit models' beliefs about how persona attributes affect trust behavior using three strategies (context-free, context-rich with trust targets, context-rich with dollar targets), then compare these beliefs to actual simulation outcomes. Consistency is measured via Spearman correlation between belief/behavior rankings and absolute effect size discrepancy via eta-squared. Individual-level consistency is assessed through multi-round forecasting accuracy using the ReAct framework. Models tested include Llama 3.1 (8B/70B) and Gemma 2 (27B) with temperature=0.05.

## Key Results
- Systematic belief-behavior inconsistencies exist across models, with context-rich belief elicitation not improving alignment
- Self-conditioning improves consistency for Llama models but degrades it for Gemma 2, while imposed priors generally reduce consistency
- Individual-level forecasting accuracy degrades over longer horizons (6 rounds), suggesting temporal inconsistency
- CTX+$ elicitation strategy yields better ranking consistency than CTX+TR but overestimates effect sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Eliciting beliefs before simulation can detect population-level misalignment between what models say and how they act.
- Mechanism: The framework queries LLMs for ranked predictions about how persona attributes (e.g., age, conscientiousness) affect trust behavior, then compares these rankings against actual simulated outcomes using Spearman correlation and eta-squared effect size discrepancy. Mismatches reveal where encoded beliefs are not applied consistently during role-play.
- Core assumption: Models can articulate their beliefs when prompted, and these beliefs are comparable to behavioral outcomes across a population of synthetic personas.
- Evidence anchors:
  - [abstract] "We establish an evaluation framework to rigorously measure how well beliefs obtained by prompting the model can predict simulation outcomes in advance."
  - [section 4.1] Describes belief ranking π(S)t, belief effect size (bη2t(S)), behavioral ranking π*t, and behavioral effect size η2t, with consistency metrics ρ(S)t = Spearman(π(S)t, π*t) and ∆η2(S)t = |bη2t(S) − η2t|.
  - [corpus] Corpus has limited direct evidence on this specific mechanism; related work on ToM benchmarks (ToMATO) and belief verbalization is tangentially relevant but does not replicate this evaluation design.
- Break condition: If models cannot produce structured belief outputs (rankings or effect sizes) that are parsable, the mechanism fails. Also, if behavioral outcomes are too noisy (high variance across personas), effect size comparisons become unreliable.

### Mechanism 2
- Claim: Self-conditioning (feeding a model its own elicited beliefs back into the role-play prompt) can improve belief-behavior consistency for some architectures, but is not universally effective.
- Mechanism: After eliciting beliefs about attribute-behavior relationships, these beliefs are injected into the role-playing prompt as explicit conditioning context. This tests whether the model uses its stated beliefs to guide decisions, or whether behavioral generation ignores them.
- Core assumption: Models can attend to and apply in-context beliefs during generation; this is not guaranteed across architectures.
- Evidence anchors:
  - [abstract] "Self-conditioning helps some models, and imposed priors generally reduce consistency."
  - [section 4.3.1] "Self-conditioning increases Spearman's ρ for Llama 3.1 70B from 0.50 to 0.80, and for Llama 3.1 8B from 0.40 to 1.00, but Gemma 2 27B's ρ drops from 0.40 to less than 0.01."
  - [corpus] Weak direct evidence in neighbors; "Belief Boxes" paper mentions maintaining propositional beliefs in prompt space but does not evaluate consistency.
- Break condition: If the model's architecture or training does not support robust in-context conditioning (e.g., attention limitations, short context windows), self-conditioning will not improve consistency.

### Mechanism 3
- Claim: Individual-level forecasting accuracy degrades over longer horizons, suggesting temporal inconsistency in multi-round role-play.
- Mechanism: The model forecasts its own future actions as a specific persona across multiple Trust Game rounds (up to 6), then actual behavior is simulated. MAE between forecasted and enacted send amounts is measured per round, showing near-monotonic degradation.
- Core assumption: The Trustee's behavior is fixed via archetypes (M1, M3, M5), so any forecast error is attributable to the Trustor model's inconsistency, not environmental stochasticity.
- Evidence anchors:
  - [abstract] "Individual-level forecasting accuracy also degrades over longer horizons."
  - [section 5.3] "We observe a near-monotonic increase in MAE as the forecast horizon extends from one to six rounds... suggesting that longer horizons could introduce accumulating uncertainty."
  - [corpus] No direct corpus evidence on temporal degradation in multi-round forecasting; this appears to be a novel contribution.
- Break condition: If game state summaries are insufficient or ambiguous, forecast errors may stem from prompt design rather than model inconsistency. Also, if the model uses a different internal strategy each round (e.g., learning or adaptation), comparing forecasts to actions becomes invalid.

## Foundational Learning

- **Trust Game (economic game theory)**
  - Why needed here: This is the core behavioral testbed. You must understand the Trustor/Trustee roles, the endowment, the tripling of transferred amounts, and how the first player's decision depends solely on persona (no prior moves from the opponent).
  - Quick check question: If the Trustor sends $4, how much does the Trustee receive before deciding how much to return?

- **Eta-squared (η²) effect size from ANOVA**
  - Why needed here: The paper uses η² to quantify how much variance in behavior is attributable to persona attributes. Understanding this helps interpret effect size discrepancy (∆η²) as a consistency metric.
  - Quick check question: If η² = 0.10 for "political views," what does that mean about the proportion of variance in trust behavior explained by political views?

- **Role-playing agent architecture (persona-conditioned LLM prompting)**
  - Why needed here: You must understand how synthetic personas (demographics + Big Five personality) are injected into prompts, how the model generates decisions, and how output is parsed (JSON schema, regex for scalar values).
  - Quick check question: What happens if the model's output does not conform to the expected JSON schema?

## Architecture Onboarding

- **Component map**: Persona Bank -> Belief Elicitation Module -> Role-Play Simulation -> Consistency Evaluation -> Conditioning Module
- **Critical path**:
  1. Split persona bank (train/val/test); use only test split for experiments.
  2. Elicit beliefs using chosen strategy (NOCTX+TR, CTX+TR, CTX+$).
  3. Run Trust Game simulations for N=50 personas per condition.
  4. Compute behavioral rankings and effect sizes.
  5. Compare to elicited beliefs via ρ and ∆η².
  6. (Optional) Apply self-conditioning or imposed priors and re-evaluate.
- **Design tradeoffs**:
  - **NOCTX+TR vs. CTX+TR**: Context-free vs. context-rich belief elicitation. Paper finds context does not improve consistency.
  - **CTX+TR vs. CTX+$**: Construct-level (trust) vs. behavioral-level ($) targets. CTX+$ yields better ranking but overestimates effect sizes.
  - **Self-conditioning vs. imposed priors**: Self-conditioning works for Llama but not Gemma; imposed priors generally reduce consistency.
- **Failure signatures**:
  - Low or negative Spearman correlation: model's stated beliefs are inverted or unrelated to behavior.
  - Large effect size discrepancy: model systematically over- or under-estimates the impact of attributes.
  - MAE increasing over rounds: temporal inconsistency in multi-round forecasting.
  - Model refuses to output structured beliefs: prompt schema too complex or model not instruction-tuned.
- **First 3 experiments**:
  1. Replicate population-level consistency analysis for a single attribute (e.g., "political views") using NOCTX+TR and CTX+TR strategies; compare ρ and ∆η² to paper's reported values.
  2. Implement self-conditioning for Llama 3.1 8B and Gemma 2 27B; verify whether Llama improves and Gemma does not (as reported).
  3. Run individual-level forecasting for one persona across 6 rounds against the M3 archetype; plot MAE per round to confirm degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reasoning models (e.g., DeepSeek-R1, OpenAI o1/o3) with extended inference processes achieve higher belief-behavior consistency than traditional single-pass LLMs?
- Basis in paper: [explicit] Section 6 states: "Reasoning models of the likes of DeepSeek-R1 and OpenAI o1 and o3 employ extended reasoning processes that could potentially bridge the gap between belief elicitation and behavioral simulation."
- Why unresolved: The study only tested traditional LLMs without explicit reasoning steps; multi-step self-reflection during inference remains unexplored.
- What evidence would resolve it: Apply the same belief-behavior consistency framework to reasoning models and compare consistency metrics against baseline LLMs.

### Open Question 2
- Question: Can knowledge editing or inference-time steering methods provide more robust belief control than in-context prompting for imposing researcher-specified priors?
- Basis in paper: [explicit] Section 6 suggests: "Future work might explore knowledge editing or inference-time steering for more robust belief control" after finding imposed priors undermine consistency.
- Why unresolved: In-context conditioning showed limited effectiveness, especially for imposed priors; alternative intervention methods were not tested.
- What evidence would resolve it: Compare belief-behavior consistency when using knowledge editing or activation steering versus prompt-based conditioning for the same imposed priors.

### Open Question 3
- Question: How do belief-behavior inconsistencies manifest in multi-agent environments, open-ended dialogues, or temporally extended tasks beyond the single-task Trust Game?
- Basis in paper: [explicit] Section 6 states: "In future work, we aim to apply our framework to multi-agent environments, open-ended dialogues, or temporally extended tasks to evaluate how these inconsistencies manifest in more complex settings."
- Why unresolved: The Trust Game provides a structured, bounded environment; real-world simulations involve richer contexts and nuanced goals.
- What evidence would resolve it: Deploy the consistency evaluation framework in complex simulation environments (e.g., multi-agent negotiations, longitudinal social simulations) and measure consistency degradation patterns.

## Limitations
- Exact train/val/test split of the GenAgents persona bank is not specified, introducing potential variability in reproducibility
- Procedure for constructing imposed priors (weak ρ=0.80, strong ρ=0.20) is underspecified
- Trust Game simulation assumes perfectly deterministic Trustee behavior, which may not reflect real-world stochasticity
- Individual-level forecasting uses ReAct framework but details on state tracking are limited

## Confidence
- **High confidence**: Population-level belief-behavior consistency can be measured using Spearman correlation and effect size discrepancy
- **Medium confidence**: Self-conditioning improves consistency for some models (e.g., Llama 3.1) but not others (e.g., Gemma 2)
- **Medium confidence**: Context does not improve belief elicitation consistency, and CTX+$ outperforms CTX+TR in ranking but overestimates effect sizes
- **Low confidence**: Individual-level forecasting accuracy degrades over longer horizons

## Next Checks
1. Replicate the population-level consistency analysis for the "political views" attribute using both NOCTX+TR and CTX+TR strategies; verify that ρ values and ∆η² discrepancies match the paper's reported ranges.
2. Implement and test self-conditioning for Llama 3.1 8B and Gemma 2 27B; confirm whether Llama's ρ increases (e.g., from 0.40 to 1.00) and Gemma's ρ drops (e.g., from 0.40 to <0.01) as stated.
3. Run individual-level forecasting for a single persona against the M3 archetype across 6 rounds; plot MAE per round to confirm the near-monotonic increase reported in the paper.