---
ver: rpa2
title: Long-Term Electricity Demand Prediction Using Non-negative Tensor Factorization
  and Genetic Algorithm-Driven Temporal Modeling
arxiv_id: '2503.22132'
source_url: https://arxiv.org/abs/2503.22132
tags:
- reconstruction
- tensor
- forecasting
- data
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a long-term electricity demand prediction
  framework that uses only historical consumption data, without external variables.
  The approach combines Non-negative Tensor Factorization (NTF) to extract low-dimensional
  temporal features from multi-way data, and a Genetic Algorithm (GA) to optimize
  time series model hyperparameters applied to the latent factors.
---

# Long-Term Electricity Demand Prediction Using Non-negative Tensor Factorization and Genetic Algorithm-Driven Temporal Modeling

## Quick Facts
- arXiv ID: 2503.22132
- Source URL: https://arxiv.org/abs/2503.22132
- Reference count: 25
- Key outcome: NTF+GA method achieves lower MSE than baselines without tensor decomposition or evolutionary optimization on Japanese electricity data

## Executive Summary
This study introduces a framework for long-term electricity demand prediction that uses only historical consumption data, without external variables. The approach combines Non-negative Tensor Factorization (NTF) to extract low-dimensional temporal features from multi-way data, and a Genetic Algorithm (GA) to optimize time series model hyperparameters applied to the latent factors. Experiments on Japanese electricity data demonstrate that the proposed method achieves lower mean squared error (MSE) than baselines without tensor decomposition or evolutionary optimization. The method also shows improved generalization by reducing model complexity through tensor decomposition.

## Method Summary
The framework builds a 10×10×53 tensor from Japanese electricity consumption data (10 utilities × 10 sectors × 53 years), then applies CP-NTF with rank R=8 to extract annual latent factors. ARIMA models are fitted to each of the R latent factors, with GA optimizing hyperparameters [p_1,d_1,q_1,...,p_R,d_R,q_R] using validation MSE as fitness. The method reconstructs forecasts by summing outer products of the factor matrices. The approach aims to reduce model complexity while maintaining forecasting accuracy through tensor decomposition and automated hyperparameter tuning.

## Key Results
- NTF+GA method achieves lower MSE than baselines without tensor decomposition or evolutionary optimization
- The approach demonstrates improved generalization by reducing model complexity through tensor decomposition
- Sensitivity analysis reveals that initialization randomness in NTF can be mitigated by using multiple runs or ensemble strategies
- The framework offers an interpretable, flexible, and scalable approach for long-term electricity demand forecasting

## Why This Works (Mechanism)

### Mechanism 1
Tensor decomposition extracts shared latent temporal patterns that improve generalization over modeling each series independently. NTF decomposes the consumption tensor X ∈ R^(10×10×53) into three non-negative factor matrices A (utilities), B (sectors), C (years) via canonical polyadic decomposition with multiplicative update rules. The annual factor matrix C captures shared temporal dynamics across all utility–sector combinations, reducing the forecasting problem from 100 independent time series to R=8 latent factors. Core assumption: electricity consumption patterns across utilities and industries share low-dimensional structure; non-negativity constraints yield interpretable, part-based representations. Evidence anchors: [abstract] "reducing the model's degrees of freedom via tensor decomposition improves generalization performance"; [Section 4.2] DOF analysis shows configurations with fewer total (p_r + q_r) yield better reconstruction; w/o NTF baseline has higher effective DOF per element but worse MSE in Pattern A2010 (21.70 vs 5.022).

### Mechanism 2
Genetic Algorithm optimization adapts ARIMA complexity per latent factor, avoiding manual tuning and improving fit. The GA evolves populations encoding [p_1, d_1, q_1, ..., p_R, d_R, q_R], using tournament selection and crossover/mutation. Fitness is evaluated as negative MSE between predicted and actual total consumption on validation data. This enables factor-specific model orders rather than fixed ARIMA(3,2,3) across all factors. Core assumption: the hyperparameter search space is sufficiently non-convex that gradient-free optimization outperforms grid search or fixed defaults; validation MSE correlates with test generalization. Evidence anchors: [Section 2.3] "GA selects the best set of ARIMA parameters by minimizing the mean squared error (MSE) between the sum of predicted consumption and the ground-truth validation tensor"; [Tables 2–5] Proposed method outperforms w/o GA baseline in all patterns except B2000 (5.995 vs 7.595 MSE improvement over w/o GA).

### Mechanism 3
Multiple NTF initializations with ensemble/selection strategies mitigate sensitivity to local minima. NTF uses random initialization, converging to different local solutions. Tables 6–9 show reconstruction accuracy varies by seed (e.g., 0.904 vs 0.862 for seeds 10 vs 20 in A2010). Selecting top-performing seeds or averaging predictions stabilizes output. Core assumption: high validation reconstruction accuracy correlates with stable, reliable forecasts; local minima vary in generalization quality. Evidence anchors: [Section 4.3] "NTF-based method exhibits variability in reconstruction accuracy depending on the random seed... Figures 8 and 9 show that averaging over top-performing initializations provides stable predictions"; [Tables 6–9] Seed-dependent reconstruction ranges from 0.601–0.904 (A2000).

## Foundational Learning

- **Concept: Canonical Polyadic (CP) Tensor Decomposition**
  - Why needed here: Understanding how a 3-way tensor factorizes into component matrices is essential for interpreting what the latent factors represent and selecting appropriate rank R
  - Quick check question: Given factor matrices A (10×8), B (10×8), C (53×8), can you reconstruct how element X[i,j,k] is computed from the factors?

- **Concept: ARIMA(p,d,q) Model Components**
  - Why needed here: The GA optimizes p (autoregressive order), d (differencing), and q (moving average order) per latent factor—understanding their roles is necessary to interpret GA outputs and diagnose poor forecasts
  - Quick check question: If a latent annual factor shows linear trend but no seasonality, what (d, p, q) values would you expect the GA to favor?

- **Concept: Evolutionary Algorithm Operators (Selection, Crossover, Mutation)**
  - Why needed here: The GA's tournament selection and genetic operators determine exploration vs. exploitation in hyperparameter space; misconfiguration leads to premature convergence or excessive variance
  - Quick check question: If the GA population collapses to near-identical individuals after few generations, which operator should be tuned?

## Architecture Onboarding

- **Component map:**
  Raw Consumption Data (10 utilities × 10 sectors × 53 years) → Tensor Construction & Train/Val/Test Split → NTF (CP decomposition, R=8, 100 iterations) → Factor matrices A, B, C (annual factors = time series) → ARIMA Modeling on C columns (R parallel models) → GA Hyperparameter Search (validation MSE fitness) → Tensor Reconstruction: X̂ = Σ a_r ◦ b_r ◦ ĉ_r → Aggregated Forecast Evaluation (MSE vs. ground truth)

- **Critical path:**
  1. Rank selection (R=8 based on reconstruction accuracy tradeoff, Figure 1)
  2. NTF initialization seed choice (impacts factor quality, Tables 6–9)
  3. GA fitness function (MSE vs. reconstruction accuracy; Section 4.1 shows context-dependent optimal choice)
  4. DOF control via (p, q) selection (Table 10 links lower DOF to better generalization)

- **Design tradeoffs:**
  - Higher rank R: Better reconstruction but more ARIMA models to tune, higher overfit risk, longer GA search
  - MSE vs. reconstruction accuracy as fitness: MSE optimizes forecast error directly; reconstruction accuracy may improve structural fidelity but not always test MSE
  - Single-seed vs. multi-seed NTF: Multi-seed robustness vs. 3–5× compute cost

- **Failure signatures:**
  - Reconstruction accuracy < 0.7 on validation: Rank R likely too low or data lacks shared structure
  - GA converges to homogeneous population: Increase mutation rate or tournament size
  - Test MSE >> validation MSE: Overfitting to validation period; reduce DOF or shorten validation window
  - Large seed-to-seed variance in forecasts: Initialization instability; implement ensemble averaging

- **First 3 experiments:**
  1. Baseline replication: Reproduce Pattern A2010 with R=8, single seed=42, fixed ARIMA(3,2,3) to verify MSE ≈ 5.022; compare against w/o NTF baseline
  2. Rank sensitivity sweep: Test R ∈ {4, 6, 8, 10, 12} measuring validation reconstruction and test MSE; identify elbow point for your data
  3. Seed ensemble test: Run 5 seeds, rank by validation reconstruction, average top-3 forecasts; quantify variance reduction vs. single-seed

## Open Questions the Paper Calls Out

### Open Question 1
Can the framework effectively integrate exogenous variables (e.g., temperature, economic indicators) without sacrificing the interpretability and generalization benefits gained from using only historical consumption data? Basis in paper: [explicit] The conclusion states that future work includes "expanding the method to support exogenous variables," while the introduction emphasizes the benefit of avoiding unreliable external data. Why unresolved: The current study deliberately excludes external variables to prove the efficacy of the latent feature extraction, leaving the interaction between NTF latent factors and external covariates unexplored. What evidence would resolve it: A comparative study showing prediction accuracy and model complexity when exogenous variables are added as additional tensor modes or regressors versus the historical-data-only baseline.

### Open Question 2
How can an adaptive mechanism be developed to automatically determine the optimal tensor rank $R$ that balances reconstruction accuracy against the risk of overfitting in the forecasting stage? Basis in paper: [explicit] The conclusion identifies "incorporating adaptive rank selection" as a specific direction for future work. Why unresolved: In the experiments, the rank ($R=8$) was selected manually based on a trade-off curve (Figure 1) between reconstruction and computational cost, rather than through an automated, data-driven optimization process. What evidence would resolve it: Implementation of an automated rank-selection algorithm (e.g., using Bayesian Optimization or stability-based selection) that correlates selected rank with minimized validation error on the latent time series factors.

### Open Question 3
Under what specific data characteristics (e.g., trend stability, noise levels) should the Genetic Algorithm optimize for reconstruction accuracy versus Mean Squared Error (MSE) to maximize forecasting performance? Basis in paper: [inferred] Section 4.1 notes that "optimizing for reconstruction accuracy leads to higher structural fidelity... while optimizing for MSE yields better forecasting performance," but the outcome varied confusingly across test cases (A2010 vs A2000). Why unresolved: The paper provides no theoretical or empirical rule to decide which fitness function is appropriate for a given dataset, treating it currently as a hyperparameter to be tested. What evidence would resolve it: A sensitivity analysis across multiple datasets with varying trend characteristics to map the relationship between the chosen fitness criterion and final out-of-sample forecasting error.

## Limitations
- Method's sensitivity to NTF initialization requires careful seed selection or ensemble strategies, increasing computational cost
- GA hyperparameter search space is not fully specified, potentially affecting reproducibility
- Approach assumes shared low-dimensional structure across utilities and sectors, which may not hold in all electricity markets

## Confidence

**High confidence**: The tensor decomposition mechanism for dimensionality reduction and its impact on model generalization; the GA optimization framework for hyperparameter tuning; the sensitivity of NTF to initialization seeds

**Medium confidence**: The claim that NTF+GA consistently outperforms all baselines across all patterns; the scalability of the approach to larger or higher-frequency datasets; the interpretability benefits of non-negative constraints

**Low confidence**: The assumption that validation MSE always correlates with test generalization; the optimal rank selection methodology; the comparative advantage over state-of-the-art deep learning approaches

## Next Checks
1. Cross-market validation: Apply the NTF+GA framework to electricity demand data from at least two different countries/regions to assess generalizability beyond Japanese data patterns
2. Rank sensitivity analysis: Systematically test R ∈ {4, 6, 8, 10, 12} across all benchmark patterns to identify optimal rank selection criteria and quantify overfitting risks
3. Deep learning comparison: Benchmark against LSTM or Transformer-based forecasting models on the same dataset using identical train/val/test splits to establish relative performance in the modern context