---
ver: rpa2
title: Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications
arxiv_id: '2508.06145'
source_url: https://arxiv.org/abs/2508.06145
tags:
- drug
- contraindication
- contraindications
- system
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops a retrieval-augmented generation system to
  improve large language model accuracy in answering drug contraindication questions
  for sensitive populations. By integrating GPT-4o mini with a hybrid retrieval system
  over structured Drug Utilization Review data, the system achieves substantial accuracy
  gains over baseline models (0.94 vs.
---

# Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications

## Quick Facts
- arXiv ID: 2508.06145
- Source URL: https://arxiv.org/abs/2508.06145
- Reference count: 29
- Primary result: RAG system achieves 0.94 accuracy vs 0.49-0.57 baseline for drug contraindication QA

## Executive Summary
This study develops a retrieval-augmented generation system to improve large language model accuracy in answering drug contraindication questions for sensitive populations. By integrating GPT-4o mini with a hybrid retrieval system over structured Drug Utilization Review data, the system achieves substantial accuracy gains over baseline models (0.94 vs. 0.49-0.57) across pregnancy, pediatric, and drug-drug interaction categories. The approach reduces unsupported or incorrect contraindication judgments and grounds responses in verified regulatory data. Category-specific performance varies, with pregnancy and pediatric queries showing strong grounding, while drug-drug interactions reveal greater retrieval challenges. The results demonstrate the value of retrieval augmentation in enabling reliable, evidence-based medical QA systems for safety-critical contexts.

## Method Summary
The system employs a hybrid retrieval-augmented generation pipeline using GPT-4o mini as the LLM backbone, with knowledge sourced from Korean Drug Utilization Review (DUR) public database. DUR entries are chunked into semantically coherent units (typically single clinical restrictions), embedded using text-embedding-3-small, and indexed in Milvus. Hybrid retrieval combines cosine similarity semantic search with BM25 lexical search, with results merged and re-ranked. The pipeline uses LangChain orchestration, retrieving top-k chunks for each query and injecting them into structured prompts that guide the LLM to select from four decision types (contraindicated with/without evidence, safe with/without evidence) with rationale. The system is evaluated on 300 test queries (100 per category) with 50 contraindicated and 50 verified non-contraindicated examples per category, assessed by licensed pharmacists.

## Key Results
- Overall accuracy improves from 0.49-0.57 baseline to 0.94 with RAG augmentation
- Pregnancy category achieves 0.94 accuracy with 44/44 contraindications correctly identified with evidence
- Drug-drug interaction category shows lowest performance at 0.79 accuracy with 21/50 contraindications missed
- Category-specific grounding rates vary: pregnancy/pediatric show strong evidence retrieval, DDI reveals fundamental retrieval gaps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid retrieval combining semantic and lexical search improves contraindication evidence retrieval over single-method approaches.
- Mechanism: Dense embeddings capture semantic similarity (drug names, clinical concepts) while BM25 provides exact term matching for regulatory keywords. Re-ranking merges both signals to prioritize contextually and lexically relevant passages.
- Core assumption: Contraindication queries contain both semantic patterns (drug relationships) and specific lexical markers (drug names, interaction types) that require dual signal matching.
- Evidence anchors:
  - "Our RAG system employs a hybrid retrieval method that integrates both dense embedding-based semantic search and sparse keyword-based lexical search."
  - Table 5 shows 44/44 pregnancy contraindications correctly identified with evidence (Type 1), demonstrating retrieval grounding success.
  - Related RAG architectures show consistent FMR scores 0.44-0.64, suggesting hybrid approaches are broadly effective but performance varies by domain complexity.
- Break condition: Multi-entity queries (drug-drug interactions) may fail when neither semantic nor lexical signals uniquely identify the relationship—evidenced by 21 missed contraindications in DDI category.

### Mechanism 2
- Claim: Context grounding in verified regulatory data reduces hallucinated contraindication judgments.
- Mechanism: Retrieved DUR passages are injected into the LLM prompt as explicit conditioning context, constraining generation to information present in retrieved documents rather than parametric memory.
- Core assumption: The base LLM (GPT-4o mini) lacks reliable pharmaceutical safety knowledge and will hallucinate when unguided; grounding corrects this by providing authoritative source material.
- Evidence anchors:
  - "The approach reduces unsupported or incorrect contraindication judgments and grounds responses in verified regulatory data."
  - "Category (3) represents predictions of contraindication that are unsupported by retrieved evidence, suggesting retrieval failure or hallucinated generation."
  - Weak corpus evidence—no direct comparison studies on hallucination rates in medical RAG systems were found in the retrieved neighbors.
- Break condition: If retrieval fails (no relevant passage retrieved), the model may still hallucinate from parametric knowledge—Type 3 errors (unsupported predictions) occurred in 2-3 cases per category.

### Mechanism 3
- Claim: Chunking strategy preserving single clinical restrictions per embedding improves retrieval precision for specific contraindication queries.
- Mechanism: DUR entries are segmented into semantically coherent units (one clinical restriction per chunk), allowing embedding vectors to represent focused concepts rather than mixed content, reducing noise during similarity matching.
- Core assumption: Contraindication queries map to discrete clinical facts that benefit from granular representation rather than document-level context.
- Evidence anchors:
  - "Each contraindication entry from the DUR database was segmented into semantically coherent units, typically corresponding to a single clinical restriction."
  - Keyword-level evaluation (Tables 6-8) shows high precision (1.00) across most categories, indicating retrieved evidence matches query intent.
  - No corpus evidence specifically addresses chunking granularity effects in medical RAG.
- Break condition: Drug-drug interactions may require multi-hop reasoning across chunks, explaining lower DDI performance (0.79 ACC) compared to single-entity categories.

## Foundational Learning

- Concept: Dense vs. Sparse Retrieval
  - Why needed here: Understanding why hybrid retrieval outperforms single-method approaches requires distinguishing semantic matching (embeddings) from lexical matching (BM25).
  - Quick check question: Can you explain why "pediatric contraindication" might require different retrieval signals than "drug-X drug-Y interaction"?

- Concept: Grounding and Hallucination in LLMs
  - Why needed here: The core value proposition of RAG is reducing hallucinations through context grounding; engineers must understand when grounding succeeds vs. fails.
  - Quick check question: If retrieval returns no documents, should the LLM still generate an answer? What does the paper's Type 3 category suggest?

- Concept: Chunking Granularity and Retrieval Precision
  - Why needed here: Chunking strategy directly affects retrieval quality; poor chunking creates semantic noise that degrades downstream accuracy.
  - Quick check question: Why might document-level chunks hurt performance for specific contraindication queries compared to restriction-level chunks?

## Architecture Onboarding

- Component map:
  - Knowledge Source: DUR database (CSV files) → Chunking → text-embedding-3-small → Milvus vector index
  - Query Path: User query → Parallel retrieval (Milvus semantic + BM25 lexical) → Merge & deduplicate → Reranker → Top-k context → Prompt template → GPT-4o mini → Structured output
  - Orchestration: LangChain pipeline coordinating retrievers, reranker, and LLM

- Critical path:
  1. Chunk quality (granularity, token limits) → embedding fidelity
  2. Hybrid retrieval balance → recall of relevant passages
  3. Reranking quality → precision of final context
  4. Prompt construction → LLM grounding effectiveness

- Design tradeoffs:
  - Chunk size: Smaller chunks improve precision but may lose context; paper uses ~1000 token max with single-restriction focus
  - Retrieval k: Larger k improves recall but adds noise; paper retrieves top-k from each retriever then re-ranks
  - Category-specific tuning: Pregnancy/pediatric perform well (0.92-0.94 ACC) but DDI underperforms (0.79 ACC), suggesting multi-entity queries need architecture changes

- Failure signatures:
  - Type 2 errors (false evidence retrieval): Retrieved contraindication evidence for safe drugs—caused by semantic similarity to different contraindicated drugs
  - Type 3 errors (unsupported predictions): Contraindication warnings without evidence—retrieval failure or model hallucination
  - DDI category failures: 21/50 contraindications missed with no evidence retrieved, indicating fundamental retrieval gaps for multi-entity queries

- First 3 experiments:
  1. Reproduce baseline vs. RAG accuracy gap: Test GPT-4o mini without retrieval on 50 pregnancy queries to confirm 0.52→0.94 improvement signal.
  2. Ablate hybrid retrieval components: Test semantic-only vs. lexical-only vs. hybrid on pediatric category to quantify contribution of each retriever.
  3. Probe DDI retrieval failures: Manually inspect the 21 missed DDI contraindications to determine if chunks exist in knowledge base or if retrieval ranking is at fault.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does knowledge graph-based expansion improve retrieval accuracy for multi-entity drug-drug interaction queries compared to the current hybrid semantic-lexical retrieval system?
- Basis in paper: The authors state future work will focus on "enhancing retrieval quality by incorporating knowledge graph-based expansion to better capture complex or rare drug interactions."
- Why unresolved: The current hybrid retriever struggled with drug-drug interactions, showing lower F1-scores (0.73) and higher rates of missed contraindications (21 false negatives) due to retrieval failures in multi-entity contexts.
- What evidence would resolve it: Comparative benchmarking showing retrieval improvement on the drug-drug interaction subset when a knowledge graph is integrated into the pipeline.

### Open Question 2
- Question: How does system accuracy and clinical utility change when extending the architecture to support multi-turn interaction and personalized patient profiles?
- Basis in paper: The authors note the system "currently assumes a single-turn, context-independent question format" and list "extending the system to support multi-turn interaction" as a primary direction for future research.
- Why unresolved: Real-world pharmaceutical counseling requires handling temporal context (e.g., stage of pregnancy) and individualized patient data, which the current static architecture cannot process.
- What evidence would resolve it: Performance metrics from a modified system tested on a dataset requiring dialogue history or structured patient profile integration to resolve queries.

### Open Question 3
- Question: Can transformer-based semantic similarity or entailment models provide a more accurate assessment of rationale validity than the keyword-based matching used in this study?
- Basis in paper: The authors acknowledge their "rationale validation approach—based on keyword matching—may overlook semantically equivalent but lexically diverse explanations," and propose developing "transformer-based similarity scoring" in future work.
- Why unresolved: Keyword matching may penalize clinically accurate explanations that use different terminology, potentially underestimating the model's reasoning capability.
- What evidence would resolve it: A study correlating human expert evaluations of rationale quality with both keyword-based F1-scores and transformer-based semantic similarity scores.

## Limitations
- The hybrid retrieval approach shows category-specific effectiveness, with drug-drug interactions revealing fundamental architectural constraints for multi-entity medical queries.
- Korean language focus and use of Korean-specific DUR data may limit generalizability to other regulatory contexts or languages.
- Category-specific grounding rates vary significantly, suggesting the need for query-type-specific optimization strategies.

## Confidence
- High confidence: Retrieval augmentation improves baseline LLM accuracy from 0.49-0.57 to 0.94 overall, with clear grounding benefits in pregnancy and pediatric categories (0.92-0.94 ACC).
- Medium confidence: The hybrid retrieval mechanism's superiority over single-method approaches is demonstrated, but specific contribution analysis (semantic vs. lexical) remains incomplete.
- Low confidence: Generalization to drug-drug interactions and other multi-entity medical scenarios requires further validation beyond the Korean DUR dataset.

## Next Checks
1. Cross-linguistic validation: Test the RAG architecture on English drug contraindication datasets to assess language dependency and generalizability.
2. Multi-entity query optimization: Implement entity-aware chunking or query decomposition specifically for drug-drug interactions and measure performance improvement.
3. Hallucination control validation: Conduct controlled experiments where retrieval returns zero documents to quantify the frequency and nature of unsupported predictions (Type 3 errors).