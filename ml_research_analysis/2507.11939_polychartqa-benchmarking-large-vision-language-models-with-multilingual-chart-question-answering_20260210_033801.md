---
ver: rpa2
title: 'POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart
  Question Answering'
arxiv_id: '2507.11939'
source_url: https://arxiv.org/abs/2507.11939
tags:
- chart
- multilingual
- data
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: POLYCHARTQA is the first large-scale multilingual chart question
  answering benchmark covering 22,606 charts and 26,151 QA pairs across 10 languages.
  It is constructed using a decoupled pipeline that separates chart data from rendering
  code, enabling efficient multilingual generation through data translation and code
  reuse.
---

# POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering

## Quick Facts
- arXiv ID: 2507.11939
- Source URL: https://arxiv.org/abs/2507.11939
- Reference count: 40
- POLYCHARTQA is the first large-scale multilingual chart question answering benchmark covering 22,606 charts and 26,151 QA pairs across 10 languages

## Executive Summary
POLYCHARTQA introduces a novel multilingual benchmark for chart question answering, addressing the lack of evaluation resources for non-English chart understanding. The benchmark covers 10 languages and 26,151 QA pairs, constructed through a decoupled pipeline that separates chart data from rendering code. This approach enables efficient multilingual generation through data translation and code reuse, leveraging state-of-the-art LLMs for translation with rigorous quality control.

The evaluation reveals significant performance gaps between English and other languages, particularly for low-resource languages with non-Latin scripts. Fine-tuning on the companion POLYCHARTQA-Train dataset demonstrates substantial improvements in multilingual chart understanding across diverse model sizes and architectures, establishing a foundation for advancing cross-lingual visual reasoning capabilities.

## Method Summary
The benchmark construction employs a decoupled pipeline that separates chart data from rendering code, enabling efficient multilingual generation through data translation and code reuse. State-of-the-art LLMs are used for translation with rigorous quality control to ensure linguistic and semantic consistency. The pipeline generates charts by first creating data tables, then using these tables to render charts through code templates, which are then translated into multiple languages. This approach allows for scalable generation of multilingual QA pairs while maintaining consistency across languages.

## Key Results
- Significant performance gaps exist between English and other languages, especially low-resource ones with non-Latin scripts
- Fine-tuning on POLYCHARTQA-Train yields substantial gains in multilingual chart understanding across diverse model sizes and architectures
- The benchmark covers 22,606 charts and 26,151 QA pairs across 10 languages

## Why This Works (Mechanism)
The decoupled pipeline approach enables efficient multilingual generation by separating data creation from chart rendering. This separation allows for code reuse across languages while only translating the underlying data and questions, reducing the computational and manual effort required for multilingual benchmark construction. The use of state-of-the-art LLMs for translation ensures high-quality multilingual content while maintaining semantic consistency across languages.

## Foundational Learning
- **Multilingual chart understanding**: Essential for evaluating cross-lingual visual reasoning capabilities; quick check involves testing models on both source and target language charts
- **Decoupled data-rendering pipeline**: Enables efficient multilingual generation through code reuse; quick check involves verifying consistent chart appearance across language variants
- **LLM-based translation quality control**: Critical for maintaining semantic consistency; quick check involves human evaluation of translation accuracy for sample QA pairs

## Architecture Onboarding

**Component map**: Data generation -> Chart rendering code -> QA pair generation -> Translation -> Quality control

**Critical path**: The pipeline flow moves from raw data creation through chart rendering, question generation, translation, and quality validation, with each stage building upon the previous one to create consistent multilingual content.

**Design tradeoffs**: The decoupled approach trades potential rendering inconsistencies for scalability and efficiency, while LLM-based translation prioritizes automation over manual quality control but requires rigorous validation.

**Failure signatures**: Performance gaps may indicate translation artifacts rather than true linguistic differences; inconsistent chart rendering across languages suggests pipeline integration issues; translation errors manifest as semantic drift in QA pairs.

**3 first experiments**:
1. Test baseline model performance on English vs. translated charts to establish baseline performance gaps
2. Evaluate translation quality by comparing model performance on human-translated vs. LLM-translated content
3. Assess fine-tuning effectiveness by measuring performance improvements across different model sizes

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Translation quality relies heavily on state-of-the-art LLMs without direct human evaluation data for all 10 languages
- The decoupled pipeline may introduce rendering artifacts or inconsistencies affecting model performance evaluation
- Performance gaps between languages could reflect dataset construction artifacts rather than true linguistic differences

## Confidence
- High confidence: Benchmark construction methodology and dataset statistics are well-documented and reproducible
- Medium confidence: Observed performance gaps between languages, as these depend on translation quality and may contain confounding factors
- Medium confidence: Effectiveness of fine-tuning, given results lack extensive ablation studies on different training set sizes

## Next Checks
1. Conduct human evaluation of translation quality for a random sample of QA pairs across all 10 languages to verify semantic and linguistic consistency
2. Perform cross-validation experiments where charts are rendered with different code variations to assess impact on model performance
3. Test whether performance improvements from fine-tuning generalize to out-of-distribution chart types not present in the training set