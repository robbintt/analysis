---
ver: rpa2
title: Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier
arxiv_id: '2510.23506'
source_url: https://arxiv.org/abs/2510.23506
tags:
- emotion
- explanation
- emotional
- explanations
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to improve the emotional coherence
  of explanations generated by multimodal LLMs during emotion recognition. The proposed
  approach uses an Emotional Rationale Verifier (ERV) to assess whether generated
  explanations align with the target emotion, and integrates this verifier into a
  reward-driven reinforcement learning framework to guide the model toward more emotionally
  consistent reasoning.
---

# Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier

## Quick Facts
- **arXiv ID:** 2510.23506
- **Source URL:** https://arxiv.org/abs/2510.23506
- **Reference count:** 29
- **Primary result:** ERV improves explanation-emotion alignment by up to 13.7 FCR and 15.4 EEA points while maintaining comparable accuracy

## Executive Summary
This paper addresses the challenge of improving emotional coherence in explanations generated by multimodal large language models during emotion recognition. The authors introduce an Emotional Rationale Verifier (ERV) that evaluates whether generated explanations align with target emotions, then integrate this verifier into a reward-driven reinforcement learning framework. Experiments on MAFW and DFEW datasets demonstrate significant improvements in explanation-emotion consistency while maintaining comparable emotion recognition accuracy.

## Method Summary
The method combines knowledge distillation, sentence-level decomposition, and GRPO-based fine-tuning. First, an ERV is trained via knowledge distillation from GPT-4.1 to RoBERTa-base using pseudo-labeled emotion-description pairs. The model then uses sentence-level decomposition with neutral filtering to compute rewards for explanation-emotion alignment. Finally, GRPO optimizes the policy model with combined rewards for format, answer accuracy, and emotional coherence. The approach is validated on MAFW and DFEW datasets, showing improved faithful consistency rate and explanation emotion accuracy.

## Key Results
- FCR increases from 29.67% to 43.57% (MAFW) and 32.63% to 53.75% (DFEW)
- EEA improves from 32.41% to 46.88% on EMER dataset
- Maintains comparable WAR/UAR to baseline R1-Omni model
- Human evaluation confirms more emotionally grounded and coherent explanations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A lightweight verifier can substitute for a large LLM in judging emotion-explanation alignment.
- **Mechanism:** Knowledge distillation transfers alignment-assessment capability from GPT-4.1 to RoBERTa-base via pseudo-labeled emotion-description pairs. The paper constructs a 23K-sample dataset by generating descriptions from DFEW/MAFW videos using R1-Omni, then asking GPT-4.1 to assign up to two emotion labels per description, with additional augmentation for underrepresented classes.
- **Core assumption:** LLMs possess sufficient linguistic and emotional knowledge to evaluate consistency between emotion and explanation.
- **Evidence anchors:** [Section 3.2]: "we construct a pseudo-labeled dataset consisting of emotional text descriptions paired with emotion labels... This process yields a pseudo-labeled dataset of 20K emotional text–label pairs" [Section 5.2]: "training the ERV solely on the EMER and MERR-fine datasets boosts EEA from 32.41% to 46.88%... Adding the augmented data to ERV further raises EEA + 1.26 pp"
- **Break condition:** If generated descriptions systematically miss emotional nuances present in ground-truth rationales, the verifier learns a distorted alignment function.

### Mechanism 2
- **Claim:** Sentence-level decomposition with neutral filtering improves reward signal fidelity for emotional coherence.
- **Mechanism:** Each explanation is split into sentences; ERV classifies each independently. Sentences classified as NEUTRAL are excluded from reward calculation. The explanation reward is the proportion of non-neutral sentences whose predicted emotions match the ground-truth emotion.
- **Core assumption:** Individual sentences carry interpretable emotional content, and neutral sentences dilute the reward signal without contributing meaningful supervision.
- **Evidence anchors:** [Section 3.3]: "Sentences that merely describe appearance or context can dilute the emotional signal and mislead ERV" [Section 5.2]: "neutral filtering, which excludes emotionally irrelevant sentences, was particularly effective in guiding the model to generate emotion-focused explanations" (ablation shows performance drop when removed)
- **Break condition:** If emotionally relevant content spans multiple sentences or requires cross-sentence context, sentence-level evaluation may misjudge coherence.

### Mechanism 3
- **Claim:** Integrating an explanation-focused reward into GRPO improves explanation-emotion consistency without degrading classification accuracy.
- **Mechanism:** Total reward R = R_E + R_format + R_answer. GRPO optimizes policy π_θ to maximize expected reward while staying close to reference π_ref via KL regularization. The explanation reward provides dense feedback on reasoning quality, not just final answer correctness.
- **Core assumption:** The three rewards are complementary and non-conflicting; improving explanation alignment does not inherently trade off against answer accuracy.
- **Evidence anchors:** [Abstract]: "significantly improves faithful explanation–prediction consistency and explanation emotion accuracy... while maintaining comparable emotion recognition accuracy" [Table 1]: FCR improves from 29.67% to 43.57% (MAFW) and 32.63% to 53.75% (DFEW) while WAR/UAR remain comparable to R1-Omni baseline
- **Break condition:** If explanation and answer rewards provide conflicting gradients, the model may learn to game one reward at the expense of the other.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: The paper builds on GRPO as its RL backbone; understanding how it differs from PPO (group-based advantage estimation, no separate value function) is essential for debugging reward design.
  - Quick check question: Why does GRPO generate G samples per input and compare within-group rather than using a learned value function?

- **Concept: Knowledge Distillation for Reward Models**
  - Why needed here: ERV is a distilled version of LLM judgment; understanding teacher-student training dynamics helps diagnose when the verifier may fail to generalize.
  - Quick check question: What types of alignment judgments might GPT-4.1 make that RoBERTa-base cannot replicate due to capacity limitations?

- **Concept: Multimodal Emotion Recognition Datasets (DFEW, MAFW, EMER)**
  - Why needed here: The method leverages different datasets for different purposes (SFT on EMER, RL on DFEW/MAFW, ERV training on augmented data); knowing their characteristics prevents data leakage and distribution mismatch.
  - Quick check question: Why is EMER suitable for SFT but insufficient for training ERV?

## Architecture Onboarding

- **Component map:** Video V, Audio A, Prompt P → Visual/Audio encoders → Projected features X_v, X_a → Concat(X_v, X_a, X_p) → X_m → Policy Model π_θ → output o → ERV (sentence-level) → emotion logits → Reward Computation (R_format + R_answer + R_E) → GRPO Optimizer → π_θ update
- **Critical path:** 1. SFT on EMER (5 epochs) to learn output format and basic reasoning 2. ERV training on augmented dataset (10 epochs) separately 3. GRPO training on DFEW/MAFW with combined rewards (2 epochs)
- **Design tradeoffs:** Lightweight ERV (RoBERTa-base ~100M params) vs. using LLM directly: Faster inference but potentially lower judgment fidelity; Sentence-level vs. document-level reward: More granular signal but risks missing cross-sentence context; Threshold τ for emotion selection: Higher values are more conservative but may miss valid multi-emotion sentences
- **Failure signatures:** Low EEA with high WAR: Model predicts correctly but explanations don't match → check ERV training quality; High EEA with low WAR: Explanations match ground truth but predictions are wrong → check answer reward weight; EPC drops during GRPO: Model focuses on answer at expense of explanation → increase explanation reward weight
- **First 3 experiments:** 1. Reproduce baseline GRPO (R_format + R_answer only) on DFEW/MAFW split 5 to confirm EEA/FCR gap exists 2. Train ERV on EMER+MERR-fine only (no augmentation) and measure EEA improvement to isolate augmentation contribution 3. Ablate neutral filtering in reward computation to verify sentence-level decomposition benefit

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of ERV-trained models generalize to emotion taxonomies beyond the 7-11 categories tested in MAFW and DFEW (e.g., dimensional emotion models like valence-arousal, or more fine-grained emotion ontologies)?
- **Basis in paper:** [explicit] The authors note that "emotion label distribution is imbalanced" and they had to specifically augment underrepresented categories (disgust, fear, contempt, disappointment, neutral, helplessness) using GPT-4.1.
- **Why unresolved:** The paper evaluates only on datasets with discrete, categorical emotion labels. It does not address whether the ERV framework can handle continuous emotional dimensions or different cultural conceptualizations of emotion.
- **What evidence would resolve it:** Experiments on datasets with dimensional emotion annotations (e.g., valence-arousal ratings) or cross-cultural emotion datasets, demonstrating whether the Explanation Reward transfers effectively.

### Open Question 2
- **Question:** To what extent does noise in GPT-4.1-generated pseudo-labels for ERV training affect the reliability and consistency of the learned reward signal?
- **Basis in paper:** [inferred] Section 3.2 describes constructing a "pseudo-labeled dataset" of 20K pairs using GPT-4.1, but acknowledges R1-Omni "does not always produce perfectly emotion-aligned outputs" despite being "often emotionally plausible." The ERV's judgment capability is explicitly noted as "constrained by the size of its training data."
- **Why unresolved:** The paper does not analyze error propagation from GPT-4.1 labeling errors into ERV training, nor compare against human-annotated rationale-quality labels.
- **What evidence would resolve it:** A controlled study comparing ERV trained on pseudo-labels vs. human-annotated labels, with analysis of label noise impact on downstream FCR/EEA metrics.

### Open Question 3
- **Question:** How dependent are the proposed evaluation metrics (EEA, FCR, EPC) on the specific closed-source LLM used for emotion recognition in explanations, and do they correlate with human judgment across diverse populations?
- **Basis in paper:** [explicit] Section 5.4 validates metrics using "a different model family, Gemini-2.5-Flash" and reports 92.9% human agreement on GPT-4.1-mini predictions. However, the human evaluation (Section 5.3, Appendix A) used only 20 raters on 28 samples.
- **Why unresolved:** The sample size for human validation is limited, and cross-demographic/cultural consistency of emotion interpretation in natural language remains unexplored.
- **What evidence would resolve it:** Large-scale human evaluation across diverse demographic groups, comparing inter-annotator agreement with multiple LLM evaluators, and analysis of systematic divergence patterns.

## Limitations
- No validation of cross-cultural emotional reasoning or multilingual generalization beyond English datasets
- Sentence-level decomposition may miss emotionally relevant context that spans multiple sentences
- Reliance on GPT-4.1 for pseudo-labeling introduces a black-box dependency that may not be replicable in all deployment contexts

## Confidence
- **High Confidence:** The core experimental results showing improved FCR and EEA on MAFW and DFEW datasets are well-supported by the reported metrics and ablation studies
- **Medium Confidence:** The claim that this approach maintains comparable emotion recognition accuracy while improving explanation quality is supported by the data, but the tradeoff analysis could be more comprehensive
- **Low Confidence:** Claims about cross-dataset generalization would benefit from more rigorous validation, as the training and evaluation distributions may overlap significantly

## Next Checks
1. **Cross-Cultural Validation:** Test the ERV on emotion recognition datasets from different cultural contexts (e.g., Japanese, Arabic) to assess whether the pseudo-labeled training data captures universal emotional patterns or introduces cultural bias.

2. **Cross-Sentence Context Analysis:** Design an experiment where emotionally coherent explanations require cross-sentence reasoning, then measure ERV performance with and without contextual window expansion beyond individual sentences.

3. **Alternative Verifier Architectures:** Compare ERV performance against larger verifier models (e.g., BERT-large, DeBERTa) and alternative reward signal formulations (e.g., contrastive learning objectives) to establish whether the RoBERTa-base choice is optimal or merely sufficient.