---
ver: rpa2
title: Am I More Pointwise or Pairwise? Revealing Position Bias in Rubric-Based LLM-as-a-Judge
arxiv_id: '2602.02219'
source_url: https://arxiv.org/abs/2602.02219
tags:
- bias
- position
- score
- evaluation
- rubric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Rubric-based LLM-as-a-judge exhibits position bias, where models
  prefer score options appearing at specific positions in the rubric list. We show
  that this bias is systematic across models and datasets, with models consistently
  favoring the first and last positions.
---

# Am I More Pointwise or Pairwise? Revealing Position Bias in Rubric-Based LLM-as-a-Judge

## Quick Facts
- arXiv ID: 2602.02219
- Source URL: https://arxiv.org/abs/2602.02219
- Reference count: 25
- Key outcome: Rubric-based LLM-as-a-judge exhibits position bias, where models prefer score options appearing at specific positions in the rubric list. We show that this bias is systematic across models and datasets, with models consistently favoring the first and last positions. To address this, we propose balanced permutation—ensuring each score appears equally often at each position—which not only reveals position bias but also improves correlation between LLM scores and human judgments. We further introduce a Bias Cost metric to select rubric orderings that minimize positional deviation. Our results suggest that rubric-based evaluation is not inherently pointwise and that simple permutation-based calibration can substantially improve its reliability.

## Executive Summary
This paper reveals that rubric-based LLM-as-a-judge evaluation exhibits systematic position bias, where models prefer scores at specific positions in the rubric list (primarily first and last) rather than evaluating based on semantic alignment. The authors show this bias is consistent across multiple models and datasets, suggesting rubric-based evaluation implicitly functions as a multiple-choice selection problem. To address this, they propose balanced permutation—generating 10 complementary orderings through cyclic rotations—which neutralizes position bias by ensuring each score appears equally often at each position. The method improves correlation with human judgments by 0.014 to 0.089 Spearman points across experiments. Additionally, they introduce a Bias Cost metric to identify optimal rubric orderings that minimize positional deviation without requiring human annotations.

## Method Summary
The method involves generating 10 balanced permutations of rubric orderings (5 forward cyclic rotations and 5 reverse cyclic rotations of [1,2,3,4,5]) to neutralize position bias. For each evaluation instance, the LLM is run 10 times with different rubric orderings, and scores are averaged to produce the final calibrated score. The Bias Cost metric is computed by running small-scale probing evaluations with varied orderings, measuring the probability of selecting each score at each position, and selecting the ordering with minimal deviation from uniform distribution (20% per position). The approach is tested across four datasets (MT-Bench, Vicuna-Bench, HANNA, SummEval) with five different judge models (GPT-4.1, GPT-4.1-mini, Qwen3-8B, Qwen3-32B, GPT-OSS-120B).

## Key Results
- Models exhibit primacy and recency biases, selecting score options at the beginning and end of rubric lists more frequently than middle positions
- Balanced permutation improves Spearman correlation with human judgments by 0.014 to 0.089 points across most model-dataset combinations
- The Bias Cost metric successfully identifies rubric orderings that rank in the top 50% of correlation performance in most cases
- Qwen3-8B uniquely showed degraded performance with permutation, indicating model-specific interactions
- Position bias appears to stem from architectural properties of transformer attention rather than task-specific learning

## Why This Works (Mechanism)

### Mechanism 1
Rubric-based evaluation implicitly functions as a multiple-choice selection problem, causing LLMs to exhibit systematic position bias. When presented with ordered rubric options, LLMs prefer scores at specific positions (primarily first and last) rather than evaluating based solely on semantic alignment. This resembles the "Lost in the Middle" phenomenon where attention mechanisms weight sequence positions unevenly. Position bias stems from architectural properties of transformer attention patterns rather than task-specific learning. Evidence shows models exhibit primacy and recency biases, with LLMs selecting score options at the beginning and end of the list more frequently. If a model shows uniform selection distribution (~20% per position for 5-option rubrics) across balanced permutations, position bias is not present.

### Mechanism 2
Balanced permutation neutralizes position bias by ensuring each score appears equally often at each position across evaluation runs. Construct 10 complementary orderings (5 forward cyclic rotations + 5 reverse cyclic rotations). Each score appears exactly twice per position. Averaging scores across permutations marginalizes out positional preference while preserving semantic signal. Position effects are additive and cancel out when uniformly distributed across positions. The permutation approach outperforms the repeat baseline, indicating that diversifying rubric orderings can effectively mitigate the impact of position bias. If permutation averaging increases variance without improving correlation metrics, the additive assumption may not hold for that model-dataset pair.

### Mechanism 3
A "Bias Cost" metric computed from probing evaluations can identify rubric orderings that minimize positional deviation without requiring human annotations. Run small-scale probing with varied orderings, compute P(p|s)—the probability that score s is selected at position p. Define Cost(π) = Σ|P(p|πp) - 0.2|. Lower cost indicates ordering with more uniform selection distribution. Lower bias cost correlates with higher evaluation quality (human correlation). In most cases, the ordering with the lowest Bias Cost ranks within the top 50% in terms of correlation. If low-cost orderings consistently rank poorly in correlation, the assumption that bias minimization improves quality is falsified.

## Foundational Learning

- **LLM-as-a-Judge evaluation paradigms** (pointwise, pairwise, rubric-based): Understanding that rubric-based evaluation occupies a middle ground—semantically pointwise but structurally multiple-choice—explains why position bias emerges. Quick check: Given a single response to evaluate with 5 rubric options, would you classify this as pointwise or pairwise evaluation? (Answer: Rubric-based is pointwise in content but multiple-choice in structure.)

- **Primacy and recency bias in sequence processing**: Explains the U-shaped selection pattern (positions 1, 2, and 5 preferred) observed across all models. Quick check: If an LLM selects position 1 at 35% frequency and position 3 at 12% frequency for a 5-option rubric, what type of bias is this? (Answer: Primacy bias / position bias with primacy effect.)

- **Correlation metrics for evaluation alignment**: Pearson's r and Spearman's ρ quantify how well LLM scores align with human judgments, enabling comparison of bias mitigation strategies. Quick check: If permutation increases Spearman correlation from 0.22 to 0.31, is this improvement meaningful? (Answer: +0.089 improvement is substantial relative to baseline; see Qwen3-32B results in Table 2.)

## Architecture Onboarding

- **Component map**:
```
Input: {instruction, response, rubric_list}
         ↓
   Rubric Permuter → Generates 10 balanced orderings
         ↓
   LLM Judge (×10) → Produces scores per ordering
         ↓
   Score Aggregator → Averages across permutations
         ↓
Output: Calibrated score + variance estimate
```

- **Critical path**:
1. Define rubric options with clear score-to-description mapping
2. Generate 10 balanced permutations (forward + reverse cyclic rotations)
3. Run LLM evaluation for each permutation (parallelize if possible)
4. Aggregate scores via averaging; compute within-item standard deviation
5. Optionally compute Bias Cost to select single best ordering if compute-constrained

- **Design tradeoffs**:
- **Permutation vs. Repeat**: 10 permutations cost 10× inference but improve correlation (+0.014 to +0.089 Spearman in experiments). Use repeat (fixed ordering) only if compute is severely constrained.
- **Temperature settings**: Reasoning models may require temperature > 0 to avoid repetitive loops (Qwen3-Think used 0.3), increasing variance.
- **Model selection**: Larger models (GPT-4.1, OSS-120B) show weaker position bias but don't always outperform smaller models after calibration. Qwen3-8B uniquely worsened with permutation—test your specific model.

- **Failure signatures**:
- **Permutation hurts performance**: Observed for Qwen3-8B. Indicates the default ordering may already align well with the evaluation task; check default ordering rank in Table 4.
- **High variance across permutations**: σ > 0.5 suggests model is highly sensitive to rubric ordering; may need more permutations or model change.
- **Uniform selection distribution but low correlation**: Position bias is not the only issue; consider rubric quality or model capability.

- **First 3 experiments**:
1. **Probe position bias**: Run balanced permutation on 50-100 samples. Plot selection frequency by position. Expect U-shape if bias exists.
2. **Correlation comparison**: Compare permutation-averaged scores vs. single-ordering scores against human annotations (if available) or gold-standard labels.
3. **Bias Cost validation**: Compute Bias Cost for all 10 orderings, select minimum, and verify it ranks above median in correlation. If not, the assumption is violated for your model-dataset pair.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does permuting rubric orderings during the training phase improve the stability of models utilizing rubrics as reward signals? The authors hypothesize that "permuting the order of training rubrics may also improve training stability" for models using rubrics as training signals. This is proposed as a future application but is not empirically validated in the current study. What evidence would resolve it: Comparative experiments training models (e.g., via RLHF) using fixed vs. permuted rubric orders and measuring training stability and final performance.

- **Open Question 2**: Can specific rubric orderings be utilized to reliably detect whether examinees are using LLMs to complete assessments? The paper proposes that the method could detect LLM reliance if "a user's final answers are biased toward such outputs" generated using specific rubric manipulations. This is proposed as a future application but is not empirically validated in the current study. What evidence would resolve it: A study correlating the position bias distribution of student submissions against the known bias distribution of specific LLM/rubric configurations.

- **Open Question 3**: How does the effectiveness of balanced permutation strategies vary across datasets with substantially different ground-truth score distributions? Section 7 lists evaluating on "more diverse datasets... especially those with substantially different score distributions" as a necessary step for future work. The experiments were limited to four datasets, and the authors suggest the interaction between bias mitigation and score distribution requires broader validation. What evidence would resolve it: Bench-marking the proposed Bias Cost and permutation methods on datasets with controlled, non-standard score distributions (e.g., highly skewed).

## Limitations

- Position bias findings are based on specific model families (GPT-4.1, Qwen3 series, OSS-120B) and controlled 5-point rubric settings; generalizability to other architectures and longer rubrics remains uncertain
- While balanced permutation shows consistent improvement across most model-dataset combinations, Qwen3-8B exhibited degraded performance, suggesting model-specific interactions not fully explained
- The architectural explanation for position bias (transformer attention patterns) is plausible but not definitively proven; alternative explanations cannot be ruled out

## Confidence

- **High Confidence**: The existence of position bias (primacy/recency effects) and the effectiveness of balanced permutation for correlation improvement are well-supported across multiple datasets and models. The Bias Cost metric reliably identifies better-performing orderings in most cases.
- **Medium Confidence**: The architectural explanation for position bias (transformer attention patterns) is plausible but not definitively proven. The assumption that position effects are additive and cancel out uniformly may not hold for all model-dataset pairs.
- **Low Confidence**: The claim that position bias stems from rubric-based evaluation's implicit multiple-choice structure is correlational rather than causal. Alternative explanations (prompt sensitivity, output distribution bias) cannot be ruled out with current evidence.

## Next Checks

1. **Cross-Architecture Validation**: Test balanced permutation on Claude, Gemini, and open-source models outside the Qwen3 family to determine if position bias patterns hold universally or are model-specific.

2. **Longer Rubric Analysis**: Extend experiments to 7-point and 10-point rubrics to assess whether position bias scales with rubric length and whether the same permutation strategy remains effective.

3. **Ablation on Reference Inclusion**: Systematically test the impact of including/excluding reference answers in rubric-based prompts to isolate whether position bias is triggered by the multiple-choice-like presentation or the semantic content itself.