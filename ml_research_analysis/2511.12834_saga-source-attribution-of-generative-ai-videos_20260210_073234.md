---
ver: rpa2
title: 'SAGA: Source Attribution of Generative AI Videos'
arxiv_id: '2511.12834'
source_url: https://arxiv.org/abs/2511.12834
tags:
- attribution
- video
- generators
- source
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SAGA, the first large-scale framework for
  source attribution of AI-generated videos. Unlike binary real/fake detection, SAGA
  identifies the specific generative model used, providing attribution across five
  granular levels: authenticity, generation task (T2V/I2V), model version, development
  team, and precise generator.'
---

# SAGA: Source Attribution of Generative AI Videos

## Quick Facts
- **arXiv ID:** 2511.12834
- **Source URL:** https://arxiv.org/abs/2511.12834
- **Reference count:** 40
- **Primary result:** First framework for multi-granular source attribution of AI-generated videos, identifying specific generative models across five levels with 97.41% accuracy on fine-grained attribution task.

## Executive Summary
This paper introduces SAGA, the first large-scale framework for source attribution of AI-generated videos. Unlike binary real/fake detection, SAGA identifies the specific generative model used, providing attribution across five granular levels: authenticity, generation task (T2V/I2V), model version, development team, and precise generator. The method uses a novel video transformer architecture initialized with vision foundation model features to capture spatio-temporal artifacts, combined with a data-efficient pretrain-and-attribute strategy using only 0.5% labeled data per class. SAGA also introduces Temporal Attention Signatures (T-Sigs), a novel interpretability method that visualizes learned temporal differences, offering the first explanation for why different video generators are distinguishable.

## Method Summary
SAGA employs a two-stage training approach: Stage 1 pretrains a binary Real/Fake classifier on full data, then Stage 2 adapts to multi-class attribution using only 0.5% labeled data per generator class. The architecture consists of a frozen foundation vision encoder (ViT-L/14), a spatial transformer block (1 block) that processes frame-level tokens, and a temporal encoder with multi-head self-attention across frames. Training uses a combined Cross-Entropy and Hard Negative Mining loss, where HNM explicitly selects the hardest negative samples to separate overlapping generator distributions.

## Key Results
- Achieves 97.41% accuracy on the most fine-grained attribution task (identifying specific generator)
- Demonstrates strong cross-domain robustness: 95.39% accuracy when trained on DeMamba and tested on DVF
- Shows 94.99% accuracy with Hard Negative Mining versus 70.31% with semi-hard mining alone
- Achieves competitive performance using only 0.5% of source-labeled data per class

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Spatio-Temporal Encoding
The two-stage transformer architecture captures generator-specific temporal inconsistencies that single-frame analysis misses. Frame-level tokens first pass through a spatial encoder (intra-frame self-attention), producing one feature vector per frame. These L frame vectors then flow through a temporal encoder with positional encoding and stacked MHSA blocks, modeling inter-frame dependencies. The penultimate block's attention scores become T-Sigs.

### Mechanism 2: Hard Negative Mining for Overlapping Generator Clusters
HNM loss forces separation between generators with highly similar outputs, where standard cross-entropy and semi-hard mining fail. Standard triplet loss pulls positive pairs together and pushes negatives away by margin α. Semi-hard mining only uses negatives farther than positive but within margin. HNM selects the closest negative to the anchor, directly maximizing the minimum inter-class margin.

### Mechanism 3: Foundation Model Initialization for Domain Robustness
Initializing with features from a web-scale pretrained vision encoder mitigates domain gap when deploying on in-the-wild videos. Freeze a foundational vision encoder (trained on image-text data) to extract domain-agnostic token embeddings zm for each frame. Stack these temporally as ζk before the trainable video transformer.

## Foundational Learning

- **Concept:** Multi-Head Self-Attention (MHSA)
  - **Why needed here:** Core building block for both spatial and temporal encoders; enables modeling relationships between patches within frames and between frames across time.
  - **Quick check question:** Can you explain why MHSA with 12 heads might capture different types of dependencies (e.g., motion direction vs. texture consistency) than a single attention head?

- **Concept:** Triplet Loss with Margin Mining
  - **Why needed here:** Drives the contrastive adaptation stage; understanding the difference between semi-hard and hard negative selection is critical for debugging low-data attribution.
  - **Quick check question:** If your model achieves 80% accuracy with semi-hard mining but 95% with HNM, what does this suggest about your class distributions?

- **Concept:** Transfer Learning / Domain Adaptation
  - **Why needed here:** The two-stage training (binary pretrain → multi-class adapt) is a form of transfer; the frozen foundation encoder is another. Recognizing when features transfer vs. when they don't is essential.
  - **Quick check question:** If cross-domain accuracy drops 30% from in-domain, would you first suspect the foundation features, the temporal encoder, or the adaptation strategy?

## Architecture Onboarding

- **Component map:**
  Input Video (L frames) → Frozen Foundation Vision Encoder → zm per frame → Stack temporally → Spatial Encoder → Positional encoding → Temporal Encoder → Classifier Head → T-Sigs

- **Critical path:** Foundation encoder outputs → Spatial encoder pooling → Positional encoding → Temporal encoder depth → HNM loss (during Stage-2). Errors in frame stacking or positional encoding will corrupt temporal patterns; incorrect negative sampling in HNM will waste gradient signal.

- **Design tradeoffs:**
  - Frozen vs. fine-tuned foundation encoder: Freezing enhances generalization but may underfit generator-specific spatial traces
  - Number of temporal encoder blocks (D): More blocks capture longer-range dependencies but increase compute and overfitting risk with limited data
  - HNM margin α: Too small → insufficient separation; too large → gradient signal weakens as most negatives become "easy"

- **Failure signatures:**
  - Accuracy near random → check if labels are correct, learning rate too high, or embeddings collapsed to a single point
  - High binary accuracy but near-zero fine-grained accuracy → Stage-1 pretrain succeeded but adaptation failed
  - Strong in-domain but <60% cross-domain → foundation features may not transfer
  - T-Sigs look identical across generators → temporal encoder may not be learning

- **First 3 experiments:**
  1. Train Stage-1 binary classifier on full data, evaluate on held-out generators. If <90% accuracy, debug data pipeline or increase model capacity.
  2. Ablate HNM: Run Stage-2 with (a) CE-only, (b) CE + semi-hard mining, (c) CE + HNM. Compare GEN-L accuracy and visualize t-SNE.
  3. Data efficiency curve: Vary Stage-2 labeled data from 0.1% to 10%. Plot attribution accuracy to identify the knee point.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can Temporal Attention Signatures (T-Sigs) be formalized to detect and reject unseen generators in an open-set scenario?
- **Open Question 2:** To what extent does heavy video compression degrade SAGA's ability to capture temporal artifacts?
- **Open Question 3:** Is the attribution model robust to adversarial perturbations designed to spoof the source identity?
- **Open Question 4:** Does the method's reliance on temporal inconsistencies limit its effectiveness against state-of-the-art generators with high motion coherence?

## Limitations

- Limited evaluation against adversarial attacks despite motivation from developing robust countermeasures
- Performance degradation on state-of-the-art generators (e.g., Sora at 66-73% accuracy) suggests temporal artifacts may disappear as generation technology improves
- Cross-domain generalization claims rely on specific foundation model without testing sensitivity to alternative models

## Confidence

- **High:** Hierarchical spatio-temporal encoding mechanism effectiveness
- **Medium:** Hard negative mining necessity
- **Medium:** Foundation model initialization benefits
- **Low:** T-Sigs interpretability claims

## Next Checks

1. **Replication audit:** Implement SAGA from paper specifications, focusing on exact HNM implementation. Verify if CE-only baseline truly achieves ~70% or if implementation details explain performance gaps.
2. **Foundation model sensitivity:** Replace frozen foundation encoder with alternative (e.g., DINOv2 instead of ViT-L/14). Test if cross-domain performance degrades significantly, indicating over-reliance on specific model.
3. **Temporal encoder depth analysis:** Systematically vary Temporal Encoder block count D from 2 to 12. Plot attribution accuracy vs. computational cost to identify optimal trade-off point.