---
ver: rpa2
title: 'FusWay: Multimodal hybrid fusion approach. Application to Railway Defect Detection'
arxiv_id: '2509.06987'
source_url: https://arxiv.org/abs/2509.06987
tags:
- detection
- defect
- fusion
- audio
- rail
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FusWay is a multimodal hybrid fusion system for railway defect
  detection that combines YOLOv8n object detection with Vision Transformer (ViT) to
  integrate image features and synthesized audio signals. The method extracts features
  from YOLO layers 7, 16, and 19, synthesizes audio representations based on domain
  knowledge (e.g., rail rupture as high-amplitude impulses), and fuses them using
  an upstream fusion module before final classification by ViT.
---

# FusWay: Multimodal hybrid fusion approach. Application to Railway Defect Detection

## Quick Facts
- arXiv ID: 2509.06987
- Source URL: https://arxiv.org/abs/2509.06987
- Authors: Alexey Zhukov; Jenny Benois-Pineau; Amira Youssef; Akka Zemmari; Mohamed Mosbah; Virginie Taillandier
- Reference count: 26
- Primary result: FusWay achieves 0.2-point improvement in precision and accuracy over vision-only methods for railway defect detection

## Executive Summary
FusWay presents a multimodal hybrid fusion system for railway defect detection that combines YOLOv8n object detection with Vision Transformer (ViT) to integrate image features and synthesized audio signals. The method extracts features from YOLO layers 7, 16, and 19, synthesizes audio representations based on domain knowledge, and fuses them using an upstream fusion module before final classification by ViT. Tested on a real-world railway dataset with two defect classes, FusWay achieved a 0.2-point improvement in precision and overall accuracy compared to vision-only approaches, with statistical significance confirmed for stricter IoU thresholds.

## Method Summary
FusWay is a multimodal hybrid fusion system that combines YOLOv8n object detection with Vision Transformer (ViT) to integrate image features and synthesized audio signals. The architecture extracts features from YOLO layers 7, 16, and 19, synthesizes audio representations based on domain knowledge about rail defects, and fuses them using an upstream fusion module before final classification by ViT. The system was tested on a real-world railway dataset with two defect classes (rail rupture and surface defect), achieving improved precision and accuracy over vision-only approaches.

## Key Results
- Achieved 0.2-point improvement in precision and accuracy over vision-only methods
- Statistical significance confirmed for stricter IoU thresholds with accuracy increases of 26.65% for rail rupture and 8.88% for surface defect detection
- Demonstrated modular design flexibility to integrate additional sensor modalities

## Why This Works (Mechanism)
FusWay works by combining complementary information from visual and audio modalities through a hybrid fusion architecture. The YOLOv8n detector provides precise spatial localization of defects in images, while the synthesized audio features capture temporal patterns and acoustic signatures of different defect types. The upstream fusion module effectively combines these heterogeneous feature representations before the ViT classifier makes final decisions. This multimodal approach leverages the strengths of both visual and acoustic information, allowing the system to detect defects that might be missed by either modality alone.

## Foundational Learning
- **Multimodal fusion** - why needed: To combine complementary information from different sensor types; quick check: Verify features from each modality maintain integrity after fusion
- **Hybrid detection architecture** - why needed: To leverage strengths of both YOLO (localization) and ViT (classification); quick check: Ensure proper feature alignment between detection and fusion stages
- **Audio feature synthesis** - why needed: To create acoustic representations when real audio data is restricted; quick check: Validate synthesized features correlate with actual defect patterns
- **Upstream fusion** - why needed: To integrate features before final classification for better decision-making; quick check: Confirm fusion preserves critical information from both modalities
- **Rail defect characteristics** - why needed: To understand what visual and acoustic patterns indicate different defect types; quick check: Validate domain knowledge aligns with detected patterns
- **Intersection over Union (IoU)** - why needed: To measure detection accuracy and determine statistical significance; quick check: Verify IoU calculations match evaluation metrics

## Architecture Onboarding

**Component map**: Image capture -> YOLOv8n detector -> Feature extraction (layers 7, 16, 19) -> Audio synthesis module -> Upstream fusion -> ViT classifier -> Output

**Critical path**: The most critical path runs from YOLO feature extraction through upstream fusion to ViT classification, as this sequence determines the final detection accuracy. The audio synthesis quality directly impacts fusion effectiveness.

**Design tradeoffs**: The system trades off between the precision of YOLO localization and the contextual understanding of ViT, while using synthesized rather than real audio to avoid industrial restrictions. The modular design sacrifices some integration efficiency for flexibility in adding new sensors.

**Failure signatures**: Performance degradation may occur when visual and acoustic patterns are misaligned, when synthesized audio poorly represents actual defect signatures, or when the fusion module cannot effectively combine highly dissimilar feature representations.

**3 first experiments**:
1. Evaluate individual YOLO layers (7, 16, 19) to determine optimal feature extraction points
2. Test fusion performance with varying audio feature synthesis parameters
3. Compare ViT classification accuracy with and without audio features

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does FusWay's performance compare when using real, raw audio streams instead of domain-knowledge-based synthesized features?
- Basis in paper: The authors state they "synthesised audio features which come from audio detector" because real-world audio data could not be used due to "industrial property rights restrictions."
- Why unresolved: It is unclear if the statistical modeling of audio (using uniform distribution) accurately captures the complexity of real-world acoustic noise and defect signatures.
- Evidence to resolve it: Comparative evaluation of the trained model on the restricted real-world audio dataset versus the synthesized dataset.

### Open Question 2
- Question: Can the upstream fusion module effectively integrate sensor modalities with different temporal resolutions or data structures, such as vibration or ultrasonic sensors?
- Basis in paper: The conclusion claims the "modular design of FusWay offers the flexibility to integrate additional primary detectors and sensor modalities."
- Why unresolved: The current study validates the architecture strictly on image and synthesized audio features, leaving the integration of other physical sensor types untested.
- Evidence to resolve it: Implementation and performance analysis of the fusion module using heterogeneous inputs like 1D vibration signals or ultrasonic data.

### Open Question 3
- Question: To what extent does the severe class imbalance of "Surface defect" samples limit the convergence and accuracy of the Vision Transformer (ViT) in this architecture?
- Basis in paper: The authors attribute the lowest metric values for "Surface defect" to its "low representation in the overall recordings" (Table 1), but did not test mitigation strategies.
- Why unresolved: It remains unknown whether the lower performance is an inherent limitation of the fusion method for this defect type or simply a result of insufficient training data.
- Evidence to resolve it: Ablation studies applying oversampling or augmentation techniques specifically to the "Surface defect" class to measure performance recovery.

## Limitations
- The synthesized audio approach may not capture real-world acoustic complexity accurately
- Performance gains were demonstrated on only two defect classes, limiting generalizability
- Statistical significance testing was limited to stricter IoU thresholds

## Confidence
- Multimodal fusion benefits: Medium - improvements shown on single dataset with limited defect types
- Modular design flexibility: Low - theoretical claim without empirical validation with additional sensors
- Synthesized audio effectiveness: Low - depends heavily on domain knowledge accuracy

## Next Checks
1. Test FusWay on datasets with more diverse defect types and varying environmental conditions to assess robustness
2. Conduct ablation studies to quantify the specific contribution of audio features versus visual features in different defect detection scenarios
3. Evaluate the system's performance with additional sensor modalities beyond image and audio to validate the claimed modularity benefits