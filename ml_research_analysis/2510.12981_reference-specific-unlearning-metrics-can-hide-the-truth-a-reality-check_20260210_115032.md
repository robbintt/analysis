---
ver: rpa2
title: 'Reference-Specific Unlearning Metrics Can Hide the Truth: A Reality Check'
arxiv_id: '2510.12981'
source_url: https://arxiv.org/abs/2510.12981
tags:
- unlearning
- fade
- author
- name
- full
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper exposes fundamental flaws in current machine unlearning
  evaluation methods, which rely on reference-specific metrics that can hide unlearning
  failures. The authors propose FADE (Functional Alignment for Distributional Equivalence),
  a novel metric that measures distributional similarity between unlearned and retain-only
  models by comparing bidirectional likelihood assignments over generated samples.
---

# Reference-Specific Unlearning Metrics Can Hide the Truth: A Reality Check

## Quick Facts
- arXiv ID: 2510.12981
- Source URL: https://arxiv.org/abs/2510.12981
- Reference count: 27
- Primary result: Current machine unlearning evaluation methods fail to detect knowledge retention, while FADE provides a more reliable metric for distributional equivalence.

## Executive Summary
This paper exposes fundamental flaws in current machine unlearning evaluation methods, which rely on reference-specific metrics that can hide unlearning failures. The authors propose FADE (Functional Alignment for Distributional Equivalence), a novel metric that measures distributional similarity between unlearned and retain-only models by comparing bidirectional likelihood assignments over generated samples. Experiments on the TOFU benchmark for LLM unlearning and UnlearnCanvas for text-to-image diffusion model unlearning reveal that methods achieving near-optimal scores on traditional metrics fail to achieve distributional equivalence, with many becoming more distant from the gold standard than before unlearning. FADE provides a more principled foundation for assessing genuine unlearning by evaluating functional alignment across entire output distributions rather than specific reference outputs.

## Method Summary
The paper introduces FADE, a metric for evaluating machine unlearning that measures distributional similarity between unlearned models and retain-only models (trained only on D_retain). FADE computes bidirectional Monte Carlo estimates of log-likelihood ratios over generated samples: E_x~p_retain[log p_retain(x|c)/p_unlearned(x|c)] + E_x~p_unlearned[log p_unlearned(x|c)/p_retain(x|c)]. The authors evaluate this on the TOFU benchmark (200 fictitious authors, 20 Q&A pairs each) for LLMs and UnlearnCanvas benchmark (50 artistic styles) for diffusion models. They compare FADE against traditional forget quality metrics across multiple unlearning methods including GA, GD, DPO, NPO, and IHL for LLMs, and EDiff, ESD, SalUn, and SHS for diffusion models.

## Key Results
- Traditional forget quality metrics show near-optimal scores while models retain unwanted knowledge, with performance dropping significantly when switching from paraphrased to original reference answers
- No unlearning method tested reduces FADE to levels comparable to the baseline established between independently trained retain-only models
- Many unlearning methods increase distributional distance from the retain-only oracle compared to before unlearning, suggesting they optimize the wrong objective
- Retain-only models produce consistent alternative behaviors when prompted for unknown content, while unlearned models produce divergent behaviors that signal tampering

## Why This Works (Mechanism)

### Mechanism 1: Reference-Specific Metrics Create Systematic Blind Spots
- Claim: Current unlearning metrics can show near-optimal scores while models retain unwanted knowledge.
- Mechanism: Metrics like TOFU's forget quality measure performance against a fixed set of reference answers (e.g., paraphrased responses). Models can optimize specifically for those references while failing to generalize unlearning behavior, leaving knowledge accessible through alternative phrasings or prompts.
- Core assumption: That unlearning success on reference outputs implies unlearning across the full output distribution.
- Evidence anchors:
  - [abstract] "This reference-specific approach creates systematic blind spots, allowing models to appear successful while retaining unwanted knowledge accessible through alternative prompts or attacks."
  - [Section 3.3] Figure 2 demonstrates forget quality dropping from -5.03 to -31.05 when switching from paraphrased to original answers, revealing hidden failures.
  - [corpus] Related work "Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods" corroborates that prompt-based attacks expose retained knowledge.
- Break condition: If reference sets become sufficiently exhaustive and diverse to cover all semantic equivalents, this blind spot could diminish—but the paper argues this is impractical.

### Mechanism 2: FADE Captures Distributional Equivalence via Bidirectional Likelihood
- Claim: FADE measures whether unlearned models genuinely match retain-only oracle behavior across the full output distribution.
- Mechanism: FADE computes a bidirectional Monte Carlo estimate: sampling outputs from each model and measuring how well the other model assigns likelihood to those samples. The symmetric formulation (both directions) ensures it captures distributional misalignment regardless of which direction diverges.
- Core assumption: That bidirectional likelihood ratios over generated samples serve as a sufficient proxy for full distributional equivalence.
- Evidence anchors:
  - [abstract] "FADE measures distributional similarity between unlearned and retain-only models by comparing bidirectional likelihood assignments over generated samples."
  - [Section 4.1] FADE formula explicitly defined with bidirectional expectations.
  - [corpus] Weak direct corpus evidence for this specific formulation; FADE appears novel to this work.
- Break condition: If output spaces are too large for adequate sampling coverage, or if likelihood estimation is unreliable (as in diffusion models where approximation is required).

### Mechanism 3: FADE Reveals Counter-Intuitive Unlearning Failures
- Claim: Many unlearning methods increase distributional distance from the retain-only oracle compared to before unlearning.
- Mechanism: Current methods optimize to suppress specific outputs (causing repetitive tokens, refusal responses, or style-neutral outputs) rather than learning to "not know" in a natural way. Retain-only models, when prompted for unknown content, produce consistent alternative outputs—but unlearned models produce divergent behaviors that signal tampering.
- Core assumption: That retain-only models trained without unwanted data exhibit stable, reproducible "unknowing" behaviors.
- Evidence anchors:
  - [Section 5.1] "No unlearning method reduces FADE to levels comparable to the baseline established between retain-only models."
  - [Section 3.4, Figure 4] Retain models produce consistent alternative styles; unlearned models produce style-neutral outputs that differ fundamentally.
  - [corpus] "The Illusion of Unlearning" (George et al., 2025, cited in paper) documents similar instabilities.
- Break condition: If unlearning objectives were redesigned to explicitly target distributional alignment rather than output suppression.

## Foundational Learning

- **KL Divergence and Distributional Distance**
  - Why needed here: FADE's bidirectional likelihood formulation is conceptually related to KL divergence properties—it's non-negative, unbounded, and approaches zero when distributions align.
  - Quick check question: Can you explain why comparing likelihood ratios in both directions captures distributional differences more robustly than a single-direction comparison?

- **Machine Unlearning Formalization**
  - Why needed here: The paper frames unlearning as achieving functional equivalence between f_unlearn and f_retain (trained only on D_retain), not merely suppressing outputs on D_forget.
  - Quick check question: What is the difference between a model that has "forgotten" information versus one that has learned to avoid producing specific outputs?

- **Variational Bounds in Diffusion Models**
  - Why needed here: FADE for diffusion models requires approximating log-likelihood differences via denoising loss differences, relying on variational bound assumptions.
  - Quick check question: Why can't we compute exact likelihoods for diffusion models, and what assumption does FADE make about the variational gap between two models?

## Architecture Onboarding

- **Component map:**
  1. Sample generation module -> Likelihood estimation module -> FADE aggregation module
  2. (Optional) Baseline calibration module

- **Critical path:**
  1. Prepare retain-only model (trained from scratch on D_retain only).
  2. For each prompt in evaluation set: generate N samples from each model, compute cross-likelihoods, accumulate bidirectional ratios.
  3. Compare FADE score to baseline; values far above baseline indicate distributional misalignment.

- **Design tradeoffs:**
  - **Sample count vs. accuracy**: Paper uses 100 samples per query; fewer samples increase variance but reduce compute.
  - **Ancestral vs. advanced decoding**: Paper deliberately avoids beam/nucleus sampling to preserve unbiased output distribution estimates.
  - **Diffusion likelihood approximation**: Exact likelihood is intractable; approximation assumes similar variational gaps between models—a potential failure mode if violated.

- **Failure signatures:**
  - Unlearned model generates repetitive tokens (GA, GD methods) → high FADE.
  - Unlearned model outputs refusal responses not seen in retain model → high FADE.
  - Unlearned model generates style-neutral images when retain model generates consistent alternative styles → high FADE.
  - FADE increases with unlearning epochs (methods optimizing wrong objective) → fundamental objective mismatch.

- **First 3 experiments:**
  1. **Reproduce forget quality sensitivity**: Implement TOFU forget quality metric with both paraphrased and original reference answers on a small unlearning run (1% forget set) to observe the discrepancy described in Section 3.3.
  2. **Establish FADE baseline**: Train multiple retain-only models with different seeds; compute FADE between them to calibrate the noise floor for your specific architecture and dataset.
  3. **Single-method FADE trajectory**: Apply one unlearning method (e.g., DPO) across multiple epochs, plotting both forget quality and FADE to observe whether they diverge as the paper predicts.

## Open Questions the Paper Calls Out
None

## Limitations
- The distributional equivalence assumption may not fully capture all aspects of successful unlearning—models could achieve low FADE while still retaining extractable information through gradient-based attacks or fine-tuning on small datasets.
- FADE's effectiveness for diffusion models depends on the validity of the variational bound approximation, which could break down if the denoising objectives differ significantly between models.
- The retain-only baseline comparison assumes that stochastic training noise is the only source of distributional variation, but architecture-specific behaviors or data ordering effects could create additional variance.

## Confidence

**High confidence**: FADE reveals systematic failures in current reference-based metrics, as demonstrated by the forget quality discrepancy between original and paraphrased references.

**Medium confidence**: FADE provides a more principled evaluation foundation, though its relationship to actual unlearning success (information removal vs. behavioral modification) requires further validation.

**Low confidence**: The claim that unlearning methods consistently increase distributional distance from retain models—while supported by the presented experiments, broader validation across diverse methods and datasets is needed.

## Next Checks

1. Test FADE's sensitivity to gradient-based extraction attacks: apply membership inference or fine-tuning attacks to high-FADE and low-FADE models to determine if FADE correlates with actual information retention.

2. Validate FADE's diffusion model approximation: train models with known variational gap differences and verify whether FADE correctly identifies these as distributional misalignments.

3. Evaluate FADE across diverse unlearning objectives: test methods specifically designed to optimize distributional alignment (not just output suppression) to determine if FADE scores improve as predicted.