---
ver: rpa2
title: 'Omni-RRM: Advancing Omni Reward Modeling via Automatic Rubric-Grounded Preference
  Synthesis'
arxiv_id: '2602.00846'
source_url: https://arxiv.org/abs/2602.00846
tags:
- reward
- preference
- arxiv
- score
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the lack of effective multimodal reward models
  (RMs) by introducing Omni-RRM, the first open-source rubric-grounded reward model
  that generates structured, multi-dimension preference judgments with justifications
  across text, image, video, and audio. The core innovation is Omni-Preference, a
  fully automated dataset created by contrasting outputs from strong and weak models
  and using teacher models to reconcile and filter preferences while providing rubric-grounded
  rationales.
---

# Omni-RRM: Advancing Omni Reward Modeling via Automatic Rubric-Ground Preference Synthesis

## Quick Facts
- arXiv ID: 2602.00846
- Source URL: https://arxiv.org/abs/2602.00846
- Reference count: 40
- Primary result: First open-source rubric-grounded reward model achieving SOTA on video/audio benchmarks and substantial gains on image tasks

## Executive Summary
Omni-RRM introduces the first open-source rubric-grounded reward model capable of generating structured, multi-dimension preference judgments with justifications across text, image, video, and audio modalities. The core innovation is Omni-Preference, a fully automated dataset created by contrasting outputs from strong and weak models, using teacher models to reconcile and filter preferences while providing rubric-grounded rationales. The model is trained in two stages: supervised fine-tuning to learn the rubric-grounded output structure, followed by reinforcement learning (GRPO) to improve discrimination on low-margin pairs. Comprehensive evaluations show Omni-RRM achieves state-of-the-art accuracy on video (80.2% on ShareGPT-V) and audio (66.8% on Audio-HH-RLHF) benchmarks, substantially outperforms existing open-source RMs on image tasks (17.7% absolute gain), and improves downstream performance via Best-of-N selection while transferring to text-only preference tasks.

## Method Summary
Omni-RRM addresses the lack of effective multimodal reward models by introducing a rubric-grounded approach that generates structured, multi-dimension preference judgments with justifications. The key innovation is Omni-Preference, a fully automated dataset created by contrasting outputs from strong and weak models and using teacher models to reconcile and filter preferences while providing rubric-grounded rationales. The training pipeline consists of supervised fine-tuning to learn the structured output format, followed by reinforcement learning (GRPO) to improve discrimination on challenging pairs. The model evaluates preferences across four modalities (text, image, video, audio) using a unified rubric-based framework that provides both scalar preferences and detailed justifications.

## Key Results
- Achieves 80.2% accuracy on ShareGPT-V video preference benchmark
- Achieves 66.8% accuracy on Audio-HH-RLHF audio preference benchmark
- Shows 17.7% absolute improvement over existing open-source RMs on image preference tasks

## Why This Works (Mechanism)
Omni-RRM works by learning to generate structured preference judgments that explicitly evaluate multiple quality dimensions rather than relying on scalar comparisons. The automatic dataset synthesis captures relative quality differences between model generations, while the rubric-grounded format provides interpretable rationales that improve both model training and human review. The two-stage training approach first establishes the structured output capability through supervised learning, then refines discrimination ability through reinforcement learning on challenging examples.

## Foundational Learning
- Multimodal reward modeling: Learning to evaluate quality across different content types (why needed: unified approach for cross-modal alignment; quick check: consistent performance across modalities)
- Automatic preference synthesis: Generating preference data by contrasting model outputs (why needed: scalable data creation without human annotation; quick check: quality of synthetic preferences matches human judgments)
- Rubric-grounded judgments: Structured evaluation with explicit quality dimensions (why needed: interpretable outputs for debugging and human review; quick check: rationales correlate with preference accuracy)
- Two-stage training (SFT + RL): Supervised learning followed by reinforcement learning (why needed: separate structure learning from discrimination refinement; quick check: performance gain from RL stage)

## Architecture Onboarding

**Component Map:** Data Generation -> SFT Training -> GRPO Training -> Evaluation

**Critical Path:** The pipeline flows from automatic preference generation through supervised fine-tuning to reinforcement learning refinement, with evaluation occurring at multiple stages.

**Design Tradeoffs:** Structured rubric outputs provide interpretability but increase inference latency (3.6-4.0s per pair) compared to simpler formats. The automated data synthesis approach trades off against potential domain-specific nuances that manual curation might capture.

**Failure Signatures:** Performance degradation on specific modalities suggests either data quality issues in Omni-Preference for that modality or insufficient representation during training. Low confidence judgments on ambiguous pairs indicate the rubric may need refinement.

**3 First Experiments:** 1) Compare preference accuracy with and without rubric-grounded justifications to quantify their contribution, 2) Evaluate model sensitivity to different teacher model choices in the data synthesis pipeline, 3) Test transfer learning performance from multimodal to unimodal tasks with varying training data ratios.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does Omni-RRM perform on authentic audio preference data with real-world acoustic characteristics?
- Basis in paper: "We acknowledge the limitations of this synthetic approach, as it lacks real-world recording noise, speaker variation, and prosody artifacts. And currently, we have not identified a suitable benchmark for real audio preferences. Experiments on authentic audio data remain a key direction for future work."
- Why unresolved: No suitable real audio preference benchmark exists; Audio-HH-RLHF uses TTS-synthesized speech.
- What evidence would resolve it: Evaluation on a newly collected benchmark containing spontaneous speech with ambient noise, multiple speakers, and natural prosody variation.

### Open Question 2
- Question: Does scaling Omni-Preference to web-scale significantly improve reward modeling performance, or does the current ~41K curated high-confidence approach approach ceiling benefits?
- Basis in paper: "We leave larger-scale data expansion and training for future work."
- Why unresolved: The paper shows strong results at moderate scale but does not test scaling curves beyond 41K samples.
- What evidence would resolve it: Controlled experiments varying dataset size (e.g., 41K, 100K, 500K, 1M+) while holding training pipeline constant, measuring preference accuracy on held-out benchmarks.

### Open Question 3
- Question: Can rubric-grounded reward models be effectively used for full policy optimization (RLHF/PPO) beyond inference-time Best-of-N selection?
- Basis in paper: The paper demonstrates Best-of-N downstream utility but does not evaluate using Omni-RRM's structured judgments as dense reward signals for policy gradient training.
- Why unresolved: Current evaluation focuses on preference accuracy and BoN; integration with on-policy RL remains unexplored.
- What evidence would resolve it: Training a policy model with Omni-RRM as the reward signal in a PPO/GRPO loop and measuring downstream task performance on multimodal benchmarks.

### Open Question 4
- Question: What is the optimal complexity-efficiency tradeoff between structured rubric-grounded outputs versus simpler chain-of-thought or scalar reward formats?
- Basis in paper: Appendix A.10 shows Omni-RRM has higher latency (3.6-4.0s per pair) than baselines due to longer output tokens; the structured format's interpretability vs. efficiency tradeoff is not analyzed.
- Why unresolved: Latency-cost analysis is provided but not compared against performance-matched simpler output formats.
- What evidence would resolve it: Ablating output format complexity (scalar-only, CoT-only, rubric-grounded) while measuring both accuracy and inference latency to quantify the tradeoff curve.

## Limitations
- The synthetic audio data lacks real-world recording noise, speaker variation, and prosody artifacts
- The approach has not been validated on authentic, spontaneous audio preference data
- The current dataset size (~41K samples) may not represent the optimal scale for maximum performance

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| SOTA performance on video and audio benchmarks | High |
| 17.7% absolute improvement on image tasks | Medium (limited scope of image-specific evaluations) |
| Being first open-source rubric-grounded reward model | High (thorough literature review) |
| Transfer learning to text-only preference tasks | Medium (demonstrated on relatively small-scale problems) |

## Next Checks
1. Independent replication of the Omni-Preference dataset generation process to verify reproducibility and assess sensitivity to different model choices
2. Ablation studies isolating the contributions of SFT versus GRPO training stages to understand which components drive performance improvements
3. Human evaluation studies comparing rubric-grounded justifications against free-form preferences to quantify the practical benefits of structured outputs for alignment tasks