---
ver: rpa2
title: 'Improving Translation Quality by Selecting Better Data for LLM Fine-Tuning:
  A Comparative Analysis'
arxiv_id: '2512.11388'
source_url: https://arxiv.org/abs/2512.11388
tags:
- data
- selection
- comet
- translation
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how different data selection strategies
  affect the fine-tuning quality of large language models (LLMs) for Japanese-English
  translation. Using a fixed-size training subset, we compare five selection methods:
  TF-IDF, FD-Score, QURATE, COMET-Kiwi, and random sampling.'
---

# Improving Translation Quality by Selecting Better Data for LLM Fine-Tuning: A Comparative Analysis

## Quick Facts
- **arXiv ID**: 2512.11388
- **Source URL**: https://arxiv.org/abs/2512.11388
- **Reference count**: 28
- **Primary result**: Semantic-based data selection (COMET-Kiwi) outperforms lexical heuristics for LLM fine-tuning, yielding higher translation quality even with <3% unique sample overlap.

## Executive Summary
This study systematically compares five data selection strategies for fine-tuning large language models (LLMs) on Japanese-English translation. Using a fixed-size training subset from the KFTT corpus, the authors evaluate TF-IDF, FD-Score, QURATE, COMET-Kiwi, and random sampling across four 7B-scale models (LLaMA, Gemma, Qwen, Mistral). The semantic-based COMET-Kiwi selector consistently achieves superior BLEU and COMET scores on WMT24 benchmarks, demonstrating that quality-aware selection yields substantial performance gains even when selected samples differ minimally from lexical methods. The findings underscore that data quality, not quantity, is critical for effective low-resource LLM fine-tuning.

## Method Summary
The paper compares five data selection methods (Random, TF-IDF, FD-Score, QURATE, COMET-Kiwi) on the KFTT parallel corpus (440k pairs). Each method selects top-k pairs (10k or 1k) using different scoring functions: lexical overlap (TF-IDF), geometric diversity (FD-Score), general text quality (QURATE), and translation-specific semantic quality (COMET-Kiwi). Selected subsets are used to fine-tune four 7B-scale models with LoRA adapters (8-bit quantization, rank unspecified) using AdamW optimizer (LR=2e-4, 1 epoch). Translation quality is evaluated on WMT24 using BLEU and COMET metrics, with additional analysis of training loss convergence and score distribution.

## Key Results
- COMET-Kiwi-based selection consistently outperforms lexical heuristics across all four 7B-scale models and both translation directions
- Semantic selectors yield higher COMET scores and smoother training loss convergence compared to lexical or geometry-based methods
- Even with <3% unique sample overlap between methods, semantic selection produces substantial performance gains
- Selected samples from semantic methods show lower score variance and higher mean COMET values

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: COMET-Kiwi-based selection produces translation models that generalize better than lexical heuristics, even when selected samples overlap by >97%.
- **Mechanism**: COMET-Kiwi uses a multilingual encoder (XLM-R) to create contextualized representations of source-translation pairs, then applies a regression head trained on human quality judgments. This captures adequacy (meaning preservation) and fluency beyond surface-level lexical features. The selected samples provide cleaner supervision signals because the filter preferentially retains pairs where cross-lingual meaning is well-aligned.
- **Core assumption**: Human-aligned quality estimates correlate with training utility for downstream translation tasks.
- **Evidence anchors**: [abstract] shows substantial performance impact with <3% sample difference; [section V.B] demonstrates COMET-Kiwi's consistent improvements across models; RAISE paper supports quality-over-quantity principle.
- **Break condition**: If your target domain diverges significantly from COMET-Kiwi's training distribution, quality estimates may misalign with actual training utility.

### Mechanism 2
- **Claim**: Semantically filtered data produces smoother training loss convergence and more stable optimization dynamics.
- **Mechanism**: Lexical selectors may select syntactically valid but semantically misaligned pairs. Semantic filters reduce conflicting gradient signals from inconsistent or noisy examples, allowing the optimizer to follow a more coherent descent path. COMET-Kiwi subsets yield lower final loss and reduced variance in score distributions.
- **Core assumption**: Training loss smoothness and reduced score variance correlate with better generalization.
- **Evidence anchors**: [section V.G] shows COMET-Kiwi yields lowest training loss and smoothest convergence; [section V.F] demonstrates narrower variance in COMET-Kiwi and QURATE subsets.
- **Break condition**: If your base model already has strong translation capabilities, noise reduction may yield diminishing returns compared to diversity-focused selection.

### Mechanism 3
- **Claim**: A small proportion of semantically unique, high-quality samples drives disproportionate performance gains.
- **Mechanism**: The uniqueness analysis shows TF-IDF and COMET-Kiwi contribute only ~3% unique samples each, yet COMET-Kiwi yields superior results. High-utility examples provide richer gradient information per sample than randomly selected pairs. Qualitative analysis confirms COMET-Kiwi preferentially selects globally coherent, explanatory sentences that serve as strong standalone training signals.
- **Core assumption**: The scoring function s(x_i) ≈ expected downstream improvement correlates with semantic coherence and self-contained meaning.
- **Evidence anchors**: [section V.D] shows TF-IDF and COMET-Kiwi subsets account for only about 3% each of unique samples; [section V.E] confirms top-ranked unique samples are well-formed, self-contained declarative sentences.
- **Break condition**: If your corpus contains very few intrinsically high-quality examples, semantic selection may filter too aggressively, leaving insufficient training signal.

## Foundational Learning

- **Concept**: Quality Estimation (QE) Models
  - **Why needed here**: COMET-Kiwi is a reference-free QE model; understanding its inputs (source + hypothesis, no reference) and training (regression on human DA/MQM scores) clarifies why it captures semantic adequacy.
  - **Quick check question**: Can you explain why COMET-Kiwi is called "reference-free" and why that matters for data selection?

- **Concept**: LoRA (Low-Rank Adaptation)
  - **Why needed here**: All experiments use LoRA adapters; understanding that only low-rank matrices are updated (not full weights) explains the parameter-efficiency and controlled comparison setup.
  - **Quick check question**: If LoRA adapters have rank r=8 and the base model has hidden size d=4096, how many trainable parameters per linear layer does this add?

- **Concept**: BLEU vs COMET Metrics
  - **Why needed here**: The paper reports both metrics; BLEU captures surface n-gram overlap while COMET captures neural semantic similarity to human judgments.
  - **Quick check question**: Why might a model show improved COMET but unchanged BLEU after semantic data selection?

## Architecture Onboarding

- **Component map**: Raw parallel corpus → Unicode normalization + MeCab tokenization → Scoring (TF-IDF / FD-Score / QuRate / COMET-Kiwi) → Top-k selection (1k or 10k) → LoRA fine-tuning → AdamW optimization → Evaluation (BLEU/COMET)
- **Critical path**: Preprocess and score full corpus with target selector (COMET-Kiwi recommended) → Extract Top-k pairs maintaining exact sample counts → Fine-tune with identical hyperparameters → Evaluate on out-of-domain test set (WMT24)
- **Design tradeoffs**: TF-IDF is fast and interpretable but ignores semantics; FD-Score promotes diversity but may select semantically weak samples; QURATE assesses general quality but isn't translation-specific; COMET-Kiwi is translation-aware and human-aligned but requires neural inference and may be domain-sensitive
- **Failure signatures**: High BLEU + low COMET indicates lexically similar but semantically poor data; high training loss variance suggests excessive noise; strong in-domain but weak out-of-domain performance indicates overfitting; empty unique-sample set means selector collapsing to high-frequency patterns
- **First 3 experiments**: 1) Baseline verification: Run random vs COMET-Kiwi on 1k samples with LLaMA-7B only; 2) Threshold ablation: Test COMET-Kiwi at top 50%/75%/90% thresholds on 10k samples; 3) Direction validation: Run both JA→EN and EN→JA to confirm semantic selection transfers across translation directions

## Open Questions the Paper Calls Out

- **Open Question 1**: Can hybrid selection frameworks or predictive utility models outperform single-method semantic selectors like COMET-Kiwi?
  - **Basis**: Authors explicitly state future work will explore hybrid selection frameworks or predictive utility models
  - **Why unresolved**: Current study only evaluated selectors in isolation, not combinations or gradient-based utility models
  - **What evidence would resolve it**: Experiments combining scores (e.g., semantic quality weighted by lexical diversity) or using gradient-based data selection, demonstrating higher BLEU/COMET scores than single-method baselines

- **Open Question 2**: Do the performance gains from semantic-aware filtering generalize to other language pairs and larger training data scales?
  - **Basis**: Limitations section notes it remains to be seen whether advantages extend to other language pairs, domains, or data scales
  - **Why unresolved**: Study restricted scope to Japanese-English translation using fixed small subsets (1k and 10k samples)
  - **What evidence would resolve it**: Replicating selection comparison on diverse language pairs and scaling training data size significantly to observe if semantic selectors retain dominance

- **Open Question 3**: How does the stability of semantic selection methods change under varying optimization hyperparameters or mixed-domain training data?
  - **Basis**: Limitations section suggests varying optimization hyperparameters or mixing multiple domains could reveal robustness under different training dynamics
  - **Why unresolved**: Experiments relied on single fine-tuning configuration (AdamW, LR 2e-4) and specific domain (Wikipedia/KFTT)
  - **What evidence would resolve it**: Ablation studies sweeping learning rates and batch sizes, or experiments utilizing multi-domain corpora, to compare variance in convergence and performance

## Limitations

- **Domain transferability concerns**: COMET-Kiwi's performance on specialized domains (medical, legal, technical) or other language pairs remains untested
- **Implementation specificity**: Critical LoRA hyperparameters (rank r, α, target modules) are unspecified, making exact reproduction difficult
- **Selection uniqueness interpretation**: The finding that semantic selectors contribute only ~3% unique samples yet drive major performance gains raises questions about selection overlap versus complementarity

## Confidence

- **COMET-Kiwi consistently outperforms lexical selectors**: High confidence (supported by systematic comparisons across four 7B models and multiple evaluation metrics)
- **Semantic quality drives better training dynamics**: Medium confidence (loss curves support this but mechanism lacks direct causal evidence)
- **Quality-over-quantity principle applies broadly**: Low confidence (3% unique sample finding is intriguing but not tested across larger datasets or different quality estimation approaches)

## Next Checks

1. **Cross-domain robustness test**: Apply the same five selection methods to a medical or legal translation corpus and compare COMET-Kiwi's relative performance to lexical methods
2. **Hyperparameter sensitivity analysis**: Systematically vary LoRA rank (r=4,8,16) and selection thresholds (top 25%, 50%, 75%) for COMET-Kiwi to establish optimal configurations
3. **Selection efficiency benchmark**: Measure wall-clock time and GPU memory required for each selection method on a 1M-pair corpus to quantify practical trade-offs between semantic accuracy and computational cost