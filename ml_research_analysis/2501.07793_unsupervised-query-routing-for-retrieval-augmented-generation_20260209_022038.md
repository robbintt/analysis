---
ver: rpa2
title: Unsupervised Query Routing for Retrieval Augmented Generation
arxiv_id: '2501.07793'
source_url: https://arxiv.org/abs/2501.07793
tags:
- search
- query
- data
- training
- routing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an unsupervised method for query routing
  in retrieval-augmented generation (RAG) that eliminates the need for expensive manual
  annotations. The key idea is to construct "upper-bound" multi-sourced responses
  as a reference to evaluate single-sourced responses, enabling automatic generation
  of training labels.
---

# Unsupervised Query Routing for Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2501.07793
- Source URL: https://arxiv.org/abs/2501.07793
- Reference count: 3
- Unsupervised method for query routing in RAG that eliminates need for manual annotations

## Executive Summary
This paper introduces an unsupervised approach for query routing in retrieval-augmented generation (RAG) that eliminates the need for expensive manual annotations. The method constructs "upper-bound" multi-sourced responses as quality references to automatically generate training labels for routing decisions. By leveraging large-scale real user queries and combining similarity-based and coherence-based metrics, the approach demonstrates strong scalability, generalization to non-i.i.d. settings, and consistent effectiveness across multiple LLMs.

## Method Summary
The unsupervised query routing method consists of four key steps: (1) generating single-sourced responses from each search engine using an LLM, (2) constructing multi-sourced "upper-bound" responses by aggregating documents from all engines and generating responses with LLMs, (3) creating training labels through a combination of BertScore similarity between single-sourced and upper-bound responses plus LLM-based coherence evaluation via pairwise comparisons, and (4) training a routing model using ListMLE loss to predict the ranking over search engines. The approach uses GTE-large as the backbone model and can automatically process large-scale real user queries without manual annotation.

## Key Results
- Achieves 86% accuracy in automatically generated training datasets
- Demonstrates excellent scalability with increasing training data
- Shows strong generalization in non-i.i.d. settings across five datasets
- Visualizes routing patterns showing Google's strength in professional queries, Bing's in news/hotspot queries, and Quark's in other domains

## Why This Works (Mechanism)

### Mechanism 1: Multi-Sourced Upper Bound as Quality Anchor
Aggregating documents from multiple search engines creates a response quality ceiling that serves as a proxy for gold-standard annotations. Due to complementarity across search engines, merged multi-sourced documents achieve better recall than any single source. When an LLM generates responses using these enriched documents, the resulting "upper-bound" response captures information that any single-sourced response might miss. Single-sourced responses are then evaluated against this anchor—high similarity indicates the single source captured most relevant information.

### Mechanism 2: Dual-Metric Label Construction
Combining similarity-based (BertScore) and coherence-based (LLM pairwise ranking) evaluation produces more reliable training labels than either metric alone. Similarity measures alignment between single-sourced response and upper-bound anchor, evaluating content coverage. Coherence uses an LLM to perform pairwise comparisons of single-sourced responses, assessing which better addresses the query. These capture complementary aspects: similarity is reference-based while coherence evaluates query-relevance rationality.

### Mechanism 3: Distribution-Matching Through Real Query Training
Training on large-scale real user queries (vs. curated datasets) enables generalization to out-of-distribution scenarios. Public annotated datasets have limited diversity and distribution mismatch with production queries. By eliminating annotation requirements, the method can process ~110k real user queries directly. This expanded scale and diversity helps the routing model learn patterns that transfer to unseen query distributions.

## Foundational Learning

- **Listwise Ranking Loss (ListMLE)**
  - Why needed: The routing model must predict a full ranking over search engines, not just binary preferences. ListMLE optimizes the probability of the correct permutation.
  - Quick check: Can you explain why ListMLE is preferred over pairwise losses when M>2 candidates exist?

- **BertScore for Text Evaluation**
  - Why needed: Core similarity metric for comparing single-sourced responses to upper bounds. Uses contextual embeddings rather than exact token overlap.
  - Quick check: How does BertScore handle synonyms and paraphrasing differently from BLEU or ROUGE?

- **Non-I.I.D. Generalization**
  - Why needed: Training queries come from real user logs; test queries come from curated datasets. Distribution shift is explicit in experimental design.
  - Quick check: Why does performance on "CDQA→CDQA" not predict performance on "CDQA→WebQA"?

## Architecture Onboarding

- Component map:
  User Query q -> [Single-Source Branch] and [Multi-Source Branch] -> [Label Constructor] -> [Routing Model Training] -> [Inference: M(q) -> p]

- Critical path:
  1. Upper-bound construction is the bottleneck—requires calling ALL search engines and multiple LLMs (Qwen2-max + GPT4 for robustness)
  2. Coherence evaluation requires M*(M-1)/2 LLM calls per query for pairwise comparisons
  3. Label quality directly limits model ceiling (86% accuracy = 14% label noise)

- Design tradeoffs:
  - Uses top-k/M from each engine rather than re-ranking to maintain simplicity and generalizability to black-box engines
  - Chooses Qwen2-max for single-sourced generation and both Qwen2-max/GPT4 for upper bounds as a cost vs. robustness tradeoff
  - Uses GTE-large over Qwen2-0.5B due to decoder-only models struggling with regression and latency requirements

- Failure signatures:
  - Low similarity scores across all engines: Upper-bound may not be sufficiently better; check multi-source construction
  - High illegal ranking rate in coherence: LLM struggling with query domain; consider fallback to similarity-only
  - No scaling with more data: Label noise may be too high; inspect automatic labels manually
  - Circular preferences (A>B>C>A): Discard these samples (~9% occur)

- First 3 experiments:
  1. Validate upper-bound assumption: On a held-out set, verify that multi-sourced RAG actually outperforms best single-source
  2. Label quality audit: Randomly sample 100-200 automatically generated labels; manually verify routing decisions
  3. Ablation on training size: Train with 20%, 50%, 80%, 100% of available queries; plot scaling curve

## Open Questions the Paper Calls Out

- **Open Question 1**: Does model performance saturate or continue to improve linearly when scaling training data significantly beyond the current 110k examples?
- **Open Question 2**: Can larger auto-regressive LLMs be effectively adapted for the routing task despite their inherent challenges with numerical prediction and latency?
- **Open Question 3**: How can the label construction process be improved to eliminate the 9% of samples lost due to illegal ranking outputs?

## Limitations
- Search engine complementarity assumption may not hold if engines have significant overlap
- 14% label noise tolerance threshold is untested at scale
- LLM dependency creates potential domain gaps and performance ceilings
- Query distribution assumptions may not hold for all production scenarios

## Confidence
- **High Confidence**: Multi-sourced upper bound consistently outperforms single sources, dual-metric label construction improves over single metrics, real query training shows better scaling in non-i.i.d. settings
- **Medium Confidence**: 86% automatic label accuracy claim based on limited testing; routing patterns across query types are observational
- **Low Confidence**: Specific claims about Google's strength in professional queries vs. Bing for news require domain labeling validation

## Next Checks
1. Upper-bound validation: Manually sample 50 queries where single-source responses rank above multi-source in automatic labels to verify labeling accuracy
2. Label quality audit across domains: Apply automatic labeling to test datasets and compare with ground truth annotations
3. Engine complementarity analysis: Measure Jaccard similarity between top-6 documents from different engines to quantify actual information overlap