---
ver: rpa2
title: LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and
  LLM Perceptions of Privacy in Textual Data
arxiv_id: '2508.12158'
source_url: https://arxiv.org/abs/2508.12158
tags:
- privacy
- human
- text
- https
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores whether LLMs can effectively evaluate the privacy
  sensitivity of textual data, comparing their judgments to human perceptions. Using
  13 LLMs and a survey of 677 human participants, the researchers found that LLMs
  generally agree with each other, especially larger models, and align well with the
  average human privacy rating but struggle with individual variations.
---

# LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data

## Quick Facts
- arXiv ID: 2508.12158
- Source URL: https://arxiv.org/abs/2508.12158
- Authors: Stephen Meisenbacher; Alexandra Klymenko; Florian Matthes
- Reference count: 40
- Primary result: LLMs align well with average human privacy ratings but struggle with individual variation; larger models show higher agreement

## Executive Summary
This study investigates whether LLMs can effectively evaluate the privacy sensitivity of textual data by comparing their judgments to human perceptions. Using 13 LLMs and a survey of 677 human participants, the researchers found that LLMs generally agree with each other, especially larger models, and align well with the average human privacy rating but struggle with individual variations. Human agreement on privacy was low, showing subjective interpretations. LLMs tend to slightly overestimate privacy sensitivity compared to humans. The study highlights the promise of LLM-as-a-Judge for scalable privacy evaluation while underscoring the need to account for diverse human perspectives.

## Method Summary
The study employed an adversarial sampling approach to curate a balanced evaluation set of 250 texts from 10 public datasets. A deBERTa-v3-base model was fine-tuned to predict user attributes, and texts were selected based on prediction confidence to capture vulnerability. Thirteen LLMs (both proprietary and open-source) evaluated the texts using a structured 5-point Likert scale prompt, with 5 inference runs per text. Human evaluation was conducted via Prolific with 677 participants rating 20 texts each using the same scale. Krippendorff's alpha was used to measure inter-rater reliability across LLM-LLM, Human-Human, and LLM-Human comparisons.

## Key Results
- LLMs show strong inter-annotator agreement (0.84 for proprietary models, 0.37 for open-source)
- LLMs align well with average human privacy ratings but poorly with individual variations
- Human-human agreement on privacy was low, indicating subjective interpretations
- LLMs slightly overestimate privacy sensitivity compared to human ratings
- Improved prompt with explicit scale descriptions increased inter-LLM agreement from 0.54 to 0.58

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger LLMs achieve higher inter-annotator agreement in privacy evaluation tasks
- Mechanism: Scale correlates with more consistent application of privacy criteria across similar model families, with proprietary models (0.84 agreement) outperforming open-source counterparts (0.37).
- Core assumption: Higher inter-LLM agreement indicates more reliable privacy assessment, though this conflates consistency with correctness.
- Evidence anchors: Within closed LLMs, there is a high agreement score of 0.84, whereas open models achieve a much lower score of 0.37.

### Mechanism 2
- Claim: LLMs align with aggregate human privacy judgments but fail to capture individual variation
- Mechanism: LLMs model a "global human privacy perspective" (high agreement with averaged ratings) while pairwise LLM-human agreement drops to levels comparable with human-human agreement.
- Core assumption: The averaged human response represents a meaningful privacy baseline rather than obscuring legitimate disagreement.
- Evidence anchors: LLMs can accurately model a global human privacy perspective while struggling with individual variations.

### Mechanism 3
- Claim: Structured prompt scaffolding improves LLM privacy evaluation consistency
- Mechanism: Providing explicit scale definitions and requiring reasoning output increased inter-LLM agreement from 0.54 to 0.58, with larger gains for proprietary model alignment.
- Core assumption: The privacy scale descriptions capture relevant distinctions without introducing artificial boundaries.
- Evidence anchors: The improved prompt increased inter-LLM agreement from 0.54 to 0.58.

## Foundational Learning

- Concept: Krippendorff's alpha inter-rater reliability
  - Why needed here: The paper uses this metric to quantify agreement between LLMs and humans on privacy ratings; understanding it is essential to interpret results correctly.
  - Quick check question: Why is Krippendorff's alpha preferred over simple percentage agreement for this task? (Answer: It handles missing data and corrects for chance agreement.)

- Concept: LLM-as-a-Judge paradigm fundamentals
  - Why needed here: The entire methodology builds on replacing human evaluators with LLMs; understanding trade-offs (scalability vs. representation) frames the contribution.
  - Quick check question: What is the core assumption underlying LLM-as-a-Judge for subjective tasks? (Answer: That LLMs trained on human preferences can approximate aggregate human judgment.)

- Concept: Privacy sensitivity vs. identifiability distinction
  - Why needed here: Both humans and LLMs in the study conflate "sensitive topic" with "identifying information," affecting how privacy is operationalized.
  - Quick check question: According to the paper, what two aspects do LLMs primarily consider when rating privacy? (Answer: Sensitivity of content and risk of identifiability.)

## Architecture Onboarding

- Component map:
  Text corpus -> Adversarial sampling module (deBERTa-based vulnerability scoring) -> Selected evaluation set (250 texts) -> Prompt templates (simple/improved) -> LLM evaluator pool (13 models) -> Multi-run aggregation (5× per text) -> Krippendorff's alpha calculation -> Human survey alignment comparison

- Critical path:
  1. Define privacy scale with explicit level descriptions (1-5 Likert)
  2. Select LLM evaluator(s) based on agreement requirements (larger models >70B parameters for consistency)
  3. Run multiple inference passes per text (paper uses 5) and average scores
  4. Validate against human ground truth if available, or compare across model families for consistency

- Design tradeoffs:
  - Proprietary vs. open-source: Higher agreement (0.84 vs. 0.37) vs. data sovereignty; cost difference was £2031 (human survey) vs. <$20 (LLM runs)
  - Global vs. individual alignment: Optimizing for average human rating (high LLM agreement) vs. capturing demographic-specific perspectives (current approach cannot)
  - Prompt complexity: Simple prompts are faster but yield lower inter-LLM agreement (0.54 vs. 0.58)

- Failure signatures:
  - Low inter-LLM agreement (<0.5 Krippendorff's alpha): Indicates prompt needs refinement or model selection is inappropriate
  - Systematic over/underestimation vs. human baseline: May require scale recalibration or few-shot examples
  - Unparseable outputs: Especially with smaller models (gemma-3-1b had unparsable responses for some texts)

- First 3 experiments:
  1. Replicate the improved prompt on a small held-out set (10-20 texts) with 2-3 LLMs to establish baseline agreement before full deployment.
  2. Test one-shot prompting with example human-rated texts to assess whether alignment improves beyond the 0.58 agreement threshold.
  3. Run demographic-stratified analysis on any available human labels to determine if LLM "global alignment" masks systematic bias against specific populations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic and semantic features in textual data drive human perceptions of privacy beyond simple agreement metrics?
- Basis in paper: The authors state, "we recommend in-depth studies that extend beyond agreement metrics to investigate what exactly (linguistically and semantically) in textual data humans perceive to be private."
- Why unresolved: The current study focused on correlation and agreement scores rather than analyzing the causal textual features or reasoning patterns that define privacy sensitivity.
- What evidence would resolve it: A study employing feature importance analysis or probing classifiers on the human-annotated dataset to identify specific tokens, entities, or semantic structures that correlate with high privacy ratings.

### Open Question 2
- Question: How do cross-cultural differences influence the perception of privacy in textual data?
- Basis in paper: The authors call for "studies of cross-cultural perceptions of privacy, and how these diverse perspectives can be comprehensively represented."
- Why unresolved: While the study included participants from 45 countries, the analysis aggregated demographic data and did not specifically isolate or model cultural differences in privacy definition.
- What evidence would resolve it: A comparative analysis using stratified sampling across distinct cultures to identify statistically significant variations in privacy thresholds for specific text types.

### Open Question 3
- Question: Can lightweight, specialized models be constructed to evaluate privacy sensitivity with accuracy comparable to large LLMs?
- Basis in paper: The paper suggests future research could "focus on the construction of lightweight, yet accurate models for evaluating privacy sensitivity in text."
- Why unresolved: The study found that smaller open-source models (1B-3B parameters) exhibited low agreement (0.37) compared to large models, indicating a performance gap in smaller architectures.
- What evidence would resolve it: Fine-tuning smaller encoder models (e.g., BERT-based) on the released human-annotated dataset to test if they can approximate the agreement levels of models like GPT-4o.

## Limitations

- Low human-human agreement suggests subjective interpretations of privacy, raising questions about whether LLM alignment captures genuine assessment or arbitrary consensus
- The adversarial sampling methodology may introduce selection bias toward ambiguous texts rather than representative real-world data
- Results may not generalize to non-English texts or domain-specific privacy contexts not represented in the 10 source datasets

## Confidence

- **High confidence:** Inter-LLM agreement findings, cost-effectiveness comparison, and directional claim that LLMs slightly overestimate privacy sensitivity
- **Medium confidence:** Claim that LLMs accurately model "global human privacy perspective" given the subjective nature of privacy judgments
- **Low confidence:** Generalizability to non-English texts and domain-specific privacy contexts

## Next Checks

1. Test LLM privacy evaluation on texts with legally-defined privacy attributes (PII categories) to assess whether agreement with human ratings translates to correct identification of regulated data types
2. Conduct demographic-stratified analysis comparing LLM ratings against privacy perceptions from different age groups, cultural backgrounds, and privacy concern levels
3. Evaluate the adversarial sampling methodology by comparing model performance on the selected 250 texts versus randomly sampled texts from the same datasets to quantify selection bias