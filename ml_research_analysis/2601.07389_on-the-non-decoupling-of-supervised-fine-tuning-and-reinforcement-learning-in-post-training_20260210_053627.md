---
ver: rpa2
title: On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning
  in Post-training
arxiv_id: '2601.07389'
source_url: https://arxiv.org/abs/2601.07389
tags:
- uni00000013
- arxiv
- reward
- loss
- post-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proves that supervised fine-tuning (SFT) and reinforcement
  learning (RL) are inherently coupled in post-training pipelines for large language
  models, making them impossible to decouple without loss of performance. The theoretical
  analysis examines two canonical pipelines: SFT-then-RL and RL-then-SFT.'
---

# On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training

## Quick Facts
- **arXiv ID:** 2601.07389
- **Source URL:** https://arxiv.org/abs/2601.07389
- **Reference count:** 6
- **Primary result:** Proves SFT and RL are inherently coupled in post-training, making pure decoupling impossible without performance loss

## Executive Summary
This paper establishes that supervised fine-tuning (SFT) and reinforcement learning (RL) cannot be treated as independent stages in large language model post-training pipelines. Through theoretical analysis of SFT-then-RL and RL-then-SFT sequences, the authors prove that each stage inherently degrades the performance achieved by the prior stage. The theoretical framework demonstrates that RL increases SFT loss under SFT optimality conditions, while SFT reduces RL reward under RL optimality conditions. This fundamental coupling implies that practitioners must treat SFT and RL as a single joint optimization problem rather than separable blocks.

## Method Summary
The authors analyze two canonical post-training pipelines: SFT-then-RL and RL-then-SFT. For the SFT-then-RL pipeline, they prove that RL increases SFT loss under SFT optimality conditions, establishing theoretical coupling. For the RL-then-SFT pipeline, they demonstrate that SFT reduces RL reward under RL optimality conditions, proving coupling in the reverse order. Empirical validation uses the Qwen3-0.6B model on the CoLA dataset, showing that cross-entropy loss increases during RL after SFT, and reward collapses during SFT after RL. The theoretical analysis assumes simplified single-head output architecture and restricted reward function classes.

## Key Results
- RL inevitably increases SFT loss when applied after SFT under SFT optimality conditions
- SFT reduces RL reward when applied after RL under RL optimality conditions
- Empirical validation shows SFT-then-RL pipeline causes cross-entropy to exceed base model baseline, while RL-then-SFT causes reward collapse below base model performance

## Why This Works (Mechanism)
The coupling arises because SFT optimizes for next-token prediction accuracy using cross-entropy loss, while RL optimizes for reward maximization using different objective functions. When SFT is applied first, it establishes a parameter configuration optimized for prediction accuracy. Subsequent RL updates parameters to maximize reward, which necessarily alters the distribution of predictions and increases prediction error. Conversely, when RL is applied first, it establishes parameters optimized for the reward function. Subsequent SFT updates then shift parameters toward prediction accuracy, which necessarily reduces the reward signal. The mathematical proofs show that these interactions are inherent to the optimization dynamics and cannot be avoided without simultaneously optimizing both objectives.

## Foundational Learning

**Convex Optimization Theory**: Understanding why convexity assumptions matter for proving theoretical results about optimization dynamics. Quick check: Verify convexity of the objective functions used in proofs.

**Reinforcement Learning Objectives**: Different reward functions and their impact on model behavior compared to supervised learning. Quick check: Compare reward landscapes before and after SFT.

**Supervised Learning Loss Functions**: Cross-entropy loss properties and how they relate to prediction accuracy. Quick check: Analyze loss surfaces for SFT-optimized models.

**Joint Optimization**: Why treating SFT and RL as a single problem differs from sequential optimization. Quick check: Compare parameter trajectories under joint vs sequential optimization.

**Transformer Architecture**: How output heads and attention mechanisms interact with different training objectives. Quick check: Examine attention patterns before and after each stage.

## Architecture Onboarding

**Component Map**: Base Model -> SFT Stage -> RL Stage (SFT-then-RL) or Base Model -> RL Stage -> SFT Stage (RL-then-SFT)

**Critical Path**: The sequence of parameter updates from initialization through both SFT and RL stages determines final model performance.

**Design Tradeoffs**: Sequential optimization offers modularity but suffers from performance degradation, while joint optimization avoids degradation but requires more complex implementation.

**Failure Signatures**: Cross-entropy loss increasing during RL indicates SFT degradation; reward collapsing during SFT indicates RL degradation.

**First Experiments**:
1. Measure cross-entropy loss trajectory during RL stage following SFT
2. Measure reward trajectory during SFT stage following RL
3. Compare parameter distance from base model after each stage to quantify degradation

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical framework assumes simplified single-head output architecture, not capturing modern transformer complexity
- Analysis relies on convexity assumptions that may not hold for highly non-convex LLM objectives
- Empirical validation limited to one 0.6B parameter model on single CoLA task, limiting generalizability

## Confidence

**High confidence**: The core mathematical proof that SFT and RL interact non-trivially in both pipeline orders, establishing that pure decoupling is theoretically impossible under the given assumptions.

**Medium confidence**: The empirical demonstration that these theoretical couplings manifest in practice, given the limited experimental scope to one model and task.

**Low confidence**: The practical implication that treating SFT and RL as a joint optimization problem is always superior to sequential approaches across all model scales and applications.

## Next Checks

1. Test the coupling phenomenon across multiple model scales (1B, 7B, 70B parameters) to assess scalability of the effect
2. Evaluate on diverse task families including multi-task benchmarks, code generation, and instruction following to determine task-specific variations
3. Implement and test alternative coupling strategies beyond the simple concatenation approach used in the current work to explore whether partial decoupling is achievable with different architectural modifications