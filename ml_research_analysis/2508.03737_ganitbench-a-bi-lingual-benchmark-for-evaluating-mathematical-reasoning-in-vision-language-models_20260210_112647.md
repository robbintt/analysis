---
ver: rpa2
title: 'GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in
  Vision Language Models'
arxiv_id: '2508.03737'
source_url: https://arxiv.org/abs/2508.03737
tags:
- questions
- answer
- language
- hindi
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GanitBench, a bilingual (English-Hindi) benchmark
  for evaluating mathematical reasoning in Vision Language Models (VLMs). The dataset
  consists of 1527 image-based questions from Indian exams (JEE Advanced, CBSE Boards),
  covering topics like calculus, geometry, and trigonometry.
---

# GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in Vision Language Models

## Quick Facts
- arXiv ID: 2508.03737
- Source URL: https://arxiv.org/abs/2508.03737
- Reference count: 22
- Introduces bilingual (English-Hindi) benchmark for evaluating mathematical reasoning in Vision Language Models

## Executive Summary
GanitBench introduces a novel bilingual benchmark for evaluating mathematical reasoning capabilities in Vision Language Models (VLMs), addressing the scarcity of non-English mathematical datasets. The benchmark comprises 1527 image-based questions from Indian competitive exams (JEE Advanced, CBSE Boards) across domains like calculus, geometry, and trigonometry. Two VLMs (GPT-4o mini and Claude 3 Haiku) were evaluated under various conditions, revealing significant performance drops when answering in Hindi and under Double Lock constraints requiring correctness in both languages.

## Method Summary
The authors constructed GanitBench by collecting and translating 1527 mathematical questions from Indian exam papers into both English and Hindi. The dataset was designed to test various mathematical domains including calculus, geometry, and trigonometry. Two VLMs (GPT-4o mini and Claude 3 Haiku) were evaluated using zero-shot and two-shot Chain-of-Thought prompting strategies. A novel "Double Lock" constraint was introduced where models had to provide correct answers in both languages to receive credit. Performance was measured across different experimental conditions to assess language-specific capabilities.

## Key Results
- GPT-4o mini achieved highest accuracy of 38.15% in normal mode, dropping to 23.33% under Double Lock constraint
- Significant performance degradation observed when answering in Hindi compared to English
- Benchmark provides challenging evaluation framework for VLMs' multilingual mathematical reasoning capabilities

## Why This Works (Mechanism)
The benchmark leverages the complexity of mathematical reasoning combined with language-specific challenges to create a rigorous evaluation framework. By requiring models to process visual mathematical content and generate accurate responses in two languages, it exposes limitations in both visual understanding and multilingual reasoning capabilities. The Double Lock constraint adds an additional layer of difficulty by requiring consistency across languages.

## Foundational Learning
- **Vision Language Model Architecture**: VLMs combine visual and language processing capabilities - needed for interpreting mathematical diagrams and text together
- **Chain-of-Thought Prompting**: Enables step-by-step reasoning - needed to break down complex mathematical problems
- **Cross-lingual Transfer Learning**: Models must generalize knowledge across languages - needed for bilingual performance
- **Mathematical Domain Knowledge**: Understanding of calculus, geometry, and trigonometry concepts - needed for accurate problem solving
- **Visual Question Answering**: Ability to interpret mathematical diagrams and symbols - needed for image-based questions

Quick check: Model must correctly parse both the visual elements and linguistic content to solve problems

## Architecture Onboarding

Component map: Image input -> Vision Encoder -> Fusion Module -> Language Model -> Bilingual Output

Critical path: Vision encoder extracts features from mathematical diagrams → Fusion module combines visual and text representations → Language model generates step-by-step reasoning → Final answer generation in target language

Design tradeoffs: 
- Balance between visual detail preservation and computational efficiency
- Trade-off between model size and multilingual capability
- Choice between zero-shot flexibility and few-shot performance

Failure signatures:
- Incorrect parsing of mathematical symbols
- Loss of visual information during fusion
- Language-specific reasoning errors
- Double Lock failures due to inconsistency between language outputs

First experiments:
1. Test individual mathematical domains (calculus, geometry, trigonometry) separately
2. Evaluate single-language performance before introducing bilingual constraints
3. Compare zero-shot vs few-shot performance across different mathematical topics

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Sample size of 1527 questions may not fully represent diversity of Indian mathematical curricula
- Study focuses on only two VLMs without broader architectural comparisons
- Double Lock constraint effects not fully disentangled from language comprehension challenges

## Confidence
High confidence in VLMs struggling with Hindi mathematical reasoning (38.15% → 23.33% accuracy drop)
Medium confidence in benchmark comprehensiveness (covers multiple domains but limited sample diversity)
Low confidence in generalizability of Double Lock results (requires further investigation)

## Next Checks
1. Expand evaluation to include additional VLMs (Gemini, LLaVA, open-source models) to assess architectural patterns
2. Conduct detailed error analysis distinguishing language vs mathematical reasoning failures
3. Test pretraining on bilingual mathematical corpora to evaluate performance improvements