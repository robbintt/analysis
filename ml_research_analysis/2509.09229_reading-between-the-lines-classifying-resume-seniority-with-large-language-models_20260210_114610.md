---
ver: rpa2
title: 'Reading Between the Lines: Classifying Resume Seniority with Large Language
  Models'
arxiv_id: '2509.09229'
source_url: https://arxiv.org/abs/2509.09229
tags:
- resumes
- seniority
- language
- resume
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of accurately classifying resume
  seniority levels in the presence of overstated or understated self-presentation.
  It introduces a hybrid dataset combining real resumes with synthetic examples designed
  to simulate exaggerated and modest portrayals of seniority.
---

# Reading Between the Lines: Classifying Resume Seniority with Large Language Models

## Quick Facts
- arXiv ID: 2509.09229
- Source URL: https://arxiv.org/abs/2509.09229
- Reference count: 0
- Key outcome: Fine-tuned RoBERTa achieved 90.60% accuracy, outperforming zero-shot GPT-4 (78.6%) and TF-IDF baseline (81.2%) in classifying resume seniority levels

## Executive Summary
This study addresses the challenge of accurately classifying resume seniority levels when candidates use overstated or understated self-presentation. The authors introduce a hybrid dataset combining real resumes with synthetic examples designed to simulate exaggerated and modest portrayals of seniority. They compare zero-shot GPT-4 classification with fine-tuned BERT models (DistilBERT and RoBERTa) against a traditional TF-IDF and logistic regression baseline. Results show that fine-tuned RoBERTa achieved the highest accuracy at 90.60%, significantly outperforming both the baseline (81.2%) and zero-shot GPT-4 (78.6%). The findings demonstrate that large language models, particularly when fine-tuned, can effectively detect nuanced linguistic cues of seniority and outperform traditional approaches in handling self-promotional language.

## Method Summary
The study uses a hybrid dataset combining real resumes from hireitpeople.com with synthetic examples generated via Mistral-7B and GPT-4o. The synthetic data is structured in triplets (normal, understated, overstated versions of identical profiles) to create controlled variation in presentation style while maintaining factual consistency. The baseline uses TF-IDF vectorization with logistic regression. Zero-shot GPT-4 employs chain-of-thought prompting for classification. Fine-tuned BERT models (DistilBERT and RoBERTa) are trained using cross-entropy loss with hyperparameter tuning via grid search and 5-fold cross-validation.

## Key Results
- Fine-tuned RoBERTa achieved 90.60% accuracy, the highest among all methods tested
- Fine-tuned DistilBERT reached 87.18% accuracy, outperforming zero-shot GPT-4 (78.6%) and TF-IDF baseline (81.2%)
- DistilBERT showed particular strength in detecting medium seniority resumes, which frequently confused other models
- The hybrid dataset approach combining real and synthetic examples proved effective for training robust classification models

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuned BERT models outperform zero-shot GPT-4 because task-specific training captures domain-relevant linguistic patterns that generic prompting misses. Fine-tuning adapts transformer attention weights to prioritize resume-specific signals (job title progression, achievement framing, temporal consistency) over general language patterns. RoBERTa's 90.60% vs. GPT-4's 78.6% suggests supervised learning on targeted examples teaches the model to weight understated cues more heavily than promotional language. This assumes the hybrid dataset's synthetic "hard examples" accurately represent real-world self-presentation strategies.

### Mechanism 2
The matched triplet structure (normal/understated/overstated versions of identical profiles) forces models to learn presentation-style invariants rather than relying on surface-level keywords. By keeping factual content identical across versions while only altering presentation style, the dataset creates a contrastive learning signal. Models must discriminate based on how achievements are framed, not just what achievements exist. This mechanism assumes Mistral-7B and GPT-4o can generate realistic manipulation strategies that mirror human self-presentation patterns.

### Mechanism 3
DistilBERT's relative strength on medium seniority classifications suggests architectural differences in handling ambiguous cases. The confusion matrices show DistilBERT distinguishing mid-level resumes more effectively than other models, potentially due to its compressed representation forcing focus on salient features rather than overfitting to extreme seniority signals. This assumes mid-level seniority has the most ambiguous linguistic markers, making it the hardest classification boundary.

## Foundational Learning

- **Fine-tuning vs. Zero-shot Classification**
  - Why needed here: The paper's central comparison assumes understanding of how supervised adaptation differs from prompting a frozen model
  - Quick check question: Can you explain why adding task-specific training data changes what attention heads prioritize, even for the same base architecture?

- **TF-IDF Baselines**
  - Why needed here: The 81.2% baseline provides context for whether LLMs add meaningful signal or just overfit
  - Quick check question: What types of seniority cues would TF-IDF capture (keyword frequency) versus miss (semantic relationships between achievements)?

- **Synthetic Data Generation for Hard Negatives**
  - Why needed here: The triplet structure is the methodological innovation; understanding how controlled manipulation creates learning signal is essential
  - Quick check question: If you generated overstated resumes by simply adding random buzzwords, what would models actually learn?

## Architecture Onboarding

- **Component map**: Real resumes (hireitpeople.com) + Mistral-7B/GPT-4o synthetic generation → triplet matching → train/val split → TF-IDF/Logistic baseline / GPT-4 zero-shot / BERT fine-tuning → classification

- **Critical path**: Recreate triplet generation prompts (Table 1) for your target domain → Validate synthetic quality via manual review (timelines, linguistic consistency) → Start with DistilBERT for faster iteration; move to RoBERTa if mid-level accuracy is critical → Grid search learning rate and batch size with 5-fold CV before final training

- **Design tradeoffs**:
  - DistilBERT vs. RoBERTa: DistilBERT trains ~2x faster; RoBERTa gives +3.4% accuracy but may overfit on smaller datasets
  - Zero-shot vs. Fine-tuned: Zero-shot requires no training data but underperforms by ~12%; fine-tuning needs labeled data creation effort
  - Real-only vs. Hybrid dataset: Synthetic examples improve robustness but introduce generator biases

- **Failure signatures**:
  - High baseline, low LLM performance: Check tokenization—resumes may exceed context windows or have formatting issues
  - Good overall accuracy, poor mid-level recall: Class imbalance or ambiguous training labels; consider class weights
  - Synthetic examples classified differently than real ones with same seniority: Generator artifacts; review synthetic prompts for unnatural patterns

- **First 3 experiments**:
  1. Replicate baseline (TF-IDF + Logistic) on your own resume data to establish floor; if <75%, data quality issues exist
  2. Ablate synthetic data: train RoBERTa on real-only vs. hybrid to quantify synthetic contribution
  3. Error analysis on confusion matrix: manually review 20 misclassified mid-level resumes to identify whether failures come from understated or overstated examples

## Open Questions the Paper Calls Out

### Open Question 1
Can the superior performance of fine-tuned BERT models in detecting seniority manipulation generalize to diverse industries, cultural contexts, and non-English resumes? The authors state that "Future research should extend this line of work to broader domains" following the conclusion. This remains unresolved because the current study relies on a specific hybrid dataset derived from a single source (hireitpeople.com) and synthetic generation, which may not capture the full variance of global professional jargon or cross-cultural self-presentation norms. Evaluating the fine-tuned models on out-of-domain datasets (e.g., medical or legal resumes) and cross-lingual resume corpora would resolve this.

### Open Question 2
Do the fine-tuned models mitigate or inadvertently amplify demographic biases (e.g., gender, race) when classifying seniority in real-world deployments? The conclusion explicitly calls to "investigate fairness and bias implications in real-world deployments," noting the potential for AI to reinforce self-presentational biases mentioned in the literature review. This remains unresolved because while the study simulates "overstated" language, it does not audit the model's performance across protected groups, which is critical given prior findings that LLMs favor specific demographics (e.g., Wilson & Caliskan, 2024). A bias audit measuring classification accuracy and false positive rates across resumes with varied demographic markers would resolve this.

### Open Question 3
Does the reliance on LLMs (Mistral-7B) to generate synthetic "ground truth" for overstated seniority introduce circularity or label noise that inflates perceived model performance? This methodological limitation regarding validation arises because the paper describes generating synthetic "hard examples" via prompting but lacks a human verification step to confirm that the "overstated" examples successfully deceive human recruiters. This remains unresolved because if the generator produces easily distinguishable text (e.g., obvious fiction) rather than subtle manipulation, the classification task becomes trivially distinct from real-world deception detection, limiting the ecological validity of the 90.60% accuracy score. A human subject study where recruiters classify the synthetic "overstated" resumes would resolve this.

## Limitations
- Synthetic data generation process introduces uncertainty about external validity due to undisclosed prompt templates and temperature settings
- Claim that DistilBERT's superior medium-seniority performance reflects architectural advantages rather than data imbalance lacks ablation studies
- The study relies on a specific hybrid dataset from hireitpeople.com that may not capture full variance of global professional contexts

## Confidence
- **High Confidence**: The comparative ranking of methods (RoBERTa > DistilBERT > GPT-4 > TF-IDF) and the overall accuracy improvements from fine-tuning
- **Medium Confidence**: The mechanism explaining why fine-tuned models capture "subtle linguistic cues" better than zero-shot approaches
- **Low Confidence**: The claim that DistilBERT's superior medium-seniority performance reflects architectural advantages rather than data imbalance or training artifacts

## Next Checks
1. **Generator Artifact Analysis**: Manually review 50 synthetic resumes to identify any consistent phrasing patterns, vocabulary choices, or structural artifacts that could explain model performance differences rather than genuine seniority signal detection

2. **Real-Only Performance Comparison**: Re-run the RoBERTa experiments using only the real resume subset to quantify how much of the 90.60% accuracy gain depends on synthetic examples versus real-world generalization

3. **Cross-Domain Transfer Test**: Evaluate the best-performing fine-tuned model on resumes from a different source (e.g., public LinkedIn profiles or other ATS datasets) to assess whether the learned patterns generalize beyond the hireitpeople.com domain