---
ver: rpa2
title: 'GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts'
arxiv_id: '2508.00476'
source_url: https://arxiv.org/abs/2508.00476
tags:
- language
- context
- answer
- getalp
- automin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents GETALP's participation in the AutoMin 2025
  Shared Task, focusing on answering questions based on meeting transcripts. The authors
  propose a retrieval-augmented generation (RAG) system that leverages dense sentence
  embeddings, Doc2Query-based document expansion, and Abstract Meaning Representation
  (AMR) to enhance answer generation.
---

# GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts

## Quick Facts
- arXiv ID: 2508.00476
- Source URL: https://arxiv.org/abs/2508.00476
- Reference count: 9
- Primary result: GETALP's RAG system with AMR achieved 5.55 in human evaluation, improving WHO question accuracy

## Executive Summary
This paper presents GETALP's participation in the AutoMin 2025 Shared Task, focusing on answering questions based on meeting transcripts. The authors propose a retrieval-augmented generation (RAG) system that leverages dense sentence embeddings, Doc2Query-based document expansion, and Abstract Meaning Representation (AMR) to enhance answer generation. They develop three system variants: IR-only, IR+AMR, and AMR-only. Results show that incorporating AMR improves performance, especially for entity-related questions like "who" questions. In human evaluation on English answers, GETALP@AutoMin and GETALP@AutoMin_amr achieved similar scores of 5.65 and 5.55 respectively, while GETALP@AutoMin_amr outperformed IR-only in 18 out of 130 questions, with half being WHO questions. Automatic evaluation using LLM-as-judge showed no significant difference between configurations. The approach successfully handles long, dispersed dialogues in meeting transcripts.

## Method Summary
GETALP's approach uses a two-stage RAG system for meeting transcript question answering. The context construction stage employs dual-index retrieval combining dense sentence embeddings (all-MiniLM-L6-v2) and Doc2Query synthetic queries, indexed in FAISS, to retrieve relevant passages with ±1 adjacent context expansion. For the IR+AMR and AMR-only variants, retrieved sentences are parsed into AMR graphs, triples are extracted via PENMAN, converted to natural language through rule-based translation, and polished with Llama-3.1-8B-Instruct. The answer generation stage uses Llama-3.1-8B-Instruct to generate concise 1-2 sentence answers from the constructed context and question. The system was evaluated on the ELITR Minuting Corpus and ELITR-Bench Dataset using both LLM-as-judge (Prometheus) and human evaluation.

## Key Results
- Human evaluation: IR-only scored 5.65, IR+AMR scored 5.55 on 0-10 scale
- AMR-only scored lowest at 3.94, confirming the importance of original sentence context
- IR+AMR outperformed IR-only on 18/130 questions, with 9 being WHO questions
- Automatic LLM-as-judge evaluation found no significant differences between configurations
- AMR improved WHO question accuracy from 39/45 to 39/45 (same or better score)

## Why This Works (Mechanism)

### Mechanism 1: Dual-Index Retrieval with Context Expansion
Combining dense sentence embeddings with Doc2Query synthetic queries, then expanding retrieved sentences with adjacent context, improves recall for dispersed meeting information. Dense embeddings capture semantic similarity while Doc2Query enables lexical matching paths. The union of both retrieval methods reduces false negatives. Adding ±1 surrounding sentences addresses the observation that answers often span multiple adjacent turns.

### Mechanism 2: AMR-Based Semantic Role Explicitation
Converting AMR graphs of retrieved sentences into natural language descriptions improves entity resolution, particularly for "who" questions. AMR graphs explicitly encode semantic roles as structured triples, making agent-patient relationships unambiguous. LLMs receive explicit entity-role bindings rather than inferring from potentially ambiguous surface text.

### Mechanism 3: Sequential Context Preservation
Ordering retrieved passages by original transcript position (not relevance score) preserves discourse coherence for answer generation. Meeting transcripts are inherently sequential—arguments develop across turns. Reordering by relevance score fragments narrative flow. Preserving original order maintains temporal and discourse markers that help LLMs track speaker contributions and event sequences.

## Foundational Learning

- **Abstract Meaning Representation (AMR)**: A rooted, directed, acyclic graph where nodes are predicates/entities and edges are semantic relations (:ARG0, :ARG1, etc.). Needed for implementing the AMR-to-text conversion pipeline.
  - Quick check: Given the AMR triple (speaker, :ARG0, announce-01), what does :ARG0 indicate about the relationship?

- **Doc2Query / Document Expansion**: A seq2seq model that generates likely queries for each document, indexing these synthetic queries to enable lexical matching at retrieval time. Needed for the dual-index retrieval strategy.
  - Quick check: Why would generating synthetic queries for a sentence improve retrieval compared to dense embeddings alone?

- **LLM-as-Judge Evaluation**: Using a large language model (Prometheus) to score answers 0-5 against reference answers. The primary automatic evaluation metric, though it introduces evaluator bias.
  - Quick check: What are two failure modes when using LLM-as-judge instead of human evaluation?

## Architecture Onboarding

- Component map: Question + Transcript → Dual-Index Retrieval (Dense + Doc2Query) → Context Expansion (±1) → Cr (IR context) → AMR Parsing → Triple Extraction → Rule-based Triple→Sentence → LLM Polish → Camr (AMR context) → Answer Generation (LLM)

- Critical path: Dense embedding quality → Retrieval recall → AMR parsing accuracy → Triple-to-text rule coverage → LLM polish quality → Final answer generation. Errors in AMR parsing or incomplete triple-to-text rules cascade into degraded Camr context.

- Design tradeoffs:
  - IR-only vs IR+AMR: IR-only is faster; IR+AMR improves WHO questions but adds latency and compute (requires GPU)
  - AMR-only vs IR+AMR: AMR-only discards original sentence phrasing—AMR-only scores lowest (3.94 human vs 5.65 IR-only)
  - Context length: Adding AMR descriptions doubles context size for IR+AMR variant, potentially hitting context window limits

- Failure signatures:
  - Low scores on factoid questions with specific numbers/dates: AMR may not preserve numerical precision
  - Cross-lingual degradation (Czech answers): "Many underlying components... are primarily trained on English data"
  - LLM-as-judge scores 2 with human scores 8-10: Ground truth is sentence fragment, LLM generates full sentence—format mismatch

- First 3 experiments:
  1. **Baseline retrieval comparison**: Run IR-only on Dev split (141 questions). Measure LLM-as-judge scores. Identify questions with scores ≤2 and categorize failure types (retrieval miss vs. generation error)
  2. **AMR ablation on WHO questions**: Filter Dev split to WHO-type questions (~45 in Test). Compare IR-only vs IR+AMR on this subset. Verify whether AMR improves entity resolution as claimed
  3. **Context expansion sensitivity**: Test ±0, ±1, ±2 sentence expansion on Dev split. Measure retrieval recall and final answer quality to validate the ±1 design choice

## Open Questions the Paper Calls Out

### Open Question 1
Does selectively applying AMR enrichment only to entity-related questions (e.g., "who," "whose") improve overall QA performance compared to uniform application across all question types? The study applied AMR uniformly but found it helped only ~35% of questions, with 50% of improved cases being WHO questions (18/130 total, 9 being WHO questions).

### Open Question 2
Why does LLM-as-judge evaluation fail to capture the qualitative improvements in AMR-enhanced answers that human evaluators recognize? Tables 1-2 and Figures 3-6 show human evaluators scored IR+AMR (5.55) nearly equal to IR-only (5.65), while LLM-as-judge scored IR+AMR lower (3.35) than IR-only (4.09), suggesting evaluation metric misalignment.

### Open Question 3
How can AMR-to-text conversion be improved to better preserve entity-role relationships for downstream QA tasks? The current conversion uses rule-based triple translation followed by LLM polishing, which may lose fine-grained semantic role information critical for distinguishing participants in meetings.

## Limitations

- AMR parser model/version not specified, making it impossible to assess whether improvements hold across different AMR parsing systems
- Exact Doc2Query synthetic query generation process and rule-based AMR-to-text conversion templates are unspecified, creating significant reproducibility gaps
- Human evaluation showed negligible difference (5.65 vs 5.55), suggesting the claimed AMR benefit is marginal and may be subject to evaluator bias

## Confidence

- **High confidence**: The dual-index retrieval mechanism combining dense embeddings and Doc2Query improves recall compared to single-method approaches
- **Medium confidence**: AMR improves entity resolution for WHO questions, based on the 39/45 WHO question success rate, though this depends critically on AMR parsing quality
- **Low confidence**: The overall system superiority claim, given that human evaluation shows negligible difference and LLM-as-judge found no significant differences

## Next Checks

1. **AMR parser dependency test**: Replace the unspecified AMR parser with two different publicly available AMR parsers (e.g., AMRBART and SPRING) and measure the variance in WHO question performance to establish whether improvements are parser-dependent

2. **Retrieval recall baseline**: Manually verify whether gold answer spans appear in retrieved context for 20 randomly sampled questions where IR-only scored ≤3, to determine if low scores stem from retrieval failure vs generation quality

3. **Question-type sensitivity analysis**: Conduct controlled ablation testing on Dev split, filtering by question type (WHO, WHAT, WHEN, WHERE) to quantify whether AMR benefits are specific to entity questions or generalize across question types