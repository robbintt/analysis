---
ver: rpa2
title: 'Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical
  vision'
arxiv_id: '2508.09681'
source_url: https://arxiv.org/abs/2508.09681
tags:
- tracking
- depth
- error
- optimisation
- camera
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Surg-InvNeRF, a novel test-time optimisation
  approach for long-term 3D point tracking in surgical environments. The method uses
  an invertible Neural Radiance Field (InvNeRF) architecture to aggregate short-term
  pixel correspondences and achieve consistent 3D tracking.
---

# Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision

## Quick Facts
- **arXiv ID:** 2508.09681
- **Source URL:** https://arxiv.org/abs/2508.09681
- **Reference count:** 40
- **Primary result:** Novel test-time optimisation approach achieving 50% improvement in 2D tracking and 6.2mm average 3D tracking error

## Executive Summary
Surg-InvNeRF introduces a novel test-time optimisation approach for long-term 3D point tracking in surgical environments using an invertible Neural Radiance Field (InvNeRF) architecture. The method aggregates short-term pixel correspondences to achieve consistent 3D tracking, incorporating bidirectional deformable-canonical mapping, efficient workspace handling, guided ray density, and multi-scale HexPlanes for fast inference. The framework uses a pixel sampling algorithm to reduce redundancy and a multi-space loss function for cross-consistency. Experiments on STIR and SCARE datasets demonstrate superior performance compared to state-of-the-art methods, with nearly 50% improvement in 2D tracking average precision and 6.2mm average tracking error for 3D tracking. The method also supports image rendering with PSNR of 22.43 and SSIM of 0.557.

## Method Summary
Surg-InvNeRF is a test-time optimisation framework that leverages invertible NeRF architecture for 3D point tracking in surgical vision. The approach aggregates short-term pixel correspondences using bidirectional deformable-canonical mapping between canonical and deformed workspaces. Key innovations include guided ray density for efficient sampling, multi-scale HexPlanes for fast inference, and a pixel sampling algorithm to reduce redundancy. The framework employs a multi-space loss function for cross-consistency across different representations. During test-time optimisation, the model incrementally refines tracking accuracy by optimizing over short-term correspondences, enabling robust long-term 3D point tracking in challenging surgical environments.

## Key Results
- Achieved nearly 50% improvement in average precision for 2D tracking compared to state-of-the-art methods
- Delivered 6.2mm average tracking error and 3.2mm average rendered location error for 3D tracking
- Rendered images with PSNR of 22.43 and SSIM of 0.557

## Why This Works (Mechanism)
The method succeeds by leveraging the geometric consistency and volumetric representation capabilities of invertible NeRF architectures. By maintaining a bidirectional mapping between canonical and deformed workspaces, the system can track points across deformations while preserving spatial relationships. The guided ray density optimization focuses computational resources on informative rays, while the multi-scale HexPlanes provide efficient hierarchical feature extraction. The pixel sampling algorithm intelligently reduces redundancy without sacrificing tracking accuracy, and the multi-space loss function ensures consistency across different representation spaces, enabling robust long-term tracking.

## Foundational Learning

**Neural Radiance Fields (NeRF):** Volumetric rendering technique that represents scenes as continuous functions mapping 5D coordinates to color and density. Why needed: Provides differentiable 3D representation for tracking and reconstruction. Quick check: Can reconstruct novel views from a single viewpoint.

**Invertible Neural Networks:** Networks designed to be invertible, allowing for efficient sampling and density estimation. Why needed: Enables bidirectional mapping between canonical and deformed workspaces. Quick check: Forward and inverse passes are computationally efficient.

**Test-time Optimization:** Dynamic model adaptation during inference rather than relying solely on pre-trained weights. Why needed: Allows incremental refinement of tracking accuracy in surgical environments. Quick check: Improves performance on novel or challenging sequences.

**HexPlanes:** Multi-scale planar representations for efficient feature extraction. Why needed: Provides hierarchical spatial representation for fast inference. Quick check: Reduces computational complexity while maintaining accuracy.

**Deformable-Canonical Mapping:** Bidirectional transformation between deformed and canonical coordinate spaces. Why needed: Enables tracking through tissue deformations and surgical manipulations. Quick check: Preserves point correspondences across workspace transformations.

## Architecture Onboarding

**Component Map:** Image input -> HexPlanes feature extraction -> Deformable-Canonical Mapping -> InvNeRF volumetric representation -> Pixel sampling -> Multi-space loss optimization -> 3D tracking output

**Critical Path:** Image acquisition → HexPlanes feature extraction → Deformable-Canonical Mapping → InvNeRF optimization → Pixel sampling → Loss computation → Parameter update

**Design Tradeoffs:** Prioritizes tracking accuracy over real-time performance, accepts test-time optimization overhead for improved precision, trades model complexity for robustness to surgical environment challenges.

**Failure Signatures:** Performance degradation under extreme lighting variations, tracking loss during severe occlusions, reduced accuracy with rapid tissue deformations exceeding model capacity.

**Three First Experiments:** 1) Evaluate tracking accuracy on synthetic surgical sequences with known ground truth, 2) Compare performance across different pixel sampling rates, 3) Assess sensitivity to initial workspace estimation accuracy.

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but the methodology suggests several areas for future investigation, including real-time performance optimization, extension to dynamic surgical scenes, and integration with other surgical vision tasks.

## Limitations
- Validation primarily focused on synthetic or semi-synthetic surgical datasets
- Limited testing in real clinical scenarios across diverse surgical procedures
- Computational efficiency claims would benefit from more detailed ablation studies

## Confidence

**Performance claims on STIR and SCARE datasets:** High
**Generalizability to real clinical settings:** Medium
**Computational efficiency claims:** Medium
**Robustness to diverse surgical scenarios:** Low

## Next Checks
1. Conduct extensive testing on real surgical video sequences from multiple institutions to validate robustness across different procedures and imaging conditions
2. Perform ablation studies comparing the proposed pixel sampling algorithm with alternative redundancy reduction approaches
3. Evaluate tracking performance under varying lighting conditions and occlusions typical in actual surgical environments