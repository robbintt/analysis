---
ver: rpa2
title: 'Combinatorial Creativity: A New Frontier in Generalization Abilities'
arxiv_id: '2509.21043'
source_url: https://arxiv.org/abs/2509.21043
tags:
- creativity
- creative
- combinatorial
- novelty
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces combinatorial creativity as a new form of
  generalization in LLMs, where models generate novel ideas by combining familiar
  concepts. The authors develop a theoretical framework and algorithmic task to evaluate
  creativity through degrees of novelty and utility, using synthetic graphs where
  models must find valid paths between nodes while satisfying logical constraints.
---

# Combinatorial Creativity: A New Frontier in Generalization Abilities

## Quick Facts
- arXiv ID: 2509.21043
- Source URL: https://arxiv.org/abs/2509.21043
- Reference count: 26
- Primary result: Introduces combinatorial creativity framework showing creativity improves with model size and benefits from wider, shallower architectures

## Executive Summary
This paper introduces combinatorial creativity as a novel form of generalization in large language models, where models generate novel ideas by combining familiar concepts. The authors develop a theoretical framework and algorithmic task to evaluate creativity through degrees of novelty and utility, using synthetic graphs where models must find valid paths between nodes while satisfying logical constraints. Through extensive experiments across 1M-100M parameter models, they reveal three key findings: creativity scales predictably with model size and compute, optimal architectural configurations exist (wider, shallower models outperform deeper, narrower ones for fixed compute budgets), and the well-known ideation-execution gap in scientific idea generation can be explained by a fundamental novelty-utility tradeoff that persists across all scales studied.

## Method Summary
The authors develop a controlled framework for evaluating combinatorial creativity using synthetic graphs where nodes represent concepts and edges represent valid combinations. Models must find paths between nodes while satisfying logical constraints, with creativity measured through degrees of novelty and utility. The framework allows systematic variation of model sizes (1M-100M parameters), architectural configurations, and task complexity. Experiments systematically test how creativity scales with compute, which architectural configurations maximize creative performance, and whether fundamental tradeoffs exist between novelty and utility in creative generation.

## Key Results
- Creativity in LLMs improves predictably with model size and compute resources
- Wider, shallower models outperform deeper, narrower models for fixed compute budgets in creative tasks
- The ideation-execution gap in scientific idea generation is explained by a fundamental novelty-utility tradeoff that persists across all model scales

## Why This Works (Mechanism)
The framework works by creating a controlled environment where creativity can be precisely measured through combinatorial problem-solving. By using synthetic graphs with logical constraints, the authors isolate the creative process from confounding factors present in real-world tasks. The degrees of novelty and utility framework provides quantifiable metrics that capture the essential tension in creative work: generating ideas that are both original and useful. The systematic variation of model architectures and sizes reveals how different configurations affect creative capabilities, while the scalability analysis shows that creativity is a fundamental capability that emerges with sufficient model capacity.

## Foundational Learning
- **Combinatorial Creativity**: The ability to generate novel ideas by recombining existing concepts; needed to understand how LLMs can produce creative outputs beyond memorization; quick check: can the model generate valid solutions that weren't in training data
- **Novelty-Utility Tradeoff**: The fundamental tension between generating original ideas and ensuring they are useful/practical; needed to explain why creative models often fail at execution; quick check: does increasing novelty decrease solution quality
- **Architectural Scaling Laws**: How different model configurations (depth vs width) affect performance; needed to identify optimal designs for creative tasks; quick check: compare performance across architectures with equal parameter counts
- **Synthetic Evaluation Tasks**: Controlled environments for measuring specific capabilities; needed to isolate creativity from other factors; quick check: can the task generalize to real-world creative benchmarks
- **Compute-Parameter Relationships**: How computational resources affect model capabilities; needed to understand scaling behavior of creative abilities; quick check: measure performance across different compute budgets
- **Constraint Satisfaction in Generation**: Ensuring generated outputs meet specific requirements; needed for measuring utility in creative outputs; quick check: verify all generated solutions satisfy logical constraints

## Architecture Onboarding
**Component Map**: Synthetic Graph Generator -> Model Architecture -> Path Finding Algorithm -> Novelty-Utility Evaluator -> Performance Metrics
**Critical Path**: The evaluation pipeline from graph generation through to performance measurement, with the model architecture as the primary variable being tested
**Design Tradeoffs**: Depth vs width in model architecture (wider models better for creativity but may use more memory), constraint strictness (tighter constraints ensure utility but may limit novelty), graph complexity (more complex graphs better test creativity but harder to solve)
**Failure Signatures**: Models getting stuck in local optima, failing to satisfy constraints despite high novelty, performance plateauing despite increased parameters, or the novelty-utility tradeoff becoming too extreme
**First 3 Experiments**: 1) Test creativity scaling by varying model size from 1M to 100M parameters, 2) Compare wider vs deeper architectures with equal parameter counts, 3) Vary constraint strictness to measure effects on the novelty-utility tradeoff

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- The synthetic nature of evaluation tasks may not capture full complexity of real-world creative domains
- The degrees of novelty and utility framework is theoretical and not validated against human creative output
- Optimal architectural findings (wider, shallower models) may be specific to this combinatorial task rather than creativity broadly

## Confidence
**High Confidence**: Model scaling effects (creativity improves with size/compute) are well-supported by experimental results
**Medium Confidence**: Architectural optimization results (wider, shallower models) are context-dependent and may not generalize to all creative tasks
**Low Confidence**: Theoretical framework claims and explanations of ideation-execution gap require more validation beyond synthetic experimental setup

## Next Checks
1. **Human Validation Study**: Compare LLM-generated creative outputs against human-generated solutions for same combinatorial problems, measuring both objective metrics and subjective creativity assessments
2. **Real-World Transfer Experiments**: Test whether optimal architectural configurations (wider, shallower models) perform better on established creative benchmarks like story generation or scientific hypothesis generation
3. **Ablation on Constraint Types**: Systematically vary types and strictness of logical constraints to determine robustness of novelty-utility tradeoff across different problem structures