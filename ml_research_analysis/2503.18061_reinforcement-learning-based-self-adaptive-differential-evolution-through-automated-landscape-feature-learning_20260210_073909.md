---
ver: rpa2
title: Reinforcement Learning-based Self-adaptive Differential Evolution through Automated
  Landscape Feature Learning
arxiv_id: '2503.18061'
source_url: https://arxiv.org/abs/2503.18061
tags:
- optimization
- rlde-afl
- algorithm
- learning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automated algorithm design
  in black-box optimization by introducing RLDE-AFL, a reinforcement learning-based
  self-adaptive differential evolution method with automated landscape feature learning.
  The core method integrates a learnable feature extraction module based on NeurELA,
  an attention-based neural network with mantissa-exponent embedding, to automatically
  extract expressive optimization states from population solutions and objective values.
---

# Reinforcement Learning-based Self-adaptive Differential Evolution through Automated Landscape Feature Learning

## Quick Facts
- **arXiv ID**: 2503.18061
- **Source URL**: https://arxiv.org/abs/2503.18061
- **Reference count**: 40
- **Primary result**: RLDE-AFL achieves superior performance on black-box optimization benchmarks compared to traditional DE and meta-learning baselines through automated landscape feature learning and reinforcement learning-based algorithm configuration.

## Executive Summary
This paper addresses automated algorithm design in black-box optimization by introducing RLDE-AFL, a reinforcement learning-based self-adaptive differential evolution method with automated landscape feature learning. The core innovation is integrating a learnable feature extraction module based on NeurELA, an attention-based neural network with mantissa-exponent embedding, to automatically extract expressive optimization states from population solutions and objective values. This eliminates the need for manual feature engineering. The method incorporates a comprehensive algorithm configuration space with 14 mutation and 3 crossover operators, controlled by a reinforcement learning agent using Proximal Policy Optimization (PPO). Extensive experiments demonstrate RLDE-AFL's superior performance compared to advanced traditional DE algorithms and recent meta-black-box optimization baselines on both synthetic and realistic problems.

## Method Summary
The method integrates automated landscape feature learning with reinforcement learning-based self-adaptive differential evolution. A learnable feature extraction module based on NeurELA automatically extracts expressive optimization states from population solutions and objective values, eliminating manual feature engineering. The RLDE-AFL agent uses PPO to select from 14 mutation operators and 3 crossover operators based on these learned features. The comprehensive algorithm configuration space allows dynamic adaptation to different optimization landscapes. The feature extraction module is co-trained with the DAC policy, enabling the system to learn both what features are important and how to use them for algorithm selection. This approach demonstrates zero-shot generalization across different problem dimensions and evaluation budgets.

## Key Results
- RLDE-AFL outperforms advanced traditional DE algorithms and recent meta-black-box optimization baselines on synthetic benchmark problems
- The method shows robust zero-shot generalization across different problem dimensions (30D, 50D, 100D) without retraining
- RLDE-AFL maintains competitive performance even with limited evaluation budgets (1000-10000 function evaluations)
- Ablation studies validate the effectiveness of the NeurELA-based feature extractor and demonstrate the necessity of co-training the feature learning module with the DAC policy

## Why This Works (Mechanism)
The success of RLDE-AFL stems from its ability to automatically learn relevant optimization landscape features and use them for intelligent algorithm selection. Traditional DE methods rely on fixed operators and manual feature engineering, which may not capture the full complexity of optimization landscapes. By contrast, RLDE-AFL's NeurELA-based feature extractor learns to identify salient characteristics of the optimization state, such as population diversity, convergence trends, and local geometry. The PPO controller then uses these learned features to select the most appropriate mutation and crossover operators for the current optimization context. This adaptive approach allows the algorithm to respond dynamically to changing landscape characteristics, rather than being constrained by predetermined operator combinations.

## Foundational Learning

**Differential Evolution (DE)**: A population-based evolutionary algorithm for continuous optimization that iteratively improves candidate solutions through mutation, crossover, and selection operations. *Why needed*: Provides the base optimization framework that RLDE-AFL enhances through self-adaptation. *Quick check*: Understand basic DE operators (rand/1, best/2, etc.) and their typical use cases.

**Reinforcement Learning (RL)**: A machine learning paradigm where an agent learns to make sequential decisions by interacting with an environment and receiving rewards. *Why needed*: Enables the algorithm to learn when to apply different DE operators based on optimization landscape features. *Quick check*: Familiarity with PPO algorithm and its advantages over other RL methods.

**Landscape Features**: Quantitative descriptors of optimization problem characteristics, such as modality, separability, and conditioning. *Why needed*: Provide the basis for algorithm selection decisions in meta-learning approaches. *Quick check*: Understand common landscape features like dispersion, convexity, and ruggedness measures.

**Neural Architecture Search (NeurELA)**: A neural network architecture specifically designed for extracting landscape features from optimization populations. *Why needed*: Automates the feature extraction process, eliminating manual engineering and capturing more complex relationships. *Quick check*: Understand the attention mechanism and mantissa-exponent embedding used in NeurELA.

**Zero-shot Generalization**: The ability of a model to perform well on new tasks or conditions without additional training. *Why needed*: Demonstrates the robustness and transferability of the learned algorithm selection policy. *Quick check*: Understand the difference between zero-shot and few-shot learning in meta-learning contexts.

## Architecture Onboarding

**Component Map**: NeurELA Feature Extractor -> PPO Controller -> DE Operator Selection -> Population Update -> Objective Evaluation -> Feature Extraction

**Critical Path**: The most time-critical path is Feature Extraction -> PPO Decision -> Operator Application -> Evaluation. Each component must operate efficiently to maintain optimization speed, with the feature extraction module being the primary computational overhead.

**Design Tradeoffs**: The method trades increased computational complexity (NeurELA feature extraction) for improved optimization performance and adaptability. Alternative approaches could use fixed landscape features or simpler controller architectures, but these would sacrifice the adaptive capabilities that drive performance gains.

**Failure Signatures**: Potential failure modes include feature extraction module overfitting to training landscapes, PPO controller converging to suboptimal policies, or computational overhead making the method impractical for expensive evaluations. Monitoring feature learning stability and policy convergence is essential.

**Three First Experiments**:
1. Test RLDE-AFL on a simple synthetic benchmark (e.g., Sphere function) to verify basic functionality and feature extraction quality
2. Compare feature importance learned by NeurELA against hand-engineered landscape features on a benchmark suite
3. Evaluate the sensitivity of RLDE-AFL's performance to different PPO hyperparameters and training durations

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond suggesting future work on extending the approach to other optimization paradigms and exploring more sophisticated feature learning architectures.

## Limitations
- The method's reliance on 100,000 function evaluations may limit applicability to truly expensive black-box optimization problems
- The NeurELA feature extraction module adds computational overhead that is not thoroughly characterized
- The ablation study confirms the importance of co-training but doesn't explore alternative architectures or hyperparameter sensitivity of the feature extractor itself

## Confidence
- **High Confidence**: The PPO-based controller implementation, comprehensive benchmark comparison methodology, and zero-shot generalization results across different problem dimensions
- **Medium Confidence**: The absolute performance improvements over baselines, as some comparisons use different evaluation budgets and the NeurELA feature extraction's contribution could be partially attributed to increased model capacity
- **Medium Confidence**: The claims about "robust" generalization, as the out-of-distribution realistic problems test set, while diverse, may not fully represent all possible real-world optimization scenarios

## Next Checks
1. Evaluate RLDE-AFL on problems with severely constrained evaluation budgets (e.g., 1,000-10,000 evaluations) to assess practical applicability to expensive optimization scenarios
2. Conduct a systematic ablation study varying the architecture and hyperparameters of the NeurELA feature extractor to quantify its exact contribution versus using fixed landscape features
3. Test RLDE-AFL on a broader range of real-world problems, including those with mixed discrete-continuous variables and noisy evaluations, to validate robustness claims beyond the current benchmark suite