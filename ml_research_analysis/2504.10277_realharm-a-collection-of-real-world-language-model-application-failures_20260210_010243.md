---
ver: rpa2
title: 'RealHarm: A Collection of Real-World Language Model Application Failures'
arxiv_id: '2504.10277'
source_url: https://arxiv.org/abs/2504.10277
tags:
- systems
- content
- language
- user
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RealHarm is a dataset of 136 annotated real-world incidents involving
  language model application failures, systematically collected from public sources.
  The dataset categorizes failures by type, impact, and cause, revealing that misinformation
  constitutes the most common hazard while reputational damage dominates organizational
  harms.
---

# RealHarm: A Collection of Real-World Language Model Application Failures

## Quick Facts
- **arXiv ID:** 2504.10277
- **Source URL:** https://arxiv.org/abs/2504.10277
- **Reference count:** 15
- **Primary result:** Dataset of 136 annotated real-world incidents showing moderation systems detect only 10-50% of unsafe content

## Executive Summary
RealHarm is a dataset of 136 annotated real-world incidents involving language model application failures, systematically collected from public sources. The dataset categorizes failures by type, impact, and cause, revealing that misinformation constitutes the most common hazard while reputational damage dominates organizational harms. Evaluation of ten moderation systems shows they detect only 10-50% of unsafe content with varying false positive rates, highlighting significant gaps in current safeguards. State-of-the-art LLMs with taxonomy-based prompts outperform traditional moderation systems. The dataset provides an evidence-based framework for assessing AI deployment risks, complementing theoretical approaches by grounding safety research in documented incidents rather than speculative scenarios.

## Method Summary
The authors collected 136 incidents through systematic review of AI Incident Database, social media, and news websites, focusing on text-to-text AI systems causing deployer harm. Each incident includes the original conversation, hazard taxonomy labels (10 categories), harm type annotations (3 types), severity ratings, and a "safe" rewritten version. Ten moderation systems were evaluated using paired TPR/FPR metricsâ€”measuring detection on unsafe samples against false positive rates on safe rewrites. Systems were processed at either conversation or message level depending on capability, with prompt injection detectors enabled where available.

## Key Results
- Misinformation and fabrication accounts for over one-third of all incidents
- Reputational damage constitutes nearly 90% of harms affecting deployers
- Commercial moderation systems detect only 10-50% of unsafe content
- Generic LLMs with taxonomy-based prompts outperform specialized guardrail systems
- Over 10% of incidents resulted in complete system shutdown

## Why This Works (Mechanism)

### Mechanism 1: Empirical Hazard Discovery via Incident Retrospective
- **Claim:** Real-world AI safety gaps can be identified more accurately by analyzing documented deployment failures than by deriving categories from theoretical frameworks alone.
- **Mechanism:** The RealHarm dataset compiles 136 annotated examples from a systematic review of over 700 public incidents. Each includes the original conversation, a taxonomy label (10 categories), and a "safe" rewritten version. This enables teams to validate guardrails against prompt/response patterns that have already caused organizational harm.
- **Core assumption:** Publicly reported failures are representative enough of production risks to guide testing priorities, despite selection bias.
- **Evidence anchors:** [abstract] "In this work, we introduce RealHarm, a dataset of annotated problematic interactions with AI agents built from a systematic review of publicly reported incidents."
- **Break condition:** If your risk profile differs substantially from public chatbot failures (e.g., fully internal enterprise tools), RealHarm's patterns may underrepresent your actual risk surface.

### Mechanism 2: Guardrail Stress-Testing With Paired Unsafe/Safe Samples
- **Claim:** Moderation systems can be evaluated on real-world failures by measuring detection rate (TPR on "unsafe" samples) against false positive rate (flags on "safe" rewrites).
- **Mechanism:** Each incident (unsafe) is paired with a corrected response (safe) that removes misbehavior but preserves context. Running guardrails on both yields paired TPR/FPR metrics, surfacing the trade-off between missing harm and over-blocking acceptable content.
- **Core assumption:** The "safe" rewrites are truly representative of acceptable model behavior for a given domain.
- **Evidence anchors:** [section 3] "The dataset is structured into two subsets of equal size: unsafe set... safe set... where the AI system responses have been corrected to remove the misbehavior."
- **Break condition:** If your moderation policy differs significantly from the generic "safe" rewrite assumptions, you must create your own safe examples to avoid misestimating guardrail performance.

### Mechanism 3: Deployer-Centric Harm Taxonomy to Prioritize Safeguards
- **Claim:** Organizational harm categories (reputation, legal, financial) plus hazard types (misinformation, operational disruption, etc.) allow teams to align safety investment with concrete business impact.
- **Mechanism:** RealHarm's taxonomy annotates each failure by hazard (10 types) and harm (3 types), with severity ratings. Misinformation accounts for ~33% of incidents; reputational damage ~87% of harms. This structured labeling enables prioritization toward the most frequent and severe business risks.
- **Core assumption:** Historical severity/frequency will correlate with future organizational risk.
- **Evidence anchors:** [section 4.1] "Reputation damage constitutes the predominant harm affecting AI system deployers, accounting for almost 90% of the incidents."
- **Break condition:** If your deployment's risk tolerance or regulatory environment differs substantially (e.g., high-regulation healthcare vs. low-risk consumer chat), RealHarm's frequency/severity weights may not match your risk ranking.

## Foundational Learning

- **Incident-based vs. Principle-based Safety Datasets**
  - **Why needed here:** RealHarm uses a bottom-up, retrospective method rather than generating synthetic prompts from regulations/policies. Understanding this distinction is necessary to interpret claims about reduced subjectivity and the biases inherent in public incident reporting.
  - **Quick check question:** Why might a dataset of public chatbot failures under-represent certain risks compared to a principle-based taxonomy?

- **Confusion Matrix Basics for Moderation Evaluation**
  - **Why needed here:** The paper reports TPR, FPR, precision, and recall. Knowing how these metrics trade off is essential to interpret results (e.g., high TPR with high FPR is often unusable in production).
  - **Quick check question:** Given a guardrail with 70% TPR and 40% FPR, would you consider it production-ready without further tuning? Why or why not?

- **Hallucination/Misinformation in LLMs**
  - **Why needed here:** Misinformation is the most common hazard in RealHarm, tied to model hallucination. Familiarity with why LLMs generate plausible but false content helps explain both the underlying cause and why generic content filters struggle to detect it.
  - **Quick check question:** Why would a content moderation API trained primarily on toxicity fail to detect a confidently stated factual error from an LLM?

## Architecture Onboarding

- **Component map:** Public incident sources -> YAML extraction -> Human annotation -> Safe rewrite generation -> Moderation system evaluation -> Paired TPR/FPR metrics
- **Critical path:** 1) Identify candidate incidents from public sources using defined scope (text-to-text AI, documented interaction, harm to deployer) 2) Extract and format conversation into YAML with metadata 3) Annotate with hazard taxonomy and severity 4) Generate and verify safe rewrite 5) Run moderation/guardrail systems against both unsafe and safe sets 6) Compute paired metrics; analyze per-category performance
- **Design tradeoffs:** Public incident bias (dataset reflects only publicly reported failures); small sample size (68 unsafe samples limit statistical power); message vs. conversation evaluation (some systems only evaluate single messages); generic vs. custom taxonomy (10-category taxonomy enables comparability but may not match domain-specific policies)
- **Failure signatures:** Guardrails miss misinformation (10-50% detection by commercial APIs); high false positives in specialized guardrails (Lakera 46.48% FPR, LLMGuard 40.23% FPR); context disconnection (systems lacking conversation support may flag individual messages while missing interaction-level harm); severity correlation (operational disruption incidents are less frequent but disproportionately high-impact)
- **First 3 experiments:** 1) Baseline guardrail assessment: Run your current moderation stack on the unsafe/safe splits; calculate paired TPR/FPR to identify the largest detection gaps by hazard category 2) Category-specific tuning: For the top 2-3 hazard categories where detection is weakest (likely misinformation, operational disruption), prototype enhanced detection (e.g., fact-checking tools, policy-aware prompts) and measure improvement 3) Incident response simulation: Select 5-10 RealHarm examples marked "high severity"; simulate your organization's incident response workflow (detection, escalation, remediation) to identify process gaps that technical safeguards alone cannot address

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do risk profiles and taxonomy categories differ for multimodal AI systems (image, audio, video) compared to text-only systems?
- **Basis in paper:** [explicit] The limitations section states "while multimodal systems (incorporating image, audio, or video) may present additional or different risk profiles that remain unexplored in our framework."
- **Why unresolved:** RealHarm exclusively covers text-to-text interactions, excluding visual and audio modalities from analysis.
- **What evidence would resolve it:** A multimodal incident dataset with comparative analysis showing whether hazard frequencies, severity distributions, and organizational harm types differ significantly from text-only deployments.

### Open Question 2
- **Question:** What methodological approaches can mitigate the selection bias introduced by relying solely on publicly reported incidents?
- **Basis in paper:** [explicit] The limitations section notes "Incidents kept private by organizations remain invisible to our analysis. This could potentially introduce bias in our classification, particularly regarding which types of harms receive public attention."
- **Why unresolved:** Organizations may systematically suppress certain incident types (e.g., financial losses vs. reputational damage), skewing observed distributions.
- **What evidence would resolve it:** Partnership with deployers to compare public vs. private incident logs, or surveys quantifying the gap between reported and actual incidents across harm categories.

### Open Question 3
- **Question:** How can misinformation detection in moderation systems be improved without access to ground truth or external knowledge sources?
- **Basis in paper:** [explicit] Section 5.2 identifies "Without access to ground truth or reliable information sources, systems miss factual inaccuracies and fabrications" as an intrinsic limitation of current content moderation approaches.
- **Why unresolved:** Misinformation and fabrication constitute the most prevalent hazard (over one-third of incidents), yet detection requires verifying claims against facts the system may not have access to.
- **What evidence would resolve it:** Development and benchmarking of retrieval-augmented or fact-checking integrated moderation systems on RealHarm misinformation samples to measure improvement over current detection rates.

## Limitations
- Reliance on publicly reported incidents introduces selection bias toward high-profile failures
- Small sample size (68 unsafe examples) limits statistical power for definitive conclusions
- Manual annotation process introduces subjective elements in hazard classification and severity assessment

## Confidence
- **High Confidence:** Dataset compilation methodology, incident collection process, and paired evaluation design are well-documented and reproducible
- **Medium Confidence:** Characterization as complementary to theoretical safety approaches and general findings about misinformation prevalence are well-supported
- **Low Confidence:** Claims about relative performance of different moderation systems and specific threshold recommendations require additional validation

## Next Checks
1. **External Validation:** Apply RealHarm to guardrail systems from organizations with different risk profiles (e.g., healthcare, finance, education) to assess cross-domain generalizability of the taxonomy and evaluation findings
2. **Statistical Power Enhancement:** Expand the dataset by collecting additional incidents from underrepresented regions, languages, or application domains to strengthen statistical conclusions about system performance
3. **Dynamic Risk Assessment:** Conduct longitudinal analysis of RealHarm incidents to identify temporal patterns in AI application failures and validate whether historical frequency/severity correlates with future risk trends