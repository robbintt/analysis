---
ver: rpa2
title: 'Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language
  Models'
arxiv_id: '2510.17028'
source_url: https://arxiv.org/abs/2510.17028
tags:
- uncertainty
- calibration
- semantic
- language
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates prompt sensitivity in large language models
  (LLMs), where semantically equivalent prompts produce inconsistent answer distributions,
  leading to poor uncertainty calibration. The authors frame this as token-level overfitting
  and propose sampling across semantic concept space using paraphrasing perturbations
  to recover better-calibrated uncertainty estimates.
---

# Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models

## Quick Facts
- arXiv ID: 2510.17028
- Source URL: https://arxiv.org/abs/2510.17028
- Reference count: 3
- Primary result: Paraphrasing perturbations improve LLM uncertainty calibration (AUROC up to 88.1±0.5) without sacrificing accuracy

## Executive Summary
This paper addresses prompt sensitivity in large language models, where semantically equivalent prompts produce inconsistent answer distributions leading to poor uncertainty calibration. The authors propose sampling across semantic concept space using paraphrasing perturbations to recover better-calibrated uncertainty estimates. They introduce an embedding-variance-based uncertainty metric that decomposes total uncertainty into epistemic and aleatoric components, improving upon entropy-based methods by modeling semantic continuities. Experiments on TriviaQA and Natural Questions show that paraphrasing perturbations significantly improve calibration without sacrificing accuracy, outperforming temperature scaling and other perturbation methods. Analysis reveals that RLHF fine-tuning increases prompt sensitivity, suggesting alignment training may degrade semantic generalization.

## Method Summary
The method generates n_p paraphrases of each question using an LLM (typically GPT-4), then samples n_s responses per paraphrase from the target black-box LLM at temperature > 0. Total samples m = n_p × n_s. All responses are embedded using graph Laplacian eigenvectors (following Lin et al. 2023), and uncertainty is computed via covariance decomposition: U_t = tr(Σ_t), U_a = tr(Σ_a), U_e = tr(Σ_e). Calibration is evaluated using AUROC, where higher values indicate better discrimination between correct and incorrect answers based on uncertainty estimates. The approach tests various (n_p, n_s) allocations and compares against baselines including temperature scaling and semantic entropy.

## Key Results
- Paraphrasing perturbations significantly improve calibration (AUROC up to 88.1±0.5) without sacrificing accuracy
- Embedding-variance-based decomposition outperforms entropy-based methods by modeling semantic continuities
- RLHF fine-tuning increases prompt sensitivity, with Llama 2-Chat showing higher ρ_u than Llama 2-Base
- Optimal allocation typically favors more perturbations over more samples per prompt (n_p ≥ 3, n_s = 1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Paraphrase sampling recovers better-calibrated uncertainty by approximating the true distribution over semantic concept space rather than individual token representations.
- Mechanism: The paper models semantic concept space as the union of token representations. Using the Birkhoff ergodic theorem, randomly sampling across paraphrases and averaging response distributions converges to the space average Q(c) = (1/m)ΣQ(tᵢ), recovering the model's true distribution over meaning rather than surface form.
- Core assumption: Token representations are equally likely and form connected intervals in semantic space; the model has a stable underlying distribution over meanings that can be recovered through sampling.
- Evidence anchors: [abstract]: "sampling across the semantic 'concept space' with paraphrasing perturbations improves uncertainty calibration"; [Section 3]: "sampling over the token representations, measuring the response distribution Q(tᵢ) at each representation, and averaging over time approaches the true distribution"; [corpus]: "PARC" paper (arxiv 2506.14808) confirms VLMs inherit LLM prompt sensitivity.
- Break condition: If the underlying model has no stable semantic representation (pure memorization with no generalization), paraphrase averaging cannot recover a meaningful distribution.

### Mechanism 2
- Claim: Embedding-variance-based uncertainty decomposition captures semantic continuities that entropy-based methods miss, enabling separate quantification of prompt-sensitivity (epistemic) vs. sampling (aleatoric) uncertainty.
- Mechanism: Rather than discretizing outputs into semantic equivalence classes, the method embeds responses and computes Cov(Y|x) = E[Cov(Y|x,θ)] + Cov(E[Y|x,θ]). Taking the trace yields Ut = Ua + Ue (Eq. 10), preserving additive decomposition while modeling continuous semantic relationships.
- Core assumption: Embedding dimensions represent latent semantic features; covariance across these dimensions meaningfully captures uncertainty dispersion.
- Evidence anchors: [abstract]: "introduce a new metric for uncertainty decomposition in black-box LLMs that improves upon entropy-based decomposition by modeling semantic continuities"; [Section 4]: "entropy-based metrics require discretizing LLM outputs... disregards semantic continuities and partial similarities"; [corpus]: "Calibration Is Not Enough" (arxiv 2601.08064) evaluates confidence estimation under language variations but does not address decomposition.
- Break condition: If the embedding space poorly represents semantic similarity for the task domain, variance decomposition may not meaningfully capture uncertainty.

### Mechanism 3
- Claim: RLHF fine-tuning increases prompt sensitivity by inducing "post-hoc overfitting" to specific response patterns, degrading the model's semantic generalization.
- Mechanism: Base models learn broader distributions during pre-training; RLHF constrains outputs toward preferred formats/behaviors, collapsing output distributions and making them more sensitive to token-level variations rather than semantic content.
- Core assumption: Prompt sensitivity in chat models reflects distribution collapse from alignment training, not an inherent property of scale.
- Evidence anchors: [abstract]: "Analysis of Llama 2-Base vs. Llama 2-Chat reveals that RLHF-tuning increases prompt sensitivity"; [Section 5, Table 2]: Chat model shows higher ρᵤ (prompt sensitivity ratio) than base across all (nₚ, nₛ) configurations; [corpus]: Weak direct corpus validation for RLHF-specific sensitivity claims.
- Break condition: If base models are already poorly calibrated, RLHF effects may be confounded with other factors.

## Foundational Learning

**Uncertainty Calibration (AUROC evaluation)**
- Why needed here: The entire framework evaluates whether uncertainty metrics predict correct vs. incorrect answers; AUROC quantifies this discrimination ability.
- Quick check question: If a model assigns 80% confidence to correct answers and 40% to incorrect answers on average, is it well-calibrated?

**Epistemic vs. Aleatoric Uncertainty**
- Why needed here: The decomposition distinguishes uncertainty about meaning (reducible by better prompts) from inherent ambiguity (irreducible); critical for interpreting ρᵤ.
- Quick check question: If you observe different answers across paraphrases of the same question, which uncertainty type is high?

**Ergodic Theory Basics (Space vs. Time Averages)**
- Why needed here: The theoretical justification for paraphrase sampling relies on the Birkhoff theorem—understanding why time averages converge to space averages explains why the method works.
- Quick check question: Why does sampling from different token representations approach the same limit as exhaustively measuring all representations?

## Architecture Onboarding

**Component map:**
Paraphrase Generator (GPT-4) -> Black-box LLM -> Embedding Model -> Variance Calculator -> Calibration Evaluator

**Critical path:**
1. Question → Paraphrase Generator → nₚ paraphrases
2. Each paraphrase → LLM → nₛ responses (total m = nₚ × nₛ samples)
3. All responses → Embedding Model → embedding matrix Y ∈ ℝ^(m×d)
4. Y → Variance Calculator → (Ut, Ua, Ue, ρᵤ)
5. (Uncertainty, Accuracy) pairs → AUROC

**Design tradeoffs:**
- **More perturbations (nₚ) vs. more samples (nₛ)**: Table 1 shows (6,1) often outperforms (1,6)—new paraphrases provide more marginal information than repeated samples from same prompt. Recommend nₚ ≥ 3 when budget constrained.
- **Embedding choice**: Paper uses Laplacian eigenvectors for evaluation; domain-specific embeddings may improve calibration for specialized tasks.
- **Paraphrase quality vs. diversity**: Aggressive paraphrasing improves calibration most (Figure 3a) but requires robust paraphrase model.

**Failure signatures:**
- **No calibration improvement from perturbations**: Model may already be well-calibrated at individual prompts (like Llama 2-Base)—check ρᵤ against 1/nₛ baseline.
- **Accuracy drops significantly**: Paraphrases may have drifted semantically; verify paraphrase quality with small held-out set.
- **High variance across runs**: Insufficient samples (m < 6); increase total budget or reduce nₚ.

**First 3 experiments:**
1. **Baseline calibration**: Measure AUROC with (nₚ=1, nₛ=6) using entropy and Variance(Total) metrics on 100 questions from your domain.
2. **Perturbation scaling**: Compare (1,6), (2,3), (3,2), (6,1) configurations; plot AUROC vs. nₚ to find optimal allocation for your budget.
3. **Sensitivity diagnosis**: Compute ρᵤ for your model; if ρᵤ > 1/nₛ + 0.1, model is prompt-sensitive and will benefit from paraphrase sampling.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Does the prompt sensitivity induced by RLHF reflect a fundamental structural change in knowledge representation, or is it a surface-level alignment artifact?
- Basis in paper: [explicit] The conclusion states: "Future work may explore whether prompt-sensitivity indicates a deeper structural shift in how these models encode and retrieve knowledge or a superficial artifact of alignment-driven constraints."
- Why unresolved: The paper demonstrates that RLHF-finetuned Llama 2-Chat is more prompt-sensitive than Llama 2-Base (higher ρ_u ratios), but does not investigate the mechanistic cause of this degradation.
- What evidence would resolve it: Probing studies comparing internal representations between base and RLHF models across paraphrases, or ablation studies isolating specific RLHF components (reward modeling vs. PPO optimization vs. SFT stage).

**Open Question 2**
- Question: What is the theoretically optimal allocation of samples between perturbations (n_p) and samples per perturbation (n_s) given a fixed budget m?
- Basis in paper: [explicit] Section 5 poses: "Given a fixed number of samples from the LLM, how should they optimally be distributed over perturbations?" and tests empirical allocations but provides no theoretical guidance.
- Why unresolved: Empirical results suggest prioritizing n_p over n_s improves calibration, but no principled framework predicts optimal ratios for different models, tasks, or uncertainty types.
- What evidence would resolve it: A theoretical model relating calibration gain to perturbation diversity and sample variance, validated across model scales and task domains.

**Open Question 3**
- Question: Do the calibration improvements from paraphrase-based perturbation sampling generalize beyond factual QA to reasoning, code generation, and open-ended tasks?
- Basis in paper: [inferred] The paper evaluates only TriviaQA and Natural Questions—both short-answer factual QA datasets. No experiments test whether semantic-invariant perturbations help where outputs are longer, more structured, or require multi-step reasoning.
- What evidence would resolve it: Experiments applying the framework to reasoning benchmarks (e.g., GSM8K, BBH), code generation (HumanEval), or summarization tasks, with appropriate embedding and evaluation adaptations.

## Limitations

- Paraphrase quality and domain dependence: The method assumes paraphrases preserve semantic meaning exactly, which may not hold in specialized domains (medicine, law, technical documentation).
- Embedding space representation: The paper references graph Laplacian eigenvectors but doesn't specify how affinity graphs are constructed or whether the embedding space captures semantic similarity appropriately for all domains.
- RLHF sensitivity claims: While the paper shows Llama 2-Chat exhibits higher prompt sensitivity than Llama 2-Base, the mechanism attributing this specifically to "post-hoc overfitting" from RLHF lacks direct causal evidence.

## Confidence

**High Confidence Claims:**
- Prompt sensitivity exists and causes calibration problems in LLMs
- Paraphrasing perturbations improve calibration (AUROC) across tested datasets
- Embedding-variance decomposition provides better uncertainty estimates than entropy-based methods

**Medium Confidence Claims:**
- The mechanism of "token-level overfitting" explains prompt sensitivity
- RLHF fine-tuning increases prompt sensitivity through distribution collapse
- Optimal perturbation strategy is (n_p ≥ 3, n_s = 1) for most budgets

**Low Confidence Claims:**
- Generalization to non-QA tasks and specialized domains
- Specific embedding methodology details and their robustness
- Causal attribution of sensitivity differences to RLHF vs. other factors

## Next Checks

1. **Domain Transfer Validation**: Test the method on a specialized domain dataset (e.g., medical QA or legal question answering) with domain-specific paraphrases and embeddings. Measure whether calibration improvements generalize beyond open-domain trivia.

2. **Ablation of Embedding Method**: Replace the graph Laplacian eigenvector embedding with a standard sentence transformer embedding. Compare AUROC improvements and uncertainty decomposition quality to assess sensitivity to embedding methodology.

3. **Controlled RLHF Experiment**: If possible, obtain or train models with varying degrees of RLHF fine-tuning (0%, 25%, 50%, 100%) on the same base model. Measure prompt sensitivity (ρ_u) across this spectrum to directly test whether sensitivity scales with RLHF intensity.