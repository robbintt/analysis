---
ver: rpa2
title: 'SeaExam and SeaBench: Benchmarking LLMs with Local Multilingual Questions
  in Southeast Asia'
arxiv_id: '2502.06298'
source_url: https://arxiv.org/abs/2502.06298
tags:
- seabench
- seaexam
- language
- questions
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces SeaExam and SeaBench, two benchmarks designed
  to evaluate Large Language Models (LLMs) on Southeast Asian (SEA) language tasks
  using real-world scenarios from SEA regions. SeaExam is based on regional educational
  exams, covering subjects like local history and literature, while SeaBench focuses
  on multi-turn, open-ended tasks reflecting daily interactions in SEA communities.
---

# SeaExam and SeaBench: Benchmarking LLMs with Local Multilingual Questions in Southeast Asia

## Quick Facts
- arXiv ID: 2502.06298
- Source URL: https://arxiv.org/abs/2502.06298
- Reference count: 40
- Primary result: Introduces SeaExam and SeaBench benchmarks for evaluating LLMs on Southeast Asian language tasks with real-world scenarios

## Executive Summary
This study introduces SeaExam and SeaBench, two benchmarks specifically designed to evaluate Large Language Models on Southeast Asian language tasks. SeaExam focuses on regional educational exams covering subjects like local history and literature, while SeaBench addresses multi-turn, open-ended tasks reflecting daily interactions in SEA communities. The benchmarks demonstrate superior ability to differentiate LLM performance on SEA language tasks compared to traditional translated benchmarks like MMLU and MT-bench.

## Method Summary
The researchers developed two complementary benchmarks: SeaExam, based on regional educational exams with subjects including local history and literature, and SeaBench, which focuses on multi-turn, open-ended tasks that reflect daily interactions in Southeast Asian communities. The benchmarks cover six SEA languages (Indonesian, Thai, Vietnamese, Tagalog, Malay, Burmese) and were designed to capture authentic regional contexts rather than relying on translated versions of Western-centric benchmarks.

## Key Results
- SeaExam and SeaBench demonstrate higher standard deviations in model performance compared to translated benchmarks like MMLU and MT-bench
- Open-ended questions in SeaBench prove more effective than multiple-choice questions in distinguishing model capabilities
- LLMs generally perform poorly on safety-related questions, highlighting the need for enhanced safety measures in multilingual applications
- SeaBench reveals performance variations within the same model across different SEA languages

## Why This Works (Mechanism)
SeaExam and SeaBench work effectively because they are grounded in authentic Southeast Asian contexts rather than translated Western content. By using real educational materials and everyday scenarios from SEA communities, these benchmarks capture linguistic nuances, cultural references, and regional knowledge that translated benchmarks miss. The multi-turn, open-ended format of SeaBench particularly challenges models to demonstrate deeper understanding and contextual reasoning in ways that single-turn or multiple-choice formats cannot.

## Foundational Learning
- Multilingual evaluation frameworks: Essential for understanding how LLMs perform across different languages and cultural contexts; quick check involves testing models across all six SEA languages in the benchmark
- Educational benchmarking methodology: Critical for creating reliable assessment tools that reflect actual educational standards; quick check involves validation against existing regional exam standards
- Safety evaluation in multilingual contexts: Important for identifying potential risks in cross-cultural applications; quick check involves analyzing failure patterns in safety-related questions

## Architecture Onboarding
Component map: Question Generation -> Benchmark Construction -> Model Evaluation -> Performance Analysis
Critical path: The evaluation pipeline requires careful balance between question authenticity, language coverage, and task diversity to ensure comprehensive assessment of model capabilities
Design tradeoffs: Choosing between open-ended and multiple-choice formats involves balancing evaluation depth against scoring consistency and computational efficiency
Failure signatures: Models showing high variance across SEA languages or poor performance on safety questions indicate limitations in cultural awareness and contextual understanding
First experiments: 1) Compare model performance across all six SEA languages using identical question types, 2) Test safety question performance against standard benchmarks, 3) Analyze correlation between SeaExam/SeaBench scores and real-world task completion rates

## Open Questions the Paper Calls Out
None

## Limitations
- Limited language coverage with only six SEA languages tested, potentially missing important regional variations
- Unclear representativeness across all SEA countries and the distribution of questions across different nations
- Lack of detailed analysis of cultural biases in question creation and handling of code-switching common in SEA multilingual contexts

## Confidence
High confidence: SeaExam and SeaBench show higher standard deviations in model performance compared to translated benchmarks; open-ended questions are more effective than multiple-choice questions in distinguishing model capabilities

Medium confidence: SeaExam and SeaBench more effectively discern LLM performance on SEA language tasks; LLMs perform poorly on safety-related questions

Low confidence: Performance variations within the same model across different SEA languages lack sufficient detail about magnitude and consistency across model families

## Next Checks
1. Conduct cross-validation studies using additional SEA languages not included in current benchmarks (Khmer, Lao, or regional dialects) to assess generalizability across broader SEA linguistic landscape
2. Implement comprehensive cultural bias audit by having questions reviewed by linguistic and cultural experts from different SEA countries to ensure balanced representation and fairness
3. Design comparative study using SeaExam and SeaBench alongside other region-specific benchmarks (SEA-HELM) to validate claims about effective performance differentiation, focusing on correlation analysis with real-world performance metrics