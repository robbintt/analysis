---
ver: rpa2
title: Graph-based Integrated Gradients for Explaining Graph Neural Networks
arxiv_id: '2509.07648'
source_url: https://arxiv.org/abs/2509.07648
tags:
- graph
- gb-ig
- nodes
- integrated
- gradients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces graph-based integrated gradients (GB-IG),
  an extension of the integrated gradients (IG) method for explaining graph neural
  networks. GB-IG addresses the limitation of IG, which assumes continuous data, by
  incorporating graph structure through paths between nodes.
---

# Graph-based Integrated Gradients for Explaining Graph Neural Networks

## Quick Facts
- arXiv ID: 2509.07648
- Source URL: https://arxiv.org/abs/2509.07648
- Authors: Lachlan Simpson; Kyle Millar; Adriel Cheng; Cheng-Chew Lim; Hong Gunn Chew
- Reference count: 32
- Primary result: GB-IG improves fidelity scores up to 0.0557 and Jaccard index up to 0.2291 on synthetic datasets compared to standard IG variants

## Executive Summary
This paper introduces Graph-based Integrated Gradients (GB-IG), a novel extension of Integrated Gradients for explaining Graph Neural Networks (GNNs). Standard IG assumes continuous data and straight-line interpolation, which is ill-defined for graph structures. GB-IG addresses this by accumulating attributions over shortest paths in the graph, capturing both node features and structural connectivity. The method demonstrates superior fidelity and Jaccard index scores on synthetic datasets with ground-truth explanations, and shows consistent improvements on real-world datasets like Cora and Pubmed.

## Method Summary
GB-IG extends Integrated Gradients by replacing straight-line interpolation with accumulation over shortest paths between a target node and a strategically chosen base-point. The base-point is selected using maximal distance and path entropy to provide a meaningful baseline. For each shortest path, gradients are computed at each node and accumulated to form the final attribution. The method satisfies a new theoretical axiom called "path-wise completeness," ensuring mathematical validity for graph-structured data.

## Key Results
- GB-IG achieves up to 0.0557 higher fidelity scores than standard IG variants on synthetic datasets
- Jaccard index improvements of up to 0.2291 demonstrate better identification of ground-truth important nodes
- On real-world datasets (Pubmed, Cora, CiteSeer), GB-IG shows higher fidelity scores but slightly lower sparsity than IG-Gaussian
- GB-IG successfully identifies known motifs (e.g., house structures) in synthetic data with ground-truth explanations

## Why This Works (Mechanism)

### Mechanism 1: Discrete Path Integration
- Replacing continuous straight-line interpolation with discrete path accumulation allows attribution to respect graph topology
- Gradients are integrated along actual graph edges rather than arbitrary geometric paths
- Core assumption: Critical information flows along shortest structural paths
- Break condition: If classification relies on non-shortest paths or in fully connected graphs

### Mechanism 2: Entropic Base-point Selection
- Uses maximum distance and path entropy to select meaningful baseline nodes
- Addresses the ill-posed nature of zero-vector baselines in graphs
- Core assumption: Distant, high-connectivity nodes provide semantically neutral baselines
- Break condition: In small-diameter graphs where maximal distance is too small

### Mechanism 3: Path-wise Completeness
- Satisfies modified Completeness axiom by summing attributions over all paths
- Ensures mathematical validity while extending IG to multi-path graph structures
- Core assumption: Axiom satisfaction correlates with explanation reliability
- Break condition: Combinatorial explosion makes exact computation intractable

## Foundational Learning

- **Concept: Integrated Gradients (IG)**
  - Why needed: GB-IG is direct extension; understanding IG's baseline requirement is essential
  - Quick check: Why does IG require a baseline and how does changing it affect attribution?

- **Concept: Graph Topology vs. Feature Space**
  - Why needed: Core motivation is that graphs are non-Euclidean
  - Quick check: Why is straight-line interpolation between graph nodes undefined?

- **Concept: Fidelity vs. Sparsity in XAI**
  - Why needed: Evaluation relies on these metrics
  - Quick check: Would highlighting every node as important yield high or low Fidelity?

## Architecture Onboarding

- **Component map:** GNN Model -> Path Finder -> Base-point Selector -> Gradient Integrator
- **Critical path:** Base-point identification -> All shortest paths calculation -> Iterative gradient accumulation
- **Design tradeoffs:** Higher fidelity but lower sparsity; increased computational cost
- **Failure signatures:** Combinatorial explosion on dense graphs; negative fidelity from poor base-point selection
- **First 3 experiments:**
  1. Motif identification in synthetic ShapeGGen data to validate Jaccard index improvements
  2. Fidelity stress test on Cora/Pubmed by occluding top-k% important nodes
  3. Base-point ablation comparing entropy-based selection against random maximal-distance nodes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the number of paths in GB-IG be restricted to optimize computational complexity and explanation sparsity?
- Basis: [explicit] Conclusion states future work will investigate limiting paths to reduce computational complexity and increase sparsity
- Why unresolved: Current method aggregates over all shortest paths, creating computational burden and complex explanations
- What evidence would resolve it: Heuristic or algorithm selecting representative path subsets while maintaining high fidelity

### Open Question 2
- Question: What is the appropriate threshold for defining important nodes when evaluating attribution methods?
- Basis: [explicit] Section 5.1 notes 0.8 threshold used but further investigation is needed
- Why unresolved: Threshold choice impacts fidelity/sparsity metrics without standard justification
- What evidence would resolve it: Sensitivity analysis or theoretical framework establishing stable, universal threshold

### Open Question 3
- Question: Can GB-IG formulation be extended to weighted graphs or graphs with edge explainability?
- Basis: [inferred] Section 4 explicitly restricts definition to unweighted graphs
- Why unresolved: Many applications use weighted edges to denote relationship strength
- What evidence would resolve it: Modification incorporating edge weights into path integral with demonstration on relevant datasets

## Limitations

- Computational complexity is prohibitive on large-scale graphs due to exponential path growth
- Tradeoff between fidelity and sparsity results in more complex (less concise) explanations
- Modest absolute gains in real-world datasets compared to synthetic data improvements

## Confidence

- **High Confidence:** Core path-based gradient accumulation mechanism is mathematically sound; synthetic dataset results are strong
- **Medium Confidence:** Real-world dataset improvements are consistent but smaller; base-point selection heuristic is reasonable but not extensively validated
- **Low Confidence:** No ablation studies on entropy-based base-point selection; no testing for robustness to graph noise or adversarial perturbations

## Next Checks

1. **Ablation Study:** Compare GB-IG with random maximal-distance base-points versus entropy-based selection to quantify heuristic contribution
2. **Scalability Test:** Evaluate GB-IG on larger graphs (OGB datasets) with path sampling to assess practical viability and identify bottlenecks
3. **Adversarial Robustness:** Test GB-IG explanation stability under small perturbations to node features or graph structure compared to baseline IG methods