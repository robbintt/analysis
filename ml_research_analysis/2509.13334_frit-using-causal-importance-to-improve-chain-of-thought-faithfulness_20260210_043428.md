---
ver: rpa2
title: 'FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness'
arxiv_id: '2509.13334'
source_url: https://arxiv.org/abs/2509.13334
tags:
- answer
- reasoning
- step
- frit
- faithfulness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness

## Quick Facts
- arXiv ID: 2509.13334
- Source URL: https://arxiv.org/abs/2509.13334
- Reference count: 40
- Primary result: FRIT increases CoT faithfulness and accuracy on Qwen3-8B and Mistral-7B using causal intervention and DPO

## Executive Summary
FRIT improves Chain-of-Thought (CoT) faithfulness by identifying and preserving only the reasoning steps that causally influence the final answer. The method uses intervention-based causal testing to detect unimportant steps, then augments traces by deleting and regenerating those steps while preserving the original answer. These faithful/unfaithful pairs are used to fine-tune models via Direct Preference Optimization (DPO), yielding improvements in both faithfulness and accuracy.

## Method Summary
FRIT operates in three stages: (1) Causal intervention testing identifies causally important CoT steps by perturbing each step with unrelated facts and measuring answer changes; (2) Faithful traces are constructed by iteratively deleting unimportant steps and regenerating reasoning until the answer matches the original; (3) Models are fine-tuned using DPO on preference pairs (faithful chosen, unfaithful rejected), with triplets regenerated each iteration to mitigate faithfulness drift. The pipeline uses LoRA-based DPO with rank-64 and low learning rates to balance faithfulness gains with general performance.

## Key Results
- FRIT increases CoT faithfulness on Qwen3-8B and Mistral-7B across multiple reasoning benchmarks
- Accuracy improvements accompany faithfulness gains, not just faithfulness alone
- The approach generalizes across different model families (Qwen3-8B, Mistral-7B) and datasets (GSM8K, SVAMP, StrategyQA, CommonsenseQA, ASDiv)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intervening on individual reasoning steps and measuring answer changes identifies causally important steps that genuinely influence outputs.
- Mechanism: Replace a CoT step with an unrelated fact (preserving style), continue generation, and check if the final answer changes. A change indicates the original step was causally utilized; no change indicates it was decorative/unfaithful.
- Core assumption: If a step is causally important, perturbing it should alter the downstream reasoning and final answer under autoregressive continuation.
- Evidence anchors:
  - [abstract] "intervening on individual reasoning steps in model-generated CoTs, creating faithful/unfaithful pairs"
  - [Section 2.1.2] Algorithm 2 formalizes this: intervene on step $s_i$, regenerate continuation, return whether $a' \neq a$.
  - [corpus] Related work uses similar causal perturbation to measure faithfulness (Lanham et al. 2023, "Measuring faithfulness in chain-of-thought reasoning").
- Break condition: If the model ignores the perturbation and produces the same answer regardless, the step is labeled unimportant and may be removed during augmentation.

### Mechanism 2
- Claim: Iteratively deleting and regenerating causally unimportant steps produces faithful CoT traces where every step demonstrably influences the answer.
- Mechanism: For each step, if causally unimportant, delete it and all subsequent steps, then regenerate from that point until the answer matches the original. Only steps that survive this process (or are regenerated while preserving the answer) remain in the faithful trace.
- Core assumption: Regeneration under constrained conditions can produce alternative reasoning paths that reach the same answer without the unfaithful step.
- Evidence anchors:
  - [Section 2.1.2] Algorithm 3: "repeat delete $s_j$ for $j \geq i$; regenerate $s_j$ for $j \geq i$ and new answer $a'$ until $a' = a$"
  - [abstract] "creating faithful/unfaithful pairs that highlight when reasoning breaks down"
  - [corpus] Weak direct evidence; related work focuses on measurement rather than augmentation.
- Break condition: If regeneration repeatedly fails to recover the original answer, the step may be essential and is retained.

### Mechanism 3
- Claim: Training on preference pairs (faithful chosen, unfaithful rejected) with DPO shifts model behavior toward producing causally consistent reasoning.
- Mechanism: DPO optimizes policy to increase likelihood of chosen (faithful) traces and decrease likelihood of rejected (unfaithful) traces, using the base model as reference. Paired examples share the same prompt and final answer but differ in reasoning quality.
- Core assumption: The model can generalize from these pairs to prefer faithful reasoning on unseen prompts, and faithfulness improvements transfer to accuracy gains.
- Evidence anchors:
  - [Section 2.2] "Each training example consists of a prompt $x$, a faithful reasoning trace $x^+$, and an unfaithful trace $x^-$"
  - [Section 3] "FRIT yields satisfactory results on both models, not only increasing faithfulness but also increasing accuracy"
  - [corpus] DPO is a standard alignment technique; corpus includes faithfulness-focused RL approaches but not identical setups.
- Break condition: Faithfulness drift—labels may become outdated as model weights change, weakening signal over iterations.

## Foundational Learning

- Concept: **Causal intervention on reasoning traces**
  - Why needed here: FRIT's core methodology depends on perturbing CoT steps and measuring downstream effects to assess causal importance.
  - Quick check question: If you replace a reasoning step with an unrelated fact and the model's answer stays the same, what does that imply about the step's causal role?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: FRIT uses DPO to fine-tune models on faithful/unfaithful pairs without explicit reward modeling.
  - Quick check question: In DPO, what is the role of the reference (base) model, and why are preference pairs necessary?

- Concept: **Chain-of-Thought faithfulness metrics**
  - Why needed here: Understanding how faithfulness is measured (e.g., fraction of causally important steps) is essential to interpret FRIT's evaluation.
  - Quick check question: How does FRIT's CoT faithfulness metric differ from "traditional faithfulness" (Lanham et al. 2023)?

## Architecture Onboarding

- Component map:
  1. Fact corpus and clustering (Wikidata + arithmetic facts, embeddings, k-means)
  2. Intervention module (Algorithm 1: style-preserving fact replacement)
  3. Causal importance tester (Algorithm 2: intervene, continue, compare answers)
  4. Augmentation pipeline (Algorithm 3: delete/regenerate unimportant steps)
  5. DPO training loop (3 iterations, rank-64 LoRA, per-iteration triplet regeneration)

- Critical path: Prompt → preliminary CoT generation → per-step causal testing → faithful trace construction → unfaithful trace generation → DPO triplet assembly → fine-tuning → evaluation

- Design tradeoffs:
  - Computational cost vs. faithfulness: Intervention and regeneration are expensive; paper reports 10–24 hours for full pipeline on 7–8B models.
  - Style preservation vs. perturbation strength: Using 17-shot rewriting maintains style but may limit intervention diversity.
  - Regeneration frequency vs. faithfulness drift: Regenerating triplets each iteration mitigates drift but increases cost.

- Failure signatures:
  - Faithfulness drift: Labels become stale as model evolves; mitigate by regenerating triplets per iteration.
  - Regeneration loops: If deletion/regeneration cannot recover original answer, process may stall; paper uses retry loop but does not specify max iterations.
  - Catastrophic forgetting: DPO hyperparameters (learning rate, $\beta$) tuned to balance faithfulness gains and general performance.

- First 3 experiments:
  1. Reproduce single-iteration FRIT on a small dataset (e.g., 100 GSM8K examples) to validate causal testing and augmentation logic.
  2. Ablate intervention style preservation: compare with simpler perturbations (e.g., random text insertion) to test sensitivity.
  3. Measure faithfulness drift: run DPO with triplet regeneration only once vs. per-iteration, comparing final faithfulness scores.

## Open Questions the Paper Calls Out
None

## Limitations
- Faithfulness drift during iterative training: Labels may become outdated as model weights update, weakening DPO signal even with per-iteration triplet regeneration.
- Reproducibility challenges: Key implementation details (e.g., regeneration parameters, exact prompt format) are underspecified, making exact replication difficult.
- Computational cost: The full pipeline is expensive (10–24 hours for 7–8B models), limiting scalability and ablation studies.

## Confidence
- **High confidence**: Core causal intervention mechanism and DPO training setup are clearly specified with pseudocode and hyperparameters.
- **Medium confidence**: Faithfulness improvements are reported, but exact implementation of key steps (regeneration, answer equivalence) is underspecified.
- **Low confidence**: Paper does not address failure modes in detail or provide extensive ablations for robustness validation.

## Next Checks
1. Reproduce single-iteration FRIT: Run causal intervention and augmentation pipeline on 100 GSM8K examples to validate core logic and identify implementation gaps.
2. Test faithfulness drift empirically: Compare DPO with vs. without per-iteration triplet regeneration to quantify impact of label staleness on final faithfulness and accuracy.
3. Ablate intervention strength: Replace style-preserving fact insertion with simpler perturbations (e.g., random text insertion) to test if current intervention method is necessary for faithfulness gains.