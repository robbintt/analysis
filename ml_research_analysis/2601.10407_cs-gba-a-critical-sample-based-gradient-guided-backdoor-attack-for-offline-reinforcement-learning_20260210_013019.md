---
ver: rpa2
title: 'CS-GBA: A Critical Sample-based Gradient-guided Backdoor Attack for Offline
  Reinforcement Learning'
arxiv_id: '2601.10407'
source_url: https://arxiv.org/abs/2601.10407
tags:
- attack
- clean
- offline
- cs-gba
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CS-GBA, a backdoor attack framework for offline
  reinforcement learning that targets critical samples with high TD errors to maximize
  policy degradation under a strict 5% poisoning budget. The method introduces a correlation-breaking
  trigger that exploits mutual exclusivity of state features to evade OOD detection,
  and a gradient-guided action generation mechanism that produces worst-case actions
  within the data manifold.
---

# CS-GBA: A Critical Sample-based Gradient-guided Backdoor Attack for Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.10407
- Source URL: https://arxiv.org/abs/2601.10407
- Reference count: 4
- Key outcome: CS-GBA achieves lower attack rewards (e.g., 452 vs 1336 on Walker2d with CQL) while maintaining higher clean performance, effectively penetrating conservative algorithms like CQL under a 5% poisoning budget.

## Executive Summary
This paper proposes CS-GBA, a backdoor attack framework for offline reinforcement learning that targets critical samples with high TD errors to maximize policy degradation under a strict 5% poisoning budget. The method introduces a correlation-breaking trigger that exploits mutual exclusivity of state features to evade OOD detection, and a gradient-guided action generation mechanism that produces worst-case actions within the data manifold. Experiments on D4RL benchmarks demonstrate that CS-GBA significantly outperforms state-of-the-art baselines, achieving lower attack rewards while maintaining higher clean performance, effectively penetrating conservative algorithms like CQL that rely on OOD detection mechanisms.

## Method Summary
CS-GBA is a data poisoning attack framework for offline RL that combines three key mechanisms: critical sample selection based on TD error, a correlation-breaking trigger designed to evade OOD detection, and gradient-guided action generation to maximize policy degradation. The attack first pre-trains a proxy Q-network to compute TD errors and identify high-impact samples. It then injects triggers by manipulating mutually exclusive state features (e.g., joint angles) to create OOD samples that evade conservative detection. Finally, it generates poisoned actions through gradient ascent on the Q-value while constraining them to remain within the data manifold using a conditional variational autoencoder. The method operates under a strict 5% poisoning budget and targets offline RL algorithms that rely on OOD detection.

## Key Results
- Achieves significantly lower attack rewards compared to state-of-the-art baselines (e.g., 452 vs 1336 on Walker2d with CQL)
- Maintains higher clean performance than competing methods while achieving effective attacks
- Successfully penetrates conservative algorithms like CQL that rely on OOD detection mechanisms
- Demonstrates effectiveness across multiple D4RL benchmark tasks under a 5% poisoning budget

## Why This Works (Mechanism)
CS-GBA exploits the fundamental vulnerability of offline RL algorithms to data poisoning by targeting samples with high TD errors, which have disproportionate influence on policy learning. The correlation-breaking trigger mechanism specifically evades conservative OOD detection by manipulating physically mutually exclusive features (like joint angles) that appear normal to detection algorithms but create adversarial states. The gradient-guided action generation ensures poisoned actions are optimized to maximize policy degradation while remaining within the data manifold to avoid detection. By combining these three mechanisms under a strict budget constraint, CS-GBA creates attacks that are both effective and stealthy against current defensive approaches.

## Foundational Learning

**Temporal Difference (TD) Error**: The difference between predicted and target Q-values for a given transition. Needed because high TD-error samples have disproportionate influence on policy learning, making them ideal targets for poisoning attacks. Quick check: Verify that the proxy Q-network can accurately compute TD errors on the clean dataset.

**Out-of-Distribution (OOD) Detection**: Conservative offline RL algorithms use OOD detection to filter or down-weight transitions that fall outside the training distribution. Needed because the attack must evade these mechanisms to be effective. Quick check: Confirm that the correlation-breaking trigger successfully bypasses the specific OOD detection method used by the target algorithm.

**Data Manifold Constraint**: The constraint that poisoned samples should remain within the distribution of the original dataset to avoid detection. Needed to ensure the attack remains stealthy while still being effective. Quick check: Verify that poisoned actions generated by the CVAE remain within the manifold of valid actions for the environment.

**Mutual Exclusivity in Physical Features**: The property that certain physical state features (like joint angles) cannot simultaneously take extreme values. Needed to create correlation-breaking triggers that appear normal to OOD detection while being adversarial. Quick check: Confirm that the trigger injection follows the physical constraints of the environment.

## Architecture Onboarding

**Component Map**: Clean dataset -> Proxy Q-network (TD error computation) -> Critical sample selection -> Trigger injection (correlation-breaking) -> Gradient-guided action generation (CVAE) -> Poisoned dataset -> Target RL algorithm -> Degraded policy

**Critical Path**: The most critical path is: Proxy Q-network pre-training → Critical sample selection (high TD error) → Trigger injection → Gradient-guided action generation → Policy degradation. Each step builds on the previous one, with the proxy model's accuracy being fundamental to identifying the right samples to poison.

**Design Tradeoffs**: The method trades computational cost (requiring pre-training a proxy model and CVAE) for attack effectiveness and stealth. The 5% budget constraint balances between achieving significant policy degradation and maintaining attack stealth. The correlation-breaking trigger trades simplicity for effectiveness against specific OOD detection mechanisms.

**Failure Signatures**: If the proxy Q-network is poorly trained, critical samples won't be correctly identified, leading to ineffective attacks. If the trigger doesn't properly exploit mutual exclusivity, OOD detection will filter poisoned samples. If the gradient-guided action generation produces actions outside the data manifold, they'll be detected or create unrealistic transitions that don't degrade performance.

**First Experiments**: 1) Train proxy Q-network on clean dataset and verify TD error computation accuracy. 2) Test correlation-breaking trigger on a single critical sample to confirm OOD evasion. 3) Generate poisoned actions for one transition and verify they remain within the data manifold while maximizing Q-value degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can offline RL algorithms be explicitly robustified against data poisoning attacks that target high TD-error transitions?
- Basis in paper: The conclusion states the authors hope this work "serves as a rigorous benchmark to motivate the development of more sophisticated, distribution-aware defense mechanisms in the future."
- Why unresolved: The paper demonstrates that current conservative defenses (like CQL) are vulnerable to CS-GBA, but it does not propose or test specific countermeasures beyond showing the failure of existing ones.
- Evidence: The development of a defense mechanism that detects or down-weights the influence of high TD-error samples when they exhibit the specific correlation-breaking patterns identified in the paper.

### Open Question 2
- Question: Is the correlation-breaking trigger mechanism effective in high-dimensional or image-based observation spaces?
- Basis in paper: The methodology (Sec 3.3) relies on computing a "Pearson correlation matrix" and injecting triggers based on the "95th percentile" of specific physical features.
- Why unresolved: The proposed trigger relies on manipulating explicit scalar features in vector-based states (e.g., MuJoCo environments). It is unclear how this "physical mutual exclusivity" logic would transfer to unstructured data like raw pixels where feature correlations are not explicitly defined.
- Evidence: Experimental results applying CS-GBA to offline RL benchmarks with image observations (e.g., D4RL Adroit or visual control tasks) showing similar attack success rates.

### Open Question 3
- Question: To what extent does the attack performance depend on the convergence level or architecture of the attacker's pre-trained proxy Q-network?
- Basis in paper: Section 3.2 states the necessity to "pre-train a proxy Q-network on the clean dataset to convergence" to compute TD errors.
- Why unresolved: The paper assumes the attacker has the resources to train a model to full convergence on the clean dataset. It leaves open whether a "cheap" proxy (e.g., under-trained or smaller architecture) would still successfully identify critical samples.
- Evidence: Ablation studies showing the correlation between the proxy model's accuracy/training steps and the final Attack Reward of the poisoned agent.

## Limitations

- The 5% poisoning budget may not be generalizable across different offline RL domains with varying data requirements and sample complexities.
- The correlation-breaking trigger's effectiveness against more sophisticated OOD detection mechanisms that use learned feature representations remains uncertain.
- Claims about maintaining higher clean performance while achieving effective attacks require validation across different model architectures and hyperparameter settings to rule out potential overfitting.

## Confidence

**High confidence**: The framework design and mathematical formulation of CS-GBA are clearly presented and internally consistent. The claim about leveraging TD errors to identify critical samples is well-supported by the methodology.

**Medium confidence**: The experimental results demonstrating performance degradation against baselines are convincing, but the specific benchmark choices (D4RL datasets) may not fully represent the diversity of real-world offline RL applications. The comparison with existing backdoor attack methods is comprehensive within the tested scope.

**Low confidence**: The claim about maintaining higher clean performance while achieving effective attacks requires more extensive validation across different model architectures and hyperparameter settings to rule out potential overfitting to specific experimental conditions.

## Next Checks

1. Test CS-GBA's effectiveness against adaptive OOD detection mechanisms that can identify correlation-breaking triggers through learned feature representations rather than hand-crafted rules.

2. Evaluate the attack performance across a broader range of offline RL algorithms beyond CQL, including non-conservative methods, to assess the universality of the approach.

3. Conduct ablation studies varying the poisoning budget from 1% to 10% to determine the sensitivity of attack effectiveness to budget constraints and identify potential threshold effects.