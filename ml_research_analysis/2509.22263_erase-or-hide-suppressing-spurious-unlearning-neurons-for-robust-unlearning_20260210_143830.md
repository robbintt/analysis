---
ver: rpa2
title: Erase or Hide? Suppressing Spurious Unlearning Neurons for Robust Unlearning
arxiv_id: '2509.22263'
source_url: https://arxiv.org/abs/2509.22263
tags:
- unlearning
- knowledge
- influence
- arxiv
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that existing unlearning methods induce "spurious
  unlearning neurons" that hide rather than erase target knowledge, leading to vulnerability
  against retraining attacks. To address this, the authors propose SSIUU, which employs
  attribution-guided regularization to suppress the emergence of these neurons and
  ensure faithful removal of target knowledge.
---

# Erase or Hide? Suppressing Spurious Unlearning Neurons for Robust Unlearning

## Quick Facts
- **arXiv ID**: 2509.22263
- **Source URL**: https://arxiv.org/abs/2509.22263
- **Reference count**: 18
- **Primary result**: SSIUU suppresses spurious unlearning neurons via attribution regularization, achieving 14.81% accuracy recovery under harmful retraining attacks vs. 68.42-75.53% for baselines.

## Executive Summary
This paper addresses a critical vulnerability in machine unlearning: existing methods often create "spurious unlearning neurons" that hide rather than erase target knowledge, making models susceptible to retraining attacks. The authors propose SSIUU, which employs attribution-guided regularization to suppress these neurons and ensure faithful knowledge removal. Through experiments on FaithUn and TOFU datasets, SSIUU demonstrates significantly better robustness against harmful retraining attacks while maintaining general utility knowledge and more stable internal attribution distributions compared to strong baselines.

## Method Summary
SSIUU is a wrapper method that adds attribution-guided regularization to base unlearning approaches like Gradient Difference (GD). It computes per-neuron attributions as activation × gradient, identifies neurons with negative attributions, and adds an L2 penalty on the deviation of these attributions between optimization steps. This forces the optimizer to reduce positive influence across multiple modules rather than inflating negative influence in specific neurons. The method uses early stopping based on forgetting score thresholds and is evaluated on Llama-3.2 (3B) and Qwen-2.5 (3B) models with specified forget/retain/test splits.

## Key Results
- SSIUU achieves 14.81% accuracy recovery under harmful retraining attacks (p=0.1) versus 68.42-75.53% for baselines
- Maintains better retention of general utility knowledge while erasing target knowledge
- Produces more stable internal attribution distributions (ρ=0.99 pre/post-attack correlation vs. 0.73-0.87 for baselines)
- Distributes positive influence variation across multiple modules rather than concentrating in late layers

## Why This Works (Mechanism)

### Mechanism 1: Spurious Neuron Suppression via Attribution Regularization
SSIUU constrains negative attribution growth to prevent shortcut neurons from masking rather than erasing knowledge. The L2 regularization penalizes deviations in negative-attribution neurons between optimization steps, forcing genuine erasure over suppression. Evidence shows Equation (3) explicitly regularizes attribution gaps for neurons with negative scores. Break condition: If attribution-to-knowledge mapping is noisy or regularization strength is mis-set.

### Mechanism 2: Module-Localized Knowledge Erasure
By blocking the easier path of negative-inflation, SSIUU forces distributed positive-influence reduction across Attention Q/K and MLP modules rather than concentrating changes in late-layer MLPs. Evidence: Figure 5 shows SSIUU produces positive influence variation across modules while GD concentrates in late layers. Break condition: If target knowledge is extremely localized, distributed updates may be inefficient.

### Mechanism 3: Attack Resistance via Distributional Stability
Stable attribution distributions before and after retraining attacks indicate faithful removal rather than hidden knowledge. Since spurious neurons are suppressed, fewer "tunable" negative contributors remain for attacks to perturb. Evidence: SSIUU achieves ρ=0.99 correlation between pre/post-attack attributions vs. lower values for baselines. Break condition: If attacks use different distributions or learning-rate regimes, correlation-based robustness may not hold.

## Foundational Learning

- **Attribution methods for interpretability (gradient × activation)**: SSIUU's regularization is defined in attribution space; understanding what attributions measure is essential. Quick check: Can you compute and interpret attribution sign/magnitude from activation and gradient?
- **Catastrophic forgetting vs. unlearning in LLMs**: Retraining attacks exploit similar dynamics; distinguishing desired unlearning from unintended forgetting is critical. Quick check: If forgotten knowledge resurfaces during benign fine-tuning, is that a failure of unlearning or retention?
- **Preference optimization (DPO/NPO) and gradient-based unlearning (GA/GD)**: SSIUU wraps around base methods; understanding their objectives helps diagnose regularization modifications. Quick check: In GA vs. GD, what does the retain-set auxiliary loss do, and why might it still not prevent shallow alignment?

## Architecture Onboarding

- **Component map**: Base unlearning loss -> Attribution computation -> Regularization term -> Early stopping
- **Critical path**: 1) Forward pass on forget-set batch 2) Compute base unlearning loss and backward pass 3) Compute attributions for neurons with A < 0 4) Compute L2 gap vs. previous step's attributions 5) Combine losses with weight λ and update 6) Check early-stopping threshold
- **Design tradeoffs**: λ too high → retention degrades; λ too low → spurious neurons emerge; attribution aggregation affects noise/cost; learning rate must balance general knowledge preservation
- **Failure signatures**: Retention/test accuracy drops sharply (λ too high); post-attack FS close to baseline (regulation ineffective); high variance in attribution distributions (optimization instability)
- **First 3 experiments**: 1) λ ablation measuring retain/test accuracy and post-attack FS 2) Attribution-distribution visualization before/after unlearning and post-attack 3) Harmful retraining attack sweep over learning rates with different p values

## Open Questions the Paper Calls Out
- How does SSIUU's effectiveness scale with model size beyond 3B parameter models?
- What is the theoretical mechanism driving the emergence of spurious unlearning neurons in gradient-based methods?
- How sensitive is SSIUU's performance to the choice of attribution method used for regularization?
- Can SSIUU be combined with other unlearning robustness techniques to achieve near-zero knowledge recovery under attack?

## Limitations
- Attribution method validity: Reliance on gradient × activation as knowledge proxy lacks independent validation
- Hyperparameter sensitivity: Critical settings like λ and learning rate not fully explored across ranges
- Attack threat model specificity: Harmful attack success measured only via narrow LR sweep, no validation against adaptive attacks
- Generalization beyond studied datasets: Results limited to FaithUn (celebrity knowledge) and TOFU (synthetic author Q&A)

## Confidence
- **High**: SSIUU improves over baselines on standard unlearning metrics (FS, RS, US) on tested datasets
- **Medium**: SSIUU reduces vulnerability to harmful retraining attacks relative to baselines
- **Low**: The spurious-neuron suppression mechanism is correctly identified and attribution-distribution correlation is a reliable robustness proxy

## Next Checks
1. **Attribution-validation experiment**: Use ablation studies with different attribution methods to confirm spurious-neuron suppression isn't an artifact of gradient × activation choice
2. **Robustness sweep**: Extend harmful attack evaluation to wider LR range (1e-7 to 1e-4) and include adaptive attacks to test gains under aggressive threat models
3. **Cross-dataset generalization**: Apply SSIUU to multi-concept unlearning benchmark to verify spurious-neuron suppression and distributed-erasure patterns generalize beyond single-concept targets