---
ver: rpa2
title: Towards Safer and Understandable Driver Intention Prediction
arxiv_id: '2510.09200'
source_url: https://arxiv.org/abs/2510.09200
tags:
- explanations
- video
- explanation
- driver
- driving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the lack of interpretability in deep learning
  models for driver intention prediction, which is crucial for safe autonomous driving.
  The authors introduce DAAD-X, a new multimodal dataset providing hierarchical, human-understandable
  explanations for driving maneuvers.
---

# Towards Safer and Understandable Driver Intention Prediction

## Quick Facts
- arXiv ID: 2510.09200
- Source URL: https://arxiv.org/abs/2510.09200
- Reference count: 40
- Key outcome: Introduces DAAD-X dataset and VCBM achieving 29.17% macro F1 and 49.11% micro F1 on explanation tasks

## Executive Summary
This work addresses the lack of interpretability in deep learning models for driver intention prediction, which is crucial for safe autonomous driving. The authors introduce DAAD-X, a new multimodal dataset providing hierarchical, human-understandable explanations for driving maneuvers. They propose a Video Concept Bottleneck Model (VCBM) that generates spatio-temporally coherent explanations by integrating learnable token merging with a localized concept bottleneck. Extensive experiments show that transformer-based models with VCBM achieve better interpretability and performance than CNN-based models, with F1 scores of 29.17% (macro) and 49.11% (micro) on explanation tasks.

## Method Summary
The Video Concept Bottleneck Model (VCBM) uses dual video encoders to process gaze-centric and front-view videos, extracting tubelet embeddings that are concatenated. Learnable Token Merging (LTM) clusters these embeddings using composite similarity (feature, spatial, and temporal distances) to create compact token representations. The Localized Concept Bottleneck Model (LCBM) processes all merged tokens through separate explanation-specific FC layers before late averaging, preserving localized contributions. The model jointly predicts maneuvers and explanations using a weighted loss with λ=0.5, balancing both tasks.

## Key Results
- VCBM with MViTv2 backbone achieves 29.17% macro F1 and 49.11% micro F1 on explanation tasks
- Learnable Token Merging with K=5 clusters outperforms both single-token and full-token aggregation approaches
- Gaze crops (R=350) significantly improve explanation accuracy compared to overlay or no-gaze variants
- Multilabel t-SNE visualizations reveal causal correlations among explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grouping video tokens by composite similarity before explanation prediction improves interpretability compared to global pooling.
- Mechanism: Learnable Token Merging (LTM) clusters semantically similar features across frames using a composite distance metric that combines feature similarity (cosine), spatial distance, and temporal distance. Each token is softly assigned to K cluster centers via softmax over negative composite distances, and cluster centers are updated as weighted sums of assigned tokens. This retains spatio-temporally relevant features while reducing redundancy.
- Core assumption: Driving explanations depend on localized, temporally coherent features rather than globally aggregated representations.
- Evidence anchors: [abstract] "integrating learnable token merging with a localized concept bottleneck" [Section 4.3] "d(i,j)_composite = αd(i,j)_feat + βd̃(i,j)_spatial + γd̃(i,j)_temporal" and "By merging similar features into a compact token representation, this approach reduces redundancy in video embeddings while ensuring that spatio-temporally relevant features are retained."

### Mechanism 2
- Claim: Processing all merged tokens through separate explanation-specific FC layers before aggregation yields more faithful explanations than CLS-token summarization.
- Mechanism: Localised Concept Bottleneck Model (LCBM) feeds all K merged tokens to each of 17 explanation-specific fully connected layers, producing K logits per explanation. These are averaged after individual processing ("late averaging"), preserving each token's contribution rather than averaging features first (which loses fine-grained spatial-temporal associations).
- Core assumption: Different explanations activate on different token subgroups; early averaging destroys this localization.
- Evidence anchors: [abstract] "spatio-temporally coherent explanations by integrating learnable token merging with a localized concept bottleneck" [Section 4.4] "Rather than immediately averaging features before the bottleneck, we introduce a late averaging strategy, allowing each merged token to retain its individual contribution to the explanation process."

### Mechanism 3
- Claim: Jointly training maneuver prediction with explanation classification via a weighted loss improves both tasks compared to maneuver-only training, up to a tradeoff threshold.
- Mechanism: The joint bottleneck objective (Equation 1) combines cross-entropy loss for maneuver prediction with aggregated binary cross-entropy for each explanation, scaled by λ. Explanations act as auxiliary supervision, aligning learned features with human-interpretable concepts.
- Core assumption: Explanations provide meaningful regularization that prevents spurious correlations in maneuver prediction.
- Evidence anchors: [Section 4.1] "minimizes the weighted sum" with "λ is the weighting factor" [Table 6] λ=0.5 yields best balance (Action Acc 73.21%, Explanation Acc 28.31%).

## Foundational Learning

- Concept: Concept Bottleneck Models (CBMs)
  - Why needed here: VCBM extends CBMs from static images to video; understanding standard CBMs (Koh et al., ICML 2020) is prerequisite to grasping why late averaging and token merging are novel.
  - Quick check question: Can you explain why a traditional CBM would struggle with video inputs where temporal order matters?

- Concept: Vision Transformers (ViT) and Video Tokenization
  - Why needed here: VCBM operates on tubelet embeddings from VideoMAE/MViTv2; understanding patch/tubelet tokenization is essential for implementing LTM's clustering logic.
  - Quick check question: How does a 16-frame video clip become a sequence of tokens in VideoMAE?

- Concept: Multi-label Classification with BCE Loss
  - Why needed here: Ego-vehicle explanations are multi-attribute (17 binary labels per video); standard softmax won't work.
  - Quick check question: Why use binary cross-entropy per label rather than softmax across all 17 explanation classes?

## Architecture Onboarding

- Component map: Dual Video Encoder -> LTM (Composite Similarity Block) -> LCBM (17 FC layers) -> Sparse MLP -> Maneuver Prediction

- Critical path:
  1. Input: paired videos (gaze crop R=350 optimal per Table 7, front view)
  2. Encoder: VideoMAE or MViTv2 backbone (Kinetics-400 pretrained)
  3. LTM: K=5 clusters works best (Table 4); α,β,γ weights tuned on validation
  4. LCBM: Late averaging before explanation logits
  5. Loss: Joint optimization with λ≈0.5

- Design tradeoffs:
  - More clusters (K>5): captures fine-grained features but introduces noise, degrading performance (Table 4)
  - Higher λ: better explanations, worse action accuracy (Table 6)
  - Gaze representation: cropped regions (R=350) outperform overlay/no-gaze (Table 7) but require ground-truth gaze coordinates

- Failure signatures:
  - Scattered GradCAM activations: indicates LTM not clustering effectively; check composite distance weights
  - Low explanation F1 with high action accuracy: λ too low, insufficient explanation supervision
  - Explanation accuracy sensitive to temporal shuffling (Figure 7): transformer backbones more vulnerable than CNNs

- First 3 experiments:
  1. Reproduce baseline comparison: Run I3D, VideoMAE, MViTv2 with and without bottleneck on DAAD-X; verify Table 2 metrics (MViTv2+LTM should achieve ~29.17% macro F1 on explanations).
  2. Ablate cluster count: Sweep K∈{1,3,5,7,10} on I3D+LTM; expect peak at K=5 (Table 4), confirming that too few clusters lose information while too many add noise.
  3. Validate gaze contribution: Compare no-gaze, gaze-overlaid, and gaze-cropped (R=350) variants; confirm cropped gaze improves explanation accuracy by ~10 points (Table 7).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does explicitly aligning the in-cabin gaze and out-cabin front views using geometric transformations (e.g., homography) improve the clustering performance of the Learnable Token Merging (LTM) module?
- Basis in paper: [explicit] The authors state, "Token merging relies on the assumption that the video frames are at least partially aligned," and explicitly suggest investigating "the effects of explicitly aligning both views before performing token merging using techniques like homography" as future work.
- Why unresolved: The current VCBM assumes partial alignment but does not implement a mechanism to correct misalignment, potentially introducing noise into the merged tokens.
- What evidence would resolve it: A comparative ablation study showing F1 scores for explanations and maneuvers using raw versus homography-aligned video pairs.

### Open Question 2
- Question: Can the training objective be modified to eliminate the performance trade-off between explanation accuracy and action prediction accuracy?
- Basis in paper: [inferred] Table 6 demonstrates an inverse relationship where increasing the scaling factor (λ) for explanation loss degrades action prediction accuracy (dropping from 74.28% to 70.35%).
- Why unresolved: The joint bottleneck approach currently forces a compromise; the model optimizes for interpretability at the expense of prediction correctness.
- What evidence would resolve it: A training strategy that achieves peak action accuracy simultaneously with high explanation F1 scores, or analysis showing the gradients of the two objectives do not conflict.

### Open Question 3
- Question: Can the VCBM framework be extended to identify causal factors for driving maneuvers that occur outside the driver's immediate gaze focus?
- Basis in paper: [inferred] The limitations section notes that GradCAM activations are "predominantly observed in the forward direction" because the model assumes "the driver's gaze should guide" the focus.
- Why unresolved: The current reliance on gaze as the primary guide may cause the model to miss relevant peripheral context or rear-view hazards that influence the maneuver but are not directly looked at.
- What evidence would resolve it: Successful explanation generation for scenarios defined by peripheral hazards without corresponding gaze annotations.

## Limitations

- Data Dependency on Gaze Annotations: The model's best performance relies on high-quality gaze ground truth (radius R=350 crops), which may not be available in real-world deployment.
- Tradeoff Sensitivity: The joint training objective with λ=0.5 represents a delicate balance where improving explanations degrades action accuracy.
- Limited Generalization Beyond DAAD-X: With only 1,568 clips in a single driving context, the model's ability to generalize across diverse driving environments remains untested.

## Confidence

- High Confidence: LTM improves spatio-temporal coherence; late averaging in LCBM preserves localized features; joint training with explanations provides meaningful auxiliary supervision
- Medium Confidence: VCBM outperforms CNN-based approaches; the 17-explanation vocabulary captures critical factors; t-SNE visualizations reveal meaningful correlations
- Low Confidence: Robustness to gaze tracking errors; performance generalization to different datasets; long-term stability under distribution shift

## Next Checks

1. **Cross-Dataset Transfer:** Evaluate the trained VCBM on a different driving dataset (e.g., BDD-AutoDrive or DRIVE-AID) without fine-tuning. Measure explanation F1 and action accuracy drop to assess true generalization.

2. **Gaze Robustness Testing:** Systematically degrade gaze quality (add Gaussian noise, simulate gaze tracking failure) and measure performance degradation. Compare to a gaze-free variant to quantify the gaze dependency.

3. **Temporal Sensitivity Analysis:** Beyond the temporal shuffling experiment, conduct a detailed analysis of which explanation types are most temporally sensitive. Use GradCAM on sequential frames to verify that temporally coherent explanations align with visually consistent regions across frames.