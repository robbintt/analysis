---
ver: rpa2
title: 'EasyMath: A 0-shot Math Benchmark for SLMs'
arxiv_id: '2505.14852'
source_url: https://arxiv.org/abs/2505.14852
tags:
- reasoning
- easymath
- accuracy
- mathematical
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EasyMath is a 0-shot benchmark for evaluating small language models
  (SLMs) on practical math reasoning. It includes 13 categories of everyday math problems,
  excluding complex topics, and uses exact, numerical, and symbolic checks on free-form
  answers.
---

# EasyMath: A 0-shot Math Benchmark for SLMs

## Quick Facts
- arXiv ID: 2505.14852
- Source URL: https://arxiv.org/abs/2505.14852
- Reference count: 22
- Primary result: EasyMath achieves consistent, non-zero scores across SLMs (14M–4B parameters), unlike existing benchmarks that yield floor effects.

## Executive Summary
EasyMath is a zero-shot benchmark designed to evaluate small language models (SLMs) on practical math reasoning tasks. Unlike existing benchmarks that often produce zero or near-random accuracy for SLMs, EasyMath includes 13 categories of everyday math problems calibrated to SLM capacity. The benchmark uses a sophisticated evaluation pipeline with exact, numerical, and symbolic checks on free-form answers, enabling reliable differentiation across models while avoiding floor effects. Testing 23 models ranging from 14M to 4B parameters, EasyMath demonstrates that accuracy improves with model size and training, while chain-of-thought prompting provides only modest gains.

## Method Summary
EasyMath is a zero-shot evaluation benchmark for SLMs (14M–4B parameters) on practical math reasoning. The dataset consists of manually written questions across 13 categories with varying distribution (10–50 questions per category). The evaluation uses a multi-layered approach: free-form responses are extracted via regex, normalized (e.g., converting √x to sqrt(x)), and validated through direct string matching, numerical evaluation, and SymPy symbolic equivalence checking. Category-specific strictness applies—arithmetic requires exactness while algebra allows equivalence. The pipeline is designed to reduce false negatives from format variation without enabling multiple-choice guessing.

## Key Results
- EasyMath yields non-zero scores across all tested SLMs (14M–4B parameters), with accuracy improving monotonically with model size and training
- Existing benchmarks (GSM8K, MathQA, Hendrycks-MATH, Math-500) produce floor effects (0.0%) for SLMs, while EasyMath achieves 8.31%–95.69% accuracy
- Chain-of-thought prompting provides modest gains (~6% absolute improvement) for SLMs, substantially smaller than gains reported for LLMs
- Larger models exhibit lower run-to-run variance, with consistency increasing from ±1.71% (135M) to ±0.30% (4B parameters)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EasyMath yields non-zero scores across SLMs (14M–4B parameters), enabling meaningful differentiation where existing benchmarks produce floor effects.
- Mechanism: The benchmark restricts problem difficulty to foundational and intermediate math (basic arithmetic through multi-step reasoning), avoiding specialized topics like linear algebra. This calibration matches SLM capacity, preventing the zero-accuracy collapse observed on GSM8K, Hendrycks-MATH, and Math-500.
- Core assumption: Floor effects on existing benchmarks reflect difficulty mismatch rather than fundamental incapacity for any mathematical reasoning.
- Evidence anchors:
  - [abstract] "EasyMath achieves consistent, non-zero scores across models, unlike existing benchmarks (GSM8K, MathQA, Hendrycks-MATH, Math-500) that often yield zero or near-random results for SLMs."
  - [Section 5.4, Table 8] GSM8K returns 0.0% across all six tested models (70M–3B); EasyMath yields 8.31%–95.69%.
  - [corpus] Related work (rStar-Math, Phi-4-Mini-Reasoning) focuses on improving SLM reasoning but targets harder benchmarks; corpus does not directly validate EasyMath's calibration claim.
- Break condition: If models above 4B parameters approach ceiling (>98%), the benchmark becomes uninformative for larger SLMs.

### Mechanism 2
- Claim: A multi-layered evaluation pipeline (exact, numerical, symbolic matching) reduces false negatives from format variation without inflating scores via multiple-choice guessing.
- Mechanism: Free-form responses are normalized (e.g., √x → sqrt(x)), then checked via: (1) direct string match, (2) numerical evaluation, (3) SymPy symbolic equivalence, (4) difference simplification to zero. Category-specific strictness applies—arithmetic requires exactness; algebra allows equivalence.
- Core assumption: Assumption: Symbolic equivalence checking via SymPy captures mathematically correct answers across notation variants without introducing systematic false positives.
- Evidence anchors:
  - [abstract] "uses exact, numerical, and symbolic checks on free-form answers"
  - [Section 4.2, Figure 3] Evaluation pipeline flowchart shows the 5-stage process.
  - [corpus] No corpus papers directly evaluate this multi-method matching approach; validation is internal to the paper.
- Break condition: If SymPy fails to normalize domain-specific notation (e.g., matrix expressions), recall drops for advanced categories.

### Mechanism 3
- Claim: Chain-of-thought (CoT) prompting yields modest accuracy gains for SLMs (~6% absolute improvement in tested cases), substantially smaller than gains reported for LLMs.
- Mechanism: CoT encourages step-by-step reasoning, but SLMs lack capacity to reliably maintain long reasoning chains. The paper observes that reasoning-distilled models (DeepScaleR-1.5B, DS-R1-Distill-Qwen-1.5B) do not outperform standard instruct models at equivalent scale.
- Core assumption: Assumption: The gap between CoT effectiveness in LLMs vs. SLMs stems from capacity constraints rather than prompt format mismatches.
- Evidence anchors:
  - [abstract] "chain-of-thought prompting provides modest gains"
  - [Section 5.3.2, Table 7] Pythia-1.4B improves from 16.77% (standard Q-A) to 22.92% (CoT prompt)—a 6.15% absolute gain.
  - [corpus] Related work (Li et al., 2025; Godey et al., 2024, cited in paper) documents SLM struggles with long CoT distillation; corpus papers like "Enhancing Reasoning Capabilities of SLMs" propose alternative approaches (blueprints, prompt template search), indirectly supporting the capacity-constraint hypothesis.
- Break condition: If future SLMs with structured training or compression match LLM CoT gains, capacity-only explanations become insufficient.

## Foundational Learning

- Concept: **Floor effect in benchmarking** — when task difficulty far exceeds model capacity, scores cluster at zero, obscuring relative capability differences.
  - Why needed here: EasyMath's core contribution is avoiding floor effects; understanding this clarifies why existing benchmarks fail for SLMs.
  - Quick check question: If a benchmark yields 0% accuracy for all models in a class, what can you conclude about model differences?

- Concept: **Zero-shot evaluation** — testing without task-specific examples in the prompt, isolating pre-trained capability from in-context learning.
  - Why needed here: EasyMath uses 0-shot to measure standalone reasoning; multi-shot would conflate reasoning with pattern matching.
  - Quick check question: Why might 8-shot GSM8K scores overestimate a model's practical problem-solving ability?

- Concept: **Response consistency vs. accuracy** — larger models exhibit lower run-to-run variance independent of absolute accuracy.
  - Why needed here: The paper quantifies consistency (Gemma-3-4B: ±0.30%; SmolLM2-135M: ±1.71%), informing evaluation reliability.
  - Quick check question: If two models have identical average accuracy but one has 3× higher variance, which provides more reliable deployment behavior?

## Architecture Onboarding

- Component map: Dataset (13 categories) → Response extraction (regex) → Normalization → Category routing → Matching method cascade (exact → numerical → symbolic) → Result aggregation

- Critical path: Normalization → category routing → matching method cascade (any success = correct). Edge-cases category requires explicit "undefined" response.

- Design tradeoffs:
  - Free-form vs. multiple-choice: Free-form increases ecological validity but requires complex equivalence checking.
  - Category-weighted vs. question-weighted averaging: Paper uses category-equal weighting; if deployment emphasizes certain skills, reweight.
  - Strictness by category: Arithmetic demands exactness; algebra permits equivalence. Adjust per use case.

- Failure signatures:
  - Zero-accuracy across categories → model lacks instruction-following or evaluation pipeline misconfigured.
  - High algebraic-expression (AE) but low large-number (LN) scores → model relies on pattern matching, not computation.
  - High variance across runs → increase evaluation samples; suggests model instability.

- First 3 experiments:
  1. **Baseline a single model**: Run EasyMath 0-shot on your target SLM; report category breakdown and variance across 3 runs.
  2. **Ablate matching methods**: Disable symbolic equivalence; compare accuracy drop to quantify reliance on format flexibility.
  3. **CoT comparison**: Test standard Q-A vs. CoT prompts on the same model; measure category-specific gains to identify where reasoning helps most.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does explicit reasoning or chain-of-thought (CoT) prompting fail to significantly improve accuracy in Small Language Models (SLMs) compared to their effectiveness in Large Language Models (LLMs)?
- **Basis in paper:** [explicit] Section 5.3.2 explicitly states: "specifically the question of why reasoning in SLMs does not significantly improve the accuracy of models, even though such behavior is expected in LLMs."
- **Why unresolved:** The authors observed that specialized reasoning models (e.g., DeepScaleR) did not outperform standard instruct models of the same size, suggesting current distillation methods fail to transfer reasoning capabilities to small scales.
- **What evidence would resolve it:** Ablation studies comparing internal representations of CoT in SLMs vs. LLMs, or the development of training curricula specifically designed to enhance step-by-step logic within strict parameter constraints.

### Open Question 2
- **Question:** Can hybrid inference strategies, which combine symbolic solvers with lightweight neural modules, overcome the capacity saturation currently observed in SLMs on mathematical tasks?
- **Basis in paper:** [explicit] Section 6 suggests "exploring hybrid inference strategies—combining symbolic solvers with lightweight neural modules—may unlock stronger performance in resource-constrained settings."
- **Why unresolved:** Current SLMs face a "softmax bottleneck" and capacity saturation; it is unknown if externalizing computation to symbolic tools bypasses these architectural limits better than training improvements alone.
- **What evidence would resolve it:** Performance evaluations of SLMs equipped with tool-use capabilities (e.g., Python interpreters) on the EasyMath benchmark compared against pure neural baselines.

### Open Question 3
- **Question:** How can the benchmark be adapted to remain diagnostic for models larger than 4B parameters that are likely to achieve near-perfect scores?
- **Basis in paper:** [inferred] Section 7 notes "Larger models are likely to score near-perfect accuracy on EasyMath, making it less informative."
- **Why unresolved:** While the benchmark differentiates current SLMs (14M–4B), the rapid scaling of model capabilities risks a ceiling effect where the benchmark can no longer distinguish state-of-the-art improvements.
- **What evidence would resolve it:** Testing larger models (e.g., 7B–13B parameters) to establish the performance ceiling and assessing if dynamic or "hard" subsets are required to maintain evaluation utility.

## Limitations

- Category coverage imbalance: The benchmark contains uneven question counts per category (10–50 per category), with algebraic expressions over-represented, potentially skewing performance metrics.
- Evaluation pipeline complexity: The multi-method matching approach introduces potential brittleness, with SymPy's symbolic engine possibly failing to normalize certain mathematical notations.
- CoT effectiveness interpretation: The paper reports modest CoT gains but does not explore whether this reflects genuine reasoning capacity limitations or poor prompt engineering.

## Confidence

**High confidence**: The observation that EasyMath yields non-zero scores across all tested SLMs while existing benchmarks produce floor effects (GSM8K: 0.0% across 6 models).

**Medium confidence**: The claim that EasyMath's multi-layered evaluation pipeline reduces false negatives without inflating scores, though external validation of the symbolic equivalence checking approach is absent.

**Medium confidence**: The assertion that CoT prompting provides only modest gains for SLMs compared to LLMs, supported by observed 6.15% absolute improvement but lacking comprehensive exploration of alternative factors.

## Next Checks

1. **External benchmark correlation study**: Test whether EasyMath scores correlate with real-world SLM performance on practical math tasks (e.g., calculator apps, spreadsheet formulas) to validate ecological relevance beyond controlled evaluation.

2. **Cross-normalization robustness test**: Systematically vary mathematical notation (unicode vs. ASCII, different spacing conventions) in both questions and answers to quantify SymPy's normalization reliability and identify failure modes.

3. **Longitudinal difficulty calibration**: Track EasyMath performance across SLM generations (2024 vs. 2025 models) to determine whether score improvements reflect genuine capability growth or benchmark inflation, establishing temporal validity boundaries.