---
ver: rpa2
title: 'Full-Batch Gradient Descent Outperforms One-Pass SGD: Sample Complexity Separation
  in Single-Index Learning'
arxiv_id: '2602.02431'
source_url: https://arxiv.org/abs/2602.02431
tags:
- have
- probability
- bound
- gradient
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the sample and iteration complexity of
  full-batch gradient descent (GD) versus one-pass stochastic gradient descent (SGD)
  for learning a single-index model with a quadratic activation. It is known that
  one-pass SGD requires $n \gtrsim d \log d$ samples to achieve weak recovery, leaving
  a logarithmic gap from the information-theoretic limit $n \gtrsim d$.
---

# Full-Batch Gradient Descent Outperforms One-Pass SGD: Sample Complexity Separation in Single-Index Learning

## Quick Facts
- **arXiv ID:** 2602.02431
- **Source URL:** https://arxiv.org/abs/2602.02431
- **Reference count:** 40
- **Primary result:** Full-batch gradient descent can achieve better sample complexity than one-pass SGD for single-index models

## Executive Summary
This paper investigates the fundamental trade-offs between full-batch gradient descent (GD) and one-pass stochastic gradient descent (SGD) for learning single-index models with quadratic activations. The authors demonstrate a surprising theoretical separation: while one-pass SGD requires n ≳ d log d samples for weak recovery, full-batch spherical GD with truncated activations achieves the same task with only n ≳ d samples. Moreover, full-batch Euclidean GD from small initialization achieves exact recovery with n ≳ d samples and T ≳ log d gradient steps. These results provide the first evidence that data reuse in full-batch methods can yield both statistical and computational advantages over online methods in non-convex feature learning.

## Method Summary
The paper analyzes three variants of gradient descent for learning single-index models: (1) full-batch spherical gradient descent on correlation loss with unbounded quadratic activation, (2) full-batch spherical gradient flow with truncated quadratic activation, and (3) full-batch Euclidean gradient descent on squared loss from small initialization. The key innovation is the use of activation truncation, which transforms the optimization landscape to be more amenable to gradient-based methods. The analysis combines tools from statistical learning theory, optimization, and random matrix theory to establish sample complexity bounds and convergence rates. The authors compare these full-batch methods against the known results for one-pass SGD, highlighting the statistical separation in sample requirements.

## Key Results
- Full-batch spherical GD with unbounded quadratic activation requires n ≳ d log d samples for weak recovery (matching one-pass SGD)
- Full-batch spherical gradient flow with truncated activation achieves weak recovery with n ≳ d samples
- Full-batch Euclidean GD from small initialization achieves exact recovery with n ≳ d samples and T ≳ log d steps
- This is the first strong recovery guarantee for full-batch GD in the information-theoretically optimal proportional n,d regime

## Why This Works (Mechanism)
The key mechanism enabling full-batch GD's superiority is activation truncation, which creates a more favorable optimization landscape. For the truncated activation, the loss function has better curvature properties that allow gradient descent to efficiently navigate toward the true parameters. Small initialization further helps by keeping the iterates in regions where the loss landscape is well-behaved. The data reuse in full-batch updates allows the algorithm to leverage statistical correlations across samples more effectively than one-pass SGD, which processes each sample only once.

## Foundational Learning

### Single-Index Models
**Why needed:** Core problem setting being analyzed - models where output depends on data through a single linear projection
**Quick check:** Verify understanding of y = f(⟨θ*, x⟩) structure

### Sample Complexity
**Why needed:** Central metric comparing algorithmic efficiency - number of samples needed for successful recovery
**Quick check:** Distinguish between weak vs strong recovery requirements

### Gradient Flow vs Gradient Descent
**Why needed:** Different continuous vs discrete time analyses of gradient-based methods
**Quick check:** Understand how discrete steps approximate continuous flow

### Activation Functions and Truncation
**Why needed:** Critical algorithmic modification enabling improved sample complexity
**Quick check:** Grasp how truncation changes optimization landscape properties

## Architecture Onboarding

### Component Map
Data → Loss Function → Gradient Computation → Update Rule → Parameter Estimate

### Critical Path
Loss evaluation → Gradient computation → Parameter update → Convergence check → Final estimate

### Design Tradeoffs
**Batch size vs sample complexity:** Full-batch requires more memory but fewer samples
**Activation choice vs optimization landscape:** Unbounded vs truncated activations trade off between expressiveness and tractability
**Initialization scale vs convergence:** Small initialization helps but may slow early progress

### Failure Signatures
- Divergence with large initialization
- Getting stuck in suboptimal basins with unbounded activations
- Slow convergence when sample size is below threshold
- Memory overflow with very large batch sizes

### First 3 Experiments
1. Compare sample complexity of full-batch vs one-pass SGD for varying d
2. Test sensitivity to initialization scale in full-batch GD
3. Validate theoretical convergence rates empirically

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Analysis is limited to specific quadratic activation models and may not generalize
- Theoretical guarantees assume idealized optimization landscapes that may not hold in practice
- Logarithmic runtime assumes exact gradient computations without accounting for computational overhead
- Comparison doesn't consider memory constraints or distributed computing scenarios

## Confidence

**High Confidence:** The theoretical separation in sample complexity between full-batch GD and one-pass SGD for weak recovery is well-established within the specific model considered.

**Medium Confidence:** The strong recovery guarantee for full-batch GD with small initialization depends on idealized assumptions about the optimization landscape.

**Low Confidence:** Generalization to other non-convex feature learning problems and practical implications need further validation.

## Next Checks
1. Empirically validate the theoretical sample complexity separation between full-batch GD and one-pass SGD on synthetic and real-world datasets with various activation functions.

2. Investigate the robustness of the logarithmic runtime complexity to practical computational constraints, including memory limitations and distributed computing scenarios.

3. Extend the theoretical analysis to other non-convex feature learning problems, such as multi-layer neural networks or non-quadratic activation functions, to assess the generality of the findings.