---
ver: rpa2
title: 'Queueing, Predictions, and LLMs: Challenges and Open Problems'
arxiv_id: '2503.07545'
source_url: https://arxiv.org/abs/2503.07545
tags:
- scheduling
- predictions
- systems
- time
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys the use of machine learning predictions in queueing
  systems, focusing on large language model (LLM) inference scheduling. It reviews
  recent work on using predicted service times to improve queue performance, showing
  that even 1-bit predictions can significantly reduce response times compared to
  FIFO.
---

# Queueing, Predictions, and LLMs: Challenges and Open Problems

## Quick Facts
- arXiv ID: 2503.07545
- Source URL: https://arxiv.org/abs/2503.07545
- Authors: Michael Mitzenmacher; Rana Shahout
- Reference count: 40
- Key outcome: Shows even 1-bit predictions can significantly reduce response times compared to FIFO in LLM inference scheduling, while highlighting challenges of KV cache management and multi-stage inference.

## Executive Summary
This paper surveys the intersection of queueing theory and machine learning predictions, with a focus on large language model (LLM) inference scheduling. It demonstrates that simple prediction mechanisms, including 1-bit classification of job lengths, can substantially improve queue performance over traditional FIFO scheduling. The work then explores the unique challenges posed by LLM inference, including variable service times, growing KV caches, and complex preemption overhead, and surveys recent approaches to address these issues. Finally, it identifies open problems in applying queueing theory insights to LLM scheduling, particularly around handling KV cache management, load balancing, and optimizing latency under cost constraints.

## Method Summary
The paper employs a combination of theoretical analysis and simulation to evaluate prediction-based scheduling policies in queueing systems. For classical queueing scenarios, it uses discrete event simulation or SOAP queueing analysis to compare SPRPT and 1-bit prediction policies against baseline FIFO and SRPT. For LLM-specific scenarios, it reviews and synthesizes results from recent systems papers (Trail, LAMPS, Splitwise) that implement various scheduling strategies. The theoretical framework includes M/G/1 queue analysis with predicted service times, size-based scheduling policies (SRPT, SJF, PSJF), and bounded multiplicative error models for prediction accuracy.

## Key Results
- Even 1-bit predictions (short/long classification) can significantly reduce mean response time compared to FIFO scheduling
- SPRPT with preemption thresholds reduces mean latency by 1.66–2.01× while limiting KV cache memory overhead
- Different KV cache handling strategies (Preserve, Discard, Swap) for API-augmented requests yield optimal choices depending on API call duration and memory pressure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Even 1-bit predictions (classifying jobs as short or long) can substantially reduce mean response time compared to FIFO scheduling.
- Mechanism: A threshold-based classifier routes predicted-short jobs to the front of the queue and predicted-long jobs to the back. This prevents long jobs from blocking multiple shorter jobs—the primary cause of large queueing delays. Theoretical analysis yields closed-form response time expressions involving modified Bessel functions for M/M/1 with exponential prediction models.
- Core assumption: Prediction errors are asymmetric in impact; mispredicting a long job as short is far more harmful than the reverse, so prediction quality should optimize for correctly identifying long jobs.
- Evidence anchors:
  - [abstract]: "showing that even 1-bit predictions can significantly improve performance compared to FIFO"
  - [section 2.2]: Tables 2 and 3 show simulation results where prediction-based scheduling approaches SPRPT performance, especially for heavy-tailed Weibull distributions where the gap between FIFO and SRPT is largest.
  - [corpus]: Weak direct support—neighbor papers focus on LLM scheduling specifically rather than general queueing predictions.
- Break condition: If prediction error rates exceed ~50% with errors concentrated on long-job misclassification, or if prediction costs exceed the latency gains from better scheduling.

### Mechanism 2
- Claim: SPRPT with preemption thresholds reduces mean latency by 1.66–2.01× while limiting KV cache memory overhead.
- Mechanism: Trail's approach allows preemption during the "young" phase when KV cache is small, but disables preemption once a job's age exceeds c × predicted request size. This captures most scheduling benefit while avoiding expensive memory management for jobs with large allocated caches.
- Core assumption: KV cache size grows linearly with tokens processed, making late-stage preemption costly; early predictions of remaining output size are sufficiently accurate.
- Evidence anchors:
  - [section 4.1]: "Trail reduces mean latency by 1.66× to 2.01× on the Alpaca dataset and achieves 1.76× to 24.07× lower mean time to the first token compared to vLLM"
  - [section 3.4]: Quantifies KV cache overhead—"GPT-3 175B request with a sequence length of 512 tokens requires about 2.3 GB of memory"
  - [corpus]: AugServe (arXiv:2512.04013) reports similar findings for augmented LLMs with API calls.
- Break condition: If predictions systematically underestimate long outputs, jobs may pass the preemption threshold early then run much longer than expected, causing head-of-line blocking.

### Mechanism 3
- Claim: Different KV cache handling strategies (Preserve, Discard, Swap) for API-augmented requests yield optimal choices depending on API call duration and memory pressure.
- Mechanism: LAMPS assigns handling strategies before scheduling based on predicted output size and API duration, then ranks requests by predicted total memory footprint. Short API calls favor Preserve; long calls with memory pressure favor Swap or Discard.
- Core assumption: API call duration and output size can be predicted sufficiently well before scheduling; memory waste is quantifiable as the integral of unused cache over time.
- Evidence anchors:
  - [section 5.1]: "LAMPS achieves end-to-end latency improvements of 27%-85% and reductions in time-to-first-token (TTFT) of 4%-96% compared to INFERCEPT"
  - [section 5.1, Figure 9]: Visualizes memory-over-time functions showing waste areas for each strategy.
  - [corpus]: Limited—neighbor papers address related scheduling but not the specific three-strategy decomposition.
- Break condition: If API durations are highly unpredictable, static strategy assignment fails; adaptive switching mid-request may be needed but introduces additional overhead.

## Foundational Learning

- Concept: **M/G/1 queue and size-based scheduling (SRPT, SJF, PSJF)**
  - Why needed here: The entire theoretical framework for prediction-based scheduling extends these classical policies; without understanding that SRPT minimizes response time with known sizes, the motivation for predictions is unclear.
  - Quick check question: Given arrival rate λ=0.7 and service time distribution, can you explain why SRPT outperforms FIFO by >40% in Table 1?

- Concept: **Transformer autoregressive inference and KV cache**
  - Why needed here: LLM scheduling constraints (memory-bound decode, growing cache, preemption cost) all derive from this architecture; the prefill/decode distinction determines compute vs. memory bottleneck.
  - Quick check question: Why does batching help throughput in the decode phase but not proportionally in prefill?

- Concept: **Consistency and robustness in algorithms with predictions**
  - Why needed here: Section 2.3's bounded multiplicative error framework (α-consistency, β-robustness, graceful degradation) defines what "good" prediction-based scheduling means theoretically.
  - Quick check question: Why does SPRPT alone fail to provide bounded competitive ratio even with multiplicative error bounds, requiring the "bounce" variant?

## Architecture Onboarding

- Component map:
  - Request pool -> Scheduler -> Prediction module -> KV cache manager -> GPU pool -> API handler

- Critical path: Request arrival → size prediction → queue insertion with priority ranking → batch formation at iteration boundary → prefill (compute-bound) → decode iterations (memory-bound with growing KV) → completion or API pause → resume or return.

- Design tradeoffs:
  - Preemption threshold c: Lower values reduce memory overhead but sacrifice scheduling flexibility for jobs that would finish soon.
  - Prediction investment: Full service-time predictions vs. 1-bit; Section 2.4 shows SkipPredict can use cheap predictions for all jobs and expensive ones only for predicted-long jobs.
  - GPU organization: Pooled GPUs simplify scheduling but mix compute/memory-bound phases; dedicated GPUs (Splitwise) optimize each phase but risk imbalance.
  - KV cache strategy: Preserve wastes memory during API waits; Discard wastes compute on recomputation; Swap adds transfer latency (~36ms for 2.3GB over PCIe 4.0×16).

- Failure signatures:
  - **Head-of-line blocking**: Long predicted-short jobs occupy GPU while short jobs wait; indicates prediction underestimation bias.
  - **Memory exhaustion**: Batch formation fails mid-iteration; KV cache growth exceeded reservation.
  - **Preemption storms**: Frequent context switches with high swap overhead; threshold c set too high or arrival burst.
  - **API deadlock**: All requests waiting on API calls with preserved caches; no GPU slots for new requests.

- First 3 experiments:
  1. **Baseline characterization**: Measure TTFT, TPOT, and mean response time under FIFO vs. SPRPT with perfect size oracle on your workload; establishes upper bound on prediction value.
  2. **Prediction error sensitivity**: Introduce controlled noise to size predictions (multiplicative error bounds α, β) and measure latency degradation; identifies robustness requirements for your predictor.
  3. **Preemption threshold sweep**: Vary c from 0.0 (never preempt) to 1.0 (always allow) and measure both latency and peak memory usage; finds operating point for your memory budget.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can scheduling policies be designed to dynamically utilize predictions based on current system load to balance prediction costs against performance gains?
- Basis in paper: [explicit] Section 2.4 asks, "Can we design and analyze scheduling policies that choose when to use predictions based on the current load, or otherwise respond dynamically?"
- Why unresolved: Current models often assume predictions are "free" or ignore the interaction between prediction overhead and system stability in heavy-load scenarios.
- What evidence would resolve it: A scheduling framework that conditionally enables prediction mechanisms (e.g., via a load threshold) with proven bounds on stability and expected response time.

### Open Question 2
- Question: How should requests be ranked and scheduled in LLM systems given the distinct split between the compute-bound prefill phase and the memory-bound decode phase?
- Basis in paper: [explicit] Section 4.1 asks, "How should we rank requests given the split between prefill and decode stages?" and notes that hardware environments may dictate different rankings.
- Why unresolved: Standard size-based scheduling policies (like SPRPT) do not account for the multi-stage nature of inference where different phases have fundamentally different hardware bottlenecks.
- What evidence would resolve it: A scheduling policy that uses a composite ranking function for input (prefill) and output (decode) sizes to minimize latency better than single-metric heuristics.

### Open Question 3
- Question: Can we systematically estimate the number of lower-capacity GPUs required to match or exceed the performance of a single high-end GPU for LLM inference?
- Basis in paper: [explicit] Section 4.3 asks, "Can we systematically estimate a number of lower-capacity GPUs... required to match or exceed the performance of a single high-end GPU?"
- Why unresolved: Current analyses often abstract distributed models as single units, failing to capture the communication overhead and memory bandwidth tradeoffs specific to prefill and decode phases.
- What evidence would resolve it: Theoretical models or empirical benchmarks that define the "breakeven" point between clusters of smaller GPUs versus single large GPUs, accounting for inter-connect overhead.

## Limitations

- The theoretical bounds assume multiplicative error models, but real-world LLM inference times exhibit high variability that may not follow clean distributions
- The analysis assumes constant preemption costs, but KV cache swapping involves variable memory transfers that depend on cache size, GPU-CPU bandwidth, and PCIe utilization
- The current framework assumes homogeneous GPUs and single-tenant workloads, not reflecting real cloud deployments with heterogeneous accelerators and competing workloads

## Confidence

- **High confidence**: The theoretical framework for M/G/1 prediction-based scheduling (Sections 2.1-2.3) is well-established and the simulation results align with queueing theory expectations. The performance claims for basic SPRPT and 1-bit classification are supported by extensive prior work.
- **Medium confidence**: The specific LLM scheduling algorithms (Trail, LAMPS) show promising results, but their performance depends heavily on workload characteristics and prediction accuracy that may not generalize across all LLM families and use cases.
- **Low confidence**: The analysis of reasoning-based LLM systems and compound AI systems remains largely speculative, with few concrete implementations or empirical results to validate the proposed scheduling approaches.

## Next Checks

1. **Preemption cost sensitivity**: Implement Trail's scheduling with varying preemption thresholds and measure actual KV cache swap times across different GPU memory bandwidths and cache sizes. Validate whether the assumed constant preemption costs hold in practice.

2. **Prediction error distribution**: Generate synthetic job streams with heavy-tailed service time distributions and evaluate how different prediction error distributions (Gaussian vs. multiplicative vs. heavy-tailed) affect the performance gap between prediction-based and oracle scheduling.

3. **Multi-tenant scheduling stress test**: Deploy a compound AI system with multiple LLM components on heterogeneous GPUs under realistic multi-tenant load. Measure the impact of resource contention on the proposed scheduling policies and identify breaking points where theoretical optimality fails in practice.