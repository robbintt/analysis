---
ver: rpa2
title: Multivariate Uncertainty Quantification with Tomographic Quantile Forests
arxiv_id: '2512.16383'
source_url: https://arxiv.org/abs/2512.16383
tags:
- uni00000013
- uni00000011
- uni00000014
- uni00000015
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Tomographic Quantile Forests (TQF), a nonparametric
  tree-based regression method for uncertainty quantification in multivariate tabular
  data. TQF learns conditional quantiles of directional projections as functions of
  the input and direction, then reconstructs the multivariate conditional distribution
  by minimizing the sliced Wasserstein distance.
---

# Multivariate Uncertainty Quantification with Tomographic Quantile Forests

## Quick Facts
- **arXiv ID:** 2512.16383
- **Source URL:** https://arxiv.org/abs/2512.16383
- **Reference count:** 22
- **Primary result:** TQF achieves strong performance on multivariate uncertainty quantification, often outperforming both parametric and non-parametric baselines, especially in small-data regimes

## Executive Summary
This paper introduces Tomographic Quantile Forests (TQF), a nonparametric tree-based regression method for uncertainty quantification in multivariate tabular data. TQF learns conditional quantiles of directional projections as functions of the input and direction, then reconstructs the multivariate conditional distribution by minimizing the sliced Wasserstein distance. Unlike prior approaches that require separate models for each direction or produce only convex quantile regions, TQF covers all directions with a single model without convexity restrictions. Experiments on synthetic and real-world datasets show TQF achieves strong performance, often outperforming both parametric methods like Gaussian mixture models and non-parametric methods like Distributional Random Forests, especially in small-data regimes.

## Method Summary
TQF is a nonparametric tree-based regression method that learns conditional quantiles of directional projections as functions of the input and direction, then reconstructs the multivariate conditional distribution by minimizing the sliced Wasserstein distance. The method augments the target variable with trigonometric features to allow standard forest splitting criteria to detect complex distributional shifts, and uses an optimization-based approach to reconstruct the full multivariate distribution from directional quantiles.

## Key Results
- TQF achieves Energy Distance scores of 0.055-0.065 on a challenging synthetic dataset, compared to 0.058-0.110 for GMM3 and 0.127-0.638 for GMM1
- TQF generalizes better than Distributional Random Forests in small-data regimes, particularly on the sliding-disk dataset with N=30 samples
- TQF covers all directions with a single model without convexity restrictions, unlike prior approaches that require separate models for each direction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multivariate distributions can be effectively learned by decomposing the problem into estimating conditional quantiles of 1-dimensional directional projections.
- **Mechanism:** TQF relies on the Cramér-Wold theorem, which states a probability measure is uniquely determined by its projections. Instead of directly modeling the joint density $p(\mathbf{y}|\mathbf{x})$, TQF learns the quantiles of the scalar projection $\mathbf{n}^\top\mathbf{y}$ conditioned on both input $\mathbf{x}$ and direction $\mathbf{n}$. This allows the model to leverage robust univariate quantile regression techniques (QRF++) while retaining multivariate dependencies.
- **Core assumption:** The conditional distribution of the projection $\mathbf{n}^\top\mathbf{y}$ can be accurately captured by a forest model given the augmented input.
- **Evidence anchors:** [abstract] "TQF learns conditional quantiles of directional projections... as functions of the input $\mathbf{x}$ and the unit direction $\mathbf{n}$."

### Mechanism 2
- **Claim:** Augmenting the target variable with trigonometric features allows standard forest splitting criteria to detect complex distributional shifts (e.g., multimodality) that purely mean-based splits miss.
- **Mechanism:** Standard Random Forests use MSE/variance reduction, which is sensitive to changes in mean but blind to changes in distribution shape (variance/multimodality) if the mean is constant. QRF++ augments the target $y$ with $\cos(y/w)$ and $\sin(y/w)$ (Eq. 14). Since the characteristic function uniquely determines the distribution, splits on these Fourier features effectively partition the data based on distributional shape.
- **Core assumption:** The distributional information is sufficiently captured by a finite set of frequency components $T$.
- **Evidence anchors:** [page 7] "Augment the learning target... followed by a per-component normalization... The rationale... E[cosy|x] jumps... creating a strong signal."

### Mechanism 3
- **Claim:** A multivariate distribution can be reconstructed from directional quantiles by optimizing a weighted point cloud to minimize the Sliced Wasserstein distance.
- **Mechanism:** The Quantile-Matching Empirical Measure (QMEM) algorithm reconstructs the distribution $p(\mathbf{y}|\mathbf{x})$ not by analytical inversion, but by optimization. It initializes a point cloud and iteratively adjusts point locations (via KDE sampling) and weights (via convex optimization) so that the cloud's own projected quantiles match the model's predicted quantiles. This loss function is the Sliced Wasserstein distance.
- **Core assumption:** The optimization landscape, while non-convex in point locations, can be navigated via alternating optimization to reach a global or satisfactory local minimum.
- **Evidence anchors:** [abstract] "...reconstructs the multivariate conditional distribution by minimizing the sliced Wasserstein distance via an efficient alternating scheme..."

## Foundational Learning

- **Concept:** **Radon Transform and Cramér-Wold Theorem**
  - **Why needed here:** This is the mathematical justification for TQF's existence. Without understanding that 1D projections uniquely define a high-dimensional object, the decomposition strategy seems lossy.
  - **Quick check question:** Can two distinct multivariate distributions have identical marginal distributions for every possible linear projection? (Answer: No, per Cramér-Wold).

- **Concept:** **Quantile Regression Forests (QRF) vs. Standard Regression Forests**
  - **Why needed here:** You must understand that standard RFs approximate the conditional *mean*, while QRFs approximate the conditional *distribution* (CDF) by weighing neighbors. TQF builds on QRF.
  - **Quick check question:** Does a standard Random Forest split maximize the difference in mean or variance? Does QRF change the splitting mechanism or just the leaf aggregation? (Answer: QRF generally keeps the split mechanism [CART] but changes the prediction aggregation, though TQF modifies the split input via augmentation).

- **Concept:** **Sliced Wasserstein Distance**
  - **Why needed here:** This is the loss function TQF minimizes to reconstruct the distribution. It makes the computation of Wasserstein distance tractable by averaging 1D Wasserstein distances over random projections.
  - **Quick check question:** Why is the standard Wasserstein distance difficult to compute in high dimensions compared to the Sliced version?

## Architecture Onboarding

- **Component map:** Preprocessing -> Data Augmentation (Stage I) -> QRF++ Backbone -> Inference (Stage II) -> QMEM Reconstruction
- **Critical path:** The QMEM reconstruction loop is the inference bottleneck and the most complex logic. It involves fitting a KDE, sampling new support points, and solving a convex weight optimization (SLSQP) repeatedly until convergence.
- **Design tradeoffs:**
  - **Sample Augmentation Factor ($G$):** Higher $G$ increases training data size (linear cost increase) but improves the model's "understanding" of directional geometry.
  - **Projection Count ($K$) & Quantile Count ($M$):** Inference fidelity depends on these. Low $K/M$ results in "streaky" or blurred reconstructions (undersampling).
  - **Point Cloud Size ($N_1$):** Larger point clouds capture finer details but slow down the optimization loop.
- **Failure signatures:**
  - **Convex "Bleed":** If $K$ is too low, the reconstruction may default to a convex hull of the true distribution, missing internal holes or concavities.
  - **Quantile Crossing:** While QRF is generally monotonic, poor hyperparameters could lead to crossing quantiles in the projection space, causing the QMEM optimizer to fail or produce invalid distributions.
  - **Sparse Support:** In small data regimes, if the QMEM initialization is poor, the point cloud may converge to a local mode rather than the full distribution.
- **First 3 experiments:**
  1. **QRF++ Validation:** Reproduce the synthetic check (Fig 2). Train QRF++ on a dataset with constant mean but changing variance/multimodality. Verify that the target importance of the Fourier features (Fig 3) is high.
  2. **QMEM Stress Test:** Run the QMEM algorithm on the "Two Moons" dataset (Fig 4) with low $K=5$ vs high $K=50$. Observe the reconstruction quality to build intuition for the tomographic sampling rate.
  3. **Small-Data Baseline:** Compare TQF vs. Distributional Random Forests (DRF) on the "sliding-disk" dataset (Sec 6.4) with $N=30$ samples to verify the claim that TQF generalizes better in low-data regimes.

## Open Questions the Paper Calls Out

- **Question:** How can the selection of hyperparameters, specifically the sample augmentation factor ($G$) and feature augmentation factor ($\tilde{G}$), be automated to make TQF more robust?
  - **Basis in paper:** The conclusion states that "TQF involves several hyperparameters, and tuning them can be time-consuming," identifying robustness to these choices as an important direction for future work.
  - **Why unresolved:** The paper notes that optimal choices for $G$ and $\tilde{G}$ are sensitive to dataset size and characteristics, currently requiring manual tuning.
  - **What evidence would resolve it:** An adaptive mechanism or heuristic for setting $G$ and $\tilde{G}$ that achieves comparable performance to manually tuned baselines across diverse datasets without extensive search.

- **Question:** Can TQF be effectively adapted for large-scale spatiotemporal forecasting tasks?
  - **Basis in paper:** Section 7 explicitly lists "applications to complex, large-scale spatiotemporal data, such as those arising in weather forecasting" as a plan for future research.
  - **Why unresolved:** The current experimental validation is limited to static tabular datasets, and it is unclear if the reconstruction step (QMEM) scales efficiently to the high-dimensional, correlated structures found in weather data.
  - **What evidence would resolve it:** Benchmarks on standard weather forecasting datasets showing TQF maintains computational efficiency and predictive accuracy compared to specialized deep learning models.

- **Question:** Can TQF be extended to quantify epistemic uncertainty (model uncertainty) rather than just aleatoric uncertainty?
  - **Basis in paper:** Section 2.1 states that "tree-based methods... are not well suited for measuring epistemic uncertainty" and the authors assume the main application is aleatoric uncertainty, leaving epistemic quantification unaddressed.
  - **Why unresolved:** Safety-critical applications often require distinguishing between data noise (aleatoric) and lack of data/model knowledge (epistemic), which the current architecture does not support.
  - **What evidence would resolve it:** A modification of the TQF framework that provides calibrated uncertainty intervals for out-of-distribution inputs, potentially via integration with conformal prediction or ensembling techniques.

## Limitations
- The QMEM reconstruction algorithm's success critically depends on the non-convex optimization of support points, which is not fully specified in the paper
- Performance may be sensitive to hyperparameter choices (G, eG) that work well for specific datasets but may not generalize universally
- Computational cost scales with the number of projections K and ensemble size E, potentially making it expensive for high-dimensional targets

## Confidence
- **High Confidence:** The theoretical foundation using Cramér-Wold theorem and sliced Wasserstein distance is well-established. The empirical comparison with GMM baselines on synthetic datasets appears robust.
- **Medium Confidence:** The QRF++ augmentation mechanism is theoretically sound, but its effectiveness depends on proper frequency selection (w parameters) which may require dataset-specific tuning.
- **Medium Confidence:** The overall TQF framework shows promising results, but the specific hyperparameter choices (G=20, eG=10) that work well may not generalize universally without careful validation.

## Next Checks
1. **Parameter Sensitivity Analysis:** Systematically vary G and eG parameters around the reported values (15,1), (20,10), and (1,10) to quantify the robustness of TQF performance to data augmentation choices.

2. **QMEM Optimization Stability:** Run the QMEM reconstruction with different random seeds and compare the stability of Energy Distance scores across runs to assess the sensitivity to the non-convex optimization component.

3. **High-Dimensional Extension:** Test TQF on a dataset with higher-dimensional targets (d_y > 5) to evaluate whether the method maintains its advantage over parametric approaches as dimensionality increases.