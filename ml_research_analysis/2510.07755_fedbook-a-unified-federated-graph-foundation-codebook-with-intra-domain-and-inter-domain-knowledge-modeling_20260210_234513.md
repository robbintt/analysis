---
ver: rpa2
title: 'FedBook: A Unified Federated Graph Foundation Codebook with Intra-domain and
  Inter-domain Knowledge Modeling'
arxiv_id: '2510.07755'
source_url: https://arxiv.org/abs/2510.07755
tags:
- graph
- fedbook
- federated
- fedgfm
- client
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses federated training of graph foundation models
  (FedGFMs) where decentralized graph data across multiple domains must be aggregated
  without centralized access. The core challenge is constructing a global codebook
  that maintains intra-domain coherence while preserving inter-domain diversity.
---

# FedBook: A Unified Federated Graph Foundation Codebook with Intra-domain and Inter-domain Knowledge Modeling

## Quick Facts
- arXiv ID: 2510.07755
- Source URL: https://arxiv.org/abs/2510.07755
- Reference count: 40
- Outperforms 21 baselines across 8 benchmarks, achieving average improvements of 5.9% in node classification, 4.2% in edge classification, and 6.9% in graph classification over isolated supervised models

## Executive Summary
This paper addresses federated training of graph foundation models (FedGFMs) where decentralized graph data across multiple domains must be aggregated without centralized access. The core challenge is constructing a global codebook that maintains intra-domain coherence while preserving inter-domain diversity. The proposed FedBook method introduces a two-phase aggregation approach: Intra-domain Collaboration refines low-frequency tokens by referencing semantically similar high-frequency tokens within the same domain, and Inter-domain Integration weights client contributions based on semantic distinctiveness across domains. FedBook consistently outperforms 21 baselines across 8 benchmarks spanning multiple domains and tasks, achieving average improvements of 5.9% in node classification, 4.2% in edge classification, and 6.9% in graph classification over isolated supervised models, with faster convergence than existing FedGFM approaches.

## Method Summary
FedBook is a federated graph foundation model pre-training method that uses a graph Vector Quantized Masked Auto-Encoder (gVQ-MAE) backbone. Each client maintains a local codebook and runs masked auto-encoding reconstruction on its graph data. The server performs two-phase aggregation: Phase 1 (Intra-domain) refines low-frequency tokens by aligning them with higher-frequency tokens from other clients based on cosine similarity, while also aggregating encoder/decoder weights using client-wise semantic similarity. Phase 2 (Inter-domain) computes domain distinctiveness weights and performs weighted aggregation of the global model. After pre-training, the global encoder is frozen and task-specific heads are trained locally. The method uses a 2-layer GraphSAGE encoder (768-dim), a multi-head codebook (4 heads × 128 tokens), and a linear decoder for graph reconstruction.

## Key Results
- Outperforms 21 baselines across 8 benchmarks spanning multiple domains and tasks
- Achieves average improvements of 5.9% in node classification, 4.2% in edge classification, and 6.9% in graph classification over isolated supervised models
- Demonstrates faster convergence than existing FedGFM approaches
- Ablation studies confirm importance of both intra-domain collaboration (3-4% drop when removed) and inter-domain integration (2-3% drop when removed)

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Guided Token Refinement
Low-frequency tokens are refined by aligning them with semantically similar high-frequency tokens, improving codebook reliability. Token access frequency during local training serves as a proxy for semantic reliability. During aggregation, each token is updated as a weighted combination of higher-frequency tokens from other clients, filtered by cosine similarity. The alignment weight mask (Eq. 6) discards contributions from lower-frequency tokens, ensuring knowledge flows only from more reliable sources.

### Mechanism 2: Client-Wise Semantic Similarity for Personalized Aggregation
Aggregating encoder/decoder weights based on semantic similarity between clients' codebooks improves domain-specific representations. Client similarity (Δ_a,b in Eq. 8) is computed as the average maximum token-wise cosine similarity between codebooks. Non-codebook parameters are aggregated using softmax-weighted combinations based on this similarity, allowing each client to receive a model personalized to its semantic neighborhood.

### Mechanism 3: Domain Distinctiveness Weighting for Global Codebook
Weighting client contributions by their semantic distinctiveness preserves diverse inter-domain knowledge that would otherwise be diluted. Domain distinctiveness (∇_a in Eq. 10) is computed as 1 minus average client similarity. Clients with more unique codebooks receive higher weights in the final global aggregation (Eq. 11), ensuring rare domain knowledge isn't overwhelmed by majority domains.

## Foundational Learning

- **Vector Quantization (VQ) in autoencoders**: gVQ-MAE backbone maps continuous graph embeddings to discrete codebook tokens. Understanding VQ-VAE-style quantization (nearest-neighbor lookup, straight-through estimator) is essential to grasp how gradients flow through non-differentiable quantization. Quick check: Can you explain why the straight-through estimator (STE) is needed to train codebook embeddings end-to-end?

- **Federated Averaging (FedAvg) and its limitations with heterogeneous data**: FedBook is positioned as improving upon naive FedAvg aggregation for GFMs. Understanding why uniform averaging fails with heterogeneous domains motivates the two-phase design. Quick check: Why might averaging model weights from clients with fundamentally different data distributions produce a worse global model?

- **Graph Neural Network message passing**: The encoder is a GNN (GraphSAGE-based) that processes node/edge features. Understanding how GNNs aggregate neighborhood information clarifies what gets quantized into the codebook. Quick check: How does a GNN's receptive field grow with layer depth, and how might this affect what semantic patterns get captured in codebook tokens?

## Architecture Onboarding

- **Component map**: GraphSAGE encoder (2-layer, 768-dim) -> Multi-head codebook (4 heads × 128 tokens) -> Linear decoder -> Feature/topology reconstruction. Local training minimizes feature + topology reconstruction + commitment loss. Server performs Phase 1 (token alignment + personalized aggregation) then Phase 2 (distinctiveness-weighted global aggregation).

- **Critical path**: Initialize global gVQ-MAE parameters and broadcast to all K clients → Clients run E local epochs of masked auto-encoding reconstruction → Clients upload codebook, encoder/decoder weights, token frequencies → Server Phase 1: For R_1 rounds, compute pairwise token similarities → align low-freq tokens → personalized aggregation → send updated params back → Server Phase 2: For R_2 rounds, compute distinctiveness → weighted global aggregation → broadcast → Post pre-training: Freeze encoder, train task heads locally.

- **Design tradeoffs**: Personalized vs. global models (Phase 1 produces client-specific models; Phase 2 collapses to single global model); Codebook size (4×128 balances expressiveness and communication overhead); λ trade-off (controls how much tokens are updated from others vs. preserved).

- **Failure signatures**: Codebook collapse (many tokens become unused; check token access frequencies); Slow convergence (if Phase 1 dominates without progress, client similarities may be uniformly low); Global model underperforms local models (may indicate Phase 2 over-aggregates or critical domain knowledge was lost in Phase 1).

- **First 3 experiments**: Validate codebook utilization (compute entropy of token access frequencies per client; target >50% tokens with non-trivial frequency); Ablate phases separately (train FedBook with only Phase 1 vs. only Phase 2 on 3-domain benchmark; expect Phase 1 ablation to hurt more on diverse datasets); Monitor distinctiveness weights (log ∇_a values per client during Phase 2; verify clients from different domains have higher distinctiveness).

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Data partitioning strategy uses exactly 3 clients across all experiments, limiting generalization to scenarios with more clients or different partitioning strategies
- Communication efficiency claims lack concrete cost comparisons with baselines or analysis of scaling with more clients
- Hyperparameter sensitivity, particularly Phase 1 vs Phase 2 round allocation (R₁ vs R₂), is not fully explored

## Confidence
- **High confidence**: Frequency-guided token refinement mechanism is well-specified with clear mathematical formulations (Eq 6-7) and supported by ablation results showing 3-4% performance drops when disabled
- **Medium confidence**: Domain distinctiveness weighting mechanism has theoretical justification and shows measurable performance improvements in ablations, but relies on assumptions about semantic uniqueness correlation
- **Low confidence**: Client-wise semantic similarity for personalized aggregation shows moderate ablation improvements but underlying assumptions remain weakly validated by related work alone

## Next Checks
1. **Multi-client scalability test**: Evaluate FedBook with 10-15 clients instead of 3 to verify that intra-domain collaboration and inter-domain distinctiveness mechanisms remain effective as client heterogeneity increases. Measure both performance and communication overhead scaling.

2. **Cross-domain transfer validation**: Train FedBook on three distinct domains (e.g., citation, molecular, social), then evaluate fine-tuned models on a held-out fourth domain not seen during pre-training. This tests whether the global codebook truly captures transferable knowledge rather than overfitting to training domains.

3. **Noise robustness analysis**: Intentionally inject corrupted codebook tokens (e.g., random embeddings) into one client during Phase 1, then measure how the frequency-guided refinement and distinctiveness weighting prevent propagation of these errors to the global model.