---
ver: rpa2
title: What does really matter in image goal navigation?
arxiv_id: '2507.01667'
source_url: https://arxiv.org/abs/2507.01667
tags:
- navigation
- trained
- image
- goal
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the architectural choices that affect learning
  directional information in image goal navigation. The authors investigate whether
  relative pose estimation capabilities can emerge through end-to-end RL training
  alone, without explicit pose supervision or pre-training.
---

# What does really matter in image goal navigation?

## Quick Facts
- arXiv ID: 2507.01667
- Source URL: https://arxiv.org/abs/2507.01667
- Reference count: 40
- Primary result: Late fusion architectures fail at image goal navigation; early fusion and cross-attention succeed, with pre-training crucial for best performance

## Executive Summary
This paper investigates architectural choices for learning relative pose estimation in image goal navigation without explicit pose supervision. Through controlled experiments across multiple fusion strategies (late fusion, channel stacking, cross-attention) and training regimes, the authors demonstrate that early fusion architectures enable agents to learn both navigation and relative pose estimation capabilities. The study reveals a strong correlation between navigation performance and emerging pose estimation ability, while highlighting the limitations of end-to-end reinforcement learning for high-capacity models without pre-training. The findings suggest that architectural design enabling direct image comparison is critical for learning effective navigation policies.

## Method Summary
The study evaluates image goal navigation in Habitat simulator using Gibson dataset with 72 training and 14 validation scenes. Agents observe 112×112 RGB images and must navigate to goal locations shown in reference images. The action space consists of discrete movements (0.25m forward, 10° turns, STOP) with maximum 1000 steps per episode. Multiple architectural variants are compared: Late Fusion with separate ResNet9 encoders for observation and goal, ChannelCat with single ResNet9 processing stacked 6-channel input, and DEBiT with frozen pre-trained CroCo-based binocular ViT plus trainable ResNet18 observation encoder. All agents use 2-layer GRU with 128 hidden units and linear policy heads, trained via PPO for 500M steps with navigation rewards. Relative pose estimation capabilities are probed by freezing trained encoders and training lightweight heads on 68M image pairs from multiple datasets.

## Key Results
- Late Fusion architectures achieve only ~12-13% Success Rate regardless of backbone (ResNet9 or ViT-Small)
- Early fusion (ChannelCat) achieves 83.6% SR with sliding enabled, 31.7% SR without sliding
- Cross-attention (DEBiT) achieves 76.7% SR with pre-training, but fails without pre-training
- Strong correlation exists between navigation performance and relative pose estimation accuracy
- Sliding simulation improves training but requires perception weight preservation for effective transfer to non-sliding settings

## Why This Works (Mechanism)
The key mechanism enabling successful image goal navigation is direct feature comparison between observation and goal images. Early fusion architectures (ChannelCat, DEBiT) allow the network to compute local correspondences by processing both images through shared or directly comparable feature spaces. This enables the agent to reason about relative pose transformations needed to reach the goal. Late Fusion fails because separate encoders prevent meaningful comparison - the network cannot establish correspondences between observation and goal feature maps. The success of pre-training on large binocular datasets provides rich visual representations that capture geometric relationships, making it easier for RL to learn navigation policies that leverage this geometric understanding.

## Foundational Learning
- **Relative pose estimation**: Ability to infer camera position and orientation relative to goal image; needed for navigation planning and directly correlates with success
- **Feature fusion strategies**: Methods for combining observation and goal representations; early fusion enables direct comparison while late fusion prevents it
- **Habitat simulator navigation**: Photorealistic 3D environment navigation with discrete actions; provides controlled testbed for architectural comparisons
- **PPO reinforcement learning**: On-policy algorithm for training navigation policies; requires careful reward shaping to encourage goal-directed behavior
- **Sliding simulation**: Physics approximation where agents move without collision constraints; speeds training but may reduce real-world transfer
- **Relative pose verification probing**: Evaluating encoder capabilities without affecting policy training; reveals learned geometric reasoning

## Architecture Onboarding

**Component Map:**
Observation and Goal Images → Visual Encoder (ϕ) → GRU → Linear Policy (π) → Actions
                          ↓
                  Previous Action Embedding (ζ) → GRU

**Critical Path:**
Visual encoder output → GRU hidden state → Policy head → Action selection

**Design Tradeoffs:**
- Late Fusion vs Early Fusion: Late fusion prevents feature comparison, causing failure; early fusion enables geometric reasoning but may increase parameter count
- Sliding vs Non-sliding: Sliding speeds training but creates unrealistic motion dynamics; non-sliding is realistic but harder to learn
- Pre-training vs From-scratch: Pre-training provides rich visual features enabling better navigation; from-scratch training struggles with high-capacity models

**Failure Signatures:**
- Low Success Rate (<20%) with Late Fusion architecture indicates inability to compare observation and goal features
- High early-stop failures indicate undertrained perception (goal rarely seen near end)
- High timeout failures indicate undertrained navigation policy despite good perception

**First 3 Experiments:**
1. Train ChannelCat agent with sliding enabled to establish baseline performance
2. Train Late Fusion agent with same settings to confirm architectural failure
3. Train ChannelCat agent without sliding to measure realistic setting performance

## Open Questions the Paper Calls Out
- **Open Question 1**: Does the "undertraining" of perception modules in non-sliding (Sliding=False) environments stem primarily from reduced goal visibility/observation diversity, or from the lack of gradient signal caused by agents getting stuck?
- **Open Question 2**: Can the visual features learned under the unrealistic "sliding" simulation regime be effectively transferred to real-world physical robots, or does the motion dynamics mismatch negate the perception benefits observed in simulation?
- **Open Question 3**: What specific inductive biases or auxiliary losses are necessary to enable high-capacity architectures (like binocular ViTs) to learn image-goal navigation from RL alone without pre-training?

## Limitations
- Heavy reliance on pre-training suggests end-to-end RL alone may be insufficient for complex navigation tasks
- Significant performance gap between sliding and non-sliding settings (83.6% vs 31.7% SR) raises questions about real-world applicability
- Study confined to Habitat simulator environments, limiting claims about physical robot deployment

## Confidence
- **High confidence**: Relative pose estimation capabilities correlate with navigation performance across all tested architectures
- **Medium confidence**: Early fusion architectures consistently outperform late fusion for both navigation and pose estimation
- **Medium confidence**: Sliding simulation enables better transfer learning, but realistic non-sliding settings remain challenging

## Next Checks
1. Test cross-attention architecture (DEBiT) without pre-training to isolate benefits of architectural choice vs pre-training
2. Evaluate transfer to physical robot platforms using depth completion from single RGB input to validate simulator-to-real generalization
3. Compare against explicit pose supervision baselines to quantify the performance gap between implicit and explicit relative pose learning