---
ver: rpa2
title: Privacy-aware Berrut Approximated Coded Computing applied to general distributed
  learning
arxiv_id: '2505.06759'
source_url: https://arxiv.org/abs/2505.06759
tags:
- secure
- training
- nodes
- learning
- computing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of integrating privacy-preserving
  approximate coded computing into general distributed learning scenarios. The authors
  extend Private Berrut Approximate Coded Computing (PBACC) to handle tensor inputs
  and multiple data owners, enabling secure aggregation and training in both centralized
  and decentralized federated learning settings.
---

# Privacy-aware Berrut Approximated Coded Computing applied to general distributed learning

## Quick Facts
- arXiv ID: 2505.06759
- Source URL: https://arxiv.org/abs/2505.06759
- Reference count: 39
- Extends Private Berrut Approximate Coded Computing (PBACC) to handle tensor inputs and multiple data owners for secure distributed learning

## Executive Summary
This paper introduces Privacy-aware Berrut Approximate Coded Computing (PBACC) to address privacy-preserving distributed learning. The method extends Private Berrut Coded Computing to handle tensor inputs and multiple data owners, enabling secure aggregation and training in both centralized and decentralized federated learning settings. PBACC uses rational interpolation with Chebyshev points for encoding, adds Gaussian noise for privacy, and employs Berrut barycentric interpolation for decoding. The privacy metric bounds mutual information leakage per data element, which can be made arbitrarily small by adjusting noise parameters.

## Method Summary
PBACC encodes input data tensors using rational interpolation with Chebyshev points, adding Gaussian noise for privacy. The core mechanism involves encoding model parameters across interpolation points, then decoding using Berrut barycentric interpolation. The method supports three configurations: secure training over centralized data (DLCD), secure aggregation over decentralized data (DLDD), and secure training over decentralized data. Privacy is quantified through mutual information bounds, with experiments showing less than 1 bit leakage per element while maintaining high model accuracy across CNN, VAE, and Cox regression models.

## Key Results
- PBACC achieves less than 1 bit privacy leakage per data element while maintaining model accuracy
- CNN models trained on MNIST reach 98% accuracy with privacy guarantees
- VAE models on Fashion MNIST and Cox regression on METABRIC dataset show similar privacy-utility tradeoffs
- Secure training in decentralized settings provides strongest privacy but incurs higher communication costs

## Why This Works (Mechanism)
PBACC leverages rational function approximation to encode model parameters across multiple interpolation points. By distributing computations across nodes using Chebyshev points and adding calibrated Gaussian noise, the method creates uncertainty that bounds information leakage. The Berrut barycentric interpolation formula enables accurate decoding from encoded shares while the noise ensures privacy. The mutual information framework quantifies privacy guarantees, making them adjustable through noise parameters.

## Foundational Learning
- **Chebyshev Interpolation Points**: Optimal points for polynomial/rational approximation that minimize Runge's phenomenon. Why needed: Provides stable encoding/decoding for PBACC. Quick check: Verify interpolation accuracy on test functions.
- **Berrut Barycentric Formula**: Efficient method for evaluating rational interpolants. Why needed: Enables decoding from encoded model parameters. Quick check: Test on known rational functions.
- **Mutual Information Privacy Metrics**: Quantifies information leakage between private data and encoded representations. Why needed: Provides rigorous privacy guarantees for PBACC. Quick check: Calculate bounds for simple Gaussian examples.
- **Secure Aggregation Protocols**: Methods for combining encoded model updates without revealing individual contributions. Why needed: Enables privacy-preserving federated learning. Quick check: Verify correctness on small-scale examples.

## Architecture Onboarding
- **Component Map**: Data → Chebyshev Encoding → Gaussian Noise → Distributed Computation → Secure Aggregation → Berrut Decoding → Model Update
- **Critical Path**: The encoding-decoding pipeline with noise injection is the core privacy mechanism. Any breakdown in rational interpolation or noise calibration directly impacts privacy-utility tradeoffs.
- **Design Tradeoffs**: Higher privacy (larger σ_n) degrades model accuracy; larger K (interpolation points) increases communication costs; secure training provides stronger privacy than aggregation alone.
- **Failure Signatures**: Accuracy degradation beyond expected privacy-utility curves, non-convergence in training, numerical overflow in encoded computations.
- **First Experiments**: 1) Validate PBACC encoding/decoding on simple polynomial functions, 2) Test privacy leakage bounds with known noise parameters, 3) Benchmark CNN accuracy with varying privacy levels on MNIST.

## Open Questions the Paper Calls Out
**Open Question 1**: Can PBACC be combined with verifiable computation mechanisms to detect malicious nodes that corrupt results, beyond the current honest-but-curious threat model? The conclusion states future work will address combining PBACC with computation verification algorithms, but the current threat model only assumes honest-but-curious behavior.

**Open Question 2**: Can model quality be preserved when the privacy leakage threshold approaches zero (ϵ → 0)? The paper identifies this as a limitation where model quality deteriorates at very low privacy thresholds, but doesn't propose solutions for extreme privacy requirements.

**Open Question 3**: Does the observed accuracy degradation in secure training over DLDD genuinely provide output privacy against membership inference attacks? The authors hypothesize that lower accuracy indicates implicit output privacy but haven't implemented membership inference attacks to confirm this.

**Open Question 4**: How can PBACC handle numerically unstable model components such as exponential functions in vanilla VAEs without overflow? The paper used Gumbel-Softmax as a workaround for VAEs but didn't solve the underlying numerical instability issue.

## Limitations
- Exact neural network architectures (layer configurations, parameter counts) remain unspecified, creating uncertainty in reproducing accuracy-privacy tradeoffs
- Aggregation protocol details for DLDD configuration are unclear regarding whether nodes aggregate locally or centrally
- Shift parameter b for noise Chebyshev points is unspecified, though results claim insensitivity to this choice
- Computational complexity analysis assumes equal parameter distribution across nodes without detailing the actual splitting mechanism

## Confidence
- **High Confidence**: Theoretical framework for PBACC privacy guarantees, feasibility of achieving <1 bit leakage while maintaining accuracy, correctness of Berrut interpolation encoding/decoding
- **Medium Confidence**: Empirical accuracy-privacy tradeoffs shown in Tables III and IV, as exact CNN/VAE architectures remain unknown
- **Low Confidence**: Absolute computational times reported in Table V due to unknown hardware configurations and unspecified aggregation functions

## Next Checks
1. **Validate privacy leakage bounds**: Implement mutual information calculation from Eq. 12 using covariance matrices from Eq. 13 for a simple 2-node case with known noise parameters, verifying that iL < 1 bit per element as claimed

2. **Replicate DLCD accuracy-privacy tradeoff**: With the exact CNN architecture, test PBACC training on MNIST with σ_n ∈ {10, 50, 100, 200, 400} while monitoring both accuracy degradation and convergence stability

3. **Benchmark DLDD secure training**: Implement the secure training protocol for the CNN model with K=1, measuring both accuracy (comparing against uncoded baseline) and actual communication costs per epoch to verify efficiency claims