---
ver: rpa2
title: Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling
arxiv_id: '2505.24185'
source_url: https://arxiv.org/abs/2505.24185
tags:
- federated
- learning
- task
- feddea
- multi-task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of federated multi-task learning
  (FMTL) where clients perform heterogeneous tasks and struggle to jointly train a
  unified global model due to parameter update interference. To address this, the
  authors propose FedDEA (Federated Decoupled Aggregation), which dynamically identifies
  and preserves task-relevant update dimensions while rescaling them to maintain optimization
  strength.
---

# Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling

## Quick Facts
- **arXiv ID:** 2505.24185
- **Source URL:** https://arxiv.org/abs/2505.24185
- **Reference count:** 40
- **Primary result:** FedDEA achieves superior unified model performance on NYUD-V2 and PASCAL-Context datasets compared to FedAvg and FedProx.

## Executive Summary
This paper addresses the challenge of federated multi-task learning (FMTL) where heterogeneous clients performing different tasks struggle to jointly train a unified global model due to parameter update interference. The authors propose FedDEA (Federated Decoupled Aggregation), a method that dynamically identifies and preserves task-relevant update dimensions while rescaling them to maintain optimization strength. FedDEA operates without requiring task labels or architectural changes, making it broadly applicable. Experiments demonstrate that FedDEA outperforms existing methods in overall performance and consistently improves various federated optimization algorithms when integrated as a plug-in.

## Method Summary
FedDEA tackles parameter update interference in FMTL by implementing a two-step process: Parameter Decoupling and Recalibration. During Parameter Decoupling, clients compute the difference between their local and global model parameters, then retain only the top ρ% of dimensions with largest absolute magnitudes (creating a binary mask) while zeroing out the rest. During Recalibration, the retained update values are rescaled by 1/ρ to preserve the original optimization energy. The server aggregates these processed updates across clients, leveraging the assumption that task-relevant updates concentrate in mutually exclusive subspaces, allowing a single unified model to master multiple heterogeneous tasks.

## Key Results
- FedDEA constructs a unified global model that outperforms existing methods in overall performance on NYUD-V2 and PASCAL-Context datasets
- When integrated as a plug-in into various federated optimization algorithms (FedAvg, FedProx, FedOpt), FedDEA consistently delivers significant improvements
- The method validates robustness and generalization under highly heterogeneous task settings without requiring task labels or architectural changes

## Why This Works (Mechanism)

### Mechanism 1
FedDEA isolates task-relevant signals by filtering parameter updates based on magnitude. Clients compute the element-wise difference between local and global models, retaining only the top ρ fraction of dimensions with largest absolute magnitudes while zeroing out the rest. This projects updates onto task-support subspaces under the assumption that task-relevant updates concentrate in high-magnitude dimensions while task-irrelevant noise resides in low-magnitude dimensions.

### Mechanism 2
To prevent convergence stall from masking-induced norm reduction, FedDEA rescales retained updates by 1/ρ. This approximates the original magnitude of the update vector, ensuring the step size remains effective in the projected subspace. The method assumes the raw magnitude reflects necessary optimization force, and reducing it via masking would proportionally slow down learning.

### Mechanism 3
FedDEA enables a single global model to master multiple heterogeneous tasks by aggregating filtered, rescaled updates that reduce directional conflict. Instead of averaging full models where conflicting gradients cancel out, the server averages only the masked updates. Since task subspaces are largely disjoint, the aggregated update applies distinct optimizations to distinct parameters within the same model.

## Foundational Learning

- **Concept: Federated Averaging (FedAvg)**
  - *Why needed here:* FedDEA is explicitly designed as a plug-in replacement for the aggregation step in FedAvg and its variants
  - *Quick check question:* How does weighting client updates by dataset size differ from uniform averaging?

- **Concept: Multi-Task Learning (MTL) Architecture**
  - *Why needed here:* The paper assumes a unified global model structure where interference happens in the shared encoder
  - *Quick check question:* Where does "negative transfer" typically occur in a hard-parameter-sharing MTL model?

- **Concept: Gradient/Update Sparsity**
  - *Why needed here:* The core intuition relies on the premise that neural network updates are sparse and lie in low-dimensional subspaces
  - *Quick check question:* Why does pruning small weights (magnitude-based pruning) often preserve accuracy?

## Architecture Onboarding

- **Component map:** Client Side (computes Δ, applies mask, rescales) -> Server Side (receives processed updates, performs weighted aggregation)
- **Critical path:** Calculating the threshold for the top-ρ percent of updates requires sorting the flattened update vector or using efficient partition algorithms on the client side
- **Design tradeoffs:**
  - Selection Ratio (ρ): Low ρ (e.g., 0.1) maximizes decoupling but risks losing critical information; high ρ (e.g., 0.9) minimizes interference suppression
  - Finding: Paper finds optimal ρ between 0.2 and 0.3
- **Failure signatures:**
  - Training Collapse/NaN: Incorrect rescaling or extremely large updates causing overflow
  - Stagnation: If ρ is set too low or magnitude distribution is flat, masking acts as random noise
  - Regression vs. Classification: Ensure metric Δ calculation accounts for direction
- **First 3 experiments:**
  1. Baseline Validation: Run standard FedAvg on NYUD-V2 with task-partitioned clients to observe interference
  2. Plug-in Test: Integrate FedDEA into FedAvg, sweep ρ to find optimal performance
  3. Ablation (Rescaling): Run FedDEA with masking but without 1/ρ rescaling to verify convergence impact

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the FedDEA framework be effectively extended to multimodal federated multi-task learning scenarios where data modalities differ significantly across clients?
- **Open Question 2:** Is the fixed selection ratio (ρ) a bottleneck for tasks with varying complexity, and would an adaptive or layer-wise ρ yield better optimization?
- **Open Question 3:** Does the element-wise masking and rescaling strategy require strict architectural homogeneity, limiting its use in cross-architecture settings?
- **Open Question 4:** What are the theoretical convergence guarantees for FedDEA in non-convex settings?

## Limitations
- The magnitude-based masking heuristic may not hold universally across all heterogeneous FL scenarios and could discard important low-magnitude task-critical updates
- The optimal selection ratio ρ appears dataset and task-dependent with limited guidance on hyperparameter tuning in general cases
- The method assumes sufficient task-level update orthogonality which may break down when tasks are highly correlated or adversarial

## Confidence
- **High confidence:** Empirical results showing FedDEA's improvement over FedAvg baselines on NYUD-V2 and PASCAL-Context datasets
- **Medium confidence:** Claim that FedDEA is broadly applicable without task labels or architectural changes based on successful integration with FedProx and FedOpt
- **Low confidence:** Universality of the magnitude-based masking heuristic across all heterogeneous FL scenarios given limited evaluated task types

## Next Checks
1. **Cross-dataset robustness test:** Evaluate FedDEA on datasets with different characteristics (e.g., medical imaging, satellite data) to verify if the magnitude-based masking heuristic generalizes beyond computer vision tasks
2. **Task correlation stress test:** Design experiments with deliberately correlated or adversarial tasks to measure how FedDEA performs when task subspaces overlap significantly
3. **Algorithm diversity validation:** Implement FedDEA as a plug-in for additional federated optimization algorithms (e.g., SCAFFOLD, FedNova) to confirm broad applicability beyond the three algorithms tested