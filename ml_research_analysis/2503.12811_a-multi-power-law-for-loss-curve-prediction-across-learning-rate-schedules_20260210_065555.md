---
ver: rpa2
title: A Multi-Power Law for Loss Curve Prediction Across Learning Rate Schedules
arxiv_id: '2503.12811'
source_url: https://arxiv.org/abs/2503.12811
tags:
- loss
- learning
- schedules
- schedule
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Multi-Power Law to predict the pretraining
  loss curve of large language models under various learning rate schedules. The law
  combines a power-law term based on the cumulative sum of learning rates with additional
  terms that capture the loss reduction induced by learning rate decay.
---

# A Multi-Power Law for Loss Curve Prediction Across Learning Rate Schedules

## Quick Facts
- **arXiv ID**: 2503.12811
- **Source URL**: https://arxiv.org/abs/2503.12811
- **Reference count**: 40
- **Primary result**: Multi-Power Law predicts LLM pretraining loss curves across learning rate schedules with R² > 0.99 accuracy

## Executive Summary
This paper introduces a Multi-Power Law (MPL) to predict pretraining loss curves of large language models under arbitrary learning rate schedules. The law combines a power-law term based on cumulative learning rate sums with additional terms capturing loss reduction from learning rate decay. Validated across Llama-2 architecture variants (25M-400M parameters), MPL accurately predicts loss curves for unseen schedules after fitting on just 2-3 training runs. The approach enables discovery of optimized schedules that outperform standard cosine baselines, while theoretical analysis shows the law emerges under specific spectral properties of Hessian and noise covariance matrices in quadratic loss settings.

## Method Summary
The Multi-Power Law predicts loss L(t) as L(t) = L0 + A·(S1(t) + SW)^(-α) - LD(t), where S1(t) is the cumulative sum of learning rates and LD(t) captures loss reduction from decay. Parameters {L0, A, B, C, α, β, γ} are fitted on 2-3 schedule-loss curve pairs using Huber loss with Adam optimization (50k steps at 5e-3 LR for α,β,γ; 50k steps at 5e-2 LR for coefficients). Validation uses Llama-2 (25M/100M/400M params) trained with constant and cosine schedules for 24k steps each (warmup 2160 steps, peak LR 3e-4, final LR 3e-5). Predictions are evaluated on unseen schedules (WSD, two-stage with varying ηB) using R², MAE, and RMSE metrics.

## Key Results
- MPL achieves R² > 0.99 for loss curve prediction on test schedules after fitting on only 2-3 training runs
- Optimized schedules discovered via MPL outperform cosine baselines on downstream benchmarks
- Theoretical analysis shows MPL emerges under specific spectral properties of Hessian and noise covariance in quadratic loss settings

## Why This Works (Mechanism)
The Multi-Power Law captures the fundamental dynamics of model pretraining by modeling how accumulated gradient steps (via cumulative learning rates) drive loss reduction while accounting for the diminishing returns from learning rate decay. The power-law term reflects the inverse relationship between total effective gradient steps and loss, while the decay term captures how reduced learning rates slow optimization progress. The empirical success across architectures and schedules suggests this formulation captures universal properties of the pretraining optimization landscape.

## Foundational Learning
- **Power-law scaling in deep learning**: Why needed - fundamental to understanding how loss decreases with training compute; Quick check - verify log-log plots of loss vs. steps follow power-law patterns
- **Learning rate scheduling effects**: Why needed - different schedules impact optimization dynamics and final performance; Quick check - compare constant vs. cosine vs. step schedules on convergence
- **Cumulative learning rate analysis**: Why needed - total effective gradient steps determine optimization progress; Quick check - compute S1(t) = Σητ for various schedules and compare to loss reduction
- **Huber loss for robust fitting**: Why needed - handles outliers in loss curves better than L2 loss; Quick check - compare parameter stability across different loss functions
- **Adam optimizer with staged learning rates**: Why needed - proper fitting requires careful optimization strategy; Quick check - verify convergence with 50k steps at each LR stage
- **Quadratic loss landscape theory**: Why needed - provides theoretical justification for MPL structure; Quick check - verify spectral assumptions hold for simple quadratic objectives

## Architecture Onboarding

**Component Map**
Data Collection -> MPL Fitting -> Schedule Prediction -> Downstream Evaluation

**Critical Path**
1. Train reference models with 2-3 schedules to collect loss curves
2. Fit MPL parameters using staged Adam optimization
3. Generate predictions for unseen schedules
4. Validate predictions and discover optimized schedules

**Design Tradeoffs**
- Fitting on 2-3 runs vs. comprehensive training: balances efficiency with prediction accuracy
- Quadratic loss theory vs. empirical fitting: provides theoretical grounding but may not fully capture non-convex dynamics
- Fixed vs. adaptive peak learning rate: current MPL requires refitting for different peak rates

**Failure Signatures**
- Poor convergence with L-BFGS (use Adam instead)
- Prediction errors at high peak learning rates (>4-6e-4)
- Long-horizon prediction drift (>3x training steps)

**First Experiments**
1. Train 25M Llama-2 with constant and cosine schedules for 24k steps, record validation loss
2. Fit MPL parameters using staged Adam optimization with Huber loss
3. Validate predictions on WSD and two-stage schedules, compute R², MAE, RMSE

## Open Questions the Paper Calls Out
1. **Non-quadratic loss landscapes**: Can MPL be theoretically derived for complex neural network structures beyond quadratic cases? The current quadratic analysis cannot capture "river valley" landscapes and Edge of Stability phenomena observed in LLM training.

2. **Varying peak learning rates**: Can MPL be generalized to handle different ηmax values within a single unified formulation? Current parameters are scale-dependent, with prediction accuracy degrading as peak LR increases.

3. **Current learning rate dependence**: What theoretical mechanism explains the explicit ηk^(-γ) scaling in the loss reduction term? The quadratic model fails to replicate this observed dependence, suggesting missing dynamics in current theory.

## Limitations
- Theoretical derivation assumes quadratic loss landscapes, which may not capture complex non-convex optimization dynamics
- Validation primarily on Llama-2 architecture with specific hyperparameters (batch size 0.5M tokens, weight decay 0.1)
- Law's accuracy depends critically on proper initialization and fitting procedures, with convergence to local minima possible
- Current formulation requires refitting for different peak learning rates, limiting practical flexibility

## Confidence
- **High confidence**: Empirical validation showing R² > 0.99 for test set predictions and successful discovery of improved schedules over cosine baselines
- **Medium confidence**: Theoretical justification under quadratic loss assumptions, which may not fully extend to non-convex LLM objectives
- **Medium confidence**: Generalization to unseen learning rate schedules after fitting on only 2-3 training runs, though validated across multiple schedule types

## Next Checks
1. Test MPL predictions on architectures beyond Llama-2 (e.g., GPT-style or transformer-XL variants) to assess architectural robustness
2. Validate the law's performance across different batch sizes and weight decay values to understand hyperparameter sensitivity
3. Implement the optimized schedules discovered via MPL in large-scale (7B+ parameter) pretraining runs to verify claimed performance improvements hold at scale