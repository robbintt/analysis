---
ver: rpa2
title: 'Prometheus Mind: Retrofitting Memory to Frozen Language Models'
arxiv_id: '2601.15324'
source_url: https://arxiv.org/abs/2601.15324
tags:
- memory
- training
- hidden
- adapters
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Prometheus Mind retrofits memory to a frozen Qwen3-4B model using
  11 modular adapters (530MB, 7% overhead) without modifying base model weights. The
  system solves four key problems: (1) extracting facts using Contrastive Direction
  Discovery (CDD) without labeled data, (2) training adapters stage-wise on simple
  proxy tasks to avoid collapse, (3) injecting retrieved answers via attention K-V
  pairs using lmhead.weight rows as value vectors (Identity V), and (4) fixing hidden
  state collapse through learned projections that recover semantic distinctions.'
---

# Prometheus Mind: Retrofitting Memory to Frozen Language Models

## Quick Facts
- **arXiv ID:** 2601.15324
- **Source URL:** https://arxiv.org/abs/2601.15324
- **Reference count:** 40
- **Primary result:** 94.4% retrieval accuracy on clean inputs using 11 modular adapters (530MB overhead) without modifying base model weights

## Executive Summary
Prometheus Mind addresses the challenge of adding persistent memory to frozen language models by introducing a modular adapter architecture that operates without weight modification. The system extracts structured facts using Contrastive Direction Discovery (CDD), stores them in a hierarchical memory structure, and retrieves answers via multi-hop reasoning with attention-based injection. Key innovations include stage-wise adapter training to prevent collapse, Identity V injection using lm_head.weight rows for generalization, and learned projections to fix hidden state collapse. The system achieves strong performance on clean inputs but shows significant degradation on adversarial cases, with relation classification identified as the primary bottleneck.

## Method Summary
Prometheus Mind retrofits memory to a frozen Qwen3-4B model through 11 modular adapters and two learned projections, totaling 530MB overhead. The approach uses Contrastive Direction Discovery to extract semantic roles without labeled data, trains adapters stage-wise on simple proxy tasks to avoid collapse, injects retrieved answers via attention K-V pairs using lm_head.weight rows (Identity V), and fixes hidden state collapse through learned projections. The system stores facts in a structured memory hierarchy and retrieves them through multi-hop reasoning, with injection occurring at layer 35 using first-token-only K-V pairs to prevent repetition loops.

## Key Results
- **L6 retrieval accuracy:** 94.4% on clean inputs (n=54, 95% CI: [84.9%, 98.1%]), 57.6% overall, 19.4% on adversarial inputs (n=36)
- **Relation classification accuracy:** 47.3%, responsible for 89% of extraction errors
- **Memory injection:** Identity V using lm_head.weight rows achieves 87.5% test accuracy vs 0% for learned V encoder
- **Stage-wise training:** Prevents collapse observed in end-to-end training (1/11 accuracy vs target 94.4%)

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Direction Discovery (CDD)
CDD extracts semantic structure from transformer hidden states without labeled data by contrasting minimal pairs. When two sentences differ in exactly one semantic aspect, their hidden state difference vector isolates that aspect. Averaging many such difference vectors yields robust semantic directions. For subject extraction, two orthogonal directions are combined: Subject-Object (relational position relative to verb) and Head-Modifier (noun vs determiner distinction). This relies on the assumption that semantic distinctions are encoded in approximately linear subspaces of hidden state space.

### Mechanism 2: Stage-Wise Training Prevents Collapse
Training 11 adapters in isolation on simple proxy tasks succeeds where end-to-end optimization collapses to near-random performance. Each adapter receives direct, task-specific gradients with no interference between components. Composition is handled by system architecture rather than joint optimization. This relies on the assumption that memory system components are functionally independent specialists whose correct behaviors compose cleanly.

### Mechanism 3: Identity V via lm_head.weight
Using the frozen model's lm_head.weight rows directly as attention value vectors generalizes to any vocabulary token without training. During pretraining, lm_head learned to map hidden states to token probabilities. Row i of lm_head is the direction that, when projected onto by a hidden state, produces high probability for token i. Injecting this row as a value vector at layer 35 makes the model attend to and output the target token. This assumes the pretrained lm_head encoding is both necessary and sufficient for token generation.

## Foundational Learning

- **Attention K-V pairs and injection mechanics:** Memory is injected as attention key-value pairs that the model attends to through its native attention computation. Understanding Q, K, V roles is essential to grasp how Identity V works. Quick check: If you inject a value vector at a late layer, what determines whether the model outputs the corresponding token vs enters a repetition loop?

- **Linear probing and semantic subspaces:** CDD assumes semantic roles are recoverable via linear projections of hidden states. The hidden state collapse problem (0.98 cosine similarity for "wife" vs "brother") and its fix via learned projection require understanding what variance components exist in hidden states. Quick check: Why would 86% shared variance in hidden states be useful for generation but problematic for classification?

- **Stage-wise vs end-to-end training dynamics:** The central training insight is that joint optimization fails but sequential isolated training succeeds. Understanding credit assignment, gradient interference, and proxy task design explains why this works. Quick check: What makes a "simple proxy task" appropriate for stage-wise training? What could go wrong if the proxy task doesn't match downstream requirements?

## Architecture Onboarding

- **Component map:**
  Frozen base (Qwen3-4B) -> Extraction adapters (BetaGate, IdentityAdapter, UserAdapter, FirstPersonValueMask, PairRelationClassifier, LocalWindowClassifier) -> Reasoning adapters (IntermediateRelationDetector, HopController, AnswerSelector, QueryEntityExtractor) -> Routing adapters (TypeClassifier, RoutingAdapter, MessageTypeClassifier, EntityClassifier) -> Projections (UniversalProjection, SentenceProjection) -> Memory structure (4 types × 64 slots × 13 relations) -> Identity V injection at layer 35

- **Critical path:**
  1. Input text → frozen model forward pass to layer 29
  2. CDD extraction → structured fact storage
  3. Query arrives → MessageTypeClassifier → IntermediateRelationDetector
  4. Multi-hop reasoning: QueryEntityExtractor → HopController loops → memory lookups
  5. Retrieved answer → Identity V construction (lm_head.weight row × norm factor)
  6. Attention K-V injection at layer 35, first generated token only
  7. Continue generation normally

- **Design tradeoffs:**
  - Stage-wise vs end-to-end: Robustness vs potential joint optimization benefits
  - Layer 35 injection (97% depth): Optimal for single-token injection; earlier layers showed lower accuracy
  - First-token-only injection: Prevents repetition loops but limits to single-token answers
  - 11 specialized adapters vs unified network: Composability and debuggability vs architectural complexity

- **Failure signatures:**
  - Repetition loops ("Mark Mark Mark..."): Injecting on multiple tokens or using hidden state blending at late layers
  - Hidden state collapse (0.98 similarity): Attempting cosine similarity comparisons on raw hidden states for relation classification
  - Zero generalization: Using learned V encoder instead of Identity V (100% train, 0% test)
  - Multi-subject/object failure: 0% accuracy on L6 for multi_subject and multi_object categories
  - Relation classification bottleneck: 47.3% accuracy, responsible for 89% of extraction errors

- **First 3 experiments:**
  1. Validate CDD on held-out minimal pairs: Create 20-30 minimal pair sentences, compute hidden state directions, measure extraction accuracy per semantic role. Compare single-direction vs combined-direction scoring.
  2. Identity V generalization test: Train a learned V encoder on 50 fact→token mappings. Test both learned encoder and Identity V on 20 held-out facts. Verify near-zero vs high generalization gap.
  3. Projection necessity ablation: Attempt relation classification on raw hidden states vs projected hidden states using a fixed classifier. Measure accuracy and similarity scores for semantically distinct vs similar word pairs.

## Open Questions the Paper Calls Out

- **Cross-model transfer:** Can CDD and Identity V mechanisms transfer effectively to other model families (e.g., Llama, Mistral) or significantly larger parameter scales? All experiments used Qwen3-4B and transfer has not been validated.

- **Scaling behavior:** Does system performance degrade gracefully or collapse as the number of stored entities scales into the thousands, given current routing bottlenecks? Testing limited to 88 entities, scaling behavior unknown.

- **Injection depth optimization:** What is the theoretical justification for 97% depth (layer 35) being optimal, and does this generalize to deeper models? This was empirically tuned with no theoretical understanding.

## Limitations
- **Single-token injection:** Identity V cannot handle multi-token answers, limiting practical utility
- **Relation classification bottleneck:** 47.3% accuracy responsible for 89% of extraction errors
- **Limited scaling validation:** Only tested on 88 entities with no analysis of performance degradation at scale

## Confidence
- **High confidence:** Stage-wise training prevents collapse, Identity V generalization advantage, hidden state collapse problem itself
- **Medium confidence:** CDD effectiveness, injection depth optimization, projection mechanism
- **Low confidence:** Long-term robustness under distribution shift, multi-hop reasoning scalability, adversarial robustness beyond 36-case subset

## Next Checks
1. **Cross-model CDD validation:** Apply CDD methodology to a different frozen model (e.g., LLaMA-3-8B) and measure semantic direction extraction accuracy on 30-40 held-out minimal pairs.

2. **Multi-token injection extension:** Modify Identity V to handle 2-3 token answers and test on 20-30 multi-token facts from existing corpus versus current first-token-only baseline.

3. **Joint optimization ablation:** Implement end-to-end trained variant with gradient checkpointing and weight decay, compare L6 retrieval accuracy against stage-wise baseline on 50 randomly selected cases.