---
ver: rpa2
title: 'From Isolation to Entanglement: When Do Interpretability Methods Identify
  and Disentangle Known Concepts?'
arxiv_id: '2512.15134'
source_url: https://arxiv.org/abs/2512.15134
tags:
- concept
- concepts
- steering
- arxiv
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how well interpretability methods identify
  and disentangle known concepts in neural networks. Using synthetic natural language
  data with controllable correlations between concepts (e.g., sentiment, domain, tense),
  the authors test whether common featurization methods like sparse autoencoders (SAEs)
  and sparse probes can learn disentangled representations.
---

# From Isolation to Entanglement: When Do Interpretability Methods Identify and Disentangle Known Concepts?

## Quick Facts
- **arXiv ID**: 2512.15134
- **Source URL**: https://arxiv.org/abs/2512.15134
- **Reference count**: 40
- **Key outcome**: This study evaluates how well interpretability methods identify and disentangle known concepts in neural networks. Using synthetic natural language data with controllable correlations between concepts (e.g., sentiment, domain, tense), the authors test whether common featurization methods like sparse autoencoders (SAEs) and sparse probes can learn disentangled representations. While SAEs achieve high scores on correlational disentanglement metrics (MCC, DCI-ES), steering experiments reveal widespread non-independence: steering one concept often affects unrelated concepts. The results show that disjointness (affecting non-overlapping subspaces) does not imply independence (selective manipulability), and that current featurization objectives may not optimize for the right notion of concept separation. These findings underscore the need for compositional evaluations in interpretability research.

## Executive Summary
This paper investigates whether interpretability methods can truly disentangle known concepts in neural networks. The authors create a synthetic natural language dataset where they control the correlations between concepts like sentiment, domain, and tense. They evaluate common featurization methods including sparse autoencoders and sparse probes to see if these methods can learn representations where concepts are truly independent. The study reveals a critical disconnect: while traditional disentanglement metrics show high scores, steering experiments demonstrate that manipulating one concept often unintentionally affects others, revealing that disjoint feature subspaces do not guarantee selective manipulability.

The key insight is that current featurization objectives may optimize for the wrong notion of concept separation. The findings suggest that interpretability methods might produce representations that appear disentangled by correlational metrics but fail to achieve true independence in practice. This has important implications for how we evaluate and develop interpretability tools, emphasizing the need for compositional evaluations that test actual concept independence rather than just statistical correlations.

## Method Summary
The authors construct a synthetic natural language dataset with controllable correlations between concepts including sentiment, domain, and tense. They train neural networks on this data and apply interpretability methods including sparse autoencoders and sparse probes to extract feature representations. The evaluation uses both correlational metrics (MCC, DCI-ES) and steering experiments where they manipulate individual concepts to test for unintended effects on other concepts. The synthetic nature of the dataset allows precise control over concept relationships, enabling rigorous testing of whether featurization methods can achieve true disentanglement.

## Key Results
- SAEs achieve high scores on correlational disentanglement metrics (MCC, DCI-ES) but fail steering experiments
- Steering one concept often affects unrelated concepts, revealing non-independence despite apparent disjointness
- Disjointness (affecting non-overlapping subspaces) does not imply independence (selective manipulability)
- Current featurization objectives may not optimize for the right notion of concept separation

## Why This Works (Mechanism)
The paper's mechanism centers on the distinction between statistical disjointness and functional independence. While SAEs and sparse probes can learn features that occupy separate subspaces in the embedding space, this spatial separation does not guarantee that these features can be manipulated independently. The steering experiments reveal that concept representations are entangled in their functional effects on model outputs, even when they appear separated in feature space. This suggests that current featurization objectives, which optimize for statistical criteria like sparsity and correlation reduction, do not capture the true notion of concept independence needed for reliable interpretability.

## Foundational Learning
The study contributes to foundational understanding by demonstrating that traditional disentanglement metrics may be insufficient for evaluating interpretability methods. The finding that disjointness does not imply independence challenges the assumption that feature separation in vector space corresponds to functional independence in neural network behavior. This reveals a gap in our theoretical understanding of what constitutes a "disentangled" representation and suggests that interpretability research needs more nuanced evaluation frameworks that test actual manipulability rather than just statistical correlations.

## Architecture Onboarding
The experiments are conducted on neural network architectures trained on synthetic natural language data, with specific focus on how interpretability methods extract and represent concepts from these models. The paper examines SAEs and sparse probes as featurization techniques, which are commonly used in mechanistic interpretability research. The synthetic dataset construction allows for controlled experiments with known ground truth about concept relationships, making it possible to rigorously evaluate whether these interpretability methods can correctly identify and disentangle the specified concepts.

## Open Questions the Paper Calls Out
The paper highlights several open questions for the interpretability community: How can we develop evaluation metrics that better capture functional independence rather than just statistical correlations? What featurization objectives would optimize for true concept disentanglement? How do these findings extend to real-world datasets where concept relationships are more complex and dynamic? The disconnect between disjointness and independence metrics suggests that current interpretability methods may need fundamental rethinking to achieve their stated goals.

## Limitations
- The synthetic nature of the dataset, while providing precise control over concept correlations, limits generalizability to real-world scenarios where concept relationships are more complex and dynamic
- The evaluation focuses on a relatively small set of linguistic concepts (sentiment, domain, tense), which may not capture the full complexity of concept entanglement in practical applications
- Results are based on specific featurization techniques and may not extend uniformly to other interpretability methods
- The steering experiments, while revealing, represent a specific type of manipulation that may not capture all forms of concept interaction

## Confidence
- **High**: Confidence in the core findings regarding the disconnect between disjointness and independence metrics
- **Medium**: Confidence for broader implications about interpretability methods, as results are based on specific featurization techniques
- **Low**: Confidence for claims about real-world neural network behavior due to synthetic experimental setup

## Next Checks
1. Replicate the experiments on a real-world dataset (such as movie reviews or news articles) with naturally occurring concept correlations to test external validity
2. Test additional featurization methods beyond SAEs and sparse probes, including modern contrastive learning approaches and supervised concept discovery methods
3. Investigate whether different steering magnitudes and directions reveal more nuanced relationships between concept manipulation and entanglement effects
4. Explore alternative evaluation metrics that capture functional independence rather than just statistical correlations
5. Examine whether incorporating task-specific objectives into featurization methods improves true disentanglement performance