---
ver: rpa2
title: Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information
  Retrieval
arxiv_id: '2511.19325'
source_url: https://arxiv.org/abs/2511.19325
tags:
- query
- expansion
- retrieval
- language
- cross-lingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines the use of multilingual large language models
  for cross-lingual query expansion in information retrieval. It compares three open-source
  models (Aya Expanse 8B, Gemma 3 4B, and Gemma 3 12B) across four prompting strategies
  (zero-shot, Chain-of-Thought, Rephrase and Respond, and few-shot) on two datasets:
  CLIRMatrix and mMARCO.'
---

# Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval

## Quick Facts
- arXiv ID: 2511.19325
- Source URL: https://arxiv.org/abs/2511.19325
- Reference count: 40
- Primary result: Query length is the key factor determining optimal prompting strategy—zero-shot for short queries, few-shot for longer queries.

## Executive Summary
This paper investigates generative query expansion using multilingual large language models (mLLMs) for cross-lingual information retrieval. The study compares three open-source mLLMs (Aya Expanse 8B, Gemma 3 4B, and Gemma 3 12B) across four prompting strategies on two multilingual datasets (CLIRMatrix and mMARCO). Results demonstrate that query length critically determines which prompting strategy performs best, with zero-shot prompting optimal for short queries and few-shot prompting excelling for longer natural language queries. The research also reveals that cross-lingual expansion provides the largest relative improvements for languages with weaker baselines, though retrieval into non-Latin script languages remains persistently challenging.

## Method Summary
The method employs multilingual LLMs to generate pseudo-documents that expand queries for cross-lingual retrieval. Four prompting strategies are tested: zero-shot, Chain-of-Thought (CoT), Rephrase and Respond (RaR), and few-shot. Two datasets are used: CLIRMatrix (8 languages, ~15M docs, 104k queries) and mMARCO (14 languages, 123.2M docs, ~7.4M queries). The pipeline involves translating queries (if needed), expanding them via mLLM-generated pseudo-documents, and performing BM25 retrieval using either the pseudo-document alone or concatenated query+pseudo-document. Fine-tuning experiments are conducted on Aya Expanse 8B using CLIRMatrix monolingual subsets. The study varies expansion order (translate-then-expand vs expand-then-translate) and retrieval formulation.

## Key Results
- Query length determines optimal prompting: zero-shot works best for short queries (1-2 words), few-shot excels for longer natural language queries
- Cross-lingual expansion yields largest relative improvements for languages with weakest baselines (e.g., Chinese, Japanese)
- Retrieval into non-Latin script languages remains persistently poor despite expansion
- Fine-tuning on CLIR-style data provides modest gains when test queries match training format; mismatched formats cause performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Zero-shot prompting is optimal for short queries (1-2 words), while few-shot prompting excels for longer natural-language queries.
- **Mechanism:** Short queries are highly ambiguous; elaborate prompts (CoT, RaR) generate meta-text and concept drift that harm sparse retrieval (BM25). Few-shot prompting provides exemplars that help the model match the query-document format expected in longer queries.
- **Core assumption:** BM25 remains representative of retrieval behavior for expanded queries; the paper does not evaluate dense retrievers.
- **Evidence anchors:**
  - [abstract]: "Results show that query length is a key factor determining the effectiveness of different prompting techniques—zero-shot prompting performs best for short queries, while few-shot prompting excels with longer queries."
  - [section 4.2]: "Because the queries are extremely short, the introduction of terms and phrases that are not directly relevant can substantially degrade retrieval. For CoT prompting and RaR, the model frequently generates meta text such as 'To answer this query, I will provide information about. . .'"
  - [corpus]: Weak direct corpus support for this specific length-dependent mechanism; related work (Exp4Fuse, Query2doc) focuses on prompting strategies without the length interaction.
- **Break condition:** When using dense retrievers instead of BM25, the length-dependent pattern may differ; also breaks if queries are in domains not seen during mLLM pretraining.

### Mechanism 2
- **Claim:** Cross-lingual query expansion yields the largest relative improvements for languages with the weakest baselines, but retrieval INTO non-Latin script languages remains persistently poor.
- **Mechanism:** Expansion adds disambiguating context to the query side, helping low-performing query languages (e.g., Chinese, Japanese in CLIRMatrix). However, document-side retrieval difficulty persists because script differences and representation quality are not addressed by query-side expansion alone.
- **Core assumption:** The observed improvements generalize beyond the specific Wikipedia (CLIRMatrix) and translated MS MARCO corpora used.
- **Evidence anchors:**
  - [abstract]: "Cross-lingual expansion improves retrieval, especially for languages with weaker baselines, but performance remains lower for non-Latin scripts."
  - [section 4.3]: "Chinese queries benefit from the largest relative improvements from the use of cross-lingual query expansion... Chinese documents show the smallest gains."
  - [corpus]: "The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English Corpora" (arXiv:2507.07543) finds similar script-dependent retrieval challenges in cross-lingual RAG.
- **Break condition:** When query and document languages share the same script (e.g., European language pairs), the relative improvement gap narrows; also breaks if document-side expansion or multilingual dense embeddings are applied.

### Mechanism 3
- **Claim:** Fine-tuning on CLIR-style data improves retrieval only when test queries match the training query format; mismatched formats cause performance degradation.
- **Mechanism:** Fine-tuning on short title-like queries (CLIRMatrix) teaches the model a specific query-document pattern that transfers poorly to longer question-style queries (mMARCO). The model overfits to the structural properties of the fine-tuning domain.
- **Core assumption:** The observed losses are due to query format mismatch rather than other dataset differences (domain, translation method).
- **Evidence anchors:**
  - [abstract]: "Fine-tuning on similar-style data yields modest gains, while mismatched fine-tuning data causes losses."
  - [section 4.4]: "For mMARCO we observe a decrease in retrieval performance for every metric across both fine-tuned models as compared to the original base model."
  - [corpus]: No direct corpus evidence on fine-tuning transfer for query expansion specifically; this is a novel contribution of the paper.
- **Break condition:** If fine-tuning data includes mixed query formats or is substantially larger, format-specific overfitting may be mitigated.

## Foundational Learning

- **Concept: Query Expansion via Pseudo-Documents**
  - Why needed here: The paper shifts from traditional synonym-based expansion to generating hypothetical documents that bridge the length gap between queries and documents for retrieval.
  - Quick check question: Can you explain why a generated pseudo-document might help dense retrieval more than adding synonyms to the query?

- **Concept: Cross-Lingual Information Retrieval (CLIR) Pipeline**
  - Why needed here: The paper varies translation-expansion order (T+E vs E+T) and retrieval formulation (doc-only vs Q+doc); understanding the pipeline is essential to interpret results.
  - Quick check question: In a CLIR pipeline, what is the difference between pre-translation and post-translation expansion?

- **Concept: Sparse vs Dense Retrieval Metrics**
  - Why needed here: The paper uses BM25 (sparse) but acknowledges dense retrievers are increasingly common; metrics like Recall@k, MRR, and nDCG@k are interpreted differently across paradigms.
  - Quick check question: Why might MRR be more informative than Recall@k for a dataset like mMARCO with a single relevant document per query?

## Architecture Onboarding

- **Component map:** Query input → Translation step (if needed) → Expansion step (mLLM generates pseudo-document via prompting) → Retrieval formulation (pseudo-doc only OR query+pseudo-doc) → BM25 retrieval → Metric evaluation (Hit@k, Recall@k, MRR, nDCG@k). Fine-tuning intervenes at expansion step.

- **Critical path:** Start with the base mLLM (Aya Expanse 8B or Gemma 3 12B), apply zero-shot prompting for short queries or few-shot for long queries, use expansion-before-translation (E+T) with query+pseudo-doc concatenation for highest recall. Verify baseline first.

- **Design tradeoffs:** Larger models (Gemma 3 12B) give better performance but higher inference cost. Few-shot prompting requires exemplar data. Pre-translation expansion may improve precision; post-translation may improve recall. Concatenating query+pseudo-doc helps more for longer queries but adds compute.

- **Failure signatures:** (1) Performance drop with CoT/RaR on short queries → meta-text contamination in BM25. (2) Low retrieval into Chinese/Arabic documents despite expansion → script representation gap not addressed. (3) Fine-tuned model underperforms base on different-format queries → format overfitting.

- **First 3 experiments:**
  1. Replicate zero-shot vs few-shot on a held-out subset of CLIRMatrix and mMARCO to confirm query-length interaction.
  2. Test expansion-before-translation (E+T) vs translation-before-expansion (T+E) on a low-resource language pair (e.g., Arabic-English) to measure precision/recall tradeoff.
  3. Fine-tune Aya Expanse 8B on a MIX of CLIRMatrix and mMARCO data to test whether format mixing mitigates the transfer loss observed in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can direct zero-shot cross-lingual generation outperform the sequential translation-then-expansion pipeline?
- Basis in paper: [explicit] The conclusion suggests future research should consider zero-shot cross-lingual generation rather than the sequential translation and expansion implementation used in the study.
- Why unresolved: The study implemented a two-stage process where queries were translated before expansion; direct generation from source to target language expansion was not evaluated.
- What evidence would resolve it: Comparative experiments measuring retrieval performance between the two-stage pipeline and direct cross-lingual generation using mLLMs.

### Open Question 2
- Question: Does fine-tuning on mixed datasets mitigate the performance degradation seen when training and test query formats differ?
- Basis in paper: [explicit] The authors propose investigating whether fine-tuning on mixed datasets could lead to gains across domains and different styles of queries.
- Why unresolved: The study only tested fine-tuning on single datasets (CLIRMatrix), which resulted in performance losses when applied to the different-styled mMARCO dataset.
- What evidence would resolve it: Experiments fine-tuning models on combined datasets (e.g., CLIRMatrix and mMARCO) and evaluating performance across both domains.

### Open Question 3
- Question: Do the optimal prompting strategies for short versus long queries remain consistent when using dense retrieval methods?
- Basis in paper: [inferred] The study relied on BM25 (sparse retrieval), explicitly noting in the limitations that the relative behavior of techniques may vary for dense retrieval methods.
- Why unresolved: The paper established that zero-shot prompting works best for short queries and few-shot for long queries using BM25, but it is unknown if this holds for embedding-based dense retrieval.
- What evidence would resolve it: Re-evaluating the prompting techniques (Zero-shot vs. Few-shot) across query lengths using dense retrieval architectures like bi-encoders.

## Limitations

- Reliance on BM25 as the sole retrieval backend limits generalizability to modern dense retrieval settings
- Fine-tuning experiments limited to short query formats (CLIRMatrix), may not generalize to mixed or longer query styles
- Does not explore document-side expansion, which could address persistent retrieval difficulties into non-Latin script languages
- mMARCO dataset uses machine-translated queries, introducing uncertainty about whether improvements stem from expansion or translation artifacts

## Confidence

- **High Confidence:** Core finding that zero-shot prompting is optimal for short queries while few-shot excels for longer queries
- **Medium Confidence:** Cross-lingual expansion yields largest relative improvements for languages with weakest baselines
- **Medium Confidence:** Format-mismatch penalty in fine-tuning is demonstrated but may be confounded by dataset domain differences

## Next Checks

1. **Dense Retriever Validation:** Replicate the length-dependent prompting experiments using a multilingual dense retriever (e.g., BGE-M3 or Cohere) to determine if the observed patterns generalize beyond BM25.

2. **Mixed-Format Fine-Tuning:** Fine-tune Aya Expanse 8B on a combined dataset of CLIRMatrix titles and mMARCO questions to test whether format mixing mitigates the observed transfer losses, and evaluate on a held-out test set with mixed query formats.

3. **Document-Side Expansion:** Implement and evaluate document-side query expansion (generating pseudo-documents for relevant documents in the target language) to assess whether this addresses the persistent retrieval difficulties into non-Latin script languages.