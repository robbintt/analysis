---
ver: rpa2
title: 'GeoThought: A Dataset for Enhancing Mathematical Geometry Reasoning in Vision-Language
  Models'
arxiv_id: '2510.21881'
source_url: https://arxiv.org/abs/2510.21881
tags:
- reasoning
- geometric
- dataset
- angle
- triangle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces GeoThought, a dataset for enhancing geometric
  reasoning in multimodal models, addressing the challenge of limited high-quality
  training data for complex geometric problem-solving. The authors construct two datasets:
  Geo-Thought-6K and Geo-Thought-Augmented-10K, featuring visual descriptions, step-by-step
  solutions, explicit reasoning chains, reflection steps, and final answers.'
---

# GeoThought: A Dataset for Enhancing Mathematical Geometry Reasoning in Vision-Language Models

## Quick Facts
- **arXiv ID:** 2510.21881
- **Source URL:** https://arxiv.org/abs/2510.21881
- **Reference count:** 40
- **Primary result:** 73.21% accuracy on GeoQA benchmarks, 22.02% gain over base model

## Executive Summary
GeoThought addresses the challenge of limited high-quality training data for geometric reasoning in multimodal models. The authors construct two datasets: Geo-Thought-6K (6,243 samples) and Geo-Thought-Augmented-10K (10,834 samples), featuring visual descriptions, step-by-step solutions, explicit reasoning chains, reflection steps, and final answers. Using this data, they train GeoThought-MLLM, achieving substantial improvements: 73.21% accuracy on GeoQA benchmarks, a 22.02% gain over the base model. The model reaches approximately 94% of the performance of larger closed-source models, demonstrating strong generalization across in-domain and out-of-domain geometric reasoning tasks.

## Method Summary
The authors first generate Geo-Thought-6K by filtering seed questions from GEOQA R1V Train 8K through a teacher model (Doubao-1.5-thinking-vision-pro) using Chain-of-Thought reasoning. They then augment this dataset by generating 5 additional questions per image, querying the teacher model 8 times per question, and filtering for 8/8 answer consensus, yielding 4,591 high-quality samples. The combined Geo-Thought-Augmented-10K (10,834 samples) is used to fine-tune Qwen2.5-VL and InternVL3 models using SFT (3 epochs, lr=1e-5). The training uses AdamW optimizer with specified hyperparameters, and evaluation is performed on GeoQA (754 test cases) and Geometry3K (601 test cases) benchmarks.

## Key Results
- **Accuracy gains:** 73.21% accuracy on GeoQA benchmarks, 22.02% improvement over base model
- **Generalization:** Model achieves 67.10% on Geometry3K (out-of-domain), demonstrating strong transfer
- **SOTA comparison:** Reaches approximately 94% of larger closed-source model performance
- **Ablation insights:** SFT alone outperforms GRPO and SFT+GRPO strategies, contrary to text-only reasoning trends

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicit Chain-of-Thought annotations with reflection steps enable models to learn decomposed geometric reasoning strategies that transfer to out-of-domain problems.
- **Mechanism:** Training data structures reasoning as: visual description → step-by-step solution → reasoning chain → reflection → final answer, using `<think/>` and `<answer/>` tags following DeepSeek-R1 schema. Models learn to replicate structured verification, not just input-output mappings.
- **Core assumption:** Models can internalize the self-correction patterns (e.g., "Wait, let me check that again") present in teacher-generated traces.
- **Evidence anchors:**
  - [abstract]: "Each entry includes visual descriptions, step-by-step solutions, explicit reasoning chains, reflection steps, and final answers"
  - [section 4.5, Table 8]: CoT data achieves 67.1% vs 28.92% without CoT on Qwen2.5-VL-3B
  - [corpus]: Related work R-CoT [8] and Visual-CoT [23] confirm multimodal CoT benefits, but corpus lacks replication of this specific reflection-annotation finding
- **Break condition:** When spatial misjudgment occurs (87.5% of errors per Section 6), CoT alone cannot correct visual perception failures—requires visual re-injection.

### Mechanism 2
- **Claim:** High-consensus filtering (8/8 answer agreement) removes noisy reasoning traces that degrade model robustness.
- **Mechanism:** Query teacher model 8 times per generated question; retain only samples achieving unanimous agreement. This filters out low-confidence reasoning and random guessing behavior.
- **Core assumption:** Consensus correlates with reasoning quality and correctness.
- **Evidence anchors:**
  - [section 3.1]: "We then extracted answers from each response and computed their frequency distribution...ultimately yielding 4,591 additional high-quality samples"
  - [section 4.5, Table 9]: Filtered data improves accuracy by +5.00% while unfiltered gives only +0.03%
  - [corpus]: Self-consistency approaches (Wang et al. [29]) support consensus-based filtering in text-only settings; this paper extends it to multimodal geometry
- **Break condition:** When teacher model has systematic bias shared across all 8 samples, consensus doesn't guarantee correctness.

### Mechanism 3
- **Claim:** Teacher models with frequent self-verification produce training data that teaches error-correction patterns.
- **Mechanism:** Doubao-1.5-thinking-vision-pro performs step-by-step verification ("Wait, actually, let me confirm") and comprehensive secondary review, generating traces with built-in correction loops.
- **Core assumption:** Student models learn to replicate the verification behavior, not just final answers.
- **Evidence anchors:**
  - [section A.1]: "Doubao-1.5-thinking-vision-pro demonstrates a frequent reflection mechanism...conducting a comprehensive secondary review of all steps upon completing the chain of thought"
  - [section A.1, Table 12]: Shows verification steps like "Wait, let me make sure about the angles" embedded in reasoning
  - [corpus]: Weak evidence—corpus mentions reflection in reasoning models broadly, but no direct comparison of reflection frequency across teacher models for geometry
- **Break condition:** Reflection adds inference overhead; real-time applications may require distillation or shorter traces.

## Foundational Learning

- **Concept: Visual Forgetting in Long-Chain Reasoning**
  - Why needed here: Paper identifies 87.5% of errors stem from spatial misjudgment during extended reasoning, where attention to visual input degrades over steps.
  - Quick check question: In a 50-step geometry proof, at which step would you expect visual attention to be weakest, and what intervention would re-engage it?

- **Concept: Consensus Filtering as Quality Proxy**
  - Why needed here: GeoThought uses 8/8 answer agreement to approximate reasoning quality without manual annotation.
  - Quick check question: If a teacher model achieves 6/8 consensus, what specific risks arise from including this sample in training data?

- **Concept: SFT vs. Reinforcement Learning Trade-offs**
  - Why needed here: Ablation shows SFT alone outperforms GRPO and SFT+GRPO, contradicting trends in text-only reasoning.
  - Quick check question: Why might SFT+GRPO underperform SFT alone when RL data consists of rejected samples from the initial filtering stage?

## Architecture Onboarding

- **Component map:**
  GEOQA R1V Train 8K (seed questions/images) → Doubao-1.5-thinking-vision-pro → 8 CoT traces per question → Filter: Keep only 8/8 consensus (yields 4,591 augmented samples) → Combine with original 6,243 → Geo-Thought-Augmented-10K → SFT on Qwen2.5-VL / InternVL3 (AdamW, 3 epochs, lr=1e-5) → Inference: vllm, greedy decoding, max 2048 tokens

- **Critical path:**
  1. Teacher model selection—Doubao chosen over GLM-Zero-Preview due to higher reflection frequency (Section A.1)
  2. Filtering threshold—8/8 consensus yields +5.00% gain; 7/8 consensus (10.2% of samples) is discarded
  3. Data format—Open-ended questions avoid random guessing noise present in multiple-choice format (Appendix A.1)

- **Design tradeoffs:**
  - Quality vs. quantity: Filtered 10,834 samples outperform unfiltered 13,688 samples
  - Training method: SFT alone outperforms GRPO (-9.73%) and SFT+GRPO (-0.50%) on Qwen2.5-VL-7B
  - Teacher model cost: 8-sample consensus requires ~8× API calls; avg reasoning chain length 1299 words

- **Failure signatures:**
  - Spatial misjudgment: Model misidentifies consecutive interior angles as corresponding angles (Table 5)
  - Positional error: Model locates angle "above" line when it should be "below" (Table 6)
  - Visual forgetting: Attention to diagram degrades during long reasoning chains without re-injection

- **First 3 experiments:**
  1. Test visual re-injection strategies (e.g., multi-turn interaction or self-reflection checkpoints) on the 26.79% of remaining errors to validate if "looking again" corrects spatial misjudgment
  2. Ablate consensus threshold (6/8 vs 7/8 vs 8/8) to quantify the quality-quantity tradeoff curve
  3. Compare SFT vs SFT+GRPO with higher-quality RL data (not filtered-out samples) to determine if RL can recover with better data selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms can effectively mitigate "visual forgetting" during long-chain geometric reasoning in MLLMs?
- Basis in paper: [explicit] The authors identify visual forgetting as a key problem and state: "These strategies represent promising directions for future research and are key focuses of our ongoing work" (referencing multi-turn visual reinjection and agent-based approaches).
- Why unresolved: The paper identifies the problem and references existing approaches but does not experimentally validate any solution within their framework.
- What evidence would resolve it: Ablation experiments comparing models with and without visual reinjection mechanisms on long-chain reasoning tasks, measuring accuracy retention across reasoning steps.

### Open Question 2
- Question: Under what conditions can reinforcement learning (GRPO) improve geometric reasoning, given that both GRPO-only and SFT+GRPO underperformed SFT alone?
- Basis in paper: [explicit] "Neither RL-based approach outperformed the use of SFT alone... The results indicated that neither RL-based approach outperformed the use of SFT alone with the Geo-Thought-6K dataset."
- Why unresolved: The authors hypothesize two causes (data quality/quantity and constrained exploration space) but do not experimentally validate which factor dominates or how to address it.
- What evidence would resolve it: Controlled experiments varying RL data quality, quantity, and exploration parameters (e.g., KL penalty coefficients) to identify conditions where RL provides gains.

### Open Question 3
- Question: How can models be trained or architected to accurately judge spatial relationships among geometric elements during reasoning, given that 87.5% of errors stem from spatial misjudgment?
- Basis in paper: [explicit] "Enabling models to accurately judge spatial relationships among geometric elements during reasoning will be a key direction for our future research."
- Why unresolved: The paper demonstrates that correcting spatial errors in CoT leads to correct answers, but does not propose methods to prevent these errors during training or inference.
- What evidence would resolve it: Architectural interventions (e.g., spatial attention modules) or training strategies specifically targeting spatial relationship accuracy, evaluated on a curated set of spatial-judgment-heavy problems.

## Limitations
- **Visual Forgetting Problem:** The paper identifies 87.5% of errors as spatial misjudgment during extended reasoning chains but does not provide solutions beyond acknowledging the issue.
- **Teacher Model Dependency:** GeoThought relies on Doubao-1.5-thinking-vision-pro for CoT generation with no validation that other strong models would produce equivalent quality annotations.
- **RL Performance Gap:** SFT-only training significantly outperforms SFT+GRPO and GRPO-only approaches, contradicting trends in text-only reasoning without clear explanation.

## Confidence
- **High Confidence:** Dataset construction methodology, training hyperparameters, and final benchmark results are clearly specified and verifiable
- **Medium Confidence:** The claim that 94% of closed-source model performance is achieved is reasonable given the SOTA result, but depends on the specific closed-source model used as reference
- **Low Confidence:** The mechanism explanation for why reflection steps improve performance lacks direct comparative evidence across teacher models

## Next Checks
1. **Visual Re-injection Validation:** Test multi-turn interaction or self-reflection checkpoints on the 26.79% of remaining errors to determine if "looking again" at diagrams corrects spatial misjudgment errors.
2. **Consensus Threshold Ablation:** Systematically evaluate 6/8 vs 7/8 vs 8/8 consensus thresholds to quantify the quality-quantity tradeoff curve and identify optimal filtering for different downstream tasks.
3. **RL Data Quality Experiment:** Compare SFT vs SFT+GRPO using higher-quality RL data (not the filtered-out samples) to determine if RL can recover performance gains when trained on better data selection.