---
ver: rpa2
title: Accurate and Consistent Graph Model Generation from Text with Large Language
  Models
arxiv_id: '2508.00255'
source_url: https://arxiv.org/abs/2508.00255
tags:
- abscon
- graph
- llms
- generation
- constraints
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AbsCon addresses the problem of generating consistent and accurate
  graph models from natural language descriptions using large language models (LLMs),
  which often produce partially correct outputs due to syntax violations, constraint
  inconsistencies, and inaccuracies. The proposed abstraction-concretization framework
  combines multiple LLM-generated candidates by first constructing a probabilistic
  partial model through element matching and similarity scoring, then refining it
  into a final consistent model via constraint optimization.
---

# Accurate and Consistent Graph Model Generation from Text with Large Language Models

## Quick Facts
- arXiv ID: 2508.00255
- Source URL: https://arxiv.org/abs/2508.00255
- Reference count: 40
- Key outcome: Framework achieves perfect consistency in 6 of 12 cases and improves F1-scores by 0.78% to 27% compared to direct generation

## Executive Summary
This paper addresses the challenge of generating accurate and consistent graph models from natural language descriptions using large language models (LLMs). LLMs often produce outputs with syntax violations, constraint inconsistencies, and inaccuracies when generating structured graphs. The proposed AbsCon framework combines multiple LLM-generated candidates by first constructing a probabilistic partial model through element matching and similarity scoring, then refining it into a final consistent model via constraint optimization. Evaluated across flowcharts, taxonomies, and executable programs using both open-source and closed-source LLMs, AbsCon significantly improves consistency and model quality, with particularly pronounced benefits for smaller LLMs through test-time compute.

## Method Summary
The AbsCon framework generates graph models through a two-phase process: abstraction and concretization. In the abstraction phase, multiple LLM candidates (typically 10) are generated using few-shot chain-of-thought prompting with temperature=0.7, then merged into a probabilistic partial model using graph matching and element frequency scoring. In the concretization phase, a constraint optimization problem is formulated to select the highest-probability valid subgraph that satisfies domain-specific metamodel constraints. The framework uses embeddings (MiniLM) for node similarity matching, incremental graph matching algorithms, and a CBC solver for constraint satisfaction. The approach is evaluated on three datasets: PAGED flowcharts, WordNet taxonomies, and Clevr executable programs, measuring consistency, quality (precision/recall/F1), and execution accuracy.

## Key Results
- Achieved perfect consistency in 6 out of 12 experimental cases
- Improved F1-scores by 0.78% to 27% compared to direct generation baselines
- Demonstrated that smaller LLMs can outperform larger models when using AbsCon framework
- Only 5-8 candidates needed for substantial improvements, balancing quality with computational overhead

## Why This Works (Mechanism)

### Mechanism 1: Uncertainty Quantification via Probabilistic Aggregation
Aggregating multiple LLM outputs into a probabilistic partial model filters out inconsistent hallucinations while retaining correct elements. The framework generates n candidate graphs and merges them using graph matching based on label similarity, assigning probabilities to each element based on frequency across candidates. This transforms discrete, noisy outputs into a continuous likelihood landscape where high-frequency elements represent consensus reality. The self-consistency hypothesis suggests correct graph elements appear more frequently across diverse LLM samples than specific hallucinations.

### Mechanism 2: Constraint-Guided Concretization via Optimization
The framework formulates model generation as a constraint optimization problem, defining binary decision variables for each element in the partial model and maximizing a joint probability objective subject to hard constraints. The solver selects the highest-probability valid subgraph, guaranteeing structural validity without manual error correction. Well-formedness constraints are expressed as logical formulas compatible with standard solvers, ensuring the final output satisfies all domain-specific rules.

### Mechanism 3: Amplification of Smaller Models via Test-Time Compute
AbsCon enables smaller, cheaper LLMs to outperform larger models by spending compute at inference time rather than training. By sampling multiple outputs from a smaller model and synthesizing them, the framework explores the solution space more thoroughly than a single greedy pass from a larger model. This "test-time compute" compensates for the smaller model's lower per-sample accuracy, allowing it to generate the correct elements partially even if it fails to assemble them perfectly in a single pass.

## Foundational Learning

- **Graph Edit Distance / Graph Matching**: Essential for the Abstraction phase to identify that "Node A" in candidate 1 represents the same entity as "Activity A" in candidate 2. Quick check: How would you align two graphs where nodes have similar semantic embeddings but different structural connections?

- **Constraint Satisfaction Problems (CSP) / ILP**: Essential for the Concretization phase to select the final graph by translating structural rules into mathematical constraints for a solver. Quick check: Can you formulate a binary integer programming problem to select edges such that a graph has no cycles?

- **Self-Consistency in LLMs**: The theoretical underpinning of the paper, relying on the distribution of outputs rather than a single token prediction. Quick check: Why does majority voting fail for structured outputs like graphs compared to simple text answers?

## Architecture Onboarding

- **Component map**: Generator -> Parser -> Abstraction Engine (Similarity -> Matcher -> Builder) -> Concretization Engine (Translator -> Optimizer)
- **Critical path**: The Constraint Formulation - the system's guarantee of consistency relies entirely on correct translation of domain rules into the solver's logic
- **Design tradeoffs**: Candidate count (5-8 suggested), temperature settings (needs to be high enough for diversity but not so high that syntax errors spike), computational overhead vs. quality gains
- **Failure signatures**: "No Feasible Solution" when partial model contains conflicting high-probability elements, low recall/high precision when solver prunes too many elements
- **First 3 experiments**: 1) Baseline consistency check with direct prompting, 2) Matching validation on similar descriptions, 3) Solver stress test with conflicting high-probability edges

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's effectiveness depends on the untested assumption that correct graph elements appear more frequently across diverse LLM samples than hallucinations
- Constraint formulation completeness and correctness for complex domain-specific rules remains unverified, with only one specific example demonstrated
- Scalability claim that only "a few candidates" are needed lacks systematic exploration of candidate count tradeoffs

## Confidence
- **High confidence**: Experimental results showing improved consistency and F1-scores
- **Medium confidence**: Generalizability of test-time compute benefits for smaller models (limited to three specific model pairs)
- **Low confidence**: Scalability claim regarding candidate count requirements

## Next Checks
1. Validate the consensus assumption by generating 100 flowcharts from the same prompt and measuring whether correct elements appear more frequently than incorrect ones across samples
2. Stress test constraint formulation by creating adversarial partial models where highest-probability elements violate constraints, then verifying solver behavior
3. Quantify candidate count tradeoff by systematically varying candidate count (n=2, 5, 10, 20) and measuring consistency gains against computational overhead