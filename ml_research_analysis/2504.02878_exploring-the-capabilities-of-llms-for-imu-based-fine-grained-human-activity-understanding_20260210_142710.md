---
ver: rpa2
title: Exploring the Capabilities of LLMs for IMU-based Fine-grained Human Activity
  Understanding
arxiv_id: '2504.02878'
source_url: https://arxiv.org/abs/2504.02878
tags:
- data
- llms
- letter
- recognition
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper demonstrates that pretrained large language models\
  \ (LLMs) struggle with fine-grained IMU-based human activity recognition, such as\
  \ air-written letter recognition, performing near-random guessing. The authors show\
  \ that fine-tuning LLMs with a self-collected dataset enables substantial performance\
  \ gains, achieving up to a 129\xD7 improvement on 2D IMU data."
---

# Exploring the Capabilities of LLMs for IMU-based Fine-grained Human Activity Understanding

## Quick Facts
- arXiv ID: 2504.02878
- Source URL: https://arxiv.org/abs/2504.02878
- Reference count: 36
- Primary result: LLMs achieve 78% word accuracy (≤5 letters) after fine-tuning on IMU data, a 129× improvement over pretrained models

## Executive Summary
This paper investigates whether pretrained large language models can recognize fine-grained human activities from IMU data, specifically air-written letters. The authors demonstrate that pretrained models perform near-random guessing on raw IMU time-series, but substantial performance gains (129× improvement) are achieved through fine-tuning with instruction-response pairs. To handle 3D mid-air writing, they develop a 3D-to-2D mapping pipeline using deep metric learning, enabling 78% word recognition accuracy for sequences up to 5 letters. The work establishes LLMs as viable tools for fine-grained HAR tasks when properly adapted to sensor data.

## Method Summary
The authors collected IMU data from two participants air-writing 26 letters in both 2D (flat surface) and 3D (mid-air) conditions, with 10 repetitions each. They generated 1,560 instruction-response pairs by prompting an LLM to explain why IMU data maps to specific letters, then fine-tuned LLaMA-3-8B and GPT-4o using LoRA. For 3D data, they implemented a 1D-CNN encoder trained with triplet loss to map 3D embeddings to their corresponding 2D gallery samples. Word recognition aggregates k samples per letter, using the LLM's language model to correct individual letter errors through semantic context.

## Key Results
- Pretrained LLMs achieve near-random guessing (<5% accuracy) on IMU data without fine-tuning
- Fine-tuned models show 129× improvement on 2D IMU data compared to pretrained versions
- 3D-to-2D mapping achieves 93.08% accuracy using triplet loss and gallery retrieval
- Word recognition reaches 78% accuracy for words up to 5 letters
- Accuracy degrades significantly for longer words (>5 letters) due to accumulated prediction errors

## Why This Works (Mechanism)

### Mechanism 1: Instruction-Tuning for Signal-to-Concept Alignment
Fine-tuning enables LLMs to interpret raw IMU time-series by bridging the semantic gap between numerical sensor readings and conceptual labels. The paper converts raw IMU vectors into instruction-answer pairs with generated reasoning chains linking signal changes to letter geometry. This trains the model to recognize signal-text correlations it missed in pre-training. Evidence shows a 129× accuracy improvement after fine-tuning, while pretrained models score near zero.

### Mechanism 2: Dimensional Mapping via Deep Metric Learning
A 1D-CNN encoder trained using triplet loss maps complex 3D mid-air data to 2D equivalents, making gesture recognition tractable. The framework learns to project 3D data onto 2D data representing the same letter, achieving 93.08% mapping accuracy. This forces 3D embeddings to align with 2D embeddings of the same letter class.

### Mechanism 3: Contextual Series Aggregation
LLMs correct isolated letter prediction errors by leveraging word formation context. Instead of single-letter classification, the system aggregates multiple samples per letter in a word. The LLM uses its internal language model to infer the most likely word from the sequence of noisy predictions, achieving 78% accuracy for words up to 5 letters.

## Foundational Learning

- **Concept: IMU Data as Pseudo-Text**
  - Why needed: Standard LLMs cannot process binary sensor streams; time-series accelerometer data must be treated as text strings
  - Quick check: How does reducing decimal precision or sampling rate affect token count and model performance?

- **Concept: Deep Metric Learning (Triplet Loss)**
  - Why needed: Bridges the 3D-to-2D gap by minimizing distance between similar classes while maximizing distance to dissimilar classes
  - Quick check: What defines a "Positive" pair in this paper's 3D-to-2D mapping architecture?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed: Enables efficient fine-tuning by freezing pre-trained weights and injecting trainable rank decomposition matrices
  - Quick check: Does LoRA modify the pre-trained weights of the LLM directly? (No)

## Architecture Onboarding

- **Component map:** Raw IMU Data -> 3D-to-2D Mapping Encoder -> 2D Proxy Data -> Fine-tuned LLM -> Word Hypothesis
- **Critical path:** Raw 3D Data -> Mapping Encoder (Critical Bottleneck) -> 2D Proxy Data -> Fine-tuned LLM -> Word Hypothesis
- **Design tradeoffs:** Precision vs. Tokens (3 decimal points create >8k tokens per prompt), Generalization vs. Overfitting (trained on one person, tested on another)
- **Failure signatures:** Random Guessing in zero-shot models, Accumulated Error for words >5 letters, 3D Confusion without explicit mapping layer
- **First 3 experiments:** 1) Sanity Check: Test base LLM on raw IMU CSV data to verify near-random guessing baseline, 2) Mapping Validation: Train 1D-CNN Triplet network and visualize embeddings, 3) Token Limit Test: Vary decimal precision and sampling rate to measure impact on accuracy vs. token count

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal trade-off between IMU sampling rates and LLM recognition performance for fine-grained HAR?
- Basis: Authors note original sampling rates create high token overhead (>8k tokens) and suggest exploring sampling rate vs. effectiveness relationship
- Evidence needed: Ablation studies measuring accuracy and latency across varying IMU sampling rates

### Open Question 2
- Question: Can the 3D-to-2D mapping pipeline generalize to non-letter fine-grained activities?
- Basis: Paper suggests extending beyond letter recognition to arm gesture recognition and daily activity monitoring
- Evidence needed: Successful application to diverse datasets like arm gestures or daily monitoring logs

### Open Question 3
- Question: Can contextual self-correction enable unsupervised continuous learning?
- Basis: Model's ability to correct letter predictions using word context could facilitate continuous learning without human labeling
- Evidence needed: System demonstration where contextual predictions are used as pseudo-labels to improve performance over time

## Limitations
- Dataset extremely small (1,040 samples across 2 participants) limiting generalizability
- All IMU data collected using single device (Intel RealSense D435i with BMI055 IMU), raising cross-device concerns
- 3D-to-2D mapping relies on gallery retrieval which may not capture full 3D writing variability
- Approach requires fine-tuning per user for optimal performance, limiting practical deployment

## Confidence

**Major Limitations:**
- Dataset extremely small (1,040 samples across 2 participants), severely limiting generalizability despite participant-split evaluation
- All IMU data collected using single device (Intel RealSense D435i with BMI055 IMU), raising cross-device sensor variability concerns
- 3D-to-2D mapping relies on gallery retrieval, may not capture full 3D writing style variability
- Approach requires fine-tuning per user for optimal performance, limiting practical deployment

**Confidence Labels:**
- High Confidence: Pretrained LLMs perform near-random on IMU data without fine-tuning (well-supported by baseline experiments)
- Medium Confidence: 129× improvement claim is specific to their dataset and methodology; external validation would strengthen
- Medium Confidence: 3D-to-2D mapping mechanism works as described (93% accuracy reported), but representational alignment assumption needs broader testing
- Low Confidence: Word-level accuracy (78%) may not scale well beyond 5 letters or to different vocabularies

## Next Checks
1. Cross-device validation: Test fine-tuned model on IMU data from different sensors (different sampling rates, noise characteristics) to assess sensor dependency
2. Scaling analysis: Evaluate word recognition accuracy for word lengths beyond 5 letters and with diverse vocabulary sets to identify true upper bounds
3. User-independent fine-tuning: Fine-tune on data from multiple users simultaneously rather than one user, then test on completely unseen participants to assess real-world generalization