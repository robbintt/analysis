---
ver: rpa2
title: 'DLM-One: Diffusion Language Models for One-Step Sequence Generation'
arxiv_id: '2506.00290'
source_url: https://arxiv.org/abs/2506.00290
tags:
- diffusion
- generation
- arxiv
- score
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DLM-One, a score distillation framework enabling
  one-step sequence generation with continuous diffusion language models. By aligning
  the student model's output scores with a pretrained teacher's score function in
  the embedding space, DLM-One eliminates iterative refinement steps.
---

# DLM-One: Diffusion Language Models for One-Step Sequence Generation

## Quick Facts
- arXiv ID: 2506.00290
- Source URL: https://arxiv.org/abs/2506.00290
- Authors: Tianqi Chen; Shujian Zhang; Mingyuan Zhou
- Reference count: 27
- One-line primary result: One-step diffusion language models achieve competitive quality metrics while providing ~500× speedup over iterative approaches

## Executive Summary
DLM-One introduces a novel score distillation framework that enables diffusion language models to generate sequences in a single step rather than through iterative refinement. By aligning a student model's output scores with a pretrained teacher's score function in continuous embedding space, the method eliminates the computational overhead of multiple diffusion steps while maintaining generation quality. The approach employs a two-stage training process with adversarial regularization to stabilize training and prevent degeneration, making one-step diffusion language modeling both practical and effective for sequence generation tasks.

## Method Summary
DLM-One operates by training a student diffusion model to mimic the score function of a pretrained teacher model within the continuous embedding space of sequences. The framework first pretrains the student to approximate the teacher's score function, then refines it through adversarial training that ensures the student's generated scores align with the teacher's while maintaining diversity. This score distillation approach allows the student to generate high-quality sequences in a single forward pass, bypassing the iterative denoising process typical of diffusion models. The method leverages continuous representations to enable direct optimization, making one-step generation computationally feasible while preserving the theoretical benefits of diffusion-based modeling.

## Key Results
- Achieves up to ~500× speedup compared to DiffuSeq while maintaining competitive BLEU, ROUGE-L, and BERTScore performance
- Demonstrates one-step generation capability on benchmark Seq2Seq tasks including question generation, text simplification, and paraphrase
- Shows that one-step diffusion language models can match teacher quality at drastically reduced computational cost

## Why This Works (Mechanism)
DLM-One works by exploiting the fact that diffusion models can be distilled through score function alignment in continuous embedding space. The student model learns to approximate the teacher's score function, which captures the gradient of log-probability with respect to the input. By aligning these score functions, the student can generate samples that follow the same distribution as the teacher's outputs but without requiring iterative refinement. The adversarial training component ensures that the student not only matches the teacher's scores but also maintains diversity in generation, preventing mode collapse that often occurs in distillation approaches.

## Foundational Learning
- **Diffusion probabilistic models**: Why needed - form the theoretical foundation for modeling sequential data; Quick check - understand forward and reverse diffusion processes
- **Score matching and score distillation**: Why needed - enables knowledge transfer from teacher to student without explicit likelihood estimation; Quick check - verify score function alignment through gradient analysis
- **Continuous representation learning**: Why needed - allows direct optimization in embedding space rather than discrete sequence space; Quick check - validate embedding space preserves semantic relationships
- **Adversarial regularization**: Why needed - prevents degeneration and ensures diversity in distilled models; Quick check - monitor generated sequence diversity metrics
- **Two-stage training**: Why needed - stabilizes learning and prevents catastrophic forgetting; Quick check - compare performance with single-stage training
- **Sequence embedding alignment**: Why needed - enables meaningful score comparison between teacher and student; Quick check - measure embedding space consistency

## Architecture Onboarding

**Component map**: Pretrained teacher model -> Embedding extractor -> Student model -> Score function -> Adversarial discriminator -> Quality metrics

**Critical path**: Teacher score function (in embedding space) → Student score function alignment → One-step generation → Quality evaluation

**Design tradeoffs**: The method trades iterative refinement steps for a more complex training process involving score distillation and adversarial regularization. This creates a one-time training cost for perpetual inference efficiency gains. The continuous embedding space alignment introduces approximation errors but enables direct optimization, while the adversarial component adds training stability at the cost of additional hyperparameters.

**Failure signatures**: Mode collapse in generated sequences, significant quality degradation compared to teacher model, training instability with exploding or vanishing gradients, poor alignment between student and teacher score functions in embedding space.

**First experiments**: (1) Verify teacher score function behavior in embedding space through gradient visualization; (2) Test student-teacher score alignment on a small validation set before full training; (3) Conduct ablation study removing adversarial regularization to measure its impact on diversity and stability.

## Open Questions the Paper Calls Out
None

## Limitations
- The two-stage training process with adversarial regularization may introduce complexity in hyperparameter tuning and stability across different sequence generation tasks
- The method's reliance on continuous embedding space alignment could face challenges when scaling to extremely long sequences or handling rare vocabulary items
- Evaluation focuses primarily on Seq2Seq tasks, leaving open questions about performance on other generation paradigms like open-ended text generation or structured output tasks

## Confidence
- **High**: One-step generation capability and efficiency gains are directly demonstrated through experimental results
- **Medium**: Quality maintenance claims show competitive performance but margins of difference need further exploration
- **Medium**: Scalability and robustness claims due to limited testing across diverse tasks and sequence lengths

## Next Checks
- Benchmark against other one-step diffusion approaches beyond DiffuSeq to establish relative performance in the broader landscape
- Conduct ablation studies to quantify the contribution of each component in the two-stage training process, particularly the adversarial regularization
- Test on extended sequence lengths and low-resource vocabulary scenarios to assess robustness beyond the current evaluation scope