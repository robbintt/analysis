---
ver: rpa2
title: Provably Efficient Sample Complexity for Robust CMDP
arxiv_id: '2511.07486'
source_url: https://arxiv.org/abs/2511.07486
tags:
- robust
- function
- value
- policy
- augmented
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses robust constrained Markov decision processes
  (RCMDPs), where the goal is to learn policies that maximize reward while satisfying
  safety constraints under model uncertainty. Unlike unconstrained robust MDPs, the
  authors show that Markovian policies can be suboptimal for RCMDPs even with rectangular
  uncertainty sets, due to the coupling between reward maximization and constraint
  satisfaction.
---

# Provably Efficient Sample Complexity for Robust CMDP

## Quick Facts
- **arXiv ID:** 2511.07486
- **Source URL:** https://arxiv.org/abs/2511.07486
- **Authors:** Sourav Ganguly; Arnob Ghosh
- **Reference count:** 40
- **Primary result:** Achieves sample complexity of Õ(|S||A|H⁵/ε²) for RCMDPs with TV, χ², KL uncertainty sets

## Executive Summary
This paper addresses robust constrained Markov decision processes (RCMDPs), where the goal is to learn policies that maximize reward while satisfying safety constraints under model uncertainty. Unlike unconstrained robust MDPs, the authors show that Markovian policies can be suboptimal for RCMDPs even with rectangular uncertainty sets, due to the coupling between reward maximization and constraint satisfaction. To overcome this, they propose augmenting the state space with the remaining utility budget and introduce a Robust Constrained Value Iteration (RCVI) algorithm. RCVI solves a per-step constrained linear program in the augmented space to find policies that ensure both sub-optimality and constraint violation are bounded by ε. The algorithm achieves a sample complexity of Õ(|S||A|H⁵/ε²) for popular uncertainty sets (TV-distance, χ²-distance, KL-divergence), which is the first such guarantee for RCMDPs without requiring a policy optimization oracle. Experiments on benchmark environments validate the effectiveness of the approach, showing faster convergence and better constraint satisfaction compared to existing methods.

## Method Summary
The RCVI algorithm augments the state space with a remaining utility budget (c_h = b - Σg_{h'}) to convert the RCMDP into a Markovian problem over the augmented space (s,c). It uses backward value iteration where at each step h, it computes worst-case Q-values using dual representations of the uncertainty sets, then solves a constrained linear program to find policies that maximize reward while satisfying utility constraints with slackness (H-h+1)ε. The algorithm requires a generative model to sample transitions and achieves sample complexity of Õ(|S||A|H⁵/ε²) for TV, χ², and KL divergence uncertainty sets.

## Key Results
- Proves Markovian policies are insufficient for RCMDPs even with rectangular uncertainty sets
- Achieves first sample complexity guarantees for RCMDPs without policy optimization oracle
- Sample complexity of Õ(|S||A|H⁵/ε²) for TV, χ², and KL divergence uncertainty sets
- Experimental validation shows faster convergence and better constraint satisfaction than RNPG baseline

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Standard Markovian policies are insufficient for Robust Constrained MDPs (RCMDPs) with rectangular uncertainty sets; policies must be history-dependent regarding utility consumption.
- **Mechanism:** The paper demonstrates that because the worst-case model depends on the policy, the coupling between reward and constraint optimization breaks the optimality of Markovian policies. The authors fix this by augmenting the state space with a "remaining utility budget" (c_h = b - Σg_{h'}), converting the problem into a Markovian one over the augmented space (s, c).
- **Core assumption:** The utility function g is deterministic (or its distribution is known), allowing the budget c to be tracked deterministically alongside the stochastic environment state.
- **Evidence anchors:** [abstract]: "...show that Markovian policies can be suboptimal for RCMDPs... To address this, we introduce a augmented state space..."; [Lemma 1]: Proves via counter-example that Markovian policies may fail; [Theorem 1]: Proves sufficiency of Markovian policies in the augmented space.
- **Break condition:** If the utility g is stochastic and its distribution is unknown, the state augmentation cannot be constructed deterministically, breaking the mechanism.

### Mechanism 2
- **Claim:** Value Iteration can be adapted to constrained robust settings by solving a linear program (LP) at each step rather than a simple max operation.
- **Mechanism:** The Robust Constrained Value Iteration (RCVI) performs backward induction. At step h, instead of max Q, it solves max_π ⟨π, Q_r⟩ subject to a constraint on the utility ⟨π, Q_g⟩ ≥ -(H-h+1)ε. Crucially, it introduces slackness (H-h+1)ε in the constraints to account for estimation errors in the empirical model.
- **Core assumption:** A generative model (simulator) is available to sample transitions for any state-action pair to build the empirical model P̂.
- **Evidence anchors:** [Algorithm 1]: Defines the LP solved at every step; [Eq (12)]: Shows the modified optimization problem with slackness; [Section 4]: "...we have added slackness... to address the finite sample estimation error..."
- **Break condition:** If the uncertainty set is non-rectangular, the "robust Bellman backup" used in the LP formulation fails to decompose, making the LP solve intractable.

### Mechanism 3
- **Claim:** Sample complexity scales with Õ(|S||A|H⁵/ε²) for specific divergence-based uncertainty sets.
- **Mechanism:** The analysis relies on the dual representation of the robust Bellman operator. For specific distance metrics (TV, χ², KL), the infimum over the uncertainty set L_P V can be converted into a convex optimization problem. This allows the authors to bound the difference between the empirical and true worst-case value functions using concentration inequalities (specifically, covering the space of value functions).
- **Core assumption:** The uncertainty set P is defined by f-divergence metrics (Total Variation, Chi-squared, KL) and satisfies (s,a)-rectangularity.
- **Evidence anchors:** [Theorem 2, 3, 4]: State the specific sample complexity bounds for different divergences; [Section 5.2]: "...one can again achieve the worst-case empirical value function using the dual representation..."; [corpus]: "Near-Optimal Sample Complexity for Iterated CVaR..." discusses similar dual representation techniques for risk-sensitive RL.
- **Break condition:** If the uncertainty set is defined by a metric without a tractable dual form (e.g., Wasserstein distance without further assumptions), the sample complexity derivation via this dual method fails.

## Foundational Learning

- **Concept: Robust Dynamic Programming (RDP)**
  - **Why needed here:** The algorithm relies on the robust Bellman equation (Eq 8) where standard expectation is replaced by an infimum over the uncertainty set. Understanding how "rectangularity" separates this optimization is required to implement the backup operator.
  - **Quick check question:** Does the robust Bellman operator satisfy the contraction mapping property for the specific uncertainty set used?

- **Concept: Linear Programming (LP) for CMDPs**
  - **Why needed here:** RCVI solves an LP at every step. You must understand how to map the policy optimization problem into a linear objective with linear constraints derived from the Q-functions.
  - **Quick check question:** How does one handle the dual variables (Lagrangian multipliers) if the constraint is infeasible during an intermediate iteration?

- **Concept: Generative Models (Sample Complexity)**
  - **Why needed here:** The theoretical guarantees assume access to a "generative model" (oracle) that provides N samples for any (s,a), rather than online trajectory sampling.
  - **Quick check question:** Why does the sample complexity scale linearly with |S||A| rather than depending on the mixing time of the policy?

## Architecture Onboarding

- **Component map:** Sampler (Generative Model) -> Estimator -> Augmented State Manager -> Robust Bellman Solver -> LP Solver
- **Critical path:** The backward induction loop (Algorithm 1, Line 2). The calculation of Q-values for step h depends entirely on the V-values of step h+1. If the discretization of the utility budget is too coarse, error accumulates rapidly through this path.
- **Design tradeoffs:**
  - Discretization Resolution: A finer grid for the budget c (larger |C|) reduces approximation error (better adherence to constraints) but exponentially increases computational complexity per step.
  - Slackness Parameter: A larger slack ε ensures feasibility (finding a valid policy) with fewer samples but results in a more conservative (potentially sub-optimal) reward.
- **Failure signatures:**
  - Infeasible LP: If the slack is too small or samples are insufficient, the constraint ⟨π, Q_g⟩ ≥ ... may be impossible to satisfy, causing the LP to return "infeasible."
  - Budget Overflow: If utility costs exceed the budget b early in an episode, the state augmentation c may go negative, potentially requiring specialized handling if the "unsafe" region is not strictly penalized.
- **First 3 experiments:**
  1. Sanity Check (Toy MDP): Implement the 4-state example from Figure 1/Lemma 1 to verify that a standard Markovian policy fails while the augmented state policy succeeds.
  2. Hyperparameter Sensitivity: Run RCVI on the Garnet environment (Appendix A.2) varying the discretization bins (|C|) to plot the trade-off between runtime and constraint violation.
  3. Baseline Comparison: Compare RCVI against RNPG (Robust Natural Policy Gradient) specifically on sample efficiency—plotting constraint satisfaction vs. number of samples drawn from the generative model.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the horizon dependence in the sample complexity for Total Variation (TV) distance uncertainty sets be improved from H⁵ to H⁴?
  - **Basis in paper:** [explicit] The authors state, "We have left this for the future to reducing the dependency on H for TV-distance."
  - **Why unresolved:** While the paper matches the best-known unconstrained bounds (H⁵), recent work in unconstrained robust MDPs has achieved H⁴ dependence.
  - **What evidence would resolve it:** A proof or algorithm demonstrating a sample complexity of Õ(|S||A|H⁴/ε²) for RCMDPs with TV-distance.

- **Open Question 2:** Can sample complexity guarantees be achieved for RCMDPs without relying on a generative model (simulator)?
  - **Basis in paper:** [explicit] The paper notes that finding guarantees without a simulator "is still an open question even for the unconstrained robust MDP."
  - **Why unresolved:** The RCVI algorithm relies on the ability to query any state-action pair via a generative model; online exploration under robust constraints adds significant complexity not addressed here.
  - **What evidence would resolve it:** An algorithm with provable sample complexity bounds for RCMDPs using only online trajectories (no simulator).

- **Open Question 3:** How can the theoretical guarantees be extended to large-scale or continuous state spaces using linear function approximation?
  - **Basis in paper:** [explicit] The authors suggest that extending guarantees "beyond finite-state settings using function approximation" is a promising direction based on recent advances in robust linear MDPs.
  - **Why unresolved:** The current sample complexity scales with state cardinality |S|, which is intractable for high-dimensional problems.
  - **What evidence would resolve it:** A modified RCVI algorithm with sample complexity bounds scaling with the feature dimension d rather than |S|.

## Limitations

- The sample complexity guarantees depend critically on the assumption of access to a generative model, which is not realistic for most real-world deployments.
- The discretization error introduced by the finite budget grid (|C| = ⌈2H/ε⌉) could be significant in practice, potentially requiring finer grids than theoretically justified.
- The assumption of known uncertainty radius ρ may be difficult to calibrate in practice.

## Confidence

- **High confidence:** The theoretical framework and sample complexity bounds for specific divergence metrics (TV, χ², KL). The counter-example showing Markovian policies are insufficient is rigorous and well-established.
- **Medium confidence:** The practical effectiveness of the discretization scheme for the budget space. While the theory provides bounds, the gap between theory and practice may be significant.
- **Medium confidence:** The numerical stability of the KL-divergence computations, particularly the closed-form solution involving exponential terms which may suffer from underflow/overflow.

## Next Checks

1. **Discretization sensitivity:** Systematically vary the number of budget discretization bins (e.g., 10, 20, 50, 100) on the Garnet environment and measure the trade-off between computational cost and constraint violation.
2. **Oracle comparison:** Compare RCVI's performance against an oracle with perfect model knowledge on the same RCMDP instances to quantify the sub-optimality bound empirically.
3. **Non-rectangular uncertainty:** Test the algorithm's performance when the uncertainty set is slightly non-rectangular (e.g., correlated across time steps) to assess the robustness of the "rectangularity" assumption.