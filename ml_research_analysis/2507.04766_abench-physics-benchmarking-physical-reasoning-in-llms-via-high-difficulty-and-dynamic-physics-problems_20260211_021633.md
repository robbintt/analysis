---
ver: rpa2
title: 'ABench-Physics: Benchmarking Physical Reasoning in LLMs via High-Difficulty
  and Dynamic Physics Problems'
arxiv_id: '2507.04766'
source_url: https://arxiv.org/abs/2507.04766
tags:
- zhang
- physics
- reasoning
- physical
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ABench-Physics introduces a rigorous benchmark to evaluate LLMs'
  physics reasoning by combining static high-difficulty problems with dynamic variants
  that test generalization. It includes 400 graduate/Olympiad-level static problems
  (Phy A) and 100 parameterized dynamic problems (Phy B) with an automatic variation
  engine.
---

# ABench-Physics: Benchmarking Physical Reasoning in LLMs via High-Difficulty and Dynamic Physics Problems

## Quick Facts
- arXiv ID: 2507.04766
- Source URL: https://arxiv.org/abs/2507.04766
- Reference count: 7
- Primary result: Even top LLMs achieve only 43% accuracy on graduate-level physics problems, with a 22.5% average drop on dynamic variants

## Executive Summary
ABench-Physics introduces a rigorous benchmark to evaluate LLMs' physics reasoning by combining static high-difficulty problems with dynamic variants that test generalization. The benchmark includes 400 graduate/Olympiad-level static problems (Phy A) and 100 parameterized dynamic problems (Phy B) with an automatic variation engine. Evaluation shows that even state-of-the-art models like Gemini 2.5 Pro achieve only 43% accuracy on static problems, with a 22.5% average drop on dynamic variants, revealing significant gaps in robust physical reasoning.

## Method Summary
ABench-Physics employs a two-part evaluation framework: Phy A consists of 400 carefully curated static problems at graduate/Olympiad level, while Phy B features 100 parameterized dynamic problems generated through an automatic variation engine. The benchmark requires precise numerical answers with strict tolerance requirements, avoiding multiple-choice or expression-based formats. Problems span classical mechanics, electromagnetism, and other physics domains, with difficulty calibrated by experts including physics professors and Olympiad winners.

## Key Results
- Top models like Gemini 2.5 Pro achieve only 43% accuracy on static graduate-level physics problems
- Dynamic variants show an average 22.5% performance drop compared to static problems
- Current LLMs demonstrate heavy reliance on memorization rather than robust physical modeling

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its dual approach of testing both memorized knowledge and generalization capabilities. By combining high-difficulty static problems with automatically generated dynamic variants, ABench-Physics exposes whether models truly understand physical principles or merely recognize patterns. The strict numerical tolerance requirements ensure that models must demonstrate precise computational reasoning rather than approximate or conceptual understanding alone.

## Foundational Learning
1. Classical Mechanics Principles
   - Why needed: Forms the core of most physics problems and tests fundamental reasoning
   - Quick check: Can models correctly apply Newton's laws to novel scenarios

2. Electromagnetic Theory
   - Why needed: Requires integration of multiple physical concepts and mathematical frameworks
   - Quick check: Ability to solve Maxwell's equation-based problems

3. Mathematical Problem Solving
   - Why needed: Physics reasoning requires precise numerical computation, not just conceptual understanding
   - Quick check: Can models perform multi-step calculations without error propagation

4. Generalization Across Problem Variants
   - Why needed: Tests whether models understand underlying principles versus memorized solutions
   - Quick check: Performance consistency between static and dynamic problem versions

## Architecture Onboarding
Component map: Problem Generator -> Evaluation Engine -> Model Interface -> Result Analyzer
Critical path: Static problem evaluation → Dynamic variant generation → Generalization assessment
Design tradeoffs: Strict numerical tolerance vs. allowing symbolic computation assistance
Failure signatures: Pattern recognition without understanding, computational errors in multi-step problems, poor generalization to variant problems
Three first experiments:
1. Test impact of providing symbolic math tools on computational accuracy
2. Compare performance across different physics subdomains
3. Evaluate fine-tuning effects on generalization capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark focuses heavily on classical mechanics and electromagnetism, with limited quantum mechanics and statistical physics representation
- Strict numerical tolerance may penalize minor computational errors that don't reflect poor conceptual understanding
- Results may conflate mathematical computation limitations with fundamental gaps in physical reasoning

## Confidence
High confidence in benchmark construction quality and problem difficulty calibration
Medium confidence in interpretation that poor performance reflects memorization over understanding
Low confidence in generalizability across different physics domains

## Next Checks
1. Test whether providing LLMs with symbolic math tools or step-by-step calculation frameworks improves performance, distinguishing between reasoning ability and computational accuracy limitations
2. Conduct ablation studies comparing performance on ABench-Physics problems versus equivalent problems phrased in different styles or contexts to assess whether the difficulty stems from problem presentation or inherent complexity
3. Evaluate whether fine-tuning models specifically on ABench-Physics training data (where available) leads to meaningful generalization improvements, or if performance gains remain limited to memorized patterns