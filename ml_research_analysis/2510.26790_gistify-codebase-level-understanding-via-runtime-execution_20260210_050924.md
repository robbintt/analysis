---
ver: rpa2
title: Gistify! Codebase-Level Understanding via Runtime Execution
arxiv_id: '2510.26790'
source_url: https://arxiv.org/abs/2510.26790
tags:
- execution
- test
- file
- code
- codebase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Gistify, a task requiring coding language
  models to generate a minimal, self-contained file that reproduces the runtime behavior
  of a given command on a codebase. The gistified file must be self-contained, exhibit
  execution fidelity, maintain minimalism, and preserve original code without hallucination.
---

# Gistify! Codebase-Level Understanding via Runtime Execution

## Quick Facts
- arXiv ID: 2510.26790
- Source URL: https://arxiv.org/abs/2510.26790
- Reference count: 38
- Key outcome: The paper introduces Gistify, a task requiring coding language models to generate a minimal, self-contained file that reproduces the runtime behavior of a given command on a codebase.

## Executive Summary
The paper introduces Gistify, a task requiring coding language models to generate a minimal, self-contained file that reproduces the runtime behavior of a given command on a codebase. The task demands execution fidelity, minimalism, and faithful code preservation without hallucination. Experiments across multiple frameworks and models reveal that even state-of-the-art models struggle with Gistify, especially for complex execution traces. Claude-4 achieves the highest execution fidelity (58.7%), while GPT-5 produces the most concise outputs. The analysis shows that tests with longer execution traces and higher coverage are harder to gistify, and dynamic file selection is more effective than static approaches.

## Method Summary
The Gistify task requires generating a single Python file that reproduces the runtime behavior of a given command on a codebase. Models are evaluated using three metrics: Execution Fidelity (whether the gistified file runs identically), Line Execution Rate (minimality), and Line Existence Rate (faithful code preservation). The evaluation uses 25 tests from each of six Python repositories (flask, requests, pylint, scikit-learn, seaborn, debug-gym) executed in Docker containers. Three agent frameworks are compared: mini-SWE-agent (bash-only), SWE-agent (file tools), and GitHub Copilot (rich toolset). Models tested include GPT-5, GPT-5-mini, Claude-3.7-Sonnet, and Claude-Sonnet-4.

## Key Results
- Even state-of-the-art models struggle with Gistify, with Claude-4 achieving highest execution fidelity at 58.7%
- Agentic models outperform static approaches, even when static models receive all relevant files as input
- Enabling execution tools provides only small performance gains (1-3 points)
- Tests with longer execution traces and higher coverage are significantly harder to gistify
- Faithful test function preservation strongly correlates with overall task success (correlation=0.76, p=0.01)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing global code context (especially execution traces) substantially improves codebase-level reasoning performance.
- Mechanism: When models receive explicit trace information showing which files and functions are touched during execution, they can bypass expensive search/navigation steps and focus cognitive resources on accurate code extraction and inlining.
- Core assumption: Models struggle more with identifying what to read than with understanding how to inline code once identified.
- Evidence anchors:
  - Performance improved when models were given access to global code context or execution traces
  - Tracing tool yielded 56.0% execution fidelity vs 42.0% baseline—a 14-point gain, the largest observed
  - Weak direct evidence; corpus papers focus on retrieval efficiency but not execution traces specifically
- Break condition: If models fail even with gold trace information, the bottleneck shifts from navigation to code composition/understanding.

### Mechanism 2
- Claim: Faithful test function preservation is a critical gating step for Gistify success.
- Mechanism: Models reason backwards from the test function to identify relevant dependencies. If they fail to locate/copy the test correctly, subsequent reasoning cascades into failure.
- Core assumption: The test function serves as an anchor point that constrains the search space for relevant code.
- Evidence anchors:
  - Strong correlation between Test F1 Score and execution fidelity (correlation=0.76, p=0.01)
  - Explicitly providing test function in prompt improved execution fidelity from 42.0% to 60.0%
  - No direct corpus evidence on test-function-as-anchor hypothesis
- Break condition: If improving test preservation doesn't improve overall success, the causal chain is broken elsewhere.

### Mechanism 3
- Claim: Agentic iteration outperforms static approaches because dynamic file selection prevents context overload.
- Mechanism: Multi-step trajectories allow models to read files selectively, maintaining focus within context limits. Static approaches that receive all relevant files at once can exceed context windows or overwhelm the model.
- Core assumption: The bottleneck is not lack of information but inability to process all relevant information simultaneously.
- Evidence anchors:
  - Agentic models outperformed static coding LLMs even when static models received all gold files as input
  - Static models achieved highest Line Existence but worst Execution Fidelity—suggesting they copy without understanding
  - "AI-Generated Code Is Not Reproducible" paper notes dependency gaps in LLM code, suggesting execution-awareness is systematically lacking
- Break condition: If a static model with sufficient context window matches agentic performance, iteration isn't the key—context window is.

## Foundational Learning

- Concept: **Execution tracing and coverage analysis**
  - Why needed here: The paper measures difficulty by trace length and files covered; understanding how to instrument code and read execution traces is essential for debugging and for implementing tools like the Tracing tool.
  - Quick check question: Can you run `python -m trace --trace script.py` and identify which functions were called?

- Concept: **Self-contained file construction (dependency inlining)**
  - Why needed here: Gistify's core requirement is producing a standalone file without external imports to the target codebase; this requires identifying transitive dependencies and copying code hierarchically.
  - Quick check question: Given `from mypkg.utils import helper`, how would you inline `helper` and all its internal dependencies into a single file?

- Concept: **Agentic tool-use patterns for codebase navigation**
  - Why needed here: The paper compares mini-SWE-agent (bash-only), SWE-agent (rich tools), and Copilot; understanding when tools help vs. add friction is critical.
  - Quick check question: What are the tradeoffs between giving an agent full bash access vs. a restricted "edit and execute" tool?

## Architecture Onboarding

- Component map: Docker image with codebase + entrypoint command -> mini-SWE-agent/mini-SWE-agent/Copilot frameworks -> optional tools (RepoGraph, Tracing, Bash/Edit-and-Execute) -> single gistified file (concise.py) -> evaluation metrics
- Critical path: 1. Parse command to identify test file/function 2. Locate and copy test function faithfully (highest-priority step) 3. Trace or predict execution to identify touched files/functions 4. Inline dependencies recursively while preserving only executed lines 5. Verify output matches original
- Design tradeoffs:
  - More tools ≠ better performance (execution tools showed only small gains; fewer tools sometimes improved results by reducing trajectory length)
  - Static context vs. agentic iteration: iteration wins despite overhead
  - Claude-4: highest Line Existence vs. GPT-5: highest Line Execution
- Failure signatures:
  - Import Error: Model imports from original codebase instead of inlining (most common in Claude-4)
  - Missing Test Function: Model strips or restructures test (most common in GPT-5)
  - Pytest Runtime Error: Incomplete inlining or syntactic issues
  - Max Steps Reached: Over-exploration with too many tools
- First 3 experiments:
  1. Baseline with Claude-4 + SWE-agent, no execution tools: Establish benchmark on 25 tests across 5 repos; measure all three metrics
  2. Ablation: Provide gold Tracing tool: Compare execution fidelity lift (expect ~14 points); analyze error type shifts
  3. Hard subset analysis: Filter for tests with top-30% trace length/coverage; verify performance drop (expect ~21% vs ~43%) to confirm difficulty scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do gistified files improve performance on downstream tasks like automated debugging and refactoring compared to full codebase context?
- Basis in paper: The authors state that gistified files "could support a range of practical applications, including debugging, refactoring, and code review, which we leave for future work."
- Why unresolved: The paper focuses solely on the generation and fidelity of the gistified files, not their utility as context for other software engineering tasks.
- What evidence would resolve it: A comparative study measuring the success rate of debugging agents when provided with gistified files versus full repositories.

### Open Question 2
- Question: How can agent architectures be modified to better leverage execution tools, such as debuggers, to solve the Gistify task?
- Basis in paper: The results show that "execution tools are not a silver bullet," noting that models did not use runtime analysis tools (like debuggers) effectively despite their availability.
- Why unresolved: Current frontier models do not instinctively use interactive debugging to trace execution flows, resulting in only marginal performance gains from tool access.
- What evidence would resolve it: Developing agent-specific training or prompting strategies that enforce debugger usage and observing a significant increase in Execution Fidelity.

### Open Question 3
- Question: Does the correlation between execution trace length and task difficulty hold across non-Python or statically-typed languages?
- Basis in paper: The evaluation is restricted to Python repositories, leaving the impact of different language paradigms on the "Gistification" process unexplored.
- Why unresolved: Python's dynamic nature may facilitate or hinder specific execution tracing steps differently than compiled languages like Java or C++.
- What evidence would resolve it: Extending the Gistify benchmark to include multi-lingual repositories and comparing model performance trends against trace complexity.

## Limitations

- The paper does not provide repository commit hashes, making exact reproduction of the evaluation environment uncertain
- Performance gains from execution tools are described as "small" but the ablation results showing which specific tool combinations were tested are not fully detailed
- The causal claims about why specific mechanisms work are not fully established through controlled experiments

## Confidence

- High confidence: The core observation that even state-of-the-art models struggle with codebase-level reasoning tasks and the general ranking of models
- Medium confidence: The specific performance numbers and the relative effectiveness of different tool configurations
- Low confidence: The causal claims about why specific mechanisms work (e.g., that agentic iteration helps specifically because of context management)

## Next Checks

1. Re-run the ablation experiments with the Tracing tool to verify the 14-point execution fidelity gain
2. Implement the Test F1 correlation analysis to confirm the strong relationship between test preservation and overall success
3. Test the hard subset analysis by filtering for tests with top-30% trace length/coverage to verify the performance drop claims