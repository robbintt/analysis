---
ver: rpa2
title: 'GUIrilla: A Scalable Framework for Automated Desktop UI Exploration'
arxiv_id: '2510.16051'
source_url: https://arxiv.org/abs/2510.16051
tags:
- task
- accessibility
- element
- tasks
- macos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GUIrilla addresses the challenge of limited, manually annotated
  desktop UI data by introducing an automated, scalable framework for macOS GUI exploration.
  Built on native accessibility APIs, it systematically crawls applications using
  specialized handlers and LLM-based agents to construct structured GUI graphs.
---

# GUIrilla: A Scalable Framework for Automated Desktop UI Exploration

## Quick Facts
- arXiv ID: 2510.16051
- Source URL: https://arxiv.org/abs/2510.16051
- Authors: Sofiya Garkot; Maksym Shamrai; Ivan Synytsia; Mariya Hirna
- Reference count: 40
- One-line primary result: Automated macOS GUI exploration framework that generates high-quality grounded task datasets, outperforming synthetic baselines while using 97% less data.

## Executive Summary
GUIrilla addresses the critical bottleneck of limited, manually annotated desktop UI data by introducing an automated, scalable framework for macOS GUI exploration. Built on native accessibility APIs, it systematically crawls applications using specialized handlers and LLM-based agents to construct structured GUI graphs. From these graphs, GUIrilla-Task—a dataset of 27,171 tasks across 1,108 apps—was generated and released, with accessibility metadata and screenshots. Fine-tuned models (GUIrilla-See series) trained on this dataset outperformed synthetic baselines on the ScreenSpot Pro benchmark while using 97% less data. The framework also introduced GUIRILLA-GOLD, a human-verified subset with a 90.26% baseline, and released macapptree for reproducible metadata collection. Results show strong cross-OS generalization and efficiency gains in GUI grounding.

## Method Summary
GUIrilla employs a three-stage pipeline: (1) a crawler that uses macOS Accessibility APIs to systematically explore applications, handling common UI patterns through specialized handlers; (2) graph construction that organizes UI states and actions into structured graphs; and (3) task generation using GPT-4 agents to convert accessibility metadata into natural, function-oriented instructions. The resulting GUIrilla-Task dataset (27,171 tasks across 1,108 apps) is used to fine-tune vision-language models via LoRA, achieving state-of-the-art grounding performance on ScreenSpot Pro while using only 6.8K images versus millions in synthetic datasets.

## Key Results
- Generated GUIrilla-Task dataset: 27,171 tasks across 1,108 macOS applications with accessibility metadata and screenshots
- GUIrilla-See models achieved 40.35% accuracy on ScreenSpot Pro (macOS subset), outperforming synthetic baselines using 97% less data
- Human-verified GUIRILLA-GOLD subset established 90.26% baseline accuracy for GUI grounding tasks
- Strong cross-OS generalization demonstrated despite training exclusively on macOS data

## Why This Works (Mechanism)

### Mechanism 1: Accessibility API-Based Exploration
Native accessibility APIs enable structured, systematic GUI exploration that surpasses surface-level visual scraping. The framework queries the macOS Accessibility API to extract a hierarchical tree of UI elements (roles, positions, descriptions) rather than relying solely on pixel-based parsing. Specialized handlers then manage common accessibility inconsistencies (invisible elements, popups, empty metadata) to generate robust interaction graphs. Core assumption: Applications expose reasonably complete and mostly accurate accessibility metadata through the OS API, though quality varies.

### Mechanism 2: LLM-Driven Task Generation
Hierarchical graph construction combined with LLM-based agents yields functionally grounded, high-quality task data. Collected UI states are organized into nodes and edges (actions). A Task Agent (GPT-4) rewrites raw accessibility-based descriptions into natural, function-oriented instructions and validates feasibility using both the accessibility tree and screenshots. Core assumption: LLMs can reliably infer functional intent from combined visual and accessibility context and produce high-quality task strings.

### Mechanism 3: Data Efficiency Through Quality
Training on a smaller, high-quality, platform-specific dataset can match or exceed larger, multi-OS synthetic datasets on grounding benchmarks. Models (GUIrilla-See series) are fine-tuned exclusively on the 6.8K-image GUIrilla-Task dataset. The function-level task supervision and full-desktop screenshots provide a dense learning signal. Core assumption: Focused, high-quality data can compensate for sheer scale in training GUI grounding models.

## Foundational Learning

**Accessibility APIs & Trees**
- Why needed here: The entire GUIrilla framework is built on parsing macOS accessibility trees. Understanding how elements are represented (roles, values, positions) is essential for debugging crawlers and data quality.
- Quick check question: Can you explain what an `AXTextField` element represents and what kind of metadata it might contain?

**GUI Grounding**
- Why needed here: The core task for the trained models is to map a natural language instruction to a specific UI element (as a coordinate or bounding box) on a screen.
- Quick check question: Given a screenshot and the instruction "Open settings," what does the model need to output?

**Low-Rank Adaptation (LoRA) for VLMs**
- Why needed here: The paper uses LoRA to fine-tune large vision-language models (Qwen, Florence) efficiently. Understanding LoRA is necessary to replicate or modify the training pipeline.
- Quick check question: What are the key hyperparameters for LoRA (rank, alpha) mentioned in the paper for GUIrilla-See-3B?

## Architecture Onboarding

**Component map:** Crawler (accessibility parser + handlers + GPT agents for input/ordering) -> Graph Builder (nodes/states, edges/actions) -> Task Agent (GPT-4 for task generation/refinement) -> Training Pipeline (LoRA-based fine-tuning of Florence/Qwen models)

**Critical path:** Quality of the final dataset and models depends directly on the Crawler's ability to robustly explore apps via accessibility APIs and the Task Agent's ability to generate high-quality function-centric instructions. Failures here propagate through the entire pipeline.

**Design tradeoffs:**
- **API-based vs. Vision-only:** Using accessibility APIs provides structured data but depends on developer annotations. A vision-only approach (like Screen2AX or OmniParser) is more robust to missing metadata but harder to implement.
- **LLM-assisted vs. Deterministic:** LLM agents improve exploration safety and task quality but add cost and latency. Deterministic crawling is cheaper but may yield lower-quality data.
- **Platform-specific vs. Multi-OS:** Focusing on macOS allows for deep integration but limits data diversity. The paper claims cross-OS generalization but is trained only on macOS.

**Failure signatures:**
- **Low task diversity/redundancy:** May indicate crawler handlers failing or app coverage issues
- **High rate of "NOT DOABLE" tasks in human verification:** Points to failures in the Task Agent or poor accessibility metadata
- **Poor grounding performance on specific app categories (e.g., creative tools):** Indicates lack of domain coverage in the training data

**First 3 experiments:**
1. **Crawler Validation on Test Apps:** Run the crawler on 5-10 diverse macOS applications from the Mac App Store. Measure graph depth, number of tasks, and manually inspect a sample of generated tasks for feasibility.
2. **Handler Ablation:** Run the crawler on a single complex application (e.g., one from the "Productivity" category) with and without the specialized handlers. Compare graph statistics (depth, task count, duplicate rate) to quantify their impact.
3. **Baseline Model Fine-tuning:** Using the provided training code, fine-tune the smallest model (GUIrilla-See-0.7B) on a subset of the data. Evaluate its grounding accuracy on the GUIrilla-Task test set to validate the end-to-end pipeline.

## Open Questions the Paper Calls Out

**Can integrating image-to-accessibility generation techniques (e.g., Screen2AX) effectively decouple the framework from the noise and sparsity of developer-provided native accessibility metadata?**
- Basis: The authors state in "Future Work" that future extensions could integrate such techniques to enable crawling where native accessibility is limited or unavailable.
- Why unresolved: The current framework relies entirely on native macOS Accessibility APIs, which the paper notes are often error-prone, contain missing elements, or have inaccurate positioning (12% "BAD" rating in quality analysis).

**How can automated exploration frameworks be adapted to effectively cover canvas-focused creative workflows (e.g., drawing) or applications that require pre-existing user-generated content to function?**
- Basis: The "Failure Mode Analysis" identifies insufficient coverage of creative workflows (e.g., "draw a circle"), and "Future Work" suggests using accounts with pre-filled user-generated content to mitigate this.
- Why unresolved: The current graph-based exploration relies on detecting standard UI element changes, which may not capture state changes in canvas-rendered applications or apps that are empty/unusable without prior user data.

**Can reinforcement learning (RL) be applied to local vision-language models (VLMs) to improve exploration coverage and discover novel interaction patterns without relying on static handlers?**
- Basis: The "Future Work" section identifies the development of local VLM agents using RL as a promising direction to improve coverage and discover novel patterns.
- Why unresolved: The current system uses specialized, static interaction handlers and relies on GPT-4 for reasoning; it does not learn new exploration strategies dynamically based on the environment feedback.

## Limitations

**Accessibility API Dependence:** The framework's effectiveness is fundamentally constrained by the quality of macOS accessibility metadata, with only ~33% of macOS apps providing full accessibility.

**LLM-Generated Task Quality:** While strong performance improvements are reported, the exact quality control mechanisms for scaling from human-verified samples to 27,171 tasks are unclear.

**Cross-Platform Generalization Claims:** Claims of strong cross-OS generalization lack comprehensive validation across diverse non-macOS applications, particularly regarding UI paradigms and accessibility implementations.

## Confidence

**High Confidence:** The core technical contributions (accessibility-based crawling framework, task generation pipeline, and LoRA-based fine-tuning approach) are well-documented and reproducible. The performance improvements over synthetic baselines on ScreenSpot Pro are clearly demonstrated with specific metrics.

**Medium Confidence:** The dataset quality and task generation methodology are well-supported by the released GUIrilla-GOLD subset and human verification results. However, the scaling process from human-verified samples to the full 27,171-task dataset introduces uncertainty about consistency.

**Low Confidence:** The cross-OS generalization claims, while supported by benchmark results, lack comprehensive validation across diverse non-macOS applications. The paper doesn't provide detailed analysis of performance degradation when models encounter unfamiliar UI patterns.

## Next Checks

1. **Handler Effectiveness Analysis:** Conduct a controlled experiment comparing crawler output with and without specialized handlers on 10-15 complex macOS applications. Quantify improvements in graph depth, task diversity, and error rates to validate the handlers' contribution to dataset quality.

2. **Accessibility Coverage Audit:** Analyze the accessibility metadata quality across the 1,108 apps in GUIrilla-Task. Calculate the percentage of apps with complete metadata, identify common failure patterns (missing roles, empty descriptions), and assess their correlation with task generation failures or model performance degradation.

3. **Cross-Platform Transfer Test:** Fine-tune GUIrilla-See models on the full GUIrilla-Task dataset, then evaluate on a representative set of Windows/Linux desktop applications not seen during training. Measure grounding accuracy degradation and identify specific UI patterns or accessibility implementations that cause performance drops.