---
ver: rpa2
title: 'LExT: Towards Evaluating Trustworthiness of Natural Language Explanations'
arxiv_id: '2504.06227'
source_url: https://arxiv.org/abs/2504.06227
tags:
- explanation
- explanations
- faithfulness
- language
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LExT, a comprehensive framework for evaluating
  the trustworthiness of natural language explanations generated by large language
  models (LLMs). The framework quantifies both Plausibility and Faithfulness of explanations
  using novel metrics like Context Relevancy, QAG score, Counterfactual Stability,
  and Contextual Faithfulness.
---

# LExT: Towards Evaluating Trustworthiness of Natural Language Explanations

## Quick Facts
- arXiv ID: 2504.06227
- Source URL: https://arxiv.org/abs/2504.06227
- Reference count: 40
- This paper introduces LExT, a comprehensive framework for evaluating the trustworthiness of natural language explanations generated by large language models (LLMs).

## Executive Summary
This paper presents LExT, a novel framework designed to evaluate the trustworthiness of natural language explanations produced by large language models (LLMs) in high-stakes domains like healthcare. The framework quantifies two key dimensions of trustworthiness: Plausibility (how convincing explanations appear to humans) and Faithfulness (how accurately explanations reflect the model's reasoning process). LExT introduces novel metrics including Context Relevancy, QAG score, Counterfactual Stability, and Contextual Faithfulness, which are combined into a unified trustworthiness score. The authors evaluate six models across two medical datasets, finding that general-purpose models like Llama3 and Gemma often outperform domain-specific models in generating trustworthy explanations.

## Method Summary
The LExT framework evaluates LLM-generated explanations through a dual-dimension approach, measuring both Plausibility (how convincing explanations appear to humans) and Faithfulness (how accurately explanations reflect the model's reasoning process). The framework introduces four novel metrics: Context Relevancy (assesses how well explanations align with input context using question generation and scoring), QAG score (evaluates the consistency between explanations and question-answer pairs), Counterfactual Stability (measures explanation consistency when input context is perturbed), and Contextual Faithfulness (quantifies alignment between explanations and model decisions). These metrics are weighted and combined into a unified trustworthiness score. The framework was tested on two medical datasets (MIMIC-III and MedExM) using six different models, comparing general-purpose and domain-specific LLMs.

## Key Results
- General-purpose models (Llama3-8B, Gemma-7B) significantly outperformed domain-specific models (MedPalm, BioMistral) in Plausibility and Faithfulness metrics
- QAG scores for Llama3-8B (0.6225) and Gemma-7B (0.6206) were substantially higher than domain-specific models (0.1974-0.4138)
- Counterfactual Stability results showed Llama3-8B and Gemma-7B had significantly lower semantic shifts (0.1214, 0.1342) compared to MedPalm (0.2076) and BioMistral (0.1947)

## Why This Works (Mechanism)
The LExT framework works by decomposing the complex problem of explanation trustworthiness into measurable dimensions with specific metrics. By quantifying both Plausibility (how convincing explanations appear) and Faithfulness (how accurately they reflect reasoning), the framework captures the dual nature of trustworthy explanations. The use of multiple complementary metrics addresses different failure modes: Context Relevancy ensures explanations stay on-topic, QAG score verifies internal consistency, Counterfactual Stability tests robustness to input changes, and Contextual Faithfulness measures alignment with actual model decisions. The weighted combination allows for nuanced evaluation that reflects real-world priorities in high-stakes domains.

## Foundational Learning
- **Plausibility vs Faithfulness**: These represent two distinct dimensions of explanation quality - plausibility is about human perception while faithfulness is about fidelity to actual reasoning. Needed to avoid explanations that are convincing but misleading. Quick check: Ask if an explanation could be convincing without being accurate (plausible but unfaithful) or accurate but unconvincing (faithful but implausible).
- **Counterfactual Stability**: Measures how much explanations change when input context is slightly modified. Critical for detecting explanations that are too tightly coupled to specific inputs rather than capturing generalizable reasoning. Quick check: Apply small, semantically equivalent changes to inputs and measure explanation variance.
- **Context Relevancy through Question Generation**: Uses generated questions about input context to assess whether explanations address the right aspects. Provides a systematic way to evaluate topical alignment. Quick check: Generate questions from context, then check if explanations answer them appropriately.
- **Semantic Similarity Limitations**: BERT embeddings can yield high scores for semantically similar but factually incorrect explanations. Highlights the need for more rigorous verification methods. Quick check: Test if blank or irrelevant explanations receive inappropriately high similarity scores.
- **Domain-specific vs General-purpose Trade-offs**: General-purpose models often outperform specialized ones in explanation quality due to better language understanding, despite lacking domain expertise. Quick check: Compare explanation quality across both model types on the same tasks.

## Architecture Onboarding

Component Map: Input Context + Model Output -> LExT Metrics (Context Relevancy, QAG Score, Counterfactual Stability, Contextual Faithfulness) -> Weighted Scoring -> Trustworthiness Score

Critical Path: Input context and model output are processed through all four metrics in parallel, with their outputs combined through weighted averaging to produce the final trustworthiness score. The most computationally intensive step is typically the question generation for Context Relevancy and the counterfactual generation for Counterfactual Stability.

Design Tradeoffs: The framework trades computational efficiency for comprehensiveness by using multiple metrics rather than a single score. It relies on larger "judge" models (e.g., Llama 70B) to evaluate smaller models, which may introduce latency and cost concerns. The choice of medical datasets limits generalizability but ensures domain relevance.

Failure Signatures: High plausibility with low faithfulness indicates potentially misleading explanations; low counterfactual stability suggests explanations are too input-specific; poor context relevancy indicates explanations that don't address the actual problem; inconsistent QAG scores suggest internal logical contradictions.

First Experiments:
1. Apply LExT to a simple classification task with known ground truth to validate metric behavior
2. Compare explanations from the same model with different prompts to test consistency
3. Test the framework's sensitivity to increasingly subtle perturbations in input context

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the LExT framework be effectively adapted to quantify social fairness and mitigate biases in LLM explanations across diverse demographic contexts?
- Basis in paper: [explicit] The authors explicitly state in Section 6 (Conclusion and Future Scope) that "The future scope involves tackling key issues such as biases in explanations, fairness, and ensuring equality across diverse contexts."
- Why unresolved: While the authors augmented the QPain dataset with demographic variations (Appendix A.1), the current study focuses on trustworthiness metrics (Plausibility/Faithfulness) rather than explicitly measuring or mitigating social bias in the generated outputs.
- What evidence would resolve it: A study applying LExT to the augmented dataset, correlating LExT scores with fairness metrics (e.g., difference in accuracy/explanation quality across demographic groups), and demonstrating a reduction in bias.

### Open Question 2
- Question: How can the "Correctness" metric be improved to rely less on semantic similarity and more on rigorous factual verification against comprehensive ground truths?
- Basis in paper: [explicit] Section 6 notes the need for "developing comprehensive ground truth datasets for factual verification to enhance model reliability," and Section 3.1.1 highlights that BERT embeddings often yield high scores for semantically similar but factually incorrect or irrelevant explanations (e.g., blank explanations scoring 0.31).
- Why unresolved: The current "Correctness" metric relies on weighted NER and cosine similarity, which are approximations; the paper acknowledges that hallucinated or irrelevant content can still score highly without robust, verified ground truth datasets.
- What evidence would resolve it: The development and integration of a verified medical knowledge base that allows the metric to strictly penalize factual contradictions, distinct from low semantic overlap.

### Open Question 3
- Question: Is it possible to compute metrics like Context Relevancy and Counterfactual Stability without relying on larger "judge" models (e.g., Llama 70B)?
- Basis in paper: [inferred] While the paper mentions optimizing smaller models in Section 6, the methodology (Section 4.2) explicitly relies on Llama 70B to generate questions for Context Relevancy and rephrase explanations for Counterfactual Stability.
- Why unresolved: The current framework's dependency on a larger, separate model to evaluate smaller models introduces potential latency, cost, and accessibility barriers, contradicting the goal of deploying efficient models in resource-constrained settings.
- What evidence would resolve it: A modified implementation where metrics are calculated using self-evaluation or smaller, fine-tuned discriminators that achieve statistical parity with the current Llama 70B-based evaluation.

## Limitations
- The framework focuses on plausibility and faithfulness but may overlook other critical aspects like calibration, robustness to adversarial inputs, or fairness considerations
- Results are based on only two medical datasets (MIMIC-III and MedExM) which may not represent the full diversity of healthcare scenarios
- The evaluation relies on proxy measures rather than direct human assessment of explanation quality
- Comparison between general-purpose and domain-specific models may be influenced by factors beyond model architecture, such as prompt engineering quality

## Confidence

**High Confidence**: The framework's methodological soundness in decomposing explanation evaluation into plausibility and faithfulness dimensions; the empirical finding that general-purpose models often outperform specialized models in generating trustworthy explanations

**Medium Confidence**: The relative performance rankings of specific models across different metrics; the generalizability of results to other healthcare domains beyond the two datasets studied

**Medium Confidence**: The sufficiency of the proposed metric set (Context Relevancy, QAG score, Counterfactual Stability, and Contextual Faithfulness) in capturing all aspects of explanation trustworthiness

## Next Checks
1. Conduct human evaluation studies with medical practitioners to validate the correlation between LExT metrics and actual trust in explanations, particularly focusing on clinical decision-making scenarios
2. Test the framework's applicability across diverse healthcare domains (e.g., radiology, pathology, public health) and non-medical high-stakes domains to assess generalizability
3. Evaluate explanation robustness under adversarial conditions, including out-of-distribution inputs and intentionally misleading prompts, to assess the framework's ability to detect untrustworthy explanations