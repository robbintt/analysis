---
ver: rpa2
title: Efficient Multi-Task Learning via Generalist Recommender
arxiv_id: '2504.05318'
source_url: https://arxiv.org/abs/2504.05318
tags:
- grec
- routing
- tasks
- task
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose Generalist Recommender (GRec), an efficient
  multi-task learning model for recommender systems. GRec addresses the scalability
  challenges of existing MTL models by using a task-sentence level routing mechanism
  within a sparse Mixture-of-Experts (MoE) architecture.
---

# Efficient Multi-Task Learning via Generalist Recommender

## Quick Facts
- **arXiv ID**: 2504.05318
- **Source URL**: https://arxiv.org/abs/2504.05318
- **Reference count**: 26
- **Primary result**: GRec improves multi-task recommendation performance by 2.1% to 20.3% over baselines while reducing FLOPs through task-sentence level routing in sparse MoE.

## Executive Summary
GRec addresses scalability challenges in multi-task learning for recommender systems by introducing a task-sentence level routing mechanism within a sparse Mixture-of-Experts architecture. The model processes multi-modal inputs through parallel Transformers, wide and deep structures, and NLP heads. Experiments demonstrate superior average precision and reduced computational cost compared to baseline methods, with successful deployment on a large telecom platform handling high-volume traffic.

## Method Summary
GRec employs a sparse MoE architecture with task-sentence level routing, where task-type tokens are combined into "task sentences" before routing to experts. The model processes multi-modal inputs through wide and deep layers, parallel Transformers, and CLIP encoders. Routing decisions are made by averaging task embeddings (main task + auxiliary task) rather than routing individual tokens. The architecture uses 8 experts with top-4 routing, batch size 2048, and includes load balancing through task sampling. Training uses 3 months of transaction data with 1 week of test data.

## Key Results
- GRec achieves 2.1% to 20.3% improvement in average precision across various tasks compared to baseline methods
- Reduces FLOPs by 50% compared to token-level routing while maintaining comparable performance
- Successfully deployed on a large telecom platform with high-volume traffic handling
- Shows consistent improvements on both public AliExpress dataset and internal telecom data

## Why This Works (Mechanism)

### Mechanism 1: Task-Sentence Level Routing in Sparse MoE
The routing mechanism combines task-type tokens into "task sentences" before routing, enabling efficient multi-task scaling with better performance-FLOPs tradeoff than token-level or pure task-level routing. The gating function computes expert selection weights by averaging task embeddings rather than routing individual tokens, allowing cross-task knowledge sharing while avoiding combinatorial explosion.

### Mechanism 2: Sparse MoE Enables Compute-Efficient Scaling
Increasing total expert count and top-k selection improves model capacity without proportional FLOPs increase. Only top-k experts process each input, so unused experts incur no computation. Adding experts increases capacity sub-linearly with compute since routing FLOPs remain small relative to expert computation.

### Mechanism 3: Multi-Modal Input Fusion via Wide-Deep and Parallel Transformers
Concatenating embeddings from categorical encoders, numerical feedforward layers, pre-trained Transformers, and CLIP creates comprehensive item representations. Each modality is encoded separately then concatenated, preserving modality-specific signals while allowing downstream MoE to learn cross-modal interactions.

## Foundational Learning

- **Mixture-of-Experts with Sparse Gating**: Core architecture enabling multi-task scaling. Understand how top-k expert selection works and why sparsity improves efficiency.
  - Quick check: Given 8 experts with top-2 routing and batch size 4, how many expert forward passes occur per layer? (Answer: 8, not 32)

- **Multi-Head Attention with Multi-Query Optimization**: GRec uses multi-query single-key-value attention. This reduces attention block size and computation versus standard multi-head attention.
  - Quick check: In standard multi-head attention with 8 heads and head dimension 64, what are K,V shapes? How do they change with multi-query attention?

- **Wide & Deep Learning for Recommendations**: GRec's input processing layer follows this paradigm—wide component (memorization via direct features) + deep component (generalization via embeddings and neural layers).
  - Quick check: Why combine wide and deep rather than using either alone? What types of feature interactions does each capture?

## Architecture Onboarding

- **Component map**: Inputs → Wide & Deep Layer → Parallel Transformer Layer → Task-Sentence MoE Layer → Task Outputs
  - Categorical embeddings → Multi-query attention → Gating network
  - Numerical FF layers → Parallel FFN → Expert networks (8 total)
  - Search Transformer → Residual connections → Top-4 selection
  - CLIP (image/text)

- **Critical path**: Input encoding → embedding concatenation → attention pooling → MoE routing → task-specific predictions. The routing decision at the MoE layer determines which experts process each sample; errors here cascade to all downstream tasks.

- **Design tradeoffs**:
  - **Routing granularity**: Token-level = highest precision, highest FLOPs; Task-sentence = best balance for this use case; Task-level = lowest FLOPs but may lose nuance
  - **Expert count vs. top-k**: More experts + higher k improves performance but increases latency; diminishing returns after k=4-6
  - **Modality depth**: Deeper per-modality encoders may improve quality but increase training complexity and latency

- **Failure signatures**:
  - **Routing collapse**: Most samples routed to 1-2 experts → others never train → monitor expert utilization histograms
  - **Task dominance**: High-data tasks (CTR) overwhelm sparse tasks (CVR) in shared experts → monitor per-task AP gaps
  - **Inference latency spike**: If top-k or expert capacity set too high, FLOPs savings disappear → profile end-to-end latency

- **First 3 experiments**:
  1. **Baseline replication**: Implement MMoE with 4 experts on internal data; verify Table 1 metrics are reproducible within 0.5% AP before adding MoE complexity
  2. **Routing ablation**: Compare token vs. sentence vs. task vs. task-sentence routing with identical expert configurations; confirm task-sentence achieves ≥90% of token-level AP at ≤60% FLOPs
  3. **Expert scaling sweep**: Fix task-sentence routing, vary total experts (8/16/32) and top-k (2/4/6); plot AP vs. FLOPs to identify Pareto frontier for your latency budget

## Open Questions the Paper Calls Out

### Open Question 1
Does the Task-Sentence routing strategy maintain its efficiency advantage over token-level routing in datasets with significantly higher task heterogeneity or conflicting optimization objectives? The paper evaluates routing on a telecom dataset with specific, correlated flows, but it's unclear if the heuristic generalizes to domains where tasks are less semantically related or conflict.

### Open Question 2
How does the proposed routing mechanism influence expert specialization, and does it lead to specific experts isolating sparse signal noise versus shared feature representations? While the authors demonstrate improved average precision for sparse tasks, they don't analyze the internal representations learned by the experts to verify if routing effectively separates task-specific logic.

### Open Question 3
What are the latency implications of the routing mechanism when scaling the total number of experts beyond the tested range (e.g., >48 experts)? Figure 4 shows FLOPs remain stable as total experts increase, but the computational overhead of the gating network itself at extreme scales is not profiled.

## Limitations
- Limited ablation studies comparing all four routing strategies (token, sentence, task, task-sentence) under identical conditions
- Doesn't address potential routing collapse or expert specialization degradation at larger scales
- Effectiveness of using pre-trained CLIP and Transformer encoders without fine-tuning is assumed but not validated

## Confidence
- **High Confidence**: Claims about offline performance improvements (2.1% to 20.3% AP gains vs. baselines) are supported by multiple datasets and explicit metric reporting
- **Medium Confidence**: Online A/B testing results (2.1% to 20.3% improvements across various tasks) lack statistical significance details and methodology
- **Low Confidence**: Claims about task-sentence routing being the "optimal" balance point are weakly supported with only one routing comparison

## Next Checks
1. **Routing Strategy Pareto Analysis**: Implement token, sentence, task, and task-sentence routing with identical MoE configurations. Measure AP, FLOPs, and latency for each to verify task-sentence achieves ≥90% of token-level AP at ≤60% FLOPs.

2. **Expert Utilization Stress Test**: Train GRec with increasing expert counts while monitoring per-expert activation frequency, task-level routing patterns, and AP per task to identify routing collapse thresholds.

3. **Pre-trained Component Ablation**: Replace CLIP and search Transformer encoders with learned encoders initialized randomly. Train GRec from scratch to quantify pre-training transfer benefits and determine if fine-tuning improves results.