---
ver: rpa2
title: On Listwise Reranking for Corpus Feedback
arxiv_id: '2510.00887'
source_url: https://arxiv.org/abs/2510.00887
tags:
- graph
- document
- listwise
- arxiv
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of graph-based
  adaptive retrieval methods that require expensive document similarity graphs. The
  proposed L2G framework implicitly constructs a document graph from listwise reranking
  outputs without additional LLM calls or explicit graph computation.
---

# On Listwise Reranking for Corpus Feedback

## Quick Facts
- arXiv ID: 2510.00887
- Source URL: https://arxiv.org/abs/2510.00887
- Reference count: 31
- Key outcome: L2G achieves graph-based adaptive retrieval performance comparable to oracle methods while maintaining sliding-window reranker LLM call budgets

## Executive Summary
This paper introduces L2G, a method that implicitly constructs document-document similarity graphs from listwise reranking outputs without explicit graph computation or additional LLM calls. By aggregating co-occurrence signals from multiple queries through matrix operations, L2G approximates the document affinity matrices used in graph-based adaptive retrieval methods. The approach demonstrates comparable effectiveness to oracle-based graph methods on TREC-DL and BEIR datasets while maintaining the same computational budget as sliding-window rerankers, offering an efficient alternative to explicit graph construction.

## Method Summary
L2G builds document graphs implicitly from listwise reranker outputs by converting ranked lists into score vectors where documents receive higher scores based on their rank position. These vectors are aggregated across queries via matrix multiplication to form a document-document affinity matrix. The method applies k-hop random walk propagation (capped at k=3) to capture higher-order document relationships, followed by IDF-style reweighting to suppress popularity bias from frequently retrieved documents. The framework supports incremental updates when new documents arrive, maintaining efficiency through block matrix operations. L2G integrates with existing adaptive retrieval frameworks like SlideGAR by providing the implicit graph structure instead of requiring explicit precomputed similarity graphs.

## Key Results
- L2G achieves nDCG@10 performance within 0.1 points of oracle-based graph methods on TREC-DL datasets
- The method maintains the same LLM call budget as sliding-window rerankers (9 calls) while providing graph-based benefits
- L2G demonstrates robustness across different query orders, retrievers (BM25, Contriever), and reranker models (RankZephyr, ReaRank)
- Per-query latency remains under 0.15 seconds on BEIR subsets with incremental graph construction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Listwise reranking outputs encode implicit document-document similarity signals that can approximate explicit corpus graphs.
- **Mechanism**: For each query $q_i$, the ranked list is converted to a score vector $a_i$ where $[a_i]_d = (k - \pi_i(d) + 1)$ for ranked documents. Aggregating across queries via $D^{(1)} = AA^\top$ accumulates co-occurrence evidence—documents frequently ranked together across queries receive higher pairwise affinity scores.
- **Core assumption**: Documents that co-occur in multiple query result sets and are ranked similarly share semantic relatedness (Assumption: validated empirically but not theoretically proven).
- **Evidence anchors**:
  - [abstract]: "implicitly induces document graphs from listwise reranker logs"
  - [section 3.1]: "we approximate pairwise document affinity as $D^{(1)} = AA^\top$, where $D^{(1)}_{d_1,d_2} = \sum_i [a_i]_{d_1}[a_i]_{d_2}$ accumulates evidence over all queries"
  - [corpus]: Weak direct support; neighbor papers discuss listwise reranking efficiency but not graph induction from ranking outputs.
- **Break condition**: Very low cross-query document overlap (<1%) yields sparse $D^{(1)}$ with insufficient signal for graph approximation (acknowledged in Appendix B).

### Mechanism 2
- **Claim**: k-hop random walk propagation recovers higher-order document connectivity absent from sparse first-order co-occurrence.
- **Mechanism**: First-order matrix $D^{(1)}$ is sparse when candidate sets rarely overlap. Propagating via $D^{(k)} = D \cdot D \cdots D$ (k times with row normalization) connects documents sharing a common neighbor. The paper caps $k \leq 3$ to prevent rank collapse toward uniform distributions.
- **Core assumption**: Multi-hop document relationships captured via propagated affinity correlate with retrieval utility (Assumption).
- **Evidence anchors**:
  - [section 3.2]: "multi-hop connections—e.g., two documents may not co-occur but may each co-occur with a shared third document—can capture higher-order similarity structure"
  - [section 3.2]: "we therefore (i) renormalize rows to unit $\ell_1$ norm after each multiplication and (ii) cap $k \leq 3$ in practice"
  - [corpus]: No direct corpus validation; neighbor papers do not address graph propagation from ranking signals.
- **Break condition**: Excessive propagation depth ($k > 3$) causes rank collapse; very small document pools provide insufficient multi-hop paths.

### Mechanism 3
- **Claim**: IDF-style reweighting suppresses spurious edges from frequently retrieved documents.
- **Mechanism**: Popular documents appearing in many query results accumulate high co-occurrence scores regardless of semantic relevance. Dividing each document's score vector by $\log(1 + \text{df}(d))$ downweights these hub nodes.
- **Core assumption**: Document frequency in retrieved sets correlates with popularity bias rather than genuine semantic centrality (Assumption: borrowed from IR IDF intuition).
- **Evidence anchors**:
  - [section 3.3]: "documents that appear frequently across many queries will accumulate high similarity scores simply due to repeated co-occurrence, not because they are genuinely related"
  - [section 3.3]: "we borrow a well-established idea from information retrieval: inverse document frequency (IDF) re-weighting"
  - [corpus]: No corpus evidence specific to this technique in reranking contexts.
- **Break condition**: Highly skewed query distributions where few documents dominate all retrievals may still produce noisy graphs despite reweighting.

## Foundational Learning

- **Concept: Listwise reranking with sliding windows**
  - **Why needed here**: L2G extracts signals from listwise reranker outputs; understanding how RankZephyr/ReaRank process candidate windows (size=20, step=10) is prerequisite to grasping what information gets encoded.
  - **Quick check question**: Can you explain why sliding windows limit cross-window document interactions?

- **Concept: Document-document similarity graphs for retrieval**
  - **Why needed here**: The paper positions L2G against SlideGAR which requires precomputed TCT bi-encoder graphs; understanding $O(N^2)$ memory cost of explicit graphs motivates the implicit approach.
  - **Quick check question**: What is the memory complexity of storing pairwise similarity for 8.8M documents (MS MARCO)?

- **Concept: Random walk / PageRank-style propagation**
  - **Why needed here**: L2G uses k-hop propagation via matrix power iteration; familiarity with how repeated multiplication diffuses affinity scores is essential.
  - **Quick check question**: Why does row normalization prevent a single high-degree node from dominating propagated scores?

## Architecture Onboarding

- **Component map**: Query stream → First-stage retriever → Top-c candidates → Listwise reranker → Ranked permutation → Score vectorizer → Sparse matrix A → Graph builder → D^(1) = AA^T → k-hop propagation → D^(3) → IDF reweighting → Final sparse graph G → Adaptive retrieval

- **Critical path**: Incremental matrix update (Section 3.4)—when new documents arrive, extend A with block updates B = AA^T_Δ, C = A_ΔA^T_Δ rather than full recomputation. This maintains O(|D||ΔD|) vs O(|D_new|^2).

- **Design tradeoffs**:
  - Pool size c: Top-100 yields faster but sparser graphs; Top-1000 increases overlap (39.7% vs 10.4% on TREC-COVID) at higher memory cost
  - Propagation depth k: k=3 balances recall vs rank collapse risk
  - Update frequency: Eager updates (U=1) vs batched updates—tunable latency/quality knob

- **Failure signatures**:
  - Very low cross-query overlap (<1%) produces sparse graphs with minimal benefit over sliding window baseline (DL'19/20 show 0.1% overlap)
  - Cold-start: Early queries have insufficient accumulated signal; expect degraded performance until graph matures
  - Memory growth: Linear in processed queries (~0.0276q MB); unbounded streams require eviction strategies

- **First 3 experiments**:
  1. **Baseline comparison**: Replicate Table 1 on TREC-DL'19/20 with BM25 Top-100—compare L2G vs Sliding Window vs SlideGAR-TCT (doc-affinity) using identical LLM call budget (9 calls). Expect L2G within ±0.1 nDCG@10 of doc-affinity baseline.
  2. **Ablation on overlap sensitivity**: Control query arrival order (max-overlap vs min-overlap vs random per Table 3) on a high-overlap dataset (TREC-COVID at 10.4%). Verify order independence.
  3. **Latency profiling**: Measure per-query latency breakdown for graph expansion + 3-hop propagation. Target <0.15s/query on BEIR subsets. Compare against SlideGAR-TCT's 0.427s baseline with bi-encoder loading.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal update interval for batching graph updates, and how does delayed propagation affect ranking quality versus latency trade-offs?
- Basis in paper: [explicit] Authors state in Section 4.3 and Appendix A.1: "delaying or batching graph updates (e.g., delay three propagation hops) further reduces constant factors without affecting ranking quality, which we leave as future work" and note that "increasing the update interval U would shrink the 0.035q slope proportionally."
- Why unresolved: The paper uses eager updates (U=1) throughout experiments but does not explore the quality-latency frontier across different U values.
- What evidence would resolve it: A sweep of update intervals (U ∈ {1, 5, 10, 50, 100}) with paired measurements of nDCG@10 degradation and per-query latency reduction on the same benchmarks.

### Open Question 2
- Question: How can L2G be augmented to remain effective in low-overlap scenarios where document co-occurrence across queries is sparse?
- Basis in paper: [explicit] Appendix B states: "Our method's effectiveness is highly affected on documents co-occurring in retrieved lists, making it less effective in scenarios with low query-document overlap. In this case, when the constructed graph remains sparse, L2G may not provide substantial benefit."
- Why unresolved: The current design relies entirely on co-occurrence signals; no fallback or augmentation strategy is proposed for sparse-overlap regimes.
- What evidence would resolve it: Experiments on corpora with controlled overlap levels (e.g., synthetic query streams with 0.1%–5% overlap) comparing L2G against hybrid approaches that supplement sparse signals with lightweight similarity computations.

### Open Question 3
- Question: Would a GPU-optimized implementation of L2G yield substantial speedups, and at what memory cost?
- Basis in paper: [explicit] Ethical Considerations note: "Our simple implementation leaves additional speed and memory gains as future work, which could help address these accessibility concerns" and mention "implementation of the GPU version of L2G" as an optimization path.
- Why unresolved: All efficiency measurements use CPU-only sparse operations; the potential benefits of parallelized sparse matrix multiplication on GPU remain unquantified.
- What evidence would resolve it: A CUDA-optimized implementation benchmarked against the current CPU version, reporting latency, throughput (queries/second), and VRAM usage across session sizes.

## Limitations
- The method's effectiveness depends heavily on document co-occurrence across queries, making it less effective in low-overlap scenarios
- Memory requirements grow linearly with processed queries, raising practical concerns for unbounded streams
- The theoretical justification for why k-hop propagation depth k≤3 prevents rank collapse while preserving useful structure is absent

## Confidence

- **High confidence**: The computational efficiency claims and memory complexity analysis are well-supported by the formulation and incremental update mechanism.
- **Medium confidence**: The empirical performance comparisons against oracle-based graph methods are convincing on the tested datasets, but generalizability to other retrieval scenarios remains unproven.
- **Low confidence**: The theoretical justification for why k-hop propagation depth k≤3 prevents rank collapse while preserving useful structure is absent; the IDF reweighting's effectiveness is justified by IR tradition rather than reranking-specific validation.

## Next Checks

1. **Cold-start performance**: Measure L2G effectiveness as a function of accumulated queries on TREC-DL'19/20, showing degradation curves for early queries and convergence patterns.
2. **Memory scalability analysis**: Profile VRAM consumption and query latency as a function of processed queries (q) on BEIR subsets, verifying the claimed O(q) growth rate and identifying practical limits.
3. **Semantic alignment test**: Conduct controlled experiments where documents are intentionally reordered in ranked lists while maintaining similar relevance scores, measuring how L2G's induced similarity matrices respond to ranking perturbations.