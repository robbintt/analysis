---
ver: rpa2
title: Semantic Caching of Contextual Summaries for Efficient Question-Answering with
  Language Models
arxiv_id: '2505.11271'
source_url: https://arxiv.org/abs/2505.11271
tags:
- caching
- document
- summaries
- cache
- summary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of Large Language
  Models (LLMs) in document-based question-answering workflows by introducing a semantic
  caching approach for intermediate contextual summaries. The method caches compressed
  summaries generated during document processing and reuses them for similar queries,
  reducing redundant computations by up to 50-60%.
---

# Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models

## Quick Facts
- arXiv ID: 2505.11271
- Source URL: https://arxiv.org/abs/2505.11271
- Reference count: 40
- Primary result: Semantic caching achieves 50-60% reduction in redundant processing while maintaining answer quality

## Executive Summary
This paper introduces a semantic caching approach to improve the computational efficiency of Large Language Models in document-based question-answering workflows. The method caches compressed contextual summaries generated during document processing and reuses them for similar queries, significantly reducing redundant computations. By leveraging cosine similarity between query embeddings to determine cache hits with adaptive thresholds, the approach achieves utility comparable to full-document processing while substantially reducing input token usage and latency. The method demonstrates effectiveness across multiple datasets including NaturalQuestions, TriviaQA, and synthetic ArXiv collections.

## Method Summary
The semantic caching approach processes documents by dividing them into 2000-token chunks and generating compressed contextual summaries using an external summarization model. These summaries are cached along with their semantic embeddings. When new queries arrive, the system computes cosine similarity between the query embedding and cached summary embeddings to identify potential cache hits. A similarity threshold (0.8) determines whether to use cached summaries or process the document anew. The approach is order-independent and adaptable to dynamic document updates, with cache hit ratios of 0.2-0.3 for diverse datasets and 0.5-0.6 for similar queries. Experiments demonstrate that cached contextual summaries achieve comparable utility to full-document processing while reducing computational overhead by 50-60%.

## Key Results
- 50-60% reduction in redundant document processing through semantic caching
- Cache hit ratios of 0.2-0.3 for diverse query datasets and 0.5-0.6 for similar queries
- Similarity threshold of 0.8 effectively balances accuracy and efficiency
- Comparable answer quality to full-document processing with significantly reduced token usage and latency

## Why This Works (Mechanism)
The approach works by exploiting the semantic similarity between queries to reuse previously computed contextual summaries. When documents are processed, the system generates compact representations that capture the essential information needed for answering questions. By storing these summaries along with their semantic embeddings, the system can quickly identify when new queries are sufficiently similar to previous ones, allowing reuse of cached results instead of reprocessing entire documents. The cosine similarity metric provides an effective measure of semantic relatedness between queries, while the adaptive threshold ensures that only sufficiently similar queries trigger cache hits, maintaining answer quality.

## Foundational Learning
**Semantic Embeddings**: Vector representations capturing meaning of text
- Why needed: Enable efficient similarity comparison between queries and cached summaries
- Quick check: Verify embeddings preserve semantic relationships through nearest neighbor analysis

**Cosine Similarity**: Metric measuring angle between vector representations
- Why needed: Provides scale-invariant measure of semantic similarity
- Quick check: Test on known similar and dissimilar query pairs

**Contextual Summarization**: Process of extracting key information from document chunks
- Why needed: Creates compact representations for efficient caching
- Quick check: Measure information retention vs compression ratio

**Chunk-based Processing**: Dividing documents into fixed-size segments
- Why needed: Enables parallel processing and manageable cache units
- Quick check: Vary chunk sizes and measure impact on cache hit rates

## Architecture Onboarding

**Component Map**: Document Chunking -> Summary Generation -> Embedding Storage -> Query Processing -> Cache Lookup -> Answer Generation

**Critical Path**: Query Embedding Computation -> Cosine Similarity Search -> Cache Hit Decision -> Cached Summary Retrieval OR Document Processing -> LLM Inference

**Design Tradeoffs**: 
- Fixed chunk size (2000 tokens) vs adaptive sizing for different document types
- Compression ratio vs information retention in summaries
- Similarity threshold (0.8) vs cache hit rate and answer quality
- External summarization dependency vs computational overhead

**Failure Signatures**:
- Low cache hit rates indicate poor query similarity clustering or suboptimal threshold
- Degraded answer quality suggests insufficient information retention in summaries
- High memory usage points to inefficient cache eviction policies
- Increased latency may indicate embedding computation bottlenecks

**3 First Experiments**:
1. Vary chunk sizes (1000, 2000, 3000 tokens) and measure impact on cache hit rates and answer quality
2. Test different similarity thresholds (0.6, 0.7, 0.8, 0.9) to optimize the accuracy-efficiency tradeoff
3. Implement LRU and LFU cache eviction policies to evaluate scalability with larger document collections

## Open Questions the Paper Calls Out
None

## Limitations
- Caching strategy depends on external summarization model, introducing additional error potential
- Fixed 2000-token chunking may not be optimal across all document types and query patterns
- 0.8 similarity threshold appears heuristic without systematic optimization across domains
- Limited analysis of how caching affects actual LLM answer quality beyond retrieval metrics

## Confidence

**High**: Computational efficiency gains (50-60% reduction) are well-supported by experimental results across multiple datasets.

**Medium**: Generalizability across different document types and query distributions, though experiments may not capture full real-world diversity.

**Low**: Long-term scalability with cache management and performance on very large document collections.

## Next Checks
1. Conduct ablation studies varying chunk size, compression ratio, and similarity threshold to identify optimal parameter configurations for different document types.

2. Implement and evaluate cache eviction policies (LRU, LFU, or semantic similarity-based) to assess performance with larger document collections.

3. Perform end-to-end user studies comparing cached versus non-cached systems on actual question-answering tasks, measuring user satisfaction and answer quality across diverse domains.