---
ver: rpa2
title: MLLM Machine Unlearning via Visual Knowledge Distillation
arxiv_id: '2512.11325'
source_url: https://arxiv.org/abs/2512.11325
tags:
- knowledge
- visual
- unlearning
- mllm
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a machine unlearning method for multimodal
  large language models (MLLMs) that selectively removes visual knowledge about a
  target entity while preserving textual knowledge. The key innovation is a Visual
  Knowledge Distillation (VKD) scheme that leverages intermediate visual representations
  as supervision signals during fine-tuning.
---

# MLLM Machine Unlearning via Visual Knowledge Distillation

## Quick Facts
- arXiv ID: 2512.11325
- Source URL: https://arxiv.org/abs/2512.11325
- Authors: Yuhang Wang; Zhenxing Niu; Haoxuan Ji; Guangyu He; Haichang Gao; Gang Hua
- Reference count: 10
- Primary result: Introduces VKD-based method that selectively removes visual knowledge while preserving textual knowledge in MLLMs

## Executive Summary
This paper proposes a machine unlearning method for multimodal large language models (MLLMs) that selectively removes visual knowledge about target entities while preserving textual knowledge. The key innovation is a Visual Knowledge Distillation (VKD) scheme that leverages intermediate visual representations as supervision signals during fine-tuning. By only updating the visual module while keeping the LLM backbone frozen, the approach achieves better unlearning effectiveness and efficiency compared to state-of-the-art methods. On benchmarks like MLLMU-Bench and CLEAR, the method outperforms baselines in forgetting target visual knowledge while maintaining non-target visual and textual knowledge.

## Method Summary
The method operates in two stages: first, a vanilla MLLM is created by fine-tuning on fictitious data containing target knowledge; second, unlearning is applied using Visual Knowledge Distillation (VKD). The approach freezes the LLM backbone and only fine-tunes the visual module (vision encoder + projector), specifically targeting MLP layers. VKD uses the vanilla MLLM as a teacher, distilling its intermediate visual features to the student model on retain-set data. The total loss combines output supervision with VKD loss. Selective forgetting is achieved through neuron pruning based on activation importance and Fisher-masked fine-tuning, where weights are masked based on their sensitivity to forget-set gradients.

## Key Results
- Outperforms state-of-the-art methods on MLLMU-Bench and CLEAR benchmarks for forgetting target visual knowledge
- Maintains non-target visual and textual knowledge more effectively than baseline approaches
- Demonstrates robustness against relearning attacks through improved Accuracy Gap measurements

## Why This Works (Mechanism)
The method works by exploiting the modular architecture of MLLMs, where visual and textual processing are largely separated. By freezing the LLM backbone, textual knowledge is preserved while visual knowledge can be selectively modified. The VKD approach ensures that non-target visual knowledge is retained by using the original model's intermediate representations as supervision. Selective forgetting through neuron pruning and Fisher masking allows targeted removal of target-specific visual features while minimizing impact on general visual capabilities.

## Foundational Learning
- **MLLM Architecture**: Multimodal models combine vision and language processing; needed to understand why visual modules can be fine-tuned independently.
- **Knowledge Distillation**: Transferring knowledge from teacher to student models; needed for the VKD component that preserves non-target knowledge.
- **Fisher Information Masking**: Using Fisher information to identify sensitive parameters; needed for the selective forgetting mechanism.
- **Neuron Importance**: Measuring activation patterns to identify relevant neurons; needed for the pruning component of selective forgetting.

## Architecture Onboarding

### Component Map
Visual Encoder -> Projector -> LLM Backbone -> Output

### Critical Path
Vision encoder → projector layers → LLM frozen backbone → output generation

### Design Tradeoffs
- **Pros**: Lightweight (only visual module fine-tuned), preserves textual knowledge, effective selective forgetting
- **Cons**: Requires synthetic data generation, hyperparameter sensitivity, limited evaluation on real-world scenarios

### Failure Signatures
- Retain VQA accuracy drops sharply: VKD weight β may be too low
- Forget QA accuracy decreases: Masking may be too aggressive or α too high
- Inconsistent forgetting across different target entities: Pruning thresholds may need adjustment

### First Experiments
1. Validate VKD preserves non-target visual knowledge on retain set
2. Test neuron pruning effectiveness on synthetic forgetting tasks
3. Evaluate Fisher masking impact on forgetting vs knowledge retention trade-off

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Effectiveness depends heavily on unprovided hyperparameter choices (d_I, d_F thresholds)
- Assumes clean separation between visual and textual processing pathways
- Evaluation relies on synthetic benchmarks that may not reflect real-world scenarios
- Robustness claims against relearning attacks lack comprehensive methodology details

## Confidence
**High confidence**: Core technical approach (visual fine-tuning with frozen LLM, VKD using intermediate features, selective forgetting via pruning + masking) is clearly specified and reproducible.

**Medium confidence**: Quantitative results on synthetic benchmarks are likely reproducible, but real-world effectiveness may vary.

**Low confidence**: Claims about practical applicability to real-world unlearning scenarios without access to specific architecture details and hyperparameter settings.

## Next Checks
1. Conduct hyperparameter sensitivity analysis across different d_I and d_F thresholds on multiple MLLM architectures
2. Apply method to real user data with actual privacy-sensitive content to verify real-world effectiveness
3. Test robustness against diverse relearning strategies including adversarial examples and prompt injection attacks