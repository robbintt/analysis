---
ver: rpa2
title: Comparison of ConvNeXt and Vision-Language Models for Breast Density Assessment
  in Screening Mammography
arxiv_id: '2506.13964'
source_url: https://arxiv.org/abs/2506.13964
tags:
- density
- breast
- classification
- biomedclip
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compared BioMedCLIP, a vision-language model, and ConvNeXt,
  a convolutional neural network, for breast density classification in mammography.
  Three learning scenarios were evaluated: zero-shot, linear probing, and fine-tuning.'
---

# Comparison of ConvNeXt and Vision-Language Models for Breast Density Assessment in Screening Mammography

## Quick Facts
- **arXiv ID:** 2506.13964
- **Source URL:** https://arxiv.org/abs/2506.13964
- **Reference count:** 25
- **Primary result:** ConvNeXt fine-tuning (accuracy: 0.73, F1: 0.78) outperformed zero-shot BioMedCLIP (accuracy: 0.47, F1: 0.31) and linear probing (accuracy: 0.64, F1: 0.63) for breast density classification

## Executive Summary
This study compares ConvNeXt, a convolutional neural network, with BioMedCLIP, a vision-language model, for breast density classification in mammography. Three learning scenarios were evaluated: zero-shot classification, linear probing, and end-to-end fine-tuning. While ConvNeXt fine-tuned achieved the highest performance metrics, BioMedCLIP showed substantial improvement when transitioned from zero-shot to linear probing. The findings highlight that despite the potential of multimodal learning, CNN-based models with domain-specific fine-tuning currently outperform VLMs in specialized medical imaging tasks. Key challenges included insufficient granularity in textual descriptors for density categories and difficulty distinguishing subtle density differences.

## Method Summary
The study evaluated two model architectures for breast density classification using screening mammography images. ConvNeXt served as the CNN baseline, while BioMedCLIP represented the vision-language model approach. Three distinct training paradigms were implemented: zero-shot classification using pretrained BioMedCLIP without fine-tuning, linear probing where BioMedCLIP's visual encoder was frozen while training a classifier head, and end-to-end fine-tuning where BioMedCLIP's parameters were updated during training. The models were trained and evaluated on a breast density dataset, with performance measured using accuracy and F1-score metrics across different density categories.

## Key Results
- Zero-shot BioMedCLIP classification achieved poor performance (accuracy: 0.47, F1-score: 0.31)
- Linear probing improved BioMedCLIP results significantly (accuracy: 0.64, F1-score: 0.63)
- ConvNeXt fine-tuned end-to-end yielded the best performance (accuracy: 0.73, F1-score: 0.78)
- CNNs with domain-specific fine-tuning outperformed VLMs in specialized medical imaging tasks
- Challenges included insufficient granularity in textual descriptors and difficulty distinguishing subtle density differences

## Why This Works (Mechanism)
CNNs like ConvNeXt excel at breast density classification because they can learn hierarchical spatial features directly from pixel data without relying on textual descriptions. The fine-tuning process allows these models to adapt their learned features to the specific patterns and nuances of breast tissue density variations. Vision-language models struggle because the textual descriptors used for density categories lack the granularity needed to capture subtle differences between density levels, and the visual concepts learned during pretraining may not align well with mammographic features.

## Foundational Learning
- **Breast density classification**: Categorization of breast tissue composition based on glandular and fibrous tissue proportion; needed to establish the clinical task and evaluation criteria
- **Vision-language models**: AI systems that connect visual and textual information; quick check: verify model can process both image and text inputs
- **Zero-shot learning**: Model inference without task-specific training; quick check: ensure no parameter updates during evaluation
- **Linear probing**: Training only the classifier head while freezing the visual encoder; quick check: verify encoder weights remain constant during training
- **Fine-tuning**: Updating all model parameters for a specific task; quick check: monitor parameter changes during training
- **Convolutional neural networks**: Deep learning architectures specialized for grid-like data processing; quick check: confirm proper spatial hierarchy of feature extraction

## Architecture Onboarding

**Component Map:** Image input -> Visual Encoder -> Feature Extraction -> Classifier Head -> Density Category Output

**Critical Path:** Image acquisition → preprocessing → model inference → post-processing → classification output

**Design Tradeoffs:** Vision-language models offer potential for zero-shot inference but suffer from concept misalignment in medical domains, while CNNs require task-specific training but achieve higher accuracy when fine-tuned on domain data.

**Failure Signatures:** Zero-shot VLMs fail due to poor alignment between pretrained visual concepts and medical imaging patterns; linear probing shows limited improvement when textual descriptors lack specificity for subtle density differences.

**First Experiments:**
1. Test zero-shot inference on held-out validation set
2. Implement linear probing with frozen encoder and trainable classifier
3. Conduct end-to-end fine-tuning with learning rate scheduling

## Open Questions the Paper Calls Out
The paper identifies several key open questions for future research: How can textual representations be enhanced to better capture the nuanced differences between breast density categories? What domain-specific adaptations are needed to improve multimodal model performance in medical imaging tasks? Can vision-language models be pretrained on medical imaging data to better align with clinical concepts? How do different vision-language model architectures compare for this specialized task?

## Limitations
- Limited evaluation to only one VLM architecture (BioMedCLIP), leaving open whether other VLMs might perform better
- Single-center dataset restricts generalizability of findings to broader populations
- Relatively modest sample size may affect the reliability of performance metrics
- Potential domain shift between pretraining data and mammography images

## Confidence

**High confidence** in the relative performance ranking between ConvNeXt fine-tuning and other approaches
**Medium confidence** in the absolute performance metrics due to dataset size limitations
**Low confidence** in the generalizability of VLM performance conclusions given single-model evaluation

## Next Checks

1. Evaluate additional VLMs (e.g., BLIP, CLIP variants) to determine if BioMedCLIP's poor performance generalizes across the VLM family
2. Test model performance on multi-center datasets with diverse demographic representation to assess real-world applicability
3. Implement ablation studies on the textual prompt engineering to quantify the impact of different density category descriptions on VLM performance