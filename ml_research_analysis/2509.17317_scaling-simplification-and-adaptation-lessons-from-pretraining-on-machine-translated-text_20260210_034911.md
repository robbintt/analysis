---
ver: rpa2
title: 'Scaling, Simplification, and Adaptation: Lessons from Pretraining on Machine-Translated
  Text'
arxiv_id: '2509.17317'
source_url: https://arxiv.org/abs/2509.17317
tags:
- native
- data
- text
- natural
- simplified
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether machine-translated text can be
  used effectively for pretraining language models in low-resource languages. The
  authors compare GPT-2 models pretrained on native text versus machine-translated
  English into Indonesian and Tamil, including versions with and without source-side
  simplification.
---

# Scaling, Simplification, and Adaptation: Lessons from Pretraining on Machine-Translated Text

## Quick Facts
- arXiv ID: 2509.17317
- Source URL: https://arxiv.org/abs/2509.17317
- Reference count: 40
- Primary result: MT pretraining + CPT outperforms native-only models with less native data

## Executive Summary
This paper investigates whether machine-translated text can effectively pretrain language models for low-resource languages. The authors compare GPT-2 models pretrained on native text versus machine-translated English into Indonesian and Tamil, with and without source-side simplification. They evaluate generalization through language modeling loss, syntactic probing, and downstream tasks. Key findings show that scaling MT-pretrained models improves performance on native text, contrary to concerns about overfitting to translation artifacts. Most importantly, continual pretraining on limited native data significantly boosts performance, often surpassing models trained only on native text, demonstrating that MT pretraining provides a strong starting point for bootstrapping target-language proficiency.

## Method Summary
The authors pretrain GPT-2 models (124M/355M/774M) on three corpora: Native text, Natural MT (translated without simplification), and Simplified MT (simplified then translated). They use English source from Dolma v1.6, FineWeb-Edu, and Wiki-40B, translating to Indonesian/Tamil via OPUS-MT. Simplification uses Llama 3.1 8B INT8. Pretraining uses 50,257-token BPE per language, 1024 context, batch size 384, and learning rate 5e-4. Continual pretraining (CPT) resumes from MT checkpoints with 10x lower learning rate (5e-5) on 1B native tokens (Indonesian) or 2.5B tokens (Tamil). Evaluation includes native text loss, LINDSEA syntax probes, and SEA-HELM NLU tasks after fine-tuning on translated task data.

## Key Results
- Scaling MT-pretrained models improves native text generalization
- Source-side simplification harms generalization due to reduced diversity
- Continual pretraining on native data from MT checkpoints is more data-efficient than native-only training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models pretrained on MT-derived data generalize better to native text rather than merely overfitting to translation artifacts.
- Mechanism: Greater model capacity captures transferable syntactic and structural patterns from MT data that remain valid in native text.
- Core assumption: MT output preserves sufficient target-language syntactic validity for structural learning to transfer.
- Evidence anchors:
  - [abstract] "MT-pretrained models benefit from scaling"
  - [section 5.1] "larger models improve generalization to native text... suggesting that greater capacity captures transferable structure rather than simply memorizing translation artifacts"
- Break condition: Diminishing returns appear at 774M parameters with fixed data budgets.

### Mechanism 2
- Claim: Continual pretraining (CPT) on native data from an MT-pretrained checkpoint is more data-efficient than native-only pretraining.
- Mechanism: MT pretraining provides a structurally informed initialization. CPT then "corrects" translationese artifacts while preserving learned syntax.
- Core assumption: Unlearning translationese patterns is cheaper than learning syntax from scratch.
- Evidence anchors:
  - [abstract] "adapting MT-pretrained models on native text often yields better performance than native-only models, even with less native data"
  - [section 5.3] "Tamil CPT models surpassed native-only models trained on 5B native tokens" with only 2.5B native tokens in CPT
- Break condition: Culturally grounded tasks (toxicity detection) show persistent gaps.

### Mechanism 3
- Claim: Source-side simplification before translation harms transfer to native text.
- Mechanism: Simplification reduces lexical and syntactic diversity, depriving the model of varied linguistic patterns needed for robust generalization.
- Core assumption: Diversity in training data matters more for transfer than raw translation accuracy.
- Evidence anchors:
  - [abstract] "source-side simplification harms generalization to native text"
  - [section 5.2] "Simplified-MT yields worse loss on native text than Natural-MT across all sizes"
- Break condition: If simplification preserved more diversity, outcomes might differ.

## Foundational Learning

- Concept: Continual Pretraining (CPT)
  - Why needed here: Core intervention—resuming training from a checkpoint on different data distribution.
  - Quick check question: Why might CPT be more efficient than training from scratch on the same total data?

- Concept: Translationese
  - Why needed here: MT data contains systematic artifacts that CPT must "unlearn."
  - Quick check question: Name two ways translated text might differ systematically from native text.

- Concept: Data-to-Parameter Scaling Ratio
  - Why needed here: Explains why scaling benefits plateau at 774M with fixed budgets.
  - Quick check question: What happens to generalization when model capacity outpaces data diversity?

## Architecture Onboarding

- Component map:
  - **Pretraining:** GPT-2 decoder (124M/355M/774M), causal LM, 1024-token context, language-specific BPE tokenizer (50,257 tokens)
  - **MT pipeline:** English source → (optional Llama-3.1-8B simplification) → OPUS-MT translation → sentence-level filtering
  - **CPT:** Resume from MT checkpoint, train on native data with 10x lower learning rate (5e-5 vs. 5e-4)
  - **Fine-tuning:** Single linear classification head, pool at final non-padding token

- Critical path:
  1. Build three corpora: Native, Natural-MT, Simplified-MT
  2. Train tokenizer on native corpus only; reuse across all conditions
  3. Pretrain all model sizes on each corpus type
  4. Run CPT: Natural-MT/Simplified-MT checkpoints + native data
  5. Fine-tune on translated task data, evaluate on native benchmarks

- Design tradeoffs:
  - Native-only vs. MT+CPT: Native-only requires more data; MT+CPT is more efficient but weak on cultural nuance
  - Natural-MT vs. Simplified-MT: Natural has better diversity; Simplified may reduce MT errors but hurts transfer
  - Model scale: Larger helps, but fixed data budgets create diminishing returns

- Failure signatures:
  - Toxicity detection: MT-pretrained models lag 3-11 points despite identical fine-tuning data
  - Causal reasoning: All models at chance (50-54%)—task may require reasoning capabilities beyond scale
  - Simplified-MT: Consistently higher loss on native text across all sizes
  - Tamil Small Simplified-MT: Largest syntactic probe gap vs. Natural-MT

- First 3 experiments:
  1. **Replicate scaling curve:** Train 124M and 355M on Natural-MT, measure native-text loss at matching token counts.
  2. **Ablate CPT budget:** Test CPT with 250M/500M/1B native tokens to find minimum effective budget.
  3. **Isolate cultural gap:** Fine-tune on *native* (not translated) toxicity data to distinguish pretraining vs. fine-tuning limitations.

## Open Questions the Paper Calls Out

- Do the scaling benefits of MT-pretraining persist at larger model scales (e.g., 1B+ parameters), or do diminishing returns or overfitting to translationese emerge?
- Can MT-pretrained models be effectively adapted through post-training regimes such as instruction tuning and preference alignment?
- Does source-side text manipulation that preserves linguistic diversity while improving MT ease enhance transfer to native text?
- How does MT quality affect pretraining outcomes, independent of target language properties?

## Limitations

- Cannot fully distinguish between unlearning translationese patterns versus learning new cultural knowledge during CPT
- Scaling benefits plateau at 774M parameters, suggesting data-to-parameter ratio constraints
- Source-side simplification results may not generalize beyond English→target translation direction

## Confidence

**High Confidence:** Scaling MT-pretrained models improves native text generalization; source-side simplification harms generalization.

**Medium Confidence:** Continual pretraining from MT checkpoints is more data-efficient than native-only pretraining; MT pretraining cannot fully substitute for native cultural grounding in specialized tasks.

**Low Confidence:** Simplified-MT would benefit from less aggressive filtering; translationese artifacts are the primary barrier rather than model architecture.

## Next Checks

1. **Mechanism isolation experiment:** Train two MT-pretrained models with identical data and architecture, but one with additional "cultural fine-tuning" on native data specifically for the toxicity task. Compare to CPT baseline to determine whether cultural gaps stem from pretraining initialization or insufficient native adaptation.

2. **Scaling continuation study:** Extend scaling experiments beyond 774M parameters with proportional data increases to identify whether the diminishing returns at current scale are data-limited or architectural.

3. **Simplification diversity ablation:** Systematically vary the degree of simplification while measuring output diversity (vocabulary richness, syntactic complexity) and transfer performance to isolate whether diversity reduction or other simplification artifacts drive the observed harm.