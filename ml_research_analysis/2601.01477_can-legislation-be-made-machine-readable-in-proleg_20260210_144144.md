---
ver: rpa2
title: Can Legislation Be Made Machine-Readable in PROLEG?
arxiv_id: '2601.01477'
source_url: https://arxiv.org/abs/2601.01477
tags:
- legal
- proleg
- rules
- reasoning
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a human-in-the-loop workflow that transforms
  natural-language legal provisions into executable PROLEG rules. It uses a single
  fixed composite LLM prompt to simultaneously generate if-then rules and a corresponding
  PROLEG encoding from GDPR Article 6, followed by expert validation.
---

# Can Legislation Be Made Machine-Readable in PROLEG?

## Quick Facts
- arXiv ID: 2601.01477
- Source URL: https://arxiv.org/abs/2601.01477
- Reference count: 17
- Can a human-in-the-loop workflow transform natural-language legal provisions into executable PROLEG rules?

## Executive Summary
This paper presents a human-in-the-loop workflow that transforms natural-language legal provisions into executable PROLEG rules. It uses a single fixed composite LLM prompt to simultaneously generate if-then rules and a corresponding PROLEG encoding from GDPR Article 6, followed by expert validation. Legal experts reviewed the if-then rules for doctrinal fidelity, while PROLEG specialists verified the logical encoding. The approach successfully produced a machine-readable, executable rule set for Article 6, demonstrating the feasibility of combining LLM generation with expert review to create traceable, auditable legal reasoning systems. The study highlights both the potential and limitations of this method, including issues with rule structure and semantic accuracy, while providing a foundation for extending the approach to other GDPR provisions and regulatory frameworks.

## Method Summary
The study employs a human-in-the-loop workflow using a single LLM prompt with five sequential instruction stages (Chain-of-Instructions) to transform GDPR Article 6 text into if-then rules and initial PROLOG encoding. The prompt identifies relevant Recitals and cross-referenced Articles, synthesizes unified legal rules, normalizes them to if-then structure, and generates PROLOG for conversion to PROLEG. Legal experts review the if-then rules for doctrinal fidelity, while PROLEG specialists verify the logical encoding. The workflow produces an executable PROLEG program that can generate human-readable explanations and reasoning traces.

## Key Results
- Successfully generated executable PROLEG rules from GDPR Article 6 using a single composite LLM prompt
- Demonstrated the necessity of human expert validation for doctrinal fidelity and logical encoding accuracy
- Identified specific structural and semantic issues in LLM-generated legal rules, including inconsistent sub-rule allocation and predicate simplification
- Produced traceable, auditable reasoning through PROLEG's execution traces showing conclusion-condition relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing legal formalization into sequential instruction stages improves output coherence over single-step prompting.
- Mechanism: The Chain-of-Instructions (CoI) prompt forces the model through five explicit stages—(1) identify Recitals, (2) identify cross-referenced Articles, (3) synthesize unified rule, (4) normalize to if-then structure, (5) generate PROLOG—reducing reasoning drift by constraining each transformation step.
- Core assumption: LLMs perform better on decomposed multi-step legal reasoning than end-to-end generation; the model correctly retrieves relevant cross-references.
- Evidence anchors:
  - [abstract]: "a single LLM prompt simultaneously transforms legal text into if-then rules and a corresponding PROLEG encoding"
  - [section 3.2]: "The prompt operates through five internal instruction stages... This stage is critical for maintaining normative fidelity"
  - [corpus]: Limited direct evidence on CoI effectiveness; corpus papers focus on extraction methods (e.g., BERT, embedding benchmarks) rather than prompt decomposition strategies.
- Break condition: Cross-referenced provisions with implicit (non-explicit) legal dependencies may be missed, causing incomplete rule sets.

### Mechanism 2
- Claim: Human-in-the-loop validation is necessary because LLMs systematically produce structural and semantic infidelities in legal rule generation.
- Mechanism: Legal experts review if-then rules for doctrinal fidelity; PROLEG specialists verify logical encoding. This two-layer review catches: (a) inconsistent allocation of information across main rules vs. sub-rules, (b) predicate simplification, (c) unwarranted inferences or scope restrictions.
- Core assumption: Experts can reliably identify infidelities; the review process is tractable for the rule volume produced.
- Evidence anchors:
  - [abstract]: "Legal experts reviewed the if-then rules for doctrinal fidelity, while PROLEG specialists verified the logical encoding"
  - [section 4.1-4.2]: Catalogues specific structural issues (questionable allocation, presupposed clauses) and semantic issues (predicate simplification, adding information not present)
  - [corpus]: Related work on privacy ontologies (PrOnto) assumes expert-driven modeling; corpus does not contradict necessity of expert validation.
- Break condition: Reviewer fatigue or domain expertise gaps may miss subtle semantic drift; does not scale without tooling support.

### Mechanism 3
- Claim: PROLEG's executable formalism provides traceable, inspectable reasoning that surfaces LLM errors for correction.
- Mechanism: PROLEG represents rules as Horn clauses with explicit exceptions, burden of proof, and rule hierarchies. Execution produces visual reasoning traces showing conclusion-condition relations and failure points, enabling targeted debugging of formalization errors.
- Core assumption: The target formalism has sufficient expressiveness for the legal concepts being modeled; conversion from PROLOG to PROLEG preserves semantics.
- Evidence anchors:
  - [abstract]: "final output is an executable PROLEG program that can produce human-readable explanations"
  - [section 2.1]: "PROLEG introduces constructs that capture the nuances of legal argumentation, such as exceptions, burden of proof, and rule hierarchies"
  - [corpus]: Corpus lacks comparative evaluation of PROLEG vs. other formalisms (e.g., LegalRuleML, Akoma Ntoso); generalization to other frameworks is assumed.
- Break condition: Legal concepts exceeding PROLEG's expressiveness (e.g., certain deontic operators, temporal reasoning) will produce incomplete or distorted formalizations.

## Foundational Learning

- Concept: **Horn Clauses and Logic Programming Basics**
  - Why needed here: PROLEG rules are Horn clauses; understanding head-body relationships and unification is required to read, debug, and modify generated code.
  - Quick check question: Given a PROLEG rule `lawful_processing(X) :- consent(X), freely_given(X).`, what happens if `freely_given(X)` is undefined for a case?

- Concept: **Legal Rule Structure (Conditions, Exceptions, Burden of Proof)**
  - Why needed here: The workflow separates main rules from sub-rules and exceptions; misallocation causes structural infidelity. Understanding presupposed vs. explicit conditions prevents redundant encoding.
  - Quick check question: In GDPR Art. 6(1)(a), is "data is being processed" a condition or a presupposition? Why does this distinction matter for rule structure?

- Concept: **GDPR Cross-Reference Network**
  - Why needed here: Article 6 references Articles 4, 7, 8, 9, 13, 14, and various Recitals; the CoI prompt retrieves these to ground interpretation.
  - Quick check question: Which Recitals provide interpretive context for consent withdrawal under Art. 7.3, and how would omitting them affect the generated sub-rules?

## Architecture Onboarding

- Component map: Article text → CoI prompt → if-then rules → legal review → PROLEG encoding → specialist review → test case execution → reasoning trace analysis

- Critical path: Article text → CoI prompt → if-then rules → legal review → PROLEG encoding → specialist review → test case execution → reasoning trace analysis

- Design tradeoffs:
  - Single composite prompt vs. multi-agent modular pipeline (current: single prompt; tradeoff: simplicity vs. interpretability/debuggability)
  - PROLOG generation then conversion vs. direct PROLEG (current: PROLOG-first for LLM familiarity; tradeoff: higher initial accuracy vs. potential conversion errors)
  - Structural resemblance assumption (one Article per rule set vs. multi-Article integration; tradeoff: simplicity vs. completeness)

- Failure signatures:
  - **Structural**: Inconsistent sub-rule allocation (e.g., Art. 7.2 conditions omitted while 7.3/7.4 included)
  - **Semantic**: Predicate scope restriction (e.g., "vital interest" narrowed to "life or physical integrity" only)
  - **Cross-reference**: Contextually essential but non-explicitly linked Articles omitted (e.g., Art. 22, Art. 49 for consent contexts)
  - **Reproducibility**: Slight variation in retrieved Recitals across runs with identical prompt

- First 3 experiments:
  1. **Baseline reproducibility test**: Run the fixed CoI prompt on Art. 6(1)(a) three times with identical input; catalog differences in retrieved cross-references and generated rule structure.
  2. **Ablation study**: Remove each CoI stage sequentially (stages 1-2, then stage 3) to measure impact on cross-reference completeness and rule fidelity; document which stages are most critical.
  3. **Cross-article transfer**: Apply the identical workflow to Art. 5 (principles) or Art. 7 (conditions for consent); compare structural/semantic issue frequency to Art. 6 results to assess generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the fixed composite prompt approach generalize to other GDPR provisions (e.g., Articles 7–9, 13–22) and to different regulatory frameworks beyond GDPR without extensive prompt re-engineering?
- Basis in paper: [explicit] "Future work will extend the methodology to multiple LLMs, additional GDPR provisions, and larger, more heterogeneous test sets to systematically assess robustness and generalizability."
- Why unresolved: The study deliberately limited scope to Article 6 only; the dense cross-reference network (Figure 1) suggests other provisions may exhibit different formalization challenges.
- What evidence would resolve it: Successful replication of the workflow on at least 3–5 additional GDPR articles and one non-EU regulatory framework, with comparable structural/semantic error rates.

### Open Question 2
- Question: Would multi-agent or modular LLM architectures—where specialized sub-models handle interpretation, logical structuring, and PROLEG encoding separately—outperform the single-model chain-of-instructions approach in doctrinal fidelity?
- Basis in paper: [explicit] "Future work could explore multi-agent or modular architectures in which specialized sub-models independently address interpretive, logical, and representational tasks."
- Why unresolved: The current study used only a single custom ChatGPT-5 configuration; the trade-off between integrated reasoning and task separation remains untested.
- What evidence would resolve it: A controlled comparison measuring structural/semantic error rates between single-model and multi-agent pipelines on identical provisions.

### Open Question 3
- Question: Can prompt modifications (e.g., explicit instructions against paraphrasing) systematically reduce the structural and semantic issues identified—particularly questionable information allocation and predicate simplification—without introducing new errors?
- Basis in paper: [explicit] "Future work will improve the prompts, for example, by explicitly instructing the model not to paraphrase the wording of GDPR Articles and Recitals, because such paraphrasing introduced additional ambiguity."
- Why unresolved: The current prompt was held static to observe baseline behavior; targeted interventions have not yet been tested.
- What evidence would resolve it: Ablation studies comparing outputs from prompts with and without anti-paraphrasing instructions, evaluated by legal experts using the same typology of issues.

### Open Question 4
- Question: How can LLM-based formalization pipelines reliably capture cross-contextual dependencies that are semantically but not syntactically connected (e.g., omitted Articles 49(1)(a), 22(1)–(2) when processing Article 6(1)(a))?
- Basis in paper: [inferred] The methodology limitations section notes the model "struggles to capture cross-contextual dependencies that are less syntactically but more semantically connected," and this was identified as a key limitation.
- Why unresolved: Current retrieval relies on explicit textual cues; semantic relatedness without syntactic markers remains an open challenge for LLMs.
- What evidence would resolve it: Quantitative measurement of retrieval completeness for semantically-linked provisions across multiple articles, compared against expert-annotated ground truth.

## Limitations
- The exact CoI prompt implementation is not provided in the paper, only referenced via external repository, creating a reproducibility barrier.
- The study's scope is limited to a single GDPR article, raising questions about generalizability to more complex multi-article provisions or different regulatory domains.
- The conversion process from PROLOG to PROLEG is not detailed, leaving open questions about potential semantic drift during transformation.

## Confidence
- **High confidence** in the core claim that human-in-the-loop validation is necessary for legal LLM outputs, supported by specific error catalogs and two-layer expert review process.
- **Medium confidence** in the effectiveness of the CoI prompt decomposition strategy, as limited direct evidence is provided beyond the single case study.
- **Low confidence** in the PROLEG formalism's expressiveness claims without comparative evaluation against other legal rule formalisms.

## Next Checks
1. **Reproduce the full workflow** using the GitHub-provided prompt on Article 6 to verify structural and semantic fidelity claims, documenting any variations in cross-reference retrieval or rule generation.
2. **Conduct an ablation study** of the CoI prompt stages to quantify the contribution of each stage to cross-reference completeness and doctrinal accuracy.
3. **Extend the approach** to Article 5 or 7 to test generalization beyond Article 6, comparing structural and semantic error rates across different GDPR provisions.