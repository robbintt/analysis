---
ver: rpa2
title: 'From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality'
arxiv_id: '2601.02224'
source_url: https://arxiv.org/abs/2601.02224
tags:
- relevancy
- table
- effect
- sarimax
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present a systematic factorial study investigating how Forecasting
  model choice, XAI method, LLM selection, and prompting strategy affect the quality
  of LLM-generated natural language explanations (NLEs) for time-series forecasting.
  Our design spans four models (XGBoost, Random Forest, Multilayer Perceptron, and
  SARIMAX), three XAI conditions (SHAP, LIME, and no-XAI baseline), three LLMs (GPT-4o,
  Llama-3-8B, DeepSeek-R1), and eight prompting strategies.
---

# From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality

## Quick Facts
- arXiv ID: 2601.02224
- Source URL: https://arxiv.org/abs/2601.02224
- Reference count: 40
- One-line primary result: LLM choice dominates all other factors in explaining time-series forecasting, with DeepSeek-R1 outperforming GPT-4o and zero-shot prompting competitive with self-consistency at 7× lower cost

## Executive Summary
This factorial study systematically investigates how forecasting model choice, XAI method, LLM selection, and prompting strategy affect the quality of LLM-generated natural language explanations (NLEs) for time-series forecasting. Using a 4×3×3×8 factorial design with 660 explanations evaluated by dual LLM judges, the study reveals that LLM choice explains 25× more variance in explanation quality than XAI method selection. DeepSeek-R1 emerges as the top performer, while zero-shot prompting achieves near-equivalent quality to complex strategies at a fraction of the cost. The study also identifies an "interpretability paradox" where classical statistical models yield lower NLE quality than black-box ML models despite higher prediction accuracy.

## Method Summary
The study employs a factorial design spanning four forecasting models (XGBoost, Random Forest, Multilayer Perceptron, and SARIMAX), three XAI conditions (SHAP, LIME, no-XAI baseline), three LLMs (GPT-4o, Llama-3-8B, DeepSeek-R1), and eight prompting strategies. Using the UCI Individual Household Electric Power Consumption dataset, the researchers generate 660 NLEs and evaluate them with G-Eval using dual LLM judges (GPT-4, DeepSeek-R1) across four criteria: Accuracy, Lay Relevancy, Expert Relevancy, and Helpfulness. The evaluation employs ANOVA with ω² effect sizes to quantify the variance explained by each factor.

## Key Results
- LLM choice dominates all other factors, with DeepSeek-R1 outperforming GPT-4o and Llama-3 (ω² = .50 for LLM vs. ω² = .02 for XAI)
- Zero-shot prompting achieves competitive quality with self-consistency at 7× lower cost (4.44 vs. 4.35 average G-Eval score)
- Interpretability paradox: SARIMAX yielded lower NLE quality than ML models despite higher prediction accuracy (d = -0.98 vs. XGBoost on Lay Relevancy)
- Chain-of-thought prompting hurts rather than helps (meta-prompting: 4.01 avg vs. zero-shot: 4.35)
- XAI provides only small improvements over no-XAI baseline, and only for expert audiences (ω² = .02)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM translation capability dominates XAI method quality for producing natural language explanations.
- Mechanism: The LLM's ability to synthesize numerical attributions into coherent narratives explains 25× more variance in NLE quality than XAI method choice (ω² = .50 for LLM vs. ω² = .02 for XAI on Expert Relevancy).
- Core assumption: The G-Eval evaluation framework accurately captures human-perceived explanation quality across lay and expert audiences.
- Evidence anchors:
  - "LLM choice dominates all other factors, with DeepSeek-R1 outperforming GPT-4o and Llama-3"
  - "Expert Relevancy... is overwhelmingly determined by LLM capability (ω² = .50). XAI shows a statistically significant but small effect (ω² = .02): including feature attributions provides marginally more technical content, but the LLM's ability to present information coherently explains 25× more variance."
  - Weak corpus support—related papers focus on XAI method improvements rather than LLM-translation bottlenecks.
- Break condition: If domain-specific terminology or multimodal data requires specialized XAI outputs beyond tabular feature attributions, the LLM translation bottleneck may shift.

### Mechanism 2
- Claim: Zero-shot prompting achieves near-equivalent NLE quality to complex strategies at fraction of the cost.
- Mechanism: Self-consistency consumes 7× more tokens than zero-shot for only 0.09 points improvement (4.44 vs. 4.35 average G-Eval score), suggesting complex reasoning chains add verbosity without proportional quality gains in explanation tasks.
- Core assumption: Cost-quality tradeoffs generalize beyond household energy forecasting to other time-series domains.
- Evidence anchors:
  - "zero-shot prompting is competitive with self-consistency at 7× lower cost"
  - "Self-consistency consumes 7× more tokens than zero-shot and requires 10× longer generation time for only 0.09 points improvement"
  - No direct corpus evidence on zero-shot vs. self-consistency cost-effectiveness for XAI translation tasks.
- Break condition: If explanations require multi-step causal reasoning (e.g., counterfactuals, temporal chains), complex strategies may outperform zero-shot.

### Mechanism 3
- Claim: Classical statistical models (SARIMAX) yield lower NLE quality than black-box ML models despite higher prediction accuracy and inherent interpretability.
- Mechanism: LLMs may struggle to translate statistical coefficient notation (AR, MA, seasonal terms) into accessible narratives; feature-importance language ("temperature contributed +0.3") may be more common in training data than statistical notation.
- Core assumption: The interpretability paradox stems from LLM training distribution rather than fundamental differences in model-type explainability.
- Evidence anchors:
  - "we observe an interpretability paradox: SARIMAX yielded lower NLE quality than ML models despite higher prediction accuracy"
  - "feature-importance language ('temperature contributed +0.3') may be more common than statistical coefficient notation (AR, MA, seasonal terms)"
  - No corpus papers report this paradox; related work assumes classical models are inherently more interpretable.
- Break condition: If fine-tuning LLMs on statistical content or improved prompting for coefficient translation closes the gap, this becomes an engineering gap rather than a fundamental limitation.

## Foundational Learning

- Concept: Factorial Experimental Design (DOE)
  - Why needed here: The paper uses a 4×3×3×8 factorial design to isolate effects of each factor on NLE quality while controlling for confounds.
  - Quick check question: Can you explain why a factorial design enables detecting interaction effects (e.g., XAI×LLM) that single-factor experiments cannot?

- Concept: Effect Sizes (ω², Cohen's d)
  - Why needed here: Statistical significance alone doesn't indicate practical importance; ω² quantifies variance explained, enabling comparison across factors.
  - Quick check question: If XAI has ω² = .02 and LLM has ω² = .50 on Expert Relevancy, what does this mean for where to invest engineering effort?

- Concept: LLM-as-Judge Evaluation (G-Eval)
  - Why needed here: Traditional metrics (BLEU, ROUGE) correlate poorly with human judgment for open-ended text; G-Eval uses chain-of-thought evaluation with probability-weighted scoring.
  - Quick check question: What biases must be mitigated when using LLMs to evaluate other LLMs' outputs (e.g., self-preference, verbosity bias)?

## Architecture Onboarding

- Component map: Prediction Layer (Forecasting models) → Attribution Layer (XAI methods) → Translation Layer (LLM + prompting) → Evaluation Layer (G-Eval with dual judges)

- Critical path: LLM choice → Prompting strategy → NLE quality. The attribution layer is optional (no-XAI baseline performs competitively for lay audiences).

- Design tradeoffs:
  - Quality vs. Cost: DeepSeek-R1 produces best explanations at 4× token cost of GPT-4o; zero-shot offers 7× cost savings over self-consistency with marginal quality loss.
  - Audience targeting: XAI only helps Expert Relevancy (ω² = .02); lay users gain nothing from feature attributions.
  - Model type vs. explainability: Classical models (SARIMAX) paradoxically yield worse NLEs despite inherent interpretability.

- Failure signatures:
  - Chain-of-thought prompting degrades performance (meta-prompting: 4.01 avg, CoT-few: 4.01 avg vs. zero-shot: 4.35).
  - Llama-3 shows no XAI benefit (+0.0 points on Expert Relevancy), suggesting weaker models cannot leverage feature attributions.
  - SARIMAX explanations underperform even lower-accuracy ML models (d = -0.98 vs. XGBoost on Lay Relevancy).

- First 3 experiments:
  1. Replicate the interpretability paradox with another classical model (e.g., VAR, exponential smoothing) to test whether the finding generalizes or is SARIMAX-specific.
  2. Ablate the XAI layer entirely: compare no-XAI baseline against SHAP/LIME for a lay-audience-only evaluation to confirm the ω² = .02 effect is negligible for non-experts.
  3. Fine-tune a smaller model (Llama-3-8B) on statistical coefficient narratives to test whether the SARIMAX gap is addressable through training data rather than prompting.

## Open Questions the Paper Calls Out

- Does the "interpretability paradox"—where interpretable models yield lower NLE quality than black-box models—stem from LLM training data biases favoring feature-importance language over statistical coefficient notation?
  - Basis in paper: Section 5.3 ("We can only hypothesize... It would need to be tested")
  - Why unresolved: The study observed the effect but did not isolate whether the cause is data distribution or the linguistic complexity of statistical terms.
  - What evidence would resolve it: Fine-tuning LLMs on statistical content to observe if the performance gap closes.

- Does the higher NLE quality observed with reasoning-focused LLMs (like DeepSeek-R1) translate into improved human decision-making or task performance?
  - Basis in paper: Limitations ("We measure explanation quality but not utility... whether users make better decisions with them remains untested")
  - Why unresolved: Evaluation was conducted via G-Eval (LLM judges) rather than human annotators or user studies.
  - What evidence would resolve it: A human-subject experiment measuring decision accuracy and reliance on the generated explanations.

- Does the interpretability paradox persist when applied to other classical time-series models (e.g., VAR, exponential smoothing) or non-tabular domains?
  - Basis in paper: Limitations ("The interpretability paradox warrants replication with additional classical models... [and] different data modalities")
  - Why unresolved: The study was limited to one classical model (SARIMAX) and tabular data.
  - What evidence would resolve it: Replicating the factorial design with Vector Autoregression models or image classification tasks.

## Limitations
- The interpretability paradox finding hinges on statistical coefficient translation rather than fundamental model-type differences, but this assumes LLM training distribution explains the gap
- The G-Eval framework may introduce systematic biases when LLMs evaluate other LLMs, particularly regarding verbosity preferences and self-preference effects
- The factorial design's unequal sample sizes require non-standard ANOVA approaches that may affect effect size estimates

## Confidence
- High: LLM choice dominates XAI method (ω² = .50 vs .02); zero-shot competitive with self-consistency (7× cost reduction, marginal quality loss)
- Medium: Interpretability paradox (requires testing with additional classical models to confirm generalizability)
- Low: SARIMAX gap addressable through fine-tuning (unproven hypothesis about training data limitations)

## Next Checks
1. Replicate the interpretability paradox with additional classical models (VAR, exponential smoothing) to test whether the finding generalizes beyond SARIMAX
2. Conduct ablation studies removing XAI entirely for lay-audience evaluations to confirm ω² = .02 effect is negligible for non-experts
3. Implement fine-tuning experiments for smaller models (Llama-3-8B) on statistical coefficient narratives to test whether the SARIMAX gap stems from training data limitations