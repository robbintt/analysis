---
ver: rpa2
title: Modelling Intertextuality with N-gram Embeddings
arxiv_id: '2509.06637'
source_url: https://arxiv.org/abs/2509.06637
tags:
- intertextuality
- https
- texts
- word
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new quantitative model of intertextuality
  using n-gram embeddings. The method computes pairwise similarities between n-gram
  embeddings from two texts and averages them as an overall intertextuality score.
---

# Modelling Intertextuality with N-gram Embeddings

## Quick Facts
- arXiv ID: 2509.06637
- Source URL: https://arxiv.org/abs/2509.06637
- Reference count: 0
- Primary result: New quantitative model of intertextuality using n-gram embeddings validated on four texts with known degrees of intertextuality and 267 diverse texts

## Executive Summary
This paper introduces a novel quantitative approach to measuring intertextuality by computing pairwise similarities between n-gram embeddings from two texts and averaging them as an overall score. The method employs a historical text normalization pipeline to enable diachronic analysis spanning 16th-20th century English. Validation on four texts with known intertextual relationships and network analysis on 267 diverse texts demonstrates the method's effectiveness in capturing and quantifying intertextual relationships.

## Method Summary
The method extracts n-grams from two texts, embeds each n-gram by summing constituent word vectors, computes all pairwise cosine similarities, filters by threshold τ, and averages the surviving scores as the intertextuality measure. A three-stage historical text normalization pipeline (contraction expansion, spelling modernization via 19 EModE rules + 7 American→British rules, lemmatization) enables diachronic analysis. Word embeddings are trained using CBOW with 350 dimensions and window size 3. Recommended parameters are n=3 and τ=0.2.

## Key Results
- Method successfully reproduces known intertextual relationships in validation set (same-author novels score highest)
- Network analysis on 267 texts reveals centrality and community structures
- Computational efficiency enables scalable analysis of intertextual relationships in large corpora
- Thesaurus graph hit score validates 350-dimensional embeddings as optimal choice

## Why This Works (Mechanism)

### Mechanism 1
Averaging pairwise cosine similarities between n-gram embeddings from two texts produces a global intertextuality score that aligns with human literary interpretation. For texts A and B, extract all n-grams, embed each as the sum of constituent word vectors, compute all pairwise cosine similarities, filter by threshold τ, then average the surviving scores. Global intertextuality emerges from aggregating local semantic similarities.

### Mechanism 2
Threshold filtering is necessary because most n-gram pairs are semantically orthogonal and contribute noise rather than signal. Only pairs where |cosine(a,b)| > τ contribute to the final score; τ controls sensitivity to distant resonances vs. near-quotations. Meaningless comparisons produce orthogonal vectors.

### Mechanism 3
A rule-based historical text normalization pipeline enables diachronic word embeddings covering 16th-20th century English. Three-stage pipeline includes contraction expansion, spelling modernization via 19 EModE rules + 7 American→British rules, and lemmatization with NLTK/Hunspell. Historical spelling variants and grammatical inflections introduce noise that obscures semantic relationships.

## Foundational Learning

- **Distributional Semantics & Cosine Similarity**: The method assumes semantically related words/n-grams have similar vector representations; cosine measures angular similarity regardless of magnitude. Quick check: Why might cosine similarity be preferred over Euclidean distance for comparing word embeddings of differing frequencies?

- **N-gram Extraction with Overlapping Windows**: The paper uses sliding-window n-grams (e.g., "gather ye rosebuds," "ye rosebuds while") rather than non-overlapping chunks. Quick check: For a 10-word sentence, how many trigrams does sliding extraction produce vs. non-overlapping extraction?

- **Graph Centrality & Community Detection (PageRank, Louvain)**: Intertextuality scores become edge weights in a document network; centrality identifies influential texts, Louvain detects thematic communities. Quick check: In a directed citation network, what does high PageRank indicate vs. high out-degree?

## Architecture Onboarding

- **Component map**: Preprocessing pipeline (contraction expansion → spelling normalization → lemmatization) → N-gram extractor (sliding window, stop-list filtering) → Word embedding model (CBOW, dim=350, window=3) → N-gram embedder (sum constituent word vectors) → Pairwise similarity engine (vectorized matrix multiply) → Threshold filter & aggregator → Network builder (NetworkX) → Centrality/community analyzers (PageRank, Louvain)

- **Critical path**: Train/fine-tune embeddings on normalized corpus → Preprocess target texts → Extract n-grams → Compute pairwise similarity matrices → Filter & aggregate → Build network → Run graph algorithms

- **Design tradeoffs**: n-gram size (n): Higher n captures more specific phrases but increases O(m²) comparisons; paper uses 3-6. Threshold τ: Low captures distant resonance but adds noise; high captures only near-quotations. Embedding dimension: Higher improves thesaurus-graph hit rate but increases memory/compute. Static vs. contextual embeddings: Word2Vec is fast but loses word order.

- **Failure signatures**: Returns -1: No pairs above threshold. Scores overly compressed: τ too high. Noisy/erratic rankings: τ too low. OOM on large documents: Dense matrix doesn't fit; must chunk into sub-documents. Missing archaic vocabulary: Embedding model lacks coverage.

- **First 3 experiments**: Replicate 4-text validation (Herbert/Donne/Brontë×2) to verify relative ordering matches Table 1. Threshold sensitivity sweep (τ ∈ {0.01, 0.05, 0.10, 0.20, 0.50}) on same 4 texts to observe score compression. Small-scale network test (20-30 documents, mixed genres) with Louvain to verify genre/thematic clustering and authorial grouping.

## Open Questions the Paper Calls Out

### Open Question 1
Can a quantitative metric of "evocativeness" be developed to weight specific n-grams during intertextuality calculation? The current model averages similarities uniformly, potentially diluting the signal from high-importance allusions with noise from common phrases.

### Open Question 2
How can semantically meaningless pairwise n-gram comparisons be filtered out before computing similarity? The current algorithm requires calculating a dense similarity matrix for all pairs, which becomes a bottleneck for very large documents.

### Open Question 3
To what extent does the "bag-of-words-in-window" embedding approach fail to capture syntactic intertextuality due to discarding word order? The approach assumes that semantic composition is commutative, potentially missing allusions that rely on specific phrasal structures or syntax.

## Limitations
- Historical normalization fragility: Rule-based spelling modernization may fail on unseen archaic forms, limiting diachronic coverage
- Threshold sensitivity ambiguity: Critical hyperparameter sensitivity not fully characterized across diverse genres
- Semantic scope constraints: May miss structural or syntactic intertextual patterns that don't manifest as direct n-gram similarity

## Confidence

- **High Confidence**: The computational framework is clearly specified and mathematically sound; ability to reproduce known literary relationships is well-demonstrated
- **Medium Confidence**: Historical text normalization pipeline's effectiveness is asserted but relies on relatively simple rule-based approach without extensive validation
- **Low Confidence**: Interpretation of threshold sensitivity (τ=0.2 as optimal) is based on limited validation; trade-off between capturing distant resonances vs. noise requires more systematic exploration

## Next Checks

1. **Threshold Sensitivity Sweep**: Systematically vary τ (0.01, 0.05, 0.10, 0.20, 0.50) on the 4-text validation set and 267-text network to quantify score compression and clustering stability

2. **Historical Normalization Robustness**: Test the normalization pipeline on a held-out historical text with known modern equivalents to measure OOV rates and semantic drift before/after processing

3. **Structural Pattern Detection**: Compare n-gram embedding scores against a small set of texts with known structural intertextuality (paraphrases, allusions) to assess whether the method captures these non-local relationships