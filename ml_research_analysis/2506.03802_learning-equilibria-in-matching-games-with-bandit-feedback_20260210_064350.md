---
ver: rpa2
title: Learning Equilibria in Matching Games with Bandit Feedback
arxiv_id: '2506.03802'
source_url: https://arxiv.org/abs/2506.03802
tags:
- agents
- matching
- agent
- where
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies a matching market where agents interact via zero-sum
  games with unknown payoff matrices and aims to learn a stable matching equilibrium
  using bandit feedback. It proposes a UCB-style algorithm where agents form preferences
  and select strategies based on optimistic estimates of game payoffs, and a central
  platform matches agents according to these preferences.
---

# Learning Equilibria in Matching Games with Bandit Feedback

## Quick Facts
- **arXiv ID**: 2506.03802
- **Source URL**: https://arxiv.org/abs/2506.03802
- **Reference count**: 40
- **Primary result**: Achieves $\tilde{O}(\sqrt{T m k p a})$ regret in matching markets with bandit feedback

## Executive Summary
This paper proposes a UCB algorithm for learning stable matching equilibria in two-sided markets where matched agents play zero-sum games with unknown payoff matrices. The central platform matches agents based on optimistic estimates of game payoffs, and agents play min-max strategies computed from these estimates. The algorithm achieves sublinear, instance-independent regret that generalizes both classical matching (when $m=k=1$) and two-player game learning (when $p=a=1$).

## Method Summary
The method combines optimistic exploration (UCB) for payoff learning with deferred acceptance matching. Agents maintain UCB estimates of payoff matrices, compute game values via minimax, and report preferences to a central platform. The platform runs deferred acceptance to produce a matching, then agents play min-max strategies based on optimistic payoffs. The cumulative regret is measured as matching instability - the minimum subsidies needed to stabilize the market.

## Key Results
- Achieves $\tilde{O}(\sqrt{T m k p a})$ regret bound for matching markets with bandit feedback
- Generalizes existing bounds for both classical two-sided matching and learning in zero-sum games
- Experiments show sublinear regret growth across different settings, with better performance when one side knows true payoffs
- Regret bound is instance-independent, unlike some game-theoretic approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Optimism in the face of uncertainty enables efficient exploration of both game payoffs and matching preferences.
- **Mechanism**: Each agent maintains upper confidence bounds (UCB) for payoff matrix entries. From these, agents compute optimistic game values via minimax, which drive preference ranking. The central platform runs Deferred Acceptance on these preferences to produce a matching. Because UCB estimates are optimistic, the algorithm systematically over-explores potentially high-value pairings until confidence intervals tighten.
- **Core assumption**: Payoff entries are 1-sub-Gaussian and entries within confidence bounds with high probability (event $\mathcal{E}_t$).
- **Evidence anchors**:
  - [abstract] "We propose a UCB algorithm in which agents form preferences and select actions based on optimistic estimates of the game payoffs"
  - [section 4] Lines 6-10: Upper confidence computation and preference formation via $\hat{\pi}_a = \text{arg sort}_i \bar{V}_{a,i}$
  - [corpus] Related work "Two-Player Zero-Sum Games with Bandit Feedback" confirms UCB optimism principle in similar settings
- **Break condition**: If payoff distributions have heavy tails (not sub-Gaussian), concentration bounds fail and regret may not be sublinear.

### Mechanism 2
- **Claim**: Matching instability provides a valid, interpretable regret measure for the coupled learning problem.
- **Mechanism**: Matching instability $\text{MI}(m, X)$ is defined as the minimum subsidies $s_a \geq 0$ required to make $(m, X)$ satisfy individual rationality, Nash rationality, and no-blocking-pair conditions. Cumulative instability $\sum_t \text{MI}(m_t, X_t)$ measures total deviation from equilibrium over time. Proposition 1 establishes MI=0 iff $(m, X)$ is a matching equilibrium.
- **Core assumption**: Utilities for remaining unmatched $U_{a,\perp}$ are known (Assumption A1).
- **Evidence anchors**:
  - [section 3.2] Definition 4: Linear program defining matching instability via subsidy minimization
  - [section 3.2] Proposition 1: Properties connecting MI to equilibrium conditions
  - [corpus] No direct corpus comparison; this appears to be a novel extension of Subset Instability from [25,26]
- **Break condition**: If agents' outside options $U_{a,\perp}$ change over time or are misspecified, subsidies no longer correctly capture deviation from equilibrium.

### Mechanism 3
- **Claim**: Independent min-max strategy selection by matched pairs implicitly coordinates toward equilibrium play.
- **Mechanism**: Once matched, each agent $a$ computes $x_a = \arg\max_x \min_y x^T \bar{A}_{a,m(a)} y$ independently using optimistic payoffs. Proposition 2 (S1) shows that under optimistic estimates, $\bar{V}^*_{a,m(a)} - \bar{U}_{a,m(a)}(x_a, x_{m(a)}) \leq 0$, meaning selected strategies achieve at least the optimistic game value. As confidence intervals shrink, optimistic values converge to true values, and strategies converge toward Nash equilibrium of the true game.
- **Core assumption**: Each agent observes their match's action (bandit feedback with opponent action observation).
- **Evidence anchors**:
  - [section 4] Lines 12-14: Min-max strategy computation and action sampling
  - [appendix C, Proposition 2] S1 proves $\bar{V}^*_{a,m(a)} - \bar{U}_{a,m(a)}(x_a, x_{m(a)}) \leq 0$
  - [corpus] "Instance-Dependent Regret Bounds for Learning Two-Player Zero-Sum Games with Bandit Feedback" studies related opponent-action-observation setting
- **Break condition**: If agents cannot observe opponent actions (uncoupled dynamics), they cannot update per-entry payoff estimates and the mechanism fails.

## Foundational Learning

- **Concept**: Nash Equilibrium in Zero-Sum Games
  - Why needed here: The matching equilibrium definition requires matched pairs to play Nash strategies within their games. You must understand minimax computation ($x^* = \arg\max_x \min_y x^T A y$) to implement agent strategy selection.
  - Quick check question: Given a 2×2 payoff matrix $A = [[1, -1], [-1, 1]]$, can you compute the mixed-strategy Nash equilibrium and game value?

- **Concept**: Deferred Acceptance Algorithm
  - Why needed here: The central platform uses DA to compute stable matchings from agent preferences. The algorithm's properties (proposer-optimality, stability guarantee) are essential for understanding why the matching step produces stable outcomes under current preferences.
  - Quick check question: Run DA with two proposers $p_1, p_2$ and two receivers $a_1, a_2$, where $p_1$ prefers $a_1 > a_2$, $p_2$ prefers $a_1 > a_2$, and receivers both prefer $p_1 > p_2$. What matching results?

- **Concept**: Upper Confidence Bound (UCB) for Bandits
  - Why needed here: The algorithm extends UCB from single-armed bandits to matrix games. You must understand why $\bar{A}(i,j) = \hat{A}(i,j) + \sqrt{2\log(1/\delta)/n(i,j)}$ balances exploration-exploitation and how optimism drives efficient learning.
  - Quick check question: Why does UCB explore actions with high uncertainty even if their empirical mean is currently low?

## Architecture Onboarding

- **Component map**:
  Agents (P ∪ A) -> Payoff estimate matrix Â_{a,a'} for each opponent -> Visit counter n_{a,a'}(i,j) for each action pair -> UCB computation: Ā = Â + confidence bonus -> Preference formation: rank by game value V* from Ā -> Central Platform -> Deferred Acceptance on reported preferences → m_t -> Per-Timestep Loop (repeated T times) -> Agents compute UCB payoffs and game values -> Agents report preferences to platform -> Platform runs DA, returns matching m_t -> Matched pairs compute min-max strategies from Ā -> Agents sample actions, observe opponent action -> Agents receive reward, update Â and counters

- **Critical path**:
  1. **UCB bonus computation** (Lines 5-6): Must use correct δ = 1/(4T²p²a²mk) to ensure high-probability concentration
  2. **Minimax solver**: Each agent needs LP solver or iterative method to compute game values and strategies from Ā
  3. **DA implementation**: Must handle incomplete preference lists and ensure proposer-optimality
  4. **Update logic**: Only update entries for actually-sampled (i,j) pairs

- **Design tradeoffs**:
  - **Centralized vs. decentralized**: Paper assumes central platform; decentralized requires each agent to propose/accept independently (future work per Section 7)
  - **UCB vs. Thompson Sampling**: UCB chosen for tractable analysis; TS may perform better empirically but lacks guarantees here
  - **Full preference reporting vs. top-k**: Paper assumes complete preference lists; partial reporting reduces communication but may yield suboptimal matchings

- **Failure signatures**:
  - **Linear regret growth**: If regret scales as O(T) rather than O(√T), check: (a) δ parameter too small, (b) payoff distribution non-sub-Gaussian, (c) opponent action not observed
  - **Empty or trivial matchings**: If most agents unmatched, check preference formation—game values may all be negative relative to $U_{a,\perp}$
  - **Strategy non-convergence**: If strategies oscillate, check that minimax solver is deterministic and UCB estimates are monotone in confidence bonus

- **First 3 experiments**:
  1. **Single-pair baseline** (p=a=1, m=k=2): Validate against known ZSG UCB bounds; regret should match O(√(Tmk)) from O'Donoghue et al. [36]
  2. **Varying market size** (p=a ∈ {2,4,8}, m=k=2): Confirm regret scales as O(√(pa)) for fixed T; plot cumulative regret vs. √(pa)
  3. **Nash-response vs. self-play** (Figure 1 reproduction): Run with one side knowing true payoffs; verify lower regret than full self-play, confirming information asymmetry benefit

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can efficient learning guarantees be established for matching markets where agents play general-sum games rather than zero-sum games?
- **Basis in paper**: [explicit] The Discussion section states that "Extending the framework to general-sum games... introduces further challenges" regarding approximate equilibrium notions.
- **Why unresolved**: The current algorithm exploits the unique value property of zero-sum games to form preferences; general-sum games lack this property, complicating the definition and computation of stable preferences.
- **What evidence would resolve it**: An algorithm with provable sublinear regret bounds for matching markets with general-sum games, or a proof of computational hardness.

### Open Question 2
- **Question**: Is it possible to learn matching equilibria in decentralized settings where agents cannot observe the actions of their match (uncoupled dynamics)?
- **Basis in paper**: [explicit] The authors list "scenarios with uncoupled dynamics, where agents cannot observe the actions of their match, as well as decentralized settings" as future research directions.
- **Why unresolved**: The current UCB-MG algorithm requires agents to observe their partner's action $i_{m(a)}$ to update empirical payoff matrices $\hat{A}$, an assumption that may not hold in decentralized environments.
- **What evidence would resolve it**: A modified algorithm achieving sublinear regret without access to opponent action information, or lower bounds establishing the necessity of this feedback.

### Open Question 3
- **Question**: Can the learning objective be modified to guarantee per-agent fairness or bounded individual regret, rather than just aggregate market stability?
- **Basis in paper**: [explicit] The Discussion notes that the current instability metric "does not necessarily ensure equitable outcomes" and suggests "a notion of regret that is bounded for each individual agent" could be more desirable.
- **Why unresolved**: The current definition of matching instability minimizes the total subsidy required to stabilize the market, which theoretically allows specific agents to suffer high regret if it benefits the global equilibrium.
- **What evidence would resolve it**: A new regret definition and corresponding algorithm that bounds the regret $R_T^{(a)}$ for every agent $a \in \mathcal{A}$ individually.

## Limitations

- **Central platform requirement**: The algorithm assumes a trusted central platform for matching, limiting applicability to decentralized settings.
- **Known outside options**: The regret metric requires agents to know their utilities for remaining unmatched, which may not hold in practice.
- **Sub-Gaussian assumption**: The theoretical regret bound relies on sub-Gaussian payoff distributions, which may not hold for all applications.

## Confidence

- **High Confidence**: The core algorithmic framework (UCB-based optimistic learning combined with DA matching) is well-established and theoretically sound. The regret analysis methodology following standard bandit techniques is rigorous.
- **Medium Confidence**: The regret bound derivation relies on multiple concentration inequalities; while the techniques are standard, the combined analysis across matching and game-learning introduces some complexity. The MI metric definition and its connection to equilibrium conditions are formally correct but depend on correct specification of outside options.
- **Low Confidence**: The empirical evaluation shows sublinear regret but uses relatively small market sizes (p,a ≤ 8) and limited horizon (T ≤ 5000). The comparison between self-play and informed scenarios is illustrative but doesn't establish statistical significance across diverse payoff distributions.

## Next Checks

1. **Robustness to payoff distribution**: Test the algorithm with heavy-tailed payoff distributions (e.g., t-distribution with low degrees of freedom) to verify that the sub-Gaussian assumption holds in practice and regret remains sublinear.

2. **Impact of outside option misspecification**: Systematically vary the specification of $U_{a,\perp}$ (using values ±0.5, ±1.5 instead of -1) and measure how MI deviates from true equilibrium deviation to assess sensitivity to this critical parameter.

3. **Scaling verification**: Run experiments with larger market sizes (p,a ∈ {16, 32}) and longer horizons (T ∈ {10⁴, 10⁵}) to empirically verify the √T scaling predicted by the theoretical bounds and identify potential computational bottlenecks in the minimax solvers.