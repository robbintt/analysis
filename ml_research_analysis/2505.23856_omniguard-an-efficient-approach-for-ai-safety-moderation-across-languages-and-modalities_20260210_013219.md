---
ver: rpa2
title: 'OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Languages
  and Modalities'
arxiv_id: '2505.23856'
source_url: https://arxiv.org/abs/2505.23856
tags:
- languages
- harmful
- arxiv
- prompts
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OMNIGUARD addresses the challenge of detecting harmful prompts
  across languages and modalities by leveraging internal representations of LLMs/MLLMs
  that are universally aligned. It identifies model layers producing language- and
  modality-agnostic embeddings using a custom U-Score metric, then trains a lightweight
  classifier on these embeddings.
---

# OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Languages and Modalities

## Quick Facts
- **arXiv ID**: 2505.23856
- **Source URL**: https://arxiv.org/abs/2505.23856
- **Reference count**: 27
- **Primary result**: OMNIGUARD achieves 86.36% accuracy on multilingual text attacks (outperforming strongest baselines by 11.57%), 88.31% on image-based prompts, and 93.09% on audio prompts, setting new state-of-the-art results.

## Executive Summary
OMNIGUARD addresses the challenge of detecting harmful prompts across 73 languages and multiple modalities by leveraging universal internal representations of large language and multimodal models. Instead of training separate guard models for each language or modality, OMNIGUARD identifies specific model layers that produce language- and modality-agnostic embeddings using a custom U-Score metric. A lightweight classifier is then trained on these embeddings, achieving state-of-the-art performance while being approximately 120× faster than existing approaches by reusing existing model computations.

## Method Summary
OMNIGUARD operates by first computing a U-Score metric for each layer of a pre-trained LLM/MLLM to identify layers producing language- and modality-agnostic embeddings. The U-Score measures the consistency of cross-lingual embeddings within a model by comparing similarity of English-translated pairs versus randomly paired translations. Once the optimal layer is identified, embeddings are extracted from this layer for each input prompt, averaged across tokens, and fed into a small MLP classifier (two hidden layers with 512 and 256 units) to predict harmful versus benign content. The approach is applied separately to text (using Llama3.3-70B-Instruct), vision (using Molmo), and audio (using Llama-Omni) modalities.

## Key Results
- Achieves 86.36% accuracy on multilingual text attacks, outperforming strongest baselines by 11.57%
- Sets new state-of-the-art results with 88.31% accuracy on image-based prompts and 93.09% on audio prompts
- Demonstrates rapid adaptation to unseen attack types with few-shot examples
- Shows 120× speedup compared to traditional guard model approaches by reusing existing model computations

## Why This Works (Mechanism)
OMNIGUARD leverages the observation that middle layers of LLMs produce embeddings that are semantically aligned across languages and modalities. This "universality" allows a single classifier to detect harmful content regardless of the input language or modality. By identifying the specific layer where this alignment is strongest through the U-Score metric, the approach can extract meaningful representations without requiring language-specific or modality-specific guard models. The method is particularly efficient because it reuses the forward pass of existing LLMs rather than training separate models.

## Foundational Learning

**U-Score Metric**
- *Why needed*: Identifies layers with language/modality-agnostic embeddings
- *Quick check*: Compute U-Score across layers and verify peak alignment occurs at intermediate layers

**Cross-Lingual Alignment**
- *Why needed*: Enables single classifier to work across 73 languages
- *Quick check*: Verify embeddings from different languages cluster together in selected layer

**Representation Averaging**
- *Why needed*: Reduces variable-length sequences to fixed-size embeddings
- *Quick check*: Compare performance with and without averaging to validate effectiveness

**Layer-Specific Embedding Extraction**
- *Why needed*: Different layers capture different semantic levels
- *Quick check*: Test classifier performance using embeddings from different layers

## Architecture Onboarding

**Component Map**
Aegis Dataset -> U-Score Computation (Flores200) -> Layer Selection -> Embedding Extraction -> MLP Classifier -> Harm/Benign Prediction

**Critical Path**
Dataset translation → U-Score computation for layer selection → Embedding extraction from selected layer → MLP training → Evaluation on benchmarks

**Design Tradeoffs**
- Reuses existing LLM computations (fast, efficient) vs. training dedicated guard models (potentially more accurate)
- Single layer selection vs. ensemble of layers (simplicity vs. robustness)
- Averaging embeddings vs. using all token embeddings (fixed size vs. information preservation)

**Failure Signatures**
- Poor performance on cipher languages (73.06% vs 86.22% accuracy gap indicates U-Score misalignment)
- Layer mismatch across model sizes (peak layer varies with model capacity)
- Class imbalance bias (monitor per-class metrics if training split not exactly 50/50)

**First Experiments**
1. Compute U-Score across all layers using Flores200 translation pairs to identify peak alignment layer
2. Train MLP classifier on 2,800 balanced Aegis examples and evaluate on held-out test set
3. Compare accuracy across all 73 languages and modalities to validate cross-lingual performance

## Open Questions the Paper Calls Out

**Open Question 1**
- *Question*: How can the internal representation alignment approach be adapted for closed-source models where access to hidden states is restricted?
- *Basis in paper*: [explicit] The authors state: "OMNIGUARD requires access to internal representations of a model, making it inapplicable to closed-source models."
- *Why unresolved*: The methodology fundamentally relies on extracting embeddings from specific layers; without API access to these activations, the current U-Score selection and classifier training pipeline cannot function.
- *What evidence would resolve it*: A demonstration of a proxy method (e.g., using output logits or API-available attention data) that correlates strongly with the internal U-Score alignments found in open-source models.

**Open Question 2**
- *Question*: Does the "universality" of middle-layer representations hold for non-Transformer architectures, such as State Space Models (e.g., Mamba) or mixture-of-experts (MoE) models?
- *Basis in paper*: [explicit] The authors note: "We did not exhaustively check all models and this assumption [of universal similarity] might not hold for models that we have not used."
- *Why unresolved*: The study relies on the specific geometric properties of Transformer layers (e.g., Llama, Molmo); different architectures process information in structurally different ways, potentially lacking a single "universal" layer.
- *What evidence would resolve it*: Reproducing the U-Score analysis on non-Transformer models (e.g., Jamba, Mamba) to see if a specific layer or state exhibits cross-lingual/cross-modal alignment.

**Open Question 3**
- *Question*: Can a single Omniguard classifier be trained on a unified embedding space to handle all modalities simultaneously, rather than training separate classifiers for text, vision, and audio?
- *Basis in paper*: [inferred] The paper trains distinct classifiers for text (using Llama3.3), vision (using Molmo), and audio (using Llama-Omni) despite aiming for a generalized "Omniguard."
- *Why unresolved*: While the authors demonstrate that individual modalities have aligned layers, they do not test if a "super-universal" representation exists in native multimodal models that can classify harm across text, image, and audio using one set of weights.
- *What evidence would resolve it*: An experiment training a single MLP on the embeddings of a native multimodal model (e.g., GPT-4o or Gemini) and evaluating its ability to perform cross-modal safety transfer.

## Limitations
- Requires access to internal representations of base models, making it inapplicable to closed-source models
- Layer selection varies with model size, requiring recomputation for different base models
- Performance gap exists for cipher languages (73.06% vs 86.22% accuracy) indicating U-Score misalignment

## Confidence

**High confidence**: The U-Score methodology for identifying language/modality-agnostic layers is clearly specified and theoretically sound. The efficiency claims (≈120× speedup) are supported by the architectural choice of reusing LLM computations rather than training separate guard models.

**Medium confidence**: The reported accuracy numbers (86.36% text, 88.31% image, 93.09% audio) are reproducible given the methodology, but the exact performance depends heavily on the unspecified MLP training details and data splits.

**Low confidence**: Claims about universal applicability across all model sizes without layer re-selection, and the assertion that OMNIGUARD "sets new state-of-the-art results" across all benchmarks, cannot be independently verified without access to the exact training configurations and full comparison methodology.

## Next Checks

1. Verify U-Score computation by implementing the Flores200-based layer selection and comparing results to the reported peak at layer 57 for Llama3.3-70B-Instruct.

2. Train the MLP classifier with systematic hyperparameter sweeps (learning rate, batch size, epochs) to establish performance bounds and identify sensitivity to these parameters.

3. Test performance degradation when applying the layer selection from one model size to another (e.g., using layer 57 from 70B model on 8B or 70B model) to validate the need for model-specific U-Score computation.