---
ver: rpa2
title: 'Slim Scheduler: A Runtime-Aware RL and Scheduler System for Efficient CNN
  Inference'
arxiv_id: '2510.09018'
source_url: https://arxiv.org/abs/2510.09018
tags:
- latency
- scheduler
- energy
- accuracy
- utilization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of efficiently scheduling neural
  network inference across heterogeneous multi-GPU systems. The proposed Slim Scheduler
  integrates a Proximal Policy Optimization (PPO) reinforcement learning policy with
  local greedy schedulers to coordinate distributed inference for slimmable CNN models.
---

# Slim Scheduler: A Runtime-Aware RL and Scheduler System for Efficient CNN Inference

## Quick Facts
- arXiv ID: 2510.09018
- Source URL: https://arxiv.org/abs/2510.09018
- Authors: Ian Harshbarger; Calvin Chidambaram
- Reference count: 9
- Primary result: Achieves up to 96.45% reduction in mean latency and 97.31% reduction in energy consumption for slimmable CNN inference across heterogeneous multi-GPU systems using hybrid PPO + greedy scheduling

## Executive Summary
This work addresses the challenge of efficiently scheduling neural network inference across heterogeneous multi-GPU systems. The proposed Slim Scheduler integrates a Proximal Policy Optimization (PPO) reinforcement learning policy with local greedy schedulers to coordinate distributed inference for slimmable CNN models. The PPO router learns global routing decisions—device selection, width ratio, and batch configuration—while local greedy schedulers handle batching and scaling within VRAM and utilization constraints. Experiments on a 3-GPU cluster using SlimResNet on CIFAR-100 show the scheduler achieves significant efficiency gains, demonstrating a trade-off between performance stability and resource utilization.

## Method Summary
The Slim Scheduler uses a hierarchical approach where a central PPO router makes global routing decisions (which GPU, which width ratio, which batch size) based on runtime telemetry, while each GPU runs a local greedy scheduler that handles batching, instance management, and opportunistic scaling. The system uses SlimResNet with four width ratios (1.00, 0.75, 0.50, 0.25) and partitions requests into segments that can be processed independently. The PPO policy receives telemetry about queue lengths, power consumption, and GPU utilization, then selects from a factored action space. The greedy scheduler implements FIFO queuing with best-fit instance assignment, opportunistic scale-up within VRAM and utilization constraints, and instance lifecycle management.

## Key Results
- Up to 96.45% reduction in mean latency and 97.31% reduction in energy consumption when optimized for efficiency
- Accuracy of 70.30% (0.25× width) to 76.43% (1.00× width) for uniform width configurations
- Mixed-width configurations achieve 71.35%–75.33% accuracy with significant efficiency gains
- Balanced configuration achieves 75.26% accuracy with moderate efficiency gains but higher variance

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical decomposition of scheduling decisions (PPO for global routing, greedy for local batching) reduces action space complexity while maintaining adaptability. The PPO router selects among N servers × W width ratios × G batch sizes, while the greedy executor handles low-level batching and memory management. This separation prevents the RL policy from needing to learn memory management heuristics directly.

### Mechanism 2
Runtime telemetry-driven reward shaping allows PPO to learn operating points just below GPU saturation thresholds (≈90–95% utilization), avoiding nonlinear latency/energy spikes. The policy learns to distribute load to keep utilization in the quasi-linear regime where latency and energy scale predictably.

### Mechanism 3
Slimmable model width selection enables per-request accuracy–efficiency trade-offs without maintaining multiple model copies. The PPO router can select different width ratios for different requests based on current telemetry, allowing fine-grained control over the accuracy-efficiency trade-off.

## Foundational Learning

- **Concept**: Proximal Policy Optimization (PPO) basics—clipped surrogate objective, advantage normalization, entropy regularization.
  - Why needed here: The PPO router uses clipped objective L_CLIP with ε=0.2, one-step advantages, and entropy bonus. Understanding how PPO balances exploration vs. exploitation is essential for tuning reward weights.
  - Quick check question: Given a positive advantage A_t > 0 and importance ratio ρ_t = 1.5, what is the clipped surrogate contribution if ε = 0.2?

- **Concept**: Slimmable Neural Networks—dynamic width scaling, switchable batch/group normalization, inference-time width selection.
  - Why needed here: The system assumes a pre-trained universally slimmable backbone; without understanding width-switchable inference, one cannot reason about accuracy–latency trade-offs.
  - Quick check question: Why does SlimResNet use Group Normalization instead of Batch Normalization for slimmable inference?

- **Concept**: GPU Utilization–Latency–Energy Coupling—how batch size drives utilization, which nonlinearly affects latency and power beyond saturation.
  - Why needed here: The reward function implicitly models this coupling; the policy's success depends on staying below the saturation inflection point.
  - Quick check question: If GPU utilization is 92% and you increase batch size by 10%, would you expect latency to increase linearly or superlinearly? Why?

## Architecture Onboarding

- **Component map**: Request arrival → enqueue (segment, w_req) → PPO selects (server, width, batch group) → greedy forms batch → instance lookup/load → inference → telemetry emission → PPO state update

- **Critical path**: Request arrival → enqueue (segment, w_req) → PPO selects (server, width, batch group) → greedy forms batch → instance lookup/load → inference → telemetry emission → PPO state update

- **Design tradeoffs**:
  - Heavy latency/energy penalties (β, γ high): Policy collapses to slimmest width (0.25×), maximizing efficiency but sacrificing accuracy (70.3%)
  - Balanced penalties: Policy explores wider widths, recovering accuracy (75.26%) but with higher latency/energy variance (σ_latency = 11.67s, σ_energy = 2125J)
  - Exploration (ε_t): Decays from ε_max to ε_min; too slow decay risks premature convergence; too fast risks instability

- **Failure signatures**:
  - Accuracy collapse to minimum (70.3%) → likely β, γ overweighted; check reward component magnitudes
  - High latency variance with modest gains → policy oscillating between widths; consider reducing ε_min or adding variance penalty
  - Frequent requeuing (greedy cannot find free instance) → VRAM cap M_max too tight or U_blk too low; check instance lifecycle
  - GPU utilization stuck low → PPO not routing to that server; check server head exploration and imbalance penalty δ

- **First 3 experiments**:
  1. **Single-GPU baseline characterization**: Replicate Figure 1–3 on your hardware (batch size vs. utilization; utilization vs. latency/energy). Validates saturation threshold assumptions before multi-GPU deployment.
  2. **Ablate reward components**: Run PPO with only accuracy reward (β=γ=δ=0), then add latency, then energy, then imbalance. Observes how each term shapes policy behavior.
  3. **Cross-device generalization**: Train PPO on 2-GPU subset, evaluate on full 3-GPU cluster. Tests whether learned policies are device-agnostic or overfit to specific hardware topology.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can adaptive reward scaling or uncertainty-aware policy mechanisms reduce the variance in latency and energy while maintaining efficiency gains? The balanced PPO configuration achieved 75.26% accuracy but suffered high latency variance (σ=11.67s) and energy variance (σ=2125J), indicating the policy oscillates between configurations.

- **Open Question 2**: Does the hierarchical PPO+greedy framework scale to larger GPU clusters (e.g., 8+ devices) without prohibitive coordination overhead? Experiments were conducted only on a 3-GPU cluster; the state space grows with N servers.

- **Open Question 3**: How does the scheduler perform under high-bandwidth interconnects (e.g., NVLink, InfiniBand) versus the Wi-Fi 5 setup used? The 0.318ms latency in Table IV includes network overhead; faster interconnects could shift the optimal batching/slimming trade-off.

## Limitations

- **Reward weight sensitivity**: The PPO policy's behavior is highly dependent on the precise values of α, β, γ, δ, making it difficult to reproduce the reported trade-offs without exact reward weight settings.
- **Hardware topology dependence**: The PPO router learns device-specific routing; results on a 3-GPU cluster may not generalize to different numbers or types of GPUs without retraining.
- **State representation assumptions**: The PPO's state vector includes FIFO queue lengths and GPU utilization. If actual queue dynamics or utilization metrics differ from the paper's environment, the learned policy may not transfer.

## Confidence

- **High Confidence**: The hierarchical decomposition mechanism (PPO + greedy) and its role in reducing action space complexity.
- **Medium Confidence**: The saturation threshold behavior (linear latency/energy until ~95% utilization, then nonlinear spikes).
- **Low Confidence**: The accuracy prior table generalization and mixed-width accuracy claims without access to the exact table or validation methodology.

## Next Checks

1. **Single-GPU saturation characterization**: Replicate Figures 2–3 on your hardware to confirm the latency/energy saturation threshold before deploying the multi-GPU scheduler.
2. **Reward ablation study**: Run PPO with isolated reward components (accuracy-only, then add latency, then energy, then imbalance) to observe how each shapes the policy and identify tipping points.
3. **Cross-device generalization test**: Train PPO on a subset of GPUs (e.g., 2 of 3), then evaluate on the full cluster to assess whether the learned policy is device-agnostic or overfits to specific hardware.