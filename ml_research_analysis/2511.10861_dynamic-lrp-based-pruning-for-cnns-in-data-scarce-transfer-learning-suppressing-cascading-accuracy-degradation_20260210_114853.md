---
ver: rpa2
title: 'Dynamic LRP-Based Pruning for CNNs in Data-Scarce Transfer Learning: Suppressing
  Cascading Accuracy Degradation'
arxiv_id: '2511.10861'
source_url: https://arxiv.org/abs/2511.10861
tags:
- pruning
- accuracy
- relevance
- filters
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of compressing CNNs in data-scarce
  transfer learning scenarios without compromising classification accuracy. The core
  problem is cascading accuracy degradation in existing LRP-based pruning methods,
  where the pruning of filters leading to accuracy drops for specific classes reduces
  the relevance scores of remaining filters, triggering further unnecessary deletions.
---

# Dynamic LRP-Based Pruning for CNNs in Data-Scarce Transfer Learning: Suppressing Cascading Accuracy Degradation

## Quick Facts
- **arXiv ID:** 2511.10861
- **Source URL:** https://arxiv.org/abs/2511.10861
- **Reference count:** 38
- **Primary result:** DPX-SD method suppresses cascading accuracy degradation in data-scarce transfer learning, outperforming existing LRP-based pruning methods

## Executive Summary
This paper addresses the challenge of compressing CNNs in data-scarce transfer learning scenarios without compromising classification accuracy. The core problem is cascading accuracy degradation in existing LRP-based pruning methods, where pruning filters leading to accuracy drops for specific classes reduces the relevance scores of remaining filters, triggering further unnecessary deletions. The authors propose DPX-SD (Dynamic Pruning by eXplain for Scarce Data), a dynamic pruning method that addresses this issue through two key innovations: (1) "Change of pruning rate," which dynamically adjusts the pruning rate to maintain the harmonic mean accuracy across classes, and (2) "Change of pruning order," which explores and prunes filters that minimally impact accuracy even if they have low relevance. Experimental results on four ImageNet-pretrained CNNs (VGG16, VGG16BN, ResNet-18, ResNet-50) demonstrate that DPX-SD consistently outperforms existing LRP-based methods (PX and DPX) in maintaining higher classification accuracy during aggressive pruning, particularly in the 40%-90% pruning rate range.

## Method Summary
The paper proposes DPX-SD, a dynamic pruning method for CNNs in data-scarce transfer learning that suppresses cascading accuracy degradation. The method introduces two key innovations: "Change of pruning rate" dynamically adjusts the pruning rate to maintain harmonic mean accuracy across classes, while "Change of pruning order" explores and prunes filters that minimally impact accuracy even if they have low relevance scores. The approach iteratively refines pruning decisions by monitoring accuracy changes and relevance score distributions, preventing the cascading effect where pruning one filter reduces relevance scores of others, leading to unnecessary further deletions.

## Key Results
- DPX-SD consistently outperforms existing LRP-based methods (PX and DPX) in maintaining higher classification accuracy during aggressive pruning
- The method effectively suppresses cascading accuracy degradation, particularly in the 40%-90% pruning rate range
- DPX-SD achieves higher "AUC with lowest class" values, indicating better inter-class accuracy balance
- While DPX-SD incurs higher computational costs than existing methods, this is acceptable for data-limited environments where model compression is prioritized

## Why This Works (Mechanism)
The method works by dynamically adjusting both the pruning rate and pruning order based on real-time accuracy feedback and relevance score analysis. When accuracy drops are detected for specific classes, the pruning rate is reduced to prevent further degradation. Additionally, the pruning order is modified to explore filters that have minimal impact on accuracy, even if they have low relevance scores. This dual approach prevents the cascading effect where pruning one filter reduces the relevance scores of remaining filters, triggering unnecessary further deletions.

## Foundational Learning
- **LRP (Layer-wise Relevance Propagation)**: A technique for explaining neural network predictions by propagating relevance scores backward through the network. Needed to identify which filters contribute most to classification decisions. Quick check: Verify relevance scores sum to the original prediction score.
- **Transfer Learning**: A machine learning method where a model trained on one task is reused for a different but related task. Needed to leverage pre-trained models in data-scarce scenarios. Quick check: Measure accuracy drop when fine-tuning on limited data.
- **Filter Pruning**: The process of removing filters from convolutional layers to compress neural networks. Needed to reduce model size and computational requirements. Quick check: Monitor FLOPs reduction after pruning.
- **Harmonic Mean Accuracy**: A metric that balances accuracy across classes, particularly useful when dealing with imbalanced datasets. Needed to ensure no class suffers significant accuracy degradation. Quick check: Calculate harmonic mean across all classes.
- **Cascading Accuracy Degradation**: A phenomenon where pruning one filter leads to reduced relevance scores of remaining filters, causing unnecessary further deletions. Needed to understand the core problem being addressed. Quick check: Track accuracy changes after each pruning iteration.

## Architecture Onboarding

**Component Map:**
Pre-trained CNN -> LRP Analysis -> Initial Filter Ranking -> Iterative Pruning (DPX-SD) -> Dynamic Rate Adjustment -> Alternative Pruning Order Exploration -> Final Compressed Model

**Critical Path:**
1. Load pre-trained model and target dataset
2. Compute LRP relevance scores for filters
3. Perform initial pruning based on relevance
4. Monitor accuracy and adjust pruning rate
5. Explore alternative pruning order if accuracy drops
6. Iterate until target compression ratio achieved

**Design Tradeoffs:**
- Accuracy vs. compression ratio: Higher compression leads to more aggressive pruning and potential accuracy loss
- Computational cost vs. precision: DPX-SD's iterative refinement is more accurate but slower than non-iterative methods
- Data efficiency vs. model quality: Limited target data constrains the ability to assess pruning impact accurately

**Failure Signatures:**
- Sudden accuracy drops indicating cascading degradation
- Relevance score distributions becoming too uniform, suggesting loss of discriminative power
- Harmonic mean accuracy decreasing faster than overall accuracy, indicating class imbalance issues

**First 3 Experiments:**
1. Compare DPX-SD against PX and DPX on VGG16 with limited data from ImageNet subclasses
2. Evaluate the impact of dynamic pruning rate adjustment alone vs. combined with pruning order exploration
3. Test the method on different CNN architectures (ResNet-18, ResNet-50) to assess generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effectively does DPX-SD perform on real-world, domain-specific datasets such as medical or satellite imagery?
- Basis in paper: [explicit] The conclusion explicitly lists "validating the method on real-world datasets, such as medical and satellite imagery" as a direction for future research.
- Why unresolved: The current study only validates the method on ImageNet subclasses, which may not fully represent the feature distributions of specialized domains.
- What evidence would resolve it: Evaluation results on benchmarks like ChestX-ray or EuroSAT showing the method's ability to maintain accuracy under high pruning rates.

### Open Question 2
- Question: Can the DPX-SD methodology be adapted for Transformer-based architectures?
- Basis in paper: [explicit] The conclusion states that "extending it to Transformer-based models" is an important direction.
- Why unresolved: The method currently targets CNN filters; Transformers rely on different structural components (e.g., attention heads) where relevance propagation and pruning dynamics may differ.
- What evidence would resolve it: A modified DPX-SD implementation for Vision Transformers (ViT) that suppresses accuracy degradation during pruning.

### Open Question 3
- Question: Can the computational overhead of the "Change of pruning order" mechanism be reduced?
- Basis in paper: [explicit] The conclusion highlights the need for "improving computational efficiency by optimizing relevance updates... and introducing approximate calculations."
- Why unresolved: The iterative search for safe-to-prune filters currently takes 1â€“3 hours, which is significantly slower than non-iterative baselines.
- What evidence would resolve it: An optimization strategy that reduces pruning time significantly while retaining the suppression of cascading degradation.

## Limitations
- Evaluation is primarily conducted on ImageNet-pretrained models using limited target datasets, constraining generalizability
- The method's reliance on LRP for filter relevance estimation may introduce sensitivity to the quality and quantity of available target data
- Computational overhead of iterative pruning refinement is not extensively benchmarked against real-world deployment constraints

## Confidence
- **High confidence**: DPX-SD's effectiveness in maintaining classification accuracy during aggressive pruning in data-scarce scenarios
- **Medium confidence**: The method's superiority in suppressing cascading accuracy degradation compared to PX and DPX
- **Medium confidence**: The trade-off between computational overhead and accuracy preservation in practical applications

## Next Checks
1. Evaluate DPX-SD's performance on non-ImageNet pretrained models and diverse target datasets to assess generalizability
2. Conduct ablation studies to isolate the impact of dynamic pruning rate adjustment versus pruning order optimization
3. Benchmark DPX-SD against non-LRP-based pruning methods (e.g., magnitude-based, gradient-based) to contextualize its advantages