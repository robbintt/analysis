---
ver: rpa2
title: Utilising Deep Learning to Elicit Expert Uncertainty
arxiv_id: '2501.11813'
source_url: https://arxiv.org/abs/2501.11813
tags:
- uncertainty
- learning
- expert
- deep
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a novel method to elicit expert uncertainty
  by using deep learning models to analyze complex data, such as images and reports,
  that experts typically use in their decision-making processes. Traditional methods
  of expert knowledge elicitation rely on interviews and direct questioning, which
  can be time-consuming, introduce bias, and require statistical expertise.
---

# Utilising Deep Learning to Elicit Expert Uncertainty

## Quick Facts
- **arXiv ID**: 2501.11813
- **Source URL**: https://arxiv.org/abs/2501.11813
- **Reference count**: 40
- **Primary result**: Novel method uses deep learning to elicit expert uncertainty from complex data like images, validated on histopathology cancer diagnosis.

## Executive Summary
This paper introduces a method to elicit expert uncertainty by analyzing the data experts use in their decision-making, rather than relying on direct interviews. The approach uses probabilistic deep learning with Monte Carlo Dropout to generate distributions that capture expert uncertainty. When tested on histopathology images for colon cancer risk prediction, the model produced more uncertain distributions when pathologists disagreed and more certain distributions when there was full agreement. This demonstrates that the method effectively captures epistemic uncertainty in expert decision-making.

## Method Summary
The method trains a ResNet18 neural network with dropout layers between residual blocks on histopathology images, using binary cross-entropy loss and stochastic gradient descent. During inference, dropout is enabled to generate 100 probability samples per image through Monte Carlo sampling. These samples are then fitted to a Beta distribution using the Method of Moments to capture expert uncertainty. The model was trained and validated on a dataset of 3,152 H&E-stained colon tissue images from DHMC, with labels based on majority vote of 7 pathologists.

## Key Results
- Model accuracy was approximately 78-79% on the histopathology dataset
- Entropy increased from 0.63 to 0.71 when comparing full agreement to three opposing pathologist opinions
- Reliability diagrams and credible interval coverage showed good calibration performance

## Why This Works (Mechanism)

### Mechanism 1
Applying dropout during inference produces probability distributions that approximate Bayesian model uncertainty. Random node elimination during each forward pass creates ensemble-like behavior, generating samples that reflect uncertainty in model weights given finite training data. This assumes dropout approximates a probabilistic Gaussian process, as shown by Gal & Ghahramani (2016).

### Mechanism 2
Model entropy correlates with expert inter-rater disagreement, validating that the model captures genuine epistemic uncertainty. Ambiguous or borderline cases naturally produce higher variance in dropout-activated forward passes because learned features are less discriminative, yielding wider probability distributions with higher entropy.

### Mechanism 3
Method of Moments fitting of Beta distributions to probability samples produces interpretable prior distributions. Given n probability samples from MC-Dropout, compute sample mean and variance, then derive Beta(α, β) parameters using the standard formula.

## Foundational Learning

- **Concept**: Aleatoric vs Epistemic Uncertainty
  - Why needed here: The paper explicitly distinguishes irreducible randomness (aleatoric) from reducible ignorance (epistemic). Pathologist disagreement is epistemic—it varies with expertise and information.
  - Quick check question: For a histopathology image where 4 of 7 pathologists disagree, is this aleatoric or epistemic uncertainty, and what would reduce it?

- **Concept**: Bayesian Approximation via Variational Inference
  - Why needed here: MC-Dropout is justified as a Bayesian approximation; understanding KL divergence explains why dropout minimizes distance between approximate and true posterior.
  - Quick check question: Why does variational inference trade some accuracy for computational tractability compared to MCMC?

- **Concept**: Calibration and Reliability Diagrams
  - Why needed here: Section 4.3 evaluates calibration; a well-calibrated model's predicted probabilities match observed frequencies—critical for uncertainty to be trustworthy.
  - Quick check question: If a model predicts 80% confidence on 100 cases but only 60% are correct, is it overconfident or underconfident?

## Architecture Onboarding

- **Component map**: Input preprocessing (resize to 100×100, normalize) -> ResNet18 backbone -> Dropout layers (between residual blocks) -> Sigmoid output -> MC inference (100 forward passes) -> Distribution fitting (Method of Moments → Beta)

- **Critical path**: Data preparation → Train with BCE loss, SGD optimizer, 100 epochs → Enable dropout at inference → Collect 100 p-samples → Fit Beta distribution → Validate via entropy vs. agreement correlation

- **Design tradeoffs**: MC-Dropout vs BNN: Paper recommends MC-Dropout (simpler, faster) over variational inference BNNs. Dropout rate: Higher = more uncertainty but potentially lower accuracy. Number of MC samples: 100 provides stable Beta estimates; fewer may be noisy.

- **Failure signatures**: Overconfidence: Low entropy even when experts disagree. Poor calibration: Calibration plot deviates from diagonal. Centred CI doesn't increase with disagreement: Violates expected conservative behavior.

- **First 3 experiments**: 1) Baseline validation: Train ResNet18+dropout on histopathology data; verify entropy increases with disagreement levels. 2) Dropout rate sweep: Compare q ∈ {0.1, 0.3, 0.5} on calibration and entropy-agreement correlation. 3) Distribution fit quality: Compare Beta vs. KDE fits on held-out samples.

## Open Questions the Paper Calls Out

- **Open Question 1**: How do the probability distributions elicited via deep learning compare to those obtained using traditional interview-based elicitation protocols? The paper suggests this comparison would be useful but hasn't been conducted.

- **Open Question 2**: How can this elicitation approach be integrated into Adversarial Risk Analysis (ARA) to inform decision-making under opponent uncertainty? The paper notes this as future research direction.

- **Open Question 3**: Can this method effectively elicit uncertainty from unstructured text data, such as clinical reports, in addition to visual imagery? The paper asserts applicability to complex data but only validates on images.

## Limitations

- Dropout rate is not specified, which critically affects the trade-off between model uncertainty and accuracy
- Beta distribution fitting assumes unimodal uncertainty, which may not capture multimodal expert disagreement
- Exact train/test split ratio is unspecified, making reproduction difficult

## Confidence

- **High**: The general framework of using MC-Dropout to approximate Bayesian uncertainty is well-established in the literature
- **Medium**: The entropy-agreement correlation is demonstrated, but the strength and robustness across different dropout rates is unclear
- **Low**: The claim that Method of Moments with Beta fitting reliably captures expert uncertainty is weakly supported without validation against alternative methods

## Next Checks

1. **Dropout Rate Sweep**: Systematically test dropout rates (0.1, 0.3, 0.5) to find the optimal balance between calibration and entropy correlation with expert disagreement

2. **Alternative Distribution Fits**: Compare Beta fits to kernel density estimates (KDE) on the same dropout samples to assess whether the Beta assumption is valid

3. **Cross-Dataset Validation**: Apply the method to a different histopathology dataset or a non-medical binary classification task to test the generalizability of the entropy-agreement correlation