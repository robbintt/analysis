---
ver: rpa2
title: 'AdiBhashaa: A Community-Curated Benchmark for Machine Translation into Indian
  Tribal Languages'
arxiv_id: '2512.04765'
source_url: https://arxiv.org/abs/2512.04765
tags:
- languages
- tribal
- translation
- language
- adibhashaa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces AdiBhashaa, a community-driven initiative
  to create the first open parallel corpora and baseline MT systems for four major
  Indian tribal languages: Bhili, Mundari, Gondi, and Santali. The authors combine
  participatory data creation with native speakers, human-in-the-loop validation,
  and systematic evaluation of both encoder-decoder MT models and large language models.'
---

# AdiBhashaa: A Community-Curated Benchmark for Machine Translation into Indian Tribal Languages

## Quick Facts
- arXiv ID: 2512.04765
- Source URL: https://arxiv.org/abs/2512.04765
- Reference count: 2
- Introduces first open parallel corpora and baseline MT systems for Bhili, Mundari, Gondi, and Santali

## Executive Summary
This paper presents AdiBhashaa, a community-driven initiative to create the first open parallel corpora and baseline machine translation systems for four major Indian tribal languages: Bhili, Mundari, Gondi, and Santali. The authors employ a participatory workflow involving native-speaking translators and independent validators to ensure linguistic quality and cultural fidelity. They establish baseline systems using fine-tuned multilingual encoder-decoder architectures like NLLB-200, mT5, and IndicTrans2, and evaluate them against LLMs in zero-shot and few-shot settings. The results show that fine-tuned multilingual MT models substantially outperform zero-shot performance for all four languages, while translation into high-resource languages systematically outperforms translation into tribal languages due to difficulties in generating morphologically rich, low-frequency vocabulary.

## Method Summary
The method combines participatory data creation with native speakers, human-in-the-loop validation, and systematic evaluation of both encoder-decoder MT models and large language models. Native-speaking translators produce initial translations focusing on semantic adequacy and cultural appropriateness, with independent validators reviewing and resolving disagreements through structured discussions. The resulting corpus of 80,000 parallel sentences (20,000 per language) is used to fine-tune multilingual models like NLLB-200, mT5, and IndicTrans2. Models are evaluated using chrF++ and BLEU metrics, with human evaluation by native speakers on curated subsets. The approach is tested across eight translation directions (four languages × two directions) and compared against zero-shot and few-shot LLM performance.

## Key Results
- Fine-tuned multilingual MT models substantially outperform zero-shot performance for all four languages
- Translation into high-resource languages (tribal→Hindi/English) systematically outperforms translation into tribal languages
- Few-shot LLMs are competitive but generally underperform dedicated fine-tuned MT models, especially for Santali in Ol Chiki script

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning multilingual encoder-decoder models on modest community-curated parallel data yields substantial gains over zero-shot for languages absent from pretraining corpora. Multilingual models encode transferable linguistic representations; fine-tuning on ~19,000 sentence pairs per language provides sufficient gradient signal to adapt these representations to unseen languages, particularly when scripts or typological features overlap with pretraining languages.

### Mechanism 2
Translation into high-resource languages (tribal→Hindi/English) systematically outperforms translation into tribal languages. Generating morphologically complex, low-frequency target vocabulary in tribal languages is harder than decoding into simpler, high-frequency vocabulary in Hindi/English. Models have stronger generation priors for high-resource languages from pretraining.

### Mechanism 3
Participatory translation with independent validation produces corpora with higher semantic adequacy and cultural appropriateness than crowdsourcing or synthetic generation. Native-speaking translators bring implicit knowledge of idioms, cultural references, and community norms. Structured validation with disagreement resolution surfaces edge cases and trains validators in quality assessment.

## Foundational Learning

- **Multilingual Encoder-Decoder Architectures**: Understanding how NLLB-200, mT5, and IndicTrans2 encode cross-lingual representations explains why fine-tuning transfers to unseen languages. Quick check: Why would a model trained on 200 languages generalize to a 201st language it never explicitly saw?

- **Zero-shot vs. Few-shot vs. Fine-tuning Paradigms**: The paper compares these approaches; selecting the right one determines deployment feasibility and performance. Quick check: Given 20,000 parallel sentences for a new tribal language, when would you choose few-shot prompting over fine-tuning?

- **Script Representation and Unicode in Low-Resource Settings**: Santali uses Ol Chiki script with limited digital infrastructure; script tokenization directly impacts model performance. Quick check: Why might a model perform worse on Ol Chiki than on Devanagari even with identical training data sizes?

## Architecture Onboarding

- **Component map**: Source curation module (Hindi sentences) → Translation interface (native speaker input) → Validation layer (independent reviewers) → Fine-tuning pipeline (NLLB-200, mT5, IndicTrans2) → Evaluation framework (chrF++, BLEU, human evaluation)

- **Critical path**: 1. Hindi source curation → 2. Community translation → 3. Independent validation → 4. Train/test split → 5. Model fine-tuning → 6. Automatic + human evaluation

- **Design tradeoffs**: Corpus size (80K total) vs. annotation quality: paper prioritizes validated quality over raw scale. Fine-tuned MT vs. few-shot LLM: fine-tuning outperforms but requires compute; few-shot is competitive when fine-tuning infeasible. Script standardization: Devanagari (3 languages) benefits from transfer; Ol Chiki (Santali) requires specialized tokenization.

- **Failure signatures**: Tribal→Hindi chrF++ significantly higher than Hindi→Tribal: expected, but gap >30 points suggests insufficient target-side tribal vocabulary. Santali (Ol Chiki) underperforms Devanagari languages by >15 chrF++: likely tokenization or vocabulary coverage issue. Validation disagreement rate >20%: translator guidelines may be underspecified or sources culturally mismatched.

- **First 3 experiments**: 1. Establish zero-shot baselines for all 8 translation directions using NLLB-200, mT5, IndicTrans2 without fine-tuning. 2. Fine-tune NLLB-200 on 95/5 split per language; report chrF++ and BLEU against zero-shot baseline. 3. Evaluate few-shot LLMs (3-5 examples) on held-out test set; compare against fine-tuned MT to quantify the fine-tuning advantage.

## Open Questions the Paper Calls Out

1. Can the proposed human-in-the-loop paradigm—where models propose translations for monolingual text and community reviewers accept, correct, or reject them—scale corpus creation while preserving the linguistic quality and cultural fidelity achieved in the initial participatory workflow?

2. What interventions beyond increasing data volume—such as morphological analysis, subword regularization, or target-side data augmentation—can reduce the systematic performance gap between tribal→high-resource and high-resource→tribal translation directions?

3. To what extent does the AdiBhashaa participatory methodology generalize to other under-resourced languages with different sociolinguistic contexts, such as languages without established orthographies or with multiple competing scripts?

## Limitations
- Limited scale of parallel corpus (80K sentences total, ~19K per language) restricts model performance potential
- Absence of comparative evaluation against other participatory or synthetic corpus creation methods
- Santali Ol Chiki script performance gaps suggest unresolved tokenization or vocabulary coverage issues

## Confidence

- **High Confidence**: Fine-tuned multilingual encoder-decoder models substantially outperform zero-shot baselines for all four languages
- **Medium Confidence**: Participatory translation with native speakers and independent validation produces higher quality corpora than alternative methods
- **Medium Confidence**: Translation into high-resource languages systematically outperforms translation into tribal languages due to morphological complexity and vocabulary frequency

## Next Checks

1. Reproduce the Santali Ol Chiki performance gap: Isolate whether tokenization issues, vocabulary coverage, or script representation limitations drive the observed performance gap between Santali and Devanagari languages.

2. Validate participatory workflow impact: Conduct a controlled comparison between corpora created with native speakers plus validation versus synthetic generation or crowdsourcing, measuring semantic adequacy and downstream MT performance differences.

3. Test fine-tuning hyperparameter sensitivity: Systematically vary learning rate, batch size, and epochs to establish optimal fine-tuning configurations for each tribal language, documenting the relationship between corpus size and model performance.