---
ver: rpa2
title: 'InsTex: Indoor Scenes Stylized Texture Synthesis'
arxiv_id: '2501.13969'
source_url: https://arxiv.org/abs/2501.13969
tags:
- texture
- diffusion
- textures
- image
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InsTex, a two-stage approach for generating
  high-quality, style-consistent textures for 3D indoor scenes. The method addresses
  the challenges of generalization and multi-view consistency by decomposing scenes
  into individual objects, using depth-to-image diffusion priors for coarse texture
  generation, and refining textures in UV space with positional encoding.
---

# InsTex: Indoor Scenes Stylized Texture Synthesis

## Quick Facts
- arXiv ID: 2501.13969
- Source URL: https://arxiv.org/abs/2501.13969
- Reference count: 27
- Outperforms baselines with CLIP scores of 24.18 and Inception Scores of 3.82, achieving user study ratings of 4.70/5 for visual quality and 4.82/5 for prompt fidelity while requiring only 2 hours for a typical living room.

## Executive Summary
InsTex introduces a two-stage approach for generating high-quality, style-consistent textures for 3D indoor scenes. The method decomposes scenes into individual objects, uses depth-to-image diffusion priors for coarse texture generation with multi-view renderings, and refines textures in UV space with positional encoding. By combining global style guidance through dual conditioning with object-level processing, InsTex achieves state-of-the-art performance in visual quality and consistency while being the fastest approach, requiring only 2 hours compared to up to 48 hours for competing methods.

## Method Summary
InsTex employs a three-stage pipeline: scene decomposition and global style generation, coarse texture generation using depth-to-image diffusion with dynamic view partitioning, and UV-space refinement with positional encoding. The method processes individual objects while maintaining style consistency through dual conditioning with text prompts and a generated global style image. Multi-view renderings at specified viewpoints generate textures progressively, with different diffusion strengths applied to "generate," "update," and "keep" regions to prevent artifacts. The refinement stage uses a position map encoder to handle UV discontinuities and an IP-Adapter for global style features.

## Key Results
- CLIP scores of 24.18 outperform baselines while achieving faster processing times
- User studies rate visual quality at 4.70/5 and prompt fidelity at 4.82/5
- Processing time of 2 hours on V100 32G GPU compared to up to 48 hours for competing approaches

## Why This Works (Mechanism)

### Mechanism 1
Decomposing a scene into individual objects while conditioning them on a shared global image enables style consistency across the full scene. The system generates a "global style image" from the text prompt first, then uses dual conditioning when texturing individual objects: the specific object prompt and the global style image via an IP-Adapter. This forces local geometry to adhere to a global aesthetic reference.

### Mechanism 2
A dynamic view partitioning strategy during back-projection reduces the "stretched artifact" common in projection-based texturing. The system classifies regions into "generate" (new viewpoints), "update" (overlapping boundaries), and "keep" (finalized areas), applying different denoising strengths to these regions to blend new detail into existing textures without overwriting stable regions.

### Mechanism 3
Adding a 3D position map as a positional encoder allows standard diffusion models to operate on discontinuous UV texture maps. UV maps are semi-continuous with seams and discontinuous islands, which standard image models struggle with. InsTex encodes the 3D coordinates of the mesh surface into a position map, feeding this into a ControlNet-style encoder to give the model spatial awareness of where texture pixels are in 3D space.

## Foundational Learning

- **UV Mapping & Texture Space:** Understanding that UV maps are 2D representations of 3D surfaces with seams and discontinuous islands is required to understand why the positional encoding mechanism is necessary. Quick check: Can a standard 2D inpainting model fill a hole in a UV map correctly without knowing which pixels are adjacent in 3D space?

- **Depth-to-Image Diffusion (ControlNet):** The coarse generation stage relies on pre-trained depth-to-image models that use depth maps as structural constraints to generate images that align with provided geometry. Quick check: If you feed a depth map of a sphere into a depth-to-image model, what ensures the generated pattern wraps around the sphere rather than appearing as a flat circle?

- **Back-projection / Render-to-Texture:** The core loop involves rendering a view, generating an image, and back-projecting pixels onto the mesh texture. Understanding how 2D screen coordinates map to 3D surface coordinates is vital for debugging texture misalignment. Quick check: In a texture generation loop, if the camera moves, how do you determine which pixels in the existing texture map to overwrite and which to leave alone?

## Architecture Onboarding

- **Component map:** Input Processor -> Scene Decomposer -> Global Generator -> Coarse Texturer -> Refiner -> Recomposer
- **Critical path:** The Coarse Texturer's dynamic view partitioning. If the "generate" vs. "update" mask logic fails, the pipeline will either over-smooth the texture or leave seams.
- **Design tradeoffs:** Object-Centric vs. Holistic processing allows parallelization and higher resolution but requires the "Global Image" trick to maintain scene coherence. Speed vs. Quality: reducing diffusion steps linearly speeds up the process but risks noisy textures.
- **Failure signatures:** "Water-wave" artifacts indicate refinement stage failure to smooth out gradients across UV islands. Stretched Artifacts occur if dynamic view partitioning doesn't correctly identify high-angle views. Style Drift happens if IP-Adapter weight is too low.
- **First 3 experiments:**
  1. **Sanity Check:** Render a simple colored cube, run coarse generator with "red cube" prompt, verify colors align with geometry.
  2. **Ablation (Global Style):** Run pipeline on multi-object scene without global style image condition, verify style consistency drops via CLIP score variance.
  3. **Parameter Sweep (Steps):** Vary "update" diffusion steps on curved surface to observe transition between sharp details and blending artifacts.

## Open Questions the Paper Calls Out
None

## Limitations
- Exact architecture of the position map encoder is underspecified with insufficient implementation details
- Scene decomposition methodology isn't detailed despite following referenced protocol
- Final post-processing diffusion model has no conditioning or operational details provided

## Confidence

- **High:** Multi-view depth-to-image coarse generation with dynamic view partitioning
- **Medium:** Global style consistency via IP-Adapter
- **Low:** Position map encoder for UV discontinuities

## Next Checks

1. **UV Coverage Validation:** Render the UV occupancy map after coarse generation to verify no regions are left untextured due to self-occlusion.
2. **Style Consistency Ablation:** Run the pipeline on a multi-object scene without global style image conditioning and measure CLIP score variance between objects.
3. **Parameter Sensitivity:** Systematically vary the "update" region diffusion steps (0, 10, 20) on a curved surface to quantify the tradeoff between detail preservation and smoothing.