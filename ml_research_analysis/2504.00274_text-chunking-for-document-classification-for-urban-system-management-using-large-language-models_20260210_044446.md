---
ver: rpa2
title: Text Chunking for Document Classification for Urban System Management using
  Large Language Models
arxiv_id: '2504.00274'
source_url: https://arxiv.org/abs/2504.00274
tags:
- text
- coding
- chunking
- whole
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study explored using large language models (LLMs) to code\
  \ textual documents on urban systems, aiming to reduce human resource constraints\
  \ and improve consistency. It compared two prompting methods\u2014whole-text analysis\
  \ and text chunking\u2014across three OpenAI models (GPT-4o, GPT-4o-mini, o1-mini)\
  \ using 10 case documents and a codebook of 17 digital twin characteristics."
---

# Text Chunking for Document Classification for Urban System Management using Large Language Models

## Quick Facts
- **arXiv ID**: 2504.00274
- **Source URL**: https://arxiv.org/abs/2504.00274
- **Reference count**: 40
- **Primary result**: Text chunking significantly improves LLM internal agreement and recall for urban systems document classification while maintaining precision

## Executive Summary
This study investigates the effectiveness of text chunking versus whole-text analysis for classifying urban systems documents using large language models (LLMs). The research compares two prompting methods across three OpenAI models (GPT-4o, GPT-4o-mini, o1-mini) using a codebook of 17 digital twin characteristics applied to 10 case documents. Results demonstrate that the chunking approach significantly enhances internal agreement and recall metrics for GPT-4o and o1-mini models compared to whole-text analysis, while maintaining precision levels. The study validates that GPT-4o and GPT-4o-mini achieve substantial agreement with human raters when serving as additional coders, addressing critical challenges in qualitative coding tasks for urban systems management.

## Method Summary
The study employed a comparative experimental design testing two prompting methods - whole-text analysis and text chunking - across three OpenAI language models using 10 case documents related to urban systems. A codebook containing 17 digital twin characteristics served as the coding framework for document classification tasks. The researchers measured model performance using Krippendorff's alpha for internal agreement, recall, and precision metrics, comparing results against human rater agreement to validate effectiveness.

## Key Results
- Text chunking approach significantly improved internal agreement (Krippendorff's alpha) for GPT-4o and o1-mini compared to whole-text analysis
- Chunking method enhanced recall metrics while maintaining precision levels across evaluated models
- GPT-4o and GPT-4o-mini achieved significant agreement with human raters when used as additional coders for document classification

## Why This Works (Mechanism)
Text chunking improves LLM performance by reducing context window limitations and enabling more focused analysis of specific document segments. This approach allows models to better identify and classify relevant digital twin characteristics within urban systems documents by processing information in manageable portions rather than attempting to synthesize entire documents at once.

## Foundational Learning
1. **Digital Twin Characteristics** - Framework for understanding urban systems components and their relationships; needed to establish consistent coding taxonomy for document classification
2. **Krippendorff's Alpha** - Statistical measure of inter-coder reliability; required to quantify agreement between LLM and human coding outputs
3. **Text Chunking Methodology** - Technique for dividing documents into meaningful segments; essential for managing context limitations in LLM processing
4. **Recall-Precision Tradeoff** - Performance metrics balancing completeness and accuracy; critical for evaluating coding effectiveness in qualitative analysis
5. **LLM Prompt Engineering** - Design of effective instructions for model responses; fundamental to achieving consistent coding results
6. **Urban Systems Management** - Domain knowledge of city infrastructure and operations; necessary context for relevant document classification

## Architecture Onboarding

**Component Map**: Document -> Text Chunking Module -> LLM Prompt Engine -> GPT-4o/GPT-4o-mini/o1-mini -> Classification Output -> Agreement Metrics

**Critical Path**: Document preprocessing → Text chunking → Prompt generation → LLM inference → Classification output → Agreement calculation

**Design Tradeoffs**: Whole-text analysis offers contextual completeness but suffers from context window limitations and reduced precision; chunking improves recall and agreement but may miss document-wide patterns

**Failure Signatures**: Low Krippendorff's alpha indicates poor inter-coder agreement; low recall suggests missed classifications; context window overflow causes incomplete analysis

**First Experiments**: (1) Compare chunking vs whole-text performance on documents of varying lengths; (2) Test different chunk sizes to optimize recall-precision balance; (3) Evaluate agreement stability across multiple human raters

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Small sample size of only 10 case documents limits generalizability to broader urban systems contexts
- Codebook with 17 digital twin characteristics may not capture full complexity of urban systems management topics
- Exclusive focus on OpenAI models omits comparisons with other LLM providers or open-source alternatives

## Confidence

| Claim | Confidence |
|-------|------------|
| Text chunking improves internal agreement and recall for GPT-4o and o1-mini | High |
| GPT-4o and GPT-4o-mini achieve significant agreement with human raters as additional coders | Medium |
| Chunking maintains precision while improving recall | Low |

## Next Checks
1. Replicate study with substantially larger corpus (minimum 100 documents) spanning diverse urban systems management domains to test scalability of chunking benefits
2. Compare OpenAI models against other leading LLMs (Anthropic, Google, open-source models) to establish relative performance across the broader LLM landscape
3. Validate against expert consensus coding rather than single human raters to establish ground truth accuracy for urban systems management classification tasks