---
ver: rpa2
title: Trend-Adjusted Time Series Models with an Application to Gold Price Forecasting
arxiv_id: '2601.12706'
source_url: https://arxiv.org/abs/2601.12706
tags:
- time
- trend
- series
- tats
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper reframes time series forecasting as a two-part task:
  predicting the trend (directional movement) at the next time step, and forecasting
  the quantitative value at that step. The Trend-Adjusted Time Series (TATS) model
  uses a binary classifier to predict the trend and a base time series model (such
  as LSTM or Bi-LSTM) to forecast the value.'
---

# Trend-Adjusted Time Series Models with an Application to Gold Price Forecasting

## Quick Facts
- arXiv ID: 2601.12706
- Source URL: https://arxiv.org/abs/2601.12706
- Reference count: 27
- Primary result: TATS model achieves lower MSE and higher trend detection accuracy than standard LSTM/Bi-LSTM on daily gold price data

## Executive Summary
This paper reframes time series forecasting as a two-part task: predicting the trend (directional movement) at the next time step, and forecasting the quantitative value at that step. The Trend-Adjusted Time Series (TATS) model uses a binary classifier to predict the trend and a base time series model (such as LSTM or Bi-LSTM) to forecast the value. An adjustment function then modifies the forecast based on the predicted trend. Theoretical analysis shows that TATS can reduce forecasting error when the trend predictor's accuracy exceeds the base model's trend detection accuracy. Empirical evaluation on daily gold price data demonstrates that TATS consistently outperforms standard LSTM and Bi-LSTM models, achieving lower mean squared error and higher trend detection accuracy.

## Method Summary
The TATS framework decomposes forecasting into trend classification and value regression. First, a binary classifier (XGBoost) predicts the direction of price movement at the next time step. Second, an LSTM or Bi-LSTM model forecasts the next value. The adjustment function then conditionally overrides the base forecast when its predicted trend disagrees with the more accurate trend classifier. The model uses daily gold price data from 2023 with 70/30 train/test split, incorporating lagged prices and exogenous features including market indices and exchange rates.

## Key Results
- TATS consistently outperforms standard LSTM and Bi-LSTM models on daily gold price data
- The model achieves lower mean squared error and higher trend detection accuracy
- Empirical results validate the theoretical claim that TATS reduces error when trend predictor accuracy exceeds base model accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The TATS model reduces forecasting error by conditionally overriding the base forecaster when its predicted trend disagrees with a more accurate trend classifier.
- Mechanism: The indicator function I_t checks whether (ŷ_t - y_{t-1}) * ĉ_t ≥ 0. If trends align (I_t=1), the base forecast passes through unchanged. If they conflict (I_t=0), the adjustment function replaces the forecast with y_{t-1} + ĉ_t * α, pulling the prediction toward the previous value plus a directional correction.
- Core assumption: The binary trend predictor achieves higher directional accuracy than the base value forecaster (P(D_B) > P(D_T)).
- Evidence anchors:
  - [abstract] "Theoretical analysis shows that TATS can reduce forecasting error when the trend predictor's accuracy exceeds the base model's trend detection accuracy."
  - [section 3.2] "In the fourth scenario (S4), TP correctly predicts the trend, while the VF's forecast lies in the wrong trend. In this case, the TATS model adjusts the forecast, reducing the forecasting error of VF."
  - [corpus] Neighbor papers on gold/Bitcoin price forecasting use hybrid architectures but do not formalize this specific trend-adjustment mechanism.
- Break condition: If the trend predictor's accuracy drops to ≤ the value forecaster's trend detection accuracy, the expected loss change becomes non-positive.

### Mechanism 2
- Claim: Error reduction follows from asymmetric scenario payoffs—gains from correcting wrong-trend forecasts (S4) outweigh losses from overwriting correct-trend forecasts (S2).
- Mechanism: Expected change in loss = E[|l_t - (Δy_t)²|] × [P(D_B)(1-P(D_T)) - (1-P(D_B))P(D_T)]. The term in brackets is positive iff P(D_B) > P(D_T).
- Core assumption: α is sufficiently small that adjusted forecasts approximate y_{t-1}; analysis assumes worst-case for scenario S2 (adjustment increases error).
- Evidence anchors:
  - [section 3.2.1, Proposition 3.1] Derivation shows E_t∈T(Δl_t) > 0 requires P(D_B) > P(D_T).
  - [corpus] No corpus papers replicate this theoretical framework.
- Break condition: Large α values violate the "sufficiently small" assumption, causing adjustments to overshoot and increase error (Tables 4–5 show α=20 degrades performance).

### Mechanism 3
- Claim: Trend detection accuracy (TD accuracy) captures decision-relevant information that MSE/MAE miss.
- Mechanism: TD accuracy = (1/T) Σ σ_t where σ_t = 1 if (ŷ_t - y_{t-1})(y_t - y_{t-1}) > 0. This measures directional correctness independently of magnitude error.
- Core assumption: For applications like trading or healthcare, directional accuracy has independent value; high MSE with high TD accuracy may still be practically useful.
- Evidence anchors:
  - [section 4.2] "Results in Table 3 show that the Bi-LSTM outperforms the LSTM; however, both models exhibit very poor performance in trend detection (TD accuracy is close to 50%)."
  - [appendix, Example 1] Two models with identical MSE have TD accuracies of 1 and 0 respectively—demonstrating MSE insufficiency.
  - [corpus] Corpus papers do not systematically incorporate trend detection metrics.
- Break condition: A model could achieve high TD accuracy but unacceptable MSE, limiting practical utility despite directional correctness.

## Foundational Learning

- Concept: Binary Classification vs. Regression in Time Series
  - Why needed here: TATS separates trend prediction (binary: up/down) from value prediction (regression). Understanding this distinction is essential for training and evaluating the two components independently.
  - Quick check question: Given a time series [100, 102, 98, 105], what are the binary labels for trend prediction at t=2, t=3, t=4?

- Concept: Indicator Functions and Conditional Logic in Neural Architectures
  - Why needed here: The adjustment function uses I_t as a discrete switch to route between the base forecast and the adjusted value.
  - Quick check question: If the base forecaster predicts ŷ_t = 103 when y_{t-1} = 100, and the trend predictor outputs ĉ_t = -1, what is I_t?

- Concept: Probabilistic Analysis of Expected Loss
  - Why needed here: Understanding Proposition 3.1 requires computing expected loss over four scenarios weighted by their probabilities.
  - Quick check question: If P(D_B) = 0.65 and P(D_T) = 0.52, calculate the sign of the expected loss change term.

## Architecture Onboarding

- Component map:
  - Trend Predictor (XGBoost) -> Value Forecaster (LSTM/Bi-LSTM) -> Adjustment Function (f(ŷ_t, ĉ_t, y_{t-1}, α))

- Critical path:
  1. Train value forecaster on time series → compute TD accuracy (P(D_T) estimate)
  2. Train trend predictor on directional labels → compute accuracy (P(D_B) estimate)
  3. Verify P(D_B) > P(D_T); if not, TATS provides no error-reduction guarantee
  4. Select α via validation (empirical results suggest 2–5 for gold price data)
  5. Deploy adjustment function at inference time

- Design tradeoffs:
  - Small α (≈0.1): Theoretically guaranteed error reduction if condition holds; limited correction magnitude
  - Moderate α (2–5): Better empirical performance on test set; no theoretical guarantee
  - Large α (≥20): Can significantly increase error, especially in volatile series
  - Trend predictor generalization: Training accuracy (75.14%) >> test accuracy (58.66%) indicates overfitting risk

- Failure signatures:
  - P(D_B) ≤ P(D_T) on validation data → do not deploy TATS
  - TD accuracy near 50% for base forecaster → base model provides no directional signal
  - α too large relative to typical price movements → adjustment overshoots
  - Trend predictor trained on same data as value forecaster without proper holdout → leakage

- First 3 experiments:
  1. Establish baseline: Train LSTM/Bi-LSTM on gold price data; report MSE, MAE, TD accuracy. Expect TD accuracy ≈50% if the model fails to capture direction.
  2. Trend predictor comparison: Train Logistic Regression, SVM, Random Forest, XGBoost on directional labels using lagged prices + exogenous features (FTSE, Dow Jones, S&P 500, exchange rates, oil). Select highest test-accuracy model.
  3. α sweep: Construct TATS with the best trend predictor; evaluate α ∈ {0.1, 1, 2, 5, 10, 20} on validation data. Compare training vs. test MSE reduction and TD accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating large language models (LLMs) and unstructured news data significantly improve the trend prediction accuracy ($P(D_B)$) compared to the current XGBoost baseline?
- Basis in paper: [explicit] The Conclusion states, "the trend predictor can be improved by incorporating large language models and news data."
- Why unresolved: The current study relies solely on structured numerical features (indices, exchange rates) and standard classifiers like XGBoost; it does not test unstructured text data.
- What evidence would resolve it: Empirical comparison of a text-based LLM trend predictor against the XGBoost model on the same gold price dataset, showing a statistically significant increase in directional accuracy.

### Open Question 2
- Question: Would replacing the fixed adjustment function with a learnable, flexible formulation further minimize forecasting error?
- Basis in paper: [explicit] The Conclusion suggests, "the adjustment function can be replaced with a more flexible formulation" rather than the current static equation dependent on a hyperparameter $\alpha$.
- Why unresolved: The current adjustment function is mathematically defined (Eq 3.2) but rigid; the paper does not explore learned adjustment mechanisms (e.g., a neural network layer) that might adapt to volatility better than a fixed formula.
- What evidence would resolve it: Developing a version of TATS with a parametric or neural-based adjustment mechanism that achieves lower MSE and MAE on the test set than the proposed hyperparameter-tuned model.

### Open Question 3
- Question: Does the TATS model maintain its performance advantages in non-financial domains with different data characteristics, such as healthcare?
- Basis in paper: [explicit] The Conclusion notes that "future studies can apply this approach to other areas, such as healthcare, where even small improvements in forecasting can make a significant difference."
- Why unresolved: The empirical validation is restricted to a single volatile financial time series (gold prices); it is unclear if the theoretical error reduction holds for time series with different noise profiles or seasonality.
- What evidence would resolve it: Application of the TATS model to healthcare datasets (e.g., patient vitals) demonstrating that the condition $P(D_B) > P(D_T)$ is achievable and results in lower error compared to base models.

## Limitations

- The theoretical error-reduction guarantee holds only when P(D_B) > P(D_T), but the paper does not specify how to reliably estimate these probabilities before deployment
- Empirical evaluation uses a single year of gold price data (2023), limiting generalizability to other commodities, assets, or longer time horizons
- The analysis assumes sufficiently small α values, but the optimal α range (2-5 for gold) may vary substantially across different markets or volatility regimes

## Confidence

- High confidence: The theoretical framework and its conditions for error reduction (Mechanism 1 and 2)
- Medium confidence: The empirical results showing TATS outperforms base models on gold price data
- Medium confidence: The claim that trend detection accuracy captures decision-relevant information (Mechanism 3)

## Next Checks

1. Test TATS on multiple asset classes (stocks, cryptocurrencies, commodities) across different time periods to assess robustness beyond gold price forecasting
2. Conduct ablation studies varying α systematically across different volatility regimes to identify conditions where TATS degrades performance
3. Compare TATS against state-of-the-art hybrid models (e.g., VMD-LSTM, attention-based architectures) to benchmark relative performance gains