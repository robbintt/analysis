---
ver: rpa2
title: Programmatic Representation Learning with Language Models
arxiv_id: '2510.14825'
source_url: https://arxiv.org/abs/2510.14825
tags:
- feature
- text
- chess
- image
- float
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LeaPR models combine LLM-generated programmatic features with\
  \ decision tree predictors to create interpretable, neural network-free classifiers.\
  \ The authors propose two algorithms\u2014F2 (black-box FunSearch adaptation) and\
  \ D-ID3 (white-box decision tree with on-demand feature generation)\u2014that synthesize\
  \ features as Python functions during training."
---

# Programmatic Representation Learning with Language Models

## Quick Facts
- arXiv ID: 2510.14825
- Source URL: https://arxiv.org/abs/2510.14825
- Authors: Gabriel Poesia; Georgia Gabriela Sampaio
- Reference count: 40
- Models combine LLM-generated programmatic features with decision tree predictors to create interpretable, neural network-free classifiers

## Executive Summary
LeaPR models use LLMs to synthesize programmatic features as Python functions, which are then combined using decision tree predictors to create interpretable classifiers without neural networks. The authors propose two algorithms: F2 (black-box FunSearch adaptation) and D-ID3 (white-box decision tree with on-demand feature generation). Experiments across chess position evaluation, MNIST/Fashion-MNIST image classification, and Ghostbuster text classification show LeaPR models often achieve accuracy comparable to neural baselines while maintaining data efficiency advantages. For chess, LeaPR models trained on 200k examples match 5M-example Transformer baselines; for image and text tasks, they approach or match neural network performance. The learned features are human-readable and enable interpretability via SHAP analysis.

## Method Summary
LeaPR combines LLM-generated programmatic features with decision tree predictors using two algorithms: F2 (global feature discovery via feature importance feedback) and D-ID3 (context-aware on-demand feature generation at decision tree leaves). Both methods validate features before inclusion, using Random Forest predictors for final classification. F2 trains a Random Forest, scores features by importance, and uses top-k features to guide LLM proposals. D-ID3 generates features specifically for leaf nodes given the decision path and local examples. The approach targets domains with structured representations amenable to programmatic feature extraction.

## Key Results
- Chess: LeaPR models trained on 200k examples match 5M-example Transformer baselines in move prediction accuracy
- Images: LeaPR approaches or matches neural network performance on MNIST and Fashion-MNIST classification
- Text: LeaPR achieves competitive F1 scores on Ghostbuster human vs AI text classification task

## Why This Works (Mechanism)

### Mechanism 1
LLM-generated programmatic features can substitute for learned neural representations when domain knowledge is available in training corpora. LLMs encode prior knowledge about domain-specific libraries and can synthesize feature functions that capture discriminative patterns without gradient-based optimization.

### Mechanism 2
Feature importance scoring from Random Forests provides effective feedback signal for LLM feature evolution. F2 trains a Random Forest on accumulated features, extracts importance weights, and feeds top-k features back to the LLM as examples, creating an evolutionary loop.

### Mechanism 3
On-demand feature generation at decision tree leaves produces locally optimal splits that collectively yield competitive predictors. D-ID3 provides the LLM with the decision path to a leaf node and training examples at that node, generating features specifically designed to distinguish those examples.

## Foundational Learning

- **Feature engineering vs. representation learning**: LeaPR occupies a middle ground—features are engineered but by LLMs rather than humans. Understanding this distinction clarifies what's being automated.
  - Quick check: Can you explain why a decision tree on raw pixels fails but succeeds on LLM-generated features?

- **Random Forest feature importance**: F2 relies on Gini importance or permutation importance to score features for LLM feedback. Misunderstanding these metrics leads to poor feature evolution.
  - Quick check: Why might a feature with high importance in isolation add little predictive value when combined with other features?

- **ID3 decision tree learning**: D-ID3 extends classical ID3 with on-demand feature generation. Understanding the base algorithm is prerequisite to understanding the extension.
  - Quick check: In ID3, what happens when all features at a node have zero information gain?

## Architecture Onboarding

- **Component map**: LLM proposer -> Feature validation -> Random Forest training -> Importance scoring -> Top-k selection -> Back to LLM with examples (F2); Decision tree with leaf selection -> LLM called with path + examples -> Feature proposals -> Best split selection -> Tree expansion (D-ID3)

- **Critical path**: 1) Define input type signature (e.g., chess.Board → float, np.ndarray → float, str → float) 2) Create API documentation for domain library for LLM prompts 3) Implement feature validation (timeout, exception handling, NaN/Inf detection) 4) Choose algorithm: F2 for global feature discovery, D-ID3 for context-aware feature generation 5) Set iteration budget (1000 features = 1000 D-ID3 iterations or 100 F2 batches of 10)

- **Design tradeoffs**: F2 has lower LLM cost (10x fewer calls) and better for globally useful features; D-ID3 has higher LLM cost but better for complex domains requiring context-specific features. Random Forest depth affects interpretability vs. performance.

- **Failure signatures**: Features crash on edge cases, features return constant values, D-ID3 produces degenerate trees, LLM proposes duplicate features.

- **First 3 experiments**: 1) Chess material balance baseline: Implement single feature computing material difference, train Random Forest, then run F2 for 100 iterations and measure improvement. 2) MNIST with D-ID3: Run D-ID3 for 100 iterations on binary MNIST, inspect generated features, compare to raw-pixel Random Forest. 3) Feature ablation on Ghostbuster: Train F2 model, use SHAP to identify top-5 features, retrain with only those 5 features, measure accuracy drop.

## Open Questions the Paper Calls Out

### Open Question 1
Can LeaPR models be extended to learn deep hierarchical features where programmatic features are computed from other generated features, rather than just raw input data? The authors state their methods do not learn deep hierarchical features. What evidence would resolve it: Successful implementation of recursive LeaPR architecture demonstrating improved performance.

### Open Question 2
Can the scalability of LeaPR models be improved to match neural networks in data-rich domains like chess? Current results show LeaPR plateaus while Transformer baselines continue to scale with more data. What evidence would resolve it: Modifications allowing LeaPR to effectively utilize training sets exceeding 200k examples.

### Open Question 3
How can the D-ID3 algorithm be adapted for visual domains to allow LLMs to generate features based on direct observation of image data rather than just class labels? Current vision-language models were not utilized to "see" the samples. What evidence would resolve it: Integration of VLMs into D-ID3 loop resulting in accuracy gains on benchmarks like Fashion-MNIST.

## Limitations
- Does not learn deep hierarchical features—each feature is directly computed from input but not from other features
- Scalability challenges in data-rich domains where neural networks improve predictably with more data
- D-ID3 limitation for visual domains—LLM cannot "see" images and must rely solely on prior knowledge

## Confidence

**High confidence** in mechanism 1 (LLM feature synthesis with domain libraries) - supported by multiple experiments across domains.
**Medium confidence** in mechanism 2 (feature importance-guided evolution) - novel approach with no direct corpus validation.
**Medium confidence** in mechanism 3 (on-demand local feature generation) - chess results support it, but other domains show mixed performance.

## Next Checks

1. Test LeaPR models on out-of-distribution data (e.g., chess positions from different time periods, rotated MNIST digits) to assess feature generalization.
2. Compare feature evolution trajectories between F2 and D-ID3 on identical seeds to isolate algorithmic differences from stochastic variation.
3. Conduct ablation studies removing Random Forest predictors and replacing with simpler linear models to quantify predictor contribution versus feature quality.