---
ver: rpa2
title: 'ImPart: Importance-Aware Delta-Sparsification for Improved Model Compression
  and Merging in LLMs'
arxiv_id: '2504.13237'
source_url: https://arxiv.org/abs/2504.13237
tags:
- impart
- ratio
- singular
- compression
- delta
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IMPART, an importance-aware delta sparsification
  approach for efficient model compression and merging in large language models. IMPART
  leverages singular value decomposition (SVD) to adaptively assign sparsity ratios
  to singular vectors based on their importance, as measured by singular values, ensuring
  preservation of critical task-specific knowledge.
---

# ImPart: Importance-Aware Delta-Sparsification for Improved Model Compression and Merging in LLMs

## Quick Facts
- **arXiv ID:** 2504.13237
- **Source URL:** https://arxiv.org/abs/2504.13237
- **Reference count:** 40
- **Key outcome:** IMPART achieves 2× higher compression ratio than baselines while maintaining performance, setting new state-of-the-art results when integrated with delta quantization and model merging techniques.

## Executive Summary
This paper introduces IMPART, an importance-aware delta sparsification approach for efficient model compression and merging in large language models. IMPART leverages singular value decomposition (SVD) to adaptively assign sparsity ratios to singular vectors based on their importance, as measured by singular values, ensuring preservation of critical task-specific knowledge. The method dynamically adjusts sparsity across different singular vectors, with larger singular values receiving smaller sparsity ratios to retain more information. IMPART achieves 2× higher compression ratio than baselines while maintaining performance, and sets new state-of-the-art results when integrated with delta quantization and model merging techniques. Extensive experiments across mathematical reasoning, code generation, and chat tasks demonstrate its effectiveness, achieving 95.8% of fine-tuned model performance at 93.75% sparsity.

## Method Summary
IMPART operates on delta parameters (ΔW = W_ft - W_base) extracted from fine-tuned models. The method performs SVD on each delta weight matrix to obtain singular vectors and values, then assigns sparsity ratios to each vector based on its importance (measured by singular value magnitude). A pre-pruning step removes the lowest-magnitude singular components, followed by adaptive sparsity allocation where more important vectors receive lower sparsity ratios. Parameters are randomly masked with Bernoulli sampling and rescaled by 1/(1-p_k) to preserve expected values. The method supports integration with quantization (IMPART-QT) and model merging techniques. Hyperparameters include pre-pruning ratio (β ∈ {0.6, 0.7, 0.8}) and regularization parameter (C ∈ {0.5, 1}), tuned per task type.

## Key Results
- IMPART achieves 95.8% of fine-tuned model performance at 93.75% sparsity (CR=16), setting new state-of-the-art results.
- Compared to DARE baseline, IMPART achieves 2× higher compression ratio while maintaining equivalent performance.
- Integration with IMPART-QT achieves up to 32× compression ratio with only 2.5% average performance drop across tasks.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: SVD decomposes delta parameters into ranked singular vectors where magnitude correlates with importance; adaptive sparsity allocation preserves critical information.
- **Mechanism**: SVD factorizes the delta weight matrix into UΣV^T. Larger singular values encode more task-specific information. IMPART assigns higher sparsity to vectors with smaller σ_k and lower sparsity to those with larger σ_k, selectively retaining parameters in the most important subspaces.
- **Core assumption**: Larger singular values in delta parameters correspond to more critical task-specific knowledge that must be preserved to maintain performance.
- **Evidence anchors**: Abstract states SVD is used to "adaptively assign sparsity ratios to singular vectors based on their importance, as measured by singular values." Section 3.1 explains larger singular values indicate greater importance, motivating smaller sparsity ratios for them. Related work leveraging spectral structure supports the premise that information is non-uniformly distributed.
- **Break condition**: If singular value magnitude does not correlate with task-specific importance (e.g., for tasks where knowledge is highly distributed across all spectral components), adaptive allocation will prune critical information and degrade performance more than random baselines.

### Mechanism 2
- **Claim**: Pre-pruning low-magnitude singular components improves compression efficiency by removing long-tail "noise" before adaptive sparsity allocation.
- **Mechanism**: A pre-pruning ratio (β) entirely removes singular vectors with indices greater than k = ⌊n·(1-β)⌋ by setting their sparsity p_k = 1, before the main adaptive sparsity allocation. The remaining sparsity budget is distributed among the top-k vectors.
- **Core assumption**: The smallest singular values and their corresponding vectors contribute little to task performance and can be treated as noise or redundancy to be aggressively removed.
- **Evidence anchors**: Section 3.2 states previous works show removing smallest singular components can achieve performance comparable to or better than full models, motivating the pre-pruning design. Ablation results (Table 3, ID 1 vs ID 2) show removing pre-pruning causes average performance to drop from 48.92 to 46.47. Claims rely on cited prior works.
- **Break condition**: If the long-tail singular vectors contain critical, non-redundant information for a specific task (e.g., fine-grained classification features), aggressive pre-pruning will cause significant performance collapse.

### Mechanism 3
- **Claim**: Theoretical expectation preservation via a 1/(1-p_k) rescaling factor ensures that the output of the sparsified model approximates the original fine-tuned model in expectation.
- **Mechanism**: Parameters in the k-th singular vectors are randomly set to zero with probability p_k using Bernoulli sampling. Surviving parameters are scaled by 1/(1-p_k). This ensures E[Ũ_ikṼ_kj] = U_ikV_kj, meaning the expected output of the reconstructed delta approximates the original.
- **Core assumption**: The approximation of the original model in expectation is sufficient for maintaining performance, and the added variance from random sampling does not overly disrupt inference.
- **Evidence anchors**: Section 3.3 provides full derivation showing with rescaling coefficient θ = ζ = 1/(1-p_k), the reconstructed hidden state E[ĥ_i] equals the original h_i. Ablation results (Table 3, ID 1 vs ID 5) show removing the rescale factor causes average performance to crash from 48.92 to 20.74. This principle is consistent with sparsification methods like DARE.
- **Break condition**: If the variance introduced by random sampling is too high for certain critical, low-tolerance operations (e.g., precise numerical reasoning chains), stochastic outputs may deviate excessively from the expected value, causing errors.

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD)**
  - **Why needed here**: This is the core mathematical transform used to analyze and sparsify the delta parameters. Understanding what singular values and vectors represent is essential for grasping the "importance-aware" logic.
  - **Quick check question**: For a given weight matrix ΔW = UΣV^T, what does a large value at Σ_kk tell you about the vectors U_:k and V_:k?

- **Concept: Delta Parameters and Model Fine-tuning**
  - **Why needed here**: The entire method operates on ΔW, the difference between a fine-tuned model and its base model. One must understand this core premise of delta compression.
  - **Quick check question**: Why is compressing ΔW often more efficient than compressing the full fine-tuned model parameters directly?

- **Concept: Sparsity and Compression Ratios**
  - **Why needed here**: The paper's primary metric is achieving higher compression ratios (e.g., 16x, 32x) via sparsity. You need to understand the relationship between sparsity percentage and storage savings.
  - **Quick check question**: A model is compressed to 93.75% sparsity. What is the theoretical compression ratio for the dense parameters?

## Architecture Onboarding

- **Component map**: Input (base + fine-tuned models) -> Delta Extraction (ΔW = W_ft - W_base) -> SVD Decomposition (U, Σ, V^T) -> Sparsity Ratio Allocation (pre-pruning + adaptive allocation) -> Stochastic Sparsification & Rescaling (Bernoulli mask + 1/(1-p_k)) -> Reconstruction (ΔŴ = ŨΣṼ^T) -> Optional Quantization (IMPART-QT) -> Deployment (W_served = W_base + ΔŴ)

- **Critical path**:
  1. SVD on delta weights (must be done per-layer)
  2. Solving for the scaling factor γ and boundary shift (lines 4-9 in Algorithm 1) to hit the exact target sparsity. This is the core logic ensuring the constraint is met.
  3. Applying the Bernoulli mask and rescaling to the singular vectors. Errors here invalidate the theoretical expectation guarantee.

- **Design tradeoffs**:
  - **β (Pre-pruning ratio)**: Higher β removes more low-rank "noise" early, leaving more budget for important vectors. Tradeoff: risks discarding useful long-tail features.
  - **C (Regularization parameter)**: Modulates the importance curve. Higher C emphasizes differences, giving top vectors much lower sparsity. Lower C flattens sparsity allocation. Tradeoff: task-dependent; math/code benefit from C=1, chat from C=0.5.
  - **Stochasticity**: Random sampling allows for non-structural sparsity but introduces variance. Deterministic seeding (Appendix B.3) trades some randomness for reproducibility and mask-free storage.

- **Failure signatures**:
  1. **Gibberish or repetitive outputs**: Often linked to removing the rescaling factor (ID 5 in ablation) or uniform sparsification (ID 3/4), causing a collapse of the expectation guarantee.
  2. **Catastrophic drop on specific tasks (e.g., chat)**: May indicate that the selected hyperparameters (β, C) are suboptimal for that task's structure (e.g., chat may need lower C).
  3. **Inability to hit target sparsity**: Algorithm 1's boundary shifting (line 8) must be executed correctly. Failure leads to an incorrect overall compression ratio.

- **First 3 experiments**:
  1. **Hyperparameter Sensitivity (β, C)**: Run a grid search over β ∈ {0.6, 0.7, 0.8} and C ∈ {0.5, 1} on a held-out validation set for your specific task, as done in Table 4. This is the single most critical setup step.
  2. **Ablation of Rescaling Factor**: Implement sparsification with and without the 1/(1-p_k) term on a small model (e.g., 7B) and observe the performance gap on a simple metric (e.g., GSM8K). This confirms the core theoretical contribution is implemented correctly.
  3. **Comparison with LowRank Baseline**: Implement a simple SVD truncation baseline (keeping top-r components) and compare against IMPART at the same compression ratio (e.g., 32x) to verify the gain from adaptive sparsity over simple truncation.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can layer-wise adaptive sparsification strategies further improve IMPART's compression performance? The paper explicitly states that "IMPART treats all weight matrices equally and does not consider the potential benefits of layer-wise pruning... Future work could explore fine-grained sparsification strategies for different layers and weight matrices to further enhance compression performance."
- **Open Question 2**: Can hyperparameter selection be automated without requiring a validation set? The paper notes that "IMPART requires a validation set to determine the optimal hyperparameters... it may not always lead to the optimal model due to the potential misalignment between the validation and test sets."
- **Open Question 3**: Does IMPART scale effectively to models larger than 13B parameters? All experiments use 7B-13B models; no results are reported for larger models (e.g., 70B, 100B+) where compression is even more critical.
- **Open Question 4**: Are singular values the optimal importance metric for delta sparsification? The method assumes "singular vectors associated with larger singular values encode more important task-specific information," but this may not universally hold across all task types and architectures.

## Limitations
- IMPART requires a validation set for hyperparameter tuning, which may not always align with test set performance.
- The method does not explore layer-wise adaptive sparsification strategies that could potentially improve compression efficiency.
- Experiments are limited to 7B-13B parameter models, leaving scalability to larger models unexplored.

## Confidence
- **Method correctness**: High - The mathematical framework is sound and well-supported by ablation studies.
- **Empirical results**: High - Extensive experiments across multiple tasks and model architectures show consistent improvements.
- **Reproducibility**: Medium - Key implementation details like deterministic seeding strategy and GPTQ parameters are not fully specified.

## Next Checks
1. Verify the 1/(1-p_k) rescaling factor is implemented element-wise per singular vector, not globally, by comparing performance with and without rescaling on a small task.
2. Confirm the pre-pruning ratio β is correctly applied by checking that singular vectors with indices > ⌊n·(1-β)⌋ are completely removed from consideration.
3. Test the hyperparameter sensitivity by running IMPART with different C values (0.5 vs 1) on a held-out validation set to observe the impact on task-specific performance.