---
ver: rpa2
title: Demystifying Long Chain-of-Thought Reasoning in LLMs
arxiv_id: '2502.03373'
source_url: https://arxiv.org/abs/2502.03373
tags:
- long
- reward
- reasoning
- length
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically investigates long chain-of-thought (CoT)
  reasoning in large language models through supervised fine-tuning (SFT) and reinforcement
  learning (RL) experiments. The key findings are: (1) Long CoT SFT scales better
  than short CoT SFT and enables further RL improvements; (2) RL requires careful
  reward shaping with cosine length-scaling and repetition penalties to stabilize
  CoT growth; (3) Leveraging noisy web-extracted solutions with filtering mechanisms
  effectively scales verifiable reward signals, particularly for out-of-distribution
  STEM tasks; (4) Core reasoning abilities like error correction exist in base models
  but require significant RL compute to incentivize effectively.'
---

# Demystifying Long Chain-of-Thought Reasoning in LLMs

## Quick Facts
- **arXiv ID**: 2502.03373
- **Source URL**: https://arxiv.org/abs/2502.03373
- **Authors**: Edward Yeo; Yuxuan Tong; Morry Niu; Graham Neubig; Xiang Yue
- **Reference count**: 40
- **Primary result**: Long CoT SFT + RL achieves 85.9% on MATH-500 and 26.9% on AIME 2024 with Llama-3.1-8B

## Executive Summary
This paper systematically investigates long chain-of-thought reasoning in LLMs through supervised fine-tuning (SFT) and reinforcement learning (RL) experiments. The key finding is that long CoT SFT establishes a higher performance ceiling than short CoT SFT and creates better initialization for RL. The study introduces cosine length-scaling rewards and repetition penalties to stabilize CoT growth during RL training. By leveraging noisy web-extracted solutions with filtering mechanisms, the approach effectively scales verifiable reward signals, particularly for out-of-distribution STEM tasks.

## Method Summary
The method combines long CoT supervised fine-tuning with reinforcement learning using carefully designed reward shaping. Long CoT data is distilled from QwQ-32B-Preview using rejection sampling based on answer correctness. RL employs Proximal Policy Optimization (PPO) with a cosine reward function that scales based on both correctness and CoT length, combined with N-gram repetition penalties applied per-token. The approach uses rule-based or model-based verifiers to provide verifiable reward signals, with the latter enabling scaling to noisy web-extracted data.

## Key Results
- Long CoT SFT achieves over 70% accuracy on MATH-500 and continues improving beyond 3.5B tokens, while short CoT SFT plateaus below 55%
- Cosine length-scaling rewards combined with repetition penalties stabilize CoT growth and enable emergent behaviors like branching and backtracking
- Llama-3.1-8B achieves 85.9% accuracy on MATH-500 and 26.9% on AIME 2024 when combining long CoT SFT with RL
- Models initialized with long CoT SFT show significant improvements from RL, while those with short CoT SFT see minimal gains

## Why This Works (Mechanism)

### Mechanism 1: Long CoT SFT as Superior Initialization
Long CoT SFT scales better than short CoT SFT because it establishes a higher performance ceiling by learning emergent reasoning patterns like branching, backtracking, and self-correction. This creates richer policy initialization for RL, allowing gradient-based optimization to explore within productive regions of the solution space rather than starting from shortcut-dependent policies.

### Mechanism 2: Cosine Length-Scaling Rewards
The cosine reward function stabilizes CoT growth by encoding three constraints: correct answers receive higher rewards than incorrect ones, shorter correct CoTs receive higher rewards than longer ones (compute efficiency), and longer incorrect CoTs receive higher rewards than shorter ones (encouraging continued exploration when uncertain). The per-token repetition penalty prevents reward hacking through artificial length extension.

### Mechanism 3: Latent Reasoning Skills in Base Models
Core reasoning abilities like error correction and branching are latently present in base models from pre-training data containing human dialogue patterns (discussion forums, collaborative problem-solving). RL with verifiable rewards reweights the policy toward productive skill composition by increasing sampling probability of trajectories that successfully deploy these latent skills.

## Foundational Learning

**Proximal Policy Optimization (PPO) and KL-constrained RL**: The paper uses PPO as its default policy optimization method. Understanding the balance between policy improvement (exploitation) and KL divergence constraints (staying close to reference policy) is essential for interpreting training stability.

*Quick check*: Why does decreasing the discount factor for correctness reward increase branching frequency but degrade performance?

**Rejection Sampling and Distillation**: Long CoT SFT data is created by distilling from QwQ-32B-Preview with rejection sampling based on answer correctness. Understanding this filtering mechanism is critical for evaluating data quality and potential biases.

*Quick check*: What tradeoff does rejection sampling introduce between data quality and diversity?

**Generalized Advantage Estimation (GAE) with Multiple Discount Factors**: The paper modifies standard GAE to accommodate multiple reward types (correctness, repetition penalty) with different discount factors. This architectural choice affects credit assignment across long trajectories.

*Quick check*: Why would a lower discount factor for repetition penalties be more effective than applying them as sparse end-of-trajectory rewards?

## Architecture Onboarding

**Component map**: Base model (Llama-3.1-8B or Qwen2.5-Math-7B) -> distillation from QwQ-32B-Preview -> rejection sampling -> long CoT SFT -> PPO with cosine reward + repetition penalty -> rule-based/model-based verifier

**Critical path**:
1. Curate high-quality long CoT SFT data via distillation with rejection sampling
2. Fine-tune base model until scaling plateau (monitor for overfitting)
3. Initialize RL with SFT checkpoint using cosine reward + repetition penalty
4. Tune discount factors separately for correctness reward (high γ) and repetition penalty (low γ)
5. Monitor for reward hacking (repetition increase, branching decrease)

**Design tradeoffs**:
- Context window size: Larger windows (16K) require more training compute; smaller windows (8K) may perform better with fixed compute budget
- Verifier type: Rule-based verifiers provide cleaner signals but require short-form answers and filtering; model-based verifiers handle free-form responses but introduce noise
- Data mixture: Pure gold-supervision (MATH) maximizes in-domain performance; adding noisy web-extracted data (WebInstruct) improves OOD generalization

**Failure signatures**:
- Length explosion without accuracy gain: Model exceeds context window, accuracy drops → check reward hyperparameters
- Repetition hacking: CoT length increases while "alternatively" keyword count decreases → increase repetition penalty strength
- Training collapse with REINFORCE: Instability compared to PPO → consider algorithm-specific tuning
- Short-term thinking: Rapid branching and abandonment → increase discount factor for correctness reward

**First 3 experiments**:
1. Baseline replication: SFT on long CoT data distilled from QwQ-32B-Preview (N=64 samples per prompt), then RL with cosine reward and repetition penalty. Monitor MATH-500 accuracy and CoT length distribution across 100 iterations.
2. Ablate reward shaping: Compare classic reward (correct=+1) vs. cosine reward vs. cosine + repetition penalty. Track training stability, downstream task performance, and qualitative behaviors.
3. Scale verifiable signals: Mix MATH data (gold supervision) with WebInstruct (noisy supervision) at 50/50 ratio for SFT. Compare RL performance with rule-based vs. model-based verifiers, focusing on OOD benchmarks.

## Open Questions the Paper Calls Out
None

## Limitations
- **Generalizability to non-STEM domains**: Findings primarily validated in mathematical contexts may not transfer to domains requiring different reasoning patterns like creative writing or legal reasoning
- **Compute requirements and accessibility**: Effective long CoT reasoning via RL requires significant compute resources, creating practical barriers for smaller research groups
- **Data quality and distillation artifacts**: Long CoT SFT data relies on distillation from QwQ-32B-Preview with rejection sampling, potentially introducing biases toward the teacher model's reasoning patterns

## Confidence
**High Confidence**: Long CoT SFT outperforms short CoT SFT and provides better RL initialization (robust experimental evidence)
**Medium Confidence**: Cosine length-scaling rewards and repetition penalties are effective but lack ablation studies showing why this specific functional form is optimal
**Low Confidence**: Core reasoning abilities are latently present in base models from pre-training data (based primarily on qualitative observations and indirect evidence)

## Next Checks
1. **Cross-domain generalization test**: Apply the long CoT SFT + RL pipeline to non-STEM domains (legal reasoning, creative writing, or social science problems) to assess whether learned reasoning patterns transfer
2. **Base model capacity probe**: Conduct systematic experiments varying base model size (3B, 8B, 34B parameters) to determine minimum capacity required for latent reasoning skills to be accessible via RL
3. **Alternative reward function ablation**: Replace cosine length-scaling reward with alternative formulations (linear scaling, step functions, or learned reward models) while keeping other hyperparameters constant to isolate the contribution of specific reward shaping