---
ver: rpa2
title: 'Red Teaming Contemporary AI Models: Insights from Spanish and Basque Perspectives'
arxiv_id: '2503.10192'
source_url: https://arxiv.org/abs/2503.10192
tags:
- spanish
- openai
- language
- salamandra
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study conducted red teaming sessions to evaluate bias and\
  \ safety issues in three leading AI models\u2014OpenAI o3-mini, DeepSeek R1, and\
  \ ALIA Salamandra\u2014across Spanish and Basque languages. Ten participants performed\
  \ 670 conversations, identifying 254 cases of inappropriate model behavior."
---

# Red Teaming Contemporary AI Models: Insights from Spanish and Basque Perspectives

## Quick Facts
- arXiv ID: 2503.10192
- Source URL: https://arxiv.org/abs/2503.10192
- Reference count: 30
- Primary result: Salamandra showed 50.6% failure rate, DeepSeek R1 31.7%, and OpenAI o3-mini 29.5% in bias and safety red teaming across Spanish and Basque

## Executive Summary
This study conducted red teaming sessions to evaluate bias and safety issues in three leading AI models—OpenAI o3-mini, DeepSeek R1, and ALIA Salamandra—across Spanish and Basque languages. Ten participants performed 670 conversations, identifying 254 cases of inappropriate model behavior. Salamandra showed the highest failure rate at 50.6%, followed by DeepSeek R1 at 31.7% and OpenAI o3-mini at 29.5%. Safety concerns were particularly severe in Salamandra, with failure rates reaching 83.3% in Basque. The findings reveal persistent challenges in developing trustworthy AI systems, especially for regional languages, highlighting the need for improved bias mitigation and safety measures in multilingual AI deployment.

## Method Summary
The study employed manual red teaming with ten expert evaluators who conducted 670 conversations across three sessions, testing three models (OpenAI o3-mini, DeepSeek R1, and Salamandra 7B) in Spanish and Basque. Evaluators used five red teaming strategies (language, rhetoric, possible worlds, fictionalizing, stratagems) to probe for 8 bias categories and 14 safety categories. Each conversation was planned using structured templates, and failures were validated through cross-review by different team members, achieving Cohen's kappa of 0.956. Data was collected via online forms and conversation logs, with the dataset made publicly available for future research.

## Key Results
- Salamandra 7B showed highest failure rate at 50.6% compared to DeepSeek R1 (31.7%) and OpenAI o3-mini (29.5%)
- Salamandra's safety failure rate reached 83.3% in Basque versus 43.1% in Spanish
- DeepSeek R1 had limited sample size (60 conversations) due to server availability issues
- Higher failure rates observed in bias categories related to physical appearance and socioeconomic status

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-expert red teaming with adaptive prompting uncovers vulnerabilities that static benchmarks miss.
- Mechanism: Expert evaluators iteratively refine adversarial prompts based on model responses, using domain knowledge to explore high-risk areas without predefined templates. This leverages human creativity to probe emergent behaviors.
- Core assumption: Experts can systematically cover risk categories and adapt strategies faster than automated methods.
- Evidence anchors:
  - [abstract] "ten participants applied their expertise and creativity to manually test three of the latest models"
  - [section 2.2] "RT for LLMs involves intentionally crafting adversarial inputs to expose model vulnerabilities... allowing evaluators to iteratively refine their interaction based on the previous responses of the model"
  - [corpus] Microsoft's red teaming paper confirms adaptive human probing as effective for discovering novel risks (FMR=0.64, but causal mechanism not empirically isolated from expert selection bias).
- Break condition: If expert coverage is uneven across categories or expertise varies significantly, vulnerability detection becomes inconsistent.

### Mechanism 2
- Claim: Safety alignment degrades proportionally with training data scarcity for a given language.
- Mechanism: Models trained primarily on English corpora develop safety behaviors that transfer imperfectly to lower-resource languages. The paper reports Salamandra's Basque failure rate (67.5%) exceeds its Spanish failure rate (43.1%), suggesting language-specific alignment gaps.
- Core assumption: Safety behaviors are learned from training data distribution and do not fully generalize across languages.
- Evidence anchors:
  - [section 4] "the number of safety risks found in Basque is almost as high as those found in Spanish, while the number of conversations performed was significantly smaller"
  - [abstract] "ALIA Salamandra, while supporting minority languages, demonstrated the most safety concerns, especially in Basque"
  - [corpus] Anecdoctoring paper documents similar language-specific safety gaps in multilingual red teaming (FMR=0.48, mechanism observational not interventional).
- Break condition: If models develop language-agnostic safety representations through architectural innovations (e.g., cross-lingual alignment), this degradation would diminish.

### Mechanism 3
- Claim: Smaller model capacity constrains safety alignment effectiveness relative to reasoning capability.
- Mechanism: Salamandra (7B parameters) showed 50.6% failure rate versus o3-mini's 29.5% and R1's 31.7%. The paper attributes this to Salamandra's "immature stage of development," suggesting parameter count and training scale limit safety behavior acquisition.
- Core assumption: Safety alignment requires sufficient model capacity to represent complex behavioral constraints alongside task performance.
- Evidence anchors:
  - [section 4] "Salamandra produced the poorest results, with 50.6% of its responses deemed harmful... confirms that the model remains at an immature stage of development"
  - [section 2.1] notes Salamandra models "are not intended to compete with those developed by large companies" and are "smaller, preliminary models"
  - [corpus] Insufficient direct evidence comparing parameter count to safety metrics across controlled experiments.
- Break condition: If safety alignment techniques improve to work efficiently with smaller models, or if failure rates stem from training data quality rather than capacity, this relationship would weaken.

## Foundational Learning

- Concept: Red teaming strategies (language, rhetoric, possible worlds, fictionalizing, stratagems)
  - Why needed here: The paper organizes adversarial probing into five strategy types. Understanding these helps practitioners systematically explore vulnerabilities rather than ad-hoc testing.
  - Quick check question: Can you name two red teaming strategies and give an example of how each might elicit different failure modes?

- Concept: Transformer architecture fundamentals
  - Why needed here: All three evaluated models (o3-mini, R1, Salamandra) are transformer-based LLMs. Basic understanding of attention mechanisms and training objectives contextualizes why certain failure patterns emerge.
  - Quick check question: What component of transformer training might embed societal biases from training corpora into model outputs?

- Concept: Safety categories and bias taxonomies
  - Why needed here: The study evaluated 14 safety categories (e.g., hate speech, self-harm) and 8 bias categories (e.g., race, gender). Structured taxonomies ensure reproducible coverage.
  - Quick check question: Why might a model pass safety tests in one language but fail equivalent tests in another?

## Architecture Onboarding

- Component map:
  - Red team interface: OpenWebUI (centralized deployment) or LM Studio (local deployment) for model interaction
  - Target models: OpenAI o3-mini (API), DeepSeek R1 (web chat), Salamandra 7B (Ollama Docker or local)
  - Data collection: Online forms capturing conversation logs, JSON exports for analysis
  - Validation layer: Cross-review by different team members (reported Cohen's kappa = 0.956)

- Critical path:
  1. Deploy or access target model -> 2. Plan category coverage per session -> 3. Execute adaptive prompting -> 4. Log failures with metadata -> 5. Cross-validate failures -> 6. Aggregate by model/language/category

- Design tradeoffs:
  - Centralized vs. local deployment: Mondragon used centralized GPU server (requires hardware, enables consistency); Seville used local instances (flexible, but deployment failures reduced Salamandra coverage)
  - Model availability: DeepSeek R1's frequent unavailability (server overload) forced fallback to other models, creating sample imbalance (60 R1 conversations vs. 349 o3-mini)
  - Expert vs. automated red teaming: Human experts provide creativity and contextual awareness but cannot scale; AI-assisted methods (noted in related work) offer scalability but may miss emergent behaviors

- Failure signatures:
  - Inconsistent sample sizes across models (R1 had only 60 conversations due to availability issues)
  - Configuration variance (temperature, top-p set to defaults but not controlled across deployment methods)
  - Subjective judgment in failure classification despite high inter-rater agreement

-