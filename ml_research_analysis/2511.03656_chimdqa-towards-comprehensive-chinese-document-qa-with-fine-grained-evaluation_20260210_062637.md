---
ver: rpa2
title: 'ChiMDQA: Towards Comprehensive Chinese Document QA with Fine-grained Evaluation'
arxiv_id: '2511.03656'
source_url: https://arxiv.org/abs/2511.03656
tags:
- questions
- document
- chimdqa
- evaluation
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChiMDQA introduces a Chinese multi-document QA dataset covering
  six domains (academic, education, finance, law, medical, news) with 6,068 QA pairs
  across ten fine-grained question types. It addresses the lack of comprehensive Chinese
  long-document benchmarks by providing explicit and implicit fact-based questions,
  including novel categories like filtering, statistical analysis, and computational
  reasoning.
---

# ChiMDQA: Towards Comprehensive Chinese Document QA with Fine-grained Evaluation

## Quick Facts
- arXiv ID: 2511.03656
- Source URL: https://arxiv.org/abs/2511.03656
- Authors: Jing Gao, Shutiao Luo, Yumeng Liu, Yuanming Li, Hongji Zeng
- Reference count: 0
- Primary result: ChiMDQA introduces a Chinese multi-document QA dataset with 6,068 QA pairs across six domains and ten fine-grained question types, achieving state-of-the-art results with GPT-4o (76.5 F1-score on factual, 81.2 BERTScore-F1 on open-ended questions).

## Executive Summary
ChiMDQA addresses the lack of comprehensive Chinese long-document QA benchmarks by introducing a dataset spanning six domains with 6,068 QA pairs and ten fine-grained question types. The dataset distinguishes between explicit (factual) and implicit (open-ended) questions, enabling nuanced evaluation of model capabilities. Using a multi-stage pipeline combining LLM generation with hybrid automated/manual validation, the dataset achieves high quality and diversity. Evaluation across eight models shows GPT-4o achieving 76.5 F1-score on factual questions and 81.2 BERTScore-F1 on open-ended questions, with RAG strategies improving F1-scores by ~4.6% and reducing perplexity by 81.2%.

## Method Summary
The ChiMDQA dataset is constructed through a multi-stage pipeline: document screening of 60 long-form PDFs, LLM-based QA generation with fine-grained question type definitions, collaborative multi-model pre-screening, and five-stage manual validation. The evaluation framework separates factual questions (measured via F1-Score) from open-ended questions (measured via BERTScore-F1 and Perplexity). RAG systems use BCEmbedding for retrieval and bce-reranker-base for reranking, implemented through Langchain. The dataset covers academic, education, finance, law, medical, and news domains, with questions requiring filtering, statistical analysis, and computational reasoning capabilities.

## Key Results
- GPT-4o achieves 76.5 F1-score on factual questions and 81.2 BERTScore-F1 on open-ended questions
- RAG-based systems improve F1-Score by 4.6% and reduce perplexity by 81.2% compared to non-RAG approaches
- All models exhibit hallucination rates exceeding 20%, with none surpassing 40 F1-score on complex, multi-domain questions
- Domain-specific performance varies significantly, with YAYI-30B excelling in financial tasks while GLM-4-Plus shows strength in medical domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG-based systems improve performance by grounding answers in retrieved context, increasing factual accuracy and reducing generation uncertainty
- Mechanism: The RAG system replaces parametric memory with a two-step process: retriever finds relevant segments, reranker prioritizes pertinent chunks, and generator LLM produces answers from focused context. This direct grounding boosts F1-Score on factual questions and reduces perplexity on open-ended ones by constraining generation space.
- Core assumption: BCEmbedding and bce-reranker can successfully locate correct evidence within long documents, and generator can attend to provided context over internal knowledge
- Evidence anchors: [abstract] "RAG-based systems improve F1-Score by 4.6% and reduce perplexity by 81.2%"; [Section 4.2] "For open-ended questions, RAG significantly reduced model perplexity... Overall, the average perplexity decreased by 81.2%"

### Mechanism 2
- Claim: Fine-grained evaluation framework reveals superior performance in one category doesn't guarantee it in another, with different models showing distinct strengths
- Mechanism: Separating questions into explicit (factual) and implicit (open-ended) types with different metrics prevents single aggregate scores from masking specific weaknesses. Factual accuracy uses F1-Score while open-ended quality uses semantic similarity and fluency metrics.
- Core assumption: F1-Score for factual and BERTScore-F1 for open-ended are reliable proxies for human judgment in their respective domains
- Evidence anchors: [Section 3.2] "For Non-RAG Evaluation for Factual Questions, We adopt... F1-Score... For open-ended questions, we apply... BERTScore-F1..."; [Section 4.1] Shows GPT-4o as top all-rounder but notes "Variability in Domain-Specific Performance"

### Mechanism 3
- Claim: Hybrid pipeline using LLMs for initial generation with multi-stage human-in-the-loop verification ensures high dataset quality, providing reliable model evaluation signals
- Mechanism: LLM-generated candidate QA pairs undergo collaborative multi-model pre-screening and context-sensitivity checks, followed by five-stage manual review. This hybrid approach scales dataset creation while mitigating hallucinations and ensuring factual accuracy.
- Core assumption: Automated pre-screening can catch significant portion of obvious errors, making subsequent manual review tractable and effective
- Evidence anchors: [Section 2.3, Step 3] Details "Multi-model Collaborative Pre-screening" and "five-stage cross-validation approach"; [Section 2.3, Step 4] "Results indicated an overall error rate of approximately 3%"

## Foundational Learning

### Concept: Retrieval-Augmented Generation (RAG)
- Why needed here: Primary method for tackling long-document QA in this paper
- Quick check question: How does adding a "reranker" model improve upon a basic retrieval step in a RAG pipeline?

### Concept: Fine-Grained Evaluation Metrics
- Why needed here: Main contribution is new benchmark requiring understanding of factual vs open-ended evaluation
- Quick check question: Why is F1-Score appropriate for factual questions but potentially misleading for open-ended, creative tasks?

### Concept: Context Window
- Why needed here: Dataset consists of very long documents (average 142 pages) requiring models with large context windows (128k tokens)
- Quick check question: What happens to a standard LLM's performance if you try to feed it a document larger than its context window without using RAG?

## Architecture Onboarding

### Component map
Document Parser/Indexer -> BCEmbedding Retriever -> bce-reranker -> LLM Generator -> RAGChecker Evaluator -> Factuality/Quality Metrics

### Critical path
1) Parse and chunk long PDF documents while preserving structure
2) Index them with BCEmbedding model
3) Query using RAG pipeline (retriever + reranker)
4) Run specialized evaluation scripts to obtain fine-grained scores

### Design tradeoffs
Prioritizes quality and domain diversity over raw dataset size (6,068 vs 100,000+ in SQuAD). Trades cost and latency of multi-stage human review for higher data integrity. Uses fine-grained evaluation instead of single aggregate score.

### Failure signatures
High hallucination rates (>20% even with RAG), low Context Precision (retriever fetches wrong info), poor domain adaptation (model failing on legal but not financial texts). Context window overflow causing truncated inputs or API errors.

### First 3 experiments
1) Baseline Run: Run target LLM (e.g., GPT-4o) on ChiMDQA questions without RAG to establish baseline for internal knowledge and hallucination rate
2) RAG Integration: Implement full RAG pipeline (BCEmbedding + reranker) and measure lift in F1-Score and drop in Perplexity
3) Ablation Study: Test impact of reranker by running RAG pipeline with and without it, comparing Context Precision metric

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can RAG systems be optimized to significantly reduce hallucination rate (currently >20%) when processing long Chinese documents?
- Basis in paper: [explicit] Section 4.2 notes that "all models exhibited a Hallucination rate exceeding 20," identifying factuality control as major challenge
- Why unresolved: Current RAG generation modules struggle to strictly adhere to retrieved context over long sequences
- What evidence would resolve it: Modified RAG architecture achieving hallucination rate below 10% on ChiMDQA benchmark without compromising answer completeness

### Open Question 2
- Question: What architectural advancements are required to improve RAG system F1-Scores beyond current ceiling of roughly 40 on complex, multi-domain datasets?
- Basis in paper: [explicit] Section 4.2 states "none of the models surpassed 40 in F1-Score," suggesting existing systems struggle with dataset's complexity
- Why unresolved: Standard retrieval and generation components may lack capacity for filtering, statistical analysis, and computational reasoning required by fine-grained tasks
- What evidence would resolve it: Model achieving F1-Score of 60+ on dataset, demonstrating superior handling of explicit and implicit reasoning tasks

### Open Question 3
- Question: How does dataset's restriction to "original (non-scanned) PDFs" limit robustness of evaluated models for real-world document digitization scenarios?
- Basis in paper: [inferred] Section 2.3 (Step 1) explicitly filters for "high-resolution, original (non-scanned) PDF files," potentially ignoring noise and OCR errors common in business archives
- Why unresolved: Models optimized for clean text may fail when facing layout distortions or character recognition errors typical of scanned legacy documents
- What evidence would resolve it: Comparative study evaluating top-performing models on version of ChiMDQA augmented with scanned document images or synthetic OCR noise

## Limitations
- Dataset size (6,068 QA pairs) is moderate compared to larger English benchmarks
- Five-domain focus may not capture all real-world document types
- High hallucination rates (>20%) persist even with RAG systems

## Confidence

**High Confidence**: Dataset construction methodology and multi-stage validation process (Section 2.3)

**Medium Confidence**: RAG performance improvements (Section 4.2) - results depend on specific retriever/reranker configurations

**Medium Confidence**: Fine-grained evaluation framework (Section 3.2) - metric choices are well-justified but may need domain-specific validation

## Next Checks
1. Test dataset's domain transfer by evaluating models on cross-domain question types not seen during training
2. Conduct human evaluation of BERTScore-F1 and Perplexity metrics to verify correlation with human judgment of open-ended answer quality
3. Perform ablation studies on RAG pipeline components (retriever vs reranker) to quantify individual contributions to performance gains