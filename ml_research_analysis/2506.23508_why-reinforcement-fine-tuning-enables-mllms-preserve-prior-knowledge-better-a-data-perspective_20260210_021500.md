---
ver: rpa2
title: 'Why Reinforcement Fine-Tuning Enables MLLMs Preserve Prior Knowledge Better:
  A Data Perspective'
arxiv_id: '2506.23508'
source_url: https://arxiv.org/abs/2506.23508
tags:
- training
- jigsaw
- knowledge
- prior
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why reinforcement fine-tuning (RFT) helps
  multimodal large language models preserve prior knowledge better than supervised
  fine-tuning (SFT) when learning new tasks. The authors introduce jigsaw puzzles
  as a novel task absent from pretraining and compare SFT and RFT on Qwen2.5-VL models.
---

# Why Reinforcement Fine-Tuning Enables MLLMs Preserve Prior Knowledge Better: A Data Perspective

## Quick Facts
- arXiv ID: 2506.23508
- Source URL: https://arxiv.org/abs/2506.23508
- Authors: Zhihao Zhang, Qiaole Dong, Qi Zhang, Jun Zhao, Enyu Zhou, Zhiheng Xi, Senjie Jin, Xiaoran Fan, Yuhao Zhou, Mingqi Wu, Yanwei Fu, Tao Ji, Tao Gui, Xuanjing Huang, Kai Chen
- Reference count: 40
- Primary result: RFT preserves prior knowledge better than SFT when learning new tasks, primarily due to differences in training data distribution rather than algorithmic differences

## Executive Summary
This paper investigates why reinforcement fine-tuning (RFT) helps multimodal large language models preserve prior knowledge better than supervised fine-tuning (SFT) when learning new tasks. The authors introduce jigsaw puzzles as a novel task absent from pretraining and compare SFT and RFT on Qwen2.5-VL models. They find that while SFT enables rapid task acquisition, it causes severe catastrophic forgetting of prior capabilities, whereas RFT learns more slowly but preserves prior knowledge. Through learning dynamics analysis, they show that RFT mainly reinforces correct samples naturally aligned with the base model's probability landscape, leading to weaker interference with prior knowledge. Training SFT on RFT-generated rollouts achieves similar accuracy on the new task while markedly reducing forgetting.

## Method Summary
The authors introduce jigsaw puzzles as a novel task absent from pretraining to compare RFT and SFT on Qwen2.5-VL models. They conduct experiments where both fine-tuning methods are applied to teach the model this new capability, then measure performance on both the new task and prior knowledge tasks. The key methodological innovation is generating training data for SFT using RFT rollouts, which they show can achieve similar accuracy on the new task while reducing catastrophic forgetting. The analysis focuses on learning dynamics and probability landscape alignment between the fine-tuning methods.

## Key Results
- RFT causes significantly less catastrophic forgetting of prior knowledge compared to SFT when learning new tasks
- RFT learns new tasks more slowly than SFT but achieves better long-term retention of both new and prior capabilities
- Training SFT on RFT-generated rollouts achieves similar accuracy on the new task while markedly reducing forgetting
- The distribution of training data, rather than algorithmic differences, plays a central role in mitigating catastrophic forgetting

## Why This Works (Mechanism)
The mechanism behind RFT's superior preservation of prior knowledge centers on how RFT-generated training data aligns with the base model's probability landscape. RFT naturally reinforces correct samples that are already aligned with the model's existing knowledge structure, whereas SFT introduces new samples that may conflict with prior knowledge. This alignment means RFT interferes less with the model's existing capabilities during fine-tuning. The key insight is that RFT's data generation process produces samples that are more naturally consistent with the base model's learned representations, reducing the interference that causes catastrophic forgetting.

## Foundational Learning
- **Catastrophic forgetting**: The tendency of neural networks to rapidly forget previously learned information when trained on new tasks. Why needed: Central problem being addressed in the paper. Quick check: Verify that performance on prior tasks degrades when training on new tasks.
- **Reinforcement fine-tuning (RFT)**: A fine-tuning approach using reinforcement learning rewards rather than supervised labels. Why needed: Alternative to SFT that shows better knowledge preservation. Quick check: Confirm that reward signals guide learning without explicit labels.
- **Supervised fine-tuning (SFT)**: Traditional fine-tuning using labeled training data. Why needed: Baseline method that shows severe forgetting. Quick check: Verify that SFT learns faster but forgets more than RFT.
- **Probability landscape alignment**: The degree to which new training data matches the base model's learned probability distributions. Why needed: Explains why RFT causes less forgetting. Quick check: Compare probability distributions between RFT and SFT samples.

## Architecture Onboarding
**Component Map**: Base Model -> RFT Pipeline -> Training Data -> Fine-tuning -> Evaluation
**Critical Path**: Model pretraining → Task introduction (jigsaw puzzles) → RFT/SFT application → Performance evaluation on new and prior tasks → Learning dynamics analysis
**Design Tradeoffs**: RFT trades faster learning for better knowledge preservation, while SFT prioritizes rapid acquisition at the cost of forgetting
**Failure Signatures**: Severe performance degradation on prior tasks after SFT, minimal degradation after RFT
**First Experiments**: 1) Compare SFT vs RFT performance on jigsaw puzzles, 2) Measure forgetting on prior tasks after each fine-tuning method, 3) Analyze probability landscape alignment between methods

## Open Questions the Paper Calls Out
None

## Limitations
- The exclusive focus on jigsaw puzzle tasks may limit generalizability to more complex or diverse new tasks
- The analysis of why RFT-generated rollouts are more effective than SFT rollouts is suggestive but not definitively proven
- The proposed mechanism relies on qualitative probability landscape observations rather than quantitative validation

## Confidence
- **High confidence**: The empirical finding that RFT causes less catastrophic forgetting than SFT in the tested setup
- **Medium confidence**: The claim that data distribution differences are the primary driver of forgetting differences
- **Medium confidence**: The proposed mechanism explaining why RFT-generated rollouts are more effective

## Next Checks
1. Test the forgetting patterns across multiple diverse task types beyond jigsaw puzzles to assess generalizability
2. Conduct ablation studies varying the data distribution while keeping the fine-tuning algorithm fixed to isolate the distribution effect
3. Quantify the alignment between RFT samples and the base model's probability landscape using formal metrics beyond qualitative analysis