---
ver: rpa2
title: 'Privacy Bias in Language Models: A Contextual Integrity-based Auditing Metric'
arxiv_id: '2409.03735'
source_url: https://arxiv.org/abs/2409.03735
tags:
- privacy
- llms
- information
- bias
- biases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces privacy bias as a novel metric to evaluate
  how large language models (LLMs) deviate from expected information-sharing norms.
  The core method uses multi-prompt assessment to handle prompt sensitivity, filtering
  for consistent responses across prompt variations.
---

# Privacy Bias in Language Models: A Contextual Integrity-based Auditing Metric

## Quick Facts
- arXiv ID: 2409.03735
- Source URL: https://arxiv.org/abs/2409.03735
- Reference count: 40
- The paper introduces privacy bias as a novel metric to evaluate how large language models deviate from expected information-sharing norms

## Executive Summary
This paper introduces privacy bias as a novel metric to evaluate how large language models (LLMs) deviate from expected information-sharing norms. The core method uses multi-prompt assessment to handle prompt sensitivity, filtering for consistent responses across prompt variations. Privacy biases were analyzed for eight different LLMs, revealing systematic differences based on model capacity, alignment, and quantization. The metric enables auditors to assess ethical impacts, select appropriate models, and evaluate policy compliance.

## Method Summary
The privacy bias metric uses contextual integrity theory to evaluate LLM responses against crowd-sourced expectations for appropriate information flows. The method employs multi-prompt assessment where each scenario is tested with multiple prompt variations to handle sensitivity issues. Responses are filtered for consistency across prompts, with only scenarios showing agreement being scored. The privacy bias delta (Δbias) quantifies deviation from expected values, calculated as the difference between LLM responses and crowd-sourced acceptability ratings. Regression analysis examines how LLMs respond to different information flow types including consent, storage duration, and purpose.

## Key Results
- Systematic differences found based on model capacity, with 13B models being more liberal than 7B models
- Aligned models showed higher acceptability ratings than base models
- LLMs were consistently more accepting of information flows with consent but less accepting of indefinite storage or advertising use
- Most models differed significantly from crowd-sourced expectations in privacy bias delta scores

## Why This Works (Mechanism)
The method works by leveraging contextual integrity theory, which posits that privacy violations occur when information flows violate social norms. By comparing LLM responses against crowd-sourced acceptability ratings, the metric quantifies deviation from expected behavior. The multi-prompt filtering approach ensures robustness against prompt sensitivity issues, while the Δbias score provides a standardized measure of privacy bias across different scenarios and models.

## Foundational Learning

**Contextual Integrity Theory** - Explains privacy as appropriate information flows; needed to frame what constitutes privacy violations; quick check: scenarios violate when context, actor, or purpose mismatches expectations

**Multi-prompt Assessment** - Uses multiple prompt variations per scenario to handle sensitivity; needed to ensure robust measurement; quick check: only scenarios with consistent responses across prompts are scored

**Privacy Bias Delta (Δbias)** - Quantifies deviation between LLM and human expectations; needed for standardized comparison across models; quick check: positive values indicate more permissive than expected, negative more conservative

**Information Flow Attributes** - Includes actor, recipient, topic, and purpose; needed to systematically categorize privacy scenarios; quick check: each scenario must specify all four attributes for proper analysis

## Architecture Onboarding

**Component Map**: Crowd-sourced ratings -> Scenario prompts -> LLM responses -> Multi-prompt filtering -> Δbias calculation -> Regression analysis

**Critical Path**: Scenario creation → Multi-prompt testing → Consistency filtering → Δbias computation → Model comparison

**Design Tradeoffs**: Multi-prompt filtering reduces noise but may exclude edge cases where reasonable privacy judgments differ; larger models show more consistency but may amplify systematic biases

**Failure Signatures**: Inconsistent responses across prompts indicate prompt sensitivity; high variance in Δbias scores suggests unreliable privacy judgments; systematic deviations from human expectations reveal model-specific biases

**First Experiments**:
1. Test simple scenarios with clear privacy expectations to validate the metric framework
2. Compare base vs aligned versions of the same model to verify sensitivity to alignment training
3. Vary prompt wording systematically to measure impact on consistency scores

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of only eight LLMs limits generalizability
- Focus on English-language prompts may miss cultural variations in privacy expectations
- Multi-prompt filtering may exclude valid edge cases where reasonable privacy judgments differ

## Confidence

**High confidence**: Methodological framework for privacy bias measurement and observed differences between base and aligned models

**Medium confidence**: Comparative rankings of specific models due to limited sample size

**Medium confidence**: Regression analysis showing consistent patterns, though effect sizes may vary with different prompt sets

## Next Checks
1. Expand evaluation to include at least 20 diverse LLMs spanning different architectures and training approaches
2. Conduct cross-cultural validation using multilingual prompts and regional privacy norms
3. Perform temporal validation by retesting models after additional fine-tuning cycles to assess stability