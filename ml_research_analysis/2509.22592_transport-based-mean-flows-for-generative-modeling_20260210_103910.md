---
ver: rpa2
title: Transport Based Mean Flows for Generative Modeling
arxiv_id: '2509.22592'
source_url: https://arxiv.org/abs/2509.22592
tags:
- flow
- matching
- optimal
- generation
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational cost of generative modeling
  by accelerating flow-matching inference. The authors introduce OT-MF, which integrates
  optimal transport-based couplings into mean-flow frameworks, enabling efficient
  one-step generative trajectories.
---

# Transport Based Mean Flows for Generative Modeling

## Quick Facts
- arXiv ID: 2509.22592
- Source URL: https://arxiv.org/abs/2509.22592
- Authors: Elaheh Akbari, Ping He, Ahmadreza Moradipari, Yikun Bai, Soheil Kolouri
- Reference count: 40
- One-line primary result: OT-MF achieves one-step generative trajectories via optimal transport couplings, outperforming vanilla mean flow with lower Wasserstein distances and FID scores

## Executive Summary
This paper addresses the computational cost of generative modeling by accelerating flow-matching inference. The authors introduce OT-MF, which integrates optimal transport-based couplings into mean-flow frameworks, enabling efficient one-step generative trajectories. By leveraging transport couplings, OT-MF preserves fidelity and diversity while avoiding multi-step ODE integration. The method generalizes conditional flow matching and is enhanced with scalable OT solvers like linear and hierarchical OT for training efficiency.

Experiments on low-dimensional synthetic data, image generation, point cloud generation, and image-to-image translation show that OT-MF significantly outperforms vanilla mean flow, achieving lower Wasserstein distances and FID scores. The framework offers both theoretical insights and practical improvements for fast, high-quality generative modeling.

## Method Summary
OT-MF integrates optimal transport couplings into mean-flow frameworks to enable efficient one-step generative trajectories. The method uses a loss function L(θ) = ||u_θ(t,r,x_t) - sg(u_tgt)||² where u_tgt is derived from OT-based transport plans. The training procedure samples mini-batches, solves OT plans, samples coupled points (X₀,X₁) from the transport plan, and updates parameters via gradient descent. Image models use ConvNeXt U-Net architectures with dual sinusoidal embeddings for time and step-size, while point clouds use 4×A6000 with 12-layer MLPs. OT solvers include vanilla OT, Sinkhorn, and hierarchical variants for scalability.

## Key Results
- OT-MF achieves one-step generative trajectories while maintaining fidelity and diversity
- Outperforms vanilla mean flow with lower Wasserstein distances and FID scores across multiple tasks
- Scales efficiently with OT solver variants (vanilla OT, Sinkhorn, LOT-LR, LOT-HR) for different quality-speed tradeoffs

## Why This Works (Mechanism)
OT-MF leverages optimal transport couplings to create straighter generative trajectories, reducing the need for expensive multi-step ODE integration. By integrating OT-based couplings into mean-flow frameworks, the method preserves both fidelity and diversity while enabling efficient one-step generation. The approach generalizes conditional flow matching and benefits from scalable OT solvers that balance training efficiency with generation quality.

## Foundational Learning
- Optimal Transport (OT): Mathematical framework for finding optimal coupling between distributions; needed for computing transport plans between source and target data distributions
- Mean Flow Models: Time-averaged velocity approaches for generative modeling; required understanding baseline method being improved
- Wasserstein Distance: Metric for comparing probability distributions; used for evaluation of generation quality
- JVP-based Loss: Jacobian-vector product loss formulation; referenced from [17] but details not provided
- ConvNeXt Architecture: Modern vision transformer-style architecture; used for image generation U-Net

Quick check: Verify OT coupling preserves marginal distributions and creates meaningful transport plans between data distributions.

## Architecture Onboarding

Component map: Data → OT Solver → U-Net → Loss → Parameters

Critical path: OT coupling computation → trajectory sampling → loss computation → gradient update

Design tradeoffs: Vanilla OT provides exact solutions but scales poorly (O(n³ log n)), while Sinkhorn and hierarchical variants offer speedups at potential quality costs.

Failure signatures: OT coupling becomes computational bottleneck at large batch sizes (>50% iteration time); incorrect stop-gradient implementation causes training instability.

First experiments:
1. Reproduce 2D toy experiment (Gaussian→8-Gaussians) using 3-layer MLP, Adam lr=1e-3, PythonOT for OT coupling
2. Implement MNIST latent generation with pre-trained VAE tokenizer and ConvNeXt-style U-Net
3. Compare OT solver variants (vanilla OT, Sinkhorn, LOT-LR, LOT-HR) for training time vs. generation quality tradeoffs

## Open Questions the Paper Calls Out
None

## Limitations
- JVP-based loss formulation with adaptive reweighting referenced from [17] but not fully detailed
- Architecture specifics for ConvNeXt-style U-Net deferred to [17], requiring cross-referencing
- Pretrained VAE tokenizer weights/architecture from [41] lack direct availability

## Confidence

High confidence: OT-MF framework integration, synthetic data results, general training procedure
Medium confidence: Image generation results (due to architecture dependencies on [17])
Medium confidence: Point cloud generation results (due to architecture and dataset preprocessing dependencies)

## Next Checks

1. Verify OT coupling computational scaling by profiling OT solver time across different batch sizes and solver variants (vanilla OT, Sinkhorn, LOT-LR, LOT-HR)
2. Validate stop-gradient implementation in u_tgt computation by testing for training stability and monitoring loss behavior
3. Confirm JVP-based loss formulation matches [17] by reproducing their baseline results before extending to OT-MF