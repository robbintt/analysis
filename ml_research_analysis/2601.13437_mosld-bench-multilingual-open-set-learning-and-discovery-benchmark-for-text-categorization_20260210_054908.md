---
ver: rpa2
title: 'MOSLD-Bench: Multilingual Open-Set Learning and Discovery Benchmark for Text
  Categorization'
arxiv_id: '2601.13437'
source_url: https://arxiv.org/abs/2601.13437
tags:
- classes
- language
- learning
- test
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first multilingual open-set learning
  and discovery (OSLD) benchmark for text categorization by topic, comprising 960K
  data samples across 12 languages. The benchmark is constructed by rearranging existing
  datasets and collecting new data from the news domain, with 4 known classes and
  6-10 unknown classes introduced progressively across three test sets per language.
---

# MOSLD-Bench: Multilingual Open-Set Learning and Discovery Benchmark for Text Categorization

## Quick Facts
- arXiv ID: 2601.13437
- Source URL: https://arxiv.org/abs/2601.13437
- Reference count: 21
- 960K samples across 12 languages, 4 known + 6-10 unknown classes introduced progressively

## Executive Summary
This paper introduces the first multilingual benchmark for Open-Set Learning and Discovery (OSLD) in text categorization, comprising 960K samples across 12 languages with 4 known and 6-10 unknown classes introduced progressively across three test sets. The proposed framework integrates outlier detection using energy-based scores, k-means clustering on [CLS] embeddings, TFIDF-based keyword extraction for class naming, and model retraining with pseudo-labeling. Two variants are evaluated: V1 uses standard cross-entropy, while V2 adds a contrastive loss term. Experimental results show the OSLD methods achieve 90.6-90.2% accuracy on Arabic T1, significantly outperforming baseline BERT models and demonstrating effective discovery of new classes.

## Method Summary
The benchmark uses 960K samples across 12 languages with 4 known classes per language in training. Test sets T1→T2→T3 progressively introduce 2-3 unknown classes each. The OSLD framework: (1) detects outliers using energy-based scores (top 15% flagged), (2) clusters outliers via k-means on [CLS] embeddings with k optimized by silhouette coefficient, (3) extracts TFIDF keywords to generate cluster centroids, and (4) retrains using pseudo-labeled samples (40% closest to centroids). V2 adds contrastive loss (λ=0.3, τ=0.07) to pull embeddings toward centroids. Baseline fine-tunes language-specific BERT on known classes only.

## Key Results
- Arabic baseline BERT: 51.7% T1 accuracy, drops to 37.0% by T3
- OSLD V1/V2: 90.6-90.2% accuracy on Arabic T1, demonstrating effective new class discovery
- Unknown class accuracy: 84.5% on Arabic T1 for V1, declining across stages
- V2 outperforms V1 in early stages for Hungarian (0.720 vs 0.682 on T1) but shows mixed results

## Why This Works (Mechanism)

### Mechanism 1
Energy-based scoring separates known from unknown samples by exploiting confidence differences. Lower energy scores correlate with higher prediction confidence (concentrated probability mass on known classes), while outliers produce higher energy due to dispersed logits. The top 15% highest-energy samples are flagged as potential new class instances. This works when unknown classes systematically produce less confident predictions than known-class samples.

### Mechanism 2
K-means clustering on [CLS] embeddings discovers latent class structure among detected outliers. After filtering outliers, [CLS] token representations from language-specific BERT models provide document-level features. Silhouette coefficient optimization selects k, implicitly determining the number of new classes without supervision. This works when [CLS] embeddings for samples from the same unknown class cluster together in semantic space.

### Mechanism 3
Contrastive loss (V2) anchors pseudo-labeled samples to cluster centroids, improving representation quality. After TFIDF keyword extraction generates cluster centroids, contrastive loss pulls sample embeddings toward their assigned centroid while pushing away from others. This grounds representations in semantic clustering structure before cross-entropy training. This works when centroids derived from keyword embeddings meaningfully represent class semantics.

## Foundational Learning

- **Open-Set Recognition vs. Zero-Shot Learning**: OSLD extends OSR by requiring not just detection but active discovery and incremental learning. ZSL assumes class labels are known a priori; OSLD does not. Quick check: Can your system identify that a sample belongs to no known class AND determine what new class it might represent?

- **Pseudo-labeling with Hungarian Matching**: Discovered clusters must be mapped to ground-truth labels for evaluation. Hungarian matching finds optimal one-to-one assignment without manual intervention. Quick check: How do you evaluate clustering quality when true labels are unavailable during inference?

- **Catastrophic Forgetting in Incremental Learning**: OSLD must add new classes without degrading known-class performance. The paper notes OSLD avoids restricted data access, mitigating forgetting. Quick check: If you retrain on new classes only, what happens to accuracy on original classes?

## Architecture Onboarding

- **Component map**: Energy Scorer → K-means Clusterer → TFIDF Keyword Extractor → Centroid Encoder → Contrastive Trainer (V2) → Cross-Entropy Classifier
- **Critical path**: 1) Initial fine-tuning on known classes only (4 classes per language), 2) Sequential test sets (T1→T2→T3) with progressively more unknown classes, 3) Per-test-set pipeline: detect → cluster → extract keywords → retrain
- **Design tradeoffs**: V1 (cross-entropy only) vs. V2 (contrastive + cross-entropy): V2 improves early-stage performance but adds hyperparameters (λ=0.3, τ=0.07). 15% outlier threshold is arbitrary and may over/under-detect. 40% centroid-proximal samples for retraining reduces noise but discards data.
- **Failure signatures**: Known-class accuracy drops sharply after T1 (Table 4 shows degradation from ~95% to ~80% by T3 for some languages). Unknown-class accuracy near zero for Romanian, Turkish, Spanish (Table 5) indicates discovery failure. GPT-4o "data leakage" (Section A.5) produces plausible but non-discovered labels.
- **First 3 experiments**: 1) Reproduce baseline vs. V1 vs. V2 on a single language (e.g., French) across all test stages to validate pipeline integration. 2) Ablate energy threshold (10%, 15%, 20%) to measure sensitivity of outlier detection on discovery quality. 3) Replace TFIDF centroids with mean [CLS] embeddings per cluster to isolate keyword extraction's contribution.

## Open Questions the Paper Calls Out

- **LLM Performance Across Full Benchmark**: How do large language models (e.g., GPT-4o) perform across all 12 languages and all three test stages in MOSLD-Bench? The paper evaluated GPT-4o only on French and Turkish at T1 due to API costs, leaving full benchmark coverage unevaluated.

- **Maintaining Discovery Quality Across Stages**: How can the severe performance degradation on unknown classes across test stages (e.g., Arabic dropping from 84.5% to 26.6%) be mitigated? The paper documents the degradation but doesn't explore mechanisms to maintain discovery quality over successive stages.

- **Detecting Data Leakage in LLM Evaluation**: How can data leakage from LLM pre-training be detected and controlled for fair open-set discovery evaluation? The paper identifies contamination risk but offers no methodology to ensure genuine discovery vs. memorization.

## Limitations

- Performance varies dramatically across languages, with Arabic showing substantially better results than Romanian, Turkish, and Spanish, suggesting potential issues with dataset quality or model pretraining.
- The 15% energy threshold for outlier detection is arbitrary and may not generalize well across different data distributions or languages.
- TFIDF-based keyword extraction may be suboptimal for languages with different morphological structures or where keywords don't adequately capture semantic meaning.

## Confidence

- **High Confidence**: Framework's multi-stage pipeline design and overall methodology for OSLD tasks
- **Medium Confidence**: Experimental results showing V2's superiority over V1 and baseline models
- **Low Confidence**: Assumption that TFIDF keywords adequately represent cluster semantics across all 12 languages

## Next Checks

1. **Ablation study on energy threshold**: Systematically vary the outlier detection threshold (10%, 15%, 20%) across all languages and measure its impact on discovery accuracy to identify optimal values per language.

2. **Centroid representation comparison**: Replace TFIDF keyword centroids with mean [CLS] embeddings from each cluster and retrain V2 to quantify the contribution of the keyword extraction step to overall performance.

3. **Cross-lingual generalization test**: Evaluate the Arabic-trained model on French and Spanish test sets (and vice versa) to assess whether learned discovery mechanisms transfer across languages or are language-specific.