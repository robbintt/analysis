---
ver: rpa2
title: 'Tradutor: Building a Variety Specific Translation Model'
arxiv_id: '2502.14385'
source_url: https://arxiv.org/abs/2502.14385
tags:
- portuguese
- language
- translation
- european
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method to create high-quality translation
  models for underrepresented language varieties, using European Portuguese as a case
  study. The core approach involves translating monolingual European Portuguese text
  into English using a high-quality translation system, creating a parallel corpus,
  and then fine-tuning a pre-trained language model on this data.
---

# Tradutor: Building a Variety Specific Translation Model

## Quick Facts
- arXiv ID: 2502.14385
- Source URL: https://arxiv.org/abs/2502.14385
- Authors: Hugo Sousa; Satya Almasian; Ricardo Campos; Alípio Jorge
- Reference count: 19
- Primary result: Method to create variety-specific translation models for underrepresented language varieties using synthetic parallel corpus creation via monolingual-to-resource-rich translation.

## Executive Summary
This paper addresses the challenge of building translation models for underrepresented language varieties by creating synthetic parallel corpora through translating monolingual text from the low-resource variety into a resource-rich language. The approach is demonstrated using European Portuguese, creating the largest European Portuguese-English parallel corpus (PTradutor) to date with 1.7 million documents. The method leverages high-quality translation systems that perform better translating into resource-rich languages, then fine-tunes pre-trained language models on the resulting synthetic data. The resulting models outperform open-source translation systems and achieve performance close to leading closed-source systems while maintaining fidelity to European Portuguese linguistic features.

## Method Summary
The method involves translating monolingual European Portuguese text into English using a high-quality translation system to create synthetic parallel data, then fine-tuning pre-trained instruction-tuned language models (Gemma-2, Phi-3, LLaMA-3) on this corpus. The largest model (LLaMA-3 8B) with full fine-tuning achieved the best performance. Training used standard causal language modeling with a translation prompt, batch sizes of 256-512, learning rate of 2e-5, and early stopping with 3,000-step patience. The approach was evaluated on two benchmarks (FRMT and NTrex) using BLEU, COMET, ROUGE-L for translation quality and VID score for variety fidelity.

## Key Results
- The best model (LLaMA-3 8B with full fine-tuning) outperforms all open-source translation systems and achieves performance close to Google Translate and DeepL.
- Full fine-tuning is necessary for translation quality; LoRA adapters caused repetition loops and insufficient capacity.
- There is a trade-off between translation quality and variety fidelity, with FFT models optimizing for fluency while potentially diluting variety-specific features.
- The models maintain strong fidelity to European Portuguese as measured by the language variety classifier.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Translating monolingual text from a low-resource variety to a resource-rich language creates usable synthetic parallel data for reverse translation training.
- **Mechanism:** Industry translation systems perform better translating INTO resource-rich languages than out of them. By translating European Portuguese → English, the synthetic parallel corpus inherits higher quality from the forward direction, then trains a model for the backward direction.
- **Core assumption:** The translation system's lack of variety differentiation does not substantially corrupt the meaning when translating from the low-resource variety to English.
- **Evidence anchors:** Google Translate produced identical translations for 87.2% of EP/BP pairs with BLEU 96.8%, validating minimal variety-induced corruption.

### Mechanism 2
- **Claim:** Full parameter fine-tuning (FFT) of instruction-tuned LMs on variety-specific parallel data produces higher translation quality than parameter-efficient alternatives.
- **Mechanism:** FFT updates all model weights, enabling the model to recalibrate its multilingual representations toward the target variety while maintaining translation coherence.
- **Core assumption:** The pre-trained LM has sufficient multilingual foundation that fine-tuning can redirect rather than reconstruct translation capability.
- **Evidence anchors:** LLaMA-3 FFT achieves BLEU 41.12 vs LoRA 25.42 on FRMT; Phi-3 FFT 38.16 vs LoRA 24.70.

### Mechanism 3
- **Claim:** There exists a trade-off between translation quality metrics and variety fidelity—FFT optimizes for fluency while potentially diluting variety-specific features.
- **Mechanism:** FFT models optimize for coherent output, which may default to dominant variety patterns from pre-training. LoRA models constrain updates, preserving more variety-specific vocabulary but sacrificing fluency.
- **Core assumption:** The variety classifier (VID score) accurately measures variety-specific linguistic features without bias.
- **Evidence anchors:** LoRA Phi-3 achieves VID 1.178 (above reference) with BLEU 24.70; FFT Phi-3 achieves VID 1.055 with BLEU 38.16.

## Foundational Learning

- **Concept: Causal Language Modeling for Translation**
  - Why needed here: The paper frames translation as conditional text generation rather than encoder-decoder MT; understanding this paradigm shift explains why decoder-only LMs can be fine-tuned for translation.
  - Quick check question: Can you explain why a decoder-only LM can perform translation without a separate encoder?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed here: The paper explicitly compares FFT vs LoRA, and understanding LoRA's limitations (repetition loops, insufficient capacity) is critical for architectural decisions.
  - Quick check question: What happens when LoRA rank is too low for a complex task like translation?

- **Concept: Translation Evaluation Metrics (BLEU, COMET, ROUGE)**
  - Why needed here: The paper relies on multiple metrics with different properties; COMET's potential bias toward dominant varieties affects interpretation.
  - Quick check question: Why might COMET scores favor Brazilian Portuguese even when evaluating European Portuguese translations?

## Architecture Onboarding

- **Component map:** Monolingual Corpus (EP) → Translation System (Google) → Parallel Corpus → Quality Filtering Pipeline → Instruction-Tuned LM (Gemma/Phi/LLaMA) → FFT or LoRA Training → Variety-Specific Translator → Evaluation: BLEU/COMET/ROUGE + VID (variety fidelity)

- **Critical path:**
  1. Data quality filtering is the highest-leverage step—1.9M documents removed by jusText alone
  2. Model size correlates with performance (2B < 3.8B < 8B parameters)
  3. FFT is necessary for translation quality; LoRA insufficient for this task complexity

- **Design tradeoffs:**
  | Decision | Option A | Option B | Paper's Finding |
  |----------|----------|----------|-----------------|
  | Training approach | FFT | LoRA | FFT required for quality; LoRA causes repetition |
  | Model size | Larger (8B) | Smaller (2B) | Larger consistently better |
  | Token limit | 900 max | Unlimited | 900 enables larger batch, minimal data loss (0.54%) |
  | Translation system for corpus | Google | Alternatives | Google adequate; variety-blindness minimal impact |

- **Failure signatures:**
  - **Repetition loops:** LoRA models generate same token repeatedly—indicates insufficient adapter capacity
  - **Low VID score:** Model defaulting to Brazilian Portuguese—insufficient variety-specific training signal
  - **Early plateau:** Training loss plateaus while eval increases—suggests model capacity issue or data quality problem

- **First 3 experiments:**
  1. **Validate forward translation quality:** Before building full corpus, sample 100-500 documents, translate EP→EN with chosen system, manually verify semantic preservation and measure BLEU against any existing EP-EN references.
  2. **Pilot LoRA with increased capacity:** Test LoRA with higher rank (512) and alpha (256) on subset to determine if repetition loops are capacity-bound before committing to FFT compute costs.
  3. **Variety classifier baseline:** Run the variety classifier on reference translations in your benchmark to establish VID ceiling—this prevents misinterpreting classifier limitations as model failures.

## Open Questions the Paper Calls Out

- **Open Question 1:** Would alternative generation techniques like beam search improve translation quality compared to the greedy decoding used in this study?
- **Open Question 2:** Can prompt optimization before and after model training enhance translation performance for variety-specific models?
- **Open Question 3:** How do the models perform in areas where they fall short compared to closed-source systems according to human linguists?
- **Open Question 4:** Can increased LoRA capacity (alpha and rank parameters) resolve the repetition loop problem observed in PEFT models?

## Limitations

- The method's effectiveness critically depends on the quality of the initial translation system, which may not generalize to other language pairs or more divergent varieties.
- COMET and other evaluation metrics may inherently favor dominant language varieties, potentially overstating translation quality improvements.
- The approach is validated only for European Portuguese, a relatively high-resource language; performance for truly low-resource varieties remains untested.

## Confidence

- **High Confidence:** The core methodology works for European Portuguese given the demonstrated data quality and translation system characteristics.
- **Medium Confidence:** The superiority of full fine-tuning over LoRA for this task, based on empirical results showing LoRA repetition loops and insufficient capacity.
- **Medium Confidence:** The trade-off between translation quality and variety fidelity, though the VID metric's reliability introduces uncertainty.
- **Low Confidence:** Generalization to other language pairs and varieties, particularly those with greater divergence or more limited resources.

## Next Checks

1. **Forward Translation Quality Validation:** Sample 200-500 documents from the monolingual European Portuguese corpus, translate them to English using Google Translate, then back-translate to Portuguese using a strong open-source system. Measure semantic preservation via human evaluation and BLEU against original texts.

2. **Classifier Bias Quantification:** Run the Portuguese variety classifier on a balanced corpus containing European and Brazilian Portuguese references. Calculate classification accuracy and determine if the classifier shows systematic bias toward either variety.

3. **Cross-Variety Generalization Test:** Apply the methodology to a different language pair with known variety divergence (e.g., Canadian French vs. European French, or Latin American Spanish vs. Iberian Spanish). Use a smaller monolingual corpus (100k-500k documents) to test scalability and evaluate whether the approach maintains effectiveness with reduced data volumes and different variety characteristics.