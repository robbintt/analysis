---
ver: rpa2
title: Dense associative memory for Gaussian distributions
arxiv_id: '2509.23162'
source_url: https://arxiv.org/abs/2509.23162
tags:
- gaussian
- wasserstein
- retrieval
- lemma
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends dense associative memories from vector spaces
  to the Bures-Wasserstein space of Gaussian distributions. The authors propose a
  log-sum-exp energy functional that operates directly on probability measures and
  define retrieval dynamics using weighted optimal transport maps.
---

# Dense associative memory for Gaussian distributions

## Quick Facts
- arXiv ID: 2509.23162
- Source URL: https://arxiv.org/abs/2509.23162
- Authors: Chandan Tankala; Krishnakumar Balasubramanian
- Reference count: 40
- Primary result: Novel extension of dense associative memories to probability distributions using Bures-Wasserstein geometry

## Executive Summary
This paper extends dense associative memories (DAMs) from vector spaces to the Bures-Wasserstein space of Gaussian distributions. The authors propose a log-sum-exp energy functional that operates directly on probability measures and define retrieval dynamics using weighted optimal transport maps. They prove exponential storage capacity and establish quantitative retrieval guarantees under Wasserstein perturbations. The method achieves accurate retrieval on synthetic and real-world Gaussian embeddings, including words, sentences, and images, demonstrating the advantages of respecting the geometric structure of probability distributions.

## Method Summary
The authors introduce a novel framework for dense associative memory that operates on probability distributions rather than vectors. They define an energy functional using log-sum-exp over probability measures and implement retrieval dynamics through weighted optimal transport maps. The approach leverages the Bures-Wasserstein geometry of Gaussian distributions to enable memory-augmented probabilistic reasoning. The framework bridges classical DAMs with modern distributional representations, allowing for uncertainty-aware generative computation and retrieval of stored probability distributions.

## Key Results
- Exponential storage capacity for Gaussian distributions under the proposed energy functional
- Quantitative retrieval guarantees established under Wasserstein perturbations
- Successful retrieval demonstrated on synthetic datasets and real-world Gaussian embeddings (words, sentences, images)
- Showcasing advantages of respecting the geometric structure of probability distributions over vector-based approaches

## Why This Works (Mechanism)
The method works by operating directly on the geometric structure of probability distributions rather than their vector representations. By using the Bures-Wasserstein metric and optimal transport theory, the approach naturally captures the geometry of Gaussian distributions, enabling more meaningful similarity comparisons and retrieval operations. The log-sum-exp energy functional provides a differentiable measure of similarity that can be optimized for retrieval, while the weighted optimal transport maps enable smooth transition dynamics between distributions.

## Foundational Learning

1. **Bures-Wasserstein Geometry**
   - Why needed: Provides natural metric for comparing Gaussian distributions
   - Quick check: Verify that the Bures distance reduces to Euclidean distance for matched covariances

2. **Optimal Transport Theory**
   - Why needed: Enables definition of meaningful retrieval dynamics between distributions
   - Quick check: Confirm transport maps preserve mass and boundary conditions

3. **Log-Sum-Exp Energy Functionals**
   - Why needed: Creates differentiable similarity measure for probability measures
- Quick check: Verify convexity properties of the energy functional

## Architecture Onboarding

Component Map: Energy Functional -> Optimal Transport Maps -> Retrieval Dynamics -> Storage Capacity

Critical Path: The energy functional evaluation drives the optimal transport computation, which determines the retrieval dynamics and ultimately enables the storage and recall of distributions.

Design Tradeoffs: The method trades computational complexity (high-dimensional optimal transport) for geometric fidelity and meaningful uncertainty handling in distribution retrieval.

Failure Signatures: Retrieval failure occurs when optimal transport maps become ill-conditioned or when distributions are too dissimilar for meaningful transport.

First Experiments:
1. Verify retrieval on synthetic 2D Gaussian distributions with known ground truth
2. Test scalability by increasing Gaussian dimensionality systematically
3. Compare retrieval accuracy against vector-based DAMs on identical datasets

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Theoretical framework relies on smoothness assumptions of optimal transport maps that may not hold for complex high-dimensional distributions
- Computational complexity scales poorly with dimension due to Wasserstein distance and transport map calculations
- Limited empirical validation scope, focusing primarily on controlled synthetic datasets and specific real-world applications
- No investigation of numerical stability or finite sample size impacts on retrieval accuracy

## Confidence

Theoretical guarantees:
- Storage capacity: High (under idealized conditions)
- Retrieval guarantees: Medium (depends on optimal transport approximation quality)

Empirical results:
- Medium (limited scope of experiments, no baseline comparisons)

## Next Checks

1. Evaluate retrieval performance on high-dimensional Gaussian distributions (e.g., 100+ dimensions) to assess scalability and potential breakdown of theoretical assumptions.

2. Test the method on noisy or corrupted real-world distributions to evaluate robustness and compare against baseline approaches.

3. Investigate the impact of finite sample sizes and numerical approximations on retrieval accuracy, particularly for distributions with complex geometries.