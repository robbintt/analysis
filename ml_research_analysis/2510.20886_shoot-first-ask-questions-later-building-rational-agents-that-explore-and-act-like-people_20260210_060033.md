---
ver: rpa2
title: Shoot First, Ask Questions Later? Building Rational Agents that Explore and
  Act Like People
arxiv_id: '2510.20886'
source_url: https://arxiv.org/abs/2510.20886
tags:
- questions
- board
- answer
- question
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework for evaluating and improving language
  models' (LMs) ability to act as rational information-seeking agents. The authors
  introduce Collaborative Battleship, a dialogue-based game where a partially-informed
  Captain must balance exploration (asking questions) and action (taking shots) to
  find hidden ships, while a fully-informed Spotter provides yes/no answers.
---

# Shoot First, Ask Questions Later? Building Rational Agents that Explore and Act Like People

## Quick Facts
- arXiv ID: 2510.20886
- Source URL: https://arxiv.org/abs/2510.20886
- Reference count: 40
- Primary result: Bayesian inference strategies enable weaker LMs to outperform humans and frontier models in information-seeking dialogue games

## Executive Summary
This paper introduces a framework for evaluating and improving language models' ability to act as rational information-seeking agents. The authors create Collaborative Battleship, a dialogue-based game where a partially-informed Captain must balance exploration (asking questions) and action (taking shots) to find hidden ships, while a fully-informed Spotter provides yes/no answers. Through human experiments and the BATTLESHIPQA dataset, they establish baselines for human-like information seeking. The key contribution is demonstrating that Bayesian-inspired inference strategies—language-to-code translation for grounded reasoning and Monte Carlo sampling for information maximization—can dramatically improve LM performance in this setting. Combined, these methods enable weaker LMs like Llama-4-Scout to outperform both humans and frontier models at approximately 1% of GPT-5's cost.

## Method Summary
The framework combines three key innovations: (1) Language-to-code translation converts natural language questions into executable Python functions for precise spatial reasoning, improving Spotter accuracy by up to 14.7% over baseline approaches. (2) Monte Carlo sampling strategies generate multiple candidate questions and select them by maximizing expected information gain (EIG), increasing per-question information yield by up to 0.227 bits (94.2% of theoretical ceiling) while eliminating redundant questions. (3) Sequential Monte Carlo (SMC) particle filtering maintains a weighted approximation of the belief state over valid board configurations, enabling explicit Bayesian updates after each observation. The Captain agent uses one-step lookahead with discount factor γ to balance immediate action value against expected future information value, while the Spotter agent answers questions by executing translated code against explicit board representations.

## Key Results
- Language-to-code translation improves Spotter QA accuracy by 13.2-14.7% absolute points over direct LM answering
- Q_Bayes (Monte Carlo sampling) increases EIG by up to 0.227 bits (94.2% of ceiling) while reducing redundant questions from 18.5% to 0.2% for Llama-4-Scout
- Combined Bayesian methods yield +0.303-0.374 F1 improvement in targeting accuracy and enable Llama-4-Scout to outperform humans (8%→82% win rate) and GPT-5 (0%→67% win rate) at ~1% of GPT-5's cost
- Framework generalizes to Guess Who? with +42.4 p.p. improvement for Llama-4-Scout and +28.3 p.p. for GPT-4o

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-to-code translation improves grounded question-answering accuracy by up to 14.7% over direct LM baselines.
- Mechanism: Natural language questions are translated to executable Python functions that operate on explicit board representations (numpy arrays), enabling precise logical operations rather than relying on implicit LM reasoning about spatial and state-dependent concepts.
- Core assumption: Questions can be formalized as deterministic functions mapping board states to Boolean answers; LMs can generate syntactically correct code even when they fail at direct reasoning.
- Evidence anchors: [abstract] "For Spotter agents, language-to-code translation improves accuracy by up to 14.7% over baseline approaches." [section §4.2] "Across models (Fig. 3b), we find that code improves Spotter QA by 13.2% absolute accuracy points over Base; combining code generation with chain-of-thought (CoT + Code) yields even greater 14.7% gains."

### Mechanism 2
- Claim: Sampling multiple candidate questions and selecting by expected information gain (EIG) increases per-question information yield by up to 0.227 bits (94.2% of theoretical ceiling) while eliminating redundant questions.
- Mechanism: The LM generates K candidate questions; each question is translated to code and executed against a particle-based belief state to compute EIG. The question maximizing EIG is selected. This converts the LM from a single-shot generator to a proposal distribution that is refined by explicit information-theoretic scoring.
- Core assumption: LMs can generate a diverse set of reasonable candidate questions even if they cannot reliably identify the optimal one; the belief state approximation is sufficiently accurate for relative EIG comparisons.
- Evidence anchors: [abstract] "Monte Carlo sampling strategies increase expected information gain (EIG) by up to 0.227 bits (94.2% of the information-theoretic ceiling) while dramatically reducing redundant questions." [section §4.3.1] "Q Bayes significantly reduces the proportion of redundant questions (EIG = 0) asked by Llama-4-Scout (18.5%→0.2%) and GPT-4o (14.6%→1.2%)."

### Mechanism 3
- Claim: Bayesian belief updates via sequential Monte Carlo (SMC) particle filtering enable weaker LMs to outperform frontier models by explicitly tracking hypothesis uncertainty.
- Mechanism: The agent maintains N weighted particles (valid board configurations). After each answer, particles are reweighted by compatibility with the observation under a noise model (binary symmetric channel). Move selection marginalizes over particles to compute hit probabilities rather than relying on the LM's implicit uncertainty representation.
- Core assumption: The space of valid board configurations can be adequately sampled; the noise parameter ε can be calibrated (paper uses ε=0.1 from Spotter accuracy); particle count is sufficient for belief approximation.
- Evidence anchors: [section §3] "We therefore maintain a weighted particle approximation {(s_j, w_j^(t))}_{j=1}^N which is updated via sequential Monte Carlo (SMC; Doucet et al., 2001) with per-turn resampling." [section §4.3.1] "Combined, these components yield sharper targeting (+0.303–0.374 F1), and enable weaker LMs, such as Llama-4-Scout, to outperform both humans (8% → 82% win rate) and frontier models (0% → 67% win rate vs. GPT-5)."

## Foundational Learning

- Concept: **Expected Information Gain (EIG)**
  - Why needed here: Central metric for question selection; quantifies how much a question reduces uncertainty about the hidden state. EIG is maximized when questions have ~50% probability of yes/no answers under current belief.
  - Quick check question: Given a belief where 80% of particles predict "yes" to question Q, would Q have higher or lower EIG than a question where 50% predict "yes"?

- Concept: **Sequential Monte Carlo (Particle Filtering)**
  - Why needed here: Approximates intractable posterior distributions over board configurations. Enables Bayesian updates without exhaustive enumeration of ~2^30 valid boards.
  - Quick check question: After observing an answer that eliminates 90% of particles as incompatible, what two operations does SMC perform to prevent particle degeneracy?

- Concept: **Explore-Exploit Tradeoff with Information-Value Discounting**
  - Why needed here: Agents must decide when to ask questions (explore) vs. take shots (exploit). One-step lookahead with discount factor γ balances immediate action value against expected future information value.
  - Quick check question: If γ=0.8 and asking a question yields expected next-turn hit probability of 0.6 while current best shot has probability 0.5, should the agent ask or shoot?

## Architecture Onboarding

- Component map:
  Captain Agent: Question Generator (LM) → Question Translator (LM) → Belief State (SMC particles) → EIG Computer → Move Selector → Decision Module
  Spotter Agent: Question Translator (LM) → Executor

- Critical path:
  1. Captain observes partial board → generates K questions → translates to code
  2. For each question: execute against all N particles → compute yes-probability p_t → compute EIG
  3. Select question with max EIG → Spotter answers via code execution
  4. Update particle weights by answer compatibility → resample if effective sample size low
  5. Decision: if γ × expected_post_question_hit > current_best_hit, ask; else shoot

- Design tradeoffs:
  - **Particle count N**: Higher N improves belief accuracy but increases per-turn latency
  - **Candidate questions K**: More samples improve EIG but increase API costs
  - **Discount factor γ**: Lower values prefer acting sooner; γ=1 is pure myopic information maximization
  - **Noise parameter ε**: Must calibrate from Spotter accuracy; paper uses ε=0.1 for GPT-5 Spotter

- Failure signatures:
  - **Redundant questions (EIG=0)**: Indicates LM not tracking what it already knows; addressed by Q_Bayes reranking
  - **Front-loaded questions**: Weak LMs exhaust question budget early; D_Bayes lookahead distributes questions across game
  - **Context-dependent question failures**: LMs degrade on discourse/stateful questions (60-68% accuracy vs 92% human); code translation partially mitigates but pragmatic reasoning remains weak

- First 3 experiments:
  1. **Spotter baseline**: Evaluate LM accuracy on BATTLESHIPQA with Base, CoT, Code, and CoT+Code conditions. Expect 10-15% gains from code generation, especially on stateful questions.
  2. **Captain ablation**: Run LM-only, +Q_Bayes, +M_Bayes, +QMD_Bayes conditions. Expect Q_Bayes alone improves EIG but not F1; M_Bayes required to convert information to better moves.
  3. **Cross-domain transfer**: Apply same architecture to Guess Who? or similar task with combinatorial hypothesis space. Verify that EIG computation and belief updates generalize without task-specific engineering.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can inferring the noise parameter ε adaptively from observed behavior improve agent performance when collaborating with Spotters of varying reliability?
- Basis in paper: [explicit] The authors state: "In place of a fixed ε, a more robust approach would be to infer ε to account for the differences in reliability across individual Spotters."
- Why unresolved: The current framework uses a fixed flip probability (ε=0.1) calibrated from GPT-5's accuracy, but human players and different LMs have varying error rates.
- What evidence would resolve it: Experiments comparing fixed vs. adaptive ε estimation across multiple Spotter types, measuring improvements in Captain targeting accuracy.

### Open Question 2
- Question: How can Bayesian inference strategies scale to domains where the generative world model must be learned rather than hand-specified?
- Basis in paper: [explicit] The authors note: "While implementable by hand in a domain like Battleship, in more general settings, we may wish to learn a generative model of world states represented as code."
- Why unresolved: The current approach relies on a known hypothesis space (valid board configurations) with tractable sampling; real-world scientific or diagnostic settings may not admit such structure.
- What evidence would resolve it: Demonstration of learned generative models (e.g., via VAEs or model synthesis architectures) that enable effective EIG computation in novel domains.

### Open Question 3
- Question: Why do weaker LMs fail to effectively leverage high-EIG questions into improved game performance?
- Basis in paper: [explicit] The paper observes: "Asking high-EIG questions is not sufficient to guarantee strong game performance... weaker models may not be able to effectively leverage information into accurate moves."
- Why unresolved: Q_Bayes improves EIG significantly but yields only marginal F1 gains for Llama-4-Scout and GPT-4o, while M_Bayes (which marginalizes over hypotheses) provides larger gains.
- What evidence would resolve it: Analysis of how different models update their internal belief states after receiving answers, potentially using probing techniques or comparing explicit vs. implicit belief representations.

## Limitations

- **Particle Count Unspecified**: The exact number of SMC particles used is not detailed, making it difficult to assess computational efficiency and belief approximation fidelity.
- **Hyperparameter Sensitivity**: Key parameters like discount factor γ, candidate question count k, and resampling thresholds are unspecified, raising questions about robustness.
- **Generalization Scope**: While results transfer to Guess Who?, the framework's scalability to domains with larger hypothesis spaces or more complex question semantics remains unclear.

## Confidence

- **High Confidence**: The core mechanism of language-to-code translation for improving Spotter accuracy is well-supported by ablation results (14.7% gain) and grounded in established practices in executable semantic parsing.
- **Medium Confidence**: The Monte Carlo sampling strategy for Captain agents shows strong empirical gains (0.227 bits EIG, 94.2% ceiling) but relies on the assumption that belief particle approximations are sufficiently accurate for relative EIG comparisons.
- **Low Confidence**: The claim that Bayesian SMC belief tracking is the primary reason weaker LMs outperform frontier models is plausible but not definitively proven, as other factors (e.g., more careful question selection) may contribute.

## Next Checks

1. **Particle Count Sensitivity**: Systematically vary the number of SMC particles (e.g., N ∈ {50, 100, 200, 500}) and measure impacts on EIG estimation accuracy, redundant question rate, and targeting F1 to determine the point of diminishing returns.
2. **Generalization to Noisy Spotters**: Evaluate the Captain's performance when the Spotter has varying error rates (ε ∈ {0.05, 0.1, 0.2}) to test the robustness of the SMC-based belief updates and the calibration of the noise parameter.
3. **Cross-Domain Evaluation**: Apply the full framework (language-to-code + Monte Carlo + SMC) to a third information-seeking domain (e.g., 20 Questions with object categories, or a grid-based maze navigation task) to assess the generality of the Bayesian inference approach beyond Battleship and Guess Who?.