---
ver: rpa2
title: 'Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large
  Language Models'
arxiv_id: '2507.11882'
source_url: https://arxiv.org/abs/2507.11882
tags:
- languages
- llms
- data
- instruction
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Marco-Bench-MIF, a multilingual extension
  of the IFEval instruction-following benchmark covering 30 languages. The authors
  address the limitations of existing multilingual datasets that rely on simple machine
  translation, which often fails to capture linguistic and cultural nuances.
---

# Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models

## Quick Facts
- arXiv ID: 2507.11882
- Source URL: https://arxiv.org/abs/2507.11882
- Reference count: 5
- A multilingual extension of IFEval covering 30 languages, revealing significant performance gaps between high- and low-resource languages and between proprietary and open LLMs

## Executive Summary
Marco-Bench-MIF addresses critical gaps in multilingual instruction-following evaluation by extending the IFEval benchmark to 30 languages through a hybrid pipeline of automated translation and human verification. The benchmark reveals that model scale strongly correlates with performance (70B+ models achieve 45-60% higher accuracy than 8B models), and that proprietary models like GPT-4o significantly outperform open LLMs, particularly on low-resource languages (25-35% gap). The study finds that machine-translated evaluation data underestimates model capability by 7-22% compared to culturally localized data, highlighting the importance of high-quality multilingual evaluation frameworks for developing globally capable language models.

## Method Summary
The authors constructed Marco-Bench-MIF by extending the English IFEval benchmark to 30 languages using a hybrid pipeline that combines automated translation with two rounds of human verification and cultural adaptation. The process involved categorizing and filtering constraints from the original 541 instruction-response pairs, then localizing them through lexical substitution, topical transposition, and pragmatic restructuring to account for linguistic and cultural nuances. Three LLM consensus and human review ensured quality control, followed by rule-based evaluation using strict and loose metrics to assess instruction adherence. The benchmark was evaluated across 20+ LLMs including both open models (Llama3.1, Qwen2.5) and proprietary models (GPT-4o, Claude3.5-Sonnet) to systematically measure multilingual instruction-following capabilities.

## Key Results
- Model scale strongly correlates with multilingual instruction-following accuracy, with 70B+ models achieving 45-60% higher accuracy than 8B models
- Proprietary models (GPT-4o, Claude3.5-Sonnet) significantly outperform open LLMs by 25-35% on low-resource languages
- Machine-translated evaluation data underestimates model accuracy by 7-22% compared to localized data

## Why This Works (Mechanism)

### Mechanism 1: Model Scale Correlates with Multilingual Instruction-Following Accuracy
- **Claim:** Increasing model scale substantially improves multilingual instruction-following accuracy, especially for complex, compositional constraints.
- **Mechanism:** Larger models (70B+ parameters) have greater capacity to internalize cross-lingual representations and compositional reasoning patterns, allowing them to better handle multiple constraints simultaneously and across languages.
- **Core assumption:** The performance gains are primarily driven by parameter count and model capacity, not solely by differences in training data or architecture.
- **Evidence anchors:**
  - [abstract] "model scale strongly correlates with performance (70B+ models achieve 45-60% higher accuracy than 8B models)"
  - [section 4.3.1] "we observe a strong correlation between model scale and performance, with larger models (70B+ parameters) achieving 45-60% higher absolute accuracy than 8B LLMs."
  - [corpus] Related work (e.g., M-IFEval, MaXIFE) also notes scale dependencies in multilingual evaluation, though the precise percentages differ by benchmark.
- **Break condition:** Performance gains diminish or reverse for certain low-resource languages or specific constraint types (e.g., script-specific challenges) as scale increases.

### Mechanism 2: Proprietary Models Outperform Open LLMs, Particularly on Low-Resource Languages
- **Claim:** Proprietary LLMs (GPT-4o, Claude3.5-Sonnet, Gemini) achieve significantly higher accuracy on low-resource languages compared to open LLMs, with a 25-35% accuracy gap.
- **Mechanism:** Proprietary models benefit from more extensive and higher-quality multilingual training data, advanced alignment techniques (RLHF, specialized fine-tuning), and potentially more sophisticated tokenization strategies for diverse scripts.
- **Core assumption:** The observed performance gap is attributable to differences in training data, alignment, and model architecture, not just model size.
- **Evidence anchors:**
  - [abstract] "proprietary models (GPT-4o, Claude3.5-Sonnet) significantly outperform open LLMs by 25-35% on low-resource languages"
  - [section 4.3.2] "performance on high-resource languages (de, zh) is 75-85% accuracy versus 50-60% for low-resource languages (yo, ne), with script-specific challenges for some languages like Arabic (ar)."
  - [corpus] Corpus neighbors (e.g., Multi-lingual Functional Evaluation for Large Language Models) discuss multilingual competence gaps but do not consistently quantify this 25-35% proprietary gap.
- **Break condition:** A new open LLM is released with comparable multilingual training and alignment, narrowing the gap.

### Mechanism 3: Localized Evaluation Data Reveals Capabilities Masked by Machine-Translated Data
- **Claim:** Evaluating LLMs on culturally and linguistically localized instruction data yields higher and more accurate performance estimates than using machine-translated data.
- **Mechanism:** Localization adapts linguistic constraints (e.g., capitalization for Chinese) and cultural references (e.g., region-specific entities), making instructions more natural and reducing translation artifacts that can confuse the model.
- **Core assumption:** The "underestimation" by MT data is due to translation artifacts and a lack of cultural grounding, not model incapability.
- **Evidence anchors:**
  - [abstract] "machine-translated data underestimates accuracy by 7-22% versus localized data"
  - [section 3.2.2] Details the localization steps: Lexical Substitution, Topical Transposition, Pragmatic Restructuring.
  - [section 4.3.4] "evaluations based exclusively on MT data can be misleading... for low-resource languages like Yoruba the gap is more larger"
  - [corpus] Corpus evidence for this specific 7-22% underestimation figure is weak or absent; related benchmarks emphasize fine-grained constraint analysis but do not report this exact metric.
- **Break condition:** Machine translation quality improves to a level where it is indistinguishable from high-quality human localization.

## Foundational Learning

- **Concept: Instruction-Following Evaluation (IFEval)**
  - **Why needed here:** Marco-Bench-MIF is a direct extension of the IFEval benchmark. Understanding its core protocol (prompting with instructions and verifying response adherence) is critical.
  - **Quick check question:** Can you explain the difference between prompt-level and instruction-level accuracy in an IFEval-style evaluation?

- **Concept: Cultural and Linguistic Localization**
  - **Why needed here:** This paper's core contribution is a localization pipeline. Understanding why and how to adapt instructions for different cultures is key.
  - **Quick check question:** Why is a direct translation of an English prompt like "capitalize all letters" problematic for a language like Chinese?

- **Concept: High- vs. Low-Resource Languages**
  - **Why needed here:** A key finding is the performance gap between language types. Understanding the concept of resource availability in NLP is crucial for interpreting results.
  - **Quick check question:** Name a low-resource language from the benchmark and explain the typical challenges LLMs face with it compared to a high-resource language like German.

## Architecture Onboarding

- **Component map:** IFEval (Source Benchmark) -> Preprocessing Module (Categorize constraints) -> Translation & Localization Pipeline (Hybrid system with human verification) -> Post-processing & Validation (Error detection and human review) -> Evaluation Harness (Response generation and rule-based verification)

- **Critical path:**
  1. Categorize constraints from IFEval
  2. Translate instructions
  3. **Localize** instructions (core innovation): adapt for linguistic/cultural context
  4. Verify and refine localized data via human review
  5. Generate model responses
  6. Evaluate responses using strict and loose metrics

- **Design tradeoffs:**
  - **Speed vs. Quality:** The two-round human verification significantly slows dataset creation but is essential for high-quality localization
  - **Cost vs. Coverage:** Localizing all 30 languages for all samples is expensive. The paper uses full localization for a subset (5 languages) and strategic sampling for the remaining 24
  - **Automation vs. Control:** LLMs are used for initial content regeneration and error detection, but human review provides the final quality gate

- **Failure signatures:**
  - **Keyword Consistency Failure:** Model fails to use a translated keyword the correct number of times (e.g., "ensure the word 'sneaker' appears at least 10 times" after translation to Yoruba)
  - **Compositional Constraint Failure:** A 10-20% gap between instruction-level and prompt-level accuracy indicates a failure to adhere to all instructions in a multi-constraint prompt simultaneously
  - **Script-Specific Failure:** Performance drops for right-to-left scripts (Arabic, Hebrew) or non-Latin scripts compared to Latin-script languages

- **First 3 experiments:**
  1. **Baseline Reproduction:** Run the Marco-Bench-MIF evaluation suite on a base model (e.g., LLaMA3.1-8B) to reproduce the reported instruction-level and prompt-level accuracy numbers
  2. **Localization Ablation:** Evaluate the same model on a subset of data (e.g., Spanish and Yoruba) using both the localized version and a machine-translated-only version to directly quantify the underestimation effect
  3. **Constraint Analysis:** Break down results by constraint type (e.g., `change_case`, `combination`, `detectable_content`) for two typologically different languages (e.g., Chinese and Arabic) to identify specific failure modes

## Open Questions the Paper Calls Out
None

## Limitations
- The 25-35% accuracy gap between proprietary and open models on low-resource languages may partly reflect differences in training data rather than inherent model capability
- The 7-22% underestimation by machine-translated data is based on the authors' localization pipeline and may not generalize to all MT approaches
- The evaluation uses strict rule-based metrics that may not capture nuanced instruction adherence, particularly for open-ended or creative tasks

## Confidence

- **High Confidence:** Model scale correlates with multilingual instruction-following accuracy (45-60% gap between 70B+ and 8B models)
- **Medium Confidence:** Proprietary models outperform open LLMs by 25-35% on low-resource languages
- **Medium Confidence:** Machine-translated data underestimates accuracy by 7-22% versus localized data

## Next Checks

1. **Localization Pipeline Generalization:** Replicate the evaluation using alternative machine translation systems (e.g., DeepL, GPT-4o translation API) to verify whether the 7-22% underestimation range holds across different MT approaches

2. **Open Model Capability Ceiling:** Evaluate recently released open models (e.g., Qwen2.5-72B, Llama3.1-70B) on Marco-Bench-MIF to determine if the 25-35% gap between proprietary and open models persists as open models scale

3. **Cross-Benchmark Consistency:** Test the same models on both Marco-Bench-MIF and M-IFEval to validate whether the observed scale and proprietary model effects are consistent across different multilingual instruction-following benchmarks