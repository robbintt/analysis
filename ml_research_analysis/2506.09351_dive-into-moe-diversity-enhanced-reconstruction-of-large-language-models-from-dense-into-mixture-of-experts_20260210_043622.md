---
ver: rpa2
title: 'DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models
  from Dense into Mixture-of-Experts'
arxiv_id: '2506.09351'
source_url: https://arxiv.org/abs/2506.09351
tags:
- uni00000013
- uni00000011
- uni00000014
- dive
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DIVE, a method for reconstructing dense large
  language models (LLMs) into Mixture-of-Experts (MoE) architectures while enhancing
  expert diversity. The approach addresses the issue of homogeneous experts in existing
  MoE reconstruction methods by leveraging domain affinity mining and pruning-based
  expert reconstruction.
---

# DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts

## Quick Facts
- arXiv ID: 2506.09351
- Source URL: https://arxiv.org/abs/2506.09351
- Reference count: 40
- Primary result: DIVE reconstructs dense LLMs into diverse MoE experts, achieving lower perplexity (13.52 vs 14.51 on LAMBADA) and higher downstream accuracy (83.00 vs 80.50 on SciQ) than existing methods

## Executive Summary
DIVE presents a novel method for reconstructing dense large language models into Mixture-of-Experts (MoE) architectures while enhancing expert diversity. The approach addresses the fundamental limitation of homogeneous experts in existing MoE reconstruction methods by leveraging domain affinity mining through hierarchical clustering of calibration datasets and pruning-based expert reconstruction. By identifying domain-specific experts through this process and applying a two-stage retraining approach, DIVE achieves superior performance on both language modeling and downstream tasks while maintaining the same number of activated parameters as baseline methods.

## Method Summary
DIVE reconstructs dense LLMs into MoE architecture through a three-phase process. First, it performs domain affinity mining by pruning the dense model with 24 different calibration datasets, computing perplexity correlations across tasks, and hierarchically clustering these into 8 groups. Second, it prunes the dense model's FFN layers by 50% on each cluster's mixed calibration data and reassembles 8 distinct pruned FFNs as experts with a random router. Third, it applies two-stage retraining: dense training of routers only on 0.5B tokens (temperature 0.05 for top-1), followed by sparse training of experts and normalization layers with LoRA on 5B tokens. The method uses TinyLlama-1.1B as the base model and evaluates on WikiText2, LAMBADA for language modeling, and 11 downstream tasks including SciQ, PIQA, WinoGrande, and MMLU.

## Key Results
- DIVE achieves lower perplexity than existing methods: 13.52 vs 14.51 on LAMBADA
- DIVE shows higher downstream accuracy: 83.00 vs 80.50 on SciQ
- The method maintains the same number of activated parameters as baseline approaches while improving performance

## Why This Works (Mechanism)

### Mechanism 1: Calibration-Driven Expert Diversification
Pruning a dense LLM using domain-specific calibration data isolates distinct sub-networks, creating specialized and diverse experts from a single monolithic model. Structured pruning methods like FLAP measure channel importance based on activation fluctuations on a calibration set. When different calibration sets (e.g., math vs. web text) are used, the importance scores diverge, resulting in different neurons being retained for each domain. This process exposes and extracts the inherent functional diversity within the dense model's weights. The pre-trained dense LLM contains latent, domain-specific sub-networks that can be selectively preserved or discarded based on the statistical properties of the calibration data.

### Mechanism 2: Correlation-Based Domain Clustering
Clustering calibration datasets using task performance correlations ensures that experts are trained on data that functionally benefits the same model weights. The method computes a correlation matrix of normalized perplexity scores across all evaluation tasks for models pruned with different calibration sets. Hierarchical clustering on this matrix groups tasks that are "solved" by the same pruning mask. This ensures an expert's calibration data is a mixture of tasks that rely on shared model capabilities. Tasks that exhibit high correlation in their response to a specific pruning mask share a fundamental dependence on the same underlying functional pathways in the model.

### Mechanism 3: Decoupled Router-Expert Optimization
Decoupling the training into a dense router warm-up followed by sparse expert fine-tuning resolves the gradient conflict between routing policy and expert specialization. The Top-K operation in routers is non-differentiable and produces sparse gradients. The paper first trains the router using all experts (dense mode) with a temperature coefficient to soften the selection, establishing a stable routing policy. It then freezes the router and backbone, training only the experts (via LoRA) and normalization layers in sparse mode to recover domain-specific capacity efficiently. The initial pruned experts retain sufficient baseline competence that the primary training challenge is learning which expert to use (router) and then refining how they function (LoRA), rather than relearning general language processing.

## Foundational Learning

- **Structured Pruning**: Needed for reducing FFN size to create specialized experts. Quick check: Why is structured pruning preferred over unstructured pruning for hardware efficiency?

- **Mixture-of-Experts Routing**: Needed to understand the Router's role in selecting top-k experts for efficiency. Quick check: In a Top-2 MoE layer, how is the final output computed from the selected experts?

- **Low-Rank Adaptation (LoRA)**: Needed for efficient expert fine-tuning in the sparse training phase. Quick check: How does LoRA modify a pre-trained weight matrix W during a forward pass, and what are its trainable parameters?

## Architecture Onboarding

- **Component map**: Domain Affinity Analyzer -> Cluster & Prune Module -> Reconstructor -> Two-Stage Trainer (Router Stage -> Expert Stage)

- **Critical path**: Data Grouping is critical - incorrectly clustering datasets will produce generalist experts instead of specialists. Router Temperature Tuning is critical - setting temperature coefficient t in Stage 1 affects stability and specialization.

- **Design tradeoffs**: Expert Count (1/8 vs 2/8) - DIVE 1/8 showed lower perplexity than DIVE 2/8, suggesting higher specialization (one expert) can outperform averaging two weaker experts for fixed compute. Retraining Scope - freezing MHA layers saves immense compute but assumes original attention patterns are domain-agnostic.

- **Failure signatures**: Router Collapse - router logits converge to uniform or prefer a single expert regardless of input (check routing entropy). Expert Uniformity - low routing variance across domains, indicating pruning failed to create diverse experts. Capacity Bottleneck - performance on specialized tasks improves but general tasks collapse, suggesting over-specialization.

- **First 3 experiments**: 1) Correlation Validation - reproduce Figure 2's heatmap to verify pruning on MathQA yields lower perplexity on GSM8K than pruning on C4. 2) Ablate Router Training - train DIVE with and without dense router stage to quantify router warm-up impact. 3) Routing Visualization - pass mixed-domain data through trained model and visualize expert activation frequency per domain to confirm specialization.

## Open Questions the Paper Calls Out
- The paper notes that DIVE demonstrates generalization across different models and sizes, but experiments were limited to models no larger than 7B due to resource constraints. Future efforts could focus on scaling up model sizes.

## Limitations
- Calibration clustering generalizability - the method assumes hierarchical clustering of 24 tasks into 8 groups is optimal, but this mapping may not transfer to larger models or different domain mixes.
- Sparse activation efficiency claims - while DIVE activates fewer parameters than dense models, the paper doesn't benchmark actual memory/compute savings during inference, only counting parameters.
- Router initialization impact - the method uses random router initialization without analyzing how different initializations affect final routing quality.

## Confidence
- High confidence: The core mechanism of using domain-specific pruning to create diverse experts is well-supported by empirical evidence.
- Medium confidence: Claims about efficiency gains are reasonable but not fully quantified.
- Low confidence: The optimality of the clustering approach and generalizability to much larger models (>1B parameters) remain uncertain without additional validation.

## Next Checks
1. Router initialization sensitivity - train DIVE models with different router initializations and compare final routing entropy and task performance.
2. Cluster count ablation - vary the number of clusters from 4 to 16 while keeping total parameters constant, and measure perplexity on LAMBADA and downstream accuracy.
3. Open-ended generation evaluation - evaluate DIVE on human-evaluated generation quality metrics to assess real-world applicability beyond classification tasks.