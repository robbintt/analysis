---
ver: rpa2
title: 'OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding'
arxiv_id: '2504.14692'
source_url: https://arxiv.org/abs/2504.14692
tags:
- medical
- arxiv
- images
- video
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of developing a unified medical
  vision-language model capable of processing diverse multimodal inputs including
  2D/3D images and videos, which existing models typically handle with separate encoders.
  The authors propose OmniV-Med, a unified framework featuring three key technical
  contributions: construction of a comprehensive 252K-sample instruction-following
  dataset (OmniV-Med-Instruct) spanning 14 medical image modalities and 11 clinical
  tasks, a rotary position-adaptive encoder that processes multi-resolution 2D/3D
  images and videos within a single architecture, and a medical-aware token pruning
  mechanism that exploits spatial-temporal redundancy to reduce visual tokens by 60%
  without performance degradation.'
---

# OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding

## Quick Facts
- **arXiv ID:** 2504.14692
- **Source URL:** https://arxiv.org/abs/2504.14692
- **Reference count:** 20
- **Primary result:** Unified framework for 2D/3D medical images and videos achieving SOTA on seven benchmarks

## Executive Summary
OmniV-Med addresses the fragmentation in medical vision-language models by introducing a unified framework capable of processing diverse multimodal inputs including 2D/3D images and videos within a single architecture. The framework introduces three key innovations: a comprehensive 252K-sample instruction-following dataset spanning 14 medical image modalities and 11 clinical tasks, a rotary position-adaptive encoder for multi-resolution image processing, and a medical-aware token pruning mechanism that reduces visual tokens by 60% without performance degradation. The model achieves state-of-the-art performance on seven benchmarks across medical imaging and video understanding tasks, with a lightweight variant offering comparable performance using only 8 RTX3090 GPUs.

## Method Summary
The OmniV-Med framework unifies the processing of diverse medical multimodal data through a single architecture rather than separate encoders for different modalities. The core innovation is a rotary position-adaptive encoder that can handle multi-resolution 2D/3D images and videos within the same framework. This is complemented by a medical-aware token pruning mechanism that exploits spatial-temporal redundancy in medical data to significantly reduce computational load. The framework is trained on OmniV-Med-Instruct, a large-scale instruction-following dataset containing 252K samples across 14 medical image modalities and 11 clinical tasks. The unified approach enables the model to process various medical imaging formats while maintaining performance and efficiency.

## Key Results
- OmniV-Med-7B achieves state-of-the-art performance on seven medical imaging and video understanding benchmarks
- Medical-aware token pruning reduces visual tokens by 60% without performance degradation
- OmniV-Med-1.5B variant achieves comparable performance to larger models while requiring only 8 RTX3090 GPUs for training
- Unified framework successfully processes 14 medical image modalities across 11 clinical tasks

## Why This Works (Mechanism)
The unified architecture enables cross-modal learning where knowledge transfer occurs between different medical imaging modalities, improving generalization. The rotary position-adaptive encoder handles varying resolutions and dimensions (2D/3D) by adapting positional encoding to the specific input characteristics. Medical-aware token pruning exploits the inherent spatial-temporal redundancy in medical imaging data, where certain regions and time points contain less diagnostically relevant information. The large-scale instruction-following dataset provides diverse training examples that enable the model to understand and respond to various clinical tasks across different modalities.

## Foundational Learning
- **Rotary Position Encoding**: Needed to handle variable input resolutions and dimensions; quick check: verify positional information is preserved across modality transformations
- **Token Pruning Optimization**: Required for computational efficiency in processing high-resolution medical data; quick check: measure inference speed vs accuracy trade-off across modalities
- **Multi-Modal Instruction Following**: Essential for unified understanding of diverse clinical tasks; quick check: test model performance on unseen task combinations
- **Medical Image Redundancy Exploitation**: Leverages characteristic patterns in medical imaging; quick check: analyze which regions/timepoints are typically pruned
- **Unified Encoder Architecture**: Enables cross-modal knowledge transfer; quick check: measure performance improvement when training with multiple modalities
- **Large-Scale Dataset Curation**: Provides comprehensive coverage of medical imaging tasks; quick check: evaluate performance with varying dataset sizes

## Architecture Onboarding
**Component Map:** Data Collection -> Dataset Construction -> Unified Encoder Training -> Token Pruning Optimization -> Benchmark Evaluation

**Critical Path:** The unified encoder with rotary position-adaptive encoding forms the core processing pipeline, with token pruning applied post-encoding for efficiency. Dataset construction and instruction-following formulation enable effective training across diverse modalities.

**Design Tradeoffs:** Unified architecture sacrifices some modality-specific optimization for cross-modal generalization and reduced complexity. Token pruning introduces potential information loss but achieves significant computational savings. Large model size (7B parameters) enables better performance but increases computational requirements.

**Failure Signatures:** Performance degradation may occur with extremely long videos beyond 1024 frames, very high-resolution 3D volumes, or rare medical conditions not well-represented in training data. Token pruning may miss diagnostically relevant information in edge cases.

**First Experiments:** 1) Ablation study removing unified encoder to measure cross-modal benefit, 2) Performance comparison with and without token pruning across all 14 modalities, 3) Training with reduced dataset size to quantify scaling benefits

## Open Questions the Paper Calls Out
None

## Limitations
- Claims of state-of-the-art performance require clarification on comparison baseline specifications and exact training conditions
- Medical-aware token pruning effectiveness may have modality-specific limitations not fully explored
- Framework performance on extremely long videos and very high-resolution 3D volumes remains untested
- Clinical applicability and safety for real-world diagnostic scenarios not validated through prospective studies

## Confidence
- **High confidence**: Technical implementation of unified architecture and rotary position-adaptive encoder are well-documented with clear ablation studies
- **Medium confidence**: Performance improvement claims are supported but exact comparison conditions need clarification
- **Medium confidence**: Token pruning effectiveness demonstrated but generalizability across all modalities needs further investigation

## Next Checks
1. Conduct systematic ablation studies isolating contribution of each component (unified encoder, token pruning, dataset scale) across different medical modalities
2. Perform cross-institutional validation using independent medical imaging datasets not included in training set
3. Evaluate model performance and safety through radiologist validation studies including failure mode analysis and confidence calibration