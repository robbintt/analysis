---
ver: rpa2
title: 'Utilizing Strategic Pre-training to Reduce Overfitting: Baguan -- A Pre-trained
  Weather Forecasting Model'
arxiv_id: '2505.13873'
source_url: https://arxiv.org/abs/2505.13873
tags:
- baguan
- pre-training
- forecasting
- weather
- lead
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Baguan is a pre-trained weather forecasting model that addresses
  overfitting in data-driven global weather prediction by using a Siamese Masked Autoencoder
  (Siamese MAE) for strategic pre-training. The model introduces locality bias through
  a challenging pre-training task, effectively mitigating overfitting while improving
  forecast accuracy.
---

# Utilizing Strategic Pre-training to Reduce Overfitting: Baguan -- A Pre-trained Weather Forecasting Model

## Quick Facts
- **arXiv ID:** 2505.13873
- **Source URL:** https://arxiv.org/abs/2505.13873
- **Reference count:** 40
- **One-line primary result:** Baguan, a pre-trained Transformer model, achieves state-of-the-art performance on medium-range global weather forecasting, outperforming Pangu-Weather by 4.7% and IFS by 23.2% in RMSE, while demonstrating strong generalization to downstream tasks.

## Executive Summary
Baguan is a pre-trained weather forecasting model that addresses overfitting in data-driven global weather prediction. The model uses a three-stage training paradigm, starting with a strategic pre-training phase using Siamese Masked Autoencoders (Siamese MAE) to introduce locality bias and regularize the model. This pre-training, combined with specialized fine-tuning stages, enables Baguan to outperform existing models like Pangu-Weather and IFS on medium-range global weather forecasting. The paper also provides theoretical analysis showing that pre-training acts as regularization by pruning the subspace of low-variance components, enhancing generalization.

## Method Summary
Baguan employs a three-stage training paradigm to address overfitting in global weather prediction. Stage 1 uses Siamese MAE pre-training with an asymmetric masking strategy (0% on the first frame, 75% on the second) to introduce locality bias and regularize the model. Stage 2 specializes the model for specific lead times by masking the second frame entirely. Stage 3 refines the model using an autoregressive scheme to minimize error accumulation over multiple steps. The architecture is based on a ViT-like encoder with a Cross-Self Decoder, processing global atmospheric states from the ERA5 dataset.

## Key Results
- Baguan achieves state-of-the-art performance on medium-range global weather forecasting, outperforming Pangu-Weather by 4.7% and IFS by 23.2% in RMSE.
- The model demonstrates strong generalization to downstream tasks such as subseasonal-to-seasonal (S2S) forecasting and high-resolution regional forecasting.
- Theoretical analysis shows that pre-training acts as regularization by pruning the subspace of low-variance components, improving generalization.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A properly tuned Siamese MAE pre-training task introduces a locality bias that mitigates overfitting in data-scarce weather forecasting.
- **Mechanism:** By presenting two consecutive frames and masking a high ratio (75%) of the second frame while leaving the first unmasked (0%), the model receives a strong cue to focus on local, short-term evolution rather than learning spurious global correlations from limited decades of data.
- **Core assumption:** Overfitting in global weather models stems from learning arbitrary, non-physical global interactions, and constraining the model to focus on local changes improves generalization.
- **Evidence anchors:**
  - [abstract] "selecting an appropriately challenging pre-training task introduces locality bias, effectively mitigating overfitting"
  - [Section 3.3.1] "This task simplifies the forecasting process and encourages the model to concentrate on the changes occurring between two input frames."
  - [Section 4.2.1] "pre-trained model exhibits a train-validation loss gap of 3.6e−4, compared to 4.0e−4 for the non-pre-trained model"
  - [corpus] Corpus signals are weak or missing for this specific causal mechanism.
- **Break condition:** The effect is highly dependent on the masking ratio; too low provides no challenge, while too high (e.g., 0.99) causes training instability (Section 4.2.4).

### Mechanism 2
- **Claim:** Pre-training acts as a regularizer by forcing the learned solution to reside within the subspace of high-variance components, improving generalization.
- **Mechanism:** Theoretical analysis shows the pre-training matrix M* functions as a high-bandwidth pass filter, pruning the subspace associated with small eigenvalues of the covariance matrix. This constrains the downstream regression to the leading eigenvectors, significantly improving the error bound from O(d^{1/4}/n^{1/2}) to O(1/n^{1/2}).
- **Core assumption:** The optimal solution for weather forecasting lies predominantly within the subspace spanned by the top eigenvectors of the data's covariance matrix.
- **Evidence anchors:**
  - [abstract] "Theoretical analysis shows that pre-training acts as regularization by pruning the subspace of low-variance components"
  - [Section 3.5] "pre-training effects analogous to pruning the subspace spanned by eigenvectors with small eigenvalues"
  - [Section 4.2.2] "energy of the leading eigenvectors is higher in the pre-trained model compared to the non-pre-trained model"
  - [corpus] Corpus signals are weak or missing for this specific causal mechanism.
- **Break condition:** This mechanism is formally proven for linear regression; its application to deep Transformers is a key assumption, though empirically supported by spectral analysis.

### Mechanism 3
- **Claim:** A three-stage training paradigm progressively specializes the model from a general foundation to a precise, stable forecaster.
- **Mechanism:** Stage 1 learns general weather dynamics via Siamese MAE. Stage 2 specializes the model for specific lead times (1h, 6h, 24h) by masking the second frame 100%, forcing pure prediction. Stage 3 further refines it using an autoregressive scheme to minimize error accumulation over multiple steps.
- **Core assumption:** A foundation model can be efficiently specialized for a harder downstream task (forecasting) through successive, targeted fine-tuning stages.
- **Evidence anchors:**
  - [Section 3.3] Describes the three distinct stages and their objectives.
  - [Section 4.3] Baguan's superior performance against baselines (IFS, Pangu-Weather) validates the effectiveness of this staged approach.
  - [Figure 2] Visually details the three-stage architecture.
  - [corpus] Corpus signals are weak or missing for this specific causal mechanism.
- **Break condition:** The success of later stages is critically dependent on the quality of representations learned in Stage 1.

## Foundational Learning

- **Concept: Siamese Masked Autoencoders (Siamese MAE)**
  - **Why needed here:** This is the core strategic pre-training technique that enables Baguan to learn from temporal continuity.
  - **Quick check question:** How does the asymmetric masking strategy (0% on frame 1, 75% on frame 2) differ from standard MAE, and how does it introduce locality bias?

- **Concept: Vision Transformers (ViT) for Spatiotemporal Data**
  - **Why needed here:** Baguan uses a ViT-like architecture to process global atmospheric states.
  - **Quick check question:** How are atmospheric variables aggregated and tokenized into a sequence for the Transformer encoder?

- **Concept: Autoregressive Error Accumulation**
  - **Why needed here:** The model forecasts iteratively, using its own output as the next input, which is prone to error growth.
  - **Quick check question:** How does the iterative fine-tuning in Stage 3 differ from simply rolling a model trained on single-step forecasts?

## Architecture Onboarding

- **Component map:** Variable Aggregation & Tokenization -> Siamese Encoder (ViT-based) -> Cross-Self Decoder
- **Critical path:**
  1.  **Input:** Two atmospheric states ($X_{t_0}$ and $X_{t_0+\Delta t}$) are tokenized.
  2.  **Pre-training:** $X_{t_0}$ is unmasked, $X_{t_0+\Delta t}$ is 75% masked noise. Both pass through the shared encoder.
  3.  **Decoder:** A cross-attention layer attends from the masked tokens to the unmasked tokens, then a self-attention layer refines the representation to reconstruct $X_{t_0+\Delta t}$.
  4.  **Fine-tuning (Stages 2 & 3):** The decoder is replaced. The second input is pure noise (100% masked), forcing the model to predict $X_{t_0+\Delta t}$ based only on $X_{t_0}$.
- **Design tradeoffs:** A higher masking ratio in pre-training increases task difficulty and potential regularization but risks training instability (Section 4.2.4). The paper chooses 0.75 for stability, despite 0.95 showing slightly better potential performance.
- **Failure signatures:** Without pre-training, the model exhibits a large train-validation loss gap (overfitting) and generates attention maps that are diffuse and non-local. With overly aggressive pre-training (mask ratio ~0.99), training loss shows significant spikes and fails to converge.
- **First 3 experiments:**
  1.  **Ablation on Masking Ratio:** Train Siamese MAE models with varying masking ratios (e.g., 0.5, 0.75, 0.95, 0.99) and evaluate the downstream 6-hour forecast RMSE. This validates the relationship between task difficulty and performance.
  2.  **Spectral Analysis of Attention:** Compare the eigenvalue distribution of the attention matrices from a pre-trained model vs. a model trained from scratch. This provides empirical evidence for the regularization mechanism.
  3.  **Baseline Comparison:** Fine-tune Baguan on the ERA5 test set (2018) and compare RMSE and ACC against Pangu-Weather and IFS for key variables (T2M, U10, Z500) across different lead times.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can model-specific locality designs (e.g., GNNs, Swin Transformers) be effectively combined with Siamese MAE pre-training to further improve forecast accuracy?
- Basis: [explicit] The paper notes it did not compare against models like GraphCast or FuXi due to their built-in locality mechanisms, but states, "Future work will explore combining model-specific locality with pre-training-induced locality to collaboratively address the overfitting issue."
- Why unresolved: It is undetermined whether the locality bias introduced by the pre-training task (Siamese MAE) is redundant, complementary, or conflicting when applied to architectures that already enforce locality structurally.
- What evidence would resolve it: A comparative study where models like GraphCast or Swin Transformers are trained using the Baguan Siamese MAE initialization versus training from scratch.

### Open Question 2
- Question: How effectively does the Baguan foundation model generalize to extreme weather events and long-term climate prediction tasks beyond standard medium-range forecasting?
- Basis: [explicit] The Future Work section states, "we aim to apply Baguan to a broader range of meteorological applications, including extreme weather forecasting and climate prediction."
- Why unresolved: While Appendix A.11 provides an initial attempt at tracking tropical cyclones, a comprehensive evaluation on diverse extreme events (e.g., heatwaves, flash floods) and climate timescales is missing.
- What evidence would resolve it: Benchmark results on extreme event datasets and climate projection tasks, demonstrating transfer learning efficiency compared to training from scratch.

### Open Question 3
- Question: Is the observed performance saturation in larger models caused by the limited temporal span (40 years) of the ERA5 dataset?
- Basis: [inferred] The scaling analysis (Fig. 7) indicates that model performance increases with size but eventually saturates; the authors explicitly link overfitting to the limited availability of real-world data.
- Why unresolved: It is unclear if the "saturation" is an architectural ceiling of the Transformer or if the model has simply exhausted the spatiotemporal diversity available in the 40-year dataset.
- What evidence would resolve it: Training scaling laws using extended datasets (e.g., synthetic data or CMIP climate simulations) to see if the saturation point shifts to larger parameter counts.

## Limitations
- The theoretical regularization mechanism relies on assumptions about the data's covariance structure and the applicability of linear regression theory to deep Transformers.
- The specific variable weighting scheme in the loss function is not provided, which could affect reproducibility.
- The model's generalization to other datasets or extreme weather events is not explicitly tested.

## Confidence
- **High Confidence:** The experimental results showing Baguan's superior performance over Pangu-Weather and IFS on medium-range forecasting are well-supported with quantitative metrics (RMSE, ACC) and ablation studies.
- **Medium Confidence:** The claim that the asymmetric Siamese MAE pre-training introduces locality bias is supported by loss gap analysis and attention map visualization, but the exact causal link between masking ratio and locality is inferred.
- **Medium Confidence:** The theoretical analysis of pre-training as regularization is formally proven for linear regression, and spectral analysis provides supporting evidence, but its direct application to the non-linear Transformer case is an assumption.
- **Medium Confidence:** The staged training paradigm is clearly described and its effectiveness is validated, but the claim that this is the optimal specialization strategy is not directly compared against alternatives.

## Next Checks
1. **Ablation on Masking Ratio:** Systematically train Siamese MAE models with varying masking ratios (e.g., 0.5, 0.75, 0.95, 0.99) and evaluate the downstream 6-hour forecast RMSE to quantify the relationship between pre-training task difficulty and final performance.
2. **Spectral Analysis of Attention:** Compute and compare the eigenvalue distribution of the attention matrices from a pre-trained Baguan model versus a model trained from scratch. This will provide empirical evidence for the claimed regularization effect of pre-training.
3. **Variable Weighting Sensitivity:** Train multiple Baguan models with different, plausible weightings for the atmospheric variables in the loss function (Eq. 13/14) to determine how sensitive the final forecast accuracy is to this unspecified parameter.