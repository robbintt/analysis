---
ver: rpa2
title: Missing Data Multiple Imputation for Tabular Q-Learning in Online RL
arxiv_id: '2510.10709'
source_url: https://arxiv.org/abs/2510.10709
tags:
- missingness
- imputation
- state
- missing
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores multiple imputation ensembles for online reinforcement
  learning with missing state space data. The key insight is that maintaining multiple
  imputation pathways can better capture uncertainty under missingness while remaining
  computationally efficient.
---

# Missing Data Multiple Imputation for Tabular Q-Learning in Online RL

## Quick Facts
- arXiv ID: 2510.10709
- Source URL: https://arxiv.org/abs/2510.10709
- Reference count: 40
- Primary result: Multiple imputation ensembles outperform single imputation and simple baselines in online RL with missing state data across various missingness mechanisms

## Executive Summary
This paper addresses the challenge of missing state data in online reinforcement learning by adapting multiple imputation ensembles to the RL setting. The authors propose maintaining multiple imputation pathways that independently draw state completions from a learned transition model, with fractional Q-updates to distribute learning across pathways. Experiments on a custom 8×8 grid world with various missingness mechanisms (MCAR, MCOLOR, MFOG) demonstrate that multiple imputation ensembles generally outperform single imputation and simple baselines across multiple metrics including total reward, steps in the river, and path length.

## Method Summary
The approach maintains K imputation pathways, each maintaining a state history. When encountering partially observed states, each pathway independently draws imputations from a learned transition model. Q-values are updated fractionally across all pathways using α/K learning rates, and action selection uses a voting ensemble over the K imputed states. The transition model can be updated conservatively (using only observed tuples) or synthetically (including imputed data with fractional counts). The fractional updates ensure proper normalization and prevent synthetic imputed data from overwhelming observed data.

## Key Results
- Multiple imputation ensembles consistently outperform single imputation (K=1) as missingness increases
- Conservative T-updates generally perform as well as or better than synthetic updates, especially in NMAR scenarios
- The approach is particularly robust as missingness increases, maintaining performance where baselines fail
- Treating missingness as a state may be beneficial under extreme missingness rates but otherwise incurs additional state space complexity

## Why This Works (Mechanism)

### Mechanism 1
Multiple imputation pathways can better represent uncertainty under missingness while mitigating path dependency errors that accumulate from single imputation chains. Each of K pathways independently draws imputations from a learned transition model, creating divergent state trajectories. Fractional Q-updates (α/K) distribute learning across pathways, diminishing the impact of any single incorrect imputation. When imputations disagree, their collective influence is proportionally reduced. Core assumption: Imputation errors across pathways are not perfectly correlated; the underlying transition model provides meaningful probability mass over plausible states.

### Mechanism 2
Fractional count updates (adding 1/K per imputation rather than 1) ensure proper normalization and prevent synthetic imputed data from overwhelming observed data in transition model learning. When updating transition counts, each imputed tuple contributes 1/K weight. Theorem A.1 shows that without this fractional weighting, imputation-based estimators would converge to incorrect values (K·p_c|s,a rather than p_c|s,a). The fraction moderates influence to match information content. Core assumption: Imputations carry no independent auxiliary information beyond what observed data provides (congeniality assumption from Meng 1994).

### Mechanism 3
Voting-based action selection over imputed states promotes exploration and reduces commitment to any single imputation's greedy action. For each pathway k, compute A_k = argmax_a Q(s_k, a), then randomly select among the K candidate actions. This averages over imputation uncertainty rather than committing to one deterministic choice. Core assumption: No single imputation pathway consistently identifies the optimal action; exploration via voting is beneficial relative to computation cost.

## Foundational Learning

- **Multiple Imputation (MI)**: The statistical framework being adapted. MI traditionally creates multiple completed datasets offline, combining estimates to quantify uncertainty. Transferring this to online RL requires understanding why the standard recipe doesn't apply directly (dependencies across time, no human review, computational constraints). Quick check: Can you explain why the standard MI approach of fitting a Bayesian model and drawing imputations offline is problematic when decisions must be made in real-time?

- **Temporal Difference (TD) Learning and Q-Learning**: The paper modifies standard Q-learning updates to handle fractional contributions from imputed states. Understanding the standard Bellman update (Q(s,a) ← Q(s,a) + α[R + γ·max_a'Q(s',a') - Q(s,a)]) is prerequisite to understanding why α/K fractional updates are used. Quick check: What happens to Q-learning convergence if the learning rate α is too large relative to the visitation frequency of a state-action pair?

- **Missingness Mechanisms (MCAR, MAR, NMAR)**: The experiments show that method performance depends on why data is missing. MCAR means missingness is random; MAR means it depends only on observed values; NMAR means it depends on unobserved values. The "missing-as-state" baseline performs differently under each mechanism. Quick check: In an NMAR scenario where high-risk states are more likely to be missing, why might treating missingness as a state value outperform imputation?

## Architecture Onboarding

- **Component map**:
  - **Imputation Model (Transition Estimator)**: Maintains counts n(s,a,s') and computes ˆT(s'|s,a). Two variants: Conservative (observed tuples only) or Synthetic (fractional counts from imputed tuples)
  - **K Imputation Pathways**: Each pathway maintains a state history (S_k^t). When state is partially observed, draw S_mis,k^t from ˆT conditioned on observed components
  - **Shared Q-Table**: Single Q-function updated fractionally across all pathways (not K separate Q-functions)
  - **Action Selector**: Voting ensemble that collects argmax actions from each pathway and randomly selects among them

- **Critical path**:
  1. Observe S_obs^{t+1}
  2. For k=1 to K: draw S_mis,k^{t+1} from ˆT(·|S_obs^{t+1}, S_k^t, A^t), set S_k^{t+1} = (S_mis,k^{t+1}, S_obs^{t+1})
  3. Compute A_k = argmax_a Q(S_k^{t+1}, a) for each k
  4. Select action uniformly from {A_1, ..., A_K} (with ε-greedy override)
  5. Observe R^{t+1}
  6. For k=1 to K: Q(s_k^t, A^t) ← Q(s_k^t, A^t) + (α/K)·TD(s_k^t, A^t, R^{t+1}, s_k^{t+1})
  7. For k=1 to K: update transition counts with +1/K for (s_k^t, A^t, s_k^{t+1}) [Synthetic only]

- **Design tradeoffs**:
  - **K selection**: Diminishing returns after K=5-10; higher K increases computation linearly but provides marginal uncertainty representation gains
  - **Conservative vs Synthetic T-updates**: Conservative is safer (no self-reinforcement) but slower learning; Synthetic leverages partial observations but risks path dependency. Paper recommends Conservative as default given similar performance and lower risk
  - **Missing-as-state baseline**: Consider as alternative when (a) missingness rate is extreme (>80%), (b) missingness is itself informative (NMAR), or (c) computational simplicity is paramount

- **Failure signatures**:
  - **Single imputation (K=1) collapse**: Under high missingness, performance degrades catastrophically due to path dependency—incorrect imputation compounds over time
  - **Synthetic update self-reinforcement in NMAR**: When missingness correlates with specific state regions (e.g., fog), synthetic updates reinforce incorrect transitions because observed visits to that region are rare
  - **Missing-as-state at moderate missingness**: U-shaped performance curve—performs well at very low and very high missingness rates, but poorly in between due to expanded state space without compensating information

- **First 3 experiments**:
  1. Replicate MCAR grid world with θ=0.4, K∈{1,5,10}, comparing MI (Conservative) vs MI (Synthetic) vs Last-Observed-State baseline. Verify that K=10 outperforms K=1 on path length metric
  2. Test NMAR scenario (MFOG with θ_in=0.5, θ_out=0) to confirm Conservative T-updates outperform Synthetic when missingness is location-informative
  3. Ablate action selection: compare voting ensemble vs argmax across K imputed Q-values on a task where exploration matters (e.g., sparse reward with multiple viable paths)

## Open Questions the Paper Calls Out

### Open Question 1
How does the relative performance of multiple imputation (MI) ensembles change as environment stochasticity increases? The authors state, "Of particular interest for future work is how performance varies with environment stochasticity." The current experiments use fixed stochasticity (wind/flood probability of 0.1). Higher stochasticity makes learning transition models for imputation harder, but it also lowers the cost of taking a sub-optimal action due to a bad imputation. Experiments varying the wind and flood transition probabilities across a wide range (e.g., 0.1 to 0.9) to observe if MI retains its advantage over baselines like "missing-as-state" or if performance degrades due to poor transition model estimation would resolve this.

### Open Question 2
Can a hybrid approach, augmenting the state with both imputed values and missingness indicators, combine the benefits of the "imputation" and "missing-as-state" methods? The authors note, "Future work could explore combining the two approaches, for example by augmenting the state to include both imputations and information about the missingness structure." The experiments show a U-shaped performance curve for the "missing-as-state" method (performing well only at very high or low missingness), whereas MI performs well generally but struggles in specific NMAR scenarios where missingness is itself a signal. A hybrid method might leverage the predictive power of imputation while retaining the signal of missingness. Implementation of a Q-learning agent that maintains state representations of the form $(S_{imputed}, M)$, tested across the MCAR, MCOLOR, and MFOG environments to see if it dominates the individual methods would resolve this.

### Open Question 3
Can information-theoretic action selection strategies be effectively integrated with multiple imputation to optimize information gathering? The authors suggest, "It would be interesting to consider how to combine information-theoretically-motivated algorithms... with the MI ideas described here to deal with missingness." The current work relies on $\epsilon$-greedy exploration and simple voting ensembles. The "stay in place" action, which could serve as an information-gathering step, was found to be underutilized or ignored by the greedy algorithm. Standard Q-learning is not designed to value information gathering for future imputation accuracy. A modified algorithm that uses the variance across the $K$ imputation pathways to calculate an information gain bonus for actions (e.g., moving to a state that reduces imputation uncertainty), showing improved sample efficiency compared to standard $\epsilon$-greedy methods would resolve this.

## Limitations

- Limited empirical validation beyond custom grid worlds; no real-world missing data benchmarks
- Unclear computational overhead of maintaining K pathways (not reported in ablation)
- Path dependency concerns under extreme missingness not quantitatively characterized

## Confidence

- **High**: Fractional count updates (Theorem A.1) prevent synthetic data from overwhelming observed data
- **Medium**: Multiple imputation pathways outperform single imputation in online RL under moderate missingness
- **Low**: Voting ensemble provides exploration benefits over argmax; Conservative T-updates are universally preferable to Synthetic

## Next Checks

1. Test Conservative vs Synthetic T-updates on NMAR benchmark where missingness is informative (e.g., medical dataset with sicker patients missing)
2. Measure computation time overhead of K pathways vs single imputation across different K values
3. Validate on a non-tabular RL environment (e.g., Atari with missing frames) to test domain generality