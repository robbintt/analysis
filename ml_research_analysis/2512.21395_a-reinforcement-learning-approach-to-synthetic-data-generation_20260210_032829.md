---
ver: rpa2
title: A Reinforcement Learning Approach to Synthetic Data Generation
arxiv_id: '2512.21395'
source_url: https://arxiv.org/abs/2512.21395
tags:
- data
- synthetic
- real
- rlsyn
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RLSyn, a reinforcement learning approach
  to synthetic data generation for biomedical datasets. Unlike existing methods like
  GANs and diffusion models, RLSyn models the generator as a stochastic policy and
  uses Proximal Policy Optimization (PPO) with discriminator-derived rewards, making
  it more stable and data-efficient.
---

# A Reinforcement Learning Approach to Synthetic Data Generation

## Quick Facts
- **arXiv ID:** 2512.21395
- **Source URL:** https://arxiv.org/abs/2512.21395
- **Reference count:** 40
- **Primary result:** RLSyn achieves state-of-the-art synthetic biomedical data generation with better stability and data efficiency than GANs and diffusion models

## Executive Summary
This paper introduces RLSyn, a reinforcement learning approach to synthetic data generation that reframes the problem as learning a stochastic policy using Proximal Policy Optimization. Unlike traditional adversarial methods, RLSyn decouples generator and discriminator training while using discriminator-derived rewards to guide policy updates. Experiments on two biomedical datasets show RLSyn achieves comparable utility to diffusion models on larger datasets and significantly outperforms them on smaller datasets, while providing better privacy guarantees through reduced memorization risk.

## Method Summary
RLSyn models synthetic data generation as a reinforcement learning problem where the generator is a stochastic policy outputting distributional parameters (Gaussian means/variances for continuous features, Bernoulli logits for categorical features) rather than point estimates. The generator samples from these distributions and receives rewards from a discriminator trained separately via binary classification. Proximal Policy Optimization updates the generator using clipped probability ratios to ensure stable training, with entropy regularization to encourage diversity and optional mean-matching penalties to maintain fidelity to real data distributions.

## Key Results
- RLSyn achieves S2R AUC of 0.902 vs 0.906 for diffusion models on MIMIC-IV, with higher fidelity (NMI 0.001 vs 0.003) and lower privacy risks (AUC 0.50 membership inference)
- On smaller AI-READI dataset, RLSyn significantly outperforms both diffusion models and GANs in utility, fidelity, and privacy
- RLSyn demonstrates more stable training than GAN-based approaches, particularly beneficial for limited biomedical datasets
- The method shows data-efficient learning, requiring fewer samples to achieve competitive performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RLSyn achieves more stable training than GAN-based approaches through bounded policy updates.
- **Mechanism:** PPO's clipped surrogate objective constrains the probability ratio between consecutive policy iterations, preventing large destabilizing shifts in the generator's distribution while still rewarding higher-quality samples.
- **Core assumption:** Stability from bounded updates translates to better convergence on limited biomedical data.
- **Evidence anchors:** [abstract] "optimizes it using Proximal Policy Optimization with discriminator-derived rewards, yielding more stable and data-efficient training"; [section 5] "RLSyn mitigates these issues via the use of PPO, which constrains generator updates through its clipped objective."

### Mechanism 2
- **Claim:** Explicit stochastic policy parameterization reduces memorization and improves sample diversity.
- **Mechanism:** The generator outputs distributional parameters rather than point estimates. Sampling from these distributions encourages diverse outputs and may act as implicit regularization against replicating training points.
- **Core assumption:** Distributional output prevents collapse onto high-probability training samples that diffusion models may memorize.
- **Evidence anchors:** [section 3.2] "For continuous features, the generator outputs the mean and variance that parameterize a Gaussian distribution"; [section 5] "PPO optimizes an expected reward under stochastic policies, allowing RLSyn to learn a distribution over outputs."

### Mechanism 3
- **Claim:** Decoupled generator-discriminator training avoids adversarial instability while maintaining useful reward signals.
- **Mechanism:** The discriminator provides only reward estimates; no gradients flow back to the generator through the discriminator. Generator updates occur via PPO using these rewards, eliminating the minimax game dynamics that cause GAN training instability.
- **Core assumption:** A discriminator trained to binary classification can provide sufficient reward signal quality without adversarial coupling.
- **Evidence anchors:** [section 3.2] "the discriminator serves only as a reward estimator and the generator is optimized independently through its own RL objective with no discriminator gradients passing through"; [section 5] "RLSyn invokes a more principled objective for generative modeling than adversarial training."

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - **Why needed here:** Core optimization algorithm replacing adversarial training; requires understanding of policy gradients, advantage estimation, and clipping mechanics.
  - **Quick check question:** Can you explain why clipping the probability ratio prevents large policy degradation while still allowing improvement?

- **Concept: Stochastic Policy Parameterization**
  - **Why needed here:** Generator outputs distributional parameters rather than samples directly; reparameterization trick enables gradient flow through sampling.
  - **Quick check question:** How does sampling from a learned Gaussian (with learned variance) differ from outputting a deterministic mean, and what does this imply for diversity?

- **Concept: Membership Inference Attacks**
  - **Why needed here:** Primary privacy evaluation metric; understanding attack methodology is essential for interpreting privacy claims.
  - **Quick check question:** If a synthetic dataset has membership inference AUC of 0.50, what does this imply about the relationship between synthetic and training records?

## Architecture Onboarding

**Component map:**
Latent noise z → Shared MLP backbone → Three output heads: Categorical head (logits → Bernoulli params) → Continuous head (mean, variance → Gaussian params) → Value head (baseline for advantage computation) → Sample x from combined distribution → Discriminator D_φ(x) → reward r = σ(D_φ(x)) → PPO update using advantage (r - V_θ), clipped ratio, entropy bonus

**Critical path:**
1. Generator samples synthetic record x with log-probability log p_θ(x) and value estimate V_θ(z)
2. Discriminator provides scalar reward r
3. Advantage computed as reward minus value baseline, then normalized
4. PPO iterates for specified epochs, recomputing log-probabilities under current policy
5. Discriminator updated separately via BCE with R1 gradient penalty

**Design tradeoffs:**
- **Mean-matching penalty (λ_m):** Light regularization keeps continuous features near real data distribution but may reduce diversity if too strong. Paper uses 0.0 for MIMIC-IV, 0.2 for AI-READI.
- **Discriminator steps per generator update (K):** More steps provide sharper reward signal but risk overfitting. Paper uses 3 for MIMIC-IV, 5 for AI-READI.
- **Entropy coefficient (β):** Higher values encourage exploration/diversity but may reduce fidelity. Paper uses 0.001 for both datasets.

**Failure signatures:**
- **Mode collapse proxy:** NMI significantly above 0.01 indicates synthetic distribution diverges from real; check if categorical feature generation has collapsed to most frequent classes.
- **Memorization signal:** Membership inference AUC above 0.55 on small datasets suggests generator replicating training points; inspect continuous feature variance parameters for near-zero values.
- **Reward hacking:** R2S performance substantially exceeding R2R baseline indicates synthetic data may be simplified copies; discriminator may have failed to provide meaningful reward gradient.

**First 3 experiments:**
1. **Sanity check:** Train on 500-record subset; verify S2R AUC > 0.70 and membership inference AUC < 0.60. If failing, check discriminator training dynamics first.
2. **Ablation (entropy term):** Set β = 0 and compare sample diversity (visualize continuous feature distributions); expect narrower distributions and potentially higher memorization risk.
3. **Scaling test:** Double dataset size from baseline; compare RLSyn vs. EHRDiff performance gap. Assumption: RLSyn advantage should diminish as data increases, per paper's small-sample emphasis.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Which specific components of RLSyn (PPO clipping, stochastic policy sampling, mean-matching penalty, entropy regularization) are most responsible for its performance gains over GANs and diffusion models?
- **Basis in paper:** [explicit] "The mechanisms by which RLSyn outperforms other methods are not yet clear... future work should investigate the theoretical underpinnings of RLSyn to better understand its efficacy and attribute which specific components drive its gains."
- **Why unresolved:** The paper evaluates RLSyn as a complete system but does not conduct ablation studies isolating individual components.
- **What evidence would resolve it:** Systematic ablation experiments removing or replacing each component (e.g., removing PPO clipping, replacing discriminator rewards with distributional metrics) while measuring utility, fidelity, and privacy.

### Open Question 2
- **Question:** Can alternative reward formulations based on statistical or distributional metrics eliminate the need for a discriminator network while maintaining or improving performance?
- **Basis in paper:** [explicit] "Future work could explore alternative reward formulations—such as those based on statistical or distributional metrics—that eliminate the need to train a discriminator altogether or more directly encode objectives such as privacy or utility."
- **Why unresolved:** The current approach still relies on a discriminator for reward signals, inheriting some potential instability issues.
- **What evidence would resolve it:** Experiments using distribution-matching rewards (e.g., maximum mean discrepancy, Wasserstein distance) comparing convergence stability, data efficiency, and privacy-utility tradeoffs against the discriminator-based approach.

### Open Question 3
- **Question:** Does RLSyn's relative advantage over diffusion models and GANs persist for longitudinal tabular data, clinical narrative text, and medical imaging generation?
- **Basis in paper:** [explicit] "Our experiments focused on cross-sectional tabular data generation... future work should explore the efficacy of RLSyn for these higher-dimensional generation problems."
- **Why unresolved:** The paper evaluates only cross-sectional tabular data; the approach's effectiveness for sequential, textual, or image data remains untested.
- **What evidence would resolve it:** Benchmarking RLSyn against domain-specific baselines on longitudinal EHR datasets, clinical note corpora, and medical imaging datasets using modality-appropriate utility, fidelity, and privacy metrics.

### Open Question 4
- **Question:** What are the theoretical connections between RLSyn and related frameworks (GANs, energy-based models, inverse reinforcement learning), and do these connections explain when RLSyn should be preferred?
- **Basis in paper:** [explicit] "There are theoretical connections between GANs, energy-based models, and inverse RL (which is the paradigm most analogous to RLSyn). Consequently, future work should investigate the theoretical underpinnings of RLSyn."
- **Why unresolved:** The empirical success is demonstrated but not rigorously explained through theoretical analysis connecting it to established frameworks.
- **What evidence would resolve it:** Formal analysis establishing equivalence or approximation relationships between RLSyn's objective and those of inverse RL or energy-based models, potentially identifying regimes where each approach is theoretically optimal.

## Limitations

- **Limited ablation studies:** The paper does not isolate which specific RL components (PPO clipping, entropy regularization, decoupled training) are most responsible for performance gains.
- **Narrow domain evaluation:** Results are based only on two biomedical datasets, limiting generalizability to other domains and data modalities.
- **Cross-sectional focus:** The approach is evaluated only on cross-sectional tabular data, with effectiveness for sequential, textual, or image data remaining untested.

## Confidence

- **High confidence:** RLSyn's superior performance on small datasets (AI-READI) and comparable performance on larger datasets (MIMIC-IV) are well-supported by experimental results.
- **Medium confidence:** The claim that decoupled generator-discriminator training provides stability over adversarial methods is supported by theoretical arguments but lacks direct ablation comparisons.
- **Medium confidence:** The privacy benefit (lower membership inference AUC) is demonstrated, though the mechanism (reduced memorization via stochastic policy) would benefit from additional analysis of variance parameter evolution during training.

## Next Checks

1. **Component ablation study:** Train RLSyn variants with (a) entropy coefficient set to 0, (b) deterministic generator (fixed variance), and (c) discriminator gradient flow enabled. Compare performance to isolate which mechanisms contribute most to gains.

2. **Cross-domain validation:** Test RLSyn on non-biomedical tabular datasets (e.g., adult, credit) to assess whether small-sample advantages persist across domains with different feature distributions.

3. **Convergence analysis:** Plot training curves for RLSyn vs. EHRDiff and GAN baselines, tracking discriminator accuracy, generator entropy, and evaluation metrics. Verify that RLSyn's stability claims hold across the full training trajectory, not just final performance.