---
ver: rpa2
title: Deep Jump Gaussian Processes for Surrogate Modeling of High-Dimensional Piecewise
  Continuous Functions
arxiv_id: '2510.21974'
source_url: https://arxiv.org/abs/2510.21974
tags:
- local
- gaussian
- latent
- projection
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Deep Jump Gaussian Processes (DJGP) is introduced to model piecewise
  continuous, high-dimensional system responses. DJGP integrates region-specific locally
  linear projections into Jump Gaussian Processes, allowing adaptation to local subspace
  structures while capturing abrupt changes.
---

# Deep Jump Gaussian Processes for Surrogate Modeling of High-Dimensional Piecewise Continuous Functions

## Quick Facts
- arXiv ID: 2510.21974
- Source URL: https://arxiv.org/abs/2510.21974
- Authors: Yang Xu; Chiwoo Park
- Reference count: 40
- Primary result: DJGP outperforms baselines in RMSE and CRPS while providing reliable uncertainty quantification for piecewise continuous, high-dimensional system responses

## Executive Summary
Deep Jump Gaussian Processes (DJGP) introduces a novel approach for modeling high-dimensional piecewise continuous system responses by integrating region-specific locally linear projections into Jump Gaussian Processes. This architecture enables the model to adapt to local subspace structures while capturing abrupt changes across different regions. The method employs a Gaussian Process prior on projection matrices to ensure smooth evolution across the input space, with scalable variational inference optimizing both projections and hyperparameters jointly.

The theoretical framework provides an oracle error decomposition into four distinct terms: gating error, projection error, linearization error, and GP regression error. Experimental results demonstrate DJGP's superior performance compared to baselines across both synthetic and real datasets, with particular strength in maintaining stable performance as dimensionality increases and providing reliable uncertainty quantification in the presence of discontinuities.

## Method Summary
DJGP extends Jump Gaussian Processes by incorporating locally linear projections that are specific to different regions of the input space. The model uses a Gaussian Process prior over the projection matrices, ensuring smooth transitions between regions while maintaining the ability to capture abrupt changes. The variational inference framework scales efficiently to high-dimensional problems and jointly optimizes the projection matrices along with other hyperparameters. This approach allows DJGP to effectively model piecewise continuous functions by learning appropriate subspaces for each region while maintaining global consistency through the GP prior.

## Key Results
- DJGP outperforms baseline methods in both RMSE and CRPS metrics on synthetic and real datasets
- Model maintains stable performance as dimensionality increases within tested ranges
- Provides reliable uncertainty quantification specifically in the presence of discontinuities
- Theoretical error decomposition identifies four distinct error sources for analysis

## Why This Works (Mechanism)
DJGP works by combining the adaptive capabilities of locally linear projections with the probabilistic modeling strength of Gaussian Processes. The locally linear projections allow the model to learn region-specific subspace structures, effectively reducing the dimensionality of the problem in each region while preserving the important features. The GP prior on projection matrices ensures smooth transitions between these regions, preventing abrupt changes in the projection itself. This combination allows DJGP to capture both the smooth evolution of the underlying function and the abrupt changes that occur at discontinuity boundaries, while the variational inference framework makes the approach computationally tractable for high-dimensional problems.

## Foundational Learning

**Gaussian Processes**
- Why needed: Provides probabilistic framework for regression with uncertainty quantification
- Quick check: Can you derive the GP posterior predictive distribution?

**Variational Inference**
- Why needed: Scalable alternative to exact inference for large datasets
- Quick check: Can you explain the evidence lower bound (ELBO)?

**Locally Linear Embeddings**
- Why needed: Reduces dimensionality while preserving local structure
- Quick check: Can you describe how local neighborhoods are defined?

**Jump Gaussian Processes**
- Why needed: Handles discontinuities in function responses
- Quick check: Can you explain how gating mechanisms work?

## Architecture Onboarding

**Component Map**
Input -> Gating Network -> Region Selection -> Local Projection Matrix (GP-prior) -> Linearized Subspace -> GP Regression -> Output

**Critical Path**
Data flows through gating to select region, applies corresponding projection matrix, then performs GP regression in the projected subspace

**Design Tradeoffs**
- Linear projections vs. nonlinear alternatives (computational efficiency vs. modeling flexibility)
- Smooth GP prior vs. hard region boundaries (continuity vs. expressiveness)
- Joint optimization vs. separate training stages (global coherence vs. local optimization)

**Failure Signatures**
- Poor performance when true discontinuities don't align with learned regions
- Instability when input dimensionality greatly exceeds sample size
- Degradation when piecewise structure is too fine-grained relative to data density

**First Experiments**
1. Test on synthetic piecewise linear function with known discontinuity locations
2. Evaluate sensitivity to projection dimensionality by varying subspace dimensions
3. Compare against standard GP and Jump GP on benchmark datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability to extremely high-dimensional spaces (d > 100) remains uncertain
- Robustness to noisy input features not fully characterized beyond d ≤ 20 experiments
- Assumption of locally linear projections may not capture highly complex piecewise structures
- Scalability for very large datasets (>100K samples) not thoroughly explored

## Confidence

**High**: Claims about improved RMSE and CRPS over baselines in tested synthetic and real datasets.

**Medium**: Assertions about stable performance as dimensionality increases (based on limited d range testing).

**Medium**: Claims of reliable uncertainty quantification in presence of discontinuities (supported by synthetic tests but limited real-world validation).

## Next Checks

1. Evaluate DJGP on datasets with d > 100 and assess performance degradation compared to d ≤ 20 cases.
2. Test DJGP's robustness to input noise levels by adding Gaussian noise to input features and measuring prediction accuracy.
3. Apply DJGP to a real-world engineering problem with known discontinuities (e.g., crash simulations) and compare against physics-informed surrogates.