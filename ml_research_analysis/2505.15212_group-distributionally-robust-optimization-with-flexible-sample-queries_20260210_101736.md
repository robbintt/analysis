---
ver: rpa2
title: Group Distributionally Robust Optimization with Flexible Sample Queries
arxiv_id: '2505.15212'
source_url: https://arxiv.org/abs/2505.15212
tags:
- lemma
- algorithm
- where
- probability
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates group distributionally robust optimization
  (GDRO) in scenarios where the number of samples per iteration can vary arbitrarily
  and dynamically. Existing GDRO algorithms are limited to processing either one or
  m samples per round, which is impractical in real-world settings where resources
  fluctuate or where processing multiple samples simultaneously can accelerate convergence.
---

# Group Distributionally Robust Optimization with Flexible Sample Queries

## Quick Facts
- **arXiv ID**: 2505.15212
- **Source URL**: https://arxiv.org/abs/2505.15212
- **Reference count**: 40
- **Primary result**: First high-probability regret bound for non-oblivious prediction with limited advice in GDRO with flexible sample queries

## Executive Summary
This paper addresses the limitation in Group Distributionally Robust Optimization (GDRO) where existing algorithms can only process either one or m samples per iteration, which is impractical in real-world scenarios with fluctuating resources. The authors formulate GDRO with flexible sample queries as a two-player game and propose a unified algorithm that dynamically switches between implicit exploration for single-sample rounds and prediction with limited advice for multiple-sample rounds. The approach achieves an optimization error bound of O(1/t · sqrt(sum_{j=1}^t m/r_j · log m)) that matches existing bounds for any fixed sample size while offering greater flexibility.

## Method Summary
The method reformulates GDRO as a two-player game where one player performs non-oblivious online convex optimization and the other tackles prediction with limited advice (PLA) with varying sample sizes. The authors develop a unified strategy for the q-player that uses implicit exploration-based methods when r_t=1 and prediction with limited advice methods when r_t≥2, eliminating the need for separate sub-algorithms. The algorithm uses FTRL updates with step sizes η_{w,t} = √(2D)/(√(5G)√t) for the w-player and η_{q,t} = √(ln m / (m∑_{j=1}^t 1/r_j)) for the q-player. For r_t≥2, DepRound sampling ensures unbiased loss estimates while maintaining the theoretical guarantees.

## Key Results
- Establishes the first high-probability regret bound for non-oblivious PLA with varying sample sizes
- Achieves optimization error bound of O(1/t · sqrt(sum_{j=1}^t m/r_j · log m)) that matches existing O(m log(m)/ε²) sample complexity for any fixed r∈[m]
- Experimental results show faster convergence when processing more samples per round compared to fixed-sample baselines
- Maintains comparable performance to Online(1) and SMD(m) baselines while offering greater flexibility in sample querying

## Why This Works (Mechanism)
The algorithm works by unifying two distinct approaches into a single framework that adapts to varying sample sizes. For single-sample rounds, it uses implicit exploration to handle the high variance, while for multiple-sample rounds it employs prediction with limited advice to leverage the additional information. The unified approach maintains the same regret guarantees as separate algorithms while eliminating computational overhead. The key insight is that the FTRL framework can accommodate both strategies through a carefully designed loss estimator and step size schedule.

## Foundational Learning
- **Non-oblivious online convex optimization**: Required for understanding how the algorithm handles adversarial sequences that depend on past decisions; quick check: verify that regret bounds account for the dependence between queries and losses.
- **Prediction with limited advice**: Needed to understand the sample efficiency when only a subset of distributions can be queried; quick check: confirm that unbiased estimators preserve the regret guarantees.
- **Distributionally robust optimization**: Provides the problem context where worst-case risk over multiple distributions must be minimized; quick check: verify that the max-risk objective is properly formulated.
- **Follow-the-regularized-leader (FTRL)**: The optimization framework that enables the unified approach; quick check: confirm that the closed-form updates match the theoretical derivations.
- **DepRound sampling**: Critical for generating unbiased samples when r_t≥2 while maintaining computational efficiency; quick check: verify that the sampling preserves the expected marginals.
- **Implicit exploration**: Technique for handling high-variance single-sample updates; quick check: confirm that the exploration parameter γ_t = 1/(2η_{q,t}) provides the necessary bias correction.

## Architecture Onboarding

**Component Map**: w-player (FTRL) -> q-player (unified PLA) -> loss estimator (IX/depRound) -> optimization error evaluation

**Critical Path**: The algorithm proceeds by alternating updates between the w-player and q-player. At each round t, the q-player selects distribution weights q_t based on past loss estimates, then samples r_t distributions to observe losses, the w-player updates based on the weighted loss, and both players' outputs are uniformly averaged for final evaluation.

**Design Tradeoffs**: The unified approach trades some implementation complexity (handling both IX and DepRound in one framework) for significant flexibility in sample querying. The choice of step sizes balances exploration (for r_t=1) with exploitation (for r_t≥2). The use of uniform averaging rather than weighted averaging simplifies the analysis but may sacrifice some statistical efficiency.

**Failure Signatures**: Performance degradation occurs when the loss estimates become highly biased (due to numerical instability in exponential weights), when DepRound fails to maintain the correct sampling probabilities, or when the step sizes decay too slowly causing oscillations. The algorithm is particularly sensitive to the choice of D and G constants, which affect the exploration-exploitation balance.

**Three First Experiments**:
1. Implement Algorithm 1 (unified q-player) with both IX estimator (γ_t = 1/(2η_{q,t})) for r_t=1 and DepRound for r_t≥2, verify that the sampling maintains the correct marginals.
2. Run Algorithm 2 (w-player FTRL) with logistic loss and confirm that the closed-form projection matches the theoretical bounds for D=1, G=1.
3. Test the complete Algorithm 3 on synthetic data with r_t ~ Uniform[1, m-1] and compare optimization error against the theoretical O(1/t · sqrt(sum_{j=1}^t m/r_j · log m)) bound.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the proposed flexible sample query framework and its theoretical guarantees be extended to Minimax Regret Optimization (MRO) algorithms? The current analysis is specific to GDRO, while MRO uses excess risk instead of vanilla risk.
- **Open Question 2**: How does the algorithm perform in practical large-scale applications with dynamic sampling, specifically for large language models (LLMs) and recommendation systems? Current experiments are limited to synthetic and small real-world datasets.
- **Open Question 3**: Can the time-uniform optimization error bound be preserved if the sequence of sample sizes {r_t} is adaptive (history-dependent) rather than oblivious? Theorem 2 explicitly requires oblivious sequences, suggesting the guarantee may fail for adaptive sampling.

## Limitations
- Theoretical guarantees rely on bounded domain diameter and gradient assumptions that may not hold exactly for logistic regression on real data
- Performance depends critically on unknown constants D and G for step size computation, requiring estimation from data
- The algorithm's complexity and dependence on DepRound may introduce numerical instability when probabilities become very small
- Experimental validation is limited to relatively small datasets, leaving scalability to large-scale applications (e.g., LLMs) untested

## Confidence
- **High confidence**: The theoretical framework for the unified q-player algorithm and the O(1/t · sqrt(sum_{j=1}^t m/r_j · log m)) optimization error bound are mathematically sound
- **Medium confidence**: The experimental methodology is well-specified, but implementation details for baseline algorithms and exact data preprocessing could affect quantitative results
- **Medium confidence**: The synthetic data generation process is clearly described, but the choice of w*_i parameters and their impact on convergence rates may vary

## Next Checks
1. Implement a numerical stability check for the exponential weights update (12) by clamping probabilities to [10^(-8), 1-10^(-8)] and verify that this doesn't affect the theoretical regret bounds.
2. Verify the DepRound implementation by running it independently with known probability distributions and checking that the output sets have the correct sizes and maintain the expected marginals.
3. Reproduce the fixed-resource setting experiments with r=5 samples per round and compare the optimization error trajectory against the theoretical O(1/t · sqrt(m/(5t) · log m)) bound to confirm the rate matches expectations.