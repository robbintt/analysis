---
ver: rpa2
title: Ranked Voting based Self-Consistency of Large Language Models
arxiv_id: '2505.10772'
source_url: https://arxiv.org/abs/2505.10772
tags:
- voting
- ranked
- answer
- answers
- oting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces a ranked voting based self-consistency approach\
  \ for enhancing the reasoning performance of large language models. Unlike traditional\
  \ majority voting that selects a single answer per response, this method generates\
  \ ranked candidate answers and applies ranked voting methods\u2014instant-runoff,\
  \ Borda count, and mean reciprocal rank\u2014to determine the most consistent answer."
---

# Ranked Voting based Self-Consistency of Large Language Models

## Quick Facts
- **arXiv ID:** 2505.10772
- **Source URL:** https://arxiv.org/abs/2505.10772
- **Reference count:** 8
- **Primary result:** Ranked voting methods (instant-runoff, Borda count, MRR) outperform traditional majority voting in LLM self-consistency, improving accuracy by 2.68-4.95% across multiple datasets and model sizes.

## Executive Summary
This paper introduces a ranked voting based self-consistency approach that enhances the reasoning performance of large language models by generating ranked candidate answers and applying voting methods to determine the most consistent answer. Unlike traditional self-consistency that uses majority voting on single answers, this method explicitly asks LLMs to provide ranked candidate answers and applies ranked voting algorithms including instant-runoff, Borda count, and mean reciprocal rank. The approach is validated across six datasets using both open-source and closed-source LLMs, consistently outperforming baseline self-consistency methods with average accuracy gains ranging from 2.68% to 4.95% depending on model size.

## Method Summary
The proposed method generates ranked candidate answers by prompting LLMs to provide a primary answer followed by alternative ranked options, then applies ranked voting methods (instant-runoff, Borda count, and mean reciprocal rank) to determine the most consistent answer. The approach explicitly constructs ranked lists rather than single answers, allowing voting algorithms to leverage the full ranking information. This differs from traditional self-consistency by capturing the model's uncertainty and preference ordering among multiple candidates, which proves more effective than binary majority voting for identifying the most consistent answer across diverse reasoning tasks.

## Key Results
- Ranked voting methods consistently outperform baseline self-consistency with accuracy gains of 2.68% to 4.95% across different model sizes
- The method reduces tie occurrences and increases robustness to few-shot example order variations
- While improvements are substantial for smaller models (up to +5.33% for GPT-3.5), gains for GPT-4 are marginal (+0.48%), suggesting potential ceiling effects
- Among the three voting methods, Borda count and MRR show the most consistent improvements across different datasets

## Why This Works (Mechanism)
The ranked voting approach works by capturing and leveraging the model's internal confidence ordering across multiple candidate answers rather than treating all answers equally. Traditional self-consistency methods lose information by only considering which answer appears most frequently, while ranked voting preserves the relative preference structure that LLMs naturally generate when asked for multiple candidates. This additional structure allows voting algorithms to identify consensus answers that may not be the plurality winner but represent the most robust and consistent choice according to the model's own ranking.

## Foundational Learning
- **Self-consistency principle**: Multiple sampling and voting improves LLM reliability by reducing variance in responses
- **Ranked voting algorithms**: Mathematical methods (instant-runoff, Borda count, MRR) for aggregating ranked preferences
- **LLM response generation**: Understanding how models generate and rank multiple candidate answers
- **Prompt engineering for multiple candidates**: Techniques for eliciting structured, ranked responses from LLMs
- **Voting theory fundamentals**: How different voting methods handle preference aggregation and consensus
- **Benchmark dataset characteristics**: Understanding the reasoning task distributions across tested datasets

## Architecture Onboarding

**Component Map:** LLM (response generator) -> Ranked answer extraction -> Voting algorithm (instant-runoff/Borda/MRR) -> Final answer selection

**Critical Path:** Prompt construction → Multiple LLM responses with ranked candidates → Aggregation via ranked voting → Answer determination

**Design Tradeoffs:** Ranked voting captures more information than majority voting but requires more complex prompting and computation; simpler methods like approval voting fail to show improvements, suggesting binary thresholds are insufficient for LLM uncertainty

**Failure Signatures:** Similar performance to baseline when models generate few candidates; reduced gains for highly capable models approaching performance ceilings; potential computational overhead from generating multiple ranked candidates

**First Experiments:** 1) Compare ranked voting performance across different voting algorithms on the same dataset, 2) Test the impact of varying the number of candidates generated by the model, 3) Evaluate the method's robustness to different prompt formulations and few-shot example orderings

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does explicitly prompting the model to expand the candidate pool beyond the autonomous default (usually 4-5) significantly improve the performance of ranked voting methods?
- Basis in paper: The authors note in the Limitations section that they did not impose additional constraints by designing prompts to expand the candidate pool, leaving the model to autonomously rank a small set.
- Why unresolved: The current study relies on the model's inherent tendency to generate a specific number of candidates rather than testing the upper bounds of how many candidates can be effectively utilized in the voting process.
- What evidence would resolve it: Experiments where the number of candidates ($c$) is forced to higher values (e.g., $c=10$ or $c=20$) via specific prompting constraints, compared against the autonomous baseline.

### Open Question 2
- Question: Why does Approval Voting fail to show improvements over majority voting in this context, and does this imply that binary approval thresholds are insufficient for capturing LLM uncertainty?
- Basis in paper: In the Limitations and Appendix C, the authors introduce Approval Voting but find it performs similarly to the majority voting baseline, unlike the other three ranked methods which showed gains.
- Why unresolved: The paper identifies the failure but does not analyze whether the issue stems from the specific threshold used ($t=2$), the voting mechanism itself, or the nature of LLM probability distributions over candidates.
- What evidence would resolve it: An ablation study testing various approval thresholds ($t$) or an analysis comparing the granularity of information preserved in approval voting versus scoring methods like Borda count.

### Open Question 3
- Question: Does the relative effectiveness of ranked voting self-consistency diminish as the base model's capability approaches state-of-the-art levels?
- Basis in paper: The results in Table 5 show that while ranked voting improves GPT-3.5 substantially (up to +5.33%), the gains for GPT-4 are marginal (Avg +0.48%), suggesting a potential ceiling effect for highly capable models.
- Why unresolved: The paper validates the method on GPT-4 but does not discuss whether the smaller improvements are due to dataset saturation or a fundamental limit of this method for high-capacity models.
- What evidence would resolve it: Evaluation on more challenging benchmarks where GPT-4 does not score near ceiling, or analysis correlating baseline model accuracy with the relative gain provided by ranked voting.

## Limitations
- Evaluation focuses on six benchmark datasets, potentially limiting generalizability to real-world reasoning tasks
- Computational overhead of generating and processing ranked candidate answers is not thoroughly quantified
- Claims about robustness to few-shot example ordering are based on specific prompt templates without broader validation

## Confidence
- **Performance improvements**: Medium confidence - significant gains demonstrated but within limited scope of tested models and datasets
- **Robustness claims**: Medium confidence - tie reduction documented but practical significance unclear
- **Generalizability**: Low confidence - results may not extend to diverse reasoning tasks or different model architectures

## Next Checks
1. **Cross-Domain Robustness Testing**: Evaluate the ranked voting method on diverse reasoning tasks beyond current benchmarks, including multi-step mathematical problems, commonsense reasoning, and domain-specific knowledge applications to assess generalizability.

2. **Computational Efficiency Analysis**: Quantify the exact computational overhead introduced by generating and processing ranked candidate answers, comparing wall-clock time and resource utilization against traditional self-consistency methods across different model sizes.

3. **Ablation Study on Voting Methods**: Conduct systematic comparison of the three voting methods (instant-runoff, Borda count, and mean reciprocal rank) across varying problem types to determine which method performs best under specific conditions and whether combining them provides additional benefits.