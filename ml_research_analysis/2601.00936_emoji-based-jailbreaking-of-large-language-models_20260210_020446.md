---
ver: rpa2
title: Emoji-Based Jailbreaking of Large Language Models
arxiv_id: '2601.00936'
source_url: https://arxiv.org/abs/2601.00936
tags:
- prompts
- safety
- emoji
- emoji-based
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates emoji-based jailbreaking of large language
  models (LLMs), where emoji sequences are embedded in prompts to bypass safety mechanisms
  and elicit harmful outputs. Four open-source LLMs (Mistral 7B, Qwen 2 7B, Gemma
  2 9B, Llama 3 8B) were tested with 50 emoji-augmented prompts, evaluating jailbreak
  success rate, ethical compliance, and latency.
---

# Emoji-Based Jailbreaking of Large Language Models

## Quick Facts
- **arXiv ID**: 2601.00936
- **Source URL**: https://arxiv.org/abs/2601.00936
- **Authors**: M P V S Gopinadh; S Mahaboob Hussain
- **Reference count**: 24
- **Primary result**: Emoji-based jailbreaking reveals significant model-specific vulnerabilities, with success rates ranging from 0% to 10% across four tested LLMs.

## Executive Summary
This study investigates emoji-based jailbreaking attacks on large language models, where emoji sequences embedded in prompts bypass safety mechanisms to elicit harmful outputs. Testing four open-source LLMs (Mistral 7B, Qwen 2 7B, Gemma 2 9B, Llama 3 8B) with 50 emoji-augmented prompts revealed significant inter-model differences in vulnerability. The findings demonstrate that current safety mechanisms are not equipped to handle non-textual adversarial inputs, highlighting the need for emoji-aware defenses in LLM safety pipelines.

## Method Summary
The study employed 50 emoji-augmented prompts using "emoji stuffing" (inserting emojis between words) and "emoji chaining" (emoji sequences representing instructions) to test four open-source LLMs locally via Ollama API. Responses were classified into Successful (restricted content generated), Partial (ambiguous), or Failed (rejected/irrelevant) categories using keyword-based triage (32 keywords per category) with human validation. Metrics included jailbreak success rate, ethical compliance percentage, and response latency, with chi-square testing confirming significant inter-model differences.

## Key Results
- Gemma 2 9B and Mistral 7B achieved 10% success rates, Llama 3 8B reached 6%, while Qwen 2 7B showed complete resistance (0% success, 100% ethical compliance)
- Chi-square test confirmed significant inter-model differences (χ² = 32.94, p < 0.001)
- Emoji-based prompts created semantic ambiguity, with partial responses ranging from 15-50% across models

## Why This Works (Mechanism)

### Mechanism 1
Emoji tokens map to internal representations that share semantic space with restricted text terms, bypassing keyword-based filters. Emojis are tokenized similarly to words but their internal representations capture contextual nuances, allowing a knife emoji to overlap with terms like "sword" or "cut" while evading explicit text filters.

### Mechanism 2
Emoji insertion creates semantic ambiguity that places prompts in a "gray area" between refusal and compliance. Emojis introduce contextual flexibility, causing models to misinterpret malicious intent as benign or educational, with partial compliance responses (15-25 per model) indicating ambiguity rather than clear refusal.

### Mechanism 3
Emojis fragment malicious n-gram patterns at token boundaries, disrupting pattern-matching filters. "Emoji stuffing" inserts emojis between words, breaking token sequences that would trigger safety filters while preserving overall semantic meaning.

## Foundational Learning

- **Tokenization and Vocabulary Construction**: Understanding how emojis become tokens (often multi-token sequences) explains why they evade text-based filters. Quick check: Can you explain why a single emoji might tokenize to 2-4 tokens in most LLM vocabularies?

- **Safety Alignment via RLHF/SFT**: Safety mechanisms are trained on text-based examples; emoji-based prompts may fall outside training distribution. Quick check: What type of prompts would be underrepresented in typical safety training datasets?

- **Embedding Space and Semantic Similarity**: The attack exploits overlapping representations between emoji embeddings and restricted term embeddings. Quick check: If a knife emoji and the word "weapon" have high cosine similarity, what filter limitation does this expose?

## Architecture Onboarding

- **Component map**: Prompt design layer → Unicode NFC normalization → API submission → Response capture → Keyword classification → Human validation → Metric aggregation → Chi-square statistical test

- **Critical path**: Emoji-augmented prompts designed → Unicode normalization applied → Model inference via Ollama API → Responses captured with latency → Keyword classification performed → Human validation completed → Metrics calculated → Statistical significance tested

- **Design tradeoffs**: 50 prompts balances coverage vs. manual validation burden but may miss emoji/cultural diversity; keyword-based classification is fast but struggles with ambiguous responses; local deployment ensures reproducibility but limits scalability

- **Failure signatures**: High "partial" response rate (up to 50% for Qwen 2 7B) indicates semantic ambiguity; Gemma 2 9B's 44.2s latency + 66% compliance suggests deeper processing may increase vulnerability; model-specific success rates (0-10%) indicate safety alignment varies significantly

- **First 3 experiments**:
  1. Replicate baseline: Run 50 prompts on target models, measure success/partial/failed distribution
  2. Emoji normalization defense: Pre-process prompts by converting emojis to text descriptions, compare success rates
  3. Cross-model transfer test: Take successful jailbreak prompts from Gemma 2 9B and test on Qwen 2 7B to assess transferability

## Open Questions the Paper Calls Out

- Would larger and more culturally diverse emoji-based prompt sets reveal different vulnerability patterns across LLMs? The study tested only 50 prompts with limited emoji combinations; cultural variations in emoji semantics remain unexplored.

- Can automated, real-time detection mechanisms effectively mitigate emoji-based jailbreaking without degrading model utility? No detection or mitigation strategies were tested; current study only quantified vulnerabilities.

- Do other non-textual inputs (symbols, images) produce similar jailbreaking vulnerabilities as emojis? Study focused exclusively on emoji sequences; tokenization behavior of other visual inputs remains unknown.

- What architectural or training factors explain why Qwen 2 7B achieved complete resistance while Gemma 2 9B showed 10% vulnerability? Paper reports outcomes but does not analyze tokenizer behavior, safety training data, or internal representations.

## Limitations

- Emoji vocabulary constrained to 16 emojis across all prompts, potentially missing context-specific or culturally diverse interpretations
- 50-prompt corpus may not capture full spectrum of emoji combinations and semantic contexts
- Small sample size (n=50) limits statistical power for detecting subtle differences between models

## Confidence

- **High Confidence**: Model-specific success rate differences (0-10%) clearly demonstrated through statistical testing (χ² = 32.94, p < 0.001)
- **Medium Confidence**: Proposed mechanism of emoji semantic overlap with restricted terms is theoretically sound but lacks direct empirical validation
- **Low Confidence**: Claims about emoji tokenization patterns disrupting safety filters based on related work rather than direct experimental evidence

## Next Checks

1. **Embedding Space Validation**: Analyze cosine similarity between emoji embeddings and corresponding restricted term embeddings across all four models, computing similarity distributions for successful vs. failed jailbreak attempts.

2. **Emoji Diversity Expansion**: Replicate study using broader emoji vocabulary (minimum 50 unique emojis) and increased prompt count (n=100-200) to assess whether success rates scale with emoji diversity.

3. **Cross-Model Transferability**: Take most successful jailbreak prompts from each model and systematically test them across all other models to quantify transferability rates.