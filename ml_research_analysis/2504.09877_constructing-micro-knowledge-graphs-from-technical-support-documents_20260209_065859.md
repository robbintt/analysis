---
ver: rpa2
title: Constructing Micro Knowledge Graphs from Technical Support Documents
arxiv_id: '2504.09877'
source_url: https://arxiv.org/abs/2504.09877
tags:
- support
- technical
- knowledge
- page
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes constructing micro knowledge graphs (micrographs)
  for each technical support document to address the granularity challenge in building
  knowledge graphs from large corpora of such documents. Instead of storing only key
  entities and actions in a knowledge graph, which leads to loss of knowledge, the
  micrograph stores all entities and actions in a page along with their exact location
  and relationships within the page.
---

# Constructing Micro Knowledge Graphs from Technical Support Documents

## Quick Facts
- arXiv ID: 2504.09877
- Source URL: https://arxiv.org/abs/2504.09877
- Reference count: 3
- One-line primary result: Proposes micro knowledge graphs per document to preserve all entities and actions with exact locations, avoiding knowledge loss from traditional KG filtering.

## Executive Summary
Technical support documents contain rich entity-action relationships and procedural steps, but building a global knowledge graph (KG) from them faces a granularity challenge: storing all entities makes the KG unusable, while storing only key entities leads to knowledge loss. This paper introduces micro knowledge graphs (micrographs) that store all entities and actions for each document, along with their exact locations and relationships within the page. By leveraging meta-information and HTML structure, the system extracts sections, entities, actions, and procedures (including conditional blocks) to construct detailed micrographs. These micrographs can enhance technical support applications by providing fine-grained, structured knowledge for reasoning and conditional guidance.

## Method Summary
The method involves manually creating meta-information (document types, section mappings, constraint entities, entity/action dictionaries) for a representative corpus of technical support documents. The system then uses this meta-information and HTML structure to identify document sections, extract entities and actions, and parse procedural steps and conditional blocks. Procedures are represented as sequences of steps with types and nested conditional structures. The extracted content is merged and output as micrograph JSON documents, which are stored in a graph store for querying by downstream applications.

## Key Results
- Micrographs preserve all entities and actions from technical support documents, avoiding knowledge loss from filtering.
- The micrograph construction system uses meta-information and HTML structure to extract sections, entities, actions, and procedures.
- Micrographs can be used as additional knowledge sources by technical support applications to provide more relevant and specific information.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preserving all entities and actions within page-specific micrographs retains granular knowledge that would otherwise be lost in a consolidated knowledge graph.
- Mechanism: By constructing a separate micrograph for each support document, the system captures every entity, action, and their intra-page relationships, including exact location metadata.
- Core assumption: Users or downstream applications benefit from fine-grained, page-local knowledge that supports disambiguation and conditional reasoning.
- Evidence anchors:
  - [abstract] The micrograph stores all entities and actions in a page and also takes advantage of the structure of the page to represent exactly in which part of that page these entities and actions appeared.
  - [section] Page 2 states that storing only key entities leads to loss of knowledge represented by entities and actions left out of the KG.
  - [corpus] Related work on emerging entities in KGs (AgREE) reinforces the broader challenge of incorporating fine-grained, evolving entities.

### Mechanism 2
- Claim: Leveraging HTML structure and meta-information enables accurate extraction of sections, procedures, and conditional logic from semi-structured support documents.
- Mechanism: The system uses manually defined meta-information combined with HTML parsing to identify sections, extract entities/actions, and recursively parse procedural steps and conditional blocks.
- Core assumption: The corpus has predictable structural patterns that can be captured in meta-information for reliable extraction.
- Evidence anchors:
  - [abstract] The micrograph construction system uses meta-information and HTML structure to identify sections, extract entities and actions, and represent procedures as sequences of steps with types and conditional blocks.
  - [section] Page 3-4 describes the extraction pipeline and Figure 1 architecture, including the procedure extraction module.
  - [corpus] Arctic-Extract and related work demonstrate that structural extraction from documents is an active area.

### Mechanism 3
- Claim: Micrographs can enhance technical support applications by providing fine-grained, structured knowledge for reasoning and conditional guidance.
- Mechanism: By storing detailed entity/action relationships, step sequences, and conditional blocks, micrographs enable applications to return specific page sections, ask disambiguating follow-up questions, or guide users through conditional procedures step-by-step.
- Core assumption: Applications are capable of consuming and reasoning over the structured micrograph data to produce more relevant outputs.
- Evidence anchors:
  - [abstract] These micrographs can be used as additional knowledge sources by technical support applications.
  - [section] Page 2-3 discusses use cases such as chatbots asking followup questions and applications providing step-by-step guidance.
  - [corpus] No direct empirical validation in neighbor papers; RAG and KG reasoning papers suggest the broader viability of structured knowledge sources for QA.

## Foundational Learning

- Concept: Knowledge Graph Granularity
  - Why needed here: Understanding the trade-off between storing all entities (high granularity, potentially unusable graph) vs. only key entities (lower granularity, knowledge loss) is central to the micrograph design.
  - Quick check question: Can you explain why a global KG that includes every entity from every document might become difficult to query effectively?

- Concept: Information Extraction from Semi-Structured Documents
  - Why needed here: The micrograph pipeline relies on extracting entities, actions, and procedural steps from HTML documents using meta-information and structural cues.
  - Quick check question: What role does meta-information (e.g., expected section headings, domain dictionaries) play in guiding extraction from a new document corpus?

- Concept: Conditional Procedural Representation
  - Why needed here: Technical support documents often contain solution procedures with conditional steps; representing these in the micrograph is key for reasoning.
  - Quick check question: How would you represent a conditional block in a knowledge graph, and why might this be useful for a troubleshooting chatbot?

## Architecture Onboarding

- Component map:
  Meta-Information & Schema Set -> HTML Parser & Section Extractor -> Procedure Extraction Module -> Entity & Action Linking Module -> Merge & Relation Creation -> Micrograph JSON Generator -> Graph Store

- Critical path:
  1. Define meta-information and schemas for the document corpus (manual, required before processing).
  2. Ingest HTML document and extract sections using structure and meta-info.
  3. Pass solution/diagnostic sections to Procedure Extraction Module.
  4. Merge procedure output with other section content and perform entity/action linking.
  5. Generate micrograph JSON and store in Graph Store.

- Design tradeoffs:
  - Manual meta-information definition vs. fully automated schema inference: Manual setup provides control and accuracy for known corpora but requires upfront effort.
  - Storing all entities/actions vs. filtering: Full storage preserves knowledge but increases data volume and query complexity.
  - Optional entity linking: Adds richness but depends on the quality of the custom linking algorithm.

- Failure signatures:
  - Incomplete or incorrect section extraction due to unexpected HTML structures or missing meta-information.
  - Failure to correctly identify conditional blocks or step sequences in procedural content.
  - Entity/action linking errors leading to incorrect or missing relationships in the micrograph.
  - Performance issues in querying micrographs if the graph store is not optimized for the expected query patterns.

- First 3 experiments:
  1. Section extraction accuracy test: Process a held-out sample of technical support pages and manually validate that sections are correctly identified using the defined meta-information.
  2. Procedure extraction validation: Compare extracted step sequences and conditional blocks from solution sections against ground-truth annotations to measure precision and recall.
  3. End-to-end retrieval quality: Integrate micrographs into a simple QA or search system and evaluate whether micrograph-backed queries return more specific/relevant results compared to a baseline that only retrieves full page URLs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the definition of document meta-information (section mappings, entity dictionaries) be automated to remove the manual examination bottleneck?
- Basis in paper: The text states that a manual step of examining a representative subset of that corpus is required to create meta-information before the system can process documents.
- Why unresolved: The proposed architecture relies on human-defined constraints to parse HTML structures, which limits scalability to diverse, unseen corpora.
- What evidence would resolve it: A method that autonomously infers section headings and entity types from document clusters without human intervention.

### Open Question 2
- Question: What is the accuracy of the dependency-parse-based extraction when handling complex conditional blocks within procedure steps?
- Basis in paper: The procedure extraction module relies on PoS tag and DEP tag from the parse tree to identify conditional blocks, a heuristic that may struggle with ambiguous technical phrasing.
- Why unresolved: The paper provides an architectural description and a single example but does not offer quantitative evaluation metrics for the extraction algorithm.
- What evidence would resolve it: Performance benchmarks of the extraction module against a ground-truth dataset of technical support documents.

### Open Question 3
- Question: Does querying a distributed network of micrographs offer better performance than traditional document retrieval for complex technical support tasks?
- Basis in paper: The authors propose these micrographs as additional knowledge sources for chatbots and QA systems, claiming they provide more relevant information than key-entity extraction.
- Why unresolved: The paper does not compare the query latency or answer quality of the micrograph approach against standard search engine or monolithic KG baselines.
- What evidence would resolve it: A user study or system benchmark comparing answer relevance and retrieval speed for micrographs versus standard keyword search.

## Limitations
- Manual meta-information creation for each new document corpus is time-consuming and limits scalability.
- The custom entity extraction and linking algorithm is not detailed, making accuracy assessment difficult.
- No empirical validation of downstream application performance improvements when using micrographs versus traditional approaches.

## Confidence
- **High Confidence:** The core problem statement and the general approach of document-specific micrographs are well-justified.
- **Medium Confidence:** The extraction pipeline using HTML structure and meta-information is logically coherent, but lacks detail on key algorithms.
- **Low Confidence:** No empirical validation of downstream application performance improvements is provided.

## Next Checks
1. Meta-information coverage test: Measure the percentage of real-world technical documents that can be accurately processed using the proposed meta-information schema without requiring manual updates.
2. Entity linking evaluation: Implement and benchmark the custom entity extraction and linking algorithm on a labeled dataset to assess precision and recall compared to standard NER approaches.
3. Application performance study: Compare end-user outcomes (e.g., resolution time, accuracy of answers) when technical support chatbots use micrograph-backed reasoning versus baseline document retrieval.