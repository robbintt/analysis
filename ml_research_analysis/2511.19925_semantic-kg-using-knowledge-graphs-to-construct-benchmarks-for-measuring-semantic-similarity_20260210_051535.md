---
ver: rpa2
title: 'Semantic-KG: Using Knowledge Graphs to Construct Benchmarks for Measuring
  Semantic Similarity'
arxiv_id: '2511.19925'
source_url: https://arxiv.org/abs/2511.19925
tags:
- name
- node
- semantic
- dataset
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for generating semantic
  similarity benchmarks using knowledge graphs (KGs), addressing the challenge of
  evaluating large language models' (LLMs) ability to capture semantic content. The
  method involves perturbing KGs to create semantic variations, generating natural-language
  statements from these graphs, and validating the outputs to ensure correctness.
---

# Semantic-KG: Using Knowledge Graphs to Construct Benchmarks for Measuring Semantic Similarity

## Quick Facts
- arXiv ID: 2511.19925
- Source URL: https://arxiv.org/abs/2511.19925
- Reference count: 40
- Primary result: Novel framework for generating semantic similarity benchmarks using knowledge graphs that reveals method performance varies significantly by domain and perturbation type

## Executive Summary
This paper introduces Semantic-KG, a framework for generating semantic similarity benchmarks using knowledge graphs (KGs). The approach involves perturbing KGs to create semantic variations, generating natural-language statements from these graphs, and validating the outputs to ensure correctness. Benchmark datasets were created across four domains (general knowledge, biomedicine, finance, and biology) and used to evaluate various semantic similarity methods, including traditional NLP metrics and LLM-as-a-judge. The results revealed that method performance varies significantly depending on the type of semantic perturbation and the domain, with no single method consistently superior. This highlights the need for domain-specific and semantic-aware evaluation when assessing LLM outputs.

## Method Summary
The Semantic-KG framework generates semantic similarity benchmarks by systematically perturbing knowledge graphs and converting them to natural language statements. The process begins with existing KGs in target domains, applies controlled semantic perturbations (such as entity substitution, relationship modification, or attribute changes), then generates natural language statements from both original and perturbed graphs. These statement pairs serve as benchmark items with known semantic relationships. The framework includes validation steps to ensure generated statements are grammatically correct and maintain the intended semantic relationships. The approach was applied to create benchmarks in four domains: general knowledge, biomedicine, finance, and biology.

## Key Results
- Method performance varies significantly across different semantic perturbation types, with no single method consistently superior
- Domain-specific performance differences were observed, requiring domain-aware evaluation strategies
- Traditional NLP metrics and LLM-as-a-judge approaches showed complementary strengths and weaknesses
- The generated benchmarks successfully exposed limitations in current semantic similarity evaluation methods

## Why This Works (Mechanism)
The framework leverages the structured nature of knowledge graphs to create controlled semantic variations. By systematically perturbing KGs and generating corresponding natural language statements, it creates test pairs with known semantic relationships. This controlled approach allows for precise measurement of how different semantic similarity methods handle specific types of semantic changes, revealing their strengths and limitations in capturing semantic content.

## Foundational Learning
- **Knowledge Graph Structure**: Understanding nodes, edges, and attributes in KGs - why needed for creating meaningful perturbations; quick check: can identify semantic impact of modifying different KG elements
- **Semantic Similarity Metrics**: Familiarity with cosine similarity, BERTScore, and other evaluation methods - why needed for benchmarking different approaches; quick check: can explain differences between embedding-based and token-based metrics
- **Natural Language Generation from KGs**: Converting structured data to natural language - why needed for creating human-readable benchmarks; quick check: can generate coherent statements from given KG fragments
- **Perturbation Theory**: Understanding how controlled changes affect semantic content - why needed for creating meaningful test cases; quick check: can predict semantic impact of specific KG modifications

## Architecture Onboarding

Component Map:
KG Repository -> Perturbation Engine -> Statement Generator -> Validator -> Benchmark Dataset

Critical Path:
KG Repository -> Perturbation Engine -> Statement Generator -> Validator (failure at any point requires redesign)

Design Tradeoffs:
- Granularity vs. Coverage: More specific perturbations provide clearer insights but reduce domain coverage
- Generation Speed vs. Quality: Faster generation may sacrifice validation thoroughness
- Domain Specificity vs. Generalizability: Domain-specific benchmarks are more accurate but less broadly applicable

Failure Signatures:
- Incorrect semantic relationships between statement pairs
- Generated statements that don't preserve intended meaning
- Validation failures indicating generation errors
- Method performance inconsistencies across similar perturbation types

First Experiments:
1. Test perturbation engine on simple KG with known expected outcomes
2. Validate statement generation on single domain before scaling
3. Compare generated benchmarks against existing semantic similarity datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability concerns for domains with different KG structures or complex semantic reasoning requirements
- Validation process details are not thoroughly specified, raising potential bias concerns
- Sample size of tested methods and domains may be insufficient for definitive conclusions
- Limited guidance for predicting method performance in new, unseen domains

## Confidence
- Methodology reproducibility: High for described domains, Medium for novel domains
- Generalization claims: Medium
- Domain-specific performance findings: Medium
- Perturbation sensitivity results: Medium

## Next Checks
1. Cross-domain generalization test: Apply the framework to at least two additional domains (e.g., legal or technical documentation) to assess scalability and identify domain-specific challenges
2. Human evaluation correlation: Conduct comprehensive human evaluation to correlate automated scores with human judgments, focusing on cases with significant performance variations
3. Perturbation type analysis: Systematically vary perturbation types and degrees to evaluate method sensitivity and identify failure modes