---
ver: rpa2
title: 'Beyond expected value: geometric mean optimization for long-term policy performance
  in reinforcement learning'
arxiv_id: '2508.21443'
source_url: https://arxiv.org/abs/2508.21443
tags:
- reward
- growth
- time-average
- rate
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel reinforcement learning algorithm
  that optimizes both the expected cumulative reward and the time-average growth rate
  of individual trajectories. The key idea is to incorporate a modified geometric
  mean estimator with an N-sliding window as a regularizer in the optimization objective,
  addressing non-ergodic reward dynamics that can lead to risky policies in conventional
  RL.
---

# Beyond expected value: geometric mean optimization for long-term policy performance in reinforcement learning

## Quick Facts
- **arXiv ID**: 2508.21443
- **Source URL**: https://arxiv.org/abs/2508.21443
- **Reference count**: 33
- **Key outcome**: Introduces RL algorithm optimizing both expected cumulative reward and time-average growth rate using modified geometric mean regularizer, demonstrating improved median performance on Lunar Lander and Cart-Pole environments

## Executive Summary
This paper addresses the limitations of conventional reinforcement learning that optimizes only expected cumulative rewards, which can lead to risky policies in non-ergodic environments where ensemble averages misrepresent individual trajectory performance. The authors propose a novel approach that incorporates a modified geometric mean estimator with an N-sliding window as a regularizer in the optimization objective, balancing the trade-off between ensemble and time-average performance through a parameter λ. Experiments on Lunar Lander and Cart-Pole environments demonstrate that this method achieves higher median cumulative rewards and more consistent performance across trajectories compared to conventional multi-step Q-learning, while maintaining robustness without requiring explicit knowledge of reward dynamics.

## Method Summary
The method introduces a modified geometric mean (MGM) estimator Ĝ_t = sgn(R̂_t)|R̂_t|^(1/N) where R̂_t is the cumulative reward over an N-window sliding window, addressing numerical issues with negative rewards by preserving the sign. This estimator is integrated into N-step Q-learning as a regularizer, with the Q-update target becoming a convex combination: Δ_t = (1-λ)δ^(N)_t + λ(1-γ^N)Ĝ_t, where λ ∈ [0,1] controls the balance between conventional expected return and time-average growth rate optimization. The algorithm maintains standard Q-network architecture but modifies the target calculation to consider both immediate rewards and compounded growth rate of reward sequences, with non-overlapping (e=1) and overlapping (e=0) window configurations tested depending on the environment.

## Key Results
- Outperforms conventional multi-step Q-learning on Lunar Lander and Cart-Pole environments
- Achieves higher median cumulative rewards while maintaining more consistent performance across trajectories
- Demonstrates improved robustness without requiring explicit knowledge of reward dynamics
- Shows different optimal λ values for different environments, suggesting environment-dependent trade-offs

## Why This Works (Mechanism)

### Mechanism 1
Optimizing the time-average growth rate via a geometric mean regularizer mitigates risk of ruin in non-ergodic environments where ensemble averages are misleading. By convexly combining expected reward with time-average growth rate, the policy avoids high-variance strategies that yield high average rewards but high risk of failure. This relies on Assumption 2: an ergodicity transformation exists for reward dynamics. Break condition: if environment is strictly ergodic, regularization provides no added value and may slow convergence.

### Mechanism 2
The Modified Geometric Mean (MGM) with N-sliding window acts as practical estimator for time-average growth rate in unknown dynamics. Standard geometric means fail with negative rewards; MGM overcomes this by taking N-th root of absolute cumulative reward while preserving sign: Ĝ_t = sgn(R̂_t)|R̂_t|^(1/N). This captures path-dependency without requiring explicit environment model. Break condition: if cumulative rewards within window are zero or oscillate wildly around zero, gradient/sign becomes unstable.

### Mechanism 3
Integrating MGM into N-step Q-learning stabilizes learning by balancing immediate reward exploitation with long-term trajectory robustness. The method modifies N-step return target Δ_t from simple discounted sum to Δ_t = (1-λ)δ^(N)_t + λ(1-γ^N)Ĝ_t, forcing value function update to consider compounded growth rate of rewards. This assumes λ correctly weights trade-off such that ergodicity improves without sacrificing absolute reward magnitude significantly. Break condition: if λ is too high, agent may become overly conservative, preferring safe, low-growth trajectories over optimal but slightly riskier ones.

## Foundational Learning

- **Concept: Ergodicity vs. Non-Ergodicity**
  - Why needed here: Standard RL relies on Ergodic Hypothesis (time-average = ensemble average). This paper targets scenarios where this breaks (e.g., multiplicative dynamics). Understanding this distinction explains why expected value fails.
  - Quick check question: In a game where you gain 50% on heads but lose 40% on tails, does "average" outcome reflect experience of single player over time? (Answer: No; expected value grows, but single-player time-average decays).

- **Concept: Geometric Mean vs. Arithmetic Mean**
  - Why needed here: Arithmetic mean optimizes expected sum (additive), suitable for ensemble analysis. Geometric mean optimizes expected growth rate (multiplicative), suitable for single-trajectory analysis over time. This is mathematical core of proposed solution.
  - Quick check question: Which mean is appropriate for calculating average return of investment over 10 years? (Answer: Geometric mean/CAGR).

- **Concept: Regularized Markov Decision Processes (MDPs)**
  - Why needed here: Paper frames solution as regularization term added to standard Q-learning. Understanding MDP regularization (like entropy or KL divergence) helps contextualize how MGM term is integrated mathematically.
  - Quick check question: Why add regularization term to RL objective? (Answer: To impose constraints, such as robustness or proximity to previous policy, which in this case is trajectory consistency).

## Architecture Onboarding

- **Component map**: Environment -> Data Buffer -> Growth Estimator -> Q-Network -> Optimizer
- **Critical path**:
  1. Sample action, observe reward r_t and state s_t
  2. Update sliding window buffer (drop oldest, append new)
  3. Calculate Ĝ_t: Sum rewards in window → Apply sign and root (Eq 8)
  4. Calculate Target: Mix standard N-step return δ^(N)_t with Ĝ_t using λ
  5. Update Q-network towards this target

- **Design tradeoffs**:
  - Window Size (N): Larger N smooths noise and captures longer trends but increases latency and computation
  - Lambda (λ): Higher λ prioritizes survival/consistency (time-average); lower λ prioritizes raw score (ensemble)
  - Dynamic Awareness (e): Overlapping windows (e=0) provide more data but higher correlation; non-overlapping (e=1) reduces correlation but lowers sample efficiency

- **Failure signatures**:
  - Sign Oscillation: If rewards fluctuate around zero, sgn(R̂_t) flips constantly, causing unstable Q-updates
  - Conservative Stagnation: If λ is too high, agent may refuse to take necessary risks to score points, stalling at safe but suboptimal state
  - Scale Mismatch: If rewards are very small (e.g., 10^-3), N-th root might be negligible; if very large, it dominates gradient

- **First 3 experiments**:
  1. Sanity Check (Coin Toss): Implement multiplicative coin-toss game from Section III. Verify standard Q-learning seeks high-expectation "ruin" strategy while proposed method seeks positive-growth strategy.
  2. Cart-Pole Ablation: Train with λ=0 (baseline), λ=0.5, and λ=1. Plot median cumulative reward to observe trade-off between consistency and peak score.
  3. Sensitivity Analysis: Fix λ, vary window size N (e.g., 3, 5, 10) on Lunar Lander to determine sensitivity of growth estimator to trajectory length.

## Open Questions the Paper Calls Out

- Can the proposed framework be extended to continuous state and action spaces while maintaining theoretical guarantees? The conclusion states plans to extend to continuous spaces, but current algorithm relies on discrete Q-learning and tabular representations.

- Does the modified geometric mean estimator converge to true time-average growth rate under general (non-GBM) reward dynamics? Proposition 1 proves convergence only for geometric Brownian motion dynamics, but algorithm applies estimator to arbitrary unknown reward dynamics without formal guarantees.

- How should hyperparameters λ, N, and e be automatically selected for given environment? Paper manually tunes these parameters without providing principled selection criteria, showing performance varies substantially with choices.

## Limitations
- Assumes ergodicity transformation exists without systematic verification or construction method for arbitrary environments
- Modified geometric mean estimator can suffer numerical instability when cumulative rewards approach zero or oscillate in sign
- Empirical validation limited to small-scale problems without comparison to alternative robustness methods like distributional RL

## Confidence
- **High**: Mathematical formulation of MGM and integration into Q-learning updates is correct and reproducible
- **Medium**: Empirical improvements in median cumulative reward are demonstrated but may not generalize to more complex environments
- **Medium-Low**: Theoretical guarantees about avoiding "risky policies" rely on assumptions that may not hold in practice

## Next Checks
1. **Sensitivity Analysis**: Systematically vary λ and N across multiple orders of magnitude to identify optimal trade-off between ensemble and time-average optimization, and determine if parameters can be learned rather than hand-tuned.

2. **Distributional Comparison**: Compare MGM-regularized policies against distributional RL methods (C51, QR-DQN) on environments with high variance in returns to isolate whether improvements come from geometric mean optimization or general variance reduction.

3. **Failure Mode Investigation**: Construct environments where cumulative rewards frequently cross zero or exhibit strong oscillatory behavior to test stability of sign-preserving MGM estimator and identify conditions under which method breaks down.