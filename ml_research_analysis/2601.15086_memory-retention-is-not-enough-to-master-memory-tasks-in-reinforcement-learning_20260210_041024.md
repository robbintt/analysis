---
ver: rpa2
title: Memory Retention Is Not Enough to Master Memory Tasks in Reinforcement Learning
arxiv_id: '2601.15086'
source_url: https://arxiv.org/abs/2601.15086
tags:
- memory
- agents
- agent
- information
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of memory rewriting in reinforcement
  learning (RL) agents, focusing on how agents can selectively update their memory
  when environmental conditions change. The authors introduce two benchmark tasks,
  Endless T-Maze and Color-Cubes, which require agents to continually overwrite outdated
  memory information rather than just retain it.
---

# Memory Retention Is Not Enough to Master Memory Tasks in Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.15086
- Source URL: https://arxiv.org/abs/2601.15086
- Reference count: 40
- Key outcome: Recurrent networks with explicit forgetting mechanisms (like LSTM) outperform other architectures in memory rewriting tasks

## Executive Summary
This paper investigates the critical distinction between memory retention and memory rewriting in reinforcement learning agents. The authors demonstrate that many standard memory architectures, while capable of retaining information, struggle when agents must actively overwrite outdated memory information in changing environments. Through carefully designed benchmark tasks (Endless T-Maze and Color-Cubes), they show that architectures with learnable forgetting mechanisms significantly outperform those without, particularly in scenarios requiring continual memory updates. The work highlights that the ability to selectively forget is as important as the ability to remember for effective reinforcement learning in dynamic environments.

## Method Summary
The authors introduce two novel benchmark tasks designed to test memory rewriting capabilities: Endless T-Maze requires agents to navigate a changing maze layout, while Color-Cubes presents a grid-world where cube colors change over time. They evaluate multiple memory architectures including PPO-LSTM, PPO-GRU, GTrXL transformers, and structured memory systems (SHM, FFM). The evaluation uses Proximal Policy Optimization (PPO) as the base RL algorithm across all architectures. The experiments systematically compare performance across different task configurations and include ablation studies to isolate the importance of forgetting mechanisms. Performance metrics include task completion rates, reward accumulation, and generalization to unseen task variations.

## Key Results
- LSTM-based architectures with explicit forgetting gates outperform other memory architectures in memory rewriting tasks
- Transformer-based models (GTrXL) struggle with sparse rewards and memory rewriting despite strong retention capabilities
- Structured memory systems (SHM, FFM) show limited flexibility in adapting to changing memory requirements
- Ablation studies confirm that learnable forgetting gates are crucial for effective memory rewriting performance

## Why This Works (Mechanism)
The paper's core mechanism insight is that memory rewriting requires selective overwriting rather than just retention. When environmental conditions change, agents must actively discard outdated information and update their memory representations. Recurrent architectures with gated mechanisms (LSTM, GRU) excel because they have dedicated forget gates that learn when to overwrite memory cells. In contrast, transformer architectures lack explicit forgetting mechanisms, leading to memory saturation and inability to adapt to changing conditions. The learnable nature of these gates allows the agent to determine what information is no longer relevant, making them crucial for dynamic environments where memory content must evolve over time.

## Foundational Learning
- **Memory Retention vs. Rewriting**: Understanding the difference between simply storing information versus actively updating it when conditions change - needed because many RL agents can retain information but fail when memory must be overwritten
- **Gated Recurrent Units**: LSTM and GRU cells use gating mechanisms to control information flow - needed to understand why these architectures perform better at memory rewriting
- **Sparse Reward Challenges**: How reward sparsity affects learning in memory-intensive tasks - needed because transformer models particularly struggle under sparse reward conditions
- **Structured Memory Systems**: How SHM and FFM organize memory differently from recurrent networks - needed to compare different memory architecture philosophies
- **Ablation Studies**: Method for systematically removing components to test their importance - needed to isolate the contribution of forgetting mechanisms

## Architecture Onboarding

**Component Map:**
Input -> Memory Module (LSTM/GRU/Transformer/SHM/FFM) -> Policy Network -> Action Output

**Critical Path:**
Observation → Memory Update (including forgetting) → Policy Decision → Action → Reward

**Design Tradeoffs:**
- Recurrent with forgetting gates: Better memory rewriting but potentially higher computational cost
- Transformers: Strong retention but struggle with sparse rewards and lack explicit forgetting
- Structured memory: Good organization but limited flexibility in dynamic environments

**Failure Signatures:**
- Memory saturation (inability to update old information)
- Over-reliance on outdated memory content
- Poor adaptation to changing environmental conditions
- Suboptimal performance on tasks requiring frequent memory updates

**First 3 Experiments:**
1. Compare LSTM vs GRU performance on Endless T-Maze with varying maze change frequencies
2. Test transformer performance with reward shaping to reduce sparsity
3. Evaluate hybrid architectures combining structured memory with learnable forgetting mechanisms

## Open Questions the Paper Calls Out
None

## Limitations
- Results are primarily based on simple 2D maze and grid-world tasks, limiting generalizability to complex real-world environments
- Evaluation focuses on discrete action spaces and relatively short time horizons, leaving uncertainty about performance in continuous control tasks
- The comparative performance rankings between architectures may vary with different reward structures and environmental dynamics not explored in this study

## Confidence
- **High**: Learnable forgetting gates are crucial for memory rewriting (consistently demonstrated across ablation studies)
- **Medium**: Comparative performance rankings between architectures (task-dependent and may vary with different conditions)
- **Low**: Generalizing transformer limitations to all transformer architectures (implementation-specific factors may influence results)

## Next Checks
1. Test the memory architectures on continuous control tasks (e.g., MuJoCo or DeepMind Control Suite environments) to assess scalability to higher-dimensional action and observation spaces

2. Evaluate performance with varying reward sparsity levels and different reward shaping strategies to determine if the observed limitations of transformer models are fundamental or implementation-specific

3. Implement and test hybrid architectures that combine structured memory systems with learnable forgetting mechanisms to investigate whether the flexibility limitations of SHM/FFM can be overcome while maintaining their organizational benefits