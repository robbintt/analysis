---
ver: rpa2
title: Annealed Relaxation of Speculative Decoding for Faster Autoregressive Image
  Generation
arxiv_id: '2601.09212'
source_url: https://arxiv.org/abs/2601.09212
tags:
- cool-sd
- lantern
- image
- generation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of autoregressive (AR) image
  generation due to sequential token prediction. The authors propose COOL-SD, an annealed
  relaxation of speculative decoding, which improves speed-quality trade-offs by using
  a principled relaxation of the acceptance criterion and deriving optimal resampling
  distributions.
---

# Annealed Relaxation of Speculative Decoding for Faster Autoregressive Image Generation

## Quick Facts
- arXiv ID: 2601.09212
- Source URL: https://arxiv.org/abs/2601.09212
- Reference count: 40
- This paper proposes COOL-SD, an annealed relaxation of speculative decoding that improves speed-quality trade-offs in autoregressive image generation by using optimal resampling distributions and an exponential decay acceptance schedule.

## Executive Summary
This paper addresses the inefficiency of autoregressive (AR) image generation due to sequential token prediction. The authors propose COOL-SD, an annealed relaxation of speculative decoding, which improves speed-quality trade-offs by using a principled relaxation of the acceptance criterion and deriving optimal resampling distributions. Experiments show COOL-SD consistently outperforms existing methods, achieving better image quality or faster inference across multiple AR models and datasets.

## Method Summary
COOL-SD introduces a relaxed acceptance criterion where tokens are accepted based on a scaled ratio involving a relaxation factor δ, rather than requiring exact matches. The method derives an optimal resampling distribution G*ᵢ that minimizes an upper bound on Total Variation distance between the generated output and the target distribution. Additionally, COOL-SD employs an exponential decay schedule for the relaxation parameter across the draft sequence, allowing higher acceptance rates at the beginning and stricter rates at the end. This annealing property is shown to reduce theoretical error bounds while maintaining image quality.

## Key Results
- COOL-SD consistently outperforms existing methods like LANTERN++ in both FID and latency trade-offs
- The method achieves better image quality or faster inference across multiple AR models (LlamaGen-XL, Lumina-mGPT) and datasets
- Experimental results show COOL-SD can achieve up to 3.1× speedup while maintaining comparable FID scores to baseline methods

## Why This Works (Mechanism)

### Mechanism 1: Optimal Resampling via Total Variation Bounds
Replacing standard rejection sampling with a derived optimal distribution minimizes the distributional drift caused by relaxation. The authors derive an optimal resampling distribution $G^*_i = \text{Norm}([P(\cdot|x) - Q(\cdot|x)\cdot f_{i+1}]^+)$ which minimizes an upper bound on the Total Variation (TV) distance between the generated output and the target distribution. The TV distance is a sufficient proxy for perceptual image quality degradation.

### Mechanism 2: Annealed Acceptance Scheduling
Decaying the relaxation parameter monotonically across the draft sequence yields better fidelity than uniform relaxation for the same inference speed. Using perturbation analysis, the paper shows that distributing the "relaxation budget" unevenly - specifically, allowing higher acceptance rates at the beginning of the draft sequence and stricter rates at the end - reduces the theoretical error bound. This is implemented via an exponential decay schedule: $\omega_i = \delta \cdot \exp(-\nu \cdot i)$.

### Mechanism 3: Relaxation for Token Ambiguity
Increasing acceptance probabilities via a relaxation factor δ improves speed by mitigating the "token selection ambiguity" inherent in image latents. In image generation, multiple tokens often have near-identical probabilities. Vanilla SD rejects draft tokens if they aren't the exact top choice of the target model. COOL-SD relaxes this by accepting tokens based on a scaled ratio $\min(1, \omega_i \cdot \frac{P(x)}{Q(x)})$, allowing "good enough" tokens to pass, thereby reducing the number of costly target model forward passes.

## Foundational Learning

- **Concept: Speculative Decoding (Draft-Verify)**
  - Why needed here: This is the base protocol being modified. You must understand how a smaller "draft" model proposes tokens that a "target" model verifies in parallel to understand why relaxation is needed.
  - Quick check question: If the target model rejects a token in standard SD, what happens to the subsequent draft tokens in that sequence? (Answer: They are discarded).

- **Concept: Total Variation (TV) Distance**
  - Why needed here: The paper grounds its "optimal resampling" logic on minimizing this metric. Understanding TV distance as a measure of distributional "drift" or error is required to interpret the theoretical justification.
  - Quick check question: Does a lower TV distance imply the generated image distribution is closer to the target model's theoretical distribution?

- **Concept: Autoregressive Image Tokenization (VQ-VAE)**
  - Why needed here: The paper references "ambiguity of image tokens." You need to know that images are flattened into discrete tokens where neighboring tokens often have similar semantic meaning, unlike discrete text words.
  - Quick check question: Why might the strict "exact match" verification of text SD fail when applied to image tokens?

## Architecture Onboarding

- **Component map:** Draft Model (Q) -> Relaxed Acceptor (fᵢ) -> Target Model (P) -> Optimal Resampler (G*ᵢ)
- **Critical path:** The verification step (Algorithm 1, Lines 5-13). The efficiency gain depends entirely on the logic inside the loop: computing fᵢ (Line 8) and potentially sampling from G*ᵢ (Line 12).
- **Design tradeoffs:**
  - **Hyperparameter δ (Relaxation Budget):** Tuning δ moves the system along the speed-quality Pareto frontier. Low δ (≈ 1.1) = high quality/moderate speed; High δ (>2.0) = lower quality/high speed.
  - **Hyperparameter ν (Decay Rate):** Controls how quickly the acceptance criteria tighten over the sequence length.
- **Failure signatures:**
  - **Quality Collapse (High FID):** δ is set too high, accepting low-probability tokens that diverge from the prompt.
  - **No Speedup:** The draft model Q is poorly trained, causing the resampler to trigger constantly, negating parallel verification benefits.
  - **Artifacts:** Visual degradation (e.g., patchy backgrounds) appearing earlier than in baseline methods when relaxation is too aggressive.
- **First 3 experiments:**
  1. **Validate Acceptance Rate:** Run COOL-SD vs. Vanilla SD on a fixed dataset (MS-COCO) and plot "Average Accepted Length" vs. "Relaxation Budget δ" to verify the monotonic increase in speed.
  2. **Verify Fidelity Bound:** Measure FID and CLIP scores for COOL-SD against LANTERN++ to confirm that the theoretical TV bound minimization translates to better perceptual quality at equivalent latency.
  3. **Ablate Annealing:** Compare "COOL-SD (Exp Decay)" vs. "UniformRSD" to isolate the impact of the annealing schedule from the mere presence of relaxation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the theoretical annealing property hold rigorously for draft lengths $L > 2$?
- Basis: Proposition 2 is proved for $L=2$, with the text stating a "similar conclusion can be drawn" for $L > 2$ via pairwise comparison.
- Why unresolved: The perturbation analysis provided is restricted to the two-step case; a formal proof for arbitrary sequence lengths is absent.
- What evidence would resolve it: A general proof for $L > 2$ or empirical evidence showing the decaying schedule remains optimal for long draft chains.

### Open Question 2
- Question: How does COOL-SD degrade if the draft model violates the target-draft proximity assumption?
- Basis: Assumption 2 in Appendix F requires the TV distance between target P and draft Q to be ≤ 2/5.
- Why unresolved: The theoretical bounds depend on this closeness; it is unclear how the speed-quality trade-off shifts if a much weaker draft model is used.
- What evidence would resolve it: Empirical analysis of COOL-SD performance as the draft model size decreases or TV distance increases beyond the 0.4 threshold.

### Open Question 3
- Question: Is the relaxed acceptance strategy applicable to text generation in Large Language Models?
- Basis: The introduction frames the problem specifically around the "ambiguity of image tokens," distinguishing it from prior SD work on LLMs.
- Why unresolved: It is unclear if the relaxed acceptance criteria introduces perceptible semantic errors in text, which typically has lower token ambiguity than images.
- What evidence would resolve it: Application of COOL-SD to standard text generation benchmarks (e.g., MT-Bench) to measure quality retention vs. speed-up.

## Limitations

- The theoretical framework relies on Total Variation distance as a proxy for perceptual quality, but this correlation is not empirically validated
- The effectiveness critically depends on the TV distance between draft and target models being below 0.4, which is treated as an assumption rather than derived constraint
- The exponential decay schedule may not generalize across different sequence lengths or model architectures

## Confidence

- **High Confidence (8/10):** The core mechanism of optimal resampling via TV distance minimization is mathematically sound
- **Medium Confidence (6/10):** The annealing schedule's empirical benefits are demonstrated on specific benchmarks but lack theoretical guarantees for general AR image generation
- **Low Confidence (4/10):** The claim that "token selection ambiguity" justifies relaxation in image generation is intuitive but under-supported

## Next Checks

1. **Ablation on TV Distance Threshold:** Systematically vary the TV distance threshold (0.2 to 0.6) between draft and target models and measure the resulting FID/quality degradation to validate whether the 0.4 assumption is critical or conservative.

2. **Cross-Architecture Annealing Testing:** Apply COOL-SD with the same ν=0.7 schedule to different AR architectures (e.g., Imagen, Stable Diffusion variants) and varying sequence lengths to test schedule generalization.

3. **Direct Ambiguity Measurement:** Quantify "token selection ambiguity" by measuring the entropy or top-5 probability mass in the target model's output distribution and compare SD rejection rates to this metric to establish whether ambiguity is the actual driver of SD inefficiency in images.