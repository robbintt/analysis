---
ver: rpa2
title: Taxonomy-based Negative Sampling In Personalized Semantic Search for E-commerce
arxiv_id: '2511.00694'
source_url: https://arxiv.org/abs/2511.00694
tags:
- items
- search
- query
- sampling
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving semantic retrieval
  in e-commerce search by developing a model that can understand subtle differences
  between similar products while incorporating user personalization. The authors propose
  a taxonomy-based hard-negative sampling (TB-HNS) strategy that leverages hierarchical
  product categories to generate semantically challenging negatives during training,
  improving the model's ability to distinguish closely related items.
---

# Taxonomy-based Negative Sampling In Personalized Semantic Search for E-commerce

## Quick Facts
- arXiv ID: 2511.00694
- Source URL: https://arxiv.org/abs/2511.00694
- Reference count: 35
- One-line primary result: Taxonomy-based hard-negative sampling improves semantic retrieval in e-commerce search with 77.89% Recall@24 and 2.70% conversion rate uplift

## Executive Summary
This paper addresses the challenge of improving semantic retrieval in e-commerce search by developing a model that can understand subtle differences between similar products while incorporating user personalization. The authors propose a taxonomy-based hard-negative sampling (TB-HNS) strategy that leverages hierarchical product categories to generate semantically challenging negatives during training, improving the model's ability to distinguish closely related items. They integrate personalization by modeling each customer's past purchase history and behavior, creating an enhanced semantic engine that falls back to non-personalized mode when needed.

The approach significantly outperforms traditional BM25 and neural baselines like ANCE on Recall@K metrics, achieving 77.89% Recall@24. Live A/B testing demonstrated substantial business impact with 2.70% increase in conversion rate, 2.04% increase in add-to-cart rate, and 0.6% increase in average order value. The taxonomy-based negatives also reduce training overhead and accelerate convergence, while the system effectively handles cold-start items and improves retrieval for both frequent and infrequent shoppers.

## Method Summary
The proposed method combines taxonomy-based hard-negative sampling with personalized semantic search for e-commerce applications. The core innovation is the taxonomy-based hard-negative sampling (TB-HNS) strategy, which generates challenging negative samples by selecting products from the same or similar hierarchical categories as positive examples. This approach creates semantically meaningful distinctions during training, helping the model learn subtle differences between closely related products.

The personalization component integrates customer purchase history and behavior patterns into the semantic search pipeline. When user data is available, the system personalizes results based on individual preferences; when insufficient data exists, it gracefully falls back to non-personalized retrieval. The combined model uses a contrastive learning framework with the TB-HNS strategy, achieving better performance than using either personalization or hard negatives alone. The training process is optimized to reduce overhead while maintaining or improving model quality.

## Key Results
- Achieved 77.89% Recall@24 on the combined personalized semantic search model
- Live A/B testing showed 2.70% conversion rate uplift and 2.04% add-to-cart rate improvement
- Outperformed BM25, ANCE, and other neural baselines on standard retrieval metrics
- Reduced training overhead and accelerated convergence compared to traditional hard-negative sampling methods

## Why This Works (Mechanism)
The taxonomy-based hard-negative sampling works by leveraging the hierarchical structure of product categories to create semantically challenging negative examples during training. Instead of randomly selecting negatives, the method intelligently chooses products from similar categories as positive examples, forcing the model to learn fine-grained distinctions between closely related items. This approach addresses the fundamental challenge in e-commerce search where products often share similar attributes but differ in crucial ways that matter to customers.

The personalization component enhances search relevance by incorporating individual user behavior patterns and purchase history. By modeling each customer's unique preferences and past interactions, the system can surface products that align with their specific tastes and needs. The fallback mechanism ensures robustness by maintaining reasonable performance even when personalization data is limited, making the system practical for real-world deployment where user data availability varies significantly.

## Foundational Learning
- **Taxonomy-based hard-negative sampling**: Why needed - Creates semantically meaningful training examples that help models distinguish between similar products; Quick check - Verify that negatives are drawn from same or similar hierarchical categories as positives
- **Contrastive learning framework**: Why needed - Enables learning fine-grained semantic representations through positive-negative pair comparisons; Quick check - Ensure margin-based loss properly separates similar products
- **User behavior modeling**: Why needed - Captures individual preferences to personalize search results effectively; Quick check - Validate that user embeddings reflect actual purchase patterns
- **Fall-back mechanisms**: Why needed - Maintains system performance when personalization data is insufficient; Quick check - Test system behavior with limited user history
- **Hierarchical category structures**: Why needed - Provides semantic organization for intelligent negative sampling; Quick check - Confirm category hierarchy accurately reflects product relationships
- **Retrieval metrics (Recall@K)**: Why needed - Measures the ability to surface relevant products in top results; Quick check - Calculate recall across different K values to assess ranking quality

## Architecture Onboarding

Component map: User query -> Query encoder -> Product encoder -> Similarity scoring -> Ranking -> Results

Critical path: User query → Query encoder → Product encoder → Similarity scoring → Ranking → Results display

Design tradeoffs: The taxonomy-based hard-negative sampling reduces training complexity compared to random sampling while improving model discrimination. The personalization integration adds computational overhead but significantly improves relevance for users with sufficient history. The fallback mechanism trades some personalization gains for robustness when data is limited.

Failure signatures: Poor performance on cold-start items indicates insufficient negative sampling diversity. Reduced personalization effectiveness suggests user behavior modeling issues. Low recall scores may indicate inadequate semantic understanding of product relationships.

Three first experiments:
1. Baseline comparison: Run BM25, ANCE, and the proposed model on a held-out test set to measure Recall@K improvements
2. Ablation study: Test models with and without TB-HNS and personalization to quantify individual contributions
3. Cold-start evaluation: Measure performance for new users and items to validate fallback mechanism effectiveness

## Open Questions the Paper Calls Out
The paper acknowledges several important open questions regarding the scalability of the taxonomy-based hard-negative sampling approach to different e-commerce domains and the long-term stability of personalization signals. The authors note that while the current implementation shows strong results, further research is needed to understand how the approach generalizes to different product categories and market segments. Additionally, they highlight the need for ongoing evaluation of personalization effectiveness as user preferences evolve over time.

## Limitations
- Limited external validation due to use of proprietary datasets rather than public benchmarks
- Single A/B testing period without statistical significance testing or seasonality analysis
- Insufficient quantitative analysis of cold-start user and item performance
- Potential domain-specific limitations in taxonomy-based negative sampling generalization

## Confidence

High confidence in the taxonomy-based hard-negative sampling methodology and its technical implementation
Medium confidence in the reported performance improvements, given the lack of public benchmark comparisons
Medium confidence in personalization integration effectiveness, though long-term user modeling effects are unclear
Low confidence in the claimed cold-start solution robustness without more detailed validation

## Next Checks

1. Conduct A/B testing across multiple time periods and seasonal cycles to verify the consistency of the reported conversion and add-to-cart rate improvements
2. Implement the taxonomy-based hard-negative sampling approach on public e-commerce datasets (e.g., Amazon product data) to validate cross-domain generalization
3. Perform detailed analysis of cold-start user and item performance, measuring the system's effectiveness for users with zero purchase history and products with no interaction data over extended periods