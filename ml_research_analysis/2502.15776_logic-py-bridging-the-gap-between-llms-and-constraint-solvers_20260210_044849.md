---
ver: rpa2
title: 'Logic.py: Bridging the Gap between LLMs and Constraint Solvers'
arxiv_id: '2502.15776'
source_url: https://arxiv.org/abs/2502.15776
tags:
- logic
- language
- solution
- constraint
- solver
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Logic.py, a domain-specific language for\
  \ formalising search-based problems, combined with an agentic solver engine that\
  \ leverages LLMs and constraint solvers. The approach significantly improves performance\
  \ on the ZebraLogicBench logic puzzle benchmark, achieving 91.4% accuracy compared\
  \ to 24.9% for the baseline Llama 3.1 70B model\u2014a 65% absolute improvement."
---

# Logic.py: Bridging the Gap between LLMs and Constraint Solvers

## Quick Facts
- arXiv ID: 2502.15776
- Source URL: https://arxiv.org/abs/2502.15776
- Reference count: 4
- Primary result: 91.4% accuracy on ZebraLogicBench logic puzzles vs 24.9% baseline (65% absolute improvement)

## Executive Summary
Logic.py introduces a novel approach to logic puzzle solving by transforming the LLM's role from direct problem-solving to formal problem specification. The method prompts LLMs to formalize puzzles in a purpose-built DSL, then solves them using constraint solvers. This neuro-symbolic approach achieves state-of-the-art performance on the ZebraLogicBench benchmark, demonstrating that task transformation can overcome LLMs' limitations in complex logical reasoning.

## Method Summary
The method uses zero-shot prompting to guide Llama 3.1 70B Instruct to generate Logic.py code defining both a data structure (using Unique and Domain decorators) and a validation function with constraints derived from puzzle clues. The Logic.py code is transformed to C via libCST, then compiled and analyzed by CBMC bounded model checker. CBMC performs constraint solving to find satisfying assignments, which are converted back to puzzle answers. The pipeline handles failures by restarting without providing error feedback to the model.

## Key Results
- Achieves 91.4% accuracy on ZebraLogicBench (1000 logic grid puzzles)
- Outperforms baseline Llama 3.1 70B (24.9% accuracy) by 65% absolute improvement
- Demonstrates effectiveness of task transformation from search to formalization
- Shows zero-shot prompting can produce valid Logic.py code for complex constraints

## Why This Works (Mechanism)

### Mechanism 1: Task Transformation from Search to Formalization
Shifting the LLM's role from directly solving logic puzzles to formalizing them in a structured DSL improves accuracy by offloading combinatorial search to a symbolic solver. The LLM defines solution space representation and validation functions, while the constraint solver performs exhaustive search over nondeterministic assignments. This works because LLMs are better at recognizing and translating logical constraints than maintaining consistent state across multi-step deductive reasoning.

### Mechanism 2: DSL Abstraction Reduces Formalization Error Surface
Purpose-built type decorators (Unique, Domain, list) reduce syntax errors and boilerplate compared to direct SAT/SMT encoding. Decorators compile to CPROVER IR initialization helpers that handle common patterns like uniqueness constraints automatically. The cognitive load of learning a minimal DSL with 3-5 constructs is lower than the error rate of generating raw SAT/SMT or C code.

### Mechanism 3: Nondeterministic Variable Pattern Enables Declarative Search
Treating uninitialized variables as free/nondeterministic allows the LLM to describe solution properties declaratively rather than constructing solutions procedurally. The harness initializes solution structures nondeterministically, then applies assumptions derived from clues. CBMC finds satisfying assignments via SAT/SMT solving, which is more reliable than LLM chain-of-thought for combinatorial search.

## Foundational Learning

- **Constraint Satisfaction Problems (CSP) and SAT/SMT Solving**: Core to understanding how constraint solvers find satisfying assignments for boolean/propositional formulas. Quick check: Can you explain why asserting `false` at the end of a CBMC harness causes the solver to produce a witness for all prior assumptions?

- **Python CST (Concrete Syntax Tree) Transformation**: Required to understand Logic.py's compilation to C via libCST. Quick check: How would you write a libCST transformer that converts `Unique[Domain[int, range(1,7)]]` into the CPROVER initialization macro pattern?

- **Declarative vs. Procedural Problem Specification**: The core insight that LLMs should specify what a valid solution looks like, not how to find it. Quick check: In the validation function pattern, why does the code use `assume` to bind a variable and then `assert` its properties, rather than directly assigning values?

## Architecture Onboarding

- **Component map**: LLM Inference -> libCST Transformer -> CBMC (CPROVER) -> Result Formatter
- **Critical path**: Puzzle text → LLM → `class PuzzleSolution` with type decorators → Puzzle clues → LLM → `def validate(solution)` with assume/assert chains → Logic.py → libCST → C harness with `__CPROVER_nondet_*` and `__CPROVER_assume` → C harness → CBMC → SAT solver → satisfying assignment or UNSAT → C struct → JSON answer
- **Design tradeoffs**: Python base vs. C base (chose Python for implicit typing), Zero-shot prompts vs. few-shot (chose zero-shot for generality), Restart-on-failure vs. error-feedback (currently restarts without informing LLM of errors)
- **Failure signatures**: Syntax error in Logic.py (caught by libCST parsing; triggers full restart), UNSAT result (model produced contradictory constraints; triggers restart), Multiple solutions detected (model under-constrained problem; prototype does not handle this case)
- **First 3 experiments**: 1) Run polymath repository on 10 ZebraLogicBench puzzles with logging enabled at each pipeline stage, 2) Ablate type decorators by replacing `Unique[Domain[...]]` with manual uniqueness assertions and measure error rate increase, 3) Implement error-feedback recovery: on syntax error or UNSAT, feed error message back to LLM for single retry and compare success rate vs. restart-only baseline

## Open Questions the Paper Calls Out
- Can providing feedback on syntax errors or unsatisfiable constraints to the LLM improve the success rate compared to the current "restart" strategy? (Section 4.1 and 7)
- Does the Logic.py approach transfer effectively to First-Order Logic (FOL) problems as evaluated on the FOLIO benchmark? (Section 7)
- Can this neuro-symbolic architecture generalize to domains like numerical reasoning or optimization problems? (Section 6 and 7)

## Limitations
- Private dataset (ZebraLogicBench) requires access request, creating reproduction barriers
- Current pipeline restarts on all failures without providing error feedback to the model
- Does not handle cases where multiple solutions exist (under-constrained problems)
- Limited evaluation to single LLM and single puzzle type (logic grid puzzles)

## Confidence
- **High**: Core mechanism of task transformation is well-supported by 65% absolute accuracy improvement
- **Medium**: DSL abstraction benefits are plausible but lack ablation studies comparing to raw constraint encoding
- **Low**: Scalability claims based on single benchmark and single LLM; generalization to other puzzle types unknown

## Next Checks
1. Implement ablation study comparing Logic.py decorators to explicit SAT/SMT constraints on 100-puzzle subset to quantify DSL abstraction benefits
2. Add error feedback recovery to pipeline and compare success rate and attempts between restart-only and error-feedback approaches on 100-puzzle validation set
3. Extend harness to detect and report multiple solutions, analyzing correlation between ambiguity and accuracy or puzzle characteristics