---
ver: rpa2
title: 'Hotter and Colder: A New Approach to Annotating Sentiment, Emotions, and Bias
  in Icelandic Blog Comments'
arxiv_id: '2502.16987'
source_url: https://arxiv.org/abs/2502.16987
tags:
- comments
- tasks
- task
- annotators
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hotter and Colder, a dataset of 12,232 Icelandic
  blog comments annotated for 25 tasks including sentiment, emotions, hate speech,
  and group generalizations. The authors developed a two-phase annotation methodology
  that combines GPT-4o mini silver labels with targeted human verification to address
  class imbalance and improve annotation agreement.
---

# Hotter and Colder: A New Approach to Annotating Sentiment, Emotions, and Bias in Icelandic Blog Comments

## Quick Facts
- arXiv ID: 2502.16987
- Source URL: https://arxiv.org/abs/2502.16987
- Reference count: 8
- 12,232 Icelandic blog comments annotated for 25 tasks including sentiment, emotions, hate speech, and group generalizations

## Executive Summary
This paper introduces Hotter and Colder, a novel dataset of Icelandic blog comments annotated for multiple dimensions including sentiment, emotions, hate speech, and various forms of bias. The authors developed a two-phase annotation methodology that combines GPT-4o mini for initial silver labeling with targeted human verification to address class imbalance challenges. The dataset aims to support research in content moderation and sentiment analysis for low-resource languages, specifically Icelandic, by providing rich annotations across 25 different tasks ranging from basic sentiment to complex social phenomena like mansplaining and microaggressions.

## Method Summary
The annotation methodology employed a two-phase approach to efficiently create high-quality labels across 25 diverse tasks. First, GPT-4o mini was used to label approximately 800,000 comments on a 5-point Likert scale, providing silver labels that could be scaled efficiently. Then, crowdworkers manually verified these labels, with particular focus on extreme cases (highest and lowest ratings) to maximize annotation quality where it matters most. This approach was specifically designed to address class imbalance issues common in sentiment and bias annotation tasks, where certain categories (like hate speech or extreme negative sentiment) are naturally rare but critically important for content moderation applications.

## Key Results
- High agreement for basic sentiment tasks: disgust (Krippendorff's alpha 0.92) and politeness (Cohen's kappa 0.80)
- Moderate agreement for most other tasks (0.60-0.79 Krippendorff's alpha range)
- Lower agreement for complex social phenomena like mansplaining and microaggressions
- Successfully created a dataset with 12,232 annotated comments across 25 tasks
- Released both the annotated dataset and annotation platform for community use

## Why This Works (Mechanism)
The two-phase annotation approach works by leveraging the efficiency of large language models for initial labeling while focusing human effort where it adds the most value. GPT-4o mini can process large volumes of text quickly and consistently, establishing a baseline for all 800,000 comments. Human annotators then verify these labels, concentrating their attention on edge cases where model confidence is lowest or where nuanced human judgment is most critical. This selective human verification strategy addresses the economic constraints of manual annotation while maintaining quality for the most challenging and important cases.

## Foundational Learning
- **Krippendorff's alpha**: A statistical measure of inter-rater reliability for categorical data that accounts for chance agreement, essential for evaluating annotation quality in multi-task datasets
- **Cohen's kappa**: Another inter-rater reliability measure specifically designed for two annotators, useful for pairwise agreement assessment
- **Silver labeling**: The practice of using automated systems (typically LLMs) to generate initial labels that are later verified or corrected by humans, enabling scalable annotation
- **Class imbalance**: The phenomenon where certain categories or labels occur much less frequently than others, requiring specialized annotation strategies
- **Likert scale annotation**: A psychometric response scale that allows respondents to specify their level of agreement or intensity on a symmetric agree-disagree scale
- **Low-resource language processing**: Research and development for languages with limited available training data, requiring specialized approaches and datasets

## Architecture Onboarding
**Component map**: GPT-4o mini (silver labeling) -> Crowdworker verification (human validation) -> Final annotated dataset
**Critical path**: Initial LLM labeling provides scalable coverage across all comments, human verification ensures quality control, particularly for rare but important categories
**Design tradeoffs**: Efficiency vs. accuracy - using LLMs enables rapid initial labeling but requires human verification to catch errors and handle nuanced cases; cost vs. coverage - focusing human effort on extreme cases balances budget constraints with quality needs
**Failure signatures**: Systematic biases from LLM labeling patterns, disagreement on complex social phenomena requiring domain expertise, persistent class imbalance despite verification efforts
**First experiments**: 1) Compare GPT-4o mini agreement with human annotators on simple vs. complex tasks; 2) Measure class distribution changes from initial LLM labeling to final human-verified dataset; 3) Test whether verification of extreme cases alone is sufficient for maintaining overall dataset quality

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on GPT-4o mini for initial silver labeling may introduce systematic biases that persist despite human verification
- Class imbalance problem is only partially addressed, with effectiveness unclear for tasks with severe imbalance
- Crowdworker verification may lack domain expertise needed for nuanced tasks like identifying mansplaining or microaggressions
- The two-phase methodology's effectiveness for very rare categories remains unproven

## Confidence
- **High**: Basic sentiment and emotion tasks (disgust, politeness) with agreement scores above 0.80
- **Medium**: Most other tasks showing moderate agreement (0.60-0.79 Krippendorff's alpha)
- **Low**: Complex social phenomena like mansplaining and microaggressions where agreement was notably lower

## Next Checks
1. Conduct inter-annotator reliability study with domain experts specifically for complex social bias tasks to establish ground truth agreement levels
2. Perform bias analysis comparing GPT-4o mini's initial labels against human-verified labels to quantify systematic annotation biases
3. Evaluate model performance trained on this dataset across different Icelandic text domains (news comments, social media, forums) to assess domain transfer limitations