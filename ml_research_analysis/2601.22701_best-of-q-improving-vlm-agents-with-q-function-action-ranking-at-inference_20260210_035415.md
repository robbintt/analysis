---
ver: rpa2
title: 'Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference'
arxiv_id: '2601.22701'
source_url: https://arxiv.org/abs/2601.22701
tags:
- action
- q-function
- agent
- data
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Best-of-Q, a method to improve VLM-based web
  agents without retraining the policy. The approach uses a frozen VLM to generate
  multiple candidate actions, then applies a lightweight, offline-trained Q-function
  to rerank and select the best action at inference time.
---

# Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference

## Quick Facts
- **arXiv ID:** 2601.22701
- **Source URL:** https://arxiv.org/abs/2601.22701
- **Reference count:** 40
- **Primary result:** Qwen2.5-VL-7B improves from 38.8% to 55.7% success rate on WebVoyager using Best-of-Q inference-time action reranking

## Executive Summary
This paper proposes Best-of-Q, a method to improve VLM-based web agents without retraining the policy. The approach uses a frozen VLM to generate multiple candidate actions, then applies a lightweight, offline-trained Q-function to rerank and select the best action at inference time. Experiments on WebVoyager show significant performance gains: a Qwen2.5-VL-7B agent improves from 38.8% to 55.7% success rate, and a GPT-4.1 agent improves from 82.4% to 88.8%. The method is model-agnostic and achieves better cost-performance trade-offs than baselines. Analysis reveals the primary bottleneck is the VLM's action-proposal ability, not the Q-function's selection accuracy.

## Method Summary
Best-of-Q improves VLM agents by decoupling action generation from action selection. A frozen VLM generates N candidate actions, then a lightweight Q-function trained via Implicit Q-Learning (IQL) scores each candidate and selects the highest-value action. The method uses offline data collected via ε-greedy exploration (ε=0.5) to train the Q-function, avoiding costly fine-tuning of the VLM. The Q-function architecture is a simple MLP that takes concatenated embeddings of state, action, and task (extracted from a frozen VLM embedder) and outputs a scalar Q-value. Training uses expectile regression for value learning and MSE for Q updates, with an expectile parameter tuned based on data quality.

## Key Results
- Qwen2.5-VL-7B improves from 38.8% to 55.7% success rate on WebVoyager (+16.9 points)
- GPT-4.1 improves from 82.4% to 88.8% success rate (+6.4 points)
- Best-of-Q outperforms VLM-as-action-selector baselines that use prompting strategies
- Performance peaks when inference candidate count N matches training N (N=3 optimal)
- Random Action baseline drops GPT-4.1 from 82.4% to 72.1%, proving selection mechanism matters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling the VLM's role as action proposer from value-based selection enables policy improvement without costly fine-tuning.
- **Mechanism:** A frozen VLM generates N diverse candidate actions; a lightweight Q-function scores each and the agent executes the highest-value action. This separates perception/generation (high-capacity VLM) from decision-making (specialized value network).
- **Core assumption:** The base VLM can propose viable actions but makes suboptimal greedy selections.
- **Evidence anchors:**
  - [abstract] "our approach decouples the VLM's role as a high-capacity action proposer from the final action selection mechanism"
  - [Section 4.3, Table 2] Random Action baseline drops GPT-4.1 from 82.4% to 72.1%, proving selection mechanism—not just more candidates—drives improvement
  - [corpus] Digi-Q (arXiv:2502.15760) uses Q-functions for relabeling data for retraining; this method differs by applying Q-function directly at inference
- **Break condition:** When the VLM fails to propose any viable action, no Q-function can recover (observed in 50.3% of failures per Section 4.5 analysis).

### Mechanism 2
- **Claim:** Implicit Q-Learning (IQL) enables stable Q-function training from mixed-quality offline data with sparse rewards.
- **Mechanism:** IQL uses expectile regression to learn V(s) without querying out-of-distribution actions, avoiding distributional shift. The learned V(s) serves as a stable target for Q-updates, enabling multi-step dynamic programming that "stitches" together partial solutions.
- **Core assumption:** The offline dataset contains sufficient state-action coverage with both successful and failed trajectories.
- **Evidence anchors:**
  - [Section 3.2] "IQL learns a value function without querying out-of-distribution (OOD) actions, thus avoiding distributional shift"
  - [Section 4.7, Figure 4] Performance exceeds baseline after just 1 training run, demonstrating sample efficiency
  - [corpus] Related work Digi-Q trains Q-functions but uses them for policy retraining; corpus confirms IQL's stability advantages in offline settings
- **Break condition:** High expectile τ on low-quality data causes overconfident Q-values; paper lowered τ from 0.8 to 0.7 for datasets with many failures (Section 4.2).

### Mechanism 3
- **Claim:** Best-of-N reranking at inference provides immediate policy improvement proportional to candidate quality and Q-function accuracy.
- **Mechanism:** Generate N actions, score each with Q-function, select argmax. When inference N matches training N, the Q-function has learned the action distribution and can reliably rank.
- **Core assumption:** At least one candidate action is near-optimal for most states.
- **Evidence anchors:**
  - [Section 4.3] Qwen2.5-VL-7B improves from 38.8% to 55.7% (+16.9 points)
  - [Section 5, Table 4] Performance degrades when inference N (5 or 8) differs from training N (3), due to distributional shift and low-quality placeholder actions
  - [corpus] VLM-as-action-selector baselines (Section 5, Table 3) improve over prompting but underperform trained Q-function, showing value of learned vs. zero-shot selection
- **Break condition:** Performance plateaus at ~55.7% for 7B model—analysis shows Q-function correctly selects in 13.6% of cases when valid action exists but cannot overcome VLM's proposal failures (Section 4.5, Figure 3).

## Foundational Learning

- **Q-functions and Temporal Difference Learning**
  - Why needed here: Core mechanism—understanding Q(s,a) as expected cumulative reward, and why stable training matters.
  - Quick check question: Why does IQL avoid querying out-of-distribution actions while standard TD-learning does not?

- **Offline Reinforcement Learning and Distributional Shift**
  - Why needed here: Method operates on static datasets; understanding why OOD actions cause extrapolation errors is essential.
  - Quick check question: What happens if the Q-function is asked to score an action type it never saw during training?

- **VLM-based Agents for GUI/Web Navigation**
  - Why needed here: Context for why fine-tuning is expensive and why inference-time improvement is valuable.
  - Quick check question: Why do VLMs struggle with precise action grounding despite strong semantic understanding?

## Architecture Onboarding

- **Component map:**
  ```
  [State: screenshot + task + history]
           ↓
  [Frozen VLM Policy] → N candidate actions
           ↓
  [Frozen VLM Embedder (Qwen2.5-VL-3B)] → state, action, task embeddings (2048-dim each)
           ↓
  [Trained MLP Q-function (~11M params)] → Q-value per candidate
           ↓
  [argmax selection] → Execute action
  ```

- **Critical path:**
  1. Data collection: ε-greedy (ε=0.5) policy collects diverse trajectories
  2. Iterative improvement: Train Q-function → collect exploitative runs → retrain (4-5 cycles)
  3. Inference: N=3 candidates, select by max Q-value

- **Design tradeoffs:**
  - **N=3 vs. larger N:** Higher N adds low-quality "placeholder" actions and causes distributional shift; N=3 optimal
  - **Embedder size:** 3B vs 7B embedder shows no significant difference (Table 12)—performance bounded by VLM proposal quality
  - **Expectile τ:** 0.8 for high-quality GPT-4.1 data; 0.7 for failure-heavy Qwen data to avoid overestimation

- **Failure signatures:**
  - **VLM proposal bottleneck (primary):** 50.3% of steps lack correct action in candidate set—evidenced by value decay in trajectory visualizations (Appendix K, Figure 12b)
  - **Q-function selection error (secondary):** 36.2% selection failures when correct action exists—inspect per-domain results for systematic patterns
  - **Distributional mismatch:** If inference N ≠ training N, expect degraded performance with "REFRESH" or "RESTART" actions winning selection

- **First 3 experiments:**
  1. **Baseline comparison:** Run standard prompting vs. Best-of-Q on 50 tasks from WebVoyager; expect ~15-17 point gain for 7B models
  2. **N ablation:** Train Q-function with N=3, test at inference with N=3,5,8; expect peak at matching N
  3. **Cross-policy Q-function:** Train Q-function on GPT-4.1 data, apply to Qwen2.5-7B inference; expect moderate transfer (Table 11 shows 43.1% vs. 55.7% matched-training)

## Open Questions the Paper Calls Out

- **Question:** Can methods that improve VLM proposal diversity or quality be combined with Best-of-Q to achieve greater gains than either approach alone?
- **Basis in paper:** [explicit] The conclusion states: "combining our value-based action selection with methods to improve VLM proposal diversity or quality could lead to even greater improvements."
- **Why unresolved:** The failure analysis (Section 4.5) shows 50.3% of failures occur because the VLM never proposes the correct action, establishing a hard ceiling on Q-function efficacy that was not addressed.
- **What evidence would resolve it:** Experiments combining Best-of-Q with techniques like diverse prompting strategies or proposal-enrichment methods, measuring whether success rates exceed additive expectations.

- **Question:** What causes the Q-function to fail to select the correct action even when it is present in the candidate set, and can selection accuracy be improved?
- **Basis in paper:** [inferred] Section 4.5 reports that when a correct action was proposed, the Q-function failed to select it in 36.2% of cases. Authors speculate about "value ambiguity between similar actions or insufficient coverage for specific state-action pairs."
- **Why unresolved:** The paper does not investigate or validate these hypothesized causes, nor does it propose mitigations for Q-function selection errors.
- **What evidence would resolve it:** Analysis of failure cases to distinguish value ambiguity vs. coverage gaps, followed by targeted interventions (e.g., contrastive training, data augmentation for under-represented state-action pairs).

- **Question:** How does Best-of-Q compare to direct fine-tuning baselines (FBC, AWR) when controlling for dataset quality and size?
- **Basis in paper:** [explicit] Section 4.6 states: "A direct comparison with fine-tuning baselines was not feasible due to computational constraints" and emphasizes the Holo1-7B comparison is "not a direct baseline."
- **Why unresolved:** The Holo1-7B comparison confounds method differences with dataset differences (1.5M expert trajectories vs. 300K mixed-quality transitions), leaving the relative efficiency of the approaches unclear.
- **What evidence would resolve it:** Training FBC and AWR baselines on the same mixed-quality dataset used for Best-of-Q, then comparing performance per training compute budget.

- **Question:** Can curriculum learning strategies improve sample efficiency when generalizing the Q-function to new domains?
- **Basis in paper:** [explicit] Appendix E.1 states: "This suggests that curriculum learning methods could boost sample efficiency in such generalization experiments."
- **Why unresolved:** The cross-domain generalization experiments showed that more successful traces were needed to achieve comparable performance when training on out-of-domain data, but no curriculum strategy was tested.
- **What evidence would resolve it:** Experiments with difficulty-ordered or similarity-based curriculum schedules for cross-domain Q-function training, comparing sample efficiency to the current uniform sampling approach.

## Limitations

- **VLM proposal bottleneck:** 50.3% of failures occur because the VLM never proposes the correct action, which no Q-function can overcome
- **Offline data requirements:** Method requires expensive ε-greedy exploration data collection with no publicly available datasets
- **Localization component:** The Holo1.5-7B localization model is not publicly available, limiting exact reproduction

## Confidence

- **High Confidence:** The mechanism of using Q-functions to rank VLM-generated candidates is well-established and the empirical gains on WebVoyager are clearly demonstrated
- **Medium Confidence:** The claim that VLM proposal quality is the primary bottleneck relies on the trajectory analysis, which shows strong evidence but doesn't fully rule out Q-function limitations
- **Medium Confidence:** The assertion that IQL's stability advantages are crucial depends on comparing to other offline RL methods, which is not extensively explored

## Next Checks

1. **VLM proposal analysis:** Systematically measure what fraction of correct actions appear in the top-N candidates across different VLM models and task types to validate the 50.3% bottleneck claim
2. **Q-function transfer limits:** Train Q-functions on high-quality data (GPT-4.1) and apply to low-quality data (Qwen2.5-7B) to quantify the performance drop and validate the distributional shift concerns
3. **N=1 vs N>1 comparison:** Compare Best-of-Q with N=3 against a simple greedy VLM approach to isolate the contribution of Q-function selection from the benefits of candidate diversity