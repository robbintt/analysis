---
ver: rpa2
title: On the Unreasonable Effectiveness of Last-layer Retraining
arxiv_id: '2512.01766'
source_url: https://arxiv.org/abs/2512.01766
tags:
- group
- training
- held-out
- neural
- collapse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why last-layer retraining (LLR) improves
  worst-group accuracy in the presence of spurious correlations. The authors initially
  hypothesized that neural collapse and implicit bias explain LLR's success, but their
  empirical investigation did not support this hypothesis.
---

# On the Unreasonable Effectiveness of Last-layer Retraining

## Quick Facts
- arXiv ID: 2512.01766
- Source URL: https://arxiv.org/abs/2512.01766
- Reference count: 34
- Last-layer retraining improves worst-group accuracy primarily through better group balance in held-out sets

## Executive Summary
This paper investigates why last-layer retraining (LLR) is effective at improving worst-group accuracy in the presence of spurious correlations. The authors challenge the conventional wisdom that neural collapse and implicit bias explain LLR's success, presenting empirical evidence that LLR's effectiveness is primarily due to better group balance in the held-out set. Their controlled experiments demonstrate that LLR performs well when the held-out set has better group balance than the training set, but fails to outperform ERM when both sets have identical group distributions.

## Method Summary
The authors conduct a comprehensive empirical study of last-layer retraining across multiple vision datasets including Waterbirds, CelebA, and ImageNet-R. They systematically control for group balance in training and held-out sets while measuring worst-group accuracy. The experiments compare standard ERM, vanilla LLR, and recent LLR variants like CB-LLR and AFR under different group balance conditions. They also investigate alternative hypotheses including neural collapse and implicit bias by examining representation collapse patterns and weight evolution during training.

## Key Results
- LLR does not improve over ERM when held-out set has same group balance as training set
- LLR shows significant improvement when held-out set is more group-balanced
- Recent LLR methods (CB-LLR, AFR) succeed primarily through implicit group-balancing mechanisms
- Neural collapse and implicit bias do not explain LLR's effectiveness according to empirical investigation

## Why This Works (Mechanism)
LLR's effectiveness stems from its ability to exploit differences in group balance between training and held-out distributions. When the held-out set contains better group balance (e.g., equal representation across groups), LLR can leverage this implicit group information to learn a more balanced classifier without explicit group annotations during training. The method works by reweighting examples based on their group membership in the held-out set, even though these groups were not explicitly labeled during training.

## Foundational Learning
- **Spurious correlation**: When models rely on dataset biases rather than true predictive features (needed to understand the problem LLR addresses; check: models perform well on training data but poorly on minority groups)
- **Group imbalance**: Uneven representation of subgroups in training data (needed to understand when LLR helps; check: training set has skewed group ratios)
- **Worst-group accuracy**: Model performance on minority or underrepresented groups (needed to measure LLR's success; check: accuracy gaps between majority and minority groups)
- **Neural collapse**: Phenomenon where features from the same class collapse to a single point in representation space (needed to evaluate alternative explanations; check: feature representations show class-wise clustering)
- **Implicit bias**: Learning dynamics that favor certain solutions without explicit regularization (needed to test alternative hypotheses; check: weight evolution patterns during training)

## Architecture Onboarding

**Component Map**: Training data (with spurious correlations) -> Feature extractor (frozen) -> Last layer retraining -> Balanced classifier

**Critical Path**: The critical path is the interaction between the held-out set's group balance and the LLR optimization. The method succeeds when the held-out set provides implicit group information through its balanced distribution.

**Design Tradeoffs**: LLR trades computational efficiency (only retraining last layer) for potential performance gains. It requires group annotations only on the held-out set, making it practical when training data lacks group labels.

**Failure Signatures**: LLR fails when training and held-out sets have identical group distributions, when the held-out set is more imbalanced than training, or when spurious correlations are absent.

**First Experiments**: 1) Compare LLR vs ERM when training and held-out sets have identical group balance 2) Test LLR performance when held-out set has better group balance 3) Evaluate recent LLR variants under controlled group balance conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses primarily on vision datasets, limiting generalizability to other domains
- Assumes perfect group annotation availability on held-out set
- Does not fully rule out other contributing factors that might combine with group balancing

## Confidence
- **High confidence**: LLR's performance improvement correlates with group balance differences between training and held-out sets
- **Medium confidence**: Recent LLR methods (CB-LLR, AFR) succeed through implicit group-balancing mechanisms
- **Medium confidence**: Neural collapse and implicit bias do not explain LLR's effectiveness

## Next Checks
1. Test the group balance hypothesis on additional domains (NLP, tabular datasets) with varying spurious correlation strengths
2. Investigate LLR performance when group annotations are partially available or noisy on the held-out set
3. Design experiments to quantify relative contribution of group balancing versus other potential mechanisms (feature learning, regularization effects) in LLR's success