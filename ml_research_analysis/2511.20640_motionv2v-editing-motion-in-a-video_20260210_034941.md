---
ver: rpa2
title: 'MotionV2V: Editing Motion in a Video'
arxiv_id: '2511.20640'
source_url: https://arxiv.org/abs/2511.20640
tags:
- video
- motion
- editing
- control
- frame
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach for editing the motion of
  objects in videos. Unlike prior methods that rely on image-to-video generation or
  are limited to specific object types, this method enables users to directly edit
  sparse trajectories extracted from the input video to control how objects move in
  the output.
---

# MotionV2V: Editing Motion in a Video

## Quick Facts
- arXiv ID: 2511.20640
- Source URL: https://arxiv.org/abs/2511.20640
- Reference count: 40
- Primary result: Novel video-to-video motion editing via sparse trajectory deviations, achieving 70%+ user preference over SOTA

## Executive Summary
This paper introduces a novel approach for editing the motion of objects in videos by directly manipulating sparse trajectories extracted from the input. Unlike prior methods that rely on image-to-video generation or are limited to specific object types, this method enables users to control how objects move in the output by editing sparse point trajectories. The key innovation is the use of "motion counterfactuals" - video pairs with identical visual content but different motion patterns - to train a motion-conditioned video diffusion model. The approach allows for editing object motion, camera motion, timing of content, and applying edits at arbitrary frames, all while preserving scene consistency. In a user study, the method significantly outperformed state-of-the-art baselines, achieving 70% preference for content preservation, 71% for motion control, and 69% for overall quality compared to 25% for the strongest baseline. Quantitative evaluation also showed lower reconstruction error compared to image-to-video methods. The system enables iterative editing and handles complex scenarios like content appearing mid-video that previous methods cannot address.

## Method Summary
The method extends a frozen video diffusion model (CogVideoX-5B) with a control branch that processes three conditioning channels: counterfactual video, counterfactual tracks, and target tracks, all rasterized as colored Gaussian blobs. Motion counterfactuals are generated by extracting clips from source videos and creating corresponding clips with altered motion via frame interpolation or temporal resampling. TAPNext bidirectional tracking extracts sparse point correspondences, which users can modify to create target trajectories. The control branch integrates with the main branch through zero-initialized MLPs at each transformer block, allowing gradient flow only through the adapter during training. At inference, 1-2 pixel jitter is added to trajectories to prevent identity copying. The system trains on 100K videos from a proprietary 500K internal dataset, with counterfactual generation handled by diffusion models and LLM-prompted interpolation.

## Key Results
- User study preference: 70% content preservation, 71% motion control, 69% overall quality vs 25% for strongest baseline
- Quantitative: L2 reconstruction error 0.024 vs 0.038 for image-to-video baseline
- Handles complex edits including object motion, camera motion, timing changes, and mid-video content appearance
- Enables iterative editing without retraining

## Why This Works (Mechanism)

### Mechanism 1: Sparse Trajectory Deviations as Motion Control
Modifying sparse point trajectories extracted from an input video enables precise motion editing while preserving content. The system tracks points throughout the input video, computes the deviation between source and user-specified target trajectories, and conditions a generative model on this delta. The model learns to synthesize video matching the target motion. This works because sparse tracking points capture sufficient motion semantics for most editing tasks. However, when objects undergo extreme deformation or occlusion that breaks point correspondence, trajectory-based control degrades.

### Mechanism 2: Motion Counterfactual Training Data Pipeline
Training on paired videos with shared content but different motion teaches the model to disentangle motion from appearance. From a source video, the pipeline extracts a target clip and generates a "counterfactual" clip via either frame interpolation with a diffusion model plus LLM prompts, or temporal resampling (speed changes, reversals). Point correspondences are established via TAPNext tracking, with geometric augmentations applied to ensure robustness. The generated counterfactual pairs must be realistic and diverse enough to teach motion editing without requiring manually labeled data. If the frame interpolation model hallucinates inconsistent content between the anchoring frames, point correspondences become noisy, degrading training quality.

### Mechanism 3: Control Branch with Zero-Initialized Integration
A ControlNet-style adapter branch enables conditioning on full video and trajectory inputs while keeping the base T2V model frozen. The control branch duplicates the first 18 DiT blocks and processes three conditioning channels (counterfactual video, counterfactual tracks, target tracks) encoded via a 3D Causal VAE. Zero-initialized MLPs add control branch tokens to the main branch, allowing gradients to flow only through the adapter during training. This works because transformer blocks can learn to align spatiotemporally misaligned inputs with output video. If the input-output misalignment exceeds what the transformer can resolve (e.g., extreme trajectory edits), the model may ignore control signals or produce incoherent motion.

## Foundational Learning

- **Concept**: Diffusion Models (DDPM/DDIM)
  - **Why needed here**: The base model is a video diffusion model (CogVideoX-5B); understanding denoising objectives, latent diffusion, and classifier-free guidance is essential.
  - **Quick check question**: Can you explain why adding noise and learning to denoise enables generative modeling?

- **Concept**: ControlNet / Conditional Control in Diffusion
  - **Why needed here**: The architecture explicitly extends ControlNet's zero-initialized adapter approach to DiT-based video models.
  - **Quick check question**: Why does zero-initialization prevent the control branch from corrupting pretrained features at training start?

- **Concept**: Point Tracking / Trajectory Representation
  - **Why needed here**: The method relies on TAPNext for bidirectional point tracking and rasterizes trajectories as colored Gaussian blobs for conditioning.
  - **Quick check question**: How does a bidirectional tracker differ from optical flow estimation, and what advantages does it offer for long-range correspondence?

## Architecture Onboarding

- **Component map**: User clicks points -> TAPNext tracks through full video -> source trajectories; User drags points -> target trajectories; Rasterize both trajectories as colored blobs; VAE encode: input video, source blobs, target blobs -> latents; Control branch processes conditioned latents; Main branch denoises with control signals integrated at each block; VAE decode -> output video matching target motion

- **Critical path**: 1) User clicks points -> TAPNext tracks through full video -> source trajectories; 2) User drags points -> target trajectories; 3) Rasterize both trajectories as colored Gaussian blob videos; 4) VAE encode: input video, source blobs, target blobs -> latents; 5) Control branch processes conditioned latents; 6) Main branch denoises with control signals integrated at each block; 7) VAE decode -> output video matching target motion

- **Design tradeoffs**: Sparse (â‰¤64) vs. dense tracking: Sparse enables user control; too many points cause model failure; Control branch depth (18 blocks): Balances adaptation capacity vs. compute; Real video targets + synthetic counterfactuals: Preserves realism while creating paired training data; Zero-init vs. direct fine-tuning: Zero-init preserves base model capabilities

- **Failure signatures**: Pixel-perfect trajectory alignment -> model copies input video semantics (identity copying); fix: add 1-2 pixel jitter at inference; Too many control points (>~20) -> model fails to follow all correspondences; Iterative edits accumulate drift (subject appearance changes over successive edits)

- **First 3 experiments**: 1) Validate control branch integration: Ablate control branch depth (e.g., 6/12/18/30 blocks) on a held-out motion-edit task; measure L2 reconstruction and user preference; 2) Test jitter sensitivity: Systematically vary jitter magnitude (0-5 pixels) on the ablation case from Section 10; quantify the tradeoff between motion-following and content preservation; 3) Probe trajectory density limits: Evaluate model performance with varying numbers of tracking points (1, 5, 10, 20, 40, 64) on complex multi-object scenes; identify the failure threshold

## Open Questions the Paper Calls Out

### Open Question 1
Can large-scale synthetic 3D datasets with precise motion counterfactuals enable training with fewer control points while improving edit precision? The authors consider creating large-scale synthetic datasets with precise motion counterfactuals made with 3d software, which would improve the precision of the training dataset, possibly allowing even less points to be used for control. Current training uses diffusion-generated counterfactuals which may lack perfect ground truth for object trajectories, physical interactions, and lighting changes.

### Open Question 2
Why does pixel-perfect trajectory alignment cause the model to copy original video semantics, and can this be addressed through training rather than inference-time jitter? The authors discovered that without jitter, the model exhibits a strong bias toward reproducing the original video's semantics and use inference-time noise (1-2 pixels) as a workaround. This suggests a fundamental training distribution issue. The root cause of this identity-copying bias and whether it reflects overfitting to aligned trajectories in training data remains unexplored.

### Open Question 3
What architectural or training improvements could enable precise control with more than ~20 tracking points? The authors note that during inference, they limit the number of point correspondences to approximately 20, as the model fails to follow all correspondences when given too many points. The rasterized Gaussian blob representation and control branch capacity may create a bottleneck; the failure mode with many points is not diagnosed.

### Open Question 4
How can subject drift in iterative editing be eliminated to enable truly infinite sequential edits? The paper shows iterative editing works but notes some degree of subject drift, which can be attributed in part to the quality of the base video model. The authors express hope that future versions will be able to be applied infinitely. Error accumulation across edits is not characterized quantitatively, and whether drift stems from base model limitations or the control mechanism is unclear.

## Limitations

- Reliance on sparse trajectory editing assumes stable point tracking across complex motions, occlusions, and viewpoint changes; precise limits and failure modes under extreme editing scenarios are not fully characterized.
- Training dataset is proprietary (500K internal Google dataset), limiting reproducibility and raising questions about diversity and generalizability of learned motion representations.
- Model's ability to handle iterative edits and avoid content drift over multiple editing rounds is noted but not thoroughly validated; limited discussion of computational efficiency or scalability to longer videos.

## Confidence

- **High Confidence**: The core mechanism of sparse trajectory-based motion editing, the use of motion counterfactuals for training, and the overall superiority in user preference studies (70% content preservation, 71% motion control) are well-supported by the paper's results and methodology.
- **Medium Confidence**: The effectiveness of the zero-initialized control branch integration for disentangling motion from content is plausible but relies on architectural choices (18 blocks, zero-init) that may not generalize to all video editing tasks without further ablation.
- **Low Confidence**: The robustness of the counterfactual generation pipeline to diverse motion patterns and the model's performance on highly complex, long-range editing tasks remain underexplored.

## Next Checks

1. **Trajectory Density Limits**: Systematically evaluate model performance with varying numbers of tracking points (e.g., 1, 5, 10, 20, 40, 64) on complex multi-object scenes to identify the failure threshold and characterize performance degradation.

2. **Counterfactual Generation Robustness**: Test the sensitivity of training outcomes to the quality and diversity of generated counterfactuals by ablating the LLM prompt generation and temporal resampling strategies; measure downstream editing accuracy and content preservation.

3. **Iterative Edit Drift**: Conduct experiments where the same video undergoes multiple rounds of motion editing; quantify changes in object appearance, background consistency, and accumulated artifacts to assess the model's stability over iterative use.