---
ver: rpa2
title: 'Towards Accurate UAV Image Perception: Guiding Vision-Language Models with
  Stronger Task Prompts'
arxiv_id: '2512.07302'
source_url: https://arxiv.org/abs/2512.07302
tags:
- task
- prompt
- visual
- enhancement
- perception
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AerialVP, an agent framework for task prompt
  enhancement in UAV image perception, addressing the challenge of image-text alignment
  inconsistency in complex UAV imagery. The core method involves using an LLM-based
  Task Engine and a Tool Repository to analyze task prompts, select appropriate enhancement
  tools, and generate enriched prompts incorporating semantic, spatial position, and
  spatial relationship information.
---

# Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts

## Quick Facts
- arXiv ID: 2512.07302
- Source URL: https://arxiv.org/abs/2512.07302
- Reference count: 40
- Key result: GPT-4o visual grounding accuracy improved from 2.04% to 45.05% using enhanced prompts

## Executive Summary
This paper addresses the challenge of image-text alignment inconsistency in UAV image perception by introducing AerialVP, a framework that enhances task prompts for vision-language models (VLMs). The approach leverages an LLM-based Task Engine and a Tool Repository to enrich initial task prompts with semantic, spatial position, and spatial relationship information. Experiments on the newly introduced AerialSense benchmark demonstrate significant performance improvements across UAV visual grounding, reasoning, and question answering tasks, with most VLMs showing consistent gains of 15-45%.

## Method Summary
AerialVP is an agent framework designed to enhance task prompts for UAV image perception by addressing image-text alignment inconsistencies in complex aerial imagery. The framework consists of an LLM-based Task Engine that analyzes task prompts, selects appropriate enhancement tools from a Tool Repository, and generates enriched prompts incorporating semantic, spatial position, and spatial relationship information. The enhanced prompts are then fed to VLMs for improved perception performance across tasks like visual grounding, reasoning, and question answering. The approach specifically targets the limitations of current VLMs in handling the complexity and diversity of UAV imagery.

## Key Results
- GPT-4o visual grounding accuracy improved from 2.04% to 45.05% with enhanced prompts
- Most VLMs showed consistent performance gains of 15-45% across UAV visual grounding, reasoning, and question answering tasks
- Performance improvements were consistent across proprietary and open-source VLMs tested on the AerialSense benchmark

## Why This Works (Mechanism)
The framework works by enriching task prompts with detailed auxiliary information that bridges the gap between complex UAV imagery and VLM understanding. By leveraging multiple enhancement tools (semantic, spatial position, and spatial relationship), the approach provides VLMs with richer context about the visual scene, compensating for their inherent limitations in handling UAV-specific imagery characteristics.

## Foundational Learning
- **Vision-Language Models (VLMs)**: AI models that process both visual and textual information to perform tasks requiring cross-modal understanding. Why needed: VLMs form the core perception engine that AerialVP aims to enhance through better prompts.
- **Task Prompt Enhancement**: The process of augmenting initial task descriptions with additional contextual information to improve model performance. Why needed: Standard prompts often lack the specificity required for complex UAV imagery interpretation.
- **Image-Text Alignment**: The degree to which textual descriptions accurately represent visual content. Why needed: Misalignment is a key challenge in UAV perception that leads to VLM performance degradation.
- **Spatial Relationship Extraction**: Identifying and describing spatial relationships between objects in an image. Why needed: UAV imagery often requires understanding complex spatial arrangements that standard VLMs struggle with.
- **Semantic Enrichment**: Adding detailed object and attribute descriptions to prompts. Why needed: UAV scenes contain diverse objects that require comprehensive semantic context for accurate interpretation.
- **LLM-based Task Analysis**: Using large language models to analyze task requirements and determine appropriate enhancement strategies. Why needed: Enables dynamic and intelligent selection of enhancement tools based on task complexity.

## Architecture Onboarding

**Component Map:**
Task Engine -> Tool Repository -> Enhanced Prompt -> VLM

**Critical Path:**
1. Initial task prompt received by Task Engine
2. Task Engine analyzes prompt and selects appropriate tools
3. Tools extract semantic, spatial, and relationship information
4. Enhanced prompt generated and sent to VLM
5. VLM performs perception task with improved accuracy

**Design Tradeoffs:**
- **Accuracy vs. Latency**: More comprehensive enhancement tools improve accuracy but increase computational overhead
- **Complexity vs. Generalizability**: Specialized tools perform well on specific tasks but may reduce framework flexibility
- **Tool Dependency**: Framework performance is constrained by the quality and capabilities of constituent enhancement tools

**Failure Signatures:**
- Individual tools introducing noise rather than useful information (e.g., spatial relationship tool degrading performance)
- Over-enhancement leading to prompt dilution or confusion
- LLM-based Task Engine making incorrect tool selection decisions

**3 First Experiments:**
1. Baseline VLM performance on standard prompts vs. enhanced prompts for visual grounding task
2. Ablation study removing each enhancement tool to quantify individual contributions
3. Performance comparison across different VLM architectures (proprietary vs. open-source) with enhanced prompts

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can adaptive enhancement mechanisms be developed to dynamically select the optimal combination of tools and prompt strategies to improve generalizability across diverse UAV scenarios?
- **Basis in paper:** [explicit] Section VIII states that future work includes "developing adaptive enhancement mechanisms to improve generalizability across scenarios."
- **Why unresolved:** The current AerialVP framework relies on a relatively fixed logic for tool selection based on initial task analysis, lacking the ability to dynamically adjust enhancement strategies based on real-time image complexity or tool confidence.
- **What evidence would resolve it:** A study demonstrating an adaptive AerialVP version that automatically adjusts the weighting or selection of semantic/spatial tools based on scene difficulty, showing superior generalization on out-of-distribution aerial data compared to the static pipeline.

### Open Question 2
- **Question:** How can the framework mitigate performance degradation when individual enhancement tools, such as spatial relationship models, introduce noise rather than useful auxiliary information?
- **Basis in paper:** [inferred] Table V shows that using the Spatial Relationship tool alone caused accuracy to drop to 0.07% for Qwen2.5-Max, and Section VIII notes that quality is "dependent on the performance of constituent tools."
- **Why unresolved:** The framework currently assumes that adding information is beneficial, but the ablation study demonstrates that specific tools can produce descriptions that mislead the VLM if not properly supported by other dimensions.
- **What evidence would resolve it:** Implementation of a validation or confidence-filtering module within the Task Engine that identifies and excludes low-confidence spatial or semantic descriptions, resulting in consistently non-negative performance deltas in ablation studies.

### Open Question 3
- **Question:** Can the prompt enhancement pipeline be optimized to maintain high perception accuracy in resource-constrained environments where large constituent models are infeasible?
- **Basis in paper:** [explicit] Section VIII lists "resource-constrained scenarios" as a limitation, noting that the quality of enhanced prompts depends on constituent tools which may restrict applicability.
- **Why unresolved:** The current implementation relies on large models (e.g., GPT-4o, YoloWorld-X) for tool execution. It is unclear if the "Prompt-Centric" gains hold when using lightweight, edge-deployable tool alternatives.
- **What evidence would resolve it:** Experiments comparing AerialVP's performance using lightweight (e.g., mobile-optimized) versus heavy tools, specifically identifying the trade-off point between latency/resource usage and visual grounding accuracy.

## Limitations
- The AerialSense benchmark may not fully capture real-world UAV scenario diversity, potentially limiting generalizability
- Reliance on LLM-based Task Engine introduces computational overhead and dependency on LLM quality
- Study focuses on performance gains without extensive analysis of failure cases or robustness under varying environmental conditions
- Comparison limited to existing VLMs without exploring complementary architectural modifications

## Confidence
- **High Confidence:** The methodology of using task-specific prompt enhancement to improve VLM performance is well-founded and the reported performance gains on AerialSense are reproducible based on the described framework.
- **Medium Confidence:** The generalizability of AerialVP to real-world UAV applications beyond the benchmark scenarios requires further validation.
- **Medium Confidence:** The computational efficiency trade-offs of the LLM-based Task Engine component need more thorough evaluation.

## Next Checks
1. Test AerialVP on diverse real-world UAV datasets (e.g., UAVid, DOTA) to assess generalizability beyond the curated AerialSense benchmark.
2. Conduct ablation studies to quantify the contribution of each enhancement tool (semantic, spatial position, spatial relationship) and evaluate computational overhead.
3. Analyze failure cases systematically to identify scenarios where prompt enhancement is ineffective or introduces errors, particularly under challenging environmental conditions.