---
ver: rpa2
title: 'Automating Personalization: Prompt Optimization for Recommendation Reranking'
arxiv_id: '2504.03965'
source_url: https://arxiv.org/abs/2504.03965
tags:
- user
- reranking
- feedback
- prompt
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces AGP, an automated prompt optimization framework
  for LLM-based reranking in recommender systems. AGP addresses the challenge of manual
  prompt engineering and unstructured item metadata by optimizing user profile generation
  prompts instead of reranking prompts.
---

# Automating Personalization: Prompt Optimization for Recommendation Reranking

## Quick Facts
- **arXiv ID:** 2504.03965
- **Source URL:** https://arxiv.org/abs/2504.03965
- **Reference count:** 16
- **Primary result:** AGP improves NDCG@10 by 5.61-20.68% over baseline models using only 100 training users

## Executive Summary
This paper introduces AGP, an automated prompt optimization framework for LLM-based reranking in recommender systems. AGP addresses the challenge of manual prompt engineering and unstructured item metadata by optimizing user profile generation prompts instead of reranking prompts. The key innovations include position-aware feedback for precise ranking correction and batched training with aggregated feedback to enhance generalization. Experiments across three datasets demonstrate significant improvements over baseline models, particularly for graph-based recommenders.

## Method Summary
AGP optimizes user profile generation prompts through a two-stage pipeline: (1) an LLM generates structured user profiles from interaction histories using an optimized prompt, and (2) a reranker LLM uses these profiles to reorder candidate items. The framework employs position-based feedback comparing actual to target item positions, which is aggregated across user batches and used to iteratively update the generation prompt. Training uses only 100 users, with experiments conducted on Amazon Movies & TV, Yelp, and Goodreads datasets.

## Key Results
- AGP improves NDCG@10 by 5.61-20.68% over baseline models
- Particularly strong performance on LightGCN (9.36-20.68% gains) compared to SASRec
- Achieves these results using only 100 training users, with marginal gains from scaling to 700 users
- Position-based feedback and batched training are key contributors to performance

## Why This Works (Mechanism)

### Mechanism 1
Optimizing user profile generation prompts yields better generalization than directly optimizing reranking prompts. By refining how profiles are generated rather than how items are ranked, the system learns transferable preference extraction patterns. Core assumption: user preferences can be meaningfully summarized from item titles alone, and better summaries lead to better reranking. Evidence: AGP's indirect approach is distinct from related work on direct reranking prompt optimization. Break condition: If item titles lack semantic richness (e.g., generic product codes), profile generation degrades.

### Mechanism 2
Position-based feedback provides more actionable optimization signals than aggregate metrics like NDCG. For each ground-truth item, AGP computes actual vs target position pairs, which are fed back as text instructions to refine the generation prompt, weighted by inverse average position. Core assumption: LLMs can interpret and act on structured positional feedback to adjust prompt behavior in a "gradient-like" manner. Evidence: Ablation shows PBF improves NDCG@10 across all three datasets. Break condition: If ground-truth items are consistently absent from the candidate list, position feedback provides no signal.

### Mechanism 3
Batched training with summarized feedback reduces overfitting compared to per-user optimization. Feedback from a batch of users is aggregated via weighted summation, then used to update the generation prompt. Summarization filters idiosyncratic user quirks while retaining generalizable patterns. Core assumption: Shared prompt improvements can benefit diverse users without per-user customization. Evidence: Ablation shows summarization reduces train-test gap and improves test NDCG. Break condition: With insufficient batch diversity, summarized feedback may average away meaningful signals.

## Foundational Learning

- **Concept: LLM-based Reranking Pipeline**
  - Why needed here: AGP sits atop an existing recommender that produces candidate lists; understanding the two-stage flow (generation → reranking) is prerequisite.
  - Quick check question: Can you explain why reranking operates on a fixed candidate list rather than generating items from scratch?

- **Concept: Prompt Optimization as Discrete Search**
  - Why needed here: Unlike gradient-based fine-tuning, prompt optimization operates in text space; AGP uses LLM-generated feedback to iteratively rewrite prompts.
  - Quick check question: What constraints does discrete prompt space impose compared to continuous embedding optimization?

- **Concept: Ranking Metrics (NDCG, Position)**
  - Why needed here: NDCG@10 is the evaluation metric; position-based feedback is the optimization proxy. Understanding their relationship is essential.
  - Quick check question: Why might optimizing position directly differ from optimizing NDCG?

## Architecture Onboarding

- **Component map:** Base Recommender → Profile Generator → Reranker → Feedback Module → Prompt Optimizer
- **Critical path:** Interaction history → Profile generation → Reranking → Position feedback → Prompt update. Errors in profile generation cascade directly to reranking quality.
- **Design tradeoffs:**
  - Batch size: Larger → more stable but slower iteration; smaller → faster but noisier (optimal: 10)
  - History length: Longer → more context but more noise (optimal: 5)
  - Training users: 100 users sufficient; 700 users only added 1.29% NDCG gain
- **Failure signatures:**
  - Flat NDCG across iterations → feedback not actionable; inspect feedback quality
  - High train-test gap → disable summarization or increase batch size
  - Poor gains on Yelp → check item metadata richness; sparse text breaks profile generation
- **First 3 experiments:**
  1. Replicate ablation on batch size (5/10/20) and history length (5/10/20) on a single dataset to validate optimal hyperparameters.
  2. Test position-based feedback vs. NDCG-only feedback to confirm mechanism contribution.
  3. Profile a single user through the pipeline: inspect generated profile, reranked list, and feedback signal to debug end-to-end flow.

## Open Questions the Paper Calls Out

### Open Question 1
Why does AGP show stronger improvements on LightGCN (9.36-20.68% gains) despite the authors' hypothesis that LLMs should align better with SASRec due to shared time-series modeling? The authors state "LLM rerankers show greater improvements on SASRec rankings than on LightGCN" and attribute this to sequential alignment, yet Table 1 shows LightGCN gains often exceed SASRec gains, creating an unexplained contradiction. What evidence would resolve it: A controlled study measuring the interaction between base model architecture characteristics (graph structure vs. sequential patterns) and AGP's profile-generation optimization, with explicit metrics for alignment quality.

### Open Question 2
What metadata quality thresholds are required for AGP to be effective, and can the framework adapt to extremely sparse or inconsistent item descriptions? The paper identifies metadata quality as a limitation but does not quantify what level of sparsity or noise breaks the approach, nor propose mechanisms to handle it beyond "dynamically refining prompts." What evidence would resolve it: Controlled experiments varying metadata richness (e.g., masking percentages of item titles/descriptions) across datasets to identify performance degradation curves and failure modes.

### Open Question 3
Does the marginal 1.29% improvement from scaling training users from 100 to 700 indicate a fundamental data efficiency ceiling for AGP, or is it dataset-specific? The paper emphasizes efficiency with 100 training users and reports that "NDCG@10 increased marginally from 0.696 to 0.705" with 700 users, but does not investigate whether this plateau stems from prompt optimization saturation, user diversity limits, or dataset characteristics. What evidence would resolve it: Scaling experiments across all three datasets with systematic analysis of prompt diversity, user clustering, and learning curves to determine if the plateau is universal or context-dependent.

## Limitations
- Performance degrades significantly on datasets with sparse or noisy item metadata (e.g., Yelp)
- The exact mechanics of text-based prompt updates (∇_text operator) remain underspecified
- No direct comparison of profile generation optimization vs. direct reranking prompt optimization

## Confidence

- **High Confidence:** The core architectural approach (two-stage pipeline with profile generation) is clearly described and experimentally validated. The dataset preparation and evaluation methodology are reproducible.
- **Medium Confidence:** The claim that AGP improves NDCG@10 by 5.61-20.68% over baselines is supported by ablation studies, though the exact contribution of each mechanism component could benefit from additional isolation tests.
- **Low Confidence:** The mechanism explaining why optimizing user profile generation prompts yields better generalization than direct reranking prompt optimization lacks direct comparative evidence within the paper.

## Next Checks

1. Implement a controlled experiment comparing position-based feedback optimization against NDCG-only optimization on a single dataset to isolate the contribution of the feedback mechanism.
2. Conduct an ablation study on prompt template structure (varying initial prompt formats) to determine sensitivity to prompt initialization.
3. Test the pipeline on a fourth dataset with richer item metadata to verify that profile generation quality directly correlates with reranking performance gains.