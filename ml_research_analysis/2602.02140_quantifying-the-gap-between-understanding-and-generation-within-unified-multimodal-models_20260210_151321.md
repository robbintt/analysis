---
ver: rpa2
title: Quantifying the Gap between Understanding and Generation within Unified Multimodal
  Models
arxiv_id: '2602.02140'
source_url: https://arxiv.org/abs/2602.02140
tags:
- image
- knowledge
- understanding
- generation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose GAPEVAL, a bidirectional benchmark to quantify the gap
  between understanding and generation within unified multimodal models (UMMs). Each
  question can be answered in both modalities, enabling symmetric evaluation of bidirectional
  inference and cross-modal consistency.
---

# Quantifying the Gap between Understanding and Generation within Unified Multimodal Models

## Quick Facts
- arXiv ID: 2602.02140
- Source URL: https://arxiv.org/abs/2602.02140
- Authors: Chenlong Wang; Yuhang Chen; Zhihan Hu; Dongping Chen; Wenhu Chen; Sarah Wiegreffe; Tianyi Zhou
- Reference count: 40
- Primary result: Proposes GAPEVAL benchmark showing persistent performance gap between understanding and generation in UMMs

## Executive Summary
This paper introduces GAPEVAL, a bidirectional benchmark designed to quantify the gap between understanding (image→text) and generation (text→image) capabilities within Unified Multimodal Models (UMMs). The benchmark evaluates 9 UMMs across 646 human-annotated questions spanning four categories, revealing that current models achieve only surface-level unification with persistent performance gaps. Knowledge manipulation experiments demonstrate that capabilities across modalities remain disjoint, with knowledge synchronization unsynchronized across modalities. The authors identify three key mechanisms: disjoint parameter spaces for different modalities, asynchronous convergence rates, and surface-level unification causing interference.

## Method Summary
The GAPEVAL benchmark consists of 646 human-annotated questions with paired text and image answers, enabling symmetric evaluation of bidirectional inference. Each sample includes a question image, text instruction (for understanding), generation prompt (for generation), and ground truth for both modalities. Evaluation uses a GPT-5-mini judge to label correctness, with scores processed through Multidimensional Item Response Theory (MIRT) to compute gap scores measuring the discrepancy between latent understanding and generation abilities. Knowledge manipulation experiments involve fine-tuning on disjoint or joint datasets to test knowledge transfer between modalities.

## Key Results
- Nine UMMs tested show persistent performance gaps between understanding and generation capabilities
- Knowledge within UMMs remains disjoint, with fine-tuning on one modality failing to propagate knowledge to the other
- Understanding and generation capabilities converge at different speeds (asynchronous convergence), causing widening gaps during training
- Models achieve only surface-level unification, with capability emergence and knowledge synchronization unsynchronized across modalities

## Why This Works (Mechanism)

### Mechanism 1: Disjoint Parameter Spaces
Unified Multimodal Models store knowledge representations for understanding and generation in separate parameter spaces rather than a shared semantic layer. Fine-tuning updates remain localized to modality-specific sub-networks, preventing gradient propagation between understanding and generation parameters. This physical separation explains why knowledge editing in one modality fails to transfer to the other.

### Mechanism 2: Asynchronous Convergence
Understanding and generation capabilities follow different optimization trajectories due to task complexity differences. Text understanding (discrete token prediction) converges faster than image generation (high-dimensional continuous pixel alignment). This differential convergence creates a "performance-lagging effect" where generation capability falls behind understanding during finite training.

### Mechanism 3: Surface-Level Unification
Integrating diffusion backbones with LLMs through joint training causes interference or sub-optimal routing between components. The unification process can degrade pre-trained weights of specialist components, resulting in models that underperform their specialized backbones. This suggests current unification approaches achieve only functional coexistence rather than true integration.

## Foundational Learning

**Bidirectional Consistency**: Needed to evaluate true knowledge possession - a model must demonstrate consistent knowledge across modalities (recognizing a concept in an image AND generating an image from that concept). Quick check: If a model can name an object in a photo but cannot generate a photo of that object from its name, it exhibits disjoint knowledge rather than unified understanding.

**Item Response Theory (IRT)**: Required because standard accuracy metrics assume equal question difficulty. MIRT accounts for varying difficulty levels, allowing fairer gap scoring by distinguishing model incompetence from task difficulty. Quick check: Simple accuracy is insufficient because failing a "hard" question means less about model capability than failing an "easy" one.

**Knowledge Manipulation (Injection vs. Editing)**: Diagnostic probe to test cross-modal knowledge propagation. By surgically altering specific facts, we can determine if models update representations holistically or modality-locally. Quick check: Teaching a model that "zorg" is a blue sphere via text but it fails to identify a blue sphere as "zorg" in an image indicates cross-modal knowledge propagation failure.

## Architecture Onboarding

**Component map**: Input → Unified Tokenizer (Text + Image tokens) → Unified Multimodal Model (Transformer/MoT) → Dual outputs (Text Head for Understanding, Diffusion/Image Head for Generation) → Evaluator (GPT-5-mini Judge + MIRT Module)

**Critical path**: 1) Data Construction: Pairing questions with both text and image answers, 2) Evaluation: Running model to get (Und_Output, Gen_Output), 3) Scoring: Judge correctness (0/1) + MIRT computes Gap Score, 4) Diagnosis: Fine-tuning on single-modality data to test transfer

**Design tradeoffs**: Unified vs. MoE - fully unified layers may suffer interference while Mixture-of-Experts may isolate knowledge but risk disjoint representations; Training Budget - allocation to understanding vs. generation impacts lagging effect

**Failure signatures**: "Lagging" Gap (Understanding >> Generation early on), Modality Drift (performance drop in non-trained modality), Zero Cross-Transfer (editing text yields 0% change in image generation)

**First 3 experiments**: 1) Bidirectional Baseline: Evaluate standard UMM (e.g., Bagel) on GAPEVAL to establish raw Gap Score, 2) Knowledge Isolation Test: Fine-tune on new entity only via text, measure if generation improves, 3) Convergence Analysis: Plot understanding vs. generation accuracy over training steps to verify asynchronous convergence pattern

## Open Questions the Paper Calls Out

**Open Question 1**: What architectural modifications are required to achieve intrinsic knowledge synchronization across modalities in UMMs? The paper shows knowledge editing in one modality fails to update the other, suggesting fundamental architectural constraints rather than training issues.

**Open Question 2**: How can training curricula be designed to synchronize the divergent convergence rates of understanding and generation capabilities? The observed "performance-lagging effect" indicates current training schedules fail to balance modality-specific learning dynamics.

**Open Question 3**: Do high-performing UMMs utilize shared internal representations for cross-modal reasoning, or rely on separate parallel subsystems? The "surface-level unification" finding suggests possible parallel subsystems, but the precise internal mechanism remains unclear.

## Limitations

- Judge reliability depends on GPT-5-mini's calibration and potential modality-specific biases in correctness judgments
- MIRT framework assumes unidimensional ability traits that may oversimplify complex multimodal capabilities
- Knowledge manipulation experiments demonstrate disjoint representations but cannot fully exclude alternative explanations like training dynamics or data limitations

## Confidence

**High Confidence**: Existence of measurable performance gap between understanding and generation in UMMs (supported by multiple experimental results across nine models)

**Medium Confidence**: Claim that knowledge representations are physically disjoint rather than poorly synchronized (based on knowledge manipulation experiments, but alternative explanations cannot be fully excluded)

**Medium Confidence**: Interpretation that "surface-level unification" causes capability dilution (supported by comparison to backbone performance, but interference effects could have other sources)

## Next Checks

1. **Judge Reliability Test**: Create a small human-annotated golden set (50 samples) and compare GPT-4o/GPT-4V judge outputs against human labels to quantify systematic biases, particularly for generated images

2. **Cross-Modal Transfer Sensitivity**: Repeat knowledge injection experiment with varying fine-tuning strategies (joint training, gradual unfreezing, contrastive learning) to test whether architectural constraints or training methodology primarily drive disjoint knowledge effects

3. **Architectural Ablation Study**: Compare gap scores across different unification strategies (fully unified vs. MoE routing vs. modality-specific adapters) on the same benchmark to isolate whether observed gaps are inherent to unification or specific to current architectural choices