---
ver: rpa2
title: 'Sample Complexity of Agnostic Multiclass Classification: Natarajan Dimension
  Strikes Back'
arxiv_id: '2511.12659'
source_url: https://arxiv.org/abs/2511.12659
tags:
- sample
- algorithm
- learning
- dimension
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the fundamental question of whether multiclass
  PAC learning can be characterized by a single combinatorial dimension, analogous
  to the Vapnik-Chervonenkis (VC) dimension in binary classification. Previous work
  by Brukhim et al.
---

# Sample Complexity of Agnostic Multiclass Classification: Natarajan Dimension Strikes Back

## Quick Facts
- **arXiv ID**: 2511.12659
- **Source URL**: https://arxiv.org/abs/2511.12659
- **Reference count**: 10
- **Key outcome**: Agnostic multiclass PAC sample complexity is characterized by both DS and Natarajan dimensions: $\frac{DS^{1.5}}{\varepsilon} + \frac{Nat}{\varepsilon^2}$, nearly tight up to $\sqrt{DS}$ factor.

## Executive Summary
This paper resolves a fundamental question in multiclass learning: whether a single combinatorial dimension characterizes agnostic sample complexity. Previous work showed the DS dimension characterizes realizable learning, while the Natarajan dimension does not. This paper demonstrates that for agnostic learning, both dimensions are essential - the DS dimension governs the first term of the sample complexity, while the Natarajan dimension dictates asymptotic behavior for small $\varepsilon$. The key insight is that these dimensions control different regimes of the excess risk, leading to a bound of the form $\frac{DS^{1.5}}{\varepsilon} + \frac{Nat}{\varepsilon^2}$.

## Method Summary
The authors develop a novel three-stage algorithm called MAPL. First, they construct a finite proxy class $F$ using sample compression schemes (SCSR) derived from the One-Inclusion Algorithm. Second, they perform label-space reduction using a self-adaptive Multiplicative Weights algorithm to learn a short list of classifiers. Third, they learn a final classifier restricted to the learned list using an $l_\mu$-sample compression scheme (LSCS). The approach departs from traditional uniform convergence methods and leverages online learning techniques, particularly the MW algorithm with a specific adaptive reward function that depends on previous rounds.

## Key Results
- Agnostic multiclass PAC sample complexity is governed by both DS and Natarajan dimensions
- Nearly tight sample complexity bound: $\frac{DS^{1.5}}{\varepsilon} + \frac{Nat}{\varepsilon^2}$
- First term controlled by DS dimension, second term by Natarajan dimension for small $\varepsilon$
- Novel online procedure based on self-adaptive multiplicative-weights algorithm for label-space reduction

## Why This Works (Mechanism)
The mechanism works because the two dimensions capture fundamentally different aspects of learnability. The DS dimension measures the complexity of the hypothesis class in terms of how many labelings can be realized, while the Natarajan dimension measures the richness of the class in terms of how many labelings can be shattered. In the agnostic setting, both aspects matter: DS governs the initial learning phase while Nat determines the asymptotic behavior as $\varepsilon$ approaches zero.

## Foundational Learning
- **One-Inclusion Algorithm**: Constructs a universal set of predictors for any concept class; needed for finite proxy class construction
  - Quick check: Verify that the algorithm produces a valid orientation of the inclusion graph
- **Multiplicative Weights Method**: Online algorithm for learning with expert advice; used for label-space reduction
  - Quick check: Confirm regret bounds scale as $O(\sqrt{T \log |F|})$
- **Sample Compression Schemes**: Techniques to compress labeled samples; used to construct finite covers
  - Quick check: Verify that reconstruction maps correctly recover hypotheses from compressed representations
- **Label-Space Reduction**: Technique to reduce multiclass problem to binary; essential for the MW approach
  - Quick check: Ensure that the reduced problem preserves the original learning objective

## Architecture Onboarding

**Component map**: Cover Construction (CC) -> SCSR booster -> Multiplicative Weights (MW) -> LSCS -> Final Classifier

**Critical path**: The MAPL algorithm follows a 3-stage pipeline: (1) Construct finite proxy class $F$ via SCSR, (2) Reduce label space using MW algorithm, (3) Learn final classifier using LSCS

**Design tradeoffs**: The approach trades computational complexity (NP-hard graph orientation) for theoretical sample complexity bounds. The use of online learning techniques enables label-space reduction but requires careful tuning of step sizes and adaptive rewards.

**Failure signatures**: 
- Empty or insufficient size finite cover indicates incorrect hypothesis class structure handling
- High regret in MW algorithm suggests poor step size tuning or incorrect reward function implementation
- Final classifier failing to achieve low excess risk indicates insufficient richness in restricted class

**First experiments**:
1. Implement cover construction for axis-aligned rectangles and verify proxy class size
2. Simulate MW-based label-space reduction on synthetic data and measure regret decay
3. Measure empirical sample complexity scaling with $\varepsilon$ for known DS/Nat dimensions

## Open Questions the Paper Calls Out
None

## Limitations
- One-Inclusion Graph orientation is NP-hard to compute for arbitrary hypothesis classes
- Constants hidden in asymptotic notation significantly affect practical sample complexity
- Theoretical existence results don't provide explicit construction methods for interesting hypothesis classes

## Confidence
- **High confidence**: Theoretical separation between DS and Natarajan dimensions is well-supported
- **Medium confidence**: Tightness of bound up to $\sqrt{DS}$ factor relies on existing lower bounds
- **Medium confidence**: Practical utility depends on efficiency of cover construction for specific classes

## Next Checks
1. Construct explicit cover for simple hypothesis class and verify proxy class size matches theoretical bounds
2. Simulate label-space reduction and verify regret scales as $O(\sqrt{T \log |F|})$
3. Empirically measure sample complexity scaling with $\varepsilon$ and compare to theoretical prediction $\tilde{O}(DS^{1.5}/\varepsilon + Nat/\varepsilon^2)$