---
ver: rpa2
title: 'PiCME: Pipeline for Contrastive Modality Evaluation and Encoding in the MIMIC
  Dataset'
arxiv_id: '2507.03165'
source_url: https://arxiv.org/abs/2507.03165
tags:
- contrastive
- modalities
- modality
- learning
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PiCME, a systematic pipeline to evaluate\
  \ and encode multimodal clinical data from MIMIC. It compares five clinical modalities\u2014\
  text, images, time-series, and structured data\u2014across all combinations in both\
  \ contrastive and supervised learning settings."
---

# PiCME: Pipeline for Contrastive Modality Evaluation and Encoding in the MIMIC Dataset

## Quick Facts
- **arXiv ID:** 2507.03165
- **Source URL:** https://arxiv.org/abs/2507.03165
- **Reference count:** 40
- **Primary result:** PiCME evaluates 5 clinical modalities; best performance with 3 (text, images, demographics); mLSTM improves 5-modality AUROC by 3.74% and AUPRC by 11.0%.

## Executive Summary
PiCME introduces a systematic pipeline to evaluate and encode multimodal clinical data from MIMIC, comparing five modalities—text, images, time-series, and structured data—across all combinations in both contrastive and supervised learning settings. The authors show that three modalities (discharge summaries, chest X-rays, demographics) yield peak performance, with contrastive learning often matching or outperforming supervised baselines. Beyond three modalities, performance plateaus due to integration noise. To address this, they propose the Modality-Gated LSTM, which uses contrastively learned modality weights to dynamically reweight inputs during supervised fine-tuning, improving AUROC from 73.19% to 76.93% and AUPRC from 51.27% to 62.26% in five-modality settings. The study also validates modality importance using attribution methods and evaluates fairness across demographic subgroups, offering actionable insights into modality selection, model design, and equitable clinical prediction.

## Method Summary
PiCME is a multimodal evaluation pipeline for the MIMIC dataset, focusing on binary in-hospital mortality (IHM) and 25-label phenotype classification. It uses five modalities: discharge summaries, radiology reports, chest X-rays, demographics, and time-series ICU data. Encoders include ClinicalBERT+LoRA for text, ResNet for images, LSTM for time-series, and MLP for demographics. Contrastive pre-training uses InfoNCE (for 2 modalities) or One-Versus-Others (OvO) loss with learnable modality weights (λ) for 3+ modalities. Fine-tuning employs three strategies: frozen contrastive embeddings with MLP, fully supervised from scratch, and the Modality-Gated LSTM (mLSTM), which uses contrastive λ to scale cell-state updates. Training uses LR 1e-2 to 1e-6, batch sizes 16–128, max 75 epochs, and weighted BCE or categorical CE. Hardware: 1× RTX 3090; pre-training 3–10h; fine-tuning 2–6.5h.

## Key Results
- Peak performance with three modalities: discharge summaries, chest X-rays, and demographics.
- Contrastive learning matches or exceeds supervised baselines for top modality sets.
- Modality-Gated LSTM improves 5-modality AUROC by 3.74% (73.19% → 76.93%) and AUPRC by 11.0% (51.27% → 62.26%).
- Time-series integration degrades performance; attribution analysis confirms low importance.
- Fairness analysis reveals high variance in small demographic subgroups.

## Why This Works (Mechanism)
PiCME works by leveraging contrastive learning to align embeddings across modalities, enabling robust representation learning without extensive labeled data. The One-Versus-Others (OvO) loss with learnable modality weights (λ) allows the model to dynamically emphasize the most informative modalities during pre-training. The Modality-Gated LSTM (mLSTM) then uses these weights to scale cell-state updates, effectively gating the influence of each modality during supervised fine-tuning. This approach addresses the challenge of integrating heterogeneous clinical data by learning to prioritize relevant information, as validated by attribution methods like Integrated Gradients. The pipeline's systematic evaluation across modality combinations provides actionable insights into optimal data usage for clinical prediction tasks.

## Foundational Learning
- **Contrastive Learning:** Why needed? Aligns embeddings across modalities without requiring extensive labeled data. Quick check: Compare cosine similarity alignment curves for different modality sets.
- **One-Versus-Others (OvO) Loss:** Why needed? Extends contrastive learning to more than two modalities by learning modality-specific weights. Quick check: Verify λ values sum to 1 and align with attribution scores.
- **Modality-Gated LSTM (mLSTM):** Why needed? Dynamically reweights modality contributions during fine-tuning using pre-trained λ. Quick check: Compare performance with and without mLSTM in 5-modality settings.
- **Integrated Gradients (IG):** Why needed? Quantifies modality importance by attributing predictions to input features. Quick check: Ensure IG scores align with ablation results.

## Architecture Onboarding

**Component Map:** ClinicalBERT+LoRA -> ResNet -> LSTM -> MLP -> OvO Contrastive Loss -> mLSTM -> MLP Head -> Prediction

**Critical Path:** Encoders (text, images, time-series, demographics) → Contrastive Pre-training (OvO loss) → Modality-Gated LSTM (mLSTM) → Fine-tuning Head → Prediction

**Design Tradeoffs:** Contrastive learning reduces labeling needs but requires careful modality selection; mLSTM improves integration but adds complexity; time-series integration often degrades performance due to noise.

**Failure Signatures:** Performance plateau beyond 3 modalities; high variance in small demographic subgroups; time-series integration fails due to low alignment.

**First Experiments:**
1. Re-implement encoders and contrastive pre-training for top-performing modality sets (TD+I+D for IHM; TR+TD+D for phenotypes).
2. Implement and compare three fine-tuning strategies: frozen encoders + MLP, full fine-tune, and mLSTM.
3. Conduct ablation studies to isolate the impact of modality selection and preprocessing on alignment and performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Missing critical hyperparameters (embedding dimensions, LoRA configuration, exact preprocessing details).
- Time-series integration often degrades performance due to noise and low alignment.
- Fairness analysis limited by small subgroup sizes and lack of statistical corrections for multiple comparisons.

## Confidence

**High:** Core claim that 3 modalities (TD, I, D) perform best; contrastive learning matches or exceeds supervised baselines for these modalities; mLSTM improves performance via learned modality weights.

**Medium:** Exact performance gains from mLSTM (AUROC +3.74%, AUPRC +11.0%) due to missing hyperparameters and preprocessing details; fairness conclusions due to small subgroup sizes and unreported statistical corrections.

**Low:** Reproducibility of time-series integration and full ablation on all 5 modalities without exact preprocessing and embedding specifications.

## Next Checks
1. Re-implement the pipeline with exact embedding dimensions and time-series preprocessing as specified in the code release (when available).
2. Conduct ablation studies to isolate the impact of time-series preprocessing and modality filtering on alignment and performance.
3. Validate fairness findings with statistical corrections for multiple comparisons and larger subgroup sample sizes.