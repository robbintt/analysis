---
ver: rpa2
title: 'Grokking and Generalization Collapse: Insights from \texttt{HTSR} theory'
arxiv_id: '2506.04434'
source_url: https://arxiv.org/abs/2506.04434
tags:
- grokking
- weight
- training
- layer
- htsr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the grokking phenomenon in neural networks,\
  \ extending training beyond typical limits to reveal a novel late-stage generalization\
  \ collapse called \"anti-grokking.\" The authors employ Heavy-Tailed Self-Regularization\
  \ (HTSR) theory through the WeightWatcher tool to analyze the spectral properties\
  \ of layer weight matrices. The primary finding is that the HTSR metric \u03B1,\
  \ which measures heavy-tailed power-law exponents in the spectral density, uniquely\
  \ tracks all three phases of grokking: pre-grokking, grokking, and anti-grokking."
---

# Grokking and Generalization Collapse: Insights from \texttt{HTSR} theory

## Quick Facts
- arXiv ID: 2506.04434
- Source URL: https://arxiv.org/abs/2506.04434
- Authors: Hari K. Prakash; Charles H. Martin
- Reference count: 23
- One-line primary result: HTSR metric α uniquely tracks all three phases of grokking including late-stage generalization collapse (anti-grokking), outperforming standard metrics.

## Executive Summary
This paper extends grokking analysis beyond typical training limits, revealing a novel late-stage generalization collapse called "anti-grokking." Using Heavy-Tailed Self-Regularization (HTSR) theory through WeightWatcher, the authors demonstrate that the power-law exponent α in layer weight matrix spectra tracks all three grokking phases: pre-grokking, grokking, and anti-grokking. The key finding is that α decreases toward optimal value ~2 during grokking and further drops below 2 during anti-grokking, providing early warning of impending generalization collapse. This metric outperforms competing measures like activation sparsity, weight entropy, circuit complexity, and l2 weight norms.

## Method Summary
The study employs a 3-layer MLP (784→200→200→10) trained on a 1k subset of MNIST using AdamW optimizer with MSE loss. Weight decay is set to 0.0 to induce anti-grokking, and training runs for 10^7 steps. Layer-wise HTSR metric α is extracted using WeightWatcher v0.7.5.5 to analyze the spectral properties of weight matrices. Correlation traps are detected by comparing randomized weight spectra against Marchenko-Pastur distribution limits. The analysis tracks how α evolves across the three grokking phases and correlates with test accuracy.

## Key Results
- HTSR metric α alone delineates all three grokking phases, while competing metrics detect only the first two
- Anti-grokking phase shows α dropping below 2 and appearance of "correlation traps" in randomized weight matrices
- Correlation traps signal overfitting and can be detected without access to test data
- Individual layer dynamics (not just average) are crucial for understanding grokking progression

## Why This Works (Mechanism)

### Mechanism 1: HTSR Alpha as a Phase Discriminator
The HTSR power-law exponent (α) measures heavy-tailed properties in the eigenvalue density of weight matrices. The paper suggests α ≈ 2 is a "universal target" for generalization, while α < 2 signals overfitting and collapse. As networks train, weight matrix spectra shift from random (Gaussian-like) to heavy-tailed, with α tracking this transition across all three grokking phases.

### Mechanism 2: Correlation Traps Signaling Collapse
Anti-grokking occurs when layers overfit and develop anomalous rank-one structures in weight matrices. When weights are randomized (W → W_rand), these structures persist as outlier eigenvalues violating the Marchenko-Pastur distribution expected of random matrices. These "correlation traps" indicate pathological overfitting.

### Mechanism 3: Layer-wise Convergence Heterogeneity
Pre-grokking delay occurs because only subset of layers learn useful structure while others remain effectively random. Generalization requires all "important" layers to converge to the "fat-tailed" regime (α ≲ 4). In pre-grokking, specific layers may stall at high α (random-like), creating bottlenecks despite low training loss.

## Foundational Learning

- **Concept: Grokking Phases**
  - Why needed here: The paper redefines grokking from 2-phase to 3-phase process (Pre → Grokking → Anti-Grokking)
  - Quick check question: Can a model have 100% training accuracy but 0% test accuracy? (Answer: Yes, in pre-grokking/anti-grokking)

- **Concept: Empirical Spectral Density (ESD) & Power Laws**
  - Why needed here: HTSR theory relies on analyzing histogram of eigenvalues (ESD) of weight correlation matrices
  - Quick check question: Does a well-trained layer have "light" or "heavy" tail in its eigenvalue distribution? (Answer: Heavy/Fat-tailed, α ≈ 2)

- **Concept: Random Matrix Theory (Marchenko-Pastur)**
  - Why needed here: Serves as "null hypothesis" for random weights; correlation trap detection compares randomized weights against MP distribution
  - Quick check question: If weight matrix is truly random, where do most eigenvalues lie relative to bulk edge λ⁺? (Answer: Inside the bulk)

## Architecture Onboarding

- **Component map:** 784 → 200 → 200 → 10 (3-layer MLP, ReLU activations)
- **Critical path:**
  1. Configure: Set WD=0 (critical for observing anti-grokking)
  2. Train: Run for 10^7 steps (collapse happens late)
  3. Monitor: Track layer-wise α and test accuracy
  4. Detect: Check for α < 2 and Correlation Traps via WeightWatcher on W_rand

- **Design tradeoffs:**
  - WD=0 vs WD>0: WD=0 allows observation of "pure" dynamics and collapse phase; WD>0 stabilizes α ≈ 2 and prevents collapse but hides phenomenon
  - Layer Analysis vs Average: FC2 collapsed before FC1; monitoring only average α might obscure specific layer failures

- **Failure signatures:**
  - False Positive: α dips below 2 during grokking phase (paper claims α ≈ 2 is stable/good, but < 2 is bad)
  - Metric Failure: Activation Sparsity continues increasing through collapse, failing to distinguish "good" grokking from "bad" anti-grokking
  - KS Test Failure: High p-value (> 0.05) on MP fit of W_rand implies no Correlation Traps present (model not in anti-grokking)

- **First 3 experiments:**
  1. Baseline Reproduction: Train 3-layer MLP on 1k MNIST with WD=0 for 10^6 steps to confirm grokking transition corresponds to α → 2
  2. Stress Test: Extend training to 10^7 steps to verify "anti-grokking" collapse and confirm α < 2 in layer FC2
  3. Trap Validation: Run WeightWatcher on final collapsed model to generate ESD of W_rand and visually confirm eigenvalues to right of MP bulk edge (λ_trap ≫ λ⁺)

## Open Questions the Paper Calls Out

### Open Question 1
Can α-guided adaptive training strategies (e.g., differentiable regularizers or loss terms encouraging convergence toward α ≈ 2) prevent anti-grokking or accelerate grokking? The paper demonstrates α's diagnostic value but does not implement or test intervention strategies that actively steer α during training.

### Open Question 2
Do the three-phase grokking dynamics and HTSR α trajectories generalize to other architectures (CNNs, Transformers), datasets beyond MNIST, and different optimizers? The empirical findings are limited to a 3-layer MLP on a 1k-sample MNIST subset with AdamW optimizer.

### Open Question 3
What is the precise mechanistic relationship between correlation traps and overfitting in the anti-grokking phase? The paper identifies correlation traps as a signal but does not establish a causal mechanism linking them to generalization collapse.

## Limitations
- Claims about anti-grokking detection rely heavily on WeightWatcher's specific implementation details not fully specified in main text
- Relationship between α < 2 and generalization collapse lacks theoretical grounding for why this threshold is universal
- "Correlation traps" mechanism assumes deviations from Marchenko-Pastur distributions specifically indicate pathological overfitting rather than benign feature learning

## Confidence
- **High Confidence:** Empirical observation that α tracks grokking phases (pre → grokking → anti-grokking) is well-supported by experimental data
- **Medium Confidence:** Claim that α < 2 specifically signals anti-grokking rather than just any form of overfitting, though supported by experiments, needs broader validation
- **Low Confidence:** Mechanistic explanation of correlation traps as root cause of generalization collapse remains correlational rather than causal

## Next Checks
1. Cross-Architecture Validation: Test whether α < 2 threshold reliably predicts anti-grokking on architectures beyond 3-layer MLPs (e.g., CNNs on CIFAR-10)
2. Temporal Robustness: Verify that correlation traps detected at α < 2 persist through collapse phase and correlate with recovery when regularization is reintroduced
3. Alternative Explanations: Test whether simpler metrics (e.g., Fisher Information, gradient norms) can predict anti-grokking as effectively as HTSR α metric