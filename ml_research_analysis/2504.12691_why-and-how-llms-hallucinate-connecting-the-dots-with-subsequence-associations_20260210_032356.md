---
ver: rpa2
title: 'Why and How LLMs Hallucinate: Connecting the Dots with Subsequence Associations'
arxiv_id: '2504.12691'
source_url: https://arxiv.org/abs/2504.12691
tags:
- subsequence
- associations
- subsequences
- input
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unified framework based on subsequence
  associations to analyze hallucinations in large language models (LLMs). The key
  insight is that hallucinations occur when dominant hallucinatory associations outweigh
  faithful ones, which can be traced back to specific subsequences in the input.
---

# Why and How LLMs Hallucinate: Connecting the Dots with Subsequence Associations

## Quick Facts
- arXiv ID: 2504.12691
- Source URL: https://arxiv.org/abs/2504.12691
- Authors: Yiyou Sun; Yu Gai; Lijie Chen; Abhilasha Ravichander; Yejin Choi; Dawn Song
- Reference count: 40
- Key outcome: Introduces a unified framework based on subsequence associations to analyze hallucinations in LLMs, showing that hallucinations occur when dominant hallucinatory associations outweigh faithful ones, and proposes a tracing algorithm to identify causal subsequences.

## Executive Summary
This paper presents a novel theoretical framework for understanding hallucinations in large language models through the lens of subsequence associations. The authors demonstrate that decoder-only transformers encode associations between subsequences in their input, and hallucinations occur when these associations become dominated by hallucinatory patterns rather than faithful ones. The framework introduces a tracing algorithm that can identify the specific subsequences responsible for hallucinations by analyzing model behavior across diverse, randomized input contexts.

The work bridges the gap between abstract notions of "faithfulness" and concrete mechanisms in transformer architectures, providing both theoretical foundations and practical tools for analyzing and tracing hallucinations. Experimental results show that the proposed tracing method outperforms traditional attribution techniques in identifying hallucination causes, while also revealing connections between identified subsequences and evidence present in the training corpus.

## Method Summary
The authors propose a unified framework for analyzing hallucinations based on subsequence associations in transformer models. They theoretically establish that decoder-only transformers encode associations between subsequences during training, creating a mapping between input patterns and output behaviors. The key insight is that hallucinations occur when dominant hallucinatory associations outweigh faithful ones in the model's learned representations. To trace these associations, the authors develop an algorithm that systematically varies input contexts across diverse samples, analyzing how hallucination probabilities change in response to specific subsequence manipulations. This approach allows identification of causal subsequences that trigger hallucinatory outputs, providing a concrete mechanism for understanding and diagnosing hallucinations beyond surface-level pattern matching.

## Key Results
- The tracing algorithm successfully identifies causal subsequences responsible for hallucinations, outperforming traditional attribution techniques on benchmark datasets.
- Identified subsequences show evidence of being supported by patterns in the training corpus, validating the theoretical framework's connection to actual model learning dynamics.
- The framework provides a systematic perspective for understanding hallucinations as emergent properties of subsequence associations rather than isolated model failures.

## Why This Works (Mechanism)
The framework works by leveraging the fundamental property of transformers to encode associations between subsequences during training. When models learn from data, they create weighted connections between input patterns and their typical continuations. Hallucinations emerge when these learned associations become dominated by hallucinatory patterns - for instance, when a model strongly associates a particular question format with a confident but incorrect answer pattern. The tracing algorithm exploits this by systematically varying input contexts to isolate which specific subsequences trigger the dominant hallucinatory associations, effectively mapping the causal pathways from input to hallucination.

## Foundational Learning

**Subsequence Associations**: The weighted relationships between input subsequences and their typical continuations in transformer models. Why needed: Forms the theoretical foundation for understanding how models develop preferences for certain outputs. Quick check: Can be verified by analyzing attention patterns and weight matrices for specific input-output pairs.

**Decoder-only Transformer Architecture**: Models that generate text autoregressively without bidirectional context encoding. Why needed: The theoretical analysis specifically applies to this architecture's attention and positional encoding mechanisms. Quick check: Confirm through architectural diagrams and attention mechanism descriptions.

**Faithfulness vs. Hallucination**: The distinction between outputs grounded in input context versus those generated from dominant but incorrect associations. Why needed: Provides the conceptual framework for categorizing and analyzing different types of model outputs. Quick check: Can be validated through human evaluation of output-context relationships.

**Causal Tracing**: The process of identifying which specific input components causally influence particular outputs. Why needed: Enables practical diagnosis of hallucination sources rather than just detection. Quick check: Verified through ablation studies where removing traced subsequences eliminates hallucinations.

## Architecture Onboarding

**Component Map**: Input text -> Subsequence tokenization -> Attention layers -> Subsequence association encoding -> Output generation. The key components are the attention mechanisms that learn to associate input subsequences with output patterns.

**Critical Path**: Input subsequences -> Attention weight computation -> Association strength calculation -> Output probability distribution -> Hallucination occurrence. The critical path flows through the attention layers where subsequence associations are encoded and weighted.

**Design Tradeoffs**: The framework trades computational complexity for explanatory power - tracing requires generating diverse contexts and analyzing multiple model runs, but provides deeper insight into hallucination mechanisms compared to black-box detection methods.

**Failure Signatures**: Hallucinations manifest as dominant outputs that persist across diverse contexts despite contradictory evidence, indicating strong hallucinatory associations that override faithful ones. The tracing algorithm identifies these through inconsistent probability shifts when specific subsequences are varied.

**3 First Experiments**:
1. Apply the tracing algorithm to a known hallucination case (e.g., factual errors in question answering) and verify identified subsequences match human-identified problematic patterns.
2. Test the framework across different model sizes to determine whether subsequence association strength scales predictably with model capacity.
3. Conduct ablation studies where identified causal subsequences are removed or modified to confirm their direct role in triggering hallucinations.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes idealized conditions that may not fully capture real-world LLM training dynamics and data distribution complexities.
- Empirical validation is primarily conducted on decoder-only transformers, limiting generalizability to other model architectures like encoder-decoder or MLM-based models.
- The tracing algorithm's effectiveness depends heavily on the diversity and quality of randomized input contexts, with potential limitations when such contexts are scarce or biased.

## Confidence

**High Confidence**: The core theoretical insight about subsequence associations in decoder-only transformers is mathematically well-founded. Experimental results demonstrating superior performance of the tracing algorithm over traditional attribution methods are robust and reproducible.

**Medium Confidence**: The conceptual framework linking dominant hallucinatory associations to model failures is sound but relies on specific experimental conditions. Claims about training corpus support for identified subsequences are supported but could benefit from more systematic verification.

**Low Confidence**: Claims about the framework providing a systematic perspective for practical hallucination mitigation are suggestive but not thoroughly validated. The real-world utility for production systems remains to be demonstrated.

## Next Checks

1. Test the tracing algorithm's effectiveness across multiple model architectures (encoder-decoder, MLM-based) and domains (code generation, scientific reasoning) to assess generalizability.

2. Conduct a systematic analysis of how identified subsequences relate to actual training data patterns through comprehensive data attribution studies.

3. Evaluate the framework's performance on long-form generation tasks where hallucinations may accumulate over extended contexts, testing whether subsequence associations remain detectable and traceable.