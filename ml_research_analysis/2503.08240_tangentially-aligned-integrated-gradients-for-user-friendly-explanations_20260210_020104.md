---
ver: rpa2
title: Tangentially Aligned Integrated Gradients for User-Friendly Explanations
arxiv_id: '2503.08240'
source_url: https://arxiv.org/abs/2503.08240
tags:
- uni00000013
- uni00000011
- base-point
- gradient
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of choosing base-points in integrated
  gradients (IG) explanations for neural networks. IG explanations depend heavily
  on the choice of base-point, but existing heuristics for selecting base-points often
  produce explanations that are not perceptually meaningful or aligned with the data
  manifold.
---

# Tangentially Aligned Integrated Gradients for User-Friendly Explanations

## Quick Facts
- arXiv ID: 2503.08240
- Source URL: https://arxiv.org/abs/2503.08240
- Reference count: 36
- This paper proposes a method to choose base-points for Integrated Gradients that maximizes tangential alignment with the data manifold, producing more meaningful explanations.

## Executive Summary
This paper addresses a fundamental problem in interpretability: choosing appropriate base-points for Integrated Gradients (IG) explanations. IG explanations are highly sensitive to base-point selection, and common choices like zero or maximum distance often produce explanations that are not perceptually meaningful or aligned with the data manifold. The authors propose a novel approach that chooses base-points to maximize the tangential alignment of explanations - how much the explanation lies in the tangent space of the data manifold at the input point. This is achieved by approximating the tangent and normal spaces using a convolutional autoencoder, then optimizing base-point selection via gradient descent.

## Method Summary
The method involves training a convolutional autoencoder to approximate the data manifold, then using the decoder Jacobian to estimate tangent and normal spaces at each point. Base-points are chosen by optimizing to minimize the component of the explanation in the normal space, which is formulated as a gradient descent problem. The approach is validated across four image datasets (MNIST, Fashion-MNIST, CIFAR10, and FER2013) and compared against common base-point choices and gradient explainability models. The key metric is tangential alignment score, measuring the fraction of the explanation that lies in the tangent space.

## Key Results
- The proposed method consistently achieves tangential alignment scores > 0.91 across all datasets
- Common base-point choices (zero, uniform, Gaussian) often produce explanations close to random noise, particularly on complex datasets like CIFAR10
- The method outperforms three gradient explainability models (Gradient, Smooth Grad, Input*Gradient) in producing tangentially aligned explanations
- The zero base-point performs poorly across all datasets, contradicting its common usage in practice

## Why This Works (Mechanism)
Integrated Gradients explanations depend on the path from a base-point to the input. If the base-point is chosen outside the data manifold (like zero), the path crosses regions that are not meaningful for classification, resulting in noise-like attributions. By choosing base-points that maximize tangential alignment, the method ensures the explanation path stays close to the data manifold, capturing only features relevant to the classification task.

## Foundational Learning
- Data manifold approximation: Using autoencoders to learn low-dimensional structure in high-dimensional data. Needed to estimate tangent spaces for complex datasets.
- Tangent space estimation: Computing the Jacobian of the decoder to approximate local tangent directions. Required for measuring alignment of explanations.
- Null space computation: Finding the orthogonal complement of the tangent space to define the normal space. Used to measure non-alignment.
- Gradient descent optimization: Iteratively refining base-point selection. Core mechanism for finding optimal base-points.
- Gershgorin circle theorem: Mathematical tool for proving sufficient conditions for tangential alignment. Provides theoretical foundation.

## Architecture Onboarding
Component map: Input -> CNN Classifier -> Integrated Gradients -> Autoencoder Decoder Jacobian -> Tangent Space -> Optimization -> Base-point

Critical path: The optimization loop that computes tangent space at input, initializes base-point, performs gradient descent to minimize normal component, then computes final IG explanation.

Design tradeoffs: The method trades computational cost (additional optimization step) for explanation quality. The autoencoder quality directly impacts tangent space approximation accuracy.

Failure signatures: Low tangential alignment scores (< 0.5) indicate poor manifold approximation or optimization failure. High reconstruction error in autoencoder suggests the tangent space basis is unreliable.

First experiments:
1. Verify autoencoder reconstruction quality on MNIST before proceeding with tangent space computation
2. Test tangent space estimation on a single CIFAR10 image and visualize normal/tangent components
3. Implement base-point optimization with fixed learning rate and verify it improves alignment over zero base-point

## Open Questions the Paper Calls Out
- Impact of Gaussian smoothing parameter Ïƒ on tangential alignment remains unexplored
- Effect of tangent space dimension n on alignment performance is unknown
- Potential for adaptive optimization settings to improve base-point approximation
- Need for broader theoretical conditions guaranteeing tangential explanations

## Limitations
- Autoencoder quality directly limits tangent space approximation accuracy
- Fixed learning rate and iteration count may not be optimal for all input points
- Method requires training an additional autoencoder model
- Computational cost of base-point optimization may be prohibitive for real-time applications

## Confidence
- Core claim (method improves alignment): High
- Theoretical framework: Medium
- Experimental validation: Medium-High

## Next Checks
1. Implement the autoencoder and classifier architecture with reasonable defaults and verify reconstruction quality on MNIST
2. Compute tangent space approximation via decoder Jacobian and validate the alignment scores on a small test set
3. Implement the base-point optimization with a fixed learning rate and iteration count, then compare tangential alignment against zero and uniform base-points