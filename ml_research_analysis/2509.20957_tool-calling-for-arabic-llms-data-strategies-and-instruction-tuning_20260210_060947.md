---
ver: rpa2
title: 'Tool Calling for Arabic LLMs: Data Strategies and Instruction Tuning'
arxiv_id: '2509.20957'
source_url: https://arxiv.org/abs/2509.20957
tags:
- tool
- arabic
- laminitial
- alefisolated
- aleffinal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether Arabic Large Language Models (LLMs)
  benefit from in-language tool-calling training data versus relying on cross-lingual
  transfer, and the value of general-purpose instruction tuning and tool-specific
  fine-tuning. Experiments with Fanar Arabic LLM across five configurations show that
  bilingual tool-calling data and tool-specific fine-tuning significantly improve
  performance, especially for previously unseen tools.
---

# Tool Calling for Arabic LLMs: Data Strategies and Instruction Tuning

## Quick Facts
- arXiv ID: 2509.20957
- Source URL: https://arxiv.org/abs/2509.20957
- Authors: Asim Ersoy; Enes Altinisik; Husrev Taha Sencar; Kareem Darwish
- Reference count: 7
- Primary result: Arabic LLMs benefit significantly from bilingual tool-calling data and tool-specific fine-tuning, achieving near-perfect ArgA (>0.99) compared to lower cross-lingual transfer performance (e.g., ArgA 0.58 for CustomTools Arabic).

## Executive Summary
This paper investigates how Arabic Large Language Models (LLMs) can effectively learn tool-calling capabilities, focusing on whether Arabic-specific training data is necessary or if cross-lingual transfer from English suffices. Through experiments with the Fanar Arabic LLM across five configurations, the study demonstrates that bilingual tool-calling datasets and fine-tuning on custom tools yield substantial performance gains. The findings reveal that while cross-lingual transfer is effective for general function selection, Arabic-specific fine-tuning is crucial for high accuracy in argument population, especially for domain-specific tools. The work introduces the Argument Population Accuracy (ArgA) metric to evaluate end-to-end correctness of tool calls.

## Method Summary
The study fine-tunes the Fanar Arabic LLM using LLaMA-Factory across five experimental configurations to assess different data strategies for tool calling. Datasets include translated Glaive and xLAM (96K samples), CustomTools (synthetic, 8 tools), and IslamicRAGTool (real API logs). Models are evaluated on both English and Arabic test sets using precision, recall, and the newly introduced ArgA metric, which requires exact matches of function names and all argument values. Training uses a cosine learning rate schedule with peak 5.0×10⁻⁷, batch size 640, and data mixing strategies to isolate the effects of language alignment and domain specificity.

## Key Results
- Cross-lingual transfer alone yields lower ArgA (e.g., 0.58 for CustomTools Arabic) compared to bilingual fine-tuning.
- General SFT improves function-calling recall but reduces precision for non-function-calling cases, suggesting over-eagerness to generate tool calls.
- Tool-specific fine-tuning on custom tools achieves near-perfect ArgA (>0.99), demonstrating the effectiveness of in-domain examples for argument formatting.
- Arabic-specific fine-tuning significantly improves performance for domain-specific tools compared to relying solely on English data.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual transfer of tool-calling capabilities is effective for function selection but incurs a significant penalty in argument population accuracy, especially for domain-specific tools.
- Mechanism: An LLM's pre-trained multilingual representations allow it to map a user query in one language (e.g., Arabic) to tool definitions learned primarily in another (e.g., English), enabling correct function selection. However, the model may fail to correctly translate the extracted arguments into the expected format or language, leading to a drop in end-to-end correctness.
- Core assumption: The base LLM has sufficiently robust cross-lingual alignment to handle general intent detection.
- Evidence anchors:
  - [abstract] "...while cross-lingual transfer is effective for general tool-calling, Arabic-specific fine-tuning significantly improves performance, particularly for domain-specific tools."
  - [section 5.1] "As for argument population accuracy (ArgA), the results show that a mismatch in the language of training versus testing data adversely affects the ability of the model to guess the correct arguments... An even sharper decline was observed for CustomTools and IslamicRAGTool..."
  - [corpus] Related work ("Arabic Prompts with English Tools: A Benchmark") confirms cross-lingual tool-calling is an active area of inquiry.
- Break condition: The mechanism fails if the base model lacks sufficient semantic alignment between languages for the concepts used in queries and tools.

### Mechanism 2
- Claim: General-purpose Supervised Fine-Tuning (SFT) improves a model's ability to identify when a tool should be called (recall) but can degrade its ability to correctly classify cases where no tool is needed (precision).
- Mechanism: General SFT data enhances the model's generative capabilities and eagerness to be helpful, which can create a bias toward generating a response (like a tool call) even in ambiguous cases where one is not appropriate. This increases recall for function-calling but can decrease precision for the "no-tool-call" class.
- Core assumption: The general SFT data does not contain a sufficient proportion of negative examples to teach proper restraint.
- Evidence anchors:
  - [abstract] "...general SFT improving function calling but reducing classification precision."
  - [section 5.3] "For function-calling cases, the General SFT data produces substantial improvements in recall performance... However, non-function-calling cases show a concerning decline in performance after applying general SFT data... The performance decline in non-function-calling (non-FC) cases is likely due to the supervised fine-tuning (SFT) data enhancing the model’s generative abilities while diminishing its classification precision..."
  - [corpus] Corpus signals were weak or missing for this specific trade-off.
- Break condition: The mechanism will be amplified if the SFT dataset is heavily skewed toward action-taking examples, leading to a high rate of false positive tool calls.

### Mechanism 3
- Claim: Fine-tuning an LLM on a specific set of high-priority tools yields dramatic performance gains, achieving near-perfect accuracy by teaching the model the exact argument formatting and syntactic nuances required.
- Mechanism: Tool-specific fine-tuning acts as a form of "memorization" for the target tool schema. The model learns not just which tool to call, but the precise syntactic format for its arguments (e.g., avoiding paraphrasing), which is the primary source of end-to-end error in less specialized models.
- Core assumption: The fine-tuning data is high-quality and contains correctly formatted examples.
- Evidence anchors:
  - [abstract] "Most notably, tool-specific fine-tuning on custom tools achieves near-perfect performance..."
  - [section 5.5] "...Paraphrasing Variance (P), accounting for 50.2% of all argument errors... The most effective approach, demonstrated in our experiments, is providing direct, in-domain examples through tool-specific fine-tuning."
  - [corpus] "Trajectory2Task: Training Robust Tool-Calling Agents..." supports the value of high-quality, verifiable data for robust agent performance.
- Break condition: Performance will not generalize if the tool-specific training data lacks diversity or contains systematic formatting errors.

## Foundational Learning

- Concept: **Supervised Fine-Tuning (SFT)**
  - Why needed here: SFT is the primary method used in the paper to adapt a general-purpose LLM into a tool-calling agent, teaching it to interpret schemas and generate structured JSON.
  - Quick check question: How might the composition of an SFT dataset (e.g., proportion of positive vs. negative examples) influence a model's tendency to call tools?

- Concept: **Cross-Lingual Transfer**
  - Why needed here: A central question of the paper is how well tool-calling ability, often learned from English data, transfers to Arabic without direct training.
  - Quick check question: If a model trained on English tool data is given an Arabic query, it might select the correct tool. What is a likely failure mode when it tries to populate the arguments?

- Concept: **Argument Population Accuracy (ArgA)**
  - Why needed here: This metric, introduced in the paper, provides a stricter and more practical measure of success than simple function name matching, evaluating the end-to-end correctness of the call.
  - Quick check question: Why is achieving a high ArgA score more challenging than achieving high precision on function name detection alone?

## Architecture Onboarding

- Component map: Fanar-1-9B (Base LLM) -> LLaMA-Factory (SFT Training Pipeline) -> Glaive/xLAM (Translated Tool-Calling Datasets) + CustomTools (Synthetic) + IslamicRAGTool (Real API Logs) -> Evaluation Framework (Precision/Recall + ArgA)
- Critical path: The most critical path is the data preparation and fine-tuning pipeline. The final model's utility is directly determined by the language-alignment and domain-specificity of the data used for SFT. The evaluation results create a feedback loop to optimize this data mix.
- Design tradeoffs: A key tradeoff is between generalization and specificity: models trained on broad datasets (Glaive/xLAM) generalize better but perform poorly on custom tools. Another is between recall and precision: general SFT can make models over-eager to call tools, increasing recall but lowering precision on "no-tool" cases.
- Failure signatures:
    - **High Recall, Low Precision:** Model calls tools inappropriately (Signature of unbalanced SFT).
    - **Good Function Recall, Low ArgA:** Model selects the right tool but malforms the arguments (Signature of language mismatch or lack of tool-specific tuning).
    - **High in-domain, Low out-of-domain:** Model performs well on test splits from training data but fails on unseen custom tools (Signature of limited generalization).
- First 3 experiments:
  1.  **Establish a baseline:** Fine-tune a base model using only English tool-calling data and evaluate on both English and Arabic test sets to quantify pure cross-lingual transfer.
  2.  **Test in-language data:** Replicate the fine-tuning using only the translated Arabic data and compare performance against the English-only baseline.
  3.  **Integrate general SFT:** Start with an already instruction-tuned model, fine-tune it with tool-calling data, and analyze the impact on both function-calling recall and non-calling precision.

## Open Questions the Paper Calls Out

- **Does improved argument population accuracy in tool calling translate to successful task completion in real-world agentic systems?**
  - Basis in paper: [explicit] The authors state in the Limitations section that their evaluation "does not account for downstream utility or correctness of tool execution in real-world agentic systems."
  - Why unresolved: The current metrics (precision, recall, ArgA) measure syntactic correctness against ground truth but do not validate if the tool executes successfully or solves the user's actual problem.
  - What evidence would resolve it: A study measuring end-to-end success rates in a live deployment environment where tools are executed rather than just simulated.

- **Do these cross-lingual transfer and fine-tuning strategies generalize to languages with significantly different morphological structures?**
  - Basis in paper: [explicit] The authors note the study is limited to English and Arabic, and "additional languages with different morphological and syntactic properties may exhibit different transfer dynamics."
  - Why unresolved: Arabic and English have specific linguistic relationships; it remains unclear if the observed effectiveness of translated data holds for agglutinative or tonal languages.
  - What evidence would resolve it: Replicating the experimental setup (base vs. fine-tuned) on diverse language families (e.g., Turkic, Sino-Tibetan) to compare transfer gaps.

- **How can general-purpose instruction tuning be optimized to avoid inducing a bias toward unnecessary function calling?**
  - Basis in paper: [inferred] Section 5.3 observes that while general SFT improves recall, it causes a "concerning decline" in non-function-calling performance, likely because the data enhances generative abilities at the cost of classification precision.
  - Why unresolved: The paper identifies the trade-off but does not determine if specific data mixing ratios or training objectives can mitigate this "hallucination" of tools.
  - What evidence would resolve it: Ablation studies varying the ratio of general SFT to tool-calling data, or employing preference optimization to penalize unnecessary tool calls.

## Limitations
- The evaluation focuses on syntactic correctness (ArgA) and does not validate successful task completion in real-world agentic systems.
- The study is limited to English and Arabic; transfer dynamics for languages with significantly different morphological structures remain unexplored.
- The paper identifies a trade-off between recall and precision in general SFT but does not provide solutions to optimize instruction tuning to avoid over-eager tool calling.

## Confidence
- **High:** The experimental methodology is clearly defined with specific datasets, model configurations, and evaluation metrics. The results are reproducible given the specified hyperparameters and data preparation steps.
- **Medium:** Some critical details like exact training epochs, full CustomTools function definitions, and precise prompt formatting are missing, which could affect faithful reproduction.
- **Low:** The study's scope is limited to two languages and does not address real-world task completion or generalization to morphologically diverse languages.

## Next Checks
1. Verify the translation quality and data format of Glaive/xLAM datasets to ensure they match the paper's specifications.
2. Confirm the exact training epochs and steps used for each experimental configuration, as these are not specified.
3. Test the reproducibility of the ArgA metric calculation, including the normalization rules for dates, numbers, and whitespace.