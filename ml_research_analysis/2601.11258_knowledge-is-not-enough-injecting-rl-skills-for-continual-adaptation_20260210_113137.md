---
ver: rpa2
title: 'Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation'
arxiv_id: '2601.11258'
source_url: https://arxiv.org/abs/2601.11258
tags:
- knowledge
- skill
- uni00000011
- uni00000013
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of updating frozen knowledge
  in large language models without compromising their reasoning skills. The authors
  propose Parametric Skill Transfer (PaST), which exploits the orthogonality of parameter
  updates from supervised fine-tuning (SFT) and reinforcement learning (RL).
---

# Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation

## Quick Facts
- arXiv ID: 2601.11258
- Source URL: https://arxiv.org/abs/2601.11258
- Reference count: 40
- Primary result: PaST achieves up to 9.9-point accuracy gains on SQuAD, 8.0-point gains on LooGLE, and +10.3-point zero-shot transfer on ToolBench by exploiting orthogonality between SFT and RL parameter updates

## Executive Summary
This paper addresses the challenge of updating frozen knowledge in large language models without compromising their reasoning skills. The authors propose Parametric Skill Transfer (PaST), which exploits the orthogonality of parameter updates from supervised fine-tuning (SFT) and reinforcement learning (RL). By extracting a domain-agnostic skill vector from a source RL-trained model and linearly injecting it into a target model after lightweight SFT, PaST enables efficient knowledge adaptation without costly on-policy RL. Experiments on SQuAD show up to 9.9-point accuracy gains over state-of-the-art baselines, LooGLE demonstrates scalability to long contexts with 8.0-point gains, and ToolBench shows +10.3-point zero-shot transfer success across tool categories, confirming strong cross-domain skill transferability.

## Method Summary
PaST operates by first training a source model with SFT on domain-specific documents, then RL (GRPO/PPO) on task-specific trajectories. The skill vector is extracted via parameter subtraction (v_skill = θ_rl - θ_sft), isolating reasoning patterns from knowledge. This vector is then injected into a target model after its own SFT by simple linear addition (θ_final = θ_sft_target + λ·v_skill, with λ=1). The method leverages the empirical observation that SFT and RL updates occur in nearly orthogonal parameter subspaces, enabling modular composition of knowledge and skills without interference.

## Key Results
- SQuAD: Up to 9.9-point accuracy improvement over SEAL baseline with iterative skill extraction
- LooGLE: 8.0-point gains on long-context QA tasks, demonstrating scalability to 21k-token inputs
- ToolBench: +10.3-point zero-shot transfer success across 50 tool categories, validating cross-domain skill transferability

## Why This Works (Mechanism)

### Mechanism 1: Parameter Orthogonality Between Knowledge and Skills
The paper observes that SFT and RL updates occur in nearly orthogonal subspaces, with layer-wise cosine similarity consistently near-zero. This functional disentanglement prevents destructive interference when knowledge and skill signals are composed. The orthogonality assumption relies on quasi-isotropic input activations (facilitated by LayerNorm), making expected signal overlap approximate weight matrix inner product.

### Mechanism 2: Skill Vector Extraction via Parameter Subtraction
Subtracting SFT parameters from RL parameters isolates domain-agnostic reasoning patterns. This works because RL updates concentrate in sparse subnetworks, and the subtraction neutralizes domain-specific declarative patterns while retaining procedural execution logic. Iterative refinement across disjoint data subsets improves performance by forcing convergence toward content-invariant patterns.

### Mechanism 3: Post-hoc Linear Injection
Injecting the skill vector after target SFT preserves both new knowledge and transferred skills. Post-hoc composition grafts reasoning geometry onto a stable knowledge manifold, while pre-injection or sequential fine-tuning on RL parameters disrupts this alignment. The fixed λ=1 scaling coefficient is acknowledged as a simplification that may need tuning for different source-target gaps.

## Foundational Learning

- **Task Arithmetic (Ilharco et al., 2022)**: Needed because PaST builds on the concept that fine-tuning deltas are composable vectors in weight space. Quick check: Can you explain why adding two task vectors doesn't necessarily mean the model performs both tasks?

- **Reinforcement Learning Optimization Dynamics**: Needed to understand why RL updates concentrate in small subnetworks and generalize better than SFT under distribution shift. Quick check: Why might RL-trained models show less catastrophic forgetting than SFT-trained models?

- **Closed-Book QA / Parametric Knowledge Retrieval**: Needed to grasp the constraint that models must retrieve from weights, not context. Quick check: In a closed-book setting, what happens if the model has knowledge but lacks retrieval skills?

## Architecture Onboarding

- **Component map**: θ_base → SFT on C_S → θ_sft_S → RL on T_S → θ_rl_S → v_skill extraction → θ_base → SFT on C_T → θ_sft_T → θ_final = θ_sft_T + λ·v_skill

- **Critical path**: 1) Train source SFT model on documents, 2) Train source RL model on task trajectories, 3) Extract skill vector via parameter subtraction, 4) Train target SFT model on new documents, 5) Inject skill vector via single matrix addition

- **Design tradeoffs**: Iterative vs. single-round extraction (iterative prevents overfitting but requires K× more training), fixed scaling coefficient (λ=1 vs. adaptive tuning), RL algorithm choice (GRPO for QA, PPO for tool-use)

- **Failure signatures**: Sequential FT underperforms baseline (SFT on RL disrupts reasoning), single-round skill vector shows no improvement (overfit to source content), zero-shot transfer fails completely (skill vector contains domain-specific artifacts)

- **First 3 experiments**: 1) Replicate orthogonality check on 5 documents from any QA dataset, 2) Minimal PaST on SQuAD single-passage (train on 50 passages, extract skill, train target on 1 passage), 3) Cross-domain sanity check (extract from Movies, inject into Finance category)

## Open Questions the Paper Calls Out

- **Adaptive scaling coefficient**: Would an adaptive or learned scaling coefficient for the skill vector significantly improve performance compared to the static value of 1? The authors acknowledge optimal λ might vary depending on the gap between source and target knowledge manifolds.

- **Model architecture generalization**: Does the orthogonality between SFT and RL parameter updates and the efficacy of linear transfer hold across diverse architectures like Mixture-of-Experts (MoE)? Experiments were primarily conducted using Qwen2.5-7B, requiring additional studies across broader model scales.

- **Functional gap between tasks**: To what extent does the functional gap between source and target tasks degrade the transferability of the skill vector? Experiments focused on knowledge-intensive QA and tool use, leaving unclear if skills transfer across fundamentally different reasoning paradigms.

- **Quasi-isotropic assumption validity**: Does the assumption of quasi-isotropic input activations remain valid in deeper transformer layers where the skill vector is injected? The theoretical proof of functional disentanglement relies on this statistical property.

## Limitations

- The orthogonal parameter update assumption may not generalize to other model families or training paradigms beyond Qwen2.5-7B
- Focus on closed-book QA and tool-use scenarios limits applicability to open-domain or multi-modal tasks with different knowledge retrieval mechanisms
- Fixed scaling coefficient (λ=1) is a significant simplification—optimal λ likely varies with source-target domain gap and model scale

## Confidence

- **High confidence**: The empirical observation of near-zero cosine similarity between SFT and RL parameter updates
- **Medium confidence**: The mechanism that post-hoc linear injection preserves both knowledge and skills better than pre-injection or sequential fine-tuning
- **Low confidence**: The claim that iterative refinement with disjoint data subsets prevents overfitting to source content

## Next Checks

1. **Orthogonality generalization test**: Replicate layer-wise cosine similarity analysis on different model architectures (Llama, Mistral) and training objectives to verify if orthogonal update property holds universally

2. **Scaling coefficient sensitivity analysis**: Systematically vary λ from 0.1 to 2.0 on held-out validation sets across all three domains to identify optimal scaling ranges

3. **Cross-task skill transferability stress test**: Extract skill vectors from non-QA RL tasks (mathematical reasoning, code generation) and attempt injection into SQuAD target models to reveal whether skill extraction captures truly domain-agnostic reasoning