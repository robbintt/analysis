---
ver: rpa2
title: 'AutocleanEEG ICVision: Automated ICA Artifact Classification Using Vision-Language
  AI'
arxiv_id: '2512.00194'
source_url: https://arxiv.org/abs/2512.00194
tags:
- icvision
- component
- components
- classification
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "EEG Autoclean Vision-Language AI (ICVision) automates ICA artifact\
  \ classification by directly interpreting EEG dashboard visualizations using a multimodal\
  \ vision-language model (GPT-4 Vision). This approach allows ICVision to classify\
  \ components into six canonical categories\u2014brain, eye, heart, muscle, channel\
  \ noise, and other noise\u2014while providing human-like explanations and confidence\
  \ scores."
---

# AutocleanEEG ICVision: Automated ICA Artifact Classification Using Vision-Language AI

## Quick Facts
- arXiv ID: 2512.00194
- Source URL: https://arxiv.org/abs/2512.00194
- Reference count: 10
- ICVision achieves κ=0.677 agreement with expert consensus, outperforming MNE-ICLabel

## Executive Summary
ICVision represents a novel approach to EEG artifact classification by leveraging GPT-4 Vision to interpret EEG dashboard visualizations rather than raw signal data. The system automatically classifies Independent Component Analysis (ICA) components into six categories - brain, eye, heart, muscle, channel noise, and other noise - while providing human-readable explanations and confidence scores. Tested on 3,168 components from 124 datasets, ICVision demonstrated superior performance to existing automated methods and maintained high interpretability across expert reviews.

## Method Summary
The ICVision system processes EEG dashboard visualizations through GPT-4 Vision, which analyzes component maps, spectra, and time series plots to classify artifacts. The multimodal approach allows the AI to interpret visual patterns similar to how human experts review EEG data, generating classifications alongside confidence scores and explanatory reasoning. The system was trained and validated using expert consensus labeling across a diverse set of EEG datasets, with particular focus on preserving clinically relevant brain signals in ambiguous cases.

## Key Results
- Achieved κ=0.677 agreement with expert consensus versus MNE-ICLabel's κ=0.661
- Over 97% of outputs rated interpretable by expert reviewers
- Successfully preserved brain signals in ambiguous artifact cases

## Why This Works (Mechanism)
The vision-language approach works because it mirrors human expert workflows, where EEG interpretation relies heavily on visual pattern recognition across multiple complementary displays. GPT-4 Vision can process these multimodal inputs simultaneously, identifying spatial patterns in component maps, frequency characteristics in spectra, and temporal dynamics in time series. This holistic interpretation captures the same visual cues that experienced neurophysiologists use when classifying artifacts, enabling the AI to provide explanations that align with human reasoning patterns.

## Foundational Learning
- **ICA decomposition fundamentals**: Understanding how EEG signals separate into independent components is essential for grasping artifact classification goals. Quick check: Can you explain why ICA helps isolate artifacts from brain signals?
- **EEG artifact taxonomy**: Knowledge of canonical artifact types (eye, muscle, heart, etc.) provides context for classification categories. Quick check: What visual patterns distinguish muscle from eye artifacts in component maps?
- **Vision-language model capabilities**: Understanding how multimodal AI processes visual inputs explains the system's reasoning approach. Quick check: How does GPT-4V differ from text-only models in handling EEG visualizations?

## Architecture Onboarding
- **Component map**: EEG dashboard visualization -> GPT-4 Vision processing -> Classification output
- **Critical path**: The system receives preprocessed EEG dashboard images, processes them through the vision model, and generates artifact classifications with confidence scores and explanations
- **Design tradeoffs**: Uses visual interpretations rather than raw signals, prioritizing explainability and alignment with human expert workflows over maximum theoretical accuracy
- **Failure signatures**: May miss subtle artifact patterns not visually apparent, could struggle with non-standard visualization formats, and may show reduced performance on unusual artifact types
- **3 first experiments**: 1) Test classification accuracy on a holdout dataset with known artifact types, 2) Compare performance using dashboard images versus raw signal inputs, 3) Evaluate robustness across different EEG visualization standards

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on visual EEG dashboard outputs rather than raw signal data may miss subtle artifact patterns
- Moderate inter-rater agreement (κ=0.677) indicates room for improvement in classification accuracy
- Subjective expert ratings for interpretability may not generalize across different clinical contexts

## Confidence
- **Technical implementation**: High - The use of GPT-4V for vision-language processing is well-established
- **Performance metrics**: Medium - Validation was performed on a specific dataset with limited reviewers
- **Generalizability**: Low - Results may not extend to different EEG systems, artifact types, or clinical populations

## Next Checks
1. Test ICVision on EEG datasets from multiple manufacturers and with different visualization standards to assess cross-platform robustness
2. Conduct larger-scale validation with a more diverse group of EEG experts from different institutions and clinical specialties
3. Implement systematic comparison of ICVision's performance when using raw signal inputs versus dashboard visualizations