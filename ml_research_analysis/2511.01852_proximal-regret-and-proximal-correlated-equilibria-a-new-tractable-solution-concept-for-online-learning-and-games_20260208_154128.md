---
ver: rpa2
title: 'Proximal Regret and Proximal Correlated Equilibria: A New Tractable Solution
  Concept for Online Learning and Games'
arxiv_id: '2511.01852'
source_url: https://arxiv.org/abs/2511.01852
tags:
- regret
- proximal
- convex
- games
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces proximal regret, a new notion of regret\
  \ that lies strictly between external and swap regret. The key insight is that Online\
  \ Gradient Descent (GD), a classic no-external-regret algorithm, actually achieves\
  \ optimal O(\u221AT) proximal regret against any \u03C1-weakly convex loss functions\
  \ with \u03C1<1, without any modification."
---

# Proximal Regret and Proximal Correlated Equilibria: A New Tractable Solution Concept for Online Learning and Games

## Quick Facts
- **arXiv ID**: 2511.01852
- **Source URL**: https://arxiv.org/abs/2511.01852
- **Reference count**: 36
- **Primary result**: Classic Online Gradient Descent achieves optimal O(√T) proximal regret against any ρ-weakly convex loss functions with ρ<1, without modification

## Executive Summary
This paper introduces proximal regret, a new notion of regret that interpolates between external and swap regret using proximal operators of weakly convex functions. The key insight is that Online Gradient Descent (GD), a classic no-external-regret algorithm, actually achieves optimal O(√T) proximal regret against any ρ-weakly convex loss functions with ρ<1, without any modification. This result provides a new explanation for GD's empirical success in games and online learning. The paper also extends these results to Optimistic Gradient Descent (OG), achieving improved O(T^(-1/4)) individual proximal regret and O(1) social proximal regret in smooth convex games.

## Method Summary
The paper introduces proximal regret as a Φ-regret where Φ contains proximal operators of ρ-weakly convex functions with ρ<1. The core method is Online Gradient Descent with step sizes η_t = 1/√t or η = 1/√T, which achieves O(√T) proximal regret without modification. For smooth convex games, Optimistic Gradient Descent with step size η = T^(-1/4) is shown to achieve faster O(T^(1/4)) individual proximal regret and O(1) social proximal regret. The analysis leverages a telescoping argument that replaces non-telescoping terms in standard GD analysis with telescoping upper bounds.

## Key Results
- GD achieves O(√T) proximal regret simultaneously against all ρ-weakly convex functions with ρ<1, without modification
- Proximal regret strictly interpolates between external and swap regret, unifying multiple existing notions
- OG achieves O(T^(1/4)) individual proximal regret and O(1) social proximal regret in smooth convex games

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GD achieves O(√T) proximal regret simultaneously against all ρ-weakly convex functions with ρ<1, without modification.
- **Mechanism**: The key insight is Lemma 1's inequality: ||x-p_x||² - ||x-p||² ≤ 2f(p) - 2f(p_x) - (1-ρ)||p-p_x||². This replaces non-telescoping terms in standard GD analysis with telescoping upper bounds, enabling √T regret bounds instead of Ω(T) that naive dynamic regret would yield.
- **Core assumption**: Loss functions are convex; f is ρ-weakly convex with ρ<1; step sizes are non-increasing.
- **Evidence anchors**:
  - [abstract]: "classic Online Gradient Descent (GD)...achieves an optimal O(√T) bound on proximal regret"
  - [section 3.1, Theorem 1]: Complete bound derivation showing (1-ρ)/(2η_t)||p_t - p_{t+1}||² telescoping term
  - [corpus]: Related work on GD convergence in games [arxiv:2507.11366] confirms GD's convergence properties in zero-sum settings, though proximal regret is novel here
- **Break condition**: If f is only ρ-weakly convex with ρ≥1, proximal operator may not be unique/well-defined. If losses are non-convex, standard GD guarantees fail.

### Mechanism 2
- **Claim**: Proximal regret strictly interpolates between external and swap regret, unifying multiple existing notions.
- **Mechanism**: By parameterizing Φ-regret through weakly convex function classes: (1) F_ext = {indicator functions} → external regret; (2) F = {linear functions} → gradient equilibrium/projection-based regret; (3) F = {quadratic functions} → symmetric linear swap regret. The proximal operator prox_f acts as a strategy modification mapping.
- **Core assumption**: X is closed convex; each function class yields valid strategy modifications (prox_f maps X→X).
- **Evidence anchors**:
  - [abstract]: "proximal regret...lies strictly between external and swap regret"
  - [section 3, Definition 4]: Formal definition Reg^f_T = Σ[ℓ_t(x_t) - ℓ_t(prox_f(x_t))]
  - [corpus]: Weak direct evidence—neighbor papers discuss NE approximation [arxiv:2504.18868] and correlated equilibria [arxiv:2510.16782] but don't address this specific interpolation
- **Break condition**: If prox_f(x) ∉ X for some x∈X, the strategy modification is invalid. This constrains allowable f to those where proximal operators are endomorphisms.

### Mechanism 3
- **Claim**: Optimistic GD achieves O(T^{1/4}) individual proximal regret in smooth convex games, versus O(√T) adversarial lower bound.
- **Mechanism**: OG's regret bound depends on gradient variation P_T = Σ||g_t - g_{t-1}||² rather than Σ||g_t||². In smooth games, gradient variation is small (||g_t - g_{t-1}||² ≤ L²||x_t - x_{t+1}||²), enabling tighter bounds. Theorem 4 combines this with Theorem 3's adversarial OG guarantee.
- **Core assumption**: Game utilities are G-Lipschitz and L-smooth; all players use OG with same fixed step size η.
- **Evidence anchors**:
  - [abstract]: "Optimistic Gradient Descent, which yields faster convergence in smooth convex games"
  - [section 4, Theorem 4]: "individual proximal regret is bounded by (D²_{X_i} + 2B_{i,f} + 4nL²G²)T^{1/4}"
  - [corpus]: Consistent with regret-based game convergence literature [arxiv:2510.17067], though proximal regret extension is novel
- **Break condition**: If games are non-smooth (L large/unbounded), gradient variation doesn't decrease and O(T^{1/4}) guarantee degrades. If players use different algorithms/step sizes, coupling analysis fails.

## Foundational Learning

- **Concept: Φ-Regret Framework**
  - Why needed here: Proximal regret is defined as Φ-regret where Φ contains proximal operators. Understanding Φ-regret is prerequisite to understanding how different regret notions relate.
  - Quick check question: Given strategy modifications Φ = {ϕ₁, ϕ₂}, what is the Φ-regret of actions x₁...x_T against losses ℓ₁...ℓ_T?

- **Concept: Proximal Operators and Weak Convexity**
  - Why needed here: The paper's core contribution hinges on prox_f being well-defined for ρ-weakly convex f with ρ<1. Understanding why this condition matters is essential.
  - Quick check question: For what values of ρ is the proximal operator of f(x) = ||Ax||² + b'x well-defined, and why does the paper require ρ<1?

- **Concept: Equilibrium Hierarchy (NE → CE → CCE → PCE)**
  - Why needed here: The paper positions PCE as a strict refinement of CCE but relaxation of CE. Understanding this hierarchy clarifies the significance.
  - Quick check question: If all players use no-proximal-regret algorithms, which equilibrium types does play converge to, and which does it not necessarily converge to?

## Architecture Onboarding

- **Component map**:
  Online Learning Module -> Regret Tracker -> Proximal Oracle -> Game Environment -> Equilibrium Monitor

- **Critical path**:
  1. Verify convexity of loss functions and ρ<1 weak convexity of f
  2. Implement GD: x_{t+1} = Π_X[x_t - η_t∇ℓ_t(x_t)] with non-increasing η_t
  3. For each f, compute prox_f(x_t) at each round (may be expensive for general f)
  4. Track proximal regret Reg^f_T = Σ[ℓ_t(x_t) - ℓ_t(prox_f(x_t))]
  5. For games: accumulate empirical distribution σ_T = (1/T)Σδ_{x_t} and verify PCE conditions

- **Design tradeoffs**:
  - Step size selection: η = 1/√T is universal but requires knowing T; η_t = 1/√t is adaptive but slightly worse constants
  - Computing prox_f: For indicator functions, prox is projection (cheap); for general weakly convex f, may require iterative solver
  - OG vs GD: OG achieves better game bounds but requires storing additional state (w_t, g_{t-1})

- **Failure signatures**:
  - Regret growing as Ω(T) instead of O(√T): Check if f is actually ρ-weakly convex with ρ<1, or if step sizes aren't non-increasing
  - Non-convergence in games: Verify all players use same algorithm class; mixing GD/OG may not yield PCE
  - prox_f undefined/ambiguous: Likely ρ≥1 or f not lower semi-continuous

- **First 3 experiments**:
  1. **Sanity check**: Run GD on 1D convex losses with f as linear function. Verify Reg^f_T ≤ C√T by comparing cumulative loss at x_t vs prox_f(x_t) = x_t - v (for f(x)=v'x).
  2. **Game convergence test**: 2-player zero-sum game, both players use GD with η=1/√T. Track empirical distribution and verify it approaches PCE by checking E[∇u_i(x) · (prox_f(x_i) - x_i)] → 0 for test functions f.
  3. **OG comparison**: Same game, both players use OG with η=T^{-1/4}. Compare proximal regret growth rate against GD baseline—should see T^{1/4} vs √T scaling difference.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can an RVU (Regret bounded by Variation of Utilities) type bound be proven for proximal regret, which would yield O(1) individual external regret in general convex games?
- **Basis in paper**: [explicit] Remark 5 states: "If one was able to prove an RVU bound for RegT_Fc, then this would lead to constant O(1) individual external regret in general convex games (which is not known to be achievable in the literature)."
- **Why unresolved**: The negative term in the OG bound (−∑‖x_t − w_t‖²) does not fully cancel the positive gradient variation terms even in smooth games, unlike the external regret RVU bound from prior work.
- **What evidence would resolve it**: A modified analysis of OG or a new algorithm achieving O(1) individual proximal regret in smooth convex games.

### Open Question 2
- **Question**: Does there exist a simple, practical algorithm (like GD) that minimizes full linear swap regret, not just symmetric linear swap regret?
- **Basis in paper**: [explicit] Section 3.2 notes that existing algorithms for general linear swap regret from [DFF+25] "heavily use the ellipsoid method and is considerably more complicated than GD."
- **Why unresolved**: The paper only proves GD minimizes symmetric linear swap regret; full linear swap regret with non-symmetric matrices remains unaddressed by simple methods.
- **What evidence would resolve it**: A polynomial-time algorithm for linear swap regret without ellipsoid-based subroutines, or a hardness result showing simplicity is impossible.

### Open Question 3
- **Question**: Do GD dynamics converge to proximal correlated equilibria in the last-iterate sense, not just in empirical distribution?
- **Basis in paper**: [inferred] The paper proves empirical distribution converges to PCE, but last-iterate convergence—a stronger property established for CCE in zero-sum games—is not addressed.
- **Why unresolved**: The proof technique relies on time-averaging; extending to last-iterate may require different analysis.
- **What evidence would resolve it**: A bound showing ‖x_t − x*_t‖ → 0 for some PCE trajectory, or a counterexample demonstrating cycling.

## Limitations

- The paper is theoretical with no empirical validation provided to demonstrate practical utility of proximal correlated equilibria
- Computational complexity of proximal operators for general weakly convex functions is not addressed
- Real-world scenarios with noisy gradients or unbounded domains may degrade the O(√T) regret bounds
- The paper assumes perfect gradient feedback and idealized conditions

## Confidence

- **High Confidence**: The O(√T) proximal regret bound for GD against ρ-weakly convex functions (Theorem 1) is mathematically rigorous and the telescoping mechanism is well-explained. The equivalence of proximal regret to existing notions (external, gradient equilibrium, swap regret) is clearly defined and plausible.
- **Medium Confidence**: The OG convergence claims in smooth games (O(T^{1/4}) individual regret, O(1) social regret) rely on specific gradient variation bounds that hold in theory but may not manifest in practice if games are non-smooth or step sizes are mismatched.
- **Low Confidence**: The practical utility of proximal correlated equilibria as a "tractable" solution concept is asserted but not demonstrated. The paper does not address how to compute or verify PCE conditions beyond theoretical existence.

## Next Checks

1. **Sanity check proximal operators**: Implement prox_f for simple weakly convex functions (e.g., quadratic) and verify it is well-defined and computationally tractable. Test edge cases where ρ≥1 to confirm the necessity of the condition.
2. **Empirical regret scaling**: Run GD on synthetic convex losses (e.g., linear or quadratic) with multiple f classes. Plot Reg^f_T vs T to confirm O(√T) scaling empirically, and compare against baselines (e.g., fixed action, random play).
3. **Game dynamics simulation**: Simulate a 2-player zero-sum game (e.g., bilinear) with both GD and OG. Track empirical distribution convergence to PCE and measure whether social proximal regret indeed plateaus at O(1) for OG while remaining O(√T) for GD.