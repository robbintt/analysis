---
ver: rpa2
title: Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right
  Data
arxiv_id: '2601.15158'
source_url: https://arxiv.org/abs/2601.15158
tags:
- chain
- lemma
- only
- data
- vertex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes theoretical foundations for the emergence
  of Chain-of-Thought reasoning in Transformers trained via outcome-based reinforcement
  learning. The authors analyze single-layer Transformers on a synthetic graph traversal
  task that requires multi-step reasoning, proving that despite training solely on
  final-answer correctness, policy gradient drives the model to converge to an efficient
  vertex-by-vertex traversal algorithm.
---

# Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data

## Quick Facts
- arXiv ID: 2601.15158
- Source URL: https://arxiv.org/abs/2601.15158
- Reference count: 40
- Primary result: Transformers trained via outcome-based RL converge to efficient chain-of-thought reasoning algorithms only when training data includes sufficient simple examples

## Executive Summary
This paper establishes theoretical foundations for the emergence of Chain-of-Thought reasoning in Transformers trained via outcome-based reinforcement learning. The authors prove that despite training solely on final-answer correctness, policy gradient drives single-layer Transformers to converge to an efficient vertex-by-vertex traversal algorithm on a synthetic graph traversal task. They characterize distributional properties required for this emergence, identifying that training distributions must place sufficient mass on "simple examples" requiring fewer reasoning steps. When simple examples are present, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, policy gradient learning becomes intractable. Experiments with synthetic data and real-world Qwen-based models on mathematical reasoning tasks validate that theoretical findings carry over to practical settings.

## Method Summary
The paper analyzes single-layer Transformers trained via outcome-based reinforcement learning on a synthetic graph traversal task where two disjoint directed chains are presented along with a starting vertex, and the model must predict the terminal vertex. The Transformer uses attention weights parameterized by α, β, γ controlling forward, backward, and switch transitions, with a fixed value matrix encoding edge endpoints. Learning proceeds via REINFORCE (policy gradient) on binary reward for correct final answer. Theoretical analysis uses Markov chain absorption probabilities and source-function representations to characterize learning dynamics. Synthetic experiments train on distributions varying in the mass of simple examples (those requiring few reasoning steps), while real-world experiments use Qwen2.5-3B on mathematical reasoning tasks involving shuffled affine equations.

## Key Results
- Training distributions with non-zero mass on simple examples enable policy gradient to converge efficiently to reasoning algorithms; without simple examples, learning becomes exponentially hard
- Policy gradient converges to efficient chain-traversal (vertex-by-vertex) rather than inefficient random-walk solutions, even though both achieve optimal reward
- Models trained only on simple examples successfully extrapolate to complex examples requiring longer reasoning chains

## Why This Works (Mechanism)

### Mechanism 1: Simple-to-Complex Gradient Domination
Training distributions with non-zero mass on simple examples enable policy gradient to converge efficiently to reasoning algorithms; without simple examples, learning becomes exponentially hard. Simple examples produce gradients dominated by long-jump absorption dynamics, while hard examples contribute negligibly. Easy examples drive learning; hard examples' gradients are suppressed exponentially in chain length. Break condition: If training data contains only examples requiring ≥an steps (where a > 0), gradient magnitude decays as O(n·e^(−cn)) and learning stalls.

### Mechanism 2: Implicit Bias Toward Efficient Algorithms
Policy gradient converges to efficient chain-traversal rather than inefficient random-walk solutions, even though both achieve optimal reward. The loss landscape's implicit bias emerges from asymmetric sensitivity: ∂αS ≥ 0, ∂βS ≤ 0, ∂γS ≤ 0 for near-terminal states. This drives α (forward logit) up and β, γ (backward/switch logits) down. Break condition: If initialization violates forward-bias assumption (pfwd ≤ pbwd), the model may converge to backward-traversal or oscillatory solutions.

### Mechanism 3: Out-of-Distribution Generalization via Algorithmic Transfer
Models trained only on simple examples successfully extrapolate to complex examples. The learned chain-traversal algorithm is depth-agnostic: once the model learns to attend to outgoing edges and suppress backward/switch transitions, this policy applies uniformly regardless of starting position. Break condition: If test examples require qualitatively different reasoning (e.g., branching graphs, cycles), the learned algorithm fails.

## Foundational Learning

- **Markov Chain Absorption Probabilities**: The analysis frames reasoning as computing same-side absorption probability in a two-chain Markov process; gradients are derived via the source-function representation for parametric Markov chains.
  - Quick check: Given a 3-state chain {A→B→C} with absorbing state C, what's the absorption probability starting from A?

- **Policy Gradient (REINFORCE)**: The learning algorithm is outcome-based RL where gradient flows through trajectory likelihood to update attention parameters; the score-function trick (∂θ log P(τ)) is central to the derivative computations.
  - Quick check: Why does REINFORCE require reward baseline for variance reduction, and what baseline does this paper implicitly use?

- **Softmax Attention Parameterization**: The attention logits are parameterized by α, β, γ controlling forward, backward, and switch transitions; the symmetric parameterization ensures permutation-invariance and reduces dynamics to 3 scalar ODEs.
  - Quick check: How does the logit gap (α − β) control the forward-vs-backward transition ratio in the resulting softmax distribution?

## Architecture Onboarding

- **Component map**: Input edges and starting vertex -> attention weights via A(α,β,γ) -> value vectors via V -> logits -> softmax -> next-token probability -> autoregressive generation until terminal

- **Critical path**: Input edge tokens → attention weights computed via A → value vectors retrieved via V → logits = attention-weighted value sum → softmax → next-token probability → autoregressive generation until terminal vertex

- **Design tradeoffs**: Single-layer vs. multi-layer (theory covers single-layer; deeper models may learn different algorithms); Linear vs. softmax attention (both analyzed; softmax provides sharper transitions but complicates Z-normalization); Fixed V vs. trained V (theory fixes V; experiments train both but results align)

- **Failure signatures**: Training stalls with near-zero gradient → check if training data excludes simple examples; Model outputs wrong terminal → check if forward-bias initialization condition violated; Oscillatory trajectories → switch probability may be too high; increase α−β gap

- **First 3 experiments**:
  1. Replicate Table 1: Train single-layer transformer on uniform distribution over chain lengths {4,8,12}; verify 100% accuracy and 100% chain-traversal behavior
  2. Ablate simple examples: Train on 15-Complex (only examples requiring 14 steps) vs. 15-Uniform; compare convergence time and final accuracy on 15-Complex test set
  3. Test OOD generalization: Train on n=12 chains with only starting positions n−4 to n−1; evaluate on starting position 1 (full-length traversal required)

## Open Questions the Paper Calls Out
None

## Limitations

- Theoretical framework assumes deterministic graph structures (two disjoint chains) and single-layer attention, which may not capture complexity of real-world reasoning tasks
- Theory only covers linear attention mechanisms while experimental results use softmax attention on Qwen2.5-3B, with potential differences in learning dynamics
- Real-world experiments use relatively small number of training steps (600) and specific model (Qwen2.5-3B), with extrapolation from synthetic single-layer transformers not rigorously proven

## Confidence

**High Confidence**: Simple examples are necessary for efficient learning of complex reasoning tasks; Policy gradient can drive Transformers to learn efficient chain-traversal algorithms when training data includes sufficient simple examples; Training on simple examples improves OOD generalization to longer chains

**Medium Confidence**: The implicit bias toward efficient algorithms emerges naturally from the loss landscape; Results from single-layer linear Transformers transfer to multi-layer softmax Transformers; Training on simple examples is more effective than direct training on complex examples

**Low Confidence**: The theoretical findings scale to arbitrary reasoning tasks beyond chain identification; The 8-step curriculum in Qwen experiments is optimal; Real-world mathematical reasoning fully captures the theoretical dynamics

## Next Checks

1. **Initialization Sensitivity Analysis**: Systematically vary the initialization conditions (ζ, υ) for α, β, γ to determine the precise boundaries where the forward-bias assumption breaks down. Test whether models converge to chain-traversal when initialized near the theoretical thresholds, and characterize the basin of attraction for the efficient algorithm.

2. **Generalization to Non-Chain Structures**: Extend the synthetic task to include branching graphs, cycles, and conditional dependencies. Measure whether the chain-traversal algorithm learned on simple chains can be adapted to these structures through fine-tuning, and identify which aspects of the reasoning mechanism transfer versus require new learning.

3. **Softmax vs Linear Attention Dynamics**: Conduct controlled experiments comparing learning dynamics between linear and softmax attention on the same synthetic task. Track the evolution of attention weights, gradient magnitudes, and convergence rates to quantify how the normalization affects the theoretical predictions about simple-to-complex gradient domination.