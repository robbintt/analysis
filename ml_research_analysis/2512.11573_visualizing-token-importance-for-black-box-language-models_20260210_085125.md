---
ver: rpa2
title: Visualizing token importance for black-box language models
arxiv_id: '2512.11573'
source_url: https://arxiv.org/abs/2512.11573
tags:
- dbsa
- sensitivity
- token
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Distribution-Based Sensitivity Analysis (DBSA),
  a novel model-agnostic method for auditing black-box large language models (LLMs)
  by evaluating how sensitive model outputs are to each input token. Unlike existing
  gradient-based or bias-focused approaches, DBSA addresses the unique challenge of
  stochastic LLM outputs by using Monte Carlo sampling and statistical hypothesis
  testing to measure the distributional impact of token perturbations.
---

# Visualizing token importance for black-box language models

## Quick Facts
- **arXiv ID:** 2512.11573
- **Source URL:** https://arxiv.org/abs/2512.11573
- **Reference count:** 40
- **Primary result:** DBSA identifies token-level sensitivities in black-box LLMs with Spearman correlations above 0.96 across different distance metrics

## Executive Summary
This paper introduces Distribution-Based Sensitivity Analysis (DBSA), a novel model-agnostic method for auditing black-box large language models (LLMs) by evaluating how sensitive model outputs are to each input token. Unlike existing gradient-based or bias-focused approaches, DBSA addresses the unique challenge of stochastic LLM outputs by using Monte Carlo sampling and statistical hypothesis testing to measure the distributional impact of token perturbations. The method approximates sensitivity by comparing output distributions when tokens are replaced with their nearest neighbors in embedding space, using energy distance as a metric and permutation tests for p-values. Experiments across seven different LLMs demonstrate that DBSA consistently identifies token-level sensitivities, with Spearman correlations above 0.96 between different distance metrics (cosine, L1, L2), validating its robustness.

## Method Summary
DBSA measures how much the output distribution shifts when individual tokens in an input prompt are perturbed. The approach uses Monte Carlo sampling to generate multiple responses for both the original and perturbed prompts, then computes Energy Distance between the resulting response distributions. Statistical significance is assessed through permutation tests. The method is designed to work with any black-box LLM without requiring access to gradients or internal parameters. It can handle stochastic outputs by comparing entire distributions rather than single responses, making it particularly suitable for practical auditing in high-stakes domains like legal and medical applications.

## Key Results
- DBSA achieves Spearman correlations above 0.96 between cosine, L1, and L2 distance metrics when measuring token sensitivity
- The method successfully identifies token-level sensitivities across seven different LLMs including GPT-3.5, Llama-3-8B, and others
- Visualizations clearly highlight important tokens in legal contracts, clinical notes, and trading data with normalized sensitivity scores
- DBSA outperforms gradient-based methods in handling the stochastic nature of LLM outputs

## Why This Works (Mechanism)
DBSA works by treating token importance as a distributional sensitivity problem rather than a point-estimate problem. By generating multiple responses for both original and perturbed inputs, it captures the full output distribution rather than relying on single responses that might be statistical outliers. The energy distance metric provides a robust way to compare distributions, while permutation testing ensures statistical significance. The nearest-neighbor perturbation strategy in embedding space provides semantically meaningful variations that test the model's true sensitivity to token changes rather than arbitrary noise.

## Foundational Learning
- **Energy Distance**: A statistical metric for comparing probability distributions without requiring distributional assumptions; needed because it's robust to different distribution shapes and doesn't assume normality
- **Permutation Testing**: Non-parametric hypothesis testing method that builds empirical null distributions by randomizing data; needed because it doesn't require distributional assumptions and works well with small sample sizes
- **Nearest Neighbor Search in Embedding Space**: Finding semantically similar tokens by measuring distances in continuous vector space; needed to create meaningful perturbations that preserve semantic context
- **Monte Carlo Sampling**: Generating multiple samples to approximate distributions; needed because single samples are insufficient for capturing stochastic model behavior
- **Spearman Correlation**: Rank-based correlation metric that measures monotonic relationships; needed to validate consistency across different distance metrics
- **Token-level Perturbation Analysis**: Systematic replacement of individual tokens to measure impact; needed to isolate the contribution of each token to final output

## Architecture Onboarding

**Component Map:**
Tokenization -> Nearest Neighbor Search -> Perturbation Generation -> Monte Carlo Sampling -> Embedding Generation -> Energy Distance Calculation -> Permutation Testing -> Visualization

**Critical Path:**
The most time-consuming component is the Monte Carlo sampling loop (40 samples × token count × 500 permutations), which dominates computational cost. The Energy Distance calculation and permutation testing are also computationally intensive but scale better with input size.

**Design Tradeoffs:**
The method trades computational efficiency for robustness - requiring 40 Monte Carlo samples and 500 permutation tests ensures statistical power but makes the approach expensive. Using embedding-space nearest neighbors instead of GPT-4 synonyms reduces costs but may introduce semantic drift. The choice of distance metric (cosine vs L1 vs L2) has minimal impact on results (correlation >0.96) but affects computational speed.

**Failure Signatures:**
High variance in sensitivity scores indicates insufficient Monte Carlo samples (n < 20). Semantic drift appears as extreme sensitivity scores for antonyms or nonsensical neighbors. Low statistical power manifests as uniformly high p-values across all tokens, suggesting the perturbation magnitude is too small.

**3 First Experiments:**
1. Run DBSA on a simple legal contract with 5-10 tokens to validate the complete pipeline works
2. Compare cosine vs L1 vs L2 distance metrics on the same input to verify correlation >0.9
3. Test with n=10 vs n=40 samples to quantify the tradeoff between computational cost and result stability

## Open Questions the Paper Calls Out
**Open Question 1:** Can DBSA be extended to a multi-level interpretability framework that accounts for interactions between tokens and semantic drift?
The paper proposes extending DBSA to handle phrases and sentences rather than just tokens, but this introduces challenges with mutual information between levels and semantic drift that the current formulation doesn't address.

**Open Question 2:** How can DBSA formally decompose output variability to distinguish between stochastic noise and sensitivity to specific perturbations?
The authors suggest using ANOVA-style variance decomposition to separate within-prompt variability from between-prompt variability, but this theoretical framework hasn't been developed.

**Open Question 3:** How does the inconsistency in nearest-neighbor magnitude within the embedding space affect the reliability of DBSA sensitivity scores?
The paper notes that nearest neighbors may vary significantly in semantic distance, potentially making sensitivity scores incomparable across different tokens.

## Limitations
- The method requires substantial computational resources (40 samples × 500 permutations × token count), making it expensive for long documents
- Neighbor selection ambiguity exists between GPT-4 synonym generation and embedding-space nearest neighbors, with unclear implementation logic
- Results depend heavily on the specific system instruction template, which is not fully specified in the paper
- Claims about superiority over gradient-based methods lack direct quantitative comparisons

## Confidence
- **High Confidence:** The core DBSA methodology using Energy Distance and permutation testing is well-established and clearly explained
- **Medium Confidence:** Experimental results showing consistent sensitivity patterns across seven LLMs are convincing but depend on unknown system prompts
- **Low Confidence:** Claims about DBSA's superiority over gradient-based methods are difficult to verify without direct comparisons

## Next Checks
1. **Instruction Template Isolation:** Run DBSA with three different system instruction templates (legal auditor, clinical assistant, neutral responder) on the same input text to quantify how much sensitivity rankings change
2. **Neighbor Selection Comparison:** Implement both GPT-4 synonym generation and embedding-space nearest neighbor approaches, then compare resulting sensitivity rankings using Spearman correlation
3. **Computational Efficiency Trade-off:** Systematically reduce Monte Carlo samples (n=10, 20, 30, 40) and permutation tests (100, 250, 500) to identify minimum viable configuration maintaining correlation >0.95 while reducing computational cost by at least 50%