---
ver: rpa2
title: Filtering Learning Histories Enhances In-Context Reinforcement Learning
arxiv_id: '2505.15143'
source_url: https://arxiv.org/abs/2505.15143
tags:
- learning
- dicp
- ours
- icrl
- histories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of suboptimal behavior inherited
  from source reinforcement learning algorithms in in-context reinforcement learning
  (ICRL) frameworks. Current ICRL methods imitate complete learning histories from
  source algorithms, which can transfer suboptimal behaviors.
---

# Filtering Learning Histories Enhances In-Context Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.15143
- Source URL: https://arxiv.org/abs/2505.15143
- Reference count: 40
- In-context reinforcement learning (ICRL) performance improved by filtering suboptimal learning histories

## Executive Summary
This paper addresses a critical limitation in in-context reinforcement learning (ICRL) where current methods inherit suboptimal behaviors from source reinforcement learning algorithms by imitating complete learning histories. The proposed Learning History Filtering (LHF) approach tackles this by preprocessing the pretraining dataset - reweighting and filtering learning histories based on their improvement and stability characteristics. The method is compatible with existing SOTA ICRL algorithms including Algorithm Distillation (AD), Decision Pretrained Transformer (DPT), and Distillation for In-Context Planning (DICP), demonstrating consistent performance improvements across multiple benchmark environments.

## Method Summary
The Learning History Filtering (LHF) approach introduces a weighted sampling strategy that retains learning histories with probability depending on a unified metric combining improvement (measured by mean and range of episodic returns) and stability (measured by performance degradation). The method preprocesses pretraining datasets by filtering out histories that show poor improvement or high instability, then reweights the remaining histories to focus on high-quality demonstrations. This filtering process is designed to be compatible with existing ICRL frameworks, allowing integration with Algorithm Distillation, Decision Pretrained Transformer, and Distillation for In-Context Planning without requiring architectural modifications.

## Key Results
- LHF achieves average relative enhancements of 8.8% (AD), 9.1% (DICP), and 11.9% (DPT) over baselines
- Performance gains become more pronounced with noisy datasets, showing average improvements of 27.8% (AD), 12.5% (DICP), and 12.4% (DPT)
- The approach maintains robustness across various suboptimal scenarios including partial learning histories, lightweight models, and different hyperparameter settings

## Why This Works (Mechanism)
The filtering mechanism works by identifying and removing learning histories that exhibit suboptimal improvement trajectories or unstable performance patterns. By focusing the pretraining data on high-quality, stable learning sequences, the model learns to generalize better from demonstrations without inheriting the noise and inefficiencies present in raw learning histories. The unified metric combining improvement and stability ensures that only trajectories that both make meaningful progress and maintain consistent performance are retained, leading to more reliable in-context generalization.

## Foundational Learning
- In-Context Reinforcement Learning (ICRL): A framework where agents learn to solve new tasks from demonstrations without fine-tuning
  - Why needed: Enables rapid adaptation to new tasks without costly retraining
  - Quick check: Verify the model can perform zero-shot task solving from demonstrations

- Algorithm Distillation (AD): An ICRL method that distills policy learning algorithms into transformers
  - Why needed: Provides the baseline framework for comparing LHF improvements
  - Quick check: Confirm AD baseline performance matches published results

- Weighted Sampling Strategy: A technique for selecting data points with probability proportional to their quality scores
  - Why needed: Enables focusing on high-quality learning histories during pretraining
  - Quick check: Verify that weighted sampling produces more stable training curves

## Architecture Onboarding

Component Map:
Pretraining Dataset -> Filtering Module -> Weighted Sampling -> Model Pretraining -> ICRL Agent

Critical Path:
1. Collect learning histories from source RL algorithms
2. Apply LHF filtering using improvement and stability metrics
3. Perform weighted sampling of filtered histories
4. Pretrain ICRL model on filtered dataset
5. Evaluate in-context performance on held-out tasks

Design Tradeoffs:
- Filtering threshold vs. dataset size: Stricter filtering reduces noise but may limit training data
- Improvement vs. stability weighting: Different environments may require different emphasis
- Computational cost vs. performance gain: More thorough filtering provides better results but increases preprocessing time

Failure Signatures:
- Over-filtering leading to insufficient training data and poor generalization
- Inappropriate threshold settings causing retention of too many suboptimal histories
- Computational overhead making the approach impractical for very large datasets

Three First Experiments:
1. Run LHF with different filtering thresholds on a simple environment to identify optimal settings
2. Compare training curves with and without filtering to visualize convergence differences
3. Test model performance on tasks with varying similarity to pretraining tasks to assess generalization

## Open Questions the Paper Calls Out
None

## Limitations
- The filtering strategy's sensitivity to hyperparameters (δ_improve, δ_stable) represents a significant uncertainty without sensitivity analysis
- Evaluation focuses primarily on benchmark environments with limited diversity in task complexity
- Computational overhead of filtering and reweighting datasets is not fully characterized

## Confidence
High confidence: The core premise that learning history filtering improves ICRL performance is well-supported by experimental results across multiple algorithms and environments.

Medium confidence: The claim that LHF maintains robustness across suboptimal scenarios is supported but experimental coverage is limited.

Low confidence: The assertion that LHF will generalize well to entirely different domains or more complex RL problems is speculative based on current evaluation.

## Next Checks
1. Conduct hyperparameter sensitivity analysis for δ_improve, δ_stable, and α across multiple environment types to determine optimal settings.

2. Test LHF on high-dimensional control tasks (e.g., humanoid locomotion, complex manipulation) to evaluate scalability in more challenging domains.

3. Measure and report the computational overhead (both preprocessing time and memory usage) introduced by the filtering process across different dataset sizes.