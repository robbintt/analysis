---
ver: rpa2
title: 'STAMP: Scalable Task And Model-agnostic Collaborative Perception'
arxiv_id: '2501.18616'
source_url: https://arxiv.org/abs/2501.18616
tags:
- agent
- feature
- agents
- fusion
- collaborative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STAMP is a scalable task- and model-agnostic collaborative perception
  framework for heterogeneous autonomous agents. It uses lightweight adapter-reverter
  pairs to transform Bird's Eye View (BEV) features between agent-specific and shared
  protocol domains, enabling efficient feature sharing without sharing model details.
---

# STAMP: Scalable Task And Model-agnostic Collaborative Perception

## Quick Facts
- arXiv ID: 2501.18616
- Source URL: https://arxiv.org/abs/2501.18616
- Reference count: 22
- Primary result: Achieves 7.2x computational savings (2.36 vs 17.07 GPU hours per additional agent) while maintaining comparable accuracy to state-of-the-art models

## Executive Summary
STAMP introduces a scalable framework for collaborative perception among heterogeneous autonomous agents without requiring model sharing. The system employs lightweight adapter-reverter pairs that transform Bird's Eye View (BEV) features between agent-specific and shared protocol domains, enabling efficient feature sharing across diverse architectures and tasks. Experiments demonstrate STAMP achieves comparable or superior accuracy to existing methods while requiring significantly less computational resources per additional agent.

## Method Summary
STAMP addresses the scalability challenge in collaborative perception by decoupling feature sharing from model architecture constraints. Instead of sharing entire models or raw sensor data, agents exchange transformed BEV features through adapter-reverter pairs. These components learn to map between agent-specific feature spaces and a shared protocol domain, enabling heterogeneous agents to collaborate while maintaining architectural independence. The framework operates at the feature level rather than raw data, preserving privacy and reducing communication overhead.

## Key Results
- Achieves 7.2x computational savings (2.36 vs 17.07 GPU hours per additional agent)
- Maintains comparable or superior accuracy to state-of-the-art collaborative perception models
- Successfully demonstrates task- and model-agnostic collaboration across diverse agent configurations

## Why This Works (Mechanism)
STAMP's efficiency stems from its adapter-reverter mechanism that operates in the BEV feature space rather than raw sensor data. By transforming features to a shared protocol domain, agents can exchange compressed, task-relevant information without exposing model architectures or raw sensor inputs. This approach leverages the structured nature of BEV representations while maintaining flexibility across different perception tasks and agent capabilities.

## Foundational Learning
- **BEV Feature Extraction**: Converting sensor data to Bird's Eye View representations provides a common spatial reference frame for multi-agent collaboration
  - Why needed: Enables agents with different sensor configurations to share spatial information consistently
  - Quick check: Verify BEV conversion maintains geometric consistency across different sensor modalities

- **Adapter-Reverter Mechanism**: Lightweight neural modules that learn bidirectional transformations between agent-specific and shared feature spaces
  - Why needed: Allows heterogeneous agents to communicate without sharing model architectures
  - Quick check: Measure adapter-reverter computational overhead relative to base perception models

- **Feature Space Alignment**: Learning to map between different feature representations while preserving task-relevant information
  - Why needed: Enables collaboration between agents trained on different objectives or architectures
  - Quick check: Evaluate feature alignment quality using domain adaptation metrics

## Architecture Onboarding

**Component Map**: Sensors -> BEV Extractor -> Adapter -> Shared Protocol -> Reverter -> BEV Extractor -> Downstream Task

**Critical Path**: The adapter-reverter transformation represents the critical computational path, as it directly impacts both communication efficiency and collaboration quality.

**Design Tradeoffs**: The framework prioritizes computational efficiency and architectural flexibility over maximum possible accuracy, accepting minor performance trade-offs for significant scalability gains.

**Failure Signatures**: Poor adapter-reverter training can lead to feature space misalignment, resulting in degraded collaborative performance or communication overhead without benefit.

**First Experiments**:
1. Validate BEV feature quality and consistency across different sensor modalities
2. Test adapter-reverter transformation accuracy with single-agent pairs
3. Measure computational overhead of adapter-reverter modules versus baseline collaborative methods

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability claims may not generalize beyond tested configurations and datasets
- Model-agnostic claim limited by requirement for common BEV feature extraction format
- Security benefits of not sharing model details need more rigorous empirical validation

## Confidence
- High: Computational efficiency claims and comparison methodology are well-supported
- Medium: Task- and model-agnostic claims require broader validation across diverse architectures
- Medium: Security and privacy benefits need more empirical validation

## Next Checks
1. Test STAMP's performance when agents use fundamentally different perception modalities (LiDAR-only vs camera-only vs radar) to validate true model-agnostic capability
2. Conduct latency analysis to quantify real-time performance impact of adapter-reverter transformations in high-speed autonomous driving scenarios
3. Perform security analysis to determine if adapter-reverter transformations could leak sensitive information about agent-specific models or local environments