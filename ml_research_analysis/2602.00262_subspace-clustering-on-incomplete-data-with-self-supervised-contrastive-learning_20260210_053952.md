---
ver: rpa2
title: Subspace Clustering on Incomplete Data with Self-Supervised Contrastive Learning
arxiv_id: '2602.00262'
source_url: https://arxiv.org/abs/2602.00262
tags:
- clustering
- subspace
- data
- contrastive
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a contrastive self-supervised learning framework,
  CSC, for subspace clustering with incomplete data. The key idea is to use random
  masking to generate augmented views of incomplete samples, which are then passed
  through a shared deep network and trained using a SimCLR-style contrastive loss
  to learn invariant embeddings.
---

# Subspace Clustering on Incomplete Data with Self-Supervised Contrastive Learning

## Quick Facts
- arXiv ID: 2602.00262
- Source URL: https://arxiv.org/abs/2602.00262
- Reference count: 40
- Key outcome: CSC framework uses disjoint random masking and contrastive learning to cluster incomplete data, outperforming classical and deep baselines especially at high missing rates.

## Executive Summary
This paper addresses subspace clustering when data has missing entries, a common real-world scenario. The authors propose CSC (Contrastive Subspace Clustering), a self-supervised contrastive learning framework that learns robust embeddings from incomplete data without imputation. By generating two disjoint masked views per sample and training with SimCLR-style contrastive loss, CSC learns representations that preserve subspace structure despite missingness. These embeddings are then clustered using Sparse Subspace Clustering. Experiments across six datasets show CSC consistently outperforms classical methods like SSC, MSC, and GSSC, as well as deep learning baselines such as DSC and MAE, particularly under high missing rates.

## Method Summary
CSC generates two disjoint binary masks per incomplete sample to create positive pairs for contrastive learning. These masked views are passed through a shared deep network (ResNet-18 for images, 1D-Conv for hyperspectral data) and trained using NT-Xent contrastive loss. After training, the frozen backbone produces embeddings that are clustered using Sparse Subspace Clustering. This approach decouples representation learning from clustering, enabling modularity and avoiding the quadratic scaling issues of classical SSC on raw incomplete data.

## Key Results
- On MNIST, CSC achieves 65.95% clustering accuracy across sampling rates versus 50.98% for MAE
- On hyperspectral datasets like IndianPines, CSC achieves 74.28% accuracy versus 65.70% for DSC-Conv10
- Training completes in under 3 hours on a single GPU
- CSC consistently outperforms classical methods (SSC, MSC, GSSC) and deep baselines (DSC, MAE) especially under high missing rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disjoint random masking creates valid augmentations for contrastive learning on incomplete data.
- Mechanism: For each partially observed sample, two disjoint binary masks are sampled (Ma ⊙ Mb = 0). This yields two views that share the same underlying subspace structure but expose different observed entries, providing positive pairs for contrastive training without requiring complete observations.
- Core assumption: The underlying subspace structure is inferable from any sufficiently large subset of features; the missing entries are missing at random (Bernoulli sampling).
- Evidence anchors:
  - [abstract] "CSC generates masked views of partially observed inputs and trains a deep neural network using a SimCLR-style contrastive loss to learn invariant embeddings."
  - [section 3, p.5] "Applying these masks yields two views ya_i = Ma_i ⊙ x_i, yb_i = Mb_i ⊙ x_i, which serve as positive pairs for contrastive training."
  - [corpus] Limited direct corpus support; neighboring papers focus on multi-view missing data rather than single-view random masking.
- Break condition: At extremely low sampling rates (ρ < 0.1), disjoint masks may leave insufficient overlap, degrading positive pair quality.

### Mechanism 2
- Claim: Contrastive learning with NT-Xent loss preserves subspace membership in learned embeddings.
- Mechanism: The SimCLR-style loss explicitly maximizes agreement between embeddings of same-sample views (positive pairs) while minimizing agreement with other samples (negative pairs). This forces the network to learn representations where samples from the same subspace cluster together, even under heavy missingness.
- Core assumption: Samples from the same low-dimensional subspace share observable patterns that survive masking; subspaces are sufficiently separated.
- Evidence anchors:
  - [abstract] "learn robust embeddings that preserve subspace structure despite missing entries"
  - [section 3, p.5] "encourages embeddings of the same original sample to be close, while pushing apart embeddings from different samples"
  - [corpus] RAC-DMVC and related incomplete multi-view papers similarly use contrastive objectives to handle missing/noisy data, but in multi-view settings.
- Break condition: If subspaces heavily overlap or within-subspace variance exceeds between-subspace variance, contrastive separation may fail.

### Mechanism 3
- Claim: Decoupling representation learning from clustering enables modularity and scalability.
- Mechanism: CSC trains a backbone network once using contrastive loss, then discards the projection head. The frozen embeddings are clustered using Sparse Subspace Clustering (SSC). This avoids the quadratic scaling of classical SSC on raw data and enables instant inference on new incomplete samples.
- Core assumption: The contrastive objective produces embeddings amenable to linear subspace clustering; SSC assumptions (sparse self-expressiveness) hold in the embedding space.
- Evidence anchors:
  - [abstract] "These embeddings are then clustered using sparse subspace clustering."
  - [section 3, p.5] "After training, the learned backbone generates embeddings... These embeddings can then be directly clustered using any standard clustering algorithm."
  - [corpus] Mini-batch DSC paper (arXiv:2507.19917) addresses scalability in deep subspace clustering but requires full-batch processing; CSC avoids this bottleneck.
- Break condition: If embeddings do not preserve linear subspace structure (e.g., become highly nonlinear), SSC performance degrades.

## Foundational Learning

- Concept: **Sparse Subspace Clustering (SSC)**
  - Why needed here: Final clustering step in CSC pipeline; assumes data points can be represented as sparse linear combinations of points from the same subspace.
  - Quick check question: Can you explain why SSC constructs an affinity matrix from self-expressive coefficients before applying spectral clustering?

- Concept: **SimCLR / Contrastive Learning**
  - Why needed here: Core representation learning mechanism; understanding positive/negative pairs, temperature scaling, and projection heads is essential.
  - Quick check question: Why does SimCLR discard the projection head after training, and what role does the NT-Xent loss play?

- Concept: **Union of Subspaces Model**
  - Why needed here: Theoretical foundation; data lies in multiple low-dimensional subspaces, and the goal is to recover this partition.
  - Quick check question: How does the union-of-subspaces assumption differ from the centroid-based assumption in k-means?

## Architecture Onboarding

- Component map:
  Input Layer -> Augmentation Module -> Backbone Network -> Projection Head -> Contrastive Loss -> Frozen Backbone -> SSC Clustering

- Critical path:
  1. Sample disjoint masks → generate positive pairs
  2. Forward pass through shared backbone + projection head
  3. Compute NT-Xent loss over batch
  4. Backpropagate, update θ, φ
  5. After training: freeze backbone, discard projection head
  6. Embed all data → run SSC → obtain cluster assignments

- Design tradeoffs:
  - **Depth vs. overfitting**: Paper finds L=5 optimal; deeper networks plateau (Figure 2a).
  - **Residual connections**: Minor impact; No Residual slightly better at large N, Full Residual better at small N (Figure 3).
  - **Batch size**: 64–256 optimal; too small degrades negative sample diversity, too large may dilute gradients.
  - **Backbone choice**: ResNet-18 for 2D images; 1D-Conv for HSI (no spatial context exploited).

- Failure signatures:
  - **Sampling rate too low (ρ < 0.2)**: Disjoint masks leave insufficient overlap; clustering accuracy drops sharply (Figure 2b, 4).
  - **High noise (σ > 0.3)**: Classical baselines collapse; CSC more robust but still degraded.
  - **Incorrect temperature τ**: Too high → weak separation; too low → collapsed representations.

- First 3 experiments:
  1. **Synthetic validation**: Generate data with k=5 subspaces, d=128, varying sampling rate ρ ∈ {0.1–0.9}. Confirm clustering error decreases with ρ and that L=4–5 is sufficient (replicate Figure 2a).
  2. **Ablation on masking strategy**: Compare disjoint masks vs. overlapping masks vs. zero-filling. Expect disjoint masks to outperform at high missing rates.
  3. **Backbone comparison**: On MNIST, compare ResNet-18 vs. MLP-10 (Table 1). Expect ResNet-18 to outperform at moderate missing rates, MLP competitive at extreme missingness due to simpler capacity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the contrastive learning objective provide theoretical guarantees that the learned embeddings preserve the underlying union-of-subspaces structure?
- Basis in paper: [explicit] The conclusion states the authors plan to "further explore theoretical guarantees for contrastive objectives under missing data."
- Why unresolved: The paper currently relies on empirical validation across six datasets to demonstrate that the embeddings capture subspace structures, without providing a formal mathematical proof linking the specific NT-Xent loss to subspace preservation properties.
- What evidence would resolve it: A theoretical derivation showing that minimizing the contrastive loss with disjoint random masking necessarily forces the network to learn representations isomorphic to the ground-truth subspace membership.

### Open Question 2
- Question: How can the CSC framework be extended to handle multi-view data where missingness may occur independently across different views?
- Basis in paper: [explicit] The conclusion explicitly lists extending the "framework to... multi-view settings" as a direction for future work.
- Why unresolved: The current implementation treats the input as a single vector (flattened image or spectral vector) and applies a single global binary mask, whereas multi-view data requires distinct handling of correlated but distinct feature sets with potentially disjoint missing patterns.
- What evidence would resolve it: A modified CSC architecture with view-specific encoders and fusion mechanisms, evaluated on standard multi-view datasets with view-specific missing rates.

### Open Question 3
- Question: Is the random disjoint masking strategy robust to structured or correlated missing data (MNAR), as opposed to the tested Missing Completely at Random (MCAR) scenario?
- Basis in paper: [inferred] Section 1 mentions real-world causes like "occlusion" and "sensor failure," which often result in block-wise or structured missingness; however, Section 4 and Section 5 exclusively utilize Bernoulli random sampling to generate masks ($M_{i\ell} \sim \text{Bernoulli}(\ρ)$).
- Why unresolved: The disjoint masking augmentation strategy assumes that random subsets of features are sufficient to identify the instance. If missingness is structural (e.g., an entire sensor fails), the "views" generated by random masking may not align with the natural data distribution, potentially causing the positive pairs to lack the necessary mutual information for learning.
- What evidence would resolve it: Experimental results on datasets where masks are applied in blocks (e.g., removing entire image quadrants or spectral bands) rather than pixel-wise randomly, comparing CSC performance against the MCAR baseline.

### Open Question 4
- Question: Can the proposed method be adapted to streaming environments where data arrives sequentially and subspace structures may drift over time?
- Basis in paper: [explicit] The conclusion identifies extending the framework to "streaming" settings as a goal for future work.
- Why unresolved: The current methodology involves a distinct "Training" phase on a dataset followed by an "Inference" phase (Figure 1), utilizing a standard SimCLR batch training approach that assumes a static distribution.
- What evidence would resolve it: An online learning variant of CSC where the model updates continuously, tested on time-series data with evolving cluster centroids or subspace bases.

## Limitations

- The disjoint masking assumption may break down at extreme missing rates (ρ < 0.1), where positive pairs become too sparse to preserve subspace structure
- The paper assumes missingness is random (Bernoulli), but doesn't validate performance under structured missingness (e.g., missing patches in images or bands in HSI)
- The claim that embeddings "preserve subspace structure" is empirical rather than theoretically proven; the contrastive objective doesn't explicitly enforce subspace constraints

## Confidence

- **High confidence**: The empirical results showing CSC outperforming baselines on all six datasets
- **Medium confidence**: The mechanism that disjoint random masking provides valid positive pairs for contrastive learning on incomplete data
- **Medium confidence**: The claim that contrastive learning can learn subspace-preserving embeddings without explicit subspace constraints

## Next Checks

1. Test CSC performance when missingness is structured (e.g., missing image quadrants or spectral bands) rather than random, to validate the random missingness assumption
2. Conduct an ablation study on the disjoint masking strategy by comparing with overlapping masks or zero-filling approaches to quantify the contribution of this design choice
3. Evaluate clustering performance on out-of-distribution incomplete samples (e.g., samples from different classes or datasets) to test generalization of learned embeddings