---
ver: rpa2
title: 'MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM
  Watermarking'
arxiv_id: '2512.04044'
source_url: https://arxiv.org/abs/2512.04044
tags:
- watermark
- text
- watermarking
- quality
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces MarkTune, an on-policy fine-tuning framework
  that improves the quality-detectability trade-off of weight-editing watermarking
  schemes like GaussMark by treating the watermark signal as a reward during training.
  MarkTune consistently achieves detection performance close to inference-time watermarking
  methods while maintaining generation quality and downstream task performance, and
  remains robust to paraphrasing and fine-tuning attacks.
---

# MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking

## Quick Facts
- arXiv ID: 2512.04044
- Source URL: https://arxiv.org/abs/2512.04044
- Reference count: 40
- Primary result: On-policy fine-tuning framework that improves quality-detectability trade-off of weight-editing watermarking

## Executive Summary
MarkTune is an on-policy fine-tuning framework that improves the quality-detectability trade-off of weight-editing watermarking schemes like GaussMark. By treating the GaussMark test statistic as a reward signal during training, MarkTune achieves detection performance close to inference-time methods while maintaining generation quality and downstream task performance. The framework remains robust to paraphrasing and fine-tuning attacks, addressing the key limitation of inference-time watermarking - the trade-off between watermark strength and generation quality.

## Method Summary
MarkTune operates by partitioning model weights into watermarked (θ_wm) and non-watermarked (θ_0) components, then applying GaussMark soft activation with a secret key ξ_σ. During on-policy fine-tuning using GRPO, it optimizes a dual objective: maximizing the watermark reward (the GaussMark test statistic) while regularizing against quality degradation through cross-entropy to a high-quality oracle model. The method targets specific transformer layers (up-projection MLP in layer 28 for Qwen3-4B, layer 30 for Llama2-7B) and uses group-relative advantages for stable training.

## Key Results
- Achieves higher AUC and TPR@1%FPR compared to GaussMark across multiple models and tasks
- Maintains generation quality with minimal perplexity increase (2-3% over base model)
- Preserves downstream task performance on MMLU, GSM8K, and MBPP benchmarks
- Remains robust to paraphrasing attacks (Dipper with diversity 20-60) and LoRA fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
Treating the GaussMark test statistic as a reward signal enables adaptive, watermark-aware weight updates that improve detectability without proportional quality loss. The reward term guides fine-tuning toward parameter regions where generated text aligns with the secret key, increasing the mean of the test statistic under the alternative hypothesis while regularization constrains quality degradation. This exploits the over-parameterized model representation space where watermark signal increases at first order while cross-entropy to a high-quality target grows only at second order.

### Mechanism 2
Regularizing against a high-quality oracle model (rather than the base model) expands the searchable parameter space, allowing watermark-sensitive updates that preserve generation quality. This permits larger deviations from the base model while remaining in high-quality basins, exploiting the insight that closeness to the suboptimal base model is not necessary for quality - closeness to p_θ* is. The base model should not be regarded as an oracle; one should prefer closeness to some optimal distribution.

### Mechanism 3
GRPO's group-relative advantage computation provides a stable, critic-free baseline for optimizing the watermark reward without requiring a learned value function. For each prompt, G outputs are sampled, rewards computed via the test statistic, and advantages normalized as standardized differences from the group mean. This within-group normalization removes prompt-specific baseline effects, stabilizing training across heterogeneous prompts.

## Foundational Learning

- **Statistical Hypothesis Testing (Null/Alternative Hypotheses, Type I/II Errors, Test Power)**
  - Why needed here: Watermarking is formalized as distinguishing H₀ (text independent of key) from H_A (text generated with key). Understanding false positive rate (α) and true positive rate (1-β) is essential for interpreting detection results.
  - Quick check question: If a watermark detector has TPR@FPR=1% of 0.96, what does this mean in terms of hypothesis testing?

- **Fisher Information Matrix and KL Geometry**
  - Why needed here: Proposition 1 shows KL divergence to base model scales with σ²d_r via the Fisher information; Appendix C uses Fisher geometry to prove first-order reward gains vs. second-order quality costs.
  - Quick check question: Why does the Fisher information matrix appear in the second-order Taylor expansion of KL divergence?

- **Policy Gradient Methods and Advantage Functions**
  - Why needed here: MarkTune uses GRPO, which computes group-relative advantages for policy updates. Understanding why advantages (not raw rewards) stabilize training is critical.
  - Quick check question: Why does subtracting a baseline (like group mean) from rewards reduce variance without introducing bias in policy gradient estimation?

## Architecture Onboarding

- **Component map:**
  Base Model (p_θ) → GaussMark Soft Activation → GRPO Fine-Tuning Loop → Watermarked Model θ*(ξ_σ) → Detection

- **Critical path:** The watermark key ξ_σ must be sampled once and fixed throughout training and detection. The reference model p_θ for gradient computation in the test statistic must be the original base model (not the fine-tuned model) to preserve statistical validity.

- **Design tradeoffs:**
  - Regularization coefficient λ (default 0.01): Higher values preserve quality but reduce detectability gains; ablation shows λ=0.01 (CE) is Pareto-optimal for Qwen3-4B.
  - Initial σ for soft activation (default 0.6-0.8): Smaller values start closer to quality-optimal region but require more training to reach detectability targets.
  - Group size G (default 8): Larger groups improve advantage estimation but increase memory/compute; minimal sensitivity observed between 4-16.

- **Failure signatures:**
  - Perplexity increases significantly (>10% over base): Regularization coefficient too low; model over-optimizes watermark reward at quality's expense.
  - TPR remains low despite training: σ may be too small, or watermark subspace (θ_wm) selection is poor; check that target layer has sufficient gradient magnitude.
  - Advantage estimates NaN or near-zero: Group size too small or reward variance collapsed; increase G or check reward computation.

- **First 3 experiments:**
  1. Reproduce quality-detectability curve (Figure 1): Train MarkTune with varying training steps (50-500) on a small model (e.g., Qwen3-4B subset), plotting TPR@1%FPR vs. perplexity to verify Pareto improvement over GaussMark baseline.
  2. Ablate regularization type: Compare CE regularization vs. KL-to-oracle vs. no regularization on a held-out validation set, measuring both detectability and downstream task accuracy (MMLU, GSM8K).
  3. Robustness stress test: Apply paraphrasing attacks (Dipper with diversity 20, 60) and LoRA fine-tuning (500-1500 steps) to watermarked model, plotting TPR decay curves to validate robustness claims.

## Open Questions the Paper Calls Out

### Open Question 1
How robust is MarkTune to aggressive full-parameter fine-tuning and model quantization attacks? The robustness evaluation is limited to LoRA fine-tuning and text-editing attacks, omitting common open-weight optimization techniques like quantization or full fine-tuning.

### Open Question 2
Does the computational overhead and trade-off efficacy of MarkTune scale effectively to models significantly larger than 7B parameters? The experiments are conducted exclusively on Qwen3-4B and Llama2-7B, leaving the scalability to 70B+ parameter models unstudied.

### Open Question 3
Does the optimization trajectory predicted by the stylized linear-softmax analysis hold for the complex loss landscapes of large-scale transformers? The theoretical motivation is derived from an idealized linear-softmax setting and relies on local assumptions that may not capture the full training dynamics of transformers.

## Limitations

- The framework's performance on model architectures beyond transformer-based models (e.g., Mamba, hybrid models) remains untested.
- The computational overhead of the on-policy fine-tuning process is not quantified against inference-time methods.
- The sensitivity to key selection and potential vulnerabilities to key-recovery attacks are not explored.
- Long-term stability of watermark detectability after extended deployment or further fine-tuning is unknown.

## Confidence

- **High Confidence**: MarkTune improves the quality-detectability Pareto frontier compared to GaussMark; the framework is robust to paraphrasing attacks and LoRA fine-tuning.
- **Medium Confidence**: MarkTune achieves detection performance close to inference-time methods while maintaining quality; the mechanism of first-order reward gains vs. second-order quality costs is theoretically sound.
- **Low Confidence**: The claim of being "asymptotically unbiased" is theoretically derived but not empirically validated; the assertion that oracle-based regularization is superior lacks ablation across diverse oracle choices.

## Next Checks

1. Evaluate MarkTune on models with different architectures (e.g., Mamba-based models, hybrid architectures) and parameter counts (e.g., 1B, 13B, 70B parameters) to assess generalizability.
2. Quantify the wall-clock time and memory overhead of MarkTune compared to inference-time watermarking methods across different model sizes and hardware configurations.
3. Conduct longitudinal studies tracking watermark detectability after 1000+ steps of continued fine-tuning, model compression (e.g., pruning, quantization), and knowledge distillation to assess long-term stability.