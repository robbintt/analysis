---
ver: rpa2
title: 'AI Governance to Avoid Extinction: The Strategic Landscape and Actionable
  Research Questions'
arxiv_id: '2505.04592'
source_url: https://arxiv.org/abs/2505.04592
tags:
- what
- development
- could
- project
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Humanity faces an unacceptably high likelihood of extinction from
  advanced AI systems that will soon exceed human performance across all domains.
  This report identifies critical research questions needed to avoid catastrophic
  outcomes, including loss of control from misaligned AI, misuse by malicious actors,
  great power war, and authoritarian lock-in.
---

# AI Governance to Avoid Extinction: The Strategic Landscape and Actionable Research Questions

## Quick Facts
- arXiv ID: 2505.04592
- Source URL: https://arxiv.org/abs/2505.04592
- Reference count: 40
- Primary result: Identifies ~400 research questions to build "Off Switch" infrastructure for halting dangerous AI development before catastrophic outcomes

## Executive Summary
Humanity faces an unacceptably high likelihood of extinction from advanced AI systems that will soon exceed human performance across all domains. This report identifies critical research questions needed to avoid catastrophic outcomes, including loss of control from misaligned AI, misuse by malicious actors, great power war, and authoritarian lock-in. The primary strategy proposed is building an "Off Switch" - the technical, legal, and institutional infrastructure to halt dangerous AI development globally when necessary. The report catalogs ~400 research questions organized around four scenarios: coordinated global Halt, US National Project for unilateral AI dominance, continued light-touch private sector development, and threat of sabotage between nations. The most urgent research focuses on understanding compute requirements, developing monitoring mechanisms, creating emergency response plans, and building international consensus for risk reduction.

## Method Summary
The report conducts qualitative strategic analysis of four AI governance scenarios, cataloging approximately 400 research questions organized by relevance to each scenario. Research questions are mapped to scenarios based on their role in enabling or preventing outcomes within each governance paradigm. Risk assessments for each scenario are provided across four dimensions (Misuse, War, Loss of Control, Lock-in) using qualitative color-coded ratings. The methodology lacks explicit inclusion criteria for research questions and relies on author judgment for risk assessments without detailed scoring rubrics.

## Key Results
- Humanity faces unacceptably high extinction risk from advanced AI systems within years
- Compute governance provides a tractable control point for enforcing AI development restrictions
- Building "Off Switch" infrastructure before crisis enables coordinated halting when political will emerges
- Getting international buy-in for risk reduction is identified as the most difficult hurdle

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Building "Off Switch" infrastructure before crisis enables coordinated halting of dangerous AI development.
- Mechanism: Pre-established technical monitoring + legal authority + institutional coordination → rapid response capability when political will emerges → global moratorium becomes enforceable → time to solve alignment problems before ASI deployment.
- Core assumption: Political consensus can eventually emerge, and infrastructure built in advance will remain functional when needed.
- Evidence anchors:
  - [abstract] "Our favored scenario involves building the technical, legal, and institutional infrastructure required to internationally restrict dangerous AI development and deployment (which we refer to as an Off Switch)"
  - [section 2.1] "Off Switch infrastructure should be developed in advance of when it is needed"
  - [corpus] "Technical Requirements for Halting Dangerous AI Activities" (FMR 0.64) addresses similar infrastructure prerequisites
- Break condition: If AI capabilities proliferate widely before Off Switch is operational, enforcement becomes infeasible.

### Mechanism 2
- Claim: Compute governance provides a tractable control point for enforcing AI development restrictions.
- Mechanism: Frontier AI training requires detectable concentrations of specialized chips → supply chain concentration enables monitoring → chip-level controls restrict dangerous training runs → breakout time extended sufficiently for response.
- Core assumption: Algorithmic progress does not drastically reduce compute requirements for dangerous capabilities, and distributed training remains limited.
- Evidence anchors:
  - [section 2.3.3] "Compute is a convenient node for governance because large numbers of chips are detectable, many chips are necessary for advanced AI training, chips are easily quantifiable, and the AI chip supply chain is concentrated"
  - [section 2.3.3] Notes algorithmic progress as key uncertainty: "Algorithmic progress may significantly reduce the compute required for dangerous AI"
  - [corpus] Limited direct empirical validation in corpus; mostly theoretical frameworks
- Break condition: If post-training or inference-time compute becomes sufficient for dangerous capabilities, or if algorithmic efficiency gains outpace hardware controls.

### Mechanism 3
- Claim: Verified international agreements can stabilize strategic dynamics and reduce racing pressures.
- Mechanism: Mutually visible AI development + credible verification mechanisms + agreed red lines → reduced fear of secret defection → racing pressure decreases → coordinated slowdown or halt becomes viable.
- Core assumption: Major powers prefer stability to racing when they can verify compliance, and breakout time exceeds detection/response time.
- Evidence anchors:
  - [section 4.2] "Achieving enough visibility and sabotage capability may require substantial multi-lateral coordination"
  - [section 4.2.3] References "Treaty on Open Skies" as historical precedent for verification building trust
  - [corpus] "The Loss of Control Playbook" (FMR 0.54) discusses preparedness frameworks but limited on international coordination specifics
- Break condition: If verification cannot keep pace with breakout capabilities, or if a major power decides unilateral advantage is preferable to stability.

## Foundational Learning

- Concept: Compute supply chain architecture
  - Why needed here: Understanding where chips are fabricated, how they're distributed, and what monitoring is feasible is foundational to compute governance proposals.
  - Quick check question: Can you explain why TSMC's role affects the viability of export controls?

- Concept: International regime verification theory
  - Why needed here: The Off Switch requires international cooperation; understanding how arms control verification historically succeeded or failed informs feasibility assessments.
  - Quick check question: What made the Treaty on Open Skies verification credible, and what would be analogous for AI?

- Concept: AI alignment difficulty
  - Why needed here: The entire risk model rests on alignment being hard; understanding the technical problem clarifies why "just be careful" is insufficient.
  - Quick check question: Why can't we simply train AI systems to be helpful and harmless through current methods?

## Architecture Onboarding

- Component map: Off Switch infrastructure consists of: (1) compute monitoring systems tracking chip locations and usage, (2) legal frameworks for domestic enforcement and international agreements, (3) institutional bodies for coordination, (4) emergency response protocols for acute risks, (5) verification mechanisms for international compliance.

- Critical path: The paper identifies "building common understanding about AI risks and getting buy-in from different actors" as the most difficult hurdle (marked with star in research taxonomy). Technical infrastructure without political will is insufficient.

- Design tradeoffs: Strong security around AI projects limits proliferation but may reduce international visibility needed for trust; centralized control enables enforcement but increases authoritarian lock-in risk; aggressive export controls may accelerate adversary self-sufficiency.

- Failure signatures: (1) Algorithmic progress makes compute governance obsolete faster than infrastructure is built, (2) Political will emerges only after catastrophic warning shot that causes unacceptable harm, (3) Verification lag time exceeds breakout time for major powers.

- First 3 experiments:
  1. Map current compute distribution: Locate major data centers, quantify untracked compute, assess minimum viable monitoring infrastructure.
  2. Prototype verification mechanisms: Test whether power monitoring, network analysis, or on-chip reporting could detect prohibited training runs with acceptable false positive rates.
  3. Analyze breakout scenarios: Model how long different actors would need to develop dangerous AI capabilities under various starting conditions (chip access, algorithmic knowledge, personnel).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the detailed design of the Off Switch, and how does the world build it?
- Basis in paper: [explicit] The authors mark this as "especially important" and state "Off Switch plans currently don't have a concrete goal. Laying out a detailed design will enable other work toward useful intermediate goals."
- Why unresolved: The paper notes that no satisfactory detailed design exists, and the infrastructure requires unprecedented technical, legal, and institutional coordination across nations.
- What evidence would resolve it: A concrete specification detailing verification mechanisms, enforcement authorities, escalation protocols, and the minimum viable infrastructure needed to halt dangerous AI development globally.

### Open Question 2
- Question: How viable is compute governance—can governments prevent uncooperative actors from obtaining sufficient compute for dangerous AI systems?
- Basis in paper: [explicit] The authors state this question is important because "a large fraction of AI governance research and regulation are premised on compute being a useful node for governing AI, and priorities would need to shift substantially if this assumption turns out to be false."
- Why unresolved: Algorithmic progress may reduce compute requirements unpredictably, smuggling pathways remain unclear, and alternative supply chains could emerge.
- What evidence would resolve it: Empirical analysis of smuggling rates, algorithmic efficiency gains over time, and whether compute-constrained actors can still achieve dangerous capability thresholds.

### Open Question 3
- Question: How can governments monitor compute they know about to ensure it isn't being used to violate a Halt?
- Basis in paper: [explicit] Marked as especially important: "the Halt strategy relies on governments having methods to monitor certain classes of compute and ensuring that compute is not being used for illicit activities."
- Why unresolved: Current monitoring tools are limited, workload classification at different compute stack layers remains difficult, and privacy concerns constrain politically feasible approaches.
- What evidence would resolve it: Demonstrated monitoring systems that can distinguish between permitted and prohibited AI workloads in privacy-preserving ways, tested in realistic data center environments.

## Limitations
- Technical feasibility of compute governance may become obsolete before infrastructure is built due to algorithmic progress
- Political will assumptions lack modeling of conditions under which consensus might fail
- Verification capability gaps not addressed - current technology may not provide required detection capabilities

## Confidence
- High confidence: Identification of loss of control and misuse as primary existential risks from advanced AI systems
- Medium confidence: Scenario analysis provides useful framework though risk ratings are subjective
- Low confidence: Research question catalog lacks explicit inclusion criteria and systematic generation process

## Next Checks
1. Compute governance feasibility study: Model relationship between algorithmic progress and compute requirements for dangerous capabilities, establishing timelines for when compute controls become technically insufficient.

2. Political feasibility analysis: Survey international relations experts on likelihood of achieving required consensus for AI development restrictions under different geopolitical scenarios.

3. Verification technology assessment: Prototype detection mechanisms for prohibited AI training activities using current monitoring technologies to establish detection capabilities and false positive rates.