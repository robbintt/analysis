---
ver: rpa2
title: 'Normalize Then Propagate: Efficient Homophilous Regularization for Few-shot
  Semi-Supervised Node Classification'
arxiv_id: '2501.08581'
source_url: https://arxiv.org/abs/2501.08581
tags:
- node
- nodes
- graph
- normprop
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of semi-supervised node classification
  with few labeled nodes, where traditional graph neural networks (GNNs) struggle
  due to insufficient supervision signals. The authors propose NormProp, a method
  that improves generalization by leveraging unlabeled nodes through a novel "normalize
  then propagate" framework.
---

# Normalize Then Propagate: Efficient Homophilous Regularization for Few-shot Semi-Supervised Node Classification

## Quick Facts
- arXiv ID: 2501.08581
- Source URL: https://arxiv.org/abs/2501.08581
- Authors: Baoming Zhang; MingCai Chen; Jianqing Song; Shuangjie Li; Jie Zhang; Chongjun Wang
- Reference count: 9
- Primary result: Achieves state-of-the-art accuracy on 6 node classification datasets in few-shot scenarios with significantly lower computational cost

## Executive Summary
This paper addresses the challenge of semi-supervised node classification when only a few labeled nodes are available. The authors propose NormProp, a method that improves generalization by leveraging unlabeled nodes through a novel "normalize then propagate" framework. This approach decouples node representation directions (class information) and norms (consistency of aggregation) by mapping features to a hyperspherical space and applying low-pass filtering. The method introduces homophilous regularization that constrains the consistency of unlabeled nodes based on the homophily assumption, providing additional supervision signals. Experimental results demonstrate that NormProp achieves state-of-the-art performance across multiple datasets while maintaining high computational efficiency.

## Method Summary
NormProp uses a "normalize then propagate" framework where node features are first encoded by a 2-layer MLP and L2-normalized to project them onto a hyperspherical space. A fixed K-step low-pass filter (similar to SGC) then propagates these normalized representations. Classification is performed using cosine similarity to hyperspherical prototypes. The key innovation is homophilous regularization, which adds a consistency constraint on high-confidence unlabeled nodes by maximizing their normalized norms. The method trains with a warm-up phase using only classification loss before adding the regularization term. The approach leverages the mathematical property that the norm of propagated representations provides an upper bound on aggregation consistency, which under homophily conditions correlates with class membership reliability.

## Key Results
- Achieves state-of-the-art performance in few-shot scenarios across Cora, Citeseer, Pubmed, Cora-ML, MS-CS, and Ogbn-arxiv datasets
- On Ogbn-arxiv with 2.5% labeled data, achieves 64.87% accuracy with 17% of Meta-PN's training time
- Demonstrates high computational efficiency with significantly lower training and inference times compared to baselines
- Outperforms methods like Meta-PN, M3S, and Violin in both accuracy and efficiency metrics

## Why This Works (Mechanism)

### Mechanism 1: Direction-Norm Decoupling via Hyperspherical Projection
- Claim: Mapping features to a unit hypersphere before propagation allows direction and Euclidean norm to independently encode class information and aggregation consistency
- Core assumption: Direction and norm can be cleanly separated - direction for discriminative class signals, norm for structural consistency
- Evidence: [abstract] "decoupling the direction and Euclidean norm of node representations"; [section 3] "Z(K) can be decouple into direction and Euclidean norm"
- Break condition: On heterophilic graphs where connected nodes tend to have different labels, higher norm may not reliably indicate positive consistency

### Mechanism 2: Upper Bound on Norm as a Consistency Proxy
- Claim: The Euclidean norm after propagation is bounded above and positively correlates with aggregation consistency
- Core assumption: Parallel (aligned) neighbor representations imply high neighbor consistency under homophily
- Evidence: [section 4.1] Derivation of upper bound; "Euclidean norm of the final node representation is positively correlated with consistency of aggregation"
- Break condition: When neighbor representations are aligned for reasons unrelated to class consistency (e.g., feature collapse)

### Mechanism 3: Homophilous Regularization from High-Confidence Unlabeled Nodes
- Claim: Regularizing consistency of high-confidence unlabeled nodes yields additional supervision that improves generalization under label scarcity
- Core assumption: High-confidence predictions should correspond to subgraphs with high neighbor consistency (homophily)
- Evidence: [abstract] "homophilous regularization, which constrains the consistency of unlabeled nodes based on the homophily assumption"; [section 4.3] "L_h serves as the contrastive loss"
- Break condition: If confidence threshold is too low, noisy nodes degrade regularization; if weight is too high, representations over-smooth

## Foundational Learning

- **Low-pass filtering in GNNs and over-smoothing**
  - Why needed: NormProp explicitly uses fixed K-step low-pass filter and avoids deep message passing to mitigate over-smoothing
  - Quick check: Can you explain why repeatedly applying normalized adjacency tends to flatten node representations, and how decoupling transformation from propagation helps?

- **Homophily assumption**
  - Why needed: The consistency interpretation of norms and homophilous regularization both presuppose that connected nodes are likely to share labels
  - Quick check: What behavior would you expect from homophilous regularization on a graph where edges preferentially connect nodes of different classes?

- **Decoupled GNN architectures (e.g., APPNP, SGC)**
  - Why needed: NormProp follows the decoupled paradigm - first transform features with MLP, then propagate with fixed low-pass filter
  - Quick check: How does separating feature transformation from propagation differ from standard GCN layer design, and what efficiency or analysis benefits does it provide?

## Architecture Onboarding

- **Component map:**
  - Node features X -> MLP encoder -> L2 normalization -> Z^(0) -> K-step propagation (P^K) -> Z^(K) -> Cosine similarity to prototypes -> Classification
  - High-confidence unlabeled nodes -> Homophilous regularization (L_h) -> Joint loss with classification loss (L_c)

- **Critical path:**
  1. Encode features with MLP → L2 normalize to Z^(0)
  2. Propagate K steps: Z^(K) = P^K Z^(0)
  3. For labeled nodes: compute L_c via cosine similarity to true class prototype
  4. After warm-up, add L_h over nodes with confidence ≥ τ
  5. Backpropagate joint loss L with mixing coefficient λ

- **Design tradeoffs:**
  - Propagation depth K: larger K enlarges receptive field but increases smoothing risk
  - Regularization weight λ: too small underuses unlabeled supervision; too large encourages over-smooth solutions
  - Confidence threshold τ: controls quality vs. quantity of nodes in regularization
  - Warm-up: training with only L_c initially stabilizes representation learning

- **Failure signatures:**
  - Accuracy plateaus when λ is too high (representations over-smooth)
  - Noisy or unstable training when τ is too low (low-confidence nodes inject conflicting signals)
  - Poor performance on heterophilic graphs (homophily assumption violated)
  - Underfitting when K is too small or encoder capacity is insufficient

- **First 3 experiments:**
  1. Lambda (λ) sweep on validation set to identify optimal regularization strength
  2. Threshold (τ) sweep on validation set to assess confidence filtering effects
  3. Ablate warm-up: compare training with and without initial warm-up phase

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical interpretation of norms as consistency proxies relies heavily on the homophily assumption, which may not hold on heterophilic graphs
- Exact hyperspherical prototype initialization method is not fully specified in the main text
- Hyperparameter sensitivity to λ, τ, and warm-up duration could affect reproducibility across datasets

## Confidence
- Mechanism 1 (Direction-norm decoupling): Medium - supported by theoretical derivation but requires empirical validation
- Mechanism 2 (Norm as consistency proxy): High - mathematically rigorous with clear triangle inequality bounds
- Mechanism 3 (Homophilous regularization): Medium - logical framework but limited direct empirical evidence
- Experimental results: High - comprehensive across multiple datasets with clear baselines

## Next Checks
1. Test NormProp on a known heterophilic graph (e.g., Texas, Wisconsin) to assess robustness when homophily assumption fails
2. Implement ablation studies varying K (propagation steps) to identify optimal trade-off between receptive field and over-smoothing
3. Conduct sensitivity analysis on the confidence threshold τ by sweeping values and measuring Ω_τ node count and classification performance