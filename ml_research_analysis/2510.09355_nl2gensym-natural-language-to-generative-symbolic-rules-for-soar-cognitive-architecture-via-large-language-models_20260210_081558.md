---
ver: rpa2
title: 'NL2GenSym: Natural Language to Generative Symbolic Rules for SOAR Cognitive
  Architecture via Large Language Models'
arxiv_id: '2510.09355'
source_url: https://arxiv.org/abs/2510.09355
tags:
- rules
- soar
- knowledge
- operator
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NL2GenSym, a framework that integrates Large
  Language Models with the SOAR cognitive architecture to automatically generate and
  optimize executable symbolic rules from natural language. The core innovation is
  an execution-grounded generator-critic loop, where rules are iteratively refined
  based on feedback from SOAR execution.
---

# NL2GenSym: Natural Language to Generative Symbolic Rules for SOAR Cognitive Architecture via Large Language Models

## Quick Facts
- arXiv ID: 2510.09355
- Source URL: https://arxiv.org/abs/2510.09355
- Reference count: 40
- Primary result: Over 86% success rate in generating executable SOAR rules from natural language, with decision cycles reduced to 1.98× optimal

## Executive Summary
NL2GenSym introduces a framework that bridges Large Language Models (LLMs) with the SOAR cognitive architecture to automatically generate and optimize executable symbolic rules from natural language. The system employs an execution-grounded generator-critic loop where rules are iteratively refined based on feedback from SOAR execution. Tested on Water Jug Problem datasets, the framework demonstrates remarkable efficiency improvements over baseline methods while showing that architectural design can outperform larger models. The approach represents a significant step toward automated cognitive system programming.

## Method Summary
The NL2GenSym framework integrates LLMs with the SOAR cognitive architecture through an iterative generator-critic loop. Natural language inputs are processed by LLMs to generate candidate SOAR rules, which are then executed within the SOAR environment. The execution results provide feedback that guides the LLM in refining subsequent rule generations. This process continues until convergence or a maximum iteration limit is reached. The framework employs specialized prompt engineering and incorporates rule verification mechanisms to ensure generated rules are both syntactically correct and functionally executable within SOAR's production system framework.

## Key Results
- Achieves over 86% success rate in generating executable symbolic rules from natural language
- Reduces decision cycles to near-optimal levels (1.98× the theoretical optimum)
- Outperforms baseline methods by a factor of 1000
- Smaller-parameter models guided by NL2GenSym surpass larger counterparts in performance

## Why This Works (Mechanism)
The framework succeeds through its execution-grounded feedback loop that aligns rule generation with actual system behavior. By continuously refining rules based on SOAR execution outcomes, the system progressively approaches optimal solutions rather than relying solely on pattern matching. The generator-critic architecture creates a closed-loop system where linguistic understanding is validated against computational execution, ensuring that generated rules are not just syntactically valid but also functionally effective. This iterative refinement process allows the system to discover and correct subtle semantic errors that would be difficult to identify through static analysis alone.

## Foundational Learning
- **SOAR Cognitive Architecture**: A symbolic cognitive architecture that uses production rules and working memory to model human problem-solving
  - Why needed: Provides the execution environment and target framework for rule generation
  - Quick check: Can the system execute and evaluate rules within SOAR's production system framework
- **LLM Prompt Engineering**: Techniques for crafting effective prompts that guide LLMs toward desired outputs
  - Why needed: Ensures LLMs generate syntactically correct and semantically appropriate SOAR rules
  - Quick check: Does prompt structure consistently produce executable rule candidates
- **Execution-Grounded Learning**: Feedback mechanisms that use system execution results to refine future generations
  - Why needed: Creates a closed-loop system that progressively improves rule quality
  - Quick check: Does each iteration show measurable improvement in rule effectiveness
- **Rule Verification Systems**: Methods for validating the syntactic and semantic correctness of generated rules
  - Why needed: Prevents propagation of invalid rules through the iterative refinement process
  - Quick check: Can the system detect and reject non-executable rule candidates
- **Iterative Refinement Processes**: Systematic approaches to progressively improve solutions through repeated cycles
  - Why needed: Enables convergence toward optimal rule sets through gradual improvement
  - Quick check: Does the system show consistent improvement across multiple iterations

## Architecture Onboarding

**Component Map**: Natural Language Input -> LLM Generator -> Rule Verification -> SOAR Execution -> Execution Feedback -> LLM Critic -> Refined Rules

**Critical Path**: NL Input → LLM Generator → Rule Verification → SOAR Execution → Execution Feedback → LLM Refinement → Output

**Design Tradeoffs**: The framework balances computational cost against rule quality through its iterative approach. While the execution-grounded loop ensures high-quality outputs, it requires multiple SOAR execution cycles, creating significant computational overhead. The system trades speed for accuracy by prioritizing functional correctness over rapid generation. The choice to use smaller LLMs with architectural guidance rather than larger models represents a resource-efficiency tradeoff that appears to yield superior results in this domain.

**Failure Signatures**: 
- Rule generation failure: LLMs produce syntactically invalid SOAR rules
- Execution failure: Generated rules cannot be executed in SOAR environment
- Non-convergence: Iterative process fails to reach acceptable solution within iteration limits
- Performance degradation: Later iterations produce worse results than earlier ones
- Computational bottleneck: Excessive execution cycles prevent practical deployment

**First 3 Experiments**:
1. Generate and execute rules for a simple Water Jug Problem instance (e.g., 3L and 5L jugs to obtain 4L)
2. Compare rule quality and execution cycles between different LLM sizes (small vs large models)
3. Test the framework's response to malformed natural language inputs and measure error recovery capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can NL2GenSym effectively translate natural language into executable symbolic rules for complex, noisy real-world tasks like logistics planning or robotic control?
- Basis in paper: [explicit] The authors state in Section V and VII that validation on the theoretical Water Jug Problem (WJP) "may not fully capture the complexities of real-world applications," explicitly listing "logistics planning or robotic task execution" as necessary next steps.
- Why unresolved: The current study relies exclusively on a "well-defined theoretical task" (WJP) that lacks the ambiguity and dynamic contexts found in practical engineering applications.
- What evidence would resolve it: Successful generation and optimization of SOAR rules on a practical dataset involving noisy data or dynamic environments, demonstrating a comparable success rate (>86%) to the WJP results.

### Open Question 2
- Question: How can the iterative refinement process be optimized to guarantee linear improvement and reduce the substantial computational costs of the execution-grounded loop?
- Basis in paper: [explicit] Section V acknowledges that the framework "incurs substantial computational costs and does not always yield a linear or monotonic improvement in rule quality."
- Why unresolved: The current generator-critic architecture relies on a feedback loop that can be inefficient and unpredictable, requiring multiple execution cycles to converge on a solution.
- What evidence would resolve it: The development of prompt engineering or RAG mechanisms that demonstrably shorten the iterative learning cycle and ensure consistent efficiency gains per iteration.

### Open Question 3
- Question: To what extent do the LLMs genuinely comprehend the underlying principles of the SOAR cognitive architecture versus simply mimicking syntactic patterns?
- Basis in paper: [explicit] The authors state in Section V that "the depth of their genuine comprehension of the underlying principles of SOAR remains an open question."
- Why unresolved: While the framework produces functional rules, it is unclear if the LLM has learned a transferable understanding of SOAR's semantics or is relying on probabilistic surface-level pattern matching.
- What evidence would resolve it: A comprehensive analysis showing that rules generated for novel, structurally distinct problems adhere to SOAR best practices without requiring extensive debugging or prior examples.

### Open Question 4
- Question: Can a universal model-adaptive prompting strategy be developed to maintain high performance across diverse LLM families without requiring specific hyperparameter tuning?
- Basis in paper: [explicit] Section VII identifies the need to investigate "model-adaptive prompting strategies" to address "varying sensitivities and learning efficacies across LLMs."
- Why unresolved: The paper demonstrates that architectural design aids smaller models, but future work is needed to ensure the framework is robust and universal across different model architectures.
- What evidence would resolve it: Empirical results showing that NL2GenSym maintains high success rates (near 86-91%) across multiple distinct LLM families using a standardized prompt configuration.

## Limitations
- Current evaluation limited to a single theoretical problem domain (Water Jug Problem)
- Substantial computational costs from iterative execution-grounded refinement
- Uncertainty about LLMs' genuine understanding versus pattern matching of SOAR principles
- Need for model-adaptive prompting strategies across diverse LLM architectures

## Confidence

| Claim | Confidence |
|-------|------------|
| 86% success rate on Water Jug Problem | High |
| 1.98× optimal decision cycles | High |
| Architectural design outperforms model scale | Medium |
| Framework generalizes to complex real-world tasks | Low |
| Execution-grounded loop guarantees convergence | Medium |

## Next Checks

1. Test NL2GenSym on multiple distinct problem domains (e.g., Tower of Hanoi, Blocks World) to assess cross-task generalization
2. Benchmark execution time and computational resources required for the generator-critic loop across different LLM sizes
3. Conduct ablation studies removing the execution feedback component to quantify its contribution to success rates