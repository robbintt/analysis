---
ver: rpa2
title: Regret Bounds for Adversarial Contextual Bandits with General Function Approximation
  and Delayed Feedback
arxiv_id: '2510.09127'
source_url: https://arxiv.org/abs/2510.09127
tags:
- regret
- algorithm
- bound
- function
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of regret minimization in adversarial
  contextual multi-armed bandits (CMAB) with delayed feedback. The authors consider
  two main settings: policy class learning, where the learner has direct access to
  a finite policy class, and online function approximation, where the learner has
  access to a (possibly infinite) contextual loss function class via an online least-squares
  regression oracle.'
---

# Regret Bounds for Adversarial Contextual Bandits with General Function Approximation and Delayed Feedback

## Quick Facts
- arXiv ID: 2510.09127
- Source URL: https://arxiv.org/abs/2510.09127
- Authors: Orin Levy; Liad Erez; Alon Cohen; Yishay Mansour
- Reference count: 40
- Primary result: Achieves optimal expected regret bounds for adversarial contextual bandits with delayed feedback, both for finite policy classes and function approximation settings

## Executive Summary
This paper addresses the problem of regret minimization in adversarial contextual multi-armed bandits (CMAB) with delayed feedback. The authors consider two main settings: policy class learning, where the learner has direct access to a finite policy class, and online function approximation, where the learner has access to a (possibly infinite) contextual loss function class via an online least-squares regression oracle. The work establishes both upper and lower bounds, demonstrating the optimality of the proposed algorithms.

For the policy class learning setup, the authors establish an optimal expected regret bound of O(sqrt(KT log |Π|) + sqrt(D log |Π|)), where D is the sum of delays and Π is the policy class. This bound is proven to be optimal via a matching lower bound. In the function approximation setting, the authors achieve an expected regret bound of O(sqrt(KT R_T(O) + sqrt(d_max D β)), assuming FIFO order, where d_max is the maximal delay, R_T(O) is an upper bound on the oracle's regret, and β is a stability parameter associated with the oracle.

## Method Summary
The authors introduce a delay-adapted version of the SquareCB algorithm that handles delayed feedback in adversarial contextual bandits. For policy class learning, they use a standard approach with delayed feedback aggregation. For function approximation, they leverage an online least-squares regression oracle and provide a novel stability analysis of a Hedge-based version of Vovk's aggregating forecaster. The key insight is that by carefully accounting for delays in the loss estimates and using appropriate concentration inequalities, they can maintain low regret even with significant feedback delays.

## Key Results
- Establishes optimal expected regret bound of O(sqrt(KT log |Π|) + sqrt(D log |Π|)) for policy class learning, with matching lower bound
- Achieves expected regret bound of O(sqrt(KT R_T(O) + sqrt(d_max D β)) for function approximation with FIFO delays
- Proves stability parameter β is bounded by log|F| for finite function classes using Hedge-based Vovk's forecaster
- Demonstrates sqrt(d_max) gap between upper and lower bounds in function approximation setting

## Why This Works (Mechanism)
The algorithm works by carefully handling delayed feedback through loss estimation and concentration bounds. For policy classes, delayed losses are aggregated and used with appropriate confidence intervals. For function approximation, the online regression oracle provides loss estimates for each context-arm pair, and the stability of the Hedge-based forecaster ensures bounded regret growth with respect to delays.

## Foundational Learning
- **Adversarial Contextual Bandits**: Online learning setting where losses are chosen by an adversary
  - Why needed: Provides the framework for sequential decision-making under uncertainty
  - Quick check: Can model non-stationary environments and worst-case guarantees

- **Delayed Feedback**: Rewards or losses are observed with arbitrary delays
  - Why needed: Many real-world applications have inherent feedback delays
  - Quick check: Must account for uncertainty in when feedback arrives

- **Online Least-Squares Regression**: Sequential prediction with linear models
  - Why needed: Enables function approximation for infinite policy classes
  - Quick check: Provides tractable updates for contextual decision-making

- **Hedge-based Aggregating Forecaster**: Combines multiple experts' predictions
  - Why needed: Enables stable combination of regression oracles
  - Quick check: Maintains bounded regret through multiplicative weights

- **Stability Analysis**: Measures how sensitive predictions are to input changes
  - Why needed: Controls regret growth with respect to delays
  - Quick check: Small stability parameter β leads to better regret bounds

- **SquareCB Algorithm**: Contextual bandit algorithm using confidence bounds
  - Why needed: Provides exploration-exploitation trade-off in adversarial setting
  - Quick check: Can be adapted to handle delayed feedback

## Architecture Onboarding

Component map:
Policy class setting: SquareCB -> Delayed loss aggregation -> Regret bound
Function approximation: Online regression oracle -> Hedge-based forecaster -> Delayed feedback handling -> Regret bound

Critical path:
1. Observe context and receive delayed losses
2. Update loss estimates for delayed feedback
3. Compute confidence bounds for each action
4. Select action using Hedge-based aggregation
5. Update regression oracle with new information
6. Repeat for next round

Design tradeoffs:
- FIFO assumption simplifies analysis but may not hold in practice
- Finite vs infinite function classes require different regret formulations
- Stability vs regret: more stable methods may have higher regret
- Computational complexity vs approximation quality for regression oracle

Failure signatures:
- Regret grows linearly with T instead of sqrt(T) indicates poor concentration bounds
- High regret with moderate delays suggests instability in forecaster
- Non-FIFO delays causing significant regret degradation indicates algorithmic sensitivity
- Regression oracle performance degrading over time suggests exploration issues

First experiments:
1. Test policy class algorithm on synthetic data with known optimal policy
2. Evaluate function approximation with varying delay distributions
3. Compare stability of different regression oracle implementations

## Open Questions the Paper Calls Out
The paper identifies several open questions, particularly regarding the sqrt(d_max) gap between upper and lower bounds in the function approximation setting. The authors note that closing this gap and extending results to non-FIFO delay patterns remain important directions for future work.

## Limitations
- Results assume FIFO delay order, which may not hold in many practical settings
- sqrt(d_max) gap between upper and lower bounds in function approximation setting suggests room for improvement
- Online least-squares regression oracle may be computationally demanding in practice
- Stability analysis limited to finite function classes; infinite class behavior unclear

## Confidence

High confidence:
- Policy class regret bounds: Upper and lower bounds match, proving optimality
- Stability analysis of Hedge-based Vovk's forecaster: Detailed proof with clear intuition

Medium confidence:
- Function approximation regret bounds: Relies on FIFO assumption and has sqrt(d_max) gap
- Lower bounds tightness: While matching in policy class setting, function approximation bounds leave room for improvement

## Next Checks

1. Test the algorithm's performance under non-FIFO delay patterns to verify robustness beyond the theoretical assumptions
2. Implement the Hedge-based Vovk's forecaster oracle and measure its computational complexity in practice
3. Evaluate the algorithm on a real-world delayed feedback dataset to assess practical regret performance