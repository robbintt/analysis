---
ver: rpa2
title: 'ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for
  LLM Agents'
arxiv_id: '2505.06416'
source_url: https://arxiv.org/abs/2505.06416
tags:
- tool
- company
- search
- tools
- openai
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose ScaleMCP, a framework for LLM agents to dynamically
  discover and invoke Model Context Protocol (MCP) tools during multi-turn interactions.
  The key innovation is an auto-synchronization tool storage pipeline that treats
  MCP servers as the single source of truth, automatically reflecting CRUD operations
  in the storage index.
---

# ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for LLM Agents

## Quick Facts
- **arXiv ID:** 2505.06416
- **Source URL:** https://arxiv.org/abs/2505.06416
- **Reference count:** 40
- **Key outcome:** Dynamic agent retrieval of MCP tools improves performance over static baselines across 5,000 financial metric servers.

## Executive Summary
ScaleMCP introduces a framework for large language model (LLM) agents to dynamically discover and invoke Model Context Protocol (MCP) tools during multi-turn interactions. The system uses an auto-synchronization tool storage pipeline that treats MCP servers as the single source of truth, automatically reflecting create, read, update, and delete operations in the storage index. A novel Tool Document Weighted Average (TDWA) embedding strategy selectively emphasizes critical components of tool documents. Experiments demonstrate improved tool retrieval and agent invocation performance compared to static baselines, with LLM-based reranking yielding the highest recall and mean average precision scores.

## Method Summary
ScaleMCP employs an auto-synchronization pipeline that hashes tool content (name, description, arguments) using SHA-256 to maintain consistency between MCP servers and storage. The system implements a Tool Document Weighted Average (TDWA) embedding strategy that weights different tool document components (name, description, synthetic questions) before averaging and normalizing. For evaluation, the framework uses a dataset of 5,000 financial metric MCP servers and 140,000 synthetic user queries. The agent is equipped with a retrieval tool that allows dynamic discovery of additional tools during interaction, with performance measured through retrieval metrics (NDCG, Recall, MAP) and agent invocation success rates via DeepEval alignment scoring.

## Key Results
- ScaleMCP improves tool retrieval and agent invocation performance compared to static baseline approaches
- LLM-based reranking with vector search achieves highest recall and MAP scores for complex multi-hop queries
- Agentic retrieval enables more flexible and accurate multi-hop tool invocation across 5,000 financial metric MCP servers

## Why This Works (Mechanism)

### Mechanism 1: Hash-Based Single Source of Truth
- **Claim:** Hash-based synchronization reduces manual updates and tool duplication by treating MCP servers as the single source of truth.
- **Mechanism:** An indexing pipeline computes SHA-256 hashes for each tool and compares them against stored hashes, triggering CRUD operations to align storage with MCP servers.
- **Core assumption:** Tool definitions are stable enough for efficient hashing and MCP server metadata is consistent.
- **Evidence anchors:** Auto-synchronization through CRUD operations with MCP servers as the single source of truth (abstract); driven by auto-synchronization tool storage indexing pipeline comparing hashes to MCP storage system hashes (section 3.2).
- **Break condition:** Dynamic schema changes at runtime trigger excessive re-indexing loops.

### Mechanism 2: Component-Weighted Embedding (TDWA)
- **Claim:** Weighted component embeddings prioritize high-signal fields like synthetic questions or tool names for improved retrieval.
- **Mechanism:** Tool documents are decomposed into components, each embedded separately, weighted by a configurable vector, then summed and normalized to unit length.
- **Core assumption:** Different documentation parts carry distinct semantic importance that uniform averaging would dilute.
- **Evidence anchors:** TDWA designed to selectively emphasize critical components (abstract); allows finer control over semantic contribution and improves document relevance when paired with scoring model (section 5.3.3).
- **Break condition:** Static weights underperform when optimal weights vary drastically across tool domains.

### Mechanism 3: Agentic Retrieval Loop
- **Claim:** Providing agents with a retrieval tool enables autonomous multi-hop reasoning and dynamic re-querying.
- **Mechanism:** Instead of pre-loading all tools, the system injects a `get_mcp_servers` function that agents can call with keywords to retrieve and bind relevant tool schemas.
- **Core assumption:** The LLM can recognize when it lacks tools and construct effective search queries.
- **Evidence anchors:** Giving agents autonomy to add tools into memory and preventing dynamic re-querying capabilities during multi-turn interactions (abstract); agent can continue to re-query MCP storage system if no successful matches are found (section 3.3).
- **Break condition:** Agent hallucination of non-existent keywords or failure to compose effective search queries results in empty retrieval.

## Foundational Learning

- **Concept: Model Context Protocol (MCP)**
  - **Why needed here:** MCP standardizes how tools expose schemas to clients, forming the foundational transport layer.
  - **Quick check question:** Does the agent call the tool directly, or does it connect to an MCP server that exposes the tool? (Answer: It connects to the server).

- **Concept: Reranking vs. Retrieval**
  - **Why needed here:** Understanding how rerankers rescore initial results is critical for interpreting performance gains over vector search alone.
  - **Quick check question:** Why does the paper suggest vector search alone is insufficient for complex financial queries? (Answer: Single embeddings struggle to capture multiple distinct intents/multi-hop requirements).

- **Concept: Tool Binding / Function Calling**
  - **Why needed here:** Once tools are retrieved, binding makes them executable by allowing the model to output structured JSON arguments.
  - **Quick check question:** What happens after the `get_mcp_servers` tool returns a list of tools? (Answer: The agent "binds" these new tools to its context to call them).

## Architecture Onboarding

- **Component map:** MCP Servers -> Auto-Sync Pipeline -> Storage Index -> Agent Loop
- **Critical path:** The Auto-Sync Pipeline. If this lags or fails, the Storage Index diverges from the MCP Servers, and the agent retrieves stale or deleted tools.
- **Design tradeoffs:**
  - Latency vs. Autonomy: Agentic retrieval adds a turn before execution, increasing latency compared to static tool loading, but enables scaling beyond context limits.
  - TDWA vs. Concat: TDWA offers control but requires tuning weights; simple concatenation was surprisingly strong on this dataset.
- **Failure signatures:**
  - High Churn: Excessive "Update" operations indicate unstable tool definitions.
  - Empty Retrieval: Agent uses `get_mcp_servers` but retrieves nothing (likely vocabulary mismatch).
  - Context Overflow: Agent retrieves too many tools for a multi-hop query, exceeding the context window.
- **First 3 experiments:**
  1. Validate Sync Logic: Deploy pipeline and intentionally change a tool description on an MCP server. Verify hash mismatch triggers an "Update" in storage.
  2. Calibrate TDWA Weights: Run retrieval tests varying weights against simple concatenation on your specific domain data to see if weighting helps.
  3. Agentic Loop Test: Give an LLM a complex multi-hop query. Measure if it successfully calls the retrieval tool → binds the result → calls the new tool, versus failing to find the tool.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can dynamic agent-driven retrieval match or exceed the performance of LLM-based reranking while maintaining lower inference costs?
- Basis in paper: Section 5.1.3 states the authors plan to evaluate whether dynamic agent-driven retrieval using ScaleMCP can match or exceed reranked performance — but at lower inference cost.
- Why unresolved: LLM-based reranking currently yields the highest scores but is computationally expensive; the efficiency trade-off of the agentic approach remains unquantified.
- What evidence would resolve it: Comparative benchmarks measuring latency, token usage, and cost against Recall/MAP scores for both approaches.

### Open Question 2
- Question: Does the Tool Document Weighted Average (TDWA) strategy outperform simple concatenation when evaluated on human-written queries?
- Basis in paper: Section 5.3.3 notes that simple concatenation won on synthetic data, but suggests TDWA may offer better interpretability and robustness in diverse deployment contexts and proposes testing with human-written queries.
- Why unresolved: The synthetic queries used for evaluation may have been biased towards the concatenation method, masking the potential benefits of weighted averaging on natural language.
- What evidence would resolve it: Retrieval performance metrics (NDCG, Recall) comparing TDWA and concatenation strategies on a dataset of organic user queries.

### Open Question 3
- Question: How does ScaleMCP perform in domains outside of financial metrics, such as healthcare or legal research?
- Basis in paper: Section 7 states the evaluation dataset focused primarily on financial metrics, which limits the generalizability of the findings to other domains.
- Why unresolved: The current dataset uses deterministic financial templates; it is unclear if the retrieval and tool invocation success rates hold for domains with more complex or ambiguous tool structures.
- What evidence would resolve it: Evaluation results from applying the ScaleMCP framework to curated datasets in healthcare, law, or other distinct professional fields.

## Limitations
- Evaluation focuses heavily on synthetically generated financial MCP servers, potentially missing real-world tool ecosystem diversity
- Hash-based synchronization assumes tool metadata stability that may not hold in dynamic environments
- TDWA weighting strategy requires careful domain-specific tuning, with fixed weights potentially underperforming across different tool types

## Confidence
- **High Confidence:** Core mechanism of auto-synchronizing tool storage via hash comparison (Section 3.2) and agentic retrieval loop design (Section 3.3) are well-specified and reproducible
- **Medium Confidence:** TDWA embedding strategy shows promise but requires domain-specific weight tuning; effectiveness may vary significantly across different tool domains
- **Low Confidence:** Scalability claims regarding 5,000 concurrent MCP servers lack detailed infrastructure validation, and real-world performance on heterogeneous tool ecosystems remains uncertain

## Next Checks
1. **Infrastructure Stress Test:** Deploy the auto-sync pipeline with 100+ dynamically changing MCP servers to measure hash synchronization overhead and identify resource bottlenecks
2. **Cross-Domain TDWA Calibration:** Evaluate TDWA performance across three distinct tool domains (finance, coding, healthcare) with domain-specific weight optimization
3. **Real-World Dataset Validation:** Replace synthetic queries with actual user interaction logs from existing MCP server repositories to test agentic retrieval performance on naturally occurring multi-hop queries