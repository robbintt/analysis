---
ver: rpa2
title: Distributional encoding for Gaussian process regression with qualitative inputs
arxiv_id: '2506.04813'
source_url: https://arxiv.org/abs/2506.04813
tags:
- encoding
- kernel
- where
- qualitative
- kernels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of incorporating qualitative
  (categorical) inputs into Gaussian process (GP) regression models, which is essential
  for many engineering applications where observations are expensive. The core contribution
  is a "distributional encoding" (DE) method that represents each category level as
  a probability distribution over the output variable, enabling the use of kernel
  methods on distributions (like Wasserstein or MMD distances) to measure similarity
  between categories.
---

# Distributional encoding for Gaussian process regression with qualitative inputs

## Quick Facts
- arXiv ID: 2506.04813
- Source URL: https://arxiv.org/abs/2506.04813
- Authors: Sébastien Da Veiga
- Reference count: 16
- Primary result: Distributional encoding method represents categorical levels as output distributions, achieving state-of-the-art GP regression performance with lower computational cost than LVGP

## Executive Summary
This paper addresses the challenge of incorporating qualitative (categorical) inputs into Gaussian process (GP) regression models, which is essential for many engineering applications where observations are expensive. The core contribution is a "distributional encoding" (DE) method that represents each category level as a probability distribution over the output variable, enabling the use of kernel methods on distributions (like Wasserstein or MMD distances) to measure similarity between categories. This approach generalizes target encoding by using all output samples per category rather than just summary statistics. The method is validated through extensive numerical experiments on synthetic and real-world datasets, demonstrating state-of-the-art predictive performance compared to existing approaches like LVGP (latent variable GP) and covariance parameterization.

## Method Summary
Distributional encoding represents each categorical level as an empirical probability distribution over observed outputs, rather than collapsing to summary statistics. For each level l, the method constructs an empirical distribution from training outputs belonging to that level, preserving distributional shape information (multimodality, skewness, variance) that summary statistics discard. The similarity between categorical levels is then measured using positive semi-definite kernels on probability distributions, specifically Wasserstein-based exponential kernels for univariate distributions and MMD-based kernels for any dimension. These distributional kernels are combined with standard continuous kernels via tensorized product kernels, allowing seamless integration into standard GP frameworks while preserving computational tractability.

## Key Results
- DE achieves similar accuracy to LVGP but with significantly lower computational cost (Appendix B timing results)
- State-of-the-art predictive performance on synthetic and real-world datasets (beam bending, OTL, Piston, Borehole test cases)
- Effective incorporation of auxiliary data from low-fidelity simulations improves encoding quality without increasing GP training complexity
- Successfully handles problems with multiple categorical inputs and demonstrates robustness across various sample sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Representing categorical levels as empirical output distributions captures richer structure than summary statistics alone.
- Mechanism: For each category level l, construct an empirical distribution over observed outputs rather than collapsing to a single scalar (mean). This preserves distributional shape information (multimodality, skewness, variance) that summary statistics discard. The empirical conditional distribution is computed as a sum of Dirac masses at observed output values for samples belonging to that level.
- Core assumption: Categories with similar functional behavior will exhibit similar output distributions; the distributional structure contains predictive signal beyond first/second moments.
- Evidence anchors:
  - [section 3] Defines distributional encoding as the empirical version of the conditional probability distribution over outputs for each categorical level.
  - [section 2.3, Figure 1-2] Shows LVGP embeddings and mean/std encodings produce similar distance matrices, suggesting summary statistics can work but distributional approaches generalize this.
  - [corpus] Weak direct evidence; neighbor papers focus on categorical kernels more broadly rather than distributional representations specifically.
- Break condition: If quantitative inputs have weak main effects on the output, the conditional distributions may not vary meaningfully across levels (see Appendix A on sensitivity analysis).

### Mechanism 2
- Claim: Positive semi-definite kernels on probability distributions enable seamless integration of distributional encodings into standard GP frameworks.
- Mechanism: Two kernel families are used: (1) Wasserstein-based exponential kernels kW2(P,Q) = exp(-γ W²β(P,Q)) valid for univariate distributions; (2) MMD-based kernels kMMD(P,Q) = exp(-γ MMD²(P,Q)) valid for any dimension. These kernels measure distributional similarity and plug directly into tensorized product kernels alongside standard continuous kernels.
- Core assumption: The chosen distributional kernel is characteristic (distinguishes different distributions) and positive semi-definite; the exponential distance-substitution form preserves valid kernel properties.
- Evidence anchors:
  - [section 3.1] Cites Bachoc et al. (2017) for W2 kernel positive definiteness when β ∈ [0,2], and Song (2008) for MMD kernel positive semi-definiteness in any dimension.
  - [section 3.1, Definition 1-2] Provides formal definitions of Wasserstein and MMD distances with computational considerations.
  - [corpus] Neighbor paper on categorical kernels (arXiv:2510.01840) compares various categorical kernel approaches but doesn't address distributional methods directly.
- Break condition: For multivariate outputs with d > 1, the standard Wasserstein kernel is not valid—must use sliced Wasserstein distance instead (section 3.3.2).

### Mechanism 3
- Claim: Auxiliary data from related sources improves encoding quality without increasing GP training complexity.
- Mechanism: Low-fidelity simulations or related datasets containing the same categorical variables can augment the empirical distributions used for encoding. Two integration modes: (1) concatenate auxiliary samples with training samples for the same encoding; (2) create separate virtual inputs from auxiliary-only encodings. The GP is still trained only on high-fidelity data.
- Core assumption: Auxiliary data shares meaningful structure with the target problem through the categorical variables; the relationship between categories is consistent across fidelity levels.
- Evidence anchors:
  - [section 3.2] Describes two integration strategies and notes the first preserves problem dimensionality while enabling prediction for unseen levels.
  - [section 4.3, Figure 8] Shows RRMSE reduction when concatenating low-fidelity samples with high-fidelity samples for encoding on Borehole test case.
  - [corpus] No direct corpus evidence on auxiliary data integration for categorical GP models.
- Break condition: If auxiliary data comes from a fundamentally different process or has biased category-level relationships, concatenation may introduce noise rather than signal.

## Foundational Learning

- Concept: **Gaussian Process Regression Basics**
  - Why needed here: The entire method builds on GP regression with kernel-based covariance structures; understanding posterior prediction and kernel composition is essential.
  - Quick check question: Can you explain how a GP prior over functions leads to a posterior distribution after observing data, and how the kernel determines smoothness?

- Concept: **Positive Semi-Definite Kernels and Characteristic Kernels**
  - Why needed here: The method relies on constructing valid kernels on probability distributions; understanding why PSD-ness matters for GP validity is critical.
  - Quick check question: Why does a kernel need to be positive semi-definite for use in a GP, and what does "characteristic" mean in the context of distribution kernels?

- Concept: **Optimal Transport / Wasserstein Distance Fundamentals**
  - Why needed here: The Wasserstein kernel is a core option; understanding its geometric interpretation (earth mover's distance) helps diagnose when it captures meaningful distributional similarity.
  - Quick check question: For two empirical distributions on the real line, how does the 1-Wasserstein distance relate to quantile functions?

## Architecture Onboarding

- Component map: Data preprocessing -> Distributional encoding computation -> Kernel selection -> Hyperparameter estimation -> Prediction
- Critical path: Distributional encoding computation → Kernel matrix assembly → Hyperparameter optimization. The encoding step is O(n × L) where L is total levels; kernel computation is O(n²) per distributional kernel evaluation.
- Design tradeoffs:
  - **W2 vs. MMD**: W2 is faster for univariate (uses quantiles) but invalid for multivariate; MMD is universal but requires choosing a base kernel. Paper uses energy distance (no hyperparameters) for MMD.
  - **Encoding enrichment**: Concatenating auxiliary data improves robustness but assumes consistent category-output relationships across sources.
  - **Latent variable vs. distributional**: LVGP learns supervised embeddings jointly with GP; DE uses weakly supervised encodings. LVGP is computationally heavier (see Appendix B timing results) but may capture more complex structure.
- Failure signatures:
  - High prediction error with small main effects (Sobol' indices near 0): quantitative inputs don't drive output variation; distributional encodings won't help. Use sensitivity analysis to detect (Appendix A).
  - Numerical issues with W2 kernel: ensure β ∈ [0, 2] and γ is appropriately scaled.
  - Poor generalization to new levels: DE can predict for unseen levels if auxiliary data exists; otherwise impossible without retraining.
- First 3 experiments:
  1. **Baseline comparison on single-categorical problem**: Implement DE with W2 kernel on the beam bending test case (one categorical input with 6 levels); compare RRMSE against mean encoding and LVGP across 10+ random training splits.
  2. **Kernel ablation**: On the OTL or Piston test case, compare W2 kernel vs. MMD kernel vs. mean/std encoding to identify which distributional kernel performs best for your data regime.
  3. **Auxiliary data integration test**: Create a synthetic multi-fidelity scenario (e.g., Borehole with high/low-fidelity versions); train on n=60 high-fidelity samples with/without n=180 low-fidelity samples for encoding. Measure RRMSE reduction from auxiliary data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical guarantees regarding universality and convergence rates of distributional encoding kernels under empirical approximation?
- Basis in paper: [explicit] Conclusion states: "However, there is still room for improvement in the theoretical understanding of these kernels, including their universality, convergence under empirical approximation, and generalization to sparse or high-dimensional auxiliary data."
- Why unresolved: The paper empirically validates DE but provides no formal analysis of consistency, sample complexity, or approximation bounds.
- What evidence would resolve it: Derivation of convergence rates for MMD and Wasserstein-based kernels as the number of samples per category increases, and proofs of universality conditions.

### Open Question 2
- Question: Can hybrid models combining distributional encodings with latent variable embeddings improve predictive performance or computational efficiency over either approach alone?
- Basis in paper: [explicit] Conclusion proposes: "hybrid models that combine distributional and latent embeddings could offer the best of both worlds: data-driven flexibility and compact representation. As a simple illustration, latent variable optimization may be initialized with distributional encodings after multi-dimensional scaling."
- Why unresolved: The hybrid approach is mentioned but never implemented or tested experimentally.
- What evidence would resolve it: Comparative experiments on benchmark datasets evaluating hybrid models (e.g., LVGP initialized with DE) against pure DE and pure LVGP.

### Open Question 3
- Question: How does distributional encoding perform when training sets are sparse, particularly with few samples per categorical level or when handling strong interactions between quantitative and qualitative inputs?
- Basis in paper: [inferred] Appendix A notes the interaction-handling strategy "requires a large training set and increases the problem dimension. We plan to investigate further its potential on large datasets in future work."
- Why unresolved: The paper does not systematically evaluate DE's sample efficiency or its degradation when categories have very few observations.
- What evidence would resolve it: Systematic ablation studies varying samples-per-level and comparing DE against baselines on datasets with controlled sparsity and strong feature interactions.

## Limitations

- DE's effectiveness degrades significantly when quantitative inputs have weak main effects, as conditional distributions may not vary meaningfully across levels
- Requires sufficient data per category level to build reliable empirical distributions, limiting applicability to highly imbalanced or small-sample regimes
- Theoretical guarantees regarding universality, convergence rates, and consistency under empirical approximation remain uncharacterized

## Confidence

- Mechanism 1 (distributional encoding captures richer structure): **High** - Well-defined mathematically with clear computational procedures and multiple empirical validations
- Mechanism 2 (valid kernel properties for distributional similarity): **High** - Built on established theory (PSD kernels, characteristic kernels) with appropriate citations
- Mechanism 3 (auxiliary data integration benefits): **Medium** - Demonstrated on specific test cases but lacks broader corpus validation and theoretical guarantees
- Computational efficiency claims vs. LVGP: **High** - Supported by explicit timing results in Appendix B across multiple test cases
- Generalization to unseen categorical levels: **Medium** - Validated on specific problems but theoretical conditions for success are not fully characterized

## Next Checks

1. **Sensitivity analysis replication**: Systematically vary the relative importance of quantitative inputs (using Sobol' indices) on the beam bending test case to confirm the theoretical prediction that DE's advantage diminishes with weak main effects.

2. **Imbalanced category test**: Create synthetic datasets with highly imbalanced category distributions (e.g., 80% of samples in one category) to test robustness of distributional encoding when empirical distributions are poorly estimated.

3. **Multi-output validation**: Extend the Borehole multi-fidelity test case to truly multivariate outputs (d > 1) to validate the sliced Wasserstein distance implementation and compare against alternative multi-output GP approaches.