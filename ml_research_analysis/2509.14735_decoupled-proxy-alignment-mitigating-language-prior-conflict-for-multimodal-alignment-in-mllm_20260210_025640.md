---
ver: rpa2
title: 'Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal
  Alignment in MLLM'
arxiv_id: '2509.14735'
source_url: https://arxiv.org/abs/2509.14735
tags:
- language
- training
- alignment
- performance
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies a previously overlooked issue in multimodal
  large language models (MLLMs) called "language prior conflict," where mismatches
  between the inherent language priors of large language models (LLMs) and the language
  priors in training datasets lead to suboptimal vision-language alignment. To address
  this, the authors propose Decoupled Proxy Alignment (DPA), a novel training method
  with two key innovations: (1) using a proxy LLM during pretraining to decouple vision-language
  alignment from language prior interference, and (2) dynamic loss adjustment based
  on visual relevance to strengthen optimization signals for visually relevant tokens.'
---

# Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM

## Quick Facts
- arXiv ID: 2509.14735
- Source URL: https://arxiv.org/abs/2509.14735
- Reference count: 36
- Key result: DPA improves vision-centric benchmark scores by 2.8 points on Llama-3.1-8B-Instruct vs. vanilla method

## Executive Summary
This paper identifies a previously overlooked issue in multimodal large language models (MLLMs) called "language prior conflict," where mismatches between the inherent language priors of large language models (LLMs) and the language priors in training datasets lead to suboptimal vision-language alignment. To address this, the authors propose Decoupled Proxy Alignment (DPA), a novel training method with two key innovations: (1) using a proxy LLM during pretraining to decouple vision-language alignment from language prior interference, and (2) dynamic loss adjustment based on visual relevance to strengthen optimization signals for visually relevant tokens. Extensive experiments demonstrate that DPA significantly mitigates language prior conflict, achieving superior alignment performance across diverse datasets, model families, and scales. For example, DPA improved vision-centric benchmark scores by 2.8 points on Llama-3.1-8B-Instruct compared to the vanilla method. The method also shows exceptional generalization capabilities, making it a robust approach for vision-language alignment.

## Method Summary
The Decoupled Proxy Alignment (DPA) framework operates in three stages: (1) Proxy LLM Pretraining using LoRA on text-only captions to capture dataset-specific language priors, (2) Proxy MLLM Pretraining where a frozen Proxy LLM trains the connector layers with Contrastive Modality Optimization (CMO) loss to focus on vision-language alignment, and (3) MLLM Instruction Tuning where the original LLM replaces the Proxy LLM and fine-tunes with CMO. The CMO loss dynamically weights token losses based on visual relevance, computed as the difference in predicted probabilities between multimodal and text-only inference. This decouples the learning of language priors from visual grounding, allowing the model to focus gradient updates on cross-modal alignment rather than relearning linguistic patterns.

## Key Results
- DPA achieved 2.8-point improvement on vision-centric benchmarks for Llama-3.1-8B-Instruct compared to vanilla training
- The framework demonstrates consistent performance gains across multiple model families (Llama-3.1, Qwen2.5, Gemma-2) and scales (7B-9B parameters)
- DPA successfully resolves the "Dataset Quality Paradox" where advanced LLMs degrade on high-quality data due to prior conflict, particularly evident in Qwen2.5-7B performance on ShareGPT4V-PT

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-training a Proxy LLM on the target dataset's text isolates language distribution adaptation from cross-modal alignment.
- **Mechanism:** By first training a "Proxy" LLM (using LoRA) exclusively on the image-caption text, the model captures the dataset's specific linguistic style and priors. This allows the subsequent vision-language alignment phase (using the frozen Proxy LLM) to focus gradient updates strictly on mapping visual features to tokens, rather than simultaneously fitting the text distribution.
- **Core assumption:** The gradient interference between learning linguistic priors and visual grounding is significant enough that separating them yields better local minima for the connector layers.
- **Evidence anchors:** [abstract] "...use of a proxy LLM during pretraining to decouple the vision-language alignment process from language prior interference..."; [Section 4.1] "Since the Proxy LLM has already captured the language priors of the training data, the model can focus more effectively on vision-language alignment rather than relearning language priors."; [corpus] Related work on "Mitigating Visual Knowledge Forgetting" supports the general efficacy of modality-decoupled gradient strategies to reduce interference.
- **Break condition:** If the dataset text is too small (<10% of full data), the Proxy LLM fails to robustly capture the priors, leading to worse performance than the baseline.

### Mechanism 2
- **Claim:** Contrastive Modality Optimization (CMO) re-weights token loss to prioritize visually grounded vocabulary over language-connective tissue.
- **Mechanism:** For every token, CMO computes a "visual relevance" score by calculating the difference between the model's predicted probability with the image present (r) versus text-only (q). Tokens whose probability changes significantly with visual input are deemed "visually relevant" and assigned higher loss weights. This suppresses overfitting to generic linguistic flourishes (e.g., "captures a lively scene") that do not rely on the image.
- **Core assumption:** The difference in token probability between multimodal and text-only inference correlates directly with the token's visual grounding and importance.
- **Evidence anchors:** [abstract] "...dynamic loss adjustment based on visual relevance to strengthen optimization signals for visually relevant tokens."; [Section 4.2] "Intuitively, tokens with higher visual relevance will exhibit greater differences in predicted probabilities between the scenarios with and without image input."; [corpus] Evidence in corpus is weak/missing for this specific probability-difference mechanism, though "contrastive alignment" is a known strategy.
- **Break condition:** If the clamping hyper-parameters [α, β] are set incorrectly (e.g., α=0), the model may completely ignore syntactic tokens, destabilizing generation.

### Mechanism 3
- **Claim:** DPA resolves the "Dataset Quality Paradox" where advanced LLMs degrade on high-quality data due to prior conflict.
- **Mechanism:** Stronger LLMs (e.g., Qwen2.5) have distinct, hardened priors. When fine-tuned on high-quality datasets with different priors (e.g., ShareGPT4V-PT), they suffer from a conflict that forces them to overfit the text style at the expense of visual features. DPA neutralizes this conflict via the Proxy LLM, allowing the strong model to utilize the high-quality visual data without destabilizing its language capabilities.
- **Core assumption:** The performance drop in high-capability models on specific datasets is primarily driven by prior mismatch rather than data noise or capacity limits.
- **Evidence anchors:** [Section 3.2.1] "For Qwen2.5-7B-Instruct, training on the high-quality dataset led to a performance decline... attributed to a significant conflict between the language priors..."; [Figure 3] Shows loss change (%) for visually vs. linguistically relevant words, indicating non-DPA methods overfit linguistically relevant words while increasing loss on visually relevant ones; [corpus] "Robust Multimodal Large Language Models Against Modality Conflict" confirms modality conflict is a significant source of hallucination and error in MLLMs.
- **Break condition:** This mechanism relies on the existence of a "prior conflict." If the base LLM and dataset priors are already well-aligned (e.g., Vicuna on ShareGPT4V), the relative gain of DPA is reduced compared to the baseline.

## Foundational Learning

- **Concept: Language Priors in LLMs**
  - **Why needed here:** The core problem defined in the paper is the mismatch between the LLM's internal "style/worldview" and the training data's "style." Without understanding that models have distinct fingerprints (priors), the "Dataset Quality Paradox" makes no sense.
  - **Quick check question:** If you train an LLM on formal encyclopedias and then fine-tune it on casual chat logs, does the model struggle more with the logic or the style transition?

- **Concept: Gradient Interference in Multimodal Learning**
  - **Why needed here:** DPA functions by decoupling the optimization of language features from visual features. Understanding that gradients from different objectives (language modeling vs. visual grounding) can compete or interfere is essential to grasp why decoupling works.
  - **Quick check question:** Why might updating a model to describe a "red ball" cause it to temporarily forget the grammatical structure of a sentence?

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** The Proxy LLM training stage relies on LoRA to adapt to the dataset's priors efficiently while preserving the base model's knowledge. The paper explicitly notes that full fine-tuning of the Proxy LLM leads to overfitting/catastrophic forgetting.
  - **Quick check question:** Why is it better to use LoRA (updating small matrices) rather than full fine-tuning when adapting a model to a proxy task that will be discarded later?

## Architecture Onboarding

- **Component map:** CLIP ViT-L (Frozen) -> 2-layer MLP Connector (Trainable) -> LLM Backbone (Trainable/Frozen depending on stage)

- **Critical path:**
  1. Stage 1 (Proxy LLM Pretraining): Train LoRA on text-only captions → Result: Proxy LLM
  2. Stage 2 (Proxy MLLM Pretraining): Freeze Proxy LLM weights. Train Connector (MLP) on Image+Text using CMO loss
  3. Stage 3 (Final Alignment): Discard Proxy LoRA. Revert to Base LLM. Train Connector + LLM (optional or full) on Instruction data using CMO

- **Design tradeoffs:**
  - Training Time: DPA introduces ~33% overhead due to dual forward passes for CMO and the Proxy stage
  - Data Sensitivity: DPA underperforms Vanilla on very small datasets (<10% of full scale) because the Proxy LLM overfits to sparse text statistics
  - Hyper-parameter Sensitivity: The clamping bounds [α, β] for CMO are empirical (default [0.05, 0.5]); incorrect settings can drop performance significantly (Table 12)

- **Failure signatures:**
  - Non-monotonic Loss: If you skip LoRA in Stage 1 (full fine-tuning Proxy), the loss curve will bounce/increase, indicating catastrophic forgetting of the LLM's general capabilities (Appendix B.1)
  - Language Overfitting: If CMO is not applied, the model decreases loss on "linguistically relevant" words (style) while increasing loss on "visually relevant" words (content) (Figure 3)

- **First 3 experiments:**
  1. Reproduce the Paradox: Train Qwen2.5-7B on ShareGPT4V-PT using Vanilla vs. DPA. Verify that Vanilla performance drops compared to training on noisier BLIP-LCS data, and that DPA recovers it
  2. Ablate LoRA Rank: Run Stage 1 with ranks [128, 256, 512] (Table 6). Confirm that rank=256 provides the "sweet spot" for capturing priors without overfitting text descriptions
  3. Token Analysis: Run a word-level loss analysis (Appendix A.2). Confirm that DPA specifically reduces loss on nouns/visual entities while keeping loss stable on function words/style tokens

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the clamping bounds [α, β] in Contrastive Modality Optimization (CMO) be automated to remove the need for empirical selection?
- **Basis in paper:** [explicit] The authors state in the Limitations section that the selection of lower and upper bounds in the CMO process is "currently determined empirically, which could be extended to more adaptive settings in further explorations."
- **Why unresolved:** The current implementation relies on static, manually tuned hyperparameters (0.05 and 0.5) derived from distribution observations, which may not be optimal for all datasets or model architectures.
- **What evidence would resolve it:** A study demonstrating a learnable or dynamic thresholding mechanism that matches or exceeds the performance of static bounds across the diverse datasets tested (e.g., BLIP-LCS, ShareGPT4V-PT).

### Open Question 2
- **Question:** What specific failure modes cause Decoupled Proxy Alignment (DPA) to underperform the Vanilla baseline in low-data regimes (≤10%)?
- **Basis in paper:** [inferred] While Section 6.3 reports that DPA lags behind Vanilla when data is scarce, attributing it to the Proxy LLM's inability to decouple priors, the exact mechanism—whether it is LoRA overfitting or CMO misestimation—remains undiagnosed.
- **Why unresolved:** The paper identifies the symptom (performance drop) but does not provide a solution or a detailed diagnostic of the Proxy LLM's behavior when trained on minimal text samples.
- **What evidence would resolve it:** An ablation study analyzing the language prior alignment of the Proxy LLM and the weighting accuracy of CMO specifically on small data subsets, potentially introducing a regularization term to mitigate the drop.

### Open Question 3
- **Question:** Why does DPA exhibit a performance decline on general benchmarks as LLM scale increases (e.g., 14B to 32B), and is this trade-off inevitable?
- **Basis in paper:** [inferred] Section 6.2 notes that for larger models (32B), performance on vision-centric benchmarks improves, but general benchmark performance declines slightly, hypothesizing that larger LLMs may still overfit linguistic priors that interfere with generation tasks.
- **Why unresolved:** The paper validates the trend but does not confirm if the "language prior conflict" is fully mitigated for the generative capabilities of very large models or if the architecture requires modification for scaling.
- **What evidence would resolve it:** A detailed loss analysis of linguistically relevant tokens for 32B models similar to Figure 1b, coupled with tests on open-ended generation benchmarks to see if the linguistic prior conflict persists despite DPA.

## Limitations

- **Dataset Size Dependency:** DPA shows reduced effectiveness on small datasets (<10% of full pretraining data) due to Proxy LLM overfitting to sparse text statistics
- **Hyperparameter Sensitivity:** CMO loss depends on empirically determined clamping parameters [α, β] that require manual tuning and can significantly affect performance
- **Computational Overhead:** DPA introduces approximately 33% additional training overhead due to dual forward passes and extra Proxy LLM pretraining stage

## Confidence

**High Confidence Claims:**
- DPA effectively mitigates language prior conflict as evidenced by consistent improvements across multiple model families (Llama-3.1, Qwen2.5, Gemma-2) and scales (7B, 8B, 9B parameters)
- The three-stage training procedure is implementable and reproducible, with clear architectural specifications and training protocols
- CMO loss with visual relevance weighting provides measurable benefits compared to uniform token weighting

**Medium Confidence Claims:**
- The performance gains specifically stem from resolving language prior conflict rather than general training improvements
- The optimal LoRA rank of 256 represents a robust sweet spot across datasets, though this requires validation on additional datasets
- The visual relevance weighting mechanism reliably identifies and prioritizes visually grounded tokens

**Low Confidence Claims:**
- The exact mechanism by which probability differences between multimodal and text-only inference correlate with visual grounding importance
- The claim that gradient interference between language and vision objectives is the primary source of alignment failure in MLLMs
- The framework's effectiveness on extremely small datasets (<10% of training data) where the Proxy LLM may fail to capture priors

## Next Checks

1. **Dataset Size Scaling Study:** Systematically evaluate DPA performance across dataset sizes ranging from 1% to 100% of full pretraining data to quantify the precise threshold where Proxy LLM pretraining becomes beneficial versus harmful.

2. **CMO Sensitivity Analysis:** Conduct a comprehensive grid search over CMO parameters [α, β] across [0.01, 0.1] × [0.3, 0.6] on multiple datasets to characterize the stability landscape and identify more robust parameter settings.

3. **Cross-Domain Generalization Test:** Evaluate DPA on specialized vision-language tasks outside the evaluated benchmark suite, such as medical imaging analysis or industrial quality control, to test whether benefits extend to domain-specific applications where visual grounding is critical.