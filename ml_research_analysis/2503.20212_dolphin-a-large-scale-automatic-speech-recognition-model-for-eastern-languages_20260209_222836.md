---
ver: rpa2
title: 'Dolphin: A Large-Scale Automatic Speech Recognition Model for Eastern Languages'
arxiv_id: '2503.20212'
source_url: https://arxiv.org/abs/2503.20212
tags:
- speech
- dolphin
- languages
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dolphin, a large-scale multilingual automatic
  speech recognition (ASR) model extending Whisper architecture to support 40 Eastern
  languages and 22 Chinese dialects. Dolphin integrates proprietary and open-source
  datasets, using E-Branchformer encoders and a joint CTC-Attention decoder.
---

# Dolphin: A Large-Scale Automatic Speech Recognition Model for Eastern Languages

## Quick Facts
- arXiv ID: 2503.20212
- Source URL: https://arxiv.org/abs/2503.20212
- Reference count: 0
- Primary result: Dolphin base model (372M parameters) achieves 24.0% WER, outperforming Whisper large-v3 (32.6% WER) despite being much smaller

## Executive Summary
Dolphin is a large-scale multilingual automatic speech recognition model extending Whisper architecture to support 40 Eastern languages and 22 Chinese dialects. Trained on over 200,000 hours of audio, Dolphin integrates proprietary and open-source datasets using E-Branchformer encoders and a joint CTC-Attention decoder. It introduces a two-level language token system (language + region) to handle dialects and accents, achieving state-of-the-art performance across model sizes while supporting real-time low-latency scenarios.

## Method Summary
Dolphin extends the Whisper architecture with E-Branchformer encoders and joint CTC-Attention decoding. The model uses 80-channel log Mel filterbank features, 4× subsampling, and a 40K BPE vocabulary with hierarchical language-region tokens. Training involves concatenating short audio clips into 25-30 second segments with duration-based bucketing to reduce deletion errors. The model is trained for 4 epochs on 16× H100 GPUs using AdamW optimizer with exponential decay learning rate scheduling, achieving 31.8%, 24.0%, 22.0%, and 20.6% WER on multilingual test sets for base, small, medium, and large models respectively.

## Key Results
- Dolphin base (372M) achieves 24.0% WER, outperforming Whisper large-v3 (32.6% WER) on multilingual test sets
- Dolphin small (168M) achieves 31.8% WER, surpassing Whisper small (36.6% WER)
- Dolphin medium (795M) achieves 22.0% WER, outperforming Whisper medium (26.4% WER)
- Dolphin large (1679M) achieves 20.6% WER, surpassing Whisper large-v3 (22.3% WER)

## Why This Works (Mechanism)

### Mechanism 1: Joint CTC-Attention Decoding
The joint CTC-Attention decoder improves training efficiency and robustness by combining monotonic CTC alignment with flexible attention mechanisms. The CTC loss weight of 0.3 acts as a regularizer while the attention decoder handles sequence modeling, reducing alignment errors during training and enabling faster convergence.

### Mechanism 2: Hierarchical Language-Region Token System
The two-level language token system (language + region) improves dialect and accent discrimination by separating language identity from regional variation. This allows the model to learn shared cross-lingual representations within language families while preserving dialect-specific phonetic patterns through explicit accent recognition.

### Mechanism 3: Duration-Balanced Audio Concatenation
Concatenating short audio clips into 25-30 second segments reduces deletion errors and improves overall WER by teaching the model to handle natural speech continuity. Duration bucketing (6 buckets) ensures balanced training across clip lengths, addressing the bias toward over-segmentation in short audio.

## Foundational Learning

- Concept: E-Branchformer Encoder Architecture
  - Why needed here: Dolphin replaces Whisper's Transformer encoder with E-Branchformer, using parallel branches for local and global dependencies
  - Quick check question: How do E-Branchformer's parallel branches differ from standard Transformer self-attention in capturing local acoustic features vs. long-range linguistic context?

- Concept: Byte-Pair Encoding (BPE) for Multilingual ASR
  - Why needed here: Dolphin uses a 40K BPE vocabulary across 40+ languages with diverse scripts
  - Quick check question: How would a 40K BPE vocabulary allocate tokens differently between logographic Chinese vs. phonemic scripts like Thai or Korean?

- Concept: SpecAugment for Robust ASR
  - Why needed here: Dolphin applies SpecAugment with global normalization for noise robustness
  - Quick check question: For tonal languages like Thai or Vietnamese, which SpecAugment parameters should be adjusted to preserve pitch information critical for meaning?

## Architecture Onboarding

- Component map: Input audio -> 80-channel log Mel fbank -> 4× subsampling -> E-Branchformer encoder -> Joint CTC-Attention decoder -> BPE detokenization -> Text output
- Critical path: 1) Audio preprocessing → feature extraction → 4× subsampling 2) E-Branchformer encoding (parallel local/global branches merge) 3) CTC + Attention decoding with hierarchical language/region conditioning 4) BPE detokenization → text output
- Design tradeoffs: CTC weight 0.3 balances alignment speed vs. attention flexibility; 40K BPE vocabulary reduces sequence length but increases OOV risk; 30-second padding improves long-form handling but wastes compute
- Failure signatures: High deletion errors on short audio (<5s) indicate insufficient duration bucketing; poor dialect discrimination suggests region token coverage issues; OOM during data loading requires data sharding strategy
- First 3 experiments: 1) Ablate CTC weight (0.0, 0.3, 0.5) on held-out dialect test set 2) Evaluate region token granularity by merging similar dialects 3) Test duration bucketing impact on short-utterance benchmark (<5s clips)

## Open Questions the Paper Calls Out

- Do larger Dolphin models continue to follow observed scaling laws to achieve state-of-the-art results across a broader range of languages?
- What specific training strategies or data curation methods are required to optimize Dolphin for underrepresented and low-resource Eastern languages?
- Can Dolphin be optimized for real-time, low-latency inference through compression techniques without significant degradation in accuracy?

## Limitations

- Evaluation relies heavily on authors' test set construction, which is not fully disclosed in terms of language coverage balance or dialect representation
- The 212,137-hour training corpus includes 137,712 hours of proprietary Dataocean AI data (65%), making independent validation difficult
- Region token granularity may be insufficient for micro-dialects (village-level variations) that exist within the claimed 22 Chinese dialects

## Confidence

- Dolphin outperforms Whisper on WER across all model sizes: High confidence
- Joint CTC-Attention architecture improves training efficiency and robustness: Medium confidence
- Two-level language-region token system improves dialect discrimination: Medium confidence
- Duration-bucketed concatenation reduces deletion errors: Medium confidence

## Next Checks

1. Ablate CTC weight contribution: Train Dolphin base with CTC weight 0.0, 0.3, and 0.5 to measure WER delta on held-out dialect test set
2. Validate region token granularity: Merge similar dialects and measure WER delta on affected dialects to quantify granularity effectiveness
3. Test duration bucketing on short utterances: Evaluate on isolated short utterances (<5s clips from CommonVoice) to confirm strategy does not degrade short-form performance