---
ver: rpa2
title: Evaluation of Missing Data Imputation for Time Series Without Ground Truth
arxiv_id: '2503.05775'
source_url: https://arxiv.org/abs/2503.05775
tags:
- data
- metrics
- ground
- truth
- imputation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of validating missing data imputation
  in time series without ground truth, a common issue in real-world applications like
  5G network management. Traditional validation relies on ground truth data, which
  is unavailable in practice.
---

# Evaluation of Missing Data Imputation for Time Series Without Ground Truth

## Quick Facts
- arXiv ID: 2503.05775
- Source URL: https://arxiv.org/abs/2503.05775
- Reference count: 18
- This paper proposes using Wasserstein Distance and Jensen-Shannon Divergence to validate missing data imputation in time series without ground truth, showing these metrics align well with traditional error measures in experiments.

## Executive Summary
This paper addresses the critical challenge of validating missing data imputation in time series when ground truth is unavailable, a common scenario in real-world applications like 5G network management. Traditional validation methods rely on comparing imputed values to actual observed data, which is impossible when dealing with truly missing data. The authors propose an innovative approach using distribution-based metrics - Wasserstein Distance (WD) and Jensen-Shannon Divergence (JSD) - to assess imputation quality by comparing the statistical distributions of imputed data with pre-gap observations. These metrics evaluate internal consistency and alignment of the imputed data with the observed patterns, providing a robust alternative to traditional error-based validation methods.

The proposed solution leverages the principle that high-quality imputation should produce data whose distribution matches the observed patterns before the gap. By comparing the statistical characteristics of imputed values with the distribution of data preceding the missing segment, these metrics can effectively assess imputation quality without requiring ground truth. Experiments with Telraam and Madrid traffic datasets demonstrate that WD and JSD correlate strongly with traditional metrics like RMSE and MAE across various imputation methods including LSTM, XGBoost, ARIMA, and interpolation techniques, validating their effectiveness as reliable tools for real-world applications where ground truth is unavailable.

## Method Summary
The authors propose a validation framework for missing data imputation that uses distribution-based metrics when ground truth is unavailable. The core approach employs Wasserstein Distance (WD) and Jensen-Shannon Divergence (JSD) to compare the statistical distributions of imputed data with pre-gap observations. The method assumes that high-quality imputation should produce data whose distribution aligns with the observed patterns before the gap. The validation process involves computing these metrics on the imputed segments and comparing them against ground truth metrics in controlled experiments where gaps are artificially created. The approach is tested across multiple imputation methods (LSTM, XGBoost, ARIMA, interpolation) and datasets (Telraam and Madrid traffic data) to demonstrate its effectiveness in capturing imputation performance.

## Key Results
- WD and JSD metrics effectively capture imputation performance across multiple methods including LSTM, XGBoost, ARIMA, and interpolation
- The proposed metrics show strong correlation with traditional ground truth metrics (RMSE and MAE) in controlled experiments
- Experiments validate the metrics as reliable tools for assessing imputation quality in the absence of ground truth data

## Why This Works (Mechanism)
The approach works by leveraging the fundamental principle that imputation quality can be assessed through distribution alignment rather than direct error comparison. When ground truth is unavailable, the key insight is that high-quality imputation should preserve the statistical characteristics of the observed data. Wasserstein Distance measures the "work" required to transform one distribution into another, effectively capturing how well the imputed data matches the pre-gap distribution. Jensen-Shannon Divergence measures the similarity between probability distributions, providing a symmetric and bounded metric for distribution comparison. Together, these metrics evaluate whether the imputed values maintain the same statistical properties as the observed data, which is a strong indicator of imputation quality even without ground truth comparison.

## Foundational Learning
- **Wasserstein Distance**: A metric measuring the cost of transforming one probability distribution into another; needed to quantify distribution alignment between imputed and pre-gap data; quick check: compare WD values for different imputation methods on the same gap
- **Jensen-Shannon Divergence**: A symmetric measure of similarity between probability distributions; needed for bounded, interpretable distribution comparison; quick check: verify JSD values fall between 0 (identical) and 1 (completely different)
- **Distribution-based validation**: Approach using statistical properties rather than point-wise errors; needed when ground truth is unavailable; quick check: ensure pre-gap and post-gap distributions are comparable
- **Imputation gap analysis**: Study of missing data segments and their characteristics; needed to understand validation requirements; quick check: analyze gap size and position effects on metric performance
- **Univariate time series analysis**: Focus on single-variable temporal data; needed for the specific application domain; quick check: verify time series stationarity assumptions
- **Cross-method validation**: Comparing multiple imputation approaches using consistent metrics; needed to establish metric reliability; quick check: ensure metrics rank methods consistently across datasets

## Architecture Onboarding

**Component Map**: Data preprocessing -> Gap creation -> Imputation methods -> Distribution comparison -> Metric computation -> Validation

**Critical Path**: The validation pipeline follows a sequence where raw time series data is first preprocessed, artificial gaps are created, multiple imputation methods are applied to fill these gaps, and then WD and JSD metrics are computed by comparing the distributions of imputed data with pre-gap observations. The critical path ends with metric computation and comparison against ground truth metrics in controlled experiments.

**Design Tradeoffs**: The approach trades direct error measurement accuracy for applicability in real-world scenarios where ground truth is unavailable. Distribution-based metrics provide a more holistic assessment but may miss localized errors that point-wise metrics would catch. The choice of comparing with pre-gap data assumes temporal stationarity, which may not hold in all scenarios.

**Failure Signatures**: Poor performance indicators include: WD values that don't correlate with traditional error metrics, JSD values that remain high across different imputation methods, or metrics that fail to distinguish between high and low quality imputations. Additionally, if the pre-gap distribution differs significantly from the post-gap distribution, the metrics may provide misleading validation results.

**First Experiments**:
1. Compare WD and JSD values across different imputation methods on the same dataset and gap configuration
2. Analyze correlation between proposed metrics and traditional metrics (RMSE, MAE) across multiple experiments
3. Test metric sensitivity to different gap sizes and positions within the time series

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation relies on artificially created gaps in datasets where ground truth is actually available, which may not fully capture real-world missing data patterns
- The study focuses on univariate time series and specific imputation methods, limiting generalizability to multivariate scenarios
- Limited qualitative analysis of how metrics behave under different missing data mechanisms or distribution shifts

## Confidence
- **High**: Empirical demonstration that WD and JSD correlate well with traditional metrics in controlled experiments
- **Medium**: Practical applicability to real-world scenarios without ground truth
- **Low**: Generalizability to multivariate time series and other domains

## Next Checks
1. Evaluate the metrics on real-world datasets with known but withheld ground truth to validate their effectiveness in authentic missing data scenarios
2. Test the proposed metrics across different missingness mechanisms (MCAR, MAR, MNAR) to assess robustness
3. Extend the evaluation to multivariate time series datasets and compare performance with univariate cases to determine if distribution-based metrics scale effectively