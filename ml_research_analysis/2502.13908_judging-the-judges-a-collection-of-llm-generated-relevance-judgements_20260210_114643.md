---
ver: rpa2
title: 'Judging the Judges: A Collection of LLM-Generated Relevance Judgements'
arxiv_id: '2502.13908'
source_url: https://arxiv.org/abs/2502.13908
tags:
- relevance
- judgments
- evaluation
- llms
- judgment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The LLMJudge resource provides a benchmark of 42 LLM-generated
  relevance judgment sets for the TREC 2023 Deep Learning track, produced by 8 international
  teams using varied prompting techniques and models. The collection enables systematic
  study of LLM assessors' effectiveness, biases, and evaluation consistency.
---

# Judging the Judges: A Collection of LLM-Generated Relevance Judgements

## Quick Facts
- arXiv ID: 2502.13908
- Source URL: https://arxiv.org/abs/2502.13908
- Authors: Hossein A. Rahmani; Clemencia Siro; Mohammad Aliannejadi; Nick Craswell; Charles L. A. Clarke; Guglielmo Faggioli; Bhaskar Mitra; Paul Thomas; Emine Yilmaz
- Reference count: 40
- Primary result: LLMJudge resource provides 42 LLM-generated relevance judgment sets for TREC 2023 Deep Learning track, enabling systematic study of LLM assessors' effectiveness, biases, and evaluation consistency.

## Executive Summary
This paper presents LLMJudge, a benchmark of 42 LLM-generated relevance judgment sets for the TREC 2023 Deep Learning track. Eight international teams contributed submissions using varied prompting techniques and models including fine-tuning, multi-prompt ensembles, and criteria decomposition. The collection enables systematic study of LLM assessors' effectiveness, biases, and evaluation consistency. Experimental results show that while most submissions maintain strong ranking correlation (Kendall's τ > 0.9), their absolute scoring consistency varies significantly (Cohen's κ ranging from 0.06 to 0.29).

## Method Summary
The LLMJudge benchmark consists of LLM-generated relevance judgments for the TREC 2023 Deep Learning track test set, where 43 queries were judged by 8 international teams using 8 different LLMs and various prompting strategies. Submissions included fine-tuning approaches, multi-prompt ensembles, criteria decomposition, and semantic-label methods. The resource provides both the raw judgments and evaluation scripts for comparing against human qrels using agreement metrics (Cohen's κ, Krippendorff's α) and ranking metrics (Kendall's τ, Spearman's ρ).

## Key Results
- Most LLM judges achieved strong ranking correlation with human judgments (Kendall's τ > 0.9)
- Fine-tuning approaches achieved the highest agreement with human judgments (Krippendorff's α up to 0.4958)
- Absolute scoring consistency varied significantly (Cohen's κ ranging from 0.06 to 0.29)
- Semantic-label methods showed more stable performance compared to numeric-label approaches
- TREMA-4prompts achieved τ = 0.9483 and ρ = 0.9919 despite moderate κ = 0.1829

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning LLMs on historical relevance judgments improves agreement with human assessors more reliably than prompting strategies alone.
- Mechanism: Fine-tuning on TREC DL qrels (2019–2022) or the LLMJudge dev set adapts model weights to the specific 4-point relevance schema, reducing distributional drift between model outputs and human labeling patterns.
- Core assumption: Past human relevance judgments are representative of the target evaluation task and do not embed systematic biases the LLM will amplify.
- Evidence anchors: [abstract] "Fine-tuning approaches achieved the highest agreement with human judgments"; [Section 5.2] "h2oloo-fewself achieved the highest Krippendorff's α (0.4958, the best among all methods)"

### Mechanism 2
- Claim: Criteria decomposition—spliting relevance into explicit sub-dimensions before aggregation—yields more stable ranking correlation even when absolute label agreement is moderate.
- Mechanism: By requiring the LLM to separately assess exactness, coverage, topicality, and contextual fit, the method reduces prompt ambiguity and forces explicit reasoning, which stabilizes system ordering (Kendall's τ) even if final label calibration varies.
- Core assumption: The decomposed criteria are sufficiently independent and collectively exhaustive for the relevance construct; the aggregation function preserves discriminative signal.
- Evidence anchors: [Section 4] "TREMA-4prompts decomposes relevance into four criteria: exactness, coverage, topicality, contextual fit"; [Table 3] TREMA-4prompts achieves τ = 0.9483, ρ = 0.9919

### Mechanism 3
- Claim: Multi-prompt or multi-model ensembles improve robustness by averaging out idiosyncratic biases of individual prompts or models.
- Mechanism: Each prompt or model encodes different inductive biases; averaging their outputs cancels uncorrelated errors, leading to more consistent label distributions and higher κ.
- Core assumption: Errors across prompts/models are not perfectly correlated; the ensemble aggregation is correctly specified.
- Evidence anchors: [Section 4] "Olz-multiprompt directly aggregated relevance judgments by averaging across nine prompts"; [Table 3] Olz-multiprompt achieves κ = 0.2445, α = 0.4551

## Foundational Learning

**Concept: Relevance judgment scales and their operationalization**
- Why needed here: The task uses a 4-point ordinal scale (0=Irrelevant, 1=Related, 2=Highly relevant, 3=Perfectly relevant); misunderstanding the granularity leads to inappropriate metric choices.
- Quick check question: Can you explain why Cohen's κ for the 4-point scale is lower than for binarized groupings (e.g., 0|123)?

**Concept: Agreement vs. ranking metrics**
- Why needed here: High Kendall's τ (>0.9) indicates systems are ranked similarly to human judgments, but low κ (<0.3) means absolute labels disagree; conflating these leads to overconfident deployment.
- Quick check question: If an LLM judge has τ=0.95 but κ=0.15, would you trust it for (a) system comparison, (b) absolute thresholding?

**Concept: Label calibration and distributional shift**
- Why needed here: LLMs systematically over- or under-assign certain labels, which biases downstream metrics like NDCG; calibration is essential for absolute scoring use cases.
- Quick check question: How would you detect if an LLM judge is systematically over-predicting "highly relevant" labels compared to human annotators?

## Architecture Onboarding

**Component map:** Input (query, passage) pairs → LLM Judge → raw relevance output → Post-processor → Evaluator (compare to human qrels)

**Critical path:**
1. Choose model (open-source Llama-3-8B/70B vs. proprietary GPT-4o)
2. Design prompt or fine-tuning data (zero-shot, CoT, criteria decomposition, few-shot)
3. Decide output format (numeric 0–3 vs. semantic labels)
4. Optionally ensemble (multi-prompt average, classifier over features)
5. Evaluate against human qrels on both agreement (κ, α) and ranking (τ, ρ)

**Design tradeoffs:**
- Fine-tuning vs. prompting: Fine-tuning improves agreement but requires labeled data and compute; prompting is cheaper but less consistent.
- Open-source vs. proprietary: Open-source (Llama) offers reproducibility and control; proprietary (GPT-4o) shows higher peak performance in some submissions but with variability.
- Single-prompt vs. ensemble: Single-prompt is simpler; ensemble improves robustness at cost of inference complexity.

**Failure signatures:**
- High τ (>0.9) with low κ (<0.2): LLM ranks systems correctly but assigns incorrectly calibrated absolute labels.
- κ near 0 for 012|3 grouping: LLM cannot reliably identify "perfectly relevant" documents.
- Large variance in label distribution vs. human (Table 5): Systematic over/under-prediction of specific labels.

**First 3 experiments:**
1. Replicate llmjudge-simple baseline (zero-shot numeric) on dev set to establish κ, τ baselines.
2. Ablate semantic vs. numeric label prompts (e.g., willia-umbrela1 vs. umbrela2) to measure impact on κ.
3. Implement a 2-prompt ensemble (one graded, one binary) and compare κ/τ to single-prompt baselines.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How can LLMs be effectively utilized to generate relevance judgments specifically within a nugget-based evaluation scenario?
- Basis in paper: [explicit] The authors explicitly state in the conclusion: "In future work, we plan to investigate how LLM can be used to generate relevance judgment in a nugget-based evaluation scenario."
- Why unresolved: The current benchmark focuses on passage-level graded relevance, whereas nugget-based evaluation requires identifying and extracting specific atomic information units.
- What evidence would resolve it: A methodology where LLMs extract nuggets from passages and a comparison of the resulting evaluation metrics against human-derived nugget assessments.

**Open Question 2**
- Question: Can the analysis of LLM-based relevance judgments be reliably extended to fully automatic collections comprising synthetic queries and documents?
- Basis in paper: [explicit] The conclusion lists a future objective to "extend the analysis to fully automatic collections, that include automatically generated queries and documents."
- Why unresolved: While synthetic test collections exist, it is unclear if the consistency observed in the TREC Deep Learning track holds when the input data itself is fully LLM-generated.
- What evidence would resolve it: Experiments demonstrating that system rankings derived from fully synthetic collections (queries, docs, and judgments) correlate highly with rankings from human-curated test collections.

**Open Question 3**
- Question: To what extent does evaluation "circularity" bias results when the same LLM is used for both document ranking and relevance assessment?
- Basis in paper: [explicit] Section 2.1 identifies "circularity" as a major challenge, noting the risk that "the same LLM might be used to generate relevance judgments and as a document ranker."
- Why unresolved: The authors note this induces strong bias, but the magnitude of this effect and whether it renders automated evaluation invalid for self-assessment is not quantified in the current results.
- What evidence would resolve it: A study measuring the discrepancy in system effectiveness scores when an LLM ranks a collection versus when it judges that same collection, compared to a human ground truth.

**Open Question 4**
- Question: How can LLM-based judges be robustly defended against adversarial manipulations, such as the injection of relevance-indicating keywords?
- Basis in paper: [explicit] Section 2.1 highlights "Vulnerability to Attacks," referencing prior work showing that introducing keywords like "relevant" can trick LLMs into assigning high relevance scores.
- Why unresolved: The paper releases the resource to foster analysis of systematic errors, but does not propose or test mitigation strategies for these specific adversarial inputs.
- What evidence would resolve it: Testing the performance of the submitted LLMJudge runs against an adversarial dataset where documents are modified with trigger words to observe if scores inflate erroneously.

## Limitations

- Limited generalizability beyond the TREC Deep Learning domain; all submissions evaluated on the same 2023 test set with similar document distributions.
- Potential overfitting to the specific 4-point relevance schema; performance on binary or 5-point scales untested.
- Unknown robustness to adversarial or out-of-distribution queries; no systematic stress testing reported.

## Confidence

- High confidence: Fine-tuning consistently outperforms prompting for absolute label agreement; multi-prompt ensembles improve robustness.
- Medium confidence: Criteria decomposition stabilizes rankings; semantic-label outputs reduce calibration issues.
- Low confidence: Proprietary model (GPT-4o) performance variability across submissions; no systematic ablation of prompt engineering techniques.

## Next Checks

1. Test LLMJudge submissions on out-of-domain collections (e.g., academic search, biomedical) to assess domain transfer.
2. Conduct adversarial query evaluation to identify systematic failure modes in LLM judges.
3. Perform controlled experiments varying the relevance scale (binary, 5-point) to quantify schema sensitivity.