---
ver: rpa2
title: 'Sound and Music Biases in Deep Music Transcription Models: A Systematic Analysis'
arxiv_id: '2512.14602'
source_url: https://arxiv.org/abs/2512.14602
tags:
- performance
- note
- music
- genre
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper systematically analyzes sound and music biases in deep\
  \ music transcription models by introducing the MDS corpus, which isolates shifts\
  \ in recording conditions (sound) and musical content (genre, dynamics, polyphony).\
  \ Using three subsets\u2014Genre, Random, and MAEtest\u2014the authors evaluate\
  \ five state-of-the-art AMT systems with both traditional IR metrics and musically\
  \ informed ones."
---

# Sound and Music Biases in Deep Music Transcription Models: A Systematic Analysis

## Quick Facts
- arXiv ID: 2512.14602
- Source URL: https://arxiv.org/abs/2512.14602
- Reference count: 40
- Primary result: Deep AMT models show significant performance drops under sound and genre distribution shifts, with velocity estimation being particularly vulnerable.

## Executive Summary
This paper presents a systematic analysis of biases in deep music transcription models by introducing the MDS corpus, which isolates shifts in recording conditions (sound) and musical content (genre, dynamics, polyphony). The authors evaluate five state-of-the-art AMT systems across three distinct test sets and find substantial performance degradation due to distribution shifts, with sound shifts causing up to 20 percentage points drop in note-level F1-score and genre shifts causing up to 14 points drop. The study demonstrates that while onset detection remains relatively robust, velocity estimation is highly sensitive to musical distribution shifts, highlighting fundamental limitations in current AMT approaches.

## Method Summary
The authors introduce the MDS corpus with three subsets: Genre (100 pieces across 10 genres), Random (72 synthetic sequences varying polyphony and dynamics), and MAEtest (20 MAESTRO pieces). They evaluate five pre-trained AMT systems (OaF, Kong, T5, Toyama, Edwards) trained on MAESTRO using both traditional IR metrics (Frame F1, Note F1 variants) and musically informed metrics (Expressive Timing, Articulation, Harmony, Dynamics). No model training was performed; instead, the study focuses on analyzing model behavior under out-of-distribution conditions through systematic evaluation.

## Key Results
- Sound distribution shift causes up to 20 percentage point drop in note-level F1-score
- Genre distribution shift causes up to 14 percentage point drop in note-level F1-score
- Velocity estimation shows highest sensitivity to musical distribution shifts, with drops up to 50 percentage points

## Why This Works (Mechanism)
The study works by systematically isolating different types of distribution shifts in music transcription. By creating controlled test sets that vary either recording conditions or musical content independently, the authors can quantify how each type of shift affects model performance. The combination of traditional IR metrics and musically informed metrics provides both quantitative performance measures and qualitative insights into specific musical aspects that models struggle with.

## Foundational Learning
- **Automatic Music Transcription (AMT)**: The task of converting audio recordings into symbolic musical notation
  - Why needed: Provides the fundamental problem context for the study
  - Quick check: Understanding that AMT involves detecting note onsets, offsets, and velocities from audio

- **Distribution Shift**: When training and test data come from different probability distributions
  - Why needed: Central concept explaining why models perform poorly on out-of-distribution data
  - Quick check: Recognizing that models trained on classical piano may not generalize to other genres or recording conditions

- **Disentanglement**: Separating different factors of variation in data
  - Why needed: The Random subset is designed to disentangle effects of polyphony and dynamics
  - Quick check: Understanding that synthetic sequences can isolate specific musical properties

- **Musically Informed Metrics**: Evaluation metrics that capture musical structure beyond note accuracy
  - Why needed: Traditional metrics may miss important musical aspects like timing and harmony
  - Quick check: Recognizing metrics like IOI (Inter-Onset Interval) and Cloud Diameter as musically meaningful

## Architecture Onboarding

Component Map: Audio -> AMT System -> Transcription Output -> Evaluation Metrics

Critical Path: Raw audio recordings → Model inference → Note detection → Metric computation → Performance analysis

Design Tradeoffs: The study prioritizes systematic analysis over model development, using pre-trained systems to focus on evaluation under distribution shifts rather than optimizing individual models.

Failure Signatures: 
- T5 produces anomalously long "stuck" notes (missed offsets)
- Velocity predictions collapse to Gaussian training distribution
- Models show corpus bias on synthetic sequences

First Experiments:
1. Validate setup by comparing Note F1 (On & Off & Vel) on MAEtest vs. reported MAESTRO baseline (~82-84%)
2. Run inference on all MDS audio files using OaF system
3. Compute all metrics using provided evaluation scripts

## Open Questions the Paper Calls Out

Open Question 1: Can hybrid modeling approaches like Differentiable Dictionary Search (DDS) be scaled to provide competitive note-level accuracy while maintaining their robustness to extreme musical distribution shifts?
- Basis: Section 5 evaluates DDS, finding it robust to corpus bias on the Random set but not practically competitive due to scalability issues
- Why unresolved: Current implementation is limited by computational costs preventing matching state-of-the-art accuracy
- What would resolve it: A DDS-based system matching Toyama/Kong F1-scores on MAESTRO while showing smaller performance degradation on Random

Open Question 2: How can the "brittleness" of velocity estimation under musical distribution shifts be mitigated so that it matches the relative robustness of onset detection?
- Basis: Conclusion states velocity detection is most brittle under musical distribution shift
- Why unresolved: Models map input features to expected training velocity distributions rather than inferring loudness independently
- What would resolve it: An AMT model maintaining high correlation between ground truth and predicted velocities on Random set's "narrow" dynamics subset

Open Question 3: How can evaluation benchmarks standardize the capture of sustain pedal ground truth given the lack of a universal calibration standard across different instruments?
- Basis: Section 6 identifies sustain pedal calibration as an inherent limitation
- Why unresolved: No universal ground truth for how pedal pressure translates to mechanical action
- What would resolve it: A standardized calibration protocol or sensor-agnostic representation for pedal data in future dataset releases

## Limitations
- Exclusive focus on piano datasets limits generalizability to other instrument families
- Synthetic Random sequences may not fully represent real musical performance complexity
- Ground truth derived from Disklavier recordings may not capture all nuances of human performance

## Confidence

High Confidence:
- Sound distribution shift effects (20pp F1 drop) and genre shift effects (14pp F1 drop) are clearly measurable and reproducible

Medium Confidence:
- Musically informed metrics reveal harmonic and dynamic structure issues, though some behaviors may be corpus-specific

Low Confidence:
- Generalization of corpus bias findings to other AMT tasks and non-piano domains requires additional validation

## Next Checks
1. Evaluate the same AMT systems on a non-piano dataset (e.g., MusicNet, URMP) to test robustness across instrument types
2. Systematically vary parameters of musically informed metrics on MDS data to determine their stability and discriminative power
3. Compare Random sequence transcriptions against real multi-instrument recordings with annotated ground truth to assess generalizability to complex musical contexts