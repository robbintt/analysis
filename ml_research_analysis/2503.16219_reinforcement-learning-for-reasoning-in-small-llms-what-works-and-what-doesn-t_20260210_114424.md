---
ver: rpa2
title: 'Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn''t'
arxiv_id: '2503.16219'
source_url: https://arxiv.org/abs/2503.16219
tags:
- reasoning
- llms
- small
- training
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether reinforcement learning (RL) can
  improve reasoning in small language models under strict resource constraints. The
  authors fine-tune a 1.5-billion-parameter model, DeepSeek-R1-Distill-Qwen-1.5B,
  using a compact mathematical reasoning dataset and the GRPO algorithm on 4 NVIDIA
  A40 GPUs for 24 hours.
---

# Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn't

## Quick Facts
- arXiv ID: 2503.16219
- Source URL: https://arxiv.org/abs/2503.16219
- Authors: Quy-Anh Dang; Chris Ngo
- Reference count: 18
- One-line primary result: Small 1.5B model achieves rapid reasoning gains via RL with minimal data/cost but faces late-stage stability challenges.

## Executive Summary
This paper demonstrates that reinforcement learning can significantly improve mathematical reasoning in small language models under strict resource constraints. The authors fine-tune a 1.5-billion-parameter model using the GRPO algorithm on a compact dataset, achieving substantial accuracy gains on AMC23 and AIME24 benchmarks at approximately $42 cost versus thousands for baselines. However, the study reveals critical challenges including optimization instability after 150-200 training steps, language drift in multilingual outputs, and sensitivity to length constraints and reward design.

## Method Summary
The authors adapt the Group Relative Policy Optimization (GRPO) algorithm to train DeepSeek-R1-Distill-Qwen-1.5B on a curated mathematical reasoning dataset. GRPO eliminates the need for a separate critic model by estimating baselines from group scores, reducing computational overhead. The training uses rule-based rewards for format and accuracy, optionally augmented with cosine-scaled length penalties, across 4 NVIDIA A40 GPUs for 24 hours. The approach focuses on mathematical reasoning within strict token limits (3584-4096 tokens) and evaluates performance on AMC23, AIME24, and MATH-500 benchmarks.

## Key Results
- AMC23 accuracy improved from 63% to 80% within 50-100 training steps
- AIME24 accuracy reached 46.7%, surpassing o1-preview baseline
- Total training cost approximately $42 versus thousands for traditional RL approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Group Relative Policy Optimization (GRPO) enables computationally efficient RL by eliminating the need for a separate critic model.
- Mechanism: For each question, GRPO samples multiple outputs and computes advantages using group statistics (mean and standard deviation of rewards) rather than a learned value function, then applies clipped policy optimization with a KL penalty term.
- Core assumption: Group-based baseline estimation provides sufficient variance reduction for stable gradient updates in small LLMs.
- Evidence anchors:
  - [abstract] "Adapting the Group Relative Policy Optimization (GRPO) algorithm and curating a compact, high-quality mathematical reasoning dataset"
  - [section] Section 2: "GRPO eliminates the need for a separate critic model—typically as large as the policy model—by estimating baselines from group scores, thereby reducing computational overhead"
  - [corpus] Weak corpus evidence—no neighboring papers directly analyze GRPO's mechanism in small models.
- Break condition: KL divergence instability after 150-200 global steps (Figure 4), causing reward collapse and language drift.

### Mechanism 2
- Claim: Mixing easy and hard problems in the training distribution produces more stable early-stage learning and shorter completion lengths.
- Mechanism: Easier problems encourage the model to develop concise reasoning patterns, while harder problems maintain exposure to complex solution structures, creating a implicit curriculum effect.
- Core assumption: Difficulty-balanced datasets reduce the variance in required reasoning chain lengths, preventing premature truncation.
- Evidence anchors:
  - [section] Experiment 2: "incorporating a mix of easy and hard problems under reduced length constraints enhances early performance and stabilizes reasoning behavior"
  - [section] Figure 3 and Figure 4 showing completion length stabilization in Experiment 2 versus Experiment 1
  - [corpus] Neighbor paper "Infi-MMR" supports curriculum-based RL phasing for multimodal small models, suggesting transferability.
- Break condition: Late-stage instability (after 150-200 steps) from length constraints and multilingual output drift.

### Mechanism 3
- Claim: Cosine-scaled rewards tied to response length control verbosity without requiring explicit length penalties or neural reward models.
- Mechanism: The cosine reward scales the accuracy reward based on completion length using a cosine schedule—shorter correct solutions receive higher rewards, while longer incorrect solutions are penalized less severely.
- Core assumption: Concise correct reasoning correlates with better generalization and is learnable via scalar reward shaping.
- Evidence anchors:
  - [section] Section 2: "Cosine Reward: This augments the accuracy reward by scaling it based on response length using a cosine schedule"
  - [section] Figure 5: "Completion lengths stabilize between 1000 and 3500 tokens" in Experiment 3
  - [corpus] No direct corpus evidence for cosine reward mechanisms in small LLMs.
- Break condition: Sacrifices peak accuracy compared to pure accuracy reward (Experiment 2 achieves 80% AMC23 vs. 72.5% in Experiment 3).

## Foundational Learning

- Concept: **Policy Gradient Methods (PPO/GRPO)**
  - Why needed here: GRPO builds on PPO's clipped objective; understanding advantage estimation and KL constraints is prerequisite.
  - Quick check question: Can you explain how GRPO computes advantages differently from standard actor-critic methods?

- Concept: **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The model must learn to produce structured reasoning traces (within 深思 and  tags) rather than direct answers.
  - Quick check question: How does explicit CoT differ from implicit reasoning in terms of reward design?

- Concept: **KL Divergence as Training Stability Metric**
  - Why needed here: Monitoring KL divergence between current and reference policy signals when optimization becomes unstable.
  - Quick check question: What does rising KL divergence after 150 steps indicate about policy drift?

## Architecture Onboarding

- Component map:
  Base model -> GRPO trainer -> Reward pipeline -> Dataset -> Infrastructure
  (DeepSeek-R1-Distill-Qwen-1.5B) (samples 6 outputs) (accuracy + format + cosine) (7k curated math) (4x A40 GPUs)

- Critical path:
  1. Filter source datasets (s1, DeepScaleR) using Qwen2.5-7B-Instruct and Qwen2.5-Math-7B-Instruct
  2. Configure reward weights (format: 1.0, accuracy/cosine: 2.0)
  3. Set max_completion_length=3584 tokens (not 4096—prevents truncation on hard problems)
  4. Train for 50-100 global steps only (performance degrades after 150-200)
  5. Select checkpoint at step 50 or 100 based on AMC23/MATH-500 validation

- Design tradeoffs:
  - Length constraint vs. problem complexity: 4096 tokens insufficient for open-s1 hardest problems; 3584 requires easier problem mixing
  - Training duration vs. stability: First 50-100 steps show rapid gains; extended training causes language drift and accuracy collapse
  - Reward simplicity vs. expressiveness: Rule-based rewards avoid neural reward model costs but cannot penalize intermediate reasoning errors

- Failure signatures:
  - Accuracy degradation after 200 steps with completion length spikes
  - Multilingual output drift (non-English tokens) despite English-only prompts
  - Unreadable content and missing \boxed{} format in late-stage outputs
  - KL divergence instability (Figure 4)

- First 3 experiments:
  1. Reproduce Experiment 1 with open-s1 (18,615 samples, max_length=4096) to observe early gains (50-100 steps) and degradation (>200 steps)
  2. Implement Experiment 2 difficulty mixing (3k open-s1 + 3k open-deepscaler + 1k easy) with max_length=3584 to stabilize lengths
  3. Add cosine reward (Experiment 3) and English-only prompt to test length control vs. peak accuracy tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the rapid reasoning gains achieved via RL in mathematical domains generalize to other reasoning modalities, such as coding or scientific problem solving, under the same resource constraints?
- Basis in paper: [explicit] The authors explicitly state that their evaluation focused "exclusively on mathematical reasoning benchmarks, leaving the generalizability of our approach to other domains - such as scientific reasoning or coding - unexplored."
- Why unresolved: The study restricted its scope to mathematical datasets (e.g., AIME, MATH) to validate the feasibility of low-resource RL, but did not test cross-domain transfer.
- What evidence would resolve it: Applying the Open-RS methodology (7k samples, $42 budget) to coding benchmarks like MBPP or HumanEval and comparing pass@1 rates against baselines.

### Open Question 2
- Question: Does adding a lightweight language reward or filtering the base model effectively mitigate language drift without compromising the reasoning performance of multilingual models?
- Basis in paper: [explicit] The paper identifies language drift (generation of non-English text) as a critical challenge and suggests that "incorporating a lightweight language reward or monolingual pre-filtering of the base model might mitigate language drift."
- Why unresolved: The authors' attempt to solve this via a system prompt ("Reply in English only") failed after 150–200 training steps, indicating a need for structural or reward-based interventions.
- What evidence would resolve it: An ablation study comparing the current training run against runs utilizing a language classifier penalty or a monolingual-initialized checkpoint.

### Open Question 3
- Question: Can multi-stage length schedules or dynamic context windows stabilize training and prevent truncation-induced degradation during prolonged reinforcement learning?
- Basis in paper: [explicit] The authors note that fixed length constraints (4096 tokens) caused instability and suggest that "extending training duration or employing multi-stage length schedules could address truncation issues."
- Why unresolved: The fixed token limit forced the model to truncate complex reasoning chains, leading to performance collapse after 200 steps; the interaction between dynamic lengths and GRPO stability was not tested.
- What evidence would resolve it: Experiments where `max_completion_length` is progressively increased or adapted based on problem difficulty, demonstrating stable accuracy beyond 200 steps.

## Limitations
- Late-stage training instability causes accuracy collapse after 150-200 steps due to KL divergence issues and language drift
- Rule-based rewards cannot capture intermediate reasoning errors, limiting correction of faulty logic paths
- Results heavily depend on carefully curated datasets and specific hyperparameters, raising reproducibility concerns across domains

## Confidence
- High confidence: Early-stage performance gains (50-100 steps) and cost efficiency ($42 vs. thousands) are well-supported by experimental data and validation sets.
- Medium confidence: Difficulty mixing and cosine rewards improve stability and length control, but these mechanisms show clear accuracy-cost tradeoffs and late-stage failure modes.
- Low confidence: Claims about GRPO's computational efficiency and the general applicability of these methods to other reasoning domains lack strong corpus support and may be overfitting to the curated math dataset.

## Next Checks
1. **Early-stop validation protocol:** Implement automated early stopping at 50-100 steps based on validation set performance to prevent accuracy collapse from late-stage instability.
2. **Cross-domain generalization:** Test the trained model on non-math reasoning tasks (e.g., logical puzzles, commonsense reasoning) to assess whether gains transfer beyond the curated dataset.
3. **Reward ablation with intermediate supervision:** Replace rule-based rewards with a lightweight neural reward model trained on intermediate reasoning steps to detect and penalize faulty logic paths, addressing the current inability to correct reasoning errors mid-chain.