---
ver: rpa2
title: 'Information Loss in LLMs'' Multilingual Translation: The Role of Training
  Data, Language Proximity, and Language Family'
arxiv_id: '2506.23340'
source_url: https://arxiv.org/abs/2506.23340
tags:
- translation
- language
- languages
- distance
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how training data, language proximity,
  and language family affect information loss in LLM multilingual translation. Round-trip
  translations were performed using GPT-4 and Llama 2 across 88 and 26 languages respectively,
  with translation quality assessed via BLEU scores and BERT similarity.
---

# Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family

## Quick Facts
- arXiv ID: 2506.23340
- Source URL: https://arxiv.org/abs/2506.23340
- Reference count: 3
- This study investigates how training data, language proximity, and language family affect information loss in LLM multilingual translation

## Executive Summary
This study investigates how training data quantity, language proximity, and language family influence information loss in large language models' multilingual translation capabilities. Using round-trip translations with GPT-4 (88 languages) and Llama 2 (26 languages), the research reveals that abundant training data compensates for linguistic divergence, while closer languages perform better when training data is scarce. The analysis demonstrates that orthographic, phylogenetic, syntactic, and geographical distances strongly predict translation quality, with language family effects operating independently of training data quantity.

## Method Summary
The study employed round-trip translation methodology, translating text from a source language to a target language and back to assess information preservation. Translation quality was evaluated using BLEU scores for n-gram overlap and BERT similarity for semantic preservation. The analysis examined GPT-4 and Llama 2 across 88 and 26 languages respectively, correlating translation quality with training data volume and various linguistic distance measures including orthographic, phylogenetic, syntactic, and geographical distances. Statistical analysis identified interaction effects between training data quantity and language proximity.

## Key Results
- Abundant training data compensates for linguistic divergence, but closer languages perform better in low-resource conditions
- Orthographic, phylogenetic, syntactic, and geographical distances strongly predict translation quality
- Translation quality was higher for Indo-European-Romance and Germanic languages, with GPT-4 showing lower BERT similarity for closely related languages, suggesting flexible translation strategies

## Why This Works (Mechanism)
The interaction between training data quantity and language proximity suggests that LLMs employ different translation strategies based on resource availability. When abundant training data is available, the model can learn robust representations that bridge linguistic gaps between distant languages. In low-resource scenarios, the model relies more heavily on structural similarities between proximate languages. The lower BERT similarity for closely related languages with GPT-4 indicates the model may be employing more flexible, context-aware translation strategies rather than rigid structural preservation, potentially generating more natural target language expressions while maintaining semantic content.

## Foundational Learning
The study builds on established principles of multilingual model training and linguistic typology. It extends previous work on translation quality assessment by incorporating multiple distance metrics (orthographic, phylogenetic, syntactic, geographical) to capture different aspects of linguistic similarity. The round-trip translation methodology follows established practices in translation evaluation, while the use of both n-gram overlap (BLEU) and semantic similarity (BERT) metrics provides a more comprehensive assessment of translation quality than single-metric approaches.

## Architecture Onboarding
The research examines two distinct LLM architectures: GPT-4 (a decoder-only transformer) and Llama 2 (an autoregressive transformer). Both models follow the standard transformer architecture with self-attention mechanisms, but differ in their training approaches and parameter configurations. GPT-4 likely employs mixture-of-experts and other architectural optimizations not present in Llama 2. The consistent patterns observed across both architectures suggest that the findings may generalize to other transformer-based LLMs, though specific performance characteristics may vary based on architectural differences and training procedures.

## Open Questions the Paper Calls Out
- How do different types of training data (parallel vs. monolingual) specifically contribute to translation quality across language distances?
- What are the optimal strategies for balancing training data quantity versus language proximity considerations in multilingual model development?
- How do these findings extend to zero-shot and few-shot translation scenarios beyond the round-trip methodology employed?
- What role do specific architectural choices (attention mechanisms, tokenization, etc.) play in mediating the effects of training data and language proximity?

## Limitations
- Analysis relies on round-trip translation methodology which may introduce artifacts not present in standard translation
- Focus on GPT-4 and Llama 2 limits generalizability to other LLM architectures and training paradigms
- Automated metrics (BLEU and BERT similarity) may not fully capture nuanced aspects of translation quality, particularly for low-resource languages
- The study does not differentiate between types of training data (parallel vs. monolingual), which may have distinct impacts on translation quality

## Confidence
- High confidence in finding that abundant training data compensates for linguistic divergence
- Medium confidence in conclusions about language family effects due to potential underweighted coverage
- Low confidence in interpretation of GPT-4's "flexible, family-sensitive translation strategies" showing lower BERT similarity for closely related languages

## Next Checks
1. Validate findings using human evaluation studies across diverse language pairs, particularly focusing on low-resource languages where automated metrics may be less reliable.

2. Replicate the analysis with additional LLM architectures and training approaches to assess whether observed patterns generalize beyond GPT-4 and Llama 2.

3. Conduct controlled experiments varying training data composition (e.g., parallel vs. monolingual data ratios) to isolate the specific contribution of different data types to translation quality across language distances.