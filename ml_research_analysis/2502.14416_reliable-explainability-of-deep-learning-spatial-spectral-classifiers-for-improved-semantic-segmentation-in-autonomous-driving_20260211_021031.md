---
ver: rpa2
title: Reliable Explainability of Deep Learning Spatial-Spectral Classifiers for Improved
  Semantic Segmentation in Autonomous Driving
arxiv_id: '2502.14416'
source_url: https://arxiv.org/abs/2502.14416
tags:
- channel
- segmentation
- class
- methods
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving explainability
  in deep learning models for hyperspectral image (HSI) segmentation in autonomous
  driving. The authors identify limitations in existing saliency methods like Grad-CAM
  and propose a new approach that analyzes activations and weights from relevant DNN
  layers to better capture the relationship between input features and predictions.
---

# Reliable Explainability of Deep Learning Spatial-Spectral Classifiers for Improved Semantic Segmentation in Autonomous Driving

## Quick Facts
- arXiv ID: 2502.14416
- Source URL: https://arxiv.org/abs/2502.14416
- Reference count: 0
- Primary result: 25-channel HSI model with per-pixel normalization improves illumination robustness while requiring careful balance with edge detection precision

## Executive Summary
This paper addresses the challenge of improving explainability in deep learning models for hyperspectral image (HSI) segmentation in autonomous driving. The authors identify limitations in existing saliency methods like Grad-CAM and propose a new approach that analyzes activations and weights from relevant DNN layers to better capture the relationship between input features and predictions. They evaluate three models using 1, 3, and 25 spectral channels on the HSI-Drive dataset, applying per-pixel normalization to the 25-channel model. The key finding is that models with more spectral channels show improved segmentation performance for classes with high intra-class variability. The 25-channel model with normalization demonstrated robustness to illumination changes, though edge detection was less precise. The study emphasizes that saliency methods should focus on accurately representing inference processes rather than creating visually appealing but superficial visualizations.

## Method Summary
The authors propose a novel approach to explainability by leveraging activations and weights from relevant DNN layers rather than gradient-based methods. They analyze a U-Net architecture applied to hyperspectral images from the HSI-Drive dataset, using 1, 3, and 25 spectral channels. The 25-channel model employs per-pixel normalization to improve illumination robustness. Explainability is achieved by examining conv2D 21 activations (32 feature channels) and conv2D 22 weights (32×5 per class) to identify discriminative features for each class. This direct analysis method aims to satisfy conservativeness property (sum of contributions equals prediction logit) that existing methods like GradCAM violate.

## Key Results
- Models with more spectral channels (3 and 25) show improved segmentation performance for classes with high intra-class variability
- 25-channel model with per-pixel normalization demonstrates robustness to illumination changes, particularly for Vegetation class (IoU improvement from 91.90 to 94.12)
- Edge detection precision degrades with per-pixel normalization, with Road Marks IoU dropping from 93.24 (3-ch) to 89.27 (25-ch PN)
- Direct analysis of layer activations and weights provides more reliable explainability than gradient-based saliency methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-pixel normalization (PN) of spectral signatures improves robustness to illumination changes but reduces edge detection precision.
- Mechanism: PN divides each of the 25 spectral components by the total sum of all components, prioritizing spectral signature shape over mean reflectance values. This homogenizes regions under varying light conditions (sunlight vs. shadow) but sacrifices reflectance-based edge discrimination.
- Core assumption: Spectral signature shape is more invariant to illumination than absolute reflectance values.
- Evidence anchors:
  - Table 1 shows Road Marks IoU drops from 93.24 (3-ch) to 89.27 (25-ch PN), while Vegetation improves from 91.90 to 94.12
  - "By applying PN, we prioritize the shape of the spectral signatures over variations in mean reflectance values" (Section 5.1)
- Break condition: If classes are primarily distinguishable by reflectance magnitude rather than spectral shape (e.g., painted road marks vs. asphalt), PN will degrade performance.

### Mechanism 2
- Claim: Direct analysis of layer activations and weights provides more reliable explainability than gradient-based saliency methods for segmentation tasks.
- Mechanism: Instead of using GradCAM-style global average pooling over gradients (which violates conservativeness), the method examines: (1) activations before the final convolution layer (conv2D 21), and (2) weights/biases of the last convolutional layer (conv2D 22). This traces direct contributions to output logits.
- Core assumption: Activations and weights at task-relevant layers more accurately reflect inference than perturbation or gradient-based proxies.
- Evidence anchors:
  - "propose an alternative approach by leveraging the data provided by activations and weights from relevant DNN layers"
  - "GradCAM passes these sanity checks, while Guided BackProp and GuidedGradCAM are invariant to parameters in higher layers, hence failing" (Section 2.1)
- Break condition: If the network has significant non-linear interactions that cannot be approximated by first-order Taylor decomposition, even direct activation analysis may misattribute contributions.

### Mechanism 3
- Claim: Extended spectral channels (25 vs. 3 vs. 1) improve segmentation for classes with high intra-class variability but require normalization for illumination robustness.
- Mechanism: Additional spectral bands provide more discriminative features for materials with similar visual appearance (metamerism). The 25-channel model with PN shows heterogeneous activations in diverse regions, indicating effective spectral utilization, while 1/3-channel models show spurious activations (e.g., vegetation incorrectly activating for road).
- Core assumption: Intra-class variability in autonomous driving scenes (e.g., vegetation under different lighting) has spectral signatures that are more consistent than spatial/RGB features.
- Evidence anchors:
  - "the activations in the 25-channel model are less homogeneous in highly diverse areas of the image, indicating an effective utilization of the spectral information" (Section 5.2.1)
  - Fig. 5 row b shows vegetation regions erroneously activated for Road in 1-channel and 3-channel models, absent in 25-channel PN
- Break condition: If sensor noise increases with channel count beyond the model's capacity to learn discriminative patterns, additional channels will degrade performance.

## Foundational Learning

- Concept: Class Activation Mapping (CAM) and conservativeness property
  - Why needed here: The paper's central critique is that GradCAM and variants violate conservativeness—the sum of contributions should equal the prediction score. Without understanding this, the proposed alternative lacks context.
  - Quick check question: Given a GradCAM map for a class with logit score 2.3, what should the sum of all spatial contributions equal if the method is conservative?

- Concept: Hyperspectral imaging and metamerism
  - Why needed here: The paper addresses metamerism as a core motivation—different materials appearing identical under certain illumination. HSI provides spectral signatures to disambiguate.
  - Quick check question: Why would two materials with identical RGB values have different 25-band spectral signatures?

- Concept: U-Net skip connections and feature propagation
  - Why needed here: The analysis specifically examines conv2D 1 (early layer) and conv2D 21 (penultimate layer) because skip connections directly link them, preserving spatial information for interpretability.
  - Quick check question: In a U-Net, how does a skip connection from encoder block 2 to decoder affect what features are available at the final segmentation layer?

## Architecture Onboarding

- Component map:
  - Input: 1/3/25 channel images from Imec 5x5 MSFA snapshot camera (NIR range)
  - Encoder: Standard U-Net convolutional blocks
  - Key layers for analysis: conv2D 1 (early spatial features), conv2D 21 (32 feature channels before output), conv2D 22 (1x1 convolution producing 5-class logits)
  - Preprocessing: Per-pixel normalization divides each spectral component by sum of all 25 (for 25-channel model only)
  - Output: 5-class segmentation (Road, Road Marks, Vegetation, Sky, Other)

- Critical path:
  1. Load HSI-Drive v2.0 image (25 bands, MSFA format)
  2. Apply per-pixel normalization: `normalized[b] = pixel[b] / sum(pixel[all_bands])`
  3. Forward pass through U-Net
  4. Extract conv2D 21 activations (32 channels) and conv2D 22 weights (32×5 per class)
  5. Correlate activation channels with class weights to identify discriminative features

- Design tradeoffs:
  - PN vs. raw reflectance: PN improves illumination robustness (Vegetation +2.22 IoU) but degrades edge-based classes (Road Marks -3.97 IoU)
  - Channel count vs. edge precision: More channels improve intra-class discrimination but introduce noise in early-layer edge detection (Fig. 6 shows noisier activations in 25-ch PN model)
  - Saliency method selection: GradCAM/variants produce visually appealing maps but violate conservativeness; direct activation analysis is less intuitive but mathematically grounded

- Failure signatures:
  - Non-conservative CAM: Sum of Lc_ij ≠ yc (prediction logit)—indicates method cannot be trusted for comparative analysis
  - Spurious cross-class activation: Vegetation regions activating for Road class in 1/3-channel models (Fig. 5 row b)
  - Edge bleeding in PN model: Road marks class showing fuzzy boundaries despite correct classification (Table 1 IoU drop)

- First 3 experiments:
  1. **Reproduce conservativeness violation**: Compute sum of GradCAM contributions for a single pixel and compare to its logit value. Confirm ≠ relationship per Eq. 5-7 extension.
  2. **PN ablation with mixed illumination**: Train 25-channel model with and without PN on HSI-Drive images containing strong shadow boundaries. Measure per-class IoU delta, expecting Vegetation improvement and Road Marks degradation.
  3. **Cross-model activation correlation**: Extract conv2D 21 activations for 1/3/25 channel models on same image. Compute channel-wise correlation matrix (as in Fig. 4) to identify which features are preserved vs. unique to spectral richness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the trade-off between per-pixel normalization (PN) for illumination robustness and the resulting degradation in edge detection precision be effectively mitigated?
- Basis in paper: The authors note in the Discussion that while PN improves robustness to lighting changes, it leads to "less accurate edge detection, particularly for classes that more strongly rely on reflectance differences."
- Why unresolved: The current method prioritizes spectral shape over reflectance, causing distinct failures in classes like Road Marks while improving performance in others like Vegetation.
- What evidence would resolve it: A training strategy or normalization technique that maintains high IoU scores for edge-dependent classes (Road Marks) simultaneously with illumination-invariant classes (Vegetation) in shadowed/sunny scenes.

### Open Question 2
- Question: Can a formal saliency method be developed for semantic segmentation that strictly satisfies the "conservativeness" property without relying on manual layer inspection?
- Basis in paper: The authors identify a "critical need for better interpretability models specifically tailored for semantic segmentation networks" after showing existing methods (GradCAM, SegGradCAM) fail to meet conservativeness criteria.
- Why unresolved: The authors currently propose a manual analysis of activations and weights as an alternative, but this is not a generalized, automated saliency method.
- What evidence would resolve it: A mathematical formulation for a segmentation saliency map where the sum of contributions over the input spatially equals the output logits for every pixel, validated on standard architectures.

### Open Question 3
- Question: What architectural strategies allow deep learning models to effectively balance the high dimensionality of HSI data with the ability to generalize across diverse, unseen driving environments?
- Basis in paper: The conclusion states future work should focus on "creating models that balance the richness of data provided by HSI with the ability to generalize effectively... for autonomous driving systems."
- Why unresolved: While the paper shows improved performance with 25 channels on HSI-Drive, it highlights the difficulty of handling high intra-class variability and the need for models that trustworthily scale with this data.
- What evidence would resolve it: Benchmarks showing that a 25-channel model trained on HSI-Drive maintains superior performance over RGB models when transferred to external driving datasets without fine-tuning.

## Limitations

- Dataset size and single-sensor constraint limit generalizability of findings
- Edge detection degradation with per-pixel normalization represents fundamental tradeoff between spectral and spatial discriminability
- Manual analysis of activations and weights is not a generalized, automated saliency method

## Confidence

- High confidence: Conservativeness violation in GradCAM (well-established theoretical foundation)
- Medium confidence: PN benefits for illumination robustness (dataset-specific, requires cross-sensor validation)
- Medium confidence: Spectral channel advantages for intra-class variability (class-specific, illumination-dependent)

## Next Checks

1. Test conservativeness property on a second segmentation dataset (e.g., Cityscapes) to verify GradCAM violation is not HSI-specific
2. Cross-sensor validation: Apply 25-channel PN model to data from different hyperspectral cameras to assess sensor dependency
3. Ablation study: Quantify edge detection precision vs. illumination robustness tradeoff by systematically varying PN strength across validation images