---
ver: rpa2
title: Multi-Scale Feature Fusion and Graph Neural Network Integration for Text Classification
  with Large Language Models
arxiv_id: '2511.05752'
source_url: https://arxiv.org/abs/2511.05752
tags:
- semantic
- text
- feature
- classification
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a hybrid text classification method integrating
  large language model feature extraction, multi-scale feature pyramid fusion, and
  graph neural network modeling. The approach addresses limitations of single-scale
  representations by capturing hierarchical semantic information and structured dependencies
  within text.
---

# Multi-Scale Feature Fusion and Graph Neural Network Integration for Text Classification with Large Language Models

## Quick Facts
- arXiv ID: 2511.05752
- Source URL: https://arxiv.org/abs/2511.05752
- Reference count: 25
- Primary result: Accuracy 0.913, F1-Score 0.905, AUC 0.947 on AG News dataset

## Executive Summary
This study introduces a hybrid text classification method that combines large language model (LLM) feature extraction with multi-scale feature pyramid fusion and graph neural network (GNN) modeling. The approach addresses the limitation of single-scale representations by capturing hierarchical semantic information and structured dependencies within text. Experimental results demonstrate superior performance on the AG News dataset compared to baseline models, with particular strengths in handling complex text classification tasks.

## Method Summary
The proposed method integrates three key components: LLM-based feature extraction for rich semantic representations, multi-scale feature pyramid fusion to capture hierarchical information across different granularities, and GNN modeling to capture structured dependencies between text elements. This hybrid architecture enables the model to leverage both the contextual understanding capabilities of LLMs and the structural reasoning abilities of graph neural networks, resulting in improved classification performance.

## Key Results
- Accuracy of 0.913 on AG News dataset
- F1-Score of 0.905 demonstrating balanced precision and recall
- AUC of 0.947 indicating strong discriminative ability
- Precision of 0.908 showing reliable positive class identification

## Why This Works (Mechanism)
The method works by addressing the fundamental limitation of single-scale representations in text classification. By integrating LLM feature extraction with multi-scale feature pyramid fusion, the model captures semantic information at multiple granularities - from individual words to phrases to entire sentences. The subsequent GNN modeling then captures the structured dependencies between these multi-scale representations, allowing the model to understand both the content and the relationships within the text. This hierarchical approach enables more nuanced understanding compared to traditional single-scale methods.

## Foundational Learning

**Graph Neural Networks (GNNs)**: Neural networks designed to operate on graph-structured data, aggregating information from neighboring nodes to learn node representations. Needed for capturing relationships between text elements. Quick check: Verify message passing mechanism works correctly for the graph construction.

**Feature Pyramid Fusion**: Technique that combines features from different scales/resolutions to create rich multi-scale representations. Needed to capture semantic information at multiple granularities. Quick check: Ensure feature alignment and fusion weights are properly learned.

**Large Language Models (LLMs)**: Pre-trained transformer models that capture contextual semantic information from large text corpora. Needed for extracting rich initial representations from text. Quick check: Validate feature extraction quality and dimensionality compatibility.

## Architecture Onboarding

**Component Map**: Text Input -> LLM Feature Extractor -> Multi-Scale Feature Pyramid -> Graph Construction -> GNN Layers -> Classification Output

**Critical Path**: The critical computational path runs through LLM feature extraction (most resource-intensive), followed by feature pyramid fusion, then GNN processing, and finally classification.

**Design Tradeoffs**: The method trades increased computational complexity for improved accuracy. Using pre-trained LLMs adds significant parameters but provides rich initial representations. GNN layers add structure modeling capability at the cost of additional training complexity.

**Failure Signatures**: Potential failures include: 1) Poor LLM feature extraction leading to weak semantic foundation, 2) Feature pyramid misalignment causing noisy multi-scale fusion, 3) Inappropriate graph construction failing to capture meaningful relationships, 4) Overfitting due to increased model complexity.

**First Experiments**:
1. Verify baseline LLM feature extraction quality on a simple classification task
2. Test feature pyramid fusion independently to ensure proper multi-scale integration
3. Validate GNN performance on a small graph-structured text dataset before full integration

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single dataset (AG News), restricting generalizability
- No comparison with recent transformer-based state-of-the-art models
- Missing computational efficiency and runtime analysis
- Limited ablation studies to quantify individual component contributions

## Confidence

**High Confidence**: Experimental methodology and result reporting (accuracy metrics, F1-score, AUC values are clearly presented and reproducible)

**Medium Confidence**: Theoretical framework of multi-scale feature fusion and GNN integration (well-established concepts but novel combination requires further validation)

**Low Confidence**: Claims of robustness across different learning rates (only tested within limited range, no sensitivity analysis provided)

## Next Checks
1. Cross-dataset validation: Test the proposed method on diverse text classification benchmarks (e.g., IMDB, Yelp, and DBpedia) to verify generalizability across different text domains and classification complexities.

2. Ablation studies: Conduct systematic experiments removing each component (multi-scale fusion, GNN modeling, LLM feature extraction) to quantify their individual contributions and identify potential redundancy.

3. Computational efficiency analysis: Measure inference time, memory consumption, and parameter counts compared to transformer-based baselines to assess practical deployment feasibility for real-world applications.