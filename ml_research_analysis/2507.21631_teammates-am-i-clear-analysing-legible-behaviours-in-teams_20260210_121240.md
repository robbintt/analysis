---
ver: rpa2
title: '"Teammates, Am I Clear?": Analysing Legible Behaviours in Teams'
arxiv_id: '2507.21631'
source_url: https://arxiv.org/abs/2507.21631
tags:
- legible
- agents
- agent
- team
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends legibility to multi-agent teamwork settings,
  proposing a centralized training / decentralized execution approach for legible
  decision-making in MDPs. The method adapts PoLMDP to team contexts by learning joint
  policies and then training legible agents as best responses to optimal teammates.
---

# "Teammates, Am I Clear?": Analysing Legible Behaviours in Teams

## Quick Facts
- arXiv ID: 2507.21631
- Source URL: https://arxiv.org/abs/2507.21631
- Reference count: 29
- This paper extends legibility to multi-agent teamwork settings, proposing a centralized training / decentralized execution approach for legible decision-making in MDPs.

## Executive Summary
This paper introduces a framework for legible decision-making in multi-agent teams, extending the concept of legibility from human-robot interaction to cooperative multi-agent systems. The approach involves training agents to act in ways that are easily interpretable by teammates, thereby improving coordination and task completion. The authors adapt the PoLMDP algorithm to team contexts, enabling agents to learn joint policies and then train legible agents as best responses to optimal teammates. Experiments in level-based foraging and pursuit-evasion scenarios demonstrate that teams with a legible leader complete tasks faster and allow followers to infer intentions more quickly than teams with only optimal agents.

## Method Summary
The method proposed in this paper extends legibility to multi-agent teamwork settings by adapting the PoLMDP algorithm for team contexts. It uses a centralized training / decentralized execution approach, where agents first learn joint policies through centralized training and then execute them independently. The key innovation is training legible agents as best responses to optimal teammates, ensuring that the actions of the legible agent are easily interpretable by others. This is achieved by optimizing the legible agent's policy to maximize the expected legibility of its actions, given the optimal policies of its teammates. The approach is evaluated in level-based foraging and pursuit-evasion scenarios, where the legible leader's behavior significantly improves team performance and intention transmission.

## Key Results
- Teams with a legible leader complete tasks faster than teams with only optimal agents.
- Followers can infer the intentions of the legible leader more quickly, especially in smaller state spaces.
- Legible behaviors improve team performance and intention transmission in cooperative tasks, with trade-offs in competitive settings.

## Why This Works (Mechanism)
The mechanism behind this approach relies on the concept of legibility in multi-agent systems, where an agent's actions are designed to be easily interpretable by teammates. By training legible agents as best responses to optimal teammates, the method ensures that the legible agent's behavior is predictable and interpretable, facilitating better coordination. The centralized training / decentralized execution framework allows for the learning of joint policies while maintaining the ability of agents to act independently during execution. This combination of legibility and optimal response training leads to improved team performance and faster intention transmission.

## Foundational Learning
- **PoLMDP (Partially Observable Leader-Follower MDP)**: Why needed: To model the interaction between a legible leader and its followers in a team setting. Quick check: Ensure the model captures the leader's intent and the followers' observations accurately.
- **Legibility in Multi-Agent Systems**: Why needed: To enable agents to act in ways that are easily interpretable by teammates, improving coordination. Quick check: Verify that the legible agent's actions are consistently predictable by its followers.
- **Centralized Training / Decentralized Execution**: Why needed: To learn joint policies while allowing agents to act independently during execution. Quick check: Confirm that the trained policies generalize well to decentralized execution.
- **Best Response Training**: Why needed: To optimize the legible agent's policy as a response to the optimal policies of its teammates. Quick check: Ensure the legible agent's policy is indeed the best response to the teammates' optimal policies.

## Architecture Onboarding
- **Component Map**: Team Environment -> Centralized Training (Joint Policies) -> Decentralized Execution (Legible Agents) -> Follower Interpretation
- **Critical Path**: The critical path involves the centralized training phase where joint policies are learned, followed by the decentralized execution where legible agents act as best responses to optimal teammates.
- **Design Tradeoffs**: The tradeoff lies in balancing the complexity of the joint policy learning with the simplicity of the legible agent's interpretation. More complex joint policies may lead to better coordination but could make the legible agent's behavior harder to interpret.
- **Failure Signatures**: If the legible agent's behavior is not consistently predictable by followers, it indicates a failure in the legibility training. Additionally, if team performance does not improve, it suggests issues with the joint policy learning or the best response optimization.
- **First Experiments**:
  1. Test the legibility of the leader's actions in a simple cooperative task, such as a grid-world navigation.
  2. Evaluate the impact of legibility on team performance in a more complex scenario, like the level-based foraging task.
  3. Investigate the trade-offs in competitive settings by comparing the performance of legible and optimal agents in a pursuit-evasion game.

## Open Questions the Paper Calls Out
None

## Limitations
- The approach may not scale well to larger state spaces, where the complexity of joint policy learning increases significantly.
- The trade-offs in competitive settings suggest that legibility may not always be beneficial, especially when agents have conflicting goals.
- The method relies on centralized training, which may not be feasible in all real-world applications due to communication constraints.

## Confidence
- High: The experimental results clearly demonstrate the benefits of legible behaviors in cooperative tasks, with significant improvements in task completion time and intention transmission.
- Medium: The scalability of the approach to larger state spaces and more complex scenarios remains to be thoroughly tested.
- Low: The trade-offs in competitive settings indicate that further research is needed to understand when and how legibility can be effectively applied in adversarial contexts.

## Next Checks
1. Test the scalability of the approach to larger state spaces and more complex team dynamics.
2. Investigate the impact of legibility in adversarial settings, where agents have conflicting goals.
3. Evaluate the robustness of the legible agent's behavior to variations in the teammates' policies or environmental conditions.