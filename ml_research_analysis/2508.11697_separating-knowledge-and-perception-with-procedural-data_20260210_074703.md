---
ver: rpa2
title: Separating Knowledge and Perception with Procedural Data
arxiv_id: '2508.11697'
source_url: https://arxiv.org/abs/2508.11697
tags:
- procedural
- data
- mixup
- real
- shaders
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to visual representation learning
  using procedural data and visual memory. Instead of training models on real-world
  images, the authors generate non-realistic images via code and train self-supervised
  vision transformers on them.
---

# Separating Knowledge and Perception with Procedural Data

## Quick Facts
- arXiv ID: 2508.11697
- Source URL: https://arxiv.org/abs/2508.11697
- Reference count: 40
- Primary result: Procedural data trained models achieve within 10% of Places on ImageNet-1K classification and demonstrate strong zero-shot segmentation while guaranteeing privacy

## Executive Summary
This paper presents a novel approach to visual representation learning using procedural data and visual memory. Instead of training models on real-world images, the authors generate non-realistic images via code and train self-supervised vision transformers on them. They then apply k-nearest neighbors (KNN) classification and segmentation using a database of reference image embeddings (visual memory) without further training. The approach achieves perfect compartmentalization with respect to real-world data while retaining strong performance, enabling efficient data unlearning and privacy guarantees.

## Method Summary
The authors train a Vision Transformer (ViT-S) using DINO self-supervision on OpenGL-generated shaders (procedural data) and apply KNN classification/segmentation using a database of labeled reference embeddings. The training involves generating procedural data through Shaders KML Mixup, which combines multiple shaders using masks derived from K-Means clustering. The trained model is then evaluated on real-world tasks (classification, segmentation) by searching a precomputed Visual Memory containing embeddings of labeled real images. This approach decouples knowledge storage from the embedding model, enabling zero-shot classification and efficient unlearning.

## Key Results
- Procedural models trained on Shaders KML Mixup outperform Places-trained models on fine-grained classification tasks (CUB200, Flowers102) by 8-15%
- Achieve ImageNet-1K classification within 10% of Places-trained models
- Demonstrate strong zero-shot segmentation performance within 10% of real-data models on COCO
- Procedural models lack gestalt perception and struggle with KNN segmentation due to not seeing real-world objects during training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training on procedural (non-realistic) data isolates visual perception skills from semantic knowledge, reducing privacy risks while retaining feature quality.
- **Mechanism:** The system trains a Vision Transformer (ViT) using DINO on OpenGL-generated shaders (procedural data). Because the data consists of abstract shapes and textures rather than natural images, the model learns local-to-global similarity (perception) without memorizing specific real-world identities, faces, or copyrighted objects.
- **Core assumption:** Visual features learned from abstract textures and geometry transfer sufficiently to real-world images to serve as a basis for retrieval.
- **Evidence anchors:**
  - [abstract]: "Procedural data is generated using OpenGL code, allowing for full compartmentalization with respect to real-world images..."
  - [Section 1]: "...procedural data is non-realistic and is generated via simple code... much less exposed to the privacy or bias risks..."
- **Break condition:** The transfer fails if the target task requires high-level semantic coherence which abstract shaders cannot simulate (Section 5).

### Mechanism 2
- **Claim:** An explicit "Visual Memory" (non-parametric database) decouples knowledge storage from the embedding model, enabling zero-shot classification and efficient unlearning.
- **Mechanism:** Instead of a linear classification head, the system uses a K-Nearest Neighbors (KNN) search over a database of labeled reference embeddings. "Knowledge" is added by inserting new labeled vectors and removed by deleting them, avoiding model retraining.
- **Core assumption:** The embedding space is structured such that semantically similar real images cluster closely enough for KNN to be effective, despite the model never seeing real images during training.
- **Evidence anchors:**
  - [abstract]: "...apply them on visual similarity... without further training by using visual memoryâ€”an explicit database..."
  - [Section 1]: "...visual memory can simply add and drop data samples... training the embedding model with procedural data [solves the problem of weights containing real data]."
- **Break condition:** Performance degrades if the Visual Memory is sparse or uncurated, or if the query embedding drifts significantly from the support set distribution.

### Mechanism 3
- **Claim:** Data-driven mixing masks (Shaders KML) improve generalization over standard Mixup by increasing texture/shape diversity.
- **Mechanism:** The authors generate training samples by mixing multiple shaders using masks derived from K-Means clustering of other shader images. This creates higher boundary complexity and diversity than standard linear interpolation (Mixup), forcing the model to learn more robust features.
- **Core assumption:** Increasing the complexity of synthetic boundaries during pre-training directly improves the model's ability to segment real-world object boundaries later.
- **Evidence anchors:**
  - [Section 3.1]: "...extracts mixing masks from the Shaders images, rather than always using a constant mixing mask... increasing dataset diversity..."
  - [Figure 5]: Shows the difference between constant Mixup masks and K-Means derived masks.
- **Break condition:** The mechanism provides no benefit (and adds computational cost) if the downstream task relies purely on global color statistics rather than shape/texture discrimination.

## Foundational Learning

- **Concept:** Self-Supervised Learning (DINO)
  - **Why needed here:** This is the engine used to train the embedding model without labels. You must understand how the local-to-global similarity loss works to grasp why the model groups "parts" of abstract shapes together.
  - **Quick check question:** How does DINO enforce similarity between global image features and local patch features, and why would this learn "parts" of objects?

- **Concept:** Non-Parametric / Memory-Based Classification
  - **Why needed here:** The core innovation is replacing the standard linear head with a database lookup. Understanding the trade-off between inference speed (KNN is slower) and flexibility (no retraining) is critical.
  - **Quick check question:** Why does KNN classification allow for "perfect unlearning" compared to a parametric linear layer?

- **Concept:** Transfer Learning / Domain Gap
  - **Why needed here:** The system trains on "Shaders" (Domain A) and tests on "Real Images" (Domain B). You need to understand why features learned in A might still be useful in B.
  - **Quick check question:** What visual properties (e.g., edges, textures) are shared between OpenGL shaders and natural photographs that allow for this transfer?

## Architecture Onboarding

- **Component map:** Data Generator (OpenGL/Shaders) -> Backbone (ViT-S) -> Visual Memory -> Inference Interface
- **Critical path:**
  1. Validate the DINO training pipeline on the "Shaders KML" data (ensure loss converges)
  2. Build the Visual Memory index (extract embeddings from the reference dataset, e.g., ImageNet training set)
  3. Run the KNN evaluation to measure transfer performance

- **Design tradeoffs:**
  - **Storage vs. Compute:** Visual Memory requires storing millions of vectors (high storage, cheap) vs. training a classifier (low storage, expensive GPU compute to retrain)
  - **Privacy vs. Performance:** Procedural pre-training guarantees no real data leakage in weights, but incurs a ~10% performance penalty on general classification compared to real-data pre-training (Table 1)

- **Failure signatures:**
  - **"Excessive Locality":** The model retrieves images with similar textures but wrong semantic classes because it lacks object-level Gestalt (Section 5, Figure 11)
  - **Sparse Memory Errors:** If the Visual Memory lacks a specific class, the system defaults to the nearest visual texture, which is often incorrect

- **First 3 experiments:**
  1. **Validation of "Perception" Quality:** Train a ViT-S on the Shaders KML dataset. Evaluate on NIGHTS (visual similarity) to confirm it matches the paper's reported ~82% alignment.
  2. **Privacy/Unlearning Test:** Construct a memory with a "poisoned" image (e.g., a specific face). Verify that removing that single entry from the memory immediately prevents the model from recognizing that face, without retraining the backbone.
  3. **Ablation of Object-Part Coherence:** Visualize PCA feature maps for a complex object (e.g., a bicycle). Confirm the failure mode: distinct parts (wheel vs. frame) should have distinct colors in the procedural model, unlike a model trained on real data (Figure 8).

## Open Questions the Paper Calls Out

- **Question:** How can procedural training methods be adapted to learn object-level grouping rather than just local similarities to reduce spurious matches in visual memory?
  - **Basis in paper:** [explicit] The authors state, "We leave finding ways of addressing these limitations [parts of objects having dissimilar representations] to future work."
  - **Why unresolved:** Current self-supervised objectives on procedural data treat parts of abstract shapes as similar but fail to associate visually distinct parts of the same object entity, leading to incorrect nearest-neighbor searches.
  - **What evidence would resolve it:** A procedural model that correctly clusters visually distinct parts of a complex object (e.g., wheel spokes and frame) in feature space, improving KNN segmentation accuracy.

- **Question:** Can unsupervised vision models be trained to inherently obey Gestalt principles (closure, continuity, etc.) using procedural data?
  - **Basis in paper:** [inferred] The authors analyze that current vision models "do not have gestalt perception," failing to align with human perceptual grouping despite high performance on recognition tasks.
  - **Why unresolved:** Current self-supervised objectives do not explicitly encode these principles, and procedural diversity alone does not seem sufficient to cause them to emerge naturally.
  - **What evidence would resolve it:** Quantitative improvements on the Gestalt segmentation benchmarks (e.g., higher $R^2$ for Closure or Continuity) achieved via a modified procedural training recipe.

- **Question:** Can the observed linear trade-off between model utility (accuracy) and the percentage of non-private training samples be decoupled?
  - **Basis in paper:** [inferred] The authors note a linear trend in Figure 12 between accuracy and the fraction of non-private samples, suggesting a fundamental tension between maximizing performance and maintaining strict privacy.
  - **Why unresolved:** While the paper demonstrates the trade-off exists, it does not propose a mechanism to improve privacy retention (minimizing non-private samples) at higher accuracy levels.
  - **What evidence would resolve it:** A training regime where KNN classification accuracy increases significantly without a proportional rise in the percentage of non-private memory samples.

## Limitations

- The core innovation relies on transfer from procedural to real-world features, yet the paper acknowledges a significant "gestalt perception" gap - models struggle to understand whole-object coherence when trained only on procedural data.
- Experimental evaluation shows strong performance on fine-grained classification but only within ~10% of real-data baselines on general ImageNet-1K.
- The paper does not adequately address whether this performance gap is acceptable for practical deployment scenarios where maximal accuracy is required.

## Confidence

- **High confidence** in the mechanism of using visual memory (KNN) to decouple knowledge storage from the embedding model, as this is a well-established approach with clear computational benefits.
- **Medium confidence** in the claim that procedural data eliminates privacy risks, given that the paper demonstrates this only through theoretical reasoning about non-realistic content rather than empirical privacy leakage tests.
- **Low confidence** in the scalability of this approach to complex real-world scenarios requiring high-level semantic understanding, due to the acknowledged gestalt perception limitations.

## Next Checks

1. **Privacy Leakage Test:** Conduct an adversarial attack where a malicious actor attempts to reconstruct sensitive information from the procedural-trained model's embeddings when queried with real-world images containing identifiable information.
2. **Object Coherence Analysis:** Systematically compare object-part embeddings from procedural-trained models versus real-data models across diverse object categories to quantify the gestalt perception gap's severity for different object types.
3. **Real-world Deployment Simulation:** Test the approach on a practical task requiring both fine-grained discrimination and object-level understanding (e.g., medical imaging diagnosis) to determine whether the 10% performance penalty is acceptable in safety-critical applications.