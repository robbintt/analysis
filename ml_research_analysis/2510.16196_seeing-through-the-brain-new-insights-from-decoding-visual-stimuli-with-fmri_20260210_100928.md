---
ver: rpa2
title: 'Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI'
arxiv_id: '2510.16196'
source_url: https://arxiv.org/abs/2510.16196
tags:
- image
- fmri
- space
- text
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of reconstructing visual stimuli
  from fMRI signals, a central challenge in neuroscience and machine learning. The
  authors propose PRISM, a framework that bridges fMRI signals and image reconstruction
  by projecting them into a structured text space rather than a vision-based one.
---

# Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI

## Quick Facts
- arXiv ID: 2510.16196
- Source URL: https://arxiv.org/abs/2510.16196
- Authors: Zheng Huang; Enpei Zhang; Yinghao Cai; Weikang Qiu; Carl Yang; Elynn Chen; Xiang Zhang; Rex Ying; Dawei Zhou; Yujun Yan
- Reference count: 35
- Primary result: PRISM achieves up to 8% reduction in perceptual loss and 17% improvement in LPIPS for fMRI-to-image reconstruction

## Executive Summary
This work addresses the challenge of reconstructing visual stimuli from fMRI signals, a central problem in neuroscience and machine learning. The authors propose PRISM, a novel framework that bridges fMRI signals and image reconstruction by projecting them into a structured text space rather than a vision-based one. This approach leverages the compositional nature of visual stimuli - objects, attributes, and relationships - through language models and generative models.

The key insight is that fMRI signals align more closely with text space representations than with vision-based or joint text-image spaces. By adapting text representations to capture compositional visual information and developing specialized modules for object-centric diffusion and attribute-relationship search, PRISM significantly improves reconstruction quality. Extensive experiments on three real-world datasets demonstrate substantial improvements over state-of-the-art methods, validating the effectiveness of using structured text as an intermediate space for fMRI-to-image reconstruction.

## Method Summary
PRISM introduces a novel framework that reconstructs visual stimuli from fMRI signals by projecting brain activity into a structured text space. The method consists of two key modules: an object-centric diffusion module that generates images by composing individual objects, and an attribute-relationship search module that automatically identifies brain-aligned object attributes and relationships. The framework leverages the compositional nature of visual stimuli - objects, their attributes, and relationships between them - to create more accurate reconstructions.

The approach involves mapping fMRI signals to text representations using language models, then adapting these representations to capture the compositional structure of visual stimuli. The object-centric diffusion model generates images by composing objects identified from the text space, while the attribute-relationship search module enhances the reconstruction by identifying relevant attributes and relationships that align with the brain activity patterns. This text-space bridging approach differs fundamentally from previous vision-based or joint text-image methods.

## Key Results
- PRISM achieves up to an 8% reduction in perceptual loss compared to state-of-the-art methods
- LPIPS metric shows up to 17% improvement with PRISM over competing approaches
- fMRI signals demonstrate stronger alignment with text space representations than with vision-based or joint text-image spaces

## Why This Works (Mechanism)
The success of PRISM stems from recognizing that visual perception and language processing share fundamental compositional structures. The brain processes visual stimuli by decomposing them into objects, attributes, and relationships - a structure that aligns naturally with how language models represent information. By projecting fMRI signals into this text space, PRISM captures the underlying cognitive processing patterns more effectively than direct vision-based approaches.

The compositional nature of visual stimuli maps well to the hierarchical structure of language, where objects correspond to nouns, attributes to adjectives, and relationships to relational phrases. This alignment allows the framework to leverage the rich semantic representations learned by language models to decode brain activity. The object-centric diffusion module then reconstructs images by composing individual objects based on these decoded representations, while the attribute-relationship search module refines the reconstruction by identifying brain-aligned attributes and relationships.

## Foundational Learning
- fMRI signal processing: Understanding how brain activity patterns encode visual information is crucial for decoding stimuli; quick check: verify signal preprocessing steps preserve relevant spatial and temporal information
- Language model representations: Language models capture compositional structures that align with visual perception; quick check: analyze embedding similarities between visual and textual concepts
- Diffusion models: These generative models can compose objects from learned representations; quick check: validate object composition quality in controlled settings
- Cross-modal alignment: Mapping between brain activity and text representations requires careful alignment; quick check: measure alignment quality across different brain regions
- Perceptual metrics: LPIPS and perceptual loss quantify reconstruction quality beyond pixel-level similarity; quick check: correlate metric improvements with human perceptual judgments

## Architecture Onboarding

Component map:
fMRI signals -> Text space projection -> Attribute-Relationship Search -> Object-centric Diffusion -> Reconstructed images

Critical path:
fMRI preprocessing -> Language model embedding -> Compositional text adaptation -> Object composition -> Image generation

Design tradeoffs:
- Text space vs. vision space: Text space provides better compositional alignment but may lose fine-grained visual details
- Object-centric vs. holistic generation: Composing objects allows more control but may introduce artifacts at boundaries
- Automatic vs. manual attribute selection: Automatic search scales better but may miss subtle brain-aligned attributes

Failure signatures:
- Poor reconstruction when visual stimuli contain complex textures or fine details not well-captured in text
- Artifacts at object boundaries when composition fails to maintain spatial coherence
- Misalignment between decoded attributes and actual visual features when language model representations don't capture domain-specific concepts

First experiments:
1. Validate fMRI-to-text alignment quality on held-out data with known stimulus-text pairs
2. Test object composition quality with synthetic object arrangements before full integration
3. Conduct ablation study removing attribute-relationship search to measure its contribution

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Performance improvements may be dataset-specific and not generalize to all types of visual stimuli
- Perceptual quality assessments remain subjective despite quantitative metric improvements
- The framework's reliance on specific language models may limit its applicability across different experimental paradigms

## Confidence
- Framework architecture and implementation: High
- Comparative performance improvements: Medium (dependent on experimental setup)
- Generalizability across datasets and paradigms: Medium

## Next Checks
1. Test PRISM on additional fMRI datasets with different visual stimulus types to assess generalizability beyond current experimental scope
2. Conduct ablation studies to isolate the contribution of individual components (object-centric diffusion vs. attribute-relationship search) to overall performance improvements
3. Perform cross-modal alignment analysis to verify that observed fMRI-text alignment is not an artifact of specific language models or training procedures used