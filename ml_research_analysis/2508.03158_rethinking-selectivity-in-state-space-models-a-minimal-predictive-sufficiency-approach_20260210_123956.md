---
ver: rpa2
title: 'Rethinking Selectivity in State Space Models: A Minimal Predictive Sufficiency
  Approach'
arxiv_id: '2508.03158'
source_url: https://arxiv.org/abs/2508.03158
tags:
- state
- information
- predictive
- principle
- mps-ssm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of a first-principle derivation for
  the selective mechanisms in modern State Space Models (SSMs) like Mamba, which raises
  questions about their optimality and robustness. The authors propose the Principle
  of Predictive Sufficiency, which states that an ideal hidden state should be a minimal
  sufficient statistic of the past for predicting the future.
---

# Rethinking Selectivity in State Space Models: A Minimal Predictive Sufficiency Approach

## Quick Facts
- arXiv ID: 2508.03158
- Source URL: https://arxiv.org/abs/2508.03158
- Reference count: 40
- Primary result: Introduces Minimal Predictive Sufficiency State Space Model (MPS-SSM), achieving state-of-the-art performance on long-term forecasting benchmarks while improving robustness to noise through a theoretically grounded selective mechanism.

## Executive Summary
This paper addresses the lack of theoretical grounding for selective mechanisms in modern State Space Models (SSMs) like Mamba. The authors propose the Principle of Predictive Sufficiency, which defines an ideal hidden state as a minimal sufficient statistic of the past for predicting the future. Based on this principle, they introduce MPS-SSM, a framework where selectivity is guided by optimizing an objective that encourages maximal compression of historical information without losing predictive power. The model achieves state-of-the-art performance on standard benchmarks while demonstrating superior robustness to noise and non-causal perturbations.

## Method Summary
MPS-SSM is built on an SSM backbone with input-dependent selection parameters. The key innovation is a dual-loss objective: a prediction loss ensuring sufficiency (the hidden state retains predictive power) and a minimality regularizer estimated via an auxiliary decoder that reconstructs the input from the hidden state. The minimality term is formulated as a variational upper bound on mutual information, optimized end-to-end with the main prediction task. The regularization coefficient λ controls the trade-off between compression and predictive accuracy, with optimal values varying by dataset. The principle is also extended as a general regularization framework applicable to non-SSM architectures like Transformers.

## Key Results
- MPS-SSM achieves state-of-the-art performance on long-term forecasting benchmarks (ETTh1, ETTh2, ETTm1, ETTm2, Weather, Electricity, Traffic, Exchange)
- Significant improvement in robustness to impulse noise and non-causal perturbations compared to existing models
- The principle extends to regularize non-SSM models, with MPS-PatchTST achieving MSE 0.328 on ETTh1 (horizon 96)

## Why This Works (Mechanism)

### Mechanism 1
- Constraining the hidden state to be a minimal sufficient statistic filters non-causal noise while preserving predictive signal
- The objective combines prediction loss with a minimality regularizer estimated via variational upper bound using an auxiliary decoder
- Core assumption: Model class is expressive enough that prediction loss can approach zero
- Evidence: Section 3.3.1 provides explicit formulas for LTotal = LPred + λ · LMin

### Mechanism 2
- Regularization coefficient λ controls trade-off between clean-data performance and robustness to non-causal perturbations
- Increasing λ strengthens compression pressure, filtering noise while preserving true signal
- Core assumption: Dataset contains spurious or non-predictive patterns that can be selectively compressed
- Evidence: Section 4.2.1 shows U-shaped performance curve with optimal λ varying by dataset (Weather: 0.5, ETTh1: 2.0, ETTm2: 100.0)

### Mechanism 3
- MPS principle is architecture-agnostic and can regularize non-SSM models by attaching an auxiliary decoder to an intermediate representation
- An auxiliary decoder is attached to a chosen hidden layer of the host model, forcing representation toward minimal sufficiency
- Core assumption: Chosen hidden layer is an appropriate bottleneck for the representation
- Evidence: Section 4.4.1 describes MPS-PatchTST, MPS-DLinear, MPS-Mamba variants

## Foundational Learning

- Concept: Minimal Sufficient Statistic
  - Why needed: Central to the Principle of Predictive Sufficiency; understanding that the goal is maximal compression under zero predictive loss distinguishes MPS from classical IB
  - Quick check: Given two representations with identical predictive performance, how do you determine which is "more minimal"?

- Concept: Variational Mutual Information Bounds
  - Why needed: Direct I(U; h) is intractable; the method relies on an auxiliary decoder to estimate an upper bound
  - Quick check: Why does optimizing a reconstruction loss with a variational decoder provide an upper bound on mutual information?

- Concept: State Space Model Discretization
  - Why needed: MPS-SSM builds on continuous-to-discrete SSM mapping; understanding input-dependent parameters is prerequisite to modifying selection gate
  - Quick check: In a discrete-time SSM, what is the role of step size ∆, and how does making it input-dependent change behavior?

## Architecture Onboarding

- Component map: Input uk → Selection Gate → SSM state update → hk → (branch 1) Prediction Head → LPred; (branch 2) Auxiliary Decoder → LMin
- Critical path: Input uk → Selection Gate Network Gϕ(uk) → SSM state update → hk → (branch 1) Prediction Head → loss LPred; (branch 2) Auxiliary Decoder → reconstruction loss → LMin. Both losses combined with λ weighting.
- Design tradeoffs: Auxiliary decoder capacity vs. tightness of MI bound; λ tuning strategy is dataset-dependent; where to attach regularizer affects effectiveness
- Failure signatures: Posterior collapse (decoder ignores hk), over-regularization (large λ degrades clean accuracy), under-regularization (small λ provides no robustness gain)
- First 3 experiments: 1) Replicate λ sweep on ETTh1 to confirm U-shaped MSE curve, 2) Inject impulse noise to verify robustness improves with λ, 3) Attach minimality module to PatchTST on ETTh1 to test extensibility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the regularization strength λ be adaptively learned during training to automatically adjust to dataset-specific noise levels and prediction horizons?
- Basis: Authors identify "Adaptive Regularization" as future work, noting optimal λ is highly context-dependent (0.5 to 100) and grows with prediction difficulty
- Why unresolved: Currently λ is static hyperparameter tuned via grid search on validation sets
- What evidence would resolve it: Dynamic scheduling mechanism or meta-learning framework that adjusts λ in real-time

### Open Question 2
- Question: Can hybrid architectures combining MPS-SSM with local feature extractors (e.g., patching) outperform models like TimesNet on datasets with strong spatial dependencies?
- Basis: Authors note model underperforms on short-horizon Traffic tasks compared to patch-based methods, proposing "Hybrid Modeling" as future direction
- Why unresolved: MPS principle prioritizes global robustness which may discard localized spatio-temporal patterns
- What evidence would resolve it: Hybrid model (e.g., MPS-PatchTST) targeting local inductive biases while retaining global theoretical guarantees

### Open Question 3
- Question: Do advanced mutual information estimators provide tighter bounds that improve theoretical approximation of minimality condition?
- Basis: Authors suggest exploring "Advanced Information Estimators" because current variational approximation serves only as upper bound
- Why unresolved: Current implementation relies on lightweight auxiliary decoder which might provide loose bound
- What evidence would resolve it: Experiments using contrastive or kernel-based estimators showing tighter bound leads to better performance

## Limitations
- Variational bound on mutual information used for minimality term is not proven to be tight and depends heavily on auxiliary decoder capacity
- λ tuning is dataset-dependent and requires separate validation per dataset, limiting practical deployment
- Extension to non-SSM models is shown on only one task and one variant, making generalization claims tentative

## Confidence
- Principle of Predictive Sufficiency is novel and useful: Medium
- MPS-SSM achieves state-of-the-art performance: High
- MPS principle improves robustness to noise: High
- MPS principle generalizes as a regularization framework: Low

## Next Checks
1. Validate the variational bound tightness by systematically varying auxiliary decoder capacity and measuring correlation between reconstruction loss and downstream performance
2. Test robustness beyond impulse noise by injecting structured noise (sinusoidal interference, missing values) and evaluating whether MPS-SSM maintains superiority
3. Test cross-dataset λ transfer by training on one dataset with optimal λ, then evaluating on a different dataset with fixed λ to measure performance drop