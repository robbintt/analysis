---
ver: rpa2
title: Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative
  Matrix Factorization
arxiv_id: '2506.10920'
source_url: https://arxiv.org/abs/2506.10920
tags:
- features
- concept
- snmf
- concepts
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of identifying interpretable
  features in large language models by decomposing MLP activations. The core method,
  semi-nonnegative matrix factorization (SNMF), factorizes MLP activations into sparse
  linear combinations of neurons and maps these to their activating inputs, providing
  intrinsic interpretability.
---

# Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization

## Quick Facts
- arXiv ID: 2506.10920
- Source URL: https://arxiv.org/abs/2506.10920
- Reference count: 40
- This paper introduces semi-nonnegative matrix factorization (SNMF) to decompose MLP activations into sparse linear combinations of neurons, outperforming sparse autoencoders in causal steering while maintaining strong concept detection performance.

## Executive Summary
This paper addresses the challenge of identifying interpretable features in large language models by decomposing MLP activations. The core method, semi-nonnegative matrix factorization (SNMF), factorizes MLP activations into sparse linear combinations of neurons and maps these to their activating inputs, providing intrinsic interpretability. Experiments on Llama 3.1, Gemma 2, and GPT-2 demonstrate that SNMF-derived features outperform both sparse autoencoders and supervised baselines in causal steering tasks while maintaining comparable performance in concept detection. Further analysis reveals a hierarchical structure where neuron combinations are reused across semantically-related features, with core neuron sets encoding general concepts and exclusive neurons refining them into specific instances.

## Method Summary
The paper introduces semi-nonnegative matrix factorization (SNMF) to decompose MLP activation matrices into interpretable features. SNMF factorizes MLP activations A into Z (features as sparse neuron combinations) and Y (nonnegative coefficients) via alternating multiplicative updates. Each feature z_i ∈ R^{d_a} specifies weights over neurons, which after projection through W_V becomes a residual stream direction f_i = W_V z_i. The method applies winner-take-all sparsification to keep top p% entries per feature (p=1% for Llama/Gemma, p=5% for GPT-2). Concept detection uses log ratio of cosine similarities to activating vs neutral inputs, while causal steering amplifies features during inference with KL-target grid search. Recursive SNMF with decreasing k reveals hierarchical feature structures.

## Key Results
- SNMF-derived features achieve higher causal steering scores (harmonic mean of concept expression and fluency) than both sparse autoencoders and supervised baselines
- Concept detection performance remains comparable to SAEs, with SNMF features detecting concepts more consistently in early transformer layers
- Hierarchical analysis reveals semantically-related features share neuron structures, with core neurons encoding general concepts and exclusive neurons encoding specific instances

## Why This Works (Mechanism)

### Mechanism 1: Parts-Based Decomposition via Nonnegativity Constraints
Constraining the coefficient matrix Y to nonnegative values yields interpretable, additive feature representations. SNMF factorizes MLP activations A into Z (features) and Y (coefficients), where Y ≥ 0 ensures each input is reconstructed as an additive combination of features. This parts-based structure directly attributes which tokens activate which features. The core assumption is that MLP activations encode concepts additively, and nonnegativity preserves this structure.

### Mechanism 2: Sparse Linear Combination of Existing Neurons
Representing features as sparse combinations of existing neurons (rather than arbitrary directions) preserves grounding in model computation. Each feature z_i ∈ R^{d_a} specifies weights over neurons. After projection through W_V, it becomes a residual stream direction f_i = W_V z_i. Sparsity (winner-take-all on top p% entries) forces features to rely on small neuron groups. The core assumption is that co-activated neuron groups encode meaningful concepts.

### Mechanism 3: Hierarchical Feature Composition via Neuron Reuse
Semantically-related features share neuron structures, enabling recursive decomposition into concept hierarchies. Recursive SNMF with decreasing k reveals fine-grained concepts merging into higher-level ones. Shared "core" neurons encode general concepts (e.g., "weekday"); exclusive neurons encode specifics (e.g., "Monday"). The core assumption is that the model represents concepts hierarchically through compositional neuron reuse.

## Foundational Learning

- Concept: Semi-Nonnegative Matrix Factorization (SNMF)
  - Why needed here: Core algorithm; differs from standard NMF by allowing negative entries in one matrix while constraining the other to nonnegative.
  - Quick check question: Given A ∈ R^{m×n}, how does SNMF differ from NMF in what matrices are constrained?

- Concept: MLP Layer as Key-Value Memory
  - Why needed here: The paper views MLP outputs as weighted combinations of columns from W_V, where neuron activations determine weights. Understanding this decomposition is essential.
  - Quick check question: In equation MLP(h) = W_V σ(W_K h), what role do the columns of W_V play?

- Concept: Sparse Autoencoders (SAEs) as Baseline
  - Why needed here: SAEs are the primary comparison method. They learn dictionary directions from scratch (usually on residual stream), unlike SNMF which composes existing neurons.
  - Quick check question: Why might SAEs struggle in causal evaluations compared to neuron-composed features?

## Architecture Onboarding

- Component map:
  - Input: MLP activation matrix A ∈ R^{d_a × n} from n tokens across d_a neurons
  - SNMF decomposition: A ≈ ZY where Z ∈ R^{d_a × k} (k features as sparse neuron weights) and Y ∈ R^{k × n}_{≥0} (nonnegative coefficients)
  - Feature projection: f_i = W_V z_i maps MLP features to residual stream directions
  - Sparsity: Winner-take-all keeps top p% entries per z_i (p=1% in experiments)
  - Recursive layer: Iteratively decompose Z to reveal hierarchies

- Critical path:
  1. Collect MLP activations from forward pass on diverse text
  2. Initialize Z ~ Uniform(0,1), Y ~ N(0,1)
  3. Alternate updates: closed-form Z update, multiplicative Y update
  4. Apply WTA sparsification to Z columns
  5. Extract top-activating contexts via Y for labeling

- Design tradeoffs:
  - k (feature count): Higher k improves concept detection but may over-fragment; paper tested k ∈ [100, 400]
  - Sparsity p: Higher sparsity (lower p) increases interpretability but risks dead features; p=1% for Llama/Gemma, p=5% for GPT-2
  - Layer selection: Early layers show higher detection scores (less entanglement); middle layers balance detection and steering

- Failure signatures:
  - Negative concept detection scores: Feature activates more on neutral than concept-related inputs (may indicate noise sensitivity)
  - Dead features: Over-aggressive sparsity leads to zero activations
  - Poor steering: Feature direction doesn't causally influence output (check if projection through W_V is correct)

- First 3 experiments:
  1. Reproduce concept detection on GPT-2 Small with k=100, verifying >75% positive S_CD scores
  2. Compare steering performance: SNMF vs. SAE trained on same MLP activation dataset (expect SNMF to outperform on harmonic mean of concept + fluency)
  3. Run recursive SNMF on a constrained domain (e.g., time units dataset) and verify hierarchical merging (weekdays → weekday → day)

## Open Questions the Paper Calls Out

- **Question**: Does SNMF scale effectively to thousands or tens of thousands of features while maintaining interpretability and causal steering performance?
  - **Basis**: The Limitations section states: "Analyzing the granularity of features that may emerge at larger values of k and whether the method scales effectively to thousands of MLP features are interesting directions for future work."
  - **Why unresolved**: All experiments used k < 500 features; scaling behavior with larger decompositions remains untested.
  - **What evidence would resolve it**: Benchmark SNMF with k ∈ {1000, 5000, 10000} on concept detection and steering tasks, comparing against large SAEs (e.g., 131k features).

- **Question**: Would alternative initialization strategies (e.g., K-means, SVD-based) and optimization methods (e.g., projected gradient descent) improve SNMF's concept detection and steering performance?
  - **Basis**: The Limitations section notes the authors "did not perform a rigorous comparison of optimization methods or initialization strategies" and suggests K-means or SVD-based initializations "may provide improved concept detection and steering."
  - **Why unresolved**: Only uniform/normal sampling initialization and Multiplicative Updates were tested; SNMF is non-convex so different initializations may converge to better local minima.
  - **What evidence would resolve it**: Systematic comparison of initialization strategies and optimization methods on identical benchmarks, measuring reconstruction error and downstream task performance.

## Limitations

- The SAE baselines are trained on MLP activation dataset rather than standard residual stream, creating apples-to-oranges comparison
- Limited model diversity with experiments only on three model families (GPT-2, Llama 3.1, Gemma 2)
- Causal steering effectiveness shows only moderate absolute performance (concept expression scores around 1.0 on 0-2 scale)

## Confidence

- **High confidence**: The mathematical framework of SNMF and its distinction from standard SAEs is well-established. The algorithm description is clear and reproducible.
- **Medium confidence**: The experimental results showing SNMF's advantage in causal steering tasks appear robust within tested conditions, but limited model diversity reduces confidence in broader claims.
- **Low confidence**: The hierarchical feature composition findings rely heavily on internal analysis of neuron sharing patterns with weak external validation.

## Next Checks

1. **SAE fair comparison**: Retrain SAEs on residual stream activations (their standard domain) and compare against SNMF on both concept detection and steering tasks to isolate whether SNMF's advantages stem from the decomposition approach or training on more suitable data.

2. **Hyperparameter robustness**: Systematically vary k ∈ [50, 100, 200, 400] and sparsity thresholds p ∈ [1%, 5%, 10%] across multiple random seeds. Report variance in concept detection and steering performance to establish whether results are stable or hyperparameter-dependent.

3. **Cross-model validation**: Apply SNMF to at least two additional model families (e.g., BERT variants and Vision Transformers) and test whether the hierarchical neuron composition pattern persists to validate whether the observed hierarchical structure is a general feature of transformer representations.