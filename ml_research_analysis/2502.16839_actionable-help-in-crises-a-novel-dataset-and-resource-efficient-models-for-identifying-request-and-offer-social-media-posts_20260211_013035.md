---
ver: rpa2
title: '"Actionable Help" in Crises: A Novel Dataset and Resource-Efficient Models
  for Identifying Request and Offer Social Media Posts'
arxiv_id: '2502.16839'
source_url: https://arxiv.org/abs/2502.16839
tags:
- tweets
- crisis
- bert
- mini
- requests
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces CrisisHelpOffer, a dataset of 101k tweets
  labelled for identifying actionable requests and offers during crises, created using
  generative LLMs and validated by humans. It also presents the first crisis-specific
  mini models optimized for resource-constrained environments, achieving competitive
  performance against state-of-the-art transformer models like BERT, RoBERTa, MPNet,
  and BERTweet.
---

# "Actionable Help" in Crises: A Novel Dataset and Resource-Efficient Models for Identifying Request and Offer Social Media Posts

## Quick Facts
- arXiv ID: 2502.16839
- Source URL: https://arxiv.org/abs/2502.16839
- Authors: Rabindra Lamsal; Maria Rodriguez Read; Shanika Karunasekera; Muhammad Imran
- Reference count: 14
- One-line primary result: Introduced CrisisHelpOffer dataset (101k tweets) and crisis-specific mini models achieving 83% size reduction and 18.6× speed while matching BERT accuracy

## Executive Summary
This paper addresses the critical need for identifying actionable help requests and offers in social media during crises. The authors introduce CrisisHelpOffer, a novel dataset of 101k tweets collaboratively labeled by four generative LLMs with human validation, specifically designed for crisis informatics. They also present the first crisis-specific miniature transformer models optimized for resource-constrained environments, demonstrating significant improvements in efficiency while maintaining competitive accuracy against state-of-the-art models like BERT, RoBERTa, and MPNet.

The research establishes new benchmarks for crisis informatics by combining high-quality labeled data with efficient model architectures. A case study on COVID-19 tweets reveals disparities in help-seeking and offering behaviors between developing and developed countries. The dataset and models are particularly valuable for humanitarian organizations operating in resource-limited settings where computational efficiency is crucial for real-time crisis response.

## Method Summary
The authors employed a multi-stage approach to create CrisisHelpOffer and develop resource-efficient models. First, they collected ~440 million crisis-related tweets and used four generative LLMs (Gemma 2 9B, Llama 3.1 8B, Ministral 8B, Mistral-Nemo 12B) to label tweets, retaining only those with unanimous agreement across all four models. A stratified sample of this LLM-labeled data was then validated by humans, achieving Cohen's Kappa > 0.92. For model development, they used knowledge distillation from CrisisTransformers CT-M1-Complete (135M parameters) to create three student architectures: Medium (58M), Small (35M), and Tiny (19M), with systematic parameter reduction. The students were trained using MSE loss for embedding approximation and KL divergence plus cross-entropy for task-specific learning, then fine-tuned on CrisisHelpOffer and 12 other crisis datasets.

## Key Results
- CrisisHelpOffer dataset contains 101k tweets with 15.7k requests, 5k offers, 38 request+offers, and 80.5k irrelevant posts
- Tiny model achieves 83% size reduction and 18.6× faster inference while matching BERT's accuracy on crisis tasks
- Medium model is 47% smaller with 3.8% higher accuracy at 3.5× speed compared to BERT
- Case study shows developing countries have higher proportions of requests and offers compared to developed countries during COVID-19

## Why This Works (Mechanism)

### Mechanism 1
Multi-LLM consensus labeling with human validation produces high-quality crisis datasets at scale. Four generative LLMs independently classify tweets, and only unanimous agreements are retained. A stratified sample is then human-validated using Cohen's Kappa to verify reliability. Core assumption: Agreement across diverse LLMs correlates with label correctness, reducing noise from individual model errors. Evidence: Cohen's Kappa was 0.934 for LLMs vs. Human 1 and 0.924 for LLMs vs. Human 2, indicating almost perfect agreement. Break condition: If kappa falls below 0.8, LLM labels become unreliable substitutes for human annotation.

### Mechanism 2
Crisis-domain knowledge distillation enables mini models to match base model accuracy at fraction of size. Student models learn to approximate either the embedding space of a pre-trained crisis teacher or the output distribution of a fine-tuned teacher via MSE and KL divergence losses. Core assumption: Crisis-specific pre-training captures compressible domain patterns that survive architecture reduction. Evidence: SG_M outperformed the baseline on 11 datasets, SG_S on 10 datasets, and SG_T on 5 datasets. Break condition: If student accuracy degrades >5% on out-of-distribution crisis types, the compression sacrifices generalization.

### Mechanism 3
Proportional reduction in hidden size, layers, and attention heads yields predictable speed-accuracy trade-offs. Three architectures scale down uniformly (Medium: 512H/8L/8A/2048I → Tiny: 256H/4L/4A/1024I), reducing FLOPs while preserving representational geometry through distillation. Core assumption: Crisis text classification requires less capacity than general NLP due to narrower vocabulary and intent patterns. Evidence: Medium model is 47% smaller with 3.8% higher accuracy at 3.5× speed. Break condition: If Tiny model underperforms baseline by >3% on any task, the speed gains may not justify deployment.

## Foundational Learning

- Concept: Knowledge Distillation (teacher-student training)
  - Why needed here: Core technique for model compression; understanding soft vs. hard labels, MSE vs. KL divergence is essential
  - Quick check question: Why does the paper use MSE for embedding-space distillation but KL divergence for task-specific distillation?

- Concept: Encoder-only Transformer architecture parameters (H/L/A/I)
  - Why needed here: Interpreting mini model designs requires understanding how hidden size, layers, attention heads, and intermediate size affect capacity and speed
  - Quick check question: Which parameter (H, L, A, or I) has the largest impact on inference latency for batched inputs?

- Concept: Crisis informatics taxonomy (actionable vs. non-actionable content)
  - Why needed here: The dataset distinguishes requests/offers from emotional support, behavioral guidance, and outdated info
  - Quick check question: Would "Pray for the victims" be classified as Request, Offer, or Irrelevant under CrisisHelpOffer?

## Architecture Onboarding

- Component map: ~440M tweets → preprocessing → teacher embedding (CT-M1-Complete) → student training (SG_M/SG_S/SG_T) → fine-tuning on CrisisHelpOffer

- Critical path: Load CT-M1-Complete from crisistransformers → generate teacher embeddings with mean-pooling → initialize student with random weights → train student via MSE loss against downsampled teacher embeddings → fine-tune downstream with class weights and early stopping

- Design tradeoffs: Mean-pooling vs. CLS token (paper shows mean-pooling yields lower loss); Generic vs. task-specific (SG_i models are reusable; ST_i models are single-task only); Speed vs. robustness (Tiny is fastest but underperforms by up to 7.67% on some datasets; Medium offers best balance)

- Failure signatures: Distillation collapse (loss plateaus without convergence—check lr scaling with batch size); Class imbalance failure ("Irrelevant" is 80.5k/101k; missing class weights causes overfitting to majority); OOD generalization (fine-tuned model fails on conflict data when trained only on natural disasters)

- First 3 experiments: Reproduce CrisisHelpOffer fine-tuning (load SG_M, fine-tune with stratified 70/10/20 splits, class weights, patience=5; target F1 > 0.78); Benchmark inference (compare SG_T vs. BERT on A100, batch=32, 1000 iterations; verify >15× throughput); Test OOD generalization (fine-tune SG_M on CrisisLex or CrisisNLP; compare macro F1 vs. BERT baseline)

## Open Questions the Paper Calls Out

- Can the CrisisHelpOffer dataset and models be effectively adapted to multi-lingual texts and platforms other than X (Twitter) to ensure global relevance?
  - Basis: Conclusion states "Future research could expand CrisisHelpOffer by incorporating multi-lingual tweets and data from diverse platforms, improving its global relevance."
  - Why unresolved: Current study focuses exclusively on English tweets from Twitter/X, limiting applicability to non-English speaking regions or alternative social media ecosystems
  - What evidence would resolve it: Performance benchmarks of the mini models on crisis datasets from platforms like Facebook or Weibo in languages such as Spanish, Hindi, or Mandarin

- Does integrating the volumetric patterns of help requests identified by these models into early warning systems demonstrably improve situational awareness for humanitarian organizations?
  - Basis: Conclusion suggests "integrating volumetric patterns of help requests into early warning systems could improve situational awareness for humanitarian organizations"
  - Why unresolved: Paper analyzes spatiotemporal trends as case study but does not implement live early warning system to validate if data actually improves decision-making
  - What evidence would resolve it: User study or field deployment with humanitarian teams showing alerts generated by model lead to faster/more accurate resource allocation decisions

- Do the proposed mini models maintain their efficiency advantages when deployed on actual edge devices or low-power hardware compared to high-performance GPUs used for benchmarking?
  - Basis: Abstract claims models are "optimized for resource-constrained environments," but Section 4.2 reports inference times based solely on NVIDIA A100 GPUs
  - Why unresolved: While theoretical throughput is measured, actual latency, memory footprint, and power consumption on target constrained devices remain unknown
  - What evidence would resolve it: Benchmarks of model inference latency and energy consumption on standard edge hardware (Raspberry Pi, NVIDIA Jetson, or smartphones)

## Limitations
- Dataset accessibility: CrisisHelpOffer dataset is not publicly available and requires formal request for access, creating reproducibility barrier
- Limited downstream task evaluation: Paper evaluates mini models on 12 crisis-specific datasets, but selection appears somewhat arbitrary without testing on more diverse crisis types
- Architecture scaling assumptions: Assumes uniform proportional scaling is optimal, but alternative scaling strategies could potentially yield better efficiency-accuracy tradeoffs

## Confidence

- High confidence: Claims about distillation mechanism working for crisis-specific tasks (evidenced by systematic improvements across 11/12 datasets); architectural specifications and training procedures are clearly described
- Medium confidence: Claims about Tiny model achieving 83% size reduction while matching BERT's accuracy (depends heavily on dataset quality and specific crisis domains tested); COVID-19 case study findings are suggestive but based on single crisis type
- Low confidence: Claims about dataset being "first" crisis-specific mini models and setting new benchmarks (cannot be fully validated without independent access to dataset and broader evaluation across diverse crisis scenarios)

## Next Checks

1. **Dataset accessibility verification**: Attempt to obtain CrisisHelpOffer dataset through formal request process and conduct independent stratified sampling analysis to verify class distribution claims (15.7k Request, 5k Offer, 38 Request+Offer, 80.5k Irrelevant) and assess label quality through random sampling

2. **Cross-crisis generalization test**: Fine-tune SG_M model on at least two crisis datasets not used in original paper (e.g., CrisisLexT6 for natural disasters and CrisisNLP for humanitarian crises) and compare macro F1 scores against reported BERT baseline to validate 3.8% accuracy improvement claim across crisis types

3. **Alternative scaling architecture experiment**: Implement variant of Tiny model that maintains 4 attention heads but reduces layers to 3 (instead of paper's 4 layers), keeping other parameters constant. Compare inference throughput and macro F1 on CrisisHelpOffer to determine if paper's uniform scaling assumption is optimal or if layer count reduction offers better efficiency gains