---
ver: rpa2
title: Do Chatbot LLMs Talk Too Much? The YapBench Benchmark
arxiv_id: '2601.00624'
source_url: https://arxiv.org/abs/2601.00624
tags:
- openai
- category
- prompt
- minimal
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The YapBench benchmark quantifies unnecessary verbosity in large
  language models by measuring excess output length beyond a minimal sufficient baseline
  on brevity-ideal prompts. The benchmark evaluates 76 assistant models across three
  categories: minimal/ambiguous inputs, short factual Q&A, and one-line coding tasks.'
---

# Do Chatbot LLMs Talk Too Much? The YapBench Benchmark

## Quick Facts
- **arXiv ID**: 2601.00624
- **Source URL**: https://arxiv.org/abs/2601.00624
- **Reference count**: 40
- **Primary result**: YapBench benchmark reveals order-of-magnitude spread in LLM verbosity, with gpt-3.5-turbo achieving lowest YapIndex (22.7) and glm-4.5 highest (1427)

## Executive Summary
YapBench is a benchmark that quantifies unnecessary verbosity in large language models by measuring excess output length beyond a minimal sufficient baseline on brevity-ideal prompts. The benchmark evaluates 76 assistant models across three categories: minimal/ambiguous inputs, short factual Q&A, and one-line coding tasks. Using the YapIndex metric, which aggregates median excess characters per category, models show an order-of-magnitude spread in verbosity, with gpt-3.5-turbo achieving the lowest YapIndex (22.7) and glm-4.5 the highest (1427). Category-level analysis reveals distinct failure modes including vacuum-filling on ambiguous inputs and explanation overhead on technical requests. A cost-oriented YapTax metric estimates the marginal token-priced overhead of over-generation, ranging from $1.14 to $2.71 per 1,000 prompts across models.

## Method Summary
YapBench evaluates 76 assistant models on 304 single-turn English prompts grouped into three categories (60 minimal/ambiguous, 126 factual Q&A, 118 one-line coding tasks). Each prompt has a curated minimal sufficient baseline answer. Models are queried via API at temperature=0 with no system prompt. YapScore computes excess characters (model length minus baseline length, minimum zero) after stripping markdown formatting. YapIndex is the uniformly weighted average of category-level median YapScores, with 95% confidence intervals from bootstrap resampling (B=1000). YapTax estimates cost overhead using model-specific tokenizers and prices.

## Key Results
- YapIndex ranges from 22.7 (gpt-3.5-turbo) to 1427 (glm-4.5) across 76 models, showing an order-of-magnitude spread in verbosity
- Category A (ambiguous inputs) triggers vacuum-filling behavior with high median excess in several models
- Category C (coding tasks) drives largest median excess for many models due to formatting and explanation overhead
- YapTax ranges from $1.14 to $2.71 per 1,000 prompts, indicating substantial cost impact of verbosity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preference-based post-training induces systematic length bias, causing models to generate longer outputs even when brevity is sufficient.
- Mechanism: RLHF reward models and LLM-as-judge evaluations learn a shortcut where longer responses receive higher scores independent of helpfulness, creating a training signal that rewards verbosity as a proxy for quality.
- Core assumption: Length bias persists at inference time even when user intent clearly favors brevity.
- Evidence anchors:
  - [abstract] "Prior work suggests that preference-based post-training and LLM-judged evaluations can induce systematic length bias, where longer answers are rewarded even at comparable quality."
  - [section 2] "Shen et al. (2023) investigate length bias in RLHF reward models, showing that vanilla reward models can learn a shortcut where longer responses receive systematically higher scores."
  - [corpus] Weak direct corpus support; related papers focus on chatbot behavior and evaluation but not specifically on length bias mechanisms.
- Break condition: If reward models explicitly penalize length or length-controlled win rates are used during training, the mechanism weakens.

### Mechanism 2
- Claim: Models over-generate on ambiguous or low-information inputs as a learned "helpfulness prior" rather than requesting clarification.
- Mechanism: Category A prompts (nonsense, minimal inputs) trigger extended proactive content because assistant-style training emphasizes substantive responses over minimal acknowledgments, causing "vacuum-filling" behavior.
- Core assumption: This behavior stems from training data distribution rather than an intrinsic property of the architecture.
- Evidence anchors:
  - [abstract] "Category-level analysis reveals distinct failure modes including vacuum-filling on ambiguous inputs."
  - [section 5.3] "Several models generate large median overhead on low-information prompts, consistent with a tendency to 'fill the vacuum' with unsolicited content rather than issuing a minimal clarification."
  - [corpus] No direct corpus evidence on vacuum-filling; related work focuses on therapeutic and educational chatbots with different behavioral goals.
- Break condition: If system prompts explicitly instruct minimal responses on unclear inputs, or if training includes negative examples for over-generation on ambiguous prompts.

### Mechanism 3
- Claim: Technical requests expose formatting and pedagogical defaults that add overhead without improving task completion.
- Mechanism: One-line coding tasks (Category C) trigger explanatory prose, markdown formatting, and code block wrappers because assistant training emphasizes teaching and thoroughness over atomic task completion.
- Core assumption: This overhead is a controllable output policy, not an inherent limitation.
- Evidence anchors:
  - [abstract] "Explanation overhead on technical requests" identified as a distinct failure mode.
  - [section 5.3] "Category C frequently drives the largest median excess for many models, reflecting formatting and explanation overhead around atomic commands or snippets."
  - [corpus] No direct corpus support for this specific mechanism.
- Break condition: If models are fine-tuned with brevity-constrained code examples or inference-time length constraints are applied.

## Foundational Learning

- Concept: Minimal sufficient response
  - Why needed here: The entire benchmark depends on defining what "short enough" means; understanding that baselines represent the minimum tokens for correctness/clarity is essential for interpreting YapScores.
  - Quick check question: Given "What is the freezing point of water in Celsius?", is "0" or "0°C at standard atmospheric pressure" the minimal sufficient baseline?

- Concept: Length bias in preference learning
  - Why needed here: Explains why modern models often perform worse than older ones (gpt-3.5-turbo leads) on brevity metrics despite improved capability.
  - Quick check question: Why might a reward model trained on human preferences systematically favor longer responses even when they add no information?

- Concept: Bootstrap confidence intervals for heavy-tailed distributions
  - Why needed here: YapBench uses medians and bootstrap resampling to report uncertainty; interpreting the ± intervals requires understanding percentile-based CI construction.
  - Quick check question: Why use median instead of mean for aggregating YapScores across prompts?

## Architecture Onboarding

- Component map: Prompts -> Baselines -> Model API calls -> YapScore computation -> Category medians -> YapIndex aggregation -> Bootstrap confidence intervals
- Critical path:
  1. Load prompts and baselines from HuggingFace dataset
  2. Query model via API (temperature=0, no system prompt, single-turn)
  3. Strip markdown formatting from output
  4. Compute YapScore per prompt
  5. Aggregate category medians → YapIndex
  6. Bootstrap resample (B=1000) for 95% CI
- Design tradeoffs:
  - Characters vs. tokens: YapScore uses characters for tokenizer-agnostic comparison; YapTax requires model-specific tokenizers
  - Median vs. mean: Median reduces sensitivity to extreme verbosity bursts but may underrepresent worst-case user experience
  - Single-turn only: No multi-turn dialogue evaluation; cannot measure verbosity accumulated across conversation
- Failure signatures:
  - Vacuum-filling: High Category A YapScore indicates model over-interprets unclear inputs
  - Explanation overhead: High Category C YapScore indicates pedagogical defaults on atomic tasks
  - Refusal bloat: Future category; models adding boilerplate to simple refusals
- First 3 experiments:
  1. Establish baseline: Run YapBench on your target model, compute category-level YapScores to identify which failure mode dominates.
  2. Ablate system prompt: Test with/without explicit brevity instructions (e.g., "Respond as concisely as possible while remaining correct") to quantify instruction-following vs. learned priors.
  3. Cross-category correlation: Check if models strong on Category B (factual) also perform well on Category C (code); low correlation suggests category-specific training gaps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can targeted post-training objectives or inference-time constraints reliably reduce over-generation on brevity-ideal prompts without compromising task quality?
- Basis in paper: [explicit] The authors state in Section 6 (Discussion) that formatting and explanation overhead in Category C "is a controllable output policy that can likely be reduced through targeted post-training objectives or inference-time constraints."
- Why unresolved: The paper measures verbosity but does not propose or evaluate mitigation strategies.
- What evidence would resolve it: A training intervention study showing reduced YapIndex with maintained performance on standard capability benchmarks.

### Open Question 2
- Question: What is the causal relationship between preference learning pipelines (RLHF, DPO) and verbosity behavior across models?
- Basis in paper: [explicit] The paper notes that unnecessary verbosity "appears to be a behavioral property that is influenced by post-training and assistant-style response priors" and cites prior work on length bias in RLHF reward models.
- Why unresolved: YapBench correlates verbosity with model families but does not isolate specific training stages as causal factors.
- What evidence would resolve it: Controlled experiments comparing models at different training stages or with controlled preference data variations.

### Open Question 3
- Question: How do reasoning modes affect worst-case "verbosity bursts" and tail statistics beyond median YapIndex?
- Basis in paper: [explicit] The authors observe that "Reasoning modes do not uniformly increase typical verbosity, but they plausibly increase worst-case over-generation" and note they complement medians with tail statistics.
- Why unresolved: The paper reports medians and percentiles but does not systematically analyze reasoning mode effects on extreme verbosity.
- What evidence would resolve it: Full distribution analysis comparing reasoning vs. non-reasoning variants across percentiles (P90, P95, P99).

### Open Question 4
- Question: How stable are minimal sufficient baselines under different annotators and cultural contexts?
- Basis in paper: [explicit] In Limitations: "Baseline selection is inherently a source of variance: there is no universal 'gold' standard for response length."
- Why unresolved: The paper uses internal review but does not quantify inter-annotator agreement or cross-cultural baseline variation.
- What evidence would resolve it: Multi-annotator baseline annotation with agreement statistics and cross-demographic baseline comparison.

## Limitations
- The benchmark only evaluates single-turn interactions, missing verbosity accumulation in multi-turn dialogue
- Results are limited to English prompts, restricting generalizability to multilingual contexts
- The exact markdown stripping implementation remains unspecified, creating potential reproducibility gaps
- The benchmark cannot assess when verbosity is genuinely helpful (e.g., complex explanations)

## Confidence
- **High Confidence**: The existence of systematic verbosity differences between models, as evidenced by the order-of-magnitude spread in YapIndex values
- **Medium Confidence**: The attribution of verbosity to specific failure modes (vacuum-filling, explanation overhead) based on category-level analysis
- **Medium-Low Confidence**: The proposed mechanism linking RLHF training to length bias as a direct causal factor

## Next Checks
1. **Ablation of system prompt effects**: Run YapBench with varying system prompt instructions (minimal vs. verbose vs. neutral) on a subset of models to quantify how much observed verbosity stems from learned priors versus inference-time instruction-following.

2. **Markdown stripping validation**: Implement and test multiple markdown removal strategies (regex-based vs. markdown AST parsing) on a small set of prompts to establish the sensitivity of YapScores to cleaning methodology and determine the most faithful approach.

3. **Cross-lingual extension**: Create a minimal multilingual YapBench subset (5-10 prompts translated into 3-4 languages) to test whether observed verbosity patterns generalize beyond English and identify any language-specific effects in the metric.