---
ver: rpa2
title: 'Dolphin: A Large-Scale Automatic Speech Recognition Model for Eastern Languages'
arxiv_id: '2503.20212'
source_url: https://arxiv.org/abs/2503.20212
tags:
- speech
- dolphin
- languages
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dolphin, a large-scale multilingual automatic
  speech recognition (ASR) model extending Whisper architecture to support 40 Eastern
  languages and 22 Chinese dialects. Dolphin integrates proprietary and open-source
  datasets, using E-Branchformer encoders and a joint CTC-Attention decoder.
---

# Dolphin: A Large-Scale Automatic Speech Recognition Model for Eastern Languages

## Quick Facts
- arXiv ID: 2503.20212
- Source URL: https://arxiv.org/abs/2503.20212
- Reference count: 0
- Primary result: Dolphin base achieves 31.8% WER, outperforming Whisper large-v3 (32.6%) despite being much smaller

## Executive Summary
Dolphin is a multilingual ASR model extending Whisper architecture to support 40 Eastern languages and 22 Chinese dialects. It achieves state-of-the-art performance across model sizes, with Dolphin base outperforming Whisper large-v3 despite being significantly smaller. The model uses E-Branchformer encoders, joint CTC-Attention decoding, and a two-level language-region token system to handle dialectal variation. Trained on over 200,000 hours of audio, Dolphin demonstrates superior performance on multilingual benchmarks while supporting real-time low-latency scenarios.

## Method Summary
Dolphin employs an E-Branchformer encoder with 4× subsampling, joint CTC-Attention decoding (CTC weight 0.3), and a hierarchical language-region token system. The model uses 40K BPE vocabulary across 40+ languages, with audio features as 80-channel log Mel filterbank. Training involves duration-balanced audio concatenation (25-30s segments) and data sharding to handle massive datasets. The architecture scales from 372M (base) to 1679M (large) parameters, trained with AdamW optimizer and SpecAugment regularization.

## Key Results
- Dolphin base (372M) achieves 31.8% WER, beating Whisper large-v3 (32.6%)
- Dolphin small (487M) achieves 24.0% WER vs Whisper small's 32.6%
- Dolphin medium (946M) achieves 22.0% WER vs Whisper medium's 26.4%
- Dolphin large (1679M) achieves 20.6% WER vs Whisper large's 22.3%

## Why This Works (Mechanism)

### Mechanism 1: Joint CTC-Attention Decoding
The joint CTC-Attention decoder improves training efficiency and robustness compared to attention-only decoders. CTC provides monotonic alignment prior that regularizes attention mechanism, reducing alignment errors during training and enabling faster convergence. The CTC loss weight of 0.3 acts as regularizer while attention decoder handles sequence modeling.

### Mechanism 2: Hierarchical Language-Region Token System
The two-level language token system (language + region) improves dialect and accent discrimination compared to single-token systems. By separating language identity from regional variation, the model learns shared cross-lingual representations within language families while preserving dialect-specific phonetic patterns, allowing explicit accent recognition.

### Mechanism 3: Duration-Balanced Audio Concatenation
Concatenating short audio clips into 25-30 second segments reduces deletion errors and improves overall WER. Short audio dominance biases models toward over-segmentation. Merging clips into longer sequences teaches model to handle natural speech continuity and reduces spurious breaks, with duration bucketing ensuring balanced training across clip lengths.

## Foundational Learning

- **E-Branchformer Encoder Architecture**: Why needed: Dolphin replaces Whisper's Transformer encoder with E-Branchformer using parallel branches for local and global dependencies. Essential for debugging encoder issues. Quick check: How do E-Branchformer's parallel branches differ from standard Transformer self-attention in capturing local acoustic features vs. long-range linguistic context?

- **Byte-Pair Encoding (BPE) for Multilingual ASR**: Why needed: Dolphin uses 40K BPE vocabulary across 40+ languages with diverse scripts. Vocabulary design affects OOV rates and subword segmentation quality. Quick check: How would 40K BPE vocabulary allocate tokens differently between logographic Chinese vs. phonemic scripts like Thai or Korean?

- **SpecAugment for Robust ASR**: Why needed: Dolphin applies SpecAugment with global normalization. Understanding time/frequency masking helps diagnose overfitting vs. underfitting on noisy vs. clean speech. Quick check: For tonal languages like Thai or Vietnamese, which SpecAugment parameters should be adjusted to preserve pitch information critical for meaning?

## Architecture Onboarding

- **Component map**: Audio preprocessing → 80-channel log Mel filterbank → 4× subsampling → E-Branchformer encoder → joint CTC-Attention decoding → BPE detokenization → text output

- **Critical path**: 1) Audio preprocessing and feature extraction with 4× subsampling, 2) E-Branchformer encoding with parallel local/global branches, 3) CTC + Attention decoding with hierarchical language/region conditioning, 4) BPE detokenization with optional timestamp prediction

- **Design tradeoffs**: CTC weight 0.3 balances alignment speed vs. attention flexibility; 40K BPE vocabulary reduces sequence length but increases OOV risk; 30-second padding improves long-form handling but wastes compute on short utterances

- **Failure signatures**: High deletion errors on short audio (<5s) may indicate insufficient duration bucketing; poor dialect discrimination suggests region token coverage issues; OOM during data loading requires data sharding strategy; slow training convergence needs logical audio merging validation

- **First 3 experiments**: 1) Ablate CTC weight (0.0, 0.3, 0.5) on held-out dialect test set, 2) Evaluate region token granularity by merging similar dialects and measuring WER delta, 3) Test duration bucketing impact on short-utterance benchmark (<5s clips)

## Open Questions the Paper Calls Out

- Do larger Dolphin models continue to follow observed scaling laws to achieve state-of-the-art results across broader range of languages? (Current validation only up to 1679M parameters)

- What specific training strategies or data curation methods are required to optimize Dolphin for underrepresented and low-resource Eastern languages? (Current dataset has 36/40 languages with >100 hours of data)

- Can Dolphin be optimized for real-time, low-latency inference through compression techniques without significant accuracy degradation? (Current report focuses on offline WER, reports latency only as capability)

## Limitations

- Evaluation relies heavily on authors' test set construction not fully disclosed in terms of language coverage balance
- 212,137-hour training corpus includes 137,712 hours of proprietary Dataocean AI data (65%), making independent validation difficult
- E-Branchformer architecture parameters are referenced but exact configurations not specified, preventing exact replication

## Confidence

- **Dolphin outperforms Whisper on WER across all model sizes**: High confidence (clear relative improvements across multiple benchmarks)
- **Joint CTC-Attention architecture improves training efficiency and robustness**: Medium confidence (claimed but no direct ablation vs attention-only baseline)
- **Two-level language-region token system improves dialect discrimination**: Medium confidence (claimed mechanism but no controlled comparison studies)
- **Duration-bucketed concatenation reduces deletion errors**: Medium confidence (9.01% WER reduction claimed but no separation of long-form vs short-form performance)

## Next Checks

1. **Ablate CTC weight contribution**: Train Dolphin base with CTC weight 0.0, 0.3, and 0.5 to measure WER delta on held-out dialect test set, confirming joint decoding provides measurable benefit

2. **Validate region token granularity**: Merge similar dialects (e.g., combine `<zh-SICHUAN>` + `<zh-SHAANXI>` into `<zh-SW>`) and measure WER delta on affected dialects to quantify whether current 22-region granularity captures sufficient variation

3. **Test duration bucketing on short utterances**: Evaluate on isolated short utterances (<5s clips from CommonVoice) to confirm that 25-30s concatenation strategy does not degrade performance on short-form speech while maintaining deletion error reduction benefits