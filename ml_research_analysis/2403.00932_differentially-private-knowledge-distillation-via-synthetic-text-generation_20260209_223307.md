---
ver: rpa2
title: Differentially Private Knowledge Distillation via Synthetic Text Generation
arxiv_id: '2403.00932'
source_url: https://arxiv.org/abs/2403.00932
tags:
- teacher
- student
- data
- private
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DistilDP, a differentially private knowledge
  distillation framework that compresses large language models (LLMs) while preserving
  strong privacy guarantees. The method fine-tunes a pre-trained teacher LLM with
  differential privacy (DP-SGD), generates synthetic text data from this teacher,
  and then trains a smaller student model on the synthetic data while aligning outputs
  with the teacher's distribution.
---

# Differentially Private Knowledge Distillation via Synthetic Text Generation

## Quick Facts
- arXiv ID: 2403.00932
- Source URL: https://arxiv.org/abs/2403.00932
- Authors: James Flemings; Murali Annavaram
- Reference count: 8
- Primary result: DistilDP reduces perplexity by at least 9.0 on Big Patent dataset while maintaining strong privacy (ε=2)

## Executive Summary
This paper introduces DistilDP, a differentially private knowledge distillation framework that compresses large language models while preserving strong privacy guarantees. The method fine-tunes a pre-trained teacher LLM with differential privacy (DP-SGD), generates synthetic text data from this teacher, and then trains a smaller student model on the synthetic data while aligning outputs with the teacher's distribution. By using DP synthetic data and teacher output alignment, DistilDP avoids the computational cost of running DP-SGD twice as in previous approaches. Experiments show that DistilDP achieves significant performance improvements over existing baselines, reducing perplexity by at least 9.0 on the Big Patent dataset while maintaining strong privacy (ε=2).

## Method Summary
DistilDP compresses a teacher LLM into a smaller student while guaranteeing (ε, δ)-differential privacy with ε=2. The method first fine-tunes a GPT-2 Large teacher model (774M params) with DP-SGD on private text data augmented with control codes (categorical metadata). The teacher then generates synthetic text samples (400K Yelp/DBpedia, 200K Big Patent) using top-k and top-p sampling. Finally, a DistilGPT2 student model (82M params) is trained on the synthetic data using a combined knowledge distillation loss that incorporates both hard labels from synthetic text and soft labels from teacher logits, with optimal λ=0.4 and temperature t=1.0.

## Key Results
- Achieves at least 9.0 perplexity improvement over baselines on Big Patent dataset
- Maintains strong privacy guarantees with ε=2
- Demonstrates that combining synthetic text data with teacher output alignment is crucial for optimal performance
- Scales effectively with synthetic data volume (performance improves from 50K to 400K samples)

## Why This Works (Mechanism)

### Mechanism 1
The student model maintains (ε, δ)-differential privacy guarantees without running DP-SGD twice by leveraging the post-processing property of differential privacy. Once the teacher model is fine-tuned with DP-SGD, any operation on its outputs (synthetic text generation, logit calculation) does not degrade the privacy budget.

### Mechanism 2
Utility is maximized by training the student on a linear combination of hard labels (synthetic text content) and soft labels (teacher's output distribution). The teacher's output distribution contains "dark knowledge" that corrects or smooths the student's learning, acting as a regularizer when synthetic data quality is imperfect.

### Mechanism 3
Control codes prepended to private training data guide the teacher to generate high-utility synthetic data tailored to specific downstream tasks. The model learns a conditional distribution P(text | control_code), and sampling from the known control code distribution ensures the synthetic dataset covers the same topic/class balance as the original private corpus.

## Foundational Learning

- **Differential Privacy (DP-SGD):** Why needed - defines the constraint requiring clipped gradients and noise addition that degrades utility. Quick check - Does running standard SGD on data generated by a DP model preserve the DP guarantee? (Yes, via post-processing)

- **Knowledge Distillation (Soft vs. Hard Labels):** Why needed - the paper's core contribution is a specific distillation loss function. Quick check - Why might a student model learn better from a teacher's soft probabilities than from ground truth labels alone?

- **Autoregressive Text Generation:** Why needed - to implement synthetic data generation. Quick check - What happens to the diversity of synthetic data if generation temperature is set too low?

## Architecture Onboarding

- **Component map:** Private Dataset + Control Codes -> DP-SGD Teacher (GPT-2 Large) -> Synthetic Data Generator -> Student (DistilGPT2) with KD Loss
- **Critical path:** 1) Preprocessing: extract metadata and prepend to private text 2) DP Training: fine-tune teacher with DP-SGD (ε=2) 3) Synthesis: generate ~400k samples using teacher 4) Distillation: train student on synthetic samples with combined loss
- **Design tradeoffs:** λ=0.4 optimal (vs. 0 or 1), t=1.0 optimal, performance scales with synthetic data volume
- **Failure signatures:** Overfitting to synthetic data artifacts, control code leakage, teacher distribution becoming uninformative
- **First 3 experiments:** 1) Baseline validation: DP-SGD teacher vs student trained directly with DP-SGD 2) Lambda sweep: test λ values 0.0 to 1.0 on Yelp dataset 3) Data scaling: generate increasing synthetic dataset sizes (50K, 100K, 200K) and plot student perplexity

## Open Questions the Paper Calls Out

1. **Synthetic data scaling limits:** At what scale of synthetic data generation does the DistilDP framework exhibit diminishing marginal returns on student model utility? The authors note this remains unexplored beyond 400K samples.

2. **Student-only DP-SGD feasibility:** Is it feasible to derive a solution that applies DP-SGD exclusively to the student model while maintaining rigorous privacy guarantees? The authors state this is non-trivial and left for future work.

3. **Advanced distillation techniques:** To what extent can advanced knowledge distillation techniques utilizing hidden representations further bridge the utility gap between teacher and student models? Preliminary MSE loss tests on hidden layers showed only mild improvement.

## Limitations

- Control codes are assumed to be non-sensitive categorical metadata but this critical privacy assumption is not validated through privacy auditing
- Limited scope of experiments (three datasets, one privacy budget ε=2) reduces generalizability across different privacy requirements
- No direct evaluation of synthetic text quality, diversity, or control code fidelity provided to validate generation mechanism effectiveness

## Confidence

- **High Confidence:** Core architectural contribution and DP guarantee via post-processing are clearly specified and theoretically sound
- **Medium Confidence:** Empirical PPL improvements are convincing for tested datasets but limited scope reduces generalizability
- **Low Confidence:** Claims about synthetic data quality and control code effectiveness lack direct validation

## Next Checks

1. Conduct membership inference or attribute inference attacks on control code distribution to verify they cannot reconstruct individual identities
2. Generate synthetic samples with different control code configurations and evaluate topic coherence, text quality, and diversity metrics
3. Repeat full experimental pipeline with ε values ranging from 0.5 to 8.0 to quantify privacy-utility trade-off and verify architecture benefits across different budgets