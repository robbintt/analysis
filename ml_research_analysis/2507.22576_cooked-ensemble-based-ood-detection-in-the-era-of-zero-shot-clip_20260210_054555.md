---
ver: rpa2
title: 'COOkeD: Ensemble-based OOD detection in the era of zero-shot CLIP'
arxiv_id: '2507.22576'
source_url: https://arxiv.org/abs/2507.22576
tags:
- clip
- detection
- classifier
- probe
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the challenge of out-of-distribution (OOD)
  detection in image recognition, where classifiers must identify inputs from unseen
  classes at test-time. The authors propose COOkeD, an ensemble-based approach that
  combines predictions from three models: a standard classifier trained end-to-end,
  a zero-shot CLIP classifier, and a linear probe trained on CLIP features.'
---

# COOkeD: Ensemble-based OOD detection in the era of zero-shot CLIP

## Quick Facts
- arXiv ID: 2507.22576
- Source URL: https://arxiv.org/abs/2507.22576
- Reference count: 40
- Ensemble-based approach combining standard classifier, zero-shot CLIP, and linear probe achieves state-of-the-art OOD detection performance

## Executive Summary
This work introduces COOkeD, an ensemble-based approach for out-of-distribution (OOD) detection that combines predictions from three complementary models: a standard classifier trained end-to-end, a zero-shot CLIP classifier, and a linear probe trained on CLIP features. The method addresses the challenge of detecting inputs from unseen classes at test-time, achieving state-of-the-art performance across diverse OOD scenarios including near-OOD, far-OOD, covariate shift, and zero-shot shift. The ensemble approach demonstrates consistent robustness and accuracy, outperforming both classical and CLIP-based OOD detection methods on established benchmarks.

## Method Summary
COOkeD leverages the complementary strengths of three distinct modeling approaches to improve OOD detection. The ensemble combines: (1) a standard classifier trained end-to-end on the target dataset, (2) a zero-shot CLIP classifier that leverages vision-language pre-training for classification without fine-tuning, and (3) a linear probe trained on CLIP features that captures semantic information from pre-trained representations. By aggregating these diverse predictions, the method captures both task-specific patterns and generalizable semantic knowledge, enabling robust detection of out-of-distribution samples across various shift types and distances.

## Key Results
- Achieves state-of-the-art OOD detection performance on CIFAR100, ImageNet200, and ImageNet1K benchmarks
- Demonstrates consistent superiority across diverse OOD scenarios including near-OOD, far-OOD, covariate shift, and zero-shot shift
- Significantly outperforms both classical OOD detection methods and CLIP-based approaches

## Why This Works (Mechanism)
The ensemble approach works by combining complementary modeling paradigms: task-specific discriminative learning from the standard classifier, zero-shot semantic generalization from CLIP, and linear probe adaptation that bridges these approaches. This multi-faceted strategy captures both narrow task-specific patterns and broader semantic relationships, enabling more robust distinction between in-distribution and out-of-distribution samples across varying degrees of distributional shift.

## Foundational Learning
- **Out-of-distribution detection**: Identifying inputs that fall outside the training distribution - needed to ensure model reliability in real-world deployment where unseen data is inevitable
- **Zero-shot learning**: Classification without task-specific training - required to leverage pre-trained vision-language models for OOD detection
- **Ensemble methods**: Combining multiple model predictions - essential for capturing complementary strengths of different modeling approaches
- **Covariate shift**: Changes in input distribution while maintaining label distribution - must be distinguished from true OOD scenarios
- **CLIP features**: Vision-language embeddings from Contrastive Language-Image Pre-training - provide semantic representations that generalize beyond specific training classes
- **Linear probe training**: Training a classifier on frozen pre-trained features - offers a middle ground between zero-shot and full fine-tuning

## Architecture Onboarding

**Component Map:**
Standard classifier -> Ensemble aggregator -> OOD score
Zero-shot CLIP classifier -> Ensemble aggregator -> OOD score
Linear probe classifier -> Ensemble aggregator -> OOD score

**Critical Path:**
Input image → All three classifiers → Individual confidence scores → Weighted aggregation → Final OOD detection score

**Design Tradeoffs:**
- Model diversity vs. computational overhead: Three separate models increase inference cost but capture complementary information
- Zero-shot vs. fine-tuned approaches: Balancing generalization with task-specific accuracy
- Ensemble weighting: Determining optimal contribution of each model to final decision

**Failure Signatures:**
- Over-reliance on any single model's confidence scores
- Poor calibration when distributional shift is subtle (near-OOD)
- Computational bottleneck at inference time due to multiple model evaluations

**First Experiments:**
1. Ablation study removing each model from ensemble to quantify individual contributions
2. Testing ensemble sensitivity to different weighting schemes
3. Evaluating performance on additional datasets from varied domains

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Ensemble approach's sensitivity to specific model combination remains unexplored
- Performance primarily validated on specific benchmark datasets (CIFAR100, ImageNet200, ImageNet1K)
- Computational overhead of maintaining three separate models not thoroughly analyzed

## Confidence
- **High**: Core claim that ensemble-based OOD detection improves performance on established benchmarks
- **Medium**: Claims about robustness across diverse OOD scenarios within tested constraints
- **Low**: Assertion of real-world superiority given limited domain testing scope

## Next Checks
1. Evaluate ensemble sensitivity to different model combinations and weightings to determine optimal configuration
2. Test performance on additional datasets from varied domains (medical imaging, satellite imagery, etc.) to assess broader generalization
3. Measure and compare computational overhead against single-model approaches to quantify practical deployment costs