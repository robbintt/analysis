---
ver: rpa2
title: 'LONGQAEVAL: Designing Reliable Evaluations of Long-Form Clinical QA under
  Resource Constraints'
arxiv_id: '2510.10415'
source_url: https://arxiv.org/abs/2510.10415
tags:
- fine-grained
- annotation
- coarse
- annotations
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LONGQAEVAL introduces a framework for evaluating long-form clinical
  question answering under resource constraints. It compares coarse answer-level and
  fine-grained sentence-level annotation designs across three dimensions: correctness,
  relevance, and safety.'
---

# LONGQAEVAL: Designing Reliable Evaluations of Long-Form Clinical QA under Resource Constraints

## Quick Facts
- arXiv ID: 2510.10415
- Source URL: https://arxiv.org/abs/2510.10415
- Reference count: 16
- Key outcome: Annotation design effectiveness varies by dimension—fine-grained improves factual correctness IAA while coarse works better for context-dependent relevance

## Executive Summary
LONGQAEVAL introduces a framework for evaluating long-form clinical question answering under resource constraints. It compares coarse answer-level and fine-grained sentence-level annotation designs across three dimensions: correctness, relevance, and safety. Using 300 real patient questions answered by physicians and LLMs, the study finds that annotation design effectiveness varies by dimension—fine-grained annotations improve inter-annotator agreement for factual correctness, while coarse annotations work better for context-dependent relevance. Annotating just three sentences per answer achieves comparable reliability to full fine-grained annotation while reducing cost. The framework also enables reliable LLM-as-judge evaluation, with fine-grained instructions improving agreement for correctness when ratings are collapsed to 3-point scales.

## Method Summary
The study compares coarse (answer-level) vs fine-grained (sentence-level) annotation designs across 300 QA pairs (100 patient questions × 3 answers each: physician, GPT-4, Llama-3.1-Instruct-405B). Six physicians from Upwork rated each QA pair on correctness, relevance, and safety using 5-point Likert scales, with coarse ratings applied to full answers and fine-grained ratings to up to 6 randomly sampled sentences. Ratings were collapsed to 3-point scales for analysis. Inter-annotator agreement was measured using Randolph's κ, with 6 repeated pairs per annotator for intra-rater reliability testing. The framework was validated using LLM-as-judge evaluation on a separate dataset.

## Key Results
- Fine-grained annotation improves IAA for correctness (0.90/0.88) while coarse improves IAA for relevance (0.71)
- Annotating 3 sentences per answer achieves correlation >0.8 with full fine-grained annotation
- GPT-4 and Llama-3.1-Instruct-405B perform comparably to physicians on correctness and relevance in fine-grained settings
- Safety remains a persistent weakness with low IAA across all annotation designs (0.24-0.43)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained sentence-level annotation improves inter-annotator agreement for factual dimensions (correctness) but not for context-dependent dimensions (relevance).
- Mechanism: Decomposing long-form answers into individual sentences constrains evaluators to assess discrete factual claims against medical knowledge, reducing subjective interpretation. Context-dependent dimensions like relevance require assessing the whole passage, which coarse evaluation preserves.
- Core assumption: Factual correctness is primarily local to individual statements; relevance is relational across sentences.
- Evidence anchors: IAA of 0.90/0.88 (fine-grained) vs 0.74 (coarse) for correctness; 0.71 (coarse) vs 0.32/0.29 (fine-grained) for relevance.

### Mechanism 2
- Claim: Partial fine-grained annotation (3 sentences per answer) achieves reliability comparable to full annotation at approximately half the cost.
- Mechanism: Variance in aggregate ratings decreases rapidly with initial samples because error signals are distributed non-uniformly across sentences; most answers contain a mix of correct and incorrect statements that can be detected with limited sampling.
- Core assumption: Sentence-level errors are not uniformly distributed; critical errors appear in a subset of sentences.
- Evidence anchors: Figure 2 shows correlation >0.8 between 3-sentence and 6-sentence annotations; variance lower than coarse for correctness/safety.

### Mechanism 3
- Claim: Fine-grained evaluation mitigates length-related bias in system-level comparisons, particularly for physician answers that are systematically shorter.
- Mechanism: Coarse evaluation conflates answer length with perceived thoroughness; sentence-level evaluation isolates factual accuracy from elaboration, reducing penalization of concise but accurate responses.
- Core assumption: Annotators unconsciously associate length with quality in coarse settings.
- Evidence anchors: Physician answers: 106 tokens avg vs GPT-4: 124, Llama: 170; coarse correctness: 0.78 (physician) vs fine-grained: 0.99.

## Foundational Learning

- Concept: Inter-annotator agreement (IAA) using Randolph's κ
  - Why needed here: Primary reliability metric for comparing annotation designs; corrects for chance agreement among multiple annotators.
  - Quick check question: Would Fleiss' κ or Krippendorff's α give different conclusions for this 3-annotator, 3-point scale setup?

- Concept: Collapse of 5-point to 3-point Likert scales
  - Why needed here: Paper finds fine distinctions introduce variability without reliability gains; this preprocessing step is critical for reproducibility.
  - Quick check question: Does collapsing categories before or after IAA calculation affect the measured agreement?

- Concept: Binary aggregation for safety/correctness (any-fail = fail)
  - Why needed here: Fine-grained sentence ratings must be aggregated for system-level comparison; choice of aggregation (any-fail vs majority) encodes risk tolerance.
  - Quick check question: How would a "majority of sentences correct" aggregation change the safety findings?

## Architecture Onboarding

- Component map: Patient Question → [Answer Source: Physician | GPT-4 | Llama-3.1] → [Annotation Design: Coarse | Fine-grained (full | partial)] → [Dimension: Correctness | Relevance | Safety] → [Aggregation: 3-point | Binary] → IAA / System Rating

- Critical path:
  1. Define evaluation dimensions and codebook (Appendix B provides definitions)
  2. Select annotation granularity per dimension (fine for correctness, coarse for relevance)
  3. Determine sample size (3 sentences if fine-grained)
  4. Choose aggregation scheme (binary for safety-critical, 3-point for research)

- Design tradeoffs:
  - Fine-grained correctness: Higher IAA, higher time cost (~460s vs ~240s per answer), better error localization
  - Coarse relevance: Higher IAA, faster, but masks sentence-level issues
  - Safety: Low IAA in all settings (~0.24-0.43); may require specialized codebooks or multiple annotators

- Failure signatures:
  - IAA <0.5: Likely dimension-granularity mismatch or unclear codebook
  - Physician answers rated significantly lower on correctness in coarse setting: Length bias active
  - Large variance between partial and full fine-grained: Sample size too small

- First 3 experiments:
  1. Replicate coarse vs fine-grained IAA comparison on a new sample of 50 QA pairs to validate dimension-specific recommendations
  2. Test 2-sentence vs 3-sentence vs 5-sentence sampling to find cost-reliability frontier for your domain
  3. Evaluate whether specialized safety codebooks (with concrete risk categories) improve IAA beyond the 0.43 ceiling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-grained annotation design improve intra-rater reliability (IRR) across all evaluation dimensions?
- Basis in paper: The IRR sample was only 6 QA pairs per annotator; fine-grained improved IRR only for safety, not correctness or relevance.
- Why unresolved: Small sample size prevented statistical significance.
- What evidence would resolve it: A larger study with more repeated annotations per annotator, powered to detect differences in self-consistency across dimensions.

### Open Question 2
- Question: What annotation designs or rubric modifications can achieve reliable expert agreement on safety judgments in clinical QA?
- Basis in paper: "Judgments on safety remain inconsistent" with κ values of 0.24–0.43. Neither coarse nor fine-grained settings yielded acceptable IAA for safety.
- Why unresolved: Safety dimension may require fundamentally different evaluation approaches.
- What evidence would resolve it: Experiments with alternative safety-specific rubrics or multi-step annotation protocols tested against the current framework.

### Open Question 3
- Question: Do the findings on annotation granularity and partial sampling generalize to specialized clinical domains and longer-form answers?
- Basis in paper: "More work is needed to confirm whether these answers provide satisfactory safety information and whether model performance generalizes to specialized care." The dataset covers only primary care with concise answers.
- Why unresolved: The K-QA dataset covers primary care with answers under 8 sentences; specialized domains may have longer, more technical responses.
- What evidence would resolve it: Replication studies using datasets from specialties (e.g., oncology, cardiology) with longer reference answers and domain-specific experts.

## Limitations

- Small sample size (100 questions, 300 QA pairs) may not capture full variability of clinical scenarios
- Safety dimension shows persistently low IAA (0.24-0.43) across all annotation designs
- LLM-as-judge validation limited to two model pairs and one clinical dataset
- Sentence sampling for partial fine-grained annotation uses random selection without accounting for sentence importance

## Confidence

- **High Confidence**: Core finding that annotation granularity should match evaluation dimension is well-supported by consistent IAA differences
- **Medium Confidence**: LLM-as-judge reliability results promising but based on limited comparison set
- **Low Confidence**: Safety dimension evaluation is fundamentally unreliable due to consistently low IAA

## Next Checks

1. Replicate dimension-granularity recommendations on a new set of 200+ clinical QA pairs from multiple sources to validate generalizability
2. Compare LLM-as-judge performance across 5+ model pairs and 2-3 additional medical domains to assess robustness
3. Develop and test a hierarchical safety codebook with concrete risk categories and severity levels to improve IAA beyond the 0.43 ceiling