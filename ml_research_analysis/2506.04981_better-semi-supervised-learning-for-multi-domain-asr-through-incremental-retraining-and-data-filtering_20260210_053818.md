---
ver: rpa2
title: Better Semi-supervised Learning for Multi-domain ASR Through Incremental Retraining
  and Data Filtering
arxiv_id: '2506.04981'
source_url: https://arxiv.org/abs/2506.04981
tags:
- data
- hours
- labeled
- speech
- incremental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fine-tuning ASR models for
  specific domains when labeled data is scarce, proposing an incremental semi-supervised
  learning pipeline. The method integrates a small in-domain labeled set with auxiliary
  data from a related domain, then applies filtering strategies (multi-model consensus
  via CER or named entity recognition) to iteratively refine pseudo-labels.
---

# Better Semi-supervised Learning for Multi-domain ASR Through Incremental Retraining and Data Filtering

## Quick Facts
- arXiv ID: 2506.04981
- Source URL: https://arxiv.org/abs/2506.04981
- Reference count: 0
- This paper proposes an incremental semi-supervised learning pipeline for domain-specific ASR models using filtering strategies and pseudo-label refinement

## Executive Summary
This paper addresses the challenge of fine-tuning ASR models for specific domains when labeled data is scarce, proposing an incremental semi-supervised learning pipeline. The method integrates a small in-domain labeled set with auxiliary data from a related domain, then applies filtering strategies (multi-model consensus via CER or named entity recognition) to iteratively refine pseudo-labels. Evaluated on Wow call center and Fisher English corpora using Zipformer and Whisper-medium models, the approach significantly outperforms single-step fine-tuning. Consensus-based filtering achieves up to 22.3% relative improvement on Wow and 24.8% on Fisher, while NER provides competitive results at lower computational cost.

## Method Summary
The proposed method involves an incremental semi-supervised learning pipeline for domain adaptation in ASR. It begins with fine-tuning on a small in-domain labeled set, then iteratively processes auxiliary data from a related domain. At each iteration, models generate pseudo-labels, which are filtered using consensus strategies based on either CER scores from multiple models or NER tag alignment. The filtered pseudo-labeled data is then used to retrain the models. This process repeats for multiple iterations, progressively improving domain-specific performance while reducing noise in the training data.

## Key Results
- Consensus filtering based on CER achieves up to 22.3% relative improvement on Wow call center data
- NER-based filtering provides competitive results with lower computational cost
- Incremental retraining consistently outperforms single-step fine-tuning across both Zipformer and Whisper-medium models
- The approach shows significant improvements on both Wow and Fisher English datasets

## Why This Works (Mechanism)
The method leverages the strengths of both supervised and unsupervised learning by combining limited in-domain labeled data with auxiliary data from related domains. The iterative filtering process progressively refines the quality of pseudo-labels by consensus between multiple models, reducing the impact of individual model errors. This creates a positive feedback loop where better models generate better pseudo-labels, which in turn further improve model performance. The filtering strategies (CER and NER) provide complementary approaches to quality control, with CER focusing on surface-level accuracy and NER capturing semantic coherence.

## Foundational Learning

**ASR model fine-tuning** - Why needed: Domain adaptation requires models to adjust to domain-specific vocabulary and patterns. Quick check: Can the model handle domain-specific terminology after fine-tuning?

**Semi-supervised learning** - Why needed: Labeled data scarcity in target domains necessitates leveraging unlabeled auxiliary data. Quick check: Does the method effectively utilize unlabeled data to improve performance?

**Pseudo-label filtering** - Why needed: Raw pseudo-labels contain errors that can degrade model performance if used directly. Quick check: Does the filtering strategy effectively remove low-quality pseudo-labels?

**Multi-model consensus** - Why needed: Single models may have systematic biases or errors that consensus can mitigate. Quick check: Does consensus filtering improve robustness compared to single-model approaches?

**Named Entity Recognition for quality control** - Why needed: NER provides semantic-level validation beyond surface-level metrics like CER. Quick check: Does NER-based filtering capture meaningful quality improvements?

## Architecture Onboarding

**Component map**: Labeled data -> Initial fine-tuning -> Auxiliary data processing -> Pseudo-label generation -> Filtering (CER/NER) -> Retraining -> (Iterate)

**Critical path**: The iterative retraining loop is the core mechanism, where each iteration's filtered pseudo-labeled data becomes the training set for the next iteration, progressively improving domain adaptation.

**Design tradeoffs**: The method balances computational cost against performance improvement. While CER-based consensus filtering provides superior results, NER-based filtering offers a more efficient alternative with competitive performance.

**Failure signatures**: Poor filtering quality can lead to error accumulation across iterations, potentially degrading model performance. Insufficient diversity in the auxiliary domain may limit adaptation effectiveness.

**First experiments**:
1. Compare single-step fine-tuning vs. incremental approach on a small in-domain dataset with auxiliary data
2. Evaluate CER vs. NER filtering effectiveness on a held-out validation set
3. Test the impact of iteration count on final model performance

## Open Questions the Paper Calls Out
None

## Limitations
- The method's reliance on automatic metrics like CER may not fully capture semantic accuracy or domain-specific terminology handling
- Evaluation is limited to two specific domains (call center and conversational English), limiting generalizability
- Computational cost remains substantial despite NER-based filtering offering a lower-cost alternative
- The approach depends on having related auxiliary domain data, raising questions about performance when such data is unavailable or dissimilar

## Confidence

- **High confidence**: The incremental fine-tuning approach consistently outperforms single-step fine-tuning across both evaluated models and datasets.
- **Medium confidence**: Consensus filtering based on CER provides superior performance compared to NER-based filtering, given the limited scope of domain comparison.
- **Medium confidence**: The computational efficiency of NER-based filtering represents a practical trade-off, though absolute cost comparisons were not provided.

## Next Checks

1. Test the approach on additional domain pairs beyond call center and conversational English to assess generalizability.
2. Compare the method against self-training baselines using unlabeled data from the target domain alone, without auxiliary data.
3. Evaluate long-term stability by analyzing error accumulation across multiple retraining iterations and comparing against model degradation.