---
ver: rpa2
title: Self-Training Large Language Models for Tool-Use Without Demonstrations
arxiv_id: '2502.05867'
source_url: https://arxiv.org/abs/2502.05867
tags:
- tools
- tool
- tool-use
- answer
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  learn to use external tools without explicit demonstrations. The authors propose
  a self-training approach where an LLM generates synthetic tool-use traces from existing
  QA datasets, then fine-tunes on these traces using supervised fine-tuning (SFT)
  and preference fine-tuning (PFT).
---

# Self-Training Large Language Models for Tool-Use Without Demonstrations

## Quick Facts
- arXiv ID: 2502.05867
- Source URL: https://arxiv.org/abs/2502.05867
- Reference count: 30
- One-line primary result: LLMs can learn tool-use without demonstrations through self-training on synthetic traces, with performance gains (+3.7% on PopQA) when tasks require external knowledge

## Executive Summary
This paper investigates whether large language models can learn to use external tools without explicit demonstrations. The authors propose a self-training approach where an LLM generates synthetic tool-use traces from existing QA datasets, then fine-tunes on these traces using supervised fine-tuning (SFT) and preference fine-tuning (PFT). Experiments show that tool-use improves performance on long-tail knowledge tasks (PopQA, +3.7%) but yields mixed results on other datasets (TriviaQA, GSM8K, NQ-Open), suggesting LLMs can learn tool-use without demonstrations but performance gains depend on task characteristics.

## Method Summary
The approach uses zero-shot prompting to generate synthetic tool-use traces on QA datasets, then filters these traces by answer correctness to identify successful tool-use patterns. The filtered traces are used for supervised fine-tuning (SFT) or preference fine-tuning (PFT) via Direct Preference Optimization (DPO). Tools include Calculator, WikipediaSearch (BM25 retrieval), and MachineTranslator. The method was tested on Llama-3-8B-Instruct using datasets including TriviaQA, GSM8K, NQ-Open, and PopQA, with evaluation metrics including Exact Match, Accuracy, Invoke Rate, Pass Rate, and Answerable Rate.

## Key Results
- Tool-use improves performance on long-tail knowledge tasks: +3.7% on PopQA
- SFT on tools data increases invoke rate (67-84.8%) but hurts accuracy on common knowledge
- DPO on single-response format outperforms conversation format but yields mixed results
- Multi-step tool-use performs worst due to error propagation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot prompting can elicit basic tool-use behavior from instruction-tuned LLMs without gradient updates
- Mechanism: The instruction-following capability learned during instruction fine-tuning transfers to tool-use scenarios when the model is prompted with tool descriptions and expected response formats
- Core assumption: The model has sufficient prior knowledge to understand tool descriptions and map them to appropriate use cases
- Evidence anchors: [abstract] "First, we analyse zero-shot prompting strategies to guide LLMs in tool utilisation"; [section 5.1] "The LLM had a positive invoke rate and pass rate for tools"; Related work confirms prompting-based methods enable tool-use in large-scale models

### Mechanism 2
- Claim: Self-training on model-generated tool-use traces filtered by answer correctness can improve tool-use on knowledge-sparse tasks
- Mechanism: Answer correctness serves as a proxy for valid tool-use traces; filtering retains successful tool-use patterns while discarding failures, creating training signal without human demonstrations
- Core assumption: Correct final answers imply valid tool-use reasoning paths; incorrect answers indicate flawed traces
- Evidence anchors: [abstract] "self-training method to synthesise tool-use traces using the LLM itself... tool-use enhances performance on a long-tail knowledge task: 3.7% on PopQA"; [section 3.2] "we used the correctness of the answer given a question to serve as a proxy for identifying 'correct' tool-use traces"

### Mechanism 3
- Claim: Preference Fine-Tuning on contrasting tool-use vs non-tool-use responses can teach when to invoke tools
- Mechanism: DPO optimizes policy to prefer tool-use traces that yield correct answers when non-tool responses fail, potentially teaching appropriate tool invocation boundaries
- Core assumption: The contrast between successful tool-use and failed non-tool responses contains sufficient signal to learn when tools are beneficial
- Evidence anchors: [abstract] "compare supervised fine-tuning and preference fine-tuning techniques"; [section 3.3] "preferred response and dispreferred response" constructed from "correct answer with access to tools" vs "wrong answer for the same question without access to tools"

## Foundational Learning

- Concept: **Supervised Fine-Tuning (SFT) Loss**
  - Why needed here: The paper uses SFT to train on filtered tool-use traces; understanding Equation 1 (negative log-likelihood) is prerequisite for implementing the training pipeline
  - Quick check question: Can you explain why SFT loss is computed only on completion tokens (non-LLM responses excluded)?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: PFT with DPO is the second training approach; Equation 2 defines the loss that implicitly adjusts likelihood of preferred vs dispreferred responses
  - Quick check question: What does the β hyperparameter control in DPO, and what happens when β is too low vs too high?

- Concept: **Zero-shot Chain-of-Thought Prompting**
  - Why needed here: The paper's prompting approach builds on zero-shot CoT (Kojima et al., 2022); the "Let's think step by step" format is central to the tool-use prompt design
  - Quick check question: Why might constrained answer formats (like the Thought/Action/Rationale/Answer schema) hurt larger models more than smaller ones?

## Architecture Onboarding

- Component map: QA dataset -> Zero-shot prompting -> Synthetic tool-use traces -> Filter by correctness -> SFT/DPO fine-tuning -> Evaluated on held-out datasets

- Critical path:
  1. Implement tool interfaces with clear input/output formats
  2. Build prompt template with tool descriptions
  3. Generate traces on QA datasets -> filter by correctness
  4. Apply LoRA fine-tuning (r=16, q_proj/v_proj only, ~0.08% parameters)
  5. Evaluate on held-out datasets using EM/Accuracy and tool-use metrics (IR/PR/AR)

- Design tradeoffs:
  - SFT (tools data) vs SFT (mixture data): Tools-only increases invoke rate but hurts accuracy on common knowledge; mixture preserves accuracy but reduces tool usage
  - DPO (conversation) vs DPO (response): Full conversation includes tool responses in loss (redundant); single-response misses learning from tool outputs
  - Single-step vs multi-step tool-use: Single-step simpler but limited; multi-step flexible but higher error propagation

- Failure signatures:
  - Tool overuse: High invoke rate (67-84.8%) with degraded accuracy indicates model learned to use tools indiscriminately
  - Knowledge redundancy: Performance drops on TriviaQA/GSM8K when tools used on tasks already solvable by parametric knowledge
  - Low-quality retrieval: BM25 returns irrelevant documents; model sometimes refuses to answer or hallucinates
  - Incorrect tool arguments: Reasoning errors propagate to tool calls

- First 3 experiments:
  1. Baseline probing: Run zero-shot prompting (no tools, tools single-step, tools multi-step) on your target dataset to establish whether tools help at all; measure IR/PR/AR metrics
  2. Data filtering ablation: Compare SFT (tools data) vs SFT (mixture data) vs SFT (no tool data) to isolate whether gains come from tool-use learning vs dataset size
  3. Long-tail evaluation: Test on PopQA or similar long-tail knowledge dataset to verify the mechanism works where parametric knowledge is insufficient

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can preference fine-tuning (PFT) objectives be adapted to effectively optimize models for multi-turn tool-use dialogues?
- **Basis in paper:** [explicit] Section 5.3 notes that while PFT on single responses works better than full conversations, it prevents the model from learning to answer based on tool responses. The authors state, "how to optimise the model in the multi-turn dialogues with PFT remains an open research question."
- **Why unresolved:** Current DPO applications include redundant tool responses in the loss calculation (conversation format) or fail to teach the model to process tool feedback (response format).
- **What evidence would resolve it:** A study comparing new loss functions or data formatting strategies specifically designed to handle intermediate tool states without penalizing necessary context.

### Open Question 2
- **Question:** Does iterative self-training improve tool-use proficiency, or does it lead to model collapse?
- **Basis in paper:** [explicit] The Limitations section states that the self-training method "typically contains multiple iterations," but this work only experimented with the first iteration. It suggests future work could benefit from exploring the "multiple-iteration setting."
- **Why unresolved:** It is unclear if reinforcing the model's own correct predictions iteratively amplifies capabilities or entrenches errors (catastrophic forgetting) without external gold demonstrations.
- **What evidence would resolve it:** Experiments tracking accuracy and tool-invoke rates over 3+ self-training iterations on the same QA datasets would determine the trajectory of performance.

### Open Question 3
- **Question:** Does self-training for tool-use generalize to a broader range of tools beyond the limited set (Calculator, WikipediaSearch, MachineTranslator) tested?
- **Basis in paper:** [explicit] The Limitations section acknowledges the use of a "limited number of tools" and states that "the generalisation of tools across various domains remains a significant research topic."
- **Why unresolved:** The selected tools cover basic math and retrieval, but it is unknown if this self-training approach works for tools requiring complex, structured inputs (e.g., code interpreters, SQL databases) or specific API calls.
- **What evidence would resolve it:** Applying the same synthetic data generation and filtering pipeline to a diverse toolset (e.g., 100+ tools) and evaluating the success rate.

## Limitations

- Tool-use gains are task-dependent and modest (+3.7% on PopQA vs mixed/no gains elsewhere)
- Performance degrades when tools add redundancy or noise to already-solvable problems
- Preference fine-tuning effectiveness for teaching when to invoke tools remains unresolved

## Confidence

- Zero-shot prompting elicits basic tool-use behavior: High confidence
- Self-training approach implementation: High confidence
- Preference fine-tuning for tool invocation boundaries: Low confidence
- Approach generalizability across model sizes and tool types: Medium confidence

## Next Checks

1. **Task Dependency Analysis**: Systematically test the approach across tasks with varying knowledge distribution (common vs. rare) to quantify when tool-use provides net benefit versus harm.

2. **Tool Quality Sensitivity**: Replace the Wikipedia BM25 retriever with higher-quality retrieval (e.g., semantic search) to isolate whether performance limitations stem from tool quality versus the learning mechanism itself.

3. **Multi-step Tool-Use Reliability**: Investigate error propagation in iterative tool-use by analyzing failed multi-step traces to determine if the approach can be extended beyond single-tool invocations.