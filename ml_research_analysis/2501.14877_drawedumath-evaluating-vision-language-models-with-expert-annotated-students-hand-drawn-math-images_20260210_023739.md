---
ver: rpa2
title: 'DrawEduMath: Evaluating Vision Language Models with Expert-Annotated Students''
  Hand-Drawn Math Images'
arxiv_id: '2501.14877'
source_url: https://arxiv.org/abs/2501.14877
tags:
- student
- questions
- question
- teachers
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces DrawEduMath, a dataset of 2,030 images of\
  \ students\u2019 handwritten math responses, annotated by teachers with detailed\
  \ descriptions and QA pairs. The dataset aims to evaluate vision language models\
  \ (VLMs) in interpreting students\u2019 work in real-world educational contexts."
---

# DrawEduMath: Evaluating Vision Language Models with Expert-Annotated Students' Hand-Drawn Math Images

## Quick Facts
- arXiv ID: 2501.14877
- Source URL: https://arxiv.org/abs/2501.14877
- Reference count: 30
- Primary result: VLMs struggle most with correctness/error detection on handwritten student math work (0.477 vs 0.770 on superficial categories)

## Executive Summary
This paper introduces DrawEduMath, a dataset of 2,030 images of students' handwritten math responses, annotated by teachers with detailed descriptions and QA pairs. The dataset aims to evaluate vision language models (VLMs) in interpreting students' work in real-world educational contexts. Teachers provided descriptions and QA pairs, which were supplemented with synthetic QA pairs generated using language models. The authors evaluated several VLMs on the dataset and found that even state-of-the-art models struggle with questions related to the correctness of students' responses. Synthetic QA pairs, while imperfect, yielded similar model rankings as teacher-written QA pairs.

## Method Summary
The authors created a dataset of student math work images annotated by teachers with descriptions and QA pairs. They used language models (Claude-3.5 Sonnet and GPT-4o) to decompose teacher descriptions into atomic "facets" and generate synthetic QA pairs. Four VLMs (GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro, Llama 3.2-11B Vision) were evaluated on both teacher-written and synthetic QA pairs using ROUGE-L, BERTScore, and LLM-based similarity judgments. Human evaluation was performed on 500 samples to validate the LLM judgments.

## Key Results
- VLMs achieved only 0.477 average score on correctness/error questions versus 0.770 on image creation/medium questions
- Synthetic QA pairs (44,362) were nearly four times more numerous than teacher-written pairs (11,661)
- LLM-based similarity judgments showed ρ=0.801 correlation with human evaluation, outperforming ROUGE-L (ρ=0.472)
- All four VLMs struggled similarly on handwritten student work, with no clear winner across question types

## Why This Works (Mechanism)

### Mechanism 1
- Decomposing teacher descriptions into atomic "facets" before generating QA pairs yields more structured, answerable questions than direct generation. The pipeline first extracts atomic snippets from paragraph-length descriptions, then rewrites each facet as a self-contained QA pair, constraining questions to have single, unambiguous answers.

### Mechanism 2
- LLM-based similarity judgments correlate more strongly with human evaluation than n-gram or embedding-based metrics for open-ended VQA. Mixtral 8x22B rates answer similarity on a 1-4 Likert scale, then responses are binarized, capturing semantic equivalence that surface metrics miss.

### Mechanism 3
- Fine-grained question taxonomy reveals that VLMs fail disproportionately on pedagogically critical question types (correctness/errors) while succeeding on superficial ones (image medium). Questions are categorized into 7 types, showing 0.477 average LLM-score on correctness questions vs. 0.770 on image creation/medium.

## Foundational Learning

- **Vision-Language Model Architecture**: Understanding how VLMs process image-text pairs clarifies why they struggle with handwritten, noisy inputs versus clean diagrams. Can you explain how a VLM aligns visual tokens with language tokens during inference?
- **Reference-Based Evaluation Metrics**: The paper uses ROUGE-L, BERTScore, and LLM-judgment; understanding their tradeoffs is essential for interpreting results. Why would ROUGE-L perform poorly on answers like "3" vs. "three" despite semantic equivalence?
- **Taxonomy-Driven Benchmark Analysis**: Aggregate accuracy masks category-specific failures; the 7-type taxonomy enables targeted debugging. If a model scores 80% on "counting" but 40% on "correctness," what does this suggest about its training data?

## Architecture Onboarding

- **Component map**: [Student Images] → [Teacher Annotation UI] → [Descriptions + QA] → [Facet Decomposition (LLM)] → [Synthetic QA] → [VLM Under Test] ← [Image + Question] ← [QA Dataset] → [Model Answer] → [Evaluation: ROUGE-L / BERTScore / LLM-Judge] → [Disaggregated Results by Question Type]

- **Critical path**: 1. Annotation quality (teacher descriptions must be complete) 2. Facet extraction (prompts in Appendix B/Figures 6-7) 3. Question-type classification (prompt in Figure 8) 4. LLM evaluator calibration (prompt in Figure 10)

- **Design tradeoffs**: Teacher-written QA (11,661 pairs, richer answers, avg 16.2 tokens) vs. synthetic QA (44,362 pairs, shorter answers, avg 2-3 tokens); Typed vs. transcribed annotations (46.7% vs. 53.3%): speech yields longer descriptions but introduces transcription errors; Human evaluation (500 samples, high cost) vs. LLM evaluation (scalable, ρ=0.801 correlation)

- **Failure signatures**: Dark images: VLMs fail even when content is human-visible; Mathematically correct but student-incorrect: VLMs answer "3" when student's number line shows "2" at 18/6 position; Ambiguous referents: "second arrow" unclear when arrows overlap

- **First 3 experiments**: 1. Baseline replication: Run GPT-4o and Claude 3.5 Sonnet on the 2,030 images with teacher-written QA; verify you recover similar LLM-eval scores (~0.70 synthetic, ~0.65 teacher). 2. Ablation on facet decomposition: Generate QA pairs directly from descriptions (skip facet extraction) on 100 images; compare answerability and correctness vs. facet-based pipeline. 3. Error analysis on correctness category: Manually inspect 50 "correctness & errors" questions where all VLMs failed; categorize failure modes (OCR error, reasoning error, student-work misalignment).

## Open Questions the Paper Calls Out

### Open Question 1
- How can VLMs be improved to accurately assess the correctness of students' mathematical work, given that current models struggle most with error detection and correctness judgments? Error detection requires pedagogical knowledge that current VLMs lack; models may correctly solve problems mathematically while misinterpreting students' actual work.

### Open Question 2
- What refinements to synthetic QA generation pipelines could reduce ambiguity (e.g., unclear referents like "second arrow") and annotation errors while maintaining scalability? Ambiguity often arises from pragmatic context that LMs cannot infer from descriptions alone.

### Open Question 3
- Should different question types be weighted differently when benchmarking VLMs for educational applications, and what weighting scheme best reflects pedagogical priorities? The current evaluation treats all QA pairs equally; no pedagogically-grounded weighting framework exists.

### Open Question 4
- How well do VLMs trained on U.S.-centric math curricula generalize to handwritten student work from different educational systems, languages, and cultural contexts? Cross-cultural and cross-curricular generalization has not been tested; drawing conventions, notation, and problem-solving strategies may differ.

## Limitations
- The synthetic QA pipeline depends entirely on teachers' descriptions being accurate and comprehensive; if descriptions omit visual details, generated questions will be unanswerable or misleading
- The LLM-based evaluator may introduce its own biases, potentially conflating semantic similarity with answer correctness, especially for questions with multiple valid formulations
- The question-type taxonomy may not be mutually exclusive in practice, potentially confounding category-level performance comparisons

## Confidence

- **High confidence**: The dataset construction methodology and basic evaluation pipeline are well-documented and reproducible. The core finding that VLMs struggle with correctness/error detection (0.477 vs 0.770 on superficial categories) is directly supported by the data.
- **Medium confidence**: The claim that facet decomposition improves QA quality is supported by the pipeline description but lacks direct ablation evidence within this paper. The correlation between LLM judgments and human evaluation (ρ=0.801) is strong but hasn't been validated against other benchmark tasks.
- **Low confidence**: The assertion that synthetic QA "can yield similar model rankings" as teacher-written QA is based on aggregate performance but doesn't account for potential category-specific differences in synthetic question quality.

## Next Checks

1. **Facet Decomposition Ablation**: Generate QA pairs directly from 100 teacher descriptions (skipping facet extraction) and compare both answerability rates and model performance against the facet-based pipeline.
2. **LLM Evaluator Calibration**: Test Mixtral 8x22B judgments on a subset of questions where human raters disagree; examine whether LLM ratings systematically favor certain answer styles or phrasings.
3. **Category-Level Synthetic Quality**: For each of the 7 question types, compute the percentage of synthetic questions that humans judge as answerable; identify whether certain categories (e.g., correctness) have systematically lower synthetic QA quality.