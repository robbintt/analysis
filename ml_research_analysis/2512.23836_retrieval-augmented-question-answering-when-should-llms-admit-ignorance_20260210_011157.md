---
ver: rpa2
title: 'Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?'
arxiv_id: '2512.23836'
source_url: https://arxiv.org/abs/2512.23836
tags:
- arxiv
- window
- language
- answer
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models can effectively
  decline to answer when faced with insufficient information in retrieval-augmented
  question answering. The authors propose an adaptive prompting strategy that splits
  retrieved information into smaller chunks and sequentially prompts the LLM to answer
  using each chunk.
---

# Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?

## Quick Facts
- **arXiv ID**: 2512.23836
- **Source URL**: https://arxiv.org/abs/2512.23836
- **Authors**: Dingmin Wang, Ji Ma, Shankar Kumar
- **Reference count**: 10
- **Primary result**: Adaptive prompting with sliding windows matches standard RAG performance while using >50% fewer tokens

## Executive Summary
This paper investigates whether large language models can effectively decline to answer when faced with insufficient information in retrieval-augmented question answering. The authors propose an adaptive prompting strategy that splits retrieved information into smaller chunks and sequentially prompts the LLM to answer using each chunk. This approach mitigates noise accumulation and improves computational efficiency compared to standard prompting that uses the full context at once. Experiments on three open-domain question answering datasets show that the adaptive strategy matches the performance of standard prompting while using over 50% fewer tokens. Crucially, analysis reveals that LLMs often generate incorrect answers instead of declining when encountering insufficient information, with zero-shot models generating false answers over 50% of the time on negative windows. This tendency toward hallucination persists even with few-shot in-context learning, suggesting that training-based interventions may be necessary to enhance LLMs' ability to appropriately decline requests.

## Method Summary
The authors propose an adaptive prompting strategy for retrieval-augmented question answering that processes retrieved documents through a sliding window approach rather than using all retrieved context at once. BM25 retrieves top-K Wikipedia pages per question, which are then sorted by descending retrieval score. A sliding window of size w pages iterates through this ranked list, with the LLM generating a structured response for each window. If an answer is found, the process terminates; if "answer not found" is output, the next window is processed. The method uses Gemini 1.5 Pro with greedy decoding and tests window sizes to find optimal performance. The approach is evaluated on three KILT benchmark datasets (Natural Questions, TriviaQA, and HotpotQA) using Exact Match accuracy and token efficiency metrics.

## Key Results
- Adaptive prompting matches standard RAG performance while using over 50% fewer tokens
- Processing high-scoring pages first dramatically reduces hallucination exposure
- Zero-shot models generate false answers over 50% of the time on negative windows
- Few-shot in-context learning fails to improve abstention performance
- Window size of approximately 60 pages achieves optimal balance between noise reduction and computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Smaller context windows reduce distractor accumulation, improving per-step reasoning quality
- Retrieved documents are split into windows of w pages, containing fewer irrelevant passages and lowering reasoning complexity
- Core assumption: Irrelevant information actively degrades generation quality even when relevant information is present
- Evidence: Longer contexts introduce more irrelevant information that hinders the model's generation process; inverted-U pattern shows performance improves when increasing window size from 50 to 100 but drops at 200

### Mechanism 2
- Processing high-scoring pages first dramatically reduces exposure to negative windows where hallucination occurs
- Windows slide through pages sorted by descending BM25 score; encountering fewer negatives before the first positive window directly improves accuracy
- Core assumption: Retrieval scores meaningfully correlate with relevance; first relevant page appears early in ranked results
- Evidence: Forward approach encounters <0.5 negative windows before positive; backward encounters 3.8 with corresponding EM drop

### Mechanism 3
- The "bias to answer" is resistant to prompt-level interventions and likely requires training-level changes
- When presented with negative windows, instruction-tuned models prefer generating plausible answers over abstaining
- Core assumption: The tendency to answer rather than abstain is deeply embedded during instruction tuning
- Evidence: Increasing the number of shots yields no significant improvement in either metric; models generate incorrect answers instead of declining to respond

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**: Understanding RAG is prerequisite as the entire method builds on RAG fundamentals. Quick check: Can you explain why RAG helps with factual accuracy but introduces the noise problem this paper addresses?

- **Context Window Limitations**: The paper's motivation hinges on how longer contexts introduce noise; understanding "lost in the middle" and U-shaped attention helps contextualize findings. Quick check: What happens to model performance when relevant information is placed in the middle of a long context?

- **In-Context Learning (Few-Shot Prompting)**: The paper tests whether few-shot examples can teach abstention; understanding ICL's capabilities and limits is essential. Quick check: Why might ICL succeed at task formatting but fail at teaching fundamentally new behaviors like abstention?

## Architecture Onboarding

- **Component map**: Retriever (BM25) → Ranker (BM25 scoring) → Window Manager (sliding window) → LLM (generation) → Output Parser (extract answer/refusal)

- **Critical path**: Question → BM25 retrieval → top-K pages → sort by descending score → slide window (size w) through pages → for each window: LLM generates structured output → if answer found → terminate → if "answer not found" → advance to next window → if all windows exhausted → return failure

- **Design tradeoffs**: Window size (w): smaller → less noise per step, more steps, higher cumulative error; larger → more noise per step, fewer steps. K (retrieval count): higher K improves recall but increases negative windows. Page-level vs passage-level retrieval: page-level avoids segmentation errors but may include more noise per unit.

- **Failure signatures**: High hallucination rate on negative windows (54.3%): model generates plausible but unsupported answers; few-shot ICL ineffective: adding examples doesn't improve abstention; order sensitivity: backward processing catastrophically underperforms forward.

- **First 3 experiments**: 1) Reproduce forward vs. backward sliding comparison on held-out subset to validate ordering sensitivity on your target model; 2) Sweep window sizes (40, 60, 80, 100) to find optimal balance for your retrieval quality and document lengths; 3) Test abstention rate on synthetic negative-only windows to establish baseline hallucination propensity before deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific training objectives or fine-tuning interventions can effectively teach LLMs to abstain from answering when retrieval context is insufficient?
- Basis: The conclusion states that "true robustness in RAG requires future research into training objectives that explicitly penalize unsupported generation and reward correct abstention."
- Why unresolved: Few-shot ICL failed to mitigate the issue, suggesting the solution requires deeper architectural or training-level changes.
- What evidence would resolve it: A fine-tuning regimen that significantly lowers the hallucination rate on negative windows without reducing accuracy on positive windows.

### Open Question 2
- Question: Is the inability to handle "negative windows" a fundamental limitation across all LLM architectures or specific to the Gemini 1.5 Pro model used?
- Basis: Experiments were restricted to Gemini 1.5 Pro, though the authors hypothesize the "bias to answer" is a general behavioral issue.
- Why unresolved: The study did not perform a cross-model evaluation to determine if other architectures exhibit the same 54.3% hallucination rate on irrelevant text.
- What evidence would resolve it: Reproducing the adaptive prompting experiments across a diverse set of state-of-the-art LLMs to compare error rates on negative windows.

### Open Question 3
- Question: Why does standard few-shot in-context learning fail to improve abstention performance in this specific task?
- Basis: Section 3.2 notes that increasing the number of shots yielded no significant improvement, indicating the model's struggle is "resistant to standard few-shot prompting."
- Why unresolved: The paper identifies the failure of ICL but does not investigate the underlying mechanisms that cause the "bias to answer" to override few-shot examples.
- What evidence would resolve it: An analysis of internal model states or attention heads to understand why the "answer not found" signal in few-shot examples is ignored.

## Limitations

- The paper assumes the "bias to answer" is an inherent property of instruction-tuned models that cannot be overcome through prompt-level interventions, though alternative prompt engineering approaches were not explored.
- The study relies on BM25 retrieval, which may not represent the full spectrum of retrieval quality conditions that would stress-test the adaptive prompting strategy.
- The conclusion that training-based interventions are necessary is based on limited testing of few-shot prompting and may not account for other prompt engineering or fine-tuning approaches.

## Confidence

- **High Confidence**: The finding that context length negatively impacts LLM performance even with perfect retrieval is well-supported by both this work and the cited corpus paper. The >50% reduction in token usage while maintaining EM scores is a concrete, measurable result.
- **Medium Confidence**: The mechanism by which smaller context windows reduce noise accumulation is theoretically sound and supported by the inverted-U pattern in window size experiments, though the optimal window size (60 pages) may be dataset-specific.
- **Medium Confidence**: The ordering effect showing that processing high-scoring pages first dramatically reduces hallucination exposure is supported by the forward vs. backward sliding comparison, but the magnitude of improvement may depend on retrieval quality and dataset characteristics.
- **Low Confidence**: The conclusion that training-based interventions are necessary to improve abstention behavior is based on limited testing of few-shot prompting and may not account for other prompt engineering or fine-tuning approaches.

## Next Checks

1. **Cross-dataset generalization test**: Apply the adaptive prompting strategy to additional open-domain QA datasets (e.g., WebQuestions, ComplexWebQuestions) with varying retrieval qualities to validate whether the 60-page optimal window size generalizes or requires tuning per dataset.

2. **Retrieval quality stress test**: Systematically degrade retrieval quality (e.g., by using lower BM25 thresholds or alternative retrievers like DPR) to determine whether the adaptive strategy maintains its advantages when relevant pages appear later in the ranked list.

3. **Alternative abstention prompt engineering**: Design and test alternative prompt structures that explicitly reward "answer not found" responses with stronger instructions or different formatting, to determine whether the bias to answer is truly resistant to prompt-level interventions.