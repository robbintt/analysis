---
ver: rpa2
title: Learning normalized image densities via dual score matching
arxiv_id: '2506.05310'
source_url: https://arxiv.org/abs/2506.05310
tags:
- energy
- probability
- score
- images
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new method for learning normalized energy
  models from image data by combining ideas from diffusion generative models with
  dual score matching. The authors derive a joint objective that matches both space
  and time scores of the energy function, ensuring consistent energy estimates across
  noise levels.
---

# Learning normalized image densities via dual score matching

## Quick Facts
- arXiv ID: 2506.05310
- Source URL: https://arxiv.org/abs/2506.05310
- Reference count: 40
- Primary result: Introduces dual score matching for learning normalized energy models, achieving competitive NLL on ImageNet64 while enabling exact log-density computation.

## Executive Summary
This paper presents a method for learning normalized energy models from image data by combining denoising score matching (DSM) with time score matching (TSM). The approach ensures that the learned energy function provides consistent log-density estimates across different noise levels. The authors demonstrate that their method achieves competitive negative log-likelihood values on ImageNet64 while enabling exact log-probability computation, unlike standard score-based diffusion models that only provide tractable sampling. The work also reveals surprising properties of natural image statistics, showing that log-probabilities follow a Gumbel distribution and that local dimensionality varies significantly across images.

## Method Summary
The method learns a normalized energy function U_θ(y,t) that represents log p(y) - log p(y|t_max) for an image y at noise level t. The energy is computed as U_θ(y,t) = ½⟨y, s_θ(y,t)⟩ where s_θ is a UNet-based score network. Training uses a dual objective combining DSM and TSM: ℓ(θ) = E_t[t/d·ℓ_DSM + (t/d)²·ℓ_TSM], where d is the data dimension. This ensures both space and time scores match their true values. The energy estimates are normalized by fitting to a Gaussian reference at t_max, enabling exact log-density computation. The approach is trained on ImageNet64 with specific architectural choices including homogeneous instance normalization and gain control conditioning for the time embedding.

## Key Results
- Achieves competitive NLL values on ImageNet64 compared to state-of-the-art score-based models
- Demonstrates strong generalization: two models trained on non-overlapping subsets assign nearly identical probabilities to all images
- Shows that image probability and local dimensionality vary significantly depending on image content, challenging traditional assumptions about high-dimensional distributions
- Validates that log-probabilities of natural images follow a Gumbel distribution across multiple datasets (ImageNet, CelebA, CIFAR-10)

## Why This Works (Mechanism)
The method works by ensuring consistency between space and time scores through joint optimization. By minimizing both DSM and TSM losses, the learned energy function satisfies ∇_y U_θ ≈ s_θ and ∂_t U_θ ≈ -½⟨y, s_θ⟩ across all noise levels. This dual constraint prevents the energy from having arbitrary additive constants in different modes and ensures the time derivative matches theoretical expectations. The energy formulation U_θ(y,t) = ½⟨y, s_θ(y,t)⟩ automatically satisfies the conservative field property (∇×s = 0) required for valid energy functions, while the inner product structure preserves the inductive biases of score networks trained with DSM.

## Foundational Learning
- **Dual score matching**: Combines space and time score objectives to learn normalized energy models - needed because standard DSM only learns unnormalized densities, quick check: verify both ℓ_DSM and ℓ_TSM are minimized during training
- **Energy-based modeling**: Energy functions represent log probability minus reference - needed to enable exact log-density computation, quick check: confirm U_θ provides consistent values across noise levels
- **Conservative vector fields**: ∇×s = 0 ensures valid energy functions - needed for physical interpretability of learned densities, quick check: verify curl of learned score is near zero
- **Time score matching**: Matches ∂_t U_θ to theoretical derivative - needed for temporal consistency across diffusion scales, quick check: compare ∂_t U_θ with -½⟨y, s_θ⟩
- **Gumbel distribution**: Limit distribution for maxima, not sums - needed to understand surprising log-probability statistics, quick check: fit Gumbel to empirical log-density values

## Architecture Onboarding

**Component Map**: Input y → Time embedding → UNet s_θ → Inner product with y → Energy U_θ(y,t)

**Critical Path**: The score network s_θ processes the noisy image y with time conditioning to produce a vector field. This output is combined with the input y via inner product to compute the energy U_θ(y,t) = ½⟨y, s_θ(y,t)⟩. Both space and time gradients of this energy are computed automatically for the dual loss.

**Design Tradeoffs**: The inner product energy formulation ensures the conservative field property but constrains the functional form. Homogeneous instance normalization prevents scale divergence but requires careful ε tuning. The dual objective increases computational cost (double backprop) but provides theoretical consistency guarantees.

**Failure Signatures**: Inconsistent energy estimates across noise levels indicate mode-specific shifts. Large gap between denoising PSNR of energy-based vs standard score network suggests incorrect score learning. Poor generalization between train/test subsets indicates overfitting to specific modes.

**First Experiments**:
1. Verify energy consistency by plotting U_θ(x,t) for same image at different t values
2. Compare denoising performance of proposed method vs standard score network baseline
3. Test normalization procedure on synthetic data with known ground truth energy

## Open Questions the Paper Calls Out

**Open Question 1**: Why do log probabilities of natural images follow a Gumbel distribution, and what does this imply about image statistics?
The authors find this surprising as Gumbel arises as a limit for maxima rather than sums, but provide no theoretical explanation for why high-dimensional image densities should exhibit this specific skewness.

**Open Question 2**: Can the dual score matching objective be mathematically proven to control error relative to true energy?
While the paper shows it minimizes score matching errors, it does not prove these errors bound the variance between learned and true energy, suggesting a Poincaré inequality framework could help.

**Open Question 3**: What specific "undocumented geometrical regularities" allow models to generalize despite high local effective dimensionality?
The results challenge the manifold hypothesis since dimensionality is often full, leaving unexplained how the model learns valid densities from finite data without traditional low-dimensional structures.

## Limitations
- High computational cost requiring double back-propagation and 5 days on H100 for ImageNet64
- Normalization relies on Gaussian reference estimation at t_max, which may introduce approximation errors
- No extensive ablation studies on architectural components or training hyperparameters

## Confidence
- **High confidence**: Dual score matching framework is mathematically rigorous with well-established theoretical connections
- **Medium confidence**: Architectural choices are justified but could be optimized further; local dimensionality findings are intriguing but based on limited analysis
- **Low confidence**: Lack of ablation studies makes it difficult to assess robustness of reported results

## Next Checks
1. Verify energy consistency across noise levels by plotting U_θ(x,t) for the same image at different t values to ensure smooth variation without mode-specific shifts
2. Compare denoising PSNR of the proposed method against a baseline score network (without energy constraints) to quantify the cost of the dual matching objective
3. Test the normalization procedure (eq. 9) on a synthetic dataset with known ground truth to assess accuracy of the Gaussian reference estimation