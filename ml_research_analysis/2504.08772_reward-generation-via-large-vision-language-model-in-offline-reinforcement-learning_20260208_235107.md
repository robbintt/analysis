---
ver: rpa2
title: Reward Generation via Large Vision-Language Model in Offline Reinforcement
  Learning
arxiv_id: '2504.08772'
source_url: https://arxiv.org/abs/2504.08772
tags:
- reward
- offline
- learning
- task
- rg-vlm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces RG-VLM, a method that uses large vision-language\
  \ models (LVLMs) to generate dense rewards for offline reinforcement learning without\
  \ human involvement. The core idea is to prompt an LVLM in two stages\u2014first\
  \ to analyze visual transitions and then to assign interpretable reward scores\u2014\
  producing reward-labeled trajectories that improve policy learning."
---

# Reward Generation via Large Vision-Language Model in Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2504.08772
- Source URL: https://arxiv.org/abs/2504.08772
- Reference count: 37
- RG-VLM achieves 1.14x higher returns than VLM baselines on ALFRED tasks

## Executive Summary
This paper introduces RG-VLM, a method that uses large vision-language models (LVLMs) to generate dense rewards for offline reinforcement learning without human involvement. The core idea is to prompt an LVLM in two stages—first to analyze visual transitions and then to assign interpretable reward scores—producing reward-labeled trajectories that improve policy learning. Experiments on ALFRED tasks show RG-VLM combined with sparse rewards achieves 1.14x higher returns than other VLM-based reward models and demonstrates stronger generalization, with only 38.94% performance drop under randomized initial states compared to 49.91% for sparse-only methods. The method scales efficiently with a window size of 8 and outperforms CLIP and RoboCLIP baselines, proving LVLMs' reasoning capabilities are effective for reward generation in complex long-horizon tasks.

## Method Summary
RG-VLM generates dense rewards for offline RL by prompting a pre-trained LVLM in two stages: first analyzing visual transitions between concatenated image frames to identify semantic changes, then assigning interpretable reward scores. The method processes ALFRED trajectories by concatenating 8 consecutive frames, using Gemini 1.5 Pro to analyze the transition and output scores between 0-10, which are normalized to [0,1]. These LVLM-generated rewards are combined with sparse environmental rewards and used to train a language-conditioned IQL agent with AWR policy optimization on a dataset of 73k trajectories.

## Key Results
- RG-VLM combined with sparse rewards achieves 1.14x higher returns than other VLM-based reward models
- Demonstrates stronger generalization with only 38.94% performance drop under randomized initial states versus 49.91% for sparse-only methods
- Outperforms CLIP and RoboCLIP baselines while scaling efficiently with window size of 8
- Maintains performance advantages across 100 ALFRED tasks with 1-6 sub-tasks each

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing reward generation into "visual analysis" and "numerical scoring" improves label quality compared to single-step inference.
- **Mechanism:** The two-stage prompting strategy forces the LVLM to explicitly ground its reward assignment in observed visual differences rather than hallucinating progress.
- **Core assumption:** The LVLM possesses sufficient visual grounding to correctly identify object states and spatial changes in concatenated frames.
- **Evidence anchors:** Two-stage prompting process described in section; Figure 3 illustrates explicit link between visual analysis and reward scoring; broader trend of using VLM reasoning for policy guidance validates capacity.
- **Break condition:** If visual observation concatenation reduces resolution below threshold required to recognize key objects, first-stage analysis will fail.

### Mechanism 2
- **Claim:** Using LVLMs as auxiliary reward signal providers mitigates the sparsity problem in offline datasets.
- **Mechanism:** Combines sparse rewards (for final success) with dense LVLM rewards (for intermediate progress) to shape the value landscape.
- **Core assumption:** Pre-trained LVLM has seen enough semantic task structures in internet-scale pre-training to estimate sub-goal utility without fine-tuning.
- **Evidence anchors:** Ablation study shows IQL with Sparse + RG-VLM outperforms RG-VLM alone; semantic solution positions RG-VLM as addressing structural gap in reward inference.
- **Break condition:** If offline dataset contains visually similar but functionally distinct actions, LVLM may assign identical rewards.

### Mechanism 3
- **Claim:** Semantic understanding of task goals enables better generalization to unseen initial states compared to visual similarity metrics.
- **Mechanism:** RG-VLM processes trajectory as narrative sequence, evaluating "progress toward goal" based on semantic cause-and-effect rather than embedding similarity.
- **Core assumption:** Tokenization of task goal effectively conditions visual processing to focus on task-relevant objects.
- **Evidence anchors:** IQL with Sparse and RG-VLM shows only 38.94% reduction in randomized states; RG-VLM leverages reasoning capabilities versus similarity scores; semantic models offer robust alternatives to handcrafted signals.
- **Break condition:** If task description is ambiguous, mechanism may fail to track correct object amidst distractors.

## Foundational Learning

- **Concept: Implicit Q-Learning (IQL)**
  - **Why needed here:** Standard off-policy RL fails in offline settings due to distributional shift. IQL is backbone algorithm used to consume generated rewards without exploring out-of-bounds.
  - **Quick check question:** How does IQL avoid querying out-of-distribution actions during value training? (Answer: By regressing on dataset's value function directly rather than maximizing over actions).

- **Concept: Chain-of-Thought (CoT) Prompting**
  - **Why needed here:** Paper's core innovation is specific prompt structure. Understanding CoT explains why model is queried twice (Analysis → Score).
  - **Quick check question:** Why is generating textual analysis of transition necessary before outputting reward score? (Answer: To force model to resolve visual ambiguities before committing to numerical value).

- **Concept: Reward Shaping & Auxiliary Rewards**
  - **Why needed here:** Paper relies on combining weak dense signal (LVLM) with strong sparse signal (Environment). Understanding how these sum is critical for implementation.
  - **Quick check question:** Why is LVLM reward treated as "auxiliary" rather than sole reward signal? (Answer: To prevent error accumulation from hallucinated LVLM rewards; sparse rewards ensure final goal is actually met).

## Architecture Onboarding

- **Component map:** Data Loader -> Preprocessor -> Reward Engine (LVLM) -> Offline Buffer -> Policy Trainer (IQL)
- **Critical path:** Formatting of concatenated image. If aspect ratio or resizing destroys visual fidelity of small objects, LVLM cannot analyze transitions, resulting in zero-information rewards.
- **Design tradeoffs:**
  - **Window Size (w=8):** Pros - reduces API calls/latency; provides longer temporal context. Cons - larger composite images require aggressive resizing, potentially blurring details.
  - **LVLM Choice:** Proprietary (Gemini/GPT) - high reasoning, high latency/cost. Open (Qwen) - lower cost, but lower accuracy (86.2% vs 93.4%) and slower local inference.
- **Failure signatures:**
  - Reward Hacking: LVLM assigns high scores to "busy" actions if visual change is misinterpreted as progress
  - Temporal Confusion: LVLM fails to distinguish order in concatenated image, assigning "completion" reward to early step
  - Token Overflow: Long trajectories exceeding context window of LVLM
- **First 3 experiments:**
  1. Prompt Validation: Run two-stage prompt on 50 manually labeled transitions to verify zero-shot accuracy on specific environment's visual domain
  2. Window Sensitivity: Compare reward quality for w ∈ {4, 8, 16} to verify tradeoff between context and resolution
  3. IQL Sanity Check: Train IQL on simple "Reach Target" task using only RG-VLM rewards vs. Ground Truth rewards to isolate performance gap

## Open Questions the Paper Calls Out
- Can RG-VLM maintain reward accuracy and policy performance when applied to more complex, long-horizon manipulation benchmarks like Calvin?
- To what extent does integrating advanced reasoning mechanisms (e.g., chain-of-thought prompting) into the LVLM improve the precision of the generated rewards?
- How sensitive is the quality of the generated rewards to the specific phrasing and structure of the two-stage prompt?
- Can open-weights LVLMs bridge the performance gap with proprietary models (like Gemini 1.5 Pro) to offer a viable, cost-effective alternative for reward generation?

## Limitations
- Method's effectiveness entirely dependent on LVLM's pre-trained reasoning capabilities, which cannot be controlled
- Concatenated image processing may destroy resolution of small but critical objects, rendering reward signal useless
- Generalization claims tested only within ALFRED domain of household tasks, unclear if semantic reasoning advantage holds for other domains

## Confidence
- **High Confidence:** Experimental results showing RG-VLM's performance improvement over CLIP and RoboCLIP baselines and lower generalization gap are based on reported data
- **Medium Confidence:** Claim that two-stage prompting improves label quality by forcing semantic grounding is plausible but not directly validated with ablation
- **Low Confidence:** Scalability claim is inferred from results but not rigorously tested across diverse unseen tasks

## Next Checks
1. **Prompt Ablation Test:** Compare RG-VLM's performance using single-step LVLM prompt versus two-stage Chain-of-Thought approach
2. **Window Size Sensitivity:** Systematically test performance and reward quality across range of window sizes (w = 4, 8, 16)
3. **Cross-Domain Transfer:** Evaluate RG-VLM on household robotics tasks with different visual appearances to test semantic reasoning robustness