---
ver: rpa2
title: More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training
  Vision-Language Driving Models
arxiv_id: '2510.04532'
source_url: https://arxiv.org/abs/2510.04532
tags:
- reasoning
- planning
- driving
- priors
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether planning in vision-language model
  (VLM) driving agents is causally driven by generated Chain-of-Thought (CoT) reasoning.
  To enable rigorous causal analysis, the authors create DriveMind, a nuPlan-based
  dataset with plan-aligned CoT reasoning and modular information structure separating
  priors from to-be-reasoned signals.
---

# More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models

## Quick Facts
- arXiv ID: 2510.04532
- Source URL: https://arxiv.org/abs/2510.04532
- Reference count: 29
- Primary result: Training data shows planning relies on textual priors rather than generated Chain-of-Thought reasoning

## Executive Summary
This study investigates whether reasoning in vision-language model (VLM) driving agents causally drives planning decisions. The authors create DriveMind, a nuPlan-based dataset with plan-aligned CoT reasoning and modular information structure that separates priors from to-be-reasoned signals. Using this dataset, they train VLM agents with supervised fine-tuning and reinforcement learning, then evaluate with nuPlan metrics. The primary finding is a reasoning-planning disconnect: removing ego/navigation priors causes large drops in planning performance, while removing CoT produces only minor changes. Attention analysis confirms planning focuses on priors rather than reasoning outputs.

## Method Summary
The authors develop DriveMind, a nuPlan-based dataset with modular information structure separating priors from to-be-reasoned signals and plan-aligned Chain-of-Thought reasoning. They train representative VLM driving agents using supervised fine-tuning and reinforcement learning (GRPO) on this dataset. To evaluate causal relationships, they systematically remove either priors or CoT from the input and measure performance changes. They also introduce a training-free causal probe that measures planning robustness against minor input perturbations, with disproportionately large performance degradation indicating shortcut reliance. Attention analysis reveals where the model focuses during planning decisions.

## Key Results
- Removing ego/navigation priors causes large drops in planning performance
- Removing CoT reasoning produces only minor changes in planning performance
- Attention analysis shows planning focuses primarily on priors rather than CoT reasoning

## Why This Works (Mechanism)
The reasoning-planning disconnect occurs because VLM agents learn to use textual priors as shortcuts for planning decisions rather than causally relying on generated reasoning. When priors are removed, the model loses essential information needed for planning, while CoT removal has minimal impact because the model wasn't actually using the reasoning for planning decisions. The causal probe detects this shortcut learning by measuring sensitivity to input perturbations - models heavily reliant on shortcuts show disproportionately large performance drops when inputs are slightly modified.

## Foundational Learning
- Causal analysis in ML: Needed to understand whether reasoning causally drives planning; Quick check: Compare performance when removing reasoning vs. removing priors
- Chain-of-Thought reasoning: Used to generate step-by-step reasoning; Quick check: Verify CoT outputs align with actual planning decisions
- Attention mechanisms: Reveals what inputs the model focuses on during planning; Quick check: Analyze attention weights when planning with/without priors
- Reinforcement learning for driving: GRPO used for fine-tuning; Quick check: Monitor reward signals during training
- nuPlan dataset: Benchmark for autonomous driving evaluation; Quick check: Validate metrics match industry standards

## Architecture Onboarding

Component map: Input signals -> VLM encoder -> Planning module -> Action output

Critical path: Prior signals → Encoder attention → Planning decision → Action selection

Design tradeoffs: Modular input structure enables systematic ablation but may not reflect real-world signal integration; CoT generation adds interpretability but may not be causally used

Failure signatures: Large performance drop when priors removed but small drop when CoT removed indicates shortcut learning; Attention focused on priors rather than reasoning outputs confirms disconnect

First experiments:
1. Ablation study: Remove priors vs. remove CoT and measure performance changes
2. Attention visualization: Map attention weights during planning with different input configurations
3. Causal probe testing: Apply input perturbations and measure robustness to detect shortcut reliance

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset may not capture all real-world driving scenarios
- Causal analysis limited to controlled ablation experiments
- Causal probe only measures robustness, not the full causal structure

## Confidence

| Claim | Confidence |
|-------|------------|
| Reasoning-planning disconnect exists | High |
| Models use priors as shortcuts | High |
| Causal probe detects shortcut learning | Medium |

## Next Checks

1. Validate causal probe on additional VLM driving models beyond those tested
2. Test whether incorporating explicit reasoning supervision improves causal fidelity
3. Evaluate model performance on out-of-distribution driving scenarios to test generalization