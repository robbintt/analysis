---
ver: rpa2
title: Dialogue Is Not Enough to Make a Communicative BabyLM (But Neither Is Developmentally
  Inspired Reinforcement Learning)
arxiv_id: '2510.20358'
source_url: https://arxiv.org/abs/2510.20358
tags:
- language
- reward
- data
- learning
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether small language models pre-trained
  exclusively on child-caregiver dialogue data can acquire communicative competence,
  and whether reinforcement learning can further enhance this ability. The authors
  train a 135M-parameter model (llamalogue) on 10M lexical tokens of CHILDES dialogue
  triplets, then fine-tune it using two approaches: Direct Preference Optimization
  (DPO) with either naturalistic or synthetic dialogue pairs, and Proximal Policy
  Optimization (PPO) with various reward functions.'
---

# Dialogue Is Not Enough to Make a Communicative BabyLM (But Neither Is Developmentally Inspired Reinforcement Learning)

## Quick Facts
- arXiv ID: 2510.20358
- Source URL: https://arxiv.org/abs/2510.20358
- Reference count: 39
- Primary result: Small dialogue-only models struggle to generalize communicative competence to formal linguistic benchmarks, even with RL fine-tuning

## Executive Summary
This study investigates whether small language models pre-trained exclusively on child-caregiver dialogue data can acquire communicative competence, and whether reinforcement learning can further enhance this ability. The authors train a 135M-parameter model (llamalogue) on 10M lexical tokens of CHILDES dialogue triplets, then fine-tune it using two approaches: Direct Preference Optimization (DPO) with either naturalistic or synthetic dialogue pairs, and Proximal Policy Optimization (PPO) with various reward functions. The base model underperforms on most BabyLM benchmarks but excels at dialogue continuation prediction in minimal pair settings (63-64% accuracy). DPO fine-tuning on naturalistic data further improves dialogue prediction accuracy to 67-68%, while PPO shows mixed results with no consistent gains on formal benchmarks. None of the fine-tuning strategies improve performance on syntactic or semantic benchmarks, suggesting that current reinforcement learning approaches are not effective at generalizing communicative competence to broader linguistic abilities.

## Method Summary
The authors train a 135M Llama model on 10M lexical tokens from CHILDES dialogue triplets, then apply two fine-tuning approaches: DPO with minimal pairs from naturalistic or synthetic dialogues, and PPO with four different reward functions (BLEU-1, semantic similarity, LLM score, teacher confidence). The base model shows strong performance on dialogue continuation prediction but weak results on formal linguistic benchmarks. DPO improves dialogue prediction accuracy while PPO results are mixed and inconsistent. Neither approach successfully transfers communicative competence to broader linguistic abilities measured by BabyLM benchmarks.

## Key Results
- Base llamalogue model achieves 63-64% accuracy on dialogue continuation minimal pairs but underperforms on BabyLM benchmarks
- DPO fine-tuning on naturalistic data improves dialogue prediction accuracy to 67-68%
- PPO shows inconsistent results with no reliable improvement on formal benchmarks
- Neither fine-tuning approach improves performance on syntactic or semantic benchmarks
- Dialogue-only training is insufficient for developing broader linguistic competence

## Why This Works (Mechanism)
The study's mechanism relies on training language models exclusively on child-directed speech dialogue data, then attempting to improve communicative appropriateness through reinforcement learning. The hypothesis is that models trained on authentic conversational data will develop better pragmatic understanding, which can be enhanced through preference-based or policy optimization fine-tuning.

## Foundational Learning
- **CHILDES corpus**: Why needed - provides authentic child-caregiver dialogue data for developmentally grounded training. Quick check - verify dialogue triplets contain ≥2 speakers and ≥5 lexical words.
- **Direct Preference Optimization (DPO)**: Why needed - aligns model outputs with human preferences using minimal pairs. Quick check - monitor reward separation during training.
- **Proximal Policy Optimization (PPO)**: Why needed - policy-based RL method for improving communicative appropriateness. Quick check - track reward stability and variance.
- **Minimal pair evaluation**: Why needed - controlled way to test dialogue continuation appropriateness. Quick check - achieve ~63% baseline accuracy on held-out pairs.

## Architecture Onboarding

**Component Map**: CHILDES triplets -> 135M Llama pre-training -> DPO/PPO fine-tuning -> BabyLM benchmarks + dialogue MP evaluation

**Critical Path**: Pre-training (10 epochs, 10M tokens) → DPO/PPO fine-tuning (10/3 epochs) → Evaluation on dialogue MP and BabyLM benchmarks

**Design Tradeoffs**: Extremely limited training data (10M tokens) maximizes developmental realism but severely constrains model capacity. Using only dialogue data ensures communicative focus but omits broader linguistic patterns found in narrative text.

**Failure Signatures**: PPO instability (reward drops/crashes), poor generalization to formal benchmarks despite good dialogue performance, reward separation degradation in DPO.

**First Experiments**: 1) Pre-train 135M Llama on CHILDES for 10 epochs and verify ~63% dialogue MP accuracy. 2) Apply DPO on naturalistic pairs and confirm improvement to ~68% while maintaining reward separation. 3) Test PPO with semantic similarity reward and monitor for consistent reward increases.

## Open Questions the Paper Calls Out
None

## Limitations
- Extremely limited training data (10M lexical tokens) may artificially constrain model performance on formal benchmarks
- Synthetic dialogue pairs may not capture authentic conversational complexity
- Lack of comparison to larger models or alternative fine-tuning approaches
- Difficulty distinguishing between data limitations and RL method effectiveness

## Confidence
- **High Confidence**: Base model's strong dialogue prediction performance (63-64%) and DPO's improvement to 67-68% are well-demonstrated
- **Medium Confidence**: Neither RL approach improves formal benchmark performance, though data limitations make this difficult to assess definitively
- **Low Confidence**: Claims about RL methods' fundamental ineffectiveness at generalizing communicative competence are hard to substantiate given data constraints

## Next Checks
1. Verify pre-trained model achieves ~63-64% accuracy on held-out dialogue minimal pairs before any fine-tuning
2. Apply DPO on naturalistic pairs for 10 epochs and confirm improvement to ~67-68% on dialogue MP while monitoring reward separation
3. Test PPO with semantic similarity reward and compare against teacher confidence reward to identify most stable learning signal