---
ver: rpa2
title: Foundations for Risk Assessment of AI in Protecting Fundamental Rights
arxiv_id: '2507.18290'
source_url: https://arxiv.org/abs/2507.18290
tags:
- rights
- risk
- legal
- fundamental
- deployment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a conceptual framework for qualitative AI risk
  assessment focused on fundamental rights protection. The authors integrate definitional
  balancing with defeasible reasoning to address legal compliance complexities under
  the EU AI Act.
---

# Foundations for Risk Assessment of AI in Protecting Fundamental Rights

## Quick Facts
- **arXiv ID**: 2507.18290
- **Source URL**: https://arxiv.org/abs/2507.18290
- **Reference count**: 38
- **Primary result**: Presents a conceptual framework using definitional balancing and defeasible reasoning for qualitative AI risk assessment focused on fundamental rights protection under the EU AI Act.

## Executive Summary
This paper develops a conceptual framework for assessing AI-related legal risks to fundamental rights through definitional balancing and defeasible reasoning. The approach treats rights as defeasible rules with limitations as defeaters, resolved through proportionality analysis. By decomposing high-level system purposes into specific deployment scenarios, the framework enables identification of potential legal violations and multi-layered rights impacts. The method distinguishes between high-risk AI systems and General Purpose AI systems, addressing their different applicability contexts. While remaining conceptual without formal algorithms, the framework aims to support responsible AI governance through practical compliance tools and enable more operative risk assessment models.

## Method Summary
The framework employs an eight-step qualitative analysis: (1) define deployment domain; (2) identify scenarios; (3) describe scenarios by features; (4) identify legal obligations; (5) identify relevant fundamental rights; (6) assess rights promotion/demotion; (7) establish preference orderings using ⊗ operator; (8) apply defeasible reasoning patterns to determine adopted rights. Legal risk is quantified via "Degree of Right Impact" = Ξ − Δ, where Ξ sums weighted positions of adopted rights and Δ sums weighted positions of demoted rights. The framework uses a "what-if" approach to drill down from abstract domains to specific applications, mapping obligations and rights to lower-level scenarios to expose hidden legal risks.

## Key Results
- Integrates definitional balancing with defeasible reasoning to address legal compliance complexities under the EU AI Act
- Treats fundamental rights as defeasible rules with limitations as defeaters, resolved through proportionality analysis
- Proposes principles for representing rights promotion/demotion and prioritization within deployment contexts
- Aims to enable more operative risk assessment models and support responsible AI governance through practical compliance tools

## Why This Works (Mechanism)

### Mechanism 1: Definitional Balancing via Proportionality
If legal conflicts between fundamental rights are treated as context-sensitive rather than absolute, resolution is possible through general rule derivation. The system employs definitional balancing using proportionality analysis (legitimate goal, suitability, necessity) to establish conditional rules for when a right may justifiably limit another. This moves beyond ad-hoc case-by-case decisions to generalizable principles. Core assumption: Rights are not absolute and their boundaries can be normatively defined through structured evaluation.

### Mechanism 2: Layered What-If Scenario Analysis
Risk magnitude is more accurately assessed by decomposing high-level system purposes into specific "deployment scenarios" (S1...Sn). A "what-if" approach drills down from abstract domains to specific applications, mapping obligations and rights specifically to these lower-level scenarios. This exposes "hidden" legal risks invisible in high-level audits. Core assumption: Legal risk is not a monolithic property of an AI model but an emergent property of its specific context of use.

### Mechanism 3: Defeasible Reasoning for Rights Adoption
The framework dynamically selects which right to "adopt" (prioritize) in a given scenario by treating rights as defeasible rules subject to exceptions. The mechanism uses inference patterns (e.g., `IF S => Demotes(S, Ri) THEN S => Choice(S, Rj)`) to automatically shift priority to the next available right in the sequence. Core assumption: A stable preference ordering (`⊗`) can be established for the rights involved in a specific conflict.

## Foundational Learning

- **Concept: Defeasibility vs. Monotonicity**
  - Why needed: Standard logic (monotonic) adds conclusions without ever retracting them. Legal reasoning is non-monotonic; new evidence (e.g., a "pandemic" exception) can invalidate a previous conclusion about privacy.
  - Quick check: If I add the fact "Public Emergency," does the system retract the conclusion "Enforce Strict Data Protection"?

- **Concept: Proportionality Analysis**
  - Why needed: This is the "balancing" logic. It determines if a right can be limited. The framework relies on this to define conditions for "demoting" a right.
  - Quick check: Does the deployment scenario pass the three-pronged test: (1) Legitimate Aim, (2) Suitability, (3) Necessity?

- **Concept: Rights as Boolean Combinations**
  - Why needed: The paper models complex rights (e.g., Privacy) as Boolean combinations of basic rights (e.g., `data_protection AND autonomy`). This reduction is necessary for the computational "Degree of Right Impact" calculation.
  - Quick check: Can I decompose "Fairness" into a set of logical AND/OR operators on base-level data attributes?

## Architecture Onboarding

- **Component map**: Scenario Generator -> Normative Mapper -> Impact Assessor -> Defeasible Inference Engine -> Risk Optimizer
- **Critical path**: The transition from Section 6.3 (Rights Promotion/Demotion) to Section 6.5 (Defeasible Reasoning). You must accurately classify a scenario's impact (Promote/Demote) before the logical reasoner can function.
- **Design tradeoffs**: Granularity vs. Computation (fine-grained scenarios create intractable search space); Qualitative vs. Quantitative (Degree calculation uses binary inputs that may mask severity differences)
- **Failure signatures**: Undefined Impact (system returns `undefined` for `Promotes/Demotes`); Priority Loop (Choice logic oscillates between two rights)
- **First 3 experiments**:
  1. Implement the "Right adoption 1/2/3" inference rules in a logic programming language to verify they handle the "Pandemic" example correctly
  2. Take a single High-Risk AI (e.g., Scholarship Allocation) and generate 3 distinct scenarios to verify if the Degree calculation correctly identifies the lowest-risk configuration
  3. Introduce a "defeater" into a solved scenario and observe if the Defeasible Engine correctly retracts the "Choice" of the demoted right

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which specific logical systems and semantics are most suitable for formalizing the integration of definitional balancing and defeasible reasoning into a complete formal model?
- Basis: The conclusion states that future work involves "a detailed exploration of the suitable logical systems and semantics" to develop a "more complete formal model."
- Why unresolved: The current work presents the framework conceptually and informally (using "gentle" logic) but stops short of committing to a specific formal logical system.

### Open Question 2
- Question: What are the formal properties (e.g., complexity, soundness) of the proposed reasoning patterns for rights adoption and collision?
- Basis: The conclusion identifies the need for "a more thorough investigation of the properties of the proposed reasoning patterns."
- Why unresolved: The paper provides inference rules for rights adoption but does not analyze their logical consequences or computational feasibility.

### Open Question 3
- Question: How can the conceptual framework be translated into effective algorithms for automated risk assessment?
- Basis: The abstract and conclusion list the "development of effective algorithms for automated risk assessment" as a primary goal of future research.
- Why unresolved: While the paper defines a "Degree of Right Impact," it does not provide the algorithmic means to compute this degree or perform the optimization automatically.

### Open Question 4
- Question: How can the "what-if" analysis be scaled to effectively manage the "superset" of deployment scenarios inherent to General Purpose AI (GPAI) systems?
- Basis: Section 3.3 notes that the deployment scenarios for GPAIs form a "superset" of those for high-risk systems, suggesting a scalability challenge for the proposed analysis method.
- Why unresolved: The methodology requires defining specific scenarios, which becomes computationally and cognitively difficult when the potential applications are unbounded.

## Limitations
- Framework remains conceptual with no implemented algorithms or empirical validation
- Critical gaps exist in deriving Promotes/Demotes assignments from scenario features and establishing systematic methods for determining right priority orderings
- Defeasible consequence relation is left undefined, and no quantitative metrics are provided for actual risk assessment

## Confidence

- **High Confidence**: Definitional balancing mechanism using proportionality analysis is well-established in legal theory and technically sound
- **Medium Confidence**: Layered scenario analysis approach is methodologically appropriate for EU AI Act compliance, though practical implementation challenges remain
- **Low Confidence**: Defeasible reasoning system for dynamic rights adoption lacks concrete algorithmic specification and depends heavily on undefined preference orderings

## Next Checks

1. **Logic Validation**: Implement the three "Right adoption" inference patterns in a logic programming environment to verify correct handling of conflicting rights scenarios
2. **Scenario Expansion Test**: Apply the framework to a concrete High-Risk AI case (e.g., scholarship allocation) and validate the Degree calculation across multiple deployment configurations
3. **Defeater Sensitivity Analysis**: Test the system's response to introducing new legal exceptions or defeaters into solved scenarios to verify proper retraction of previously adopted rights