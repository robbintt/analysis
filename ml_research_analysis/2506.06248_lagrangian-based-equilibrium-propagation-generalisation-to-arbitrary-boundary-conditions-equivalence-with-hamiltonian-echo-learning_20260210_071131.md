---
ver: rpa2
title: 'Lagrangian-based Equilibrium Propagation: generalisation to arbitrary boundary
  conditions & equivalence with Hamiltonian Echo Learning'
arxiv_id: '2506.06248'
source_url: https://arxiv.org/abs/2506.06248
tags:
- boundary
- learning
- gradient
- hamiltonian
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper establishes a fundamental theoretical connection between
  two seemingly distinct approaches to learning in physical systems: Equilibrium Propagation
  (EP) and Hamiltonian Echo Learning (HEL). The authors introduce Generalized Lagrangian
  Equilibrium Propagation (GLEP), which extends EP''s variational principles to time-varying
  inputs by carefully handling arbitrary boundary conditions.'
---

# Lagrangian-based Equilibrium Propagation: generalisation to arbitrary boundary conditions & equivalence with Hamiltonian Echo Learning

## Quick Facts
- arXiv ID: 2506.06248
- Source URL: https://arxiv.org/abs/2506.06248
- Reference count: 40
- Primary result: Proves theoretical equivalence between Equilibrium Propagation and Hamiltonian Echo Learning through Generalized Lagrangian Equilibrium Propagation with Parametric Final Value Problems

## Executive Summary
This paper establishes a fundamental theoretical connection between two seemingly distinct approaches to learning in physical systems: Equilibrium Propagation (EP) and Hamiltonian Echo Learning (HEL). The authors introduce Generalized Lagrangian Equilibrium Propagation (GLEP), which extends EP's variational principles to time-varying inputs by carefully handling arbitrary boundary conditions. They demonstrate that different boundary condition choices lead to dramatically different computational properties, with most formulations suffering from intractable boundary residual terms. The key insight is that the Parametric Final Value Problem (PFVP) formulation, combined with time-reversibility, eliminates these problematic residuals while maintaining desirable implementation properties. Through the Legendre transformation, the authors prove that HEL emerges as a special case of GLEP, revealing that HEL's distinctive properties—local learning and forward-only computation—arise naturally from variational principles rather than being artifacts of Hamiltonian mechanics.

## Method Summary
The authors develop Generalized Lagrangian Equilibrium Propagation (GLEP) as a variational extension of Equilibrium Propagation to handle time-varying inputs. They formalize different boundary condition formulations (Constant Initial Value Problem and Parametric Final Value Problem) and analyze their gradient computation properties. The critical insight is that PFVP, combined with time-reversibility, eliminates intractable boundary residuals that plague standard formulations. They then establish the mathematical equivalence between GLEP with PFVP and Recurrent Hamiltonian Echo Learning through the Legendre transformation, demonstrating that HEL's distinctive properties emerge naturally from variational principles.

## Key Results
- GLEP extends EP's variational principles to continuous time-varying trajectories by minimizing an action functional
- PFVP formulation eliminates intractable boundary residual terms that appear in standard CIVP approaches
- HEL is mathematically equivalent to GLEP-PFVP, derived via Legendre transformation
- Different boundary condition choices dramatically affect computational tractability and implementation properties

## Why This Works (Mechanism)

### Mechanism 1: Variational Extension to Trajectories
GLEP extends EP from static fixed points to time-varying trajectories by minimizing an action functional $A = \int L(s, \dot{s}, \theta) dt$ rather than finding a single state $\hat{s}$ that minimizes energy $E$. The gradient is estimated by comparing "free" (natural dynamics) and "nudged" (perturbed by cost) trajectories, assuming the system can be described by a Lagrangian $L$ satisfying the principle of least action.

### Mechanism 2: Boundary Residual Cancellation via PFVP
Standard CIVP formulations produce complex gradient terms at the final time boundary $t=T$ (boundary residuals) involving high-dimensional parameter derivatives. PFVP constrains the trajectory using the final state of the free phase, and combined with time-reversibility, this cancels the problematic residuals, reducing the gradient estimate to a simple integral of Lagrangian differences.

### Mechanism 3: Hamiltonian-Lagrangian Equivalence
Through the Legendre transformation, reversible Lagrangian systems can be mapped to Hamiltonian systems. The "echo" phase in HEL (running backward with momentum flip and nudging) physically implements the time-reversed integration required to solve PFVP in GLEP without explicit variational solvers, making HEL a special case of GLEP.

## Foundational Learning

- **Concept: Euler-Lagrange Equations**
  - **Why needed here:** This is the mathematical engine of GLEP. You must understand that minimizing the action functional $A = \int L dt$ results in a differential equation (EL equation) that describes the system's motion.
  - **Quick check question:** Can you explain why integrating the Euler-Lagrange equation minimizes the "action" of a trajectory?

- **Concept: Legendre Transformation**
  - **Why needed here:** This is the bridge connecting the paper's two main worlds: Lagrangian formulations (GLEP) and Hamiltonian formulations (HEL/RHEL). Understanding this clarifies why "momentum flip" ($\Sigma_z$) in HEL corresponds to "velocity reversal" in GLEP.
  - **Quick check question:** How does one derive the Hamiltonian $H(q, p)$ from a Lagrangian $L(q, \dot{q})$, and what does the variable $p$ represent physically?

- **Concept: Contrastive Learning (Free vs. Nudged Phases)**
  - **Why needed here:** The entire gradient estimation mechanism relies on comparing a "free" system (natural inference) against a "nudged" system (inference + small error signal).
  - **Quick check question:** In Equilibrium Propagation, why is the gradient of the cost function proportional to the difference in states (or parameters) between the free and nudged equilibria (or trajectories)?

## Architecture Onboarding

- **Component map:** Input $x(t)$ and target $y(t)$ → Core Physics (reversible system defined by $L$ or $H$) → Free Phase (integrate forward from $t=0$ to $T$) → Echo/Nudged Phase (flip momentum/velocity, integrate backward from $T$ to $0$ with cost injection) → Gradient Computer (calculates $\Delta \theta$)

- **Critical path:** The implementation of the Parametric Final Value Problem (PFVP). You must ensure that the "nudged" trajectory is initialized exactly at the *final state* of the "free" trajectory (with reversed velocity) to cancel boundary residuals.

- **Design tradeoffs:**
  - **CIVP vs. PFVP:** CIVP is easier to simulate (standard ODE solver) but yields unusable gradients due to boundary terms. PFVP requires strict reversibility and careful boundary handling but produces clean gradients.
  - **Digital vs. Analog:** On digital hardware, you must store the trajectory or re-compute it (BPTT-style). On analog hardware, the physics *is* the computation, but you need a mechanism to physically reverse the dynamics (time-reversibility) and inject the nudging signal.

- **Failure signatures:**
  - **Gradient drift:** If the system is not perfectly reversible (e.g., numerical damping in simulation or dissipation in analog hardware), the boundary residuals will not cancel, leading to biased gradients.
  - **Instability:** If the cost nudging $\beta$ is too large, the echo trajectory will diverge too far from the free trajectory, breaking the infinitesimal assumption of the gradient derivation.

- **First 3 experiments:**
  1. **Reversibility Test:** Run the system forward, flip the sign of the momentum/velocity, and run backward. Check if the system exactly retraces its path to the initial state. (Do this *before* implementing learning).
  2. **Static Gradient Check:** Implement GLEP on a simple linear system (e.g., harmonic oscillator) with a fixed input. Compare the gradient estimate from GLEP against finite-difference gradients on the cost function to verify the PFVP math.
  3. **Trajectory Tracking:** Train the system on a simple time-varying waveform (e.g., sine wave) to match a target phase. Monitor if the "forward-only" nature of the update holds (i.e., does gradient accuracy depend on the length of the trajectory $T$?).

## Open Questions the Paper Calls Out
None

## Limitations
- Dependence on strict reversibility conditions, which are rarely satisfied in real-world dissipative systems
- Limited numerical demonstrations of PFVP superiority over CIVP in practical scenarios
- Scalability concerns for large, complex systems with significant non-conservative components

## Confidence
- **High confidence** in the mathematical equivalence between GLEP-PFVP and HEL (Theorem 5), as this follows from well-established Legendre transformation properties
- **Medium confidence** in the practical superiority of PFVP over CIVP, as the analysis is rigorous but numerical demonstrations are limited
- **Low confidence** in the scalability of this approach to large, complex systems with significant non-conservative components

## Next Checks
1. **Reversibility robustness test:** Systematically introduce small amounts of dissipation or numerical damping to quantify how quickly gradient estimates degrade as reversibility is violated
2. **Comparison with BPTT baselines:** Implement a direct BPTT version of HEL on the same hardware platform to measure actual computational efficiency gains
3. **Boundary condition sensitivity:** Test alternative boundary condition formulations (e.g., mixed initial-final conditions) to map the full tradeoff space between computational tractability and physical realizability