---
ver: rpa2
title: Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population
  Health Management
arxiv_id: '2509.09772'
source_url: https://arxiv.org/abs/2509.09772
tags:
- risk
- conformal
- subgroup
- safe
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses safe and fair decision-making in population\
  \ health management (PHM) programs, where care coordinators must choose among coordination\
  \ actions (e.g., outreach modality, service routing) while minimizing near-term\
  \ risk of adverse events (e.g., emergency department visits). To tackle this, the\
  \ authors propose HACO (Hybrid Adaptive Conformal Offline Reinforcement Learning),\
  \ a two-stage framework that decouples risk calibration from preference optimization:\
  \ a conformal risk model first identifies a safe action set at a tunable risk level\
  \ \u03B1, and a preference policy is then trained on this subset."
---

# Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management

## Quick Facts
- arXiv ID: 2509.09772
- Source URL: https://arxiv.org/abs/2509.09772
- Reference count: 21
- Key result: Conformal gating reduces observed harm rate from ~1.82% to ~1.15% while retaining ~88% of steps as safe

## Executive Summary
This paper introduces HACO (Hybrid Adaptive Conformal Offline Reinforcement Learning), a two-stage framework for safe and fair decision-making in population health management. The approach decouples risk calibration from preference optimization: a conformal risk model first identifies a safe action set at a tunable risk level α, and a preference policy is then trained on this subset. Evaluated on 2.77 million steps from Medicaid operations, HACO's risk model achieves an AUC of ~0.81 and significantly reduces harm rates while maintaining coverage. Subgroup analyses reveal systematic differences in estimated value across demographics, highlighting the need for fairness auditing in healthcare decision support.

## Method Summary
HACO operates through a two-stage process where a conformal risk model first filters unsafe actions based on a threshold derived from held-out calibration data, and then a preference policy is trained only on the remaining safe actions. The risk model uses logistic regression to predict harm probability from lightweight features, while the preference model employs multinomial logistic regression. Policy evaluation uses version-agnostic Fitted Q Evaluation with linear function approximation. The framework includes comprehensive subgroup auditing to detect disparities across age, sex, and race demographics.

## Key Results
- Risk model achieves AUC of ~0.81 on held-out test data
- At α=0.10, reduces observed harm rate from ~1.82% to ~1.15% while retaining ~88% of steps
- Systematic differences in estimated value across age, sex, and race subgroups
- Conformal calibration provides conservative safety threshold with finite-sample coverage guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conformal calibration provides a distribution-free safety threshold that controls marginal risk at a user-specified level α
- Mechanism: Conformal prediction uses a held-out calibration slice to select a threshold τ such that at least (1−α) of calibration scores fall below τ, then masks any action whose predicted harm probability exceeds this threshold
- Core assumption: The calibration and deployment data are exchangeable
- Evidence anchors: [abstract] "derives a conformal threshold to mask unsafe actions at a target risk level"; [section 4.2] "compute scores S_j = p̂(harm|s_j) and select a threshold τ(α) such that Pr(S ≤ τ) ≥ 1−α"
- Break condition: Distribution shift between calibration and deployment violates exchangeability

### Mechanism 2
- Claim: Decoupling risk calibration from preference optimization preserves model flexibility while enforcing an auditable safety constraint
- Mechanism: The risk model produces a safe action set via conformal gating; the preference model is trained only on this safe subset, optimizing for higher-reward actions without being directly penalized for safety
- Core assumption: The safe set retains sufficient action diversity and coverage of high-value actions
- Evidence anchors: [abstract] "a two-stage framework that decouples risk calibration from preference optimization"; [section 5.1] "This separation gives a tunable safety dial without constraining the preference model class"
- Break condition: If α is set too low, the safe set becomes very small and degrades performance

### Mechanism 3
- Claim: Version-agnostic Fitted Q Evaluation (FQE) with a linear function approximator provides stable, comparable policy value estimates across different learned policies
- Mechanism: FQE iteratively applies Bellman backups using ridge regression to learn a Q-function from offline data, then estimates V_0 under each policy
- Core assumption: A linear function class is sufficient to capture meaningful value differences
- Evidence anchors: [abstract] "evaluate policies with a version-agnostic fitted Q evaluation (FQE) on stratified subsets"; [section 5.3] "This FQE is robust to environment/library drift"
- Break condition: Distributional shift between behavior and evaluation policies or underparameterized Q-function

## Foundational Learning

- **Conformal Prediction**: Provides finite-sample, distribution-free coverage guarantees for safety thresholds; critical when deployment decisions affect patient outcomes. Quick check: If calibration data is not exchangeable with deployment data, what happens to the coverage guarantee?

- **Offline Reinforcement Learning**: Enables learning policies from fixed historical trajectories without prospective experimentation; essential in healthcare where online exploration is unethical. Quick check: Why do conservative methods (e.g., CQL, IQL) help mitigate extrapolation error in offline RL?

- **Off-Policy Evaluation (FQE)**: Allows comparison of candidate policies to baselines without deploying them; supports safe model selection. Quick check: How does FQE differ from importance sampling, and why might it be more stable in high-dimensional action spaces?

## Architecture Onboarding

- Component map: Data Loader -> Risk Model -> Conformal Calibrator -> Safe Set Masker -> Preference Model -> FQE Evaluator -> Subgroup Auditor
- Critical path: 1. Load and normalize trajectories → 2. Train risk model on train split → 3. Compute conformal threshold on calibration split → 4. Filter to safe subset → 5. Train preference model → 6. Evaluate with FQE → 7. Audit subgroup performance
- Design tradeoffs:
  - Safety vs. coverage: Lower α yields stricter safety but reduces safe-set size and potentially policy value
  - Model complexity vs. interpretability: Lightweight logistic models prioritize auditability and stability
  - FQE function class: Linear approximator ensures comparability and robustness but may underfit
- Failure signatures:
  - Calibration CDF overly flat → Risk model has poor discrimination (low AUC)
  - Safe coverage drops sharply as α decreases → Risk scores are not well-separated
  - Subgroup calibration curves deviate from identity line → Systematic over/under-estimation of risk
  - FQE values identical across all policies → Linear Q-function underfits
- First 3 experiments:
  1. Sweep α from 0.05 to 0.20; plot safe coverage, observed harm rate, and FQE V_0
  2. Compare risk model classes (logistic regression vs. gradient-boosted trees) on AUC and calibration
  3. Run subgroup-specific calibration to test whether per-group thresholds reduce observed disparities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating recurrent or transformer-based architectures into the preference model significantly improve policy ranking compared to the current lightweight logistic regression?
- Basis in paper: [explicit] The authors explicitly list "richer representation learning (e.g., recurrent or transformer architectures)" as a limitation and area for future work
- Why unresolved: The current study deliberately uses a simple multinomial logistic regression; complex architectures have not been evaluated
- What evidence would resolve it: Experiments training HACO with deep sequence models on the same dataset, showing improved FQE values

### Open Question 2
- Question: How can conformal calibration be adapted to provide conditional (context-specific) coverage in sequential settings rather than relying solely on marginal guarantees?
- Basis in paper: [explicit] The limitations section states that "contextual or conditional coverage remains an open area for future adaptation in sequential settings"
- Why unresolved: The current framework guarantees safety only marginally, which may not hold equally for all patient subgroups or specific states
- What evidence would resolve it: A modified conformal procedure that maintains valid coverage rates across pre-defined subgroups or state clusters

### Open Question 3
- Question: Can fairness constraints or regularization be integrated into the preference learning stage to reduce the systematic differences in episodic returns observed across demographic subgroups?
- Basis in paper: [inferred] The results show "systematic differences in estimated value across demographics" but the paper only audits rather than mitigates these disparities
- Why unresolved: While the pipeline supports auditing, it does not currently include algorithmic mechanisms to enforce equity
- What evidence would resolve it: A variant of the preference policy that minimizes performance variance across subgroups while maintaining safety gating

## Limitations
- Conformal safety guarantee depends critically on exchangeability between calibration and deployment data
- Linear FQE function class may underfit complex value functions, potentially masking meaningful policy differences
- The paper documents but does not test mitigation strategies for observed subgroup disparities

## Confidence
- **High confidence**: Conformal risk calibration mechanism and empirical performance (AUC ~0.81, harm reduction from ~1.82% to ~1.15%)
- **Medium confidence**: Decoupling safety from preference optimization as a generalizable design pattern; FQE stability claims
- **Low confidence**: Generalization of safety guarantees under distribution shift; adequacy of linear Q-function for capturing value differences

## Next Checks
1. **Distribution shift stress test**: Simulate population changes and measure degradation in calibration coverage and risk prediction accuracy
2. **Richness of value estimation**: Re-run FQE with a nonlinear function approximator and compare value estimates to assess potential underfitting
3. **Fairness mitigation trial**: Implement subgroup-specific conformal thresholds and evaluate impact on observed harm disparities across demographics