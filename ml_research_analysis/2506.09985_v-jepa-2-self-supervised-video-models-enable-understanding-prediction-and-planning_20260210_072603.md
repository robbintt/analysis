---
ver: rpa2
title: 'V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and
  Planning'
arxiv_id: '2506.09985'
source_url: https://arxiv.org/abs/2506.09985
tags:
- v-jepa
- video
- training
- arxiv
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents V-JEPA 2, a self-supervised video model trained
  on over 1 million hours of internet video that achieves state-of-the-art performance
  on motion understanding (77.3 top-1 accuracy on Something-Something v2), action
  anticipation (39.7 recall-at-5 on Epic-Kitchens-100), and video question answering
  (84.0 on PerceptionTest, 76.9 on TempCompass) at the 8 billion parameter scale.
  The model uses a joint-embedding predictive architecture with mask-denoising in
  representation space, scaled through larger models (up to 1 billion parameters),
  longer training, and progressive resolution training.
---

# V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning

## Quick Facts
- arXiv ID: 2506.09985
- Source URL: https://arxiv.org/abs/2506.09985
- Reference count: 40
- Primary result: V-JEPA 2 achieves state-of-the-art 77.3 top-1 accuracy on Something-Something v2 at 8B parameters, plus zero-shot robot control via post-training on 62 hours of robot data

## Executive Summary
This paper presents V-JEPA 2, a self-supervised video model trained on over 1 million hours of internet video that achieves state-of-the-art performance on motion understanding, action anticipation, and video question answering at the 8 billion parameter scale. The model uses a joint-embedding predictive architecture with mask-denoising in representation space, scaled through larger models (up to 1 billion parameters), longer training, and progressive resolution training. After pre-training, V-JEPA 2 is post-trained with less than 62 hours of robot interaction data to create V-JEPA 2-AC, an action-conditioned world model that enables zero-shot robot control for prehensile manipulation tasks without task-specific training or rewards.

## Method Summary
V-JEPA 2 uses a two-stage training approach: pre-training a joint-embedding predictive architecture on curated web-scale video data using mask-denoising in representation space, followed by action-conditioned post-training on limited robot interaction data. The pre-training phase learns general visual representations from 1M+ hours of diverse video without action labels using an encoder-predictor architecture with exponential moving average teachers. The post-training phase freezes the encoder and trains only an action-conditioned predictor on 62 hours of robot data to learn action dynamics. For zero-shot robot control, the model uses model-predictive control with cross-entropy method to optimize action sequences that minimize L1 distance between predicted and goal representations in latent space.

## Key Results
- State-of-the-art 77.3 top-1 accuracy on Something-Something v2 motion understanding benchmark
- 39.7 recall-at-5 on Epic-Kitchens-100 action anticipation task
- 84.0 on PerceptionTest and 76.9 on TempCompass video question answering benchmarks
- Zero-shot robot manipulation success rates: 25-45% for grasping different objects, 25% for pick-and-place tasks
- Planning efficiency: 16 seconds per action vs 4 minutes for video-generation baselines

## Why This Works (Mechanism)

### Mechanism 1: Representation-Space Prediction Filters Predictable Structure
Predicting in learned representation space rather than pixel space yields representations that capture predictable aspects of scenes while ignoring irrelevant details. The model encodes video patches via an encoder, then a predictor estimates representations of masked regions using an L1 loss against an EMA teacher. This forces the encoder to produce representations where semantically meaningful content (object motion, actions) is predictable, while unpredictable pixel-level details (e.g., grass blade positions) are abstracted away.

### Mechanism 2: Stage-Wise Learning Separates Observation Knowledge from Action Dynamics
Pre-training on internet video followed by action-conditioned post-training on limited robot data yields better generalization than end-to-end training on interaction data alone. Stage 1 learns general visual representations from 1M+ hours of diverse video without action labels. Stage 2 freezes the encoder and trains only an action-conditioned predictor on 62 hours of robot data. This decouples "what the world looks like and how it typically moves" from "how my actions affect it."

### Mechanism 3: Planning via Learned Energy Minimization Enables Zero-Shot Generalization
Optimizing action sequences to minimize L1 distance between predicted and goal representations in latent space transfers across environments without task-specific training. Given current state representation and goal representation, the model samples action trajectories, autoregressively predicts future representations, and selects actions minimizing L1 distance via cross-entropy method. The smooth energy landscape enables gradient-free optimization.

## Foundational Learning

- **Vision Transformer (ViT) Patchification**: V-JEPA 2 operates on video by dividing clips into tubelets (2×16×16 spatiotemporal patches) that become transformer tokens. Understanding token structure is essential for interpreting masking strategies and computational costs.
  - Quick check question: Given a 16-frame video at 256×256 resolution with tubelet size 2×16×16, how many tokens does the encoder process per clip?

- **Exponential Moving Average (EMA) Teacher Networks**: The prediction target is not a fixed target but an EMA-updated copy of the encoder. This prevents collapse and stabilizes training at scale. Understanding EMA decay rates and update schedules is critical for reproduction.
  - Quick check question: Why does using an EMA teacher prevent representation collapse in contrastive-style objectives?

- **Model-Predictive Control (MPC) with Cross-Entropy Method**: Robot planning uses CEM to optimize action sequences by sampling, evaluating with the learned energy function, and refining the sampling distribution. This is the inference-time algorithm that converts world model predictions into executable actions.
  - Quick check question: In CEM, after sampling N action trajectories and evaluating their energies, how are the top-k trajectories used to update the sampling distribution?

## Architecture Onboarding

- **Component map**: Video clip → ViT encoder (ViT-L/H/g, 300M-1B params) → Encoder outputs + mask tokens → ViT-s predictor (22M) → Predicted representations → L1 loss vs EMA teacher → Update predictor only

- **Critical path**:
  1. Pre-training data curation: Cluster-based retrieval filters YT-Temporal-1B to match target distributions (Kinetics, SSv2, COIN, EpicKitchen). Without curation, uncurated YT1B degrades performance.
  2. Progressive resolution training: Start at 256×256, 16 frames during warmup/constant phases; increase to 384×384, 64 frames during cooldown. Enables 8.4× speedup over full-resolution training.
  3. Post-training on Droid: Sample 4-second clips at 4 fps, discard shorter videos (≤62 hours used). Train action-conditioned predictor with teacher-forcing + 2-step rollout loss.

- **Design tradeoffs**:
  - Representation vs pixel prediction: JEPA avoids pixel-level reconstruction, gaining efficiency but losing ability to visualize predictions directly. Paper trains a separate decoder for interpretability (Appendix B.3).
  - Planning horizon vs error accumulation: Longer horizons increase search space exponentially and accumulate autoregressive error. Paper uses horizon=1 for most tasks with sub-goals for pick-and-place.
  - Camera sensitivity: Model implicitly infers action coordinate frame from visual input. If robot base is not visible, coordinate inference degrades. Requires manual camera positioning (Appendix B.4).

- **Failure signatures**:
  - Camera misalignment: Rotation error in inferred coordinate axis correlates linearly with camera position angle (Figure 16). Manifests as systematic reaching errors despite correct visual features.
  - Object-specific grasping failures: Cup requires precise rim targeting; box requires gripper width control. Success rates vary 20-30% between objects (Table 2).
  - Cosmos baseline planning time: 4 minutes per action vs 16 seconds for V-JEPA 2-AC, making video-generation planning impractical for real-time control (Table 3).

- **First 3 experiments**:
  1. Reproduce EMA decay sensitivity: Train ViT-L on SSv2 subset with varying EMA decay rates (0.99, 0.999, 0.99925, 0.9999). Verify that 0.99925 matches paper's stability claims; document collapse or instability symptoms.
  2. Validate representation quality probe: Train a linear probe on frozen encoder outputs for a held-out classification task not in the paper (e.g., UCF-101). Compare against DINOv2 baseline to verify motion understanding claims generalize.
  3. Camera position ablation: Deploy V-JEPA 2-AC on simulated Franka with varied camera angles. Quantify coordinate frame rotation error vs camera angle to confirm Figure 16 trend; test calibration procedure described in Appendix B.4.

## Open Questions the Paper Calls Out

### Open Question 1
Can hierarchical modeling enable V-JEPA 2 to handle long-horizon planning without manual sub-goals?
The authors state that "developing approaches for hierarchical models capable of making predictions across multiple spatial and temporal scales" is a necessary future direction to solve complex tasks like pick-and-place. Current autoregressive prediction suffers from error accumulation and exponentially increasing search spaces over long horizons, limiting the model to simpler, shorter tasks or those with provided sub-goals.

### Open Question 2
How can V-JEPA 2-AC be extended to accept language-based goals?
The conclusion identifies "extending the V-JEPA 2-AC to accept language-based goals" as an important direction, noting that language may be more natural than image goals for deployment in the wild. The current optimization target relies strictly on image goals, creating a dependency on specific visual examples that may not always be available or convenient to provide.

### Open Question 3
What specific pre-training recipes are required to sustain performance improvements when scaling vision encoders beyond 1 billion parameters?
The authors note that "additional work is needed... to develop scalable pre-training recipes" to achieve sustained improvements when scaling to sizes comparable to previous 20B parameter models. The current study scaled only up to a "modest" 1B parameters; the specific data or architectural interventions needed to efficiently scale further without diminishing returns remain unexplored.

## Limitations
- Success rates vary significantly by object type (cups 25% vs boxes 45%), suggesting the learned representations are not uniformly effective across manipulation primitives.
- The planning mechanism assumes a smooth energy landscape in latent space, but this property is empirically observed rather than theoretically guaranteed.
- Camera positioning significantly affects performance, with rotation errors in inferred coordinate frames correlating linearly with camera angle.

## Confidence
- **High confidence**: Mask-denoising in representation space improves downstream classification tasks (SSv2 77.3 top-1 accuracy is state-of-the-art at time of publication)
- **Medium confidence**: Stage-wise learning (pre-training + post-training) enables better generalization than end-to-end training on limited interaction data, though direct comparisons are not provided
- **Medium confidence**: Zero-shot planning via energy minimization transfers across environments, but planning horizon is limited to 1 step with sub-goals, and camera positioning significantly affects performance

## Next Checks
1. **EMA stability ablation**: Train ViT-L on a subset of SSv2 with varying EMA decay rates (0.99, 0.999, 0.99925, 0.9999) to verify the reported 0.99925 value is optimal and document collapse/oscillation symptoms at other values.
2. **Coordinate frame calibration validation**: Implement the camera calibration procedure from Appendix B.4 and measure rotation error across the 35-85 degree camera position range, verifying the linear relationship between camera angle and action inference error reported in Figure 16.
3. **Planning horizon scaling study**: Extend the planning horizon from 1 to 3-5 steps on the block stacking task, measuring success rate degradation and autoregressive error accumulation to quantify the limits of the current energy-based planning approach.