---
ver: rpa2
title: 'Beyond Length: Quantifying Long-Range Information for Long-Context LLM Pretraining
  Data'
arxiv_id: '2510.25804'
source_url: https://arxiv.org/abs/2510.25804
tags:
- data
- context
- training
- long-context
- longfilter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the inefficiency in long-context pretraining
  caused by data containing sequences that do not genuinely benefit from extended
  context. To address this, it introduces LongFilter, a data curation framework that
  quantifies the information gain from long contexts by contrasting model predictions
  under long- and short-context settings.
---

# Beyond Length: Quantifying Long-Range Information for Long-Context LLM Pretraining Data

## Quick Facts
- arXiv ID: 2510.25804
- Source URL: https://arxiv.org/abs/2510.25804
- Authors: Haoran Deng; Yingyu Lin; Zhenghao Lin; Xiao Liu; Yizhou Sun; Yi-An Ma; Yeyun Gong
- Reference count: 10
- Key outcome: LongFilter improves long-context model performance by up to 10% on recall tasks through data curation that identifies sequences with meaningful long-range dependencies

## Executive Summary
This paper addresses the inefficiency of long-context pretraining where much data doesn't benefit from extended context. The authors introduce LongFilter, a data curation framework that quantifies information gain from long contexts by comparing model predictions under different context lengths. The method computes token-level scores based on KL divergence between next-token distributions and uses these to select high-quality long-context data. Experiments with LLaMA-3-8B extended to 64K context demonstrate substantial improvements on benchmarks like HELMET, LongBench, and RULER without requiring model modifications.

## Method Summary
LongFilter scores sequences by measuring the difference in next-token predictions when using full long context versus shorter sliding windows. For each token position, it computes the KL divergence between long-context (64K) and short-context (4K) predictions using a surrogate model, then aggregates these token scores to rank sequences. The top 20% of sequences by score are selected for training. The method is applied to SlimPajama data, with corpora filtered by length thresholds (ArXiv: 16K, Books: 64K, CommonCrawl: 32K) and packed into 64K-token sequences. The resulting dataset (~19B tokens) is used to continue pretraining LLaMA-3-8B from 8K to 64K context using ProLong training configuration.

## Key Results
- Up to 10% accuracy gains on HELMET recall tasks
- Improved performance across HELMET (Recall/RAG/Re-rank/ICL/QA), LongBench, and RULER benchmarks
- Average gains exceeding 2 points on HELMET evaluation
- Effective data selection without model architecture modifications

## Why This Works (Mechanism)
LongFilter works by quantifying the marginal value of extended context through contrastive prediction. When long-range dependencies exist in a sequence, the model's next-token distribution changes significantly when given more context. By measuring this change via KL divergence between long-context and short-context predictions, LongFilter identifies sequences where extended context genuinely improves prediction quality rather than sequences where local context suffices.

## Foundational Learning
- KL divergence for information gain measurement: Needed to quantify how much additional context changes predictions; Quick check: Verify KL divergence computation between two categorical distributions matches theoretical values
- Sliding window context extraction: Required for generating short-context predictions at each token position; Quick check: Ensure sliding windows overlap correctly and cover all token positions
- Next-token probability scoring: Core mechanism for measuring prediction confidence differences; Quick check: Validate logprob extraction from language models matches expected distribution

## Architecture Onboarding

**Component map:** Raw text → Length filtering → Sequence packing → LongFilter scoring (64K vs 4K windows) → Sequence ranking → Top 20% selection → Pretraining

**Critical path:** Data preparation → LongFilter scoring → Sequence selection → Model pretraining → Benchmark evaluation

**Design tradeoffs:** The method trades computational cost of scoring (requiring multiple forward passes per sequence) for improved data quality. Using a 4K sliding window balances local context adequacy with computational efficiency, while the 20% selection threshold aims to retain sufficient diversity while excluding low-value sequences.

**Failure signatures:** Near-zero scores across all sequences indicate incorrect logprob extraction or misaligned context windows. GPU OOM during scoring suggests insufficient memory for 64K context processing. Poor benchmark performance despite high scores may indicate the surrogate model's predictions don't generalize to the target model.

**First experiments:** 1) Verify scoring method on synthetic sequences with known long-range dependencies; 2) Test sensitivity to sliding window overlap parameter; 3) Compare selected data quality against random selection using a held-out validation set

## Open Questions the Paper Calls Out
None

## Limitations
- Requires access to a pretrained 64K-context scoring model, creating a circular dependency for new model development
- Missing critical hyperparameters (optimizer settings, sliding window overlap) that affect reproducibility
- The 20% selection threshold appears arbitrary without systematic ablation studies

## Confidence
- Core methodological contributions (KL divergence scoring, data selection protocol): High confidence
- Benchmark improvement claims: Medium confidence (limited by missing experimental details)
- Claim of no model modifications needed: Medium confidence (requires significant engineering infrastructure)

## Next Checks
1. Implement scoring with multiple sliding window overlap values (512, 1024, 2048 tokens) to assess sensitivity
2. Run ablation studies varying the selection threshold (10%, 20%, 30%) to test robustness
3. Apply scoring method to different model architectures to verify generalizability of selected data rankings