---
ver: rpa2
title: 'MGTS-Net: Exploring Graph-Enhanced Multimodal Fusion for Augmented Time Series
  Forecasting'
arxiv_id: '2510.16350'
source_url: https://arxiv.org/abs/2510.16350
tags:
- time
- series
- temporal
- forecasting
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MGTS-Net introduces a multimodal graph-enhanced framework to address
  time series forecasting challenges, specifically capturing fine-grained temporal
  patterns, effectively integrating multimodal information, and adapting to dynamic
  multi-scale features. The core method involves a Multimodal Feature Extraction layer
  (MFE) with customized encoders for time series, images, and text; a Multimodal Feature
  Fusion layer (MFF) using a heterogeneous graph to model intra-modal temporal dependencies
  and cross-modal alignment; and a Multi-Scale Prediction layer (MSP) with dynamic
  weighting of short-term, medium-term, and long-term predictors.
---

# MGTS-Net: Exploring Graph-Enhanced Multimodal Fusion for Augmented Time Series Forecasting

## Quick Facts
- arXiv ID: 2510.16350
- Source URL: https://arxiv.org/abs/2510.16350
- Reference count: 10
- Primary result: Graph-enhanced multimodal framework achieves state-of-the-art forecasting with average MSE improvements of 1.95% and 5.6% over Time-VLM, and 0.045 MSE reduction vs. Time-LLM

## Executive Summary
MGTS-Net introduces a multimodal graph-enhanced framework for time series forecasting that addresses challenges in capturing fine-grained temporal patterns, integrating multimodal information, and adapting to dynamic multi-scale features. The model employs a Multimodal Feature Extraction layer with customized encoders for time series, images, and text; a Multimodal Feature Fusion layer using a heterogeneous graph to model intra-modal temporal dependencies and cross-modal alignment; and a Multi-Scale Prediction layer with dynamic weighting of short-, medium-, and long-term predictors. Extensive experiments demonstrate superior performance over state-of-the-art baselines across four benchmarks, with particular effectiveness in few-shot scenarios.

## Method Summary
MGTS-Net is a three-layer architecture for multimodal time series forecasting. The Multimodal Feature Extraction (MFE) layer processes time series through TimeFTCMoE (MoE Transformer with FTC frequency-time cells), images through MGTS-ViT (asymmetric patching with interpolated positional encoding), and text through BERT (CLS token embeddings). The Multimodal Feature Fusion (MFF) layer constructs a heterogeneous graph with multimodal, past, and future relations, applying GNN layers to aggregate neighbor features and refine temporal embeddings. The Multi-Scale Prediction (MSP) layer employs three prediction heads (short/medium/long term) with adaptive MLP-generated weights to dynamically combine predictions. The model is trained end-to-end using MSE/MAE loss with Adam optimizer, batch size 32, and early stopping after max 10 epochs with learning rate halving per epoch.

## Key Results
- Achieves average MSE improvements of 1.95% and 5.6% over Time-VLM on benchmark datasets
- Reduces MSE by 0.045 compared to Time-LLM on standard forecasting tasks
- Demonstrates strong performance in few-shot scenarios, confirming effectiveness with limited training data
- Ablation studies show FTC frequency-time cells and MSP multi-scale prediction contribute 0.007 and 0.014 average MSE improvement respectively

## Why This Works (Mechanism)

### Mechanism 1: Heterogeneous Graph-Based Multimodal Fusion
The MFF layer constructs a heterogeneous graph with three relation types—multimodal relations (cross-modal edges between time-series, image, and text nodes), past relations (edges to preceding neighbors within a temporal window), and future relations (edges to subsequent neighbors). GNN layers aggregate neighbor features via relation-specific weight matrices, refining temporal node features through cross-modal message passing. This enables more effective multimodal knowledge aggregation than simple concatenation by modeling cross-modal alignment and intra-modal temporal dependencies jointly.

### Mechanism 2: Frequency-Time Cell (FTC) for Joint Time-Frequency Modeling
The FTC module splits input into frequency and time branches. The frequency branch applies a linear transformation followed by a Fourier-based harmonic function to map back to the time domain. The expert output is a learnable weighted fusion between FTC and FFN outputs. This integration of frequency-domain features with time-domain features within MoE layers improves capture of periodic patterns and fine-grained fluctuations that time-domain alone may miss.

### Mechanism 3: Adaptive Multi-Scale Prediction with Dynamic Weighting
Three prediction heads operate at different horizons (30, 50, 100 steps). An MLP takes the temporal feature sequence and generates adaptive weights via softmax, allowing the model to emphasize short-term heads for volatile inputs and long-term heads for stable periodic sequences. This dynamic weighting improves adaptability to varying temporal patterns compared to fixed-scale heads by treating predictions at different scales as complementary rather than redundant.

## Foundational Learning

- **Concept: Heterogeneous Graph Neural Networks (HGNNs)**
  - Why needed here: The MFF layer uses relation-typed edges (multimodal, past, future) with relation-specific aggregation weights. Understanding how HGNNs differ from homogeneous GNNs is essential for debugging fusion quality.
  - Quick check question: Given nodes of types {time-series, image, text}, how would you design the adjacency matrix to ensure text nodes influence all time-series nodes but not vice versa?

- **Concept: Mixture-of-Experts (MoE) Routing**
  - Why needed here: The TimeFTCMoE module uses Top-K routing with frequency-aware scores. Understanding routing dynamics helps diagnose expert under-utilization or collapse.
  - Quick check question: If 90% of inputs route to the same expert, what does this indicate about the data distribution or routing mechanism?

- **Concept: Multi-Horizon Time Series Forecasting**
  - Why needed here: The MSP layer generates predictions at different horizons (30, 50, 100 steps) and truncates to target length. Understanding horizon-specific error patterns is critical for interpreting ablation results.
  - Quick check question: Why might a model excel at short-term but fail at long-term forecasting, and how would you detect this in validation metrics?

## Architecture Onboarding

- **Component map:** Time series → Patch embedding → TimeFTCMoE → Temporal nodes → MFF graph aggregation → Enhanced temporal embeddings → MSP weighted fusion → Final prediction (Image and text features flow through parallel MFE branches to enhance temporal nodes via graph edges)

- **Critical path:** Time-series input → Patch embedding → TimeFTCMoE → Temporal nodes → MFF graph aggregation → Enhanced temporal embeddings → MSP weighted fusion → Final prediction

- **Design tradeoffs:**
  - Graph window size (w_p, w_f): Larger windows capture longer dependencies but increase memory/compute
  - FTC coefficient α: Fixed vs. learned; learned α adds flexibility but may overfit small datasets
  - Prediction horizons {p_s, p_m, p_l}: Defaults (30, 50, 100) may not suit all domains (e.g., hourly vs. daily data)

- **Failure signatures:**
  - Modality collapse: UMAP visualization shows text/image clusters remain isolated from temporal features after MFF—indicates graph edges not propagating signal
  - Scale imbalance: Adaptive weights w converge to near-uniform or single-scale dominance—MLP may need regularization or richer input features
  - FTC ineffectiveness: Ablation shows no difference between w/ and w/o FTC—check if input data has insufficient periodicity

- **First 3 experiments:**
  1. Ablation sanity check: Run MFF with only past relations (no cross-modal edges) to quantify multimodal fusion contribution on validation split
  2. Visualization of graph attention: Extract and visualize edge weights from MFF layer on sample batch to verify text nodes attending to relevant time-series segments
  3. Hyperparameter sweep on prediction horizons: Test alternative {p_s, p_m, p_l} configurations (e.g., {24, 48, 96} for hourly data) to confirm MSP adaptability across domains

## Open Questions the Paper Calls Out

### Open Question 1
How sensitive is the Multi-Scale Prediction layer (MSP) to the selection of fixed horizon parameters (s=30, m=50, l=100) when applied to datasets with significantly different intrinsic periodicities? The paper demonstrates improved performance with the specific set {30, 50, 100} but does not analyze performance degradation or the need for re-tuning these fixed scales on datasets with vastly different dominant frequencies.

### Open Question 2
To what extent does the Multimodal Feature Fusion (MFF) layer compromise accuracy when processing noisy or contradictory "event description" texts compared to the curated inputs used in benchmarks? The graph-based fusion mechanism assumes valid cross-modal alignment; it is unclear if the attention mechanism effectively filters out irrelevant or misleading textual nodes.

### Open Question 3
Does the "internal visual/textual generation" capability yield comparable forecasting accuracy to utilizing authentic, external multimodal metadata? The paper states the model "operates on raw time series alone" via "internal visual/textual generation," yet does not explicitly compare the performance of using self-generated modalities versus using ground-truth external information.

## Limitations
- Core hyperparameters (hidden dimension d, patch length pl, stride st, GNN layers, MoE expert count K, graph window sizes w_p/w_f) are unspecified, requiring assumption-driven reproduction
- Performance improvements are stated but lack statistical significance testing
- Graph construction assumes temporal alignment across modalities; misalignment could degrade multimodal fusion effectiveness
- Text and image generation procedures are not detailed—rule-based generation may not capture real-world multimodal signal quality

## Confidence

- **High confidence** in the mechanism of heterogeneous graph-based multimodal fusion—clear mathematical formulation and explicit relation-typed edges
- **Medium confidence** in FTC effectiveness—ablation shows benefit but assumes periodic patterns exist in datasets; may not generalize to non-periodic data
- **Medium confidence** in adaptive multi-scale prediction—validated via ablation but dynamic weighting may overfit or fail during concept drift

## Next Checks
1. Run MFF with only past relations (no cross-modal edges) to quantify multimodal fusion contribution on validation split
2. Extract and visualize edge weights from MFF layer on sample batch to verify cross-modal signal propagation
3. Test alternative {p_s, p_m, p_l} configurations (e.g., {24, 48, 96} for hourly data) to confirm MSP adaptability across domains