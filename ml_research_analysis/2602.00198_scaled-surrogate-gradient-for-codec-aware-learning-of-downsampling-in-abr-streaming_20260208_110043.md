---
ver: rpa2
title: 'SCALED : Surrogate-gradient for Codec-Aware Learning of Downsampling in ABR
  Streaming'
arxiv_id: '2602.00198'
source_url: https://arxiv.org/abs/2602.00198
tags:
- codec
- compression
- video
- training
- proxy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing video downsampling
  for adaptive bitrate (ABR) streaming by leveraging non-differentiable standard codecs
  during training. Traditional approaches either ignore the codec or use differentiable
  proxy models, which may not fully capture codec behavior.
---

# SCALED : Surrogate-gradient for Codec-Aware Learning of Downsampling in ABR Streaming

## Quick Facts
- arXiv ID: 2602.00198
- Source URL: https://arxiv.org/abs/2602.00198
- Reference count: 29
- Key outcome: Novel surrogate-gradient method achieves 5.19% BD-BR (PSNR) improvement by enabling end-to-end training with non-differentiable standard codecs

## Executive Summary
This paper addresses the challenge of optimizing video downsampling for adaptive bitrate (ABR) streaming by leveraging non-differentiable standard codecs during training. Traditional approaches either ignore the codec or use differentiable proxy models, which may not fully capture codec behavior. The authors propose a novel framework that uses surrogate gradients derived from actual compression errors to enable end-to-end training with real codecs. The method involves modifying the Straight-Through Estimator (STE) to account for compression noise, resulting in two variants: one focused on distortion minimization and another incorporating rate constraints. Experiments show that the proposed approach outperforms codec-agnostic baselines, achieving a 5.19% improvement in BD-BR (PSNR) across multiple scaling ratios and rate-distortion convex hulls.

## Method Summary
The method trains a learned downsampler that optimizes end-to-end rate-distortion performance using actual standard codecs (x264 H.264) during training. The key innovation is a modified STE that incorporates compression error statistics into the backward pass, enabling gradient flow through the non-differentiable codec operation. Two variants are proposed: SCALED-D optimizes distortion only, while SCALEDD-RD adds a differentiable rate proxy. The downsampler architecture (ProgDownLite) uses 10 convolutional layers with bilinear downsampling for fractional scales. Training is performed per quantization parameter (QP) and scale ratio on a 800-video Google dataset, with evaluation on Xiph and UVG datasets.

## Key Results
- 5.19% BD-BR (PSNR) improvement over Lanczos baseline across 6 scaling ratios
- SCALEDD-RD variant achieves -6.11 BD-BR (PSNR) on XIPH dataset, outperforming codec-agnostic baseline (-4.67)
- Consistent improvements across all three tested codecs (H.264, H.265, AV1) with 2.47% average BD-BR (PSNR) gain
- Distortion-only variant (SCALED-D) performs comparably to codec-agnostic baseline at low bitrates but improves at higher quality settings

## Why This Works (Mechanism)

### Mechanism 1
Standard Straight-Through Estimators (STE) fail for compression-aware training because they disconnect compression error from gradient flow, causing divergence via increased L1 norm of downsampler outputs. The proposed modified-STE injects compression error statistics into the backward pass. Instead of treating compression error ϵ as pure noise with zero gradient, the surrogate gradient incorporates ∂σ(ϵ)/∂y scaled by ϵ/σ(ϵ). This creates a gradient signal proportional to how input changes affect compression error variance, modulated by normalized error magnitude.

### Mechanism 2
Training with actual codec compression errors outperforms differentiable codec proxies because proxies trained on MSE fail to capture codec-specific artifacts (blocking, ringing) that affect downstream upsampling quality. The forward pass computes actual compression error ϵ = ϕ(f(x; θf)) - f(x; θf) using the real codec. This error is injected via stop-gradient for forward computation but contributes gradient signal through the σ(ϵ) modulation. The downsampler thus learns to produce representations that minimize actual codec-induced distortion rather than proxy-approximated distortion.

### Mechanism 3
Adding a differentiable rate proxy to distortion-based training (SCALEDD → SCALED RD) improves BD-BR(PSNR) by enabling explicit rate-distortion tradeoff optimization. SCALED RD combines the modified-STE distortion term with a differentiable rate estimator Ȓ based on DCT coefficient ℓ₀ approximation. The λ parameter controls rate-distortion balance. This hybrid approach uses ground-truth codec for distortion while avoiding non-differentiable rate measurement.

## Foundational Learning

- Concept: **Straight-Through Estimator (STE)**
  - Why needed here: STE is the baseline technique for handling non-differentiable operations, but understanding its failure mode (gradient disconnection) is essential to appreciate why the modified version is necessary.
  - Quick check question: Can you explain why treating compression error as pure additive noise with zero gradient would cause the downsampler to increase output magnitude?

- Concept: **Stop-gradient operator**
  - Why needed here: The paper uses sg(·) extensively to selectively block gradients while allowing values to pass through in forward computation. Understanding this is critical to parsing equations 5-8.
  - Quick check question: What is the difference between sg(x) in the forward pass versus the backward pass?

- Concept: **Rate-Distortion optimization (R-D)**
  - Why needed here: The entire framework optimizes for R-D performance. The λ parameter trades off distortion (MSE) against rate (bits). BD-BR measures bitrate reduction at equivalent quality.
  - Quick check question: If BD-BR(PSNR) = -5%, what does this mean in practical terms?

## Architecture Onboarding

- Component map:
  - Downsampler f(x; θf) → Codec ϕ → Rate proxy Ȓ (SCALED RD only) → Upsampler g (fixed bicubic)

- Critical path:
  1. Forward: x → f(x; θf) → y (downsampled) → ϕ(y) → decode → g(·) → reconstruction
  2. Compression error: ϵ = decoded(y) - y
  3. Loss computation: ‖x - g(ŷ)‖² + λȒ (where ŷ from equation 7)
  4. Backward: Surrogate gradient ∂ŷ/∂y = I + ϵ/σ(ϵ) · ∂σ(ϵ)/∂y flows through downsampler

- Design tradeoffs:
  - SCALEDD vs SCALED RD: Distortion-only is simpler but ignores rate; RD version requires rate proxy integration but achieves better BD-BR
  - Per-QP training: One model per QP and scale ratio (6 scales × multiple QPs) vs single universal model—paper trains separate models
  - Fixed bicubic upsampler: Ensures client-side compatibility but limits reconstruction quality compared to learned upsamplers

- Failure signatures:
  - Convergence failure with standard STE: Downsampler outputs grow unbounded (increasing |f(x)|₁) with no quality improvement
  - Proxy mismatch at low bitrates: Codec proxy methods show degraded performance at low rates (Fig. 2) due to omitted syntax costs and quantization effects
  - Metric misalignment: SCALED RD improves PSNR but slightly degrades VMAF/SSIM vs SCALEDD—rate constraint may favor different artifact profiles

- First 3 experiments:
  1. Sanity check—STE baseline: Train ProgDownLite with standard STE (equation 6) on a single QP/scale. Verify divergence or poor convergence matches expected failure mode before implementing modified-STE.
  2. Ablation—surrogate gradient components: Test SCALEDD with (a) only σ(ϵ) modulation, (b) only ϵ/σ(ϵ) scaling, (c) full equation (8). Compare BD-BR to isolate contribution of each term.
  3. Single-QP validation: Train SCALEDD for one QP (e.g., 27) and one scale (e.g., s=1/2) on a small dataset subset. Verify BD-BR improvement over Lanczos and codec-agnostic ProgDownLite before scaling to full multi-QP/multi-scale training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed surrogate gradient formulation generalize effectively to modern video coding standards (e.g., AV1, VVC) with different compression artifact distributions?
- Basis in paper: [explicit] The authors limit their experimental validation to the H.264 standard via x264, noting this choice reflects "real-world deployment scenarios" without testing newer standards.
- Why unresolved: The modified STE relies on the statistical properties of the compression error $\epsilon$. Newer codecs utilize different in-loop filters and partitioning schemes, potentially altering the error distribution and invalidating the gradient approximation.
- What evidence would resolve it: Reproducing the BD-BR improvement experiments using the SCALED framework on datasets encoded with VVC or AV1 reference software.

### Open Question 2
- Question: To what extent does the constraint of a fixed, standard bicubic upsampler limit the achievable rate-distortion performance compared to a learned upsampler?
- Basis in paper: [explicit] The authors adopt a fixed bicubic upsampler to "avoid non-standard and computationally intensive technology at the end-client," explicitly trading off potential performance for ease of deployment.
- Why unresolved: While learned upsamplers add client-side complexity, they can recover high-frequency details lost during downsampling and compression. The joint optimization landscape remains unexplored in this specific codec-aware context.
- What evidence would resolve it: Comparative experiments optimizing both the downsampler and a neural upsampler within the SCALED framework against the fixed-upsampler baseline.

### Open Question 3
- Question: How does the mismatch between the exact distortion gradient (from the real codec) and the approximate rate gradient (from the proxy $\hat{R}$) affect the stability of the $\lambda$ trade-off in SCALED_RD?
- Basis in paper: [inferred] The paper combines a real codec for distortion with a differentiable proxy for rate estimation; however, it does not analyze if inaccuracies in the rate proxy cause the optimizer to converge to sub-optimal bitrates relative to the target $\lambda$.
- Why unresolved: The rate proxy is an approximation based on DCT coefficients, which may diverge from the actual entropy coding costs of the standard codec, potentially skewing the rate-distortion trade-off.
- What evidence would resolve it: Ablation studies plotting the actual bitrate achieved by SCALED_RD against the target rate estimated by the proxy during training.

## Limitations
- Only tested on H.264 codec (x264), limiting generalizability to modern codecs like AV1 or VVC
- Fixed bicubic upsampler may limit achievable performance compared to learned upsampling
- Requires separate models for each QP and scale ratio, increasing deployment complexity

## Confidence
- High confidence: The failure of standard STE for codec-aware training (Mechanism 1)
- Medium confidence: The superiority of real-codec compression error over proxy models (Mechanism 2)
- Low confidence: The specific form of the modified-STE surrogate gradient derivation

## Next Checks
1. Implement and validate the modified-STE gradient calculation on a simple toy problem with known compression behavior
2. Replicate the single-QP training results on a smaller dataset subset to verify BD-BR improvements
3. Conduct ablation studies comparing standard STE, proxy-only, and hybrid approaches to isolate contribution of each component