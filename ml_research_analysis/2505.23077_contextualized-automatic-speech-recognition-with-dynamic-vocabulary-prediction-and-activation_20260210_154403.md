---
ver: rpa2
title: Contextualized Automatic Speech Recognition with Dynamic Vocabulary Prediction
  and Activation
arxiv_id: '2505.23077'
source_url: https://arxiv.org/abs/2505.23077
tags:
- bias
- contextual
- recognition
- speech
- phrases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of recognizing contextual phrases
  in speech by proposing an encoder-based method that dynamically predicts and activates
  phrase-level tokens instead of treating subwords independently. It integrates a
  bias-aware module, dynamic vocabulary, and confidence-activated decoding to improve
  contextual phrase integrity.
---

# Contextualized Automatic Speech Recognition with Dynamic Vocabulary Prediction and Activation

## Quick Facts
- arXiv ID: 2505.23077
- Source URL: https://arxiv.org/abs/2505.23077
- Reference count: 0
- Primary result: Relative WER reductions of 28.31% on Librispeech and 23.49% on Wenetspeech, with contextual phrase WER reduced by 72.04% and 75.69%, respectively.

## Executive Summary
This paper addresses the challenge of recognizing contextual phrases in speech by proposing an encoder-based method that dynamically predicts and activates phrase-level tokens instead of treating subwords independently. It integrates a bias-aware module, dynamic vocabulary, and confidence-activated decoding to improve contextual phrase integrity. Experiments on Librispeech and Wenetspeech show relative WER reductions of 28.31% and 23.49%, with contextual phrase WER reduced by 72.04% and 75.69%, respectively.

## Method Summary
The method builds on a frozen CTC-based Conformer ASR model and introduces a bias-aware module consisting of a context encoder, multi-head attention, and a transformer layer. Dynamic vocabulary expansion adds contextual phrase tokens to the output layer, allowing direct prediction of complete phrases. A confidence-activated decoding mechanism ensures accurate phrase substitution by verifying high-confidence paths during inference. The approach is trained with a combination of CTC and bias losses, and uses either Word-by-word Replacement (WR) or Tail Addition (TA) labeling strategies.

## Key Results
- Relative WER reductions of 28.31% on Librispeech and 23.49% on Wenetspeech
- Contextual phrase WER reduced by 72.04% and 75.69%, respectively
- Confidence-activated decoding reduces WER from 10.31 to 6.81 when enabled
- Performance degrades as bias list size increases beyond 100 phrases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Phrase-level tokens preserve subword dependencies better than independent subword optimization.
- Mechanism: The model introduces dynamic vocabulary tokens where each token represents an entire contextual phrase (e.g., "Alexander" as one token rather than ["A", "lex", "ander"]). These tokens are concatenated with the standard vocabulary (V + n dimensions) in the CTC output layer, allowing frame-level predictions to directly reference complete phrases.
- Core assumption: Contextual phrases have internal acoustic-linguistic dependencies that independent subword models cannot capture.
- Evidence anchors:
  - [abstract] "most existing methods enhance subwords in a contextual phrase as independent units, potentially compromising contextual phrase integrity"
  - [section 2.1] "This enables the model to incorporate n additional predicted labels beyond the V vocabulary tokens, representing the n contextual phrases in B"
  - [corpus] DYNAC (arXiv:2506.00422) applies dynamic vocabulary to autoregressive models but notes slower inference; corpus evidence for non-autoregressive variants is limited.
- Break condition: If bias list contains highly acoustically similar phrases, phrase-level tokens may introduce confusion rather than clarity.

### Mechanism 2
- Claim: Cross-attention between audio and bias embeddings enables context-aware frame representations.
- Mechanism: The bias-aware module computes multi-head attention with hidden states H as query and bias embeddings E as key/value (Equation 1), then processes through a transformer (Equation 2). A separate attention head computes HdV scores indicating how much each frame attends to each contextual phrase. This creates audio representations that are explicitly conditioned on the bias list.
- Core assumption: Relevant contextual phrases share acoustic patterns with the input that attention can isolate.
- Evidence anchors:
  - [abstract] "integrates a bias-aware module"
  - [section 2.1] "The multi-head attention layer uses H as the query while E serves as the key and the value"
  - [corpus] Weak corpus validation for this specific attention formulation; related work (TurboBias, CATT) uses alternative biasing mechanisms.
- Break condition: If bias list size grows substantially (N > 1000), attention computation may become noisy or computationally expensive.

### Mechanism 3
- Claim: Confidence-activated decoding prevents incorrect phrase substitution.
- Mechanism: When a bias token is predicted, the system identifies the termination frame (CTC peak for <bi>) and starting frame (j-th peak before). It searches for the highest-probability path matching the contextual phrase sequence within that span. If confidence exceeds threshold (ki * threshold, where ki is phrase length), substitution proceeds.
- Core assumption: High posterior probability along the phrase path indicates genuine acoustic match rather than spurious prediction.
- Evidence anchors:
  - [abstract] "confidence-activated decoding method that ensures the complete output of contextual phrases while suppressing incorrect bias"
  - [section 2.3] "If the maximum confidence surpasses the confidence threshold, the identified sequence is replaced with the predicted phrase"
  - [corpus] Corpus lacks comparative studies on confidence thresholds for bias activation; this appears novel to this work.
- Break condition: If threshold is too low, false positives increase; if too high, valid phrases are missed.

## Foundational Learning

- Concept: **CTC (Connectionist Temporal Classification)**
  - Why needed here: The entire architecture builds on CTC's frame-level outputs and label merging rules. Understanding CTC peaks, blank tokens, and the collapse function is essential to comprehend how bias tokens integrate with standard decoding.
  - Quick check question: Can you explain why CTC requires a collapse/merging step and how blank tokens function?

- Concept: **Multi-head Cross-Attention**
  - Why needed here: The bias-aware module and output layer both use cross-attention between audio representations and bias embeddings. Understanding Q/K/V formulations is necessary to trace information flow.
  - Quick check question: Given query dimension d_q and key dimension d_k, what is the shape of the attention score matrix?

- Concept: **Dynamic Vocabulary Expansion**
  - Why needed here: The core innovation extends the output vocabulary from V to V + n at inference time based on the bias list. This differs from fixed-vocabulary ASR and requires understanding how softmax dimensions change.
  - Quick check question: If base vocabulary is 5000 tokens and bias list has 100 phrases, what is the output dimension before and after dynamic expansion?

## Architecture Onboarding

- Component map:
  Audio Encoder (Conformer-12) -> H (hidden states)
  Context Encoder (Conformer-6) -> E (bias embeddings from phrase list)
  Bias-aware Module (MHA + Transformer) -> H_CA (contextualized audio representation)
  Output Layer (MHA) -> H_dV (frame-to-phrase attention scores)
  CTC Projection -> H_v concatenated with H_dV -> Softmax(V + n)
  Confidence Decoder -> Post-hoc phrase substitution

- Critical path:
  1. Freeze pre-trained ASR model weights
  2. Train only bias module components (context encoder, bias-aware module, bias projection)
  3. At inference: encode bias list -> compute cross-attention -> predict with expanded vocabulary -> apply confidence-activated decoding

- Design tradeoffs:
  WR vs TA labeling: WR (word-by-word replacement) slightly better for English; TA (tail addition) better for longer Chinese phrases due to pronunciation-level modeling
  Bias list size (N): Performance degrades as N increases from 100 to 1000 (B-WER improvement drops from 72% to 54% relative on Librispeech)
  Threshold scaling: Threshold multiplied by phrase length (ki) normalizes across variable-length phrases

- Failure signatures:
  High B-WER with low U-WER: Model over-biasing, triggering false phrase substitutions (threshold too low)
  High U-WER: Bias module interfering with general recognition (check if base model parameters were frozen)
  Performance collapse on long phrases: TA strategy may fail; consider WR or increase j-search range from [ki-2, ki+2] to larger window

- First 3 experiments:
  1. Reproduce baseline vs bias module with N=100 on Librispeech test-other to validate B-WER reduction claim (~72% relative improvement)
  2. Ablate confidence-activated decoding: compare with/without to quantify its contribution (Table 3 shows WER jumps from 6.81 to 10.31 without it)
  3. Test scaling behavior: measure B-WER degradation as bias list size increases from 100 to 500 to 1000 to determine practical limits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a unified strategy be developed to dynamically select between "Word-by-word Replacement" (WR) and "Tail Addition" (TA) based on phrase length or linguistic characteristics?
- Basis in paper: [explicit] Section 3.2 observes that WR outperforms TA on English (test-other) but underperforms significantly on Chinese (Wenetspeech), hypothesizing that WR has "limited ability to model long contextual phrases."
- Why unresolved: The authors manually selected TA based on overall results but did not investigate an adaptive mechanism to leverage the strengths of WR for shorter sequences.
- What evidence would resolve it: An experiment evaluating a hybrid model that switches strategies based on input phrase length or subword count, compared against static-strategy baselines.

### Open Question 2
- Question: Does the quadratic complexity of the attention mechanism in the bias-aware module limit the scalability of this approach for bias lists exceeding 1,000 phrases?
- Basis in paper: [explicit] Section 3.3 notes that "model performance declines as the bias list size increases" (relative B-WER drops from 72% to 54% when moving from N=100 to N=1000).
- Why unresolved: The paper demonstrates effectiveness up to N=1,000 but does not analyze the computational cost or accuracy degradation curve at larger scales typical of production scenarios (e.g., 10k+ contacts).
- What evidence would resolve it: Benchmarking results showing latency, memory usage, and B-WER trends as the bias list scales to 5,000 and 10,000 phrases.

### Open Question 3
- Question: How robust is the confidence-activated decoding strategy when speech rate variations cause frame alignments to fall outside the heuristic $[k_i-2, k_i+2]$ search window?
- Basis in paper: [inferred] Section 2.3 defines a fixed search window relative to phrase length ($k_i$) to locate the confidence path, which assumes a consistent speech rate and may be brittle for fast speakers.
- Why unresolved: While ablation studies confirm the module is necessary, they do not test the sensitivity of the hard-coded window size boundaries against varying speech rates.
- What evidence would resolve it: An evaluation of the substitution error rate on a dataset specifically annotated for fast or hurried speech, comparing the fixed window against a dynamic or duration-based window.

## Limitations
- Performance gains limited to English and Chinese datasets with short contextual phrases
- Confidence-activated decoding lacks theoretical grounding for threshold selection
- Freezing base ASR model may limit adaptability to diverse acoustic conditions
- TA labeling strategy shows language-specific performance differences without clear explanation

## Confidence
- High Confidence: Dynamic vocabulary expansion improves contextual phrase recognition (72.04% B-WER reduction on Librispeech)
- Medium Confidence: Confidence-activated decoding meaningfully reduces false positives (WER reduction from 10.31 to 6.81)
- Low Confidence: Scalability claims regarding bias list size are based on limited experiments (N=100, 500, 1000) without theoretical justification

## Next Checks
1. Systematically vary the base confidence threshold from 0.1 to 0.9 in increments of 0.1, measuring B-WER, U-WER, and false positive rate on non-contextual phrases to validate threshold optimization.

2. Apply the trained model to a dataset with substantially different acoustic characteristics (e.g., noisy environments, accented speech) to verify whether freezing base model weights limits performance when domain shift occurs.

3. Extend bias list size experiments beyond N=1000 to N=2000 and N=5000, measuring both computational complexity (inference time) and recognition accuracy to establish practical limits and identify at what point the attention mechanism becomes ineffective.