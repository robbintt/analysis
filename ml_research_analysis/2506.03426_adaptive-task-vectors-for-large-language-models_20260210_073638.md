---
ver: rpa2
title: Adaptive Task Vectors for Large Language Models
arxiv_id: '2506.03426'
source_url: https://arxiv.org/abs/2506.03426
tags:
- input
- task
- nquestion
- vector
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Adaptive Task Vectors (ATV) address the inefficiency and inflexibility
  of traditional in-context learning (ICL) and fixed task vector approaches by dynamically
  generating input-conditioned task vectors using a small language model. These vectors
  are expanded and injected into a frozen large language model to guide task-specific
  behavior.
---

# Adaptive Task Vectors for Large Language Models

## Quick Facts
- arXiv ID: 2506.03426
- Source URL: https://arxiv.org/abs/2506.03426
- Reference count: 40
- Primary result: Dynamic task vectors generated by small LM and injected into frozen LLM achieve higher accuracy (62.1% avg) with fewer tokens than ICL/BM25 while maintaining strong generalization to unseen tasks

## Executive Summary
Adaptive Task Vectors (ATV) introduces a parameter-efficient approach for adapting large language models to diverse tasks without modifying model weights. The method uses a small generator model to create task vectors conditioned on input queries, which are then expanded and injected into a frozen large language model. ATV achieves superior performance to both traditional in-context learning and fixed task vector approaches while using fewer tokens and maintaining strong generalization to unseen tasks.

## Method Summary
ATV generates input-conditioned task vectors using a small language model, then expands these vectors to match the hidden size of a frozen large language model. The expanded vectors are injected into early layers of the LLM to guide task-specific behavior. This approach combines the flexibility of in-context learning with the efficiency of fixed task vectors, creating a modular system where the generator and base model can be independently selected. The method employs rank decomposition for efficient vector expansion and demonstrates both theoretical expressivity advantages and practical performance gains across diverse tasks.

## Key Results
- ATV achieves highest average accuracy (62.1%) across 20 in-domain and 5 unseen tasks
- Uses fewer tokens than both ICL and BM25 baselines while maintaining performance
- Demonstrates strong generalization with 63.4% accuracy on unseen tasks
- Ablation studies show lightweight generators suffice and early-layer injection is most effective
- Visualization reveals ATV produces more query-sensitive vector distributions than fixed vector methods

## Why This Works (Mechanism)
ATV works by creating dynamic task representations that adapt to each input query rather than relying on static task conditioning. The small generator learns to produce task vectors that capture task-relevant information specific to each query, which are then expanded and injected into the frozen LLM to modify its internal representations. This approach allows the model to maintain task awareness throughout processing while avoiding the inefficiency of long prompts or the rigidity of fixed task vectors.

## Foundational Learning
**Rank decomposition**: Breaking down high-dimensional matrices into lower-rank components for efficiency. Why needed: Enables compact representation of task vectors while maintaining expressivity. Quick check: Verify that decomposition preserves sufficient variance for task adaptation.

**In-context learning limitations**: Traditional ICL requires long prompts and struggles with generalization. Why needed: Motivates need for more efficient task conditioning approaches. Quick check: Compare token usage and accuracy between ICL and proposed method.

**Layer injection techniques**: Methods for inserting external vectors into transformer layers. Why needed: Core mechanism for integrating adaptive task vectors into frozen models. Quick check: Test different injection positions to identify optimal placement.

**Expressivity analysis**: Theoretical comparison of model capacity under different parameterizations. Why needed: Validates that ATV can represent as much or more than existing approaches. Quick check: Confirm rank constraints don't limit task representation capacity.

## Architecture Onboarding

**Component map**: Query -> Small Generator -> Task Vectors -> Expansion -> Frozen LLM -> Output

**Critical path**: Input query flows through small generator to produce task vectors, which are expanded and injected into early LLM layers before processing continues through the rest of the model.

**Design tradeoffs**: 
- Generator size vs. expressivity: Smaller generators are more efficient but may limit task representation
- Injection layer position: Earlier layers provide stronger conditioning but may interfere with base model processing
- Rank decomposition level: Higher ranks increase expressivity but reduce parameter efficiency

**Failure signatures**: 
- Degraded performance on complex tasks suggests generator capacity limitations
- Inconsistent behavior across similar queries indicates poor vector conditioning
- Overfitting to training tasks shows insufficient generalization capability

**First experiments**:
1. Compare ATV performance against ICL and fixed vector baselines on a simple classification task
2. Test different generator sizes to identify minimum effective capacity
3. Evaluate injection position sensitivity by comparing early vs. late layer injection

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical expressivity claims rely on idealized assumptions about task distribution that may not hold in practice
- Fixed-size generator architecture could become a bottleneck for very complex or high-dimensional tasks
- Inherits potential limitations of LoRA-style rank decomposition in capturing certain task relationships

## Confidence
- High confidence: Empirical performance claims on LLaMA3 and Mistral, ablation study results, and efficiency comparisons to ICL/BM25
- Medium confidence: Theoretical expressivity claims (particularly "strictly more expressive" relative to Prefix-Tuning) rely on idealized assumptions
- Medium confidence: Generalization claims to unseen tasks supported but could benefit from broader task diversity testing

## Next Checks
1. Test ATV on tasks requiring longer context windows or more complex reasoning patterns to evaluate generator scalability limitations
2. Conduct head-to-head comparisons against full fine-tuning on a subset of tasks to quantify the expressivity gap in practical scenarios
3. Evaluate the robustness of ATV across different base model sizes (beyond LLaMA3-8B) and different generator architectures to establish generalizability