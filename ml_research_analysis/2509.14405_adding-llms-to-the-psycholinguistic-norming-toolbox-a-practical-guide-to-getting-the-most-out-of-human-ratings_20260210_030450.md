---
ver: rpa2
title: 'Adding LLMs to the psycholinguistic norming toolbox: A practical guide to
  getting the most out of human ratings'
arxiv_id: '2509.14405'
source_url: https://arxiv.org/abs/2509.14405
tags:
- words
- estimates
- human
- data
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a methodology for using Large Language Models
  (LLMs) to estimate psycholinguistic word characteristics, addressing the challenge
  of obtaining human-based norms which are often costly and time-consuming. The authors
  propose leveraging LLMs to directly predict various word features, both through
  direct use of base models and fine-tuning approaches.
---

# Adding LLMs to the psycholinguistic norming toolbox: A practical guide to getting the most out of human ratings

## Quick Facts
- arXiv ID: 2509.14405
- Source URL: https://arxiv.org/abs/2509.14405
- Reference count: 16
- Primary result: LLMs estimate English word familiarity with Spearman correlation ρ = 0.8 (base) and ρ = 0.9 (fine-tuned) vs. human ratings

## Executive Summary
This paper presents a practical methodology for using Large Language Models to estimate psycholinguistic word characteristics, addressing the costly and time-consuming process of obtaining human-based norms. The authors demonstrate that LLMs can predict various word features through direct use of base models and fine-tuning approaches, achieving correlations of 0.8 with human ratings for word familiarity tasks. The work provides a comprehensive framework with validation protocols, prompt engineering guidance, and parameter configurations while acknowledging limitations like model inscrutability. A case study using English word familiarity ratings from MRC and Glasgow databases validates the approach, showing fine-tuned models achieve 0.9 correlation versus human ratings.

## Method Summary
The methodology involves using LLMs to estimate psycholinguistic word characteristics through direct API calls and fine-tuning approaches. The process uses 2,545 words with human familiarity ratings from MRC and Glasgow databases, split into 1,500 training and 1,045 test words for fine-tuning experiments. Base model predictions use temperature=0 and independent calls per word, while fine-tuning employs random 60/40 splits with default OpenAI settings. The framework supports both commercial and open-weight models through a config.yaml-driven pipeline. Validation against human "gold standard" norms uses Spearman correlation, with baseline human inter-rater agreement at ρ = 0.80. The approach emphasizes careful validation, prompt engineering, and parameter configuration to ensure reliable estimates.

## Key Results
- Base models achieve Spearman correlation ρ = 0.8 with human word familiarity ratings
- Fine-tuned models improve correlation to ρ = 0.9 with human ratings
- Batch processing introduces context contamination, requiring independent word queries
- Model estimates show high consistency (ρ > 0.96) when word order is permuted

## Why This Works (Mechanism)
LLMs leverage their extensive training on natural language text to capture statistical patterns about word usage frequency and context, which correlates with human familiarity ratings. The fine-tuning process allows models to calibrate their predictions specifically to human rating scales, improving accuracy beyond zero-shot capabilities. Temperature=0 ensures deterministic outputs, while independent API calls prevent context contamination that occurs when processing words in batches. The approach exploits the fact that word familiarity correlates with corpus frequency and contextual diversity, which LLMs have learned during pretraining.

## Foundational Learning
- **Psycholinguistic norms**: Standardized ratings of word characteristics (familiarity, concreteness, etc.) used in cognitive research; needed because human-based norms are costly and time-consuming to collect
- **Spearman correlation**: Non-parametric measure of monotonic relationship between ranked variables; used here because rating scales are ordinal
- **Context contamination**: When batch processing causes models to reference previous outputs, creating dependencies; diagnosed by checking correlation between different word order runs
- **Few-shot prompting**: Providing examples in prompts to guide model behavior; critical for establishing the rating scale interpretation
- **Fine-tuning hyperparameters**: Parameters like learning rate, batch size, and epochs that control model adaptation; defaults used but optimal values unknown
- **Zero-shot vs. fine-tuned performance**: Comparison of model capability without adaptation versus after training on human ratings

## Architecture Onboarding
**Component map**: config.yaml -> prepare_experiment.py -> execute_experiment.py -> evaluate.py -> fine-tune pipeline (create_finetuning_dataset.py -> execute_finetuning_dataset.py -> re-evaluate.py)

**Critical path**: Data preparation (MRC+Glasgow dataset) → API configuration (temperature=0, logprobs) → Base model prediction → Correlation evaluation → Fine-tuning (60/40 split) → Re-evaluation

**Design tradeoffs**: Independent API calls (more expensive, prevents contamination) vs. batch processing (cheaper, but introduces dependencies); few-shot examples (better calibration, more tokens) vs. minimal prompts (cheaper, less guidance)

**Failure signatures**: Correlation < 0.96 between different word order runs indicates context contamination; missing outputs for some words suggests API issues or model refusal; fine-tuning collapse indicates poor training data quality or hyperparameter issues

**First experiments**: (1) Run base model predictions on 10-20 words and verify correlation with human ratings; (2) Test context contamination by running same words in different orders and checking inter-run correlation; (3) Execute fine-tuning pipeline on 100-word subset and verify improved correlation

## Open Questions the Paper Calls Out
**Cross-lingual transfer**: Does fine-tuning in one language improve LLM ability to estimate psycholinguistic norms in under-represented languages? The authors observed deteriorating performance in zero-shot queries across languages and note theoretical principles for this transfer have not been developed.

**Optimal fine-tuning parameters**: What are the best sample sizes, stimulus selection strategies, and feedback granularities for fine-tuning LLMs on psycholinguistic tasks? The authors list specific questions about sample size and informative stimuli but note the optimal strategy remains undefined.

**Statistical non-independence**: How should researchers account for non-independence in LLM-generated datasets when the same model generates multiple responses? Standard psycholinguistic models account for human dependencies, but the analogue for LLMs is theoretically ambiguous.

## Limitations
- Missing exact prompt templates (v02_standard_prompt, v09_logprobs) and default fine-tuning hyperparameters limit exact replication
- Batch processing introduces context contamination requiring more expensive independent queries
- Model inscrutability means researchers cannot verify how LLMs arrive at their estimates
- Results may not generalize beyond the familiarity task without additional validation

## Confidence
**High**: Core correlation results (ρ = 0.8 base, ρ = 0.9 fine-tuned) and practical recommendations are well-supported
**Medium**: Reproducibility is challenged by missing prompt text and fine-tuning hyperparameters
**Low**: Generalizability beyond word familiarity task without additional validation studies

## Next Checks
1. Reproduce the base model correlation (ρ = 0.8) using the provided framework and MRC-Glasgow dataset with specified configuration
2. Verify context contamination effect by running batch queries in different word orders and confirming inter-run correlation stays above 0.96
3. Test the fine-tuning pipeline (ρ = 0.9) with the specified 60/40 split and default OpenAI settings on the full dataset