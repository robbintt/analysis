---
ver: rpa2
title: 'COGITAO: A Visual Reasoning Framework To Study Compositionality & Generalization'
arxiv_id: '2509.05249'
source_url: https://arxiv.org/abs/2509.05249
tags:
- grid
- objects
- generalization
- transformations
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: COGITAO is a visual reasoning framework designed to study compositionality
  and generalization in machine learning models. It generates rule-based tasks that
  apply sequences of transformations to objects in grid-like environments, supporting
  adjustable depth composition over 28 interoperable transformations with extensive
  control over grid and object properties.
---

# COGITAO: A Visual Reasoning Framework To Study Compositionality & Generalization

## Quick Facts
- arXiv ID: 2509.05249
- Source URL: https://arxiv.org/abs/2509.05249
- Reference count: 40
- Key outcome: COGITAO evaluates compositional generalization by generating rule-based tasks with adjustable depth over 28 transformations, showing state-of-the-art models consistently fail on novel combinations despite strong in-domain performance.

## Executive Summary
COGITAO is a visual reasoning framework designed to study compositionality and generalization in machine learning models. It generates rule-based tasks that apply sequences of transformations to objects in grid-like environments, supporting adjustable depth composition over 28 interoperable transformations with extensive control over grid and object properties. This flexibility enables the creation of millions of unique task rules, surpassing concurrent datasets by several orders of magnitude. The framework evaluates models' abilities to apply learned transformations in novel combinations and environments. Baseline experiments using state-of-the-art vision models (Vanilla ViT, Grid ViT, and LLaDA) demonstrate consistent failures to generalize to novel combinations of familiar elements, despite strong in-domain performance. For instance, accuracies dropped to as low as 5.1% for Grid ViT and 6.4% for LLaDA when faced with unseen transformation sequences. COGITAO is fully open-sourced to support continued research in compositional generalization.

## Method Summary
COGITAO generates datasets by applying sequences of 28 atomic transformations (e.g., translate, rotate, crop, change color) to objects in grid-like environments. The framework ensures valid, solvable tasks through constraint-based generation that dynamically filters out invalid samples. It supports two evaluation modes: Compositional Generalization (testing novel transformation combinations) and Environmental Generalization (varying grid/object parameters). The system uses a discrete grid with tokens 0-9 to minimize visual complexity while maximizing relational structure. Models are trained to predict output grids given input grids and transformation sequences, with evaluation based on exact grid accuracy.

## Key Results
- State-of-the-art vision models (Vanilla ViT, Grid ViT, LLaDA) consistently fail to generalize to novel combinations of familiar transformations, with OOD accuracy dropping to near-zero levels
- Grid ViT shows partial mitigation of generalization failures through relative positional encodings, but still fails on deeper composite sequences
- The framework supports adjustable depth composition, enabling tests from atomic transformations through deep composite sequences with exponential task space growth
- Models achieve high in-domain accuracy (>75%) but show catastrophic failure on out-of-distribution tests, demonstrating the fundamental challenge of compositional generalization

## Why This Works (Mechanism)

### Mechanism 1: Decoupling Visual Complexity from Relational Structure
The framework isolates failures in reasoning by minimizing visual noise (pixels 0-9) while maximizing relational complexity (transformation depth). Unlike natural images where texture and lighting dominate, COGITAO uses a discrete grid system. This forces the model to attend to the execution of rules (e.g., `rotate_90`) rather than statistical correlations between pixels. Assumption: Models failing on these "simple" grids lack fundamental symbolic manipulation capabilities, not just visual perception. Evidence: "constructs rule-based tasks which apply a set of transformations to objects in grid-like environments... surpassing concurrent datasets."

### Mechanism 2: Constraint-Based Generative Engineering
The generator creates valid, solvable tasks by enforcing spatial constraints dynamically, preventing degenerate or impossible samples. The `SetInitialGrid()` and `transform_and_position()` functions operate in a loop. If a transformation moves an object out of bounds or causes a collision, the specific sample is discarded and regenerated, ensuring the dataset contains only executable logic. Assumption: The cost of re-sampling is negligible compared to the value of a 100% valid dataset. Evidence: "If contact occurs, or objects go out of bounds, the entire input-output pair is discarded and generation is restarted."

### Mechanism 3: Grid ViT Inductive Biases (Relative Positioning)
The Grid ViT architecture partially mitigates generalization failures by explicitly encoding relative spatial relationships, which standard ViTs lack. It replaces absolute positional embeddings with Rotary Position Embeddings (RoPE) and adds Object Positional Encoding (OPE). This allows the attention mechanism to generalize "rotate" relative to an object's center, rather than learning absolute coordinates that fail when grid sizes change. Assumption: Spatial reasoning requires explicit relative distance encoding, not just learned global context. Evidence: "Grid ViT... incorporates task-specific biases: object positional encoding (OPE)... modified positional encoding schemes."

## Foundational Learning

- **Concept: Compositional Generalization (CompGen)**
  - Why needed here: This is the primary metric of the paper. You must understand that the system tests systematicity—the ability to recombine known primitives (A+B) into unseen sequences (B+A).
  - Quick check question: If a model learns "jump" and "left," can it execute "jump left" without explicit training on that specific phrase?

- **Concept: Object-Centric Tokenization**
  - Why needed here: The input is not an image but a grid of discrete tokens (0-9). Understanding how neural networks process this as a sequence or a map is crucial for selecting the right architecture (CNN vs. Transformer).
  - Quick check question: How does a Vision Transformer (ViT) handle a 20x20 grid differently than a ResNet?

- **Concept: Out-of-Distribution (OOD) Evaluation**
  - Why needed here: The paper emphasizes the gap between In-Domain (ID) and OOD performance. High ID accuracy with low OOD accuracy is the signature failure mode of current SOTA models.
  - Quick check question: Why does training on grid sizes 10x10-15x15 and testing on 16x16-20x20 (Setting G2) cause Vanilla ViT performance to drop to 0%?

## Architecture Onboarding

- **Component map:** Generator -> Tokenizer -> Model (Encoder -> MLP Head -> Grid Reconstruction)
- **Critical path:** 1. Configure `init_params` (depth, grid size, object count) 2. Run generation loop (check for collisions/out-of-bounds) 3. Train model with Task Embedding prefix (for C-studies) or Grid-only input (for G-studies) 4. Evaluate exact grid match (strict accuracy)
- **Design tradeoffs:** Atomic Depth: Increasing transformation depth (k) exponentially increases task space ($28^k$) but makes generation slower and learning harder. Grid Size vs. Generalization: Training on small grids speeds up iteration but leads to catastrophic failures on larger test grids (Vanilla ViT fails G2).
- **Failure signatures:** Shortcut Learning: High ID accuracy (>90%) with 0% OOD accuracy on size/scaling tasks (indicative of absolute positional memorization). Mode Collapse: Model predicts empty grids or static patterns regardless of input transformation. Collision Errors: In custom generation, failure to filter invalid samples leads to ambiguous targets during training.
- **First 3 experiments:** 1. Reproduce C1 (Atomic/Composite to Unseen): Train Vanilla ViT on depth=1 & 2, test on unseen depth=2 sequences to verify the generalization gap. 2. Ablate Positional Encodings: Compare Vanilla ViT (learned absolute) vs. Grid ViT (RoPE/OPE) on G2 (Grid Scaling) to validate the RoPE mechanism. 3. Stress Test Depth: Train on depth=3, test on depth=4. This checks if models fail simply due to sequence length or lack of iterative reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can models trained on COGITAO develop explicit object-centric or transformation-centric internal representations?
- Basis in paper: [explicit] The authors explicitly suggest that "analyzing internal model representations trained on COGITAO may reveal whether and how models develop object-centric or transformation-centric abstractions."
- Why unresolved: The provided experiments only measure behavioral performance (grid accuracy) and do not include mechanistic interpretability analyses or probing of the latent space.
- What evidence would resolve it: Probing classifiers that successfully decode specific transformation parameters or object identities from the model's hidden states, or causal interventions that disrupt specific reasoning steps.

### Open Question 2
- Question: Does curriculum learning on adjustable depth transformations improve generalization compared to standard training?
- Basis in paper: [explicit] The authors state that "due to its controllable environment and adjustable difficulty, COGITAO is well-suited for curriculum learning," implying this as a research direction.
- Why unresolved: The baseline experiments trained models on fixed datasets without adjusting the difficulty schedule or dynamically increasing transformation depth during training.
- What evidence would resolve it: Experiments demonstrating that models trained with a curriculum of increasing transformation depth achieve significantly higher out-of-distribution (OOD) accuracy on deep composite tasks (C3) than models trained on uniform distributions.

### Open Question 3
- Question: Can large foundation models master COGITAO tasks through in-context learning without weight updates?
- Basis in paper: [explicit] The discussion proposes "extending the framework to in-context learning, for example, by providing demonstration examples... particularly in large foundation models."
- Why unresolved: The paper evaluates small models (approx. 1.5M parameters) trained from scratch, explicitly excluding foundation models from the scope.
- What evidence would resolve it: Benchmarking large vision-language models (e.g., GPT-4V) using the COGITAO generator to provide in-context examples, showing successful zero-shot or few-shot generalization to novel transformation combinations.

## Limitations
- The exact task embedding implementation details for Compositional Generalization experiments are not fully specified, potentially affecting reproducibility
- The claim of "millions of unique task rules" is mathematically sound but lacks empirical sampling validation against all concurrent datasets
- High variance in performance across random seeds (up to 15%) suggests some instability in the experimental setup

## Confidence

**High:** The framework's mechanism for decoupling visual complexity from relational structure, the constraint-based generative engineering process, and the documented ID-OOD generalization gaps are well-supported by the methodology and results.

**Medium:** The claim about Grid ViT's relative positioning inductive biases mitigating generalization failures is plausible given the architectural changes described, but the ablation studies needed for definitive proof are not shown.

**Medium:** The assertion of "millions of unique task rules" is mathematically sound given the combinatorial space but lacks empirical sampling validation.

## Next Checks
1. **Task Embedding Fidelity:** Implement and validate the exact task embedding token vocabulary and encoding scheme for CompGen experiments, ensuring consistent mapping between transformation sequences and model input.
2. **Random Seed Sensitivity:** Run the core experiments (C1, C3, G2) with at least three different random seeds for object generation and dataset sampling, reporting mean accuracy ± SEM to quantify variance.
3. **Direct Dataset Comparison:** Benchmark COGITAO against at least one directly comparable compositional generalization dataset (e.g., Compositional-ARC) using the same model architecture and evaluation protocol to empirically validate the "orders of magnitude" diversity claim.