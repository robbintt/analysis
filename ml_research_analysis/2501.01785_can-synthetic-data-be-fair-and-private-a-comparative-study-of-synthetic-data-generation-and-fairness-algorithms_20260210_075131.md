---
ver: rpa2
title: Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic Data
  Generation and Fairness Algorithms
arxiv_id: '2501.01785'
source_url: https://arxiv.org/abs/2501.01785
tags:
- fairness
- data
- privacy
- synthetic
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the balance between privacy and fairness
  in synthetic data generation for learning analytics, addressing the challenge that
  privacy-preserving techniques often compromise fairness. Five synthetic data generators
  (CTGAN, DistilGPT2, ADSGAN, PATEGAN, and DECAF) were evaluated across three real-world
  datasets using privacy metrics (Jensen-Shannon Distance, Wasserstein Distance, membership
  inference accuracy, and k-anonymity) and fairness metrics (ABROCA, ERD, and TPRD).
---

# Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic Data Generation and Fairness Algorithms

## Quick Facts
- arXiv ID: 2501.01785
- Source URL: https://arxiv.org/abs/2501.01785
- Reference count: 40
- Synthetic data generation with fairness pre-processing improves fairness more effectively than when applied to real data, with CTGAN-RW showing the highest improvement (21.5% on one dataset).

## Executive Summary
This study investigates the balance between privacy and fairness in synthetic data generation for learning analytics, addressing the challenge that privacy-preserving techniques often compromise fairness. Five synthetic data generators (CTGAN, DistilGPT2, ADSGAN, PATEGAN, and DECAF) were evaluated across three real-world datasets using privacy metrics (Jensen-Shannon Distance, Wasserstein Distance, membership inference accuracy, and k-anonymity) and fairness metrics (ABROCA, ERD, and TPRD). Results show that DECAF achieves the best balance between privacy and fairness but suffers in predictive utility (AUC-ROC). Notably, applying pre-processing fairness algorithms (Suppression, Correlation Remover, Disparate Impact Remover, and Reweighing) to synthetic data improved fairness more effectively than when applied to real data, with CTGAN-RW showing the highest improvement (21.5% on one dataset). These findings suggest that combining synthetic data generation with fairness pre-processing offers a promising approach to creating fairer learning analytics models while maintaining acceptable privacy levels.

## Method Summary
The study evaluates five synthetic data generators (CTGAN, DistilGPT2, ADSGAN, PATEGAN, and DECAF) across three real-world datasets. Each SDG generates synthetic training data from a 70% training split of real data. Pre-processing fairness algorithms (Suppression, Correlation Remover, Disparate Impact Remover, and Reweighing) are optionally applied to the synthetic data before training downstream ML models. Models are evaluated on a 30% real test set for fairness and utility, while privacy metrics are computed directly on the synthetic data. The evaluation uses Same Train, Real Test paradigm to measure how well synthetic data preserves signal for fair downstream classification.

## Key Results
- DECAF achieves the best balance between privacy and fairness among tested SDGs but suffers in predictive utility (AUC-ROC 0.47-0.71 vs. 0.84-0.97 for real data).
- Applying pre-processing fairness algorithms to synthetic data improves fairness more effectively than when applied to real data, with CTGAN-RW showing the highest improvement (21.5% on one dataset).
- CTGAN + Reweighing/Suppression provides a moderate baseline with significant fairness gains when combined with post-hoc debiasing.

## Why This Works (Mechanism)

### Mechanism 1: Causal Debiasing for Dual Privacy-Fairness Gains
DECAF leverages causal inference to remove specific causal paths that encode bias during data generation. By allowing deletion of protected attributes (e.g., age, gender) during debiasing, it naturally reduces similarity to the original dataset, which incidentally improves privacy metrics. The causal structure enables targeted unfairness removal without requiring full distributional changes. Core assumption: the causal graph correctly identifies unfair causal paths, and removing them preserves sufficient signal for downstream tasks. Evidence anchors: [abstract], [section 4.1, p.10], [corpus]. Break condition: if the underlying causal graph is misspecified or if protected attributes are redundantly encoded in other features, DECAF's debiasing will fail to achieve fairness gains.

### Mechanism 2: Amplified Fairness Gains from Two-Stage Debiasing (SDG + Pre-processing)
SDGs inherently perform some debiasing by generating balanced distributions and smoothing outliers. When a pre-processing fairness algorithm (e.g., Reweighing, Suppression) is applied post-generation, it operates on data that has already been partially regularized, leading to compounding fairness improvements. Synthetic data also lacks the complex historical correlations in real data that can resist single-stage debiasing. Core assumption: the synthetic data preserves enough signal for the fairness algorithm to meaningfully adjust weights or remove correlations, and the two debiasing stages do not conflict. Evidence anchors: [abstract], [section 4.2, p.12], [corpus]. Break condition: if the SDG overly distorts the distribution (e.g., through aggressive differential privacy), subsequent fairness algorithms may have insufficient signal to correct remaining disparities.

### Mechanism 3: Privacy-Fairness-Utility Ternary Trade-off
Privacy-preserving mechanisms (e.g., differential privacy, identifiability reduction) and fairness constraints both require deviation from the original data distribution. While these deviations can align (removing sensitive attributes helps both goals), they also remove predictive signal. The paper operationalizes this via a Pareto frontier showing DECAF dominates privacy-fairness space but has the lowest AUC-ROC (0.47-0.71 vs. 0.84-0.97 for real data). Core assumption: utility is measured by AUC-ROC on downstream classification, and the fairness metrics (ABROCA, ERD, TPRD) fully capture relevant fairness dimensions. Evidence anchors: [abstract], [section 5, p.13], [corpus]. Break condition: if the downstream task is not classification, or if fairness is defined differently (e.g., individual fairness), the trade-off curve may shift.

## Foundational Learning

- **Concept: Differential Privacy (DP) and Membership Inference Attacks (MIA)**
  - **Why needed here:** PATEGAN uses DP (ε=1) to guarantee privacy; MIA is a key privacy metric. Understanding how noise injection affects both privacy and downstream fairness is essential for interpreting results.
  - **Quick check question:** Why does PATEGAN (DP-based) perform worse on fairness than ADSGAN (non-DP identifiability reduction)?

- **Concept: Causal Fairness and Counterfactual Reasoning**
  - **Why needed here:** DECAF's mechanism relies on causal graphs to identify unfair paths. Without this, you cannot understand why DECAF succeeds where correlation-based methods fail.
  - **Quick check question:** If a protected attribute is a mediator (not a direct cause) of the outcome, would removing it help or harm fairness?

- **Concept: Pre-processing vs. In-processing Fairness Interventions**
  - **Why needed here:** The study isolates pre-processing algorithms (Suppression, Correlation Remover, Disparate Impact Remover, Reweighing). Knowing when to apply pre-processing vs. constraints during training is critical for replication.
  - **Quick check question:** Why might Suppression fail if sensitive attributes are redundantly encoded in other features?

## Architecture Onboarding

- **Component map:** Real tabular datasets -> SDG Layer (5 options) -> Optional Fairness Layer (4 pre-processing algorithms) -> Downstream ML Models (4) -> Evaluation (Privacy, Fairness, Utility)

- **Critical path:**
  1. Split real data 70/30 (train/test)
  2. Train SDG on 70% training split -> generate synthetic training data
  3. (Optional) Apply pre-processing fairness algorithm to synthetic training data
  4. Train ML model on (debiased) synthetic data
  5. Evaluate on 30% real test set (Same Train, Real Test paradigm) for fairness and utility
  6. Evaluate synthetic data directly for privacy metrics

- **Design tradeoffs:**
  - **DECAF:** Best privacy-fairness, worst utility -> use when fairness/privacy are regulatory requirements and accuracy loss is acceptable
  - **PATEGAN:** Highest privacy, variable fairness -> use when privacy is paramount and fairness can be addressed separately
  - **CTGAN + Reweighing/Suppression:** Moderate baseline; fairness algorithms improve it significantly (up to 21.5% gain) -> use for balanced utility-fairness with post-hoc debiasing
  - **Small datasets (<500 rows):** Synthetic data less stable; expect higher variance in fairness algorithm effectiveness

- **Failure signatures:**
  - **DECAF AUC-ROC < 0.55:** Expected on small or highly biased datasets; signals causal graph may be removing too much signal
  - **PATEGAN fairness score drops while privacy improves:** DP noise disproportionately affects minority groups -> consider relaxed ε or hybrid approach
  - **Fairness algorithm causes negative improvement (e.g., ADSGAN + CoR, -3.8%):** Algorithm-SDG mismatch -> test alternative fairness method

- **First 3 experiments:**
  1. **Baseline SDG comparison:** Generate synthetic data from your dataset using all 5 SDGs; evaluate privacy (MIA, JSD) and fairness (ABROCA) without any debiasing. Identify Pareto frontier.
  2. **Fairness algorithm pairing:** For the SDG with lowest fairness, apply all 4 pre-processing algorithms. Measure which combination yields highest fairness improvement with acceptable utility loss.
  3. **Utility threshold test:** Using DECAF, sweep over causal path removal intensity (if adjustable) or try hybrid DECAF + mild Reweighing to test if utility can be recovered while maintaining fairness gains.

## Open Questions the Paper Calls Out
None

## Limitations
- Small dataset sizes (<500 rows for law and student datasets) may limit generalizability and increase variance in SDG behavior.
- Causal graph correctness for DECAF is critical but not validated against ground truth causal structures.
- Fairness definitions (ABROCA, ERD, TPRD) focus on group-level disparities; individual fairness and intersectional fairness are not addressed.
- "Same Train, Real Test" paradigm assumes synthetic data generation preserves sufficient signal for fair representation in the real test set.

## Confidence
- **High Confidence:** DECAF's dual privacy-fairness gains through causal path removal; ternary privacy-fairness-utility tradeoff; SDG + pre-processing synergy (21.5% improvement on CTGAN-RW)
- **Medium Confidence:** Causal graph specification in DECAF; utility measurements across all datasets; effectiveness of pre-processing algorithms on small datasets
- **Low Confidence:** Claims about SDGs inherently performing debiasing; generalization to larger or different domains; intersectional fairness implications

## Next Checks
1. Validate DECAF's causal graph specification using domain expert review or causal discovery algorithms to ensure unfair paths are correctly identified.
2. Test the SDG + pre-processing synergy on larger datasets (>10,000 rows) to assess scalability and variance reduction.
3. Evaluate intersectional fairness by extending ABROCA/ERD to combinations of protected attributes (e.g., race × disability) and measuring whether SDG + pre-processing gains persist.