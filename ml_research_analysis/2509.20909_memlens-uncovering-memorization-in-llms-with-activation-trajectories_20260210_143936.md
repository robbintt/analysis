---
ver: rpa2
title: 'MemLens: Uncovering Memorization in LLMs with Activation Trajectories'
arxiv_id: '2509.20909'
source_url: https://arxiv.org/abs/2509.20909
tags:
- entropy
- variance
- samples
- layer
- memorization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MemLens, a representation-based method for
  detecting memorization in large language models (LLMs) by analyzing activation trajectories
  during generation. The key insight is that contaminated samples exhibit "shortcut"
  behaviors, locking onto an answer with high confidence early in the forward pass,
  while clean samples show more gradual evidence accumulation across layers.
---

# MemLens: Uncovering Memorization in LLMs with Activation Trajectories

## Quick Facts
- **arXiv ID:** 2509.20909
- **Source URL:** https://arxiv.org/abs/2509.20909
- **Reference count:** 40
- **Primary result:** MemLens detects memorization in LLMs by analyzing activation trajectories, outperforming baselines on original, rephrased, translated, and perturbed inputs, with causal validation via LoRA injection.

## Executive Summary
This paper introduces MemLens, a representation-based method for detecting memorization in large language models by analyzing activation trajectories during generation. The key insight is that contaminated samples exhibit "shortcut" behaviors, locking onto an answer with high confidence early in the forward pass, while clean samples show more gradual evidence accumulation across layers. MemLens extracts features from intermediate activations, focusing on digit token probabilities, entropy, and confidence metrics across layers, and trains a CNN-based discriminator to classify samples as contaminated or clean. Experiments on multiple LLMs show MemLens achieves robust detection across various input transformations, significantly outperforming existing methods. A LoRA injection study provides causal validation, demonstrating that fine-tuning on clean data systematically increases detection scores, confirming that MemLens captures genuine memorization signals.

## Method Summary
MemLens detects memorization by extracting 24-channel activation trajectories from intermediate hidden states during generation. For each sample, it tokenizes the input, runs a forward pass with output_hidden_states=True, and applies logit lens projection at each layer to compute digit token probabilities. The method constructs a trajectory containing 10 digit probabilities, entropy, max confidence, and their first-order differences across layers. A TinyTSConv CNN discriminator (3 Conv1d layers with kernel size 3, ReLU, global average pooling) is trained on these trajectories to classify samples as contaminated or clean. The approach focuses on digit tokens for mathematical benchmarks, normalizes trajectories per-sample per-channel, and uses Youden's J index for threshold selection.

## Key Results
- MemLens achieves 89-95% F1 on original inputs and 60-90% F1 on rephrased variants across four LLMs
- Outperforms perplexity and output distribution baselines on all four text distributions (original, rephrased, translated, perturbed)
- LoRA injection study shows detection scores increase from 2.2% to 45.1% as rank increases from 0 to 512
- Full trajectory analysis outperforms early-layer-only approaches (F1 improves from 64-81% to 89-95%)

## Why This Works (Mechanism)

### Mechanism 1: Early-Layer Confidence Locking in Memorized Samples
Contaminated samples exhibit "shortcut" behaviors where the model locks onto the correct answer with high confidence in early layers, while clean samples show gradual evidence accumulation across the full depth. Memorization creates compressed retrieval pathways that bypass deeper reasoning, activating strongly in early layers. The logit lens projection of intermediate hidden states reveals this as early dominance of a single digit channel.

### Mechanism 2: Full Trajectory Shape as a Memorization Fingerprint
The entire activation pattern—how digit probabilities evolve, compete, and resolve across depth—is a more reliable signal than single-point metrics. MemLens constructs a 24-channel trajectory per sample and uses a learned CNN discriminator to capture complex temporal patterns that hand-crafted rules cannot detect.

### Mechanism 3: Causal Sensitivity to Injected Memorization (LoRA Injection)
When memorization is artificially injected via LoRA fine-tuning on clean samples, MemLens detection scores increase monotonically with LoRA rank. This causal validation demonstrates that the detector captures genuine memorization signals rather than spurious correlations, as the rising scores must arise from the injected exposures with fixed discriminator and test data.

## Foundational Learning

- **Logit Lens and Intermediate Activation Probing:** Why needed: MemLens projects hidden states at each layer to vocabulary logits, requiring understanding of how to extract and interpret pℓ from hℓWLM. Quick check: Given a hidden state hℓ at layer ℓ and language modeling head WLM, how would you compute the probability distribution over digit tokens only?

- **Time-Series Classification with 1D CNNs:** Why needed: The discriminator treats activation trajectories as multi-channel time series, using 1D convolutions to detect patterns. Quick check: For a trajectory X ∈ R24×T (24 channels, T layers), what does a Conv1d layer with kernel size 3 compute, and how does global average pooling produce a fixed-size representation?

- **LoRA Fine-Tuning Mechanism:** Why needed: The causal validation experiment requires understanding how LoRA injects memorization through low-rank updates. Quick check: If LoRA adds ΔW = BA where B ∈ Rd×r and A ∈ Rr×d, how does increasing rank r affect the expressivity of the injected knowledge?

## Architecture Onboarding

- **Component map:** Feature Extraction -> Trajectory Construction (24 channels) -> CNN Discriminator (TinyTSConv) -> Binary Classification
- **Critical path:** 1) Identify answer position in generated output, 2) Extract trajectory up to layer T, 3) Z-score normalize per sample per channel, 4) Feed to trained discriminator, 5) Apply threshold via Youden's J
- **Design tradeoffs:** Full depth vs. early layers (full depth most reliable), hand-crafted features vs. learned discriminator (CNN captures complex patterns), digit-only restriction works for math benchmarks but may need adaptation for other domains
- **Failure signatures:** High false positives on clean samples (threshold too aggressive), low detection on rephrased/translated inputs (tokenizer issues), inconsistent scores across runs (check random seeds and GPU nondeterminism)
- **First 3 experiments:** 1) Baseline replication with Qwen2.5-7B on AIME 1983-2024 vs. AIME 2025, 2) Layer ablation testing first 1/3, 1/2, and full depth, 3) LoRA injection sanity check on Llama-3.1-8B with r=64

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- **Scope restriction:** The digit-centric analysis inherently limits applicability to arithmetic benchmarks, with unclear generalization to non-numeric domains
- **Threshold calibration:** Youden's J index may not align with all real-world use cases where the relative cost of false positives versus false negatives varies
- **Causal validation assumptions:** The LoRA injection study assumes equivalence between LoRA fine-tuning dynamics and natural pretraining contamination

## Confidence
- **High Confidence:** Early-layer confidence locking observation, LoRA injection results demonstrating monotonic increases in detection scores
- **Medium Confidence:** Cross-model generalization claims, superiority over baseline methods
- **Low Confidence:** Uniform digit token extraction across tokenizer architectures, CNN discriminator capturing complex patterns rather than overfitting

## Next Checks
- **Cross-Domain Generalization:** Apply MemLens to non-numeric benchmarks (e.g., code generation, factual QA) to test trajectory-based memorization signal extension
- **Threshold Sensitivity Analysis:** Systematically vary detection threshold around Youden's J optimum and plot precision-recall curves
- **Alternative Memorization Mechanisms:** Design experiments to detect memorization without early-layer confidence locking (deep reasoning pathways reproducing training data)