---
ver: rpa2
title: A Multi-Memory Segment System for Generating High-Quality Long-Term Memory
  Content in Agents
arxiv_id: '2508.15294'
source_url: https://arxiv.org/abs/2508.15294
tags:
- memory
- content
- long-term
- retrieval
- units
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of low-quality memory content
  in long-term memory systems for agents, which negatively impacts recall performance
  and response quality. Most existing methods simply store summarized versions of
  historical dialogues, lacking the multi-dimensional and multi-component generation
  process observed in human long-term memory formation.
---

# A Multi-Memory Segment System for Generating High-Quality Long-Term Memory Content in Agents

## Quick Facts
- arXiv ID: 2508.15294
- Source URL: https://arxiv.org/abs/2508.15294
- Reference count: 9
- Primary result: Multi-memory segment system (MMS) outperforms existing methods on LoCoMo dataset, achieving leading performance across multiple LLMs in both recall and generation tasks

## Executive Summary
This paper addresses the problem of low-quality memory content in long-term memory systems for agents, which negatively impacts recall performance and response quality. Most existing methods simply store summarized versions of historical dialogues, lacking the multi-dimensional and multi-component generation process observed in human long-term memory formation. To address this issue, the authors propose a multi-memory segment system (MMS) inspired by cognitive psychology theory, which processes short-term memory into multiple long-term memory segments and constructs retrieval memory units and contextual memory units based on these segments. During retrieval, MMS matches the most relevant retrieval memory units based on the user's query, and the corresponding contextual memory units are used as context for the response stage to enhance knowledge. Experiments on the LoCoMo dataset demonstrate that MMS achieves leading performance on multiple mainstream large language models, significantly outperforming existing methods in both recall and generation tasks. The system shows robust performance across different numbers of memory segments and exhibits practical value despite slightly higher latency and token overhead compared to simpler methods.

## Method Summary
The Multi-Memory Segment System processes short-term memory (dialogue content) into four specialized long-term memory segments: keywords, cognitive perspectives, episodic memory, and semantic memory. These segments are extracted using LLM prompts and then organized into two distinct unit types: retrieval memory units (containing keywords, STM content, cognitive perspectives, and episodic memory) for matching user queries, and contextual memory units (containing keywords, STM content, cognitive perspectives, and semantic memory) for enhancing generation responses. The system employs a one-to-one correspondence between these unit types, allowing relevant context to be retrieved alongside matching results. Vector embeddings are created for retrieval units and stored in a vector database, enabling efficient cosine similarity-based retrieval during query processing.

## Key Results
- MMS achieves leading performance across multiple mainstream LLMs (GPT-4o, Qwen2.5-14B, Gemini-2.5-pro-preview) on LoCoMo dataset
- Outperforms existing methods in both recall tasks (R@1, R@3, R@5) and generation tasks (F1, BLEU-1)
- Shows robust performance across different numbers of memory segments (n=1 to 9)
- Demonstrates practical value despite slightly higher latency and token overhead compared to simpler methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing short-term memory into multiple specialized memory segments (keywords, cognitive perspectives, episodic, semantic) improves retrieval recall by matching diverse user query formulations.
- Mechanism: Instead of storing a single summary vector, MMS extracts and vectorizes four distinct components from dialogue. A user query is matched against this richer representation set, increasing the probability that at least one component aligns semantically with the query's specific angle (encoding specificity principle).
- Core assumption: Users query historical information from varied cognitive angles that a single summary fails to capture.
- Evidence anchors:
  - [abstract] "...processes short-term memory into multiple long-term memory segments... and constructs retrieval memory units..."
  - [section 3.2] "...Mkey, Mshort, Mcog, Mepi, and Msem are used... Mkey, Mshort, Mcog and Mepi as retrieval memory units M Uret..."
  - [corpus] Related work like "Memory OS of AI Agent" (FMR=0.51) also proposes structured memory management, suggesting the general approach of multi-component memory systems is a recognized research direction, though MMS's specific 4-segment split is its contribution.

### Mechanism 2
- Claim: Separating "Retrieval Memory Units" (for matching) from "Contextual Memory Units" (for generation) optimizes each stage for its specific task, improving final response quality.
- Mechanism: Retrieval units include episodic memory (event descriptions) but exclude semantic memory (factual knowledge points) because episodic content is more query-like. Contextual units include semantic memory but exclude episodic memory to avoid redundancy, as the original short-term memory already contains event details. This one-to-one mapping ensures the retrieval match pulls the most relevant knowledge context.
- Core assumption: Information optimal for semantic matching (retrieval) differs from information optimal for knowledge enhancement (generation).
- Evidence anchors:
  - [abstract] "...constructs retrieval memory units and contextual memory units... with a one-to-one correspondence..."
  - [section 5.4] "...retrieval rate decreases after adding semantic memory... generation quality decreases after adding episodic memory... Thus, the composition modules... are reasonable."

### Mechanism 3
- Claim: Using a Large Language Model (LLM) to extract cognitive perspectives from dialogue creates "multi-dimensional cognition" that enriches memory content beyond simple keywords.
- Mechanism: The system uses an LLM to analyze dialogue and generate a list of "cognitive perspectives," capturing diverse angles (e.g., emotional, logical, practical) of the conversation. This structured insight is stored as a memory segment, allowing future queries to match against these high-level interpretations.
- Core assumption: LLMs can reliably extract meaningful, distinct, and query-relevant cognitive perspectives from dialogue.
- Evidence anchors:
  - [section 3.2] "Cognitively analyze short-term memory from different perspectives... to enhance the matching effect."
  - [section 5.1] "...incorporating this part of content from multiple cognitive perspectives... can indeed enhance high recall rates."

## Foundational Learning

- **Tulving's Memory System Classification (Episodic vs. Semantic Memory)**
  - Why needed here: The entire MMS architecture is built on splitting memory into these two types. Understanding that *episodic* memory is about personal events/situations while *semantic* memory is about facts/knowledge points is critical for interpreting why they are assigned to different memory units.
  - Quick check question: In MMS, which memory segment type is excluded from retrieval units because it's considered "higher-level knowledge extraction" less similar to a user's question phrasing?

- **Encoding Specificity Principle**
  - Why needed here: This principle justifies the multi-segment approach. It states that memory retrieval is most effective when the context at retrieval matches the context at encoding. MMS creates multiple "contextual hooks" (perspectives, keywords, etc.) to increase the odds of a match with a user's varied query.
  - Quick check question: How does storing multiple cognitive perspectives of a conversation directly apply the encoding specificity principle to improve recall?

- **Cosine Similarity in Vector Space**
  - Why needed here: This is the core mathematical operation for the retrieval mechanism. You must understand that both memory units and user queries are embedded as vectors, and their semantic relatedness is measured by the cosine of the angle between them.
  - Quick check question: Given two vectors, what does a cosine similarity score of 0.95 versus 0.2 indicate about their semantic relationship?

## Architecture Onboarding

- **Component map:** Dialogue -> LLM Extraction -> Segment Assembly -> Vector Storage -> (Query -> Vector Match -> Context Lookup) -> LLM Response
- **Critical path:** The quality of the initial LLM extraction is the single most critical point of failure
- **Design tradeoffs:**
  - Recall Quality vs. Token/Latency Cost: MMS generates 4x more content per memory than a simple summary. This increases storage and processing cost, though the paper claims the latency increase is minimal and justified by performance gains
  - Retrieval Precision vs. Context Richness: Excluding semantic memory from retrieval may hurt recall if a user's query is purely fact-based
- **Failure signatures:**
  - Retrieval Degradation: If R@N scores drop, check the LLM's extracted segments. Are cognitive perspectives generic? Are keywords missing?
  - Hallucination/Redundancy in Response: If generation quality (F1, BLEU) drops, verify that episodic memory is not leaking into contextual units
  - Latency Spikes: If memory construction latency becomes unacceptable, consider processing dialogues in batches or using a smaller, fine-tuned model for the extraction task
- **First 3 experiments:**
  1. Baseline Reproduction: Implement the Naive RAG and MMS using the prompts in Table 5. Run a small set of dialogues through both to see the difference in extracted memory content quality firsthand
  2. Ablation of Segments: Using the LoCoMo dataset, re-run the MMS pipeline but systematically remove one segment at a time. Compare the R@1 and F1 scores to the paper's results
  3. Segment Composition Swap: Experiment by including semantic memory in retrieval units and episodic memory in contextual units. Measure the performance drop to empirically confirm the paper's design rationale

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a dedicated, fine-tuned memory model utilizing Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) outperform the current prompt-based approach in generating high-quality long-term memories?
- Basis in paper: [explicit] Section 7 (Limitations) states, "We are considering constructing a dedicated memory model in the future specifically for generating high-quality memories, employing Supervised Fine-Tuning and Reinforcement Learning techniques."

### Open Question 2
- Question: Would a dynamic architecture that adapts the composition of retrieval and contextual units based on query type (e.g., Single-hop vs. Multi-hop) improve performance over the current static configuration?
- Basis in paper: [inferred] The ablation study (Table 3) reveals that removing specific components occasionally improves performance in specific tasks, and the text notes that "different memory segments can be used... based on different character scenarios."

### Open Question 3
- Question: Does the absence of an explicit forgetting or memory consolidation mechanism limit the system's scalability when memory banks grow significantly larger than those in the LoCoMo dataset?
- Basis in paper: [inferred] While Section 5.5 tests robustness with n=1 to 9 memory segments, the paper relies solely on content quality to differentiate signal from noise, unlike related work (MemoryBank) which uses the Ebbinghaus curve

## Limitations
- Experimental setup details (dataset splits, evaluation protocols) are not fully specified, making direct replication difficult
- LLM dependency for memory extraction creates variability and potential model-specific performance differences
- Token overhead and latency increases from 4x memory expansion could become prohibitive at scale

## Confidence

**High Confidence**: The core architectural design (segmenting memory into keywords, cognitive perspectives, episodic, and semantic components) is well-supported by ablation studies and empirical evidence shows clear performance improvements over baselines.

**Medium Confidence**: The specific assignment of segments to retrieval vs contextual units is supported by ablation results but could be task-dependent. The paper's evidence shows this is generally beneficial but doesn't explore edge cases.

**Low Confidence**: The claim that MMS "achieves leading performance" is based on comparison with only 4 baselines on a single dataset. Without broader benchmarking, this claim overstates generalizability.

## Next Checks

1. **Segment Contribution Validation**: Run systematic ablations on each memory segment across multiple datasets beyond LoCoMo to verify that the reported contributions hold in different contexts.

2. **Prompt Robustness Testing**: Test the same MMS pipeline with different LLM providers using the same prompts to measure how sensitive the memory extraction quality is to the underlying model.

3. **Cost-Benefit Scaling Analysis**: Measure how retrieval quality and generation performance scale with increasing memory volume while tracking token costs and latency to validate whether performance gains justify overhead in practical deployments.