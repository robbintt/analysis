---
ver: rpa2
title: 'Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study'
arxiv_id: '2504.02733'
source_url: https://arxiv.org/abs/2504.02733
tags:
- https
- wang
- language
- instructions
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how to make LLMs more robust to small changes
  in their task instructions. The authors focus on perturbations like typos or word
  replacements that significantly hurt performance.
---

# Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study

## Quick Facts
- arXiv ID: 2504.02733
- Source URL: https://arxiv.org/abs/2504.02733
- Reference count: 36
- Primary result: Iterative self-denoising reduces performance drop by up to 59.2% on perturbed instructions

## Executive Summary
This paper investigates how to improve large language models' robustness to small instruction perturbations like typos or word replacements that significantly degrade performance. The authors evaluate multiple approaches including self-denoising, perplexity smoothing, instruction ensembling, and hidden representation alignment. Their main finding is that iterative self-denoising - where the model iteratively refines its understanding of perturbed instructions - substantially outperforms other methods, reducing performance degradation by up to 59.2% compared to baselines. The approach works by guiding the model to reconstruct clean instructions from perturbed versions, effectively teaching it to recognize and correct instruction-level noise.

## Method Summary
The paper tests several methods to enhance LLM robustness to instruction perturbations. Self-denoising involves either fine-tuning the model on perturbed-to-clean instruction pairs or using the base model iteratively with a meta-prompt containing in-context examples. Perplexity smoothing applies temperature scaling to smooth probability distributions but performs poorly without in-context examples. Instruction ensembling combines predictions from multiple perturbed instruction versions. Hidden representation alignment uses a separate encoder to map perturbed and clean instruction representations to a shared space. The most effective approach uses iterative self-denoising with a meta-prompt containing in-context examples, where the model refines its understanding through multiple reasoning passes.

## Key Results
- Iterative self-denoising reduces performance drop by up to 59.2% compared to baselines
- The method works with both fine-tuned and base models when using meta-prompts with in-context examples
- Perplexity smoothing actually worsens performance when used without in-context examples
- Instruction ensembling and representation alignment show modest improvements but less than self-denoising

## Why This Works (Mechanism)
Self-denoising works by explicitly training or prompting the model to recognize and correct instruction-level noise. When given a perturbed instruction, the model learns to reconstruct the intended clean instruction through iterative refinement. This process leverages the model's existing reasoning capabilities while focusing attention on instruction comprehension rather than being derailed by superficial changes. The meta-prompt with in-context examples provides concrete demonstrations of how to handle perturbations, making the correction process more systematic and reliable across different perturbation types.

## Foundational Learning
- **Instruction Perturbations**: Small changes to task instructions (typos, word replacements) that significantly degrade LLM performance - needed to understand the problem space and measure robustness improvements
- **Self-Denoising**: Training or prompting models to reconstruct clean inputs from corrupted versions - quick check: can the model recover original meaning from noisy instructions?
- **In-Context Learning**: Providing examples within prompts to guide model behavior - quick check: does including examples improve perturbation handling?
- **Iterative Refinement**: Multiple reasoning passes to improve outputs - quick check: does allowing multiple attempts reduce error rates?
- **Meta-Prompts**: Prompts that guide how to use other prompts or examples - quick check: does framing the task as instruction correction help?
- **Hidden Representation Alignment**: Mapping different input representations to a shared space - quick check: do aligned representations improve cross-perturbation consistency?

## Architecture Onboarding

### Component Map
Base LLM -> Meta-prompt (with in-context examples) -> Iterative refinement loop -> Final prediction

### Critical Path
Perturbed instruction → Meta-prompt processing → Self-denoising iteration → Clean instruction reconstruction → Task execution

### Design Tradeoffs
Fine-tuning vs. prompting: Fine-tuning requires additional training data but may generalize better; prompting is more flexible but depends heavily on example quality. Iterative vs. single-pass: Multiple iterations improve accuracy but increase computation time and latency.

### Failure Signatures
- Performance degrades when perturbation types fall outside training distribution
- Self-denoising may get stuck in local minima with complex perturbations
- Meta-prompt effectiveness heavily depends on example quality and relevance
- Representation alignment may fail when perturbed and clean instructions have very different semantic embeddings

### First Experiments
1. Test self-denoising on a new perturbation type (e.g., word deletions) not covered in the original study
2. Compare fine-tuned vs. prompted self-denoising on the same dataset with identical evaluation conditions
3. Evaluate whether improvements persist after deployment with unseen perturbation patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses on limited perturbation types (typos, word replacements) and may not generalize to other noise patterns
- Some methods compared under different conditions (e.g., perplexity smoothing without in-context examples), making direct comparisons difficult
- Uneven dataset sampling (40 examples for arithmetic vs 20 for WikiSQL) may affect method comparison validity

## Confidence

**High Confidence**: Self-denoising significantly improves robustness (up to 59.2% reduction in performance drop) across multiple datasets with consistent comparative advantage.

**Medium Confidence**: Effectiveness rankings of different methods are generally supported but uneven evaluation conditions introduce uncertainty about exact magnitude differences.

**Low Confidence**: Claim that perplexity smoothing "exacerbates" performance issues is based on specific implementation choice (without in-context examples) and may not generalize.

## Next Checks
1. Test self-denoising approach on additional datasets and perturbation types not included in original study
2. Re-run all methods under identical conditions to establish more definitive effectiveness rankings
3. Evaluate whether self-denoising improvements persist after deployment with new, unseen perturbation patterns