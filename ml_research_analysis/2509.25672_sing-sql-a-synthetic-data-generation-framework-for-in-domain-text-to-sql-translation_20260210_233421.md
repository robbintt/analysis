---
ver: rpa2
title: 'SING-SQL: A Synthetic Data Generation Framework for In-Domain Text-to-SQL
  Translation'
arxiv_id: '2509.25672'
source_url: https://arxiv.org/abs/2509.25672
tags:
- schema
- synthetic
- text-to-sql
- data
- column
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SING-SQL, a two-stage framework for generating
  high-quality synthetic Text-to-SQL data tailored to any specific relational database
  without relying on SQL logs or manual annotations. The method hierarchically partitions
  a database schema into sub-schemas, synthesizes SQL queries across multiple complexity
  levels, and applies a quality-aware pipeline that includes LLM-as-a-judge validation,
  executability checks, automatic repair, and column balancing.
---

# SING-SQL: A Synthetic Data Generation Framework for In-Domain Text-to-SQL Translation

## Quick Facts
- arXiv ID: 2509.25672
- Source URL: https://arxiv.org/abs/2509.25672
- Reference count: 40
- Primary result: Introduces SING-SQL, a two-stage framework for generating high-quality synthetic Text-to-SQL data tailored to any specific relational database without relying on SQL logs or manual annotations.

## Executive Summary
This paper introduces SING-SQL, a two-stage framework for generating high-quality synthetic Text-to-SQL data tailored to any specific relational database without relying on SQL logs or manual annotations. The method hierarchically partitions a database schema into sub-schemas, synthesizes SQL queries across multiple complexity levels, and applies a quality-aware pipeline that includes LLM-as-a-judge validation, executability checks, automatic repair, and column balancing. Experiments on the BIRD benchmark and synthetic evaluation sets show that SingSQL-LM models fine-tuned on this data achieve strong in-domain generalization, with SingSQL-LM-3B-R64 reaching 82.87% Soft F1 and 73.03% EX upper bound on BIRD-Dev, and outperforming baselines by large margins on synthetic splits.

## Method Summary
SING-SQL is a two-stage framework that generates synthetic Text-to-SQL training data tailored to a specific relational database. It hierarchically partitions the database schema into sub-schemas using foreign-key constraints and sliding windows, synthesizes SQL queries at multiple complexity levels, and translates them to natural language questions using an LLM. A quality-aware pipeline validates and filters the generated pairs, performs automatic repair of non-executable queries, and balances column coverage through focused re-generation. The synthetic dataset is then used to fine-tune compact models (1.5B-3B parameters) with LoRA, achieving strong in-domain generalization on BIRD benchmarks.

## Key Results
- SingSQL-LM-3B-R64 achieves 82.87% Soft F1 and 73.03% EX upper bound on BIRD-Dev (California Schools)
- Outperforms baseline models by large margins on synthetic evaluation splits
- Schema-free fine-tuning with schema-only inference provides the most robust results
- Column balancing ensures all schema elements are adequately covered in the synthetic data

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical sub-schema partitioning improves semantic alignment and coverage by decomposing the target database into table-level and column-level sub-schemas using foreign-key constraints and sliding windows. This structure reduces generation noise and ensures each schema element appears in at least one focused context, enabling the LLM to generate more semantically coherent SQL-text pairs.

### Mechanism 2
Multi-stage quality-aware pipeline with validation, repair, and balancing enhances reliability by applying LLM-as-a-judge validation to discard flawed pairs, executability checks with automatic repair for non-executable queries, and column-focused balancing to ensure underrepresented columns receive additional coverage. This filtering reduces noise and improves overall quality and schema coverage.

### Mechanism 3
Schema-free fine-tuning combined with schema-only inference yields robust performance by forcing the model to internalize SQL patterns during training without explicit schema context, then providing schema grounding at inference without the noise of few-shot examples or reasoning traces. This approach shows better generalization than schema-aware training methods.

## Foundational Learning

- **Text-to-SQL Translation**: Converting natural language questions into SQL queries requires schema linking and understanding query semantics. Understanding these challenges is essential to grasp why SING-SQL's components are designed as they are. Quick check: Can you explain the difference between schema linking and candidate generation in a Text-to-SQL pipeline?

- **Synthetic Data Generation for Training**: Generating synthetic training data involves trade-offs between cost and quality, coverage and noise. Understanding these trade-offs helps explain the motivation behind SING-SQL's multi-stage quality-aware pipeline. Quick check: What are two common problems with synthetic Text-to-SQL data that SING-SQL tries to address?

- **Parameter-Efficient Fine-Tuning (PEFT/LoRA)**: SING-SQL uses LoRA for fine-tuning compact models on synthetic data, reducing computational costs for enterprise settings. Understanding PEFT is crucial for grasping how the approach remains scalable. Quick check: How does LoRA reduce the computational cost of fine-tuning compared to full model fine-tuning?

## Architecture Onboarding

- **Component map**: Sub-Schema Generator -> SQL-Text Synthesizer -> Quality Assurance Pipeline -> Column Balancer -> LoRA Fine-Tuning -> Schema-Only Inference

- **Critical path**: Sub-Schema Generation → SQL-Text Synthesis → Quality Assurance & Balancing → LoRA Fine-Tuning → Schema-Only Inference. The quality and coverage of synthetic data directly determine the fine-tuned model's in-domain performance.

- **Design tradeoffs**: 
  - Window size (w) vs. stride (s): Larger window with smaller stride increases sub-schema overlap and coverage but also generation cost
  - Schema-aware (T2SWS) vs. schema-free (T2S) training: Schema-aware provides explicit grounding but risks overfitting; schema-free with schema-only inference shows better generalization
  - Validation strictness vs. data volume: Aggressive filtering improves quality but may discard useful examples

- **Failure signatures**:
  - Low schema coverage: Many columns appear in <N generated SQL queries
  - High non-executable rate: >20-30% of generated SQL queries fail to execute even after repair
  - Performance drop with few-shots: Adding few-shot examples at inference significantly reduces accuracy compared to schema-only

- **First 3 experiments**:
  1. **Sub-Schema Ablation**: Vary window size (w) and stride (s) parameters and measure resulting sub-schema count, data volume, and schema coverage to find cost-effective configuration
  2. **Context Management Validation**: Replicate experiment by fine-tuning models with T2S and T2SWS datasets and evaluating with different inference contexts to confirm schema-free training + schema-only inference yields best results
  3. **End-to-End Quality Check**: Generate synthetic data for a sample database, manually inspect SQL-text pairs for flaws, and measure executability, logical validity, and column coverage before/after quality pipeline

## Open Questions the Paper Calls Out

**Open Question 1**: Can automated techniques effectively identify and preserve semantically related column groupings during sub-schema partitioning to prevent separation of functionally linked data? The current framework treats non-key columns randomly, lacking semantic understanding to keep frequently co-queried columns together without manual intervention.

**Open Question 2**: Does integrating reinforcement learning with execution-based rewards provide significant performance gains over the supervised fine-tuning approach? The authors note they do not incorporate RL and leave exploring execution-result alignment via RL as future work.

**Open Question 3**: Does generating diverse reasoning paths for synthetic data improve model robustness compared to the single divide-and-conquer prompting strategy? The authors state they use only a single prompting strategy and leave exploring multiple reasoning paths to future work due to costs.

**Open Question 4**: How does the framework perform on databases where foreign-key constraints are absent, incomplete, or inconsistent? The reliance on foreign-key constraints is listed as a limitation, noting that missing constraints currently require manual specification.

## Limitations

- **Data Quality Dependencies**: Performance heavily depends on LLM-as-a-judge validation and automatic repair mechanisms, which lack detailed evaluation of accuracy or quantification of false rates
- **Generalizability Constraints**: Evaluation limited to a single database (California Schools with 3 tables and 89 columns), effectiveness on databases with different characteristics remains untested
- **Computational Cost Considerations**: Paper does not provide detailed cost analysis of generation process; computational expense of multiple LLM calls at scale for enterprise databases could be substantial

## Confidence

**High Confidence**: Experimental results showing SingSQL-LM-3B-R64 achieving 82.87% Soft F1 and 73.03% EX upper bound on BIRD-Dev are directly supported by ablation studies and comparisons with baselines.

**Medium Confidence**: Claim that schema-free fine-tuning combined with schema-only inference provides most robust results is supported by paper's own ablation study but based on experiments with single database and model architecture.

**Low Confidence**: Assertion that hierarchical sub-schema partitioning significantly improves semantic alignment and coverage is primarily supported by final performance metrics rather than direct ablation studies comparing different partitioning strategies.

## Next Checks

**Check 1**: Generate synthetic data for a small database (5-10 tables, 50-100 columns) and manually evaluate a random sample of 100 SQL-text pairs before and after each quality assurance stage to quantify effectiveness of LLM-as-judge and repair mechanisms.

**Check 2**: Apply SING-SQL to three databases with different characteristics - one with complex many-to-many relationships, one with extensive use of views, and one with non-standard schema patterns - to assess generalizability beyond California Schools database.

**Check 3**: For an enterprise-scale database (50+ tables, 500+ columns), measure total computational cost of complete SING-SQL pipeline and compare to performance gains achieved to determine break-even point where generation cost outweighs benefits.