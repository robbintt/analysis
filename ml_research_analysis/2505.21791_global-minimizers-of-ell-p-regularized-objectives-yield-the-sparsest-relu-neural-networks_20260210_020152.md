---
ver: rpa2
title: Global Minimizers of $\ell^p$-Regularized Objectives Yield the Sparsest ReLU
  Neural Networks
arxiv_id: '2505.21791'
source_url: https://arxiv.org/abs/2505.21791
tags:
- sout
- which
- solutions
- networks
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies sparse single-hidden-layer ReLU neural networks,\
  \ where the goal is to find the sparsest interpolating network\u2014one with the\
  \ fewest nonzero parameters or neurons. The main contribution is showing that minimizing\
  \ the \u2113^p quasinorm of network weights for 0 < p < 1 provably yields the sparsest\
  \ interpolating ReLU network, for sufficiently small p."
---

# Global Minimizers of $\ell^p$-Regularized Objectives Yield the Sparsest ReLU Neural Networks

## Quick Facts
- arXiv ID: 2505.21791
- Source URL: https://arxiv.org/abs/2505.21791
- Authors: Julia Nakhleh; Robert D. Nowak
- Reference count: 40
- Primary result: Minimizing $\ell^p$ path norm for $0 < p < 1$ provably yields sparsest interpolating ReLU networks

## Executive Summary
This paper establishes that minimizing $\ell^p$ quasinorms of network weights for $0 < p < 1$ can provably recover the sparsest interpolating single-hidden-layer ReLU neural networks. The work bridges the gap between combinatorial $\ell^0$ minimization and smooth optimization by showing that sufficiently small $p$ values enable gradient-based training to find truly sparse solutions without pruning. The theoretical results leverage the Bauer maximum principle to reformulate $\ell^p$ minimization as a concave optimization problem over a polytope, providing the first rigorous justification for using smooth $\ell^p$ penalties in sparse ReLU network training.

## Method Summary
The method trains single-hidden-layer ReLU networks by minimizing MSE loss plus $\ell^p$ path norm regularization, where the path norm is defined as $\sum_k \|v_k w_k\|_p^p$ for multivariate inputs or $\sum_k |w_k v_k|^p$ for univariate inputs. The training uses an iteratively reweighted $\ell^1$ algorithm (Algorithm 1) with weighted soft-thresholding proximal updates. For univariate cases, a balancing constraint $\|w_k\|_2 = |v_k|$ is enforced. The algorithm is implemented with Adam optimizer, tested on both univariate peak/plateau datasets and multivariate synthetic data, comparing sparsity across different $p$ values and against unregularized and weight decay baselines.

## Key Results
- Global minimizers of $\ell^p$ path norm are unique for almost every $0 < p < 1$ and require no more than $N-2$ neurons in univariate cases
- For any input dimension, there exists a data-dependent threshold $p^*$ such that $\ell^p$ minimization recovers the sparsest $\ell^0$ solution for all $p < p^*$
- The reweighted $\ell^1$ algorithm with small $p$ recovers much sparser solutions earlier in training than unregularized or weight decay-regularized networks

## Why This Works (Mechanism)
The mechanism relies on the mathematical properties of $\ell^p$ quasinorms for $0 < p < 1$. As $p$ decreases, the $\ell^p$ penalty increasingly favors sparse solutions by creating a sharper penalty for nonzero weights. The paper shows that for sufficiently small $p$, the global minimizer of the $\ell^p$ regularized objective coincides with the sparsest $\ell^0$ solution. This occurs because the $\ell^p$ ball becomes increasingly "spiky" near the axes as $p \to 0$, making sparse solutions optimal. The iteratively reweighted $\ell^1$ algorithm approximates this behavior through a sequence of weighted $\ell^1$ problems, where the weights are updated based on current parameter magnitudes raised to the power $p-1$.

## Foundational Learning
**$\ell^p$ quasinorms**: For $0 < p < 1$, these are not true norms but satisfy relaxed triangle inequality. They create sharper penalties for nonzero values compared to $\ell^1$, promoting sparsity. Needed to understand why small $p$ favors sparse solutions.

**Bauer maximum principle**: States that a continuous concave function on a compact convex set achieves its maximum at an extreme point. Used to prove uniqueness of global minimizers. Needed to establish theoretical guarantees for $\ell^p$ minimization.

**Proximal gradient methods**: Iterative optimization technique combining gradient steps with proximal operators (like soft-thresholding). Used in the reweighted $\ell^1$ algorithm. Needed to implement the optimization procedure efficiently.

**ReLU network path norm**: Sum of $\ell^2$ norms of incoming weights to each neuron, weighted by output weights. Measures network complexity. Needed to formulate the regularization objective.

**Convex vs concave optimization**: The paper reformulates $\ell^p$ minimization as a concave problem over a polytope. Needed to apply the Bauer maximum principle and prove theoretical results.

## Architecture Onboarding
**Component map**: Data -> ReLU network -> $\ell^p$ path norm regularization -> MSE loss -> Iteratively reweighted $\ell^1$ optimization -> Sparse solution

**Critical path**: The key insight is that $\ell^p$ minimization for small $p$ transforms a combinatorial $\ell^0$ problem into a smooth optimization problem while preserving sparsity properties. The balancing constraint $\|w_k\|_2 = |v_k|$ ensures the network can represent any continuous piecewise linear function.

**Design tradeoffs**: Small $p$ values promote sparsity but may increase numerical instability in weight updates. The balancing constraint simplifies the univariate case but adds complexity to the multivariate formulation. The iteratively reweighted approach approximates $\ell^p$ minimization but requires careful weight scheduling.

**Failure signatures**: Non-convergence or oscillation when regularization $\lambda$ is too large relative to data scale. Numerical instability when computing thresholds $C_{k,i} = \lambda p |v_k w_{k,i}|^{p-1}$ for small $p$. Failure to reach interpolation if $\lambda$ is too small or $p$ is too close to 1.

**First experiments**: 
1. Verify univariate peak/plateau experiment recovers 3-neuron sparsest solution with $p=0.4$
2. Test multivariate synthetic experiment with $N=10$ points in $d=50$ dimensions
3. Compare sparsity trajectories across $p \in \{0.4, 0.7, 1\}$ for both datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical results are asymptotic and require $p < p^*$ for some data-dependent threshold
- Proof techniques rely on specific geometric properties that may not generalize to deeper networks
- Computational experiments limited to small-scale synthetic datasets with simple patterns

## Confidence
**High**: Main theoretical claims (Theorem 1, Theorem 2) follow from rigorous mathematical arguments
**Medium**: Algorithmic framework is well-defined but lacks convergence guarantees
**Low**: Practical scalability to complex real-world datasets not demonstrated

## Next Checks
1. Reproduce univariate peak/plateau experiment to verify $p=0.4$ recovers the known 3-neuron sparsest solution
2. Implement multivariate synthetic experiment with $N=10$ points in $d=50$ dimensions and verify sparsity improvements
3. Test algorithm on regression dataset with more complex patterns (e.g., sine waves or polynomials) to evaluate generalization beyond simple interpolation tasks