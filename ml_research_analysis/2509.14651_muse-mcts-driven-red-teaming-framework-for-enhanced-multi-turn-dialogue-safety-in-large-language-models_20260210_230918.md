---
ver: rpa2
title: 'MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety
  in Large Language Models'
arxiv_id: '2509.14651'
source_url: https://arxiv.org/abs/2509.14651
tags:
- attack
- safety
- multi-turn
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MUSE introduces a dual-strategy framework to defend against multi-turn
  jailbreak attacks on large language models. It combines MUSE-A, which uses frame
  semantics and Monte Carlo Tree Search to systematically generate diverse adversarial
  prompts across dialogue turns, and MUSE-D, which applies turn-level preference optimization
  to enhance safety alignment.
---

# MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models

## Quick Facts
- arXiv ID: 2509.14651
- Source URL: https://arxiv.org/abs/2509.14651
- Reference count: 17
- Primary result: MUSE framework doubles attack success rates compared to state-of-the-art methods while reducing jailbreak success by up to 24% without harming model capabilities

## Executive Summary
MUSE introduces a dual-strategy framework to defend against multi-turn jailbreak attacks on large language models. It combines MUSE-A, which uses frame semantics and Monte Carlo Tree Search to systematically generate diverse adversarial prompts across dialogue turns, and MUSE-D, which applies turn-level preference optimization to enhance safety alignment. Experiments show MUSE-A doubles attack success rates compared to state-of-the-art methods, while MUSE-D reduces jailbreak success by up to 24% without harming model capabilities. The framework is effective across model scales and extends to both multi-turn and single-turn attack settings.

## Method Summary
MUSE employs a dual-strategy framework: MUSE-A generates diverse adversarial prompts using frame semantics and MCTS, while MUSE-D strengthens safety through turn-level preference optimization. The action space leverages three strategies (expansion, decomposition, redirection) to create varied semantic trajectories. MCTS with UCT selection explores attack sequences, and successful attacks plus high-risk intermediate nodes are used to fine-tune target models via DPO, intervening early in dialogues to prevent safety bypasses.

## Key Results
- MUSE-A achieves 22.2% ASR on JailbreakBench, doubling state-of-the-art methods
- MUSE-D reduces ASR by up to 24% compared to standard DPO on out-of-distribution targets
- Framework maintains model capabilities, preserving helpfulness on GSM8K and MMLU benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Frame Semantics Action Space for Diverse Semantic Trajectories
- **Claim**: Structured action space using frame semantics enables systematic exploration of multi-turn attack paths beyond superficial modifications.
- **Mechanism**: Three complementary strategies (Aexp, Adec, Ared) generate candidate conversational moves: expansion adds frame-adjacent concepts to build contextual legitimacy; decomposition splits malicious goals into innocuous sub-queries; redirection reframes requests through professional/educational personas.
- **Core assumption**: Frame semantics theory accurately models how humans structure conceptual relationships in dialogue. Assumption: the three strategies sufficiently cover the space of realistic multi-turn attacks.
- **Evidence anchors**:
  - [abstract]: "uses frame semantics and heuristic tree search to explore diverse semantic trajectories"
  - [section]: Ablation shows 3.5-10% performance drop when any single strategy removed (Table 5)
  - [corpus]: Neighboring work "X-Teaming" also emphasizes multi-agent diversity; "M2S" consolidates multi-turn to single-turn, suggesting cross-method relevance
- **Break condition**: If target models are trained specifically on frame-semantic patterns, or if action space generation becomes too stereotyped, diversity gains diminish.

### Mechanism 2: MCTS-Guided Search for Efficient Vulnerability Discovery
- **Claim**: Monte Carlo Tree Search with UCT balances exploration-exploitation to systematically discover high-yield attack sequences.
- **Mechanism**: Each dialogue turn is a node; edges are actions from the frame-based action space. UCT selection prioritizes actions with high cumulative reward relative to visit count. Rollouts simulate trajectories to terminal states; backpropagation updates Q-values. Binary reward (GPT-4o judge score <5 = success) provides sparse but decisive feedback.
- **Core assumption**: The attack success signal at trajectory end provides sufficient credit assignment for intermediate turns. Assumption: GPT-4o's safety judgments (88.8% human agreement at threshold 5, Appendix A.5) are reliable proxies for true harm.
- **Evidence anchors**:
  - [abstract]: "heuristic tree search to explore diverse semantic trajectories"
  - [section]: "MUSE-A achieves faster convergence, exhibiting a steeper initial increase in success rate" (Figure 3, Section 5.7)
  - [corpus]: "Tree-based Dialogue Reinforced Policy Optimization" (arxiv 2510.02286) shows related tree-based approaches achieve comparable FMR~0.52, suggesting tree search is a productive direction
- **Break condition**: If maximum turn depth T is insufficient for convergence, or if reward sparsity prevents learning, efficiency degrades. Section B.1 notes saturation at 3-5 turns.

### Mechanism 3: Turn-Level Preference Optimization with High-Risk Node Targeting
- **Claim**: Fine-grained DPO on both attack endpoints and intermediate high-risk nodes strengthens early-turn safety activation.
- **Mechanism**: MCTS-derived risk scores Q(ct-1,at)/N(ct-1,at) identify high-risk intermediate nodes above threshold τ. Training data includes preference pairs (c̃t, yt, ysafe_t) from successful attacks AND high-risk nodes. Self-reflection generates safer responses. Loss optimizes turn-level preference, encouraging safe outputs at each decision point.
- **Core assumption**: High-risk nodes (not just successful attacks) represent vulnerability patterns worth training against. Assumption: self-reflection can reliably generate safer alternatives.
- **Evidence anchors**:
  - [abstract]: "fine-grained safety alignment approach that intervenes early in dialogues"
  - [section]: Up to 24% ASR reduction vs standard DPO (Table 2); Table 4 shows single-turn defense transfer
  - [corpus]: "MTSA: Multi-turn Safety Alignment" (arxiv 2505.17147) proposes related multi-round red-teaming for alignment; corpus evidence for turn-level optimization specifically is limited
- **Break condition**: If threshold τ is poorly calibrated, or if self-reflection fails to produce genuinely safer outputs, training signal degrades. Section A.4 sets τ=5 based on human evaluation.

## Foundational Learning

- **Concept: Monte Carlo Tree Search (MCTS)**
  - **Why needed here**: Core search algorithm for MUSE-A; must understand selection, expansion, simulation, backpropagation, and UCT balancing.
  - **Quick check question**: Can you explain why UCT's exploration term (λ√(ln N/N)) prevents premature convergence to locally optimal paths?

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here**: MUSE-D extends DPO with turn-level granularity; need to understand the preference learning objective and how β controls signal sharpness.
  - **Quick check question**: How does DPO differ from RLHF in its treatment of reward modeling?

- **Concept: Frame Semantics (Fillmore)**
  - **Why needed here**: Theoretical foundation for action space construction; understanding semantic frames as networks of related concepts.
  - **Quick check question**: What is a semantic frame, and how does "expansion" vs "decomposition" leverage different frame relationships?

## Architecture Onboarding

- **Component map**:
  - Action Space Generator (Aexp/Adec/Ared via prompted LLM) -> MCTS Engine (selection via UCT, expansion, rollout simulation) -> Reward Evaluator (GPT-4o judge with threshold 5)
  - Trajectory Collector (from MUSE-A rollouts) -> Risk Node Identifier (τ threshold filter) -> Preference Dataset Builder (c̃t, yt, ysafe triples) -> Fine-tuning Loop (DPO-style loss)

- **Critical path**:
  1. Generate action space entries for target question using three prompt templates (Appendix C)
  2. Run Nsim MCTS simulations with T_max=5 turns, collecting trajectories
  3. Filter trajectories for successful attacks + high-risk nodes (risk ratio > τ=5)
  4. Generate safer responses via self-reflection prompts
  5. Fine-tune target model on preference pairs for 3 epochs with β=0.4

- **Design tradeoffs**:
  - **Exploration constant λ**: Higher values increase diversity but slow convergence; paper doesn't specify value (check code)
  - **Maximum turns T_max**: 5 turns balances efficiency vs saturation (Figure 5); shorter may miss late-turn vulnerabilities
  - **Threshold τ**: τ=5 maximizes human agreement (88.8% accuracy); higher thresholds reduce training data, lower may include noise
  - **Uncensored attack model size**: 30B achieves 22.2% ASR vs 7B's 15.4% (Table 10); capability matters for context maintenance

- **Failure signatures**:
  - **Low ASR improvement**: Check action space diversity; ablation shows each strategy contributes 3.5-10%
  - **Defense ineffective**: Verify high-risk node identification; if τ too high, insufficient training signal
  - **Capability degradation**: Monitor GSM8K/MMLU; Table 2 shows MUSE-D preserves helpfulness but verify on your target domain
  - **Slow convergence**: Examine cumulative ASR curves (Figure 3); MUSE-A should show steeper initial rise than CoA/ActorAttack

- **First 3 experiments**:
  1. **Validate MUSE-A baseline**: Replicate Table 1 on JailbreakBench with Llama-3-8B target; expect ~22% ASR with std<2% (Table 8)
  2. **Ablate action space**: Remove one strategy at a time; confirm each contributes 3.5-10% to ASR per Table 5
  3. **Test MUSE-D transfer**: Train on Llama-3-8B data, evaluate on Qwen2.5-7B (OOD setting from Table 2); expect defense to hold but verify no capability drop on GSM8K

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can online reinforcement learning be integrated with MUSE-D to enable real-time safety adaptation during deployment, rather than relying solely on offline preference optimization?
- Basis in paper: [explicit] The Limitations section states: "Future work could integrate online reinforcement learning... to adapt responses via real-time feedback, enhancing resilience to evolving attacks."
- Why unresolved: Current MUSE-D uses static DPO with fixed preference pairs; online RL would require continuous feedback loops and reward modeling during inference.
- What evidence would resolve it: A modified framework showing reduced attack success rates against adaptive adversaries that change strategies mid-conversation, compared to the offline-only baseline.

### Open Question 2
- Question: What is the minimum attacker model capability (parameter count, reasoning ability) required for MUSE-A to achieve effective multi-turn jailbreaks?
- Basis in paper: [inferred] Appendix B.5 shows that replacing the 30B attacker model with a 7B model reduced ASR from 22.2% to 15.4%, suggesting a capability threshold exists but not defining it precisely.
- Why unresolved: The paper demonstrates capability dependence but does not systematically evaluate the boundary conditions or specific reasoning abilities (e.g., logical consistency, semantic understanding) that are critical.
- What evidence would resolve it: A controlled study varying attacker model size and specific capabilities, identifying the minimum threshold where MUSE-A matches baseline performance.

### Open Question 3
- Question: How effectively does MUSE-D generalize across linguistic and cultural contexts, given that the human evaluation used Chinese annotators on English benchmarks?
- Basis in paper: [inferred] The human evaluation (Appendix A.5) employed Chinese annotators, and all benchmarks (HarmBench, JailbreakBench) are English-based; the multilingual attack baseline showed strong cross-lingual transfer but defense generalization remains untested.
- Why unresolved: Safety norms and adversarial patterns differ across cultures; it is unclear whether frame-semantic strategies trained on English data transfer effectively.
- What evidence would resolve it: Experiments applying MUSE-D to multilingual benchmarks (e.g., native-language HarmBench translations) with annotators from corresponding cultural backgrounds.

### Open Question 4
- Question: Can iterative adversarial training, where models are continuously exposed to newly discovered attack tactics, improve MUSE-D's robustness compared to the current static alignment approach?
- Basis in paper: [explicit] The Limitations section suggests: "adding iterative adversarial training... that continually exposes models to new tactics could more effectively reveal and fortify vulnerabilities."
- Why unresolved: Current MUSE-D uses a fixed dataset from MUSE-A; a dynamic training loop would require automated attack discovery and integration without human labeling bottlenecks.
- What evidence would resolve it: A curriculum learning experiment where MUSE-D is periodically fine-tuned on newly generated attacks, showing sustained or improved defense performance over time against evolving attack strategies.

## Limitations

- Critical design parameters (MCTS simulation count Nsim, exploration constant λ) are underspecified, affecting reproducibility and computational efficiency.
- Reliance on GPT-4o as both judge and attack model introduces potential biases, though human agreement rates provide some validation.
- Frame semantics coverage remains untested against novel attack patterns; three strategies may not comprehensively capture all realistic multi-turn attacks.

## Confidence

- **High confidence**: MUSE-A's ability to double ASR compared to state-of-the-art methods (Table 1, Table 8 with std<2%), and MUSE-D's up to 24% ASR reduction (Table 2)
- **Medium confidence**: The effectiveness of turn-level preference optimization as a defense mechanism, given limited corpus validation beyond related multi-round approaches
- **Medium confidence**: The scalability of frame semantics-based action spaces to novel attack patterns, pending systematic coverage analysis

## Next Checks

1. **Validate action space coverage**: Systematically test MUSE-A's performance when individual frame strategies (Aexp, Adec, Ared) are removed to confirm each contributes 3.5-10% ASR (per Table 5 ablation) and assess whether additional strategies could further improve coverage.

2. **Test defense transfer robustness**: Replicate the out-of-distribution defense evaluation by training MUSE-D on Llama-3-8B data and evaluating on Qwen2.5-7B, monitoring both ASR reduction and capability preservation on GSM8K/MMLU to ensure no degradation.

3. **Benchmark judge reliability**: Conduct controlled experiments varying the GPT-4o judge threshold (5±1) to quantify sensitivity of MUSE-A's attack success rates to judgment criteria, validating the 88.8% human agreement benchmark.