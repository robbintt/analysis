---
ver: rpa2
title: Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement
  Learning
arxiv_id: '2505.20621'
source_url: https://arxiv.org/abs/2505.20621
tags:
- robustness
- poisoning
- arxiv
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the vulnerability of offline reinforcement
  learning to data poisoning attacks, which can compromise both per-state action stability
  and overall policy performance. The proposed approach, Multi-level Certified Defense
  (MuCD), leverages differential privacy mechanisms to provide robustness guarantees
  at two levels: action-level robustness ensuring stable state-specific actions and
  policy-level robustness providing lower bounds on expected cumulative reward.'
---

# Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.20621
- Source URL: https://arxiv.org/abs/2505.20621
- Authors: Shijie Liu; Andrew C. Cullen; Paul Montague; Sarah Erfani; Benjamin I. P. Rubinstein
- Reference count: 40
- Primary result: Provides robust guarantees against data poisoning attacks in offline RL with 50% performance drop tolerance at 7% poisoned data

## Executive Summary
This work addresses the critical vulnerability of offline reinforcement learning systems to data poisoning attacks that can compromise both state-specific action stability and overall policy performance. The proposed Multi-level Certified Defense (MuCD) framework leverages differential privacy mechanisms to provide two distinct robustness guarantees: action-level robustness ensuring stable state-specific actions and policy-level robustness providing lower bounds on expected cumulative reward. The framework is designed to work across both discrete and continuous action spaces, as well as deterministic and stochastic environments.

The approach employs a randomized training process using differential privacy mechanisms and derives theoretical bounds through outcomes guarantees. Experimental results demonstrate significant improvements over prior work, with policy performance drops limited to 50% even with 7% of training data poisoned—a substantial improvement over the 0.008% achieved by previous methods. Additionally, the certified radii are 5 times larger than existing approaches, validating the framework's effectiveness in enhancing safety and reliability in offline RL systems.

## Method Summary
The Multi-level Certified Defense (MuCD) framework addresses offline RL poisoning attacks through a differential privacy-based randomized training process. The method provides two levels of robustness guarantees: action-level robustness that ensures stable state-specific actions, and policy-level robustness that provides lower bounds on expected cumulative reward. The framework is applicable to both discrete and continuous action spaces, as well as deterministic and stochastic environments. By leveraging DP mechanisms during training, the approach creates certified bounds against adversarial data contamination while maintaining reasonable performance levels in poisoned scenarios.

## Key Results
- Policy performance drops limited to 50% even with 7% of training data poisoned, compared to 0.008% in previous methods
- Certified radii are 5 times larger than existing approaches
- Framework provides robustness guarantees at both action-level and policy-level
- Effective across both discrete and continuous action spaces, as well as deterministic and stochastic environments

## Why This Works (Mechanism)
The framework works by introducing differential privacy mechanisms during the training process of offline reinforcement learning. By randomizing the training procedure, the approach creates inherent robustness against data poisoning attacks. The DP noise injection ensures that small perturbations in the training data (such as those introduced by an adversary) do not significantly affect the learned policy. The multi-level approach provides both fine-grained guarantees at the action level (ensuring stable state-specific actions) and coarse-grained guarantees at the policy level (ensuring reasonable cumulative reward bounds), making it comprehensive in its defense against various attack strategies.

## Foundational Learning
- **Differential Privacy**: A mathematical framework for quantifying privacy guarantees by adding calibrated noise to data processing. Needed to provide provable robustness against data poisoning attacks. Quick check: Verify that the privacy budget ε is appropriately tuned for the RL application.
- **Offline Reinforcement Learning**: Learning policies from pre-collected datasets without environment interaction. Needed as the target domain where poisoning attacks are most critical. Quick check: Ensure the dataset size and quality are sufficient for stable learning.
- **Certified Robustness**: Providing mathematical guarantees about model behavior under adversarial perturbations. Needed to move beyond empirical defenses to provable security. Quick check: Validate that the certified bounds hold under different attack strengths.
- **Randomized Training Mechanisms**: Using stochastic processes during training to create inherent robustness. Needed to transform the learning process into one that's naturally resistant to poisoning. Quick check: Monitor training stability with and without DP noise.

## Architecture Onboarding
- **Component Map**: Data Pipeline → DP Randomization Layer → Policy Network → Robustness Verifier → Performance Evaluator
- **Critical Path**: The DP randomization layer is the critical component that provides the core defense mechanism, connecting the data pipeline to the policy network
- **Design Tradeoffs**: Higher DP noise provides stronger robustness guarantees but may degrade baseline performance; lower noise improves performance but weakens defenses
- **Failure Signatures**: Degradation in certified radii, increased variance in policy performance, or violation of theoretical bounds under attack
- **First Experiments**: 1) Baseline performance without DP noise, 2) Robustness testing with synthetic poisoning at controlled rates, 3) Certified radius validation across different privacy budgets

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead of DP-based randomized training is not quantified, potentially limiting practical deployment
- Performance in large-scale, high-dimensional RL problems common in real-world applications remains unverified
- Claims about practical deployment readiness exceed the empirical evidence from tested scenarios

## Confidence
- **High Confidence**: Theoretical framework for action-level and policy-level robustness using DP mechanisms is sound and well-established
- **Medium Confidence**: Experimental results showing improved robustness against poisoning attacks are convincing for tested scenarios
- **Low Confidence**: Claims about performance in real-world large-scale RL systems are not substantiated by current experimental setup

## Next Checks
1. **Benchmark Diversity Test**: Validate the framework across a broader suite of RL benchmarks (e.g., Atari, MuJoCo, continuous control tasks) to assess generalization across different state and action space complexities.

2. **Computational Overhead Measurement**: Conduct detailed analysis of training time, memory usage, and sample efficiency compared to non-certified baselines across varying levels of privacy guarantees (different ε values).

3. **Adaptive Attack Robustness**: Evaluate the framework against adaptive poisoning strategies that specifically target DP noise patterns, testing whether certified bounds hold under more sophisticated attack models.