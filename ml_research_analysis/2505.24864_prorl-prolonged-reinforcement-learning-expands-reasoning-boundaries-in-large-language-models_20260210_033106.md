---
ver: rpa2
title: 'ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large
  Language Models'
arxiv_id: '2505.24864'
source_url: https://arxiv.org/abs/2505.24864
tags:
- reasoning
- pass
- final
- smoothed
- deepseek-r1-distill-qwen-1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether reinforcement learning can genuinely
  expand a language model's reasoning boundaries or simply amplify existing capabilities.
  The authors introduce ProRL, a prolonged RL training methodology incorporating KL
  divergence penalties, reference policy resets, and diverse task exposure to maintain
  training stability over extended periods.
---

# ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models

## Quick Facts
- arXiv ID: 2505.24864
- Source URL: https://arxiv.org/abs/2505.24864
- Reference count: 40
- Key outcome: ProRL enables performance on tasks where base models fail entirely, demonstrating reasoning boundary expansion rather than simple capability amplification

## Executive Summary
This work investigates whether reinforcement learning can genuinely expand a language model's reasoning boundaries or simply amplify existing capabilities. The authors introduce ProRL, a prolonged RL training methodology incorporating KL divergence penalties, reference policy resets, and diverse task exposure to maintain training stability over extended periods. Their 1.5B model, Nemotron-Research-Reasoning-Qwen-1.5B, trained via ProRL, achieves substantial improvements over the base model: +14.7% on math, +13.9% on coding, +54.8% on logic puzzles, +25.1% on STEM, and +18.1% on instruction following benchmarks. Most significantly, ProRL enables performance gains on tasks where the base model fails entirely, demonstrating expansion of reasoning boundaries. The improvements correlate with initial base model competence and training duration, with extended training producing novel reasoning trajectories and sustained gains on challenging tasks. The results challenge prior assumptions about RL limitations and establish that prolonged training with appropriate techniques can meaningfully expand reasoning capabilities beyond pretraining distributions.

## Method Summary
The authors introduce ProRL (Prolonged Reinforcement Learning), a methodology designed to expand reasoning boundaries through extended RL training. The approach incorporates three key stability mechanisms: KL divergence penalties to prevent catastrophic forgetting, periodic reference policy resets to maintain training direction, and diverse task exposure to prevent overfitting to specific domains. The training is conducted on a 1.5B parameter model, Nemotron-Research-Reasoning-Qwen-1.5B, with prolonged training periods that allow for the emergence of novel reasoning capabilities. The methodology is specifically designed to address the challenge of maintaining stable RL training over extended periods while enabling genuine capability expansion rather than simple optimization of existing abilities.

## Key Results
- ProRL achieves +14.7% improvement on math benchmarks, +13.9% on coding, +54.8% on logic puzzles, +25.1% on STEM, and +18.1% on instruction following
- Most significantly, ProRL enables performance on tasks where the base model fails entirely, demonstrating reasoning boundary expansion
- Improvements correlate with initial base model competence and training duration, with extended training producing novel reasoning trajectories

## Why This Works (Mechanism)
ProRL works by maintaining stable reinforcement learning over extended training periods while preventing the collapse into local optima that typically limits RL effectiveness. The KL divergence penalties ensure the model doesn't deviate too far from its learned knowledge base, preventing catastrophic forgetting of previously acquired capabilities. Reference policy resets periodically realign the training trajectory with the original model's strengths, maintaining diversity in reasoning approaches. The prolonged training duration allows the model to explore and develop novel reasoning strategies that weren't present in the original pretraining distribution, effectively expanding its reasoning boundaries rather than just optimizing existing capabilities.

## Foundational Learning
- **Reinforcement Learning Fundamentals**: Why needed - to understand how reward signals shape model behavior; Quick check - verify understanding of policy gradient methods and their limitations
- **KL Divergence Regularization**: Why needed - to prevent catastrophic forgetting during extended training; Quick check - confirm understanding of how KL penalties maintain stability
- **Catastrophic Forgetting**: Why needed - to recognize why standard RL often fails on extended training; Quick check - identify scenarios where models lose previously learned capabilities
- **Policy Optimization**: Why needed - to grasp how reference resets guide training; Quick check - explain the difference between on-policy and off-policy optimization
- **Extended Training Dynamics**: Why needed - to understand how prolonged exposure enables capability expansion; Quick check - describe how training duration affects model exploration

## Architecture Onboarding

Component Map:
Base Model -> RL Training (with KL penalties) -> Reference Policy Resets -> Diverse Task Exposure -> Expanded Reasoning Capabilities

Critical Path:
Initial model → KL-stabilized RL → Policy reset checkpoints → Task diversification → Performance gains on novel tasks

Design Tradeoffs:
- Stability vs. exploration: KL penalties ensure stability but may limit exploration
- Reset frequency: Too frequent limits learning; too infrequent risks divergence
- Task diversity: Broad exposure enables expansion but may dilute domain-specific gains

Failure Signatures:
- Performance degradation on base tasks (over-exploration)
- Training collapse or divergence (insufficient stabilization)
- Plateauing without capability expansion (inadequate training duration)

First Experiments:
1. Baseline RL without KL penalties to demonstrate stability necessity
2. Shorter training duration to show boundary expansion correlation with time
3. Single-task vs. diverse-task training to isolate task exposure effects

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis of novel reasoning trajectories could potentially reflect optimization artifacts rather than genuinely new reasoning capabilities
- The 1.5B parameter model size raises questions about scalability to larger models where training dynamics may differ significantly
- The prolonged training methodology's stability mechanisms are not fully characterized in terms of their individual contributions

## Confidence
- **High confidence**: The empirical demonstration that ProRL enables performance on previously failed tasks
- **Medium confidence**: The characterization of reasoning boundary expansion versus capability amplification
- **Medium confidence**: The correlation patterns between base model competence, training duration, and performance gains

## Next Checks
1. Ablation study to isolate the individual contributions of KL divergence penalties, policy resets, and diverse task exposure to the observed improvements
2. Scaling experiments to test whether ProRL's boundary-expanding effects persist on larger models (10B+ parameters)
3. Detailed analysis of reasoning trajectories using mechanistic interpretability methods to verify that novel reasoning patterns represent genuine capability expansion rather than optimization artifacts