---
ver: rpa2
title: 'Robust Vision-Language Models via Tensor Decomposition: A Defense Against
  Adversarial Attacks'
arxiv_id: '2509.16163'
source_url: https://arxiv.org/abs/2509.16163
tags:
- tensor
- adversarial
- decomposition
- defense
- rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a tensor decomposition-based defense mechanism
  to improve the robustness of vision-language models (VLMs) like CLIP against adversarial
  attacks. The method works by applying low-rank tensor decomposition (CP, Tucker,
  and Tensor Train) to internal representations of the VLM, effectively filtering
  high-frequency adversarial noise while preserving semantic meaning.
---

# Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks

## Quick Facts
- **arXiv ID**: 2509.16163
- **Source URL**: https://arxiv.org/abs/2509.16163
- **Reference count**: 21
- **Primary result**: Tensor decomposition defense recovers 12.3% performance on Flickr30K and 8.1% on COCO against PGD attacks.

## Executive Summary
This paper introduces a tensor decomposition-based defense mechanism to improve the robustness of vision-language models (VLMs) like CLIP against adversarial attacks. The method works by applying low-rank tensor decomposition (CP, Tucker, and Tensor Train) to internal representations of the VLM, effectively filtering high-frequency adversarial noise while preserving semantic meaning. The approach requires no retraining or architectural changes, making it a practical plug-and-play solution. Experiments on MS-COCO and Flickr30K datasets show that Tensor Train decomposition with low rank (8-32) and low residual strength (α=0.1-0.2) achieves the best results, recovering 12.3% performance on Flickr30K and 8.1% on COCO.

## Method Summary
The defense applies tensor decomposition to vision encoder representations at specific transformer layers. Forward hooks intercept tensors from normalization layers, which are then decomposed using TensorLy with specified rank. The decomposed representation is blended with the original via residual connection: T_final = α·T + (1-α)·T̂. The method uses three decomposition types: CP (rank-one tensor sums), Tucker (single core tensor), and Tensor Train (sequential 3D core sequences). The optimal configuration found was Tensor Train decomposition with rank 8-32, α=0.1-0.2, applied to normalization layers of the last 5 transformer blocks. Implementation uses PyTorch and TensorLy with random initialization, 50 iterations, and 10⁻⁴ tolerance.

## Key Results
- Tensor Train decomposition achieves 19.8% Recall@1 on Flickr30K vs. 7.5% baseline (12.3% recovery)
- On COCO, accuracy improves from 3.8% to 11.9% (8.1% recovery)
- Multi-layer configuration (5 layers) provides 3.93x overhead vs. 1.22x for single-layer
- Lower rank (8-32) and lower α (0.1-0.2) values provide optimal defense

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Tensor Approximation Filters High-Frequency Adversarial Noise
Adversarial perturbations manifest as high-frequency components in VLM feature representations, which low-rank tensor decomposition preferentially removes while preserving semantic structure. The decomposition reconstructs feature tensors using fewer components (rank 8-32), acting as a low-pass filter that retains primary semantic information but discards the high-frequency adversarial patterns that require higher-rank terms to represent.

### Mechanism 2: Residual Connection Controls Defense-Utility Tradeoff
Blending original and decomposed representations via residual strength parameter α allows tunable balance between adversarial filtering and semantic preservation. T_final = α·T + (1-α)·T̂, where lower α values (0.1-0.2) weight the filtered representation more heavily, removing more adversarial signal at potential cost to clean accuracy.

### Mechanism 3: Tensor Train Decomposition Preserves Structural Information Better Than CP/Tucker
Tensor Train decomposition achieves superior adversarial filtering because its sequential 3D core structure captures higher-order interactions more effectively than CP's rank-one sums or Tucker's single core tensor. TT decomposes T[i₁,...,i_d] ≈ G₁[i₁]·...·G_d[i_d], preserving local structural relationships across tensor modes that CP's simple summation and Tucker's global core cannot maintain.

## Foundational Learning

- **Tensor decomposition methods (CP, Tucker, Tensor Train)**: Understanding how each method factorizes tensors determines which preserves semantic structure while filtering noise.
  - *Quick check*: Can you explain why CP uses rank-one tensor sums while TT uses sequential core products?

- **Adversarial attacks and perturbation budgets (ℓ∞ constraint, PGD)**: Knowing how attacks are generated clarifies what signal the defense must remove.
  - *Quick check*: What does ε = 8/255 mean in terms of maximum pixel perturbation?

- **Vision-language model architecture (CLIP vision encoder, transformer blocks, normalization layers)**: Selecting target layers for decomposition requires knowing where representations are most vulnerable.
  - *Quick check*: Why might normalization layers be better targets than MLP outputs?

## Architecture Onboarding

- **Component map**: Input image → CLIP ViT-B/32 vision encoder → Forward hooks intercept tensors at specified layers → Tensor decomposition (CP/Tucker/TT) → Residual reconstruction T_final = α·T + (1-α)·T̂ → Text-image similarity output

- **Critical path**: Hook placement → decomposition rank selection → α tuning → multi-layer configuration

- **Design tradeoffs**:
  - Lower rank (8-32): Better adversarial filtering, risk of semantic loss
  - Lower α (0.1-0.2): Stronger defense, potential clean accuracy drop
  - More layers (5 vs. 1): Higher recovery (12.3% vs. ~10%), 3.93x vs. 1.22x overhead

- **Failure signatures**:
  - High rank (>96): Adversarial noise preserved, minimal recovery
  - High α (>0.5): Insufficient filtering, weak defense
  - Wrong layer (MLP vs. norm): Suboptimal results per Figure 5

- **First 3 experiments**:
  1. Single-layer TT (rank=32, α=0.2) on final normalization layer to establish baseline defense performance on your dataset.
  2. Parameter sweep: α ∈ {0.1, 0.2, 0.3} × rank ∈ {8, 16, 32} to find optimal configuration.
  3. Multi-layer extension to last 5 transformer normalization blocks to quantify performance-overhead tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
Would adaptive rank selection dynamically tuned per layer or per input improve defense effectiveness over fixed rank values (8-32)? The paper states future work will explore adaptive rank selection and effectiveness in other VLM architectures and attack types.

### Open Question 2
Does the tensor decomposition defense generalize to other VLM architectures (e.g., ALIGN, BLIP, Florence) and vision encoder designs beyond CLIP's ViT-B/32? The paper notes their tests focused on the CLIP model and effectiveness on other architectures remains untested.

### Open Question 3
Is the defense effective against adaptive attacks specifically designed to bypass tensor decomposition filtering? The paper tested only PGD attacks and notes that adaptive adversaries who optimize perturbations accounting for the decomposition-reconstruction pipeline remain unevaluated.

### Open Question 4
Would advanced tensor decomposition methods (Hierarchical Tucker, Block-Term Decomposition) achieve superior adversarial filtering compared to CP, Tucker, and Tensor Train? The paper states advanced methods could be more effective than the three classical techniques tested.

## Limitations
- The assumption that adversarial perturbations occupy distinct high-frequency subspaces from semantic content remains theoretically unproven.
- The method's effectiveness depends critically on proper parameter tuning (rank, α, layer selection) with no automated selection procedure.
- The defense shows moderate success on two datasets but lacks evaluation against diverse attack types beyond PGD.
- Computational overhead scales significantly with multi-layer deployment.

## Confidence

- **High Confidence**: The empirical effectiveness of Tensor Train decomposition for adversarial filtering, the inverse relationship between rank/α values and robustness, and the superiority of normalization layer targeting.
- **Medium Confidence**: The theoretical mechanism explaining why low-rank tensor decomposition filters adversarial noise (frequency-domain separation) is plausible but not rigorously proven.
- **Low Confidence**: The assumption that Tensor Train's sequential core structure universally outperforms CP/Tucker across different architectures and attack types lacks comparative validation.

## Next Checks

1. **Cross-Attack Robustness Test**: Evaluate the defense against adaptive attacks (e.g., projected gradient descent with knowledge of the decomposition defense) and other attack families (CW, FGSM) to assess vulnerability to attack-aware adversaries.

2. **Layer Sensitivity Analysis**: Systematically test decomposition at different transformer layers (early, middle, late) and block positions (attention vs. MLP vs. normalization) to determine optimal deployment points across architectures.

3. **Multi-Dataset Generalization**: Validate the defense on diverse vision-language tasks (e.g., visual question answering, image captioning) and datasets beyond retrieval (e.g., COCO detection, GQA) to assess real-world applicability.