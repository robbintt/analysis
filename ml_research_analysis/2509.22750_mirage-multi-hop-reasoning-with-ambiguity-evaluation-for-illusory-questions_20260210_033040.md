---
ver: rpa2
title: 'MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions'
arxiv_id: '2509.22750'
source_url: https://arxiv.org/abs/2509.22750
tags:
- ambiguity
- question
- answer
- multi-hop
- ambiguous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multi-hop QA models struggle when ambiguity appears at any reasoning
  step, since early misinterpretations can lock in incorrect retrieval paths and cause
  cascading errors. To study this, researchers introduced MARCH, a benchmark of 2,209
  ambiguous multi-hop questions spanning syntactic, semantic, and constraint ambiguity,
  each paired with clarified queries and evidence-grounded answers.
---

# MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions

## Quick Facts
- **arXiv ID:** 2509.22750
- **Source URL:** https://arxiv.org/abs/2509.22750
- **Reference count:** 40
- **Primary result:** State-of-the-art multi-hop QA models struggle with ambiguity at any reasoning step, often producing incomplete or one-sided answers.

## Executive Summary
Multi-hop QA models frequently fail when ambiguity arises during reasoning, as early misinterpretations can lead to incorrect retrieval paths and cascading errors. To address this, researchers introduced MARCH, a benchmark of 2,209 ambiguous multi-hop questions covering syntactic, semantic, and constraint ambiguity, each paired with clarified queries and evidence-grounded answers. Despite the challenge, existing models—including agentic baselines—performed poorly, often pruning alternative interpretations prematurely. To overcome this, the team developed CLARION, a two-stage framework that first enumerates interpretations via a Planning Agent and then retrieves and reasons for each branch before synthesizing a final answer. CLARION significantly outperformed all baselines on MARCH, especially in Disambig-F1, demonstrating the importance of decoupling ambiguity planning from evidence retrieval for robust multi-hop reasoning.

## Method Summary
The paper introduces MARCH, a benchmark designed to evaluate multi-hop QA models' ability to handle ambiguity at any reasoning step. MARCH includes 2,209 ambiguous questions spanning syntactic, semantic, and constraint ambiguity, each paired with clarified queries and evidence-grounded answers. To address the shortcomings of existing models, the authors propose CLARION, a two-stage framework that first enumerates interpretations via a Planning Agent and then retrieves and reasons for each branch before synthesizing a final answer. This approach explicitly decouples ambiguity planning from evidence retrieval, enabling more robust reasoning in ambiguous settings.

## Key Results
- State-of-the-art multi-hop QA models struggle when ambiguity appears at any reasoning step, often producing incomplete or one-sided answers.
- CLARION significantly outperformed all baselines on MARCH, especially in Disambig-F1, by decoupling ambiguity planning from evidence retrieval.
- Early misinterpretations in multi-hop reasoning can lock in incorrect retrieval paths, causing cascading errors.

## Why This Works (Mechanism)
CLARION works by explicitly decoupling ambiguity planning from evidence retrieval. The Planning Agent first enumerates all possible interpretations of an ambiguous question, ensuring that no potential meaning is overlooked. Each interpretation is then processed independently through retrieval and reasoning, allowing the model to gather evidence for all plausible answers. Finally, the results are synthesized into a comprehensive final answer. This approach prevents premature pruning of alternative interpretations, which is a common failure mode in existing multi-hop QA models.

## Foundational Learning
- **Ambiguity in Multi-hop QA:** Ambiguity at any reasoning step can lead to cascading errors. *Why needed:* Understanding how ambiguity propagates is crucial for designing robust QA systems. *Quick check:* Test models on ambiguous questions to observe error patterns.
- **Synthetic Ambiguity Generation:** Creating controlled ambiguous questions to evaluate model performance. *Why needed:* Real-world ambiguous questions are scarce, so synthetic generation is necessary for systematic evaluation. *Quick check:* Verify that synthetic ambiguity aligns with human judgment.
- **Two-stage Reasoning Framework:** Separating interpretation planning from evidence retrieval. *Why needed:* This decoupling prevents premature pruning of interpretations and improves answer completeness. *Quick check:* Compare performance of single-stage vs. two-stage models on ambiguous questions.

## Architecture Onboarding

**Component Map:**
Planning Agent -> Interpretation Enumerators -> Evidence Retrievers -> Reasoning Module -> Answer Synthesizer

**Critical Path:**
1. Input ambiguous question
2. Planning Agent generates all interpretations
3. Each interpretation is processed by Evidence Retrievers
4. Reasoning Module evaluates evidence for each branch
5. Answer Synthesizer combines results into final answer

**Design Tradeoffs:**
- **Pros:** Explicit handling of ambiguity reduces cascading errors and improves answer completeness.
- **Cons:** Increased computational cost due to processing multiple interpretations in parallel.

**Failure Signatures:**
- Premature pruning of interpretations leading to incomplete answers.
- Inconsistent retrieval paths due to early misinterpretations.
- Over-reliance on single interpretations in ambiguous cases.

**First Experiments:**
1. Evaluate CLARION on MARCH to measure performance gains over baselines.
2. Test CLARION on naturally occurring ambiguous questions to assess robustness.
3. Conduct human evaluation to compare CLARION's answers with human judgment on ambiguity resolution.

## Open Questions the Paper Calls Out
None

## Limitations
- MARCH may not fully capture real-world ambiguity types, particularly domain-specific or cross-modal ambiguities.
- Performance evaluation relies heavily on synthetic ambiguity generation, which may not reflect natural ambiguity distributions.
- The study lacks qualitative analysis of answer completeness or user satisfaction in ambiguous cases.

## Confidence
- **High:** Multi-hop QA models struggle with ambiguity, leading to cascading errors.
- **High:** CLARION's two-stage framework effectively improves performance on the MARCH benchmark.
- **Medium:** Generalizability of findings to real-world applications remains uncertain due to synthetic evaluation.

## Next Checks
1. Evaluate CLARION on naturally occurring ambiguous questions from real-world datasets to assess robustness beyond synthetic benchmarks.
2. Conduct human evaluation studies to determine whether CLARION's disambiguated answers align with human judgment on ambiguity resolution quality.
3. Test CLARION's performance on cross-modal ambiguous questions (e.g., involving images or tables) to evaluate scalability to multimodal reasoning scenarios.