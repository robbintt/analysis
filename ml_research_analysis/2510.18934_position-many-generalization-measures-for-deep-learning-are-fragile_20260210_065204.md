---
ver: rpa2
title: 'Position: Many generalization measures for deep learning are fragile'
arxiv_id: '2510.18934'
source_url: https://arxiv.org/abs/2510.18934
tags:
- learning
- arxiv
- measures
- generalization
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This position paper demonstrates that many post-mortem generalization\
  \ measures in deep learning are fragile: small training modifications\u2014such\
  \ as learning rate changes, optimizer swaps, or hyperparameter tweaks\u2014can cause\
  \ substantial shifts in the measure\u2019s value or trend, even when the underlying\
  \ network performance remains stable. Through systematic experiments on ResNet-50\
  \ and other architectures across FashionMNIST, CIFAR-10, and MNIST datasets, the\
  \ authors show that norms (path, spectral, Frobenius), margin-based metrics, and\
  \ PAC-Bayes surrogates often fail to track changes in data complexity or dataset\
  \ difficulty."
---

# Position: Many generalization measures for deep learning are fragile
## Quick Facts
- **arXiv ID:** 2510.18934
- **Source URL:** https://arxiv.org/abs/2510.18934
- **Reference count:** 40
- **Key outcome:** Small training changes can cause large shifts in generalization measure values without changing network performance, revealing fragility.

## Executive Summary
This position paper demonstrates that many post-hoc generalization measures in deep learning are highly sensitive to minor training modifications, such as learning rate adjustments, optimizer changes, or hyperparameter tweaks. Despite stable network performance, these measures—including norms, margin-based metrics, and PAC-Bayes surrogates—often fail to track data complexity or dataset difficulty. The authors contrast this fragility with the marginal-likelihood PAC-Bayes bound, which robustly captures data complexity and learning-curve scaling due to its function-space basis. The work also reveals how scale invariance can inflate parameter magnitudes without altering the learned function, further exposing the limitations of magnitude-sensitive measures.

## Method Summary
The authors conducted systematic experiments using ResNet-50 and other architectures on FashionMNIST, CIFAR-10, and MNIST datasets. They examined how generalization measures respond to training modifications like learning rate changes, optimizer swaps, and hyperparameter tweaks. Fragility was quantified using novel metrics (CMS, eCMS) that assess measure stability across training regimes. The marginal-likelihood PAC-Bayes bound was highlighted for its robustness, attributed to its function-space basis. A novel equivalence for scale-invariant networks was also introduced, showing how parameter magnitudes can be inflated without changing the learned function.

## Key Results
- Small training modifications (e.g., learning rate changes, optimizer swaps) can cause large shifts in generalization measure values without affecting network performance.
- Norms (path, spectral, Frobenius), margin-based metrics, and PAC-Bayes surrogates often fail to track data complexity or dataset difficulty.
- The marginal-likelihood PAC-Bayes bound robustly captures data complexity and learning-curve scaling due to its function-space basis.

## Why This Works (Mechanism)
The fragility of generalization measures arises from their sensitivity to parameter magnitudes and training dynamics, which can vary independently of the learned function. Scale invariance in networks allows parameter inflation without functional change, exposing the limitations of magnitude-based measures. The marginal-likelihood PAC-Bayes bound's robustness stems from its function-space basis, which aligns better with data complexity and learning dynamics.

## Foundational Learning
- **Scale Invariance**: Networks where parameter scaling does not affect the learned function. *Why needed:* Explains how parameter magnitudes can inflate without functional change. *Quick check:* Verify if a network's output remains unchanged under uniform parameter scaling.
- **PAC-Bayes Bounds**: Generalization bounds that account for data complexity. *Why needed:* Provides a theoretical framework for assessing generalization. *Quick check:* Ensure the bound captures data complexity and learning-curve scaling.
- **Norms and Margins**: Measures like path, spectral, and Frobenius norms, and margin-based metrics. *Why needed:* Commonly used to estimate generalization. *Quick check:* Assess sensitivity to training modifications.
- **Data Complexity**: The inherent difficulty of the dataset. *Why needed:* Critical for evaluating generalization measures. *Quick check:* Compare measure trends across datasets of varying complexity.
- **Learning Curve Scaling**: How measures evolve with training. *Why needed:* Indicates measure stability over time. *Quick check:* Track measure trends across epochs.
- **Fragility Metrics (CMS, eCMS)**: Quantitative scores for measure stability. *Why needed:* Provide a benchmark for fragility assessment. *Quick check:* Validate metrics across diverse training regimes.

## Architecture Onboarding
- **Component Map**: Input -> Network Layers -> Generalization Measure -> Output (e.g., path norm, margin, PAC-Bayes bound).
- **Critical Path**: Measure computation depends on parameter magnitudes and training dynamics.
- **Design Tradeoffs**: Magnitude-sensitive measures are fragile but computationally simple; function-space measures are robust but complex.
- **Failure Signatures**: Large measure shifts without performance change; poor tracking of data complexity.
- **First Experiments**: 1) Vary learning rate and track measure trends. 2) Swap optimizers and assess measure stability. 3) Test scale invariance by scaling parameters and measuring output change.

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis focuses on a narrow sample of network architectures, potentially missing broader trends.
- Data augmentation and regularization effects are not considered, which could influence measure stability.
- The fragility metrics (CMS, eCMS) lack external validation, raising questions about their sensitivity to non-generalization factors.
- The theoretical link between scale invariance and parameter inflation is compelling but not universally applicable.
- The marginal-likelihood PAC-Bayes bound's robustness is highlighted but not conclusively proven across all conditions.
- The focus on classification benchmarks may not generalize to regression or other task types.

## Confidence
- **Core fragility findings:** High
- **Proposed metrics and theoretical extensions:** Medium

## Next Checks
1. Validate fragility metrics (CMS, eCMS) across diverse architectures and tasks.
2. Test the marginal-likelihood PAC-Bayes bound's robustness with data augmentation and regularization.
3. Explore generalization measures for regression and non-classification tasks.