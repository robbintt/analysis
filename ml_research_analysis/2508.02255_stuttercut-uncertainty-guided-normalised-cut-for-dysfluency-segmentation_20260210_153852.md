---
ver: rpa2
title: 'StutterCut: Uncertainty-Guided Normalised Cut for Dysfluency Segmentation'
arxiv_id: '2508.02255'
source_url: https://arxiv.org/abs/2508.02255
tags:
- dysfluency
- speech
- fluencybank
- segmentation
- stuttercut
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StutterCut introduces a semi-supervised graph-based framework for
  dysfluency segmentation without strong labels or transcriptions. It formulates the
  task as graph partitioning using overlapping speech windows as nodes, refining connectivity
  via a pseudo-oracle classifier weighted by Monte Carlo dropout uncertainty.
---

# StutterCut: Uncertainty-Guided Normalised Cut for Dysfluency Segmentation

## Quick Facts
- arXiv ID: 2508.02255
- Source URL: https://arxiv.org/abs/2508.02255
- Reference count: 0
- Introduces a semi-supervised graph-based framework for dysfluency segmentation using overlapping speech windows and Monte Carlo dropout uncertainty.

## Executive Summary
StutterCut addresses the challenge of dysfluency segmentation in stuttered speech by formulating it as a graph partitioning problem. Unlike traditional frame-level classification, it leverages overlapping speech windows as nodes and uses normalized cut to find coherent segments. The framework incorporates a pseudo-oracle classifier trained on weak labels to guide segmentation, with its influence modulated by Monte Carlo dropout uncertainty. The authors also introduce FluencyBank++, a labeled corpus for benchmarking. Experiments show improved F1 scores and onset detection accuracy over existing methods.

## Method Summary
StutterCut constructs a graph where nodes are embeddings of overlapping speech windows. It refines graph connectivity using a pseudo-oracle classifier (trained on weak labels) weighted by Monte Carlo dropout uncertainty. The framework solves a normalized cut problem to partition the graph into fluent and dysfluent segments. By integrating weak supervision with uncertainty-guided refinement, it achieves precise segmentation without requiring strong frame-level labels.

## Key Results
- Achieves higher F1 scores than existing methods on both real and synthetic dysfluency datasets.
- Outperforms baseline models in stuttering onset detection, with errors under 0.5 seconds.
- Ablation studies confirm the importance of classifier guidance and uncertainty weighting for robust performance.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Representing speech as a graph of overlapping windows allows the model to segment dysfluencies based on global acoustic consistency rather than isolated frame-level errors.
- **Mechanism:** The architecture constructs a fully-connected graph where nodes are embeddings of overlapping windows. It solves a Normalised Cut (N-Cut) problem to partition this graph. By minimizing the cut between disjoint subsets (fluent vs. dysfluent) while normalizing for cluster size, the mechanism prevents the "over-fragmentation" common in frame-wise classification, ensuring that segmented regions are temporally coherent.
- **Core assumption:** Acoustic embeddings of fluent speech segments cluster distinctly from dysfluent segments in latent space, such that cosine similarity is a reliable proxy for class membership.
- **Evidence anchors:**
  - [abstract] "...formulates dysfluency segmentation as a graph partitioning problem..."
  - [Page 2] "N-Cut aims to partition the graph to minimise edges between subsets while preserving strong connectivity within each subset."
  - [corpus] Corpus signals indicate related work in "Token-Level Modeling" and "Dysfluent WFST" confirms the difficulty of frame-level modeling, supporting the need for structural approaches, though no direct validation of the graph-cut mechanism was found in neighbors.
- **Break condition:** Fails if the acoustic features of dysfluencies (e.g., blocks) are acoustically identical to silence/pauses, resulting in high similarity edges between distinct classes.

### Mechanism 2
- **Claim:** A classifier trained on cheap, weak (utterance-level) labels can guide precise (frame-level) segmentation if treated as a "pseudo-oracle" that modifies graph topology.
- **Mechanism:** A classifier $C$ (trained on weak labels) generates a secondary similarity matrix $W^{(2)}$ based on the cosine similarity of its prediction probabilities. This matrix represents "semantic" similarity (likelihood of sharing a dysfluency label). This is fused with the acoustic similarity matrix, effectively warping the graph landscape so that acoustically similar nodes are pulled apart if their semantic predictions differ.
- **Core assumption:** The weakly-supervised classifier $C$ has learned a generalizable representation of dysfluency types, despite lacking boundary training data.
- **Evidence anchors:**
  - [Page 2] "We refine the connections between nodes using a pseudo-oracle classifier... mimicking expert guidance by imposing soft segmentation constraints."
  - [Page 4] "Disabling both classifier constraints... (SCut [-M-C])... further degrades performance, reducing mean t-F1 by 10%."
  - [corpus] Related work on "Synthetic Data Generation" highlights the scarcity of strong labels, validating the utility of weak-label strategies.
- **Break condition:** Fails if the pseudo-oracle is systematically biased or overfits the weak labels, providing confident but incorrect semantic links that corrupt the graph structure.

### Mechanism 3
- **Claim:** Weighting the pseudo-oracle’s influence by Monte Carlo (MC) dropout uncertainty prevents noisy classifier predictions from degrading the segmentation.
- **Mechanism:** The fusion of acoustic and semantic graphs is modulated by a confidence mask $M$. $M$ is derived from the entropy of stochastic forward passes (MC dropout). High uncertainty (high entropy) lowers the weight of the classifier’s contribution, defaulting to acoustic similarity. This filters out misleading guidance from the pseudo-oracle on ambiguous frames.
- **Core assumption:** The uncertainty quantified by MC dropout correlates with the actual error likelihood of the pseudo-oracle.
- **Evidence anchors:**
  - [Page 2] "...influence controlled by an uncertainty measure from Monte Carlo dropout."
  - [Page 4] "Retaining classifier guidance but removing the entropy uncertainty mask... lowers the overall mean t-F1... from 69.3% to 63.4%."
  - [corpus] No specific corpus validation for MC Dropout in dysfluency; anchor relies on internal paper evidence.
- **Break condition:** Fails if the model is erroneously over-confident (low entropy) on wrong predictions (calibration error), causing the mask to trust a faulty oracle.

## Foundational Learning

- **Concept: Spectral Graph Theory (Normalized Cut)**
  - **Why needed here:** The core engine of StutterCut isn't a convolution or recurrence, but an eigenvalue problem. You must understand how the "Fiedler vector" (the second smallest eigenvector) mathematically represents the optimal bi-partition of the graph.
  - **Quick check question:** Why does N-Cut use the second smallest eigenvector rather than the smallest (which is always trivial)?

- **Concept: Monte Carlo Dropout (Uncertainty)**
  - **Why needed here:** The paper uses dropout at inference time not for regularization, but to estimate epistemic uncertainty. Understanding the difference between softmax probability (often over-confident) and entropy derived from stochastic passes is crucial for the "Uncertainty-Guided" component.
  - **Quick check question:** How does running inference 100 times with dropout activated provide information that a single inference run cannot?

- **Concept: Semi-Supervised Learning (Weak vs. Strong Labels)**
  - **Why needed here:** The system bridges the gap between "this audio has a stutter" (weak) and "the stutter starts at 1.2s and ends at 1.8s" (strong). Understanding how to project weak constraints onto strong predictions is key to the architecture.
  - **Quick check question:** In StutterCut, which component is trained on weak labels, and which component utilizes them without direct training?

## Architecture Onboarding

- **Component map:**
  1. Input Processing: Audio → Overlapping Windows (0.75s, stride 0.1s) → Embeddings (WavLM/Whisper)
  2. Graph Construction: Nodes = Embeddings; Edges = Cosine Similarity ($W^{(1)}$)
  3. Pseudo-Oracle: External Classifier (WhisterML) → Probability Vectors ($P_0, P_{max}$) → Semantic Similarity ($W^{(2)}$)
  4. Uncertainty Module: MC Dropout (100 passes) → Entropy → Confidence Mask ($M$)
  5. Fusion: $W^{(1)}$ + $W^{(2)}$ (masked by $M$) → Refined Adjacency Matrix $\tilde{W}$
  6. Solver: N-Cut Eigen-decomposition → Fiedler Vector → Thresholding
  7. Post-processing: Merge consecutive windows → Final Boundaries

- **Critical path:** The **Knowledge Integration** step (Equation 3). If the mask $M$ or the semantic similarity $W^{(2)}$ is incorrectly scaled, the fused graph $\tilde{W}$ will have noisy edges, causing the eigensolver to produce a Fiedler vector that splits fluent speech into fragments or merges dysfluencies into fluent regions.

- **Design tradeoffs:**
  - **Window Size (0.75s):** A tradeoff between temporal resolution and acoustic context. Larger windows smooth over short dysfluencies (like brief interjections); smaller windows lack the context to distinguish blocks from pauses.
  - **Threshold $\tau$ (0.25):** Pruning weak edges. Too high, and the graph becomes disconnected (fragmented segments); too low, and noise dominates the cut.

- **Failure signatures:**
  - **Interjection Misses:** The paper notes failures on interjections like "um" that lack pauses.
  - **Block vs. Silence Confusion:** Blocks are often acoustically similar to silence, leading to misclassification if the pseudo-oracle is uncertain.
  - **Over-segmentation:** If the classifier guidance is removed (SCut [-M-C]), the model defaults to pure acoustics, resulting in chopped up, non-semantic segments.

- **First 3 experiments:**
  1. **Oracle Validation:** Train the "WhisterML" classifier on the weak-label dataset (Sep-28k) and verify its frame-level accuracy *before* integrating it into the graph. If the oracle is poor, the graph cut will fail.
  2. **Ablation on Synthetic Data:** Run StutterCut on VCTK-TTS (synthetic) with the Mask $M$ enabled vs. disabled. Synthetic data has cleaner patterns; verify if uncertainty guidance is less critical here (as suggested by results).
  3. **Window Sensitivity Analysis:** Sweep window lengths (e.g., 0.5s vs. 0.75s vs. 1.0s) on FluencyBank++ to determine optimal resolution for different dysfluency types (e.g., short interjections vs. long repetitions).

## Open Questions the Paper Calls Out
None

## Limitations
- Weak Label Generalization: Effectiveness depends on weak labels capturing sufficient signal for frame-level segmentation; standalone oracle performance is not evaluated.
- Dataset Representativeness: FluencyBank++ is small and unbalanced, limiting generalizability to broader stuttering patterns.
- Uncertainty Calibration: MC dropout entropy may not reliably correlate with prediction error, risking trust in miscalibrated classifier outputs.

## Confidence
- **High Confidence:** Graph-based formulation avoids over-fragmentation; synthetic data performance validates core algorithm.
- **Medium Confidence:** Weak-label classifier guidance is beneficial (ablation shows drop without it); uncertainty weighting helps on real data.
- **Low Confidence:** Generalization to diverse dysfluency patterns; pseudo-oracle accuracy; uncertainty calibration quality.

## Next Checks
1. **Oracle Standalone Test:** Train and evaluate the WhisterML classifier on weak labels alone (e.g., frame-level F1). This isolates whether poor segmentation stems from weak supervision or graph fusion errors.
2. **Out-of-Domain Robustness:** Test StutterCut on speakers or speech styles not represented in FluencyBank++ (e.g., children, different accents) to assess generalizability limits.
3. **Uncertainty Quality Audit:** Compare MC dropout entropy against actual classifier error rates on a validation set. If entropy poorly correlates with error, the confidence mask may need recalibration or replacement with methods like temperature scaling.