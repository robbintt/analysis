---
ver: rpa2
title: 'DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router'
arxiv_id: '2507.22050'
source_url: https://arxiv.org/abs/2507.22050
tags:
- deepsieve
- routing
- reasoning
- retrieval
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepSieve addresses the challenge of reasoning over heterogeneous
  knowledge sources by introducing an LLM-as-a-knowledge-router framework that performs
  multi-stage information sieving. The method decomposes complex queries into structured
  sub-questions, routes each to the most appropriate source via LLM-based decision-making,
  iteratively refines answers through reflexion, and fuses results into a coherent
  final response.
---

# DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router

## Quick Facts
- arXiv ID: 2507.22050
- Source URL: https://arxiv.org/abs/2507.22050
- Authors: Minghao Guo; Qingcheng Zeng; Xujiang Zhao; Yanchi Liu; Wenchao Yu; Mengnan Du; Haifeng Chen; Wei Cheng
- Reference count: 25
- Primary result: DeepSieve achieves 58.9 F1 (DeepSeek-V3) and 51.2 F1 (GPT-4o) on multi-hop QA, outperforming RAG baselines while using 10x fewer tokens.

## Executive Summary
DeepSieve addresses the challenge of reasoning over heterogeneous knowledge sources by introducing an LLM-as-a-knowledge-router framework that performs multi-stage information sieving. The method decomposes complex queries into structured sub-questions, routes each to the most appropriate source via LLM-based decision-making, iteratively refines answers through reflexion, and fuses results into a coherent final response. Experiments on three multi-hop QA benchmarks demonstrate that DeepSieve achieves superior performance while using significantly fewer tokens than alternative approaches.

## Method Summary
DeepSieve implements a four-stage pipeline: (1) LLM decomposition of queries into DAG-structured subquestions, (2) LLM routing to select appropriate (Tool, Corpus) pairs based on source profiles and query semantics, (3) retrieval with reflexion loop for failure recovery, and (4) LLM fusion of sub-answers. The system uses DeepSeek-V3 or GPT-4o with temperature=0.0, and evaluates on MuSiQue, 2WikiMultiHopQA, and HotpotQA datasets. The approach handles heterogeneous sources by maintaining separate retrieval contexts rather than unified indexes, preventing context pollution during multi-hop reasoning.

## Key Results
- Achieves 58.9 F1 and 51.2 F1 on multi-hop QA benchmarks using DeepSeek-V3 and GPT-4o respectively
- Uses 3.9K average tokens per query versus 37.9K+ for RAG and agentic RAG baselines
- Reflexion module provides largest performance gain (F1 drops from 68.4 to 15.4 when removed on 2WikiMultiHopQA)
- Maintains strong performance under simulated heterogeneous conditions where standard RAG fails

## Why This Works (Mechanism)

### Mechanism 1: Query Decomposition as a Semantic Sieve
- Claim: Decomposing complex queries into a DAG of atomic subquestions reduces retrieval noise and enables structured multi-hop reasoning.
- Mechanism: The LLM-based planner breaks down a monolithic query Q into structured subqueries {q1, q2, ..., qn} forming a directed acyclic graph. Each node represents an atomic reasoning unit; edges encode dependencies resolved at fusion. This prevents retrieval systems from returning semantically similar but irrelevant passages that often mislead flat RAG.
- Core assumption: The LLM planner can correctly identify atomic reasoning units and their dependencies without introducing errors or spurious subquestions.
- Evidence anchors:
  - [section 2.1]: "This step acts as a semantic sieve, transforming monolithic input into a directed acyclic graph (DAG) of sub-goals."
  - [abstract]: "DeepSieve breaks down complex queries into structured sub-queries and recursively routes each to the most appropriate knowledge source."
  - [corpus]: Related work on decomposition (DISC, ADaPT) supports this mechanism, though corpus lacks direct validation of DAG structure benefits for heterogeneous routing.

### Mechanism 2: LLM-as-Knowledge-Router for Source-Aware Retrieval
- Claim: An LLM router selecting (Tool, Corpus) pairs based on query semantics and source profiles improves retrieval precision over flat, unified indexes.
- Mechanism: For each subquestion qi, the router receives a structured prompt encoding: (i) qi semantics, (ii) profile metadata of each source (domain, format, privacy), and (iii) fail history. It returns a specific source si = (Ti, Ci), enabling targeted retrieval from heterogeneous sources that cannot be merged due to privacy, schema incompatibility, or access modality.
- Core assumption: Source profiles accurately capture corpus content and boundaries, and the LLM can generalize to novel query-source mappings.
- Evidence anchors:
  - [section 2.2]: "This selection is guided by a structured routing prompt that encodes (i) the semantics of qi, (ii) the profile metadata of each source."
  - [section 3.5, RQ4]: "DeepSieve achieves strong performance across both retrieval modes [Naive RAG, GraphRAG], outperforming prior RAG baselines while maintaining flexible source integration."
  - [corpus]: MSRS paper (FMR=0.586) evaluates multi-source RAG but doesn't isolate routing mechanism; limited direct corpus validation.

### Mechanism 3: Reflexion with Memory-Guided Failure Recovery
- Claim: Storing failed retrieval attempts in memory and re-routing to alternative sources enables recovery from initial retrieval errors.
- Mechanism: When a retrieved answer ai is unsatisfactory, DeepSieve logs (qi, si, Result) to Mfail. The reflection loop re-routes qi to an alternative source s'i from S \ {(Ti, Ci)}, avoiding redundant attempts. Successful results stored in Msucc become trusted evidence for fusion.
- Core assumption: The LLM can reliably distinguish satisfactory from unsatisfactory answers, and alternative sources contain valid information for failed queries.
- Evidence anchors:
  - [section 2.3]: "Failed retrievals are stored in Mfail as tuples (qi, si, Result), helping the router avoid redundant sources in future attempts."
  - [table 3, ablation]: Removing reflexion drops F1 from 68.4 to 15.4 on 2WikiMultiHopQA—the largest single-component drop.
  - [corpus]: No corpus papers validate iterative reflection loops specifically; mechanism relies on paper's ablation evidence.

## Foundational Learning

- Concept: **Multi-hop Question Answering**
  - Why needed here: DeepSieve is explicitly designed for compositional queries requiring reasoning across multiple evidence sources (e.g., "What country is the birthplace of Erik Hort a part of?" requires two hops: birthplace → country).
  - Quick check question: Can you identify a 3-hop query and trace the reasoning chain it would require?

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: DeepSieve extends standard RAG by adding query-side sieving (decomposition) and source-side sieving (routing) to address limitations of flat retrieval.
  - Quick check question: What is the core limitation of flat RAG indexes that DeepSieve addresses?

- Concept: **Directed Acyclic Graphs (DAGs) for Task Dependencies**
  - Why needed here: Decomposed subquestions are structured as a DAG, not a linear chain, allowing parallel execution of independent subquestions while preserving dependency order.
  - Quick check question: Given subquestions [q1, q2, q3] where q3 depends on q1's answer but not q2's, how would the DAG edges be structured?

## Architecture Onboarding

- Component map:
  - **Decomposer** (LLM-based planner): Q → {qi} DAG
  - **Router** (LLM-as-knowledge-router): qi + source profiles + fail history → (Tool, Corpus) pair
  - **Retriever**: Executes tool Ti on corpus Ci → candidate answer ai
  - **Reflexion Module**: Evaluates ai; if unsatisfactory, triggers re-route with logged failure
  - **Memory Module**: Stores Mfail (failed attempts) and Msucc (validated answers)
  - **Fusion Module**: Aggregates {ai} along DAG paths → final answer Â

- Critical path: Decompose → Route (for each qi in topological order) → Retrieve → Reflexion loop (if needed) → Fuse. Reflexion is the highest-cost component (~1,872 tokens on HotpotQA; Figure 5) but also most critical per ablation.

- Design tradeoffs:
  - **Token efficiency vs. accuracy**: Reflexion provides largest accuracy gain but dominates token cost. Consider capping max_reflection_attempts (default 3) based on latency budget.
  - **Routing granularity**: Current coarse-grained (Tool, Corpus) pairs vs. fine-grained parameters (retrieval depth, temperature). Paper notes this as a limitation for future work.
  - **DAG vs. linear decomposition**: DAG supports parallel execution but increases fusion complexity; linear is simpler but may miss inter-subquestion dependencies.

- Failure signatures:
  - **Infinite reflexion loop**: Repeated failures without progress—check Mfail for cycling patterns.
  - **Hallucinated subquestions**: Decomposition creates unanswerable qi (no source contains evidence).
  - **Fusion conflicts**: Contradictory answers along different DAG paths—currently resolved by LLM inference; may need explicit conflict detection.

- First 3 experiments:
  1. **Reproduce ablation on single dataset**: Run DeepSieve (Naive RAG) on HotpotQA with and without reflexion to validate Table 3 drop (61.6 → 21.6 F1).
  2. **Test routing accuracy with synthetic profiles**: Create 2-3 corpora with clear domain boundaries (e.g., medical vs. legal), measure routing precision to validate whether profiles capture content boundaries.
  3. **Profile routing latency overhead**: Measure wall-clock time for Route() call alone vs. full pipeline to quantify the ~0.03s overhead noted in MedQA+CaseHOLD experiment and identify bottlenecks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can extending DeepSieve's action space to include fine-grained tool parameters (e.g., retrieval depth, temperature) optimize performance and cost?
- Basis in paper: [explicit] The authors note the routing mechanism currently selects only coarse-grained (tool, source) pairs, limiting adaptive behavior.
- Why unresolved: The current design abstracts away tool-specific configurations that might yield better results for specific sub-queries.
- What evidence would resolve it: Performance comparisons between static and parameterized tool selection across heterogeneous tasks.

### Open Question 2
- Question: Does incorporating user-specific memory or preference profiles into the routing module improve retrieval accuracy over uniform treatment?
- Basis in paper: [explicit] The paper identifies the uniform treatment of subquestions across users as a limitation in real-world settings.
- Why unresolved: The system currently lacks a mechanism to learn or store personalized retrieval paths or access patterns.
- What evidence would resolve it: Benchmarks measuring retrieval success rates in multi-user, long-term interaction scenarios.

### Open Question 3
- Question: How does DeepSieve's routing mechanism adapt to dynamic, interactive environments compared to static multi-hop QA benchmarks?
- Basis in paper: [explicit] The authors state they have "not yet explored the applicability... in more dynamic, interactive, or simulation-based environments."
- Why unresolved: Current evaluation is restricted to static text-based QA datasets, leaving performance in evolving states unknown.
- What evidence would resolve it: Testing DeepSieve in environments like computer-use agents or digital twin simulations where state evolves.

## Limitations

- The LLM-based routing and decomposition mechanisms are empirical black boxes with unclear decision boundaries for source selection and subquestion creation.
- The memory-guided reflexion system lacks theoretical grounding for why 3 attempts is optimal or when reflexion is futile versus recoverable.
- The heterogeneous simulation via LLM-based partitioning may not fully capture real-world source distribution complexities, privacy constraints, or schema incompatibilities.

## Confidence

- **High Confidence**: Token efficiency claims (3.9K vs 37.9K+ for alternatives) are directly measurable and well-supported by the ablation study showing reflexion as the primary cost driver. The modular architecture allowing plug-and-play source integration is clearly demonstrated through experiments across three benchmarks.
- **Medium Confidence**: The core mechanism claims about query decomposition reducing retrieval noise and LLM routing improving precision are empirically validated but mechanistically underspecified. The paper shows these components work but doesn't fully explain why the LLM planner succeeds at identifying atomic reasoning units or how source profiles capture corpus boundaries accurately.
- **Low Confidence**: The reflexive failure recovery mechanism's generalization beyond the tested benchmarks is uncertain. The ablation shows dramatic impact on 2WikiMultiHopQA but limited testing on other datasets makes it unclear whether this is universally critical or dataset-specific.

## Next Checks

1. **Profile Routing Accuracy Validation**: Create controlled synthetic corpora with clear, non-overlapping domains (e.g., medical vs. legal vs. technical documentation) and measure routing precision when source profiles are manipulated - remove specific keywords from profiles, add irrelevant content, or create ambiguous boundary cases to determine routing failure conditions.

2. **Reflexion Loop Generalization Test**: Run DeepSieve on a fourth benchmark not used in the paper (e.g., HotpotQA from different domains or a newly created multi-hop dataset) with systematic ablation of reflexion attempts (0, 1, 3, 5 attempts) to verify whether the 3-attempt default is optimal or dataset-dependent, and measure the point of diminishing returns.

3. **Real Heterogeneous Environment Deployment**: Implement DeepSieve in an environment with actual heterogeneous sources (e.g., SQL database, REST API, local document store, external web search) with real privacy boundaries and access controls, measuring not just accuracy but also latency, cost, and failure modes when sources have different availability patterns or authentication requirements.