---
ver: rpa2
title: Scaling Probabilistic Circuits via Monarch Matrices
arxiv_id: '2506.12383'
source_url: https://arxiv.org/abs/2506.12383
tags:
- matrices
- monarch
- probabilistic
- butterfly
- hidden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scaling probabilistic circuits
  (PCs) for efficient generative modeling. The authors propose replacing dense matrices
  in PC sum blocks with sparse Monarch matrices, which are parameterized by structured
  tensors.
---

# Scaling Probabilistic Circuits via Monarch Matrices

## Quick Facts
- arXiv ID: 2506.12383
- Source URL: https://arxiv.org/abs/2506.12383
- Reference count: 39
- Primary result: Monarch-PCs achieve state-of-the-art generative modeling performance with significantly reduced FLOPs

## Executive Summary
This paper introduces Monarch-PCs, which replace dense matrices in probabilistic circuits (PCs) with sparse Monarch matrices parameterized by structured tensors. This approach dramatically reduces memory and computation costs while maintaining tractability for efficient generative modeling. The method achieves state-of-the-art performance on Text8, LM1B, and ImageNet benchmarks, outperforming both dense and other sparse PC baselines. Notably, Monarch-PCs achieve the same performance with substantially fewer floating-point operations, demonstrating superior scaling behavior for large-scale probabilistic modeling.

## Method Summary
The authors propose replacing dense matrices in PC sum blocks with Monarch matrices, which are parameterized by structured tensors. Monarch matrices use a recursive parameterization where higher-dimensional matrices are constructed from lower-dimensional building blocks. The key insight is connecting circuit multiplication to structured matrices, allowing efficient inference while maintaining expressiveness. The method employs stochastic mini-batch Expectation-Maximization (EM) optimization, with specific training configurations for different benchmarks (Text8: batch 4096, 20 epochs; ImageNet: batch 20,000, 20 epochs).

## Key Results
- Monarch-PCs achieve state-of-the-art performance on Text8, LM1B, and ImageNet benchmarks
- Significant reduction in floating-point operations (FLOPs) compared to dense PC baselines while maintaining or improving performance
- Outperforms other sparse PC approaches on standard generative modeling tasks
- Demonstrates superior scaling behavior, particularly at larger model sizes

## Why This Works (Mechanism)
Monarch matrices provide a structured, sparse parameterization that reduces computational complexity while preserving the expressiveness needed for probabilistic modeling. The recursive construction allows building higher-dimensional matrices from lower-dimensional components, maintaining tractability. The connection between circuit multiplication and structured matrices provides theoretical justification for why this approach works. By replacing dense operations with Monarch operations, the method achieves significant computational savings without sacrificing model quality.

## Foundational Learning
- **Probabilistic Circuits (PCs)**: Tractable probabilistic models that support efficient inference operations; needed because they provide the foundation for tractable generative modeling
- **Monarch Matrices**: Structured sparse matrices parameterized by tensors; needed because they reduce computational complexity while maintaining expressiveness
- **Circuit Multiplication**: Operation connecting PC structure to matrix operations; needed because it provides the theoretical foundation for replacing dense matrices with Monarch matrices
- **Stochastic EM Optimization**: Training procedure using mini-batches; needed because it enables efficient training of large-scale probabilistic models
- **Bits-per-character/dimension metrics**: Evaluation metrics for generative modeling; needed because they provide standardized measures for comparing model performance

## Architecture Onboarding

**Component Map**: Input -> Monarch Layer -> Hidden State -> Monarch Layer -> Output

**Critical Path**: The core computational path involves applying Monarch matrix transformations to hidden states, which scales as O(dnhB) for gradient computation, making deeper Monarch architectures (3+ layers) memory-prohibitive despite lower FLOPs.

**Design Tradeoffs**: Monarch matrices offer superior FLOP efficiency but face memory bottlenecks with deeper architectures. The 2-layer Monarch approach balances computational savings with practical memory constraints, while 3-4 layer variants show better theoretical efficiency but are impractical due to gradient caching requirements.

**Failure Signatures**: Training instability at large hidden sizes suggests initialization issues. Memory overflow when increasing Monarch layers indicates gradient computation overhead. Poor convergence may indicate insufficient model capacity or improper hyperparameter tuning.

**First Experiments**:
1. Implement 2-layer Monarch matrix layer and verify basic functionality on synthetic data
2. Train Monarch-PC on Text8 with batch size 4096, 20 epochs, targeting ~1.57 BPC
3. Compare FLOP counts between Monarch-PC and dense PC baselines while maintaining comparable performance

## Open Questions the Paper Calls Out
- Can explicitly sparsifying hidden state representations overcome the memory bottleneck associated with high-layer Monarch matrices? (The Conclusion suggests this as future work to address gradient caching overhead)
- To what extent does the scaling efficiency of Monarch-PCs translate to improvements in downstream tasks like controllable language generation? (The Conclusion suggests utilizing improved scaling for downstream applications)
- Can a homogeneous Monarch-based architecture be developed for image modeling to close the performance gap with neural generative models? (Section 5.4 notes current image experiments use HCLT structures rather than homogeneous architectures)

## Limitations
- Memory-efficiency advantage diminishes for deeper Monarch architectures (3-4 layers) due to gradient computation overhead
- Image modeling experiments use composition of two Monarch-2 layers without detailed specification of weight-sharing mechanisms
- Custom CUDA kernels or optimized implementations appear necessary for efficient batched operations, affecting reproducibility

## Confidence
- **High confidence** in core contribution: Monarch-PCs achieve superior scaling with lower FLOPs while maintaining/improving performance
- **Medium confidence** in theoretical connections between circuit multiplication and structured matrices (solid intuition but limited formal proofs)
- **Medium confidence** in practical implementation details (particularly efficient tensor operations and image model composition)

## Next Checks
1. Implement and verify efficient batched linear transformation and permutation logic for Monarch matrices, measuring actual FLOPs reduction versus theoretical estimates
2. Reproduce Text8 results (targeting ~1.57 BPC with hidden size $2^{19}$) and systematically vary Monarch layers to characterize memory-accuracy tradeoff curve
3. For ImageNet experiments, clarify and implement exact architecture of "composition of two Monarch-2 layers" to verify reported BPD improvements of 0.13-0.16 bits over baseline PCs