---
ver: rpa2
title: 'Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form
  Preferences'
arxiv_id: '2510.23451'
source_url: https://arxiv.org/abs/2510.23451
tags:
- response
- your
- uni00000011
- criteria
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenges of modality imbalance and preference
  rigidity in reward models by proposing Omni-Reward, a framework for generalist omni-modal
  reward modeling with free-form preferences. The authors construct Omni-RewardBench,
  the first benchmark covering nine tasks across five modalities (text, image, video,
  audio, 3D) with free-form evaluation criteria, and collect Omni-RewardData comprising
  248K general preference pairs and 69K instruction-tuning pairs.
---

# Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences

## Quick Facts
- arXiv ID: 2510.23451
- Source URL: https://arxiv.org/abs/2510.23451
- Reference count: 40
- Key outcome: Achieves 65.36% accuracy on generalist omni-modal reward modeling across 5 modalities with free-form preferences

## Executive Summary
This paper addresses the challenges of modality imbalance and preference rigidity in reward models by proposing Omni-Reward, a framework for generalist omni-modal reward modeling with free-form preferences. The authors construct Omni-RewardBench, the first benchmark covering nine tasks across five modalities (text, image, video, audio, 3D) with free-form evaluation criteria, and collect Omni-RewardData comprising 248K general preference pairs and 69K instruction-tuning pairs. They develop Omni-RewardModel, including discriminative Omni-RewardModel-BT and generative Omni-RewardModel-R1, achieving strong performance on Omni-RewardBench with 73.68% accuracy (w/o Ties) and 65.36% accuracy (w/ Ties), while also matching or exceeding state-of-the-art performance on other multimodal reward benchmarks.

## Method Summary
The authors propose Omni-Reward, a framework for training generalist omni-modal reward models capable of handling free-form preferences across text, image, video, audio, and 3D modalities. They construct Omni-RewardBench, a comprehensive benchmark with nine tasks and free-form evaluation criteria, and collect Omni-RewardData from multiple sources including Skywork-Reward-Preference, RLAIF-V, OmniAlign-V-DPO, HPDv2, EvalMuse, VideoDPO, and VisionRewardDB-Video. Two reward models are developed: Omni-RewardModel-BT (discriminative) using Bradley-Terry loss and frozen modality encoders, and Omni-RewardModel-R1 (generative) using GRPO-based reinforcement learning with chain-of-thought reasoning. Training uses MiniCPM-o-2.6 and Qwen2.5-VL-7B-Instruct as base models with specific hyperparameters for each variant.

## Key Results
- Achieves 65.36% accuracy (w/ Ties) and 73.68% accuracy (w/o Ties) on Omni-RewardBench
- Mixed-modality training outperforms single-modality variants by 7-9 percentage points
- Instruction-tuning with free-form preferences improves performance by 6.69 percentage points
- Demonstrates strong generalization across VL-RewardBench and Multimodal RewardBench

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Mixed-modality preference training enables cross-task generalization more effectively than single-modality training.
- **Mechanism**: When reward models observe preference patterns across text, image, video, and audio modalities simultaneously, they learn transferable evaluation heuristics (e.g., coherence, alignment, quality) rather than modality-specific shortcuts. The paper demonstrates this through ablation: single-modality training yields marginal gains, while mixed training achieves 65.36% accuracy (w/ Ties) vs. ~58% for single-modality variants.
- **Core assumption**: Shared latent evaluation criteria exist across modalities that joint training can surface.
- **Evidence anchors**:
  - [abstract]: "training on fixed binary preference pairs fails to capture the complexity and diversity"
  - [section 5.1, Table 6]: Single-modality T2T training achieves 57.13% vs. full multimodal training at 65.36%
  - [corpus]: Related work (Skywork-Reward-V2) confirms preference data quality critically impacts generalization, though omni-modal extension is unexplored.
- **Break condition**: If modalities have fundamentally incompatible evaluation criteria (e.g., temporal coherence in video vs. spatial composition in static images), joint training may yield negative transfer.

### Mechanism 2
- **Claim**: Instruction-tuning with free-form preference descriptions enables dynamic reward score adjustment.
- **Mechanism**: By training on structured data (c, x, y₁, y₂, p) where c specifies free-form criteria, the reward model learns to condition its scoring on textual instructions. This creates a conditional preference model that can adapt to user-specified criteria at inference time. The paper's ablation shows removing instruction-tuning data drops performance from 65.36% to 58.67%.
- **Core assumption**: Language models can compositionally understand and apply novel textual criteria to multimodal evaluation.
- **Evidence anchors**:
  - [abstract]: "instruction-tuning pairs for training generalist omni-modal RMs"
  - [section 3.1]: "we propose constructing instruction-tuning data specifically for RMs, where each data instance is formatted as (c, x, y₁, y₂, p)"
  - [corpus]: Weak—no direct corpus evidence on free-form preference conditioning; neighboring papers focus on scalar reward outputs.
- **Break condition**: If free-form criteria are ambiguous, contradictory, or out-of-distribution from training criteria, the model may fail to properly condition rewards.

### Mechanism 3
- **Claim**: Generative reward models with explicit chain-of-thought reasoning improve interpretability without sacrificing accuracy.
- **Mechanism**: By training Omni-RewardModel-R1 to generate textual critiques before predictions using GRPO-based RL, the model's reasoning becomes inspectable. This addresses the opacity of scalar BT scores. Interestingly, CoT helps weaker models but provides marginal gains for stronger models that have internalized reasoning.
- **Core assumption**: Generating explicit reasoning does not introduce systematic bias in final predictions.
- **Evidence anchors**:
  - [section 3.3]: "It encourages the RM to engage in explicit reasoning by generating a textual critic in addition to producing a scalar score"
  - [appendix I.1, Figures 7-8]: CoT improves performance in weaker models but yields mixed results in stronger models
  - [corpus]: Multimodal RewardBench 2 confirms generative reward evaluation is underexplored.
- **Break condition**: If CoT reasoning becomes verbose or tangential, it may mislead final predictions or introduce computational overhead.

## Foundational Learning

- **Concept: Bradley-Terry Loss for Preference Modeling**
  - Why needed here: The discriminative Omni-RewardModel-BT uses this to convert pairwise preferences into scalar reward differences.
  - Quick check question: Given two responses with rewards 2.5 and 1.8, what's the probability the first is preferred? (Answer: σ(2.5-1.8) ≈ 0.67)

- **Concept: Multimodal Fusion Architectures**
  - Why needed here: The model must process text, image, video, audio, and 3D inputs through modality-specific encoders before fusion.
  - Quick check question: Why does the paper freeze vision/audio encoder parameters while training the LM decoder? (Answer: Preserve pretrained representations while adapting to preference scoring)

- **Concept: Free-Form vs. Fixed Preferences**
  - Why needed here: Traditional RMs use binary labels; free-form criteria enable personalized evaluation but require instruction-tuning.
  - Quick check question: How does Omni-RewardBench's "w/ Ties" setting increase difficulty? (Answer: Models must recognize when responses are equivalent, not just rank them)

## Architecture Onboarding

- **Component map**:
  Input (x, y₁, y₂, c) → Modality Encoders (Vision, Audio frozen) → LM Decoder (trainable) → Value Head (trainable) → Reward Score r(c, x, y)

- **Critical path**:
  1. Data preprocessing: Unify preference pair formats across 9 tasks and 5 modalities
  2. Training Omni-RewardModel-BT on 317K pairs (248K general + 69K instruction-tuning)
  3. Optional: Train R1 variant on 10K samples with GRPO-based RL
  4. Evaluation on Omni-RewardBench with w/o Ties and w/ Ties settings

- **Design tradeoffs**:
  - BT model (discriminative): Faster inference, opaque scoring, requires full 317K training data
  - R1 model (generative): Interpretable reasoning, trained on 3% data, but may have slightly lower accuracy (60.18% vs 65.36% on w/ Ties)
  - Pairwise vs. pointwise scoring: Paper finds pairwise scoring outperforms pointwise by 18-29% (Table 11)

- **Failure signatures**:
  - Modality imbalance: Poor performance on T2A (34.75%), T23D (53.98%), TI2I (48.60%) tasks indicates underrepresentation in training data
  - Free-form criteria misalignment: 18-26% accuracy drop when criteria conflict with model's inherent preferences (appendix I.2)
  - Tie calibration: Models struggle with "w/ Ties" setting (average 56.68%) vs. "w/o Ties" (67.29%)

- **First 3 experiments**:
  1. **Baseline establishment**: Evaluate pretrained MiniCPM-o-2.6 on Omni-RewardBench to establish baseline (46.67% w/ Ties), confirming training necessity.
  2. **Ablation on data composition**: Train separate models on T2T-only, TI2T-only, and T2I+T2V data to verify mixed-modality advantage (results in Table 6 show 57-58% vs. 65.36% full).
  3. **Instruction-tuning impact**: Compare models trained with vs. without free-form preference descriptions to isolate the contribution of conditional preference learning (58.67% vs. 65.36%).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reward models maintain robust performance when free-form preference criteria fundamentally conflict with the model's learned general preferences, beyond the 18-26% accuracy drop currently observed?
- Basis in paper: [inferred] Appendix I.2 shows GPT-4o-mini suffers 26.32% accuracy drop and Claude-3.5-Sonnet shows 18.50% drop when inherent preferences conflict with free-form criteria ("shifted" group), suggesting current approaches struggle with preference conflicts.
- Why unresolved: The paper only quantifies the problem but does not propose or evaluate mechanisms to improve robustness against such conflicts.
- What evidence would resolve it: Experiments with adversarial or highly non-standard criteria showing improved accuracy retention compared to baseline models.

### Open Question 2
- Question: How should generative reward models scale with training data, given that Omni-RewardModel-R1 achieves competitive performance using only 3% of the preference data?
- Basis in paper: [explicit] Section 3.3 states Omni-RewardModel-R1 was trained "with only 3% of the Omni-RewardData" (10K samples), and Appendix B notes "the reinforcement learning technique in training the Omni-RewardModel-R1 is limited to a preliminary exploration, and further investigation is needed."
- Why unresolved: No systematic study of data scaling behavior for RL-trained generative RMs was conducted.
- What evidence would resolve it: Ablation experiments varying training data size from 3% to 100% with controlled comparisons on benchmark performance.

### Open Question 3
- Question: Can omni-modal reward models trained on single-turn preference pairs generalize effectively to multi-turn conversational scenarios with evolving user preferences?
- Basis in paper: [explicit] Appendix B, limitation (3): "The current preference data is limited to single-turn interactions and does not capture multi-turn conversational preferences, which are increasingly important for modeling real-world dialogue scenarios."
- Why unresolved: The benchmark and training data explicitly exclude multi-turn interactions.
- What evidence would resolve it: Construction of a multi-turn preference benchmark and evaluation showing whether single-turn-trained models transfer, or whether multi-turn-specific training is required.

## Limitations
- Modality imbalance causes poor performance on underrepresented tasks (T2A, T23D, TI2I)
- Instruction-tuning data relies on synthetic verification rather than human judgment
- Claims about cross-modal transfer learning lack mechanistic understanding

## Confidence
- **High Confidence**: Core architecture and training methodology are well-specified with reproducible hyperparameters. Mixed-modality advantage is empirically demonstrated.
- **Medium Confidence**: Instruction-tuning mechanism's effectiveness is supported by ablation but relies on synthetic verification. Generative CoT shows interpretability benefits but mixed performance gains.
- **Low Confidence**: Claims about cross-modal transfer learning are extrapolated from correlation without mechanistic understanding.

## Next Checks
1. **Modality Balance Analysis**: Conduct systematic per-task performance analysis across all nine tasks to identify which modalities benefit most from mixed training and whether negative transfer occurs between incompatible modalities.
2. **Instruction Generalization Test**: Evaluate the model on novel free-form criteria outside the training distribution to assess true compositional generalization versus memorization of training patterns.
3. **Human Preference Alignment**: Compare model predictions against human judgments on a held-out subset of Omni-RewardBench to validate that accuracy metrics translate to meaningful preference alignment beyond numerical scoring.