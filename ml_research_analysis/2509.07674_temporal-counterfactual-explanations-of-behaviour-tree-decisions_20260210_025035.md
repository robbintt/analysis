---
ver: rpa2
title: Temporal Counterfactual Explanations of Behaviour Tree Decisions
arxiv_id: '2509.07674'
source_url: https://arxiv.org/abs/2509.07674
tags:
- explanations
- state
- explanation
- behaviour
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel method for generating counterfactual
  explanations for decisions made by robots controlled by behavior trees (BTs). The
  approach automatically constructs a causal model from the BT structure and domain
  knowledge, then performs a counterfactual search to generate explanations for contrastive
  "why" queries.
---

# Temporal Counterfactual Explanations of Behaviour Tree Decisions

## Quick Facts
- arXiv ID: 2509.07674
- Source URL: https://arxiv.org/abs/2509.07674
- Reference count: 4
- Novel method for generating counterfactual explanations for behavior tree decisions

## Executive Summary
This paper introduces a method for generating counterfactual explanations for robot behaviors controlled by behavior trees (BTs). The approach constructs a causal model from the BT structure and domain knowledge, then performs counterfactual searches to answer contrastive "why" queries about decision differences. The method was evaluated on both randomly generated BTs and a real-world serial recall task, demonstrating its ability to identify the causes of behavioral differences. The explanations generated are valid, minimal, and maximally diverse, providing a foundation for more transparent and trustworthy robotic systems.

## Method Summary
The approach constructs a causal model from the behavior tree structure and domain knowledge, mapping each BT node to a causal mechanism. A counterfactual search is then performed to generate explanations for contrastive "why" queries by identifying minimal state changes that would alter the decision outcome. The method handles the temporal nature of BT execution through the causal model, enabling explanations that capture the sequential decision-making process. Evaluation was conducted on randomly generated BTs and a serial recall task with a 33-node BT, measuring target recovery rates across various conditions.

## Key Results
- Overall target recovery rate of 39.2% for randomly generated BTs (706/1800 runs)
- 98% target recovery rate for serial recall task with 196 comparisons
- Generated explanations are valid, minimal, and maximally diverse
- Method successfully identifies causes of behavior differences in real-world scenarios

## Why This Works (Mechanism)
The method works by automatically constructing a causal model that captures the decision logic of behavior trees. This model maps each node in the BT to a causal mechanism, allowing the system to reason about how changes in state variables would affect the overall behavior. The counterfactual search then identifies minimal sets of state changes that would lead to different decisions, providing contrastive explanations for "why" queries. By handling the temporal nature of BT execution through the causal model, the approach can generate explanations that accurately reflect the sequential decision-making process of the robot.

## Foundational Learning
1. **Behavior Trees (BTs)** - Hierarchical control structures for robot decision-making. Needed because BTs are a common framework for robot control. Quick check: Verify understanding of BT nodes (condition, action, sequence, selector, etc.) and their execution semantics.

2. **Causal Models** - Graphical representations of cause-effect relationships between variables. Essential for reasoning about counterfactuals and generating explanations. Quick check: Confirm ability to construct causal models from system descriptions and interpret causal mechanisms.

3. **Counterfactual Explanations** - Explanations that describe how different conditions would lead to different outcomes. Provide contrastive "why" answers for decision-making systems. Quick check: Practice formulating counterfactual queries and evaluating the minimality and validity of generated explanations.

## Architecture Onboarding
**Component Map**: Domain Knowledge -> Causal Model Construction -> Counterfactual Search -> Explanation Generation
**Critical Path**: The causal model construction from BT structure and domain knowledge is the foundation that enables the counterfactual search and explanation generation.
**Design Tradeoffs**: The method balances explanation quality (validity, minimality, diversity) against computational efficiency of the counterfactual search. More complex causal models may yield better explanations but increase search complexity.
**Failure Signatures**: Poor target recovery rates may indicate incomplete domain knowledge, overly complex BTs that exceed search capabilities, or noisy state observations that obscure causal relationships.
**3 First Experiments**:
1. Generate a simple BT with 3-5 nodes and manually construct its causal model to verify understanding of the mapping process.
2. Perform counterfactual searches on small synthetic state spaces to validate the search algorithm and explanation generation.
3. Compare explanations generated for the same BT with different domain knowledge inputs to assess sensitivity to knowledge completeness.

## Open Questions the Paper Calls Out
None

## Limitations
- Overall confidence in major claims is Medium due to reliance on synthetic random BT generation rather than diverse real-world robotic systems
- 39.2% target recovery rate for random BTs suggests significant room for improvement in handling complex or noisy decision scenarios
- Evaluation metrics focus primarily on target recovery without extensive analysis of explanation quality or human interpretability

## Confidence
- Overall claim (method effectiveness): Medium
- Synthetic BT evaluation results: Medium
- Real-world serial recall task results: Medium-High
- Explanation quality and human interpretability: Low (not extensively evaluated)

## Next Checks
1. Evaluate the approach on a diverse set of real-world robotic behaviors beyond the serial recall task, including scenarios with noisy sensors and incomplete domain knowledge.
2. Conduct a user study to assess the quality, interpretability, and usefulness of the generated counterfactual explanations compared to alternative methods or baseline approaches.
3. Perform scalability testing with increasingly complex BTs (100+ nodes) and high-dimensional state spaces to establish computational requirements and potential bottlenecks for real-time applications.