---
ver: rpa2
title: Contextual morphologically-guided tokenization for Latin encoder models
arxiv_id: '2511.09709'
source_url: https://arxiv.org/abs/2511.09709
tags:
- morphological
- latin
- language
- tokenization
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates morphologically-aware tokenization for Latin
  language models, comparing standard BPE/WordPiece methods with approaches that incorporate
  morphological analysis. The key innovation is using a contextual morphological segmenter
  based on part-of-speech tagging to guide tokenization.
---

# Contextual morphologically-guided tokenization for Latin encoder models

## Quick Facts
- arXiv ID: 2511.09709
- Source URL: https://arxiv.org/abs/2511.09709
- Reference count: 27
- Primary result: Morphologically-guided tokenization improves Latin LM performance, especially for out-of-domain generalization

## Executive Summary
This paper evaluates morphologically-aware tokenization approaches for Latin language models, comparing standard BPE/WordPiece methods with approaches that incorporate morphological analysis. The key innovation is using a contextual morphological segmenter based on part-of-speech tagging to guide tokenization. Results show morphologically-guided tokenization improves performance on downstream tasks, particularly for out-of-domain texts. Morphological feature tagging accuracy improves by 1.5-1.3 percentage points, while NER F1 improves by 7.6-6.6 points. These gains are most pronounced for unseen text, demonstrating better generalization. The study highlights how linguistic resources can compensate for limited pretraining data in morphologically rich languages.

## Method Summary
The authors compare three tokenization approaches: standard WordPiece/ULM, suffix seeding (MorphSeed) that seeds a list of 480 Latin suffixes into the initial vocabulary, and morphological pre-tokenization (MorphPreTok) that pre-segments words using the Lemlat morphological analyzer before tokenizer training. For contextual disambiguation, they use LatinCy POS tagger predictions to filter Lemlat's multiple analyses. All tokenizers use 30k vocabulary size. The models are pretrained on a 195M word Latin corpus using RoBERTa-base architecture (110M parameters, 12 layers, 768 hidden size) with masked language modeling. Downstream evaluation includes morphological feature tagging, NER, WSD, and authorship verification on various Latin datasets.

## Key Results
- Morphological pretokenization improves out-of-domain NER F1 by 7.6-13.2 points
- Morphologically-guided tokenization increases exact match alignment with Lemlat from 61.8% to 65-84%
- Contextual vs. acontextual morphological segmentation shows minimal performance differences
- Gains are most pronounced for out-of-domain texts, demonstrating improved generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Morphological pretokenization improves downstream task performance by enforcing linguistically meaningful subword boundaries during vocabulary construction.
- Mechanism: By pre-segmenting words into morphemes before tokenizer training, morphological boundaries become hard constraints that prevent the tokenizer from merging morphemes or splitting them in linguistically arbitrary ways. This results in subword units that correspond to meaningful grammatical components (roots, inflectional endings), enabling the model to generalize inflection patterns to unseen words.
- Core assumption: Subword units aligned with morphological boundaries carry more transferable grammatical information than statistically-derived subwords.
- Evidence anchors:
  - [abstract] "morphologically-guided tokenization improves overall performance on four downstream tasks... Performance gains are most pronounced for out of domain texts, highlighting our models' improved generalization ability"
  - [section 5.2, Table 4] Out-of-domain NER F1 improves by up to +13.2 points; out-of-domain morph accuracy improves by +4.4 points with MorphPreTok
  - [corpus] Related work on Turkish (Toraman et al. 2022) found no improvement with morphological pre-tokenization, suggesting the mechanism depends on analyzer quality
- Break condition: If the morphological analyzer has high error rates, or if the target downstream task is semantics-focused rather than grammar-focused (e.g., WSD showed mixed results), gains diminish or reverse.

### Mechanism 2
- Claim: Contextual disambiguation of morphological segmentations provides modest improvements over acontextual approaches.
- Mechanism: A POS tagger first predicts the grammatical category of each word; this prediction filters multiple possible morphological analyses to select the segmentation consistent with the predicted POS. For Latin, a word like "adversari" could be segmented as [advers-ar-i] (noun) or [advers-ari] (verb infinitive) depending on context.
- Core assumption: POS-tagged segmentations are more accurate than taking the first analysis from a type-level morphological analyzer.
- Evidence anchors:
  - [section 4.1] "we construct a contextual morphological segmenter by first running an off-the-shelf part-of-speech tagger on the corpus, and filtering Lemlat's output to an analysis with a matching POS tag"
  - [section 6] "we found minimal differences between contextual and acontextual variants. Given the added complexity of contextual pretokenization, its application to other languages may not be justified"
  - [corpus] Insufficient external evidence on contextual vs. acontextual morphological segmentation comparison
- Break condition: When POS tagger accuracy is low, or when morphological ambiguity is rare in the target language, contextual disambiguation adds complexity without meaningful gains.

### Mechanism 3
- Claim: Linguistic resources (morphological analyzers, lexical databases) can compensate for limited pretraining data in morphologically rich languages.
- Mechanism: Standard tokenizers optimize for compression and fertility, which works well with large-scale data where statistical patterns emerge. For medium/low-resource languages, incorporating expert-curated morphological knowledge injects linguistic structure that would otherwise require more data to learn implicitly.
- Core assumption: High-quality linguistic resources exist or can be developed for the target language; the language has regular morphological patterns amenable to rule-based analysis.
- Evidence anchors:
  - [abstract] "For low-resource languages that lack large-scale pretraining data, the development and incorporation of linguistic resources can serve as a feasible alternative to improve LM performance"
  - [section 1] Latin is "medium-resource in terms of pretraining data, but high-resource in terms of curated lexical resources"
  - [corpus] Related work (Chang et al. 2024, Ogueji et al. 2021) shows small models can work with limited data, but does not directly address linguistic resource substitution
- Break condition: If linguistic resources are low-quality (e.g., Wiktionary extractions with errors noted by Gorman et al. 2019), or if the language lacks expert-curated resources, this mechanism fails.

## Foundational Learning

- Concept: **Subword tokenization algorithms (BPE, WordPiece, Unigram LM)**
  - Why needed here: The paper modifies WordPiece and ULM tokenizers; understanding their vocabulary construction and decoding is prerequisite to understanding how morphological guidance is injected.
  - Quick check question: Can you explain why WordPiece uses PMI for merge selection while BPE uses frequency?

- Concept: **Morphological typology (inflectional morphology, morpheme boundaries)**
  - Why needed here: The core hypothesis is that morphological alignment matters; you need to understand what morphemes are and why they might be better subword units for morphologically rich languages.
  - Quick check question: In Latin, why might separating "puell-ae" (girl, dative/genitive singular) into [puell, ae] be more useful than splitting as [pu, ella, e]?

- Concept: **Encoder-only transformer architectures (RoBERTa, BERT-style pretraining)**
  - Why needed here: The paper pretrains RoBERTa models; understanding masked language modeling and how tokenization affects embedding quality is essential.
  - Quick check question: How does the MLM objective interact with subword tokenization quality?

## Architecture Onboarding

- Component map:
  - Pretraining corpus → Morphological analyzer (Lemlat) → Optional POS tagger (LatinCy) → Tokenizer training (WordPiece/ULM with morphological constraints) → RoBERTa pretraining → Downstream fine-tuning

- Critical path:
  1. Obtain/develop high-quality morphological analyzer for target language
  2. Run analyzer on pretraining corpus to generate morpheme segmentations
  3. Modify tokenizer training to respect morpheme boundaries (MorphPreTok) or seed suffixes (MorphSeed)
  4. Pretrain encoder model with modified tokenizer
  5. Evaluate on morphology-sensitive downstream tasks

- Design tradeoffs:
  - **MorphSeed vs. MorphPreTok**: MorphSeed is lightweight (suffix list only) but provides ~1.4% exact match improvement; MorphPreTok requires full analyzer but achieves 65-84% exact match alignment
  - **Contextual vs. acontextual pretokenization**: Contextual adds POS tagging pipeline complexity for minimal performance difference
  - **ULM vs. WordPiece**: Baseline WordPiece had better Lemlat alignment (+9.4% EM); ULM showed larger NER gains (+7.6 vs. +6.6 F1)

- Failure signatures:
  - High fertility with single-character subwords (occurred when ULM trained on full corpus—use 5% subset)
  - No improvement on semantic tasks (WSD showed -4.1 F1 for ULM MorphPreTok)
  - Analyzer errors propagate to tokenization and downstream performance

- First 3 experiments:
  1. **Baseline replication**: Train standard WordPiece and ULM tokenizers on your Latin corpus (30K vocab), pretrain RoBERTa, evaluate on morphological feature tagging and NER to establish baseline metrics.
  2. **MorphSeeding ablation**: Create suffix list from your morphological analyzer, seed into tokenizers, measure exact match alignment against gold segmentations and downstream performance change.
  3. **MorphPreTok with acontextual analyzer**: Pre-segment corpus using morphological analyzer (taking first analysis), retrain tokenizers and models, compare out-of-domain generalization gains vs. in-domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do errors in the initial POS tagger propagate through the morphologically-guided tokenization pipeline and affect downstream task performance?
- Basis in paper: [explicit] Footnote 8 states: "Investigating how initial tagging errors propagate would be interesting future work," noting the potential circularity when predicted POS tags guide tokenization.
- Why unresolved: The authors acknowledge the circularity concern but did not conduct controlled experiments measuring error propagation from the LatinCy tagger through to downstream predictions.
- What evidence would resolve it: Experiments comparing downstream performance using gold-standard POS tags versus predicted tags for morphological disambiguation, with analysis of specific error cascades.

### Open Question 2
- Question: Why do ULM tokenizers exhibit pathological behavior (high fertility, single-character subwords) when trained on the full corpus but perform well on smaller samples?
- Basis in paper: [explicit] Section 8 states: "When ULM tokenizers were trained on the full corpus, we observed pathological behavior... for reasons unclear to us; future work could examine if it is an issue with the HuggingFace implementation."
- Why unresolved: The authors observed the issue and worked around it by training on 5% of the corpus, but did not investigate whether this stems from the ULM algorithm, HuggingFace bugs, or hyperparameter sensitivity.
- What evidence would resolve it: Systematic experiments varying corpus size, debugging HuggingFace's ULM implementation, and comparing against alternative implementations.

### Open Question 3
- Question: Do morphologically-guided tokenization gains transfer to decoder-based architectures and larger model scales?
- Basis in paper: [explicit] Section 8 states: "We only experiment with encoder-based models. The performance gains we observed may not scale to larger models or other architecture types."
- Why unresolved: The study evaluated only RoBERTa (110M parameters), leaving open whether findings generalize to modern decoder LLMs or larger encoder models.
- What evidence would resolve it: Pretraining decoder models (e.g., GPT-style) and larger encoder models with morphologically-guided tokenization, evaluating on comparable tasks.

## Limitations
- Limited to Latin with high-quality morphological resources, may not generalize to languages with lower-resource linguistic tools
- Uses only RoBERTa-base architecture, leaving open whether gains transfer to other encoder designs or larger models
- Relies on rule-based morphological analyzer that may miss irregular forms or modern usages

## Confidence

**High confidence**: The morphological pretokenization mechanism (MorphPreTok) improving out-of-domain generalization, as evidenced by consistent 7-13 point F1 improvements across NER and morphological tagging tasks. The claim that linguistically-informed tokenization can compensate for limited pretraining data in morphologically rich languages is well-supported by the data.

**Medium confidence**: The suffix seeding approach (MorphSeed) providing meaningful improvements, as the exact match gains (1.4 percentage points) are modest and the mechanism's effectiveness likely depends heavily on the quality of the suffix inventory and morphological regularity of the target language.

**Low confidence**: The contextual morphological segmentation providing benefits over acontextual approaches, given the paper's finding of "minimal differences" and the added implementation complexity without clear performance justification.

## Next Checks

1. **Cross-linguistic validation**: Replicate the morphological pretokenization approach on a morphologically rich language with lower-resource linguistic tools (e.g., Turkish, Finnish) to test whether gains persist when morphological analyzers have higher error rates or incomplete coverage.

2. **Resource quality sensitivity analysis**: Systematically vary the quality of morphological resources (e.g., using Wiktionary-derived segmentations vs. expert-curated analyzers) to quantify how resource quality impacts downstream performance, particularly for the MorphSeed approach.

3. **Computational overhead quantification**: Measure pretraining time, memory usage, and inference latency for morphologically-guided tokenizers compared to baseline approaches across different vocabulary sizes to assess practical deployment costs.