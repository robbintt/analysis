---
ver: rpa2
title: 'SETUP: Sentence-level English-To-Uniform Meaning Representation Parser'
arxiv_id: '2512.07068'
source_url: https://arxiv.org/abs/2512.07068
tags:
- parsing
- data
- graphs
- english
- smatch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of parsing English text into
  Uniform Meaning Representation (UMR) graphs, a semantic representation designed
  to capture core meaning across languages. The authors propose two main approaches:
  fine-tuning existing Abstract Meaning Representation (AMR) parsers on UMR data,
  and converting Universal Dependencies (UD) trees into partial UMR graphs, then training
  a model to complete them.'
---

# SETUP: Sentence-level English-To-Uniform Meaning Representation Parser

## Quick Facts
- arXiv ID: 2512.07068
- Source URL: https://arxiv.org/abs/2512.07068
- Reference count: 0
- Primary result: SETUP achieves AnCast score of 84 and SMATCH++ score of 91 on UMR v2.0

## Executive Summary
This paper addresses the challenge of parsing English text into Uniform Meaning Representation (UMR) graphs, a semantic representation designed to capture core meaning across languages. The authors propose two main approaches: fine-tuning existing Abstract Meaning Representation (AMR) parsers on UMR data, and converting Universal Dependencies (UD) trees into partial UMR graphs, then training a model to complete them. The best-performing model, called SETUP, fine-tunes AMR parsing architectures on UMR data. SETUP achieves an AnCast score of 84 and a SMATCH++ score of 91, representing substantial improvements over the previous baseline. These results establish strong foundational work for automatic UMR parsing, enabling the exploration of UMR's potential in multilingual and low-resource language applications.

## Method Summary
The authors propose SETUP, a sentence-level English-to-UMR parser that fine-tunes pre-trained AMR parsers on UMR data. They experiment with five AMR parsing architectures (amrlib/T5, SPRING, BiBL, LeakDistill, AMRBART) fine-tuned on the UMR v2.0 English dataset for 10 epochs with learning rate 4e-5. An alternative UD-based pipeline converts UD trees to partial UMR graphs using a converter, then trains a T5 model to complete them. The UD approach is evaluated separately due to structural validity issues. The UMR v2.0 English split contains 22,938 training, 6,680 development, and 2,941 test sentences, excluding 66 graphs overlapping with AMR training data.

## Key Results
- SETUP achieves AnCast score of 84.35 and SMATCH++ score of 90.98 on UMR v2.0
- The UD-based pipeline achieves AnCast score of 20.47 and SMATCH++ score of 34.29
- SETUP substantially outperforms the baseline Chun & Xue (2024) pipeline approach
- The UD pipeline shows domain sensitivity, with poor performance on Minecraft dialogue data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning pre-trained AMR parsers on UMR data transfers learned semantic representations to the new graph schema.
- **Mechanism:** Models pre-trained on large AMR corpora (v2.0/v3.0) adapt to UMR's extended annotation schema through direct fine-tuning. This works because AMR and UMR share foundational graph structure and predicate-argument semantics, allowing learned semantic relationships to transfer while adapting to UMR-specific features (tense/aspect/modality, document-level relations).
- **Core assumption:** UMR's semantic graph structure is sufficiently similar to AMR that core meaning representation capabilities transfer effectively despite UMR's additional features.
- **Evidence anchors:**
  - [abstract] "one of which fine-tunes existing parsers for Abstract Meaning Representation"
  - [Section 3.2] "enabling the models to adapt to the UMR structures while retaining the semantic knowledge acquired during AMR training."
  - [corpus] Limited corpus support; neighbors focus on UMR applications and cross-lingual AMR but not specifically on AMR-to-UMR transfer mechanisms.
- **Break condition:** May break for languages or domains with semantic structures fundamentally different from AMR training data, or if UMR-specific features (document-level coreference) are too structurally different for simple fine-tuning.

### Mechanism 2
- **Claim:** UD trees provide a structural foundation that can be systematically converted to partial UMR graphs, which a seq2seq model then completes.
- **Mechanism:** Universal Dependencies provides consistent cross-lingual syntactic structure. A converter transforms UD dependency trees into partial UMR graphs capturing core semantic information. A T5 model is then trained to expand these partial graphs into complete UMR representations.
- **Core assumption:** UD syntactic structure contains sufficient information to derive core UMR semantic structure through deterministic conversion, and remaining semantic details can be inferred by a seq2seq model.
- **Evidence anchors:**
  - [abstract] "the other, which leverages a converter from Universal Dependencies"
  - [Section 3] "These partial UMR graphs capture core semantic information from the text, which we then use to train a model that completes them into fully detailed UMR representations."
  - [corpus] Limited corpus evidence; related dependency parsing work exists but does not directly validate the UD-to-UMR conversion pathway.
- **Break condition:** May fail for languages where syntactic structure doesn't map cleanly to semantic structure, or for constructions requiring information not present in dependency trees.

### Mechanism 3
- **Claim:** Direct text-to-UMR fine-tuning outperforms multi-stage pipeline approaches that rely on intermediate representations.
- **Mechanism:** Directly fine-tuning AMR parsers on UMR data yields significantly higher scores (AnCast 84.3, SMATCH++ 90.975 for BiBL) compared to the baseline pipeline approach (AnCast ~20.5, SMATCH++ ~34.3). The direct approach avoids error propagation through multiple pipeline stages and allows end-to-end learning optimized for UMR's requirements.
- **Core assumption:** End-to-end learning captures complex semantic relationships more effectively than decomposing into stages with intermediate representations.
- **Evidence anchors:**
  - [abstract] "achieves an AnCast score of 84 and a SMATCH++ score of 91, indicating substantial gains"
  - [Section 4.1] "These UMR v2.0 scores are considerably lower than the 72.2 SMATCH score of the pipeline approach on UMR v1.0"
  - [corpus] No direct corpus evidence comparing pipeline vs. direct approaches for UMR.
- **Break condition:** May not generalize to low-resource languages where insufficient UMR training data exists for effective fine-tuning, potentially making pipeline approaches necessary.

## Foundational Learning

- **Concept: Abstract Meaning Representation (AMR)**
  - **Why needed here:** UMR builds directly on AMR's graph structure. Understanding nodes as concepts and edges as semantic roles is essential for comprehending UMR's extensions.
  - **Quick check question:** Can you sketch an AMR graph for "The boy wants to believe" using PENMAN notation?

- **Concept: Semantic Graph Parsing**
  - **Why needed here:** The core task maps linear text to graph structures with nodes, edges, and attributes—foundational to all technical contributions.
  - **Quick check question:** For "John gave Mary a book," what nodes and labeled edges would a semantic graph contain?

- **Concept: Fine-tuning Pre-trained Language Models**
  - **Why needed here:** The primary approach fine-tunes pre-trained AMR parsers (SPRING, AMRBART, BiBL, LeakDistill) on UMR data.
  - **Quick check question:** If you fine-tune a BART model pre-trained on 36K AMR examples using 23K UMR examples, what might happen to its original AMR parsing performance?

## Architecture Onboarding

- **Component map:**
  1. **Input:** Raw English sentences
  2. **Approach A (SETUP):** Pre-trained AMR parser → Fine-tune on UMR pairs → Direct UMR output
  3. **Approach B (UD Pipeline):** Sentence → Stanza UD parser → UD-to-partial-UMR converter → T5 completion → Full UMR
  4. **Baseline Pipeline:** Sentence → AMR parser → LEAMR alignment + UD → Rule-based converter → UMR
  5. **Evaluation:** AnCast, SMATCH, SMATCH++ against gold references

- **Critical path:**
  1. Obtain UMR v2.0 English split (22,938 train, 6,680 dev, 2,941 test)
  2. Select pre-trained AMR parser (BiBL or AMRBART recommended)
  3. Fine-tune for 10 epochs, learning rate 4e-5
  4. Generate test predictions
  5. Evaluate with AnCast, SMATCH, SMATCH++

- **Design tradeoffs:**
  - **Direct fine-tuning vs. UD Pipeline:** Direct yields higher scores but requires substantial in-domain UMR data. UD pipeline is more portable to low-resource languages but produces lower-quality outputs with structural validity issues.
  - **Data split:** Excluding repetitive Minecraft sentences vs. using all data affects generalization.
  - **Model choice:** BiBL achieves highest scores (AnCast 84.3) but AMRBART is close second with potentially different resource requirements.

- **Failure signatures:**
  - **Domain shift:** AnCast drops from ~64 (non-Minecraft) to ~13 (Minecraft) for pipeline approach
  - **Structural errors:** UD pipeline produces invalid graphs with mismatched parentheses
  - **Missing nuance:** UD approach omits fine-grained details like `:mode expressive`

- **First 3 experiments:**
  1. Replicate Chun & Xue (2024) pipeline on UMR v2.0 to establish baseline (expect AnCast ~20)
  2. Fine-tune AMRBART on UMR v2.0 training split (10 epochs, lr=4e-5), evaluate on test
  3. Evaluate fine-tuned model separately on Minecraft vs. non-Minecraft subsets to assess domain generalization

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the current fine-tuning approaches be effectively extended to automatically parse document-level UMR components, such as cross-sentence coreference and temporal dependencies?
- **Basis in paper:** [explicit] The authors state in the Limitations section: "Future work might also work on automatically parsing the document-level components of the UMR graphs."
- **Why unresolved:** The study explicitly constrained its scope to sentence-level parsing due to the complexity of the graph structures and the "enormous amount of linguistic information" involved.
- **What evidence would resolve it:** A model architecture that, when fine-tuned on document-level UMR data, achieves significant AnCast++ scores for modal and temporal dependencies and coreference relations.

### Open Question 2
- **Question:** Would utilizing more recent AMR parsing architectures, such as CLAP, StructBART, or MBSE, yield higher performance than the BiBL and AMRBART models currently used in SETUP?
- **Basis in paper:** [explicit] The authors note in the Limitations section: "While we experiment with five AMR parsers, future work might explore other AMR parsers such as CLAP (Martinez Lorenzo and Navigli, 2024), StructBART (Zhou et al., 2021), or MBSE (Lee et al., 2022)."
- **Why unresolved:** The experiments were restricted to five specific parser architectures, leaving the potential improvements offered by newer or different architectural designs untested.
- **What evidence would resolve it:** Benchmarking SETUP's performance using the suggested architectures on the UMR v2.0 dataset and comparing the resulting AnCast and SMATCH++ scores against the current BiBL baseline.

### Open Question 3
- **Question:** How effectively does the SETUP methodology, which relies on pre-existing AMR parsers, transfer to low-resource languages where such pre-trained models are unavailable?
- **Basis in paper:** [inferred] The paper frames English parsing as a "foundation for transferring these models... to low-resource languages," yet notes that for languages like Navajo, pre-existing text-to-AMR parsers "are largely unavailable."
- **Why unresolved:** The proposed method relies heavily on fine-tuning models pre-trained on English AMR data; it is untested whether this approach is viable without the foundational AMR resources available for English.
- **What evidence would resolve it:** Applying the SETUP framework (or necessary adaptations) to low-resource languages in the UMR dataset (e.g., Arápaho, Kukama) and evaluating the degradation in performance compared to English.

### Open Question 4
- **Question:** Can the UD-based parsing approach be modified to ensure structural validity in the generated PENMAN notation without relying on post-processing scripts to correct parenthesis mismatches?
- **Basis in paper:** [inferred] The authors identify a specific weakness in the UD-to-UMR approach where "T5's tendency to misalign, omit, or insert extra parentheses" results in invalid graphs, requiring a post-hoc correction script.
- **Why unresolved:** The underlying sequence-to-sequence generation mechanism struggles with the structural constraints of the graph format, and it is unclear if the model can learn these constraints natively.
- **What evidence would resolve it:** A trained UD-based model that generates structurally valid UMR graphs with a significantly reduced error rate (e.g., >95% validity) on the test set without external rule-based correction.

## Limitations

- The evaluation focuses exclusively on English text-to-UMR parsing, leaving open questions about performance on other languages and text-to-text UMR parsing scenarios.
- The reliance on pre-trained AMR parsers assumes sufficient semantic overlap between AMR and UMR, which may not hold for all linguistic phenomena.
- The UD-based pipeline approach shows notably poor performance on Minecraft data, indicating potential domain sensitivity that could limit practical deployment across diverse text types.

## Confidence

- **High confidence:** The quantitative results (AnCast 84, SMATCH++ 91) and their superiority over the baseline pipeline are well-supported by the experimental evidence and reproducible methodology.
- **Medium confidence:** The claimed mechanisms of AMR fine-tuning and UD conversion are theoretically sound but lack direct empirical validation comparing different fine-tuning strategies or UD conversion approaches.
- **Medium confidence:** The domain-specific performance differences (non-Minecraft vs. Minecraft) are observed but could benefit from more systematic analysis of what linguistic features cause these discrepancies.

## Next Checks

1. **Cross-linguistic validation:** Test SETUP's performance on UMR data for other languages (Spanish, Chinese, etc.) to assess its multilingual capabilities and identify language-specific limitations.

2. **Ablation study on domain effects:** Systematically evaluate SETUP on different text domains (news, dialogue, technical writing) to quantify domain generalization and identify which UMR features cause performance drops.

3. **End-to-end vs. pipeline comparison:** Conduct a controlled experiment comparing the direct fine-tuning approach against the UD pipeline on identical subsets to measure error propagation and determine when pipeline approaches might be preferable despite lower scores.