---
ver: rpa2
title: Can LLMs extract human-like fine-grained evidence for evidence-based fact-checking?
arxiv_id: '2511.21401'
source_url: https://arxiv.org/abs/2511.21401
tags:
- evidence
- llms
- fine-grained
- text
- claim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new dataset of Czech and Slovak texts with
  fine-grained evidence annotations, created by two independent annotators for each
  sample. The dataset enables the computation of inter-annotator agreement and provides
  a foundation for evaluating the alignment of LLM-generated evidence with human judgments.
---

# Can LLMs extract human-like fine-grained evidence for evidence-based fact-checking?

## Quick Facts
- arXiv ID: 2511.21401
- Source URL: https://arxiv.org/abs/2511.21401
- Reference count: 20
- Key outcome: LLMs struggle with verbatim span extraction despite large size; best models achieve F1 ~55-56, with alignment plateauing around 14-32B parameters

## Executive Summary
This paper introduces a novel Czech/Slovak dataset for fine-grained evidence extraction, where annotators independently select verbatim text spans supporting claims. The study evaluates multiple LLMs on this task, finding that while larger models generally produce more valid outputs, performance improvements plateau beyond 14-32B parameters. All models struggle with verbatim span generation, highlighting the need for constrained decoding mechanisms. The Hungarian matching algorithm enables fair comparison across non-exhaustive annotations, revealing human-human agreement at F1 ~48 and best LLMs achieving comparable or slightly better alignment.

## Method Summary
The authors create a dataset of 186 claim-text pairs with two independent human annotations per sample, collected via two different interfaces. LLMs are prompted to extract verbatim supporting spans in JSON format, though this constraint is not technically enforced. Evaluation uses token-level F1 computed via Hungarian matching algorithm after stopword removal. Models are evaluated on their ability to generate valid evidence spans and their alignment with human annotations, with results reported against both annotator sets and the maximum F1 achieved.

## Key Results
- Evidence extraction alignment improves up to 14-32B parameters, then plateaus
- All models struggle with verbatim span generation, with invalid output rates ranging from 4-62%
- Best-performing models (qwen3:14b, deepseek-r1:32b, gpt-oss:20b) achieve F1 scores of 55-56 with human annotations
- Human-human agreement is approximately 48 F1, establishing a baseline for LLM evaluation
- Label Studio annotations show systematically higher LLM alignment than custom tool annotations

## Why This Works (Mechanism)

### Mechanism 1: Verbatim Span Extraction via Instruction Following
- Claim: LLMs can identify supporting evidence spans but frequently fail to reproduce them verbatim from source text.
- Mechanism: Models receive claim-text pairs with explicit instructions to output exact character-for-character substrings in JSON format. Without technical enforcement, models occasionally generate paraphrased or hallucinated spans.
- Core assumption: Verbatim reproduction is achievable through prompting alone without constrained decoding.
- Evidence anchors:
  - [abstract] "LLMs often fail to copy evidence verbatim from the source text, leading to invalid outputs."
  - [section] "Although the LLMs were explicitly instructed to generate only spans appearing verbatim in the text, this constraint was not enforced technically."
  - [corpus] Related work on constrained decoding (Geng et al., 2023) is cited as a potential solution.
- Break condition: Error rates exceed 50% for models <8B parameters; even 70B+ models show 24-30% invalid outputs.

### Mechanism 2: Diminishing Returns on Model Scale for Evidence Alignment
- Claim: Evidence extraction alignment improves with model size up to a threshold (~14-32B), then plateaus.
- Mechanism: Larger models follow instructions more reliably (lower error rates), but token-level F1 alignment with human annotations saturates. The 685B DeepSeek-R1 and 120B gpt-oss-120b do not outperform 14-32B models.
- Core assumption: Evidence alignment is a reasoning task with bounded difficulty rather than a generation capacity problem.
- Evidence anchors:
  - [section] "Performance improves from small to medium-sized models, but beyond a certain threshold additional parameters do not yield better extraction accuracy."
  - [section] Figure 3 shows F1 scores plateau around 0.55 for models 14B-72B.
  - [corpus] Corpus signals weak on scaling laws specifically for evidence extraction; related work cites general LLM scaling literature (Liu et al., 2025).
- Break condition: Adding 10x parameters (32B→685B) yields no F1 improvement; larger models may even underperform on instruction following (gpt-oss-120b shows higher error rates than expected).

### Mechanism 3: Hungarian Matching for Non-Exhaustive Span Alignment
- Claim: Optimal span-to-span assignment via Hungarian algorithm enables fair comparison when annotators select different numbers of spans.
- Mechanism: Token-level F1 is computed for all span pairs across two annotation sets. Hungarian algorithm solves the assignment problem to maximize total F1, ensuring each span matches at most one counterpart. Average F1 of matched pairs yields the final score.
- Core assumption: Partial overlap (F1 < 100) is valid; exact boundary agreement is not required.
- Evidence anchors:
  - [section] "Since neither human annotators nor LLMs are instructed to produce exhaustive span selections... we use the Hungarian matching algorithm to find optimal assignment."
  - [section] Human-human agreement: F1 = 48; best LLMs achieve F1 = 55-56 with at least one annotator.
  - [corpus] No corpus papers reference Hungarian matching for evidence evaluation specifically.
- Break condition: High invalid output rates reduce comparable sample sizes, potentially biasing F1 upward by excluding harder cases.

## Foundational Learning

- Concept: **Token-level F1 for span overlap**
  - Why needed here: The primary evaluation metric; measures how well generated spans overlap with ground truth at token granularity after stopword removal.
  - Quick check question: Given predicted span "the vaccine is effective" and gold span "vaccine is highly effective," what is the token-level F1? (Answer: Tokens overlap on "vaccine," "is," "effective" → precision=3/4, recall=3/4, F1=0.75)

- Concept: **Constrained decoding**
  - Why needed here: Identified as necessary future work; current LLMs produce 4-62% invalid outputs because they cannot reliably copy verbatim text.
  - Quick check question: Why does standard autoregressive decoding fail for verbatim extraction even with explicit prompts? (Answer: Model maximizes token probability, not substring fidelity; paraphrases may have higher probability than exact copies)

- Concept: **Inter-annotator agreement**
  - Why needed here: Human-human F1 of 48 establishes an upper bound on expected alignment; LLMs exceeding this may be overfitting to one annotator's style.
  - Quick check question: If two annotators achieve F1=48 and an LLM achieves F1=56 with annotator 1 but F1=35 with annotator 2, what does this suggest? (Answer: The LLM may have systematic bias toward one annotation style/interface)

## Architecture Onboarding

- Component map:
  Input layer (claim-text pairs) -> Evidence extractor (LLM with prompt) -> Validation layer (substring check) -> Evaluation layer (Hungarian matching + F1)

- Critical path:
  1. Load 186 claim-text pairs from dataset
  2. Run LLM inference with evidence extraction prompt (Appendix B)
  3. Filter invalid outputs (spans not found in source text)
  4. Compute F1 against both annotators; report max F1 per sample

- Design tradeoffs:
  - Prompt-only vs. constrained decoding: Current design relies on prompting (faster iteration, 4-62% error rate); constrained decoding would reduce errors but requires grammar-based generation infrastructure.
  - Single vs. dual annotator evaluation: Reporting max F1 inflates scores; reporting both reveals annotator-specific biases (Label Studio annotations show systematically higher LLM alignment).
  - Stopword removal: Reduces noise but may over-penalize models that correctly include function words.

- Failure signatures:
  - High invalid % + low F1: Model too small or poorly instruction-tuned (e.g., mixtral:8x7b at 61.8% invalid)
  - Low invalid % + low F1: Model follows instructions but extracts wrong spans (potential reasoning failure)
  - High invalid % despite large size: Unexpected instruction-following degradation (e.g., gpt-oss-120b underperforms expectations)

- First 3 experiments:
  1. **Constrained decoding baseline**: Implement grammar-constrained generation forcing spans to be source text substrings; measure error rate reduction vs. prompt-only.
  2. **Model size ablation**: Test 7B, 14B, 32B, 70B variants within same model family (e.g., Qwen) to isolate scaling effects from architecture differences.
  3. **Annotator bias analysis**: Compare F1 distribution across annotators; investigate why Label Studio annotations yield systematically higher LLM alignment than custom interface annotations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can constrained decoding or structured output generation effectively eliminate the invalid evidence spans currently generated by LLMs?
- Basis in paper: [explicit] The authors state that models struggle with verbatim copying and explicitly call for "constrained decoding mechanisms" in future work (Conclusion).
- Why unresolved: The current study allowed unconstrained generation, resulting in high error rates (up to 61.8%) where models failed to follow instructions.
- What evidence would resolve it: A comparative study measuring the reduction in error rates when applying grammar-constrained decoding to the same set of models.

### Open Question 2
- Question: Do LLMs exhibit comparable alignment with human annotators when extracting fine-grained refuting evidence as they do for supporting evidence?
- Basis in paper: [explicit] The authors note they focused solely on supporting evidence and leave the analysis of "refuting evidence" to future work (Section 1).
- Why unresolved: The current dataset and experiments were restricted to text spans that justify or confirm claims, ignoring contradictions.
- What evidence would resolve it: Annotating the dataset for refuting evidence and evaluating the same LLMs (e.g., qwen3:14b, deepseek-r1:32b) on the new labels.

### Open Question 3
- Question: What specific factors in the annotation interface (Label Studio vs. custom tool) caused the systematic differences in granularity and style between the two sets of human annotations?
- Basis in paper: [explicit] The results note that all LLMs aligned better with Label Studio annotations, concluding that "Further analysis is required to reveal the differences between annotations" (Section 4).
- Why unresolved: While the discrepancy was observed (human-human F1 of 48), the underlying cause—whether UI design, instructions, or annotator bias—was not investigated.
- What evidence would resolve it: A qualitative review of the annotation differences or an ablation study using the same annotators on both interfaces.

## Limitations

- Dataset accessibility: The 186 Czech/Slovak claim-text pairs with dual annotations are not publicly available, preventing independent replication and cross-linguistic validation.
- Invalid output rates: High error rates (4-62%) indicate prompt-only constraints are insufficient, but the relationship between model architecture, instruction tuning, and error rates is not systematically analyzed.
- Annotation interface effects: Systematic biases in LLM alignment toward Label Studio annotations suggest interface-specific style matching rather than generalizable evidence extraction capability.

## Confidence

**High confidence**: The core finding that evidence extraction alignment plateaus beyond 14-32B parameters is well-supported by comparative F1 scores across multiple model families and sizes.

**Medium confidence**: Claims about Hungarian matching providing fair comparison across non-exhaustive annotations are methodologically sound, but the absence of alternative evaluation metrics limits confidence in whether the chosen approach captures all aspects of evidence extraction quality.

**Low confidence**: The paper's assertion that all evaluated LLMs "struggle" with valid evidence generation lacks comparative context — the absolute invalid rates (4-62%) span an order of magnitude, and the relationship between model architecture and error rates is not systematically analyzed.

## Next Checks

1. **Constrained decoding implementation**: Implement grammar-constrained generation forcing spans to be source text substrings; measure error rate reduction vs. prompt-only approach and assess impact on F1 scores.

2. **Cross-linguistic generalization**: Evaluate the same evidence extraction task on English and other language datasets to determine whether the 14-32B plateau and invalid output patterns hold across languages.

3. **Annotator style disentanglement**: Train a style classifier on the two annotation interfaces; measure whether LLM alignment differences persist after controlling for annotator-specific patterns in span selection and boundary placement.