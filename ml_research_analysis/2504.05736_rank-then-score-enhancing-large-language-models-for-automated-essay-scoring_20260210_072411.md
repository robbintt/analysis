---
ver: rpa2
title: 'Rank-Then-Score: Enhancing Large Language Models for Automated Essay Scoring'
arxiv_id: '2504.05736'
source_url: https://arxiv.org/abs/2504.05736
tags:
- essay
- scoring
- essays
- score
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Rank-Then-Score (RTS), a novel fine-tuning
  framework for enhancing large language models (LLMs) in automated essay scoring
  (AES) tasks. RTS consists of two models: a Ranker and a Scorer.'
---

# Rank-Then-Score: Enhancing Large Language Models for Automated Essay Scoring

## Quick Facts
- arXiv ID: 2504.05736
- Source URL: https://arxiv.org/abs/2504.05736
- Reference count: 40
- Rank-Then-Score (RTS) framework improves automated essay scoring by 1.9-1.1% QWK over direct prompting

## Executive Summary
This paper introduces Rank-Then-Score (RTS), a novel fine-tuning framework for enhancing large language models (LLMs) in automated essay scoring (AES) tasks. RTS consists of two models: a Ranker and a Scorer. The Ranker uses pairwise ranking with feature-enriched data to identify a candidate score set for a target essay, while the Scorer predicts the final score based on the candidate set and essay content. Experiments on Chinese (HSK) and English (ASAP) datasets demonstrate that RTS consistently outperforms the direct prompting method, achieving improvements of 1.9% on HSK and 1.7-1.1% on ASAP in terms of average Quadratic Weighted Kappa (QWK). On the HSK dataset, RTS surpasses other methods using smaller models, highlighting its effectiveness and cross-lingual adaptability.

## Method Summary
Rank-Then-Score (RTS) is a two-stage fine-tuning framework that leverages LLMs for automated essay scoring. The first stage employs a Ranker model that uses pairwise ranking with feature-enriched data to identify a candidate score set for each target essay. This ranking approach helps narrow down the possible score range before final prediction. The second stage uses a Scorer model that predicts the final essay score based on the candidate set identified by the Ranker and the essay content itself. The framework is designed to leverage LLMs' strengths in specialized fine-tuning and pairwise ranking, offering a robust solution for nuanced essay evaluation across different languages and scoring rubrics.

## Key Results
- RTS achieved 1.9% improvement in average QWK on Chinese HSK dataset compared to direct prompting
- On English ASAP dataset, RTS showed improvements of 1.7-1.1% in average QWK
- RTS outperformed other methods using smaller models on the HSK dataset

## Why This Works (Mechanism)
RTS works by breaking down the complex essay scoring task into two specialized subtasks. The Ranker model first identifies a narrowed candidate score set through pairwise ranking, which reduces the search space for the final score prediction. This two-stage approach leverages the LLM's ability to handle both ranking and scoring tasks effectively. By using feature-enriched data for pairwise ranking, the Ranker can capture nuanced differences between essays that might be difficult to assess in a single scoring pass. The Scorer then focuses on precise score prediction within this narrowed range, improving accuracy by reducing the cognitive load of choosing from all possible scores.

## Foundational Learning

**Quadratic Weighted Kappa (QWK)**: A statistical measure for inter-rater agreement that accounts for the degree of disagreement between predicted and actual scores. Needed to evaluate the quality of essay scoring systems where the distance between scores matters. Quick check: Ensure QWK is appropriate for your scoring scale (e.g., 0-6 vs 0-100).

**Pairwise Ranking**: A method of comparing two items at a time to determine preference ordering. Needed for the Ranker model to efficiently narrow down candidate score sets. Quick check: Verify that pairwise comparisons are meaningful for your specific essay scoring context.

**Feature Enrichment**: Adding additional information or derived features to the input data to improve model performance. Needed to provide the Ranker with more context for making accurate pairwise comparisons. Quick check: Identify which features are most relevant for your essay scoring domain.

**Fine-tuning**: The process of adapting a pre-trained model to a specific task by continuing training on task-specific data. Needed to adapt LLMs from general language understanding to specialized essay scoring. Quick check: Ensure your fine-tuning dataset is representative of the target scoring domain.

## Architecture Onboarding

**Component Map**: Raw Essays -> Feature Extraction -> Ranker (Pairwise Ranking) -> Candidate Score Set -> Scorer (Final Score Prediction) -> Final Score

**Critical Path**: The essential flow is from raw essays through feature extraction to the Ranker for candidate selection, then to the Scorer for final prediction. The pairwise ranking step is critical as it determines the candidate score set that the Scorer will use.

**Design Tradeoffs**: The two-stage approach trades computational overhead for improved accuracy. While maintaining two models increases complexity and inference time, it allows each model to specialize in its task. The framework also assumes pairwise comparisons are meaningful, which may not hold for all essay types.

**Failure Signatures**: If the Ranker poorly identifies candidate score sets, the Scorer will struggle regardless of its quality. Poor performance on essays with scores outside the training distribution. Inconsistent results across different essay prompts or domains.

**First Experiments**:
1. Test Ranker performance on identifying candidate score sets using pairwise ranking on a held-out validation set
2. Evaluate Scorer accuracy when given ground-truth candidate sets versus Ranker-generated sets
3. Compare end-to-end RTS performance against direct prompting on a small subset of essays before full-scale evaluation

## Open Questions the Paper Calls Out

None

## Limitations

- Performance improvements need statistical significance testing across all test folds
- Framework's reliance on pairwise ranking may not be suitable for all essay scoring contexts, particularly open-ended or highly subjective prompts
- Computational overhead of maintaining two models and inference latency at scale is not addressed

## Confidence

- **High confidence**: RTS architecture design and basic experimental setup on HSK and ASAP datasets
- **Medium confidence**: Reported QWK improvements and cross-lingual adaptability claims, pending significance testing
- **Low confidence**: Generalizability to other essay types, scoring rubrics, and scalability assertions

## Next Checks

1. Conduct statistical significance testing (e.g., paired t-tests or bootstrap confidence intervals) on QWK scores across all test folds to confirm consistent improvement
2. Evaluate RTS performance on additional essay datasets with different languages, scoring scales (e.g., 0-6 vs 0-100), and multi-trait rubrics to assess generalizability
3. Measure computational overhead and inference latency of the two-stage RTS pipeline compared to direct prompting methods, particularly at scale