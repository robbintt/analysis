---
ver: rpa2
title: Self-Training Large Language Models with Confident Reasoning
arxiv_id: '2505.17454'
source_url: https://arxiv.org/abs/2505.17454
tags:
- reasoning
- confidence
- self-training
- answer
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses a limitation in existing confidence-based self-training
  methods for LLMs, which rely solely on answer-level confidence and may ignore errors
  in reasoning paths. The authors propose CORE-PO, a new self-training method that
  incorporates reasoning-level confidence to identify high-quality reasoning paths.
---

# Self-Training Large Language Models with Confident Reasoning
## Quick Facts
- arXiv ID: 2505.17454
- Source URL: https://arxiv.org/abs/2505.17454
- Reference count: 24
- Key outcome: CORE-PO improves reasoning accuracy on multiple benchmarks by incorporating reasoning-level confidence into self-training

## Executive Summary
This paper addresses a critical limitation in existing self-training methods for large language models (LLMs), which typically rely on answer-level confidence and may overlook errors in intermediate reasoning steps. The authors propose CORE-PO, a novel self-training method that leverages reasoning-level confidence to identify and fine-tune LLMs on high-quality reasoning paths. By using Policy Optimization and P(True) scores to measure confidence, CORE-PO enhances the model's ability to generate accurate and reliable reasoning chains. Experiments demonstrate significant improvements in accuracy on both in-distribution and out-of-distribution reasoning tasks, with particular success on complex reasoning challenges like Game of 24.

## Method Summary
CORE-PO introduces a self-training framework that goes beyond traditional answer-level confidence by incorporating reasoning-level confidence. The method generates multiple reasoning paths for each query, evaluates their confidence using P(True) scores, and fine-tunes the LLM to prefer high-confidence reasoning paths through Policy Optimization. This approach ensures that the model learns to produce not only correct answers but also reliable and interpretable reasoning processes. By focusing on the quality of intermediate steps, CORE-PO addresses a key gap in existing self-training methods and enhances the model's overall reasoning capabilities.

## Key Results
- CORE-PO improves accuracy on four in-distribution benchmarks (GSM8K, ARC-Challenge, GPQA, MATH) and two out-of-distribution tasks (CRUXout, Game of 24).
- Achieves higher accuracy with greedy decoding than base models using inference-time scaling on ARC-Challenge and GPQA.
- Demonstrates the best performance on Game of 24, a challenging reasoning task.

## Why This Works (Mechanism)
CORE-PO works by incorporating reasoning-level confidence into the self-training process, ensuring that the LLM learns to generate high-quality reasoning paths rather than just correct answers. By using P(True) scores to evaluate confidence, the method identifies and prioritizes reliable reasoning steps, leading to more accurate and interpretable outputs. Policy Optimization fine-tunes the model to prefer these high-confidence paths, enhancing its reasoning capabilities. This approach addresses the limitations of answer-level confidence, which can overlook errors in intermediate reasoning steps, and provides a more robust foundation for complex reasoning tasks.

## Foundational Learning
- **P(True) scores**: Used to measure reasoning-level confidence by evaluating the likelihood of each reasoning step being correct. **Why needed**: Provides a granular assessment of reasoning quality. **Quick check**: Verify that P(True) scores correlate with human judgments of reasoning reliability.
- **Policy Optimization**: Fine-tunes the LLM to prefer high-confidence reasoning paths. **Why needed**: Ensures the model learns to generate reliable reasoning processes. **Quick check**: Compare the quality of reasoning paths before and after fine-tuning.
- **Self-training**: Leverages the model's own outputs to improve performance. **Why needed**: Enables iterative refinement of reasoning capabilities. **Quick check**: Assess the stability of improvements over multiple self-training cycles.

## Architecture Onboarding
- **Component map**: LLM -> Multiple reasoning path generation -> P(True) confidence evaluation -> Policy Optimization -> Fine-tuned LLM
- **Critical path**: The sequence from reasoning path generation to fine-tuning via Policy Optimization is essential for achieving improved accuracy and reasoning quality.
- **Design tradeoffs**: Balancing the computational cost of generating and evaluating multiple reasoning paths against the benefits of improved accuracy and interpretability.
- **Failure signatures**: Over-reliance on P(True) scores may lead to bias in reasoning path selection; insufficient diversity in generated paths may limit generalization.
- **First experiments**: 1) Test CORE-PO on a broader range of reasoning tasks. 2) Compare P(True) scores with alternative confidence metrics. 3) Evaluate computational efficiency in real-world deployment.

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to larger and more diverse reasoning tasks remains uncertain.
- Reliance on P(True) scores may not fully capture reasoning quality in complex scenarios.
- Computational overhead of generating and evaluating multiple reasoning paths could limit practical deployment.

## Confidence
- The improvement in accuracy on in-distribution and out-of-distribution benchmarks is supported by experimental results, but the magnitude of gains may vary with task complexity. **Medium confidence.**
- The claim that CORE-PO achieves higher accuracy with greedy decoding than base models using inference-time scaling is promising but may not generalize to all reasoning tasks. **Medium confidence.**
- The assertion that CORE-PO enhances reasoning capabilities through improved reasoning-level confidence is plausible but requires further validation across diverse domains. **Medium confidence.**

## Next Checks
1. Test CORE-PO on a broader range of reasoning tasks, including those with open-ended or multi-step solutions, to assess scalability and generalizability.
2. Compare the performance of CORE-PO against other confidence-based self-training methods using alternative confidence metrics (e.g., entropy-based or ensemble-based measures) to validate the robustness of P(True) scores.
3. Evaluate the computational efficiency of CORE-PO in real-world deployment scenarios, including the trade-offs between reasoning path generation and inference speed.