---
ver: rpa2
title: Label Smoothing Improves Gradient Ascent in LLM Unlearning
arxiv_id: '2510.22376'
source_url: https://arxiv.org/abs/2510.22376
tags:
- forget
- data
- unlearning
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Smoothed Gradient Ascent (SGA), a method for
  large language model unlearning that addresses the instability and utility collapse
  issues of traditional Gradient Ascent. SGA improves upon GA by combining forget
  data with generated normal data using a tunable smoothing rate, enabling more stable
  forgetting while better preserving model performance.
---

# Label Smoothing Improves Gradient Ascent in LLM Unlearning

## Quick Facts
- arXiv ID: 2510.22376
- Source URL: https://arxiv.org/abs/2510.22376
- Reference count: 32
- This paper proposes Smoothed Gradient Ascent (SGA), a method for large language model unlearning that addresses the instability and utility collapse issues of traditional Gradient Ascent.

## Executive Summary
This paper introduces Smoothed Gradient Ascent (SGA), a novel approach to LLM unlearning that stabilizes the unstable Gradient Ascent method while better preserving model utility. SGA combines forget data with generated normal data using a tunable smoothing rate, creating a unified loss that balances forgetting with learning. The method is theoretically grounded with guidance on selecting optimal smoothing rates, and empirically validated across three benchmarks (TOFU, Harry Potter, MUSE-NEWS), demonstrating consistent improvements over GA and achieving top-2 performance among baseline methods.

## Method Summary
SGA addresses LLM unlearning by combining forget data with generated normal data through generalized label smoothing. For each forget instance, K-1 normal samples are generated (via embedding retrieval or GPT generation), and a unified loss is constructed using a smoothing rate r that controls the balance between forgetting and learning. The method theoretically derives an optimal smoothing rate r* based on gradient geometry, though in practice a fixed r is used for efficiency. The approach stabilizes the divergent Gradient Ascent while preventing the utility collapse seen in alternative methods.

## Key Results
- SGA consistently outperforms the original GA method across all metrics on three benchmarks
- Achieves top-2 performance among all baseline methods on several key metrics
- Optimal smoothing rate strongly depends on model scale (Llama2-7B: r=-0.8, OPT-2.7B: r∈{-2,-4,-8}, Phi-1.5B: r=-0.8)
- Successfully prevents both divergence (PPL explosion) and utility collapse observed in GA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient redirection mitigates divergence during unlearning
- Mechanism: SGA constructs a combined update d(r) = -g_f + r[u] where u = ḡ_p - (1-1/K)g_f, deflecting the trajectory away from pure maximization. This introduces a stabilizing component from normal data gradients that partially counteracts the divergent forget gradient.
- Core assumption: The normal data gradients ḡ_p carry semantic relevance to the forget domain (achieved via similarity-based selection or GPT generation), ensuring the deflection direction is meaningful rather than arbitrary.
- Evidence anchors:
  - [abstract] "SGA combines forget data with generated normal data...redirecting the gradient update direction and stabilizing the unlearning process"
  - [Section 4.2] "A key reason why SGA effectively suppresses the divergence issue of GA is that it alters GA's gradient-ascent direction"
  - [corpus] Related work on gradient ascent instability (e.g., "Ascent Fails to Forget") corroborates that pure ascent directions are problematic, though corpus lacks direct mechanistic validation of the redirection hypothesis
- Break condition: If normal data is semantically unrelated or if r is set to extreme values (e.g., large positive r causing overshooting, or r ≫ r*), the deflection may either over-correct (preserving too much) or under-correct (still diverging). Empirical results show PPL explosion at r = -0.2 for OPT-2.7B despite the correction.

### Mechanism 2
- Claim: Generalized label smoothing enables joint forget-learn optimization within a single loss
- Mechanism: Rather than alternating between ascent (forget) and descent (normal) steps—which can conflict—SGA constructs a unified label distribution y_GLS,r = (-(1 - (K-1)/K)r, -r/K, -r/K, ...) over K classes (1 forget + K-1 normal). Negative coefficients indicate ascent (forgetting), positive indicate descent (learning). This allows gradient flow from both objectives simultaneously, balanced by r.
- Core assumption: The K-class label space construction correctly captures the forget/normal trade-off, and the smoothing rate r appropriately weights the relative importance of these competing objectives.
- Evidence anchors:
  - [Section 3.3] Defines generalized label smoothing with r ∈ (-∞, 1], showing how r controls the label distribution
  - [Section 4.1] "The label distribution becomes (-[1 - (K-1)/K]r, -r/K, -r/K, ...)"
  - [corpus] No direct corpus evidence on this specific GLS construction for unlearning; gap in validation
- Break condition: When r approaches 1, the forget component coefficient shrinks toward 0, potentially under-unlearning. When r is very negative, normal data coefficients become large positive (strong learning) but forget coefficients become large negative (strong ascent), potentially causing oscillation or instability.

### Mechanism 3
- Claim: Optimal smoothing rate r* exists and depends on gradient geometry
- Mechanism: The theoretical analysis derives r* = ⟨g_f, u⟩ / ||u||², where the optimal rate minimizes update norm ||d(r)||² while maintaining forgetting direction. This balances the trade-off between effective forgetting (requiring sufficient -g_f component) and stability (requiring sufficient deflection toward normal gradients).
- Core assumption: The one-step update norm minimization is a valid proxy for the multi-step unlearning objective, and the gradients g_f and ḡ_p remain approximately constant during early training (justifying fixed r).
- Evidence anchors:
  - [Section 4.3] Equation 9 provides the closed-form r* derivation
  - [Section 5.2] Figure 2 validates that empirical optimal r values align with sign(⟨g_f, u⟩) predictions
  - [corpus] No corpus papers provide alternative derivations or counter-theories for optimal smoothing rate selection
- Break condition: The assumption of static gradients breaks as θ evolves during training; the paper acknowledges r* "changes dynamically" but fixes r for efficiency. If the true r* shifts significantly during unlearning, a fixed r may become suboptimal mid-training.

## Foundational Learning

- Concept: Gradient Ascent for Unlearning
  - Why needed here: Understanding why GA is unstable is prerequisite to appreciating SGA's contribution. GA inverts SFT loss to "forget" by maximizing prediction loss on forget data, but this drives parameters toward divergent regions where perplexity explodes.
  - Quick check question: If you run GA for 1000 steps on a forget set without any regularization, would you expect the model's next-token prediction loss on normal text to increase, decrease, or become unpredictable? (Answer: Increase—often catastrophically, due to divergence)

- Concept: Label Smoothing in Classification
  - Why needed here: SGA extends standard label smoothing (which softens one-hot labels to prevent overconfidence) to a generalized form with negative smoothing rates. Understanding the standard form helps grasp why this generalization enables "negative learning" (forgetting).
  - Quick check question: Standard label smoothing with r=0.1 converts [1, 0, 0] to what distribution? How would r=-0.1 change it? (Answer: [0.9, 0.05, 0.05] vs. [1.1, -0.05, -0.05]—negative coefficients enable ascent)

- Concept: Catastrophic Forgetting vs. Targeted Unlearning
  - Why needed here: The paper navigates between two failure modes: insufficient forgetting (privacy leakage) and excessive forgetting (utility collapse). Understanding this trade-off clarifies why SGA's balance matters.
  - Quick check question: If an unlearning method reduces forget set accuracy to 0% but also reduces retain set accuracy from 95% to 20%, is this a successful unlearning? (Answer: No—utility collapse defeats the purpose; effective unlearning must preserve general capabilities)

## Architecture Onboarding

- Component map: Forget Set D_f -> Normal Data Generator -> Label Constructor -> SGA Optimizer -> Model Update
- Critical path:
  1. Prepare forget set D_f (given by unlearning request)
  2. Generate K-1 normal samples per forget instance (critical quality step—irrelevant normal data degrades performance)
  3. Estimate optimal r range via gradient analysis or pilot sweep
  4. Train with SGA loss, monitoring both forget quality (KS test p-value) and model utility (ROUGE-L, perplexity)
  5. Validate with privacy leakage tests (MIA AUC-ROC)

- Design tradeoffs:
  - **Normal data source**: GPT-generated vs. retrieved. Retrieved preserves domain alignment but may be unavailable; generated is flexible but requires prompt engineering (see Appendix D prompts)
  - **r selection**: Fixed vs. dynamic. Fixed is simple and efficient; dynamic (if implemented) could adapt to changing gradient geometry but adds complexity
  - **K (number of normal samples)**: Higher K provides more stable gradient averaging but increases computation. Paper uses K=4 (1 forget + 3 normal) across experiments

- Failure signatures:
  - **PPL explosion (>1e10)**: Indicates extreme divergence; typically from r too negative or pure GA (r=0) without stabilization
  - **Forget quality FQ < 0.01**: Under-unlearning; model retains too much forget knowledge. Often from r too positive (over-weighting normal data)
  - **Model utility MU < 0.4**: Over-unlearning; general capabilities collapsed. Balance is off—check r and normal data quality
  - **PrivLeak >> 0 or << 0**: Privacy leakage or over-forgetting on MUSE benchmark. Indicates poor calibration of forget/retain trade-off

- First 3 experiments:
  1. **Baseline reproduction**: Implement GA (r=0) on TOFU forget01 split with Llama2-7B; observe divergence (PPL should spike, MU should drop). This establishes the problem SGA solves.
  2. **r sweep**: With K=4 normal samples (retrieved from retain set), sweep r ∈ {-4, -2, -0.8, -0.4, -0.2, 0.2, 0.4, 0.8}; plot FQ vs. MU curve. Identify Pareto frontier—expect r* ≈ negative range per Section 5.2.
  3. **Ablation on normal data source**: Compare (a) embedding-based retrieval vs. (b) GPT-generated normal data at fixed r=-0.8. Expect similar or better performance with GPT per Table 4, but optimal r may shift.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can dynamically adjusting the smoothing rate during the unlearning process improve the balance between forgetting and model utility?
  - Basis in paper: [explicit] Section A.2 states "open questions remain as, whether dynamically adjusting the smoothing rate during the unlearning process could yield better results."
  - Why unresolved: The current implementation fixes $r$ at the start of training for efficiency, despite the theoretical analysis suggesting the optimal $r^*$ evolves as model parameters change during unlearning.
  - What evidence would resolve it: A comparative study where the fixed-$r$ baseline is evaluated against a method where $r$ is updated periodically based on Equation 9 or a predefined schedule.

- **Open Question 2**: What is the impact of normal data semantics (retrieved vs. generated) on the stability and optimal smoothing rate of SGA?
  - Basis in paper: [inferred] Section 4.1 uses different strategies (embedding retrieval vs. GPT-4o generation) for different benchmarks, and Section 5.5 notes the optimal $r$ shifts with the data source.
  - Why unresolved: The paper does not establish a theoretical link between the semantic quality of normal data and gradient stability, leaving the choice of generation strategy heuristic.
  - What evidence would resolve it: Controlled experiments where "normal data" is systematically varied in semantic similarity to "forget data" to measure the impact on Model Utility and the location of $r^*$.

- **Open Question 3**: Is there a predictable scaling law for the optimal smoothing rate $r^*$ relative to LLM parameter size?
  - Basis in paper: [inferred] The abstract notes that "the optimal smoothing rate strongly depends on model scale," but the paper provides no formula to predict $r^*$ for unseen model sizes.
  - Why unresolved: While empirical differences are observed between 1.5B, 2.7B, and 7B models, it is unclear if these differences follow a monotonic trend or are merely architecture-dependent.
  - What evidence would resolve it: A regression analysis of optimal $r^*$ values against parameter counts across a wider range of model scales (e.g., 13B, 70B) within a single model family.

## Limitations

- The theoretical guidance for selecting optimal smoothing rates relies on static gradient assumptions that may not hold during multi-step training
- The quality of generated normal data significantly affects performance, yet the paper provides limited analysis of generation failure modes or robustness to noisy normal samples
- The claim that SGA achieves "top-2 performance among all baseline methods" lacks sufficient comparative analysis with state-of-the-art approaches beyond GA

## Confidence

- **High confidence**: SGA outperforms standard GA in preventing divergence and utility collapse (empirically validated across three benchmarks)
- **Medium confidence**: The theoretical guidance for selecting optimal r values is useful, though the static assumption limits practical applicability
- **Medium confidence**: Generalized label smoothing with negative rates effectively combines forget and learn objectives, but the mechanism's sensitivity to normal data quality needs more exploration
- **Low confidence**: The claim that SGA achieves "top-2 performance among all baseline methods" lacks sufficient comparative analysis with state-of-the-art approaches beyond GA

## Next Checks

1. **Dynamic r selection experiment**: Implement and evaluate a scheme that updates r based on observed gradient geometry changes during training, measuring stability and performance trade-offs against fixed r

2. **Normal data quality ablation**: Systematically vary the quality of generated normal samples (e.g., introduce noise, semantic drift) and measure the impact on unlearning effectiveness and model utility to quantify robustness requirements

3. **Cross-task transferability test**: Apply SGA with fixed hyperparameters (r, K) across diverse unlearning tasks (beyond the three benchmarks) to assess generalization and identify task-specific failure modes