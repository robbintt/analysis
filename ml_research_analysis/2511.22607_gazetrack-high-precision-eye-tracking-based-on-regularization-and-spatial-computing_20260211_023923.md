---
ver: rpa2
title: 'GazeTrack: High-Precision Eye Tracking Based on Regularization and Spatial
  Computing'
arxiv_id: '2511.22607'
source_url: https://arxiv.org/abs/2511.22607
tags:
- gaze
- pupil
- ellipse
- loss
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GazeTrack, a high-precision eye tracking
  framework based on regularization and spatial computing for virtual and augmented
  reality applications. The core contribution is a novel shape error regularization
  method for pupil ellipse fitting combined with semantic segmentation and a coordinate
  transformation approach.
---

# GazeTrack: High-Precision Eye Tracking Based on Regularization and Spatial Computing

## Quick Facts
- **arXiv ID:** 2511.22607
- **Source URL:** https://arxiv.org/abs/2511.22607
- **Reference count:** 7
- **Primary result:** 2.84 degrees gaze vector error vs 4.38 degrees for prior SOTA

## Executive Summary
This paper introduces GazeTrack, a high-precision eye tracking framework based on regularization and spatial computing for virtual and augmented reality applications. The core contribution is a novel shape error regularization method for pupil ellipse fitting combined with semantic segmentation and a coordinate transformation approach. The method processes eye images through semantic segmentation, pupil localization, and gaze vector generation, achieving reduced gaze angle error with lower computational complexity. Experiments demonstrate superior performance compared to existing methods, with GazeTrack achieving 2.84 degrees gaze vector error compared to 4.38 degrees for the previous state-of-the-art, while also providing faster computation times.

## Method Summary
GazeTrack processes eye tracking through a three-stage pipeline: (1) semantic segmentation of the pupil using a modified U-Net architecture with attention and residual connections, (2) coordinate transformation that normalizes gaze points using a geometric "paper unfolding" approach, and (3) gaze vector generation using a self-attention based neural network. The key innovation is the addition of an Ellipse Fit Error (L_EFE) regularization term to the segmentation loss function, which constrains the model to produce elliptical pupil shapes rather than irregular blobs. The method uses the LPW dataset for training segmentation, the GazeTrack dataset for end-to-end evaluation, and the ExCuSe dataset for benchmarking. The pipeline achieves real-time performance with reduced computational complexity compared to prior methods.

## Key Results
- Achieved 2.84 degrees gaze vector error compared to 4.38 degrees for prior state-of-the-art
- Reduced pixel-level error from 7.20 to 3.84 through coordinate transformation
- Improved segmentation accuracy from 88.8% to 90.1% with shape-prior regularization
- Lower computational complexity enabling real-time performance

## Why This Works (Mechanism)

### Mechanism 1: Shape-Prior Regularization for Segmentation
- **Claim:** If pupil segmentation is constrained by a geometric shape prior during training, the model produces more robust ellipse fits under occlusion or motion blur than standard pixel-wise loss alone.
- **Mechanism:** The architecture adds an Ellipse Fit Error ($L_{EFE}$) term to the standard Binary Cross Entropy ($L_{BCE}$). This term calculates the Euclidean distance between predicted boundary pixels and the closest point on the ground truth ellipse boundary, pushing the network to predict convex, elliptical shapes rather than irregular blobs.
- **Core assumption:** The pupil boundary in a 2D image can be accurately approximated by an ellipse equation, and deviations from this shape are errors to be penalized.
- **Evidence anchors:** Mentions a "novel shape error regularization method to constrain pupil ellipse fitting" and defines $L_{EFE}$ which forces boundary pixels toward the ellipse ground truth.
- **Break condition:** The mechanism fails if the pupil is significantly occluded or distorted such that its projection is no longer elliptical.

### Mechanism 2: Spatial Normalization via Coordinate Transformation
- **Claim:** If raw gaze coordinates are transformed into a standardized spatial reference frame (CoordTransNet) before vector generation, pixel-level errors decrease significantly.
- **Mechanism:** The method identifies the farthest gaze point from the center to define a reference diagonal and applies linear interpolation to map diverse gaze distributions onto a standard square coordinate system.
- **Core assumption:** The relationship between raw collected gaze points and a standardized grid is linear and elastic, allowing for consistent reconstruction across different collection angles.
- **Evidence anchors:** Mentions a "novel coordinate transformation method similar to paper unfolding" and reports reduction in pixel error from 7.20 to 3.84 when using CoordTransNet.
- **Break condition:** The mechanism assumes distinct feature points to calculate transformation ratios; it may break if the input gaze data lacks sufficient angular spread.

### Mechanism 3: Temporal Context for Vector Regression
- **Claim:** If the gaze vector model (GVnet) utilizes a sliding window of input data during inference, it achieves lower angular error than single-frame inference.
- **Mechanism:** GVnet uses a self-attention layer and fully connected layers to process input features, with results suggesting that aggregating context over a window acts as a temporal smoother.
- **Core assumption:** Gaze movement possesses temporal continuity, and averaging or attending to a sequence of coordinates yields a more accurate vector estimate than a solitary point.
- **Evidence anchors:** Shows error dropping from 9.37 degrees (W1) to 0.94 degrees (W20) and states the model achieves reduced error while emphasizing "single image" capability after training.
- **Break condition:** During rapid eye movements (saccades), a large sliding window may introduce latency or "smear" the predicted vector.

## Foundational Learning

- **Concept: Semantic Segmentation with Attention (U-ResAtt)**
  - **Why needed here:** You must understand how the paper modifies the standard U-Net to handle the low signal-to-noise ratio in eye images.
  - **Quick check question:** How does adding an attention mechanism theoretically help the model distinguish the pupil from dark eyelashes or shadows?

- **Concept: Regularization in Loss Functions**
  - **Why needed here:** The core innovation is modifying the loss function ($L = \alpha L_{BCE} + \beta L_{EFE}$) rather than just the network architecture.
  - **Quick check question:** If the $L_{EFE}$ weight ($\beta$) is set too high, what artifact might appear in the predicted pupil shape during inference?

- **Concept: Coordinate Transformation / Homography**
  - **Why needed here:** The "CoordTransNet" relies on geometric logic rather than learned neural weights.
  - **Quick check question:** Why is it necessary to identify the "farthest point from the center" before calculating the transformation ratios?

## Architecture Onboarding

- **Component map:** Eye Image (384x288) -> U-ResAtt -> Outputs segmentation mask -> Post-Processing (Edge Drawing + Ellipse Fitting) -> Outputs coordinates -> CoordTransNet -> Outputs normalized coordinates -> GVnet (Self-attention + Dense layers) -> Outputs gaze vector

- **Critical path:** The *Ellipse Fit Error* in the U-ResAtt training phase. If this regularization does not converge, the downstream ellipse fitting will fail, propagating noise to the Coordinate Transformation and final Vector Generation.

- **Design tradeoffs:**
  - **Accuracy vs. Speed:** The paper reduces U-Net layers to meet real-time constraints (60FPS input) but compensates with the $L_{EFE}$ regularization to maintain accuracy.
  - **Flexibility vs. Standardization:** CoordTransNet standardizes diverse data distributions, but this assumes the "unfolding" logic holds for all eye shapes.

- **Failure signatures:**
  - **Reflection Artifacts:** If the segmentation includes the glint (reflection) as part of the pupil, the ellipse fitting will skew toward the bright spot.
  - **Coordinate Collapse:** If CoordTransNet receives gaze points that are nearly collinear, the "diagonal" reference calculation may become unstable or divide by zero.

- **First 3 experiments:**
  1. **Baseline Segmentation:** Train U-ResAtt on the LPW dataset with and without the $L_{EFE}$ term to quantify the improvement in pixel accuracy (Target: ~90% vs ~89%).
  2. **Transformation Ablation:** Pass validation data through the pipeline with CoordTransNet disabled vs. enabled to verify the reduction in pixel error (Target: < 4.0 pixel error).
  3. **Latency Test:** Measure inference time for GVnet on a single core with varying window sizes (W1 vs W20) to validate the "lower computational complexity" claim against the NN30LM baseline.

## Open Questions the Paper Calls Out
None

## Limitations
- Critical hyperparameters (loss weights α/β, network dimensions) are not specified
- No open-source implementation available for verification
- Core innovation relies on assumptions about pupil shape that may not hold under severe occlusion
- Coordinate transformation assumes linear relationships that could fail with sparse or collinear gaze distributions

## Confidence

**Confidence Labels:**
- Shape-prior regularization mechanism: High confidence in theoretical validity
- CoordTransNet transformation accuracy: Medium confidence (depends on dataset characteristics)
- Temporal context improvement: Medium confidence (window size selection unclear)

## Next Checks

1. Implement ablation study on L_EFE weight (β) to identify optimal regularization strength and quantify trade-off between shape accuracy and boundary precision
2. Test CoordTransNet with artificially sparse gaze distributions to measure failure threshold when diagonal reference becomes unstable
3. Benchmark GVnet with varying window sizes (W1, W5, W10, W20) on validation set to determine if gains plateau or degrade at larger windows