---
ver: rpa2
title: Enhancing compact convolutional transformers with super attention
arxiv_id: '2508.18960'
source_url: https://arxiv.org/abs/2508.18960
tags:
- attention
- accuracy
- zhang
- wang
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a compact convolutional transformer variant
  that improves upon existing architectures by adopting token mixing, sequence-pooling,
  and convolutional tokenizers. The authors introduce super attention, which reduces
  the number of parameters in the attention layer by 25% and total parameters by 40%,
  while maintaining or improving performance.
---

# Enhancing compact convolutional transformers with super attention

## Quick Facts
- arXiv ID: 2508.18960
- Source URL: https://arxiv.org/abs/2508.18960
- Authors: Simpenzwe Honore Leandre; Natenaile Asmamaw Shiferaw; Dillip Rout
- Reference count: 6
- Primary result: 46.29% top-1% validation accuracy on CIFAR100, outperforming baseline CCT with standard attention (36.50%)

## Executive Summary
This paper proposes a compact convolutional transformer variant that improves upon existing architectures by adopting token mixing, sequence-pooling, and convolutional tokenizers. The authors introduce super attention, which reduces the number of parameters in the attention layer by 25% and total parameters by 40%, while maintaining or improving performance. Experiments on CIFAR100 show that the proposed model achieves 46.29% top-1% validation accuracy and 76.31% top-5% validation accuracy, outperforming the baseline compact convolutional transformer with standard attention (36.50% and 66.33% respectively). The model demonstrates better training stability, faster convergence, and parameter efficiency without requiring data augmentation, positional embeddings, or learning rate scheduling.

## Method Summary
The proposed method implements a 6-layer transformer with super attention mechanism, embedding dimension 768, and 24 attention heads. The architecture uses convolutional tokenization (3×3 Conv + GELU + MaxPool) to produce sequence tokens, followed by transformer blocks with super attention (token mixing via $W_A$ matrix), and sequence pooling for classification. The model is trained with AdamW optimizer (lr=0.01 constant, β1=0.9, β2=0.999, weight decay=0.01) for 75 epochs with batch size 1024, orthogonal weight initialization, and no positional embeddings or data augmentation.

## Key Results
- Achieves 46.29% top-1% and 76.31% top-5% validation accuracy on CIFAR100
- Reduces parameters by 40% compared to baseline CCT with standard attention
- Demonstrates better training stability without requiring learning rate scheduling
- Shows faster convergence and improved generalization compared to baseline

## Why This Works (Mechanism)

### Mechanism 1: Token Mixing via Value-side Transformation
The "Super Attention" mechanism improves parameter efficiency by applying a learned mixing matrix ($W_A$) directly to the value vectors, replacing redundant projections found in standard Scaled Dot-Product Attention. This matrix transforms the Value vectors ($V'_i = W_A V_i$) along the sequence dimension before the attention softmax is applied, explicitly modeling token relationships in the value space and reducing overall parameter count by 40%.

### Mechanism 2: Sequence Pooling for Classification
The architecture discards the standard [CLS] token and instead applies a linear transformation ($W_p$) to the output sequence, computes a softmax to generate attention weights over the sequence, and then computes a weighted sum. This allows the model to "attend" to the most relevant parts of the final feature map for classification dynamically.

### Mechanism 3: Training Stability via Orthogonal Initialization
Weights are initialized orthogonally to prevent early layer gradients from vanishing or exploding, allowing the model to train effectively with a high, constant learning rate (0.01) and no warm-up scheduling. This stability is particularly effective given the shallow depth (6 layers) of the network.

## Foundational Learning

- **Concept: Scaled Dot-Product Attention (SDPA)** - Understanding the standard $Softmax(QK^T/\sqrt{d})V$ formulation is required to see how adding $W_A$ changes the flow of information. Quick check: In standard attention, how does the interaction between Query and Key determine the final output?
- **Concept: Convolutional Tokenization** - The paper uses a convolutional tokenizer rather than standard patching. You must understand how a Conv2D layer flattens into a sequence to implement the input pipeline correctly. Quick check: How does a convolutional tokenizer output a sequence of tokens from an image, and why might this remove the need for positional embeddings?
- **Concept: Overfitting vs. Generalization** - The key result is that the baseline SDPA diverges/overfits while Super Attention generalizes. Recognizing the gap between training and validation loss is critical for interpreting the experiment results. Quick check: If training loss goes down but validation loss goes up, what is happening, and how does the paper claim Super Attention addresses it?

## Architecture Onboarding

- **Component map:** Image (3×32×32) -> Tokenizer (Conv2D → GELU → MaxPool → Flatten) -> Backbone (6×[LayerNorm → Super Attention → LayerNorm → MLP]) -> Head (Sequence Pooling → Weighted Sum → MLP Classifier)
- **Critical path:** The efficiency gain relies on the condition context length (ℓ) < embedding dimension (d_m). Ensure your input resolution and patch size maintain this relationship, or the model will be less efficient than the baseline.
- **Design tradeoffs:** The model is highly stable (no LR schedule) but limited to fixed context lengths; it is not designed for variable-length generation tasks (LLM-style) out of the box. The authors explicitly remove Mixup/CutMix to prove data efficiency, but you might need to re-enable augmentation for extremely small custom datasets.
- **Failure signatures:** If implementing the baseline SDPA comparison, expect divergence after epoch ~40. If using high-resolution images (large ℓ), the $W_A$ matrix (ℓ×ℓ) will become a computational bottleneck.
- **First 3 experiments:**
  1. **Sanity Check (Overfit):** Train on a single batch of CIFAR100. The model should reach near-zero loss quickly; if not, check the orthogonal initialization implementation.
  2. **Baseline Comparison:** Run the provided SDPA config vs. Super Attention config for 20 epochs on CIFAR100. Verify that SDPA starts overfitting (gap between train/val loss) while Super Attention tracks closer.
  3. **Resolution Scaling:** Increase image size to 64×64. Measure inference time to verify the ℓ < d_m efficiency boundary (it should slow down significantly relative to the standard attention baseline).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the super attention mechanism be adapted to handle variable context lengths required for generative tasks?
- Basis in paper: The authors explicitly note that the architecture uses a fixed context length, which "may constrain performance on generation tasks requiring variable length contexts," and list extending this as a future direction.
- Why unresolved: The current architectural design relies on fixed dimensions for the token mixing and pooling operations, making it structurally incompatible with the dynamic input sizes typical of generation tasks.
- What evidence would resolve it: Demonstration of the model performing sequence generation (e.g., text or time-series) where input lengths vary dynamically during inference without performance degradation.

### Open Question 2
- Question: Is the proposed architecture compatible with IO-aware optimization techniques like FlashAttention-3?
- Basis in paper: The conclusion suggests "Incorporating recent optimization techniques like those proposed by Shah et al. (2024)" as a specific avenue for future research.
- Why unresolved: Super attention modifies the standard attention calculation (token mixing from the left), which may alter the memory access patterns that FlashAttention optimizes, creating a compatibility challenge.
- What evidence would resolve it: Successful integration of FlashAttention-3 kernels into the super attention codebase resulting in measurable memory reduction and speed improvements.

### Open Question 3
- Question: Do the efficiency and accuracy gains of super attention scale to larger datasets and model parameter counts?
- Basis in paper: The paper states that "computational resource constraints limited the scale of our experiments" and suggests "Investigating scaling laws through larger model sizes and training runs."
- Why unresolved: The results are currently confined to the CIFAR-100 benchmark (60,000 images) and a 6-layer model; it is unverified if the 10% accuracy boost holds for ImageNet-scale data or deeper networks.
- What evidence would resolve it: Benchmarking the model on high-resolution datasets (e.g., ImageNet) and varying layer depths to plot scaling curves relative to the baseline.

## Limitations
- Results are based on a single CIFAR100 experiment with no ablation studies
- Parameter efficiency claims lack independent verification
- Generalization to larger datasets and higher resolutions remains untested
- Orthogonal initialization benefits lack comparison to other initialization schemes
- Sequence pooling superiority over [CLS] tokens is asserted but not empirically validated

## Confidence

**High Confidence**: The architectural description of Super Attention (replacing standard Q/K/V projections with value-side mixing via $W_A$) is clearly specified and internally consistent. The parameter efficiency claim (25% reduction in attention layer, 40% total) follows logically from the mathematical formulation.

**Medium Confidence**: The training stability claim (no LR scheduling needed) is supported by CIFAR100 results but lacks ablation studies comparing orthogonal initialization to standard schemes. The sequence pooling mechanism's superiority over [CLS] tokens is asserted but not empirically validated against alternatives.

**Low Confidence**: The efficiency claims for inference when context length is less than embedding dimension are theoretically sound but unverified experimentally. The generalizability of the 46.29% top-1% accuracy to other datasets remains unknown.

## Next Checks

1. **Ablation Study on Initialization**: Train identical models with Xavier/Glorot initialization versus orthogonal initialization under identical conditions (same learning rate schedule or lack thereof) to isolate the contribution of orthogonal init to training stability.

2. **Resolution Scaling Experiment**: Test the model across input resolutions from 32×32 to 256×256, measuring both accuracy and inference time to verify the efficiency boundary where context length exceeds embedding dimension.

3. **Mechanism Isolation Test**: Implement a variant that keeps standard SDPA but adds the sequence pooling mechanism alone, and another that keeps sequence pooling but uses standard [CLS] token, to quantify the individual contributions of each proposed modification.