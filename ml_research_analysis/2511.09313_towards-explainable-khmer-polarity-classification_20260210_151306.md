---
ver: rpa2
title: Towards Explainable Khmer Polarity Classification
arxiv_id: '2511.09313'
source_url: https://arxiv.org/abs/2511.09313
tags:
- khmer
- dataset
- polarity
- text
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an explainable Khmer polarity classifier that
  not only predicts sentiment labels accurately but also provides reasoning for its
  predictions. This is achieved by fine-tuning instruction-tuned Qwen-3 models using
  reasoning and non-reasoning prompt templates.
---

# Towards Explainable Khmer Polarity Classification

## Quick Facts
- **arXiv ID**: 2511.09313
- **Source URL**: https://arxiv.org/abs/2511.09313
- **Reference count**: 20
- **Primary result**: Fine-tuned Qwen-3 models achieve 84% accuracy on Khmer polarity classification while providing self-explanations identifying sentiment keywords

## Executive Summary
This paper introduces an explainable Khmer polarity classifier that not only predicts sentiment labels accurately but also provides reasoning by identifying polarity-related keywords or phrases. The approach leverages fine-tuning of instruction-tuned Qwen-3 models using reasoning and non-reasoning prompt templates. The models learn to generate self-explanations as part of their output, creating an explainable system for Khmer sentiment analysis. The authors also contribute new Khmer polarity datasets (formal and casual) and demonstrate that their approach outperforms classical machine learning methods while maintaining strong performance on casual, romanized Khmer text.

## Method Summary
The method fine-tunes an instruction-tuned Qwen-3 model using Low-Rank Adaptation (LoRA) on two Khmer polarity datasets: a formal KP dataset with sentiment keywords and a casual CKP dataset without keywords. The model is trained using reasoning prompts that enforce a "thinking mode" where it first extracts polarity-related keywords before generating the final label. A non-reasoning prompt is used for CKP data. Training uses 4-bit quantized weights on a single GPU with LoRA (r=32, Î±=32) targeting self-attention and feed-forward layers. Inference always uses the reasoning prompt to trigger thinking mode and generate explanations.

## Key Results
- Fine-tuned Qwen-3 models achieve 84% accuracy on the Khmer Polarity (KP) dataset, outperforming classical ML methods
- On the Casual Khmer Polarity (CKP) dataset, models achieve 87% accuracy, comparable to dedicated deep learning models
- Models successfully generate self-explanations identifying sentiment keywords, though qualitative assessment only
- When thinking mode is disabled, model performance significantly degrades

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Enabling the model's "thinking mode" via a structured prompt template improves classification accuracy by conditioning the final label prediction on an intermediate reasoning step.
- Mechanism: The prompt template uses special tokens (`<think >` and `</think >`) to enforce a chain-of-output generation. The model is trained to first extract polarity-related keywords/phrases from the input text, and *then* generate the final label based on this extracted evidence. This forces the model to attend to relevant signals before committing to a classification, reducing shallow pattern matching.
- Core assumption: Assumption: The explicit extraction of keywords creates a more reliable internal representation for the final classification decision than direct label prediction, and that this generated reasoning is causally linked to the improved performance.
- Evidence anchors:
  - [abstract] "The fine-tuned model not only predicts labels accurately but also provides reasoning by identifying polarity-related keywords or phrases to support its predictions."
  - [section] "When the thinking mode was not enabled (i.e., baseline), the model performance significantly degraded." (Page 5, 5.2.1)
  - [corpus] Corpus evidence for this specific "thinking mode" mechanism in Khmer is weak; no directly comparable papers were found in the provided neighbors.
- Break condition: If the input text contains no clear sentiment keywords (e.g., highly contextual or sarcastic statements), the model's reasoning trace may be empty or nonsensical, failing to provide the conditioning needed for an accurate label.

### Mechanism 2
- Claim: Fine-tuning on a combination of formal (KP) and casual (CKP) data using distinct prompt templates allows the model to generalize across domains while learning the specific output structure for each.
- Mechanism: The model is trained on two distinct input-output mappings simultaneously. For the KP dataset, it learns the `[text] -> [reasoning] -> [label]` structure using the reasoning prompt (Figure 1). For the CKP dataset, it learns the `[text] -> [label]` structure using the non-reasoning prompt (Figure 2). This mixed training prevents the model from overfitting to a single style of Khmer text and forces it to learn a more robust underlying task representation.
- Core assumption: Assumption: The model can successfully learn both output structures without catastrophic interference, and that the latent reasoning skills from the KP dataset will transfer to or support performance on the CKP dataset.
- Evidence anchors:
  - [abstract] "...achieved by fine-tuning... using reasoning and non-reasoning prompt templates."
  - [section] "Thus, by leveraging the proposed reasoning and non-reasoning prompting templates together with the KP dataset (with reasoning) and the CKP dataset (without reasoning), we fine-tune an explainable Khmer polarity classifier..." (Page 5)
  - [corpus] Evidence is weak; no corpus papers discuss this exact mixed-template fine-tuning strategy.
- Break condition: If the non-reasoning (CKP) dataset significantly out-weighs the reasoning (KP) dataset, the model may unlearn or weaken its ability to generate coherent reasoning traces, degrading the explainability feature.

### Mechanism 3
- Claim: Parameter-Efficient Fine-Tuning (PEFT) with LoRA enables effective adaptation of a large multilingual model (Qwen-3) to a low-resource language task without full parameter updates.
- Mechanism: LoRA injects trainable low-rank decomposition matrices into the model's attention and feed-forward layers. During training, only these small adaptor weights are updated. This preserves the powerful pre-trained multilingual and reasoning capabilities of the base Qwen-3 model while efficiently specializing a small subset of parameters for Khmer sentiment classification.
- Core assumption: Assumption: The base Qwen-3 model already possesses sufficient latent Khmer language understanding that can be "unlocked" or redirected with minimal parameter updates.
- Evidence anchors:
  - [abstract] "...fine-tuned Qwen-3 models achieve 84% accuracy... outperforming existing classical machine learning methods."
  - [section] "Default Qwen-3 models (i.e., without fine-tuning) achieve an accuracy of 0%... We apply LoRA fine-tuning to reduce memory requirements." (Page 3 & 5)
  - [corpus] Related work on Fine-Tuned Agentic Reflection (FT-ARM, arXiv:2510.24980) supports the general mechanism of using fine-tuned LLMs with reasoning for classification tasks.
- Break condition: If the base model has poor fundamental support for the target language (as suggested by the 0% zero-shot accuracy), the low-rank updates may be insufficient to bridge the performance gap, resulting in limited accuracy gains.

## Foundational Learning

- **Concept: Instruction Tuning & Prompt Engineering**
  - Why needed here: The system's core functionality is controlled by prompt templates (Figures 1, 2, 3). An engineer must understand how these templates define the task, the output format, and the "thinking mode" behavior.
  - Quick check question: What is the functional difference between the "reasoning prompt template" used for training and the "inference prompt template"? (Hint: Check for the presence of the `reasoning` field in the input).

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: The proposed solution relies on LoRA for efficient training. An engineer needs to know what parameters (rank `r`, `alpha`, `modules`) to configure and why they matter.
  - Quick check question: Which layers of the Qwen-3 model were targeted by LoRA in this paper, and what was the configured rank (`r`)?

- **Concept: Self-Explanation vs. Post-Hoc Explainability**
  - Why needed here: The paper claims "explainability," but it's crucial to distinguish this from other forms. This model generates its own justification as part of the output, rather than using an external explainer.
  - Quick check question: According to the paper, what is the nature of the "explainability" provided? (Answer: It is a self-explanation generated by the model itself).

## Architecture Onboarding

- **Component map:**
  - **Base Model:** `Qwen-3` (variants: 1.7B, 4B, 8B).
  - **Adaptor:** `LoRA` (Low-Rank Adaptation). Injected into projection layers.
  - **Prompt Controller:** Logic to select between `Reasoning Prompt` (Figure 1) for KP data, `Non-Reasoning Prompt` (Figure 2) for CKP data, and `Inference Prompt` (Figure 3) for all test/production inputs.
  - **Data Sources:** `Khmer Polarity (KP)` dataset and `Casual Khmer Polarity (CKP)` dataset (available at `rinabuoy/khmerpolarity_nonreasoning`).

- **Critical path:**
  1. **Data Prep:** Format the KP dataset with reasoning prompts and the CKP dataset with non-reasoning prompts. Combine into a single training set.
  2. **Model Init:** Initialize Qwen-3 with 4-bit quantization. Configure LoRA (`r=32`, `alpha=32`, `target_modules="projection layers"`).
  3. **Training:** Run fine-tuning on the combined data for 2 epochs. Update *only* the LoRA parameters.
  4. **Inference:** For any input, use the inference prompt (Figure 3) to trigger the thinking mode. The model will output a reasoning trace followed by a label.

- **Design tradeoffs:**
  - **Accuracy vs. Explainability:** The mixed fine-tuning approach may slightly reduce peak accuracy on casual data (CKP) compared to a dedicated CNN model, but it provides the unique benefit of reasoning.
  - **Efficiency vs. Adaptation Power:** LoRA drastically reduces memory usage, but by freezing most parameters, the model may fail to learn complex new linguistic patterns not present in the base model's pre-training.
  - **Qualitative vs. Quantitative Evaluation:** The paper evaluates reasoning quality qualitatively. A quantitative metric for reasoning quality is a known gap.

- **Failure signatures:**
  - **Zero-shot Failure:** An un-fine-tuned Qwen-3 model will show 0% accuracy on Khmer text, indicating a complete lack of task-specific capability.
  - **Missing Reasoning Trace:** If the model outputs only a label without the `<think >...</think >` block during inference, it likely was not fine-tuned correctly or is using the wrong prompt template.
  - **Degraded Accuracy on Baseline:** If the model's accuracy is close to the "Baseline (w/o thinking)" scores in Table 5/6, it indicates the thinking mechanism is not active.

- **First 3 experiments:**
  1. **Baseline Reproduction:** Fine-tune `Qwen3-1.7B` with LoRA on the combined KP+CKP data using the templates from Figure 1 and 2. Measure accuracy on the KP and CKP test sets. Compare to Table 5 and 6 results.
  2. **Thinking Mode Ablation:** Fine-tune a separate model on the same data but using *only* the non-reasoning template (Figure 2) for all data. Compare its accuracy to the model from Experiment 1. The expected result is a drop in accuracy, confirming the "thinking mode" is the cause of improvement.
  3. **Reasoning Quality Assessment:** Run inference on 20-50 samples from the KP test set using the inference prompt (Figure 3). Manually check if the keywords generated in the `<think >` block are present in the input text and correctly identify the sentiment. This validates the explainability claim using the paper's own qualitative method.

## Open Questions the Paper Calls Out

None

## Limitations

- The paper's "explainability" relies entirely on self-generated reasoning traces, which may be post-hoc justifications rather than causally linked to the classification decision. No external verification method is provided.
- The mixed fine-tuning approach (reasoning + non-reasoning prompts) lacks ablation studies showing whether performance gains come from the reasoning mechanism itself or simply from additional training data.
- Dataset construction details for the CKP dataset are sparse; the paper mentions heuristic labeling followed by human correction but doesn't specify the correction criteria or inter-annotator agreement.

## Confidence

- **High confidence**: The paper demonstrates that fine-tuned Qwen-3 models achieve 84% accuracy on KP and 87% on CKP datasets, outperforming classical ML methods. The basic training pipeline (LoRA + instruction-tuned base model) is well-established.
- **Medium confidence**: The claim that thinking mode improves accuracy is supported by ablation showing degraded performance when disabled, but the causal mechanism linking reasoning extraction to better classification is not rigorously proven.
- **Low confidence**: The explainability benefit is qualitatively assessed but not quantitatively validated. The paper doesn't demonstrate that the generated reasoning traces are accurate, relevant, or useful for humans.

## Next Checks

- **Quantitative reasoning quality assessment**: Run the fine-tuned model on 100 test samples and manually evaluate whether the extracted keywords in the `<think >` block are (a) present in the input text and (b) correctly identify the sentiment. Calculate precision/recall of reasoning vs. ground truth.
- **Ablation on reasoning mechanism**: Retrain the model using only the non-reasoning prompt template for *all* data (both KP and CKP). Compare accuracy and reasoning generation capability to quantify the specific contribution of the thinking mode.
- **Cross-dataset robustness test**: Evaluate the KP-trained model on the CKP test set and vice versa. Analyze the generated reasoning traces for inputs from the dataset that lacked them during training to assess generalization of the reasoning skill.