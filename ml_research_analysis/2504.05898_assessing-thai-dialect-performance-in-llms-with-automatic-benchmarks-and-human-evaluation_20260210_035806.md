---
ver: rpa2
title: Assessing Thai Dialect Performance in LLMs with Automatic Benchmarks and Human
  Evaluation
arxiv_id: '2504.05898'
source_url: https://arxiv.org/abs/2504.05898
tags:
- local
- dialects
- thai
- llms
- dialect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study addresses the lack of evaluation benchmarks and metrics\
  \ for Thai local dialects (Isan, Lanna, Dambro) in large language models (LLMs).\
  \ It introduces a benchmark covering five NLP tasks\u2014summarization, question\
  \ answering, translation, conversation, and food-related tasks\u2014with data translated\
  \ into local dialects."
---

# Assessing Thai Dialect Performance in LLMs with Automatic Benchmarks and Human Evaluation

## Quick Facts
- arXiv ID: 2504.05898
- Source URL: https://arxiv.org/abs/2504.05898
- Authors: Peerat Limkonchotiwat; Kanruethai Masuk; Surapon Nonesung; Chalermpun Mai-On; Sarana Nutanong; Wuttikorn Ponwitayarat; Potsawee Manakul
- Reference count: 13
- Results show that while proprietary models like GPT-4o and Gemini2 exhibit some fluency, most LLMs perform significantly worse on local dialects than standard Thai, with traditional metrics like BLEU and ROUGE-L failing to capture dialect fluency.

## Executive Summary
This study addresses the lack of evaluation benchmarks and metrics for Thai local dialects (Isan, Lanna, Dambro) in large language models (LLMs). It introduces a benchmark covering five NLP tasks—summarization, question answering, translation, conversation, and food-related tasks—with data translated into local dialects. A human evaluation metric is proposed to assess dialect-specific accuracy and generation fluency, rewarding correct dialect usage and penalizing errors. Results show that while proprietary models like GPT-4o and Gemini2 exhibit some fluency, most LLMs perform significantly worse on local dialects than standard Thai, with traditional metrics like BLEU and ROUGE-L failing to capture dialect fluency. Only Gemini2 achieves high scores in the proposed metric, demonstrating the need for specialized evaluation methods for dialectal text.

## Method Summary
The study created a benchmark of 400 samples (100 per dialect) covering five NLP tasks, with native speakers translating Central Thai prompts into Isan, Lanna, and Dambro dialects. Models including Llama-3.1, Typhoon1.5, GPT-4o, and Gemini2 were evaluated using traditional metrics (BLEU for translation, ROUGE-L for QA/summarization) and a novel human evaluation metric that separately scores "Generation" (presence of dialect units) and "Fluency" (correctness). The proposed metric uses phonetic normalization to handle spelling variations and requires human annotators to assess dialect-specific accuracy rather than relying on automatic metrics alone.

## Key Results
- Proprietary models (GPT-4o, Gemini2) demonstrate some dialect fluency, while open-weight models (Llama, Typhoon) perform significantly worse
- Traditional metrics like BLEU and ROUGE-L fail to capture dialect fluency, often giving high scores to models that output Central Thai instead of the target dialect
- Only Gemini2 achieves high scores in the proposed human evaluation metric, with substantial inter-annotator agreement (Cohen's Kappa = 0.79)
- Code-switching behavior (mixing Central and local dialects) represents a distinct failure mode in current models

## Why This Works (Mechanism)

### Mechanism 1: Isolation of Dialect-Specific Units
Traditional metrics conflate shared vocabulary with dialect-specific markers. The proposed evaluation separates "Generation" (presence of dialect units) from "Fluency" (correctness), explicitly tagging unique dialect tokens and requiring independent scoring. This penalizes models defaulting to standard dialect.

### Mechanism 2: Phonetic Invariance Scoring
Evaluating dialect generation requires phonetic normalization rather than string matching to handle spelling variations. The guideline allows different spellings if they share the same pronunciation, preventing false negatives where the model captures the spoken dialect correctly but misspells it.

### Mechanism 3: Scale-Dependent Code-Switching
Larger models may form internal representations of linguistic variation that allow them to adhere to dialect prompt instructions. Smaller models may overfit to Central Thai distribution in their training data, ignoring style instructions.

## Foundational Learning

- **Lexical Overlap vs. Semantic Adequacy**: Engineers must understand why high ROUGE-L doesn't imply dialect competence. Quick check: If a model translates a dialect prompt into perfect Central Thai, should the evaluation score be high or low?

- **Inter-Annotator Agreement (Cohen's Kappa)**: The paper relies on a new human-evaluation metric; understanding the reliability (0.79 agreement) is crucial for trusting results. Quick check: Why is "substantial agreement" (0.7+ Kappa) necessary to validate a subjective metric like "fluency"?

- **Code-Switching in LLMs**: The paper identifies code-switching as a distinct failure mode. Quick check: Can the model distinguish between the prompt language (Dialect) and the response language (Central Thai), or does it mix them?

## Architecture Onboarding

- **Component map**: Native Translation (Central → Dialect) → Model Inference → Phonetic Normalization → Human/Metric Scoring
- **Critical path**: 1) Translate Central Thai benchmarks to dialects using native speakers, 2) Run inference with strict dialect prompts, 3) Apply dual-metric (Generation + Fluency) rather than standard BLEU/ROUGE
- **Design tradeoffs**: Human vs. LLM-as-Judge (human annotators required due to low correlation with automated methods); Data Volume (100 samples/dialect is small but sufficient)
- **Failure signatures**: False Positive (Central Thai output → High ROUGE-L → Low Generation Score); Hallucination (invents dialect words → Zero Fluency Score)
- **First 3 experiments**: 1) Baseline Verification: Run Typhoon1.5 on Dambro set (high ROUGE-L, near-zero Generation score), 2) Proprietary Comparison: Run Gemini2 on Lanna set (Generation score >90), 3) Metric Correlation: Calculate correlation between BLEU and Human Fluency scores

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed generation and fluency evaluation framework be effectively adapted for dialects in other languages? The current guidelines are tailored specifically to Thai dialect characteristics and spelling variations. What evidence would resolve it: Successful application to a different linguistic context demonstrating high inter-annotator agreement.

### Open Question 2
Can a specialized "LLM-as-a-Judge" or automated metric be developed to correlate highly with human fluency assessments for Thai local dialects? Appendix B notes that current proprietary models fail to judge dialect fluency accurately, resulting in low correlation (53.6 Spearman) with human evaluators. What evidence would resolve it: An automated evaluation pipeline achieving >0.8 Kappa with native human annotators.

### Open Question 3
To what extent do tokenizer limitations versus lack of pre-training data cause the failure of open-source models to generate local dialects? The paper identifies the performance gap but doesn't ablate whether the issue is primarily tokenization or fundamental lack of linguistic knowledge. What evidence would resolve it: An ablation study comparing standard vs. dialect-adapted tokenizers on the same model architecture.

## Limitations
- Human annotators required for dialect fluency scoring, limiting scalability
- Sample size (100 per dialect) is relatively small, constraining statistical power
- Proprietary model results depend on API access and may not be reproducible locally

## Confidence

**High Confidence Claims:**
- Traditional metrics (BLEU, ROUGE-L) fail to capture dialect fluency in Thai local dialects
- Open-weight models (Typhoon, Llama) perform significantly worse than proprietary models (Gemini2, GPT-4o) on dialect tasks
- The proposed human evaluation metric successfully identifies dialect generation capability where traditional metrics fail

**Medium Confidence Claims:**
- Scale and data diversity are primary factors determining dialect fluency capability
- Phonetic normalization is necessary for accurate dialect evaluation
- Code-switching behavior represents a distinct failure mode

**Low Confidence Claims:**
- Exact quantitative performance gaps between models
- Generalizability to other Thai dialects beyond the three studied
- Relative importance of scale vs. instruction tuning

## Next Checks
1. Calculate inter-annotator agreement (Cohen's Kappa) on a larger sample of human evaluations to confirm the reported substantial agreement (0.79) and assess metric reliability across different dialect pairs.

2. Test whether instruction tuning smaller models specifically on Thai dialect data can close the performance gap with proprietary models, isolating the effects of scale versus specialized training.

3. Apply the proposed evaluation framework to another low-resource dialect pair (e.g., German dialects or Indian language dialects) to validate whether the metric design generalizes beyond Thai.