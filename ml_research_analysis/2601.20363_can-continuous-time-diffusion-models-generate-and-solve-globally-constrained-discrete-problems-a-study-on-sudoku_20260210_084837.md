---
ver: rpa2
title: Can Continuous-Time Diffusion Models Generate and Solve Globally Constrained
  Discrete Problems? A Study on Sudoku
arxiv_id: '2601.20363'
source_url: https://arxiv.org/abs/2601.20363
tags:
- sampling
- score
- diffusion
- sudoku
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies whether continuous-time generative models\u2014\
  flow matching and score-based diffusion models\u2014can learn to represent and sample\
  \ from highly constrained discrete distributions, using completed Sudoku grids as\
  \ a testbed. The authors train models along a Gaussian probability path and compare\
  \ deterministic (ODE) and stochastic (SDE) sampling strategies, as well as DDPM-style\
  \ ancestral samplers derived from the same continuous-time training."
---

# Can Continuous-Time Diffusion Models Generate and Solve Globally Constrained Discrete Problems? A Study on Sudoku

## Quick Facts
- arXiv ID: 2601.20363
- Source URL: https://arxiv.org/abs/2601.20363
- Reference count: 31
- Continuous-time diffusion/flow models can generate and solve Sudoku, with DDPM ancestral sampling achieving >83% valid grids.

## Executive Summary
This paper investigates whether continuous-time generative models—specifically flow matching and score-based diffusion models—can learn to represent and sample from highly constrained discrete distributions using completed Sudoku grids as a testbed. The authors train models along Gaussian probability paths and compare deterministic (ODE) and stochastic (SDE) sampling strategies, as well as DDPM-style ancestral samplers derived from the same continuous-time training. The key finding is that stochastic sampling is essential for unconditional Sudoku generation, with DDPM ancestral samplers substantially outperforming continuous-time methods (83.6% vs ~26% validity). The same models can be repurposed for guided Sudoku solving via repeated stochastic sampling under clamped clues, though this approach is far less sample-efficient than classical solvers.

## Method Summary
The paper trains flow matching and score-based models on flattened 81-cell Sudoku grids (each cell encoded as 9-dimensional logits) using Gaussian probability paths with linear and cosine schedules. A lightweight transformer (4 layers, 128 dim, 8 heads) with Fourier time features and Sudoku-specific positional embeddings serves as the backbone. Training employs a rescaled score objective for numerical stability. For inference, the paper compares ODE, SDE, and DDPM/DDIM ancestral samplers. Guided solving is implemented through stochastic sampling with clamped clue values.

## Key Results
- DDPM ancestral sampling achieves 83.6% validity for unconditional Sudoku generation, far exceeding ODE (~0.00-0.35%) and SDE (~10-26%) sampling
- Stochastic sampling is essential for traversing the sparse manifold of valid Sudoku grids
- Guided solving via repeated stochastic sampling under clamped clues is feasible but far less sample-efficient than classical solvers
- Continuous-time training can assign non-zero probability mass to globally constrained combinatorial structures

## Why This Works (Mechanism)

### Mechanism 1: Stochastic Sampling for Sparse Manifolds
- **Claim:** Stochastic sampling is necessary to traverse the extremely sparse manifold of valid discrete solutions; deterministic transport alone is insufficient.
- **Mechanism:** In highly constrained discrete spaces like Sudoku, the data manifold is sparse. Deterministic ODE trajectories get trapped in invalid regions satisfying only local constraints. Noise injection via SDEs allows exploration and escape from these invalid configurations to find valid solutions.
- **Core assumption:** The learned vector field creates invalid local attractors escapable via perturbation.
- **Evidence anchors:** Abstract shows stochastic sampling substantially outperforms deterministic flows; Table 1 shows ODE yields near-zero success rates (~0.00-0.35%) while SDE variants achieve >10% and DDPM reaches >80%.

### Mechanism 2: Marginal Distribution Alignment
- **Claim:** Discrete-time ancestral sampling (DDPM/DDIM) derived from continuous training outperforms continuous-time simulation because it aligns inference with trained marginal distributions.
- **Mechanism:** Continuous solvers integrate an approximate drift field, allowing score estimation errors to accumulate and drift samples off the data manifold. Ancestral samplers jump directly between marginal distributions $q(x_t|x_0)$ used during training, correcting errors at each step by re-sampling from the known Gaussian structure.
- **Core assumption:** The model learns marginal score/velocity accurately even if continuous dynamics simulation is imperfect.
- **Evidence anchors:** Section 4.1 states DDPM/DDIM samplers operate directly on the same discrete marginal distributions ensuring consistency; Table 2 shows DDPM achieves 83.6% validity compared to ~26% for best continuous SDE.

### Mechanism 3: Stochastic Search for Guided Solving
- **Claim:** Guided solving functions as stochastic search rather than exact probabilistic inference.
- **Mechanism:** The model doesn't calculate $p(solution|clues)$ analytically but repeatedly samples trajectories while clamping known values. It relies on chance that stochastic trajectories satisfy all global constraints simultaneously.
- **Core assumption:** The learned distribution assigns non-zero probability mass to valid completions, allowing them to be found via repeated trials.
- **Evidence anchors:** Abstract notes diffusion/flow formulations can assign non-zero probability mass to globally constrained structures and be used for constraint satisfaction via stochastic search; Section 5 states guided solver doesn't perform symbolic reasoning but relies on stochastic exploration.

## Foundational Learning

- **Concept: Gaussian Probability Paths (Linear vs. Trigonometric)**
  - **Why needed here:** The choice of schedule ($\alpha_t, \beta_t$) determines whether the path is linear interpolation or variance-preserving diffusion process used for DDPM. Understanding this is required to switch between "Flow" and "Diffusion" modes.
  - **Quick check question:** Does the linear schedule ($\alpha_t=t, \beta_t=1-t$) satisfy the variance-preserving property $\alpha_t^2 + \beta_t^2 = 1$ required for standard DDPMs? (Answer: No).

- **Concept: Score Matching vs. Flow Matching**
  - **Why needed here:** The paper trains separate models for velocity field ($u_\theta$) and score ($s_\theta \approx \nabla \log p$). Understanding that for Gaussian paths these are mathematically related but behave differently regarding numerical stability (scores blow up as $t \to 1$) is crucial.
  - **Quick check question:** Why does the paper rescale the score target to $\beta_t^2 \nabla \log p_t(x)$ instead of learning $\nabla \log p_t(x)$ directly? (Answer: Numerical stability/divergence near $t=1$).

- **Concept: Ancestral vs. Markovian Sampling**
  - **Why needed here:** The paper shows dramatic performance gap between these two ways of discretizing diffusion. Ancestral uses learned network to predict clean image $\hat{x}_1$ and re-noises it, while Markovian estimates gradient step directly.
  - **Quick check question:** Which sampling method aligns directly with the marginal distributions $q(x_t|x_0)$ used in training? (Answer: Ancestral).

## Architecture Onboarding

- **Component map:** Flattened 81-cell sequence (9-dim logits each) -> Lightweight Transformer (4 layers, 128 dim, 8 heads) -> Velocity $u_\theta$ or Score $s_\theta$ (shape 81×9)

- **Critical path:** Training: Sample $t$ (biased toward $t=1$) -> Noise data -> Predict velocity/score -> MSE Loss. Inference: Noise -> Loop $t=0\to 1$ (SDE/DDPM) -> Argmax/Discretize.

- **Design tradeoffs:**
  - Use Linear schedule for guided solving (better exploration with $\beta(t)$ noise) and Cosine for unconditional generation (better density matching)
  - Always use Stochastic (SDE/DDPM) over Deterministic (ODE) for Sudoku
  - Keep dropout (0.01) enabled during inference for DDIM to prevent mode collapse

- **Failure signatures:**
  - Mode Collapse (DDIM): Generating exact same valid grid for every input noise (caused by disabling inference dropout)
  - Invalid Constraint Drift (ODE): Grids satisfy local digit features but fail global row/col/box rules (caused by lack of noise/exploration)
  - Score Instability: Training loss diverges near $t=1$ (caused by not rescaling score target)

- **First 3 experiments:**
  1. Train score model on linear path and verify ODE sampling yields ~0% validity while SDE sampling yields >10%
  2. Convert trained model to Ancestral DDPM sampler and confirm validity jumps from ~13% (SDE) to >80% (DDPM)
  3. Run DDIM sampling with dropout=0 vs dropout=0.01 and check if valid grid set drops to cardinality 1 with dropout=0

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why do diffusion dynamics that deviate from the probability path (specifically linear schedules with $\beta(t)$-scaled noise) outperform path-consistent dynamics in guided constraint satisfaction?
- **Basis:** Page 16 states "A deeper theoretical understanding of why diffusion dynamics that deviate from the probability path can outperform path-consistent dynamics under conditioning remains an important open direction."
- **Why unresolved:** The authors observe empirically that $\beta(t)$ sampler improves stability and success rates despite not following exact probability path SDE, but lack theoretical justification for this violation of standard diffusion assumptions.
- **What evidence would resolve it:** Theoretical analysis comparing effective drift/diffusion terms of heuristic sampler against learned score, or ablation studies showing if benefit derives from specific noise annealing schedule rather than path itself.

### Open Question 2
- **Question:** Do performance trade-offs between continuous-time (SDE) and discrete-time (DDPM) samplers generalize to other combinatorial domains beyond Sudoku?
- **Basis:** Page 16 notes "Experiments were limited to a single dataset and model architecture, and results may not generalize directly to other combinatorial domains."
- **Why unresolved:** The paper establishes DDPMs excel at unconditional generation while SDEs become competitive under guidance for Sudoku, but it's unverified if this is universal property of globally constrained discrete problems or artifact of Sudoku's specific constraint topology.
- **What evidence would resolve it:** Replicating comparison of ODE, SDE, and DDPM samplers on other constraint satisfaction problems (e.g., graph coloring or boolean satisfiability) using same continuous relaxation approach.

### Open Question 3
- **Question:** Can principled constraint injection methods be developed to improve sample efficiency of diffusion-based solvers to compete with classical search algorithms?
- **Basis:** Page 16 identifies "Future work could explore principled methods for constraint injection, adaptive noise control, and hybrid approaches."
- **Why unresolved:** Current guided solving method relies on heuristic "hard clamping" or simple soft injection, far less sample-efficient than classical solvers, suggesting constraint enforcement mechanism is suboptimal.
- **What evidence would resolve it:** Development of gradient-based or energy-based constraint injection mechanism that significantly reduces number of required sampling batches (currently ~3.5-8.9 batches) for hard puzzles.

## Limitations
- The study focuses exclusively on 9x9 Sudoku without establishing scalability to larger or more complex constraint satisfaction problems
- Absence of baseline comparisons with other generative models specifically designed for combinatorial structures (autoregressive, flow-based, or specialized CSP learning approaches)
- Guided solving approach lacks quantitative analysis of search efficiency compared to established CSP solvers beyond noting sample inefficiency

## Confidence
- **High Confidence:** Experimental results showing DDPM ancestral sampling outperforming ODE and SDE sampling for Sudoku generation (83.6% vs ~0.00-26%) are well-supported by data
- **Medium Confidence:** Explanation for why continuous-time solvers underperform (accumulation of score estimation errors) is plausible but not rigorously proven
- **Low Confidence:** Scalability claims and broader applicability to other CSP domains remain speculative without empirical validation beyond Sudoku

## Next Checks
1. **Architectural Ablation Study:** Compare proposed transformer-based diffusion model against standard autoregressive transformer trained directly on Sudoku completions, using identical datasets and evaluation protocols
2. **Constraint Violation Analysis:** Perform detailed breakdown of which constraints (row, column, box) are most frequently violated by ODE, SDE, and DDPM samplers to understand where each method fails
3. **Search Efficiency Quantification:** For guided solving, measure actual number of stochastic samples required to find solutions across different puzzle difficulties, and compare this to computational cost of classical CSP solvers like backtracking with constraint propagation