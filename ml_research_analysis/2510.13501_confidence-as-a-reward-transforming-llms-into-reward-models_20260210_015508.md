---
ver: rpa2
title: 'Confidence as a Reward: Transforming LLMs into Reward Models'
arxiv_id: '2510.13501'
source_url: https://arxiv.org/abs/2510.13501
tags:
- reward
- confidence
- arxiv
- training
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Confidence-as-a-Reward (CRew), a training-free
  approach that uses the mean token-level probability of a model's final answer as
  a reward signal for evaluating solutions, particularly for close-ended tasks. CRew
  is based on the observation that a model's confidence in its final answer correlates
  with its correctness.
---

# Confidence as a Reward: Transforming LLMs into Reward Models

## Quick Facts
- **arXiv ID:** 2510.13501
- **Source URL:** https://arxiv.org/abs/2510.13501
- **Reference count:** 21
- **Primary result:** Confidence-as-a-Reward (CRew) is a training-free approach that uses the mean token-level probability of a model's final answer as a reward signal, outperforming trained reward models on mathematical reasoning tasks.

## Executive Summary
This paper introduces Confidence-as-a-Reward (CRew), a novel training-free method that leverages a model's internal confidence (mean token-level probability of the final answer) as a reward signal for evaluating solutions. The approach is particularly effective for close-ended tasks where final answers can be extracted. The authors also propose CRew-DPO, a training method that constructs preference data from confidence scores and correctness signals to enhance the model's judging capabilities. Experimental results on mathematical reasoning benchmarks (MATH500 and RewardMATH) demonstrate that CRew outperforms other training-free reward methods and even surpasses most trained reward models. Additionally, CRew shows strong correlation with model reasoning performance and can effectively filter high-quality training data.

## Method Summary
The method introduces Confidence-as-a-Reward (CRew), which computes the mean token-level probability of the final answer tokens as a reward signal. For mathematical reasoning tasks, the final answer is extracted from tokens within `\boxed{}` delimiters. CRew-DPO extends this by sampling multiple solutions per question, pairing correct and incorrect solutions based on confidence differences, and training the model using DPO with these preference pairs. The approach requires questions with gold answers and uses a zero-shot prompt requiring step-by-step reasoning. The training procedure involves generating 30 solutions per question, selecting top K pairs with the largest confidence margin, and training with specified hyperparameters (β=0.3, LR=5e-7, batch size 128) for 2 epochs.

## Key Results
- CRew outperforms other training-free reward methods on MATH500 and RewardMATH benchmarks
- CRew-DPO improves judging capabilities beyond the base model
- Models trained on low-confidence correct solutions perform significantly better than those trained on high-confidence data
- CRew effectively filters high-quality training data and demonstrates strong correlation with model reasoning performance

## Why This Works (Mechanism)
The approach leverages the observation that a model's confidence in its final answer correlates with its correctness. By using the mean token-level probability as a reward signal, CRew captures the model's internal calibration about its solution quality. This correlation allows for effective preference learning in CRew-DPO, where pairs of solutions with large confidence differences between correct and incorrect answers provide strong training signals. The method assumes that when a model generates an incorrect answer with high confidence, it represents a systematic error pattern that can be learned and corrected through preference optimization.

## Foundational Learning
- **Token-level probability extraction:** Why needed - to compute confidence scores from the model's output distribution. Quick check - verify that log-probabilities are correctly captured for all tokens in the final answer.
- **Answer boundary detection:** Why needed - to identify which tokens constitute the final answer for confidence calculation. Quick check - test parsing logic on varied answer formats (fractions, decimals, LaTeX).
- **Preference pair construction:** Why needed - to create training data for DPO by pairing correct and incorrect solutions. Quick check - ensure top-K selection actually captures the largest confidence differences.
- **Confidence-calibration correlation:** Why needed - the core assumption that confidence correlates with correctness. Quick check - measure correlation between confidence scores and actual accuracy on held-out data.
- **DPO optimization:** Why needed - to fine-tune the model using preference pairs for improved judging. Quick check - monitor training loss and validate against development set.

## Architecture Onboarding

**Component map:** Input prompt → Solution generation → Answer extraction → Confidence calculation → (CRew-DPO) Preference pair filtering → DPO training → Improved judge model

**Critical path:** The critical path for CRew is: prompt → generate solution → extract answer tokens → compute mean probability → output confidence score. For CRew-DPO, it extends to: generate multiple solutions → validate against gold answers → create preference pairs → DPO training loop.

**Design tradeoffs:** The method trades computational overhead (generating 30 solutions per question) for training-free reward modeling capability. It assumes a predictable answer format (`\boxed{}`), limiting generalizability but enabling robust confidence extraction. The sampling-based approach may miss rare correct solutions but captures confidence variance effectively.

**Failure signatures:** The method fails when models produce answers without `\boxed{}` delimiters, when confidence doesn't correlate with correctness (miscalibration), or when sampling yields insufficient correct/incorrect pairs. It also degrades when the model hallucinates confident but wrong answers.

**First experiments:**
1. Implement CRew on a small MATH subset and verify correlation between confidence scores and gold answer correctness
2. Test answer extraction robustness across different LaTeX formatting variations
3. Run a single DPO update with synthetic preference pairs to validate the training pipeline

## Open Questions the Paper Calls Out
- Can CRew be effectively adapted for open-ended generative tasks where a distinct "final answer" token sequence is not extractable?
- Why does fine-tuning on low-confidence correct solutions yield better performance than high-confidence correct solutions, and does this generalize beyond mathematical reasoning?
- How robust is CRew against "hallucinations" where a model generates an incorrect answer with high token-level confidence?

## Limitations
- The method requires ground-truth answers for data construction, limiting applicability to domains without clear correctness criteria
- Computational overhead scales poorly with larger datasets due to sampling 30 solutions per question
- Assumes predictable answer formatting (e.g., `\boxed{}` delimiters), which may not generalize across all tasks
- Relies on the assumption that confidence correlates with correctness, which may break down for miscalibrated models

## Confidence
- **High Confidence:** CRew as a training-free reward signal performs comparably to trained reward models on MATH500 and RewardMATH benchmarks
- **Medium Confidence:** CRew-DPO improves judging capabilities beyond the base model and effectively filters high-quality training data
- **Low Confidence:** CRew-DPO performance generalizes to non-mathematical domains and scales effectively to larger model sizes

## Next Checks
1. Apply CRew to non-mathematical reasoning tasks (e.g., commonsense reasoning or code generation) to validate whether confidence scores maintain their correlation with correctness across different task types
2. Test CRew with alternative answer formatting strategies (plain text, JSON, multiple choice) to assess whether the method breaks down when final answers aren't contained in predictable delimiters
3. Conduct ablation studies on the number of sampled solutions per question ($N$) to determine the minimum effective sample size for CRew-DPO, quantifying the computational trade-offs at different dataset scales