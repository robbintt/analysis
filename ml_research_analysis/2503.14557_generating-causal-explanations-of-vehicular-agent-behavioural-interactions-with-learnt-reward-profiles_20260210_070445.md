---
ver: rpa2
title: Generating Causal Explanations of Vehicular Agent Behavioural Interactions
  with Learnt Reward Profiles
arxiv_id: '2503.14557'
source_url: https://arxiv.org/abs/2503.14557
tags:
- causal
- agent
- reward
- work
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of generating causal explanations
  for vehicular agent behavioral interactions, focusing on transparency and explainability
  in autonomous vehicles. The proposed method integrates one-shot inverse reinforcement
  learning with twin-world counterfactual inference to learn reward profiles that
  capture agent motivations at the time of decision-making.
---

# Generating Causal Explanations of Vehicular Agent Behavioural Interactions with Learnt Reward Profiles

## Quick Facts
- arXiv ID: 2503.14557
- Source URL: https://arxiv.org/abs/2503.14557
- Authors: Rhys Howard; Nick Hawes; Lars Kunze
- Reference count: 40
- Primary result: Learns reward profiles via one-shot IRL and applies twin-world counterfactual inference to generate causal explanations of vehicular agent interactions

## Executive Summary
This paper addresses the challenge of generating causal explanations for vehicular agent behavioral interactions by learning context-specific reward profiles and applying twin-world counterfactual inference. The approach integrates one-shot inverse reinforcement learning with a structural causal model to capture agent motivations at decision points and infer causal relationships between agent actions. Experiments on the highD dataset demonstrate significant improvements over previous reward-based methods, achieving precision of 0.774, recall of 0.835, and F1 score of 0.768 at threshold zero, while providing more expressive qualitative explanations of agent interactions.

## Method Summary
The method learns reward profiles using one-shot inverse reinforcement learning by simulating alternative actions through a structural causal model and solving a linear regression problem via Householder QR decomposition. Twin-world counterfactual inference is then applied by constructing observed and counterfactual world states, replanning optimal actions under each, and comparing action divergence via a threshold to infer causal links. The approach uses interpretable reward metrics (lane transitions, collisions, speed) weighted by the learned profiles to enable causal reasoning about agent interactions.

## Key Results
- Quantitative experiments on highD dataset show significant improvements over reward-based baselines with precision 0.774, recall 0.835, and F1 0.768 at threshold zero
- The method achieves higher precision than even the best-performing baseline at higher thresholds while maintaining competitive recall
- Qualitative results on multiple datasets demonstrate enhanced expressiveness of the approach in explaining agent interactions and motivations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning context-specific reward weights enables more accurate causal inference by capturing what the agent valued at the moment of decision.
- Mechanism: The system simulates multiple alternative actions using the SCM's generative capabilities, compares their simulated outcomes to the observed outcome via a distance function, and formulates reward inference as a linear regression task where the reward profile vector is solved via Householder QR decomposition.
- Core assumption: Agents act as intentional systems maximizing a weighted combination of interpretable reward metrics at each decision point.
- Evidence anchors:
  - [abstract]: "Our work aims to learn a weighting of reward metrics for agents such that explanations for agent interactions can be causally inferred."
  - [section]: Sec. IV describes the distance function (Eq. 8) and linear regression formulation (Eq. 9) for recovering the reward profile vector.
  - [corpus]: Corpus papers on multi-agent RL behavior prediction underscore the difficulty of inferring agent motivations from behavior alone; this method provides a structured approach but remains conditioned on reward metric design.
- Break condition: If reward metrics are insufficiently expressive to capture the agent's true motivations, proxy weights will distort counterfactual inference, leading to spurious causal conclusions.

### Mechanism 2
- Claim: Comparing planned actions under observed and counterfactual world states identifies whether one agent's action was causally necessary for another's decision.
- Mechanism: For each candidate cause-effect action pair, the system constructs the observed world and a counterfactual world where the causing action was not executed. Using the learned reward profile, it computes optimal actions under both worlds and declares a causal link if action divergence exceeds a threshold.
- Core assumption: The SCM adequately models vehicle dynamics and environment interactions; causality can be assessed via action divergence under intervention.
- Evidence anchors:
  - [abstract]: "We... apply contrastive twin-world counterfactual inference... to infer causal links."
  - [section]: Sec. V details the counterfactual world construction and distance-based action comparison threshold.
  - [corpus]: Related work on multi-agent causal explanation uses different attribution schemas; twin-world inference provides an explicit structural approach conditional on the SCM's fidelity.
- Break condition: If the simulation horizon or dynamics model poorly captures real-world physics, counterfactual trajectories diverge unrealistically, producing false positives/negatives.

### Mechanism 3
- Claim: A modular SCM representation enables generative simulation and principled intervention for causal reasoning.
- Mechanism: The SCM decomposes the vehicle system into modules (Point Mass, Rigid Body, Vehicle, Entity, Link, Controller, Planner), each with defined structural equations. The Planner module simulates candidate actions using downstream modules, enabling both reward learning and counterfactual evaluation.
- Core assumption: The system is Markovian with single-step time lags; all relevant physical interactions are captured by the Link and Entity modules.
- Evidence anchors:
  - [abstract]: Implicit—causal inference relies on the SCM's generative properties.
  - [section]: Sec. III-B and Fig. 2 describe the SCM architecture and its modules.
  - [corpus]: Weak direct corpus evidence for this specific SCM decomposition; related work uses different representational frameworks.
- Break condition: If unmodeled external factors (exogenous variables) significantly influence behavior, causal claims may be confounded.

## Foundational Learning

### Concept: Structural Causal Models (Pearl)
- Why needed here: The entire method rests on SCM-based representation, intervention, and counterfactual inference.
- Quick check question: Can you distinguish between observational, interventional, and counterfactual queries in a simple DAG?

### Concept: Inverse Reinforcement Learning basics
- Why needed here: Reward profile learning adapts IRL principles to single-timestep, interpretable weight recovery.
- Quick check question: Given a set of demonstrated actions and a feature vector, how would you formulate a linear reward recovery problem?

### Concept: Linear regression with regularization/QR decomposition
- Why needed here: The reward profile is solved via linear regression; understanding numerical stability and rank deficiency is critical.
- Quick check question: What does rank-revealing QR decomposition provide when the design matrix is ill-conditioned?

## Architecture Onboarding

- **Component map**: Parse trajectory data -> Extract time-action pairs -> Learn reward profile (SCM simulation + linear regression) -> Construct observed/counterfactual worlds -> Replan optimal actions -> Compare action divergence -> Output causal graph + reward profile interpretation
- **Critical path**: 1. Parse trajectory data → extract time-action pairs. 2. For each affected agent: learn reward profile (Sec. IV). 3. For each candidate action pair: build observed and counterfactual worlds, simulate, compare actions. 4. Threshold comparison → output causal graph + reward profile interpretation.
- **Design tradeoffs**:
  - **Reward metric expressiveness vs. interpretability**: Linear weights are interpretable but may not capture complex motivations.
  - **Simulation fidelity vs. computational cost**: More accurate dynamics improve counterfactual realism but increase rollout time.
  - **Threshold sensitivity**: Lower thresholds increase recall but risk false positives; authors recommend threshold=0 for practical use.
- **Failure signatures**:
  - Agents veering off-road or selecting nonsensical actions → undefined lane-branch behavior or missing lane-following reward metrics.
  - Low sensitivity (recall) despite high precision → reward metrics may not capture true motivations, leading to proxy weights.
  - High false-positive rate at low thresholds → dynamics model mismatch or insufficient simulation horizon.
- **First 3 experiments**:
  1. **Reproduce quantitative benchmark**: Run SimCARS-v2 on highD scenario extraction (115 scenes), compute precision/recall/F1 at threshold=0, compare against reported values (0.774/0.835/0.768).
  2. **Ablate reward metrics**: Remove one metric at a time (e.g., collision penalty) and observe impact on causal detection performance to validate expressiveness claims.
  3. **Stress-test dynamics model**: Introduce perturbations to vehicle parameters (mass, tire friction) and measure sensitivity of causal inference results to model misspecification.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can data-driven models replace linear weighted reward profiles without sacrificing the interpretability required for causal explanations?
- Basis in paper: [explicit] The discussion suggests considering "some form of data-driven model (e.g. a neural net)" to approximate motivations, but notes the challenge of maintaining the inherent interpretability of the current linear model.
- Why unresolved: The authors currently rely on linear regression specifically for its ease of human interpretation, and have not validated if more complex "black-box" models can offer better accuracy while remaining explainable.
- What evidence would resolve it: A comparative study showing that a non-linear model can learn reward profiles that yield higher causal inference accuracy while remaining transparent to human users.

### Open Question 2
- Question: How can the system ensure causal accuracy when the predefined set of reward metrics fails to capture the agent's true motivations?
- Basis in paper: [explicit] The authors identify the dependence on a suitable set of reward metrics as a limitation, noting that unexpressive metrics may be used as proxies, potentially leading to incorrect causal links.
- Why unresolved: The method relies on a fixed set of heuristic metrics (e.g., lane transitions, speed) which may not cover all real-world driving motivations, leading to sensitivity issues.
- What evidence would resolve it: Demonstrating a mechanism that dynamically identifies missing metrics or a learning framework that is robust to gaps in the predefined metric set.

### Open Question 3
- Question: Can integrating behavioural interaction causal modelling into a reinforcement learning loop improve efficiency and social awareness?
- Basis in paper: [explicit] The future work section proposes integrating the causal modelling into an RL loop to differentiate the approach from existing egocentric causal reasoning methods.
- Why unresolved: The current work focuses on post-hoc explanation generation; the computational and behavioural implications of using these explanations for online policy learning are unexplored.
- What evidence would resolve it: An implementation where the generated causal graphs successfully update an agent's policy in real-time, resulting in quantifiable gains in social compliance or navigation efficiency.

## Limitations
- The approach assumes reward metrics are sufficiently expressive to capture true agent motivations, but validation of metric completeness is not addressed
- The structural causal model's faithfulness to real-world dynamics is assumed rather than empirically verified, particularly for edge cases like off-road trajectories
- The method is computationally intensive due to multiple simulation rollouts for each candidate causal link

## Confidence

- **High confidence**: The linear regression formulation for reward profile learning (mechanistically sound, well-defined equations)
- **Medium confidence**: Causal inference via twin-world comparison (depends on SCM fidelity and threshold selection)
- **Medium confidence**: Quantitative improvements over baselines (limited by dataset size and potential evaluation metric sensitivity)

## Next Checks

1. **Metric expressiveness validation**: Systematically ablate individual reward metrics and measure impact on causal detection performance to identify which metrics are critical for accurate inference
2. **Dynamics model sensitivity analysis**: Introduce controlled perturbations to vehicle parameters and measure how changes propagate through causal inference results
3. **Threshold sensitivity characterization**: Systematically vary the action divergence threshold across scenarios to map the precision-recall tradeoff and identify optimal threshold ranges for different traffic densities