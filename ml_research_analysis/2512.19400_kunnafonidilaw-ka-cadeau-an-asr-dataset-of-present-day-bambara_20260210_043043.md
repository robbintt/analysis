---
ver: rpa2
title: 'Kunnafonidilaw ka Cadeau: an ASR dataset of present-day Bambara'
arxiv_id: '2512.19400'
source_url: https://arxiv.org/abs/2512.19400
tags:
- speech
- data
- bambara
- dataset
- kunkado
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Kunkado, a 160-hour Bambara automatic speech
  recognition (ASR) dataset compiled from Malian radio archives to capture present-day
  spontaneous speech, including code-switching, disfluencies, background noise, and
  overlapping speakers. The authors developed pragmatic transcript normalization to
  reduce variability in number formatting, tags, and code-switching annotations, and
  finetuned Parakeet-based models on a 33.47-hour human-reviewed subset.
---

# Kunnafonidilaw ka Cadeau: an ASR dataset of present-day Bambara

## Quick Facts
- arXiv ID: 2512.19400
- Source URL: https://arxiv.org/abs/2512.19400
- Authors: Yacouba Diarra; Panga Azazia Kamate; Nouhoum Souleymane Coulibaly; Michael Leventhal
- Reference count: 6
- 160-hour Bambara ASR dataset reduces WER from 44.47% to 37.12% on real-world test sets

## Executive Summary
Kunkado is a 160-hour Bambara automatic speech recognition (ASR) dataset compiled from Malian radio archives to capture present-day spontaneous speech with code-switching, disfluencies, background noise, and overlapping speakers. The authors developed pragmatic transcript normalization to reduce variability in number formatting, tags, and code-switching annotations, and finetuned Parakeet-based models on a 33.47-hour human-reviewed subset. Evaluation on two real-world test sets showed that finetuning with Kunkado reduced WER from 44.47% to 37.12% on one test set and from 36.07% to 32.33% on the other. Human evaluation indicated that the resulting model outperformed a comparable system trained on 98 hours of cleaner, less realistic speech, demonstrating the value of more authentic, challenging speech data for practical ASR deployment in predominantly oral languages.

## Method Summary
The authors created Kunkado by extracting audio from Malian radio archives and segmenting using pydub's split_on_silence with 600ms min_silence_len and -35dBFS threshold. They developed pragmatic transcript normalization through the bambara-normalizer package to handle variability in number formatting, tags, and code-switching markers. Parakeet-based models (FastConformer encoder + TDT + CTC decoders) were finetuned on a 33.47-hour human-reviewed subset using NeMo toolkit with 13k steps, batch size 64, AdamW optimizer, Noam scheduler with 3k warmup steps, and bf16 precision. Evaluation was conducted on two test sets: a 5-hour Kunkado test set and Nyana-Eval (3 min, 45 entries).

## Key Results
- Finetuning with Kunkado reduced WER from 44.47% to 37.12% on the Kunkado test set
- Finetuning with Kunkado reduced WER from 36.07% to 32.33% on Nyana-Eval test set
- Human evaluation showed the finetuned model outperformed a comparable system trained on 98 hours of cleaner speech
- Pragmatic normalization improved model performance on limited training data (under 40 hours)

## Why This Works (Mechanism)
The success stems from training on authentic, challenging speech data that better reflects real-world Bambara usage patterns, including code-switching and disfluencies, combined with normalization that reduces annotation variability without losing essential linguistic features.

## Foundational Learning
- **Bambara language characteristics**: Understanding that Bambara is predominantly oral with frequent code-switching between Bambara and French is crucial for designing appropriate ASR systems.
- **Pragmatic transcript normalization**: Needed to handle variability in human annotations; quick check: verify normalization pipeline removes diacritics, punctuation, and code-switching markers consistently.
- **Parakeet ASR architecture**: Understanding the FastConformer encoder and TDT/CTC decoder combination is essential for proper finetuning.
- **Code-switching in ASR**: Recognizing that language alternation is natural in Bambara speech and should be handled rather than penalized.
- **Radio archive processing**: Understanding audio segmentation from broadcast sources with background noise and overlapping speakers.
- **NeMo toolkit usage**: Familiarity with NVIDIA's NeMo framework for ASR model training and finetuning.

## Architecture Onboarding
**Component Map**: Radio archives -> pydub segmentation -> human review -> bambara-normalizer -> Parakeet model finetuning -> evaluation
**Critical Path**: Raw audio → segmentation → normalization → finetuning → evaluation metrics
**Design Tradeoffs**: Limited training data (33.47h) vs. annotation richness; normalization complexity vs. model learnability; radio-derived data quality vs. authenticity
**Failure Signatures**: High WER (>70%) indicates normalization issues; poor code-switching handling suggests marker removal problems; low improvement indicates insufficient training data or improper base model selection
**First Experiments**:
1. Verify normalization pipeline produces consistent output across all transcripts
2. Test model performance on small validation set before full finetuning
3. Compare WER on cleaned vs. raw transcripts to measure normalization impact

## Open Questions the Paper Calls Out
**Open Question 1**: How should code-switching-aware ASR evaluation be designed to accurately measure model performance on spontaneous speech in predominantly oral languages?
- Basis in paper: The authors call for "future work on code-switching-aware evaluation" in their conclusion
- Why unresolved: Standard ASR evaluation metrics treat code-switching as errors or ignore it, but code-switching is a natural feature of Bambara speech
- What evidence would resolve it: Development and validation of evaluation protocols with dedicated metrics for code-switched segments, tested across multiple POL language ASR systems

**Open Question 2**: What is the optimal balance between transcript annotation richness and model learnability when training data is limited (under 40 hours)?
- Basis in paper: The authors applied pragmatic normalization after finding that "with only just over 30 hours of training data, the models were struggling with the human-annotation variability"
- Why unresolved: Rich annotations capture linguistic authenticity but increase modeling complexity; the threshold for when each approach becomes preferable remains unknown
- What evidence would resolve it: Systematic experiments varying annotation granularity across different training data volumes, measuring both WER and linguistic feature preservation

**Open Question 3**: How well does WER reduction on radio-derived test sets predict human-perceived ASR quality improvement for real-world Bambara applications?
- Basis in paper: The paper notes soloni-v3 was excluded from human evaluation despite achieving "substantial WER improvement" (32.33% vs 36.07%)
- Why unresolved: The correlation between automatic metrics and human judgments remains unclear, especially for spontaneous speech with disfluencies and code-switching
- What evidence would resolve it: Human evaluation of soloni-v3 using the same Nyana-Eval protocol to directly compare against WER-based predictions

## Limitations
- Exact train/dev/test splits within the 33.47-hour reviewed subset are not specified
- Base learning rate value for Noam scheduler is not explicitly provided
- bambara-normalizer package location and API are not directly linked
- Limited human evaluation only compares one finetuned model against cleaner speech baseline

## Confidence
- WER improvements: Medium (lack of explicit cross-validation and base learning rate specification)
- Practical value demonstration: High (human evaluation shows real-world benefits)
- Methodology reproducibility: Medium (requires access to specific packages and base models)

## Next Checks
1. Verify exact train/dev/test splits and base learning rate from released model configs
2. Confirm bambara-normalizer package availability and API consistency
3. Replicate WER improvements on both test sets using provided normalization pipeline