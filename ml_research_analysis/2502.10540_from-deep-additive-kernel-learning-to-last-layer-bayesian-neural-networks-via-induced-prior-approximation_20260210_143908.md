---
ver: rpa2
title: From Deep Additive Kernel Learning to Last-Layer Bayesian Neural Networks via
  Induced Prior Approximation
arxiv_id: '2502.10540'
source_url: https://arxiv.org/abs/2502.10540
tags:
- kernel
- additive
- training
- learning
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Deep Additive Kernel (DAK), which reinterprets
  deep kernel learning as a last-layer Bayesian neural network (BNN) via induced prior
  approximation. The key innovation is incorporating an additive structure for the
  last-layer Gaussian process and applying induced prior approximation for each GP
  unit, resulting in a model that enjoys both the interpretability of DKL and the
  computational advantages of BNN.
---

# From Deep Additive Kernel Learning to Last-Layer Bayesian Neural Networks via Induced Prior Approximation

## Quick Facts
- arXiv ID: 2502.10540
- Source URL: https://arxiv.org/abs/2502.10540
- Reference count: 40
- Key outcome: Proposes DAK, a scalable deep kernel learning method using additive GP structure and sparse Cholesky decomposition, achieving closed-form ELBO for regression and outperforming state-of-the-art DKL methods

## Executive Summary
This paper introduces Deep Additive Kernel (DAK), a method that reinterprets deep kernel learning as a last-layer Bayesian neural network through induced prior approximation. The key innovation is incorporating an additive structure for the last-layer Gaussian process and applying induced prior approximation for each GP unit, resulting in a model that enjoys both the interpretability of DKL and the computational advantages of BNN. The method uses sparse Cholesky decomposition on induced grids, achieving linear complexity in the size of induced grids. Experiments show DAK outperforms state-of-the-art DKL methods on both regression and classification tasks, with better RMSE, NLPD, accuracy, and ELBO metrics. Notably, DAK provides closed-form solutions for both predictive distribution and ELBO in regression tasks, eliminating the need for Monte Carlo sampling.

## Method Summary
DAK decomposes a complex Gaussian Process into a sum of one-dimensional GPs using an additive structure, bypassing the cubic complexity of standard DKL. The model applies induced prior approximation on fixed dyadic grids rather than learning inducing point locations, and crucially uses the Laplace kernel which possesses the Markov property. This allows the Cholesky decomposition of the kernel matrix to be sparse, reducing inversion costs from cubic to linear O(M). The architecture consists of a DNN feature extractor, linear embedding layer, kernel activation layer using sparse Cholesky factors, and a Bayesian last layer with Gaussian weights. For regression tasks, DAK provides closed-form solutions for both predictive distribution and ELBO, eliminating Monte Carlo sampling.

## Key Results
- DAK achieves RMSE ≈ 0.35 on Gas dataset and comparable or better performance than SV-DKL on UCI regression tasks
- On CIFAR-10, DAK achieves Top-1 Accuracy ≈ 94.8% using ResNet-18 backbone
- The method demonstrates linear scaling O(M) in training time as grid size increases, confirmed by ablation studies
- DAK provides closed-form ELBO computation for regression, eliminating Monte Carlo sampling overhead

## Why This Works (Mechanism)

### Mechanism 1: Computational Scalability via Additive Structure
The method achieves scalability by decomposing a complex GP into a sum of one-dimensional GPs rather than approximating a single high-dimensional GP. This bypasses the cubic complexity O(N³) associated with inverting large covariance matrices in standard DKL. The core assumption is that complex relationships can be modeled as a sum of independent non-linear contributions from each feature dimension.

### Mechanism 2: Linear Complexity via Sparse Induced Prior
DAK transforms the GP layer into a BNN with a fixed, sparse architecture by using induced prior approximation on fixed dyadic grids with the Laplace kernel. The Markov property of the Laplace kernel enables sparse Cholesky decomposition, reducing inversion costs from cubic to linear O(M). This eliminates the O(M³) cost of learning inducing points.

### Mechanism 3: Sampling-Free Training via Analytical ELBO
For regression tasks, the additive structure and mean-field variational assumptions result in analytically Gaussian predictive distributions. This allows closed-form computation of the ELBO, removing the need for Monte Carlo sampling. The expectation in the ELBO can be solved explicitly for Gaussian likelihoods.

## Foundational Learning

- **Concept: Deep Kernel Learning (DKL)** - Why needed: DAK is a solution to scalability problems inherent in vanilla DKL, which maps data to a feature space via DNN and applies a GP with poor scaling. Quick check: How does DKL handle the trade-off between NN representational power and GP uncertainty quantification?
- **Concept: Variational Inference (VI) & ELBO** - Why needed: The model is trained by maximizing a variational lower bound (ELBO). Understanding the balance between fit (log-likelihood) and complexity penalty (KL divergence) is essential. Quick check: In the ELBO formula, what does the KL divergence term penalize?
- **Concept: Markov Property in Kernels** - Why needed: This is the mathematical trick enabling the speedup. The paper exploits the Markov property of the Laplace kernel to create sparse covariance structure. Quick check: Why does the Markov property allow for sparse Cholesky decomposition of the covariance matrix?

## Architecture Onboarding

- **Component map:** Backbone DNN -> Linear Embedding -> Kernel Activation Layer (sparse Cholesky) -> Bayesian Last Layer
- **Critical path:** The "Linear Embedding" must normalize inputs to the range of the fixed grids (e.g., [0,1]). If this normalization fails, the Induced Prior Approximation on the grids will be invalid.
- **Design tradeoffs:** Fixed dyadic grids provide O(M) speed but less flexibility than learned inducing points (O(M³)). The additive assumption may struggle if DNN features don't disentangle necessary interactions.
- **Failure signatures:** Using non-Laplace kernels without Markov properties will lose sparse Cholesky benefits. Grid size too small causes poor induced prior approximation.
- **First 3 experiments:** 1) Replicate sine-wave 1D regression to verify uncertainty increases outside training domain. 2) Run UCI regression with varying grid size M to observe accuracy-time trade-off. 3) Profile training time comparing DAK-CF (closed-form) vs DAK-MC (Monte Carlo) on regression.

## Open Questions the Paper Calls Out

### Open Question 1
Can computational efficiency (O(M) complexity) be extended to non-Laplace kernels like RBF? The current linear complexity relies on the Markov property of the Laplace kernel, which is lost with general kernels. Evidence would be sparse Cholesky derivation for non-Markov kernels or new approximation techniques maintaining linear scaling.

### Open Question 2
Does using a more expressive variational family (full/low-rank covariance) instead of mean-field assumption significantly improve predictive performance? The authors suggest this may lead to superior performance but limit to mean-field for efficient training. Evidence would be comparative study with full-covariance VI on high posterior complexity tasks.

### Open Question 3
How does restriction to first-order additive interactions affect performance on tasks requiring high-order feature correlations? The proposed method restricts to first-order interactions for efficiency, but evaluation on synthetic datasets requiring high-order interactions would reveal if additive bias causes performance degradation.

## Limitations
- Computational efficiency strictly applies only to Laplace kernel, not general kernels like RBF or Matern
- Closed-form ELBO is limited to Gaussian likelihoods, making DAK-MC necessary for classification with no computational advantage
- Experimental validation focuses on standard benchmarks, leaving questions about robustness on noisy or highly non-additive data

## Confidence
- **High:** Computational complexity claims (linear in grid size M) and regression performance metrics (RMSE/NLPD on UCI datasets)
- **Medium:** Classification accuracy improvements over DKL methods, additive structure effectiveness
- **Low:** Generalization to non-Laplace kernels and non-Gaussian likelihoods, performance on highly non-additive functions

## Next Checks
1. Verify sparse Cholesky decomposition maintains O(M) complexity when replacing Laplace kernel with standard RBF kernel on a small 1D regression problem
2. Test DAK performance on a synthetic dataset with known strong non-additive feature interactions to measure the additive approximation error
3. Profile training time comparison between DAK-CF (closed-form) and DAK-MC (Monte Carlo) on a regression task to confirm the sampling-free advantage