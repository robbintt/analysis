---
ver: rpa2
title: 'AMAQ: Adaptive Mixed-bit Activation Quantization for Collaborative Parameter
  Efficient Fine-tuning'
arxiv_id: '2510.05468'
source_url: https://arxiv.org/abs/2510.05468
tags:
- amaq
- quantization
- activation
- training
- bits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AMAQ, an adaptive mixed-bit activation quantization
  method for collaborative parameter-efficient fine-tuning. AMAQ dynamically allocates
  bit budgets across channels based on feature and layer importance using bit regularization,
  progressively compressing activations from high precision (6-8 bits) to low precision
  (3-4 bits).
---

# AMAQ: Adaptive Mixed-bit Activation Quantization for Collaborative Parameter Efficient Fine-tuning

## Quick Facts
- **arXiv ID:** 2510.05468
- **Source URL:** https://arxiv.org/abs/2510.05468
- **Reference count:** 11
- **Primary result:** Adaptive mixed-bit activation quantization improves generation accuracy by ~2.5% and classification accuracy by ~1.3% over fixed-precision approaches under same bit budgets

## Executive Summary
This paper introduces AMAQ, an adaptive mixed-bit activation quantization method designed for collaborative parameter-efficient fine-tuning in split learning setups. AMAQ dynamically allocates bit budgets across activation channels based on feature and layer importance using bit regularization, progressively compressing activations from high precision (6-8 bits) to low precision (3-4 bits). The method integrates with LoRA fine-tuning and achieves superior inference accuracy while significantly reducing communication overhead in multi-machine training scenarios.

## Method Summary
AMAQ uses learnable gating parameters Q to control per-channel bit-widths via sigmoid gating: `Bit-width = min + (max - min) × σ(α·Q)`. Training begins at 6-8 bits and progressively decreases to 3-4 bits over 500-1200 steps using L2 bit regularization. The method applies to input/output activations in split learning setups where the model is partitioned between client and server, with LoRA adapters jointly optimized with Q parameters. Communication overhead is reduced by transmitting compressed activations rather than full-precision tensors.

## Key Results
- Achieves ~2.5% higher generation accuracy and ~1.3% better classification accuracy than fixed-precision approaches under same bit budgets
- Reduces communication overhead significantly (7.4MB vs 226.5MB per batch for LoRA-only setup)
- Enhances training stability and prevents ultra-low bit representation collapse during progressive quantization
- Demonstrates effective integration into practical multi-machine collaborative training setups

## Why This Works (Mechanism)

### Mechanism 1
Adaptive per-channel bit allocation improves accuracy under fixed average bit budgets compared to uniform quantization. A learnable gating parameter vector Q controls effective bit-width for each activation channel via sigmoid gating. Channels with higher learned Q values retain more precision while less important channels compress more aggressively. Core assumption: feature importance is learnable via gradient descent and correlates with task-relevant information content.

### Mechanism 2
Progressive quantization from high to low precision stabilizes training and prevents representation collapse at ultra-low bits. Training begins at 6-8 bits and gradually descends to 3-4 bits over 500-1200 steps using L2 bit regularization added to main loss. Core assumption: models can adapt to quantization noise incrementally better than abruptly.

### Mechanism 3
Split learning with activation quantization reduces client memory and communication overhead while maintaining LoRA fine-tuning quality. The model is partitioned across client and server with activations and gradients quantized before transmission. LoRA adapters remain trainable locally with modest memory overhead. Core assumption: network bandwidth is the bottleneck and small quantization metadata overhead is acceptable.

## Foundational Learning

- **Quantization-Aware Training (QAT)**: Understanding STE and fake-quantization is prerequisite for AMAQ's quantization approach. *Quick check:* Can you explain how the straight-through estimator (STE) enables gradient flow through the non-differentiable rounding operation?

- **Split Learning Architecture**: Familiarity with client-server model partitioning and activation exchange is essential. *Quick check:* In a split learning setup, which components run on the client vs. server, and what data crosses the network boundary?

- **LoRA (Low-Rank Adaptation)**: Knowledge of how LoRA injects trainable rank-decomposed matrices is crucial for understanding the fine-tuning approach. *Quick check:* How does LoRA reduce trainable parameters while keeping the base model frozen, and where are LoRA matrices typically inserted in transformers?

## Architecture Onboarding

- **Component map:** AMAQQuantizer -> BitRegularizer -> SplitLearningClient -> SplitLearningServer -> LoRAManager
- **Critical path:**
  1. Initialize Q with high bit-width (6-8 bits); set α inverse to learning rate
  2. Forward pass: quantize activations using current bit-widths; apply STE for gradient approximation
  3. Compute loss = QAT_loss + β × Bits_Loss; backpropagate
  4. Update Q via separate optimizer group with controlled learning rate
  5. Monitor mean bit-width; apply clipping once target (3-4 bits) is reached
  6. Transmit compressed activations/gradients in split learning; decompress on receive
- **Design tradeoffs:**
  - L1 vs L2 regularization: L1 encourages sparsity (better compression); L2 is more stable
  - Per-channel vs per-token quantization: Per-channel (1×H) is simpler; per-token (1×seqLen) captures sequence-level importance
  - Input/output-only vs all-layer quantization: Former is more stable; latter requires higher β and careful tuning
- **Failure signatures:**
  - Training loss spikes early: β too high or initial bit-width too low
  - Bit-width stuck above target: β too low or Q learning rate too small
  - Representation collapse at 3 bits: model-specific sensitivity; consider starting from 4 bits
  - Communication overhead higher than expected: check if scale factors and bit allocation metadata are being over-transmitted
- **First 3 experiments:**
  1. Single-layer LoRA sanity check: Apply AMAQ to input/output activations only on LLaMA3-8B with GSM8K; verify perplexity improves over AQ-SGD baseline at 4-bit target
  2. β sweep: Run β ∈ {0.01, 0.02, 0.03} on CodeAlpaca; plot steps-to-target-bits vs final loss to find stability frontier
  3. Two-machine split learning deployment: Replicate Table 4 setup with A6000 + RTX 3090 over Wi-Fi; measure actual transmission size and training time overhead vs BF16 baseline

## Open Questions the Paper Calls Out

### Open Question 1
**How does AMAQ perform in a multi-party collaborative training framework involving more than two participants?**
The current evaluation is restricted to a two-machine split learning setup (client and server), leaving the behavior in decentralized or federated settings with N>2 parties unknown. Future work will explore multi-party collaborative training framework.

### Open Question 2
**Can an automated strategy be developed to dynamically adjust the bit regularization coefficient (β) and gating learning rate?**
The current method relies on a heuristic of starting with high values and decreasing them; a theoretical or automated framework for this tuning is missing. Future work will develop hyperparameter sensitivity analysis strategies.

### Open Question 3
**Does applying AMAQ to the Key-Value (KV) cache during inference yield similar stability and accuracy benefits as observed in activation compression?**
Since past key values in prefix tuning resemble the KV cache, AMAQ suggests a promising direction for future research: adapting AMAQ for efficient and accurate KV cache quantization.

## Limitations

- **Missing implementation details**: The quantization scheme specifics (symmetric vs asymmetric, per-channel scale computation, zero-point handling) are not provided
- **Vague optimizer settings**: The "separate group" optimizer implementation for Q parameters lacks specific configuration details
- **Incomplete clipping function**: The clipping function implementation details are unclear regarding qmin/qmax determination methodology

## Confidence

- **High confidence**: Adaptive per-channel bit allocation mechanism is well-specified and theoretically sound
- **Medium confidence**: Progressive quantization training procedure is described but clipping implementation introduces uncertainty
- **Medium confidence**: Split learning integration claim is supported by transmission size comparisons but depends on network conditions

## Next Checks

1. **Single-layer LoRA sanity check**: Implement AMAQ for input/output activations only on LLaMA3-8B with GSM8K dataset. Verify that perplexity improves over AQ-SGD baseline at 4-bit target with rank-64 LoRA adapters, batch size 16, learning rate 1e-4, and β=0.02 over 5000 steps.

2. **β parameter sweep analysis**: Run experiments with β ∈ {0.01, 0.02, 0.03} on CodeAlpaca classification task. Plot steps-to-target-bits vs final loss to empirically determine the stability frontier and identify optimal β values.

3. **Two-machine split learning deployment validation**: Replicate the transmission size comparison from Table 4 using A6000 + RTX 3090 over Wi-Fi. Measure actual communication overhead (transmission size, training time) compared to BF16 baseline to verify the claimed reduction and validate the "modest communication overhead" claim.