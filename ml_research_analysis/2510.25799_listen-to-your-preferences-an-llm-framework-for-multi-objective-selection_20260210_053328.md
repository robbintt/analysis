---
ver: rpa2
title: 'LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection'
arxiv_id: '2510.25799'
source_url: https://arxiv.org/abs/2510.25799
tags:
- listen-u
- listen-t
- figure
- utility
- average
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LISTEN, a framework for using large language
  models (LLMs) to assist humans in selecting the best option from a large set of
  items with multiple competing objectives. The framework operates by having a human
  expert describe their preferences in natural language, which the LLM uses as a zero-shot
  preference oracle.
---

# LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection

## Quick Facts
- arXiv ID: 2510.25799
- Source URL: https://arxiv.org/abs/2510.25799
- Reference count: 40
- Key outcome: LISTEN uses LLMs as zero-shot preference oracles for multi-objective selection, with concordance predicting parametric method success

## Executive Summary
This paper introduces LISTEN, a framework that leverages large language models to help humans select optimal items from large multi-objective datasets. The system takes natural language preference descriptions and uses an LLM as a zero-shot preference oracle to guide selection. Two algorithms are proposed: LISTEN-U, which iteratively refines a parametric utility function, and LISTEN-T, which uses tournament-style selection. The framework is evaluated on flight booking, headphone shopping, and exam scheduling tasks, demonstrating that LLMs can effectively translate natural language goals into high-quality selections while maintaining the original mixed-attribute formats.

## Method Summary
LISTEN operates by having a human expert describe their preferences in natural language, which an LLM uses as a zero-shot preference oracle. The framework includes two iterative algorithms: LISTEN-U refines a parametric utility function based on LLM assessments, while LISTEN-T performs tournament-style selections over small batches of solutions. The approach is evaluated on three realistic tasks with mixed numerical and categorical attributes, using a concordance metric to predict when parametric approaches will succeed.

## Key Results
- LISTEN-U significantly outperforms other methods when human preferences align with linear utility models (high concordance)
- LISTEN-T provides robust performance across all tested problems, regardless of concordance
- Concordance metric serves as a useful predictor of parametric approach success
- Iterative refinement can overcome poor initial linear approximations in some low-concordance tasks

## Why This Works (Mechanism)

### Mechanism 1
An LLM can function as a zero-shot preference oracle when given structured context and natural language priorities. The prompt encodes persona context, metric definitions, user priorities, candidate solutions, and format instructions, enabling the LLM to simulate the decision-maker's preferences without fine-tuning. This works because the LLM's pre-trained knowledge includes sufficient domain understanding to make preference-aligned judgments across numerical and non-numerical attributes.

### Mechanism 2
Iterative utility refinement (LISTEN-U) can converge toward high-quality selections even when initial preferences are poorly approximated by linear weights. The LLM critiques the current top-scoring solution using unnormalized attribute values and proposes adjusted weights, allowing local hill-climbing toward better solutions even with non-linear true preferences.

### Mechanism 3
Concordance—the fraction of random linear utilities that select the human-preferred item—predicts when parametric methods will succeed. Low concordance indicates preferences are non-linear or involve complex interactions; high concordance suggests simpler structure amenable to parametric modeling.

## Foundational Learning

- **Pareto frontier and multi-objective trade-offs**: Essential because the selection problem operates over Pareto-optimal solution sets where improving one objective requires sacrificing another. Quick check: If you have two solutions where A has lower price but longer duration than B, can you say which is better without knowing user preferences?

- **Utility functions and preference elicitation**: Critical because LISTEN-U explicitly constructs a linear utility function u(s) = w^T s, and the framework's core goal is to discover weights that reflect implicit human preferences. Quick check: If a user says "price matters twice as much as battery life," what weight ratio would you assign?

- **Zero-shot learning with LLMs**: Important because the framework relies on LLMs performing preference reasoning without task-specific training. Understanding zero-shot capabilities and limitations informs prompt design and failure mode analysis. Quick check: Why might an LLM fail to correctly rank exam schedules even if it understands the metric definitions?

## Architecture Onboarding

- **Component map**: Preference Utterance (U) -> Prompt Constructor -> LLM Oracle -> Scoring Engine (LISTEN-U) or Tournament Manager (LISTEN-T) -> Solution Set (S) -> Selected item s*

- **Critical path**: 
  1. Receive natural language utterance U from user
  2. For LISTEN-U: Initialize weights via LLM → score all items → select top → iterate with critique prompts
  3. For LISTEN-T: Sample batches → LLM selects champions per batch → final playoff among champions
  4. Return selected item s*

- **Design tradeoffs**:
  - LISTEN-U vs. LISTEN-T: LISTEN-U can achieve better performance when concordance is high and iterative refinement converges, but fails on strongly non-linear preferences. LISTEN-T is more robust but requires more LLM calls.
  - Iteration budget: More iterations improve LISTEN-U but with diminishing returns; convergence typically within 10-15 iterations.
  - Batch size (LISTEN-T): Larger batches provide more context per call but reduce coverage of the solution space per iteration.

- **Failure signatures**:
  - LISTEN-U plateau: Selected item stops improving; weights oscillate or converge to suboptimal values. Often indicates low concordance.
  - LISTEN-T randomness: High variance across runs due to batch sampling. Reduce by increasing batch count or using stratified sampling.
  - LLM format errors: LLM returns unparseable output. Mitigate with explicit format instructions and fallback parsing.

- **First 3 experiments**:
  1. Compute concordance using 1,000 random weight vectors on your dataset.