---
ver: rpa2
title: Unifying Model and Layer Fusion for Speech Foundation Models
arxiv_id: '2511.08389'
source_url: https://arxiv.org/abs/2511.08389
tags:
- fusion
- speech
- hconv
- tasks
- upstream
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified framework for model and layer fusion
  of speech foundation models, addressing the limitation that previous work treated
  these two fusion strategies separately. The authors introduce an interface module
  that enables joint fusion across multiple upstream speech models while integrating
  information across their layers.
---

# Unifying Model and Layer Fusion for Speech Foundation Models

## Quick Facts
- arXiv ID: 2511.08389
- Source URL: https://arxiv.org/abs/2511.08389
- Reference count: 34
- Primary result: Unified model+layer fusion outperforms separate fusion strategies across ASR and paralinguistic tasks

## Executive Summary
This paper addresses a fundamental limitation in speech foundation model fusion by proposing a unified framework that jointly fuses across both model and layer dimensions. Previous work treated model fusion and layer fusion as separate problems, missing potential synergies. The authors introduce an interface module that enables joint fusion of multiple upstream speech models while integrating information across their layers. Their method demonstrates consistent improvements across speech recognition, speaker verification, and emotion recognition tasks, with the best performance achieved when fusing self-supervised models with supervised models, suggesting these approaches capture complementary aspects of speech.

## Method Summary
The proposed method extracts all hidden layers from multiple frozen upstream speech models, aligns their dimensions through upsampling (linear interpolation for frame rates and layer counts), merges the hidden states using either addition or concatenation, and processes them through a Hierarchical Convolution (HConv) interface across the layer dimension. This interface is then combined with a downstream task head for training. The framework enables joint fusion across multiple models while integrating information across their layers, addressing the limitation of previous work that treated model and layer fusion separately. The approach is evaluated on LibriSpeech for ASR and the ML-SUPERB benchmark for both monolingual and multilingual settings, as well as on SUPERB for speaker verification and emotion recognition tasks.

## Key Results
- The unified model+layer fusion approach consistently outperforms both model-only and layer-only fusion baselines across all tested tasks
- Fusing self-supervised models (HuBERT, WavLM) with supervised models (Whisper) yields the best performance, indicating complementary learning of speech characteristics
- The Hierarchical Convolution interface consistently outperforms weighted sum fusion and dimension-wise Gumbel selection methods
- Performance gaps between the proposed method and baselines increase for larger-scale models, demonstrating scalability

## Why This Works (Mechanism)
The method works by enabling cross-model layer interaction through the HConv interface, which captures hierarchical relationships between layers from different models. By jointly fusing across models and layers, the framework can leverage complementary strengths - self-supervised models excel at general speech representations while supervised models capture task-specific patterns. The HConv architecture with its increasing receptive field allows the model to aggregate information across layers effectively, creating richer fused representations than simple weighted combinations.

## Foundational Learning
- **Speech Foundation Models**: Pre-trained models that learn general speech representations from large datasets
  - *Why needed*: Provide rich feature representations that can be adapted to downstream tasks
  - *Quick check*: Verify upstream models are properly frozen during fine-tuning

- **Self-supervised vs Supervised Learning**: SSL learns from unlabeled data through pretext tasks, while supervised learning uses labeled data
  - *Why needed*: These paradigms capture different aspects of speech, enabling complementary fusion
  - *Quick check*: Compare performance of SSL-only, supervised-only, and fused approaches

- **Hierarchical Convolution**: Series of 1D convolutions with increasing receptive fields applied across layer dimension
  - *Why needed*: Enables cross-layer interaction and hierarchical aggregation of features
  - *Quick check*: Verify convolution receptive fields increase appropriately across layers

## Architecture Onboarding

**Component Map**: Upstream Models (Frozen) -> Alignment/Upsampling -> Merge (Add/Concat) -> HConv Interface -> Downstream Head

**Critical Path**: The HConv interface is the critical component that enables joint model+layer fusion. It processes the merged tensor across the layer dimension, allowing cross-model layer interactions that wouldn't be possible with simple weighted sums or concatenation alone.

**Design Tradeoffs**: The choice between Add and Concat merge strategies involves a speed-accuracy tradeoff - Concat preserves more information but increases computational cost, while Add is faster but may lose some detail. The HConv depth and kernel sizes represent another tradeoff between model capacity and computational efficiency.

**Failure Signatures**: Performance degradation compared to single models indicates issues with alignment, improper freezing of upstream models, or suboptimal HConv architecture. Tensor shape mismatches during fusion typically point to incorrect interpolation logic or dimension handling.

**3 First Experiments**:
1. Implement HuBERT Base + WavLM Base+ alignment and Add merge, then apply basic HConv interface to verify tensor shapes and basic functionality
2. Train a linear classifier on LibriSpeech using the fused features and compare against Weighted Sum baseline
3. Test different merge strategies (Add vs Concat) with the same HConv configuration to measure impact on performance

## Open Questions the Paper Calls Out
None

## Limitations
- The HConv interface architecture details are not fully specified in the paper, requiring reference to prior work for complete implementation
- Claims about scalability rely on qualitative comparisons rather than rigorous scaling studies with larger model pairs
- Computational cost of the proposed method is not explicitly quantified, though stated to be lightweight

## Confidence

**High confidence**: The core empirical finding that unified model+layer fusion outperforms separate strategies is well-supported by controlled experiments across multiple tasks and model pairs.

**Medium confidence**: Claims about the HConv interface being superior to weighted sum and dimension-wise Gumbel selection are supported but depend on specific architectural choices from prior work that are not fully detailed here.

**Medium confidence**: The assertion that supervised+SSL fusion is optimal relies on a limited set of model combinations and could benefit from broader validation across different supervised model architectures.

## Next Checks

1. **Architecture replication verification**: Implement the full Hierarchical Convolution interface based on reference [29] and verify that it produces the stated performance improvements on the HuBERT+WavLM LibriSpeech setup before scaling to larger model combinations.

2. **Computational overhead measurement**: Quantify the actual parameter count and inference time overhead introduced by the HConv interface compared to baseline fusion methods, particularly when scaling to larger model pairs like Whisper Medium+XL.

3. **Cross-task generalization test**: Evaluate whether the observed superiority of supervised+SSL fusion extends to tasks beyond those tested, such as keyword spotting or diarization, using a consistent set of model combinations across all tasks.