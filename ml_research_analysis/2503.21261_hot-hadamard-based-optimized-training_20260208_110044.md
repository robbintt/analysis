---
ver: rpa2
title: 'HOT: Hadamard-based Optimized Training'
arxiv_id: '2503.21261'
source_url: https://arxiv.org/abs/2503.21261
tags:
- quantization
- training
- memory
- lora
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hadamard-based Optimized Training (HOT),
  a novel approach for optimizing backpropagation in neural network training by reducing
  memory usage and computational overhead. HOT leverages Hadamard transformations
  combined with selective quantization and low-rank approximation to optimize gradient
  computations.
---

# HOT: Hadamard-based Optimized Training

## Quick Facts
- arXiv ID: 2503.21261
- Source URL: https://arxiv.org/abs/2503.21261
- Reference count: 40
- Memory savings up to 75% and 2.6× acceleration on real GPUs compared to FP32 training

## Executive Summary
HOT (Hadamard-based Optimized Training) introduces a novel approach for optimizing backpropagation in neural network training by reducing memory usage and computational overhead. The method leverages Hadamard transformations combined with selective quantization and low-rank approximation to optimize gradient computations. HOT achieves significant efficiency gains through differentiated optimization of activation gradients (using 4-bit quantization) and weight gradients (using Hadamard low-rank approximation with INT8), while introducing activation buffer compression and layer-wise quantizer selection. Extensive experiments validate HOT's superiority over existing methods across vision and language tasks.

## Method Summary
HOT optimizes backpropagation by treating activation gradients (gx) and weight gradients (gw) differently based on their tensor properties. For gx, it applies Hadamard Transform followed by 4-bit quantization, exploiting the fact that quantization noise averages out across batch dimensions. For gw, it uses Hadamard Low-rank Approximation with INT8 quantization, leveraging the natural low-pass filtering effect of averaging across sequence dimensions. The method also includes Activation Buffer Compression (ABC) that compresses activations immediately after forward pass using HLA + INT8 quantization, and Layer-wise Quantizer Selection (LQS) that calibrates per-token vs per-tensor quantization based on MSE thresholds.

## Key Results
- Achieves up to 75% memory savings compared to FP32 training
- Provides up to 2.6× acceleration on real GPUs
- Shows negligible accuracy loss on vision and language tasks
- Effectively integrates with LoRA for additional memory efficiency
- Outperforms existing methods like LUQ and LBP-WHT

## Why This Works (Mechanism)

### Mechanism 1: Differentiated Gradient Path Optimization
Activation gradients (gx) and weight gradients (gw) require different optimization strategies due to their distinct tensor properties. gx uses Hadamard Quantization (HQ) with INT4 because quantization noise averages out across batch dimension, while gw uses Hadamard Low-rank Approximation (HLA) with INT8 because averaging across sequence dimension acts as natural low-pass filter. This assumption may break with very small batch sizes or models where gradient information concentrates in high-frequency components.

### Mechanism 2: Hadamard Transform for Quantization Robustness
Applying Hadamard Transform before quantization converts irregular data with outliers into frequency distributions more resilient to low-precision representation. HT's orthogonality property allows redistribution of outlier values across frequency components, preventing single extreme values from dominating quantization range. This may add overhead without benefit when original gradient distribution is already well-conditioned.

### Mechanism 3: Activation Buffer Compression with Early Quantization
Compressing activations immediately after forward pass achieves up to 75% memory reduction. The approach applies HLA (reducing to 8 low-frequency vectors = ~50% reduction) followed by INT8 quantization (additional 4x compression from FP32) before storing activations. This may degrade with tasks requiring precise activation reconstruction, such as certain attention mechanisms.

## Foundational Learning

- **Backpropagation gradient paths**: Understanding why gx and gw are computed differently (gx = gy·w, gw = gy^T·x) is essential for grasping HOT's optimization strategy. Quick check: Given output gradient gy, which path reuses stored activations from forward pass?

- **Hadamard Transform properties**: HT is the core primitive; understanding its O(n log n) complexity via Fast Walsh-Hadamard Transform and orthogonality (H^T·H = I) explains why it preserves matrix multiplication semantics while enabling quantization. Quick check: Why does applying HT before quantization reduce outlier impact?

- **Stochastic quantization and unbiased estimation**: HOT uses pseudo-stochastic quantization to ensure quantization is unbiased; understanding that random rounding preserves expected values on average is critical for grasping why training converges. Quick check: What property must a quantizer have to avoid introducing systematic bias in gradient updates?

## Architecture Onboarding

- **Component map**: Forward pass (FP32) -> ABC compression (HLA+INT8) -> Backward with differentiated gx/gw optimization (HT+INT4 for gx, HLA+INT8 for gw) -> Weight update with LQS module for quantizer selection

- **Critical path**: 1) Calibration phase: Run backward pass on small dataset to compute MSE for LQS decisions 2) Training loop: Forward → ABC compression → Backward with differentiated gx/gw optimization → Weight update

- **Design tradeoffs**: Rank selection (r=8) balances accuracy vs compression/speedup; per-token vs per-tensor quantization balances outlier reduction vs scale computation overhead; LoRA co-design requires full BP for decomposed weights to preserve accuracy

- **Failure signatures**: Accuracy drops >1%: Check if LQS calibration was skipped or if batch size is too small; Memory not reducing: Verify ABC is applied; NaN losses: Ensure INT8 warmup (first 25 epochs) for pre-training stability

- **First 3 experiments**: 1) Validate gradient path sensitivity: Replicate Table 2 on small model comparing FP vs HT+4bit vs HLA on each path independently 2) Ablate ABC and LQS incrementally: Following Table 7, measure memory/accuracy tradeoff by adding ABC then LQS to baseline HOT 3) Profile kernel latency breakdown: Using provided CUDA kernels, measure HT, HLA, quantization, and INT GEMM components to identify bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
Can the performance degradation observed in very deep or large-scale models be mitigated without sacrificing the efficiency gains of HOT? The paper validates performance up to Llama3-8B but notes degradation trend with increased scale remains a limitation. Success with models significantly larger than Llama3-8B with negligible accuracy loss would resolve this.

### Open Question 2
Does the Layer-wise Quantizer Selection (LQS) strategy generalize well to datasets significantly different from the calibration set? LQS relies on a hand-crafted, empirical selection process using small calibration set, and trade-offs require further detailed evaluation. Experiments showing LQS maintains accuracy on target datasets that differ substantially from calibration data would resolve this.

### Open Question 3
Is the selection of r=8 for Hadamard low-rank approximation universally optimal across diverse architectures and batch sizes? The paper fixes r=8 based on CIFAR-100 experiments without validation on larger models. A comprehensive ablation study on rank r within large language models would identify if dynamic or higher rank is required.

## Limitations

- Rank selection generalization: Fixed r=8 may not generalize across architectures with different sequence length distributions
- LQS calibration protocol: Calibration dataset details and sampling strategy remain unclear
- INT4/INT8 implementation: Exact implementation details for PyTorch compatibility not provided

## Confidence

**Memory Savings Claims (75%)** - High confidence: Directly follows from mathematical combination of HLA (50% reduction) and INT8 quantization (4× compression)

**Acceleration Claims (2.6×)** - Medium confidence: Theoretical speedup is clear but actual GPU performance depends on unspecified implementation details

**Accuracy Preservation Claims** - Medium confidence: Shows minimal loss on standard benchmarks but differentiated approach assumes gradient noise behavior that may not hold universally

## Next Checks

1. **Gradient Path Sensitivity Validation**: Replicate Table 2 on small model (ResNet-50 on CIFAR-100) comparing FP vs HT+4bit vs HLA on each path independently to confirm asymmetric sensitivity

2. **ABC and LQS Ablation Study**: Following Table 7, measure memory/accuracy tradeoff by adding ABC then LQS to baseline HOT on ViT-B/CIFAR-100

3. **Kernel Latency Breakdown Profiling**: Using provided CUDA kernels, measure HT, HLA, quantization, and INT GEMM components to identify bottlenecks and validate performance analysis