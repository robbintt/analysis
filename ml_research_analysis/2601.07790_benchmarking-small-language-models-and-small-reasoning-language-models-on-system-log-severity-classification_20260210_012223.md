---
ver: rpa2
title: Benchmarking Small Language Models and Small Reasoning Language Models on System
  Log Severity Classification
arxiv_id: '2601.07790'
source_url: https://arxiv.org/abs/2601.07790
tags:
- system
- logs
- classification
- accuracy
- severity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates small language models (SLMs) and small reasoning
  language models (SRLMs) for system log severity classification, a task critical
  for automated monitoring and digital twin systems. Using real-world journalctl logs,
  we test nine models under zero-shot, few-shot, and retrieval-augmented generation
  (RAG) prompting.
---

# Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification

## Quick Facts
- arXiv ID: 2601.07790
- Source URL: https://arxiv.org/abs/2601.07790
- Reference count: 40
- One-line primary result: Qwen3-4B achieves 95.64% accuracy with RAG on system log severity classification; Phi-4-Mini-Reasoning exceeds 228 seconds per log with <10% accuracy

## Executive Summary
This study evaluates small language models (SLMs) and small reasoning language models (SRLMs) for system log severity classification, a task critical for automated monitoring and digital twin systems. Using real-world journalctl logs, we test nine models under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting. Qwen3-4B achieves the highest accuracy of 95.64% with RAG, while the tiny Qwen3-0.6B reaches 88.12% accuracy despite weak performance without retrieval. However, several SRLMs, including Qwen3-1.7B and DeepSeek-R1-Distill-Qwen-1.5B, degrade substantially when paired with RAG. Efficiency measurements reveal that most Gemma and Llama variants complete inference in under 1.2 seconds per log, whereas Phi-4-Mini-Reasoning exceeds 228 seconds per log while achieving <10% accuracy. These findings highlight that architectural design, training objectives, and retrieval integration capabilities jointly determine performance. By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin systems and demonstrates that severity classification serves as a lens for evaluating model competence and deployability.

## Method Summary
The study benchmarks nine models (Qwen3, Gemma3, Llama3.2, Phi-4-Mini) on Linux `journalctl` log severity classification using 46,774 logs split 80/20 for training/evaluation. Three prompting strategies are tested: zero-shot (role-based framing), few-shot (5 examples), and RAG (FAISS index with Nomic Embed, k=5). Models are evaluated on accuracy (single-digit severity match) and average inference time per log. RAG implementation uses 768-dim embeddings with L2 similarity, retrieving top-k neighbors from training data to augment zero-shot prompts.

## Key Results
- Qwen3-4B achieves highest accuracy at 95.64% with RAG prompting
- Qwen3-0.6B reaches 88.12% accuracy with RAG despite weak standalone performance (~28%)
- SRLMs like Qwen3-1.7B and DeepSeek-R1-Distill-Qwen-1.5B degrade significantly with RAG (accuracy drops of 14-14.5 percentage points)
- Phi-4-Mini-Reasoning exceeds 228 seconds per log with RAG while achieving <10% accuracy
- Gemma3-1B improves from 20.25% (few-shot) to 85.28% (RAG), demonstrating RAG's effectiveness for certain architectures

## Why This Works (Mechanism)

### Mechanism 1: RAG Enhancement via Architectural-Context Alignment
- Claim: Retrieval-augmented generation substantially improves severity classification accuracy when model architecture aligns with the structure of retrieved context.
- Mechanism: Gemma's interleaved local-global attention blocks process within-log structure (local layers) and integrate across the query-plus-neighbors window (global layers). Qwen3-0.6B's strong-to-weak distillation from larger reasoning models enables it to leverage retrieved exemplars despite limited standalone capacity.
- Core assumption: Retrieved examples provide semantically aligned severity patterns that the model's attention mechanism can ground against.
- Evidence anchors:
  - [abstract]: "Qwen3-4B achieves the highest accuracy at 95.64% with RAG, while Gemma3-1B improves from 20.25% under few-shot prompting to 85.28% with RAG. Notably, the tiny Qwen3-0.6B reaches 88.12% accuracy despite weak performance without retrieval."
  - [section 6.3]: "Gemma's local blocks can focus on within-log structure...while its global blocks integrate across the query-plus-neighbors window. This architecture may be well aligned with 'clustered' context layouts."
  - [corpus]: Limited direct evidence on local-global attention for log classification specifically; corpus focuses on cross-system transfer and anomaly detection rather than severity classification with RAG.
- Break condition: When model lacks sufficient query heads, context capacity, or architectural mechanisms to distinguish retrieved signal from noise.

### Mechanism 2: RAG Degradation in Reasoning-Heavy Models
- Claim: Models optimized for extended chain-of-thought reasoning can degrade when paired with RAG under strict output constraints.
- Mechanism: CoT-centric training biases models toward internally generated reasoning traces rather than short, label-bearing retrieved snippets. The task's single-digit output constraint conflicts with verbose reasoning tendencies, causing format violations and accuracy collapse.
- Core assumption: The tension arises between generative reasoning behaviors and retrieval-grounded classification when output format is constrained.
- Evidence anchors:
  - [abstract]: "Several SRLMs, including Qwen3-1.7B and DeepSeek-R1-Distill-Qwen-1.5B, degrade substantially when paired with RAG."
  - [section 5.3]: "Qwen3-1.7B dropped from 43.30% (few-shot) down to 28.96% (RAG), while DeepSeek-R1-Distill-Qwen-1.5B fell from 17.63% to just 3.17%...Phi-4-Mini-Reasoning failed to produce any correct classification with RAG (0% accuracy)."
  - [corpus]: No direct corpus evidence on RAG-induced degradation in reasoning models; related work focuses on log analysis with LLMs without systematic RAG-compatibility evaluation.
- Break condition: When output constraints are relaxed, when reasoning models are explicitly trained for retrieval integration (e.g., RA-DIT, Self-RAG), or when model capacity is sufficient for both reasoning and retrieval modes.

### Mechanism 3: Retrieval Depth Non-Monotonicity
- Claim: Reducing retrieval context volume does not recover performance for models that struggle with retrieval integration.
- Mechanism: Degradation stems from inability to incorporate retrieved information into the decision process, not from context overload. Reducing neighbors (k=5→k=1) decreases latency proportionally but does not address the underlying integration limitation.
- Core assumption: The model's challenge is integration capability rather than context volume.
- Evidence anchors:
  - [section 6.1]: "Accuracy declined steadily from 28.96% at k=5 to 26.47% at k=1, despite inference speed decreasing proportionally...performance did not recover with smaller context windows."
  - [corpus]: No corpus evidence on retrieval depth tuning for log classification; this appears to be a novel finding in this paper.
- Break condition: If degradation were purely from context overload, reducing k should improve accuracy. The observed pattern contradicts this, suggesting deeper architectural or training mismatches.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The paper's central intervention is RAG-based prompting; understanding how external retrieval augments inference is prerequisite to interpreting the results.
  - Quick check question: Can you explain why Qwen3-0.6B's accuracy jumps from ~28% to 88.12% when retrieval is added, while Qwen3-1.7B's accuracy drops?

- Concept: **Chain-of-Thought (CoT) Reasoning and Distillation**
  - Why needed here: SRLMs are differentiated by CoT supervision and distillation from larger models; this explains the divergent RAG compatibility patterns.
  - Quick check question: Why might a model trained to generate extended reasoning traces struggle with a task requiring single-digit output plus retrieved context?

- Concept: **Grouped-Query Attention (GQA) and Local-Global Attention**
  - Why needed here: Architectural differences (query heads, KV heads, attention patterns) partially explain why some models integrate retrieval effectively while others degrade.
  - Quick check question: How might Gemma's interleaved local-global blocks be better suited to RAG contexts than a uniform attention pattern?

## Architecture Onboarding

- Component map:
  Data layer: journalctl logs → JSON standardization (key-value dictionary) → 80/20 train/eval split
  Retrieval layer: Training logs embedded via Nomic Embed (768-dim) → FAISS index with L2 similarity → top-k neighbor retrieval (default k=5)
  Prompting layer: Zero-shot (role-based framing) / Few-shot (5 examples) / RAG (zero-shot + retrieved neighbors)
  Model layer: 9 models (4 SLMs, 5 SRLMs) served via LM Studio with OpenAI-compatible API
  Evaluation layer: Accuracy (single-digit match) + latency (seconds per log)

- Critical path: RAG configuration is the most sensitive component—FAISS index quality, embedding model choice, retrieval depth (k), and model architecture alignment jointly determine success. Start with Gemma3-1B or Qwen3-4B for RAG experiments; avoid Phi-4-Mini-Reasoning and DeepSeek-R1-Distill-Qwen-1.5B for initial setup.

- Design tradeoffs:
  - **Accuracy vs. latency**: Qwen3-4B (95.64% accuracy, 7.14s/log) vs. Gemma3-1B (85.28% accuracy, 0.70s/log) vs. Gemma3-4B (81.84% accuracy, 1.16s/log)
  - **Model scale vs. retrieval dependency**: Qwen3-0.6B requires RAG for competence (88.12% with RAG, ~28% without); larger models may perform adequately in few-shot
  - **Reasoning capability vs. RAG compatibility**: SRLMs outperform in zero-shot/few-shot but can degrade catastrophically with RAG

- Failure signatures:
  - **Verbose outputs**: Models appending reasoning after the single-digit answer (Phi-4-Mini-Reasoning in zero-shot, Llama3.2-1B in few-shot) → format violation → accuracy collapse
  - **Extreme latency**: >200 seconds per log (Phi-4-Mini-Reasoning with RAG) indicates excessive thinking tokens without corresponding accuracy
  - **Retrieval degradation**: Accuracy dropping below zero-shot baseline when RAG is added (DeepSeek-R1-Distill-Qwen-1.5B: 11.54% zero-shot → 3.17% RAG) signals incompatibility

- First 3 experiments:
  1. **Establish zero-shot baseline** for your target model(s) with the paper's prompt framing ("Linux System Log Specialist" role, single-digit output constraint). This isolates intrinsic log comprehension.
  2. **Test few-shot with 5 training examples** (severity levels 1-7 sampled from training set). Compare accuracy gain vs. latency increase. If the model fails to follow output format (verbose outputs), it may be unsuitable for constrained classification.
  3. **Deploy RAG with k=5 neighbors** using FAISS + Nomic Embed. If accuracy improves significantly (e.g., Gemma3-1B pattern), tune retrieval depth (k=3, k=1) for latency optimization. If accuracy degrades (e.g., Qwen3-1.7B pattern), test alternative models from the same family or switch to non-reasoning SLMs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the retrieval-induced performance degradations observed in specific SRLMs (e.g., Qwen3-1.7B, DeepSeek-R1-Distill-Qwen-1.5B) stem primarily from model capacity constraints, training objectives, or deeper architectural dynamics?
- Basis in paper: [explicit] The Conclusion states, "Future work will investigate whether these effects stem primarily from capacity constraints, training objectives, or deeper architectural dynamics."
- Why unresolved: The study documents the degradation but does not isolate the specific mechanism causing it, noting only that reducing retrieval context ($k$) fails to recover performance.
- What evidence would resolve it: Ablation studies re-training these specific architectures with modified reasoning objectives or varying capacity to isolate the variable causing retrieval interference.

### Open Question 2
- Question: Do the retrieval-augmented generation (RAG) benefits and degradation patterns identified in SLMs and SRLMs generalize to domain-specific tasks outside of system log severity classification?
- Basis in paper: [explicit] The Conclusion notes plans to "extend this benchmark beyond system logs to other domain-specific tasks to test whether these retrieval patterns generalize across domains."
- Why unresolved: The current benchmark is confined to `journalctl` data; it is unknown if the "tension between generative reasoning and retrieval" is unique to log semantics.
- What evidence would resolve it: Application of the same zero-shot, few-shot, and RAG benchmarking methodology to different technical domains (e.g., financial logs, medical records) and comparison of stratification results.

### Open Question 3
- Question: How do these models perform under live Digital Twin (DT) constraints involving streaming telemetry, dynamic memory policies, and shifting baselines?
- Basis in paper: [explicit] The Conclusion states that integrating the framework into a "live DT system will enable evaluation under streaming telemetry... where dynamic memory policies, time-aware retrieval, and continual context updates are essential."
- Why unresolved: The current study evaluates static datasets in a controlled environment; real-time integration requires handling evolving states not present in the fixed evaluation set.
- What evidence would resolve it: Deployment of the top-performing models (e.g., Qwen3-4B) into a live monitoring pipeline to measure accuracy and latency drift over time.

### Open Question 4
- Question: How does the absence of emergency-level (severity 0) logs in the dataset limit the evaluation of model sensitivity to critical system failures?
- Basis in paper: [inferred] Table 2 notes "No emergency-level entries were observed," and the text describes this as "a limitation for evaluating model sensitivity to extreme conditions."
- Why unresolved: The models were benchmarked on a semi-balanced dataset lacking the most critical priority level, leaving their behavior on system-catastrophic events untested.
- What evidence would resolve it: Generating or sourcing synthetic emergency-level logs to test if models can correctly identify the highest severity class without false negatives.

## Limitations
- SRLM RAG degradation mechanisms remain unexplained despite documentation of performance drops
- Efficiency comparison lacks GPU/CPU hardware specifications, limiting generalizability of latency findings
- Dataset lacks emergency-level (severity 0) logs, limiting evaluation of model sensitivity to critical failures

## Confidence
- **High Confidence**: Accuracy rankings of SLMs vs. SRLMs in zero-shot/few-shot settings; the Qwen3-4B RAG result (95.64%); the Phi-4-Mini-Reasoning latency anomaly (>200s/log)
- **Medium Confidence**: Mechanisms for RAG enhancement in Gemma and Qwen3-0.6B; explanations for SRLM RAG degradation; non-monotonicity of retrieval depth effects
- **Low Confidence**: Architectural alignment hypotheses (local-global attention benefits); generalizability of RAG-compatibility patterns to other reasoning models not tested; external validity for non-journalctl log formats

## Next Checks
1. **Prompt Template Fidelity Test**: Implement the exact prompt structure described (role-based framing, single-digit constraint) and measure accuracy variance when introducing common deviations (e.g., allowing prose output, changing example count)
2. **Attention Mechanism Ablation**: Modify Gemma's attention configuration (disable local/global blocks) and test RAG performance to isolate architectural contributions to retrieval integration
3. **Reasoning Model RAG Compatibility Survey**: Evaluate additional SRLMs (e.g., Qwen2.5-Coder, DeepSeek-VL) with RAG to determine whether degradation patterns are model-family-specific or generic to reasoning-trained architectures