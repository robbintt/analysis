---
ver: rpa2
title: Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with
  a Billion-Parameter Instruction-Tuned Model
arxiv_id: '2509.08381'
source_url: https://arxiv.org/abs/2509.08381
tags:
- data
- json
- extraction
- samples
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ETLCH, a 1B-parameter LLaMA-based model fine-tuned
  with low-rank adaptation (LoRA) on only 100-1000 samples per task for structured
  information extraction. The model was evaluated on JSON extraction, knowledge graph
  extraction, and named entity recognition tasks, outperforming larger baseline models
  including Qwen2.5-7B and Breeze-7B across most metrics.
---

# Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model

## Quick Facts
- arXiv ID: 2509.08381
- Source URL: https://arxiv.org/abs/2509.08381
- Reference count: 13
- Primary result: 1B-parameter model fine-tuned on 100-1000 samples per task outperforms 7B-parameter baselines on structured extraction tasks

## Executive Summary
This paper introduces ETLCH, a 1B-parameter LLaMA-based model fine-tuned with low-rank adaptation (LoRA) on only 100-1000 samples per task for structured information extraction. The model was evaluated on JSON extraction, knowledge graph extraction, and named entity recognition tasks, outperforming larger baseline models including Qwen2.5-7B and Breeze-7B across most metrics. ETLCH achieved statistically significant improvements in JSON and knowledge graph tasks, with winning rates exceeding 70% against baselines. The model demonstrated exceptional data efficiency, showing rapid performance gains with minimal training data and plateauing after approximately 300 samples.

## Method Summary
ETLCH uses LoRA fine-tuning on Llama-3.2-1B-Instruct with rank=32, α=64, dropout=0.4, batch size=2, learning rate=1e-7, and 100 epochs. The training data consists of 100-1000 samples per task (NER, KGE, JSON extraction) generated via ChatGPT. The three tasks are trained simultaneously using multi-instruction format with task chaining (NER → KGE → JSON). Evaluation uses ROUGE-L, cosine similarity, and JSON parse success rate across diverse long-form texts up to 1500 tokens.

## Key Results
- Outperformed Qwen2.5-7B and Breeze-7B baselines across most metrics
- Achieved winning rates exceeding 70% against baselines on JSON and knowledge graph tasks
- JSON parse success rate reached 267/300 after 300 training samples
- Performance gains plateaued after approximately 300 samples per task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-rank adaptation on instruction-tuned 1B models enables sample-efficient structured extraction with minimal compute.
- Mechanism: LoRA (rank=32, α=64) updates a small subset of weights while preserving pretrained knowledge, allowing the model to learn task-specific output formatting without catastrophic forgetting. The instruction-tuned base already provides schema-following capability; LoRA specializes it.
- Core assumption: The base model's instruction-following ability transfers to structured extraction tasks after minimal parameter updates.
- Evidence anchors:
  - [abstract] "fine-tuned with low-rank adaptation on only 100–1000 samples per task"
  - [section 3.1] "LoRA configuration adopted a rank of 32 and α = 64... learning rate of 1×10⁻⁷ and a maximum gradient norm of 0.1"
  - [corpus] Related work on clinical IE (arxiv 2507.20859) shows LoRA enables resource-constrained extraction, though on larger models; no direct 1B LoRA benchmarks found in neighbors.

### Mechanism 2
- Claim: Multi-task co-training with task chaining (NER → KGE → JSON) creates mutual verification that reduces error accumulation.
- Mechanism: Shared representations across tasks enforce consistency—entities extracted in NER must match subjects in KGE; JSON schema constraints penalize incoherent outputs. Loss gradients from one task regularize others.
- Core assumption: Tasks share underlying linguistic representations such that joint training yields transfer benefits rather than interference.
- Evidence anchors:
  - [abstract] "JSON extraction, knowledge graph extraction, and named entity recognition" trained together
  - [section 3.5] "When the three tasks are trained simultaneously, the model's output for one task is constrained by the results of the others"
  - [corpus] arxiv 2505.06303 (Collaborative Multi-LoRA Experts) validates multi-task IE unification, but uses T5-based generation; cross-architecture generalization unconfirmed.

### Mechanism 3
- Claim: High dropout (0.4) and small batch size (2) in low-data regimes prevent overfitting while forcing robust pattern learning.
- Mechanism: Noise injection via dropout and gradient stochasticity from tiny batches act as implicit regularization. With only 100–300 samples, this prevents memorization and encourages learning generalizable extraction patterns.
- Core assumption: The signal-to-noise ratio in training data is sufficient for learning despite aggressive regularization.
- Evidence anchors:
  - [section 3.1] "effective batch size was set to 2... dropout rate of 0.4 was further applied"
  - [section 6] "performance gains across multiple tasks when the training data size increases from 100 to 300 samples, followed by a clear plateau"
  - [corpus] Weak direct evidence; no neighbor papers explicitly study dropout/batch size in low-resource LLM fine-tuning.

## Foundational Learning

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Core parameter-efficient method enabling 1B model fine-tuning on consumer GPU with <1000 samples.
  - Quick check question: Can you explain why LoRA's rank parameter controls the tradeoff between expressiveness and overfitting risk?

- Concept: **Instruction Tuning for Structured Outputs**
  - Why needed here: The base model's instruction-following capability is prerequisite for reliable JSON/schema-constrained generation.
  - Quick check question: How does instruction tuning differ from task-specific fine-tuning in terms of output controllability?

- Concept: **ROUGE-L and Cosine Similarity for Structured IE Evaluation**
  - Why needed here: Paper's primary metrics; ROUGE-L captures sequence alignment while cosine similarity measures semantic similarity.
  - Quick check question: Why might ROUGE-L be preferred over exact match for evaluating JSON extraction from varied text inputs?

## Architecture Onboarding

- Component map: Base model (Llama-3.2-1B-Instruct) -> LoRA adapter (rank=32, α=64) -> LlamaFactory framework -> Multi-task instruction dataset -> Training (100 epochs, lr=1e-7, batch=2) -> Evaluation (ROUGE-L, cosine similarity, JSON parse rate)

- Critical path: Base model selection → LoRA config → Multi-task data formatting → Training (100 epochs, lr=1e-7, batch=2) → Evaluation (ROUGE-L, cosine similarity, JSON parse success)

- Design tradeoffs:
  - Rank 32 vs. lower: More expressive but higher overfitting risk; paper chose 32 without ablation.
  - 100 epochs on tiny data: Risk of memorization mitigated by dropout/batch noise; no early stopping used.
  - Multi-task vs. single-task: Potential interference vs. mutual reinforcement; no single-task baselines reported.

- Failure signatures:
  - JSON parse failures indicate schema adherence collapse (Taide-8B: 0% parse rate)
  - ROUGE-L plateau after 300 samples suggests data saturation, not capacity limits
  - NER task showing p=0.053 (not significant) vs. Qwen2.5 suggests task-specific ceiling effects

- First 3 experiments:
  1. **LoRA rank ablation**: Test r=8, 16, 32, 64 on JSON task with 100 samples to validate 32 was optimal (not just selected).
  2. **Single-task vs. multi-task comparison**: Train separate models per task to quantify mutual reinforcement benefit.
  3. **Data quality sensitivity**: Replace ChatGPT-generated annotations with human-annotated samples (if available) to measure synthetic data ceiling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed performance plateau at ~300 training samples reflect a fundamental capacity limit of 1B-parameter models, or could alternative training strategies (e.g., curriculum learning, data augmentation) extend gains further?
- Basis in paper: [explicit] Figures 4–7 show rapid improvement from 100 to 300 samples followed by diminishing returns; the authors note "gains diminish markedly" beyond 300 but do not investigate causes.
- Why unresolved: The paper reports the plateau descriptively but does not isolate whether it stems from model capacity, data quality saturation, or suboptimal hyperparameters.
- What evidence would resolve it: Ablation experiments varying learning rate schedules, augmentation techniques, or model capacity (e.g., 1.5B variants) at the 300+ sample regime.

### Open Question 2
- Question: Can the ETLCH training paradigm generalize to real-world domain-specific documents (e.g., actual financial filings, legal contracts, medical records) rather than synthetic ChatGPT-generated texts?
- Basis in paper: [explicit] The authors motivate the work with financial compliance, legal analytics, and medical digitization use cases, but all training and evaluation data consists of ChatGPT-generated long-form texts on general knowledge topics.
- Why unresolved: No experiments on authentic domain corpora were conducted; synthetic data may lack the lexical diversity, formatting irregularities, and domain-specific jargon found in real documents.
- What evidence would resolve it: Evaluation on benchmark datasets of real financial reports, legal proceedings, or clinical notes with identical training configurations.

### Open Question 3
- Question: What architectural or fine-tuning factors caused the 8B-parameter Taide model to produce zero successfully parsed JSON outputs, and can this failure mode be diagnosed or remediated?
- Basis in paper: [explicit] The paper notes that "Taiwan-localized large language model, fine-tuned from LLaMA3-8B... failed to produce a single correctly readable JSON string," calling this "striking," but offers no explanation.
- Why unresolved: The complete failure of a larger model on structured output generation is reported as an observation without diagnostic analysis.
- What evidence would resolve it: Systematic comparison of instruction formatting, tokenizer behavior, and fine-tuning data composition between Taide-8B and ETLCH to identify the failure source.

## Limitations

- Synthetic Data Quality Ceiling: Paper relies entirely on ChatGPT-generated annotations without validation against human-annotated gold standards.
- Task-Specific Generalization: NER task shows marginal statistical significance (p=0.053) suggesting potential task-specific limitations not fully characterized.
- Architecture Choice Assumptions: Superiority of LoRA over other parameter-efficient methods assumed rather than empirically validated.

## Confidence

- **High Confidence**: Core finding that instruction-tuned 1B models with LoRA can achieve competitive structured extraction performance on 100-1000 samples is well-supported by quantitative results across multiple metrics with statistical significance testing.
- **Medium Confidence**: Mechanism claims about multi-task mutual reinforcement and dropout/batch size regularization are plausible but lack direct empirical validation through ablation studies.
- **Low Confidence**: Claims about computational efficiency advantages are based on inference-only comparisons without accounting for training setup differences or full system-level optimizations.

## Next Checks

1. **Human Annotation Validation**: Replace 100 synthetic samples per task with human-annotated data (if available) to establish an upper bound on performance and quantify synthetic data quality limitations.

2. **LoRA Configuration Ablation**: Systematically vary LoRA rank (8, 16, 32, 64) and dropout rates (0.2, 0.4, 0.6) on the JSON extraction task with 100 samples to empirically validate the optimal configuration choice.

3. **Single-Task Baseline Comparison**: Train three separate models, each fine-tuned on only one task (NER, KGE, or JSON extraction), to quantify the actual benefit of multi-task learning versus potential interference effects.