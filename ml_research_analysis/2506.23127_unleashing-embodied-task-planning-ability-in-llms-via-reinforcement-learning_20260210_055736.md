---
ver: rpa2
title: Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning
arxiv_id: '2506.23127'
source_url: https://arxiv.org/abs/2506.23127
tags:
- embodied
- wang
- planning
- task
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of enabling large language models
  (LLMs) to perform embodied task planning, which requires continuous environmental
  understanding and action generation. Existing methods struggle in partially observable
  environments because they generate static action scripts based on pre-trained knowledge
  without learning causal relationships between actions and environmental feedback.
---

# Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2506.23127
- **Source URL:** https://arxiv.org/abs/2506.23127
- **Reference count:** 40
- **Primary result:** Achieves 97.78% completion on ALFWorld and 79.92% on ScienceWorld using outcome-driven RL without dense supervision

## Executive Summary
This paper introduces Embodied Planner-R1, a reinforcement learning framework that enables large language models to perform embodied task planning in partially observable environments. The key innovation is an outcome-driven approach that learns from autonomous exploration using completion-only rewards, avoiding the reward-hacking risks of intermediate supervision. Through Group Rollout with In-Environment Interaction and Interactive Policy Optimization (IPO), the framework achieves state-of-the-art performance on two text-based benchmarks while maintaining strong generalization to unseen environments.

## Method Summary
Embodied Planner-R1 trains LLMs using a novel RL framework that combines group rollout sampling with outcome-driven rewards. The method uses parallel environment instances to generate trajectories, computes group-normalized advantages from binary completion rewards, and updates the policy via IPO with PPO-style clipping and KL regularization. The framework requires minimal supervision—only task completion signals—and operates through ReAct-style Thought→Action→Observation cycles in text-based environments.

## Key Results
- **ALFWorld performance:** 97.78% completion rate on seen environments, 94.12% on unseen environments
- **ScienceWorld performance:** 79.92% completion rate on seen environments, 76.26% on unseen environments
- **Generalization:** Only -3.66% drop in unseen environments compared to seen environments
- **Efficiency:** Eliminates invalid actions after 50 training steps without explicit penalties

## Why This Works (Mechanism)

### Mechanism 1: Group Rollout for Sparse Reward Credit Assignment
Parallel trajectory sampling enables stable advantage estimation under extremely sparse rewards. For each task, n environment replicas execute simultaneously, computing advantage as group-normalized reward differences. This creates relative baselines without learned critics, assuming the task distribution contains completable trajectories within model's exploration capacity.

### Mechanism 2: Completion-Only Reward Reduces Reward Hacking
Removing intermediate rewards forces genuine environmental understanding rather than shallow pattern exploitation. The binary reward function (1 if complete, else 0) combined with natural language feedback from failed actions enables causal learning without explicit penalties for invalid actions.

### Mechanism 3: IPO's Step-Level Probability Ratios for Long-Horizon Credit
Computing probability ratios over trajectory prefixes rather than full sequences mitigates probability degradation in multi-turn interactions. The prefix-conditioned probability ratio conditions on all tokens up to each step, with KL penalty preventing catastrophic forgetting of reference model capabilities.

## Foundational Learning

- **POMDPs (Partially Observable Markov Decision Processes)**
  - Why needed: The framework formalizes embodied planning as POMDPs where agents receive observations rather than full states
  - Quick check: Given that the agent cannot observe states directly, how should it maintain and update its belief over possible states?

- **ReAct Paradigm (Reasoning + Acting)**
  - Why needed: The framework structures trajectories as "Thought → Action → Observation" cycles
  - Quick check: How does interleaving explicit reasoning steps with actions differ from direct action prediction?

- **Policy Gradient with Group Baselines**
  - Why needed: IPO eliminates the critic by using group statistics for advantage estimation
  - Quick check: Why is the mean of group rewards a valid baseline but the mean of a single trajectory's rewards is not?

## Architecture Onboarding

- **Component map:** Environment Wrapper -> Group Rollout Engine -> Reward Calculator -> IPO Optimizer -> Reference Model
- **Critical path:**
  1. Sample batch of K task-environment pairs
  2. For each pair, spawn n replicas → run ReAct loops → collect n trajectories
  3. Assign binary rewards per trajectory
  4. Compute group-normalized advantages
  5. Update policy via IPO objective with PPO clipping and KL penalty

- **Design tradeoffs:**
  - Group size n: Larger n improves baseline stability but increases compute; paper uses n=5
  - Step limit: 30 steps balances long-horizon planning with training efficiency
  - KL coefficient β: Too high prevents learning; too low causes divergence from pretrained capabilities

- **Failure signatures:**
  - Zero group variance → all trajectories succeed or all fail → no gradient signal
  - Exploding response length → model not learning to terminate → check "done" action frequency
  - High invalid action rate persisting → environment feedback not being incorporated → verify observation parsing

- **First 3 experiments:**
  1. Run frozen Qwen2.5-7B-Instruct with ReAct prompting on ALFWorld; expect ~30% completion
  2. Compare n=1, 3, 5, 8 group sizes on subset of tasks; monitor learning speed and final performance
  3. Add intermediate rewards (format compliance, valid action bonus) and compare against pure completion reward

## Open Questions the Paper Calls Out
- Can the framework be extended to multimodal or visual domains while maintaining autonomous exploration capabilities?
- How can high computational resource requirements of group rollout be reduced to improve accessibility?
- Does completion-driven binary reward prevent learning in environments where random exploration yields almost no successful trajectories?

## Limitations
- Computational requirements may limit accessibility for researchers with limited computing capacity
- Binary completion rewards may be insufficient in environments with very sparse successful states
- Framework relies on environments providing sufficiently informative observations for causal learning

## Confidence
- **High confidence:** Empirical results on ALFWorld and ScienceWorld with clear baselines and generalization metrics
- **Medium confidence:** Mechanism explanations for group rollout advantages and completion-only rewards are theoretically sound
- **Low confidence:** Claims about IPO's superiority over traditional RL methods lack direct comparison to established baselines

## Next Checks
1. **Group size ablation study:** Systematically test n=1, 3, 5, 8 group sizes on subset of tasks to quantify impact of baseline stability on learning speed and final performance
2. **Reward structure comparison:** Implement intermediate reward variants and compare against pure completion reward to test for reward hacking and measure generalization differences
3. **Cross-domain robustness test:** Deploy on more challenging environment with less informative feedback to test critical assumption about environment observations enabling causal learning