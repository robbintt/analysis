---
ver: rpa2
title: 'Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual
  Streams'
arxiv_id: '2507.21107'
source_url: https://arxiv.org/abs/2507.21107
tags:
- curvature
- semantic
- residual
- salience
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Curved Inference introduces a geometric framework for analyzing\
  \ how large language models\u2019 residual streams bend in response to semantic\
  \ concern. By tracking curvature and salience in the native activation space under\
  \ a pullback semantic metric, the method quantifies how internal trajectories reorient\
  \ when exposed to emotional, moral, or logical shifts in prompts."
---

# Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual Streams

## Quick Facts
- arXiv ID: 2507.21107
- Source URL: https://arxiv.org/abs/2507.21107
- Reference count: 28
- Primary result: Introduces geometric curvature metrics to quantify how semantic concerns bend transformer residual stream trajectories.

## Executive Summary
Curved Inference introduces a geometric framework for analyzing how large language models' residual streams bend in response to semantic concern. By tracking curvature and salience in the native activation space under a pullback semantic metric, the method quantifies how internal trajectories reorient when exposed to emotional, moral, or logical shifts in prompts. Experiments on Gemma3-1b and LLaMA3.2-3b reveal that concern-shifted variants reliably induce measurable curvature changes, with LLaMA showing stronger, statistically significant scaling than Gemma. These findings support a two-layer geometric model—latent conceptual structure and dynamic contextual trajectory—and offer a principled tool for diagnosing alignment, abstraction, and inference dynamics in transformer models.

## Method Summary
The method computes curvature κ_i and salience S(t) from residual stream trajectories under a pullback semantic metric G = U^⊤U derived from the unembedding matrix. Forward passes capture layer-wise activations x_ℓ for control and concern-shifted prompts. Discrete 3-point central differences estimate velocity v_i and acceleration a_i, with curvature κ_i = √(∥a_i∥²_G·∥v_i∥²_G − ⟨a_i,v_i⟩²_G) / ∥v_i∥³_G. Delta heatmaps (CS − control) reveal concern-induced trajectory divergence, with statistical tests comparing moderate vs. strong variants.

## Key Results
- Concern-shifted prompts induce measurable curvature changes in residual streams (κ_i scaling with concern intensity).
- LLaMA3.2-3b shows stronger and more consistent curvature scaling than Gemma3-1b across domains.
- Pullback metric G = U^⊤U ensures measurements reflect token-aligned semantic geometry rather than arbitrary coordinate distances.

## Why This Works (Mechanism)

### Mechanism 1: Semantic Alignment via Pullback Metric
- **Claim:** If the unembedding matrix is used to define a metric tensor over the residual stream, geometric measurements reflect token-level semantic distances rather than arbitrary coordinate distances.
- **Mechanism:** The unembedding matrix U maps residual vectors to logits (output scores). By constructing a pullback metric G = U^⊤U, the inner product in residual space inherits the geometry of the logit space. Distances and angles in this "semantic space" correspond directly to changes in output probabilities.
- **Core assumption:** The geometry of the output logit space meaningfully represents the "semantic" relationships between tokens as perceived by the model.
- **Evidence anchors:** [abstract] Mentions "pullback semantic metric derived from the unembedding matrix, ensuring that all measurements reflect token-aligned geometry." [Methods 3.4] Explicitly defines G = U^⊤U and states it "ensures that all curvature estimates reflect token-aligned semantic geometry." [corpus] Corpus papers discuss non-Euclidean geometry in neural networks (e.g., "Learning Beyond Euclid"), supporting the validity of manifold analysis, but do not specifically validate the U^⊤U construction for LLM residual streams.
- **Break condition:** If the unembedding matrix U is rank-deficient or effectively random (e.g., in an untrained model), the metric G will fail to capture meaningful semantic structure, reducing curvature measurements to coordinate noise.

### Mechanism 2: Residual Accumulation as a Trajectory
- **Claim:** The additive nature of transformer residual connections creates a cumulative path (trajectory) through activation space, allowing second-order geometric properties (curvature) to emerge from layer-wise updates.
- **Mechanism:** A token's representation x starts as an embedding. At each layer, attention and MLP outputs are added as delta vectors (Δx). The sequence of residual states x_0, x_1, ..., x_L forms a discrete curve. "Lensing" occurs when these delta vectors change direction, causing the trajectory to bend.
- **Core assumption:** The sequential, layer-wise evolution of the residual stream is the primary locus of semantic integration, rather than isolated layer activations.
- **Evidence anchors:** [Figure 1] Visualizes "Semantic Lens" where "attention and MLP layers act like lenses curving the residual stream." [Results 4] States "Attention and MLP outputs are delta vectors—they cause curvature. The residual stream is the curve." [corpus] "Transformer Dynamics" (neighbor paper) supports analyzing transformers as dynamical systems, consistent with the trajectory view, but does not use the specific "lensing" metaphor.
- **Break condition:** This mechanism relies on standard Pre-LN or Post-LN residual connections. Architectures without additive residuals (or with gated residuals that frequently approach zero) would not form analyzable trajectories in the same way.

### Mechanism 3: Concern-Induced Directional Deviation
- **Claim:** Introducing a "semantic concern" (e.g., emotional or moral framing) alters the direction of layer-wise updates (Δx) relative to a neutral baseline, resulting in measurable trajectory divergence.
- **Mechanism:** A "concern-shifted" prompt is processed in parallel with a neutral control. If the update vector Δv at any layer is non-colinear with the control update, the trajectory bends. This deviation is quantified as curvature κ.
- **Core assumption:** The model's response to semantic pressure is localized enough to be traced to specific layer updates but persistent enough to propagate through depth.
- **Evidence anchors:** [Abstract] Reports that "concern-shifted variants reliably induce measurable curvature changes." [Appendix C] Formalizes the condition: "directional deviation implies curvature... if [delta] is not colinear with [control step]." [corpus] Weak direct evidence. "Algorithmic Primitives" (neighbor paper) links reasoning to activation patterns, supporting the link between task semantics and internal geometry, but does not validate the specific "concern-shift" paradigm.
- **Break condition:** If the model generalizes such that all semantic nuances are projected into a single "safe response" direction early in the network, distinct concern shifts may fail to produce divergent trajectories, collapsing the signal.

## Foundational Learning

- **Concept: Pullback Metric (Riemannian Geometry)**
  - **Why needed here:** To understand how the paper converts raw vector space into "semantic space." You must grasp how a matrix U defines a new inner product ⟨u, v⟩_G = u^⊤Gv that weights dimensions by output relevance.
  - **Quick check question:** If the unembedding matrix U were orthogonal, how would the metric G = U^⊤U affect the measured curvature compared to the standard Euclidean metric?

- **Concept: Residual Stream Structure (Transformer Architecture)**
  - **Why needed here:** The paper analyzes the stream as a time-series (depth-wise). You need to distinguish the residual stream (the accumulating total) from the Attention/MLP outputs (the incremental deltas).
  - **Quick check question:** In a standard transformer block, where is the "residual stream" typically accessed relative to the LayerNorm and sublayers (Attention/MLP)?

- **Concept: Finite Difference Methods (Numerical Analysis)**
  - **Why needed here:** The paper uses discrete 3-point central differences to estimate derivatives (velocity, acceleration) from layer activations. Understanding the error terms (e.g., sensitivity to noise) is critical for evaluating the robustness of the κ metric.
  - **Quick check question:** Why does a 3-point central difference generally provide a better derivative estimate than a 2-point forward difference, and what assumption about the data smoothness does it require?

## Architecture Onboarding

- **Component map:** Token IDs & Model Weights (E, U, Layer Weights) -> Forward pass hooks capture residual vectors x_ℓ -> Geometric Engine computes G-norms, finite differences (v_i, a_i), and curvature κ_i -> Analysis compares κ distributions for Control vs. Concern-Shifted prompts

- **Critical path:** Hook Registration -> Metric Construction -> Discrete Differentiation -> Curvature & Salience Computation -> Statistical Comparison

- **Design tradeoffs:** Native Space vs. Projection (reject low-dim projections to preserve angles); Spline vs. Finite Difference (moved to finite differences for stability)

- **Failure signatures:** Early-Layer Artifacts (spikes at Layer 0/1 likely embedding noise); Uniform Curvature (identical κ for "nonsense" and "concern" prompts suggests degenerate metric); Metric Collapse (near-singular G causes numerical instability)

- **First 3 experiments:** Replicate "Neutral vs. Concern" delta heatmap (Figure 2) on single prompt pair; Ablate G (use Identity matrix) to quantify semantic metric value; Replicate Table 5 (Moderate vs. Strong concern) on third model (e.g., GPT-2 small)

## Open Questions the Paper Calls Out

- **Open Question 1:** Does high curvature causally influence generation, or is it epiphenomenal? Basis: [explicit] "Whether curvature reflects a causal locus of computation remains unclear." Why unresolved: The study relied on observational forward-pass analysis without intervention. What evidence would resolve it: Patching or ablating high-curvature layers to observe behavioral changes.

- **Open Question 2:** Do concern-induced geometric patterns scale to larger model families? Basis: [explicit] "A systematic scaling sweep is needed" beyond the 1B–3B range. Why unresolved: Experiments were restricted to Gemma3-1b and LLaMA3.2-3b. What evidence would resolve it: Applying curvature metrics to models with 70B+ parameters.

- **Open Question 3:** Is curvature specific to semantic concern or reactive to syntactic shifts? Basis: [explicit] Metrics "may respond to syntactic or lexical changes" without "null controls." Why unresolved: No scrambled or synonym-swapped baselines were tested to isolate semantic effects. What evidence would resolve it: Testing against scrambled-token or synonym-swapped null baselines.

## Limitations

- **Metric Construction Assumptions:** The pullback metric G = U^⊤U assumes the unembedding matrix meaningfully encodes token-level relationships; no direct validation against alternatives is provided.
- **Model and Domain Specificity:** Strong LLaMA scaling effects may be architecture-specific rather than universal; only two models were tested.
- **Statistical Aggregation:** P-value calculations for moderate vs. strong concern effects lack full specification (e.g., paired vs. unpaired tests, aggregation level).

## Confidence

- **High Confidence:** The geometric framework (curvature κ, salience S(t)) is mathematically sound and the finite-difference implementation is standard.
- **Medium Confidence:** Empirical claim that concern-shifted prompts induce measurable curvature is supported, but magnitude and consistency across models require further validation.
- **Low Confidence:** Claim that G = U^⊤U is optimal for semantic geometry is not empirically defended; interpretation of curvature as universal diagnostic is speculative.

## Next Checks

1. **Metric Ablation:** Replicate analysis using alternative semantic metrics (e.g., attention-weighted embeddings, learned semantic spaces) and compare curvature detection power to G = U^⊤U baseline.

2. **Architecture Scaling:** Extend experiment to broader set of models (GPT-2, OPT, Mistral) with varying depths/widths to test whether LLaMA scaling effect generalizes.

3. **Cross-Domain Robustness:** Generate prompt sets in additional semantic domains (scientific reasoning, humor, deception) to assess whether curvature remains reliable signal across diverse tasks.