---
ver: rpa2
title: 'SRDiffusion: Accelerate Video Diffusion Inference via Sketching-Rendering
  Cooperation'
arxiv_id: '2505.19151'
source_url: https://arxiv.org/abs/2505.19151
tags:
- diffusion
- arxiv
- quality
- video
- cogvideox
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SRDiffusion, a novel framework for accelerating
  video diffusion inference by leveraging cooperation between large and small models.
  The method uses a large model during high-noise steps for strong semantic and motion
  fidelity (Sketching) and a smaller model during low-noise steps for faster, high-quality
  detail refinement (Rendering).
---

# SRDiffusion: Accelerate Video Diffusion Inference via Sketching-Rendering Cooperation

## Quick Facts
- **arXiv ID:** 2505.19151
- **Source URL:** https://arxiv.org/abs/2505.19151
- **Reference count:** 40
- **Primary result:** Over 3× speedup on Wan and 2× on CogVideoX with negligible quality loss on VBench.

## Executive Summary
SRDiffusion introduces a novel framework for accelerating video diffusion inference by partitioning the denoising process between a large model (for high-noise steps) and a small model (for low-noise steps). The method leverages the observation that large models excel at preserving semantic fidelity and motion during the early denoising steps, while small models are sufficient for refining details in later steps. An adaptive switching mechanism determines the optimal transition point based on the rate of change in the denoised sample, enabling seamless cooperation between models that share a common VAE architecture. Experiments demonstrate significant speedup while maintaining quality, outperforming existing acceleration methods.

## Method Summary
SRDiffusion accelerates video diffusion inference by coordinating between a large model (Sketching) and a small model (Rendering) based on noise level dynamics. The large model handles high-noise steps to ensure semantic and motion fidelity, while the small model refines visual details in low-noise steps. An adaptive switching mechanism monitors the second-order derivative of noise differences between consecutive timesteps, transitioning from the large to small model when the derivative drops below a threshold δ (e.g., 0.01) and remains positive. Both models share the same VAE architecture, allowing direct handoff of the latent representation without alignment layers. The framework achieves 3× speedup on Wan and 2× on CogVideoX while preserving VBench scores.

## Key Results
- Achieves over 3× speedup on Wan and 2× on CogVideoX video models
- Maintains VBench quality scores with negligible loss (within 0.5 points)
- Outperforms existing acceleration methods like PAB and TeaCache in both efficiency and quality preservation
- Compatible with other acceleration techniques, providing orthogonal benefits

## Why This Works (Mechanism)

### Mechanism 1: Semantic-Complexity Asymmetry
Large video diffusion models significantly outperform smaller counterparts in semantic adherence and motion fidelity during high-noise steps, but offer diminishing returns for detail refinement in low-noise steps. Perturbation analysis shows that disrupting high-noise steps alters composition significantly, while disrupting low-noise steps only affects texture. This creates an opportunity to offload detail refinement to smaller, faster models without sacrificing quality.

### Mechanism 2: Latent Space Handoff via Shared VAE
Seamless switching between models is possible without intermediate projection layers if they share the same Variational Autoencoder (VAE). Since the VAE (compression rate and latent space) is identical across model sizes in families like Wan and CogVideoX, the output latent from the large model can be directly fed as input to the small model without transformation.

### Mechanism 3: Adaptive Switching via Noise Dynamics
The optimal transition point from sketching to rendering varies by prompt and can be detected by monitoring the rate of change in the denoised sample. The method monitors the second-order derivative of noise difference ($D_{deriv}^t = D_t - D_{t+1}$), switching when this metric drops below a threshold and indicates that high-frequency structural changes have stabilized.

## Foundational Learning

- **DDIM / Flow Matching Schedulers:** The framework relies on discrete timesteps and the relationship between noise levels. Understanding how schedulers map latent noise to clean images is critical to grasp why high-noise steps control structure.
  - *Quick check:* Does the scheduler operate on the pixel space or the latent space, and what does the timestep $t$ represent in terms of signal-to-noise ratio?

- **3D Variational Autoencoders (VAE) in Video DiTs:** The compatibility of SRDiffusion depends entirely on the large and small models sharing a VAE. Understanding the compression factors is critical for debugging shape mismatches during the handoff.
  - *Quick check:* If the Large Model outputs a latent of shape `[B, C, T, H, W]`, can you swap to a Small Model that uses a different temporal compression factor $C_T$?

- **VBench Evaluation Dimensions (Semantic vs. Quality):** The paper's hypothesis rests on the divergence between "Semantic Score" and "Quality Score." You need to distinguish between "subject consistency" (quality) and "multiple objects" (semantic) to interpret the results.
  - *Quick check:* If a video renders high-texture details but ignores the prompt's instruction about object placement, which dimension fails?

## Architecture Onboarding

- **Component map:** Text Encoder -> Sketching Model (Large DiT) -> Rendering Model (Small DiT) -> 3D VAE Decoder
- **Critical path:**
  1. Initialize noise $z_T$
  2. Run Large Model loop: denoise $z_t \to z_{t-1}$
  3. **Check condition:** Compute L1 distance between current and previous denoised estimation
  4. **Handoff:** If condition met, unload Large Model and load Small Model
  5. Run Small Model loop: denoise $z_{t-1} \to z_0$

- **Design tradeoffs:**
  - **Threshold $\delta$:** Lower $\delta$ = later switch = higher quality but lower speedup
  - **Model Selection:** Speedup ratio is bounded by latency ratio of Large vs. Small models

- **Failure signatures:**
  - **Semantic Drift:** Video morphs into different subject after switch (switching too early or small model too weak)
  - **Artifacts at Step $t$:** Visual glitches appear exactly at transition frame (latent space mismatch)
  - **No Speedup:** Latency remains high (model loading overhead dominates compute savings)

- **First 3 experiments:**
  1. **Fixed-Step Baseline:** Run with hard-coded switch step (e.g., step 10 of 50) to isolate sketching-rendering logic
  2. **Ablation on $\delta$:** Sweep $\delta$ values (0.01, 0.02, 0.03) on fixed prompt set to plot VBench Score vs. Latency
  3. **Latent Consistency Check:** Compare hybrid output against large-only and small-only baselines to verify semantic and detail inheritance

## Open Questions the Paper Calls Out

- **Generalization to different VAEs:** Currently focuses on models within the same family using a shared VAE; future work plans to explore cross-model cooperation by aligning latent spaces
- **Application to UNet architectures:** The semantic/detail decomposition observed in DiT architectures may not hold true for UNet-based video diffusion models
- **Learned switching optimization:** The current heuristic threshold $\delta$ could potentially be optimized via training or reinforcement learning for more accurate transition point prediction

## Limitations

- Method requires model families with shared VAE architectures, limiting generalizability
- Performance depends on finding model pairs with sufficient capacity differences
- The optimal switching threshold may require per-domain tuning for different prompt complexities

## Confidence

- **Speedup Claims:** Medium confidence - promising results but based on single-point measurements with hardware-dependent latency ratios
- **Quality Preservation:** High confidence - well-supported by experimental results maintaining VBench scores
- **Generalizability:** Low confidence - demonstrated only on Wan and CogVideoX models with specific architectural constraints

## Next Checks

1. **Ablation Study on Threshold $\delta$:** Run inference with multiple $\delta$ values (0.005, 0.01, 0.02, 0.03) on diverse prompt set to quantify precision-recall tradeoff between quality and speedup.

2. **Latent Space Consistency Verification:** Generate videos using only large model, only small model, and SRDiffusion hybrid; compute LPIPS and PSNR between hybrid output and each baseline to verify semantic and detail inheritance.

3. **Cross-Model Family Generalization:** Apply SRDiffusion framework to different model family (e.g., Stable Video Diffusion variant); document whether adaptive switching successfully identifies transition points and preserves quality across different VAE configurations.