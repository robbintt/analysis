---
ver: rpa2
title: 'SuperCoder: Assembly Program Superoptimization with Large Language Models'
arxiv_id: '2505.11480'
source_url: https://arxiv.org/abs/2505.11480
tags:
- code
- arxiv
- assembly
- program
- speedup
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models can act as
  superoptimizers, generating assembly programs that outperform code already optimized
  by industry-standard compilers. The authors construct a large-scale benchmark of
  8,072 assembly programs (averaging 130 lines) and evaluate 23 LLMs on this task.
---

# SuperCoder: Assembly Program Superoptimization with Large Language Models

## Quick Facts
- **arXiv ID**: 2505.11480
- **Source URL**: https://arxiv.org/abs/2505.11480
- **Reference count**: 26
- **Primary result**: LLMs can act as superoptimizers, achieving 1.46x average speedup over gcc -O3 with 95% correctness

## Executive Summary
This paper investigates whether large language models can function as superoptimizers for assembly programs, generating code that outperforms standard compiler optimizations. The authors construct a benchmark of 8,072 assembly programs (averaging 130 lines) and evaluate 23 LLMs, finding that reinforcement learning fine-tuned models significantly outperform baseline approaches. The resulting SuperCoder model achieves 95% correctness and 1.46x average speedup over gcc -O3, demonstrating that LLMs can discover novel assembly optimizations beyond traditional compiler heuristics.

## Method Summary
The authors create a large-scale benchmark of 8,072 x86-64 assembly programs and evaluate 23 different LLMs on superoptimization tasks. The strongest baseline (Claude-opus-4) achieves 51.5% test-passing rate and 1.43x speedup. To improve performance, they develop SuperCoder through reinforcement learning fine-tuning using a reward function that combines correctness and performance metrics. The approach incorporates Best-of-N sampling and iterative refinement to further enhance optimization quality. The evaluation framework tests both functional correctness and performance improvements against gcc -O3 optimization level.

## Key Results
- SuperCoder achieves 95.0% test-passing rate versus 51.5% for best baseline
- SuperCoder provides 1.46x average speedup over gcc -O3 optimization
- Reinforcement learning fine-tuning with combined reward function proves effective for superoptimization

## Why This Works (Mechanism)
The approach works by leveraging LLMs' pattern recognition capabilities to discover novel optimization strategies that may not be captured by traditional compiler heuristics. Through reinforcement learning with carefully designed reward functions that balance correctness and performance, the models learn to generate assembly code that maintains functionality while improving execution efficiency. The iterative refinement and Best-of-N sampling strategies allow the model to explore multiple optimization paths and converge on superior solutions.

## Foundational Learning

**Assembly language optimization**: Understanding low-level code transformations and instruction scheduling is essential because superoptimization requires identifying performance bottlenecks and opportunities at the instruction level. Quick check: Can the reader explain the difference between instruction-level parallelism and memory access optimization?

**Compiler optimization techniques**: Knowledge of gcc -O3 and other optimization strategies provides the baseline against which superoptimization must compete. Quick check: Can the reader list three common compiler optimization techniques (e.g., loop unrolling, inlining, dead code elimination)?

**Reinforcement learning for code generation**: Understanding RL concepts like reward shaping, policy gradients, and exploration-exploitation tradeoffs is crucial since the approach uses RL fine-tuning to improve optimization quality. Quick check: Can the reader explain how reward functions guide the optimization process?

## Architecture Onboarding

**Component map**: Benchmark generator -> LLM inference -> Correctness validation -> Performance measurement -> Reward computation -> RL fine-tuning -> SuperCoder deployment

**Critical path**: Program input → LLM generation → Functional testing → Performance benchmarking → Reward calculation → Model update (for RL phase)

**Design tradeoffs**: The authors balance between exploration (generating diverse solutions) and exploitation (refining known good patterns), choosing RL fine-tuning over supervised learning to enable discovery of novel optimizations rather than just mimicking existing code.

**Failure signatures**: Common failures include functional correctness issues (broken logic), performance regressions (slower code), and architecture-specific incompatibilities (x86-64 assumptions that don't generalize).

**3 first experiments**: 1) Run baseline LLM on small benchmark subset to establish performance baseline, 2) Implement and test reward function on known optimizations, 3) Validate RL fine-tuning converges on functional correctness before optimizing for speed.

## Open Questions the Paper Calls Out
None

## Limitations
- Results focus specifically on x86-64 architecture, limiting generalizability to ARM, RISC-V, or other ISAs
- Benchmark programs average 130 lines, raising questions about scalability to larger, more complex codebases
- Performance improvements measured against gcc -O3 may not reflect commercial compiler optimizations

## Confidence

**Correctness claims**: High confidence - 95% test-passing rate provides strong empirical evidence
**Speedup claims**: Medium confidence - results may vary with different hardware, compiler versions, and benchmark selections
**Scalability claims**: Medium confidence - approach validated on average 130-line programs but not extensively tested on larger codebases

## Next Checks

1. Cross-hardware validation: Test SuperCoder on ARM, RISC-V, and other architectures to assess generalizability beyond x86-64
2. Compiler version comparison: Evaluate against newer compiler versions (gcc 13+, clang 16+) and commercial compilers to establish the true optimization frontier
3. Long-term stability assessment: Measure performance consistency across multiple runs and varying input distributions to ensure optimization gains are not artifacts of specific test cases