---
ver: rpa2
title: 'GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge'
arxiv_id: '2512.14766'
source_url: https://arxiv.org/abs/2512.14766
tags:
- answer
- reasoning
- paths
- question
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GR-Agent, a training-free adaptive agent
  for knowledge graph question answering under incomplete knowledge graphs. The authors
  propose a benchmark construction methodology that removes direct supporting triples
  while preserving alternative reasoning paths, enabling rigorous evaluation of reasoning
  ability.
---

# GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge

## Quick Facts
- arXiv ID: 2512.14766
- Source URL: https://arxiv.org/abs/2512.14766
- Reference count: 40
- Primary result: GR-Agent achieves the highest Hard Hits Rate among non-training methods on incomplete KGQA benchmarks

## Executive Summary
This paper introduces GR-Agent, a training-free adaptive agent for knowledge graph question answering under incomplete knowledge graphs. The authors propose a benchmark construction methodology that removes direct supporting triples while preserving alternative reasoning paths, enabling rigorous evaluation of reasoning ability. GR-Agent operates over an interactive KG environment using three tools: relation-path exploration, reasoning-path grounding, and answer synthesis. Experiments on Family and FB15k-237 benchmarks show GR-Agent outperforms non-training baselines and achieves performance comparable to training-based methods, with the highest Hard Hits Rate among non-trained approaches, demonstrating robust reasoning under incompleteness.

## Method Summary
GR-Agent is a training-free agent that performs KGQA by interacting with an incomplete knowledge graph through three tools: `aexplore` for relation-path exploration using BFS, `aground` for grounding abstract paths to reasoning chains, and `asynthesis` for answer generation. The agent maintains a state memory of explored paths, grounded chains, and frontier entities. The benchmark is constructed by mining Horn rules with AMIE3, removing head triples while preserving body triples, and generating natural language questions. The agent's LLM controller selects tools iteratively to reconstruct missing answers through multi-hop reasoning.

## Key Results
- GR-Agent outperforms non-training baselines (ToG, StructGPT) on both Family and FB15k-237 benchmarks
- Achieves Hard Hits Rate of 59.13% on Family and 46.29% on FB15k-237, highest among non-trained methods
- Demonstrates comparable performance to training-based methods while being training-free
- Shows strong robustness to incompleteness with consistent performance across different rule types

## Why This Works (Mechanism)

### Mechanism 1: Abstract Relation-Path Exploration
The agent first executes `aexplore` to identify abstract relation sequences via BFS, decoupling structural pattern discovery from entity retrieval. This allows the agent to identify potential inference paths even when the target fact is absent, assuming the necessary logical structure exists in the graph.

### Mechanism 2: Iterative Frontier Expansion via State Memory
The agent maintains a state tuple (P, C, E) where new entities discovered during grounding are added to the frontier set. This allows iterative chaining of inferences rather than requiring single-hop retrieval, with the LLM deciding when to stop exploring and synthesize an answer.

### Mechanism 3: Rule-Based Benchmark Constraints
The benchmark uses AMIE3-mined Horn rules (B ⇒ H), deleting head triples while keeping body triples. Performance gains demonstrate the model's ability to reconstruct missing conclusions from preserved supporting facts, though this depends on the validity of the underlying rules.

## Foundational Learning

- **Concept:** Horn Clauses & Rule Mining (AMIE3)
  - **Why needed here:** Understanding how "inferable" facts are defined is crucial since the entire evaluation relies on the assumption that B ⇒ H represents valid logical inference
  - **Quick check question:** If a rule A -> B has confidence 0.4, does removing B guarantee the answer is inferable via A? (Answer: No, it's probabilistic)

- **Concept:** Breadth-First Search (BFS) in Graphs
  - **Why needed here:** The `explore` tool uses BFS to generate relation paths, and understanding complexity is key since a hop limit of 3 on a dense node can explode the search space
  - **Quick check question:** Why does the agent limit the hop count H during exploration? (Answer: To manage computational cost and combinatorial explosion of paths)

- **Concept:** Agent State Machines (MDP Formalism)
  - **Why needed here:** GR-Agent is formalized as a tuple (S, A, T), and recognizing this pattern helps separate the LLM (the policy) from the Environment (the KG tools)
  - **Quick check question:** In this framework, is the LLM part of the State or the Transition function? (Answer: It acts as the policy selecting actions, while T is the mechanical execution of tools)

## Architecture Onboarding

- **Component map:** LLM (Planner) -> Environment (KG tools) -> State Store (P, C, E) -> LLM (Synthesis)
- **Critical path:** User Query + Topic Entity -> `aexplore` (Returns abstract paths) -> Select promising paths -> `aground` (Returns chains) -> Update State -> `asynthesis` (LLM outputs answer)
- **Design tradeoffs:** High hop limit finds deep paths but introduces noise and latency; training-free approach relies heavily on LLM's tool-use instruction following
- **Failure signatures:** Premature synthesis after 1-hop on 3-hop questions; path hallucination where structurally valid paths are semantically irrelevant
- **First 3 experiments:** 1) Sanity Check: Verify `aexplore` returns required relation paths for test questions; 2) Grounding Accuracy: Check if `aground` correctly identifies target entities in controlled subgraphs; 3) Inference Loop: Construct incomplete KG and trace state updates to confirm alternative path discovery

## Open Questions the Paper Calls Out

- **Question:** Can reinforcement learning (RL) effectively optimize GR-Agent's tool usage to improve reasoning efficiency?
  - **Basis in paper:** [explicit] The Conclusion states plans to extend GR-Agent with reinforcement learning to optimize tool usage and further improve reasoning efficiency
  - **Why unresolved:** Current agent is training-free; integrating RL requires defining a reward structure for multi-hop graph navigation without destabilizing adaptive exploration
  - **What evidence would resolve it:** Experiments showing RL-enhanced GR-Agent achieves comparable or better accuracy with fewer interaction steps than current version

## Limitations

- The evaluation framework relies on AMIE3-mined rules, but low-confidence rules (e.g., 0.3) may represent spurious correlations rather than valid logical inferences
- The benchmark construction's downsampling threshold τ for balancing answer distributions is unspecified, potentially affecting reported metrics
- The reliance on anonymized entity IDs may introduce noise if the LLM struggles to track relationships across hops without semantic context

## Confidence

- **High Confidence:** GR-Agent achieves highest Hard Hits Rate among non-trained methods (well-supported by Table 2 experimental results)
- **Medium Confidence:** Abstract relation-path exploration improves robustness (plausible given mechanism but depends on rule quality)
- **Low Confidence:** Performance gains are driven solely by rule-based benchmark constraints (uncertain as LLM reasoning capability and prompt design also play significant roles)

## Next Checks

1. **Rule Confidence Validation:** Test GR-Agent on high-confidence rules (confidence > 0.6) versus low-confidence rules to quantify the impact of rule quality on performance
2. **Semantic Context Ablation:** Replace anonymized IDs with entity labels in controlled experiment to assess whether semantic context improves multi-hop path tracking
3. **Exploration Budget Sensitivity:** Systematically vary hop limit H to identify optimal exploration depth for incomplete KGQA by observing trade-off between Hits@Any and Hard Hits Rate