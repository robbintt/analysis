---
ver: rpa2
title: Towards Budget-Friendly Model-Agnostic Explanation Generation for Large Language
  Models
arxiv_id: '2505.12509'
source_url: https://arxiv.org/abs/2505.12509
tags:
- qwen
- gpt-4o
- explanations
- llama
- proxy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational cost of generating
  model-agnostic explanations for large language models (LLMs) by introducing a budget-friendly
  proxy framework. The method leverages smaller, efficient models to approximate the
  decision boundaries of expensive LLMs, using a statistical screen-and-apply mechanism
  to ensure fidelity before deployment.
---

# Towards Budget-Friendly Model-Agnostic Explanation Generation for Large Language Models

## Quick Facts
- arXiv ID: 2505.12509
- Source URL: https://arxiv.org/abs/2505.12509
- Authors: Junhao Liu; Haonan Yu; Xin Zhang
- Reference count: 40
- Primary result: Proxy explanations achieve >90% fidelity while reducing costs by 88.2% compared to oracle explanations

## Executive Summary
This paper addresses the high computational cost of generating model-agnostic explanations for large language models (LLMs) by introducing a budget-friendly proxy framework. The method leverages smaller, efficient models to approximate the decision boundaries of expensive LLMs, using a statistical screen-and-apply mechanism to ensure fidelity before deployment. Experiments across 12 state-of-the-art LLMs and seven diverse tasks demonstrate that proxy explanations achieve over 90% fidelity while reducing costs by 88.2%. The framework also proves effective in downstream optimization tasks, such as prompt compression and poisoned example removal, enabling actionable interpretability for LLM development.

## Method Summary
The framework generates model-agnostic explanations for expensive LLMs by using smaller, budget-friendly models as proxies. It employs a two-stage screening mechanism: task-level screening uses sequential hypothesis testing to verify if proxy explanations meet fidelity thresholds, while instance-level screening ensures prediction agreement between proxy and target models before applying proxy explanations. The method uses perturbation-based attribution (LIME or Kernel SHAP) with 1,000 samples per instance and implements fallback to oracle when screening fails.

## Key Results
- Proxy explanations achieve over 90% fidelity compared to oracle explanations across 12 state-of-the-art LLMs
- Cost reduction of 88.2% achieved through the proxy framework
- Proxy explanations demonstrate actionable utility in downstream tasks like prompt compression (91.7% of oracle performance) and poisoned example removal (comparable to oracle results)
- Task-level screening achieves 98.9% precision on average, with false positives still exceeding 89% of oracle fidelity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller, budget-friendly LLMs can serve as proxies to approximate the local decision boundaries of expensive LLMs for explanation generation.
- Mechanism: Leverages the observed "homogeneity among LLMs" where different models exhibit similar local decision behavior on similar inputs. By querying a budget-friendly proxy model instead of the expensive target, the framework generates perturbation-based explanations (via LIME or Kernel SHAP) that approximate what would be obtained from the oracle.
- Core assumption: Local decision boundaries of proxy and target models are sufficiently aligned for the task at hand. This homogeneity holds generally but is not guaranteed for every instance.
- Evidence anchors:
  - [abstract] "By leveraging the homogeneity among LLMs, the authors use smaller, budget-friendly models as proxies to approximate the decision boundaries of expensive, closed-source models like GPT-4o."
  - [section 2.1] "recent research reveals the homogeneity among state-of-the-art LLMs... different LLMs are obtained to behave similarly on similar inputs in many scenarios. This observation motivated our methodology."
  - [corpus] Weak direct evidence. Related work on LLM homogeneity is cited (Jiang et al., 2025; Lee et al., 2025; Zhou et al., 2024), but corpus papers do not directly validate this specific transfer mechanism.
- Break condition: Homogeneity may weaken for tasks requiring extreme reasoning capabilities (e.g., complex symbolic logic), causing misalignment between proxy and oracle explanations.

### Mechanism 2
- Claim: A two-stage screening mechanism statistically verifies alignment between proxy and target models, ensuring reliable explanation transfer.
- Mechanism: **Task-level screening** (offline): Sequential one-sided paired t-test on fidelity differences between proxy and oracle explanations. If the confidence interval for mean fidelity difference lies entirely above zero (with threshold τ=0.9), the proxy is approved. **Instance-level screening** (online): For each input, check if proxy and target produce the same prediction; if not, fall back to the oracle. This acts as a "safety valve" filtering cases where local decision behavior diverges.
- Core assumption: Agreement on the prediction label implies sufficient alignment of local decision boundaries for explanation purposes. Disagreement indicates different local decision behavior.
- Evidence anchors:
  - [abstract] "A key innovation is the screen-and-apply mechanism, which ensures reliability by statistically verifying the alignment between proxy and target model explanations before deployment."
  - [section 3.1] "We conduct a sequential one-sided paired t-test on the paired differences... At step n we update the sample mean and variance... If the entire interval lies above zero, we accept H₁."
  - [section 4.3.2] Table 3 shows screening achieves 98.9% precision on average; false positives still exceed 89% of oracle fidelity.
- Break condition: High-precision screening with lower recall means some suitable proxy models may be rejected, requiring users to screen multiple candidates.

### Mechanism 3
- Claim: Proxy explanations maintain actionable utility for downstream optimization tasks, effectively guiding prompt compression and poisoned example removal.
- Mechanism: Attribution vectors from proxy models identify which input tokens contribute most/least to predictions. For prompt compression, least important examples are iteratively removed. For poisoned example removal, negatively attributed examples are flagged and removed. The proxy explanations achieve comparable effectiveness to oracle explanations.
- Core assumption: Feature attributions from proxy models correlate with those from the target model sufficiently to guide the same optimization decisions.
- Evidence anchors:
  - [abstract] "the proxy explanations demonstrate actionable utility in downstream tasks such as prompt compression and poisoned example removal"
  - [section 5.1, Table 4] Proxy explanations achieve 91.7% of oracle's performance in prompt compression, outperforming random deletion and state-of-the-art methods (AttnComp, LLMLingua).
  - [section 5.2, Table 5] For poisoned example removal, proxy explanations recover GPT-4o accuracy to 94.0% (SST), 93.5% (HellaSwag), 90.7% (PIQA)—comparable to oracle at 94.2%, 93.7%, 91.5%.
- Break condition: Optimization effectiveness depends on task type; results vary across subjects (social sciences show higher fidelity than natural sciences per Appendix G analysis).

## Foundational Learning

- Concept: **Model-Agnostic Local Explanation (LIME/SHAP)**
  - Why needed here: The entire framework builds on perturbation-based attribution methods that query models with locally perturbed samples to estimate feature importance.
  - Quick check question: Can you explain why LIME requires ~1,000 perturbed samples per explanation, and how the local surrogate model approximates the target model's behavior around a specific input?

- Concept: **Statistical Hypothesis Testing (Sequential t-test)**
  - Why needed here: Task-level screening uses a sequential one-sided paired t-test to determine if proxy fidelity meets the threshold τ with confidence level 1-δ.
  - Quick check question: Given paired fidelity differences dᵢ = q_proxy(xᵢ) - τ·q_oracle(xᵢ), how would you determine whether to accept H₁ (proxy is sufficient) based on confidence intervals?

- Concept: **Cost-Fidelity Tradeoff in Explanation Generation**
  - Why needed here: The framework's value proposition hinges on achieving >90% fidelity at ~11% of oracle cost; understanding this tradeoff is essential for practical deployment.
  - Quick check question: If a proxy model costs 1/10th of GPT-4o per query but achieves only 85% fidelity, would it pass task-level screening with τ=0.9? What factors determine the break-even point?

## Architecture Onboarding

- Component map:
  Buffer System -> Task-Level Screening Module -> Instance-Level Screening Module -> Explanation Generator -> Fallback Handler

- Critical path:
  1. Select candidate proxy models from budget-friendly pool (e.g., Qwen 2.5 7B/14B, LLaMA 3.1 8B)
  2. Run parallel task-level screening across candidates using shared buffer
  3. For passing proxies, apply instance-level screening per input
  4. Generate proxy explanations for screened inputs; fall back to oracle for disagreements
  5. Aggregate results with cost calculation: CRR = C_oracle / (C_proxy + C_fallback + C_screen)

- Design tradeoffs:
  - **Precision vs Recall in Screening**: High precision (98.9%) ensures fidelity but may reject suitable proxies. Solution: Screen multiple candidates in parallel since budget-friendly models are cheap
  - **Cost vs Coverage**: Instance-level fallback ensures fidelity but reduces savings when proxy-target disagreement is high. The paper reports 88.2% cost reduction overall
  - **Local vs API Deployment**: Running proxies locally (CRR_local) achieves higher cost reduction than API access (CRR_max), but requires GPU infrastructure

- Failure signatures:
  - **Complex reasoning tasks**: Alignment weakens for symbolic logic or extreme reasoning (per Section 6 limitations)
  - **High disagreement rate**: If proxy and target frequently disagree on predictions, instance-level screening triggers excessive fallback, negating cost benefits
  - **Natural science domains**: Lower oracle explanation fidelity observed in chemistry/physics vs social sciences (per Appendix G)

- First 3 experiments:
  1. **Baseline fidelity check**: For your target LLM and task, generate oracle explanations on a small validation set (n=50). Compute baseline fidelity to establish the upper bound
  2. **Multi-proxy screening**: Run task-level screening on 3-5 candidate proxy models in parallel. Measure precision/recall against held-out ground truth. Identify the highest-fidelity passing proxy
  3. **Cost-ablation study**: Compare CRR_mean (average across passing proxies), CRR_max (best passing proxy via API), and CRR_local (best proxy run locally) to determine deployment strategy

## Open Questions the Paper Calls Out
None

## Limitations
- Homogeneity assumption may break down for tasks requiring extreme reasoning capabilities like complex symbolic logic
- High-precision screening with lower recall means some suitable proxy models may be rejected, requiring multiple candidates to be screened
- Variability in optimization effectiveness across domains, with natural sciences showing lower fidelity than social sciences

## Confidence
- **Medium Confidence**: Core claim relies on empirical observation of LLM homogeneity rather than theoretical guarantee; break condition identified but boundaries not fully characterized
- **Low Confidence**: Practical deployment guidance lacks optimal proxy selection strategies, parameter tuning guidance beyond tested values, and clear recommendations for local vs API deployment
- **Medium Confidence**: Downstream optimization results show unexplained variability across domains without mechanistic explanations

## Next Checks
1. **Task-Homogeneity Boundary Testing**: Systematically test the proxy framework across a gradient of reasoning complexity to empirically map boundaries where LLM homogeneity breaks down
2. **Multi-Threshold Screening Experiment**: Replicate task-level screening with varying τ values (0.8, 0.85, 0.95) to understand precision-recall tradeoff and identify optimal thresholds
3. **Cross-Domain Fidelity Analysis**: Conduct detailed analysis of why social sciences show higher proxy fidelity than natural sciences to clarify when proxy explanations are most reliable for optimization tasks