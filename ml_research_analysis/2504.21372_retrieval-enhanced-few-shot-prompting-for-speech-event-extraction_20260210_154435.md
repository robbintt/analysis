---
ver: rpa2
title: Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction
arxiv_id: '2504.21372'
source_url: https://arxiv.org/abs/2504.21372
tags:
- event
- extraction
- language
- speech
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a pipeline-based framework for speech event
  extraction that combines ASR transcription with LLM-driven few-shot learning enhanced
  by semantic search. The system first filters utterances likely to contain events
  using rule-based, BERT-based, and LLM-based classifiers, then extracts event triggers
  and arguments via retrieval-augmented LLM prompting.
---

# Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction

## Quick Facts
- arXiv ID: 2504.21372
- Source URL: https://arxiv.org/abs/2504.21372
- Reference count: 9
- Primary result: Pipeline achieves 63.3% F1 on trigger classification using o1-mini with retrieval-augmented prompting

## Executive Summary
This paper introduces a modular pipeline approach for speech event extraction that combines ASR transcription with LLM-driven few-shot learning enhanced by semantic search. The system first filters utterances likely to contain events using a consensus of rule-based, BERT-based, and LLM-based classifiers, then extracts event triggers and arguments via retrieval-augmented LLM prompting. Evaluated on the SpeechEE benchmark, the approach achieves strong performance while maintaining interpretability and modularity compared to end-to-end systems.

## Method Summary
The pipeline processes audio through Whisper or Canary ASR, then applies a three-way filtering ensemble (rule-based lexicon, BERT fine-tuning, and o1-mini classifier) to identify high-probability event candidates. For filtered transcripts, the system retrieves top-10 semantically similar training examples using FAISS with sentence-transformer embeddings, constructs few-shot prompts, and uses LLMs (Llama3, GPT-4o-mini, or o1-mini) to extract event triggers and arguments. A final post-processing step formats outputs as JSON.

## Key Results
- Trigger Classification (TC) F1: 63.3% using o1-mini (best model)
- Argument Classification (AC) F1: 27.8% using o1-mini (best model)
- The three-vote consensus filter consistently outperforms single classifiers across ablation studies
- o1-mini significantly outperforms standard models (47.4% vs 63.3% F1) primarily through improved recall

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A multi-model consensus filter significantly reduces false positives by isolating high-probability event candidates before extraction.
- **Mechanism:** The system routes utterances through three diverse classifiers (Rule-based, BERT-based, LLM-based). By requiring agreement before extraction, the pipeline filters out non-event noise that typically confuses LLMs.
- **Core assumption:** The capabilities of rule-based lexicons, fine-tuned transformers (BERT), and generative LLMs are complementary and sufficiently distinct that their intersection yields higher precision.
- **Evidence anchors:** [abstract] Mentions a "hybrid filtering mechanism including rule-based, BERT-based, and LLM-based models." [section 6] (Ablation): Tables 3 and 4 show that the "three" criterion consistently yields the highest or near-highest F1 scores.

### Mechanism 2
- **Claim:** Dynamic example retrieval (RAG) outperforms static few-shot prompting by providing contextually relevant schemas.
- **Mechanism:** Instead of fixed examples, the system embeds the input transcript and retrieves the top-10 most similar examples from the training set using FAISS.
- **Core assumption:** The embedding space of the sentence-transformer (`all-MiniLM-L6-v2`) correlates well with the structural complexity of event extraction tasks.
- **Evidence anchors:** [section 4.4] Details the retrieval of top ten examples using FAISS to "improve robustness and generalization." [abstract] Attributes performance gains to "semantic search-enhanced prompting."

### Mechanism 3
- **Claim:** Specialized reasoning models (e.g., o1-mini) mitigate the precision-recall trade-off common in standard LLMs.
- **Mechanism:** The paper observes that standard models (GPT-4o-mini) favor precision, while o1-mini maintains high precision while significantly improving recall.
- **Core assumption:** The architectural improvements in "reasoning" models allow them to handle the noise and disfluencies of speech transcripts better than standard instruction-tuned models.
- **Evidence anchors:** [section 5] (Results): Reports that o1-mini achieved 63.3% F1 (TC) vs GPT-4o-mini's 47.4%, largely driven by a massive increase in Recall (59.2% vs 36.6%).

## Foundational Learning

- **Concept: Event Extraction (EE) Ontology**
  - **Why needed here:** The system must distinguish between a **Trigger** (the word signaling the event, e.g., "election") and **Arguments** (entities involved, e.g., "man" as "Person").
  - **Quick check question:** Given the sentence "The plane landed in Paris," can you identify the Trigger, the Event Type (Transport), and the Argument with its role?

- **Concept: ASR Error Propagation**
  - **Why needed here:** This is the primary motivation for the pipeline architecture. The LLM must be robust to transcription errors (e.g., "plane" transcribed as "plan").
  - **Quick check question:** How would a phonetic error (e.g., "plant" vs "plan") change the event type classification, and can the LLM context correct it?

- **Concept: Dense Retrieval (Embeddings)**
  - **Why needed here:** The core engine of the few-shot learning is not keyword matching but vector similarity using `all-MiniLM-L6-v2`.
  - **Quick check question:** If the input transcript is "The bomb exploded downtown," would a dense retriever prioritize the word "bomb" (keyword) or the context of urban conflict (semantic)?

## Architecture Onboarding

- **Component map:** Raw Audio -> ASR (Whisper/Canary) -> Filter Ensemble (Rule + BERT + LLM) -> Retrieval Database (FAISS) -> Extraction Engine (LLM + Dynamic Prompt) -> Post-Processor (JSON output)

- **Critical path:** The **Filtering Ensemble** is the primary bottleneck for latency and the main guard against false positives. If this module is too aggressive, the system fails to extract events (Low Recall); if too loose, the system hallucinates events (Low Precision).

- **Design tradeoffs:**
  - **Latency vs. Accuracy:** The "three-vote" consensus requires three forward passes (Rule, BERT, LLM) per audio segment plus the final extraction LLM call.
  - **Cost vs. Performance:** Using o1-mini for the filter and extractor yields the best F1 (63.3%) but is likely cost-prohibitive compared to local Llama3-8B (F1 33.9%).
  - **Modularity vs. End-to-End:** The pipeline decouples ASR from NLP, allowing easy swapping of Whisper/Canary, but risks error cascading if the ASR quality drops below the threshold the LLM can tolerate.

- **Failure signatures:**
  - **Format Drift:** Llama3-8B failed to comply with JSON output schemas, requiring an automated re-execution loop.
  - **False Positive Spikes:** Occurs if the filtering step is disabled or if the LLM extractor is applied to non-event transcripts without the voting gate.
  - **Missing Arguments:** AC performance is universally lower than TC (27.8% vs 63.3%), indicating the system struggles to link roles to entities effectively.

- **First 3 experiments:**
  1. **ASR Sensitivity Test:** Run the pipeline with Whisper vs. Canary on a subset of noisy audio. Verify the authors' claim that ASR choice has <1% impact on F1.
  2. **Filter Ablation:** Disable the filtering ensemble and run extraction directly on all transcripts. Measure the drop in Precision to quantify the value of the consensus gate.
  3. **Static vs. Dynamic Prompting:** Replace the FAISS semantic retrieval with a static set of 10 fixed examples. Compare F1 scores to quantify the contribution of the "Semantic Search-Enhanced" mechanism.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can incorporating direct audio-based features (e.g., prosody, speaker tone) into the pipeline improve extraction performance compared to text-only approaches?
  - **Basis in paper:** [explicit] The conclusion states that the work "opens pathways for future hybrid models combining textual and acoustic features," and the Discussion notes that the current "approach does not leverage speech-related cues present in the audio."

- **Open Question 2:** Can specific prompt engineering strategies, such as structured reasoning (Chain-of-Thought), close the performance gap between trigger detection and argument classification?
  - **Basis in paper:** [explicit] The Limitations section highlights that argument classification performance is low (27.8% F1) and suggests that "improving argument role assignment... requires further attention, potentially through the use of more structured reasoning."

- **Open Question 3:** To what extent is the superior performance of o1-mini attributable to its reasoning capabilities versus its model scale or training data?
  - **Basis in paper:** [inferred] The Discussion observes that "reasoning capability also largely seems to help the task," but admits that "to make these claims more confidently, a more thorough research would be needed across several models."

- **Open Question 4:** Can the pipeline be optimized for real-time application through model distillation or component reduction without significant loss in accuracy?
  - **Basis in paper:** [explicit] The Conclusion suggests "exploring methods to reduce computational requirements, such as model distillation or selective component invocation," and the Limitations section notes the system "introduces an additional layer of complexity and latency."

## Limitations

- The system's performance is heavily dependent on proprietary LLM APIs (o1-mini, GPT-4o-mini), making direct cost and latency estimates speculative.
- The reliance on the SpeechEE benchmark (derived from ACE2005-EN+) raises questions about generalizability to other event schemas or languages.
- The current pipeline discards all acoustic information after ASR transcription, relying solely on linguistic semantics.

## Confidence

**High Confidence (â˜‘ï¸)** - The multi-model consensus filter demonstrably reduces false positives. Table 3 and 4 ablation results clearly show that unanimous agreement ("three" criterion) outperforms single classifiers across models.

**Medium Confidence (ðŸŸ¡)** - The o1-mini performance advantage is real but mechanism is unclear. While the paper shows o1-mini achieving 63.3% F1 vs GPT-4o-mini's 47.4%, the authors attribute this to "reasoning" capabilities without specifying what architectural differences enable this.

**Low Confidence (ðŸ”´)** - The semantic search enhancement's contribution is difficult to isolate. While FAISS retrieval is implemented and described, the paper doesn't provide ablation results comparing static vs. dynamic few-shot prompting.

## Next Checks

1. **Cost-Latency Profile Assessment:** Run the pipeline end-to-end on a 100-sample subset with o1-mini for both filtering and extraction, measuring total cost (API calls Ã— pricing) and wall-clock latency per sample. Compare against Llama3-8B baseline to quantify the accuracy-cost tradeoff.

2. **ASR Error Tolerance Boundary:** Systematically degrade Whisper transcriptions with synthetic noise (word deletions, substitutions, insertions) at 5% increments up to 25% error rate. Measure TC/AC F1 degradation to identify the error threshold where the pipeline breaks down.

3. **Schema Generalization Test:** Apply the trained SpeechEE pipeline to a different event extraction benchmark (e.g., MAVEN or RAMS datasets) without fine-tuning the classifier or retrieval database. Measure TC/AC F1 to quantify domain adaptation capability.