---
ver: rpa2
title: 'Beyond English: The Impact of Prompt Translation Strategies across Languages
  and Tasks in Multilingual LLMs'
arxiv_id: '2502.09331'
source_url: https://arxiv.org/abs/2502.09331
tags:
- language
- english
- languages
- source
- pre-translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates optimal strategies for prompting multilingual
  large language models (LLMs) by systematically evaluating selective pre-translation
  approaches. The authors formalize prompts as modular components (instruction, context,
  examples, output) that can be selectively translated into English or kept in the
  source language.
---

# Beyond English: The Impact of Prompt Translation Strategies across Languages and Tasks in Multilingual LLMs

## Quick Facts
- arXiv ID: 2502.09331
- Source URL: https://arxiv.org/abs/2502.09331
- Reference count: 25
- Key outcome: Selective pre-translation of prompt components consistently outperforms full pre-translation or direct inference across 35 languages and four tasks

## Executive Summary
This paper investigates optimal strategies for prompting multilingual large language models by systematically evaluating selective pre-translation approaches. The authors formalize prompts as modular components that can be selectively translated into English or kept in the source language. Through experiments across 35 languages and four tasks (QA, NLI, NER, summarization) using three models, they demonstrate that selective pre-translation consistently outperforms both full pre-translation and direct inference in the source language, with particularly strong gains for low-resource languages. The results show that context and examples in the source language benefit extractive tasks, while output language preferences vary by task type.

## Method Summary
The authors conducted systematic experiments evaluating four translation strategies across 35 languages and four tasks: full pre-translation (all components translated to English), selective pre-translation (context and examples kept in source language, instruction and output translated), direct inference (all in source language), and English prompts (all in English). They used GPT-3.5, GPT-4, and mBERT models across QA, NLI, NER, and summarization tasks. The selective pre-translation approach preserved task-relevant context in the source language while translating instruction and output components to English, allowing for better alignment with English training data while maintaining linguistic nuance in task-specific content.

## Key Results
- Selective pre-translation consistently outperformed both full pre-translation and direct inference across all tasks and languages
- Context and examples in the source language particularly benefit extractive tasks like NER
- The approach effectively mitigates translation quality issues, especially for low-resource languages
- Output language preferences vary by task type, with extractive tasks performing better with source language outputs

## Why This Works (Mechanism)
Selective pre-translation works by preserving the linguistic integrity of task-specific content while leveraging English-language alignment benefits. By keeping context and examples in the source language, the model maintains access to culturally and linguistically relevant information that might be lost in translation. Meanwhile, translating instructions and outputs to English aligns the prompt structure with the model's primary training language, reducing confusion and improving response quality. This dual approach balances the need for linguistic fidelity with the practical benefits of English-language optimization.

## Foundational Learning
- **Prompt modularization**: Breaking prompts into instruction, context, examples, and output components allows targeted translation strategies - needed to systematically test which parts benefit from translation
- **Cross-linguistic transfer**: Understanding how language-specific information transfers across translation boundaries - critical for predicting performance gains
- **Low-resource language adaptation**: Techniques for optimizing model performance when training data is scarce - essential for addressing the digital divide
- **Translation quality impact**: Measuring how translation errors propagate through prompting pipelines - necessary for evaluating real-world viability
- **Task-type sensitivity**: Recognizing how different NLP tasks respond differently to translation strategies - important for task-specific optimization
- **Multilingual alignment**: Understanding how models align representations across languages - fundamental for designing effective multilingual prompts

## Architecture Onboarding
**Component map:** Source language -> Translation layer (selective) -> English instruction/output + Source context/examples -> LLM
**Critical path:** Prompt construction → Selective translation → Model inference → Result evaluation
**Design tradeoffs:** Linguistic fidelity vs. model alignment, computational overhead of selective translation vs. performance gains, complexity of implementation vs. robustness benefits
**Failure signatures:** Degraded performance when context/examples are mistranslated, reduced gains for high-resource languages, increased latency from translation steps
**First experiments:** 1) Test selective translation on single low-resource language across all four tasks, 2) Compare translation quality impact by varying translation service quality, 3) Evaluate performance on mixed-language prompts with varying translation ratios

## Open Questions the Paper Calls Out
None

## Limitations
- Findings primarily based on specific LLM architectures (GPT-3.5, GPT-4, mBERT) and may not generalize to other models
- Focus on four specific tasks limits generalizability to more specialized applications
- Does not extensively explore performance with non-English target languages
- Controlled experimental conditions may not fully capture real-world deployment scenarios

## Confidence
- **High confidence**: Comparative effectiveness of selective pre-translation over full pre-translation and direct inference
- **Medium confidence**: Task-specific recommendations for which prompt components to translate
- **Medium confidence**: Relationship between translation quality and selective pre-translation benefits

## Next Checks
1. Cross-model validation: Test selective pre-translation strategies across a broader range of multilingual LLMs including smaller, task-specific models
2. Real-world deployment testing: Implement selective pre-translation in production systems with naturally occurring prompts
3. Low-resource language impact: Conduct deeper analysis of how selective pre-translation performs across the full spectrum of low-resource languages