---
ver: rpa2
title: 'Consensus Is All You Need: Gossip-Based Reasoning Among Large Language Models'
arxiv_id: '2508.18292'
source_url: https://arxiv.org/abs/2508.18292
tags:
- consensus
- answer
- each
- gossip
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a gossip-based consensus framework for multi-agent
  LLM reasoning, inspired by distributed systems protocols. Instead of relying on
  a single model, several LLMs act as peers in a network, exchanging answers and thought
  processes iteratively until they reach agreement.
---

# Consensus Is All You Need: Gossip-Based Reasoning Among Large Language Models

## Quick Facts
- **arXiv ID:** 2508.18292
- **Source URL:** https://arxiv.org/abs/2508.18292
- **Reference count:** 9
- **Primary result:** Multi-agent gossip-based consensus among LLMs improves accuracy on MMLU by up to 10.7% for high-end models and 30.4% for low-end models, with cost savings when using weaker ensembles.

## Executive Summary
This paper introduces a gossip-based consensus framework for improving LLM reasoning by leveraging distributed systems protocols. Multiple LLMs act as peers in a network, exchanging answers and reasoning iteratively until agreement is reached. The method is inspired by how humans collaborate and deliberate, making AI reasoning more interpretable and collaborative. Evaluations show consistent accuracy gains across high-end and low-end models, with low-end ensembles achieving performance comparable to top-tier models at half the cost. The work also raises questions about scalability, robustness to adversarial inputs, and generalizability beyond MMLU.

## Method Summary
The proposed method employs a gossip-based consensus protocol where multiple LLM instances act as peers in a distributed network. Each peer generates an answer and shares its reasoning process with others. The system iterates through rounds of answer exchange and refinement, using variants such as simple voting with context, voting with a rotating judge, or hierarchical multi-layer consensus. The process continues until the agents converge on a consensus answer. This approach mimics human deliberation and is designed to improve reasoning accuracy through collaborative refinement, especially for weaker models that benefit more from peer input.

## Key Results
- Consensus accuracy on MMLU improved from 82.6–89.4% to 93.3% for high-end models (o4-mini, Gemini-2.5-pro, Grok-4-0709, DeepSeek-reasoner).
- Low-end model ensembles improved from 62.2–77.3% to 84.2% accuracy, with a +6.9 point gain and 30.4% fewer errors.
- Running low-end consensus cost only about half as much as a single high-end model run, highlighting potential cost savings.
- Improvement was more pronounced in weaker models (+6.9 points) compared to stronger ones (+4.3 points).

## Why This Works (Mechanism)
The method works by exploiting the collective reasoning power of multiple models, similar to how distributed systems achieve reliability through redundancy and consensus. Each model acts as a peer, generating and refining answers through iterative exchange, which helps correct individual errors and biases. The gossip protocol allows information to spread organically, and the iterative refinement process mimics human-like deliberation. This distributed approach leverages the diversity of model responses, with consensus emerging as a more robust answer than any single model could provide.

## Foundational Learning

**Distributed Systems Consensus** - Needed to understand how multiple agents can reliably agree on a solution; check by tracing how gossip protocols ensure convergence and fault tolerance.

**Ensemble Learning** - Needed to see how combining multiple models' outputs improves robustness; check by comparing individual vs. ensemble accuracy gains.

**Gossip Protocols** - Needed to understand the iterative, decentralized exchange of information; check by analyzing how answer refinement progresses over rounds.

**LLM Reasoning Interpretability** - Needed to assess how peer exchanges make reasoning more transparent; check by examining the quality and clarity of shared thought processes.

**Cost-Performance Tradeoffs** - Needed to evaluate when consensus is more efficient than single high-end models; check by comparing token usage and latency.

## Architecture Onboarding

**Component Map:** User Query -> Multiple LLMs (Peers) -> Iterative Answer Exchange (Gossip) -> Consensus Voting -> Final Answer

**Critical Path:** Query input → parallel model generation → iterative gossip rounds → majority vote or judge selection → output consensus

**Design Tradeoffs:** Higher accuracy and interpretability vs. increased latency and token usage; stronger models benefit less than weaker ones; cost savings possible with weak model ensembles.

**Failure Signatures:** Oscillation between answers, slow convergence, or consensus on incorrect answers if most peers are biased or wrong.

**First 3 Experiments:**
1. Run consensus with high-end models on MMLU and compare accuracy to individual runs.
2. Test low-end model consensus vs. single high-end model on MMLU for cost and accuracy.
3. Introduce noisy or adversarial inputs to test robustness of consensus process.

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- Evaluation limited to MMLU; performance on other benchmarks or open-ended tasks untested.
- No latency or token usage data reported, making real-world efficiency unclear.
- Assumption that self-consistency correlates with correctness is not validated.
- Unclear how the method scales with larger ensembles or under noisy/adversarial conditions.

## Confidence

**High confidence:** Empirical improvements on MMLU across tested model suites, relative cost advantage of low-end consensus over high-end single runs.

**Medium confidence:** Generalizability to other tasks and benchmarks; cost-efficiency in real-world usage scenarios.

**Low confidence:** Assumptions about error independence across models; correlation between consensus stability and correctness; performance under adversarial or noisy conditions.

## Next Checks

1. Evaluate consensus reasoning on non-MMLU benchmarks (e.g., human preference datasets, open-ended QA, multi-step reasoning tasks) to test robustness beyond multiple-choice accuracy.
2. Measure end-to-end latency and token usage for consensus runs versus single-model inference, including variations in iteration count and model diversity.
3. Test the effect of introducing noisy or adversarial responses in the gossip exchange to quantify robustness to model disagreement or strategic behavior.