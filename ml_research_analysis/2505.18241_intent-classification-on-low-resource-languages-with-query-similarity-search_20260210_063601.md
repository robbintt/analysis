---
ver: rpa2
title: Intent Classification on Low-Resource Languages with Query Similarity Search
arxiv_id: '2505.18241'
source_url: https://arxiv.org/abs/2505.18241
tags:
- classification
- intent
- query
- language
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of intent classification for
  low-resource languages, where data annotation is difficult and expensive. The authors
  propose a novel approach that casts intent classification as a query similarity
  search problem.
---

# Intent Classification on Low-Resource Languages with Query Similarity Search

## Quick Facts
- arXiv ID: 2505.18241
- Source URL: https://arxiv.org/abs/2505.18241
- Authors: Arjun Bhalla; Qi Huang
- Reference count: 23
- Primary result: Achieves reasonable intent classification for low-resource languages (Swahili, Urdu, Indonesian) in zero-shot setting using query similarity search with LaBSE and XLM-RoBERTa

## Executive Summary
This paper addresses the challenge of intent classification for low-resource languages where data annotation is difficult and expensive. The authors propose a novel approach that casts intent classification as a query similarity search problem, using pre-existing example queries to define intents and classifying incoming queries based on the labels of their most similar queries in latent space. They demonstrate that this approach can achieve reasonable performance for low-resource languages in a zero-shot setting without requiring annotated data from the target language, outperforming alternatives like machine translation and direct classification training in low-resource scenarios.

## Method Summary
The method uses multilingual sentence encoders (LaBSE or XLM-RoBERTa) to map queries into a shared embedding space, then employs k-nearest neighbor search to find the most similar queries in the index. Classification is performed via majority voting among the k nearest neighbors. The approach requires only labeled example queries from high-resource languages, with no need for target-language annotations. The system uses FAISS for efficient approximate nearest neighbor search and tests different values of k through grid search, finding k=31 optimal across five different encoder models.

## Key Results
- LaBSE achieves 0.486 accuracy on Swahili intent classification using high-resource language indexing, compared to 0.119 with XLM-RoBERTa
- High-resource-only indexing (English, Chinese, Spanish, French, Japanese) performs nearly identically to indexing all available languages for low-resource classification
- The method outperforms direct classification training and machine translation approaches in low-resource settings while being simpler to implement

## Why This Works (Mechanism)

### Mechanism 1
Multilingual sentence encoders map semantically equivalent queries from different languages to proximate regions in a shared latent space, enabling cross-lingual similarity search. Pre-trained models like LaBSE and XLM-RoBERTa learn language-agnostic sentence representations during training on large multilingual corpora, allowing Swahili queries to land near semantically similar English queries for label transfer via nearest neighbors.

### Mechanism 2
Majority voting among k nearest neighbors reduces sensitivity to individual noisy similarity matches and stabilizes predictions. By aggregating labels from k neighbors rather than relying on a single nearest match, sporadic semantic misalignments are averaged out. The paper empirically found k=31 optimal across five different encoder models.

### Mechanism 3
Indexing only high-resource language queries yields classification performance on low-resource targets comparable to indexing queries from all available languages. High-resource languages provide semantically diverse query coverage that transfers to low-resource queries without requiring target-language annotations, as the semantic intent space is adequately covered by the indexed high-resource queries.

## Foundational Learning

- **Sentence Embeddings and Multilingual Representation Learning**: Understanding how models like LaBSE map sentences from different languages into a shared vector space where semantic similarity corresponds to cosine similarity is critical. Quick check: Can you explain why LaBSE might outperform XLM-RoBERTa on Swahili intent classification despite both being multilingual models?

- **k-Nearest Neighbors and Approximate Nearest Neighbor (ANN) Search**: The method uses FAISS for efficient ANN search and majority voting among k neighbors for label resolution. Understanding the accuracy-efficiency tradeoff is critical for deployment. Quick check: What happens to prediction latency and accuracy as the query index grows from 10K to 10M queries?

- **Zero-Shot Cross-Lingual Transfer**: The core value proposition is deploying intent classification to low-resource languages without target-language training data. Understanding what enables and limits this transfer is essential. Quick check: If LaBSE was never trained on Quechua, would you expect this approach to work for Quechua queries? Why or why not?

## Architecture Onboarding

- **Component map**: Sentence Encoder -> ANN Search Module -> Label Resolver -> Intent Prediction
- **Critical path**: Index construction: Encode labeled queries → Store in FAISS with labels; Inference: Encode incoming query → ANN search → Majority vote → Return predicted intent
- **Design tradeoffs**: Encoder choice (LaBSE vs XLM-RoBERTa), index composition (high-resource-only vs all-languages), k value optimization, and accuracy vs. alternative methods trade-offs
- **Failure signatures**: Very low accuracy on specific language indicates poor encoder representation; accuracy drops when using high-resource index vs. all-languages index indicates coverage gaps; performance varies wildly across intent classes indicates class imbalance issues
- **First 3 experiments**: (1) Encoder validation - test both LaBSE and XLM-RoBERTa on held-out sample of target language, (2) k-value sweep - run grid search on k values for validation set, (3) Index ablation - compare English-only, 5 high-resource languages, and all languages indexing strategies

## Open Questions the Paper Calls Out

1. What is the quantitative relationship between the number of indexed queries per class and classification performance, and does this scaling behavior differ across datasets with varying intent taxonomies?

2. How does fine-tuning sentence encoder models on available high-resource language data affect zero-shot cross-lingual intent classification performance?

3. How does class imbalance in the indexed query set affect kNN-based intent classification, and what mitigation strategies are effective?

4. Can more sophisticated label resolution mechanisms (beyond simple majority voting) close the performance gap with supervised classification while maintaining zero-shot deployability?

## Limitations

- Strong dependence on sentence encoder's cross-lingual alignment quality, with performance varying dramatically between models (LaBSE 0.486 vs XLM-RoBERTa 0.119 accuracy on Swahili)
- Assumes high-resource language query coverage adequately represents the semantic intent space, which may not hold for domain-specific concepts
- k-NN mechanism is sensitive to class imbalance, potentially requiring adjustment when some intent classes have fewer than k examples
- Practical overhead of maintaining and updating large FAISS indices as query volumes grow is not addressed

## Confidence

**High confidence**: The claim that query similarity search enables zero-shot intent classification for low-resource languages is well-supported by experimental results across three languages and two different encoders.

**Medium confidence**: The assertion that high-resource-only indexing performs nearly as well as indexing all available languages requires more extensive validation beyond the single dataset tested.

**Low confidence**: The claim about deployment simplicity overlooks practical overhead of maintaining large FAISS indices and computational costs of embedding large query volumes.

## Next Checks

1. **Encoder Coverage Validation**: Systematically test the approach across 10-15 additional low-resource languages spanning different language families to quantify the minimum pre-training data threshold required for LaBSE to produce meaningful embeddings.

2. **Class Imbalance Robustness**: Evaluate performance under controlled class imbalance conditions (e.g., 1:10, 1:100 ratio between most/least frequent classes) to determine the maximum imbalance ratio the k-NN mechanism can handle before accuracy degrades significantly.

3. **Index Size Scalability**: Measure accuracy, latency, and memory usage as the index grows from 10K to 1M queries, comparing exact vs. approximate nearest neighbor search, to establish practical limits for production deployment.