---
ver: rpa2
title: 'DepressLLM: Interpretable domain-adapted language model for depression detection
  from real-world narratives'
arxiv_id: '2508.08591'
source_url: https://arxiv.org/abs/2508.08591
tags:
- depression
- confidence
- phq-9
- score
- depressllm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DepressLLM is a domain-adapted language model for depression detection\
  \ trained on a novel corpus of 3,699 autobiographical narratives reflecting both\
  \ happiness and distress. The model uses a Score-guided Token Probability Summation\
  \ (SToPS) method to provide interpretable predictions and reliable confidence estimates,\
  \ achieving an AUC of 0.789, which rises to 0.904 on samples with confidence \u2265\
  \ 0.95."
---

# DepressLLM: Interpretable domain-adapted language model for depression detection from real-world narratives

## Quick Facts
- **arXiv ID:** 2508.08591
- **Source URL:** https://arxiv.org/abs/2508.08591
- **Reference count:** 40
- **Primary result:** AUC of 0.789, rising to 0.904 on high-confidence samples; robust generalization across diverse datasets

## Executive Summary
DepressLLM is a domain-adapted language model designed to detect depression from autobiographical narratives. It employs a novel Score-guided Token Probability Summation (SToPS) method to provide interpretable predictions and reliable confidence estimates. Trained on a novel corpus of 3,699 narratives reflecting both happiness and distress, the model demonstrates strong generalization across heterogeneous datasets including daily mood recordings and clinical interviews. Notably, psychiatric review of high-confidence misclassifications revealed that the model's predictions were often more consistent with clinical judgment than with self-reported PHQ-9 scores, suggesting potential as a complementary screening tool.

## Method Summary
DepressLLM fine-tunes LLMs (GPT-4.1, LLaMA-3.3 70B, Phi-4 14B, Qwen3-32B) to predict PHQ-9 depression scores (0-27) as discrete tokens. The SToPS method sums token probabilities above clinical cutoffs to produce calibrated confidence scores. Training uses a novel TREND-P corpus of paired happy/distress narratives from the same individuals, enabling the model to learn emotional contrast rather than author style. The architecture includes an explainer module that generates natural language rationales alongside predictions. Models were trained with LoRA/QLoRA on NVIDIA A100 80GB, with performance evaluated on DAIC-WOZ and VEMOD datasets.

## Key Results
- Achieved AUC of 0.789 overall, rising to 0.904 on samples with confidence ≥ 0.95
- Demonstrated robust generalization across diverse datasets (daily mood recordings, clinical interviews)
- Psychiatric review showed model predictions often aligned better with clinical judgment than self-reported PHQ-9 scores
- High-confidence filtering effectively isolates predictions consistent with expert assessment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggregating probabilities across clinical score tokens (SToPS) yields more reliable confidence estimates than single-token prediction.
- **Mechanism:** Instead of deterministic prediction, model outputs probability distribution over tokens 0-27. SToPS sums probability mass above clinical cutoff, transforming generation task into calibrated classification problem.
- **Core assumption:** Language model's output logits over score tokens correlate linearly with clinical severity of input text.
- **Evidence anchors:** Abstract states SToPS "delivers reliable confidence estimates"; Section 2.4 shows higher AUC values at every threshold; neighbor paper *Probabilistic Textual Time Series Depression Detection* emphasizes uncertainty estimates for clinical decision support.
- **Break condition:** Fails if tokenizer splits multi-digit scores (e.g., "12") into separate tokens without proper chain-rule probability calculation.

### Mechanism 2
- **Claim:** Training on paired happy and distress narratives from same individuals forces model to learn emotional contrast rather than author style.
- **Mechanism:** By providing contrasting contexts (happy vs. distress) for same participant, model learns to distinguish specific linguistic markers of distress from user's baseline idiolect.
- **Core assumption:** Linguistic markers of depression are context-dependent and can be isolated by comparing positive/negative narratives from single source.
- **Evidence anchors:** Section 2.1 describes "balanced corpus of first-person narratives featuring paired positive and negative contexts"; Section 2.5 shows hardship-related terms in both contexts led to depression prediction, implying context sensitivity; neighbor *DepFlow* highlights risk of "semantic shortcuts," supporting need for disentangled/contrastive data.
- **Break condition:** Degrades if applied to populations with drastically different narrative styles from training demographics.

### Mechanism 3
- **Claim:** High-confidence filtering effectively isolates subset of predictions aligning closely with clinical judgment, potentially more so than self-reported surveys.
- **Mechanism:** Model filters outputs using SToPS confidence score. On high-confidence errors, psychiatric review revealed model often detected clinical signs (somatization, lack of insight) that patient denied in self-reports.
- **Core assumption:** High-confidence "errors" relative to PHQ-9 are often correct identifications of hidden distress, validated by "Gold Standard" of expert psychiatric review rather than self-report label.
- **Evidence anchors:** Abstract states "...psychiatric review... showed the model's predictions were often more consistent with clinical judgment than with self-reported PHQ-9 scores"; Section 2.6 notes "...discrepancies... included somatization, limited emotional awareness, and reduced insight..."; evidence is specific to this paper.
- **Break condition:** If model systematically overweights specific keywords (e.g., past trauma) without temporal context, may generate high-confidence false positives for resolved issues.

## Foundational Learning

- **Concept: Calibration in LLMs**
  - **Why needed here:** Raw LLM probabilities are often poorly calibrated. Understanding temperature scaling or probability aggregation (like SToPS) is required to trust confidence scores.
  - **Quick check question:** Does the model output a single token "10", or a probability vector for [0...27]?

- **Concept: Clinical Cutoffs (PHQ-9)**
  - **Why needed here:** Architecture relies on binary classification derived from continuous score. Must understand that threshold (e.g., ≥ 10) determines where probability mass is split.
  - **Quick check question:** If clinical cutoff changes from 10 to 15, which part of SToPS equation requires updating?

- **Concept: Tokenization Granularity**
  - **Why needed here:** Paper explicitly handles multi-digit vs. single-digit tokenization (Llama vs. Qwen).
  - **Quick check question:** If model tokenizes "14" as single ID, calculation is direct; if as "1" + "4", do you multiply or add probabilities?

## Architecture Onboarding

- **Component map:** De-identified transcript of autobiographical memory -> Instruction-tuned LLM (GPT-4.1, Llama-3.3 70B, etc.) -> SToPS Layer (extracts logits for tokens 0-27, sums probability mass, calculates confidence) -> Explainer (generates natural language rationale alongside score)

- **Critical path:** Mapping of predicted tokens to integer scale 0-27. If tokenizer maps " 10" to ID 990 and "10" to ID 880, logit extraction fails.

- **Design tradeoffs:**
  - **Closed (GPT-4.1) vs. Open (Llama-3.3):** Closed offers higher AUC (0.846 vs 0.830 at cutoff 10); Open offers privacy and local control
  - **Accuracy vs. Coverage:** Raising confidence threshold to 0.95 boosts AUC to 0.904 but discards 88.5% of data

- **Failure signatures:**
  - **Temporal confusion:** High confidence predictions of depression based on past trauma descriptions despite current resilience (Case 15 in Supplement)
  - **Short context:** Over-confidence on text with "limited linguistic content" (Section 2.6)

- **First 3 experiments:**
  1. **Tokenization Validation:** Verify logit extraction for scores 0-27 on chosen backbone; ensure "10" is not tokenized as "<1><0>" without chain-rule application
  2. **Threshold Sweep:** Reproduce Figure 2a on validation set to find optimal confidence threshold for specific deployment (balancing volume vs. accuracy)
  3. **Error Analysis:** Run inference on 50 samples with confidence ≥0.95 to check if "significant phrases" align with clinical symptoms or merely keyword matching

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does training DepressLLM on clinical consensus labels rather than self-reported PHQ-9 scores improve diagnostic accuracy?
- **Basis in paper:** [explicit] Authors state that PHQ-9 scores may lack accuracy due to bias and anticipate that "training with labels more closely aligned with clinical ground truth could further improve model performance."
- **Why unresolved:** Current study relied exclusively on self-reported PHQ-9 scores as ground truth for training and evaluation.
- **What evidence would resolve it:** Comparative experiment training models on dataset labeled via structured clinical interviews compared to self-report.

### Open Question 2
- **Question:** How can the model be refined to distinguish between historical descriptions of trauma and current depressive symptoms?
- **Basis in paper:** [explicit] Psychiatric review of high-confidence misclassifications highlighted "insufficient consideration of temporal context," noting model often overemphasized past events to predict current mood.
- **Why unresolved:** Current architecture relies on lexical cues that conflate past distress with present state.
- **What evidence would resolve it:** Analysis showing improved precision on narratives containing past trauma but low current depression, potentially via temporal attention mechanisms.

### Open Question 3
- **Question:** Can the SToPS confidence estimation mechanism be calibrated to better reflect uncertainty in cases with ambiguous linguistic signals?
- **Basis in paper:** [explicit] Authors note that model assigned high confidence to cases where psychiatrists disagreed, indicating a "limitation in its ability to recognize uncertainty" in ambiguous contexts.
- **Why unresolved:** High token probability sums do not necessarily correlate with clinical certainty in edge cases.
- **What evidence would resolve it:** Validation showing that confidence scores decrease proportionally to inter-rater disagreement between clinical experts.

## Limitations

- **Dataset accessibility:** TREND-P and VEMOD datasets are in-house and not publicly available, restricting full reproducibility
- **High-confidence filtering trade-off:** Performance gains (AUC rising from 0.789 to 0.904) come at cost of excluding 88.5% of samples, raising questions about clinical utility in population-level screening
- **Single-reviewer validation:** Psychiatric review validating high-confidence misclassifications was conducted by single psychiatrist, introducing potential subjectivity

## Confidence

- **SToPS method reliability:** Medium-High - technically sound with explicit mathematical formulation, but validation limited to specific datasets without cross-linguistic or cross-cultural testing
- **Paired narrative training advantage:** Medium - mechanism for learning emotional contrast is logically coherent, but paper lacks ablation studies comparing this approach to standard single-context training
- **Model outperforming self-report in high-confidence cases:** Medium - psychiatric review provides some validation, but single-reviewer nature and limited sample size constrain generalizability

## Next Checks

1. **Temporal context validation:** Test whether model's high-confidence predictions on narratives containing past trauma but current resilience can be mitigated by explicitly prompting for "current emotional state" versus "past experiences" to prevent misclassification of resolved issues

2. **Cross-cultural generalization:** Evaluate DepressLLM on diverse corpus from different cultural contexts (e.g., non-Western autobiographical narratives) to test whether emotional contrast learning mechanism generalizes beyond training demographics

3. **Multi-expert validation:** Replicate psychiatric review component with 3-5 independent clinicians reviewing same set of high-confidence misclassifications to establish inter-rater reliability and reduce single-reviewer bias in determining whether model or self-report is more clinically accurate