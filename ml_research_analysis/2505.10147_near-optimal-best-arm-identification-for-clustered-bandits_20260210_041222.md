---
ver: rpa2
title: Near Optimal Best Arm Identification for Clustered Bandits
arxiv_id: '2505.10147'
source_url: https://arxiv.org/abs/2505.10147
tags:
- best
- agents
- bandits
- agent
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses best arm identification in federated multi-agent
  multi-armed bandits with unknown agent-bandit mappings and agent clustering. The
  authors propose two novel algorithms, Cl-BAI (Clustering then Best Arm Identification)
  and BAI-Cl (Best Arm Identification then Clustering), both leveraging successive
  elimination for computational efficiency.
---

# Near Optimal Best Arm Identification for Clustered Bandits

## Quick Facts
- arXiv ID: 2505.10147
- Source URL: https://arxiv.org/abs/2505.10147
- Authors: Yash; Nikhil Karamchandani; Avishek Ghosh
- Reference count: 40
- Primary result: Two novel algorithms (Cl-BAI and BAI-Cl) achieve order-wise minimax optimal sample complexity for best arm identification in clustered multi-agent bandit settings when the number of clusters M is small relative to the number of agents N.

## Executive Summary
This paper addresses best arm identification in federated multi-agent multi-armed bandits with unknown agent-bandit mappings and agent clustering. The authors propose two novel algorithms, Cl-BAI (Clustering then Best Arm Identification) and BAI-Cl (Best Arm Identification then Clustering), both leveraging successive elimination for computational efficiency. Theoretical guarantees establish δ-probably correct (δ-PC) performance for both algorithms, with sample complexity bounds derived. When the number of clusters M is small (constant), a variant of BAI-Cl is shown to be order-wise minimax optimal. Experiments on synthetic and real-world datasets (MovieLens, Yelp) demonstrate significant improvements in sample and communication efficiency, particularly in settings where M ≪ N. For example, BAI-Cl++ achieves a 72% improvement in sample complexity over a naive scheme in a MovieLens-based movie recommendation experiment with 100 users clustered into 6 age groups.

## Method Summary
The paper proposes two algorithms for best arm identification in clustered bandit settings. Cl-BAI first clusters agents based on their bandit problems, then identifies the best arm for each cluster. BAI-Cl reverses this order, identifying best arms first and then clustering agents. Both algorithms leverage successive elimination for computational efficiency. Theoretical guarantees establish δ-probably correct (δ-PC) performance for both algorithms, with sample complexity bounds derived. When the number of clusters M is small (constant), a variant of BAI-Cl is shown to be order-wise minimax optimal. The algorithms exploit the structure of clustered bandit problems where agents within the same cluster share identical bandit problems.

## Key Results
- Cl-BAI and BAI-Cl algorithms achieve δ-probably correct performance for best arm identification in clustered bandit settings
- When M is small (constant), BAI-Cl++ is order-wise minimax optimal with sample complexity improvements
- Empirical evaluation shows 72% improvement in sample complexity over naive schemes in MovieLens experiment with 100 users clustered into 6 age groups
- Both algorithms demonstrate significant improvements in sample and communication efficiency, particularly when M ≪ N

## Why This Works (Mechanism)
The proposed approach exploits the structure of clustered bandit problems where agents within the same cluster share identical bandit problems. By first clustering agents or identifying best arms, the algorithms reduce the effective problem size from N agents to M clusters, dramatically improving sample efficiency. The successive elimination approach ensures computational efficiency while maintaining theoretical guarantees. The algorithms leverage the fact that identifying the best arm for each cluster requires fewer samples than solving N independent bandit problems.

## Foundational Learning
- Clustered bandit problems: Why needed - to model federated learning scenarios with heterogeneous agents; Quick check - verify agents within clusters have identical reward distributions
- Successive elimination: Why needed - for computational efficiency in best arm identification; Quick check - ensure elimination thresholds balance exploration and exploitation
- δ-probably correct guarantees: Why needed - to provide theoretical performance bounds; Quick check - verify confidence parameters scale appropriately with problem size
- Agent clustering: Why needed - to exploit problem structure and reduce sample complexity; Quick check - assess clustering quality through purity metrics
- Federated multi-agent learning: Why needed - to model distributed decision-making scenarios; Quick check - confirm communication constraints are respected
- Sample complexity bounds: Why needed - to quantify learning efficiency; Quick check - verify bounds scale appropriately with N, K, and M

## Architecture Onboarding

Component map: Agents -> Clustering Module -> Best Arm Identification Module -> Cluster-level Policies

Critical path: Data collection from agents → Clustering algorithm execution → Best arm identification per cluster → Policy deployment

Design tradeoffs: Clustering accuracy vs. computational efficiency; Exploration vs. exploitation in arm selection; Communication overhead vs. sample complexity

Failure signatures: Poor clustering quality leading to suboptimal arm selection; Insufficient exploration causing incorrect arm identification; Communication bottlenecks in federated setting

First experiments:
1. Verify clustering quality on synthetic data with known cluster structure
2. Test best arm identification accuracy for individual clusters
3. Evaluate sample complexity improvements compared to naive baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Relies heavily on assumption that agents within same cluster share identical bandit problems
- Clustering quality directly impacts downstream performance, with limited analysis of robustness to noisy clusters
- Theoretical guarantees assume known cluster structure in certain bounds
- Empirical evaluation uses relatively small-scale problems and limited comparison to state-of-the-art algorithms
- Does not address computational complexity of clustering subroutines or scalability to large agent populations

## Confidence
- Theoretical claims: Medium (rely on specific structural assumptions)
- Empirical results: High (within tested regimes, uncertain for broader applications)

## Next Checks
1. Test algorithm performance under varying degrees of cluster purity and overlap
2. Evaluate scalability to large agent populations (N > 1000) and cluster counts
3. Benchmark against existing clustered bandit algorithms on more diverse datasets and reward structures