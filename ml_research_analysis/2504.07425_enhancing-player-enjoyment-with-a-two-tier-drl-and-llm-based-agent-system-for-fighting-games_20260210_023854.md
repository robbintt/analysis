---
ver: rpa2
title: Enhancing Player Enjoyment with a Two-Tier DRL and LLM-Based Agent System for
  Fighting Games
arxiv_id: '2504.07425'
source_url: https://arxiv.org/abs/2504.07425
tags:
- agent
- reward
- agents
- player
- opponent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a Two-Tier Agent (TTA) system to enhance player
  enjoyment in fighting games, specifically Street Fighter II. The system combines
  Deep Reinforcement Learning (DRL) agents with a Large Language Model (LLM) Hyper-Agent
  to dynamically select opponents based on player data and feedback.
---

# Enhancing Player Enjoyment with a Two-Tier DRL and LLM-Based Agent System for Fighting Games

## Quick Facts
- **arXiv ID**: 2504.07425
- **Source URL**: https://arxiv.org/abs/2504.07425
- **Reference count**: 40
- **Primary result**: LLM-based opponent selection with DRL agents improves player enjoyment by 42.73% and special move usage by 156.36% over baseline

## Executive Summary
This paper presents a Two-Tier Agent (TTA) system that combines Deep Reinforcement Learning (DRL) agents with a Large Language Model (LLM) Hyper-Agent to enhance player enjoyment in fighting games, specifically Street Fighter II. The DRL agents are trained with modularized reward functions and hybrid training (PvE + self-play) to exhibit diverse play styles and execute advanced skills. A small-scale user study demonstrates that the LLM-based opponent selection significantly improves overall enjoyment (42.73% improvement) and difficulty suitability compared to random opponent selection.

## Method Summary
The system trains DRL agents using PPO with ResNet18 CNN + LSTM + MLP architecture, employing modularized reward functions (15+ configurable terms) and hybrid training (30% PvE vs built-in AI, 70% self-play). These agents are organized into an archive by play style (projectile, defensive, special move, etc.). A Game Manager collects player statistics and optional natural language feedback, which are embedded into prompts for a reasoning-optimized LLM (DeepSeek-R1 variants) to select appropriate opponents. The LLM performs chain-of-thought reasoning to balance difficulty, diversity, and player preferences.

## Key Results
- DRL agents improve special move usage by 156.36% over baseline methods
- Agents achieve 66.7% win rate advantage against baseline opponents
- LLM-based selection improves overall enjoyment by 42.73% in user study
- Reasoning-optimized LLMs achieve >80% format correctness vs <40% for non-reasoning models

## Why This Works (Mechanism)

### Mechanism 1: Modularized Reward Functions Shape Diverse Agent Behaviors
By decomposing rewards into configurable components (special move bonus, projectile bonus, distance reward, etc.), agents can be steered toward distinct play styles without architectural changes. Each reward term is individually computed and weighted via coefficients. For example, a "defensive" agent receives positive distance reward for maintaining separation, while a "special move" agent gets amplified bonuses for advanced techniques. The final reward is a linear combination where coefficients encode the configuration.

Core assumption: Reward shaping translates to observable behavioral differences that players perceive as distinct play styles. Assumption: Players prefer opponents with varied, recognizable behaviors over uniformly optimized agents.

Evidence anchors: Experiments demonstrate improvements from 64.36% to 156.36% in the execution of advanced skills over baseline methods; defensive agents maintain greater average distance and higher projectile usage rates compared to default agents.

### Mechanism 2: Hybrid Training Prevents Policy Collapse into Repetitive Patterns
Combining built-in AI opponents (PvE tasks) with self-play prevents agents from overfitting to deterministic patterns or converging to exploitative but uninteresting local optima. Training alternates between PvE tasks (30%) against rule-based Capcom AI and self-play tasks (70%) against historical agent versions. PvE provides stable learning signal for core mechanics; self-play introduces stochasticity and prevents macro-like open-loop behavior.

Core assumption: Pure PvE leads to deterministic overfitting; pure self-play leads to local optima (e.g., crouch kick spamming). Hybrid ratio of 30/70 balances these failure modes. Assumption: Built-in AI demonstrates sufficient skill diversity to teach useful behaviors.

Evidence anchors: Because the game emulator is deterministic, after multiple training iterations, the DRL agent tends to overfit to a fixed pattern, becoming a quasi-open-loop controller; in experiments, this manifested as persistent crouch kick or jump kick spamming.

### Mechanism 3: LLM Hyper-Agent Enables Contextual Opponent Selection via In-Context Learning
A reasoning-optimized LLM can dynamically select opponents from an archive by processing player statistics and natural language feedback without explicit fine-tuning. The Game Manager aggregates player data (win rate, streaks, special move usage, faced opponents) and optional text feedback. This is embedded into a structured prompt with selection principles (difficulty adjustment, opponent diversity, player behavior analysis). The LLM performs chain-of-thought reasoning and outputs JSON specifying the next opponent.

Core assumption: LLMs possess sufficient reasoning capacity to balance multiple selection criteria (difficulty, diversity, player skill, feedback) without domain-specific training. Assumption: Players can articulate useful feedback in natural language. Assumption: Inference latency (1-2 minutes simulated) is acceptable in casual play contexts.

Evidence anchors: A small-scale user study demonstrates the TTA system significantly enhances overall enjoyment (42.73% improvement) and difficulty suitability; reasoning-optimized models achieve >80% format correctness vs <40% for non-reasoning models; user study shows experimental group scores higher on Overall Enjoyability, Difficulty Suitability, and Preferred Group vs random selection control.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - **Why needed here:** The DRL agents use PPO as their core training algorithm. Understanding the clipping objective, value function estimation, and entropy regularization explains why certain hyperparameters (vf_coef=1.0, ent_coef=0.01) were chosen.
  - **Quick check question:** What happens to policy diversity if entropy coefficient is set too low? (Answer: Policy collapses to deterministic exploitation; exploration ceases.)

- **Concept: Recurrent Neural Networks for Temporal Credit Assignment**
  - **Why needed here:** Special moves in SF2 require precise input sequences (e.g., → + ↓ + → + punch). The LSTM component must learn these multi-step dependencies from action history (past 100 steps). Without understanding sequential modeling, one might incorrectly attribute temporal learning to the CNN.
  - **Quick check question:** Why can't a feedforward CNN learn special move execution? (Answer: CNNs process single frames; they lack explicit memory of prior actions needed to complete multi-input sequences.)

- **Concept: In-Context Learning (ICL) in Large Language Models**
  - **Why needed here:** The LLMHA operates via prompt engineering rather than weight updates. The prompt includes few-shot examples and selection principles that guide behavior at inference time. Understanding ICL explains why model scale and reasoning optimization matter more than fine-tuning.
  - **Quick check question:** What is the difference between ICL and fine-tuning for adapting an LLM to opponent selection? (Answer: ICL modifies behavior via prompt context at inference; fine-tuning updates model weights. ICL requires no gradient computation but is limited by context window and model's pre-existing capabilities.)

## Architecture Onboarding

- **Component map:**
  Game Environment: Street Fighter II via OpenAI Gym Retro → Observation Extraction → Feature Fusion → Policy Output → Game Manager → LLM Hyper-Agent → Agent Archive

- **Critical path:**
  1. Reward design first: Define which behaviors you want before training. Map each desired play style to a specific reward configuration.
  2. Hybrid training pipeline: Implement parallel environment collection (12 envs) with mixed opponent sampling (30% built-in AI, 70% historical self-play).
  3. LLM integration second: Once agent archive is populated, implement GM data collection and prompt construction. Test with DeepSeek-R1-Distill-Llama-8B locally before scaling.

- **Design tradeoffs:**
  - ResNet18 vs lighter CNN: ResNet18 provides robust visual features but may be overkill; the paper uses unmodified architecture, suggesting feature extraction wasn't a bottleneck. Trade-off: training speed vs feature quality.
  - LSTM vs Transformer for temporal modeling: LSTM chosen for simplicity; paper speculates Transformers might improve precision but doesn't validate. Trade-off: inductive bias for local sequences vs global attention.
  - 8B vs 671B LLM: 8B runs locally on RTX 3080 with 8-bit quantization; 671B requires API/cloud. Trade-off: latency, cost, privacy vs reasoning quality and consistency.
  - Self-play ratio (70/30): Chosen empirically; not ablated in paper. Higher self-play may yield stronger agents but risks local optima; lower self-play risks overfitting.

- **Failure signatures:**
  - Agent overfits to PvE only—becomes "macro" pattern executor. Diagnostic: Agent fails against novel opponents or in self-play; fix by enforcing hybrid training ratio.
  - Self-play converges to crouch-kick/jump-kick spam. Diagnostic: Low special move count despite reward shaping; fix by increasing special move bonus, adding vulnerability frame costs.
  - LLMHA outputs invalid JSON or ignores format. Diagnostic: Parse failures; fix with stronger prompt constraints or use DeepSeek-R1-671B over smaller variants.
  - Players report boredom despite diverse agent types. Diagnostic: Selection logic may be failing to adapt; check LLM entropy and verify feedback is being incorporated.

- **First 3 experiments:**
  1. Baseline comparison for special move execution: Train agents with default HP-only reward vs modular reward with special move bonus. Measure special moves per round. Expect 64-156% improvement as reported.
  2. Ablation of LSTM component: Train identical architecture with and without action history input. Compare special move execution and win rates. Hypothesis: LSTM is necessary for complex move sequences.
  3. LLM selection vs random selection (small user study): Recruit 6-10 players. Each plays 5 matches with LLMHA selection and 5 with random selection. Collect enjoyment ratings. Expect LLMHA to score higher on difficulty suitability and overall enjoyment.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Transformer-based architectures improve the temporal precision of advanced skill execution compared to the current LSTM approach?
- **Basis in paper:** Section V-B notes that current agents learn "approximate action patterns" rather than precise inputs, and Section VII proposes investigating "Transformer-based models" to address this.
- **Why unresolved:** The current LSTM extracts temporal features but results in agents "mashing" inputs to trigger special moves rather than mastering the precise command sequences required in Street Fighter II.
- **What evidence would resolve it:** A comparative analysis of special move execution success rates and input precision between agents using LSTM versus Transformer-based architectures.

### Open Question 2
- **Question:** Do high-level human strategies (e.g., spacing, hit-confirm) emerge in DRL agents given significantly extended training durations or larger model capacities?
- **Basis in paper:** Section V-C hypothesizes that agents lack strategies like "Spacing" and "Hit Confirm" due to insufficient training time or architectural limitations, suggesting emergence might be possible under the right conditions.
- **Why unresolved:** Current agents rely on raw reaction speed and special move invincibility rather than the tactical positioning and psychological anticipation used by human experts.
- **What evidence would resolve it:** Training agents for orders of magnitude longer steps and observing the emergence of spatial control metrics or frame-trapping behaviors in the policy distribution.

### Open Question 3
- **Question:** Can Reinforcement Learning from Human Feedback (RLHF) effectively mitigate LLM Hyper-Agent latency and format errors for real-time commercial deployment?
- **Basis in paper:** Section V-E highlights that output validation retries introduce "noticeable delays" that reduce enjoyment, and Section VII suggests RLHF as a potential solution.
- **Why unresolved:** While reasoning models like DeepSeek-R1 perform well, autoregressive generation inherently risks format inconsistencies and high inference times that disrupt the game flow.
- **What evidence would resolve it:** A user study measuring player tolerance for Hyper-Agent inference delays and a performance comparison of standard vs. RLHF-tuned models on valid JSON generation speed.

## Limitations
- Small user study sample size (n=10) limits generalizability of enjoyment claims
- Opponent selection latency (1-2 minutes for 671B model) may not be acceptable in live play contexts
- Some methodological details underspecified including total training duration and exact memory address mappings

## Confidence
- **High confidence**: Modular reward functions effectively shape agent behaviors (supported by 64.36-156.36% special move improvement); hybrid training prevents pattern overfitting; LLM reasoning optimization improves format correctness
- **Medium confidence**: Overall enjoyment improvement claims (42.73% improvement in small user study); win rate advantage (66.7%); difficulty suitability improvements
- **Low confidence**: Generalization to other fighting games or genres; latency acceptability in casual play; scalability of 671B model inference without API costs

## Next Checks
1. Conduct larger user study (n≥30) across different skill levels and demographics to validate enjoyment and difficulty suitability claims
2. Perform latency benchmarking comparing 8B local inference (RTX 3080) against 671B API inference to assess real-world feasibility
3. Test opponent selection robustness with adversarial or edge-case player data (extreme win streaks, contradictory feedback) to evaluate LLM reasoning limits