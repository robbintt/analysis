---
ver: rpa2
title: 'CANDY: Benchmarking LLMs'' Limitations and Assistive Potential in Chinese
  Misinformation Fact-Checking'
arxiv_id: '2509.03957'
source_url: https://arxiv.org/abs/2509.03957
tags:
- fact-checking
- llms
- claim
- date
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces CANDY, a benchmark for evaluating LLMs\u2019\
  \ fact-checking capabilities on Chinese misinformation. It presents CANDYSET, a\
  \ ~20k multi-domain dataset with ~5k annotated LLM-generated explanations, revealing\
  \ that LLMs struggle with accuracy, especially in contamination-free settings."
---

# CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese Misinformation Fact-Checking

## Quick Facts
- **arXiv ID:** 2509.03957
- **Source URL:** https://arxiv.org/abs/2509.03957
- **Reference count:** 40
- **Primary result:** LLM-assisted fact-checking improves accuracy across educational levels, but models struggle with factual fabrication, especially in contamination-free settings.

## Executive Summary
This paper introduces CANDY, a comprehensive benchmark for evaluating large language models (LLMs) on Chinese misinformation fact-checking. The benchmark includes CANDYSET, a ~20k multi-domain dataset with temporal annotations and ~5k human-annotated LLM-generated explanations. The study reveals that LLMs perform significantly worse when evaluating claims outside their knowledge cutoff dates (contamination-free evaluation), showing an average accuracy drop of 6.9%. Factual fabrication emerges as the most common error mode in model explanations. Human studies demonstrate that LLM assistance improves fact-checking accuracy across all educational levels, positioning LLMs as valuable collaborative tools rather than standalone decision-makers.

## Method Summary
The study evaluates 16 LLMs and 3 Large Reasoning Models across three tasks: (1) Fact-Checking Conclusion (verdict on claims), (2) Fact-Checking Explanation (quality of reasoning), and (3) LLM-Assisted Fact-Checking (human performance boost). The CANDYSET dataset (~20k Chinese claims) is partitioned by model knowledge cutoff dates for contamination-free evaluation. Models are prompted using four schemes (Zero-shot with/without Chain-of-Thought, Few-shot with/without Chain-of-Thought) with the role "extremely strict fact-checking expert." Human annotators evaluate explanation quality using a hierarchical taxonomy (Faithfulness, Factuality, Reasoning), achieving Fleiss' Kappa 0.76. The human study compares performance using Search alone versus Search plus LLM assistance.

## Key Results
- LLMs show an average 6.9% accuracy drop in contamination-free settings, revealing reliance on memorized knowledge
- Factual fabrication is the dominant error mode in explanations, accounting for the majority of flawed outputs
- Reformulating claims as questions significantly reduces factual fabrication (from ~40% to ~14% in GPT-4o)
- LLM assistance improves fact-checking accuracy across all educational levels in human studies
- The hierarchical error taxonomy reveals that binary accuracy metrics miss critical diagnostic signals about explanation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Partitioning evaluation data by model knowledge cut-off dates reveals performance degradation on unseen information (contamination-free evaluation).
- Mechanism: By strictly separating test instances occurring *after* a model's training window, the benchmark isolates the model's reasoning and generalization capabilities from its ability to retrieve memorized facts. The paper reports an average performance drop of 6.9% accuracy in these contamination-free settings, indicating reliance on static knowledge.
- Core assumption: The model does not have access to external, real-time retrieval tools during the inference process (closed-book setting).
- Evidence anchors:
  - [abstract] "...revealing that LLMs struggle with accuracy, especially in contamination-free settings."
  - [section] Page 4, Table 4: Shows performance gaps between "contamination-free" (values outside parentheses) and "contamination" (values inside) evaluations.
  - [corpus] Related work (RealFactBench) reinforces the need for realistic, dynamic evaluation beyond static knowledge bases.
- Break condition: This mechanism fails if the deployed model is upgraded with Retrieval-Augmented Generation (RAG) or web-browsing tools, rendering time-based partitions less indicative of true capability limits.

### Mechanism 2
- Claim: Reformulating claims as interrogative expressions (questions) reduces factual fabrication compared to declarative assertions.
- Mechanism: The paper suggests that declarative framing (e.g., "Event X happened") triggers "sycophancy," where models align with the user's premise. Interrogative framing (e.g., "Did Event X happen?") shifts the model from assertion to exploration/analysis, reducing the tendency to fabricate supporting details.
- Core assumption: The reduction in fabrication is due to a shift in model "intent" (analysis vs. alignment) rather than simply increased token length.
- Evidence anchors:
  - [section] Page 7, Section 6.2: "Reformulating claims into interrogative expressions significantly reduces fabricated content... reducing factual fabrication to 14% in GPT-4o."
  - [figure] Figure 11: Demonstrates the shift in model behavior based on claim framing.
- Break condition: This effect may diminish if the model is specifically fine-tuned to treat all inputs as truth-seeking queries regardless of syntax.

### Mechanism 3
- Claim: A hierarchical error taxonomy (Faithfulness, Factuality, Reasoning) provides diagnostic signals that binary accuracy metrics miss.
- Mechanism: Binary classification (Rumor vs. Non-rumor) hides *why* a model failed. By annotating errors into categories like "Factual Fabrication" or "Logical Inconsistency," the system identifies that the primary failure mode is not a lack of logic but a tendency to hallucinate (Factuality Hallucination), which accounted for the majority of flawed explanations.
- Core assumption: Human annotators can consistently distinguish between "Fabrication" (making things up) and "Inconsistency" (contradicting known facts).
- Evidence anchors:
  - [abstract] "...identify factual fabrication as the most common failure mode."
  - [section] Page 2, Table 2: Shows high counts for Factual Fabrication (1,703 instances) vs. other error types.
  - [corpus] Weak corpus support; while other benchmarks exist, few provide this specific fine-grained annotation for Chinese explanations.
- Break condition: If the taxonomy categories are ambiguous (e.g., overlapping definitions for "Logical" vs. "Context" inconsistency), the diagnostic signal degrades into noise.

## Foundational Learning

- Concept: **Contamination vs. Contamination-Free Evaluation**
  - Why needed here: Crucial for distinguishing between a model "knowing" a fact because it was trained on it (contamination) versus "reasoning" it out from context or general knowledge (contamination-free).
  - Quick check question: If a model performs well on 2022 data but fails on 2024 data, is it a reasoning failure or a knowledge cutoff issue?

- Concept: **Sycophancy (User Alignment Bias)**
  - Why needed here: Explains the mechanism of "Factual Fabrication." Models often prioritize helpfulness/alignment with the user's likely intent over truthfulness, leading them to invent evidence that supports a false claim.
  - Quick check question: Why would a model invent a "police report" to support a fake crime story?

- Concept: **Ground-Truth Evidence Linking**
  - Why needed here: To evaluate an LLM's *explanation*, one must have authoritative "Gold Evidence" to compare it against. Without this, you cannot distinguish a correct conclusion reached by luck from one reached by valid reasoning.
  - Quick check question: Can you reliably grade an essay if you don't have the answer key?

## Architecture Onboarding

- Component map: CANDYSET (Source) -> Time-Splitter -> Inference Engine -> Evaluation Layer -> Human Annotation
- Critical path: **Data Collection -> Temporal Filtering -> Inference (w/ Taxonomy Prompts) -> Human Annotation of Explanations -> Human-LLM Collaboration Study.**
- Design tradeoffs:
  - **Static Knowledge (Closed-book) vs. Real-time (RAG):** The paper evaluates static knowledge, which is safer for measuring intrinsic capability but less reflective of deployed systems using search tools.
  - **Binary Classification vs. Explanation Generation:** Optimizing for binary accuracy may encourage guessing; requiring explanations forces reasoning but increases inference cost and hallucination risk.
- Failure signatures:
  - **"Templated Authority":** Model cites generic authorities (e.g., "According to CCTV") without specifics -> Flag for Factual Fabrication.
  - **"Temporal Inertia":** Model uses outdated facts for current events -> Flag for Factual Inconsistency.
  - **"Sycophantic Validation":** Model agrees with a claim's premise even when factually wrong -> Flag for Context Inconsistency.
- First 3 experiments:
  1. **Calibration Run:** Test a baseline model (e.g., GPT-3.5) on the contamination-free split to establish the "forgetting curve" or accuracy drop-off post-cut-off date.
  2. **Prompt Ablation:** Compare "Declarative" vs. "Interrogative" prompt structures on a subset of 200 false claims to quantify the reduction in Factual Fabrication rates (hypothesis: >20% reduction).
  3. **Human-in-the-Loop Simulation:** Replicate Task 3 by having a human verify claims using only Search vs. using LLM output to measure the efficiency gain (time saved) and accuracy delta.

## Open Questions the Paper Calls Out

- **Question:** Do the specific error distributions, particularly the dominance of factual fabrication, persist in fact-checking benchmarks for linguistically distinct or low-resource languages?
  - Basis in paper: [explicit] The authors state in the Limitations section that their "findings are inherently constrained by the Chinese-only scope" and explicitly request that "Future work should validate these findings across languages."
  - Why unresolved: Syntactic structures, slang, and cultural contexts differ by language, potentially altering the frequency of specific failure modes like logical inconsistency or cultural misinterpretation.
  - What evidence would resolve it: Applying the CANDY taxonomy to comparable datasets in English, Spanish, or low-resource languages to compare the statistical prevalence of error types.

- **Question:** Can prompting strategies be optimized to reduce factual fabrication without simultaneously inducing the overconfidence observed with Chain-of-Thought (CoT) reasoning?
  - Basis in paper: [inferred] The authors note in Section 5 that CoT often leads to overconfidence while being less accurate, and in Section 6.2 that interrogative prompts reduce fabrication. They explicitly list "fine-tuning prompts" as a substantial challenge in the Limitations.
  - Why unresolved: There appears to be a trade-off where methods that encourage reasoning (CoT) worsen calibration (overconfidence), while format changes (interrogative) help accuracy but may not be robust.
  - What evidence would resolve it: A new prompting framework that achieves high fact-checking F1 scores on contamination-free data while maintaining a low Expected Calibration Error (ECE).

- **Question:** How can models be improved to robustly acknowledge knowledge cutoffs and temporal context to prevent errors on time-sensitive claims?
  - Basis in paper: [inferred] The analysis in Section 6.2 highlights that models struggle to "recognize outdated knowledge" and often refuse to acknowledge their own cutoff dates (acknowledgment rate <30%).
  - Why unresolved: This indicates a lack of "temporal awareness" in current training, causing the model to hallucinate facts for events occurring after its training data window.
  - What evidence would resolve it: A model architecture or fine-tuning method that successfully labels claims as "unverifiable" or correctly applies temporal constraints for post-cutoff events in the "Disasters" and "Society" domains.

## Limitations

- The benchmark's contamination-free evaluation relies on accurate temporal filtering, which may be compromised by vague publication dates or model knowledge cutoff ambiguities
- The taxonomy-based explanation evaluation depends on subjective human judgment with inherent inter-annotator variability
- The human study's online format may introduce sampling bias toward digitally literate participants, limiting generalizability

## Confidence

- **High confidence:** Performance degradation in contamination-free settings (6.9% average accuracy drop) and factual fabrication as the dominant error mode
- **Medium confidence:** The effectiveness of interrogative prompting to reduce sycophancy
- **Low confidence:** The generalizability of human-LLM collaboration benefits beyond the specific fact-checking domain and participant demographics

## Next Checks

1. Replicate the contamination-free evaluation using different model knowledge cutoff date sources to test robustness of the temporal filtering methodology
2. Conduct cross-cultural validation by testing the interrogative vs. declarative prompt effects on non-Chinese LLMs and datasets
3. Extend the human study to include offline verification tasks and participants with varying levels of digital literacy to assess real-world applicability