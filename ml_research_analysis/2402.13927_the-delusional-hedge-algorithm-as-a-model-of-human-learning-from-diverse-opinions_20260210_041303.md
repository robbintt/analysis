---
ver: rpa2
title: The Delusional Hedge Algorithm as a Model of Human Learning from Diverse Opinions
arxiv_id: '2402.13927'
source_url: https://arxiv.org/abs/2402.13927
tags:
- source
- hedge
- learning
- algorithm
- sources
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the classic hedge algorithm to semi-supervised
  learning scenarios where learners must judge which information sources to trust
  when ground-truth labels are sparse. The proposed "delusional hedge" algorithm updates
  source trust using a hallucinated loss on unlabeled trials, based on how often a
  source agrees with others.
---

# The Delusional Hedge Algorithm as a Model of Human Learning from Diverse Opinions

## Quick Facts
- **arXiv ID:** 2402.13927
- **Source URL:** https://arxiv.org/abs/2402.13927
- **Authors:** Yun-Shiuan Chuang; Jerry Zhu; Timothy T. Rogers
- **Reference count:** 5
- **Primary Result:** The delusional hedge algorithm significantly outperformed baseline models in predicting human trust and learning behavior when integrating diverse opinions.

## Executive Summary
This paper presents the "delusional hedge" algorithm, an extension of the classic hedge algorithm for semi-supervised learning scenarios where ground-truth labels are sparse. The model updates trust in information sources by hallucinating losses on unlabeled trials based on majority agreement among sources. Two human experiments tested this model against baselines, showing that people's behavior aligns with the delusional hedge approach, particularly favoring majority sources in fully unsupervised conditions and adjusting trust based on agreement with trusted sources in semi-supervised settings.

## Method Summary
The study developed the delusional hedge algorithm to model how humans learn from diverse opinions when labeled data is limited. The algorithm extends the traditional hedge algorithm by incorporating a hallucination mechanism that estimates losses on unlabeled data based on how often a source agrees with others. Two experiments were conducted where participants evaluated information from multiple sources, some with labeled examples and some without. The model's predictions were compared against human behavior and two baseline approaches: the standard hedge algorithm and an accuracy-majority heuristic.

## Key Results
- The delusional hedge significantly outperformed both the standard hedge algorithm and accuracy-majority heuristic in fitting human data
- In fully unsupervised conditions, participants favored sources that were in the majority, consistent with the hallucination mechanism
- Trust adjustments depended on agreement with trusted sources in semi-supervised settings, matching the model's predictions

## Why This Works (Mechanism)
The delusional hedge works by addressing the core challenge of semi-supervised learning: how to update trust in information sources when ground-truth labels are unavailable. Traditional hedge algorithms fail in these scenarios because they cannot compute losses. The delusional hedge solves this by using majority agreement as a proxy for correctness on unlabeled data, effectively "hallucinating" losses based on social consensus. This mechanism allows the algorithm to continuously update source trust even when explicit feedback is absent, mirroring how humans often rely on consensus when direct verification is impossible.

## Foundational Learning
- **Hedge Algorithm:** Online learning method for combining expert advice; needed because it provides the base framework for sequential decision-making with multiple sources; quick check: verifies it's a multiplicative weight update rule
- **Semi-Supervised Learning:** Machine learning approach using both labeled and unlabeled data; needed because the problem involves sparse ground-truth labels; quick check: confirms unlabeled data is leveraged for learning
- **Multi-Armed Bandit Problems:** Sequential decision-making framework with exploration-exploitation tradeoffs; needed because the trust allocation problem has similar structure; quick check: ensures understanding of reward uncertainty
- **Social Learning Theory:** Framework for how people learn from others' behavior and opinions; needed because the model addresses learning from diverse human opinions; quick check: verifies it involves inference from social information
- **Computational Rationality:** Framework for modeling human decision-making as optimal under constraints; needed because the model tests if human behavior follows rational trust-updating rules; quick check: confirms it involves bounded-optimal strategies

## Architecture Onboarding

**Component Map:**
Input sources (labeled/unlabeled) -> Delusional Loss Calculator -> Trust Weight Updates -> Decision Making -> Performance Evaluation

**Critical Path:**
The core innovation is the delusional loss calculation: unlabeled data -> majority agreement counting -> hallucinated loss assignment -> trust weight update. This path enables learning when traditional loss functions are unavailable.

**Design Tradeoffs:**
- **Pro:** Enables semi-supervised learning where traditional methods fail
- **Con:** Relies on potentially unreliable majority consensus as proxy for truth
- **Pro:** Computationally simple and interpretable
- **Con:** May overfit to agreement patterns that don't reflect actual accuracy
- **Pro:** Matches observed human behavior in controlled experiments
- **Con:** Theoretical justification for hallucination mechanism is limited

**Failure Signatures:**
- Poor performance when majority consensus is systematically wrong
- Over-reliance on agreement patterns in early learning stages
- Failure to detect sources that occasionally disagree but are actually more accurate
- Sensitivity to initial trust weight assignments

**First 3 Experiments:**
1. Test performance when sources systematically disagree with the majority but are actually correct
2. Evaluate robustness to noise in agreement patterns among sources
3. Compare against alternative consensus-based semi-supervised algorithms

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- The hallucination mechanism's theoretical justification is weak, relying on majority agreement without clear cognitive grounding
- The model may not generalize well to real-world scenarios where consensus doesn't reflect truth
- Lack of detailed hypotheses makes it difficult to predict model behavior in untested conditions

## Confidence
- **High confidence:** Model performance significantly exceeds baselines in controlled experiments
- **Medium confidence:** The delusional hedge captures aspects of human learning behavior
- **Low confidence:** Theoretical justification for the hallucination mechanism

## Next Checks
1. Test the model's performance in more naturalistic settings with diverse and noisy information sources
2. Conduct a theoretical analysis to justify the hallucination mechanism and its alignment with cognitive processes
3. Compare the "delusional hedge" model against alternative semi-supervised learning algorithms in a broader range of tasks