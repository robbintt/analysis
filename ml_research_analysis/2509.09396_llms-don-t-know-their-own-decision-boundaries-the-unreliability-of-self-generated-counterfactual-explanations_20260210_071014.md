---
ver: rpa2
title: 'LLMs Don''t Know Their Own Decision Boundaries: The Unreliability of Self-Generated
  Counterfactual Explanations'
arxiv_id: '2509.09396'
source_url: https://arxiv.org/abs/2509.09396
tags:
- sces
- distance
- dataset
- minimal
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Self-generated counterfactual explanations (SCEs) are a method
  for LLMs to explain their decisions by modifying inputs to change predictions. This
  study evaluates whether LLMs can produce SCEs that are both valid (change the model's
  prediction) and minimal (make the smallest necessary edit).
---

# LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations

## Quick Facts
- arXiv ID: 2509.09396
- Source URL: https://arxiv.org/abs/2509.09396
- Reference count: 40
- Primary result: No model consistently produces self-generated counterfactual explanations that are both valid and minimal

## Executive Summary
This study evaluates whether large language models can generate reliable self-generated counterfactual explanations (SCEs) - explanations where models modify their own inputs to change predictions. Across multiple LLMs, datasets, and evaluation settings, the paper reveals a fundamental trade-off: models produce valid but non-minimal explanations when asked generally, but make overly conservative edits when instructed to minimize changes, resulting in invalid explanations. Analysis suggests models fail to engage in self-prediction, a necessary component for identifying minimal valid counterfactuals. These findings indicate SCEs are unreliable as explainability tools and may mislead decision-making in high-stakes applications.

## Method Summary
The paper uses synthetic tabular datasets with discrete features (Income, House Prices, Heart Disease) converted to natural language prompts via templates. Models first predict outcomes across all dataset instances to establish decision boundaries. Then, in separate continuations, they generate counterfactual explanations under two conditions: unconstrained ("make a revision") and minimal ("make a minimal revision"). SCEs are parsed back into tabular form and evaluated for validity (whether they flip predictions) and minimality (using Gower's distance compared to true minimal counterfactuals). The approach enables precise quantification of explanation quality in a controlled setting.

## Key Results
- Models exhibit a robust validity-minimality trade-off: valid SCEs are non-minimal, and minimal SCEs are often invalid
- No model consistently satisfies both validity and minimality across multiple datasets and evaluation settings
- Analysis of reasoning traces shows models don't spontaneously engage in self-prediction, a necessary condition for identifying minimal valid counterfactuals
- The trade-off persists across different prompt formulations and distance metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The validity-minimality trade-off in SCEs is a robust phenomenon across models, datasets, and evaluation settings.
- Mechanism: The paper converts tabular datasets to natural language prompts, then evaluates SCEs in structured tabular space, enabling precise quantifiable assessment.
- Core assumption: Decision boundaries in simplified synthetic datasets are representative of behavior in complex real-world settings.
- Evidence anchors:
  - "The observed validity-minimality trade-off is consistent across several LLMs, datasets, and evaluation settings."
  - "The core innovation of our approach is to use tabular datasets... and then use natural language templates to convert numerical tabular data to natural language inputs."
  - Related work (Dehghanighobadi et al., 2025; Randl et al., 2025) reports high validity but doesn't address minimality.

### Mechanism 2
- Claim: LLMs struggle to spontaneously engage in self-prediction, a necessary condition for identifying minimal valid counterfactuals.
- Mechanism: When asked for minimal SCEs, models treat it as general reasoning rather than self-explanation, appealing to general intuitions instead of their own decision-making process.
- Core assumption: Accurate self-prediction is a necessary condition for generating minimal valid SCEs.
- Evidence anchors:
  - "Models struggle to recognize the self-explanation aspect of the task and do not spontaneously engage in self-prediction, a necessary condition for identifying minimal valid counterfactuals."
  - "DeepSeek-R1 70B does not explicitly attempt to self-predict... It frequently refers to the decision boundary of an external model... and does not question whether the SCE would flip its own decision."
  - Premakumar et al. (2024) discusses self-modeling in neural systems, providing theoretical basis for importance of self-prediction.

### Mechanism 3
- Claim: Current training objectives don't incentivize models to develop an accurate "self-model" required for faithful self-explanation.
- Mechanism: Standard pre-training focuses on external world knowledge, while RLHF optimizes for helpfulness/harmlessness, not self-prediction accuracy.
- Core assumption: The ability to accurately self-predict is not an emergent scaling capability but requires new learning objectives.
- Evidence anchors:
  - "Whether accurate self-prediction is a fundamental limitation of LLMs is unclear... In short, there is no optimisation pressure to develop a self-model (Premakumar et al., 2024)."
  - The corpus contains papers questioning LLM metacognitive abilities and self-knowledge, supporting this as a fundamental unsolved challenge.

## Foundational Learning

- **Self-Generated Counterfactual Explanations (SCEs)**: Explanations where models modify their own inputs to change predictions. Why needed: SCEs are the core object of study. Quick check: How does an SCE differ from a standard post-hoc explanation provided by an LLM?
- **The Validity-Minimality Trade-off**: SCEs can be valid (change prediction) or minimal (smallest edit), but not both. Why needed: This is the paper's central finding. Quick check: When asked for a "minimal" counterfactual, what is the most common failure mode of the LLMs studied?
- **Self-Prediction as a Necessary Condition**: Without ability to predict own behavior, LLM cannot reliably identify minimal valid counterfactuals. Why needed: Explains the "why" behind failure. Quick check: According to the paper, why do current LLM training paradigms fail to encourage development of a "self-model"?

## Architecture Onboarding

- **Component map**: Tabular datasets with discrete features -> Natural language templates -> LLM predictions for decision boundary mapping -> SCE generation (unconstrained/minimal) -> SCE parsing to tabular space -> Validity and minimality evaluation
- **Critical path**:
  1. Design or select simplified tabular dataset with discrete features
  2. Create natural language template to present tabular data to LLM
  3. Elicit predictions for all instances to map decision boundary
  4. For given prediction, prompt model for counterfactual explanation in unconstrained and minimal settings
  5. Parse model's output back into tabular space
  6. Evaluate Validity (by checking pre-computed decision boundary) and Minimality (using Gower's Distance)
  7. Analyze failure modes: decision boundary consistency, distance function operationalization, self-prediction evidence
- **Design tradeoffs**:
  - Synthetic vs. Real Datasets: Synthetic datasets enable tractable complete input space for precise minimality measurement, at cost of ecological validity
  - Gower's Distance vs. Other Metrics: Chosen for simplicity and LLM reliability in-context; results robust to other metrics (L1, L2, semantic)
  - Reasoning Traces Analysis: Used as proxy for internal reasoning, acknowledging debate around faithfulness but providing valuable insights
- **Failure signatures**:
  - Trivial Validity: Model jumps to extreme opposite of feature space, producing valid but non-minimal SCEs
  - Conservative Invalidation: Model makes tiny edit that fails to cross decision boundary, prioritizing minimality over validity
  - Lack of Self-Prediction in Traces: Reasoning discusses general principles without questioning how model would classify new input
- **First 3 experiments**:
  1. Reproduce core finding: Select one model and dataset, run full SCE generation and evaluation pipeline to confirm validity-minimality trade-off
  2. Operationalizing Distance Test: Provide model with starting point and four candidate points, ask to identify closest using Gower's Distance
  3. Prompt Sensitivity Check: Generate variations of SCE-eliciting prompts using different model, test target model's performance

## Open Questions the Paper Calls Out
None

## Limitations
- Findings rely on synthetic, simplified datasets with discrete features rather than real-world continuous, high-dimensional data
- The paper doesn't fully resolve debates about faithfulness of reasoning traces as indicators of internal model reasoning
- Whether training paradigms fundamentally preclude self-model development versus requiring architectural changes remains speculative

## Confidence
- **High Confidence**: Existence of validity-minimality trade-off across multiple models and datasets
- **Medium Confidence**: Mechanism explanation linking self-prediction failure to SCE unreliability
- **Low Confidence**: Whether current training paradigms fundamentally preclude self-model development

## Next Checks
1. **Ecological Validity Test**: Replicate SCE evaluation using real-world tabular dataset with continuous features to assess if validity-minimality trade-off persists
2. **Alternative Distance Metrics**: Verify robustness by computing minimality using L1, L2, and semantic similarity metrics instead of Gower's distance
3. **Self-Prediction Ablation**: Design controlled experiment where models must explicitly predict own behavior before generating SCEs, testing if forced self-prediction improves minimality without sacrificing validity