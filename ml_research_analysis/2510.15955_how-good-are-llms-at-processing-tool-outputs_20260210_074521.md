---
ver: rpa2
title: How Good Are LLMs at Processing Tool Outputs?
arxiv_id: '2510.15955'
source_url: https://arxiv.org/abs/2510.15955
tags:
- answer
- response
- code
- json
- none
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the challenge of extracting information from
  complex JSON tool responses using large language models (LLMs). The authors formulate
  this as a question-answering task, create a dataset of real-world API responses
  and questions, and evaluate 15 open and closed weight models using multiple prompting
  approaches.
---

# How Good Are LLMs at Processing Tool Outputs?

## Quick Facts
- arXiv ID: 2510.15955
- Source URL: https://arxiv.org/abs/2510.15955
- Reference count: 40
- Primary result: JSON processing remains difficult even for frontier models, with code generation outperforming answer generation for filtering/aggregation tasks by 3-50%

## Executive Summary
This paper studies the challenge of extracting information from complex JSON tool responses using large language models (LLMs). The authors formulate this as a question-answering task, create a dataset of real-world API responses and questions, and evaluate 15 open and closed weight models using multiple prompting approaches. Results show that JSON processing remains difficult even for frontier models, with performance varying significantly based on response length, question type, and prompting strategy. Code generation outperforms answer generation for filtering and aggregation tasks, while including the response schema and full JSON response improves performance. Performance differences of 3% to 50% are observed across approaches.

## Method Summary
The study creates a dataset of 1,298 QA triplets from 6 RapidAPI endpoints, covering hotel booking, car rentals, flight seats, SEC filings, and product search. Questions are categorized as extractive (580), filtering (394), or aggregation (324). The authors evaluate 15 models using zero-shot prompting across four control points: output type (answer vs code generation), prompt type (Chain-of-Thought vs none), schema inclusion, and response handling (full/reduced/excluded). Code generation executes generated Python in a sandbox. Evaluation uses three metrics: exact match with relaxations, contains check, and LLM-as-judge using Llama-3-3-70b-instruct.

## Key Results
- Code generation outperforms answer generation for filtering and aggregation tasks, with performance improvements ranging from 3% to 50%
- Including JSON response schema improves performance by up to 12%, with stronger effects for code generation than answer generation
- LLM performance degrades with increasing JSON response length, with some models showing 7-91% accuracy drops
- Models exhibit recency bias, with 5-75% variance based on answer position in JSON responses

## Why This Works (Mechanism)

### Mechanism 1: Code Generation for Structured Processing
- **Claim:** Code generation outperforms answer generation for filtering and aggregation tasks, with performance improvements ranging from 3% to 50%.
- **Mechanism:** LLMs generate executable Python code that systematically traverses JSON structures, applies filters, and performs aggregations with precise logic, avoiding the ambiguity and inconsistency inherent in natural language reasoning over structured data.
- **Core assumption:** The generated code executes without errors and correctly implements the intended logic.
- **Evidence anchors:** [Abstract] "Code generation outperforms answer generation for filtering and aggregation tasks"; [Section 4.3] "In general, the total accuracy when generating code is better for 13 out of 15 models"
- **Break condition:** Code execution failures increase with context length; models with higher execution error rates may negate the benefits of code generation

### Mechanism 2: Schema-Informed Reasoning
- **Claim:** Including the JSON response schema in prompts improves performance by up to 12%, with stronger effects for code generation than answer generation.
- **Mechanism:** The schema provides explicit type information, required fields, and structural constraints that reduce ambiguity in key selection and data interpretation, enabling more accurate code generation and reasoning.
- **Core assumption:** The schema accurately represents the response structure and the model can effectively use schema information during generation.
- **Evidence anchors:** [Abstract] "including the response schema and full JSON response improves performance"; [Section 4.4] "both prompting for the answer and code generally benefit from adding the schema information"
- **Break condition:** Schema quality matters; incomplete or inaccurate schemas may mislead models rather than help them

### Mechanism 3: Context Length and Position Effects
- **Claim:** LLM performance degrades with increasing JSON response length, with some models showing 7-91% accuracy drops; models also exhibit recency bias (5-75% variance based on answer position).
- **Mechanism:** Longer contexts strain attention mechanisms and working memory, causing models to lose track of relevant information mid-sequence; recent tokens receive disproportionately higher attention, creating position-dependent performance.
- **Core assumption:** The observed degradation reflects fundamental limitations in how models process long contexts rather than artifacts of the specific dataset or prompting setup.
- **Evidence anchors:** [Section 4.1] "Performance generally declines as response size increases"; [Section 4.2] "recency bias was observed in answer position in the JSON"
- **Break condition:** For JSON processing tasks, this mechanism suggests chunking or hierarchical processing may help; condensed responses (12x smaller) in oracle experiments improved accuracy 8-38 points

## Foundational Learning

### Concept: JSON Schema Definition
- **Why needed here:** Understanding schemas is prerequisite to implementing the paper's most effective processing strategies. Without schema knowledge, you cannot evaluate whether adding schema information will help your specific use case.
- **Quick check question:** Given a sample JSON response from your target API, can you write a JSON schema that captures all required fields, data types, and nested structure?

### Concept: Question Type Taxonomy (Extractive, Filtering, Aggregation)
- **Why needed here:** The paper demonstrates that optimal processing strategy depends entirely on question type. Routing extractive queries to answer generation and filtering/aggregation queries to code generation can maximize overall accuracy.
- **Quick check question:** For a query "List all products with price under $50, sorted by rating," which category does it belong to, and which processing approach should you use?

### Concept: Context Window and Token Limits
- **Why needed here:** The paper only evaluates models with 65K+ token context windows due to large JSON responses. Understanding token economics is essential for determining whether your tool responses will fit and what fallback strategies are needed.
- **Quick check question:** If your average API response is 50,000 characters, approximately how many tokens does this represent, and what context window size do you need?

## Architecture Onboarding

### Component Map
User Query → Query Classifier (optional) → Processing Router
                                           ↓
                    ┌──────────────────────┴──────────────────────┐
                    ↓                                              ↓
            Answer Generation                              Code Generation
            (full response + schema)                (schema + response variant)
                    ↓                                              ↓
            Direct Answer                                   Code Interpreter
                                                                  ↓
                                                           Executed Result

### Critical Path
1. **Query classification:** Determine if query is extractive, filtering, or aggregation (paper shows DeepSeek-V3 achieves 80% accuracy on this task)
2. **Routing decision:** Route extractive → answer generation; filtering/aggregation → code generation
3. **Context management:** If response exceeds context window, use condensed response (keep one sample of each unique key) rather than excluding response entirely
4. **Code execution sandboxing:** All generated code must execute in isolated sandbox (paper notes security concerns)

### Design Tradeoffs
- **Answer generation vs. code generation:** Answer generation produces verbose output (harder to parse programmatically) but requires no execution environment; code generation produces structured output but requires sandboxed execution and may fail on edge cases
- **Full vs. condensed response:** Full response maximizes accuracy but risks context overflow; condensed response reduces context usage (12x smaller) with some accuracy tradeoff
- **Schema inclusion:** Helps all code generation and most answer generation, but requires schema generation/maintenance overhead

### Failure Signatures
1. **Semantic ambiguity:** Multiple similar keys (e.g., "name", "room_name", "name_without_policy") cause wrong key selection
2. **Format misalignment:** Models ignore formatting instructions, add units when told not to, or wrap answers in extra JSON
3. **Structural confusion:** Models select correct key from wrong nested record (e.g., room area from wrong room type)
4. **Code execution errors:** Wrong data type assumptions, incorrect JSON traversal, or missing null checks (up to 45% of errors in some models)
5. **Verbosity in answer generation:** Some models output 1000+ characters for questions requiring 6-character answers

### First 3 Experiments
1. **Baseline comparison:** Run your JSON processing task with both answer generation and code generation (both with schema) on a sample of 50 queries, measure exact match accuracy, and identify which query types fail for each approach
2. **Context length stress test:** Vary JSON response size from 10K to 80K tokens and measure accuracy degradation rate for your chosen model; this establishes your safe operating envelope
3. **Query classifier evaluation:** Implement zero-shot query classification using DeepSeek-V3 or GPT-4o on your query distribution, measure classification accuracy, and calculate the potential accuracy gain from routing vs. single-approach baseline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do the performance trade-offs of answer versus code generation manifest in multi-turn, end-to-end agentic workflows compared to the isolated tool-response task?
- **Basis in paper:** [explicit] "However, it would be important to evaluate the different tool response processing approaches in an end-to-end setting."
- **Why unresolved:** This study isolated the processing step; cumulative errors or context growth in a full agent loop were not measured.
- **What evidence would resolve it:** Results from applying these strategies within a full agent architecture (like WebArena) on complex, multi-step tasks.

### Open Question 2
- **Question:** How do Retrieval-Augmented Generation (RAG) or agentic code generation approaches compare to the zero-shot answer and code generation strategies evaluated in this paper?
- **Basis in paper:** [explicit] "Moreover, the processing approaches we considered are limited to answer and code generation, but there are other approaches such as RAG on the API response or an agentic approach to code generation."
- **Why unresolved:** The authors excluded RAG and agentic approaches despite their popularity, citing preliminary observations of information loss or scope limitations.
- **What evidence would resolve it:** Comparative metrics (Exact Match, Contains) for RAG and iterative code-refinement agents on the same dataset used in the paper.

### Open Question 3
- **Question:** Can a heuristic or learned pre-processing step approximate the "Oracle Projection" performance gains without access to ground truth information?
- **Basis in paper:** [inferred] The authors note that the "Oracle Projection" method (simplifying JSON knowing the ground truth) significantly improves performance, but admit it "assumes the ground truth is known which is not realistic."
- **Why unresolved:** It is currently unknown how to effectively simplify large JSON structures to aid LLMs without knowing beforehand which specific data points are relevant to the query.
- **What evidence would resolve it:** A study evaluating schema-aware or semantic-pruning algorithms that reduce JSON size blindly, achieving accuracy gains closer to the reported oracle baseline.

## Limitations

- The study relies on a curated dataset of 1,298 QA samples from 6 specific RapidAPI endpoints, which may not generalize to all JSON processing scenarios.
- The response reduction heuristic lacks full implementation details in the paper, making exact reproduction difficult.
- The evaluation uses a fixed context window of 65K+ tokens, but real-world tool responses could exceed available context in practical deployments.

## Confidence

**High Confidence:** The core finding that code generation outperforms answer generation for filtering and aggregation tasks is well-supported by the experimental results across 15 models. The performance degradation with response length and the benefits of schema inclusion are consistently observed patterns.

**Medium Confidence:** The routing strategy recommendations (extractive→answer, filtering/aggregation→code) depend on the accuracy of query classification, which achieved 80% on the test set. This suggests potential accuracy gains but also indicates 20% misclassification risk that could negate benefits.

**Low Confidence:** The absolute performance numbers (accuracy percentages) are difficult to contextualize without knowing the baseline human performance on these tasks or how representative the sample questions are of real-world usage patterns.

## Next Checks

1. **Dataset generalization test:** Apply the same processing strategies to a different JSON processing dataset (e.g., from different API domains or synthetic generation) to verify that the code generation advantage holds across domains.

2. **Context overflow stress test:** Systematically evaluate models with varying context window sizes (32K, 128K, 200K tokens) to determine the exact point where performance degradation becomes severe and whether the condensed response strategy scales effectively.

3. **Query classification accuracy impact:** Implement the recommended routing strategy on a held-out test set and measure the actual accuracy improvement compared to always using the better-performing single approach, accounting for classification errors.