---
ver: rpa2
title: 'Learning Through Little Eyes: Attribute Discrimination Beyond Objects'
arxiv_id: '2512.18951'
source_url: https://arxiv.org/abs/2512.18951
tags:
- discrimination
- cvcl
- color
- size
- attribute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces a controlled benchmark to evaluate attribute\
  \ discrimination\u2014color, size, and texture\u2014within object categories in\
  \ vision-language models trained on infant-scale egocentric video. Building on prior\
  \ infant learning models, it systematically varies these attributes across everyday\
  \ object classes using synthetic data to isolate attribute recognition from object\
  \ identity."
---

# Learning Through Little Eyes: Attribute Discrimination Beyond Objects

## Quick Facts
- **arXiv ID:** 2512.18951
- **Source URL:** https://arxiv.org/abs/2512.18951
- **Reference count:** 0
- **Primary result:** CVCL outperforms CLIP in size discrimination but fails on color, while CLIP excels at color but struggles with size and texture.

## Executive Summary
This work introduces a controlled benchmark to evaluate attribute discrimination—color, size, and texture—within object categories in vision-language models trained on infant-scale egocentric video. Building on prior infant learning models, it systematically varies these attributes across everyday object classes using synthetic data to isolate attribute recognition from object identity. Evaluations compare an infant-scale model (CVCL) and a web-scale model (CLIP) under prototype and text-vision modes. CVCL excels at size discrimination, especially for infant-relevant objects, while CLIP strongly grounds color but fails on size and texture. Both models encode texture visually but lack linguistic grounding. The findings reveal complementary strengths of different training regimes and highlight gaps in cross-modal attribute representation, offering a framework for probing fine-grained perceptual learning in human-like learning conditions.

## Method Summary
The benchmark uses synthetic images based on the Konkle 'things' dataset, systematically varying color (9 values), size (3 bins), and texture (2 variants) across 67 object classes. For each class, 108 images are generated using OmniGen2, creating balanced coverage of all attribute combinations on uniform backgrounds. The evaluation uses 4-way forced-choice tasks where models must select the target attribute within the same object class. Two models are evaluated: CVCL (ResNeXt-50, trained on ~37K infant egocentric frames) and CLIP (ResNet-50, trained on 400M web image-text pairs). Performance is measured in two modes: prototype (mean of normalized embeddings for objects sharing the target attribute) and text-vision (cosine similarity between image and text embeddings).

## Key Results
- CVCL shows 62.3% accuracy on size discrimination versus CLIP's 50.5% in prototype mode
- CLIP achieves 69.2% accuracy on color discrimination versus CVCL's 19.6% in prototype mode
- Both models perform similarly well on texture discrimination (87.2% CVCL vs 89.0% CLIP) in prototype mode but fail on size and texture in text-vision mode
- CVCL achieves 78.0% overall classification accuracy in prototype mode but collapses to 26.7% near chance in text-vision mode

## Why This Works (Mechanism)

### Mechanism 1: Training Data Regime Induces Attribute-Specific Biases
Different training data distributions (infant egocentric vs. web-scale) lead to complementary attribute discrimination strengths. Infant egocentric video naturally contains objects at varying distances and perspectives, biasing CVCL toward robust size representations. Web-scale image-text pairs frequently include color adjectives in captions, biasing CLIP toward color grounding. This reflects the statistical structure of training data rather than architectural differences alone.

### Mechanism 2: Visual-Linguistic Grounding Asymmetry
Visual representation and linguistic grounding are dissociable; models can form consistent visual clusters without grounding attributes to language. Contrastive learning aligns image and text embeddings at the object-category level but may not enforce fine-grained attribute alignment. Texture features are encoded visually but lack consistent text associations in both training regimes.

### Mechanism 3: Synthetic Benchmark Enables Controlled Attribute Isolation
Systematic synthetic image generation with controlled attribute variation isolates specific perceptual abilities while minimizing confounds. By crossing color × size × texture across 67 object classes on uniform backgrounds, the benchmark ensures any performance difference reflects attribute discrimination rather than contextual cues.

## Foundational Learning

- **Concept: Contrastive Language-Image Pretraining (CLIP-style)**
  - Why needed here: Both CVCL and CLIP use contrastive learning to align image and text embeddings; understanding this is essential for interpreting why attributes may be visually encoded but linguistically ungrounded.
  - Quick check question: Can you explain why contrastive loss at the object-category level might not enforce fine-grained attribute alignment?

- **Concept: Prototype/Nearest-Centroid Classification**
  - Why needed here: The benchmark uses prototype mode to evaluate perceptual grouping without language, analogous to pre-linguistic infant perception.
  - Quick check question: How would you compute a prototype embedding for "green ball" from a set of images, and why is re-normalization necessary after averaging?

- **Concept: Cross-Modal Grounding**
  - Why needed here: The text-vision evaluation mode tests whether linguistic descriptions map correctly to visual features—the core question of whether attributes are linguistically grounded.
  - Quick check question: Why might a model correctly cluster "bumpy" objects visually but fail when prompted with the word "bumpy"?

## Architecture Onboarding

- **Component map:** Input image/text → Visual encoder (ResNeXt-50/ResNet-50) → 512-dim embedding → L2 normalization → Cosine similarity computation → Argmax selection
- **Critical path:** 1. Input: Image (224×224) or text prompt (e.g., "red cup") 2. Encoder produces 512-dim embedding 3. L2 normalization 4. Cosine similarity computation between query and candidates 5. Argmax selection
- **Design tradeoffs:** Prototype mode (image-only) vs. text-encoder mode: tests perceptual grouping vs. linguistic grounding separately; Synthetic vs. real images: control vs. ecological validity; 4-way forced choice: balances granularity with statistical power
- **Failure signatures:** CVCL text-vision collapse (26.7% near chance on overall classification) indicates weak language-vision alignment despite reasonable prototype performance (78.0%); Both models near chance on size/texture in text-vision mode indicates these attributes lack linguistic grounding in either training regime; CVCL color at 19.6% (prototype) suggests color is not reliably encoded visually
- **First 3 experiments:**
  1. Hybrid training test: Combine infant-scale egocentric data with web-scale color-captioned data to test whether complementary strengths merge
  2. Texture word analysis: Examine text encoder embeddings for texture words (smooth/bumpy) to quantify their linguistic representation quality
  3. Size prompt engineering: Test alternative size prompts (e.g., "tiny/medium/huge" vs. "small/medium/large") to probe whether vocabulary choice affects grounding

## Open Questions the Paper Calls Out

- **Open Question 1:** Can hybrid training strategies effectively combine the complementary strengths of infant-scale and web-scale learning to create models robust in both size and color discrimination? The study only evaluates models trained in isolation, finding they have opposing strengths (size vs. color) rather than a unified solution.
- **Open Question 2:** Why do models successfully encode texture in visual embeddings but fail to ground these features linguistically? The paper identifies the disconnect but does not isolate whether the failure stems from the text encoder's vocabulary limitations or the alignment objective.
- **Open Question 3:** Do the observed inductive biases of infant-scale (size) and web-scale (color) training persist when evaluating attributes like shape and material? The current benchmark is restricted to color, size, and texture, leaving the generalizability of these training regime effects unknown.

## Limitations

- The synthetic benchmark may not fully capture naturalistic attribute grounding, potentially limiting ecological validity
- Text-encoder prompts and exact trial generation procedures are incompletely specified, creating potential reproducibility gaps
- The dissociation between visual encoding and linguistic grounding is less conclusively shown for size, where both models fail similarly in text-vision mode

## Confidence

- **High:** CVCL's superior size discrimination and CLIP's superior color discrimination in prototype mode (directly measured, consistent with training data differences)
- **Medium:** The visual-linguistic grounding asymmetry for texture (visually encoded but not linguistically grounded by either model)
- **Low:** CVCL's poor color performance in prototype mode—may reflect training data limitations or synthetic image artifacts

## Next Checks

1. **Hybrid training experiment:** Train a model on combined infant-scale egocentric video and web-scale color-captioned data to test whether complementary attribute strengths can be merged
2. **Texture vocabulary analysis:** Quantify text encoder embeddings for texture-related words (smooth, bumpy, rough, etc.) to measure linguistic representation quality independent of visual features
3. **Size prompt ablation study:** Systematically vary size prompt vocabulary (tiny/small/medium/large/huge) to determine whether vocabulary choice affects linguistic grounding success