---
ver: rpa2
title: 'Retriv at BLP-2025 Task 2: Test-Driven Feedback-Guided Framework for Bangla-to-Python
  Code Generation'
arxiv_id: '2511.07382'
source_url: https://arxiv.org/abs/2511.07382
tags:
- code
- bangla
- test
- cases
- python
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a test-driven, feedback-guided iterative refinement
  approach for Bangla-to-Python code generation, combining instruction prompting with
  a fine-tuned Qwen2.5-14B model. Their method generates code from Bangla instructions,
  tests it against unit tests, and iteratively refines failing outputs through three
  evaluation passes using test feedback.
---

# Retriv at BLP-2025 Task 2: Test-Driven Feedback-Guided Framework for Bangla-to-Python Code Generation

## Quick Facts
- arXiv ID: 2511.07382
- Source URL: https://arxiv.org/abs/2511.07382
- Reference count: 19
- Primary result: 2nd place in BLP-2025 Task 2 with Pass@1 = 0.934

## Executive Summary
This work introduces a test-driven, feedback-guided iterative refinement approach for Bangla-to-Python code generation. The method combines instruction prompting with a fine-tuned Qwen2.5-14B model, generating code from Bangla instructions, testing against unit tests, and iteratively refining failing outputs through three evaluation passes using test feedback. The approach achieved 2nd place in the BLP-2025 shared task with a Pass@1 score of 0.934. The work addresses challenges in Bangla instruction understanding and Python code generation for low-resource languages, demonstrating that careful translation, efficient fine-tuning, and test-case-aware inference can yield state-of-the-art performance with open-weight models.

## Method Summary
The framework uses a three-stage pipeline: (1) Translation of Bangla instructions to English using a test-case-aware prompt that includes the full test suite, (2) Code generation from the translated English instruction using a Qwen2.5-14B model fine-tuned with QLoRA (rank r=128, learning rate 2e-4) on a combined dataset of BLP training samples and MBPP, and (3) Execution and feedback-driven refinement where failed code is re-prompted up to three times with escalating temperature (0.1→0.3→0.5) and error traces. The system runs on a single RTX 3090 Ti with 24GB memory, using 768 max tokens per generation and a 30-second timeout for execution.

## Key Results
- Achieved 2nd place in BLP-2025 Task 2 with Pass@1 = 0.934
- Translation from Bangla to English improved Pass@1 from 0.74 to 0.81 on Qwen2.5-Coder-14B
- Feedback-guided refinement provided +4.44% gain over fine-tuning alone
- Most recoveries came from correcting off-by-one errors, handling edge cases, or aligning outputs with expected formats

## Why This Works (Mechanism)

### Mechanism 1: Test-Case-Aware Translation
Including unit tests in the translation prompt improves semantic fidelity when converting Bangla instructions to English by conditioning on both the natural language instruction and the expected input/output behavior from test cases. This reduces ambiguity in idiomatic or technical phrasing, though semantic drift can still occur with mistranslated idioms.

### Mechanism 2: Parameter-Efficient Fine-Tuning with High Adapter Rank
QLoRA fine-tuning with high adapter rank (r=128) captures nuanced instruction-to-code mappings better than lower ranks by increasing adapter capacity while preserving pretrained knowledge. The approach found that any added dropout degraded performance, suggesting that preserving the full training signal was more important than regularization on the limited training data.

### Mechanism 3: Iterative Self-Correction with Escalating Temperature
Re-prompting with execution feedback and gradually increasing sampling temperature recovers certain error classes by encouraging exploration of alternative solutions. Failed tests provide concrete error traces that the model can interpret, though failures from mistranslations or deep reasoning gaps are rarely resolved by this approach.

## Foundational Learning

- **Pass@k Metric**: The core evaluation metric measuring whether the single best prediction passes all hidden tests. Quick check: If a model generates 10 candidates and 5 pass, would Pass@1 necessarily be 0.5? (Answer: No—Pass@1 is the probability the top-1 sample passes, not the pass rate across samples.)

- **QLoRA (Quantized Low-Rank Adaptation)**: Enables fine-tuning a 14B model on 24GB GPU memory by combining 4-bit weight quantization with trainable low-rank matrices. Quick check: What two techniques does QLoRA combine to reduce memory requirements? (Answer: 4-bit weight quantization + low-rank adapter modules.)

- **Temperature in Sampling**: Controls exploration vs. exploitation in generation, with this system escalating temperature across retries. Quick check: Would temperature = 0.1 produce more or less diverse outputs than temperature = 0.5? (Answer: Less diverse—lower temperature concentrates probability mass on fewer tokens.)

## Architecture Onboarding

- **Component map**: Bangla input → Translation (with tests) → Code generation → Execution → (if fail) Retry with error feedback × 3 max
- **Critical path**: Translation module conditions on Bangla instruction + test_list to produce English instruction; fine-tuned generator produces Python code; execution environment runs code against unit tests with 30s timeout; feedback loop controller manages up to 3 retries with escalating temperature
- **Design tradeoffs**: Translation adds a potential failure point but aligns with English-centric pretraining; higher adapter rank (r=128) increases capacity but may overfit to limited training data; temperature escalation aids exploration but may increase variance in later retries
- **Failure signatures**: Semantic drift from mistranslated idioms propagating incorrect logic; unrecoverable errors from deep reasoning gaps or translation errors; timeout/execution failures for complex generated code
- **First 3 experiments**: 1) Baseline benchmark on all candidate models with original Bangla instructions to establish Pass@1 baselines and select base model; 2) Translation ablation comparing Pass@1 on Bangla vs. translated English instructions to quantify translation benefit per model; 3) Feedback loop analysis logging which error types are recovered in each retry pass to characterize correction boundaries

## Open Questions the Paper Calls Out

### Open Question 1
How can translation fidelity for idiomatic Bangla expressions be improved to prevent semantic drift that propagates errors into code generation? The paper explicitly states this remains a key challenge, providing an example where "প্রতিদ্বন্দ্বিতা" was mistranslated as "competition" instead of "reciprocal." A systematic analysis of translation error types and their downstream impact would help resolve this.

### Open Question 2
What is the upper bound of inference-time self-correction for fixing errors rooted in semantic misunderstanding versus syntactic mistakes? Section 6.4 notes that failures caused by mistranslations or deeper reasoning gaps were rarely resolved, underscoring the limits of inference-time self-correction. Categorizing failure types and their recovery rates across multiple refinement passes would help characterize these boundaries.

### Open Question 3
How much performance gain could full-precision LoRA or larger open-weight models provide over QLoRA on a 14B model for Bangla-to-Python generation? Hardware constraints restricted the authors to QLoRA fine-tuning on a single GPU, preventing exploration of full-precision LoRA or larger models. A controlled comparison would quantify the potential improvements.

### Open Question 4
How dependent is feedback-guided refinement on the informativeness of error tracebacks, and can structured error representations improve correction success? The feedback loop depended on the quality of error traces, with retries seldom leading to meaningful corrections when tracebacks did not identify issues. Comparing raw tracebacks vs. summarized/structured error feedback would evaluate this dependency.

## Limitations
- Translation-to-English pipeline may introduce semantic drift difficult to detect without manual inspection of instruction-test alignment
- High adapter rank (r=128) with no dropout may overfit to the small training set (474 samples), with generalization to diverse Bangla instructions untested
- Feedback loop effectiveness is bounded, recovering only ~4.44% of errors while failing to resolve mistranslations or deep reasoning gaps
- MBPP data preprocessing, mixing ratio, and prompt template are unspecified, making exact replication challenging

## Confidence
- **High confidence** in the overall Pass@1 result (0.934) and 2nd-place ranking, as these are task-reported metrics
- **Medium confidence** in translation benefit (Pass@1 from 0.74 to 0.81) and feedback loop gain (+4.44%), based on ablation studies but lacking detailed error-type breakdowns
- **Low confidence** in long-term generalization of high-rank QLoRA configuration, as no out-of-distribution or follow-up task evaluation is reported

## Next Checks
1. **Translation fidelity audit**: Manually inspect a stratified sample of Bangla instructions and their translated English counterparts to quantify semantic drift and identify common mistranslation patterns
2. **Generalization probe**: Evaluate the fine-tuned model on a held-out subset of MBPP or other Bangla-to-code datasets not seen during training to test robustness to instruction style variation
3. **Error recovery classification**: Log and categorize the specific error types recovered in each retry pass (e.g., off-by-one, edge case, formatting) to bound the feedback loop's practical utility and identify irreducible failure modes