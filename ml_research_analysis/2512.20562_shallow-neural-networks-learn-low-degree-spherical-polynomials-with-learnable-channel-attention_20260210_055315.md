---
ver: rpa2
title: Shallow Neural Networks Learn Low-Degree Spherical Polynomials with Learnable
  Channel Attention
arxiv_id: '2512.20562'
source_url: https://arxiv.org/abs/2512.20562
tags:
- theorem
- follows
- learning
- neural
- channel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies learning low-degree spherical polynomials using\
  \ over-parameterized two-layer neural networks with channel attention. The problem\
  \ setup considers nonparametric regression on the unit sphere in Rd, where the target\
  \ function is a degree-\u21130 polynomial with \u21130 = \u0398(1) \u2265 1."
---

# Shallow Neural Networks Learn Low-Degree Spherical Polynomials with Learnable Channel Attention

## Quick Facts
- arXiv ID: 2512.20562
- Source URL: https://arxiv.org/abs/2512.20562
- Authors: Yingzhen Yang
- Reference count: 40
- Primary result: Achieves sample complexity n ≍ Θ(dℓ₀/ε) for learning degree-ℓ₀ spherical polynomials, improving previous bound of Θ(dℓ₀ max{ε⁻², log d})

## Executive Summary
This paper establishes that over-parameterized two-layer neural networks with learnable channel attention can optimally learn low-degree spherical polynomials. The work introduces a novel two-stage training approach that first identifies the correct number of channels ℓ₀ using one-step gradient descent, then trains the second-layer weights. The method achieves minimax optimal nonparametric regression risk of order Θ(dℓ₀/n) with significantly improved sample complexity compared to previous results.

## Method Summary
The approach employs a two-stage training process. In Stage 1, a learnable channel selection algorithm uses one-step gradient descent to identify the ground-truth channel number ℓ₀ among L ≥ ℓ₀ initial channels. Stage 2 then trains the second-layer weights using standard gradient descent with the selected channels. The analysis considers nonparametric regression on the unit sphere in ℝᵈ where the target function is a degree-ℓ₀ polynomial with ℓ₀ = Θ(1) ≥ 1.

## Key Results
- Achieves sample complexity n ≍ Θ(dℓ₀/ε) with probability 1-δ
- Trained network achieves minimax optimal nonparametric regression risk of order Θ(dℓ₀/n)
- Represents first finite-width neural network training approach to achieve minimax optimality for learning low-degree spherical polynomials with feature learning capability

## Why This Works (Mechanism)
The method works by combining learnable channel attention with a two-stage optimization process. The channel selection mechanism identifies the correct model capacity needed to represent the target function, avoiding overfitting while maintaining sufficient expressivity. The spherical polynomial structure allows for efficient gradient-based selection of relevant channels.

## Foundational Learning
- **Spherical polynomials**: Polynomials restricted to the unit sphere in high dimensions; needed for understanding the function class being learned; quick check: verify ℓ₀ ≤ d-1
- **Nonparametric regression**: Statistical framework for function approximation without parametric assumptions; needed for analyzing generalization; quick check: confirm ε ∈ (0,1) for risk bounds
- **Channel attention**: Mechanism for selecting relevant feature dimensions; needed for the learnable selection process; quick check: verify L ≥ ℓ₀ initial channels
- **Minimax optimality**: Theoretical benchmark for best possible sample complexity; needed for establishing the significance of results; quick check: compare against known lower bounds
- **Gaussian initialization**: Standard neural network initialization scheme; needed for analysis of gradient-based optimization; quick check: verify initialization variance is appropriate

## Architecture Onboarding
**Component map**: Input features -> First layer (ℓ₀ channels) -> Channel selection (Stage 1) -> Second layer training (Stage 2) -> Output

**Critical path**: The learnable channel selection mechanism is critical, as it determines the effective model capacity. If channel selection fails, the subsequent training stage cannot recover optimal performance.

**Design tradeoffs**: The method trades increased computational complexity in Stage 1 for improved sample efficiency. The one-step gradient descent approach balances between computational tractability and selection accuracy.

**Failure signatures**: Poor channel selection leading to either underfitting (too few channels) or overfitting (too many channels). The analysis assumes idealized conditions that may not hold in practice.

**First experiments**:
1. Verify channel selection accuracy on synthetic spherical polynomial data with known ℓ₀
2. Test regression risk scaling with sample size n across different values of d and ℓ₀
3. Compare performance against standard two-layer networks without channel attention

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis assumes idealized conditions including Gaussian initialization and infinite-width behavior
- Channel selection mechanism relies on one-step gradient descent, potentially sensitive to initialization
- Results are specific to spherical polynomial setting and may not generalize to other function classes

## Confidence
- **Mathematical correctness**: High confidence in theoretical analysis and proofs
- **Practical applicability**: Medium confidence in translation to real neural networks
- **Generalizability**: Medium confidence in extending beyond spherical polynomials

## Next Checks
1. Implement the proposed two-stage training algorithm on synthetic spherical polynomial data to verify claimed sample complexity improvements empirically
2. Test channel selection mechanism's robustness to different initialization schemes and noise levels
3. Evaluate whether similar sample complexity improvements can be achieved for learning low-degree polynomials on other domains (hypercubes or Euclidean space)