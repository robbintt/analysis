---
ver: rpa2
title: Proactive Assistant Dialogue Generation from Streaming Egocentric Videos
arxiv_id: '2506.05904'
source_url: https://arxiv.org/abs/2506.05904
tags:
- assistant
- task
- video
- user
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of generating real-time, proactive
  task guidance from streaming egocentric videos, addressing challenges of high-cost
  data collection and lack of evaluation frameworks. It introduces PROASSIST, a large-scale
  synthetic dialogue dataset created by transforming annotated egocentric videos into
  task-oriented dialogues using LLM-based generation.
---

# Proactive Assistant Dialogue Generation from Streaming Egocentric Videos

## Quick Facts
- **arXiv ID**: 2506.05904
- **Source URL**: https://arxiv.org/abs/2506.05904
- **Reference count**: 40
- **Primary result**: Synthetic egocentric video dialogues with automatic metrics validated by human studies

## Executive Summary
This paper introduces PROASSIST, a large-scale synthetic dialogue dataset for streaming egocentric video understanding, and develops an end-to-end model for proactive task guidance. The work addresses the challenge of real-time dialogue generation from egocentric videos, where models must decide both when to speak and what to say. Through automatic evaluation metrics and human studies, the authors demonstrate that task-specific knowledge and memory mechanisms significantly improve performance, while improved visual perception alone provides limited gains. The results establish a foundation for real-time AI task assistance, though limitations remain in generalization and evaluation robustness.

## Method Summary
The method transforms annotated egocentric videos into task-oriented dialogues using LLM-based generation, creating PROASSIST (30,135 dialogues, 479 hours). The end-to-end model is based on VideoLLM-Online with two key innovations: negative frame sub-sampling to handle class imbalance in speaking decisions, and iterative progress summarization to process long-horizon videos. The model uses LLaMA-3.1-8B-Instruct with SigLIP-SO400M frame encoder, visual tokens per frame (I=1/5/10), and LoRA for fine-tuning. Two automatic evaluation metrics are proposed: pairwise matching for response alignment and LLM-as-a-judge for quality assessment, both validated through human studies.

## Key Results
- Task-specific knowledge (recipes) significantly improves guidance quality more than enhanced visual perception alone
- Negative frame sub-sampling improves response timing decisions under class imbalance (F1 improved from 30.1 to 58.7 for action narration)
- Iterative progress summarization enables long-horizon task tracking within fixed context windows

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Negative frame sub-sampling improves response timing decisions under class imbalance
- **Mechanism**: During training, gradients are computed only for positive (speaking) frames and a uniformly sampled subset (ρ=0.1) of negative (silent) frames
- **Core assumption**: The imbalance ratio between silent and speaking frames is the primary bottleneck for learning when-to-speak decisions
- **Evidence anchors**: F1 improved from 30.1 to 58.7 for action narration, 32.9 to 34.4 for dialogue generation

### Mechanism 2
- **Claim**: Iterative progress summarization enables long-horizon task tracking within fixed context windows
- **Mechanism**: When context window approaches capacity (L=4096 tokens), the model generates a structured summary and injects it into the system prompt for the next chunk
- **Core assumption**: Progress summaries can capture sufficient task state without requiring specialized memory architectures
- **Evidence anchors**: IPS outperforms Drop-Middle baseline (F1: 32.9 vs 25.7), 86% of Ego4D samples must be truncated when L=4096

### Mechanism 3
- **Claim**: Task-specific knowledge (e.g., recipes) significantly improves guidance quality more than enhanced visual perception
- **Mechanism**: Providing ground-truth recipes as system prompts allows the model to align its guidance with the specific procedure shown in the video
- **Core assumption**: The evaluation video represents one valid approach among many; without knowledge, models may suggest correct-but-unseen alternatives
- **Evidence anchors**: Overall score improved from 2.15 to 2.46 (I=1), 2.10 to 2.41 (I=5), 2.15 to 2.42 (I=10)

## Foundational Learning

- **Concept**: Class imbalance in sequential decision-making
  - **Why needed here**: Speaking decisions are sparse (~1-2% of frames), causing models to over-predict silence
  - **Quick check question**: If your validation F1 is low but precision is high, is your threshold too conservative or your training biased toward silence?

- **Concept**: Context window management for streaming inputs
  - **Why needed here**: Hour-long videos exceed typical LLM context limits; naive truncation loses early task goals
  - **Quick check question**: What information must be preserved when resetting context—initial goal, completed steps, or both?

- **Concept**: Synthetic data validation via human alignment
  - **Why needed here**: LLM-generated dialogues require quality checks; automatic metrics must correlate with human judgment
  - **Quick check question**: If your synthetic dialogue F1 is 0.8 but human alignment score is 2.5/5, which signal do you trust?

## Architecture Onboarding

- **Component map**: Frame extraction -> SigLIP encoding -> Token projection -> Interleaved text-visual sequence -> Speaking decision head -> Progress summarizer
- **Critical path**: 1. Frame extraction (2 FPS) → SigLIP encoding → Token projection 2. Interleaved text-visual sequence construction 3. Per-frame speaking decision with threshold θ 4. If context > L, trigger summary generation and restart with compressed state
- **Design tradeoffs**:
  - Tokens per frame (I=1/5/10): Higher I improves action narration (+14 F1) but not dialogue generation (+0.2 F1)
  - Sub-sampling ratio (ρ): ρ=0.1 optimal; lower ratios lose diversity, higher ratios reintroduce imbalance
  - Threshold θ: Precision-recall tradeoff; select via validation F1 local maximum (typically 0.2-0.5)
- **Failure signatures**:
  - Model never speaks: θ too high or training without NFS
  - Model speaks at wrong times: Check temporal cost parameters
  - Lost task goal mid-video: IPS not triggered or summary missing critical fields
  - Hallucinated steps not in video: Recipe over-constraining
- **First 3 experiments**:
  1. Sanity check: Run inference with θ=0.5 on 10 validation videos; verify speaking frequency matches ground-truth ratio
  2. Ablation: Train without NFS; expect F1 drop of ~25 points on action narration
  3. Context limit stress test: Evaluate on videos >20 minutes; compare IPS vs. Drop-Middle on task-goal retention

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can synthetic dialogue datasets be generated directly from raw egocentric videos without relying on pre-existing human annotations?
- **Basis in paper**: The dataset's reliance on pre-existing video annotations limits its scalability
- **Why unresolved**: Current pipeline requires timestamped action descriptions from existing datasets
- **What evidence would resolve it**: A pipeline that produces dialogues of comparable quality to annotation-based methods

### Open Question 2
- **Question**: How can proactive assistant models better generalize to unseen tasks and underrepresented domains?
- **Basis in paper**: Performance analysis shows significant variation across PROASSIST subsets
- **Why unresolved**: Current models show strong task familiarity effects
- **What evidence would resolve it**: Experiments showing sustained performance on held-out domains with novel task types

### Open Question 3
- **Question**: Can the speaking threshold θ be selected adaptively per-task without requiring a validation set?
- **Basis in paper**: The chosen threshold is optimal for average performance across a set of videos rather than for individual tasks
- **Why unresolved**: Current approach uses validation F1 to select a fixed threshold
- **What evidence would resolve it**: An adaptive method achieving higher human preference alignment than fixed-threshold baselines

## Limitations
- Synthetic data generation approach creates potential gap between synthetic and naturally occurring task guidance scenarios
- Model's dependence on ground-truth task knowledge raises concerns about knowledge-poor environments
- Evaluation metrics may not fully capture nuanced quality of proactive assistance in practical settings

## Confidence

**High Confidence**: Negative frame sub-sampling effectiveness (F1 improvements of 28.6 points for action narration)

**Medium Confidence**: Superiority of task-specific knowledge over enhanced visual perception (results show consistent improvements but rely on ground-truth knowledge availability)

**Medium Confidence**: Pairwise matching metric alignment with human judgment (validation shows correlation but based on limited subset)

## Next Checks

1. **Real-world deployment stress test**: Deploy the model on a small set of real egocentric videos with actual human-in-the-loop feedback, measuring performance degradation compared to synthetic evaluation

2. **Knowledge-free generalization study**: Evaluate the model on tasks without available recipes or procedural knowledge, measuring how performance degrades and whether alternative knowledge acquisition methods can compensate

3. **Temporal precision benchmark**: Create a fine-grained temporal alignment test set where precise timing of assistance is critical, measuring whether the model's speaking decisions maintain accuracy over extended sequences and under varying frame rates