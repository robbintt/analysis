---
ver: rpa2
title: 'SPGISpeech 2.0: Transcribed multi-speaker financial audio for speaker-tagged
  transcription'
arxiv_id: '2508.05554'
source_url: https://arxiv.org/abs/2508.05554
tags:
- speaker
- spgispeech
- snippets
- dataset
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPGISpeech 2.0 is a 3,780-hour speaker-tagged transcription dataset
  derived from earnings calls, designed to improve speaker-related ASR tasks. It provides
  41,593 unique speakers with fully-formatted transcriptions and speaker alignment
  information for multi-talker ASR.
---

# SPGISpeech 2.0: Transcribed multi-speaker financial audio for speaker-tagged transcription

## Quick Facts
- **arXiv ID:** 2508.05554
- **Source URL:** https://arxiv.org/abs/2508.05554
- **Reference count:** 0
- **Primary result:** 3,780-hour speaker-tagged transcription dataset enabling 15.88% cpWER and 7.25% WER on multi-speaker earnings calls

## Executive Summary
SPGISpeech 2.0 is a 3,780-hour speaker-tagged transcription dataset derived from earnings calls, designed to improve speaker-related ASR tasks. It provides 41,593 unique speakers with fully-formatted transcriptions and speaker alignment information for multi-talker ASR. The dataset includes 50–90 second audio snippets with speaker change markers and word-level timestamps, enabling training of models for speaker diarization and recognition. Experiments with a Canary-170M ASR model and a Sortformer-based model fine-tuned on SPGISpeech 2.0 show reduced concatenated minimum permutation WER (cpWER) from 20.53% to 15.88% and WER from 7.96% to 7.25% when using speaker supervision, outperforming baseline models. SPGISpeech 2.0 is released free for non-commercial use, aiming to advance speech recognition and speaker diarization research.

## Method Summary
The authors construct SPGISpeech 2.0 by processing earnings call audio from S&P Global Market Intelligence, applying voice activity detection, forced alignment, and quality filtering to create 50-90 second snippets with speaker annotations. They evaluate using a Canary-170M ASR model fine-tuned end-to-end with a Sortformer-123M architecture that jointly optimizes transcription and speaker diarization through sort loss. The dataset provides multiple transcription variants including style-guide compliant and algorithmically denormalized versions to support both clean and literal training objectives.

## Key Results
- Achieved 15.88% cpWER and 7.25% WER on multi-speaker earnings call audio
- Reduced cpWER from 20.53% to 15.88% and WER from 7.96% to 7.25% with speaker supervision
- Dataset contains 3,780 hours, 41,593 unique speakers, 154,971 training snippets

## Why This Works (Mechanism)

### Mechanism 1
Longer audio snippets (50-90 seconds) with multiple speaker segments enable more robust speaker diarization training than short-form clips. Extended duration increases exposure to speaker transitions (up to 25 per snippet, 2-7 speakers per snippet), providing supervision signal for turn-taking detection and speaker embedding discrimination across longer temporal contexts.

### Mechanism 2
Integrating speaker supervision during ASR fine-tuning jointly improves transcription accuracy and speaker-tagging performance. The Sortformer architecture adds a parallel encoder for speaker diarization to the Canary decoder, enabling shared representations and sort loss to resolve speaker permutation ambiguity during end-to-end training.

### Mechanism 3
Algorithmic denormalization of professionally transcribed text recovers literal speech content for more accurate ASR training. Professional transcriptions following style guides remove disfluencies, filler words, and normalize numeric expressions. The authors run a literal-trained ASR model to align with gold labels, then cross-reference to reintroduce removed elements (stutters, filler words, denormalized numbers).

## Foundational Learning

- **Concept: Speaker Diarization**
  - Why needed here: This is a core task SPGISpeech 2.0 enables—partitioning audio into segments attributed to individual speakers.
  - Quick check question: Given a 60-second audio clip with 3 speakers, can you explain why permutation-invariant training is necessary?

- **Concept: cpWER (Concatenated Minimum Permutation Word Error Rate)**
  - Why needed here: This is the primary evaluation metric for speaker-tagged transcription, combining ASR accuracy with speaker attribution correctness.
  - Quick check question: Why does cpWER require finding the "optimal permutation" of predicted speaker segments before computing WER?

- **Concept: Forced Alignment**
  - Why needed here: The dataset relies on alignment pipelines (Gentle, NeMo) to generate word-level timestamps that enable per-word speaker annotations.
  - Quick check question: What could go wrong if alignment quality is poor at snippet boundaries?

## Architecture Onboarding

- **Component map:**
Raw earnings call audio (16kHz, mono)
↓
Voice Activity Detection (py-webrtcvad)
↓
Forced Alignment (Gentle + NeMo cross-reference)
↓
Snippet Selection (50-90s, ≥2 speakers, quality filters)
↓
Output files:
- Audio snippet (.wav)
- Human transcription (with speaker-change markers "|")
- Speaker alignments (SegLST format)
- Word-level timestamps + speaker IDs (JSON)
- Algorithmically normalized transcription

- **Critical path:**
1. Load snippet and corresponding SegLST/JSON alignment files
2. Verify speaker ID consistency across snippet groupings from same call
3. Handle unknown speakers (ID = -1) appropriately during training (exclusion or special treatment)
4. Ensure chronological split integrity (train < dev < test) to prevent leakage

- **Design tradeoffs:**
- Longer snippets (50-90s) improve diarization training but reduce pool of usable candidates (Q&A portions are messier)
- Style-guide compliant transcriptions are cleaner but less literal; use algorithmically adjusted version for literal training
- Professional transcription quality comes at cost of removed disfluencies and normalized numbers

- **Failure signatures:**
- High WER on numerical entities → style guide normalization not properly denormalized
- Poor speaker separation at segment boundaries → alignment quality issues at snippet start/end
- Unexpectedly low cpWER with reasonable WER → speaker ID mapping errors or unknown speaker handling

- **First 3 experiments:**
1. Establish baseline by evaluating pre-trained Canary or Whisper on test set (both with-PnC and without-PnC settings) to quantify domain gap.
2. Fine-tune ASR-only model on SPGISpeech 2.0 to isolate transcription improvement from speaker supervision contribution.
3. Compare cpWER vs. WER gap with and without Sortformer speaker supervision to validate joint training benefit.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology and limitations suggest several areas for future research, including testing domain generalizability beyond financial audio, validating the effectiveness of algorithmic denormalization against human-verified verbatim transcriptions, and evaluating model robustness on conversational datasets with high speaker overlap.

## Limitations
- Domain-specific financial vocabulary and corporate personnel demographics may limit generalizability to other speech contexts
- Algorithmic reconstruction of disfluencies and filler words may introduce noise compared to ground-truth verbatim transcription
- Relatively low prevalence of overlapping speech in earnings calls may limit robustness to chaotic, conversational environments

## Confidence

- **High Confidence:** The dataset's basic specifications (3,780 hours, 41,593 speakers, 50-90 second snippets with speaker change markers) and its release under non-commercial license terms are clearly stated and verifiable.
- **Medium Confidence:** The reported performance improvements (cpWER reduction from 20.53% to 15.88%, WER from 7.96% to 7.25%) are supported by the methodology description, but the exact reproduction would require access to specific model checkpoints and hyperparameter configurations not fully detailed in the paper.
- **Low Confidence:** The relative importance and effectiveness of individual components (algorithmic denormalization, Sortformer architecture, joint optimization) cannot be independently validated without ablation studies that are not provided.

## Next Checks

1. **Reproduce baseline performance:** Establish a baseline by fine-tuning a standard ASR model (e.g., Whisper or NeMo's pre-trained ASR) on SPGISpeech 2.0 without speaker supervision to quantify the contribution of speaker-tagged training data alone.

2. **Validate denormalization quality:** Conduct a human evaluation study comparing transcriptions generated from algorithmically denormalized labels versus style-guide compliant labels to verify that literal speech content recovery improves ASR accuracy as claimed.

3. **Test domain generalizability:** Evaluate the fine-tuned models on multi-speaker datasets from different domains (e.g., conversational telephone speech or broadcast news) to assess whether the earnings-call-specific training generalizes beyond financial audio.