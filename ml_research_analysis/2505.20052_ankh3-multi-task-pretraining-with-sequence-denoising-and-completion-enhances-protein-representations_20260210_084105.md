---
ver: rpa2
title: 'Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion Enhances
  Protein Representations'
arxiv_id: '2505.20052'
source_url: https://arxiv.org/abs/2505.20052
tags:
- protein
- sequence
- tasks
- ankh3
- sequences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether multi-task pre-training can enhance
  protein language models beyond traditional single-task approaches. It introduces
  Ankh3, which combines masked language modeling with multiple masking probabilities
  and sequence completion tasks using only protein sequence inputs.
---

# Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion Enhances Protein Representations

## Quick Facts
- arXiv ID: 2505.20052
- Source URL: https://arxiv.org/abs/2505.20052
- Reference count: 4
- Multi-task pre-training with denoising and completion improves protein representation learning

## Executive Summary
Ankh3 introduces a novel multi-task pre-training approach for protein language models that combines masked language modeling with sequence completion tasks. The model leverages only protein sequence inputs and demonstrates state-of-the-art performance across diverse protein prediction tasks. By using multiple masking probabilities and sequence completion, Ankh3 addresses the limitations of single-task pre-training approaches and achieves significant improvements in secondary structure prediction, fluorescence prediction, GB1 fitness, and contact prediction.

## Method Summary
Ankh3 employs a T5 encoder-decoder transformer architecture with two variants (Large and XL) containing 1.88B and 5.73B parameters respectively. The model is pre-trained on 59 million UniRef50 protein sequences using a combination of masked language modeling with three different masking probabilities (15%, 20%, 30%) and sequence completion for the remaining 50% of input sequences. This multi-task approach aims to capture diverse protein sequence patterns and improve generalization across downstream tasks. The pre-training strategy leverages protein sequence data alone, avoiding the need for additional structural or functional annotations.

## Key Results
- Outperforms previous protein language models in secondary structure prediction (SSP-3: 84.4% accuracy vs 81.4% previous best)
- Achieves superior fluorescence prediction with Spearman correlation of 64.6 vs 62.0
- Excels in GB1 fitness prediction (90.3 vs 89.6) and contact prediction (ProteinNet L/5: 83.9 vs 60.95 precision)

## Why This Works (Mechanism)
The multi-task pre-training approach works by forcing the model to learn robust protein representations through diverse training objectives. Masked language modeling with multiple masking probabilities encourages the model to capture local sequence patterns and dependencies at different granularities. Sequence completion tasks require understanding of global sequence context and long-range dependencies, which are crucial for protein structure and function prediction. By combining these tasks during pre-training, Ankh3 develops more comprehensive protein representations that generalize better to downstream tasks.

## Foundational Learning
- **Protein sequence representation**: Learning distributed representations of amino acid sequences that capture functional and structural information
  - Why needed: Proteins are sequences of amino acids where local and global patterns determine structure and function
  - Quick check: Can the model predict missing amino acids in partially masked sequences?

- **Transformer self-attention**: Capturing long-range dependencies in protein sequences through multi-head attention mechanisms
  - Why needed: Protein function often depends on interactions between distant amino acids
  - Quick check: Does attention weight distribution show meaningful patterns across sequence positions?

- **Masked language modeling**: Learning to predict masked tokens in protein sequences
  - Why needed: Forces the model to understand context and dependencies in sequences
  - Quick check: Can the model accurately predict masked amino acids given surrounding context?

- **Sequence completion**: Reconstructing entire missing subsequences from protein sequences
  - Why needed: Requires understanding of global sequence patterns and protein domain structures
  - Quick check: Can the model complete biologically plausible protein subsequences?

## Architecture Onboarding

**Component map:**
T5 encoder-decoder transformer -> Multi-task pre-training (MLM + sequence completion) -> Downstream protein prediction tasks

**Critical path:**
Protein sequence input → T5 encoder → Multi-task pre-training objectives → Encoder-decoder attention → Protein representation → Downstream task prediction

**Design tradeoffs:**
The study chose T5 architecture for its proven effectiveness in sequence-to-sequence tasks and flexibility in handling both denoising and completion objectives. The multi-task approach trades increased pre-training complexity for improved downstream performance. Using only sequence inputs simplifies data requirements but may limit the model's ability to capture structural information compared to models using structural data.

**Failure signatures:**
- Poor performance on tasks requiring understanding of protein structure despite sequence-level accuracy
- Overfitting to specific masking patterns if masking probabilities are not varied appropriately
- Degraded performance if sequence completion tasks are too difficult relative to the model's capacity

**First experiments:**
1. Evaluate SSP-3 accuracy on held-out protein sequences with known secondary structures
2. Test fluorescence prediction on proteins with experimentally measured fluorescence properties
3. Assess contact prediction precision using ProteinNet dataset with known residue-residue contacts

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains vary significantly across tasks, suggesting multi-task pre-training may be more beneficial for certain protein prediction problems
- The model relies on T5 architecture modifications, raising questions about whether similar gains could be achieved with alternative backbone architectures
- Lack of direct comparison to other multi-task protein language models limits claims about the superiority of the multi-task approach

## Confidence
- **High confidence**: Performance improvements on evaluated downstream tasks, architectural details, and pre-training methodology
- **Medium confidence**: Generalization claims to all protein modeling tasks, as evaluation was limited to specific benchmark datasets
- **Medium confidence**: Claims about the superiority of multi-task pre-training, given lack of direct comparison to other multi-task approaches

## Next Checks
1. Conduct ablation studies comparing single-task vs multi-task pre-training using identical architectures and training data to isolate the contribution of multi-tasking
2. Test model performance on additional protein prediction tasks (e.g., protein-protein interaction prediction, subcellular localization) to assess broader generalization
3. Compare Ankh3's performance against multi-task protein language models from other domains (e.g., biomedical text or genomics) to validate domain-specific advantages