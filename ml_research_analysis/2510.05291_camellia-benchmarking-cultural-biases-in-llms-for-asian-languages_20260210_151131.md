---
ver: rpa2
title: 'Camellia: Benchmarking Cultural Biases in LLMs for Asian Languages'
arxiv_id: '2510.05291'
source_url: https://arxiv.org/abs/2510.05291
tags:
- entities
- asian
- cultural
- language
- western
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Camellia, a benchmark for measuring entity-centric
  cultural biases in LLMs across nine Asian languages. It provides 19,530 manually
  annotated cultural entities and 2,173 naturally occurring masked contexts from social
  media posts, covering six distinct Asian cultures.
---

# Camellia: Benchmarking Cultural Biases in LLMs for Asian Languages

## Quick Facts
- arXiv ID: 2510.05291
- Source URL: https://arxiv.org/abs/2510.05291
- Reference count: 38
- Key outcome: LLMs show 30-40% Western entity preference in Asian-language contexts, distinct sentiment biases across model families, and large extraction gaps that shrink in English translation

## Executive Summary
This paper introduces Camellia, a benchmark for measuring entity-centric cultural biases in large language models across nine Asian languages. The benchmark provides 19,530 manually annotated cultural entities and 2,173 naturally occurring masked contexts from social media posts, covering six distinct Asian cultures. The authors evaluate four multilingual LLM families across three tasks: cultural context adaptation, sentiment association, and extractive QA. Results show that LLMs struggle with cultural adaptation in Asian languages, assigning better likelihood to Western entities in 30-40% of cases. Different LLM families exhibit distinct sentiment biases, with Qwen associating Asian entities more positively and Llama/Gemma associating Western entities more negatively. LLMs also show large performance gaps in entity extraction between Asian and Western entities in Asian languages, though these gaps shrink significantly when tested in English, highlighting the challenges of cultural and linguistic diversity in multilingual models.

## Method Summary
The Camellia benchmark evaluates cultural bias through three tasks: Cultural Bias Score (CBS) measuring likelihood preferences for Western vs. Asian entities in masked contexts, sentiment association classification with entity-swapped contexts, and extractive QA accuracy for entity recognition. The benchmark uses 19,530 cultural entities (6 types) and 2,173 masked social media contexts across 9 Asian languages. Models are evaluated via zero-shot inference using vLLM with temperature=0, top_p=1, top_k=1, max_tokens=30. CBS requires computing log-likelihoods for multi-token entities, while sentiment and QA tasks use prompted classification and extraction respectively.

## Key Results
- LLMs assign better likelihood to Western entities in 30-40% of Asian-language cultural contexts
- Qwen models associate Asian entities more positively while Llama/Gemma associate Western entities more negatively
- Entity extraction accuracy gaps between Asian and Western entities are 10-20% in Asian languages but shrink to 1-5% in English
- Different LLM families show distinct bias patterns consistent with their training data provenance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs assign higher likelihood to Western entities in Asian-language contexts even when culturally inappropriate, as measured by Cultural Bias Score (CBS).
- Mechanism: Pre-training data skews toward Western entities and contexts; the model learns stronger statistical associations for Western entity tokens, which dominate over sparser Asian entity representations during masked-token probability calculation.
- Core assumption: Entity likelihood approximates the model's learned cultural association strength; higher token probability reflects overrepresentation in pre-training data (not explicit cultural preference).
- Evidence anchors:
  - [abstract] "LLMs struggle with cultural adaptation in Asian languages, assigning better likelihood to Western entities in 30-40% of cases."
  - [section 3.1] CBS definition and formula: CBS measures percentage of LLM preference for Western over Asian entities within the same cultural context.
  - [corpus] Related work (e.g., "On The Origin of Cultural Biases in Language Models") suggests pre-training data and linguistic phenomena contribute to Western entity preference.

### Mechanism 2
- Claim: Different LLM families exhibit distinct sentiment associations with Asian vs. Western entities, influenced by training data provenance.
- Mechanism: Spurious correlations between cultural entities and sentiment emerge from context co-occurrence patterns in pre-training corpora (e.g., Asian entities in positive contexts in Qwen's data; Western entities in negative contexts in Llama/Gemma's data).
- Core assumption: Sentiment prediction is sensitive to entity identity, not just sentence context; bias reflects training data distribution rather than explicit programming.
- Evidence anchors:
  - [abstract] "Qwen associating Asian entities more positively and Llama/Gemma associating Western entities more negatively."
  - [section 3.2] Experimental setup: identical contexts with swapped entities show sentiment prediction shifts.
  - [corpus] "Measuring South Asian Biases in Large Language Models" similarly finds sentiment and stereotype biases tied to training data representation.

### Mechanism 3
- Claim: Entity extraction accuracy gaps between Asian and Western entities in Asian languages shrink in English, indicating language-specific context understanding limitations.
- Mechanism: Asian entities may be tokenized into sub-optimal units or have multi-sense overlap, causing confusion; lower representation in pre-training reduces robustness for implicit context reasoning.
- Core assumption: Extraction gaps stem from linguistic and representational issues, not fundamental inability to understand cultural concepts; English translation simplifies tokenization and increases entity familiarity.
- Evidence anchors:
  - [abstract] "large performance gaps in entity extraction between Asian and Western entities in Asian languages... these gaps shrink significantly when tested in English."
  - [section 3.3] Table 1 shows ΔAccuracy for Asian vs. Western entities: gaps of 10-20% in Asian languages vs. 1-5% in English.
  - [corpus] Weak direct evidence in provided neighbors; related work (e.g., "MCEval") focuses on cultural knowledge gaps, not specifically entity extraction.

## Foundational Learning

- **Cultural Bias in LLMs**: Why needed here: The entire paper operationalizes and measures cultural bias; without this, CBS, sentiment association, and extraction gaps cannot be interpreted. Quick check question: How does CBS quantify Western bias? (Answer: Percentage of cases where LLM assigns higher likelihood to Western over Asian entities in a shared cultural context.)

- **Multilingual Representation**: Why needed here: Explains why performance gaps exist in Asian languages but not in English; ties to tokenization, sub-word splitting, and training data proportions. Quick check question: Why might an Asian entity be harder to extract in its native language than in English? (Answer: Tokenization differences, multi-sense overlap, lower pre-training frequency.)

- **Entity-Centric Evaluation**: Why needed here: Camellia focuses on entities (names, foods, locations) as cultural markers, not just general language proficiency; evaluation tasks revolve around entity handling. Quick check question: What are the three Camellia tasks? (Answer: Cultural context adaptation, sentiment association, extractive QA.)

## Architecture Onboarding

- **Component map**: Data Layer (19,530 entities, 2,173 contexts) -> Evaluation Layer (CBS, Sentiment, QA) -> Model Interface (LLM inference via log-likelihood and prompts)

- **Critical path**: 1) Load entities and contexts for target language/culture. 2) Run CBS: compute log-likelihoods for Asian vs. Western entities in grounded contexts. 3) Run sentiment association: classify sentiment for contexts filled with random entity samples. 4) Run extractive QA: prompt models to extract entities from paragraph contexts. 5) Compare metrics across entities, languages, and models.

- **Design tradeoffs**:
  - Entity parallelization: Western entities parallelized across languages for comparability; required manual translation for low-resource languages (trade-off: consistency vs. annotation cost).
  - Context sourcing: Used social media for naturalness; may introduce domain bias (trade-off: ecological validity vs. controllability).
  - Entity types: Focused on 6 types with cultural variation; excluded clothing due to daily-use differences (trade-off: breadth vs. cultural universality).

- **Failure signatures**:
  - High CBS (>50%) in grounded contexts → model severely overfits Western entities.
  - Large sentiment false positive/negative gaps (>50%) → entity identity overrides context.
  - Extraction accuracy gaps >15% in Asian languages with near-zero gaps in English → language-specific representational issues.

- **First 3 experiments**:
  1. Replicate CBS for one language (e.g., Hindi) with two LLMs to verify bias magnitude.
  2. Ablate entity type (e.g., names vs. food) to identify which types drive extraction gaps.
  3. Translate a subset of contexts to English and run extraction to confirm gap reduction as reported.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the robust context understanding demonstrated in English be effectively transferred to Asian languages to minimize the performance gap in entity extraction?
- Basis in paper: [inferred] The abstract and Section 3.3 highlight that performance gaps between Asian and Western entities are large (10-20%) in native languages but shrink significantly (to 1-5%) when tested on English translations.
- Why unresolved: The paper identifies the gap but does not propose methods to align native language performance with the superior English performance.
- What evidence would resolve it: Experiments showing that specific instruction-tuning or alignment techniques successfully close the extraction accuracy gap in low-resource Asian languages.

### Open Question 2
- Question: To what extent are the distinct sentiment biases (e.g., Qwen's "Asian positivity" vs. Llama's "Western negativity") caused by specific spurious correlations in pre-training data versus model architecture?
- Basis in paper: [explicit] Section 3.2 states that "different LLM families hold their distinct biases" and speculates these are "likely a reflection of differences in their training data."
- Why unresolved: The paper observes the divergent behaviors across model families but lacks a causal analysis of the specific data sources or training dynamics that produce these opposing sentiment associations.
- What evidence would resolve it: A causal mediation analysis or data ablation study identifying specific corpus subsets responsible for the observed sentiment shifts.

### Open Question 3
- Question: How can cultural benchmarks be designed to remain robust against temporal drift, given that static sources like Wikidata often contain outdated naming conventions?
- Basis in paper: [explicit] Section 4 discusses challenges in data collection, noting that "Entity naming conventions can be subject to temporal change" and that Wikidata lists are often "mostly outdated names."
- Why unresolved: The paper relies on static snapshots of data which inherently suffer from staleness, and does not offer a solution for maintaining benchmark validity over time.
- What evidence would resolve it: A dynamic benchmarking framework or a methodology that automatically filters or weights entities based on contemporary usage frequency.

## Limitations

- Limited cultural coverage beyond Asian contexts: Findings may be specific to Asian-Western dichotomy rather than universal LLM phenomenon
- Dataset construction artifacts: Social media contexts may not fully represent each culture's linguistic diversity
- Task-specific sensitivity: CBS and other metrics may not capture all manifestations of cultural bias in downstream generation

## Confidence

- **High confidence** in core finding that LLMs exhibit measurable cultural bias favoring Western entities in Asian language contexts
- **Medium confidence** in mechanistic explanations linking pre-training data distribution to observed biases
- **Medium confidence** in language-specific findings (extraction gaps shrinking in English)

## Next Checks

1. **Cross-cultural generalization test**: Apply Camellia framework to different cultural pairing (e.g., African vs. European entities in African languages) to determine if bias patterns are specific to Asian-Western contexts or represent broader LLM phenomenon.

2. **Ablation study on training data composition**: Systematically vary representation of cultural entities during fine-tuning of open-weight models to measure causal impact on CBS scores and extraction accuracy, directly testing data distribution hypothesis.

3. **Downstream generation bias analysis**: Extend beyond evaluation tasks to measure how cultural biases manifest in actual generation scenarios using different sampling strategies, validating whether evaluation metrics predict real-world behavior.