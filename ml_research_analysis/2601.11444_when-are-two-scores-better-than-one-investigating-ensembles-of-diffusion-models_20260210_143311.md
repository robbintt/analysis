---
ver: rpa2
title: When Are Two Scores Better Than One? Investigating Ensembles of Diffusion Models
arxiv_id: '2601.11444'
source_url: https://arxiv.org/abs/2601.11444
tags:
- diffusion
- score
- section
- uni00000013
- ensemble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Ensembling diffusion models typically improves the score matching
  loss but fails to consistently enhance perceptual quality metrics like FID on image
  datasets. The study investigates various aggregation strategies, including arithmetic
  mean, geometric mean, and mixture of experts, across CIFAR-10 and FFHQ datasets.
---

# When Are Two Scores Better Than One? Investigating Ensembles of Diffusion Models

## Quick Facts
- **arXiv ID:** 2601.11444
- **Source URL:** https://arxiv.org/abs/2601.11444
- **Reference count:** 40
- **Primary result:** Ensembling diffusion models typically improves score matching loss but fails to consistently enhance perceptual quality metrics like FID.

## Executive Summary
This paper systematically investigates whether ensembling multiple diffusion models improves generative performance beyond individual models. Through extensive experiments on CIFAR-10 and FFHQ datasets, the study reveals that while ensembling consistently reduces the score matching loss (DDSM), it fails to translate these gains into improved perceptual quality metrics like FID. The research examines various aggregation strategies including arithmetic mean, geometric mean, mixture of experts, and dominant feature approaches, finding that standard democratic ensembling offers limited returns relative to computational costs. Theoretical analysis demonstrates that diffusion and model composition operations do not commute, explaining why simple score averaging fails to produce the expected Product-of-Experts sampling behavior.

## Method Summary
The study trains K independently trained diffusion models (U-Nets for images, Random Forests for tabular data) using identical architectures but different random seeds. These models generate score estimates at each timestep during the reverse diffusion process. Various aggregation strategies are then applied to combine these K score estimates before feeding them to the SDE solver. The aggregation occurs per-timestep, with arithmetic mean being the primary baseline. For image datasets, DDPM++ architecture is used with DDIM sampling (100 steps, entropy 0.5). For tabular data, Random Forests are trained on disjoint data subsets with 100 trees each. The key experimental design varies ensemble size (K=5 for CIFAR-10, K=4 for FFHQ) and aggregation method while measuring both the training objective (LDDSM) and perceptual quality (FID-10k, KID).

## Key Results
- Ensembling consistently reduces score matching loss (LDDSM) across all aggregation methods tested
- Perceptual quality (FID/KID) typically stays between individual model performances, rarely exceeding the best individual model
- Mixture of Experts strategy achieves marginal improvements on FFHQ (FID 20.36 vs best individual 21.7)
- Dominant feature aggregation works for Random Forests but catastrophically fails for neural networks (FID 46.79 on FFHQ)
- Theoretical proof shows score averaging does not sample from the geometric mean (Product-of-Experts) distribution

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Averaging K score estimators monotonically improves the expected DDSM loss.
- **Mechanism:** Given K identically distributed score estimators, Jensen's inequality applied to the convex MSE loss yields LDDSM(s_θ^(K+1)) ≤ E[LDDSM(s_θ^K)] in expectation. The arithmetic mean of outputs corresponds to averaging over the implicit distribution induced by training randomness.
- **Core assumption:** Individual score model outputs are identically distributed conditional on (x_t, t) — satisfied by independent training runs with same architecture and data.
- **Evidence anchors:** [abstract] "ensembling the scores generally improves the score-matching loss and model likelihood"; [section] Proposition 4.1 proves monotonicity; Section C.1 provides full derivation.

### Mechanism 2
- **Claim:** Improved score matching loss does not translate to improved perceptual quality metrics.
- **Mechanism:** FID/KID respond chaotically to small perturbations in score estimates. Training loss drops sharply early then plateaus while FID improves steadily. Adding noise to scores at magnitudes similar to ensemble improvements causes unpredictable FID swings, indicating that FID captures different signal than LDDSM.
- **Core assumption:** Perceptual metrics capture qualities orthogonal to denoising accuracy at individual timesteps.
- **Evidence anchors:** [abstract] "fails to consistently enhance perceptual quality metrics such as FID"; [section] Figure 7 shows divergence between LDDSM and FID trajectories; Figure 7b shows chaotic FID response to score noise.

### Mechanism 3
- **Claim:** Score averaging does not sample from the geometric mean (Product-of-Experts) of the underlying distributions.
- **Mechanism:** Diffusion and composition operations do not commute: adding noise to distributions then composing (geometric mean) ≠ composing then diffusing. Proposition 4.2 provides a Gaussian counterexample where equality holds only when all initial distributions are identical (trivial case).
- **Core assumption:** The step-wise score s_θ(x_t, t) approximates ∇_x log q_t(x) for the marginal at each noise level.
- **Evidence anchors:** [abstract] "Theoretical insights reveal that diffusion and model composition do not commute"; [section] Proposition 4.2 proves non-commutativity for Gaussians with diagonal covariance; Section C.4 provides full proof.

## Foundational Learning

- **Concept:** Score-based diffusion SDEs (forward/backward process, score function ∇_x log q_t(x))
  - **Why needed here:** Ensembling operates on score estimates at each timestep; understanding what scores represent is essential for interpreting aggregation effects.
  - **Quick check question:** Can you explain why the backward SDE requires estimating the score of the marginal distribution q_t?

- **Concept:** Denoising Diffusion Score Matching (LDDSM) objective
  - **Why needed here:** This is what improves with ensembling — understanding its formulation explains why Jensen's inequality applies.
  - **Quick check question:** What does LDDSM measure, and why is it convex in the score estimator?

- **Concept:** Product-of-Experts (PoE) / geometric mean of densities
  - **Why needed here:** The intuitive (but incorrect) interpretation of score averaging as sampling from a PoE underlies many guidance methods; understanding why this fails is central to the paper.
  - **Quick check question:** If you average scores from two models, what distribution would you expect to sample from if composition and diffusion commuted?

## Architecture Onboarding

- **Component map:** K independently trained models → aggregation module → SDE solver
- **Critical path:** Train K models → compute K score estimates at each timestep → aggregate scores → feed to SDE solver for backward sampling
- **Design tradeoffs:**
  - Arithmetic mean: Best for loss reduction, rarely beats best individual model on FID
  - Mixture of Experts: Better on FFHQ by preserving diversity, requires models to be complementary
  - MC Dropout: Cheaper than DE but degrades both LDDSM and FID on CIFAR-10
  - Dominant feature: Works for Random Forests, catastrophic for neural nets (FID 46.79 on FFHQ)
- **Failure signatures:** Ensemble FID between individual FIDs but never beats best individual; models insufficiently diverse; ensemble worse than worst individual (CIFAR-10, Table 6); arithmetic mean with upscale initialization (λ=1) → higher individual FID but still no ensemble benefit
- **First 3 experiments:**
  1. Replicate Figure 3: Train K=5 U-Nets on CIFAR-10, evaluate FID/KID/LDDSM vs ensemble size to confirm loss-perception disconnect
  2. Test Mixture of Experts on your target domain: if models are complementary (different specializations), this aggregation may provide gains
  3. If using tree-based models for tabular data, compare Dominant feature aggregation against arithmetic mean to diagnose noise underestimation bias

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can advanced aggregation strategies like weighting, boosting, or "train one, get K for free" methods overcome the limited returns of standard democratic ensembling?
- **Basis in paper:** [explicit] The conclusion states future work could explore "train one, get K for free" strategies or "relax the equal or independent constraint... through weighting or boosting."
- **Why unresolved:** The paper restricted its scope to democratic, unweighted aggregation of independently trained models.
- **What evidence would resolve it:** Experiments demonstrating that weighted ensembles significantly outperform the best individual model on FID/KID without the computational cost of training multiple full models.

### Open Question 2
- **Question:** Can MCMC or Feynman–Kac correctors effectively adjust the sampling process to target the true Product of Experts (PoE) distribution, given that simple score averaging fails to do so?
- **Basis in paper:** [explicit] The conclusion notes that while correctors like Feynman–Kac could adjust sampling to target the PoE, the authors "deliberately omitted them" to study baseline ensembling.
- **Why unresolved:** The paper theoretically proves score averaging does not commute with diffusion (it doesn't sample the PoE), but did not empirically test correction methods.
- **What evidence would resolve it:** Empirical results showing that corrected sampling trajectories yield the theoretical benefits of PoE without degrading perceptual metrics.

### Open Question 3
- **Question:** Is it possible to design a training objective that aligns improved score matching with improved perceptual quality (FID/KID)?
- **Basis in paper:** [inferred] The study finds a fundamental "disconnect" where ensembling improves the training objective (score matching) but fails to improve perceptual metrics.
- **Why unresolved:** The paper demonstrates this discrepancy extensively but does not propose a solution to align the mathematical objective with visual evaluation metrics.
- **What evidence would resolve it:** A modified loss function or aggregation rule where a reduction in training loss reliably correlates with a decrease in FID.

## Limitations

- **Metric misalignment gap:** The paper demonstrates a disconnect between score-matching loss and perceptual quality metrics, but the precise mechanisms behind this divergence remain incompletely understood.
- **Theoretical generality:** Proposition 4.2 proving non-commutativity is demonstrated only for Gaussian distributions with diagonal covariance, potentially limiting generalization.
- **Ensemble complementarity requirement:** Mixture of Experts shows promise but requires models to be complementary, with conditions for when complementarity emerges not fully specified.

## Confidence

- **High confidence:** Score-matching loss improvement through ensembling - directly proven via Jensen's inequality with clear experimental validation across multiple architectures.
- **Medium confidence:** Perceptual quality disconnect - supported by experimental evidence but theoretical understanding of the precise relationship between LDDSM and FID/KID is limited.
- **Medium confidence:** Non-commutativity of diffusion and composition - proven for specific Gaussian case with broader implications suggested but not exhaustively validated.

## Next Checks

1. **Validate metric alignment hypothesis:** Generate controlled perturbations in score estimates matching the magnitude of ensemble improvements, then measure the resulting FID/KID variation to quantify the "chaotic" relationship.

2. **Test generalization of non-commutativity:** Apply Proposition 4.2 framework to non-Gaussian distributions or real model outputs to assess whether the non-commutativity insight extends beyond the theoretical Gaussian case.

3. **Characterize ensemble complementarity conditions:** Systematically vary training data splits and architectures to identify when Mixture of Experts provides gains versus when arithmetic mean suffices, clarifying when complementarity emerges.