---
ver: rpa2
title: 'Interpreting Time Series Forecasts with LIME and SHAP: A Case Study on the
  Air Passengers Dataset'
arxiv_id: '2508.12253'
source_url: https://arxiv.org/abs/2508.12253
tags:
- shap
- series
- arima
- lime
- xgboost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified framework for interpreting time-series
  forecasts using LIME and SHAP methods. The approach converts a univariate time series
  into a supervised learning problem with lagged features, rolling statistics, and
  seasonal encodings, then applies post-hoc explainability to gradient-boosted tree
  and ARIMA models.
---

# Interpreting Time Series Forecasts with LIME and SHAP: A Case Study on the Air Passengers Dataset

## Quick Facts
- arXiv ID: 2508.12253
- Source URL: https://arxiv.org/abs/2508.12253
- Authors: Manish Shukla
- Reference count: 2
- Primary result: Framework converts univariate time series to supervised learning with lagged features, then applies LIME/SHAP for post-hoc explainability

## Executive Summary
This paper presents a unified framework for interpreting time-series forecasts using LIME and SHAP methods. The approach converts a univariate time series into a supervised learning problem with lagged features, rolling statistics, and seasonal encodings, then applies post-hoc explainability to gradient-boosted tree and ARIMA models. Using the Air Passengers dataset, the framework demonstrates that a small set of features—particularly the twelve-month lag and seasonal encodings—explain most forecast variance.

The study shows XGBoost achieves slightly better accuracy than ARIMA (RMSE 12.97 vs 13.25, MAPE 5.21% vs 5.42%) on the hold-out period, though differences are not statistically significant. SHAP and permutation importance analyses consistently identify the twelve-month lag as the dominant feature, with short-term lags and seasonal encodings providing secondary contributions. Local LIME explanations are stable across different kernel widths, with median R² of 0.86 for surrogate models.

## Method Summary
The framework transforms univariate time series forecasting into a supervised learning problem by creating lagged features, rolling statistics, and seasonal encodings. These engineered features serve as inputs to both gradient-boosted tree models (XGBoost) and traditional time series models (ARIMA). Post-hoc explainability methods including LIME and SHAP are then applied to interpret feature contributions to forecasts. The approach uses a hold-out validation strategy with the last 24 months of the Air Passengers dataset (1949-1960) for testing model performance and explanation quality.

## Key Results
- XGBoost achieves slightly better accuracy than ARIMA (RMSE 12.97 vs 13.25, MAPE 5.21% vs 5.42%) on hold-out period, though differences are not statistically significant
- SHAP and permutation importance analyses consistently identify twelve-month lag as dominant feature, with short-term lags and seasonal encodings as secondary contributors
- Local LIME explanations are stable across different kernel widths, with median R² of 0.86 for surrogate models

## Why This Works (Mechanism)
The framework works by converting temporal dependencies into spatial features that supervised learning algorithms can process. Lagged features capture autocorrelation patterns, rolling statistics encode local trends and volatility, and seasonal encodings represent periodic patterns. This transformation allows complex models like XGBoost to learn nonlinear relationships while maintaining interpretability through post-hoc methods.

## Foundational Learning
- **Time series to supervised learning conversion**: Transforms temporal data into feature matrix format
  - Why needed: Enables use of standard ML algorithms for forecasting
  - Quick check: Verify lag features capture known autocorrelation structure

- **Feature engineering for time series**: Creates lagged variables, rolling statistics, and seasonal encodings
  - Why needed: Provides model with temporal context beyond raw values
  - Quick check: Confirm engineered features improve baseline forecast accuracy

- **Post-hoc explainability methods**: LIME and SHAP provide feature importance after model training
  - Why needed: Enables interpretation of complex models without sacrificing performance
  - Quick check: Compare explanation stability across different kernel widths or perturbation strategies

## Architecture Onboarding
**Component map**: Time series data -> Feature engineering -> Model training -> Post-hoc explanation -> Interpretability analysis

**Critical path**: Feature engineering (lags, rolling stats, seasonal encodings) -> Model training (XGBoost/ARIMA) -> Explanation generation (LIME/SHAP) -> Performance evaluation

**Design tradeoffs**: The framework prioritizes interpretability over marginal accuracy gains, using post-hoc methods rather than inherently interpretable models. This allows complex models to be used while maintaining explanation capabilities.

**Failure signatures**: 
- Explanations show high variance across kernel widths or perturbations
- Feature importance rankings change dramatically between LIME and SHAP
- Model performance degrades significantly on out-of-sample periods

**3 first experiments**:
1. Apply framework to synthetic time series with known feature importance to verify explanation accuracy
2. Test explanation stability across different hold-out periods or temporal splits
3. Compare framework performance on datasets with varying seasonality patterns and noise levels

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single univariate dataset (Air Passengers) spanning 1949-1960, restricting generalizability
- Small hold-out period (last 24 months) may not capture performance under varying market conditions
- Claim of comparable LIME/SHAP rankings based on visual inspection rather than statistical tests

## Confidence
- Framework generalizability: Medium - limited to single dataset testing
- Explanation stability claims: Medium - relies on visual inspection without statistical validation
- Accuracy improvement claims: Low - differences not statistically significant yet presented as "slightly better"

## Next Checks
1. Test framework performance across multiple datasets with varying seasonality patterns, noise levels, and structural breaks to assess robustness beyond the Air Passengers dataset
2. Conduct statistical tests (e.g., paired t-tests or bootstrap confidence intervals) on forecast accuracy metrics to determine if observed differences between models are significant
3. Evaluate explanation stability by applying the framework to out-of-sample periods or synthetic time series with known feature importance to verify that LIME and SHAP consistently recover ground truth feature contributions