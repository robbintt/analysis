---
ver: rpa2
title: Investigating Literary Motifs in Ancient and Medieval Novels with Large Language
  Models
arxiv_id: '2504.21742'
source_url: https://arxiv.org/abs/2504.21742
tags:
- occurrences
- similarity
- motifs
- love
- novel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study used fine-tuned large language models to extract and
  analyze literary motifs from a corpus of ancient and medieval Greek novels. The
  method involved chunking texts, fine-tuning GPT-4o on annotated examples, clustering
  motif sentences, and summarizing results.
---

# Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models

## Quick Facts
- arXiv ID: 2504.21742
- Source URL: https://arxiv.org/abs/2504.21742
- Reference count: 0
- Primary result: Fine-tuned LLMs extracted literary motifs from ancient/medieval Greek novels, revealing stable and fluctuating themes across historical periods

## Executive Summary
This study applied large language models to extract and analyze literary motifs from a corpus of ancient and medieval Greek novels. The approach involved chunking texts, fine-tuning GPT-4o on annotated examples, clustering motif sentences, and summarizing results. The analysis revealed both stable motifs (e.g., fate, leadership) and period-specific patterns, with Komnenian novels showing increased focus on beauty and transitions from day to night. The method effectively identified recurrent narrative patterns and enabled quantitative analysis of genre evolution and cross-textual relationships.

## Method Summary
The method involved chunking Greek novels into 1000-token segments with two preceding chunks as context, then fine-tuning GPT-4o on 74 manually annotated examples to extract motif sentences. These sentences were embedded using all-mpnet-base-v2, clustered with BERTopic using UMAP and HDBSCAN, and summarized with Llama-3.1-8B-Instruct. The resulting clusters were analyzed for frequency patterns across three historical periods (Imperial, Komnenian, Palaiologan), with similarity scores and uniqueness metrics used to compare inter-textual relationships.

## Key Results
- Stable motifs across periods: leadership, nobility, fate
- Fluctuating Komnenian motifs: maidens' beauty, transitions from day to night
- Highest similarity: Aithiopica and Leucippe and Clitophon (0.81)
- Unique Komnenian motif: war (40% frequency)

## Why This Works (Mechanism)

### Mechanism 1: Fine-Tuning for Domain-Specific Semantic Extraction
Fine-tuning GPT-4o on 74 annotated examples shifts the model's attention mechanism to recognize "units of meaning" specific to ancient Greek literary corpus rather than generic semantic similarity. The supervised adjustment allows the model to internalize a specific theoretical definition of "literary motif" and apply it to low-resource texts.

### Mechanism 2: Clustering Sentence Embeddings to Identify Diachronic Trends
Converting extracted motif sentences into vector embeddings and clustering them enables quantification of literary evolution. By grouping semantically similar descriptions across centuries, the system can count cluster sizes across different time periods and calculate fluctuation scores.

### Mechanism 3: Vector Similarity for Inter-Textual Relationship Mapping
Representing entire novels as vectors of motif distributions creates a reliable metric for "intertextuality." Calculating cosine similarity between these vectors quantifies the "distance" between texts, allowing identification of literary lineages based on shared thematic DNA rather than just plot points.

## Foundational Learning

- **Concept: Fine-Tuning vs. Few-Shot Prompting**
  - Why needed: Fine-tuning shifts the model's parameters to internalize the specific "motif" definition, while few-shot prompting only adjusts style temporarily
  - Quick check: Why would fine-tuning be superior to few-shot prompting when dealing with a specialized definition like "literary motif" in a low-resource language?

- **Concept: Sentence Embeddings (Transformer Models)**
  - Why needed: The mechanism relies on converting text sentences into vectors where semantic similarity is captured mathematically
  - Quick check: What property of vector space allows the system to group "A maiden's beauty" and "Admiration of exceptional beauty" into the same cluster?

- **Concept: Standard Deviation vs. Mean in Diachronic Analysis**
  - Why needed: The paper distinguishes between "stable" motifs (high mean frequency across all periods) and "fluctuating" motifs (high standard deviation)
  - Quick check: If a motif has a high mean frequency but low standard deviation, what does that tell us about its presence in the Imperial vs. Komnenian corpora?

## Architecture Onboarding

- **Component map:** TikToken tokenizer -> Fine-tuned GPT-4o -> all-mpnet-base-v2 embedder -> BERTopic clusterer -> Llama-3.1-8B-Instruct summarizer -> Cosine similarity analyzer
- **Critical path:** Manual annotation of the 74 fine-tuning examples serves as the "ground truth" that defines the entire system's understanding of a "motif"
- **Design tradeoffs:** Context window vs. isolation (feeds 2 previous chunks for context but extracts only from current chunk); cluster granularity (larger clusters provide broader themes, smaller clusters provide specific motifs)
- **Failure signatures:** High outlier count suggests clustering parameters are too strict or extractor generates unique, non-recurring sentences; label hallucination risks oversimplifying complex literary patterns
- **First 3 experiments:**
  1. Validation of Fine-Tuning: Run a subset of corpus through non-fine-tuned GPT-4o with same prompt and compare extracted motifs
  2. Parameter Sensitivity Analysis: Rerun clustering with min_cluster_size set to 5 and 20, observe changes to "Fluctuating Motifs" chart
  3. Uniqueness Stress Test: Take novel with highest uniqueness score and perform close reading of source text to verify extracted motifs support the "unique" label

## Open Questions the Paper Calls Out

- **Question:** How effective are large language models at extracting literary motifs from low-resource languages like Ancient Greek compared to high-resource languages?
  - Basis in paper: The author explicitly states that "their effectiveness in dealing specifically with literary motifs and in texts written in various low-resource languages... remains an open research question"
  - Why unresolved: The study demonstrates the method works but doesn't rigorously benchmark performance degradation caused by limited Ancient Greek pre-training data
  - What evidence would resolve it: A comparative benchmark evaluating motif extraction accuracy across high-resource and low-resource languages using standardized, human-annotated ground truth

- **Question:** To what extent do LLM-generated summarizing labels accurately represent the semantic nuance of the underlying clustered text segments?
  - Basis in paper: The author notes that "to get a fair representation of what these motifs really consist of, it would be necessary to examine them thoroughly, not just look at the summarizing label"
  - Why unresolved: The current study relies on summary labels generated by Llama-3.1, which risks oversimplifying complex literary patterns or missing context within clusters
  - What evidence would resolve it: A qualitative study comparing raw text chunks within high-frequency clusters against their generated labels to measure semantic fidelity

- **Question:** How sensitive are the identified motif clusters to changes in the dimensionality reduction and clustering hyperparameters?
  - Basis in paper: The paper notes that clustering is "the part of the work process where more time could be spent" and that adjustable parameters affect cluster size and structure
  - Why unresolved: Results depend on specific UMAP and HDBSCAN settings, but stability of resulting motifs relative to these parameters is not tested
  - What evidence would resolve it: A sensitivity analysis showing whether "persistent" motifs remain dominant when clustering parameters are varied

## Limitations
- Data sparsity: 74 examples for multi-century literary domain risks both underfitting and overfitting
- Cluster parameter sensitivity: HDBSCAN parameters significantly influence which motifs appear "stable" vs. "fluctuating"
- Greek language constraints: Low-resource status means LLM relies on transliteration that may miss nuanced literary meaning

## Confidence
- **High confidence:** The method successfully extracts and clusters motif sentences
- **Medium confidence:** Identified motif trends reflect genuine patterns, though clustering parameters influence specific results
- **Medium confidence:** Inter-textual similarity scores meaningfully capture thematic relationships

## Next Checks
1. Run the same prompt on non-fine-tuned GPT-4o with held-out chunks to validate fine-tuning improved adherence to theoretical definition
2. Re-cluster with min_cluster_size set to 5 and 20 to assess robustness of identified trends
3. Select novel with highest uniqueness score and perform close reading of source text to confirm extracted motifs support the assigned "unique" label