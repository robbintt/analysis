---
ver: rpa2
title: Dynamic Feature Selection based on Rule-based Learning for Explainable Classification
  with Uncertainty Quantification
arxiv_id: '2508.02566'
source_url: https://arxiv.org/abs/2508.02566
tags:
- uni00000048
- feature
- uni00000055
- uni00000003
- uni00000046
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of dynamic feature selection (DFS)
  with uncertainty quantification and interpretability. Existing DFS methods rely
  on opaque neural models and lack uncertainty quantification, limiting their use
  in high-stakes domains.
---

# Dynamic Feature Selection based on Rule-based Learning for Explainable Classification with Uncertainty Quantification

## Quick Facts
- arXiv ID: 2508.02566
- Source URL: https://arxiv.org/abs/2508.02566
- Reference count: 14
- Primary result: Achieves competitive accuracy (69.43% average) against state-of-the-art methods while providing interpretable predictions and uncertainty quantification

## Executive Summary
This paper addresses the problem of dynamic feature selection (DFS) with uncertainty quantification and interpretability in high-stakes domains. Existing DFS methods rely on opaque neural models and lack uncertainty quantification, limiting their use in critical applications. The authors formalize additional uncertainty sources in DFS, including epistemic uncertainty from model adaptation and challenges in estimating aleatoric uncertainty during sequential acquisition. They propose a rule-based DFS framework that adapts a global rule-based classifier to arbitrary feature subsets by removing inactive conditions and re-estimating rule confidences, enabling interpretable predictions without training subset-specific models.

## Method Summary
The proposed approach trains a global rule-based classifier (CART or FuzzyCART) on full features, then adapts it to partial observations by removing conditions on missing features and re-estimating rule confidences using only observed features. A greedy feature selection policy minimizes prediction divergence from the global model while accounting for epistemic uncertainty through auxiliary models or fuzzy rule firing patterns. The value function combines prediction improvement (KL divergence reduction) and epistemic uncertainty penalty, estimated by a neural network. At inference, features are selected greedily until budget exhaustion, producing interpretable predictions via rule firing patterns.

## Key Results
- Achieves 69.43% average accuracy across five datasets compared to state-of-the-art methods
- Prediction entropy decreases with feature acquisition regardless of selection strategy, but this does not translate to improved calibration
- Tree-based methods benefit most from intelligent feature selection, while other methods' performance stems from predictor robustness
- Epistemic uncertainty quantification using fuzzy rule firing patterns eliminates need for auxiliary models in some cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A global rule-based classifier can be adapted to arbitrary feature subsets by removing inactive conditions and re-estimating rule confidences, enabling DFS without training subset-specific models.
- Mechanism: Given a pre-trained rule model on full features, when only subset S is observed, conditions referencing unavailable features are removed from rule antecedents. Rule confidences are re-estimated using only the observed features: Confidence(r) = Σμᵣ(x)·I(y=c) / Σμᵣ(x). This yields unbiased estimates of p(y|xˢ) without treating missing conditions as true.
- Core assumption: The global model's rule structure captures meaningful feature interactions that remain valid when sub-rules are evaluated independently.
- Evidence anchors:
  - [abstract] "A rule-based DFS framework that adapts a global rule-based classifier to arbitrary feature subsets by removing inactive conditions and re-estimating rule confidences."
  - [Section 3.1] "This re-estimation is straightforward because we can compute the predictive power of any rule... we can cache these values for all sub-rules to avoid any computational overhead in inference time."
  - [corpus] Weak direct corpus support for this specific adaptation mechanism; related work focuses on static or neural-based DFS.
- Break condition: When rule antecedents have strong conditional dependencies (e.g., "IF A > 5 AND B < 3"), removing one condition may invalidate the learned confidence, as the sub-rule never appeared in training data.

### Mechanism 2
- Claim: Minimizing KL divergence between sub-model and global model predictions is proportional to maximizing Conditional Mutual Information (CMI), under the assumption that sub-model predictions scale with global predictions.
- Mechanism: The value function v_q(i, xˢ) = E[q(xᵢ, xˢ)] where q combines prediction improvement u (KL divergence reduction) and epistemic uncertainty penalty e. The paper proves: if p(ŷ|x) ∝ p(ŷ|xˢ, xᵢ), then -E[Δuᵢ] ∝ I(xᵢ; ŷ|xˢ). This links greedy divergence minimization to information-theoretic optimality.
- Core assumption: The current feature subset S already recovers a prediction distribution proportional to the global model—this is strictly necessary for CMI equivalence.
- Evidence anchors:
  - [Section 3.2.1] "We can show that this expression is minimized by the same xᵢ that maximizes the CMI, as long as the following proportionality holds."
  - [Appendix B, Proposition 1] Formal proof of proportionality between KL reduction and CMI.
  - [corpus] DIME (Covert et al. 2023) and related CMI-based methods corroborate the information-theoretic framing; no corpus contradiction.
- Break condition: Early in acquisition when S is small, sub-model predictions may diverge significantly from global predictions, breaking the proportionality assumption and decoupling the policy from CMI optimality.

### Mechanism 3
- Claim: Epistemic uncertainty in rule-based DFS can be quantified using auxiliary model disagreement (crisp rules) or membership-weighted divergence across co-firing rules (fuzzy rules).
- Mechanism: For crisp rules, train auxiliary classifiers I and measure average KL divergence: e_R(xˢ) = (1/|I|) Σ D_KL(pᵢ || p_global). For fuzzy rules, exploit that multiple rules fire simultaneously; compute membership-weighted divergence: e_R(xˢ) = Σᵣ μᵣ(xˢ) · D_KL(pᵣ || pᵣ*), where r* is the winning rule. High firing agreement → low epistemic uncertainty.
- Core assumption: Fuzzy rule systems produce meaningful membership distributions where co-firing rules encode competing hypotheses; crisp rules require auxiliary models to capture model uncertainty.
- Evidence anchors:
  - [Section 3.2.2] Equations 11-12 define both uncertainty measures.
  - [Section 3.2.2] "This formulation eliminates the need for other auxiliary models using the weighted divergences against the predictions of other rules in the system."
  - [corpus] Fuzzy rough feature selection literature (Margin-aware FRFS) addresses uncertainty in feature selection but not the specific rule-firing epistemic measure proposed here.
- Break condition: When fuzzy partitions are poorly calibrated (e.g., fixed partitions that don't match data distribution), membership values become uninformative, and the divergence measure may not reflect true epistemic uncertainty.

## Foundational Learning

- Concept: **Aleatoric vs. Epistemic Uncertainty Taxonomy**
  - Why needed here: DFS introduces new epistemic uncertainty from model adaptation (Δ_S risk difference between global and subset-specific optima) that doesn't exist in static selection. Understanding this distinction is essential for interpreting the paper's uncertainty quantification.
  - Quick check question: Given a partially observed sample xˢ, would adding more training data reduce the uncertainty from using a globally-trained model on this subset?

- Concept: **KL Divergence and Mutual Information**
  - Why needed here: The feature selection policy is derived from KL divergence minimization and formally connected to CMI. Without this foundation, the relationship between the proposed greedy policy and information-theoretic optimality is opaque.
  - Quick check question: If D_KL(p(y|x) || p(y|xˢ)) = 0.5, what does this imply about how much information x\xˢ provides about y?

- Concept: **Rule-based Classifier Semantics (Antecedent → Consequent)**
  - Why needed here: The adaptation mechanism relies on removing conditions from antecedents and re-estimating consequent confidences. Understanding conjunctive rule structure is prerequisite to grasping why sub-rule caching works.
  - Quick check question: For rule "IF x₁ > 5 AND x₂ < 3 THEN class A with confidence 0.8", what is the valid sub-rule when x₂ is unobserved, and why must confidence be re-estimated?

## Architecture Onboarding

- Component map: Global Rule Model (pre-trained) -> Subset Adapter: removes conditions on x_¯S, re-estimates confidences -> Value Function Estimator (neural): predicts v_q(i, xˢ) for all unobserved i -> Greedy Selector: queries argmax_i v_q(i, xˢ) until budget exhausted -> Adapted Sub-Model: produces interpretable p(ŷ|xˢ) via rule firing

- Critical path:
  1. Train global rule model (CART/FuzzyCART) on full training data with all features.
  2. Pre-compute and cache sub-rule confidences for all rule antecedent subsets (feasible for rules ≤4 conditions).
  3. Train neural value estimator with two heads (u reduction, e reduction) using Eq. 13—sample feature subsets during training.
  4. At inference: observe xˢ, adapt rules, estimate v_q for all unobserved features, query highest, repeat.

- Design tradeoffs:
  - Global vs. joint training: Pre-training global model is simple but may have high Δ_S on rare subsets. Joint selector-predictor training (Eq. 4) reduces epistemic uncertainty but risks overfitting to selector trajectories.
  - Crisp vs. fuzzy rules: Fuzzy rules enable epistemic uncertainty estimation without auxiliary models but require well-calibrated partitions; crisp rules need ensembles (computational overhead).
  - Neural estimator vs. direct computation: Neural approximation of v_q is necessary at test time (unobserved xᵢ); direct expectation computation requires generative model of p(xᵢ|xˢ).

- Failure signatures:
  - **Monotonicity violation**: Accuracy decreases as more features are acquired (observed in FuzzyCART-DFS on Diabetes: peaked at 35% budget, degraded to 56.45% at full budget). Indicates global model ≠ optimal for full feature set.
  - **Flat selection benefit**: Accuracy with trained selector ≈ accuracy with random selection (observed with DIME). Indicates predictor robustness, not effective selection—selector is redundant.
  - **High entropy with high confidence**: ECE increases despite entropy decreasing (observed across all methods). Indicates miscalibration—confidence grows unjustified.

- First 3 experiments:
  1. **Global model quality check**: Train global rule model on full data; measure accuracy degradation when evaluating on random 30%/50%/70% feature subsets with adaptation. If degradation >15%, consider joint training or more robust base model.
  2. **Value estimator validation**: Before deployment, compare neural estimator predictions of v_q against ground-truth computed values (using held-out complete samples as oracle). Report MSE for both heads (u, e); if >0.1, increase estimator capacity or training epochs.
  3. **Selector vs. random baseline**: On validation set, compare accuracy curves (budget % vs. accuracy) for trained selector vs. random feature acquisition. If gap <3% at 50% budget, the learned policy is not providing value—investigate predictor robustness or policy exploration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can dynamic feature selection policies be designed to guarantee that increased predictive confidence (lower entropy) correlates with improved calibration, rather than models simply becoming "confidently wrong"?
- Basis in paper: [explicit] The authors conclude that "DFS models become more confident without necessarily becoming more reliable," noting that ECE analysis showed no correlation between budget size and calibration quality.
- Why unresolved: The paper demonstrates that entropy reduction is driven by information accumulation regardless of selection strategy, but it does not propose a mechanism to enforce monotonic improvement in reliability.
- What evidence would resolve it: A modification to the value function (Eq. 7) that incorporates a calibration penalty or explicit reliability term, showing positive correlation between feature acquisition and Reduced Expected Calibration Error.

### Open Question 2
- Question: Under what theoretical conditions does a learned feature selection policy provide significant utility over random selection, given that robust predictors like DIME appear to render the selector "almost redundant"?
- Basis in paper: [explicit] The analysis reveals that DIME's strong performance "stems primarily from predictor robustness rather than effective feature selection," whereas tree-based methods rely heavily on the selector.
- Why unresolved: The paper identifies this divergence in behavior between model architectures but does not determine the specific model capacities or data distributions that necessitate an active selector versus a robust imputer.
- What evidence would resolve it: A comparative analysis varying model capacity and feature missingness ratios to identify the "tipping point" where the selection policy becomes more important than the predictor's robustness.

### Open Question 3
- Question: What robust stopping criteria can be established for DFS when prediction entropy is non-monotonic and traditional confidence thresholds fail?
- Basis in paper: [inferred] Section 2.3 formalizes the "Non-Monotonicity of Uncertainty," arguing that entropy-based stopping is unreliable because gaining information can initially reduce confidence (e.g., rare disease diagnosis example).
- Why unresolved: While the paper formalizes this problem, the proposed framework relies on exhausting a fixed budget rather than solving the optimal stopping problem based on uncertainty estimates.
- What evidence would resolve it: A stopping policy derived from the proposed epistemic uncertainty bounds (Eq. 11/12) that successfully halts acquisition when the risk of model error outweighs the cost of features.

## Limitations

- The core rule-based adaptation mechanism lacks direct corpus validation, relying primarily on internal empirical results
- Non-monotonic performance on certain datasets (e.g., FuzzyCART-DFS degrading after 35% budget on Diabetes) suggests the global model may not be optimal for full feature sets
- Epistemic uncertainty quantification depends critically on well-calibrated fuzzy partitions, which are not detailed in the paper
- The paper demonstrates entropy reduction but shows this does not translate to improved calibration, raising questions about practical utility

## Confidence

- High: The mechanism of removing inactive conditions and re-estimating rule confidences for subset adaptation (Mechanism 1) is well-specified and theoretically sound.
- Medium: The connection between KL divergence minimization and CMI (Mechanism 2) is formally proven but assumes proportionality that may break early in acquisition.
- Medium: The epistemic uncertainty quantification using fuzzy rule firing patterns (Mechanism 3) is innovative but depends on implementation details not provided.

## Next Checks

1. **Global Model Robustness Test**: Evaluate the accuracy degradation of the global rule model when tested on random feature subsets with varying acquisition budgets (30%, 50%, 70%, 100%) to quantify the Δ_S risk difference that the DFS framework aims to mitigate.

2. **Selector Policy Validation**: Compare the trained DFS policy against random feature selection on a held-out validation set across multiple budgets (25%, 50%, 75%, 100%). If the accuracy gap is consistently below 3%, investigate whether predictor robustness or policy overfitting explains the result.

3. **Uncertainty Calibration Analysis**: Compute Expected Calibration Error (ECE) at each acquisition step (every 10% budget increment) and correlate it with prediction entropy to verify whether decreasing entropy corresponds to improved calibration confidence.