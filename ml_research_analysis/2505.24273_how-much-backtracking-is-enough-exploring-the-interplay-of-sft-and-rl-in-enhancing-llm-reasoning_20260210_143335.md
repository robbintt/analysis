---
ver: rpa2
title: How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing
  LLM Reasoning
arxiv_id: '2505.24273'
source_url: https://arxiv.org/abs/2505.24273
tags:
- reasoning
- training
- zhang
- backtrack
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the interplay between supervised fine-tuning
  (SFT) and reinforcement learning (RL) in enhancing large language model (LLM) reasoning.
  Through systematic experiments across eight reasoning tasks, the authors find that
  short CoT data used in SFT as a warm-up moderately improves RL training compared
  to cold-start RL, but this benefit diminishes for more difficult tasks.
---

# How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing LLM Reasoning

## Quick Facts
- arXiv ID: 2505.24273
- Source URL: https://arxiv.org/abs/2505.24273
- Reference count: 40
- Eight reasoning tasks tested; backtracking in SFT warm-up improves RL training; RL prioritizes structure over content correctness

## Executive Summary
This paper investigates how supervised fine-tuning (SFT) with synthetic reasoning data, particularly focusing on backtracking frequency, affects subsequent reinforcement learning (RL) for enhancing large language model reasoning capabilities. Through systematic experiments across eight reasoning tasks, the authors discover that short CoT data used in SFT as a warm-up moderately improves RL training compared to cold-start RL, but this benefit diminishes for more difficult tasks. The study reveals that RL training is largely unaffected by the correctness of long CoT sequences, suggesting that RL prioritizes structural patterns over content correctness. Additionally, the authors construct synthetic datasets varying the number of backtracking steps and find that longer CoT sequences with backtracks generally induce better and more stable RL training, with more challenging problems requiring higher numbers of backtracks during the SFT stage.

## Method Summary
The study employs a two-stage pipeline: SFT warm-up followed by RL fine-tuning on Qwen2.5-3B-Instruct. Synthetic datasets are generated for Countdown and Sudoku using DFS solvers, and for Arc 1D using heuristic search, with controllable backtracking frequencies. The RL stage uses PPO/GRPO with rollout buffers and rule-based verifiable rewards (0.1 for format compliance, 0.9 for answer correctness). Experiments compare cold-start RL versus SFT warm-up with various trajectory types (self-sampled, distilled, synthetic with different backtrack counts, shuffled). Evaluation uses pass@1 accuracy on final answers within <answer></answer> tokens across eight reasoning tasks from Reasoning Gym.

## Key Results
- Short CoT SFT warm-up moderately improves RL training compared to cold-start RL, but benefit diminishes for more difficult tasks
- RL training is largely unaffected by the correctness of long CoT sequences, suggesting prioritization of structural patterns over content correctness
- Longer CoT sequences with backtracks generally induce better and more stable RL training, with more challenging problems requiring higher numbers of backtracks during SFT stage

## Why This Works (Mechanism)

### Mechanism 1: Structure-over-Content Prioritization in RL
RL training for reasoning prioritizes learning structural patterns (e.g., backtracking behavior, reasoning format) over the correctness of individual reasoning trajectories. The reward signal from verifiable final answers provides sparse, outcome-based feedback, allowing the policy to learn that certain structural behaviors correlate with eventually reaching correct answers, even when specific reasoning steps in training data contain errors.

### Mechanism 2: Difficulty-Scaled Backtracking Requirement
The optimal number of backtracking demonstrations in SFT warm-up scales positively with problem difficulty (specifically, the size of the effective search space). Harder combinatorial problems have larger search spaces with more dead-ends. SFT with backtracking demonstrations teaches the model a search strategy—recognizing when to abandon unproductive paths. RL then optimizes when to apply this strategy.

### Mechanism 3: Internal Consistency as a Precondition for RL
RL training fails when SFT data contains internal inconsistencies (e.g., reasoning trajectories mismatched to problem prompts), even if the structural patterns are correct. SFT creates strong priors over the joint distribution of (problem, reasoning trajectory). When shuffled data teaches the model that Problem A's prompt pairs with Problem B's solution, the model learns a spurious correlation that RL's policy gradient is too weak to overcome within practical training budgets.

## Foundational Learning

- **Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO)**: These RL algorithms are the core post-training method. Understanding how PPO constrains policy updates via the clipping objective and how GRPO reduces memory overhead is essential for reproducing results.
  - *Quick check*: Can you explain why PPO uses a clipped surrogate objective instead of a simple policy gradient?

- **Chain-of-Thought (CoT) Reasoning and Backtracking**: The entire paper investigates how CoT structure (length, backtrack frequency) influences RL. You must understand what backtracking means in a language model context (verbalized self-correction like "Wait, let me reconsider...") versus in classical search.
  - *Quick check*: How does backtracking in LLM CoT differ from backtracking in depth-first search?

- **Rule-Based Verifiable Rewards (RLVR)**: The paper uses a specific reward structure: 0.1 for format compliance (proper <answer> tags), 0.9 for answer correctness. This sparse, binary reward scheme is central to why structure matters more than content.
  - *Quick check*: Why might sparse, outcome-based rewards lead RL to prioritize behavioral patterns over step-by-step correctness?

## Architecture Onboarding

- **Component map**: Synthetic Data Generator -> SFT Stage -> RL Stage -> Reward Engine -> Policy Update
- **Critical path**: 1) Select task and assess difficulty (baseline accuracy → search space size) 2) Generate synthetic SFT data with backtracks scaled to difficulty 3) Run SFT warm-up for 1-3 epochs 4) Initialize RL from SFT checkpoint 5) Monitor reward curve for convergence in 100-200 steps
- **Design tradeoffs**: More backtracks vs. training stability (too few starves RL of search behavior; too many causes model degeneration); Correct vs. incorrect CoT data (incorrect saves annotation cost with minimal performance penalty if structural consistency preserved); Cold-start RL vs. SFT warm-up (cold-start works for easy tasks; SFT warm-up necessary for hard tasks)
- **Failure signatures**: Reward stagnation at ~0.1 (indicates format-only rewards; model learned structure but not valid search); Response length explosion without accuracy gain (model learned to generate longer outputs without effective search); Training collapse after initial improvement (likely learning rate too high or KL penalty misconfigured)
- **First 3 experiments**: 1) Baseline cold-start RL: Run RL on Countdown without SFT warm-up; measure reward trajectory and final accuracy 2) Ablation on backtrack count: For Sudoku, test SFT warm-up with 0, 1, 5, and 10 backtracks; plot reward curves 3) Correctness robustness check: For Countdown, compare SFT warm-up using only correct vs. only incorrect distilled trajectories; run RL to convergence and compare final accuracies

## Open Questions the Paper Calls Out

### Open Question 1
How can the optimal number of backtracking steps be predicted for a given task's difficulty and search space size? The paper empirically determines optimal backtrack counts per task but provides no theoretical framework or predictive formula linking problem characteristics to required backtracks.

### Open Question 2
Do the findings on SFT-RL interplay and backtracking generalize to larger models (7B+) and more complex mathematical reasoning tasks? The paper's experiments are limited to Qwen2.5-3B-Instruct and synthetic/toy tasks, leaving scaling behavior unknown.

### Open Question 3
What mechanism causes RL to prioritize structural patterns (backtracking frequency) over content correctness in reasoning traces? The paper demonstrates this empirical observation but offers no explanation for the underlying learning dynamics or credit assignment mechanism.

### Open Question 4
Why does internal consistency (prompt-completion matching) critically affect RL while answer correctness does not? The paper documents this discrepancy but does not investigate why some inconsistencies are tolerated while others are fatal to training.

## Limitations
- Findings based on rule-based reward systems (RLVR) with sparse, outcome-based feedback; generalizability to other reward schemes unclear
- Focus on small models (Qwen2.5-3B-Instruct) and synthetic datasets; scaling to larger models and real-world reasoning tasks remains untested
- Limited sample size of tasks used to establish difficulty-scaled backtracking mechanism; needs validation on additional tasks

## Confidence

**High Confidence**: The discovery that RL is largely unaffected by the correctness of long CoT sequences while being highly sensitive to internal inconsistency between prompts and completions. This is directly supported by controlled experiments showing clear, replicable patterns.

**Medium Confidence**: The difficulty-scaled backtracking requirement mechanism. While experiments across three tasks show consistent patterns, the sample size is limited and the mapping between problem difficulty and optimal backtrack count needs validation on additional tasks.

**Medium Confidence**: The structural patterns over content correctness mechanism in RL. The evidence is strong within the RLVR framework used, but the mechanism's robustness across different reward structures and model scales requires further investigation.

## Next Checks

1. **Reward Scheme Robustness Test**: Replicate main experiments using dense, process-based rewards instead of sparse RLVR to test whether structure-over-content prioritization persists across different reward architectures.

2. **Scaling Experiment**: Test the difficulty-scaled backtracking mechanism on larger models (e.g., Qwen2.5-7B or 14B) to determine if optimal backtrack counts scale with model capacity or if the relationship is model-size invariant.

3. **Generalization Test**: Apply the SFT+RL pipeline to a reasoning task outside the Reasoning Gym (e.g., mathematical proof generation or code debugging) to validate whether discovered mechanisms transfer to domains with different reasoning structures and evaluation metrics.