---
ver: rpa2
title: Fine-tuning of lightweight large language models for sentiment classification
  on heterogeneous financial textual data
arxiv_id: '2512.00946'
source_url: https://arxiv.org/abs/2512.00946
tags:
- sentiment
- financial
- data
- datasets
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Lightweight open-source large language models can achieve competitive
  sentiment classification performance on heterogeneous financial textual data, even
  when trained on as little as 5% of available training data. This approach is particularly
  effective for researchers and practitioners with limited computational resources,
  offering a cost-effective alternative to proprietary models like FinBERT.
---

# Fine-tuning of lightweight large language models for sentiment classification on heterogeneous financial textual data

## Quick Facts
- arXiv ID: 2512.00946
- Source URL: https://arxiv.org/abs/2512.00946
- Reference count: 34
- Lightweight open-source LLMs outperform FinBERT on financial sentiment classification even with minimal training data

## Executive Summary
This study evaluates whether lightweight open-source large language models can match or exceed the performance of proprietary models like FinBERT for sentiment classification on heterogeneous financial textual data. The research compares three open-source models—DeepSeek-LLM 7B, Llama3 8B Instruct, and Qwen3 8B—against FinBERT across five public datasets in English and Chinese. Through zero-shot, few-shot, and fine-tuning experiments at varying training data fractions, the authors demonstrate that these models achieve competitive results while requiring significantly less computational resources. The domain-balanced multi-dataset training approach proves particularly effective, with Qwen3 8B and Llama3 8B showing superior performance in most scenarios.

## Method Summary
The study employs a domain-balanced multi-dataset training pipeline using four language models: FinBERT (baseline), DeepSeek-LLM 7B, Llama3 8B Instruct, and Qwen3 8B. All models are fine-tuned using LoRA with 4-bit quantization via BitsAndBytes on a single NVIDIA A100 40GB GPU. The training follows a three-phase schedule: initialization (20% steps with warm-up), balanced phase (60% steps with weighted sampling and cosine learning rate decay), and finalization (20% steps with per-layer learning rate reduction and early stopping). Evaluation includes zero-shot, 3-shot, 5-shot, and fine-tuning at 5%, 10%, 20%, 40%, 75%, and 100% of available training data. The Role-Playing prompt format is used throughout, with metrics focusing on accuracy and macro F1 scores to account for class imbalance.

## Key Results
- Lightweight open-source models (Qwen3 8B, Llama3 8B) outperform FinBERT in most financial sentiment classification tasks
- Models maintain competitive performance even when trained on as little as 5% of available data
- Zero-shot and few-shot learning capabilities show strong results across English and Chinese datasets
- Domain-balanced training effectively mitigates interference from larger datasets

## Why This Works (Mechanism)
Lightweight LLMs leverage transformer-based architectures with attention mechanisms that capture contextual relationships in financial text. The domain-balanced training approach prevents overfitting to larger datasets while maintaining performance across heterogeneous data sources. LoRA fine-tuning enables efficient parameter updates without full model retraining, while 4-bit quantization reduces memory requirements without significant accuracy loss.

## Foundational Learning
- **LoRA fine-tuning**: Why needed - enables efficient parameter updates without full model retraining; Quick check - verify rank r and alpha values match domain requirements
- **4-bit quantization**: Why needed - reduces memory footprint for deployment on limited hardware; Quick check - measure accuracy drop versus 16-bit baseline
- **Domain-balanced sampling**: Why needed - prevents larger datasets from dominating training and drowning out smaller domain signals; Quick check - monitor per-domain batch composition during training
- **Cosine learning rate decay**: Why needed - helps models converge smoothly while avoiding overfitting; Quick check - plot training/validation loss curves for learning rate patterns
- **Role-Playing prompt format**: Why needed - provides consistent instruction format across zero-shot, few-shot, and fine-tuning scenarios; Quick check - verify prompt structure follows established RP conventions
- **Macro F1 scoring**: Why needed - accounts for class imbalance in financial sentiment datasets; Quick check - compare macro F1 versus accuracy for imbalanced datasets

## Architecture Onboarding
- **Component map**: Datasets (FPB, FiQA, GSD, TSD, CSD) -> Data Loader -> Domain-balanced Sampler -> Model (Qwen3/Llama3/DeepSeek/FinBERT) -> LoRA + 4-bit quantization -> Training pipeline -> Evaluation
- **Critical path**: Data loading → domain-balanced sampling → model inference → LoRA parameter updates → evaluation
- **Design tradeoffs**: Multi-dataset training improves generalization but risks catastrophic interference on small datasets; few-shot learning reduces data requirements but increases variance
- **Failure signatures**: Poor performance on FiQA indicates dataset size interference; large accuracy gaps between domains suggest sampling imbalance
- **Three first experiments**: 1) Verify domain-balanced sampling by logging per-domain batch composition, 2) Test LoRA configuration impact on FiQA performance, 3) Compare zero-shot results across different prompt templates

## Open Questions the Paper Calls Out
- Integrating RAG for more contextual predictions in financial sentiment analysis
- Fine-tuning LLMs on multilingual sentiment datasets beyond English and Chinese
- Extending to sentiment tasks outside of finance with the domain-balanced pipeline
- Mitigating performance degradation in small datasets through dataset-specific hyperparameter tuning

## Limitations
- Critical hyperparameters for LoRA fine-tuning and optimizer settings are not specified
- Prompt template and Role-Playing format details are not provided
- Training times and computational costs for different approaches are not reported
- Exact dataset splits and shot selection strategies are unspecified

## Confidence
- High confidence: Lightweight open-source models outperform FinBERT on financial sentiment classification
- Medium confidence: Zero-shot and few-shot learning effectiveness claims
- Low confidence: Exact performance numbers without specified hyperparameters and configurations

## Next Checks
1. Implement domain-balanced multi-dataset training and evaluate per-domain macro F1 stability
2. Test catastrophic interference hypothesis on FiQA with per-domain early stopping
3. Replicate zero-shot and few-shot evaluations with multiple random seeds for shot selection