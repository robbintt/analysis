---
ver: rpa2
title: 'Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test
  for Multimodal Reasoning'
arxiv_id: '2511.11182'
source_url: https://arxiv.org/abs/2511.11182
tags:
- reasoning
- agents
- agent
- counterfactual
- undercover
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses hallucination in multimodal reasoning by introducing
  the Multi-agent Undercover Gaming (MUG) protocol, which uses counterfactual image
  editing to create asymmetric information among agents. MUG reframes traditional
  multi-agent debate as a game to detect "undercover" agents prone to hallucinations
  by having them reason from subtly modified images while others use the original.
---

# Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning

## Quick Facts
- **arXiv ID:** 2511.11182
- **Source URL:** https://arxiv.org/abs/2511.11182
- **Reference count:** 19
- **Primary result:** MUG improves multimodal reasoning accuracy by 5.3-6.0% on MMMU and achieves 88.4% accuracy on POPE for hallucination detection

## Executive Summary
This paper addresses hallucination in multimodal reasoning by introducing the Multi-agent Undercover Gaming (MUG) protocol, which uses counterfactual image editing to create asymmetric information among agents. MUG reframes traditional multi-agent debate as a game to detect "undercover" agents prone to hallucinations by having them reason from subtly modified images while others use the original. The method introduces three key innovations: counterfactual testing for factual verification beyond consensus, cross-evidence reasoning through dynamically modified images, and active reasoning through strategic questioning. Experiments show MUG improves accuracy by 5.3-6.0% on MMMU and achieves 88.4% accuracy on POPE for hallucination detection, outperforming baselines like MAD and self-refine while maintaining computational efficiency with only 0.91s additional time per sample.

## Method Summary
MUG creates counterfactual images through Step1X-Edit with constraints on visual similarity, semantic consistency, and naturalness. The method assigns modified images to undercover agents and original images to normal agents, then conducts multi-round reasoning games where agents vote to eliminate suspected undercover members. Once the undercover is eliminated, remaining agents collaborate on final answers. The system uses question classification, scene graph generation, counterfactual editing, and a two-phase game structure (detection then summarization) with post-hoc validation using ViT embeddings, CLIP similarity, and FID scores.

## Key Results
- MUG achieves 50.3% accuracy on MMMU (vs 45.2% for MAD and 46.9% for self-refine)
- MUG achieves 62.4% accuracy on MMStar (vs 56.3% for MAD and 57.7% for self-refine)
- MUG achieves 88.4% accuracy on POPE hallucination detection (vs 84.7% for MAD and 81.3% for self-refine)
- MUG adds only 0.91 seconds per sample compared to baseline MAD

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Ground Truth for Hallucination Detection
Creating intentionally modified images provides verifiable ground truth for identifying agents prone to hallucination. When an image is systematically modified (e.g., changing hair color from red to black), any agent receiving the modified image and correctly describing the modification demonstrates reliable visual reasoning. Conversely, an agent receiving the original but describing modified features—or failing to describe modifications they should see—reveals hallucination tendencies. The controlled nature of the modification means discrepancies can be definitively attributed to agent reasoning quality rather than ambiguous visual evidence.

### Mechanism 2: Information Asymmetry Induces Detectable Reasoning Patterns
Assigning different visual evidence to different agents creates observable behavioral patterns that reveal reasoning quality. The undercover agent must reason from counterfactual visual evidence while trying to blend in with the group. This creates tension: accurate reasoning based on their image would reveal their undercover status, while mimicking the group requires reasoning inconsistent with their evidence. This tension produces detectable patterns—vague responses, defensive language, or inconsistencies when probed about specific visual details. Normal agents can confidently reference specific visual details demonstrating access to the original image.

### Mechanism 3: Active Probing Through Strategic Questioning
Framing debate as a social deduction game incentivizes active, probing reasoning rather than passive question-answering. Traditional MAD has agents passively respond to questions. MUG's gaming format gives agents strategic objectives (identify undercover or avoid detection). This incentivizes agents to ask probing questions about visual details, cross-examine claims, and actively seek inconsistencies. The competitive structure creates pressure for more thorough reasoning than cooperative settings.

## Foundational Learning

- **Concept: Multi-Agent Debate (MAD)**
  - Why needed here: MUG builds on MAD but critiques its assumption of agent rationality. Understanding baseline MAD (majority voting, debate rounds, consensus-seeking) is essential to understand what MUG modifies.
  - Quick check question: In traditional MAD, what mechanism determines the final answer when agents disagree?

- **Concept: Counterfactual Reasoning in AI**
  - Why needed here: MUG's core innovation uses counterfactual image edits as verification tools. Understanding counterfactual reasoning (what would happen if X were different?) grounds the approach in causal inference principles.
  - Quick check question: What makes a counterfactual useful for verification versus just a random modification?

- **Concept: Hallucination in Multimodal LLMs**
  - Why needed here: MUG targets hallucination as its core problem. Understanding what hallucination is (generating outputs not grounded in input evidence) and why consensus approaches can fail is essential.
  - Quick check question: Why might consensus-based approaches fail to detect hallucination when all agents share similar underlying model biases?

## Architecture Onboarding

- **Component map:** Question Analysis Module -> Scene Graph Generator -> Counterfactual Image Generator -> Agent Pool -> Undercover Detection Game -> Summarization Game -> Constraint Validator
- **Critical path:** Question classification -> Scene graph -> Counterfactual generation -> Agent assignment -> Detection rounds (reasoning/voting/elimination) -> Summarization -> Final answer
- **Design tradeoffs:**
  - Counterfactual subtlety vs. detectability: Too subtle (Figure 9a) = weak signal; too obvious = trivial detection
  - Number of rounds: Performance peaks at round 1 (Table 2); extended rounds can mislead normal agents (Figure 10)
  - Agent count: Fewer agents reduce cost but provide less robust voting
  - Generation constraints: Tighter constraints improve edit quality but increase generation time and retries
- **Failure signatures:**
  - Overly subtle editing (Figure 9a): Changes imperceptible -> undercover undetectable
  - Editing failures (Figure 9b): Modification unsuccessful -> semantic meaning unchanged
  - Unnatural artifacts (Figure 9c): Obvious manipulation -> trivial detection
  - Extended debate misleading (Figure 10): Normal agents swayed by undercover arguments after multiple rounds
  - High undercover survival (Figure 6): ~5% survive in round 3+
- **First 3 experiments:**
  1. Ablate counterfactual editing: Run with original images only. Expect 1.49-3.61 point drops (Figure 5). Validates that counterfactual evidence, not debate structure alone, drives improvements.
  2. Vary game round count: Test 0, 1, 2, 3 rounds. Expect peak at round 1 per Table 2. Validates optimal debate duration.
  3. Analyze undercover survival by question type: Categorize failures and survival cases by question type (Table 6). Identify which edit types produce reliable detection versus which need generation improvements.

## Open Questions the Paper Calls Out

### Open Question 1
How can the counterfactual image generation module be refined to minimize failure modes, such as overly subtle edits, failed modifications, or unnatural artifacts? The authors explicitly categorize failure cases in Appendix G (Figure 9), identifying "overly subtle editing," "editing failures," and "unnatural artifacts" as key challenges. They state these failures "highlight challenges in generating high-quality counterfactual evidence and motivate future work on more robust editing techniques." The current reliance on the Step1X-Edit model and a constrained generation score (Equation 3) does not guarantee semantic validity or naturalness, particularly for complex scene graphs or fine-grained attributes.

### Open Question 2
To what extent does the MUG framework need modification to prevent "normal" agents from being misled by persuasive "undercover" agents during extended debate rounds? Appendix G and Figure 10 demonstrate that normal agents can be "misled" or "swayed" by the arguments of the undercover agent, particularly on subjective, reasoning-based questions (e.g., luxury vs. cramped), leading to a decline in performance after the first round (Table 2). The current defense mechanism (Equation 5) optimizes for accuracy but lacks a robustness term to resist adversarial persuasion from peers who present plausible but counterfactual reasoning.

### Open Question 3
Can the MUG protocol be generalized to pure text-based reasoning tasks where counterfactual evidence must be synthesized linguistically rather than visually? While the introduction frames hallucination as a general "obstacle in the reasoning capabilities of large language models," the method is strictly defined for multimodal reasoning using "Counterfactual Visual Editing" (Figure 1). The mechanism relies on visual similarity constraints and editing tools which lack direct equivalents in text; applying MUG to text would require generating semantically distinct but contextually similar text documents, introducing different constraints.

## Limitations
- Counterfactual editing quality is critical and can fail through overly subtle edits, failed modifications, or unnatural artifacts
- Extended debate rounds can mislead normal agents on subjective questions, causing accuracy to drop after round 1
- Several critical hyperparameters are unspecified: number of agents, voting weights, confidence thresholds, and acceptance coefficients

## Confidence
**High Confidence:**
- Core methodology of counterfactual image generation and asymmetric agent assignment
- Reported accuracy improvements (+5.3-6.0% on MMMU, 88.4% on POPE)
- Basic architectural components (question classification, scene graph generation, detection/summarization phases)

**Medium Confidence:**
- Optimal round count (1 round peak performance)
- Generalizability across different hallucination types and question categories
- Computational efficiency claims (0.91s additional time)

**Low Confidence:**
- Specific hyperparameter values affecting performance
- Robustness to adversarial agent behaviors
- Performance in extreme failure scenarios (all agents hallucinate similarly)

## Next Checks
1. **Ablate counterfactual editing**: Run MUG with original images only (no counterfactual modifications). Expect 1.49-3.61 point accuracy drops per Figure 5. This validates that counterfactual evidence—not debate structure alone—drives improvements.

2. **Vary game round count**: Test 0, 1, 2, 3 rounds systematically. Expect peak performance at round 1 per Table 2, with degradation for extended rounds on subjective questions. This confirms optimal debate duration and identifies when normal agents become vulnerable to persuasion.

3. **Analyze undercover survival by question type**: Categorize detection success/failure cases by question type (Quantity, Object/Entity, Attribute, Spatial). Cross-reference with Table 6 to identify which edit types produce reliable detection versus which need generation improvements.