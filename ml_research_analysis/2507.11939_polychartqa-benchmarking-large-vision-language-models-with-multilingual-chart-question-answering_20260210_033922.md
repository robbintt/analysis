---
ver: rpa2
title: 'POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart
  Question Answering'
arxiv_id: '2507.11939'
source_url: https://arxiv.org/abs/2507.11939
tags:
- chart
- multilingual
- data
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: POLYCHARTQA is the first large-scale multilingual chart question
  answering benchmark covering 22,606 charts and 26,151 QA pairs across 10 languages.
  It is constructed using a decoupled pipeline that separates chart data from rendering
  code, enabling efficient multilingual generation through data translation and code
  reuse.
---

# POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering

## Quick Facts
- **arXiv ID:** 2507.11939
- **Source URL:** https://arxiv.org/abs/2507.11939
- **Reference count:** 40
- **Key outcome:** First large-scale multilingual chart QA benchmark with 22,606 charts, 26,151 QA pairs across 10 languages, revealing substantial performance gaps for low-resource languages.

## Executive Summary
POLYCHARTQA introduces the first comprehensive multilingual benchmark for chart question answering, spanning 10 languages including low-resource ones with non-Latin scripts. The benchmark is constructed using a decoupled pipeline that separates chart data from rendering code, enabling efficient translation and reuse across languages. State-of-the-art LLMs are employed for translation with rigorous quality control to ensure linguistic and semantic consistency. Experimental results reveal significant performance disparities between English and other languages, particularly for low-resource languages, which can be substantially reduced through fine-tuning on the companion POLYCHARTQA-Train dataset.

## Method Summary
The benchmark is constructed through a two-stage pipeline: first, English seed data is prepared by extracting JSON specifications and Plotly rendering code from existing datasets using Gemini-2.5-Pro, with rigorous quality control for visual fidelity and QA validity. Second, LLM-based translation generates multilingual data by translating JSON data and QA pairs across 9 non-English languages, followed by chart re-rendering and visual inspection. The decoupled approach enables efficient reuse of chart structures while adapting textual content to different languages. Fine-tuning employs LoRA with vision encoder frozen, achieving substantial improvements in multilingual chart understanding across diverse model sizes and architectures.

## Key Results
- Significant performance gap between English and other languages, with low-resource languages showing 20-30% accuracy drops
- Fine-tuning on POLYCHARTQA-Train yields substantial gains in multilingual chart understanding
- OCR failures dominate errors (27.8%–51.4% per analysis), especially for non-Latin scripts
- METEOR and back-translation consistency checks effectively filter translation quality issues

## Why This Works (Mechanism)
The decoupled pipeline separates chart data from rendering code, enabling efficient multilingual generation through data translation and code reuse. LLM-based translation with rigorous quality control ensures linguistic and semantic consistency across languages. The approach leverages the structural consistency of chart types while adapting textual content, allowing models to learn cross-lingual chart understanding patterns. Fine-tuning on the large multilingual dataset provides exposure to diverse language-specific chart interpretations and question-answering patterns.

## Foundational Learning
- **Decoupled Chart Representation** (why needed: enables efficient multilingual generation; quick check: verify JSON + Plotly code pairs render correctly)
- **Type-aware Relaxed Accuracy** (why needed: accommodates numerical tolerance; quick check: confirm 5% relative error threshold implementation)
- **METEOR Translation Evaluation** (why needed: measures translation quality; quick check: verify METEOR scores meet thresholds)
- **Back-translation Consistency** (why needed: validates semantic preservation; quick check: compare source-target-source translations)
- **LoRA Fine-tuning** (why needed: efficient adaptation to multilingual data; quick check: verify rank=128, lr=1e-5 settings)
- **OCR Error Analysis** (why needed: identifies performance bottlenecks; quick check: measure OCR accuracy per language)

## Architecture Onboarding

**Component Map:** Seed Data -> Gemini Extraction -> Translation Pipeline -> Chart Rendering -> Quality Control -> Benchmark

**Critical Path:** English seed data preparation → LLM-based translation → Chart re-rendering → Quality validation → Model evaluation

**Design Tradeoffs:** Decoupled representation enables efficient multilingual generation but requires careful synchronization between data and code; LLM translation provides scalability but introduces potential cultural biases; type-aware accuracy accommodates numerical tolerance but may mask semantic understanding gaps.

**Failure Signatures:** Rendering failures due to font support issues for non-Latin scripts; translation inconsistency between JSON data and QA pairs causing unanswerable questions; OCR failures dominating errors for low-resource languages.

**3 First Experiments:**
1. Verify rendering of translated charts with non-Latin scripts using different font configurations
2. Test translation quality filtering by varying METEOR thresholds and analyzing rejected samples
3. Evaluate model performance with and without English text introduction to identify pivot language dependency

## Open Questions the Paper Calls Out

**Open Question 1:** Can the performance gains from fine-tuning transfer to complex visual formats like infographics or interactive dashboards? The benchmark is limited to 16 static chart types, and it's unknown if learned patterns scale to unstructured layouts.

**Open Question 2:** To what extent does LLM-based translation introduce cultural biases despite quality control? While automated metrics check accuracy, they may miss nuanced cultural framing differences in data topics selected for specific languages.

**Open Question 3:** How can reliance on English as a pivot language be reduced in architectures like InternVL and LLaVA? The paper identifies this dependency but doesn't propose specific architectural modifications to decouple reasoning from English.

**Open Question 4:** Does the decoupled representation effectively support Text-to-Chart generation across all languages, particularly complex non-Latin scripts? The pipeline generates chart images but it's unclear if JSON/Code pairs are robust enough for generation tasks.

## Limitations

- Benchmark construction relies heavily on LLM-based translation, potentially introducing cultural biases
- Performance gaps partly reflect OCR limitations rather than true language understanding, especially for non-Latin scripts
- Translation quality thresholds and validation criteria are not fully specified, raising reproducibility concerns
- Fine-tuning configuration lacks complete hyperparameter details (batch size, optimizer specifics)

## Confidence

- **High Confidence:** Existence of POLYCHARTQA benchmark with 22,606 charts and 26,151 QA pairs across 10 languages is verifiable
- **Medium Confidence:** Performance gap between English and low-resource languages is demonstrated, though exact attribution to OCR vs. understanding remains uncertain
- **Medium Confidence:** Fine-tuning effectiveness is shown, but results may depend on unspecified implementation details

## Next Checks

1. Implement full METEOR threshold and semantic validation criteria from translation pipeline to verify filtering effectiveness
2. Conduct ablation studies comparing performance with different OCR approaches to isolate text recognition error contributions
3. Replicate fine-tuning experiments with varied batch sizes and optimizer configurations to establish robustness of performance improvements