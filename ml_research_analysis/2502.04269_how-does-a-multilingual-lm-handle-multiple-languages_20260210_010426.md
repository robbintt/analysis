---
ver: rpa2
title: How does a Multilingual LM Handle Multiple Languages?
arxiv_id: '2502.04269'
source_url: https://arxiv.org/abs/2502.04269
tags:
- languages
- multilingual
- linguistic
- across
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates multilingual language models (MLMs) like BLOOM-1.7B
  and Qwen2 on semantic consistency, named entity recognition, and cross-lingual knowledge
  transfer across languages including English, French, Spanish, German, Chinese, Arabic,
  and Tamil. Using cosine similarity, it finds BLOOM-1.7B shows high semantic alignment
  with related languages (French, Spanish, German) but low alignment with Chinese.
---

# How does a Multilingual LM Handle Multiple Languages?

## Quick Facts
- arXiv ID: 2502.04269
- Source URL: https://arxiv.org/abs/2502.04269
- Reference count: 8
- Primary result: BLOOM-560m outperforms BERT-base Multilingual Cased on XNLI cross-lingual transfer (0.70 vs 0.54 English accuracy, 0.59 vs 0.47 Arabic, 0.50 vs 0.40 Swahili)

## Executive Summary
This study evaluates multilingual language models on semantic consistency, named entity recognition, and cross-lingual knowledge transfer across English, French, Spanish, German, Chinese, Arabic, and Tamil. BLOOM-1.7B shows high semantic alignment with related languages but low alignment with Chinese, while experiencing degradation in deeper layers. Qwen2 maintains more stable performance across layers. Cross-lingual transferability experiments reveal BLOOM-560m outperforming BERT-base on XNLI tasks, with transfer efficiencies of 0.87 for Arabic and 0.73 for Swahili, though performance gaps persist for low-resource languages.

## Method Summary
The study employs three main experimental approaches: (1) semantic alignment evaluation using cosine similarity of translated word embeddings across languages, (2) probing hidden states of BLOOM-1.7B and Qwen2 for layer-wise performance on sentence similarity and NER tasks, and (3) cross-lingual transfer on XNLI dataset using English fine-tuning with zero-shot evaluation on Arabic and Swahili. Models are fine-tuned on English training data, evaluated on multiple languages, and transfer efficiency is computed as target_accuracy / source_accuracy. Layer-wise hidden state extraction and PCA/t-SNE visualization support the analysis.

## Key Results
- BLOOM-1.7B shows high semantic alignment with related languages (French, Spanish, German) but low alignment with Chinese
- BLOOM-1.7B degrades in deeper layers while Qwen2 maintains stable performance
- BLOOM-560m achieves 0.70/0.59/0.50 accuracy on English/Arabic/Swahili XNLI vs BERT's 0.54/0.47/0.40
- Transfer efficiency for BLOOM-560m is 0.87 for Arabic and 0.73 for Swahili

## Why This Works (Mechanism)

### Mechanism 1
Languages with shared linguistic roots and similar grammatical structures produce higher semantic alignment in embedding space. Transformer-based embeddings encode morphological and syntactic features; related languages (French, Spanish, German with English) share structural patterns that map to proximate vector regions, while typologically distant languages (Chinese) occupy distinct clusters. Cosine similarity of translated word embeddings faithfully captures meaningful linguistic relationships rather than artifacts of training data distribution.

### Mechanism 2
Deeper transformer layers can degrade semantically useful representations in some architectures, with degradation severity being model-specific. Initial layers capture general semantics; mid-layers process task-relevant features; deeper layers may suffer from optimization instability or over-specialization that corrupts cross-lingual alignment. Hidden state magnitude changes correlate with representational quality rather than being benign normalization effects.

### Mechanism 3
Cross-lingual transfer efficiency is higher when source and target languages share structural properties, but absolute performance drops persist for low-resource targets. Knowledge encoded during high-resource language fine-tuning generalizes via shared representation space; transfer efficiency measures retained performance relative to source baseline. The transfer gap reflects representational limitations rather than tokenizer or data quality issues.

## Foundational Learning

- Concept: Cosine similarity as semantic alignment metric
  - Why needed here: Core evaluation method for quantifying cross-lingual embedding relationships across all experiments
  - Quick check question: Can you explain why cosine similarity ranges from -1 to 1 and what 0.85 vs 0.50 implies for word embedding alignment?

- Concept: Hidden state probing in transformer layers
  - Why needed here: Required to interpret layer-wise representation quality and diagnose architectural degradation
  - Quick check question: What does it mean when hidden state values approach zero versus when they show high variance across layers?

- Concept: Transfer efficiency ratio
  - Why needed here: Normalizes cross-lingual performance against source-language baseline to compare transfer quality independent of absolute accuracy
  - Quick check question: If English accuracy is 0.70 and Arabic accuracy is 0.59, how would you compute transfer efficiency and what does 0.84 mean?

## Architecture Onboarding

- Component map: Text input → Tokenizer → Model forward pass → Layer-wise hidden states → Pooling strategy → Task-specific classifiers
- Critical path: Tokenization consistency check across all target languages → Layer-wise hidden state extraction → Cosine similarity computation → Transfer experiment with matched hyperparameters
- Design tradeoffs: BLOOM-1.7B shows strong initial semantic alignment but deeper layer degradation; Qwen2 offers more stable cross-layer representations; GPU constraints limited language coverage
- Failure signatures: Sharp hidden state value drop in final layers signals representation collapse; consistent 0.15-0.20 gap between language groups suggests structural transfer barriers
- First 3 experiments: Replicate layer-wise cosine similarity for high-resource (Spanish) and low-resource (Arabic) pairs; compare hidden state distributions at layers 5, 15, and 25; fine-tune with dynamic layer-wise learning rates

## Open Questions the Paper Calls Out

### Open Question 1
Can architectural modifications such as dynamic layer-wise learning rates or advanced attention mechanisms effectively mitigate the degradation of useful representations observed in the deeper layers of BLOOM? The paper identifies degradation patterns but doesn't implement specific architectural solutions to fix instability in later layers.

### Open Question 2
Does incorporating code-switched text and data from underrepresented domains into pretraining significantly enhance cross-lingual transfer efficiency for low-resource languages? The current study used standard benchmark datasets without evaluating code-switching or domain-specific data impact.

### Open Question 3
To what extent can advanced fine-tuning techniques like meta-learning or few-shot learning bridge the performance gap between high-resource and low-resource languages compared to standard fine-tuning? The experiments relied on standard fine-tuning, resulting in persistent performance drops for Arabic and Swahili.

## Limitations
- Language coverage restricted by GPU constraints, testing only six languages despite broader multilingual focus
- Low-resource language experiments show performance degradation but don't isolate whether this stems from representational limitations, tokenization coverage, or data quality issues
- Semantic alignment measurements rely on Google Translate, potentially introducing noise from translation artifacts rather than genuine linguistic relationships

## Confidence
- High Confidence: Semantic alignment patterns between related versus distant languages; BLOOM-1.7B degradation in deeper layers
- Medium Confidence: Cross-lingual transfer efficiency improvements for BLOOM-560m over BERT-base for tested languages
- Low Confidence: Attribution of low-resource performance gaps solely to representational limitations without accounting for tokenization or corpus quality issues

## Next Checks
1. Apply dynamic learning rates with higher values for deeper layers during English XNLI fine-tuning to test whether hidden state degradation and cross-lingual transfer performance improve
2. Measure token coverage for each target language on validation sets and correlate with transfer efficiency to determine if tokenization gaps explain performance drops
3. Repeat XNLI cross-lingual transfer experiments using French or Spanish as source language instead of English to test for English-specific advantages