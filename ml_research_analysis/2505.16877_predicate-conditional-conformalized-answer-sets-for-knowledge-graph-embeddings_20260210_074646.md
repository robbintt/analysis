---
ver: rpa2
title: Predicate-Conditional Conformalized Answer Sets for Knowledge Graph Embeddings
arxiv_id: '2505.16877'
source_url: https://arxiv.org/abs/2505.16877
tags:
- kgcp
- prediction
- coverage
- condkgcp
- covgap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of uncertainty quantification
  in Knowledge Graph Embedding (KGE) methods, specifically aiming to achieve predicate-conditional
  coverage guarantees. The authors propose COND KGCP, a novel method that approximates
  predicate-conditional coverage while maintaining compact prediction sets.
---

# Predicate-Conditional Conformalized Answer Sets for Knowledge Graph Embeddings

## Quick Facts
- arXiv ID: 2505.16877
- Source URL: https://arxiv.org/abs/2505.16877
- Reference count: 40
- Primary result: COND KGCP achieves superior trade-offs between conditional coverage probability and prediction set size compared to five baseline methods across six KGE models and two benchmark datasets

## Executive Summary
This paper addresses the challenge of uncertainty quantification in Knowledge Graph Embedding (KGE) methods, specifically aiming to achieve predicate-conditional coverage guarantees. The authors propose COND KGCP, a novel method that approximates predicate-conditional coverage while maintaining compact prediction sets. The key innovation involves merging predicates with similar vector representations to increase calibration data and augmenting the calibration process with rank information to reduce prediction set sizes. Theoretical guarantees are provided, showing that COND KGCP achieves conditional coverage probabilities tightly centered around the desired confidence level.

## Method Summary
COND KGCP approximates predicate-conditional coverage by partitioning predicates into groups based on embedding similarity and data availability, then applying dual calibration with rank and score thresholds. The method trains KGE models using LibKGE, extracts predicate embeddings, and uses Algorithm 1 to merge predicates with similar vector representations when calibration data is insufficient. Rank calibration filters entities by rank position, while score calibration determines the nonconformity threshold per group. The approach uses hyperparameters γ (coverage/efficiency balance) and φ (minimum group size) to control the trade-off between coverage guarantee tightness and prediction set compactness.

## Key Results
- COND KGCP outperforms five baseline methods across six KGE models and two benchmark datasets (WN18 and FB15k)
- The method achieves coverage gaps closest to the empirical lower bound while maintaining compact prediction sets
- Efficiency rates are consistently lower than competing approaches, with coverage gaps within [1-ε-0.013, 1-ε+0.042]

## Why This Works (Mechanism)

### Mechanism 1
Merging predicates with similar vector representations stabilizes calibration thresholds by increasing per-subgroup calibration data. Algorithm 1 partitions predicates into groups where each group has at least φ calibration triples. Predicates with insufficient data (|Tcal[{r}]| < φ) are merged with the most similar "data-rich" predicate using negative Manhattan distance between predicate embeddings. This ensures conformal prediction operates on reliable empirical quantiles rather than noisy thresholds from sparse data. The core assumption is that predicates with similar vector representations have similar nonconformity score distributions, justifying shared thresholds.

### Mechanism 2
Rank calibration reduces prediction set size by filtering entities with implausibly high ranks before score thresholding. The dual calibration schema first applies a rank threshold k̂(g) such that ϵk̂(g) < ε, excluding entities ranked beyond k̂(g). Score calibration then operates only on the remaining candidates with adjusted error rate ε'(g) = ε - γ·ϵk̂(g). This reduces set size when the condition in Equation 13 holds. The core assumption is that KGE models assign correct answers higher ranks on average, making rank a useful filter.

### Mechanism 3
The γ hyperparameter enables controlled trade-off between coverage guarantee tightness and prediction set compactness. γ ∈ [0,1] adjusts how coverage error is allocated between rank and score calibration. Larger γ raises the score threshold (via smaller ε'(g)), producing larger sets but tightening the upper coverage bound; smaller γ does the opposite. The coverage bounds remain symmetric around 1-ε, but γ controls asymmetry in how deviations manifest. Users can accept small, controlled coverage deviations in exchange for substantially more compact prediction sets.

## Foundational Learning

- **Conformal Prediction Fundamentals**: The entire method builds on conformal prediction's guarantee that prediction sets cover the true answer with probability ≥ 1-ε, assuming exchangeability. Why needed: Understanding this foundation is essential for grasping why the method works. Quick check: Can you explain why conformal prediction requires a held-out calibration set distinct from training data?

- **Marginal vs Conditional Coverage**: The paper's core motivation is that marginal coverage (averaged over all queries) can mask poor coverage for specific predicates, which is unacceptable in high-stakes applications. Why needed: This distinction explains the paper's focus on predicate-conditional coverage. Quick check: Why is predicate-conditional coverage stronger than marginal coverage, and why is it harder to achieve?

- **Knowledge Graph Embedding Scoring Functions**: The nonconformity score S(tr) = -Mθ(tr) derives from KGE model outputs. Understanding how TransE, RotatE, ComplEx, etc. score triples is essential for interpreting why score distributions vary across predicates. Quick check: How does the scoring function of TransE differ from RotatE, and why might this affect nonconformity score distributions?

## Architecture Onboarding

- **Component map**: Pre-trained KGE Model -> Predicate Partitioning Module -> Rank Calibration -> Score Calibration -> Prediction Set Constructor
- **Critical path**: 1) Train KGE model → extract entity/predicate embeddings and scoring function, 2) Compute similarity matrix for all predicates (Manhattan distance), 3) Run Algorithm 1 with chosen φ to obtain partition P, 4) For each group g ∈ P: compute rank calibration statistics and score quantiles on Tg, 5) At test time: given query q, identify its group g, rank all candidate entities, apply dual thresholds
- **Design tradeoffs**: φ (minimum group size) - larger φ → more stable thresholds but coarser predicate grouping; γ (coverage/efficiency balance) - controls trade-off between coverage guarantee tightness and prediction set compactness; Similarity function - Manhattan distance used, cosine or Euclidean may yield different groupings
- **Failure signatures**: CovGap ≈ KGCP baseline indicates predicate merging failed to create meaningful groups; AveSize explodes while CovGap minimal indicates rank calibration ineffective; Coverage highly variable across predicates indicates φ too small causing unstable per-group thresholds
- **First 3 experiments**: 1) Reproduce Table 1 for one KGE model-dataset pair to validate the full pipeline, 2) Ablation on φ and γ to internalize the trade-off landscape, 3) Verify condition in Equation 13 to confirm rank calibration is theoretically justified

## Open Questions the Paper Calls Out
- Can COND KGCP be extended to provide valid coverage guarantees under covariate shift? The authors state they are "working on extending our approach to handle covariate shift" in the Limitations section. This is unresolved because the current theoretical bounds rely on the i.i.d. assumption.
- Does incorporating semantic predicate features improve the robustness of the predicate merging process? The authors suggest "incorporating additional features, such as the semantic meaning of predicates" to enhance merging. This is unresolved because the current merging strategy relies solely on vector similarity.
- How effectively can the method be adapted to provide guarantees conditioned on entity types rather than predicates? The Conclusion claims the method "can be easily adapted to quantify uncertainty under other types of conditions, such as entity-type," but this is not demonstrated. This is unresolved because the algorithm is currently optimized for predicate embeddings.

## Limitations
- The method assumes predicate embedding similarity correlates with nonconformity score distribution similarity, but this relationship is not empirically validated
- Rank calibration depends critically on base KGE models assigning correct answers higher ranks, which may fail for models with poor Hits@K performance
- The minimum group size parameter φ creates a fundamental tension between stable thresholds and fine-grained grouping

## Confidence
- **High confidence**: Theoretical framework and conditional coverage guarantees
- **Medium confidence**: Empirical superiority claims, as results depend heavily on optimal hyperparameter tuning
- **Low confidence**: Predicate embedding similarity assumption without direct validation that similar embeddings produce similar score distributions

## Next Checks
1. Validate the predicate similarity assumption by comparing empirical nonconformity score distributions before and after merging for a subset of groups
2. Stress-test rank calibration across KGE models with varying Hits@K scores to identify when it becomes ineffective
3. Analyze coverage heterogeneity by computing variance of conditional coverage probabilities across predicates within each group