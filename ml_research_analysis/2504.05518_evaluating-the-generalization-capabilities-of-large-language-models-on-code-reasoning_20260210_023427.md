---
ver: rpa2
title: Evaluating the Generalization Capabilities of Large Language Models on Code
  Reasoning
arxiv_id: '2504.05518'
source_url: https://arxiv.org/abs/2504.05518
tags:
- program
- code
- programs
- reasoning
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates how large language models (LLMs) generalize
  to different program distributions in code reasoning tasks. The authors create three
  datasets: LLM-List (list-processing functions generated by LLMs), DSL-List (functions
  sampled from a domain-specific language), and LeetCode (human-written competitive
  programming solutions).'
---

# Evaluating the Generalization Capabilities of Large Language Models on Code Reasoning

## Quick Facts
- **arXiv ID**: 2504.05518
- **Source URL**: https://arxiv.org/abs/2504.05518
- **Reference count**: 40
- **Primary result**: Recent reasoning models (QwQ, DeepSeek-R1, o3-mini) achieve near-perfect generalization on code reasoning tasks, while traditional models show significant drops on out-of-distribution programs.

## Executive Summary
This paper evaluates how large language models generalize to different program distributions in code reasoning tasks. The authors create three datasets—LLM-List (list functions generated by LLMs), DSL-List (functions sampled from a domain-specific language), and LeetCode (human-written competitive programming solutions)—and introduce mutation operators to create out-of-distribution variants. They measure model performance on original vs. mutated programs to distinguish pattern matching from genuine reasoning. The study finds that recent reasoning models demonstrate strong generalization abilities across all program types and distributions, while earlier models rely heavily on pattern matching, showing high correctness on in-distribution problems but significant drops on mutated or out-of-distribution programs.

## Method Summary
The paper evaluates LLM generalization on code reasoning via execution prediction (predict output given program+input) and execution choice (select between original/mutated program, then predict output). Three datasets are created: LLM-List (112 list functions from GPT-4o), DSL-List (100 programs from custom DSL), and LeetCode (184 before cutoff May-Aug 2023, 190 after cutoff Aug 2024+). Mutation operators (arithmetic, relational, logical, keyword, literal) create out-of-distribution variants while preserving line coverage. Models are evaluated using pass@1 metrics on original/mutated programs (OC/MC) and reversion rates (OR/MR). Traditional models use temperature 0.2/top_p=0.95 with one-shot prompts; reasoning models use 0.6 with few-shot prompts.

## Key Results
- Reasoning models (QwQ, DeepSeek-R1, o3-mini) achieve near-perfect performance (99.9-100%) across all program types and distributions
- Traditional models show large correctness gaps between original and mutated programs on LLM-List, with high reversion rates indicating pattern matching
- On DSL-List, traditional models show comparable low performance on both versions with low reversion, indicating both are out-of-distribution
- Performance on original vs. mutated programs serves as a diagnostic tool for distinguishing memorization from generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mutation operators create semantically distinct programs that remain structurally similar, enabling isolation of reasoning from memorization
- Mechanism: Apply single-operator mutations that preserve line coverage and complexity while changing outputs. Models relying on pattern matching will "revert" to predicting the original program's output when given the mutated version
- Core assumption: In-distribution programs are memorized; mutations shift distribution without trivializing the task
- Evidence anchors: Abstract mentions mutated versions; section 3.2 describes Algorithm 1 filtering mutants by executability/output-difference and selecting by coverage similarity; related work focuses on semantics-preserving transformations, not semantic-changing mutations

### Mechanism 2
- Claim: The "reversion" metric quantifies pattern matching vs. genuine reasoning
- Mechanism: When given a mutated program, a pattern-matching model retrieves the memorized original output. High mutation reversion with high original correctness indicates memorization; low MR with high MC indicates generalization
- Core assumption: Memorized outputs are stored associatively with program structure; small syntactic changes don't fully disrupt retrieval
- Evidence anchors: Section 5.1 states high OC with high MR indicates pattern matching; section 6.1.1 shows DC-33B has 91.7% OC but 54% MR on LLM-List while DeepSeek-R1 has 100% OC and 0.6% MR; Mirzadeh et al. (2025) support pattern-matching hypothesis for earlier models

### Mechanism 3
- Claim: DSL sampling from combinatorially large program spaces guarantees out-of-distribution programs
- Mechanism: Define a CFG from DSL primitives and constraints, sample programs probabilistically. The space is large enough that any sampled program is unlikely to appear in training data. Comparable OC/MC on DSL-List with low reversion indicates both versions are equally OOD
- Core assumption: Training data does not systematically cover the DSL's program space
- Evidence anchors: Section 3.1 states CFG defines combinatorially large space; section 6.1.1 shows DC-33B has comparable OC/MC on DSL-List (45.8% vs 45.5%) with low reversion (~7%); Fijalkow et al. (2022) provides underlying DSL sampling framework

## Foundational Learning

- **Distribution shift vs. memorization in evaluation**: Why needed here? The paper's central goal is distinguishing whether high performance comes from generalization or from seeing similar programs during training. Without this concept, you cannot interpret OC/MC gaps.
  - Quick check question: If a model scores 95% on benchmark A and 60% on benchmark B, what are three possible explanations before concluding it "cannot generalize"?

- **Mutation testing (software engineering)**: Why needed here? The paper repurposes mutation testing—originally for measuring test suite quality—to generate OOD programs. Understanding the original intent clarifies why coverage-preservation matters.
  - Quick check question: Why does the algorithm select mutants with "most similar coverage" rather than random valid mutants?

- **Chain-of-thought reasoning in LLMs**: Why needed here? Reasoning models (QwQ, DeepSeek-R1, o3-mini) are trained specifically for extended CoT; traditional models require CoT prompting. This distinction affects experimental design (zero-shot vs. few-shot).
  - Quick check question: Why might a reasoning model fail if forced to use a one-shot prompt designed for traditional models?

## Architecture Onboarding

- **Component map**: DSL Compiler -> Mutation Engine -> Evaluation Harness -> Metric Calculator
- **Critical path**: Dataset creation → Mutation → Prompt construction → Model inference → Output parsing → Metric computation. The mutation step's coverage filtering is the key quality gate.
- **Design tradeoffs**: Lines of code as complexity proxy vs. cyclomatic complexity (LOC chosen for simplicity); single-operator mutations vs. multi-operator chains (single preserves interpretability); temperature 0.2 for traditional models (determinism) vs. 0.6 for reasoning models (prevent repetition loops)
- **Failure signatures**: High OC with high MR on mutated programs → pattern matching (expected for early models on LLM-List); low OC and low MC with low reversion → both versions OOD, model lacks capability (expected for early models on DSL-List); high preference for original programs in Execution Choice but low MC when forced → model detects distribution but cannot reason about OOD
- **First 3 experiments**:
  1. Replicate Execution Prediction on LLM-List with a traditional model (e.g., Qwen2.5-Coder-7B) to verify OC >> MC with high MR pattern
  2. Add a new mutation operator (e.g., loop bound modification) and measure whether MR changes, testing whether reversion is mutation-type dependent
  3. Run Execution Choice on DSL-List with a reasoning model to confirm ~50% preference (neither version is in-distribution) with high correctness on both

## Open Questions the Paper Calls Out

- **Cross-task transfer**: Do strong code reasoning capabilities in execution prediction tasks directly transfer to improved performance on generative tasks like code synthesis or automated program repair? The paper establishes reasoning models can predict outputs for out-of-distribution code but doesn't empirically validate if this capability improves their ability to write or fix that code. Evidence needed: cross-task evaluation where high-reasoning models are tested on code generation benchmarks using the same mutated/out-of-distribution datasets.

- **Domain generalization**: Does the observed generalization hold for program domains beyond list-processing, such as object-oriented programming, concurrency, or systems-level code? The DSL-List dataset is explicitly limited to "list-processing" primitives. Evidence needed: constructing new DSLs for diverse paradigms (e.g., concurrency DSL) and evaluating whether reasoning models maintain low reversion rates on those generated programs.

- **Mutation adequacy**: Are the mutation operators used sufficient to challenge future reasoning models, or do they fail to test complex logical fallacies? Results show reasoning models achieving "near-perfect" scores even on mutated problems, suggesting current perturbations may be too simple. Evidence needed: testing models against higher-order mutations or semantic mutations that alter control flow to see if performance degrades.

## Limitations

- The mutation operators' semantic impact is not fully validated across all program types, and the DSL sampling framework's coverage of realistic code patterns remains unclear
- The distinction between pattern matching and reasoning relies heavily on the reversion metric, which assumes memorized outputs are retrieved associatively
- The claim that standard benchmarks like LiveCodeBench insufficiently test OOD generalization is based on qualitative observation rather than systematic comparison

## Confidence

- **High confidence**: Claims about reasoning models' superior generalization and the validity of mutation/reversion as diagnostic tools
- **Medium confidence**: The interpretation that early models rely primarily on pattern matching
- **Low confidence**: The claim that standard benchmarks insufficiently test OOD generalization

## Next Checks

1. **Mutation Type Analysis**: Run experiments with individual mutation types (arithmetic only, relational only, etc.) to determine whether reversion rates vary by mutation category
2. **Coverage Preservation Validation**: Measure actual semantic complexity of selected mutants vs. originals to confirm that coverage similarity truly preserves reasoning difficulty
3. **Pattern Transfer Test**: Create a hybrid dataset where mutated programs share structural patterns with original programs but differ in key semantic features. Compare reversion rates between reasoning and traditional models to distinguish pattern transfer from genuine reasoning