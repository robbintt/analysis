---
ver: rpa2
title: 'MULTI-Bench: A Multi-Turn Interactive Benchmark for Assessing Emotional Intelligence
  ability of Spoken Dialogue Models'
arxiv_id: '2511.00850'
source_url: https://arxiv.org/abs/2511.00850
tags:
- dialogue
- arxiv
- multi-turn
- emotion
- interactive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multi-Bench introduces the first benchmark designed to evaluate
  emotional intelligence in spoken dialogue models (SDMs) through genuine multi-turn
  interactive dialogues. Unlike existing benchmarks focused on single-turn exchanges,
  Multi-Bench employs a hierarchical structure assessing both basic emotion understanding
  and advanced emotion application.
---

# MULTI-Bench: A Multi-Turn Interactive Benchmark for Assessing Emotional Intelligence ability of Spoken Dialogue Models

## Quick Facts
- arXiv ID: 2511.00850
- Source URL: https://arxiv.org/abs/2511.00850
- Reference count: 0
- Introduces first benchmark evaluating emotional intelligence in spoken dialogue models through multi-turn interactive dialogues

## Executive Summary
MULTI-Bench introduces the first benchmark specifically designed to evaluate emotional intelligence in spoken dialogue models (SDMs) through genuine multi-turn interactive dialogues. Unlike existing benchmarks focused on single-turn exchanges, MULTI-Bench employs a hierarchical structure assessing both basic emotion understanding and advanced emotion application. The benchmark comprises five tasks covering emotion recognition, paralinguistic recognition, style inference, emotion inference, and interactive dialogue, totaling approximately 3.2K samples. Experiments with six representative SDMs reveal that while models achieve good performance on basic understanding tasks, they struggle with advanced multi-turn interactive dialogue and reasoning tasks, particularly in emotion awareness and application. GPT-4o demonstrates the best overall performance, with Step Audio 2 showing strong results in emotion-related tasks.

## Method Summary
MULTI-Bench evaluates SDMs through five hierarchical tasks: Emotion Recognition, Paralinguistic Recognition, Style Inference, Emotion Inference, and Interactive Dialogue. The benchmark uses ~3,212 samples from 7 datasets and employs both acoustic and textual assessment methods. Evaluation uses Gemini 2.5 Pro for utterance-level acoustic scoring and DeepSeek-R1 for dialogue-level text scoring, validated against human judgment (0.885 correlation). The user simulator combines DeepSeek-V3.1 with Step-Audio-TTS-3B using 38 emotional prompts, supporting up to 10 dialogue turns with termination conditions including explicit stop, emotional relief, or stagnation detection.

## Key Results
- SDMs achieve good performance on basic understanding tasks (emotion recognition, paralinguistic recognition, style inference) but struggle with advanced multi-turn interactive dialogue and reasoning tasks
- GPT-4o demonstrates the best overall performance across all tasks, while Step Audio 2 shows strong results in emotion-related tasks
- Models particularly struggle with emotion awareness and application in complex interactive scenarios

## Why This Works (Mechanism)
MULTI-Bench's hierarchical task design effectively isolates and evaluates different aspects of emotional intelligence, from basic recognition to complex application in multi-turn dialogues. The use of both acoustic and textual assessment methods provides comprehensive evaluation coverage, while the user simulator with emotional TTS ensures realistic dialogue contexts. The LLM-based scoring system, despite potential variability, offers scalable and nuanced assessment of emotional intelligence dimensions.

## Foundational Learning
- **Emotional Intelligence Assessment**: Evaluating models' ability to understand and apply emotions in conversation. Why needed: Essential for realistic conversational AI. Quick check: Verify models can maintain emotional consistency across dialogue turns.
- **Multi-turn Dialogue Management**: Handling context and emotional states across extended conversations. Why needed: Real conversations require sustained emotional awareness. Quick check: Test models' ability to reference earlier emotional states.
- **Acoustic-Emotional Correlation**: Linking vocal characteristics to emotional states. Why needed: Paralinguistic cues are crucial for emotion recognition. Quick check: Validate acoustic scoring correlates with human emotion judgments.
- **LLM-based Evaluation**: Using language models as judges for complex emotional responses. Why needed: Provides scalable, nuanced assessment beyond simple metrics. Quick check: Measure inter-judge reliability and human correlation.
- **TTS Emotional Expressiveness**: Generating emotionally varied speech output. Why needed: Creates realistic dialogue contexts for testing. Quick check: Spot-check synthesized audio against intended emotion labels.
- **Dialogue Termination Detection**: Identifying appropriate endpoints for emotional conversations. Why needed: Prevents unnatural conversation continuation. Quick check: Validate termination conditions trigger appropriately across different emotional scenarios.

## Architecture Onboarding

**Component map**: User Profile + TTS Prompt -> User Simulator -> SDM -> LLM Judges -> Scores

**Critical path**: User Simulator generates emotional audio -> SDM responds with dialogue context -> LLM judges score acoustic and textual responses -> Scores aggregated for final evaluation

**Design tradeoffs**: LLM-based scoring offers scalability and nuanced assessment but introduces potential judge bias and inconsistency. The hierarchical task design provides structured evaluation but may not fully capture real-world emotional complexity. TTS emotional expressiveness enables realistic testing but depends on prompt quality and synthesis capabilities.

**Failure signatures**: 
- Inconsistent LLM judge scores across runs (0.885 human correlation suggests some variability)
- Weak TTS emotional expressiveness for nuanced prompts
- SDMs without dialogue history support failing multi-turn tasks
- Termination conditions not detecting appropriate dialogue endpoints

**First 3 experiments**:
1. Run single-turn emotion recognition task to establish baseline SDM performance on basic understanding
2. Test multi-turn interactive dialogue with termination condition monitoring to assess emotional application capabilities
3. Perform inter-judge reliability testing by running identical dialogues through scoring system multiple times

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation framework relies heavily on LLM-based scoring, introducing potential judge bias and inconsistency
- Benchmark scope limited to English-only dialogues, restricting multilingual generalizability
- Hierarchical task design may not fully capture real-world emotional complexity in natural conversations

## Confidence
- **High confidence**: Basic emotion understanding task performance claims due to clear metrics and results
- **Medium confidence**: Advanced multi-turn interactive dialogue performance claims due to LLM judge dependency
- **Medium confidence**: GPT-4o superiority claims given cross-model comparison variance potential

## Next Checks
1. Conduct inter-judge reliability testing by running the same dialogues through Gemini 2.5 Pro and DeepSeek-R1 multiple times to quantify scoring variance and calibrate against human-annotated samples
2. Perform ablation studies on the 38 emotional speech prompts to determine which specific prompts most influence TTS emotional expressiveness and overall benchmark performance
3. Test the termination condition algorithms (emotional relief, stagnation detection) across multiple SDM types to validate consistent and appropriate dialogue termination across diverse conversational scenarios