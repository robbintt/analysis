---
ver: rpa2
title: Unraveling the Capabilities of Language Models in News Summarization
arxiv_id: '2501.18128'
source_url: https://arxiv.org/abs/2501.18128
tags:
- news
- summaries
- evaluation
- summarization
- qwen1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper benchmarks 20 recent language models, including both
  large models like GPT-3.5-Turbo and GPT-4, and smaller models like Qwen1.5-7B and
  SOLAR-10.7B, for the task of news summarization. The study focuses on zero-shot
  and few-shot learning settings, using three datasets: CNN/DM, Newsroom, and XSum.'
---

# Unraveling the Capabilities of Language Models in News Summarization

## Quick Facts
- arXiv ID: 2501.18128
- Source URL: https://arxiv.org/abs/2501.18128
- Reference count: 40
- This paper benchmarks 20 recent language models for news summarization across zero-shot and few-shot settings using CNN/DM, Newsroom, and XSum datasets

## Executive Summary
This study comprehensively evaluates 20 recent language models for news summarization, comparing both large models like GPT-3.5-Turbo and GPT-4 with smaller models such as Qwen1.5-7B and SOLAR-10.7B. The research examines performance across zero-shot and few-shot learning scenarios using three established news summarization datasets. Results show that while large models generally outperform smaller ones, several compact models demonstrate competitive capabilities, particularly Qwen1.5-7B, SOLAR-Instruct-v1.0, Meta-Llama-3-8B, and Zephyr-7B-Beta.

The study reveals unexpected findings, including that few-shot learning sometimes degraded performance, potentially due to poor quality gold summaries used as demonstrations. The evaluation methodology combines automatic metrics (ROUGE, BERTScore, METEOR), human evaluation, and LLM-as-a-judge approaches to provide a comprehensive assessment of model capabilities in news summarization tasks.

## Method Summary
The study benchmarks 20 recent language models on news summarization tasks using three datasets: CNN/DM, Newsroom, and XSum. The evaluation covers both zero-shot and few-shot learning settings, with few-shot involving 10 demonstration examples. Models are assessed using automatic metrics including ROUGE, BERTScore, and METEOR, supplemented by human evaluation and LLM-as-a-judge methodologies. The study compares large models (GPT-3.5-Turbo, GPT-4) against smaller models (Qwen1.5-7B, SOLAR-10.7B, etc.) to identify performance differences and competitive capabilities across model sizes.

## Key Results
- Large language models generally outperform smaller models in news summarization tasks
- Few-shot learning sometimes degraded performance, contrary to expectations
- Qwen1.5-7B, SOLAR-Instruct-v1.0, Meta-Llama-3-8B, and Zephyr-7B-Beta demonstrated competitive performance relative to larger models

## Why This Works (Mechanism)
The study demonstrates that language models can effectively summarize news content through both zero-shot and few-shot learning approaches. The mechanism relies on the models' pre-trained understanding of language structure and context, allowing them to generate coherent summaries without extensive fine-tuning. The combination of automatic metrics, human evaluation, and LLM-as-a-judge provides a multi-faceted assessment of summarization quality, capturing both quantitative and qualitative aspects of model performance.

## Foundational Learning
- Zero-shot learning: Why needed - tests model's ability to generalize without task-specific examples; Quick check - provide prompts without demonstrations
- Few-shot learning: Why needed - evaluates how demonstration examples affect performance; Quick check - compare with zero-shot results
- Automatic metrics (ROUGE, BERTScore, METEOR): Why needed - provide quantitative, reproducible evaluation; Quick check - ensure metric scores correlate with human judgment
- LLM-as-a-judge: Why needed - scalable evaluation method that can assess multiple dimensions; Quick check - validate against human evaluation results
- Human evaluation: Why needed - captures subjective quality aspects metrics miss; Quick check - ensure inter-annotator agreement
- Dataset diversity (CNN/DM, Newsroom, XSum): Why needed - ensures findings generalize across different summarization styles; Quick check - compare performance consistency across datasets

## Architecture Onboarding

Component map:
Input text -> Model (zero-shot/few-shot) -> Summary generation -> Evaluation (automatic metrics + human evaluation + LLM-as-a-judge)

Critical path:
Data preprocessing → Prompt engineering (zero-shot/few-shot) → Model inference → Evaluation pipeline → Results aggregation

Design tradeoffs:
- Model size vs. performance: larger models perform better but cost more
- Zero-shot vs. few-shot: few-shot expected to improve results but sometimes degrades performance
- Evaluation methods: automatic metrics provide scalability but may miss nuances; human evaluation is accurate but expensive

Failure signatures:
- Few-shot learning degrading performance suggests poor demonstration quality
- Inconsistent results across evaluation methods indicate potential bias
- Models failing on specific datasets may indicate domain adaptation issues

First experiments:
1. Run zero-shot baseline on all three datasets to establish performance floor
2. Test few-shot learning with varying numbers of demonstrations (5, 10, 20)
3. Compare LLM-as-a-judge results against human evaluation on a subset of outputs

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Few-shot learning unexpectedly degraded performance, raising questions about demonstration quality and generalizability
- LLM-as-a-judge evaluation introduces potential circularity by using similar model types for judging
- Limited generalizability to fine-tuned scenarios common in production systems
- Human evaluation sample size and methodology not fully specified

## Confidence

| Claim | Confidence |
|-------|------------|
| Large models outperforming smaller models | High |
| Few-shot learning potentially harming performance | Medium |
| Comparative rankings between specific models | Medium |

## Next Checks
1. Replicate few-shot learning experiments with different gold summary examples to determine if performance degradation is consistent or demonstration-dependent
2. Conduct ablation studies on LLM-as-a-judge evaluation to quantify potential bias from using similar model architectures
3. Expand human evaluation to include more diverse annotators and provide inter-annotator agreement metrics