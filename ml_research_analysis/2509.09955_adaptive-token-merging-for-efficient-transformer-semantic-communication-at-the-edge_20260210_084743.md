---
ver: rpa2
title: Adaptive Token Merging for Efficient Transformer Semantic Communication at
  the Edge
arxiv_id: '2509.09955'
source_url: https://arxiv.org/abs/2509.09955
tags:
- merging
- communication
- token
- tokens
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational and communication bottlenecks
  of deploying large-scale transformer models on resource-constrained edge devices
  in 6G IoT systems. The authors propose a training-free, adaptive token merging framework
  that dynamically reduces the number of transformer tokens at each layer based on
  per-layer similarity thresholds, enabling data-dependent compression without retraining.
---

# Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge

## Quick Facts
- **arXiv ID**: 2509.09955
- **Source URL**: https://arxiv.org/abs/2509.09955
- **Reference count**: 40
- **Primary result**: Training-free adaptive token merging achieves 30% fewer FLOPs and <20% of original communication cost on ImageNet while maintaining accuracy.

## Executive Summary
This paper addresses the computational and communication bottlenecks of deploying large-scale transformer models on resource-constrained edge devices in 6G IoT systems. The authors propose a training-free, adaptive token merging framework that dynamically reduces the number of transformer tokens at each layer based on per-layer similarity thresholds, enabling data-dependent compression without retraining. By formulating the search for optimal merging strategies as a multi-objective Bayesian optimization problem, the method balances accuracy, computational cost (FLOPs), and communication cost (token count). Experiments on ImageNet classification and visual question answering (VQA) tasks show that the approach matches the accuracy of the unmodified transformer with 30% fewer FLOPs and under 20% of the original communication cost, while on VQA it achieves performance competitive with full LLaVA at less than one-third of the compute and one-tenth of the bandwidth. The method also improves robustness under varying wireless channel conditions and provides inherent privacy benefits by degrading the efficacy of model inversion attacks.

## Method Summary
The method implements a training-free adaptive token merging framework for transformer semantic communication. At each layer ℓ, tokens are split into source and destination sets (odd/even indices), and cosine similarity between Value vectors is computed. If similarity exceeds threshold τℓ, tokens are merged via norm-weighted averaging. The system uses multi-objective Bayesian optimization (with independent GPs and EHVI acquisition) to find Pareto-optimal threshold vectors that balance accuracy, FLOPs, and communication cost. Final tokens are transmitted via SwinJSCC over AWGN channels. The approach is evaluated on ImageNet classification and VQA tasks without any model retraining.

## Key Results
- Achieves 30% fewer FLOPs and <20% of original communication cost on ImageNet while maintaining accuracy
- On VQA tasks, matches performance of full LLaVA at less than one-third of compute and one-tenth of bandwidth
- Provides inherent privacy benefits by degrading model inversion attack efficacy as communication cost decreases

## Why This Works (Mechanism)

### Mechanism 1: Data-Dependent Token Merging
Adaptively merging tokens based on per-layer semantic similarity preserves task-critical information while reducing computational and communication overhead. The framework splits token indices into disjoint sets (sources and destinations) at each layer. It computes cosine similarity between the Value vectors of source and destination tokens. If the similarity of a pair exceeds a specific threshold (τℓ), the tokens are merged via a norm-weighted average. This reduces the token count (Nℓ) dynamically based on input redundancy rather than a fixed ratio. The core assumption is that high cosine similarity in Value space corresponds to semantic redundancy, such that averaging these tokens retains sufficient information for the downstream task. Break condition: If similarity does not equal redundancy for a specific task, merging may erase critical features and cause accuracy collapse.

### Mechanism 2: Multi-Objective Bayesian Optimization
Formulating the configuration of merging thresholds as a multi-objective Bayesian optimization problem allows for the automated discovery of Pareto-optimal trade-offs between accuracy, compute, and bandwidth. The system treats the vector of layer-wise thresholds (τ) as the search space. It uses Gaussian Processes (GPs) with a Matérn-5/2 kernel to model the expensive black-box objectives (Accuracy, FLOPs, Communication Cost). An acquisition function (Expected Hypervolume Improvement) iteratively selects the next threshold configuration expected to expand the Pareto front, balancing exploration and exploitation. The core assumption is that the objective functions are smooth enough to be modeled by GPs, and the expensive cost of evaluating a full inference pass justifies the overhead of the surrogate model. Break condition: If the optimization landscape is highly noisy or discontinuous, the GP surrogate may fail to guide the search effectively.

### Mechanism 3: Privacy Through Information Bottleneck
Aggressive token merging acts as an information bottleneck that inherently degrades the efficacy of model inversion attacks, providing privacy benefits without explicit privacy training. By coarsening representation details (merging similar tokens) before transmission, the system reduces the granularity of information available to an adversary. In the paper's model inversion setup, a reconstruction network (Gψ) trained on merged tokens produces lower-fidelity images (lower SSIM) compared to those trained on full token sets. The core assumption is that model inversion attacks rely heavily on the preservation of fine-grained, low-level features in the token stream, which are destroyed or averaged out during the merging process. Break condition: If an adversary develops inversion techniques robust to low-resolution or coarsened features, the privacy benefits may diminish.

## Foundational Learning

**Transformer Self-Attention Complexity**
- Why needed: The paper's primary motivation is the quadratic cost (O(N²)) of self-attention. Understanding why token reduction directly translates to speed and memory gains is essential.
- Quick check: Why does reducing the number of tokens (N) in a layer reduce the FLOPs quadratically rather than linearly?

**Pareto Efficiency & Multi-Objective Optimization**
- Why needed: The core contribution is a framework for navigating trade-offs. One must understand that "optimal" here means "non-dominated" (improving one objective worsens another), not a single global maximum.
- Quick check: If Policy A has higher accuracy than Policy B but uses more bandwidth, under what condition is Policy B considered Pareto-optimal?

**Bayesian Optimization (BO)**
- Why needed: The system relies on BO to search the high-dimensional space of thresholds efficiently.
- Quick check: Why is Bayesian Optimization preferred over Random Search or Grid Search when evaluating the objective function (inference) is expensive?

## Architecture Onboarding

**Component map**: Edge Device (Pre-trained Transformer Encoder, Token Merging Module, JSCC Encoder) -> Channel (AWGN) -> Server (JSCC Decoder, Task Head)

**Critical path**: The Token Merging Module is the critical insertion point. It intercepts the token stream Zℓ-1, computes Value vectors, performs the matching algorithm (Algorithm 1), and outputs a shorter sequence Zℓ.

**Design tradeoffs**:
- Early-layer merging drastically reduces FLOPs for all subsequent layers but risks losing low-level spatial features early
- Late-layer merging preserves representation quality but offers fewer computational savings (since early layers still process full tokens)
- Threshold Strictness: High thresholds (τ ≈ 1.0) preserve accuracy; lower thresholds (τ ≈ 0.5) maximize compression and privacy

**Failure signatures**:
- Accuracy Collapse: Merging too aggressively on complex images (e.g., crowds) where distinct objects have high color/texture similarity
- Stagnant Search: BO failing to improve hypervolume, indicating the kernel hyperparameters or search space bounds may be misconfigured

**First 3 experiments**:
1. Threshold Sweep (Sanity Check): Run inference on a small batch (e.g., 10 images) with constant thresholds (τ=0.6, 0.7, ...) across all layers to observe the basic accuracy-vs-FLOP trade-off curve
2. Single-Layer Isolation: Enable merging on only one layer at a time (e.g., Layer 4 vs. Layer 10) to characterize which layers are most sensitive to token reduction
3. BO Convergence Check: Run the full optimization loop for a fixed budget (e.g., 50 trials) on a held-out subset and visualize the Pareto front to ensure the optimizer is finding diverse solutions rather than clustering in one region

## Open Questions the Paper Calls Out

**Open Question 1**: How can a formal privacy metric be incorporated as a direct optimization objective without incurring prohibitive computational costs? The authors note that formally incorporating a privacy metric as a fourth objective could allow the system to explicitly discover policies, but this was excluded due to the prohibitive cost of evaluating privacy. Currently, privacy is an emergent property of token reduction rather than a driver of the optimization.

**Open Question 2**: Can the framework be adapted to optimize for hardware-specific constraints like energy consumption and latency rather than generic FLOPs? Section VIII suggests extending the framework to include hardware-aware objectives to find policies that are optimal for a given hardware target. The current study relies on FLOPs as a proxy for computational cost, which does not always correlate linearly with on-device latency or power consumption.

**Open Question 3**: Does fine-tuning a lightweight, learnable merging module offer significant gains over the training-free approach in specific domains? Section VIII proposes exploring a lightweight, learnable merging module that could be fine-tuned for specific data domains to potentially yield further performance gains. The training-free constraint ensures versatility and zero-cost deployment, but this rigidity might limit the maximum achievable compression or accuracy compared to a solution adapted to a specific data distribution.

## Limitations

- **Bayesian optimization overhead**: The absolute number of inference passes required to build a useful Pareto policy bank is not specified, which could render the method impractical for dynamic edge scenarios
- **SwinJSCC integration gap**: The paper assumes pretrained weights exist but does not provide implementation details or source links, making faithful reproduction impossible without access to compatible modules
- **Privacy mechanism uncertainty**: While the paper demonstrates reduced SSIM under aggressive merging, it does not establish whether this degradation translates to meaningful protection against modern inversion techniques that incorporate generative priors

## Confidence

**High Confidence**: The basic token merging mechanism works as described and provides measurable FLOPs reduction without catastrophic accuracy loss when thresholds are carefully chosen; the multi-objective optimization framework is technically sound and can identify non-dominated configurations when objective functions are smooth; the method is training-free and compatible with existing pretrained transformers

**Medium Confidence**: The reported Pareto-optimal trade-offs (30% FLOPs reduction, <20% communication cost at minimal accuracy loss) are achievable with the described BO approach, though exact numbers depend on implementation details and optimization budget; the privacy benefits are real but may be limited against sophisticated attacks that are robust to low-resolution inputs

**Low Confidence**: The claimed robustness improvements across varying wireless channel conditions, as the interaction between merging decisions and JSCC error resilience is not deeply analyzed; the absolute performance gains on VQA tasks relative to full LLaVA, given the complexity of multimodal reasoning and the sensitivity of VQA to representation quality

## Next Checks

1. **Single-Layer Sensitivity Analysis**: Run the merging module on each layer independently (enabling only Layer 4, then only Layer 7, etc.) to identify which layers tolerate token reduction best. This will reveal whether the BO is discovering layer-specific patterns or if aggressive early-layer merging is universally harmful.

2. **BO Convergence Validation**: Execute the full optimization loop for a fixed budget (e.g., 100 trials) and visualize the hypervolume growth curve. If hypervolume plateaus early, investigate whether the kernel hyperparameters or search space bounds need adjustment, or whether the objective function landscape is simply flat.

3. **Cross-Dataset Generalization Test**: After generating Pareto policies using the 500-sample ImageNet subset, evaluate these exact policies on a completely different dataset (e.g., CIFAR-10 or a subset of COCO). Large performance drops would indicate overfitting to the optimization set, suggesting the need for more diverse or stratified sampling strategies.