---
ver: rpa2
title: LLMs Can Plan Only If We Tell Them
arxiv_id: '2501.13545'
source_url: https://arxiv.org/abs/2501.13545
tags:
- block
- table
- input
- l1-0
- list
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether large language models (LLMs) can
  autonomously generate long-horizon plans rivaling human performance. It introduces
  AoT+ (Algorithm-of-Thoughts+), an enhanced prompting technique that builds on AoT
  by incorporating periodic state regeneration and random solution traces to reduce
  state hallucinations and computational complexity.
---

# LLMs Can Plan Only If We Tell Them

## Quick Facts
- arXiv ID: 2501.13545
- Source URL: https://arxiv.org/abs/2501.13545
- Authors: Bilgehan Sel; Ruoxi Jia; Ming Jin
- Reference count: 40
- Primary result: AoT+ achieves state-of-the-art results across multiple planning benchmarks, outperforming prior methods and demonstrating efficiency with lower token usage and faster execution times.

## Executive Summary
This paper investigates whether large language models can autonomously generate long-horizon plans rivaling human performance. It introduces AoT+ (Algorithm-of-Thoughts+), an enhanced prompting technique that builds on AoT by incorporating periodic state regeneration and random solution traces to reduce state hallucinations and computational complexity. AoT+ achieves state-of-the-art results across multiple planning benchmarks, including Blocksworld (exceeding human performance), Logistics, List Functions, and ACRE. The results show that LLMs possess latent planning capabilities that can be unlocked through structured prompting without requiring external tools.

## Method Summary
The AoT+ method is a prompting technique that teaches LLMs to emulate search algorithms through in-context learning. It works by providing the model with in-context examples that demonstrate a search process, including exploring paths, hitting dead-ends, and backtracking. The key innovations are periodic state regeneration (restating the current state after actions to reduce hallucinations) and random trajectory augmentation (using random search paths instead of only optimal solutions to prevent fixation). The method is tested across multiple planning and reasoning tasks including Blocksworld, Logistics, List Functions, and ACRE, comparing against baseline methods like Chain-of-Thought and iterative approaches.

## Key Results
- AoT+ achieves state-of-the-art results across multiple planning benchmarks, including exceeding human performance on Blocksworld
- The method consistently outperforms prior approaches including those using external verification tools
- AoT+ demonstrates significant efficiency gains with lower token usage and faster execution times compared to iterative approaches
- Random trajectory augmentation performs surprisingly well compared to intuition-based examples, challenging assumptions about example quality

## Why This Works (Mechanism)
AoT+ works by teaching LLMs to emulate a search algorithm through structured prompting rather than fine-tuning. The periodic state regeneration mechanism forces the model to maintain accurate state representations by restating the current configuration after key actions, preventing the accumulation of errors that lead to hallucinations. The random trajectory augmentation introduces diversity in the search process, preventing the model from fixating on suboptimal paths by exposing it to various exploration strategies. Together, these mechanisms enable the LLM to perform systematic exploration and backtracking similar to classical search algorithms, while the in-context learning framework allows the model to adapt this behavior to new problems without architectural modifications.

## Foundational Learning
- **Concept: Classical Planning (PDDL)** - Why needed here: Blocksworld and Logistics are formalized as classical planning problems using PDDL. Understanding states, actions, preconditions, and goals is essential to grasp what the LLM is being asked to do. Quick check question: Given a Blocksworld state "A on B on Table," what are the preconditions for the action "unstack A from B"?
- **Concept: Search Algorithms (Backtracking, Heuristics)** - Why needed here: The core improvement of AoT+ is teaching the LLM to emulate a search process, including exploring paths, hitting dead-ends, and backtracking. Familiarity with basic search concepts is crucial. Quick check question: In a search tree, what is "backtracking" and why is it necessary when a leaf node represents an invalid or non-goal state?
- **Concept: In-Context Learning / Few-Shot Prompting** - Why needed here: The entire AoT+ method is a prompting technique that relies on providing a few examples within the prompt itself to guide the model's behavior, rather than fine-tuning its weights. Quick check question: How does providing a few examples of input-output pairs in a prompt enable a pre-trained LLM to perform a new task?

## Architecture Onboarding
- **Component map:** Prompt Generator -> LLM -> (Optional External Verifier)
- **Critical path:** 1. Problem Formalization: Convert natural language planning task into structured problem instance (initial state, goal state). 2. Prompt Construction: Generate AoT+ prompt with in-context examples featuring random trajectories with backtracking and periodic state restatements. 3. Inference: Query LLM with constructed prompt. 4. Validation: Check LLM-generated plan for validity.
- **Design tradeoffs:** Random Trajectory Augmentation trades human curation cost for prompt generation efficiency, sacrificing potentially optimal heuristics for generalizable examples. Periodic State Regeneration adds token overhead but reduces overall token usage by avoiding iterative re-prompting.
- **Failure signatures:** State Hallucination (LLM generates actions based on incorrect internal state model) and Repeating Ineffective Actions (model fails to backtrack from unrecognized dead-ends).
- **First 3 experiments:** 1. Baseline vs. AoT+ Ablation: Compare Blocksworld benchmark using CoT, vanilla AoT, AoT with only memoization, and full AoT+. 2. Trajectory Quality Ablation: Compare performance on Game of 24 using human-curated vs. random traces. 3. Cross-Model Scaling Analysis: Test full AoT+ pipeline across LLaMA 3.1 8B, 70B, 405B on Logistics domain.

## Open Questions the Paper Calls Out
- **Open Question 1:** To what extent can random search trajectories replace human-curated examples in Algorithm-of-Thoughts (AoT+) without performance degradation? The authors note random trajectories perform surprisingly well compared to intuition-based ones, but the theoretical justification remains incomplete.
- **Open Question 2:** Is there a minimum model scale or architecture requirement for LLMs to effectively utilize the AoT+ state regeneration mechanism? Table 3 shows significant performance variance across model sizes, but the paper doesn't analyze if the technique requires specific emergent capabilities.
- **Open Question 3:** Does periodic state regeneration in AoT+ fully resolve the "cognitive load" of long contexts, or does error accumulation still impose a limit on planning horizon depth? While AoT+ reduces hallucinations, it's unclear if the technique scales indefinitely or just pushes the failure point further out.

## Limitations
- The evaluation is confined to synthetic planning tasks with structured state representations, leaving open questions about performance on open-ended real-world planning scenarios
- The study does not compare against domain-specific planning algorithms (e.g., Fast-Forward, SATPlan) which are known to outperform LLMs on classical planning tasks
- The assertion that AoT+ achieves "human-level" performance on Blocksworld lacks proper controls for expertise level, time constraints, and familiarity with the task domain

## Confidence
- **High Confidence**: Experimental results demonstrating AoT+ superiority over CoT and AoT baselines on tested benchmarks with clear quantitative metrics
- **Medium Confidence**: The claim that AoT+ "unlocks latent planning capabilities" is supported within experimental scope but overstates generality
- **Low Confidence**: The assertion that AoT+ achieves "human-level" performance lacks proper controls for expertise and task familiarity

## Next Checks
1. **Cross-Domain Generalization Test**: Apply AoT+ to planning tasks outside evaluated domains (e.g., STRIPS problems, hierarchical task networks, or real-world robotics planning scenarios) to assess whether the prompting strategy generalizes beyond benchmark tasks.
2. **Comparison with Classical Planners**: Benchmark AoT+ against state-of-the-art classical planning algorithms on identical PDDL problems to establish whether the LLM-based approach offers advantages beyond token efficiency.
3. **Human Expert Comparison Protocol**: Design a controlled study comparing AoT+ performance against planning experts under matched conditions (time limits, problem complexity, domain knowledge) to validate "human-level" performance claims with proper controls.