---
ver: rpa2
title: 'OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Languages
  and Modalities'
arxiv_id: '2505.23856'
source_url: https://arxiv.org/abs/2505.23856
tags:
- languages
- harmful
- arxiv
- prompts
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OMNIGUARD addresses the challenge of detecting harmful prompts
  across languages and modalities by leveraging internal representations of LLMs/MLLMs
  that are universally aligned. It identifies model layers producing language- and
  modality-agnostic embeddings using a custom U-Score metric, then trains a lightweight
  classifier on these embeddings.
---

# OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Languages and Modalities

## Quick Facts
- **arXiv ID:** 2505.23856
- **Source URL:** https://arxiv.org/abs/2505.23856
- **Reference count:** 27
- **Primary result:** OMNIGUARD sets new state-of-the-art for multilingual and multi-modal AI safety moderation, achieving 86.36% accuracy on multilingual text attacks, 88.31% on image-based prompts, and 93.09% on audio prompts while being ~120× faster than baselines.

## Executive Summary
OMNIGUARD addresses the challenge of detecting harmful prompts across languages and modalities by leveraging internal representations of LLMs/MLLMs that are universally aligned. It identifies model layers producing language- and modality-agnostic embeddings using a custom U-Score metric, then trains a lightweight classifier on these embeddings. This approach avoids separate guard models and reuses existing computation, making it highly efficient and effective at detecting both traditional and emerging attack types.

The method demonstrates state-of-the-art performance across multilingual text, image, and audio prompts, outperforming existing baselines by significant margins while maintaining efficiency through computational reuse. OMNIGUARD's ability to rapidly adapt to new attack types with few-shot examples further distinguishes it from traditional safety moderation approaches.

## Method Summary
OMNIGUARD leverages the universally aligned internal representations of LLMs and MLLMs to detect harmful prompts across languages and modalities. The approach identifies the optimal layer in a model's architecture that produces language- and modality-agnostic embeddings using a custom U-Score metric. Once the optimal layer is identified, a lightweight classifier is trained on these embeddings to detect harmful prompts. This method reuses the existing computational resources of the target model rather than requiring separate guard models, resulting in significant efficiency gains. The approach is designed to handle both traditional textual attacks and emerging multi-modal threats without requiring separate moderation models for each language or modality.

## Key Results
- Achieves 86.36% accuracy on multilingual text attacks, outperforming strongest baselines by 11.57%
- Reaches 88.31% accuracy on image-based prompts and 93.09% on audio prompts
- Demonstrates ~120× speedup compared to baseline approaches by reusing existing LLM computations

## Why This Works (Mechanism)
OMNIGUARD works by exploiting the fact that modern LLMs and MLLMs develop universal internal representations that are aligned across languages and modalities. These models learn to map different inputs into a shared semantic space during pre-training. By identifying the specific layer where these representations become most universally aligned (using the U-Score metric), OMNIGUARD can extract embeddings that capture the semantic meaning of prompts regardless of their language or modality. The lightweight classifier then operates on these universal embeddings to detect harmful content, avoiding the need for separate moderation models for each language or modality.

## Foundational Learning

**Universal Representation Alignment**
- Why needed: Modern models develop shared semantic spaces across languages/modalities during pre-training
- Quick check: Verify that embeddings from different languages/modalities cluster together in representation space

**Layer Selection via U-Score**
- Why needed: Not all layers produce equally aligned representations; optimal layer varies by model
- Quick check: Compute U-Score across all layers to identify the most universally aligned layer

**Computational Reuse**
- Why needed: Separate guard models are inefficient; reusing existing computation provides ~120× speedup
- Quick check: Compare inference time with and without separate guard model

## Architecture Onboarding

**Component Map**
Target LLM/MLLM -> U-Score Layer Selection -> Universal Embedding Extraction -> Lightweight Classifier -> Harmful Prompt Detection

**Critical Path**
The critical path is: Input prompt → Target model forward pass → Universal embedding extraction → Classifier inference. The bottleneck is the forward pass through the target model, but this is unavoidable as we need the internal representations.

**Design Tradeoffs**
- Efficiency vs. Accuracy: Using existing model computation provides massive speedups but requires the target model to be accessible
- Generalization vs. Specificity: Universal embeddings work across languages/modalities but may miss modality-specific nuances
- Few-shot vs. Full Training: Rapid adaptation to new attacks vs. potentially higher accuracy with full fine-tuning

**Failure Signatures**
- Poor performance on languages/modalities with limited pre-training data
- Degradation when target model is fine-tuned on task-specific data
- Failure to detect novel attack types that exploit modality-specific vulnerabilities

**First Experiments**
1. Compute U-Score across all layers for a multilingual model to verify optimal layer selection
2. Test classifier accuracy on held-out languages/modalities not seen during training
3. Measure inference time with and without computational reuse to confirm ~120× speedup

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the generalizability of OMNIGUARD's performance to truly unseen attack types beyond the few-shot examples tested, the potential brittleness of relying on internal representations that may shift with model updates or fine-tuning, and whether the approach will remain effective as models evolve or as attackers develop new evasion strategies.

## Limitations
- Performance may degrade when target models are fine-tuned on task-specific data, potentially disrupting universal representation alignment
- Limited evaluation of robustness against truly novel attack types that exploit potential weaknesses in the U-Score layer selection process
- Assumes internal representations remain consistently aligned across languages and modalities, which may not hold under adversarial conditions

## Confidence
- **High confidence:** Reported accuracy improvements over baselines and efficiency gains (~120× speedup) due to reuse of existing LLM computations
- **Medium confidence:** Few-shot adaptation claims, as evaluation only considers limited new attack types without testing long-term adaptability
- **Low confidence:** Assertion that OMNIGUARD will remain effective as models evolve or as attackers develop new evasion strategies

## Next Checks
1. Test OMNIGUARD's robustness against a broader, more diverse set of novel attack types and prompts not included in original training or validation sets, including those designed to exploit potential weaknesses in the U-Score layer selection process.

2. Evaluate the approach's performance on a wider range of LLM and MLLM architectures, especially those not pre-trained with multilingual or multi-modal objectives, to confirm that internal representation alignment is not architecture-specific.

3. Conduct a longitudinal study to assess the stability of OMNIGUARD's performance over time as target models receive updates, fine-tuning, or exposure to new data, and as adversarial tactics evolve.