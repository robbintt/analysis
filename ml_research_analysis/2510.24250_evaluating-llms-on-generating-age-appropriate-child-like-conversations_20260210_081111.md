---
ver: rpa2
title: Evaluating LLMs on Generating Age-Appropriate Child-Like Conversations
arxiv_id: '2510.24250'
source_url: https://arxiv.org/abs/2510.24250
tags:
- liker
- barn
- language
- leke
- children
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated five LLMs' ability to generate age-appropriate
  Norwegian child-like conversations for ages 5 and 9. Expert education professionals
  assessed text samples, achieving good inter-rater reliability (ICC=0.75).
---

# Evaluating LLMs on Generating Age-Appropriate Child-Like Conversations

## Quick Facts
- arXiv ID: 2510.24250
- Source URL: https://arxiv.org/abs/2510.24250
- Reference count: 40
- Primary result: Expert education professionals achieved ICC=0.75 reliability in evaluating Norwegian child-like LLM conversations, with higher accuracy for 5-year-olds (MAE=1.20) than 9-year-olds (MAE=2.69)

## Executive Summary
This study evaluated five LLMs' ability to generate age-appropriate Norwegian child-like conversations for ages 5 and 9 using expert blind assessments. Education professionals demonstrated strong inter-rater reliability (ICC=0.75) in judging age-appropriateness, with significantly higher accuracy for 5-year-olds than 9-year-olds. Most models produced language perceived as more linguistically advanced than target ages, suggesting adult-centric training data biases. While computational metrics like AoA and word frequency failed to capture developmental differences, expert evaluation proved reliable for this specialized application.

## Method Summary
The study used five LLMs (GPT-4, RUTER-LLAMA-2-13b, GPTSW-6.7b, NorMistral-7b, NorBloom-7b) to generate Norwegian conversational responses to 10 neutral questions. Expert education professionals conducted blind assessments, predicting the age of speakers without knowing source or target age. Evaluators achieved strong inter-rater reliability (ICC=0.75). Computational analyses using AoA ratings, word frequency, and structural characteristics were compared against human judgments to evaluate metric adequacy.

## Key Results
- Expert evaluators achieved ICC=0.75 inter-rater reliability in age-appropriateness judgments
- MAE was 1.20 years for 5-year-olds vs 2.69 years for 9-year-olds (t(9) = -5.352, p < 0.001)
- 7 out of 10 LLM-generated texts were perceived as more linguistically advanced than target ages
- Computational metrics failed to capture developmental appropriateness that experts readily detected

## Why This Works (Mechanism)

### Mechanism 1: Expert Blind Evaluation Protocol
- Claim: Education professionals can reliably distinguish age-appropriate from inappropriate child language when evaluating LLM output without knowing source or target age.
- Mechanism: Evaluators rely on implicit developmental linguistic knowledge accumulated through professional observation. Blind conditions prevent expectancy bias. Inter-rater reliability (ICC=0.75) emerges from shared professional training and repeated exposure to children's natural language patterns across developmental stages.
- Core assumption: Education professionals' judgments about age-appropriateness reflect genuine linguistic developmental markers rather than arbitrary preferences.
- Evidence anchors:
  - [abstract] "eleven education professionals conducted blind assessments...achieving strong inter-rater reliability (ICC=0.75)"
  - [section 3.5.1] "evaluators were not informed which samples were real versus LLM-generated, nor were they told the intended target ages"
  - [corpus] Related work "Can Large Language Models Describe Pictures Like Children?" similarly uses expert comparison but lacks reliability metrics; this paper's ICC strengthens validity claims
- Break condition: If evaluators had prior specialized research training in child language development, their judgments may not generalize to typical professional contexts.

### Mechanism 2: Developmental Asymmetry in Recognition Accuracy
- Claim: Linguistic markers of early childhood (age 5) are more distinct and consistently recognized than those of middle childhood (age 9), producing higher prediction accuracy for younger ages.
- Mechanism: Early language acquisition follows more constrained, universal developmental trajectories with clearer milestones (vocabulary bursts, syntax emergence). Middle childhood shows greater individual variability, topic-dependent sophistication, and less abrupt transitions, making age estimation noisier.
- Core assumption: The observed accuracy difference (MAE 1.20 vs 2.69 years) reflects genuine properties of developmental markers rather than evaluator bias toward younger ages.
- Evidence anchors:
  - [abstract] "evaluators demonstrated higher accuracy in age prediction for 5-year-olds (MAE=1.20 years) compared to 9-year-olds (MAE=2.69 years)"
  - [section 4.3.1] "Texts from 5-year-olds were predicted significantly more accurately than those from 9-year-olds...t(9) = -5.352, p < 0.001"
  - [corpus] No direct corroboration in neighbor papers; mechanism remains paper-specific
- Break condition: If 9-year-old texts in the study were systematically more ambiguous (e.g., atypical children or unusual prompts), the asymmetry may not generalize.

### Mechanism 3: Linguistic Overestimation from Adult-Centric Training
- Claim: LLMs systematically generate child language perceived as more linguistically advanced than target ages because training corpora predominantly contain adult-authored content about children rather than authentic child-produced text.
- Mechanism: Adult-written "child-appropriate" content preserves adult sentence structure, vocabulary selection logic, and coherence patterns. When prompted to simulate children, models retrieve this adult-constructed representation rather than genuine developmental patterns.
- Core assumption: The systematic overestimation pattern (7/10 LLM texts perceived as older than target) reflects training data composition, not prompt engineering failures.
- Evidence anchors:
  - [abstract] "most models produced language perceived as more linguistically advanced than target ages"
  - [section 1] "Most LLMs training datasets predominantly consist of adult-authored content from web crawls, books, and articles"
  - [section 5.1] "this systematic bias toward linguistic sophistication likely reflects the adult-centric nature of most training corpora"
  - [corpus] "LLM Safety for Children" and related work note content appropriateness challenges but do not establish training data mechanisms; claim remains inferential
- Break condition: If prompt engineering explicitly specifying vocabulary constraints eliminated the bias, training data composition would be less causal than assumed.

## Foundational Learning

- Concept: **Age-of-Acquisition (AoA) ratings**
  - Why needed here: Provides computational grounding for whether LLM word choices match developmental vocabulary. Paper uses AoA to analyze generated texts objectively.
  - Quick check question: If an LLM generating 5-year-old language uses words with average AoA of 7 years, what does this suggest about its developmental appropriateness?

- Concept: **Inter-rater reliability (Intraclass Correlation Coefficient)**
  - Why needed here: Establishes that evaluator judgments reflect shared perceptual standards rather than individual idiosyncrasy. ICC=0.75 justifies aggregating expert ratings.
  - Quick check question: Why is ICC preferred over simple agreement percentage when evaluators rate on a continuous scale (age in years)?

- Concept: **Low-resource language constraints**
  - Why needed here: Norwegian lacks comprehensive age-stratified child language corpora, limiting both LLM training data and computational evaluation benchmarks.
  - Quick check question: How does training data scarcity for Norwegian differ from English, and what architectural implications does this have for model selection?

## Architecture Onboarding

- Component map:
Prompt Construction → LLM Generation → Text Anonymization → Expert Evaluation → Computational Analysis
        ↓                    ↓                                    ↓                        ↓
   Age/Persona         5 models compared                    10 evaluators              AoA, frequency,
   specifications     (GPT-4, RUTER-LLAMA-2-13b,           (blind, fixed order)       structural metrics
                      GPTSW, NorMistral-7b, NorBloom-7b)

- Critical path: Prompt design → model selection → evaluation protocol. The fixed presentation order across evaluators (Table 6) is a confound; future implementations should randomize per rater.

- Design tradeoffs:
  - Norwegian-specific models (NorBloom-7b, NorMistral-7b) vs. multilingual (GPT-4): Smaller specialized models showed competitive performance on age-appropriateness, suggesting pre-training on language-specific data may outperform fine-tuning larger multilingual models for this task.
  - Expert evaluation vs. computational metrics: AoA, word frequency, and structural characteristics failed to capture qualitative differences experts detected. Relying solely on computational metrics risks missing developmental inauthenticity.

- Failure signatures:
  - **Verbose over-sophistication**: GPT-4's longer responses (67-81 words avg) with complex syntax may signal adult-like coherence masquerading as child speech.
  - **Under-generation**: NorMistral-7b's short responses (8-15 words) perceived as younger than target (predicted 3.85 years for 5-year-old prompt, 8.55 for 9-year-old).
  - **Computational metric blindness**: Minimal AoA differences between age groups (3.39-3.60 for age 5, 3.60-3.94 for age 9) despite expert-detected quality gaps—metrics alone insufficient.

- First 3 experiments:
  1. **Randomized presentation replication**: Re-run evaluation with text order randomized per evaluator to separate true inter-rater agreement from order effects (fatigue, practice).
  2. **Few-shot prompting with authentic samples**: Provide models with 1-3 authentic child interview excerpts before generation to test whether in-context developmental examples reduce overestimation bias.
  3. **Cross-age prompt calibration**: Systematically vary prompt specificity (generic "act like a 5-year-old" vs. detailed developmental constraint lists including vocabulary, sentence length, topic restrictions) to identify which prompt components most reduce linguistic overestimation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automated metrics that capture developmental linguistic markers be developed to reliably assess age-appropriateness of child-like language generation, given that current metrics (AoA, word frequency, structural characteristics) failed to correlate with expert human judgments?
- Basis in paper: [explicit] "The inadequacy of purely computational metrics for evaluating age-appropriate language generation, as objective measures failed to capture the qualitative differences human evaluators readily detected, underscoring the need for both comprehensive age-stratified linguistic resources and continued expert evaluation in specialized applications."
- Why unresolved: The Norwegian linguistic database was heavily skewed (83% of words associated with children under age 5), preventing meaningful computational differentiation between 5 and 9-year-old text characteristics.
- What evidence would resolve it: Creation of comprehensive age-stratified lexical resources for middle childhood (ages 6-12) in Norwegian, followed by correlation analysis between new automated metrics and expert human evaluations.

### Open Question 2
- Question: Do the findings from Norwegian generalize to other low and medium-resource languages, or are the limitations in generating age-appropriate child language language-specific?
- Basis in paper: [explicit] Authors state in Future Directions: "expand to cross-linguistic studies comparing Norwegian findings with other low or medium resource languages to identify universal versus language-specific patterns."
- Why unresolved: This study only examined Norwegian; the fundamental challenges identified (limited language-specific training data, bias toward adult language patterns) may manifest differently across languages with varying resource availability and linguistic structures.
- What evidence would resolve it: Replication of this evaluation methodology across multiple low and medium-resource languages using comparable expert evaluation protocols.

### Open Question 3
- Question: Does LLM-generated age-appropriate language improve educational outcomes and child engagement in actual applications, compared to generic or adult-level language?
- Basis in paper: [explicit] Authors state in Future Directions: "conduct longitudinal studies tracking LLM-generated age-appropriate language effectiveness in actual educational applications, measuring learning outcomes and engagement."
- Why unresolved: The study only evaluated perceived age-appropriateness through blind expert assessment, not whether this translates to practical effectiveness in educational contexts.
- What evidence would resolve it: Longitudinal field studies measuring learning outcomes, engagement metrics, and developmental appropriateness when children interact with systems using age-calibrated LLM-generated language versus control conditions.

### Open Question 4
- Question: Can few-shot prompt engineering using authentic child language examples improve LLM accuracy in generating age-appropriate responses?
- Basis in paper: [explicit] Authors state in Future Directions: "explore advanced prompt engineering techniques that explicitly incorporate developmental linguistic knowledge, potentially using few-shot learning with authentic child language examples."
- Why unresolved: Current prompting approaches produced systematic overestimation bias (7/10 LLM texts perceived as more advanced than target ages), suggesting implicit model knowledge is insufficient.
- What evidence would resolve it: Controlled experiments comparing zero-shot prompting against few-shot prompting with authentic age-matched child language examples, measuring prediction error reduction.

## Limitations
- Small real-child baseline (one 5-year-old and one 10-year-old interview) constrains generalizability
- Fixed presentation order across evaluators introduces potential fatigue or learning effects
- Absence of explicit prompt constraints means overestimation may reflect prompt engineering limitations
- Norwegian language context limits external validation to other low-resource languages

## Confidence

- **High Confidence**: Expert evaluation reliability (ICC=0.75) and the observed accuracy asymmetry between age groups (MAE 1.20 vs 2.69) are directly measured and statistically significant. The systematic overestimation pattern across most models is consistently demonstrated.
- **Medium Confidence**: The mechanism linking adult-centric training data to linguistic overestimation is plausible but inferential, as the paper does not directly analyze training corpora composition. The claim about Norwegian-specific models' competitive performance relative to larger multilingual models is based on limited comparisons without statistical significance testing.
- **Low Confidence**: The computational metrics' inadequacy for developmental appropriateness is asserted but not systematically validated against the expert ground truth. The paper demonstrates metrics fail to distinguish age groups, but doesn't test whether metrics would succeed with better-tuned or different metrics.

## Next Checks

1. **Presentation Order Randomization**: Replicate the expert evaluation with text presentation order randomized per evaluator to isolate true inter-rater reliability from potential order effects (practice, fatigue, anchoring).
2. **Prompt Constraint Variation**: Systematically test prompts with explicit developmental constraints (vocabulary lists, sentence length limits, topic restrictions) against the current generic persona prompts to quantify the contribution of prompt engineering to linguistic overestimation.
3. **Cross-linguistic Transfer**: Validate whether the computational metrics inadequacy generalizes by applying the same AoA and frequency analysis to English child language corpora with established developmental benchmarks, testing if metrics show similar blindness to developmental differences.