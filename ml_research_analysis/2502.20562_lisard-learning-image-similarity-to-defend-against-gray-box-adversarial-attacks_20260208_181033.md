---
ver: rpa2
title: 'LISArD: Learning Image Similarity to Defend Against Gray-box Adversarial Attacks'
arxiv_id: '2502.20562'
source_url: https://arxiv.org/abs/2502.20562
tags:
- adversarial
- attacks
- images
- gray-box
- lisard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of deep neural networks
  to gray-box adversarial attacks, where attackers have knowledge of the model architecture
  and training dataset but not the gradients. The authors propose LISArD, a defense
  mechanism that improves robustness without relying on adversarial training or additional
  models.
---

# LISArD: Learning Image Similarity to Defend Against Gray-box Adversarial Attacks

## Quick Facts
- **arXiv ID**: 2502.20562
- **Source URL**: https://arxiv.org/abs/2502.20562
- **Reference count**: 40
- **Primary result**: Achieves 84.19% accuracy against Auto-Attack on CIFAR-10 without adversarial training

## Executive Summary
This paper introduces LISArD, a novel defense mechanism against gray-box adversarial attacks that improves robustness without requiring adversarial training or additional models. The method leverages image similarity learning by minimizing the cross-correlation matrix between clean and perturbed image embeddings while simultaneously performing classification. Experiments demonstrate superior performance compared to state-of-the-art adversarial distillation methods, achieving up to 84.19% accuracy against Auto-Attack on CIFAR-10 without adversarial training. The approach is effective across multiple architectures and applicable to both gray-box and white-box attack scenarios.

## Method Summary
LISArD implements a Barlow Twins-inspired similarity learning mechanism that forces the model to produce similar embeddings for both clean and randomly perturbed images. During training, the model generates random images by adding Gaussian noise to clean images, then optimizes a combined loss function consisting of classification loss for both clean and random images plus a similarity loss that penalizes off-diagonal elements in the cross-correlation matrix of their embeddings. A dynamic weighting schedule gradually shifts emphasis from similarity learning to classification over training epochs. The method does not require adversarial training, making it computationally efficient while achieving strong robustness against gray-box attacks where the attacker knows the architecture and training data but not the gradients.

## Key Results
- Achieves 84.19% accuracy against Auto-Attack on CIFAR-10 without adversarial training
- Outperforms state-of-the-art adversarial distillation methods in gray-box settings
- Shows effectiveness across multiple architectures (ResNet, VGG, MobileNet) and datasets (CIFAR-10, CIFAR-100, Tiny ImageNet)
- Demonstrates faster training time (00:25 min/epoch) compared to adversarial training (09:52 min/epoch)

## Why This Works (Mechanism)

### Mechanism 1: Embedding Decorrelation via Cross-Correlation Constraints
The method constrains the cross-correlation matrix of clean and noisy embeddings to approximate the identity matrix, forcing the model to learn perturbation-invariant features. This redundancy reduction approach compels the network to map both clean and noisy inputs to similar latent representations by penalizing off-diagonal elements in the cross-correlation matrix.

### Mechanism 2: Gaussian Noise as a Proxy for Adversarial Perturbations
Training with simple Gaussian noise provides superior generalization against gray-box attacks compared to training with specific adversarial examples. This stochastic noise distribution smooths the decision boundary more broadly than attack-specific hardening, creating a more robust feature space.

### Mechanism 3: Dynamic Weighting of Similarity vs. Classification
A gradual shift in loss weighting from similarity learning to classification prevents compromising the classifier's plasticity. The coefficient α starts at 0.5 (equal weight) and increments to 1.0 (classification only) over training epochs, allowing the model to first learn robust features and then refine decision boundaries.

## Foundational Learning

- **Concept: Barlow Twins (Self-Supervised Learning)**
  - Why needed here: LISArD adapts the Barlow Twins objective (redundancy reduction) for supervised defense. Understanding this helps grasp why diagonalizing the cross-correlation matrix forces embedding similarity.
  - Quick check question: Does minimizing off-diagonal terms encourage or discourage dependence between feature dimensions?

- **Concept: Gray-box vs. White-box Threat Models**
  - Why needed here: The paper argues standard defenses overfit to white-box gradients. Understanding that gray-box implies "known architecture, unknown gradients" is crucial to valuing the transferability of the defense.
  - Quick check question: In a gray-box attack, can the attacker compute the loss gradient of the target model?

- **Concept: The "Clean Accuracy" Trade-off**
  - Why needed here: Table II shows LISArD lowers clean accuracy (e.g., ResNet50 drops from 96.65% to 88.07%) to gain robustness. This is a fundamental system design choice.
  - Quick check question: Does the proposed method improve clean accuracy at the cost of robustness, or vice versa?

## Architecture Onboarding

- **Component map**: Input -> CNN Backbone -> Embedding Layer -> LIS Module (Cross-Correlation) -> Loss Aggregator -> Output
- **Critical path**:
  1. Implementing the noise injection: `x_random = x_clean + sqrt(mu) * torch.randn_like(x_clean)`
  2. Correct implementation of Equation 4 (matrix normalization) is vital; unnormalized cross-correlation will destabilize training
  3. Scheduling α correctly (Equation 6)

- **Design tradeoffs**:
  - Speed vs. Specificity: Using Random noise (LISArD) is faster (00:25 min/ep) than Adversarial Training (09:52 min/ep) but may be less effective against specific white-box gradient attacks than specialized distillation
  - Architecture Agnosticism: The method works on ResNet, VGG, MobileNet, but requires access to the embedding layer (before logits)

- **Failure signatures**:
  - Background Blending: Figure 7 shows failures where objects blend into the background
  - Over-regularization: If α increases too slowly, clean accuracy may drop significantly

- **First 3 experiments**:
  1. Sanity Check (Table V replication): Train ResNet18 on CIFAR-10 using only Gaussian noise for x_R to verify if the cross-correlation loss converges
  2. Ablation on α (Table VII): Run a sweep on the decay degree δ to observe sensitivity of robustness vs. clean accuracy on a small dataset
  3. Gray-box Evaluation: Train a "surrogate" model with same architecture but standard loss, generate PGD attacks on surrogate, test against LISArD model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the injection of different noise types, specifically fractional Gaussian noise with persistence, compare to the standard Gaussian noise used in LISArD regarding defense robustness?
- Basis in paper: [explicit] The conclusion states this will be subject of further analysis
- Why unresolved: Current implementation solely utilizes standard Gaussian noise
- What evidence would resolve it: Comparative ablation study measuring robustness and clean accuracy when training with fractional Gaussian noise versus standard Gaussian noise

### Open Question 2
- Question: How does LISArD's robustness hold in scenarios where the attacker knows only the training data and the general type of architecture, rather than the specific architecture implementation?
- Basis in paper: [explicit] Suggests evaluating models when attacker only knows training data and architecture type
- Why unresolved: Current gray-box evaluation assumes attacker knows exact architecture
- What evidence would resolve it: Empirical results from attacks generated by surrogate models with different architectures trained on same data

### Open Question 3
- Question: Why does training with simple Gaussian noise yield significantly higher robustness against Auto-Attack than training with adversarial examples within the LISArD framework?
- Basis in paper: [inferred] Table V shows Random noise results in 84.19% AA accuracy versus 38.79% with AA-generated images
- Why unresolved: Suggests gap in understanding feature space dynamics
- What evidence would resolve it: Theoretical analysis of loss landscape or feature space geometry when using random versus adversarial noise

### Open Question 4
- Question: Can the LISArD training strategy be modified to minimize the drop in clean accuracy while maintaining adversarial robustness?
- Basis in paper: [inferred] Table II consistently shows drop in clean accuracy when applying LISArD
- Why unresolved: Current loss function forces invariance to noise which may degrade sensitivity to fine-grained features
- What evidence would resolve it: Experiments adjusting weighting parameter α or temperature τ to find Pareto optimal point

## Limitations

- **Missing hyperparameter specifications**: Critical parameters like noise magnitude μ, temperature τ, and balancing constant λ are not explicitly defined
- **Clean accuracy trade-off**: Method consistently lowers clean accuracy compared to standard training (e.g., ResNet50 drops from 96.65% to 88.07%)
- **Scalability concerns**: Performance degradation on complex datasets like Tiny ImageNet where clean accuracy drops to 52.34%

## Confidence

- **High Confidence** in core mechanism: Barlow Twins-inspired redundancy reduction is well-established in self-supervised learning
- **Medium Confidence** in empirical claims: Reported numbers are impressive but lack complete hyperparameter specification for independent verification
- **Low Confidence** in generalizability: Performance degradation on complex datasets and failure modes with background blending raise scalability questions

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary μ, τ, and λ around plausible values to determine their impact on both clean accuracy and adversarial robustness

2. **Cross-Dataset Transferability Test**: Train LISArD on CIFAR-10 and evaluate directly on Tiny ImageNet (without fine-tuning) to assess whether learned perturbation-invariant features generalize to different datasets

3. **Gray-Box Attack Fidelity**: Implement the surrogate model attack scenario where attacks generated on a standard-trained ResNet are tested against the LISArD model to verify gray-box setting produces different results than white-box attacks