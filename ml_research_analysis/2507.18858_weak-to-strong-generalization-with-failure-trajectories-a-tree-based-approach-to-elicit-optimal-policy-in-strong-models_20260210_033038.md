---
ver: rpa2
title: 'Weak-to-Strong Generalization with Failure Trajectories: A Tree-based Approach
  to Elicit Optimal Policy in Strong Models'
arxiv_id: '2507.18858'
source_url: https://arxiv.org/abs/2507.18858
tags:
- strong
- weak
- trajectory
- tree
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of supervising large language
  models (LLMs) that may exceed human capabilities in complex interactive decision-making
  tasks. Traditional weak-to-strong generalization (W2SG) approaches focus on simple
  classification tasks and struggle to capture the nuanced reasoning and decision-making
  required in interactive environments.
---

# Weak-to-Strong Generalization with Failure Trajectories: A Tree-based Approach to Elicit Optimal Policy in Strong Models

## Quick Facts
- arXiv ID: 2507.18858
- Source URL: https://arxiv.org/abs/2507.18858
- Reference count: 36
- This work proposes a novel framework leveraging weak model trajectories to improve strong model performance in complex interactive decision-making tasks.

## Executive Summary
This paper addresses the challenge of supervising large language models (LLMs) that may exceed human capabilities in complex interactive decision-making tasks. Traditional weak-to-strong generalization (W2SG) approaches focus on simple classification tasks and struggle to capture the nuanced reasoning and decision-making required in interactive environments. To overcome this limitation, the authors propose a novel framework that leverages trajectories of intermediate actions generated by a weak model to fine-tune a strong model, allowing it to learn not only from successful outcomes but also from failure experiences. The core innovation lies in constructing trajectory trees, a hierarchical representation that organizes weak model-generated action trajectories, capturing both successful and failed reasoning paths.

## Method Summary
The proposed method constructs trajectory trees from weak model-generated action sequences, creating a hierarchical representation of both successful and failed reasoning paths. Monte Carlo Tree Search (MCTS) is then applied to these trees to efficiently explore and generalize from the diverse experiences. The strong model is fine-tuned using these trajectories, learning optimal policies through exposure to the weak model's complete decision-making process, including failures. Theoretical analysis provides formal guarantees for the effectiveness of this method in improving W2SG performance.

## Key Results
- Substantial improvements in reasoning and decision-making capabilities across three complex task domains (WebShop, ScienceWorld, and AlfWorld)
- W2SG methods trained with weak model trajectories generalize better than strong supervised fine-tuning baselines
- The MCTS-based approach achieves the best performance, with the W2SG model even outperforming the SFT strong model in certain tasks

## Why This Works (Mechanism)
The framework works by capturing the complete reasoning process of weak models, including both successful and failed trajectories. By organizing these trajectories into hierarchical trees and applying MCTS, the strong model can efficiently explore the decision space and learn from both positive and negative experiences. This approach addresses the key limitation of traditional W2SG methods, which focus only on final outcomes rather than the intermediate reasoning process.

## Foundational Learning
- **Weak-to-Strong Generalization (W2SG)**: Why needed - to leverage weaker models to supervise stronger ones in complex tasks. Quick check - ability to learn from weaker supervisor.
- **Trajectory Trees**: Why needed - to hierarchically organize and represent reasoning paths. Quick check - effective capture of both success and failure modes.
- **Monte Carlo Tree Search (MCTS)**: Why needed - to efficiently explore and generalize from diverse experiences. Quick check - improved exploration efficiency.
- **Interactive Decision-Making**: Why needed - the target domain for the approach. Quick check - performance in complex sequential tasks.
- **Policy Elicitation**: Why needed - to extract optimal decision-making strategies from strong models. Quick check - superior policy performance over baselines.

## Architecture Onboarding

**Component Map:**
Weak Model -> Trajectory Generation -> Trajectory Tree Construction -> MCTS Application -> Strong Model Fine-tuning

**Critical Path:**
The critical path flows from weak model trajectory generation through tree construction to MCTS-guided fine-tuning of the strong model.

**Design Tradeoffs:**
- Hierarchical tree structure vs. flat trajectory representation
- MCTS exploration vs. direct trajectory imitation
- Inclusion of failure trajectories vs. only successful paths

**Failure Signatures:**
- Poor trajectory tree construction leading to suboptimal exploration
- MCTS inefficiency in large state spaces
- Weak model generating fundamentally flawed trajectories

**First Experiments:**
1. Baseline comparison with strong supervised fine-tuning
2. Ablation study removing MCTS from the pipeline
3. Evaluation on additional task domains to test scalability

## Open Questions the Paper Calls Out
None

## Limitations
- Limited generalizability to domains beyond the three evaluated (WebShop, ScienceWorld, and AlfWorld)
- Scalability concerns for larger state spaces or more complex action spaces
- Theoretical guarantees limited to simplified conditions

## Confidence
- High: Effectiveness of using weak model trajectories to improve strong model performance in evaluated tasks
- Medium: Scalability and robustness claims, given limited experimental scope
- Low: Theoretical guarantees, derived under simplified assumptions

## Next Checks
1. Evaluate the approach on additional complex task domains with larger state and action spaces to test scalability.
2. Compare the performance of the trajectory tree and MCTS method against state-of-the-art RL and imitation learning baselines.
3. Conduct ablation studies to isolate the contributions of the trajectory tree structure and MCTS to the overall performance gains.