---
ver: rpa2
title: 'Playing the Player: A Heuristic Framework for Adaptive Poker AI'
arxiv_id: '2512.04714'
source_url: https://arxiv.org/abs/2512.04714
tags:
- hand
- player
- patrick
- poker
- range
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the prevailing "solver" orthodoxy in poker
  AI by introducing Patrick, an AI designed to exploit human weaknesses rather than
  play unexploitably. The core method is a heuristic framework that models opponent
  behavior and psychological tendencies through adaptive modules like Search and Destroy
  (exploits player archetypes), Ranges (refines opponent hand probability), and The
  Lawnmower (models opponent perception).
---

# Playing the Player: A Heuristic Framework for Adaptive Poker AI

## Quick Facts
- arXiv ID: 2512.04714
- Source URL: https://arxiv.org/abs/2512.04714
- Authors: Andrew Paterson; Carl Sanders
- Reference count: 27
- Primary result: +3.7 BB/100 net win rate, outperforming average players by 16.0 BB/100

## Executive Summary
This paper introduces Patrick, a poker AI that rejects the prevailing "solver" orthodoxy in favor of exploiting human weaknesses through heuristic modeling. Unlike unexploitable Nash equilibrium approaches, Patrick uses a modular framework to model opponent behavior, psychological tendencies, and adapt strategies accordingly. In a 64,267-hand trial at 1¢/2¢ stakes against 7,159 players, Patrick achieved a +3.7 BB/100 net win rate, significantly outperforming the field average of -13.0 BB/100. The work demonstrates that targeted exploitation of human imperfection can be more effective than perfect theoretical play in real-world poker environments.

## Method Summary
Patrick implements a heuristic framework that models opponent behavior through adaptive modules including Search and Destroy (exploits player archetypes), Ranges (refines opponent hand probability), and The Lawnmower (models opponent perception). The system abstracts hand evaluation through a Relative Strengths Matrix (RSM) that maps absolute hand strength and board texture to an 11-point scale, replacing combinatorial complexity with efficient lookups. Opponent ranges are progressively narrowed through Range Reshaping Templates (RETs) that apply probabilistic action interpretations. Learning is anchored on prediction accuracy rather than financial outcomes, with the system improving its decision framework through post-showdown analysis and reinforcement deltas.

## Key Results
- Achieved +3.7 BB/100 net win rate in 64,267-hand trial against 7,159 players at 1¢/2¢ stakes
- Outperformed field average (-13.0 BB/100) by 16.0 BB/100
- Demonstrated targeted exploitation, strategic adaptability, and disciplined risk management
- Successfully operated on consumer hardware with commercial poker platform integration

## Why This Works (Mechanism)

### Mechanism 1
The Relative Strengths Matrix (RSM) enables modular, maintainable strategy logic by abstracting hand evaluation into an 11-point scale based on absolute strength and board texture. This replaces combinatorial explosion in the General Algorithm with single lookup queries, encapsulating complexity into an efficient decision layer.

### Mechanism 2
Range Reshaping Templates (RETs) enable progressive opponent hand narrowing through probabilistic action interpretation. Each opponent action triggers predefined RETs that re-weight hand strength categories, allowing the system to narrow from initial wide distributions to precise final reads based on interpretable action patterns.

### Mechanism 3
Anchoring learning on prediction accuracy rather than financial outcomes enables improvement despite high variance. The system re-runs hands with complete information post-showdown, comparing predictions against ground truth to provide reinforcement deltas that improve decision-making without being misled by short-term results.

## Foundational Learning

- **Incomplete information games vs. complete information analysis**: Essential for understanding Patrick's learning method that depends on knowing hidden cards only post-hoc. Quick check: Can you explain why All-in Adjusted (AIVAT) metrics differ fundamentally from bankroll outcomes in raked cash games?

- **Variance and sample size requirements**: Critical for interpreting results and avoiding overfitting. Quick check: Why does the paper argue that the All-in Adjusted "yellow line" accounts for only ~2% of total variance?

- **Exploitative vs. Nash Equilibrium strategies**: Necessary for evaluating Patrick's core philosophy of rejecting unexploitable play for targeted exploitation. Quick check: What specific vulnerability does Patrick's "sword" approach accept that a "shield" (GTO) approach would avoid?

## Architecture Onboarding

- **Component map**: World Interface (WI) -> Game and Translation Engine (GTE) -> Brain (Supporting Modules: HAA, RSM, Memory; Decision Modules: GA, Ranges, SAD, Lawnmower) -> Master Algorithm -> GTE -> WI

- **Critical path**: WI captures screen state → GTE validates game state and legal actions → GTE populates variables (SPR, pot odds, positions, stack sizes) → HAA sets baseline strategic mode → RSM provides relative strength value → Ranges calculates opponent probability distribution → SAD checks for identified vulnerabilities → Lawnmower evaluates deception opportunities → GA provides baseline recommendation → Master Algorithm synthesizes weighted recommendations → final action → GTE translates to mouse/keyboard → WI executes

- **Design tradeoffs**: Abstraction vs. precision (RSM may miss edge-case strategic factors), Exploitability vs. robustness (SAD enables exploitation but makes Patrick vulnerable to counter-exploitation), Consumer hardware vs. compute scale (enables accessibility but limits complexity), Micro-stakes testing vs. generalization (high variance stress test but may not transfer to higher stakes)

- **Failure signatures**: UI changes trigger system crash (documented from poker site UI alteration), Network/power failures (four of five failures were external; ~20% incur direct equity loss), Prediction-accuracy divergence (if RSM deltas drift without convergence)

- **First 3 experiments**: 1) RET sensitivity analysis: Vary RET parameters for fixed hand and measure ChiB output stability, 2) RSM boundary testing: Input edge-case hand/board combinations and verify output values, 3) Module ablation study: Disable SAD module for 5,000-hand sample and compare win rate delta

## Open Questions the Paper Calls Out

### Open Question 1
How would Patrick's exploitative strategy perform against higher-stakes, more technically proficient opponents compared to the micro-stakes player pool? The trial was conducted exclusively at 1¢/2¢ micro-stakes, which is not representative of the more strategically uniform and technically proficient opponents found at higher stakes.

### Open Question 2
Would a larger sample size (200,000+ hands) confirm the long-term sustainability of Patrick's +3.7 BB/100 win rate? A larger sample would be required to achieve higher confidence in the long-term sustainability of this win rate.

### Open Question 3
Can the "sword-based" philosophy and cognitive modelling framework generalize to domains beyond poker? Future work will seek to generalize this approach beyond a single game, shifting from "What is the opponent's most likely action?" to "What is the underlying cognitive state that motivates that action?"

## Limitations

- The trial was conducted exclusively in the 1¢/2¢ micro-stakes environment, which is not representative of the more strategically uniform and technically proficient opponents found at higher stakes.
- The 64,267-hand sample, while statistically significant, still leaves approximate ±2 BB/100 uncertainty due to poker's high variance.
- The framework's reliance on heuristic modeling may not generalize to other decision-making domains without significant adaptation.

## Confidence

- **High Confidence**: The core methodology of modeling opponent behavior through RETs and the demonstration that Patrick achieved positive win rates against a large player sample.
- **Medium Confidence**: The specific mechanisms of the RSM abstraction and RET implementation, though lacking external validation.
- **Low Confidence**: Claims about Patrick's adaptability to counter-exploitation and sophisticated opponent adjustments.

## Next Checks

1. **Multi-stakes transferability test**: Deploy Patrick at multiple stake levels (e.g., 5¢/10¢, 25¢/50¢, $1/$2) with stratified sampling across opponent skill distributions. Measure win rate consistency and identify thresholds where the heuristic framework requires recalibration.

2. **Ablation study of abstraction layers**: Systematically remove or modify the RSM abstraction and RET mechanisms in controlled environments. Compare performance against the full system to quantify the contribution of each abstraction layer to overall success, and identify specific scenarios where the abstraction fails.

3. **Long-term adaptation stability monitoring**: Run Patrick continuously for 500,000+ hands with periodic performance audits. Track RSM delta convergence patterns, identify any learning instability or catastrophic forgetting, and measure whether prediction accuracy remains correlated with win rate over extended periods.