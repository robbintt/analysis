---
ver: rpa2
title: 'PIM-LLM: A High-Throughput Hybrid PIM Architecture for 1-bit LLMs'
arxiv_id: '2504.01994'
source_url: https://arxiv.org/abs/2504.01994
tags:
- architecture
- llms
- arxiv
- operations
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PIM-LLM, a hybrid analog-digital architecture
  that leverages analog processing-in-memory (PIM) and digital systolic arrays to
  accelerate 1-bit large language models (LLMs). The key idea is to use PIM for low-precision
  matrix multiplications in projection layers and digital systolic arrays for high-precision
  matrix multiplications in attention heads.
---

# PIM-LLM: A High-Throughput Hybrid PIM Architecture for 1-bit LLMs

## Quick Facts
- arXiv ID: 2504.01994
- Source URL: https://arxiv.org/abs/2504.01994
- Reference count: 40
- Primary result: 80× improvement in tokens per second and 70% increase in tokens per joule for 1-bit LLMs

## Executive Summary
PIM-LLM presents a hybrid analog-digital architecture that leverages analog processing-in-memory (PIM) for low-precision matrix multiplications in projection layers and digital systolic arrays for high-precision matrix multiplications in attention heads. This partitioning strategy achieves significant performance gains by exploiting the computational characteristics of 1-bit large language models, where projection layers dominate the workload. The architecture demonstrates up to 80× improvement in tokens per second and a 70% increase in tokens per joule compared to conventional hardware accelerators, setting new benchmarks for PIM-based LLM acceleration.

## Method Summary
The PIM-LLM architecture combines analog PIM with memristive crossbars (256×256 RRAM) for binary/ternary matrix-vector multiplications in projection layers, with a digital 32×32 systolic array (8-bit MACs, OS dataflow, 100 MHz) for high-precision attention operations. The system uses LPDDR memory for storing activations and weights, with SRAM buffers for intermediate results. Custom components include a Softmax unit (ConSMax) and post-processing units for LayerNorm and GELU operations. The architecture is evaluated using cycle-accurate simulations (SCALE-Sim for digital TPU, MNSIM 2.0 for PIM) across multiple model sizes (GPT-355M to LLaMA-7B) and context lengths (128-4096).

## Key Results
- Achieves up to 80× improvement in tokens per second compared to conventional hardware accelerators
- Demonstrates 70% increase in tokens per joule energy efficiency
- Outperforms previous PIM-based LLM accelerators with at least 2× and 5× improvement in GOPS and GOPS/W respectively

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Workload Partitioning
The architecture partitions computation based on precision requirements, mapping low-precision (1-bit) projection layers to analog PIM while routing high-precision (8-bit) attention operations to digital systolic arrays. This avoids accuracy loss and write-energy overhead from performing analog computation on dynamic attention data. The approach leverages the fact that projection layers exceed 99% of operations in larger models, making them ideal candidates for PIM acceleration.

### Mechanism 2: Analog In-Memory Computation
By storing binary/ternary weights directly in memristive crossbars and performing matrix-vector multiplication in the analog domain via Kirchhoff's laws, the architecture eliminates the need to fetch weights from off-chip memory for every operation. This reduces the "memory wall" bottleneck for the dominant projection layers, though the signal-to-noise ratio must be sufficient to maintain accuracy for 1-bit weights.

### Mechanism 3: Output-Stationary Dataflow Optimization
The digital TPU component uses output-stationary (OS) dataflow, which keeps partial sums stationary in processing elements while inputs and weights flow past. This approach is optimized for decoder-only LLM attention patterns where inference generates one token per iteration, reducing register file access frequency compared to weight-stationary or input-stationary approaches.

## Foundational Learning

- **Processing-in-Memory (PIM) vs. Von Neumann**: The core value proposition moves compute to data, reducing latency and energy costs of fetching weights from DRAM. Quick check: Does PIM reduce latency by faster clock speeds or by reducing data movement distance?

- **1-bit LLMs (Binary/Ternary Quantization)**: The hardware is co-designed for this specific model type where weights are {-1, 0, +1}. Quick check: In a 1-bit LLM, which part of the transformer usually retains high precision (e.g., 8-bit) to maintain accuracy?

- **Systolic Arrays**: The digital half of the hybrid architecture uses rhythmic data flow through a grid of processing elements. Quick check: In an Output-Stationary systolic array, what remains fixed inside the PE while inputs and weights flow past?

## Architecture Onboarding

- **Component map**: PIM Banks (Tiles → PEs → RRAM Crossbars, ADCs, DACs, Post-Processing) → TPU (32×32 Systolic Array, 8-bit MACs, SRAM, Softmax) → LPDDR Memory → Control (Scheduler, Controller, CPU)

- **Critical path**: Projection Path: Input Vector → DAC → RRAM Crossbar (Analog MVM) → ADC → Post-Processing → Output; Attention Path: Input/Weights → SRAM → Systolic Array (Digital MVM) → Nonlinear Unit → Output

- **Design tradeoffs**: Area vs. Precision (digital TPU guarantees 8-bit precision but consumes area); Energy vs. Speed (PIM offers high speed but has high static/active power dissipation)

- **Failure signatures**: Accuracy degradation from ADC quantization errors or RRAM noise; throughput saturation when context length grows >1024; endurance failure from frequent writes to PIM

- **First 3 experiments**: 1) Latency profiling vs. context length (128-4096) to verify TPU crossover point; 2) Energy breakdown measuring PIM vs. TPU power consumption; 3) Noise injection test to determine maximum tolerable noise floor before perplexity degradation

## Open Questions the Paper Calls Out

### Open Question 1: Digital Systolic Array Latency Bottleneck
How can the latency bottleneck in digital systolic arrays be mitigated for 1-bit LLMs at very long context lengths? The paper acknowledges this bottleneck (over 97% of total latency at context length 4096) but dismisses it as "less significant for edge applications" without addressing architectural limitations for long-context scenarios.

### Open Question 2: Analog Non-Idealities Impact
What is the impact of analog non-idealities and device variations on inference accuracy? The paper relies on behavioral simulation without evaluating functional correctness or perplexity under analog noise, leaving practical deployment viability uncertain.

### Open Question 3: PIM for Attention Heads
Can the analog PIM architecture be adapted to support activation-to-activation matrix multiplications in attention heads without compromising device endurance? The current hybrid approach excludes PIM for attention heads due to write energy overheads and potential device failures, blocking a major efficiency opportunity.

## Limitations

- The hybrid architecture's effectiveness depends on workload partitioning that may not generalize across different model architectures or quantization schemes
- Energy efficiency claims are sensitive to relative power consumption of ADCs, DACs, and memristive crossbars, which are not fully characterized
- Performance superiority at long context lengths relies on the assumption that projection layers will remain computationally dominant, which may not hold for all LLM variants

## Confidence

**High Confidence**: The hybrid architecture design rationale is well-justified based on transformer model characteristics and the claim that projection layers account for >99% of operations in larger models.

**Medium Confidence**: The 80× throughput improvement and 70% energy efficiency gains are based on cycle-accurate simulations with reasonable assumptions, though real silicon performance depends on factors not fully explored.

**Low Confidence**: The absolute performance numbers relative to previous PIM-based accelerators lack clear benchmarking methodology details for the specific metrics claimed.

## Next Checks

1. **Noise Floor Validation**: Conduct systematic noise injection study on analog crossbar simulation parameters to determine tolerance limits before 1-bit LLM accuracy (perplexity) degrades.

2. **Context Length Scalability**: Implement detailed profiling experiment across full context length range (128-4096) to verify predicted crossover points where TPU latency dominates PIM latency.

3. **Energy Breakdown Validation**: Perform comprehensive power measurement campaign isolating PIM subsystem (crossbars, ADCs, DACs, post-processing) from digital TPU subsystem during actual inference workloads.