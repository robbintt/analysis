---
ver: rpa2
title: 'Vanishing Depth: A Depth Adapter with Positional Depth Encoding for Generalized
  Image Encoders'
arxiv_id: '2503.19947'
source_url: https://arxiv.org/abs/2503.19947
tags:
- depth
- encoding
- encoders
- rgbd
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Vanishing Depth addresses the lack of generalized depth encoders
  for vision-guided robotics by proposing a self-supervised training pipeline that
  extends pretrained RGB encoders with metric depth understanding. The method introduces
  positional depth encoding (PDE) for stable depth embeddings, randomizes depth distributions
  and densities to ensure generalization, and uses multi-scale balanced scale-invariant
  loss for stable training.
---

# Vanishing Depth: A Depth Adapter with Positional Depth Encoding for Generalized Image Encoders

## Quick Facts
- **arXiv ID:** 2503.19947
- **Source URL:** https://arxiv.org/abs/2503.19947
- **Reference count:** 40
- **Primary result:** Achieves SOTA on RGBD tasks (56.05 mIoU on SUN-RGBD, 88.3 RMSE on Void depth completion, 83.8 Top 1 on NYUv2 classification) without fine-tuning the RGB encoder.

## Executive Summary
Vanishing Depth introduces a self-supervised training pipeline that extends pretrained RGB encoders with metric depth understanding for vision-guided robotics. The method employs positional depth encoding (PDE) to stabilize depth feature extraction, randomizes depth distributions and densities to ensure generalization, and uses multi-scale balanced scale-invariant loss for stable training. By keeping the RGB encoder frozen and fusing its features into a trainable depth encoder, the approach enables depth-aware feature extraction without fine-tuning. Experiments demonstrate state-of-the-art performance across multiple RGBD benchmarks, outperforming non-finetuned encoders like DinoV2, EVA-02, and Omnivore.

## Method Summary
The method trains a depth encoder using self-supervised depth completion while keeping a pretrained RGB encoder frozen. Depth inputs are transformed using positional depth encoding (sinusoidal functions) and augmented with random scaling, offsets, and masking (1-99% density). RGB features are fused into the depth encoder at layers 3, 6, 9, and 12 using squeeze-and-excitation blocks. Training uses a balanced scale-invariant loss across multiple scales. After pretraining, the decoder is discarded and the depth encoder is used for downstream tasks with lightweight task-specific heads.

## Key Results
- Achieves 56.05 mIoU on SUN-RGBD segmentation
- Achieves 88.3 RMSE on Void depth completion
- Achieves 83.8 Top 1 accuracy on NYUv2 classification
- Outperforms non-finetuned encoders like DinoV2, EVA-02, and Omnivore in 6D object pose estimation
- PDE proves more stable and precise than normalization-based methods, especially under varying depth distributions

## Why This Works (Mechanism)

### Mechanism 1: Positional Depth Encoding (PDE) for Distribution Stability
PDE stabilizes depth feature extraction by mapping unbounded metric depth values into a bounded frequency domain using sinusoidal functions. This prevents gradient explosion common with normalization-based methods by projecting scalar depth into a high-dimensional vector where large magnitude changes result in stable, periodic feature updates rather than linear scaling issues.

### Mechanism 2: Randomized Depth for Invariance
Randomizing depth distributions and densities during training forces the model to fuse semantic RGB context with depth embeddings rather than relying on monocular depth priors. The pipeline applies random offsets, scaling, and Perlin noise masking to input depth maps, breaking the fixed statistical relationship between RGB textures and absolute depth.

### Mechanism 3: Frozen RGB Alignment via Intermediate Fusion
Keeping the RGB encoder frozen while injecting its features into the depth encoder preserves the generalized semantics of the foundation model while efficiently aligning depth features to them. Features from the frozen RGB ViT are fused into the trainable Depth ViT at specific layers using squeeze-and-excitation blocks, allowing depth embeddings to be grounded by semantics without catastrophic forgetting.

## Foundational Learning

- **Concept: Sinusoidal Positional Encoding**
  - Why needed here: PDE is the core contribution. You must understand how sine/cosine functions allow a model to attend to relative magnitudes and frequencies to grasp why this solves the "unbounded depth" problem better than normalization.
  - Quick check question: If you double the input depth value, how does the PDE vector change compared to a simple normalized scalar?

- **Concept: Scale-Invariant Loss (Eigen et al.)**
  - Why needed here: The paper uses a "balanced scale-invariant loss." Understanding that this loss penalizes errors in relative depth differences (log-space) rather than absolute squared errors is key to seeing why it handles the varying depth scales in training.
  - Quick check question: Why would a standard MSE loss fail when training on a mix of macro-scale (outdoor) and micro-scale (indoor) depth data?

- **Concept: Encoder Freezing vs. Fine-Tuning**
  - Why needed here: The authors explicitly argue against fine-tuning the RGB backbone. You need to understand "feature corruption" to evaluate this design choice.
  - Quick check question: What is the risk of fine-tuning a foundation model (like DinoV2) on a small, task-specific dataset like NYUv2?

## Architecture Onboarding

- **Component map:** Input Depth -> Positional Depth Encoding -> Randomization (Scale/Mask) -> Dual ViT Encoders (Frozen RGB + Trainable Depth) -> Squeeze-and-Excitation Fusion (Layers 3,6,9,12) -> U-Net FPN Decoder (training only)

- **Critical path:**
  1. Load frozen weights (DinoV2/EVA) into both encoders
  2. Transform Depth -> PDE (ensure maxd and channel count Ch. are configured)
  3. Pass inputs; fuse RGB features into Depth stream at designated blocks
  4. Train depth adapter using balanced scale-invariant loss
  5. Discard decoder; use the Depth Encoder (or fused features) for downstream tasks

- **Design tradeoffs:**
  - PDE Channels (Ch.): 32 vs. 64. The paper notes 32 is "lightweight and nearly as precise" for 15m range, but 64 is safer for large distances. Higher channels increase compute in the first layer linearly.
  - Global vs. Dynamic maxd: Fixing maxd (e.g., 15m) simplifies decoding but requires rescaling for long-range outdoor data. Dynamic maxd (encoding the max value in the CLS token) is more flexible but complex to implement.

- **Failure signatures:**
  - Precision Collapse: If depth completion RMSE is high on sparse inputs but low on dense inputs, check PDE frequency configuration (may be too coarse).
  - Semantic Misalignment: If the model detects objects but fails to estimate pose, the fusion blocks may not be receiving sufficient RGB signal (check S&E weighting).

- **First 3 experiments:**
  1. Ablate Input Encoding: Train two small models (quick run) on NYUv2â€”one with PDE, one with standard normalization. Plot validation RMSE to verify PDE stability.
  2. Probe Fusion Depth: Run inference with frozen weights and visualize attention maps from the Depth Encoder (using PCA). Confirm that "Depth" features correlate with "RGB" semantic boundaries.
  3. Density Stress Test: Feed the trained model depth maps with increasing sparsity (0% to 99% missing) to verify the "Vanishing Depth" robustness claim before deploying to a real sensor.

## Open Questions the Paper Calls Out

### Open Question 1
How does high object occlusion affect the relative importance of metric depth precision compared to semantic features in 6D object pose estimation tasks? The authors note that results on LM-O (a dataset with high occlusion) suggest that "high occlusion could decrease the importance of depth precision," but state explicitly that "these speculations require further research."

### Open Question 2
To what extent does the curation and balancing of the training dataset improve the generalization of the Vanishing Depth encoder compared to the current uncurated collection? The authors state regarding their training data: "This array of datasets is not curated and properly balanced. In future work, we would like to... build a more sophisticated and balanced database for our training."

### Open Question 3
Why does Positional Depth Encoding (PDE) significantly outperform normalization in depth completion but offers negligible advantages in semantic segmentation? The results show PDE achieving SOTA in depth completion while the authors state regarding segmentation: "we can not observe much of a difference in segmentation tasks" between PDE and normalization.

### Open Question 4
Does the linear increase in computational cost associated with Positional Depth Encoding (PDE) channels impose a latency bottleneck for real-time robotics applications? Section 3.1 notes that "PDE increases the computational cost linear by a factor of ch. in the first convolution or fully-connected encoding layer," while the introduction emphasizes the need for generalized encoders for "vision-guided robotics" which often have strict latency constraints.

## Limitations
- The approach may not generalize to tasks requiring fine-grained depth understanding (e.g., millimeter-scale metrology) without additional domain-specific fine-tuning
- Performance on outdoor scenes is untested, and the fixed 15m depth range may be insufficient for applications like autonomous driving
- The computational overhead of PDE (32 extra channels) is non-trivial for real-time deployment on resource-constrained hardware

## Confidence
- **High confidence:** PDE's superiority over normalization-based methods (validated by ablation in Section 4.1 and consistent performance across diverse datasets)
- **Medium confidence:** Generalization to unseen sensors and extreme sparsity (supported by Void dataset results but not tested on raw LiDAR or event-based depth)
- **Low confidence:** Claims about the frozen RGB encoder preserving "generalized semantics" (lacks ablation studies comparing frozen vs. fine-tuned RGB backbones)

## Next Checks
1. **Cross-Sensor Robustness:** Evaluate on raw LiDAR (e.g., KITTI) and event-based depth (e.g., DDFF) to test generalization beyond simulated sparsity
2. **Outdoor Generalization:** Fine-tune on outdoor datasets (e.g., ScanNet Outdoor, nuScenes) to verify scalability beyond the 15m indoor depth range
3. **Real-Time Viability:** Profile PDE's computational overhead on edge devices (e.g., Jetson Orin) to assess feasibility for robotics applications