---
ver: rpa2
title: Generative Visual Code Mobile World Models
arxiv_id: '2602.01576'
source_url: https://arxiv.org/abs/2602.01576
tags:
- action
- world
- mobile
- iacc
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of visual world modeling for
  mobile GUI agents, where existing methods either sacrifice visual fidelity for text-based
  representations or struggle with precise text rendering in pixel-based approaches.
  The authors propose a novel paradigm: visual world modeling via renderable code
  generation, where a single Vision-Language Model (VLM) predicts the next GUI state
  as executable web code rather than generating pixels directly.'
---

# Generative Visual Code Mobile World Models

## Quick Facts
- arXiv ID: 2602.01576
- Source URL: https://arxiv.org/abs/2602.01576
- Authors: Woosung Koh; Sungjun Han; Segyu Lee; Se-Young Yun; Jamin Shin
- Reference count: 40
- Primary result: gWorld achieves 79.6% instruction accuracy vs 55.7% for 235B model and 67.4% for 106B model

## Executive Summary
This paper introduces a novel approach to visual world modeling for mobile GUI agents through renderable code generation rather than direct pixel generation. The authors propose that Vision-Language Models (VLMs) can leverage their linguistic priors for precise text rendering and structured web code pretraining for high-fidelity visual generation by predicting next GUI states as executable web code. This paradigm addresses the fundamental tradeoff between visual fidelity and text rendering precision that has limited existing approaches.

The gWorld models (8B and 32B parameters) represent the first open-weight visual mobile GUI world models built on this paradigm. Using a data generation framework that synthesizes code-based training data, the models establish a new performance frontier across multiple benchmarks. The 32B gWorld model achieves an average Instruction Accuracy of 79.6%, significantly outperforming much larger models (up to 50.25× larger) while maintaining computational efficiency through its innovative code-based generation approach.

## Method Summary
The proposed approach leverages Vision-Language Models to predict GUI states as executable web code rather than generating pixels directly. This method capitalizes on VLMs' pretraining on structured web code for visual generation while utilizing their linguistic capabilities for precise text rendering. The authors developed a data generation framework to synthesize code-based training data, enabling the creation of gWorld models with 8B and 32B parameters. This paradigm shift allows the models to maintain high visual fidelity while achieving precise text rendering, addressing a fundamental limitation in existing visual world modeling approaches.

## Key Results
- gWorld 32B achieves 79.6% average Instruction Accuracy across all benchmarks
- Outperforms Qwen3 VL 235B-A22B (235B parameters) with 55.7% accuracy and GLM-4.6V 106B (106B parameters) with 67.4% accuracy
- Establishes new pareto frontier in accuracy versus model size, outperforming eight frontier open-weight models up to 50.25× larger
- Demonstrates predictable performance gains following power law scaling with increased training data
- Shows that stronger world modeling translates to improved downstream mobile GUI policy performance

## Why This Works (Mechanism)
The effectiveness of gWorld stems from its innovative code-based generation paradigm that leverages the dual strengths of Vision-Language Models. By generating executable web code rather than pixels directly, gWorld exploits VLMs' pretraining on structured web code for visual generation while utilizing their linguistic capabilities for precise text rendering. This approach circumvents the fundamental tradeoff between visual fidelity and text precision that has plagued existing methods, allowing the model to maintain both high-quality visuals and accurate text representation in mobile GUI environments.

## Foundational Learning
**Vision-Language Models (VLMs)**: Multimodal models trained on both visual and textual data, capable of understanding and generating content across modalities. Why needed: VLMs provide the foundation for understanding GUI structure and content. Quick check: Verify the model can accurately describe and generate basic GUI elements from text prompts.

**Code-based Generation**: Generating executable code (HTML/CSS/JavaScript) rather than direct pixel outputs. Why needed: Enables leveraging structured web code pretraining while maintaining text rendering precision. Quick check: Confirm generated code produces accurate visual representations when rendered.

**World Modeling**: Predicting future states of an environment based on current observations and actions. Why needed: Essential for planning and decision-making in GUI navigation tasks. Quick check: Validate the model can accurately predict simple state transitions in controlled environments.

**Data Synthesis**: Automatically generating training data through programmatic means. Why needed: Enables scalable creation of diverse training examples for GUI world modeling. Quick check: Ensure synthetic data covers the full range of expected GUI elements and interactions.

**Power Law Scaling**: The relationship between training data size and model performance following a predictable mathematical pattern. Why needed: Guides efficient resource allocation for model training. Quick check: Verify performance improvements scale predictably with increased training data.

## Architecture Onboarding

**Component Map**: VLM Input -> Code Generation Module -> Render Engine -> Visual Output -> Performance Evaluation

**Critical Path**: The model receives GUI observations as input, processes them through the VLM architecture, generates executable web code through the code generation module, renders this code into visual output, and evaluates performance against ground truth.

**Design Tradeoffs**: The code-based generation approach trades computational overhead of code rendering for significant improvements in text precision and visual fidelity. While this adds complexity compared to direct pixel generation, the benefits in accuracy and leveraging existing web technologies outweigh the costs.

**Failure Signatures**: Common failure modes include incorrect HTML/CSS generation leading to layout issues, text rendering errors when handling complex fonts or styling, and challenges with highly customized UI components that deviate from standard web patterns. The model may also struggle with dynamic content loading and real-time interactions.

**First Experiments**:
1. Test basic GUI element generation and rendering with simple HTML/CSS to validate the core code generation pipeline
2. Evaluate text rendering accuracy across different font styles, sizes, and languages to assess linguistic prior effectiveness
3. Benchmark state prediction accuracy on controlled GUI transitions to establish baseline world modeling capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Performance validation limited to benchmark environments, not tested on real-world mobile applications with complex, dynamic content
- Code-based generation may struggle with highly customized or proprietary UI components that deviate from standard web design patterns
- No evaluation of model robustness across different device specifications, screen resolutions, and network conditions
- Potential saturation effects in performance scaling not fully explored at larger data scales

## Confidence
- High Confidence: Empirical performance comparisons showing gWorld outperforming much larger models (50.25×) with clear numerical improvements (79.6% vs 55.7% vs 67.4%)
- Medium Confidence: Claims about code-based generation superiority for text rendering based on logical reasoning about VLM pretraining, would benefit from systematic ablation studies
- Medium Confidence: Power law scaling relationship observed but based on limited data scaling experiments that may not capture saturation effects

## Next Checks
1. Test gWorld models on real-world mobile applications with non-standard UI components and dynamic content loading to assess generalization beyond benchmark environments
2. Conduct ablation studies comparing code-based generation approach directly against pixel-based world models on text-heavy GUI scenarios
3. Evaluate model performance across different device specifications, screen resolutions, and network conditions to establish robustness in practical deployment scenarios