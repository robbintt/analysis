---
ver: rpa2
title: 'AdaFuse: Adaptive Multimodal Fusion for Lung Cancer Risk Prediction via Reinforcement
  Learning'
arxiv_id: '2602.00347'
source_url: https://arxiv.org/abs/2602.00347
tags:
- fusion
- modality
- adafuse
- modalities
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adaptive multimodal fusion
  in medical diagnosis, specifically for lung cancer risk prediction. The key insight
  is that different patients may require different modality combinations, and existing
  methods that process all modalities uniformly may be suboptimal.
---

# AdaFuse: Adaptive Multimodal Fusion for Lung Cancer Risk Prediction via Reinforcement Learning

## Quick Facts
- arXiv ID: 2602.00347
- Source URL: https://arxiv.org/abs/2602.00347
- Reference count: 13
- Primary result: Achieves 0.762 AUC on NLST lung cancer prediction, outperforming single-modality baselines and other fusion strategies

## Executive Summary
AdaFuse addresses the challenge of adaptive multimodal fusion for lung cancer risk prediction by formulating modality selection as a sequential decision process using reinforcement learning. The method learns to select the optimal combination of CT images, clinical variables, and text reports for each patient, achieving higher accuracy than static fusion approaches while using fewer computational resources. By conditioning each selection on previously observed modalities and terminating early when sufficient information is available, AdaFuse mimics clinical decision-making workflows and filters out less informative data sources.

## Method Summary
AdaFuse formulates adaptive multimodal fusion as a Markov Decision Process where a policy network sequentially selects among 15 pre-trained fusion classifiers covering different modality combinations. The method uses frozen pre-trained classifiers to provide stable reward signals, trains encoders and policy via REINFORTE with mixed BCE+AUC rewards, and employs temperature annealing for exploration. The sequential formulation allows conditioning each selection on previously observed modalities, while the mixed reward addresses class imbalance in lung cancer screening (6% positive rate).

## Key Results
- Achieves 0.762 AUC on NLST dataset, outperforming single-modality baselines (0.732) and fusion methods DynMM (0.754) and MoE (0.742)
- Uses 29% fewer FLOPs than triple-modality methods by selectively excluding less informative modalities
- Ablation studies show mixed BCE+AUC reward outperforms either component alone, and frozen classifiers provide more stable training than unfrozen alternatives

## Why This Works (Mechanism)

### Mechanism 1: Sequential Conditional Selection
Conditioning each modality selection on previously observed modalities improves decisions over parallel selection by allowing the policy to observe acquired information before deciding whether additional modalities are needed. This mirrors clinical workflow where physicians interpret initial results before ordering more tests.

### Mechanism 2: Mixed Reward for Imbalanced Settings
Combining BCE and AUC rewards stabilizes policy learning when positive prevalence is low (6%). BCE provides dense per-sample signal while AUC rewards correct ranking across the batch, addressing the limitations of either metric alone in imbalanced settings.

### Mechanism 3: Frozen Classifiers for Stable Reward Signals
Freezing pre-trained classifiers while training encoders prevents reward instability from shifting decision boundaries. Unfrozen classifiers receive gradients only when selected, causing undertrained combinations to produce unreliable rewards that discourage their future selection—a feedback loop.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) and Policy Gradient**
  - Why needed here: AdaFuse formulates modality selection as an MDP with states, actions, and rewards; REINFORCE is used to optimize the policy.
  - Quick check question: Can you explain why REINFORCE uses $\nabla_\theta \log \pi_\theta(a|s) \cdot (r - \bar{r})$ and what the baseline $\bar{r}$ accomplishes?

- **Concept: Multimodal Fusion Strategies (Concat, Mean, Tensor)**
  - Why needed here: The policy selects among 15 pre-trained classifiers covering different modality-fusion combinations; understanding fusion types is required to interpret the action space.
  - Quick check question: What is the output dimension of tensor fusion for 3 modalities with 32-dim encodings, and why might this cause overfitting with limited data?

- **Concept: Class Imbalance and AUC Optimization**
  - Why needed here: Lung cancer prevalence is ~6%; standard accuracy is misleading; AUC measures ranking quality across thresholds.
  - Quick check question: Why does a model achieving 94% accuracy by always predicting "negative" fail for clinical screening, and how does AUC address this?

## Architecture Onboarding

- **Component map:**
  - Modality encoders (E_A, E_B, E_C) -> 32D shared representation
  - State encoder (g_θ) -> 64D state vector
  - Policy heads -> Action logits
  - Fusion classifiers -> 15 pre-trained models

- **Critical path:**
  1. Pre-train all 15 fusion classifiers independently
  2. Freeze classifiers; initialize policy network and encoders from pre-trained weights
  3. Train policy via REINFORCE with mixed reward (BCE + AUC), entropy regularization, and supervised auxiliary loss
  4. Inference: greedy decoding (τ → 0) for modality selection

- **Design tradeoffs:**
  - Sequential vs. parallel selection: Sequential allows conditioning but adds decision latency; paper shows +0.8% AUC over parallel DynMM
  - Discrete vs. soft selection: Discrete excludes modalities entirely (saves FLOPs); soft selection (MoE) processes all but weights them. AdaFuse uses 29% fewer FLOPs than DynMM
  - Reward complexity: Mixed BCE+AUC outperforms either alone, but requires batch-wise AUC computation

- **Failure signatures:**
  - Policy collapse to single modality: Check entropy regularization weight (λ_ent=0.1); too low causes premature convergence
  - Unstable training: Verify classifiers are frozen; unfrozen classifiers cause noisy rewards
  - Poor generalization on external data: VLSP results show 0.749 AUC vs. 0.771 CT-only; policy may over-rely on clinical features that don't transfer

- **First 3 experiments:**
  1. Reproduce single-modality baselines: Train CT-only, clinical-only, text-only classifiers to verify feature extraction pipeline matches reported AUCs (0.732, 0.662, 0.576)
  2. Ablate reward components: Train with BCE-only, AUC-only, and mixed reward; confirm mixed reward yields highest AUC per Table 3
  3. Analyze policy behavior on held-out set: Run inference with greedy decoding; verify text modality skip rate (~31%) and CT/clinical skip rates (~2.6%) match Appendix Figure 5 patterns

## Open Questions the Paper Calls Out

### Open Question 1
Does the inclusion of authentic, radiologist-authored text reports alter the learned modality selection policy compared to the synthetic text approach used in this study? The current study utilized synthetic text reports derived from structured variables, which resulted in the text modality being largely uninformative (AUC 0.576) and frequently skipped by the policy. It remains unclear if the policy would learn to leverage complex narrative data if it provided genuine complementary value.

### Open Question 2
For which specific patient subgroups or clinical phenotypes does the adaptive integration of clinical variables provide the most significant marginal benefit over imaging alone? While the paper demonstrates aggregate improvements and the general tendency to select CT-based combinations, it does not characterize the specific clinical profiles that trigger the policy to request supplementary modalities versus relying on a single modality.

### Open Question 3
How can the reinforcement learning policy be constrained or trained to ensure it consistently outperforms the best single-modality baseline during external validation? In the external VLSP validation, AdaFuse achieved an AUC of 0.749, which was lower than the CT-only baseline (0.771). This suggests the policy failed to sufficiently filter the clinical modality (AUC 0.471) despite its low quality in that cohort.

## Limitations
- Superior performance relies on intra-study comparisons rather than external validation against competing methods
- Sequential conditioning advantage lacks external validation studies beyond this work
- Claims about adaptive modality selection being clinically meaningful beyond statistical improvement

## Confidence
- **High confidence:** Core RL framework (MDP formulation, REINFORCE optimization) and observed benefit of frozen classifiers
- **Medium confidence:** Sequential vs. parallel selection advantage (+0.8% AUC) and mixed BCE+AUC reward design
- **Low confidence:** Claims about adaptive modality selection being clinically meaningful and 29% FLOPs reduction assumptions

## Next Checks
1. **External benchmarking:** Replicate AdaFuse and DynMM on a shared dataset (e.g., MIMIC-CXR) to verify the +0.8% AUC sequential advantage holds externally
2. **Ablation on classifier freezing:** Systematically test training classifiers during RL vs. freezing across different reward structures to confirm the feedback loop mechanism
3. **Transferability analysis:** Evaluate AdaFuse's policy on an external dataset (e.g., VLSP) with the same modality setup to determine if the learned selection strategy generalizes or overfits to NLST-specific patterns