---
ver: rpa2
title: Task Addition and Weight Disentanglement in Closed-Vocabulary Models
arxiv_id: '2511.14569'
source_url: https://arxiv.org/abs/2511.14569
tags:
- task
- addition
- weight
- disentanglement
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the applicability of task arithmetic, a
  technique for editing pre-trained models, to closed-vocabulary models. Unlike open-vocabulary
  models like CLIP, closed-vocabulary models lack language supervision and require
  task-specific classification heads.
---

# Task Addition and Weight Disentanglement in Closed-Vocabulary Models

## Quick Facts
- arXiv ID: 2511.14569
- Source URL: https://arxiv.org/abs/2511.14569
- Reference count: 40
- This paper extends task arithmetic to closed-vocabulary models, showing that weight disentanglement emerges across different pre-training schemes and enabling efficient multi-task learning through small-scale weight perturbations.

## Executive Summary
This paper investigates whether task arithmetic—a technique for editing pre-trained models by adding task vectors—can be applied to closed-vocabulary models like vision transformers. Unlike open-vocabulary models such as CLIP, closed-vocabulary models lack language supervision and require task-specific classification heads. The authors introduce a two-stage fine-tuning process (linear probing followed by encoder fine-tuning) and apply task arithmetic across various pre-training schemes including supervised, self-supervised (MAE, DINO), and contrastive (CLIP) methods. They find that weight disentanglement, crucial for task arithmetic, is a general property of pre-training that appears across different closed-vocabulary models. Task addition performance is high across most pre-training schemes, with CLIP models achieving the best results, though linear probing alone achieves competitive performance, making it a cost-effective alternative.

## Method Summary
The method involves two-stage fine-tuning for each task: first performing linear probing (freezing the encoder while training a randomly initialized classification head), then fine-tuning the encoder with the frozen, probed head. Task vectors are computed as the difference between fine-tuned and pre-trained weights, and new multi-task models are created by adding scaled task vectors to pre-trained weights. The scaling coefficient λ is optimized via line search. This approach addresses the alignment challenges specific to closed-vocabulary models, where random head initialization can create misalignment between the head and encoder features during fine-tuning.

## Key Results
- Weight disentanglement—enabling task arithmetic—emerges across different pre-training schemes including supervised, MAE, DINO, and CLIP
- CLIP models achieve the highest task addition performance (~95% normalized accuracy), followed by supervised models (~89%), with MAE performing worst (~73%)
- Linear probing alone achieves competitive performance to task addition (within 2-3%), making it a cost-effective alternative that avoids task vector storage
- Full simultaneous fine-tuning of encoder and head dramatically reduces task addition accuracy (40-50% absolute) compared to the aligned two-stage approach (80-90%)

## Why This Works (Mechanism)

### Mechanism 1: Weight Disentanglement as Pre-training Emergent Property
Pre-training organizes the weight space such that perturbations along different task vector directions remain relatively independent, enabling linear composition of task-specific knowledge. This disentanglement emerges regardless of supervision type and is preserved during fine-tuning. The property appears across all tested pre-training schemes, though self-supervised methods like MAE show weaker disentanglement.

### Mechanism 2: Head-Encoder Alignment via Two-Stage Fine-tuning
Random head initialization creates misalignment between the head and encoder representations. The probing phase first aligns the head with frozen encoder features, then encoder fine-tuning makes incremental adjustments rather than destructive changes. This alignment is critical—full fine-tuning destroys task arithmetic benefits by 20-40 percentage points.

### Mechanism 3: Small-Magnitude Task Vector Addition Preserves Pre-training
Optimal scaling coefficients (λ = 0.05-0.15) indicate that task addition success derives primarily from pre-trained representations with minimal encoder modification. Pre-trained encoders already encode transferable features; task vectors add small directional adjustments that leverage disentangled subspaces without disrupting shared representations.

## Foundational Learning

- **Task Vectors and Task Arithmetic**
  - Why needed: The entire method builds on computing τ_t = θ_ft - θ_pre and combining vectors via θ_new = θ_pre + Σλ_t τ_t
  - Quick check: Can you explain why subtracting pre-trained weights from fine-tuned weights isolates task-specific knowledge?

- **Linear Probing vs. Fine-tuning**
  - Why needed: The two-stage process requires understanding when freezing the encoder is beneficial versus when joint optimization fails
  - Quick check: If probing achieves 82% of fine-tuning performance, what does this imply about the pre-trained encoder's feature quality?

- **Closed- vs. Open-Vocabulary Models**
  - Why needed: Closed models require task-specific heads; this constraint motivates the alignment phase and limits transfer flexibility
  - Quick check: Why can't closed-vocabulary models use zero-shot classification like CLIP with text embeddings?

## Architecture Onboarding

- **Component map**: Pre-trained Encoder (frozen during probing) → Random Head → [Linear Probe] → Aligned Head (frozen during encoder FT) → [Encoder Fine-tune] → Fine-tuned Encoder → Task Vector Extraction: τ = θ_ft - θ_pre → Multi-task Model: θ_new = θ_pre + λ₁τ₁ + λ₂τ₂ + ... → Task-specific heads plugged in per inference task

- **Critical path**: 
  1. Start with pre-trained ViT checkpoint (Supervised/MAE/DINO/CLIP encoder only)
  2. For each task: linear probe head (2000 iterations, AdamW, LR from {1e-3 to 3e-1})
  3. For each task: fine-tune encoder with frozen probed head (LR from {1e-6 to 1e-1})
  4. Extract task vectors, search λ ∈ {0, 0.05, ..., 1.0} to maximize normalized accuracy
  5. Deploy: load θ_pre + Σλ_t τ_t, attach task-specific head for inference

- **Design tradeoffs**:
  - Probing-only: ~2-3% accuracy loss, but 0% encoder training cost and no task vector storage
  - CLIP vs. Supervised: CLIP achieves ~95% normalized accuracy; Supervised (IN21k) achieves ~89%; MAE achieves ~73%
  - Model scale: ViT-L gains ~2-3% over ViT-B but triples parameter count
  - Task count: Performance degrades linearly with more tasks; expect 10-15% drop from 2 to 8 tasks

- **Failure signatures**:
  - Full fine-tuning regime: 40-50% absolute accuracy (vs. 80-90% aligned) indicates head-encoder misalignment
  - MAE pre-training: 73% normalized accuracy with λ = 0.05 suggests weak disentanglement
  - Large λ values (>0.5): Monotonic accuracy decline indicates over-modification of pre-trained weights
  - DINO with any λ > 0: Performance decreases, suggesting encoder changes are harmful for this scheme

- **First 3 experiments**:
  1. Validate alignment necessity: Compare three regimes on 2 tasks—(a) probing only, (b) aligned fine-tuning, (c) full fine-tuning. Expect (b) > (a) > (c) with ~20-40% gaps.
  2. Measure disentanglement per pre-training scheme: Compute pairwise disentanglement error ξ(λ₁, λ₂) for Supervised vs. MAE vs. DINO. Expect CLIP < Supervised < Self-supervised in error magnitude.
  3. Ablate task count scaling: Add tasks incrementally (2, 4, 6, 8) and plot normalized accuracy decay. Expect linear degradation; identify when performance drops below acceptable threshold (e.g., 85%).

## Open Questions the Paper Calls Out

- **Cross-architecture task arithmetic**: Can task arithmetic be applied to closed-vocabulary models that share a backbone but differ in their final layers (e.g., merging an image classification model with a segmentation model)? The current study focuses exclusively on image classification tasks where heads can be easily swapped or aligned.

- **Disentanglement emergence mechanisms**: What are the precise mechanisms and dynamics governing the emergence of weight disentanglement during small-scale pre-training? While the paper establishes that disentanglement occurs in various pre-trained models, it relies on existing checkpoints and does not analyze how training dynamics or data properties cause this property to emerge.

- **Task negation effectiveness**: Is task negation (model editing via subtraction) effective for closed-vocabulary models, or is the utility of task arithmetic limited to addition in this setting? The paper restricts its methodology to "task addition" despite defining task arithmetic broadly in the Background, leaving the viability of other arithmetic operations untested.

## Limitations

- Task arithmetic performance degrades linearly with increasing task count, limiting practical scalability for many-task scenarios
- The method shows poor performance on self-supervised models like DINO, suggesting incompatibility with certain pre-training representations
- The two-stage fine-tuning process requires careful hyperparameter tuning per task, which may limit practical efficiency gains

## Confidence

- **High Confidence**: Weight disentanglement exists across pre-training schemes (supported by quantitative heatmaps) and CLIP achieves superior task addition performance compared to other methods
- **Medium Confidence**: Linear probing achieves competitive performance to task addition (valid for current task set, but extrapolation to more diverse tasks is uncertain)
- **Low Confidence**: The assertion that task arithmetic "efficiently enables multi-task learning" in diverse settings overstates the evidence—results show linear degradation with task count and poor performance on self-supervised models

## Next Checks

1. **Disentanglement mechanism validation**: Measure task vector orthogonality and subspace overlap for each pre-training scheme using Singular Value Decomposition to verify whether claimed linear subspace independence actually holds at the representation level.

2. **Cross-task generalization stress test**: Evaluate task addition when combining tasks from different modalities (e.g., object classification + fine-grained scene recognition) to test whether the method truly generalizes beyond the similar image classification tasks used here.

3. **Cost-benefit analysis of two-stage fine-tuning**: Measure total training time and parameter changes across the full pipeline (probing + encoder FT + task addition) versus simple full fine-tuning of all tasks, to quantify claimed efficiency gains empirically.