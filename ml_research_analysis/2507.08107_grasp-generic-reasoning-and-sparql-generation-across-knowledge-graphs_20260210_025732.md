---
ver: rpa2
title: 'GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs'
arxiv_id: '2507.08107'
source_url: https://arxiv.org/abs/2507.08107
tags:
- knowledge
- dblp
- sparql
- query
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRASP is a zero-shot approach for generating SPARQL queries from
  natural language questions over arbitrary RDF knowledge graphs. It uses a large
  language model to explore the graph by executing SPARQL queries and searching for
  relevant IRIs and literals through a fixed set of functions.
---

# GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs

## Quick Facts
- arXiv ID: 2507.08107
- Source URL: https://arxiv.org/abs/2507.08107
- Reference count: 40
- Primary result: Zero-shot SPARQL query generation with 72.5% F1 on QALD-10 Wikidata benchmark

## Executive Summary
GRASP is a zero-shot approach for generating SPARQL queries from natural language questions over arbitrary RDF knowledge graphs. It leverages a large language model to explore the graph by executing SPARQL queries and searching for relevant IRIs and literals through a fixed set of functions. The method achieves state-of-the-art performance on Wikidata benchmarks without requiring fine-tuning or few-shot examples, demonstrating strong generalization across multiple knowledge graphs including DBLP, DBpedia, and ORKG.

## Method Summary
GRASP uses a large language model to explore RDF knowledge graphs by executing SPARQL queries and searching for relevant entities through a fixed set of functions. The approach operates in a zero-shot manner, requiring no fine-tuning or few-shot examples. During exploration, the LLM generates and executes SPARQL queries to discover relevant IRIs and literals, then constructs the final SPARQL query to answer the natural language question. Performance can be further improved with feedback and few-shot examples, though the core method works without them.

## Key Results
- Achieves 72.5% F1 score on QALD-10 Wikidata benchmark
- Achieves 79.4% F1 score on QALD-7 Wikidata benchmark
- Competitively performs on Freebase benchmarks and other knowledge graphs (DBLP, DBpedia, ORKG)
- Open-source models like Qwen2.5 72B also perform competitively

## Why This Works (Mechanism)
The approach works by using the LLM's reasoning capabilities to systematically explore the knowledge graph through query execution and function-based searches. The fixed set of exploration functions guides the LLM to discover relevant entities and relationships without requiring task-specific fine-tuning. The method leverages the LLM's ability to understand natural language questions and map them to semantic structures in the RDF graph through iterative exploration and refinement.

## Foundational Learning
- RDF and SPARQL fundamentals: Understanding the data model and query language is essential for interpreting how GRASP interacts with knowledge graphs. Quick check: Can you write a basic SPARQL query to retrieve all subjects of a given predicate?
- Zero-shot learning concepts: Understanding how models can perform tasks without task-specific training data. Quick check: What distinguishes zero-shot from few-shot learning in practical terms?
- Large language model prompting techniques: Understanding how to structure prompts for effective task completion. Quick check: What are the key differences between zero-shot and few-shot prompting?

## Architecture Onboarding
- Component map: Natural Language Question -> LLM Prompt -> SPARQL Query Generation -> Graph Exploration (Query Execution + Function Calls) -> Final SPARQL Query
- Critical path: Question understanding -> Graph exploration -> Query construction -> Answer extraction
- Design tradeoffs: Zero-shot operation (generality) vs. potential performance gains from fine-tuning; computational cost of iterative query execution vs. accuracy; fixed exploration functions (simplicity) vs. adaptability to diverse query types
- Failure signatures: Incorrect entity linking, failure to discover relevant graph paths, malformed SPARQL syntax, over-reliance on surface text matching rather than semantic understanding
- First experiments: 1) Test on simple single-hop queries to verify basic functionality, 2) Evaluate on multi-hop queries to assess reasoning capabilities, 3) Compare performance across different knowledge graphs to test generalization

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Computational efficiency concerns due to iterative SPARQL query execution during exploration
- Fixed set of exploration functions may limit handling of complex or uncommon query patterns
- Performance may not generalize well to knowledge graphs with different schemas or data quality levels
- Potential benchmark-specific optimizations that may not transfer to real-world scenarios

## Confidence
- State-of-the-art claims on Wikidata: Medium (limited comparison with recent methods, potential benchmark-specific optimizations)
- Generalization across knowledge graphs: Medium (success on Wikidata may not extend to all RDF datasets)
- Computational scalability: Low (exploration phase query execution costs not fully characterized)

## Next Checks
1. Test GRASP on additional knowledge graphs with varying schemas and data quality to assess generalization
2. Measure the computational cost and query execution time for complex multi-hop queries
3. Compare GRASP's performance against established semantic parsing approaches using the same evaluation framework and dataset splits