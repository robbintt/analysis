---
ver: rpa2
title: Multivariate Uncertainty Quantification with Tomographic Quantile Forests
arxiv_id: '2512.16383'
source_url: https://arxiv.org/abs/2512.16383
tags:
- uni00000013
- uni00000011
- uni00000014
- uni00000015
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Tomographic Quantile Forests (TQF), a nonparametric
  tree-based regression method for uncertainty quantification in multivariate tabular
  data. TQF learns conditional quantiles of directional projections as functions of
  the input and direction, then reconstructs the multivariate conditional distribution
  by minimizing the sliced Wasserstein distance.
---

# Multivariate Uncertainty Quantification with Tomographic Quantile Forests

## Quick Facts
- arXiv ID: 2512.16383
- Source URL: https://arxiv.org/abs/2512.16383
- Authors: Takuya Kanazawa
- Reference count: 22
- Primary result: TQF achieves strong performance on multivariate UQ, outperforming both parametric (GMM) and non-parametric (DRF) baselines, especially in small-data regimes

## Executive Summary
This paper proposes Tomographic Quantile Forests (TQF), a nonparametric tree-based regression method for uncertainty quantification in multivariate tabular data. TQF learns conditional quantiles of directional projections as functions of the input and direction, then reconstructs the multivariate conditional distribution by minimizing the sliced Wasserstein distance. Unlike prior approaches that require separate models for each direction or produce only convex quantile regions, TQF covers all directions with a single model without convexity restrictions. Experiments on synthetic and real-world datasets show TQF achieves strong performance, often outperforming both parametric methods like Gaussian mixture models and non-parametric methods like Distributional Random Forests, especially in small-data regimes.

## Method Summary
TQF transforms multivariate distribution estimation into an ensemble of univariate quantile regression problems by projecting the target onto random 1D lines (directions). The method uses QRF++ - a modified Random Forest that augments the input with direction vectors and random rotations, and the target with random Fourier features - to learn conditional quantiles of these projections. At inference, the Quantile-Matching Empirical Measure (QMEM) algorithm reconstructs the joint distribution by generating a point cloud that minimizes the sliced Wasserstein distance between its own projected quantiles and the model's predicted quantiles. This approach bypasses the curse of dimensionality and avoids convexity restrictions inherent in previous methods.

## Key Results
- On a challenging synthetic dataset, TQF attains Energy Distance scores of 0.055-0.065, compared to 0.058-0.110 for GMM3 and 0.127-0.638 for GMM1
- TQF outperforms Distributional Random Forests (DRF) in small-data regimes by generating new support points rather than just reweighting training data
- The method successfully captures complex distribution geometries including non-convex regions and holes in synthetic benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Tomographic Decomposition
TQF avoids the curse of dimensionality by transforming multivariate distribution estimation into an ensemble of univariate quantile regression problems. It projects the d-dimensional target onto random 1D lines and learns quantiles of the scalar projection conditioned on the input and direction. This relies on the Cramér-Wold theorem, which guarantees that a distribution is uniquely determined by its 1D projections.

### Mechanism 2: QRF++ (Distributional Splitting)
Standard regression trees fail to detect distributional shifts if the mean is constant. TQF uses QRF++, which augments the input with direction vectors and random rotations, and the target with random Fourier features (sin/cos). This forces the tree splits to be sensitive to higher-order moments and specific directional variations rather than just the conditional mean.

### Mechanism 3: Distribution Reconstruction (QMEM)
The system reconstructs the joint distribution by generating a point cloud that minimizes the discrepancy between its own projected quantiles and the model's predicted quantiles. At inference, QMEM optimizes the location and weights of a set of points using an alternating scheme: sampling locations from a KDE and optimizing weights via convex optimization to minimize the Sliced Wasserstein Distance.

## Foundational Learning

- **Quantile Regression Forests (QRF)**: Standard Random Forests predict the mean, while QRF retains the target values in leaves to predict the full Conditional Distribution Function (CDF). *Why needed*: TQF is an extension of QRF. *Quick check*: How does QRF estimate the 0.9 quantile for a new input x? (Answer: It aggregates training responses in leaves x falls into, then finds the empirical 0.9 quantile of that weighted set).

- **Radon Transform (Tomography)**: A 2D/3D object can be reconstructed from its 1D "shadows" (projections). *Why needed*: The paper frames multivariate UQ as a tomography problem. *Quick check*: Why does TQF require "projections"? (Answer: To reduce the multivariate estimation problem into a univariate one, bypassing the lack of a natural ordering in higher dimensions).

- **Sliced Wasserstein Distance (SWD)**: Converts the intractable Wasserstein distance in high dimensions into an average of 1D Wasserstein distances. *Why needed*: This is the loss function for the reconstruction phase. *Quick check*: Why "Sliced"? (Answer: Because it integrates the distance over all 1D linear projections/slices of the distributions).

## Architecture Onboarding

- **Component map**: Preprocessing -> Augmentation (Stage I) -> QRF++ Backbone -> QMEM Reconstruction (Stage II)
- **Critical path**: The QMEM reconstruction at inference is the computational bottleneck. While training is standard for a forest, inference requires solving an optimization problem per query.
- **Design tradeoffs**: TQF generates new support points (better for small data) vs. DRF which reweights training data; higher augmentation G improves directional awareness but linearly increases training time.
- **Failure signatures**: "Fuzzy" reconstruction if K or M is too low; quantile crossing (unlikely); edge effects at covariate space boundaries.
- **First 3 experiments**: 1) Run TQF on a univariate dataset to verify it collapses to standard QRF performance; 2) Reconstruct "Two Moons" with K=5, 10, 25 to observe when the two moons separate; 3) Compare TQF vs. DRF on N=50 samples and visualize how TQF generates support "between" data points.

## Open Questions the Paper Calls Out
1. Making the method more robust to hyperparameter choices (G, G̃) to reduce time-consuming manual tuning
2. Extending TQF to quantify epistemic uncertainty in addition to aleatoric uncertainty
3. Applying TQF to complex, large-scale spatiotemporal data such as weather forecasting

## Limitations
- The inference-time computational burden of QMEM reconstruction requires iterative optimization for each prediction
- The method's reliance on a large number of projection directions (K) and quantile levels (M) creates potential scalability concerns
- The exact mechanism by which random Fourier features enable distributional shift detection remains somewhat heuristic

## Confidence
- **High Confidence**: The tomographic decomposition mechanism is mathematically sound and well-supported by the Cramér-Wold theorem and empirical results
- **Medium Confidence**: The QRF++ augmentation effectively detects distributional shifts, but the precise sensitivity to Fourier features is not fully characterized
- **Medium Confidence**: The QMEM reconstruction produces accurate point clouds as shown in synthetic experiments, but sensitivity to hyperparameters and convergence guarantees are not rigorously established

## Next Checks
1. Benchmark TQF inference time against DRF and GMM baselines on datasets of varying dimensions (d=2, 5, 10) to quantify the scalability of the QMEM reconstruction
2. Systematically vary K (projection directions) and M (quantile levels) for a fixed synthetic dataset and plot Energy Distance vs. computational cost to identify optimal tradeoffs
3. Construct a synthetic dataset where the conditional variance changes but the mean is constant. Train TQF and standard QRF, then compare their predicted quantile ranges across the feature space to verify QRF++ sensitivity