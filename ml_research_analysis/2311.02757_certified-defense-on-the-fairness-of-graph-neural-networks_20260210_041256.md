---
ver: rpa2
title: Certified Defense on the Fairness of Graph Neural Networks
arxiv_id: '2311.02757'
source_url: https://arxiv.org/abs/2311.02757
tags:
- certi
- fairness
- budgets
- bias
- elegant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of certifying fairness for Graph
  Neural Networks (GNNs) under adversarial attacks. The authors propose a plug-and-play
  framework, ELEGANT, that provides theoretical guarantees that the fairness level
  of GNN predictions cannot be corrupted beyond a threshold under certain perturbation
  budgets.
---

# Certified Defense on the Fairness of Graph Neural Networks

## Quick Facts
- **arXiv ID:** 2311.02757
- **Source URL:** https://arxiv.org/abs/2311.02757
- **Reference count:** 40
- **Primary result:** Achieves >90% fairness certification rate while maintaining utility on benchmark datasets

## Executive Summary
This paper addresses the challenge of certifying fairness for Graph Neural Networks (GNNs) under adversarial attacks. The authors propose a plug-and-play framework, ELEGANT, that provides theoretical guarantees that the fairness level of GNN predictions cannot be corrupted beyond a threshold under certain perturbation budgets. The method leverages randomized smoothing with both Gaussian and Bernoulli noise to defend against attacks on node attributes and graph structure simultaneously. Experiments on real-world datasets demonstrate that ELEGANT achieves high fairness certification rates (around or above 90%), maintains comparable utility with vanilla GNNs, and often improves fairness even without being explicitly designed for debiasing. The approach is generalizable across different GNN backbones and does not require re-training or assumptions about model structure.

## Method Summary
ELEGANT provides certified fairness guarantees for GNNs through a randomized smoothing framework. The method injects Gaussian noise into node attributes and Bernoulli noise into graph structure, then uses Monte Carlo sampling to estimate the probability that predictions remain fair under perturbations. A Beta distribution is used to derive certified radii for both attribute and structure perturbations. The framework operates as a post-processing step on pre-trained GNNs, requiring no retraining. The approach is evaluated across three real-world datasets (German Credit, Recidivism, Credit Defaulter) using standard fairness metrics like ΔSP and ΔEO, comparing against both vanilla GNNs and fairness-aware debiasing methods.

## Key Results
- Achieves fairness certification rates of 92.8% on German Credit, 91.8% on Recidivism, and 93.1% on Credit Defaulter datasets
- Maintains comparable node classification accuracy with vanilla GNNs (e.g., 72.5% vs 73.0% on German Credit)
- Often improves fairness metrics even without explicit debiasing design, with ΔSP improvements of 3-8% over baselines
- Demonstrates robustness against both attribute and structure perturbations, with certified radii computed for practical perturbation budgets

## Why This Works (Mechanism)
The framework leverages randomized smoothing theory to provide certified fairness guarantees. By injecting carefully calibrated noise and using concentration inequalities, it establishes probabilistic bounds on how much an adversary can manipulate predictions before fairness constraints are violated. The dual noise injection (Gaussian for attributes, Bernoulli for structure) allows simultaneous certification against different types of attacks. The Beta distribution-based certification provides tight bounds that are computationally efficient to compute.

## Foundational Learning
- **Randomized Smoothing:** A technique that adds noise to inputs to create provably robust predictions
  - Why needed: Provides the theoretical foundation for certified defense against adversarial perturbations
  - Quick check: Verify that adding noise and taking majority vote creates smoother decision boundaries

- **Fairness Metrics (ΔSP, ΔEO):** Statistical measures quantifying group fairness disparities in predictions
  - Why needed: Define the fairness constraints that the certification protects against
  - Quick check: Ensure metric calculations match standard definitions for binary classification

- **Beta Distribution Concentration:** Used to derive tight probabilistic bounds from empirical estimates
  - Why needed: Enables computation of certified radii from finite Monte Carlo samples
  - Quick check: Validate that the inverse CDF calculations produce reasonable bounds for small sample sizes

## Architecture Onboarding

**Component Map:** Graph Data -> Noise Injection -> Frozen GNN -> Bias Indicator -> Monte Carlo Estimation -> Beta Bounds -> Certified Radii

**Critical Path:** Noise generation → perturbed forward pass → bias evaluation → probability estimation → certification calculation

**Design Tradeoffs:** 
- Noise levels vs. certification tightness: Higher noise provides better guarantees but may hurt utility
- Sample size vs. computation: More samples improve bound accuracy but increase runtime
- Subset selection vs. coverage: Smaller vulnerable sets improve certification but may miss broader fairness issues

**Failure Signatures:**
- Zero certification rate indicates threshold too strict or noise too low
- Accuracy collapse suggests noise levels destroying signal
- Inconsistent bounds point to implementation errors in Beta distribution calculations

**First Experiments:**
1. Verify certification on synthetic graph with known vulnerability distribution
2. Test sensitivity to noise parameters (σ, β) across wider ranges
3. Benchmark against simple debiasing baseline to quantify certification value

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the certification framework be extended to ensure fairness in graph learning tasks beyond node classification, such as link prediction or graph classification?
  - Basis in paper: Conclusion explicitly states this as future work
  - Why unresolved: Current metrics and theory are formulated specifically for node-level predictions

- **Open Question 2:** Does reversing the order of randomized smoothing (smoothing graph topology before node attributes) result in tighter certification bounds?
  - Basis in paper: Appendix B.4 asks this question and notes it's feasible but costlier
  - Why unresolved: Authors analyzed proposed order for efficiency but didn't explore reversed order's theoretical benefits

- **Open Question 3:** Can theoretical certification bounds be tightened to better match observed empirical robustness where models remain fair beyond certified perturbation radius?
  - Basis in paper: Figure 2 shows models maintaining fairness beyond certified budgets
  - Why unresolved: Current bounds appear conservative compared to empirical performance

## Limitations
- Certification focuses on subset of vulnerable nodes rather than overall graph fairness
- Theoretical bounds may be conservative compared to empirical robustness
- Limited comparison with state-of-the-art fairness improvement techniques

## Confidence
- **High Confidence:** Theoretical framework and Monte Carlo methodology are mathematically rigorous and standard
- **Medium Confidence:** Hyperparameter choices are justified but may require tuning for new datasets
- **Low Confidence:** Limited benchmarking against explicit fairness debiasing baselines

## Next Checks
1. Replicate certification on synthetic graph with known vulnerability distribution to validate lower bound estimation
2. Test sensitivity to hyperparameters by varying σ and β over wider ranges to identify breaking points
3. Benchmark against a debiasing baseline (e.g., adversarial debiasing) to quantify trade-off between certified fairness and actual bias reduction