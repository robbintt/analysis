---
ver: rpa2
title: Learning Dynamics of Deep Learning -- Force Analysis of Deep Neural Networks
arxiv_id: '2509.19554'
source_url: https://arxiv.org/abs/2509.19554
tags:
- learning
- training
- more
- which
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis investigates deep learning systems through a physics-inspired
  lens, focusing on how one training example influences another during learning. The
  central framework, called learning dynamics or force analysis, uses an AKG decomposition
  to break down this influence into three interpretable components: similarity (K),
  normalization (A), and prediction gap (G).'
---

# Learning Dynamics of Deep Learning -- Force Analysis of Deep Neural Networks

## Quick Facts
- arXiv ID: 2509.19554
- Source URL: https://arxiv.org/abs/2509.19554
- Reference count: 0
- One-line primary result: A physics-inspired "learning dynamics" framework decomposes how training examples influence each other through similarity, normalization, and prediction gap components.

## Executive Summary
This thesis introduces a physics-inspired framework called learning dynamics or force analysis to understand how one training example influences another during deep learning. The central AKG decomposition breaks down this mutual influence into three interpretable components: similarity (K), normalization (A), and prediction gap (G). This approach treats learning as a series of small, local interactions analogous to forces shaping motion. The framework is applied across supervised classification, large language model finetuning, transfer learning, and simplicity bias analysis, providing interpretable explanations for various learning phenomena and inspiring novel methods like Filter-KD for knowledge distillation under noisy labels.

## Method Summary
The learning dynamics framework treats deep learning training as a sequence of small, local interactions between training examples, analogous to forces in physics. The AKG decomposition mathematically captures how example A influences example B's learning by breaking down the gradient of B's loss with respect to A's representation into three components: similarity (K) measures how similarly the model represents examples A and B, normalization (A) captures the effect of the softmax denominator, and prediction gap (G) represents the difference between current and target predictions for example B. This framework is applied across various learning scenarios including supervised classification with cross-entropy loss, large language model finetuning with different objective functions, and transfer learning contexts, providing a unified lens for understanding mutual influences during training.

## Key Results
- Non-trivial learning trajectories like "zig-zag" paths in hard examples are explained through the AKG decomposition
- Filter-KD knowledge distillation method improves supervision under noisy labels by extracting better soft targets during training
- Negative gradients in LLM finetuning can cause unintended global effects like probability mass concentration ("squeezing effect")
- Simpler compositional mappings are learned faster due to more coherent mutual influences between training examples

## Why This Works (Mechanism)
The framework works by decomposing the gradient of one example's loss with respect to another example's representation into interpretable components. The similarity term captures how similarly the model represents examples, the normalization term accounts for the softmax denominator's effect on relative logits, and the prediction gap term measures the difference between current and target predictions. This decomposition reveals how training examples mutually influence each other's learning trajectories through these three mechanisms. The framework's power lies in treating learning as a series of small, local interactions rather than a monolithic optimization process, making it possible to track and interpret the dynamics of individual example pairs throughout training.

## Foundational Learning
- **Cross-entropy loss**: The standard classification loss function that measures the difference between predicted and true probability distributions. Why needed: Provides the objective that training examples optimize against, and its gradient structure is what the AKG decomposition analyzes.
- **Gradient-based optimization**: The process of updating model parameters using gradients of the loss function. Why needed: The framework analyzes how gradients from one example affect the learning of another example.
- **Softmax function**: Converts raw logits into probabilities that sum to one. Why needed: The normalization term in AKG specifically captures the effect of the softmax denominator on mutual influences.
- **Mutual influence**: How the learning of one training example affects the learning of another. Why needed: The core concept the framework analyzes - understanding how examples "push" each other during training.
- **Hidden representations**: The intermediate feature vectors that neural networks learn. Why needed: The framework tracks how these representations evolve and influence each other during training.
- **Learning speed**: The rate at which examples converge to correct predictions. Why needed: The framework measures and compares learning speeds across different examples and tasks.

## Architecture Onboarding
- **Component map**: Input examples → Representation layer → Loss computation → Gradient computation → AKG decomposition → Mutual influence analysis
- **Critical path**: The path from input examples through the model to loss computation is critical, as the AKG framework analyzes gradients flowing backward through this path
- **Design tradeoffs**: The framework trades computational complexity (tracking influences between all example pairs) for interpretability and insight into learning dynamics
- **Failure signatures**: When mutual influences become incoherent or contradictory, learning can slow down or exhibit pathological behaviors like the squeezing effect
- **First experiments**:
  1. Apply AKG decomposition to a simple binary classification problem to verify the three components capture intuitive influences
  2. Track AKG values for individual examples during training to observe how mutual influences evolve
  3. Compare AKG-based explanations with actual learning trajectories to validate the framework's predictive power

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's empirical success is demonstrated primarily on standard benchmark tasks and architectures (ResNet-50, BERT, MLP-Mixer), with uncertain generalization to more complex scenarios
- The exact quantitative relationship between AKG components and final model performance metrics requires further investigation
- The framework remains limited by the gap between confidence changes (captured by AKG) and actual capabilities

## Confidence
- **High Confidence**: AKG decomposition mathematically captures confidence changes and the three-component breakdown is theoretically sound; observed phenomena like zig-zag learning trajectories are empirically validated
- **Medium Confidence**: Simpler compositional mappings are learned faster due to more coherent mutual influences; Filter-KD method's improved performance under noisy labels is demonstrated
- **Low Confidence**: Negative gradients have an often-underestimated role in shaping model behavior lacks quantitative measures of their relative importance

## Next Checks
1. Apply the AKG framework to analyze learning dynamics in Vision Transformers, Large Language Models (beyond finetuning), and multi-modal architectures to verify if the three-component decomposition remains interpretable and useful across these settings

2. Extend the AKG analysis to pre-training scenarios where feature learning is the primary objective, investigating whether the framework can capture the emergence of semantic representations in self-supervised learning contexts

3. Design experiments to quantify the relationship between confidence changes (captured by AKG) and actual downstream task performance/capability improvements, addressing the acknowledged gap between confidence changes and real capabilities