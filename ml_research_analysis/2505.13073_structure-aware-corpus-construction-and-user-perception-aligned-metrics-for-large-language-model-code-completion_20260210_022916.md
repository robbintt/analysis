---
ver: rpa2
title: Structure-Aware Corpus Construction and User-Perception-Aligned Metrics for
  Large-Language-Model Code Completion
arxiv_id: '2505.13073'
source_url: https://arxiv.org/abs/2505.13073
tags:
- code
- metrics
- completion
- semantic
- adoption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes two user-aligned evaluation metrics\u2014\
  LCP and ROUGE-LCP\u2014for code completion tasks, derived from probabilistic modeling\
  \ of LLM output distributions. Through theoretical analysis and empirical validation\
  \ on 10,769 user completion samples, the metrics show strong correlation with user\
  \ adoption behavior (Pearson r 0.7, p < 0.05), outperforming traditional metrics\
  \ like EM and ROUGE-L."
---

# Structure-Aware Corpus Construction and User-Perception-Aligned Metrics for Large-Language-Model Code Completion

## Quick Facts
- arXiv ID: 2505.13073
- Source URL: https://arxiv.org/abs/2505.13073
- Reference count: 28
- Key outcome: Proposed LCP and ROUGE-LCP metrics show strong correlation with user adoption behavior (Pearson r > 0.7, p < 0.05), outperforming traditional metrics, while SPSR-Graph methodology improves LCP by up to 18.55% and BLEU by 2.74 points

## Executive Summary
This paper addresses two critical challenges in code completion systems: the lack of user-aligned evaluation metrics and the absence of structural awareness in large language models. The authors propose two novel metrics (LCP and ROUGE-LCP) derived from probabilistic modeling of LLM output distributions that demonstrate strong correlation with actual user adoption behavior. They also introduce a Structure-Preserving and Semantically-Reordered Code Graph (SPSR-Graph) method for pretraining corpus construction that explicitly models cross-file and cross-module dependencies, leading to significant performance improvements in C/C++ code completion tasks.

## Method Summary
The authors develop two user-aligned evaluation metrics through theoretical analysis of LLM output distributions, then validate their effectiveness on 10,769 user completion samples. They introduce the SPSR-Graph methodology to construct pretraining corpora that preserve structural dependencies between code files and modules while maintaining semantic relationships. The approach involves reordering code based on structural and semantic considerations during corpus construction, followed by evaluation using both traditional metrics (EM, ROUGE-L) and the proposed LCP/ROUGE-LCP metrics. Experimental validation demonstrates consistent performance gains across multiple C/C++ code completion tasks.

## Key Results
- LCP and ROUGE-LCP metrics show strong correlation with user adoption behavior (Pearson r > 0.7, p < 0.05)
- Outperform traditional metrics like EM and ROUGE-L in user-aligned evaluation
- Structural-aware pretraining corpora increase LCP values by up to 18.55% and BLEU scores by 2.74 points
- Consistent performance gains observed across multiple C/C++ code completion tasks

## Why This Works (Mechanism)
The proposed metrics work by aligning evaluation with actual user behavior patterns through probabilistic modeling of LLM output distributions. The structural awareness mechanism improves performance by explicitly modeling cross-file and cross-module dependencies during pretraining, allowing the model to better understand code organization and relationships that are critical for accurate completion suggestions.

## Foundational Learning

**Probabilistic modeling of LLM output distributions** - Needed to derive user-aligned metrics that capture actual adoption behavior rather than syntactic similarity alone. Quick check: Verify that metric formulation properly accounts for output probability distributions.

**Code graph representation** - Required to model structural dependencies between code files and modules. Quick check: Ensure graph construction captures both syntactic and semantic relationships accurately.

**Cross-file dependency modeling** - Essential for understanding how code components interact across different modules. Quick check: Validate that dependency edges reflect actual code usage patterns.

**Semantic reordering techniques** - Needed to preserve meaningful relationships while optimizing for training efficiency. Quick check: Confirm that reordering maintains logical code flow and dependencies.

**User behavior analysis** - Critical for validating that metrics align with actual developer preferences. Quick check: Ensure sample size and diversity adequately represent target user population.

## Architecture Onboarding

Component map: Corpus Construction -> SPSR-Graph Generation -> Model Pretraining -> Code Completion Inference -> User Adoption Tracking

Critical path: SPSR-Graph Generation -> Model Pretraining -> Code Completion Inference -> User Adoption Tracking

Design tradeoffs: Structural awareness vs. computational overhead, metric alignment vs. implementation complexity, corpus diversity vs. structural preservation

Failure signatures: Poor LCP scores indicate user misalignment; low BLEU suggests generation quality issues; structural awareness gaps manifest as cross-module completion errors

First experiments:
1. Validate metric correlation with user adoption using held-out samples
2. Compare structural-aware vs. traditional pretraining performance on simple code completion tasks
3. Measure computational overhead of SPSR-Graph generation on different corpus sizes

## Open Questions the Paper Calls Out

None specified in the source material.

## Limitations

The validation set of 10,769 user completion samples may not represent broader programming contexts or diverse developer populations. Performance improvements were primarily evaluated on C/C++ tasks, limiting generalizability to other programming languages with different structural characteristics. The computational overhead and scalability implications for large-scale pretraining with SPSR-Graph methodology remain unexplored.

## Confidence

Theoretical foundation connecting probabilistic modeling to user perception alignment: High confidence
Empirical validation across diverse programming languages: Medium confidence
Computational scalability of structural-aware pretraining: Low confidence

## Next Checks

1. Evaluate metric performance and structural improvements across a broader range of programming languages beyond C/C++
2. Conduct A/B testing with real developers to validate that LCP score improvements translate to measurable productivity gains
3. Perform ablation studies to quantify the specific contributions of structural awareness versus other factors in the observed performance improvements