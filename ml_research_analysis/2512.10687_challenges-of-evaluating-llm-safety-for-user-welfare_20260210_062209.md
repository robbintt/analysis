---
ver: rpa2
title: Challenges of Evaluating LLM Safety for User Welfare
arxiv_id: '2512.10687'
source_url: https://arxiv.org/abs/2512.10687
tags:
- user
- safety
- context
- advice
- health
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated how user context affects LLM safety evaluations
  for personal advice. The research found that safety scores drop significantly when
  evaluators are provided with user vulnerability information, with high-vulnerability
  users seeing scores fall from safe (5/7) to somewhat unsafe (3/7).
---

# Challenges of Evaluating LLM Safety for User Welfare

## Quick Facts
- arXiv ID: 2512.10687
- Source URL: https://arxiv.org/abs/2512.10687
- Reference count: 40
- User context significantly impacts LLM safety evaluations for personal advice, with vulnerability information reducing safety scores from 5/7 to 3/7

## Executive Summary
This study reveals a critical blind spot in LLM safety evaluation: context-blind assessments systematically overestimate safety for vulnerable users. The research demonstrates that evaluators unaware of user circumstances rate identical LLM responses significantly safer than those who have full context, with high-vulnerability users experiencing a 2-point drop on a 7-point safety scale. Contrary to expectations, simply enriching prompts with disclosed context factors only partially closes this gap, indicating that effective safety evaluation requires evaluators to assess responses against diverse user profiles rather than relying on realistic context disclosure alone.

## Method Summary
The study employed an LLM-as-judge evaluation framework using GPT-o3 to assess safety across four themes (finance and health domains) with three models (GPT-5, Claude Sonnet 4, Gemini 2.5 Pro). Researchers collected Reddit-inspired prompts, generated context-free questions, and built vulnerability-stratified profiles using 14 demographic factors. Responses were evaluated under both context-blind and context-aware conditions, with context enrichment testing using factors ranked by professionals and users. The evaluation scored likelihood of harm, severity, and safeguard adequacy to produce overall 1-7 safety scores.

## Key Results
- Context-blind evaluation overestimates safety for vulnerable users, with high-vulnerability users seeing scores drop from safe (5/7) to somewhat unsafe (3/7)
- Adding realistic user context to prompts only partially closed the safety gap, even when using factors ranked as most relevant by professionals or most likely to be disclosed by users
- High-vulnerability users showed the largest improvements with context enrichment, but scores remained 1 point below context-blind evaluations (3→4 on 7-point scale)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context-blind evaluation systematically overestimates safety for vulnerable users by defaulting to an implicit "average user" baseline.
- Mechanism: When evaluators lack user demographic context, they assess responses against a generic risk profile. This masks heterogeneity—advice rated "safe" (5/7) for an average user becomes "somewhat unsafe" (3/7) for high-vulnerability users whose compounding constraints (low income, social isolation, health barriers) amplify harm likelihood.
- Core assumption: The directional effect generalizes beyond the tested domains (finance, health) and models (GPT-5, Claude Sonnet 4, Gemini 2.5 Pro).
- Evidence anchors:
  - [abstract] "identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7)"
  - [section 3.2] "context-blind evaluation defaults to an implicit 'average user' baseline that masks the heterogeneity of actual risk across populations"
  - [corpus] Weak direct validation; corpus focuses on multi-turn evaluation and LLM-as-judge reliability, not vulnerability-stratified assessment.
- Break condition: If evaluator prompts explicitly instruct scoring against "most vulnerable plausible user," the gap may narrow—but this trades off against over-conservatism for low-vulnerability users.

### Mechanism 2
- Claim: Enriching user prompts with contextual information yields only partial safety improvements because models fail to fully adapt advice to disclosed vulnerabilities.
- Mechanism: Even with 5 contextual factors added to prompts, models provide generically sound advice that doesn't sufficiently customize safeguards. Users disclose what they deem relevant, not necessarily what professionals identify as safety-critical—but the study found near-complete overlap between these rankings (4.5/5 intersection), suggesting the bottleneck is model adaptation, not disclosure choice.
- Core assumption: Stated disclosure preferences (what users say they'd share) approximate real behavior.
- Evidence anchors:
  - [abstract] "Adding realistic user context to prompts only partially closed this gap"
  - [section 4.3] "high-vulnerability users showed the largest improvements, with scores increasing from somewhat unsafe (3/7) to almost moderately safe (4/7), there remains a 1-point gap"
  - [corpus] Weak; corpus doesn't address context-enriched prompting efficacy.
- Break condition: If models were explicitly trained or prompted to generate vulnerability-calibrated responses (e.g., "if user mentions prior eating disorder, prioritize relapse prevention over weight loss guidance"), the gap could narrow further.

### Mechanism 3
- Claim: Vulnerability level moderates the context-awareness gap because high-vulnerability profiles concentrate multiple risk dimensions that compound.
- Mechanism: Low-vulnerability users have resource buffers (income, support networks, health) that absorb generic advice gaps. High-vulnerability users lack these buffers; a single oversight (e.g., not flagging calorie tracking for eating disorder history) creates disproportionate harm. The 14-factor profile structure (financial, social, health, literacy) captures this compounding.
- Core assumption: The three profiles per vulnerability level are canonical examples, not a representative sample.
- Evidence anchors:
  - [section 3.2] "For low-vulnerability users, context-aware safety scores indicate that model responses are safe to very safe (5-6/7)... More strikingly, for high-vulnerability users, we observe a two-point drop"
  - [Appendix H] Case studies show how generic advice (calorie deficit guidance, investment allocation) becomes actively harmful for specific profiles (anorexia recovery, $18K income with debt)
  - [corpus] Weak; no direct validation of compounding-risk mechanism in neighbors.
- Break condition: If vulnerability were defined along a single dimension (e.g., income only) rather than holistically, compounding effects would be underestimated.

## Foundational Learning

- Concept: **LLM-as-Judge Evaluation**
  - Why needed here: The study uses GPT-o3 as an evaluator with chain-of-thought reasoning; understanding this paradigm is essential to interpret validity concerns and replication.
  - Quick check question: Can you explain why the authors used temperature 0.2 for the judge model and what threat this mitigates?

- Concept: **Risk Matrix (Likelihood × Severity)**
  - Why needed here: The evaluation framework adapts standard risk assessment into 7-point scales; understanding this structure is prerequisite to modifying or extending it.
  - Quick check question: If likelihood=5 and severity=6, what overall safety score range should result according to the scoring logic?

- Concept: **Context-Blind vs. Context-Aware Evaluation**
  - Why needed here: The core experimental manipulation; you must understand this distinction to design similar studies or interpret the gap findings.
  - Quick check question: Why does the same LLM response receive different safety scores under context-blind vs. context-aware conditions?

## Architecture Onboarding

- Component map:
  Reddit API -> gpt-3.5-turbo (advice-seeking filter) -> gpt-4o-mini (theme classification) -> gpt-4o (question synthesis) -> researcher review
  -> Prolific professionals create low/medium/high vulnerability profiles
  -> GPT-5, Claude Sonnet 4, Gemini 2.5 Pro (T=1.0) collect responses
  -> gpt-o3 judge (T=0.2) with context-blind and context-aware prompts
  -> Context enrichment: factor ranking -> clause generation -> prompt synthesis

- Critical path:
  1. Define themes and collect Reddit posts
  2. Generate context-free questions (6 per theme)
  3. Build vulnerability-stratified profiles (3 per level per theme)
  4. Collect model responses
  5. Run context-blind and context-aware evaluations in parallel
  6. For RQ2: rank factors, enrich prompts, re-collect and re-evaluate

- Design tradeoffs:
  - **Exploratory vs. validated**: LLM-as-judge lacks formal human expert validation; iterative prompt refinement provides face validity only. Trade-off: speed/feasibility vs. claim strength.
  - **Breadth vs. depth**: 4 themes × 2 domains × 3 models provides cross-condition robustness but limited within-theme coverage. Trade-off: generalizability vs. exhaustiveness.
  - **Stated vs. revealed preferences**: User likelihood rankings rely on self-report, not observed behavior. Trade-off: ethical feasibility vs. ecological validity.

- Failure signatures:
  - **Judge prompt misinterpretation**: If the judge applies context-blind logic to context-aware prompts, the gap will artificially shrink. Detection: review chain-of-thought outputs for user-specific reasoning.
  - **Profile stereotyping**: If profiles rely on caricatures rather than professional-constructed nuance, harm mechanisms may not generalize. Detection: qualitative review by domain experts.
  - **Phrasing bias**: Single prompt phrasing may elicit atypical responses. Mitigation: 5 phrasing variants per context level.

- First 3 experiments:
  1. **Replicate on a single domain**: Pick one theme (e.g., Debt Management), generate 3 new questions, build fresh profiles, and run context-blind vs. context-aware evaluation on one model to verify the gap magnitude.
  2. **Test judge reliability**: Run the same 20 response-context pairs through the judge 3 times each; compute score variance to assess consistency.
  3. **Pilot human expert validation**: Recruit 2 financial advisors to rate 10 responses (5 context-blind, 5 context-aware) and compare against LLM judge scores to estimate systematic bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the LLM-as-judge evaluation methodology be reliably validated against human domain expert judgments?
- Basis in paper: [Explicit] The authors state a "fundamental limitation is the missing validation of our LLM-as-judge against expert human judgments" and strongly encourage such validation for definitive claims.
- Why unresolved: The study relied on iterative prompt refinement to establish face validity, but without calibration against human experts, the system may suffer from LLM-specific biases or hallucination errors in safety assessment.
- What evidence would resolve it: A study showing high correlation and inter-annotator agreement between the LLM-judge's safety scores and those provided by qualified human financial advisors and clinicians for the same response sets.

### Open Question 2
- Question: How does the safety profile of LLM advice change in multi-turn conversations or when models utilize persistent memory features?
- Basis in paper: [Explicit] The authors acknowledge they "do not address the memory features now deployed by major providers" and state that "evaluating safety in such dynamic interactions presents both methodological challenges and important opportunities."
- Why unresolved: The current methodology evaluates single-turn advice requests, whereas real-world harm may emerge through interaction trajectories where context is accumulated or advice is incrementally modified over time.
- What evidence would resolve it: A longitudinal evaluation framework that assesses safety outcomes in multi-turn dialogue sessions compared to the single-turn baseline scores established in this paper.

### Open Question 3
- Question: To what extent do actual user disclosure behaviors in natural LLM interactions differ from the stated preferences modeled in this study?
- Basis in paper: [Inferred] The paper notes it relied on "stated preferences" which "can be subject to hypothetical or introspection biases," and explicitly calls for "large-scale studies of how users actually engage with LLMs."
- Why unresolved: The finding that context disclosure only partially closes the safety gap is based on synthetic prompts derived from surveys rather than observed user behavior; actual users may disclose less (privacy concerns) or different context than they report.
- What evidence would resolve it: Comparative analysis of safety scores using prompts constructed from real-world usage logs (e.g., via data donation initiatives) versus the survey-based disclosure models used in the current study.

## Limitations
- LLM-as-judge evaluation lacks formal human expert validation, relying instead on iterative prompt refinement for face validity
- The study uses single prompt phrasings, which may not represent the full space of how users would naturally disclose vulnerability
- Self-reported user disclosure preferences may not accurately reflect real disclosure behavior

## Confidence
- **High confidence**: The fundamental finding that context-blind evaluation overestimates safety for vulnerable users (the 2-point safety score gap from 5→3 on 7-point scale)
- **Medium confidence**: The partial nature of context enrichment improvements, as the study's user disclosure measures are self-reported rather than observed
- **Medium confidence**: The claim that model adaptation failure rather than disclosure choice drives the remaining gap, given the near-complete overlap between professional and user rankings

## Next Checks
1. Replicate the core context-blind vs. context-aware evaluation on a single domain (e.g., Debt Management) with fresh profiles to verify the gap magnitude
2. Run the same response-context pairs through the judge 3 times each to assess score consistency and reliability
3. Pilot human expert validation by having domain professionals rate a subset of responses to estimate systematic bias in the LLM judge