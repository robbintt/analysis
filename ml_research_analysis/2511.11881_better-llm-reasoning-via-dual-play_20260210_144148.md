---
ver: rpa2
title: Better LLM Reasoning via Dual-Play
arxiv_id: '2511.11881'
source_url: https://arxiv.org/abs/2511.11881
tags:
- knowledge
- proposer
- question
- solver
- external
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PasoDoble improves LLM reasoning through a dual-play adversarial\
  \ framework that trains two models\u2014a Proposer that generates challenging questions\
  \ and a Solver that attempts to solve them. By enriching the Proposer with external\
  \ pre-training knowledge and rewarding it for creating valid, diverse questions\
  \ that push the Solver's limits, the system achieves sustained mutual improvement\
  \ without external supervision."
---

# Better LLM Reasoning via Dual-Play

## Quick Facts
- arXiv ID: 2511.11881
- Source URL: https://arxiv.org/abs/2511.11881
- Reference count: 40
- Primary result: 11-16 point accuracy gains on math benchmarks via dual-play adversarial training

## Executive Summary
PasoDoble introduces a dual-play adversarial framework where two models—a Proposer that generates challenging questions and a Solver that attempts to solve them—mutually improve through competition. The system enriches the Proposer with external knowledge and uses validity checks to prevent reward hacking. Experiments show average accuracy gains of 11-16 points across various model sizes (0.5B to 4B parameters), with larger models benefiting most. The framework sustains improvements for hundreds of training steps without external supervision.

## Method Summary
PasoDoble trains two models adversarially: a Proposer generates QA pairs from a knowledge base, and a Solver attempts to solve them. Both undergo GRPO training with custom rewards—the Proposer receives difficulty (inversely proportional to Solver accuracy) and diversity rewards, while the Solver receives correctness rewards. Validity filtering ensures only solvable questions train the Solver. The framework supports online (joint updates) and offline (buffered updates) modes. Models are first cold-started with 2K SFT examples each, then trained for 60 offline iterations or 600 online steps.

## Key Results
- 11-16 point average accuracy gains across math benchmarks (AIME, AMC, GSM8K, MATH-500, OlympiadBench)
- Larger models (1.5B+) benefit most from external knowledge grounding
- Dual-play outperforms self-play by 3% average across benchmarks
- Maintains stability through validity checks and joint/decoupled training modes

## Why This Works (Mechanism)

### Mechanism 1
Adversarial co-evolution between specialized Proposer and Solver roles produces sustained improvement beyond single-model self-play. The Proposer receives reward inversely correlated with Solver accuracy, creating competitive pressure that prevents stagnation. Core assumption: difficulty and diversity rewards prevent reward hacking. Evidence: dual-play outperforms self-play by 3% average; co-evolution effective in related work. Break condition: improper τ_low threshold filters valid questions or allows incorrect answers.

### Mechanism 2
External knowledge grounding enables Proposer to generate correct ground-truth answers despite same-base initialization as Solver. Knowledge pieces from MegaMath-Pro-Max compensate for inherent strength differences. Core assumption: pre-training corpus contains high-quality, domain-relevant content. Evidence: ~55-65% of answers derivable from external knowledge; larger models benefit more. Break condition: knowledge exceeds model comprehension capacity.

### Mechanism 3
Validity filtering via Solver pass-rate creates quality gate maintaining training signal integrity. Questions with p_i ≤ τ_low (too hard/invalid) or p_i = 1 (too easy) are excluded. Core assumption: τ_low = 0.2 balances quantity vs. quality. Evidence: filters ~73% of incorrect ground-truth answers while retaining challenging problems. Break condition: Solver improves faster than Proposer can generate harder questions.

## Foundational Learning

- **GRPO/PPO Reinforcement Learning**: Both models trained via policy gradient methods with custom reward functions. Why needed: custom rewards for adversarial training. Quick check: Why use outcome-based rewards rather than process supervision?

- **Reward Hacking in LLM Training**: Validity gating and diversity rewards prevent Proposer from gaming difficulty reward. Why needed: maintain training integrity. Quick check: What happens if diversity reward is removed? (Answer: ~4 point average degradation)

- **GAN-style Adversarial Dynamics**: Framework inherits instability concerns from GAN literature; offline paradigm addresses this. Why needed: manage training instability. Quick check: Why does offline paradigm decouple Proposer/Solver updates?

## Architecture Onboarding

- Component map: Knowledge Base → Proposer → Questions → Solver → Answers → Reward Computation → RL Update → Updated Proposer/Solver
- Critical path: Knowledge sampling → Proposer generation → Solver multi-attempt → Validity filtering → Reward computation → Joint update
- Design tradeoffs:
  - Online vs. Offline: Online simpler but higher variance; Offline more stable but needs buffer
  - τ_low threshold: Higher = cleaner data but less efficient; 0.2 recommended
  - External knowledge: Benefits larger models, can hurt smaller models
- Failure signatures:
  - Performance plateaus quickly → check if Proposer frozen
  - Solver accuracy collapses → check for reward hacking
  - Out-of-domain degradation observed
- First 3 experiments:
  1. Replicate cold-start SFT with 2K examples per role
  2. Run offline PasoDoble with τ_low ∈ {0.1, 0.2, 0.3} to calibrate
  3. Ablate external knowledge on target model size

## Open Questions the Paper Calls Out

- **Does performance scale with larger models?**: Experiments only tested up to 4B parameters; scaling behavior at larger scales unknown. Would require running on 7B, 14B models and reporting accuracy curves.

- **Can PasoDoble adapt to out-of-domain tasks?**: Limited gains on GPQA suggest domain dependence. Would require applying with domain-appropriate knowledge bases to tasks like coding or legal QA.

- **What sustains training beyond saturation?**: Current framework shows plateauing after hundreds of steps. Would require analyzing training dynamics, testing curriculum renewal strategies, or adaptive difficulty scheduling.

## Limitations

- Generalization beyond math benchmarks is limited, with minimal gains on GPQA
- Performance heavily dependent on quality and coverage of external knowledge base
- Scaling assumptions primarily validated up to 4B parameters; larger models may introduce new failure modes

## Confidence

- **High Confidence**: Dual-play framework's ability to improve math reasoning accuracy (11-16 points) is well-supported by controlled ablations and benchmark comparisons
- **Medium Confidence**: Claims about superior scaling capacity relative to R-Zero are supported by iteration comparisons but lack direct head-to-head training curves
- **Low Confidence**: Assertion that PasoDoble "sustains improvements for hundreds of training steps" lacks precise quantification and convergence criteria

## Next Checks

1. **Cross-domain transfer**: Apply PasoDoble to non-mathematical domains (e.g., commonsense reasoning or code generation) to test framework's robustness to domain shift
2. **Long-term stability analysis**: Extend offline training beyond 60 iterations (e.g., to 200+) and monitor whether Proposer-Solver arms race continues improving
3. **Ablation of knowledge quality**: Systematically vary external knowledge base quality to quantify how knowledge reliability affects Proposer's ability to generate valid, challenging questions