---
ver: rpa2
title: 'Towards conversational assistants for health applications: using ChatGPT to
  generate conversations about heart failure'
arxiv_id: '2505.03675'
source_url: https://arxiv.org/abs/2505.03675
tags:
- patient
- conversations
- reasoning
- heart
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study explored using ChatGPT to generate culturally sensitive,
  patient-initiated dialogues for African American heart failure patients, a group
  underserved by existing healthcare resources. Four prompting strategies were tested:
  domain-specific topics, African American Vernacular English, Social Determinants
  of Health (SDOH) features, and SDOH-informed reasoning.'
---

# Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure

## Quick Facts
- arXiv ID: 2505.03675
- Source URL: https://arxiv.org/abs/2505.03675
- Authors: Anuja Tayal; Devika Salunke; Barbara Di Eugenio; Paula G Allen-Meares; Eulalia P Abril; Olga Garcia-Bedoya; Carolyn A Dickens; Andrew D. Boyd
- Reference count: 17
- One-line primary result: ChatGPT can generate culturally sensitive, patient-initiated dialogues for African American heart failure patients but struggles with maintaining natural two-way dialogue and empathy.

## Executive Summary
This study explores using ChatGPT to generate culturally sensitive, patient-initiated dialogues for African American heart failure patients, a group underserved by existing healthcare resources. Four prompting strategies were tested: domain-specific topics, African American Vernacular English, Social Determinants of Health (SDOH) features, and SDOH-informed reasoning. Results showed that while ChatGPT could generate relevant, domain-specific content and adapt conversations based on SDOH features, the model struggled with maintaining natural two-way dialogue, often dominated by the educator. Incorporating reasoning before generating responses improved contextual appropriateness, but empathy and engagement remained lacking. The study underscores the importance of prompt design and highlights the need for further refinement to create more engaging, empathetic healthcare dialogues.

## Method Summary
The study used ChatGPT (3.5-turbo and 4) to generate dialogues across four prompting strategies with structured output format [speaker][utterance]. Four approaches were tested: Domain (topic-specific prompts), AAVE (patient uses AAVE, educator uses Standard English), SDOH (include patient description with SDOH features), and SDOH-informed reasoning (two-step chain prompting with structured reasoning). Inputs included self-care domains (food, exercise, fluid intake), conversation turn lengths (5, 10, 15), and SDOH features (gender, age, neighborhood safety, socioeconomic status). Quantitative metrics measured format adherence, word ratios, and follow-up ratios, while qualitative evaluations used Likert-scale ratings from domain experts on answer quality, actionability, empathy, and SDOH appropriateness.

## Key Results
- ChatGPT generated relevant, domain-specific content and adapted conversations based on SDOH features, but struggled with natural two-way dialogue
- Incorporating reasoning before dialogue generation significantly improved contextual appropriateness compared to direct SDOH prompting
- The model consistently dominated conversations with educator turns, failing to achieve balanced dialogue even with explicit word limit constraints

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Generating SDOH-informed reasoning before dialogue improves contextual appropriateness of healthcare conversations.
- **Mechanism**: A two-phase prompting approach—first generating structured reasoning (Premise → Reasoning → Solution → Anticipate Barrier → Solve Barrier → Educate), then generating dialogue conditioned on that reasoning—enables the model to explicitly consider patient constraints (e.g., unsafe neighborhood, poverty) before formulating responses.
- **Core assumption**: LLMs can be guided through explicit reasoning chains to produce more contextually grounded outputs than direct generation.
- **Evidence anchors**: Abstract states "incorporating reasoning before generating responses improved contextual appropriateness," and Section 4.4 shows significant improvement when comparing Approach 3 to Approach 4 results.
- **Break condition**: When SDOH features are complex or contradictory; when reasoning generation itself fails to follow the six-step template (Format Adherence Rate for reasoning: 0.23–0.96 depending on model/approach).

### Mechanism 2
- **Claim**: Prompt-level constraints can partially control dialogue balance and format but not conversational dynamics.
- **Mechanism**: Explicit instructions (e.g., "limit each educator turn to 20 words," "use format [speaker][utterance]") reduce word count ratios but do not increase follow-up questions or patient-initiated turns—the model defaults to educator-dominated Q&A patterns.
- **Core assumption**: Surface-level prompt constraints can shape output structure without architectural changes to the generation process.
- **Evidence anchors**: Abstract notes "ChatGPT struggled with maintaining natural two-way dialogue, often dominated by the educator," and Section 4.1 shows that word limits made responses "vague and unsatisfying" while follow-up ratio remained near zero.
- **Break condition**: When word limits are too restrictive (responses become "vague and unsatisfying"); when follow-up ratio is explicitly required but not prompted (follow-up ratio remained near zero across approaches).

### Mechanism 3
- **Claim**: Cultural linguistic adaptation (AAVE) is unstable and risks boundary violations.
- **Mechanism**: Prompting patient-side AAVE with educator-side Standard English fails as a specification—the model sometimes generates educator responses in AAVE, which the authors note "may come across as offensive and disrespectful."
- **Core assumption**: LLMs can maintain consistent persona-specific linguistic registers when explicitly instructed.
- **Evidence anchors**: Section 3.5.2 documents instances where "the educator also used AAVE" despite instructions, with Table 3 showing an example where the educator says "Yup, it sure does... so you ain't guzzlin' too much at once."
- **Break condition**: When cultural-linguistic boundaries are critical; when model training data contains code-switching patterns that override prompt instructions.

## Foundational Learning

- **Social Determinants of Health (SDOH)**
  - Why needed here: The entire personalization mechanism depends on understanding how factors like neighborhood safety, socioeconomic status, age, and gender affect healthcare access and behavior.
  - Quick check question: Can you explain why suggesting "walking in a safe area" to a patient in an unsafe neighborhood represents both an empathy failure and an SDOH reasoning failure?

- **Chain-of-thought prompting**
  - Why needed here: Approach 4 (SDOH-informed reasoning) uses this technique to decompose patient needs into logical steps before dialogue generation.
  - Quick check question: What is the difference between few-shot prompting, chain-of-thought, and the six-step reasoning template used in this paper?

- **Health literacy and plain language communication**
  - Why needed here: Patients with lower health literacy require simpler explanations; the paper specifies "6th grade reading level" and "simple English" as constraints.
  - Quick check question: Why might limiting educator turns to 20 words conflict with the goal of providing actionable, comprehensible health advice?

## Architecture Onboarding

- **Component map**: Input Layer (Domain selector + SDOH attributes) → Prompt Construction (Strategy-specific templates) → Generation Phase 1 (Approach 4 only: Structured reasoning) → Generation Phase 2 (Dialogue generation) → Output (formatted dialogue) → Evaluation (Quantitative + Qualitative)

- **Critical path**: Prompt design → Reasoning generation (if Approach 4) → Dialogue generation → Format/round adherence check. The paper shows GPT-4 outperforms 3.5-turbo on format adherence (1.0 vs 0 for Domain approach) but still struggles with round adherence (0–0.64).

- **Design tradeoffs**:
  - Word limits vs. informativeness: 20-word limit reduces educator dominance but makes responses "vague and unsatisfying."
  - Reasoning step vs. efficiency: Two-phase generation improves appropriateness but doubles API calls and latency.
  - AAVE specification vs. cultural safety: Attempting dialect control risks boundary violations if model overrides instructions.

- **Failure signatures**:
  - Medical accuracy failures: Advising HF patients to "drink more water" when they should limit fluids (Table 1).
  - Empathy gaps: Responding to "exercises safe in my neighborhood" with "walking on safe streets" (ignores stated constraint).
  - Format violations: Incorrect number of turns (Round Adherence Rate as low as 0.02).
  - Cultural boundary violations: Educator adopting AAVE unprompted (Table 3).
  - Persistent educator dominance: Word ratio HE:patient consistently >1.5 despite constraints.

- **First 3 experiments**:
  1. **Baseline comparison**: Generate dialogues across all four approaches with identical SDOH profiles; measure round adherence, follow-up ratio, word ratio, and expert Likert ratings for SDOH appropriateness.
  2. **Ablation on reasoning steps**: Test which of the six reasoning components (Premise, Reasoning, Solution, Anticipate Barrier, Solve Barrier, Educate) contribute most to improved SDOH personalization by selectively removing each.
  3. **Empathy intervention**: Add explicit empathy prompting (e.g., "acknowledge patient's emotional state before advising") and measure change in qualitative empathy ratings; compare to baseline Approach 4.

## Open Questions the Paper Calls Out
None

## Limitations
- Only 84 out of 1,080 generated conversations were evaluated qualitatively, raising questions about representativeness.
- Evaluation relied on 7 PhD students with NLP backgrounds rather than healthcare domain experts, potentially limiting medical accuracy assessments.
- The paper does not provide full prompt templates, making exact reproduction challenging without inferring missing details.

## Confidence
- **High confidence**: The mechanism that SDOH-informed reasoning improves contextual appropriateness is well-supported by direct comparisons between Approaches 3 and 4 showing significant improvements in SDOH appropriateness ratings.
- **Medium confidence**: The claim that cultural linguistic adaptation (AAVE) is unstable has strong example-level evidence but limited systematic testing across different AAVE variants or patient profiles.
- **Low confidence**: The assertion that prompt-level constraints can control dialogue balance is contradicted by results showing persistent educator dominance despite explicit word limit instructions.

## Next Checks
1. **Prompt template reconstruction**: Attempt to reconstruct complete prompt templates from available examples and test whether they reproduce the reported format adherence rates across both GPT-3.5-turbo and GPT-4.
2. **Expert review validation**: Have the 84 qualitatively evaluated conversations re-assessed by healthcare professionals with heart failure expertise to verify the original Likert ratings and identify any medical accuracy issues.
3. **Empathy prompting intervention**: Test whether explicitly prompting for empathy statements before or within educator responses improves qualitative empathy ratings compared to the baseline Approach 4 results.