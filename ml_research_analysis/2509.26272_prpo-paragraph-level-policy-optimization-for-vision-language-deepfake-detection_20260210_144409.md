---
ver: rpa2
title: 'PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake Detection'
arxiv_id: '2509.26272'
source_url: https://arxiv.org/abs/2509.26272
tags:
- reasoning
- detection
- deepfake
- image
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PRPO (Paragraph-level Relative Policy Optimization),
  a novel reinforcement learning algorithm for deepfake detection that aligns multimodal
  LLM reasoning with visual evidence at the paragraph level. The authors create DF-R5,
  a reasoning-annotated dataset of 115k deepfake images with detailed explanations,
  and design DX-LLaVA, a multimodal architecture using CLIP ConvNeXT for fine-grained
  artifact detection.
---

# PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake Detection

## Quick Facts
- arXiv ID: 2509.26272
- Source URL: https://arxiv.org/abs/2509.26272
- Reference count: 40
- Key outcome: PRPO achieves 89.91% accuracy and 4.55/5.0 reasoning score, outperforming state-of-the-art baselines and MLLMs in deepfake detection with explainable reasoning.

## Executive Summary
This paper introduces PRPO (Paragraph-level Relative Policy Optimization), a novel reinforcement learning algorithm that addresses the challenge of deepfake detection with explainable reasoning. PRPO aligns multimodal LLM reasoning with visual evidence at the paragraph level, using a combination of visual consistency reward (VCR) and prediction consistency reward (PCR) computed across multiple sampled outputs. The authors create DF-R5, a reasoning-annotated dataset of 115k deepfake images with detailed explanations, and design DX-LLaVA, a multimodal architecture using CLIP ConvNeXT for fine-grained artifact detection. Experiments demonstrate PRPO's superiority over baseline LLaVA and GRPO in balancing precision and recall while maintaining high-quality, evidence-grounded explanations.

## Method Summary
PRPO is a test-time reinforcement learning algorithm that optimizes multimodal deepfake detection models by aligning their reasoning with visual evidence. The method uses DX-LLaVA architecture (Vicuna-7B + CLIP ConvNeXT-Large-320 + GAP classifier) fine-tuned on DF-R5 dataset with combined supervised loss (L_lm + αL_binary). At test time, PRPO samples L outputs per image, segments responses into paragraphs, and computes paragraph-level advantages using visual consistency reward (YAKE keyword extraction + CLIP similarity) and prediction consistency reward (majority voting across paragraphs). The policy is updated with clipped PPO-style objective plus KL regularization. This approach addresses hallucination and inconsistency issues in deepfake detection while maintaining high precision and recall.

## Key Results
- PRPO achieves 89.91% accuracy and 4.55/5.0 reasoning score, significantly outperforming state-of-the-art baselines and MLLMs.
- Combined VCR+PCR reward achieves 96.67% precision and 93.37% recall, superior to VCR-only (99.30%/80.11%) or PCR-only (99.66%/55.11%).
- CLIP ConvNeXT encoder improves F1 from 78.08% to 89.91% compared to CLIP ViT, demonstrating better local artifact detection capability.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Paragraph-level reward computation improves reasoning alignment with visual evidence compared to token-level alternatives.
- Mechanism: PRPO segments each response into M+1 paragraphs and computes advantages at the paragraph level (Eq. 5), rather than assigning uniform outcome rewards to all tokens as in GRPO. This increases likelihood for paragraphs that align with visual evidence while decreasing misaligned ones.
- Core assumption: Reasoning about deepfake artifacts naturally decomposes into semantically coherent paragraph-level units.
- Evidence anchors:
  - [abstract] "aligns LLM reasoning with image content at the paragraph level"
  - [section 3.3] "Different from GRPO, where each token is treated with the same advantage, PRPO computes advantages at the paragraph level"
  - [corpus] TabR1 (arXiv 2510.17385) applies related GRPO adaptations for tabular reasoning, suggesting domain-specific reward structuring is an emerging pattern

### Mechanism 2
- Claim: Combining visual consistency reward (VCR) and prediction consistency reward (PCR) balances hallucination reduction with decision-reasoning alignment.
- Mechanism: VCR (Eq. 3) uses CLIP ConvNeXT to compute cosine similarity between extracted paragraph keywords and image features. PCR evaluates whether the final prediction matches the majority vote across preceding paragraphs. The combined reward (Eq. 4) averages both signals.
- Core assumption: Keywords extracted via YAKE sufficiently represent paragraph semantic content for visual alignment; majority voting across paragraphs approximates ground-truth reasoning direction.
- Evidence anchors:
  - [section 4.4] VCR-only achieves 99.30% precision but 80.11% recall; PCR-only achieves 99.66% precision but 55.11% recall; combined achieves 96.67% precision and 93.37% recall
  - [section 3.3] "each reward alone is insufficient: VCR mitigates hallucinations but lacks decision-level alignment, whereas PCR enforces alignment but encourages overly generic predictions"
  - [corpus] Corpus evidence on reward combination for multimodal RL is limited; no directly comparable dual-reward frameworks found

### Mechanism 3
- Claim: CLIP ConvNeXT encoder captures fine-grained deepfake artifacts better than CLIP ViT due to stronger texture bias and pixel-level embeddings.
- Mechanism: DX-LLaVA uses Stage 3 ConvNeXT outputs (10×10 feature map → 100 pixel embeddings) rather than ViT patch embeddings, providing higher sensitivity to local texture irregularities (hairlines, pores, background artifacts).
- Core assumption: Deepfake artifacts manifest primarily as local texture anomalies rather than global semantic inconsistencies.
- Evidence anchors:
  - [section 3.2] "While ViT captures global semantics effective for VQA, deepfake detection demands sensitivity to local, high-frequency artifacts, making ViT suboptimal"
  - [Table 3] Llm+αL_binary improves from 59.40% to 70.40% accuracy; Table 4 shows DX-LLaVA achieves 78.08% average F1, with PRPO reaching 89.91%
  - [corpus] TruthLens (arXiv 2503.15342) and related detection work do not specifically compare ConvNeXT vs ViT for deepfake tasks

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: PRPO is explicitly inspired by GRPO but modifies reward computation from token-level to paragraph-level. Understanding GRPO's relative advantage normalization is prerequisite to grasping PRPO's modification.
  - Quick check question: Can you explain why GRPO normalizes rewards across multiple sampled outputs, and what problem this addresses compared to absolute reward scaling?

- Concept: **Vision-Language Projector Alignment**
  - Why needed here: The paper identifies poor projector-to-visual-cue alignment as a key failure mode (section 3.2), motivating the binary classifier addition and ConvNeXT substitution.
  - Quick check question: What role does the multimodal projector play in LLaVA-style architectures, and why might it fail to transfer discriminative visual features to the LLM?

- Concept: **Deepfake Artifact Taxonomy**
  - Why needed here: DF-R5 uses 74 curated forensic features (Table 9) spanning eyes, skin, mouth, hair, lighting, etc. Understanding these categories is essential for interpreting VCR behavior and debugging reward signals.
  - Quick check question: Which artifact categories does the paper identify as most frequently annotated in DF-R5, and why might certain categories be harder for models to detect?

## Architecture Onboarding

- Component map:
  - Image (320×320) → CLIP ConvNeXT (frozen, Stage 3) → 100 pixel embeddings (1536-dim)
  - Projector: Linear projection to 4096-dim → Vicuna-7B token space
  - Classifier: GAP pooling → Binary classifier (real/fake) trained with L_binary
  - LLM: Vicuna-7B fine-tuned with combined L_lm + αL_binary (α=10.0)
  - PRPO Layer: Test-time RL with VCR (YAKE keywords → CLIP similarity) and PCR (majority voting across paragraphs)

- Critical path:
  1. Fine-tune DX-LLaVA on DF-R5 with Eq. 2 objective (L_lm + αL_binary)
  2. Apply PRPO at test time: sample L outputs per image, segment into paragraphs, compute VCR+PCR rewards, normalize advantages, update policy with clipped objective (Eq. 5) + KL regularization (Eq. 6, β=0.01)
  3. Final prediction from majority vote across sampled outputs

- Design tradeoffs:
  - ViT vs ConvNeXT: ViT provides better global semantics for general VQA; ConvNeXT provides stronger texture bias for artifact detection
  - VCR-only vs PCR-only vs combined: VCR prioritizes visual grounding but sacrifices recall; PCR enforces internal consistency but risks generic predictions; combined balances both
  - Test-time vs training-time RL: PRPO can run at test time without retraining, but requires multiple samples per inference (increasing compute)

- Failure signatures:
  - High precision, low recall → likely VCR-dominant behavior (over-conservative)
  - Contradictory final answer vs reasoning paragraphs → PCR failure (check paragraph-level label extraction dictionaries)
  - Hallucinated artifacts not in image → VCR not penalizing sufficiently (check YAKE keyword quality)

- First 3 experiments:
  1. **Ablation on encoder choice**: Compare CLIP ViT-L/14 vs CLIP ConvNeXT on inter-domain transfer (hold out one generative domain) using same fine-tuning protocol
  2. **Reward component analysis**: Run VCR-only, PCR-only, and combined on SD-2.1 domain (as in Table 7) and analyze precision-recall tradeoffs
  3. **Paragraph segmentation sensitivity**: Test whether varying the number of paragraphs (M) affects reasoning quality scores (using GPT-4o evaluation protocol from section 4.3)

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but the following critical limitations are implied:

### Open Question 1
- Question: Does the Visual Consistency Reward (VCR) using YAKE keyword extraction fail to penalize reasoning that correctly identifies the absence of artifacts (negations), such as "no distortion"?
- Basis in paper: [inferred] Section 3.3 defines VCR using YAKE to extract keywords for CLIP similarity. YAKE is unsupervised and extracts representative terms (e.g., "distortion") without capturing semantic negation or context.
- Why unresolved: A sentence stating "no distortion" may yield keywords like "distortion," resulting in high similarity to a fake image containing distortions, potentially rewarding hallucinated explanations or penalizing correct negations.
- What evidence would resolve it: An ablation study measuring reward scores for generated sentences containing explicit negations (e.g., "no artifacts") versus affirmative sentences to verify if the reward signal aligns with semantic truth.

### Open Question 2
- Question: Is PRPO's detection capability constrained by the fixed set of 74 forensic features curated during the dataset generation phase?
- Basis in paper: [inferred] Section 3.1 describes a pipeline where features are distilled from MLLMs and consolidated into a specific list of 74 features (Table 9) used for reasoning generation.
- Why unresolved: The model is optimized to explain deepfakes using this specific feature vocabulary. If a new generative model produces artifacts outside these 74 categories, the system may struggle to detect or explain them.
- What evidence would resolve it: Evaluation on deepfakes generated by architectures released after the dataset creation, analyzing whether the model identifies novel artifact types or hallucinates explanations using only the 74 known features.

### Open Question 3
- Question: How does the computational overhead of generating multiple sample outputs ($L$ samples) for relative reward normalization affect the practical viability of PRPO in real-time detection scenarios?
- Basis in paper: [inferred] Section 3.3 (Eq. 5) requires computing the mean and standard deviation of rewards across a set of sampled outputs $O$ for each prompt, and the abstract claims "test-time" applicability.
- Why unresolved: While accurate, generating a group of responses per image to calculate the advantage $A_j$ significantly increases inference latency compared to single-pass models, potentially limiting deployment in high-throughput or resource-constrained environments.
- What evidence would resolve it: A latency-vs-accuracy trade-off analysis measuring time-to-detection as the sample group size $L$ is varied, specifically comparing single-sample inference versus the optimized $L$ used in experiments.

## Limitations

- Reliance on synthetic annotations from Gemini-2.5 for DF-R5 may not capture human expert reasoning patterns or ground-truth visual evidence.
- Paragraph-level reward structure assumes reasoning can be cleanly segmented into coherent units, which may not hold for all deepfake detection scenarios where artifacts span multiple contexts.
- Dual-reward system (VCR + PCR) lacks ablation studies on reward weighting, leaving open questions about optimal balance between visual grounding and prediction consistency.

## Confidence

High confidence in: (1) The overall performance improvement of PRPO over baseline LLaVA and GRPO, supported by consistent numerical results across multiple domains. (2) The architectural choice of CLIP ConvNeXT over ViT for local artifact detection, given the substantial F1 improvement. (3) The general approach of combining visual and prediction consistency rewards, as evidenced by the precision-recall tradeoff analysis.

Medium confidence in: (1) The specific YAKE keyword extraction effectiveness for VCR computation, as keyword quality directly impacts reward signal quality. (2) The paragraph segmentation methodology's generalizability beyond DF-R5's curated features. (3) The optimal values for PRPO hyperparameters (learning rate, β, paragraph count M) without broader sensitivity analysis.

Low confidence in: (1) Real-world deployment performance on unseen generative models or adversarial deepfakes. (2) The robustness of PCR majority voting when reasoning paragraphs contain mixed or ambiguous signals. (3) The scalability of PRPO to larger models or different multimodal architectures without retraining.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate PRPO on an entirely new generative model domain (e.g., Midjourney or DALL-E) not represented in DF-R5 to assess true zero-shot capability and identify domain-specific failure modes.

2. **Human Evaluation of Reasoning Quality**: Conduct expert human review of PRPO's reasoning outputs to verify that high GPT-4o scores correspond to accurate, evidence-grounded explanations rather than superficial coherence.

3. **Adversarial Robustness Analysis**: Test PRPO against deepfakes specifically designed to fool local texture detectors (e.g., semantic-level manipulations) to evaluate whether ConvNeXT's local bias becomes a vulnerability.