---
ver: rpa2
title: The Actor-Critic Update Order Matters for PPO in Federated Reinforcement Learning
arxiv_id: '2506.01261'
source_url: https://arxiv.org/abs/2506.01261
tags:
- policy
- einit
- fedrac
- have
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses data heterogeneity in federated reinforcement
  learning when applying Proximal Policy Optimization (PPO). The authors identify
  that the conventional actor-critic update order (critic first, then actor) causes
  gradient divergence across clients due to local policy evaluation, hindering convergence.
---

# The Actor-Critic Update Order Matters for PPO in Federated Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2506.01261
- **Source URL**: https://arxiv.org/abs/2506.01261
- **Reference count**: 40
- **Primary result**: FedRAC achieves higher cumulative rewards and faster convergence than baseline PPO in federated RL under data heterogeneity by reversing actor-critic update order.

## Executive Summary
This paper addresses data heterogeneity in federated reinforcement learning when applying Proximal Policy Optimization (PPO). The authors identify that the conventional actor-critic update order (critic first, then actor) causes gradient divergence across clients due to local policy evaluation, hindering convergence. To resolve this, they propose FedRAC, which reverses the update order (actor first, then critic), ensuring all clients update their actors based on the same global critic estimate from the previous round. Theoretical analysis shows FedRAC's convergence bound is immune to data heterogeneity under bounded heterogeneity and accurate policy evaluation conditions. Empirical results across five environments—including classical RL tasks and a heterogeneous autonomous driving scenario—demonstrate FedRAC achieves higher cumulative rewards and faster convergence than the baseline. The method is also compatible with other federated learning algorithms like FedAvg, FedProx, and SCAFFOLD.

## Method Summary
The paper proposes FedRAC, a federated PPO algorithm that reverses the conventional actor-critic update order to address gradient divergence across heterogeneous clients. In the baseline approach, clients update their critic based on local environment dynamics before updating the actor, leading to diverse critic estimations across clients. FedRAC forces all clients to use the same global critic from the previous round to update their actors, acting as a common anchor for gradient updates. The algorithm consists of synchronizing global actor and critic parameters, collecting trajectories using the actor, updating the actor using the received global critic, updating the critic using local data, and aggregating updated parameters through weighted averaging.

## Key Results
- FedRAC achieves higher cumulative rewards and faster convergence than baseline PPO across five environments including MountainCar, Hopper, and HongKongOSMs.
- The convergence bound of FedRAC is theoretically immune to data heterogeneity under bounded heterogeneity and accurate policy evaluation conditions.
- FedRAC performance is sensitive to critic network capacity—under-parameterized critics eliminate the method's advantage.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reversing the update order (Actor first, then Critic) aligns local gradient directions across heterogeneous clients.
- **Mechanism:** In conventional Federated PPO (Critic first, then Actor), clients update their Critic based on local environment dynamics ($P_n$) before updating the Actor. This results in diverse Critic estimations ($Q^{w}_{n}$) across clients. When these diverse Critics guide the Actor updates, the resulting gradients point in different directions ("gradient divergence"), hindering global aggregation. FedRAC forces all clients to use the *same* global Critic ($Q^{w}_{t}$) from the previous round to update their Actors. This acts as a common anchor, ensuring that even if local data distribution varies, the immediate gradient step for the policy is derived from a shared value estimation.
- **Core assumption:** The global Critic from the previous communication round remains a sufficiently valid estimator for the current local policy updates (temporal consistency).
- **Evidence anchors:**
  - [Abstract] "...ensuring all clients update their actors based on the same global critic estimate from the previous round."
  - [Section 4.2 - Aggregation Error] Lemma 4.6 shows the aggregation error bound $\Omega_t$ includes a heterogeneity term $p\beta^{-1}\kappa$ where $p=1$ for the baseline and $p=0$ for FedRAC.
  - [Corpus] "Bi-Level Policy Optimization with Nyström Hypergradients" (Arxiv 2505.11714) discusses the dependency of the actor on the critic update, providing a conceptual basis for why synchronization order matters, though it does not explicitly cover the federated update order swap.
- **Break condition:** If the environment dynamics shift too rapidly between rounds, the "stale" global Critic becomes a poor estimator for current local policies, causing the Actor to follow incorrect gradients.

### Mechanism 2
- **Claim:** The convergence bound is theoretically decoupled from the level of environment heterogeneity ($\kappa$).
- **Mechanism:** Standard Federated RL convergence bounds typically contain a term proportional to the dissimilarity between client environments (heterogeneity $\kappa$). This term represents an irreducible error floor caused by conflicting local objectives. By standardizing the gradient estimator (Mechanism 1), FedRAC removes the dependency on $\kappa$ from the aggregation error term. Theoretical analysis indicates that while the baseline's error scales with $\kappa$, FedRAC's error bound remains constant regardless of how different the clients' environments are, provided the Critic is accurate.
- **Core assumption:** The level of heterogeneity is bounded (Assumption 4.1), and the policy evaluation (Critic training) is accurate.
- **Evidence anchors:**
  - [Abstract] "Theoretical analysis shows that the convergence bound of FedRAC is immune to data heterogeneity..."
  - [Section 4.3 - Global Convergence] Theorem 4.8 and subsequent derivation explicitly show the removal of the $\kappa$ term for FedRAC compared to the baseline.
  - [Corpus] Evidence regarding the specific removal of heterogeneity terms via update order is not explicitly found in the provided corpus neighbors; this appears to be a specific contribution of the target paper.
- **Break condition:** If the "bounded heterogeneity" assumption is violated (e.g., a client has an adversarial or completely unrelated environment), the shared Critic fails to generalize, and the convergence guarantees no longer hold.

### Mechanism 3
- **Claim:** Performance gains are contingent on the Critic's ability to generalize across the federated network (Critic Accuracy).
- **Mechanism:** FedRAC shifts the burden of performance from "local adaptation" to "global generalization." Since the Actor relies on a global Critic, that Critic must effectively estimate values for all local environments. If the Critic network is under-parameterized or trained poorly, it cannot provide accurate gradients for any specific local environment. Empirical results show that with narrow networks (poor generalization), FedRAC loses its advantage because the shared Critic is universally inaccurate, whereas the baseline's local Critics might still be locally accurate.
- **Core assumption:** The Critic network has sufficient capacity (width/depth) to approximate the value functions of all clients simultaneously.
- **Evidence anchors:**
  - [Abstract] "...FedRAC is sensitive to the accuracy of the critic estimation."
  - [Section 5.3 - Comparison of Different Parameterizations] Figure 2 and analysis show FedRAC loses advantage with narrow (2-layer) networks where the baseline might still function.
  - [Corpus] "Quasi-Newton Compatible Actor-Critic..." (Arxiv 2511.09509) discusses compatible function approximation, which reinforces the general importance of Critic accuracy, though not specific to this failure mode.
- **Break condition:** If the Critic is under-fitted (e.g., due to insufficient epochs $E$ or network size), the global guidance is noisy, and the "Actor-first" approach fails to outperform local adaptation.

## Foundational Learning

- **Concept:** **Actor-Critic Methods (specifically PPO)**
  - **Why needed here:** The paper modifies the fundamental update loop of PPO. You must understand the standard "Critic evaluates, Actor improves" alternation to grasp why reversing it is significant.
  - **Quick check question:** In standard PPO, does the Actor update using the reward directly or the Critic's value estimation?
- **Concept:** **Federated Learning (FedAvg)**
  - **Why needed here:** The paper introduces a synchronization barrier (aggregation) into the RL loop. Understanding how local updates are aggregated (averaging weights) is necessary to see why gradient divergence hinders convergence.
  - **Quick check question:** If Client A updates a weight to 1.0 and Client B updates it to -1.0, what is the aggregated weight in FedAvg?
- **Concept:** **Non-IID Data / Environment Heterogeneity**
  - **Why needed here:** This is the core problem the paper solves. You need to distinguish between heterogeneity in data distribution (standard FL) and heterogeneity in environment dynamics (transition functions $P_n$), which is specific to FRL.
  - **Quick check question:** If two autonomous vehicles drive on different road maps, is this data heterogeneity or environment heterogeneity?

## Architecture Onboarding

- **Component map:** Global Server -> Local Client -> Environment
- **Critical path:**
  1.  **Synchronization:** Server sends $\theta_t, w_t$ to clients.
  2.  **Collection:** Clients collect trajectories using $\pi_{\theta_t}$.
  3.  **FedRAC Update (The Swap):**
      - **Step A:** Update Actor $\theta_t \rightarrow \theta_{t+1}$ using the *received* global Critic $w_t$.
      - **Step B:** Update Critic $w_t \rightarrow w_{t+1}$ using the *newly collected* local trajectories.
  4.  **Aggregation:** Upload $\theta_{t+1}, w_{t+1}$ to server; average weights.
- **Design tradeoffs:**
  - **Critic Staleness vs. Gradient Consensus:** You accept a "stale" global Critic (from round $t$) to ensure all Actors move in the same direction. If environments change fast, this staleness hurts; if environments are static but diverse, this consensus helps.
  - **Network Capacity:** You need a larger Critic network than usual to handle the multi-task nature of estimating values for different environments $P_n$.
- **Failure signatures:**
  - **Narrow Networks:** If Critic loss remains high or fluctuates, FedRAC performance will drop below baseline (Figure 2c/2d).
  - **High Policy Lag:** If the KL-divergence between $\pi_{\theta_{t+1}}$ and $\pi_{\theta_t}$ is too large, the old Critic $w_t$ is no longer valid for the new policy, causing instability.
- **First 3 experiments:**
  1.  **Sanity Check (IID):** Run FedRAC on a homogeneous environment (e.g., standard CartPole). It should perform similarly to the baseline, confirming the swap doesn't break standard learning.
  2.  **Heterogeneity Stress Test:** Modify a simple env (e.g., MountainCar) with different action shifts per client. Plot reward vs. communication rounds for Baseline vs. FedRAC to visualize the convergence speed gap.
  3.  **Critic Capacity Ablation:** Fix the heterogeneity level and vary the Critic network size (e.g., 2-layer vs 4-layer). Verify that FedRAC only wins when the Critic is sufficiently deep.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the trade-off between approximation error and aggregation error manifest when utilizing log-linear policies in FedRAC, and does the theoretical preference for this parameterization hold empirically?
- Basis in paper: [explicit] The conclusion states that although theory suggests log-linear policies minimize aggregation error, they may suffer from under-fitting. The authors note, "We leave this investigation to future work."
- Why unresolved: The authors theoretically identify the trade-off but did not empirically validate the performance of log-linear policies against neural network policies regarding this specific error balance.
- What evidence would resolve it: Empirical results comparing the convergence speed and final performance of log-linear policies versus neural network policies under varying levels of data heterogeneity.

### Open Question 2
- Question: Why does the performance gap between FedRAC and the baseline fail to scale monotonically with the level of data heterogeneity, and how do changes in attainable rewards affect this comparison?
- Basis in paper: [inferred] Section 5.4 notes that the authors did not observe an increase in the performance gap as heterogeneity increased. They conjecture this is because "the attainable reward changes as the heterogeneity level changes."
- Why unresolved: The paper provides empirical data showing the lack of scaling but only offers a high-level conjecture for the underlying cause without isolating the variable of "attainable reward" to prove the hypothesis.
- What evidence would resolve it: An analysis that normalizes rewards relative to the optimal attainable reward for each specific heterogeneity level to isolate the algorithm's efficiency from the environment's difficulty.

### Open Question 3
- Question: To what extent does FedRAC's reliance on stricter policy evaluation error constraints limit its applicability compared to the baseline in scenarios with limited local data?
- Basis in paper: [inferred] Remark 4.3 highlights that FedRAC requires satisfying $N^2$ constraints for policy evaluation error (condition 10), which can be "much stricter" than the baseline's $N$ constraints, particularly as heterogeneity increases.
- Why unresolved: While the theoretical analysis proves convergence under these strict conditions, the practical impact of these harder-to-satisfy constraints on sample efficiency or stability in data-scarce environments was not fully quantified.
- What evidence would resolve it: Experiments measuring the frequency of constraint violation and resulting performance degradation when clients are restricted to small local batch sizes or fewer environment steps.

## Limitations

- **Under-parameterized critics eliminate FedRAC's advantage:** The method requires sufficiently large critic networks to generalize across diverse environments, with narrow networks performing worse than the baseline.
- **Theoretical assumptions may not hold in practice:** The convergence guarantees rely on bounded heterogeneity and accurate policy evaluation assumptions that could be violated in highly dynamic or adversarial environments.
- **Staleness sensitivity:** FedRAC's reliance on global critic estimates from previous rounds makes it vulnerable to performance degradation if environment dynamics change rapidly between communication rounds.

## Confidence

- **High Confidence:** The actor-critic update order swap mechanism and its effect on gradient alignment (Mechanism 1) is clearly demonstrated through both theoretical analysis (Lemma 4.6) and empirical results (Figure 2).
- **Medium Confidence:** The theoretical decoupling of convergence bounds from heterogeneity levels (Mechanism 2) is well-supported mathematically, though real-world violations of the bounded heterogeneity assumption could affect practical performance.
- **Medium Confidence:** The critic accuracy dependency (Mechanism 3) is empirically validated, but the threshold at which network capacity becomes insufficient is not precisely characterized.

## Next Checks

1. **Parameter Sensitivity Analysis:** Systematically vary the Critic network width (e.g., 2-layer 32-32 vs 4-layer 128-128) across all environments to quantify the exact threshold where FedRAC loses its advantage.

2. **Staleness Tolerance Test:** Modify the algorithm to use the global Critic from multiple rounds ago (2-3 rounds stale) to empirically measure how temporal inconsistency affects performance under different heterogeneity levels.

3. **Adversarial Environment Stress Test:** Create a client with environment dynamics that are deliberately adversarial to the global Critic (e.g., completely different action space or reward structure) to test the bounded heterogeneity assumption's practical limits.