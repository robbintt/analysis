---
ver: rpa2
title: 'ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class
  Accelerators'
arxiv_id: '2512.09427'
source_url: https://arxiv.org/abs/2512.09427
tags:
- bucket
- odma
- memory
- serving
- predictor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ODMA is an on-demand memory allocation framework for serving LLMs
  on accelerators with random-access-constrained device memory (RACM), such as LPDDR5-based
  Cambricon MLUs. It combines a generation-length predictor with dynamic bucket partitioning
  and a large-bucket safeguard to improve memory utilization and throughput over static
  worst-case pre-allocation.
---

# ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators

## Quick Facts
- arXiv ID: 2512.09427
- Source URL: https://arxiv.org/abs/2512.09427
- Reference count: 40
- Primary result: ODMA raises device-memory utilization from 55.05% to 72.45% on Alpaca and from 42.54% to 61.79% on Google-NQ, boosting throughput by 23% and 27% respectively over static baseline.

## Executive Summary
ODMA is an on-demand memory allocation framework for serving LLMs on accelerators with random-access-constrained device memory (RACM), such as LPDDR5-based Cambricon MLUs. It combines a generation-length predictor with dynamic bucket partitioning and a large-bucket safeguard to improve memory utilization and throughput over static worst-case pre-allocation. ODMA periodically re-learns bucket boundaries from online histograms and routes high-uncertainty or overflowed requests to a reserved large bucket for robustness. Serving DeepSeek-R1-Distill-Qwen-7B on a four-MLU370-X4 node, ODMA raises device-memory utilization from 55.05% to 72.45% on Alpaca and from 42.54% to 61.79% on Google-NQ, and boosts throughput by 23% and 27% respectively over a static baseline. Prediction accuracy improves from 98.60% to 99.55% on Alpaca and from 82.68% to 93.36% on Google-NQ. These results demonstrate that predictor-driven, hardware-aware allocation unlocks efficient LLM serving on RACM accelerators without hardware changes, complementing paging-centric designs for HBM systems.

## Method Summary
ODMA implements a predictor-driven bucketed memory allocator that addresses the unique challenges of serving LLMs on LPDDR-class RACM accelerators. The system uses a DistilBERT-scale encoder to predict generation length and uncertainty score for each request, then routes requests to appropriately sized contiguous memory buckets. Dynamic bucket boundaries are periodically re-learned from online histograms of realized generation lengths, adapting to workload drift. High-uncertainty or overflowed requests fall back to a reserved large bucket, triggering efficient contiguous migration on LPDDR hardware. The framework targets DeepSeek-R1-Distill-Qwen-7B on MLU370-X4 hardware, evaluating against Alpaca and Google-NQ datasets.

## Key Results
- Device memory utilization improves from 55.05% to 72.45% on Alpaca dataset
- Device memory utilization improves from 42.54% to 61.79% on Google-NQ dataset
- Throughput increases by 23% on Alpaca and 27% on Google-NQ compared to static baseline
- Prediction accuracy improves from 98.60% to 99.55% on Alpaca and from 82.68% to 93.36% on Google-NQ

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generation-length prediction with uncertainty-aware routing reduces KV-cache memory waste while maintaining correctness on RACM platforms.
- Mechanism: A lightweight encoder (DistilBERT-scale, 66M parameters) maps prompt features and request metadata to a length estimate L̂ and uncertainty score u. When u exceeds threshold τ, requests bypass regular buckets and allocate from a reserved large bucket, avoiding under-allocation risk.
- Core assumption: Prediction errors are correlated with measurable uncertainty; high-uncertainty requests have higher probability of extreme lengths.
- Evidence anchors:
  - [abstract] "ODMA builds on generation-length prediction while addressing distribution drift and heavy-tailed request lengths"
  - [Section 3.2] "The allocator uses L̂ to select the smallest bucket that can accommodate the predicted KV footprint, while u governs conservatism: high-uncertainty requests are treated as risky and routed directly to the large bucket"
  - [corpus] BucketServe (arXiv 2507.17120) explores similar bucket-based batching but targets HBM GPUs; no direct LPDDR/RACM comparison available.
- Break condition: If uncertainty scores become uncorrelated with actual length variance (e.g., adversarial prompts), the large-bucket reservation may exhaust, causing allocation failures.

### Mechanism 2
- Claim: Dynamic bucket partitioning based on online histograms tracks workload drift and reduces internal fragmentation compared to static boundaries.
- Mechanism: The system maintains a sliding-window histogram of realized generation lengths. Bucket boundaries B = {b₁, ..., b_B} are periodically recomputed as quantiles Q_{p_i}(L) to equalize bucket occupancy. Updates apply only to new allocations; in-flight requests retain original blocks.
- Core assumption: Generation-length distributions drift slowly enough that quantile estimates from recent windows remain valid for near-future requests.
- Evidence anchors:
  - [abstract] "bucket boundaries are periodically re-learned from online histograms"
  - [Section 3.3] "When the window advances, the bucket manager updates B and the allocator applies the new boundaries only to subsequent requests; in-flight requests keep their originally assigned blocks"
  - [corpus] Corpus evidence for dynamic bucket re-partitioning in LLM serving is limited; related work focuses on static bucketing (S3) or paging (vLLM).
- Break condition: Rapid distribution shifts faster than the re-learning window cause boundary staleness, increasing both fragmentation and overflow rates.

### Mechanism 3
- Claim: Contiguous allocation with large-bucket overflow recovery preserves streaming-friendly access patterns essential for LPDDR-class RACM efficiency.
- Mechanism: Regular buckets allocate contiguous blocks sized to quantile-based bounds. On mid-decode overflow, the runtime performs a sequential copy to a larger block from the large bucket and atomically updates references—exploiting LPDDR's streaming bandwidth advantage over random access.
- Core assumption: Overflows are rare enough that copy overhead remains amortized; LPDDR sequential bandwidth significantly exceeds random-access bandwidth for KV-cache access patterns.
- Evidence anchors:
  - [abstract] "high-uncertainty or overflowed requests fall back to a reserved large bucket for robustness"
  - [Section 3.4] "Although migration introduces additional work, the copy is bandwidth-efficient on LPDDR because it is a contiguous, streaming transfer rather than a sequence of random gathers"
  - [Section 2.2] "simulation-based measurements report that with burst length 16, LPDDR4 random traffic at high data-rates reaches only ∼33–35.5% of maximum"
  - [corpus] CXL-SpecKV (arXiv 2512.11920) addresses KV-cache disaggregation but targets datacenter HBM/PCIe; no LPDDR-specific overflow mechanism reported.
- Break condition: If overflow frequency rises substantially (e.g., heavy-tail thicker than predicted), migration overhead may dominate decode latency.

## Foundational Learning

- Concept: Random-Access-Constrained Memory (RACM)
  - Why needed here: LPDDR-class memories exhibit B_rand/B_seq ratios of ~0.3–0.7, making page-level indirection inefficient for KV-cache access. Understanding this constraint motivates contiguous allocation over paging.
  - Quick check question: Given a memory system with B_seq = 300 GB/s and B_rand/B_seq ≈ 0.4, what is the approximate random-access bandwidth, and why does this favor contiguous KV-cache blocks?

- Concept: KV-Cache Memory Scaling
  - Why needed here: KV-cache footprint scales roughly linearly with sequence length and model width; it can dominate device memory. This drives the need for length-aware allocation.
  - Quick check question: For a 7B model with hidden dimension 4096, 32 layers, and sequence length 2048, estimate the approximate KV-cache size in FP16 (ignore compression).

- Concept: Quantile-Based Bucket Partitioning
  - Why needed here: Dynamic buckets use empirical quantiles to balance occupancy across buckets, adapting to workload drift. Understanding quantile selection is essential for tuning bucket counts and update frequency.
  - Quick check question: If bucket boundaries are set at the 25th, 50th, 75th, and 100th percentiles of observed lengths, what fraction of requests should each bucket ideally contain under a stationary distribution?

## Architecture Onboarding

- Component map:
  - Predictor: DistilBERT-scale encoder → (L̂, u) per request; ~300 MB footprint, millisecond latency
  - Task Pool: Buffers tagged requests; exposes bucket tags to scheduler
  - Scheduler: Batches requests with compatible tags; coordinates with Cluster Manager in multi-device setup
  - Allocator: reserve(request_id) → contiguous block; release(request_id) → free list + histogram update
  - Memory Pool: Per-device bucket-specific free lists (min/mid/max granularities); large-bucket reserve
  - Cluster Manager: Tracks device load/memory; assigns batches; synchronizes bucket config

- Critical path: Request arrival → Predictor (L̂, u) → Task Pool insertion → Scheduler batching → Runtime calls reserve() → Allocator selects bucket → Decode proceeds with contiguous KV-cache → On completion, release() updates histogram → Background thread periodically refreshes bucket boundaries

- Design tradeoffs:
  - **Bucket count vs. fragmentation**: More buckets reduce internal fragmentation but increase metadata and complexity; paper uses small set (e.g., min/mid/max + large)
  - **Large-bucket size vs. utilization**: Larger reserve improves robustness but reduces available memory for regular buckets
  - **Update frequency vs. stability**: Faster boundary updates track drift better but may cause oscillation; paper uses periodic background updates

- Failure signatures:
  - **Large-bucket exhaustion**: Many high-uncertainty or overflowed requests → allocation failures; mitigate by increasing reserve or tuning uncertainty threshold
  - **Predictor drift**: Accuracy drops after workload shift → retrain predictor or shorten histogram window
  - **Overflow cascade**: Frequent migrations saturate memory bandwidth → loosen bucket boundaries or increase inflation factor α

- First 3 experiments:
  1. **Baseline comparison**: Run static worst-case pre-allocation vs. ODMA on Alpaca/Google-NQ traces; measure utilization, throughput (TPS), and overflow rate to reproduce reported 23–27% gains.
  2. **Ablation on bucket dynamics**: Disable dynamic re-partitioning (freeze boundaries) and compare utilization/overflow rates to quantify drift-tracking contribution.
  3. **Large-bucket sensitivity**: Vary large-bucket reservation size (e.g., 5%, 10%, 15% of device memory) and measure overflow handling success rate and throughput under heavy-tailed synthetic workloads.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the specific impact of ODMA's mid-decode overflow mechanism on tail latency (e.g., P99) under heavy load?
- Basis in paper: [explicit] Section 3.4 describes the "mid-decode overflow recovery" which requires allocating a new block and performing a sequential copy of the KV-cache.
- Why unresolved: While the paper claims the amortized overhead is small because overflows are rare, it does not quantify the latency spike experienced by individual requests triggering this migration.
- What evidence would resolve it: P99 latency measurements under workload conditions specifically engineered to trigger the large-bucket fallback.

### Open Question 2
- Question: How does the system perform under sudden, massive distribution shifts that render the online histogram lag obsolete?
- Basis in paper: [inferred] Section 3.3 relies on a sliding window to re-learn bucket boundaries, assuming the workload evolves smoothly enough for periodic updates to track drift effectively.
- Why unresolved: The paper does not analyze the "convergence" time required for buckets to adapt to a completely new prompt length profile, nor the memory waste incurred during the transition period.
- What evidence would resolve it: Experiments injecting abrupt concept drift (e.g., switching from short-chat to long-code generation) to measure adaptation lag and temporary utilization loss.

### Open Question 3
- Question: What is the precise crossover point for the $B_{rand}/B_{seq}$ ratio ($\alpha$) at which paged allocation outperforms contiguous bucketing?
- Basis in paper: [inferred] The paper defines RACM using $\alpha$ (Eq. 1) and asserts contiguous allocation is preferable, but does not define the threshold where paging penalties become acceptable.
- Why unresolved: It is unclear if ODMA retains its advantage on future LPDDR generations or hybrid memory systems that might offer slightly better random access performance than current MLU370 hardware.
- What evidence would resolve it: Sensitivity analysis simulating varying $\alpha$ values to identify the specific hardware bandwidth characteristics where ODMA fails against a paging baseline.

## Limitations
- Evaluation limited to single model (DeepSeek-R1-Distill-Qwen-7B) and two datasets (Alpaca, Google-NQ), raising questions about generalizability across architectures and workloads.
- Dynamic bucket re-partitioning assumes slow distribution drift; no analysis of adaptation speed or performance under rapid distribution shifts.
- Large-bucket overflow mechanism lacks empirical quantification of migration overhead frequency and tail latency impact under heavy-tail scenarios.

## Confidence

- **High Confidence**: The core mechanism of combining length prediction with uncertainty-aware routing and contiguous allocation is well-specified and supported by theoretical analysis of LPDDR memory characteristics.
- **Medium Confidence**: The reported improvements in utilization (72.45% vs 55.05% on Alpaca) and throughput (23% gain) are plausible given the mechanism, but depend critically on unknown configuration parameters (bucket counts, update frequency, uncertainty thresholds).
- **Medium Confidence**: The predictor architecture and training approach are reasonable, but actual performance will depend heavily on feature engineering and training data quality, neither of which are fully specified.

## Next Checks

1. **Configuration sensitivity analysis**: Systematically vary bucket count (2-8 buckets), update frequency (every 10K-1M requests), and uncertainty threshold (0.1-0.9) to identify optimal settings and understand parameter robustness across different workload types.

2. **Cross-model generalization test**: Evaluate ODMA with models of varying sizes (1B, 13B, 70B parameters) and architectures (LLaMA, Mistral, Qwen) to verify that predictor accuracy and utilization gains hold beyond the single DeepSeek-R1-Distill-Qwen-7B evaluation.

3. **Heavy-tail workload stress test**: Generate synthetic workloads with controllable tail heaviness (exponential, Pareto distributions with varying shape parameters) to measure overflow rates, migration frequency, and throughput degradation as the gap between predicted and actual lengths widens.