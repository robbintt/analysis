---
ver: rpa2
title: 'Break the Tie: Learning Cluster-Customized Category Relationships for Categorical
  Data Clustering'
arxiv_id: '2511.09049'
source_url: https://arxiv.org/abs/2511.09049
tags:
- distance
- clustering
- data
- category
- categorical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of categorical data clustering
  by learning customized category relationships tailored to different clusters. Traditional
  methods assume fixed topological relationships between categories, limiting adaptability
  to varying cluster structures.
---

# Break the Tie: Learning Cluster-Customized Category Relationships for Categorical Data Clustering

## Quick Facts
- **arXiv ID**: 2511.09049
- **Source URL**: https://arxiv.org/abs/2511.09049
- **Reference count**: 30
- **Primary result**: Proposed DISC method achieves average ranking of 1.25 vs 5.21 for best competitor on 12 real-world datasets

## Executive Summary
This paper addresses categorical data clustering by learning customized category relationships tailored to different clusters, breaking away from traditional fixed-topology approaches. The DISC method constructs subspace relation trees for each cluster based on Conditional Probability Distributions (CPD), enabling more accurate and compact clustering. Experimental results demonstrate significant improvements over state-of-the-art methods, with theoretical guarantees for convergence and Euclidean compatibility.

## Method Summary
DISC learns customized distance metrics by constructing relation trees for each cluster-attribute combination. For each cluster, it builds a fully connected graph of attribute values with edge weights derived from CPD differences, then extracts a Minimum Spanning Tree (MST) to define a deterministic distance metric. The method iteratively optimizes cluster partitions and relation trees through an alternating process, starting from k-modes initialization. The learned metrics are theoretically proven to form linear structures, allowing seamless extension to mixed datasets through Euclidean compatibility.

## Key Results
- Achieves average ranking of 1.25 on 12 UCI datasets, significantly outperforming best competitor at 5.21
- Demonstrates superior clustering accuracy and compactness across multiple evaluation metrics
- Validated on both pure categorical and mixed (categorical+numerical) datasets

## Why This Works (Mechanism)

### Mechanism 1: Cluster-Specific Subspace Relation Trees
Tailoring category relationships for each specific cluster yields higher accuracy than using uniform global relationships. The method constructs a fully connected graph of attribute values per cluster, with edge weights from CPD differences, then extracts an MST as the relation tree. This works when intra-cluster statistics are meaningful and non-uniform.

### Mechanism 2: Joint Distance-Cluster Optimization
Iteratively updating the distance metric based on current partitioning converges to compact clusters. The alternating optimization loop updates cluster partition and centers using current relation trees, then re-infers trees based on new cluster statistics. This aligns the metric with emerging structure when the objective function is tractable.

### Mechanism 3: Euclidean-Compatible Linear Embedding
The learned categorical distance metric is mathematically compatible with Euclidean distance, enabling seamless integration for mixed datasets. The relation tree derived from sorted probabilities always forms a linear structure, effectively mapping categorical values to one-dimensional Euclidean space for direct summation with numerical distances.

## Foundational Learning

- **k-modes Clustering**: Understanding how k-modes defines categorical centers (modes) is prerequisite to understanding DISC's initialization and optimization loop. Quick check: How does k-modes define a "center" for categorical attributes compared to k-means?
- **Minimum Spanning Tree (MST)**: The core novelty is reducing complex relationships into a relation tree (MST) to define unique, deterministic distances. Quick check: Why is extracting an MST necessary for defining a deterministic distance metric, rather than using the fully connected graph directly?
- **Conditional Probability Distribution (CPD)**: Edge weights and final distances are entirely dependent on CPDs (p(v|C)). Quick check: How does the CPD of a value v change if the cluster assignment C changes?

## Architecture Onboarding

- **Component map**: Input -> k-modes initialization -> Relation Tree construction -> Distance metric computation -> Cluster optimization -> Output
- **Critical path**: The "Reinfer T" step (Step 13 in Algorithm 1) is where the method diverges from standard clustering; errors in probability calculation here corrupt the distance metric for the next iteration.
- **Design tradeoffs**: Trades computational efficiency (iterative MST construction overhead) for adaptability (custom metrics per cluster). Complexity is linear O(nlkIE), but constant factors depend on unique values per attribute.
- **Failure signatures**:
  - Uniform Weights: If T produces uniform edge lengths, algorithm degrades to standard Hamming distance
  - Oscillation: If H changes drastically every iteration, T never stabilizes, indicating k may be wrong or data structure is weak
  - Poor initialization: If initial k-modes partition is poor, relation trees may reinforce incorrect structures
- **First 3 experiments**:
  1. Baseline Sanity Check: Run DISC vs. Standard k-modes on "CA" (Car Evaluation) dataset to verify "Customized" strategy dominance shown in Figure 1
  2. Mixed Data Validation: Construct mixed dataset using "DT" (Dermatology) and verify DISC (mixed) outperforms K-Prototypes (KPT)
  3. Convergence Visualization: Plot objective function z over iterations to confirm convergence within approximately 20 iterations as per Figure 2

## Open Questions the Paper Calls Out
- **Open Question 1**: How can DISC be extended to automatically determine the optimal number of clusters (k) without relying on prior knowledge or ground truth labels? The current setup assumes k is known, setting it to ground truth k* for all experiments.
- **Open Question 2**: How does the method perform in high-dimensional spaces or datasets with significant noise, and what modifications are needed for robustness? The method relies on CPDs that could be distorted by noise, and tested datasets have relatively low dimensionality.
- **Open Question 3**: Does computational efficiency and accuracy of relation tree inference degrade for categorical attributes with very high cardinality (large o_r)? The performance of MST construction and CPD-based distances is unverified for attributes containing hundreds or thousands of unique values.

## Limitations
- Theoretical proof of convergence relies on continuous objective function properties that may not fully hold for discrete MST operations
- Method may be prohibitive for high-cardinality categorical attributes or datasets with many clusters due to computational requirements
- Performance is highly dependent on initial k-modes partition quality, with poor initialization potentially leading to suboptimal local minima

## Confidence
- **High Confidence**: Core mechanism of using CPDs to derive edge weights and linear embedding property are well-supported by theoretical proofs and experimental validation
- **Medium Confidence**: Joint optimization framework shows convergence in theory, but practical convergence speed and quality depend heavily on initialization and dataset characteristics
- **Low Confidence**: Scalability claims and computational complexity analysis lack empirical validation on large-scale datasets with high-cardinality attributes

## Next Checks
1. **Convergence Analysis**: Run DISC on "CA" dataset with 50 different k-modes initializations to empirically verify Theorem 3's convergence claims and measure variance in final objective values
2. **Scalability Test**: Construct synthetic datasets with varying attribute cardinalities (o_r = 10, 100, 1000) and measure runtime growth, comparing against theoretical O(nlkIE) complexity
3. **Initialization Sensitivity**: Compare DISC performance starting from random partitions versus k-modes initialization on datasets where ground truth is known, quantifying impact of initialization quality