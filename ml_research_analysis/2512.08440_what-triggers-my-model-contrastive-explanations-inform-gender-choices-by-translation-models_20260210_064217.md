---
ver: rpa2
title: What Triggers my Model? Contrastive Explanations Inform Gender Choices by Translation
  Models
arxiv_id: '2512.08440'
source_url: https://arxiv.org/abs/2512.08440
tags:
- gender
- words
- translation
- bias
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of contrastive explanations and saliency
  attribution to understand how translation models make gender choices, moving beyond
  bias measurement to uncover its origins. The study applies interpretability techniques
  to gender-ambiguous natural data, examining which source words most influence a
  model's gender translation decisions.
---

# What Triggers my Model? Contrastive Explanations Inform Gender Choices by Translation Models

## Quick Facts
- arXiv ID: 2512.08440
- Source URL: https://arxiv.org/abs/2512.08440
- Reference count: 0
- Primary result: Translation models and humans are influenced by similar contextual cues for gender perception, with an 83% overlap between model-identified salient words and human annotations

## Executive Summary
This paper introduces a novel approach to understanding gender bias in machine translation by examining which source words influence a model's gender translation decisions. Rather than simply measuring bias, the authors use contrastive explanations and saliency attribution to uncover the origins of gender choices. By comparing model outputs for gender-opposite translations, they identify which input tokens drive gender decisions and validate these findings against human annotations. The study reveals that while models and humans share similar contextual cues for gender perception, they differ in the types of words considered most influential.

## Method Summary
The authors apply interpretability techniques to gender-ambiguous natural data, using contrastive explanations to compare model outputs for original and gender-swapped translations. They compute saliency attribution scores on the source sentences to identify which tokens influence gender decisions, then test four different attribution approaches to determine the optimal threshold for identifying influential words. The study focuses on a dataset of 60 English sentences with gender-ambiguous target referents, translated to German using the OPUS-MT model. Results are validated against human annotations from 20 annotators per sentence, with linguistic analysis examining part-of-speech distributions and dependency distances between salient words and target referents.

## Key Results
- 83% overlap between model-identified salient words and human annotations when using top 20% of cumulative attribution scores per sentence
- Nouns and verbs are the most influential parts of speech for model gender decisions
- Proper nouns and adjectives dominate human perception of gender-relevant words
- Translation models and humans are influenced by similar contextual cues for gender perception, though with different linguistic feature priorities

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Saliency Attribution
Comparing model outputs for gender-opposite translations reveals which input tokens influence gender decisions through L1-normalized attribution scores computed on contrastive pairs.

### Mechanism 2: Cumulative Threshold Selection
Selecting tokens contributing to top 20% of cumulative attribution scores maximizes overlap with human gender perception by identifying minimum subsets whose cumulative scores reach threshold.

### Mechanism 3: Part-of-Speech Influence Pattern
POS tagging of salient tokens reveals systematic differences in what linguistic features drive model vs. human gender perception, with nouns/verbs dominating for models versus proper nouns/adjectives for humans.

## Foundational Learning

**Concept: Contrastive Explanations**
- Why needed here: The core methodology relies on comparing model behavior for minimally different outputs (gender-opposite translations) rather than single predictions
- Quick check question: Given a sentence translated as both "Der Arzt" (masc.) and "Die Ärztin" (fem.), what contrastive explanation reveals which source words drove each output?

**Concept: Saliency Attribution with Normalization**
- Why needed here: Raw attribution scores require L1 normalization to sum to 1, enabling comparison across sentences of different lengths
- Quick check question: If two tokens have attribution scores 0.08 and 0.04 in a sentence where remaining tokens sum to 0.88, what preprocessing step ensures these scores are comparable across sentences?

**Concept: Dependency Distance**
- Why needed here: Measures grammatical proximity between salient words and target referents, revealing whether models use local (distance 1-2) or global context (distance 3+)
- Quick check question: Why might a model's reliance on distance-1 words vs. a human's use of distance-3 words suggest different processing strategies?

## Architecture Onboarding

**Component map:**
OPUS-MT EN-DE -> inseq toolkit -> SpaCy -> Human annotation dataset

**Critical path:**
1. Translate EN sentences → DE using OPUS-MT (88.3% defaulted to masculine in study)
2. Manually create contrastive translations (gender-swapped: der→die, Arzt→Ärztin)
3. Compute saliency attribution via inseq on contrastive pairs
4. Preprocess: L1 normalize → remove target referent, punctuation, stopwords → merge subwords
5. Apply Approach 4 (cumulative top 20% threshold)
6. Compare with human annotations → compute overlap percentage
7. Run linguistic analysis (POS distribution, dependency distance)

**Design tradeoffs:**
- Approach 4 (cumulative) vs. Approach 1 (fixed top X%): Cumulative adapts to sentence length but requires per-sentence computation; fixed percentage is simpler but less precise
- Preprocessing choices: Removing stopwords focuses on content words but may miss discourse markers; subword merging aggregates scores but may obscure morphological signals
- Model selection: OPUS-MT enables open-source analysis but results may not generalize to commercial LLMs

**Failure signatures:**
- Model-human overlap drops below 70% → threshold too permissive/restrictive or annotation quality issue
- High proportion of outliers (unrecognized tokens like `<unk>`, numbers) → tokenization artifacts
- Skewed dependency distances (all distance 1) → model over-relying on local syntax

**First 3 experiments:**
1. Reproduce threshold analysis on the 60-sentence dataset to validate the 83% overlap at top 20% cumulative threshold
2. Test alternative threshold approaches (5%, 10%, 15%, 25%, 30%) to identify performance degradation curve
3. Extend to a different language pair (EN-FR or EN-ES) to assess whether POS influence patterns (nouns/verbs dominant) generalize across target languages

## Open Questions the Paper Calls Out

### Open Question 1
Do the observed gender saliency patterns and the 83% model-human overlap generalize across different model architectures (e.g., LLMs) and diverse language pairs? The authors explicitly state they "tested only one model (OPUS-MT) and one language pair (EN–DE)" and suggest future work could compare different models and languages.

### Open Question 2
Can faithfulness metrics like Plausibility Evaluation of Context Reliance (PECORE) provide a more robust quantification of model-human agreement than the attribution thresholds used in this study? The authors note that normalized values prevent direct comparison across intra-sentential tokens for Approach 3 and aim to quantify model-human comparison by different means, such as using PECORE.

### Open Question 3
How does the linguistic discrepancy between model saliency (favoring nouns/verbs) and human perception (favoring proper nouns/adjectives) affect the quality of gender-fair translation interventions? While the overlap is high, the specific differences in Part-of-Speech reliance suggest that models may be using different cues than humans to resolve ambiguity, potentially undermining human-in-the-loop debiasing strategies.

## Limitations

- Results are based on a single translation model (OPUS-MT EN-DE) and may not generalize to other architectures or commercial LLMs
- Human annotation quality depends on 68% inter-annotator agreement threshold, introducing uncertainty about ground truth
- Preprocessing decisions (removing stopwords, punctuation, target referents) may systematically underestimate certain types of gender cues

## Confidence

**High Confidence Claims:**
- The contrastive explanation methodology is technically sound and the attribution computation process is reproducible
- The 83% overlap between model-identified salient words and human annotations is well-documented for this specific experimental setup
- POS differences between model and human salient words (nouns/verbs vs proper nouns/adjectives) are clearly demonstrated in the analyzed dataset

**Medium Confidence Claims:**
- The 20% cumulative threshold represents an optimal balance between precision and recall for salient word identification across different datasets
- Translation models and humans are influenced by similar contextual cues for gender perception, though with different linguistic feature priorities
- Dependency distance analysis reveals meaningful differences in local vs. global context usage between models and humans

**Low Confidence Claims:**
- Findings generalize to other language pairs beyond EN-DE
- The same POS influence patterns would hold for commercial LLMs or models with different training objectives
- The threshold selection methodology would yield identical results on datasets with different characteristics

## Next Checks

1. **Cross-Dataset Threshold Validation**: Test the top 20% cumulative threshold approach on 2-3 additional datasets with varying sentence lengths and ambiguity levels to determine if this threshold generalizes or requires dataset-specific calibration.

2. **Commercial Model Comparison**: Apply the same methodology to at least two commercial translation models (e.g., GPT-4, Gemini) to assess whether POS influence patterns (nouns/verbs vs proper nouns/adjectives) and dependency distance preferences remain consistent across different model architectures.

3. **Stopword Impact Analysis**: Repeat the salient word identification with and without stopword removal to quantify their actual contribution to model gender decisions and assess whether their exclusion systematically underestimates certain types of gender cues.