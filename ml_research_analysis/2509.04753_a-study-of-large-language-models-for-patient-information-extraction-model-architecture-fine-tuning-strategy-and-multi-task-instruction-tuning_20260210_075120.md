---
ver: rpa2
title: 'A Study of Large Language Models for Patient Information Extraction: Model
  Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning'
arxiv_id: '2509.04753'
source_url: https://arxiv.org/abs/2509.04753
tags:
- llms
- clinical
- fine-tuning
- extraction
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates large language models (LLMs)
  for patient information extraction from clinical narratives, focusing on model architecture,
  fine-tuning strategies, and multi-task instruction tuning. The researchers benchmarked
  both encoder-based models (BERT, GatorTron) and decoder-based generative models
  (GatorTronGPT, Llama 3.1, GatorTronLlama) across five diverse clinical datasets
  for clinical concept and relation extraction tasks.
---

# A Study of Large Language Models for Patient Information Extraction: Model Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning

## Quick Facts
- arXiv ID: 2509.04753
- Source URL: https://arxiv.org/abs/2509.04753
- Reference count: 0
- Primary result: Decoder-based LLMs with prompt-based PEFT achieved best performance for clinical concept extraction (F1 0.8981) and relation extraction (F1 0.8978)

## Executive Summary
This study systematically evaluates large language models for patient information extraction from clinical narratives, comparing encoder-based models (BERT, GatorTron) with decoder-based generative models (GatorTronGPT, Llama 3.1, GatorTronLlama) across five diverse clinical datasets. The research demonstrates that decoder-based LLMs with parameter-efficient fine-tuning (LoRA) and multi-task instruction tuning provide superior performance and efficiency for clinical concept and relation extraction tasks. Multi-task instruction tuning significantly enhances zero-shot and few-shot learning capabilities, enabling comparable performance with only 20% of the training data.

## Method Summary
The study benchmarks encoder-only models (BERT, GatorTron) with traditional token classification and full fine-tuning against decoder-only models (Llama 3.1-8B, GatorTronLlama) using prompt-based parameter-efficient fine-tuning with LoRA adapters. Five clinical datasets were used: 2010 i2b2, 2018 n2c2, 2022 n2c2, RadGraph, and UFHealth. Models were evaluated on clinical concept extraction and relation extraction tasks using micro-averaged F1 score with strict matching. Multi-task instruction tuning was implemented through leave-one-dataset-out evaluation with unified text-to-text prompts. Training used NVIDIA NeMo framework with 4× A100-80G GPUs.

## Key Results
- Decoder-based LLMs (Llama 3.1, GatorTronLlama) achieved highest average F1 scores of 0.8964 for clinical concept extraction and 0.8981 for relation extraction
- Prompt-based PEFT (LoRA) demonstrated superior performance and efficiency over traditional fine-tuning, with <1% parameter updates and 6× faster training
- Multi-task instruction tuning significantly improved zero-shot and few-shot learning, with models using only 20% of training data achieving comparable performance to full fine-tuning
- GatorTronLlama-8B achieved highest average F1-scores of 0.3596 for concept extraction in multi-task setting, outperforming non-instruction tuned counterpart by 21.4%

## Why This Works (Mechanism)

### Mechanism 1
Decoder-based generative LLMs outperform encoder-based models for clinical information extraction when using prompt-based learning. Decoder models reformulate extraction tasks into unified text-to-text generation, eliminating task-specific classification heads. This allows a single model to handle both concept extraction (CE) and relation extraction (RE) through natural language instructions, enabling better knowledge transfer across tasks.

### Mechanism 2
Prompt-based PEFT (LoRA) achieves comparable or superior performance to full fine-tuning while updating <1% of parameters. LoRA injects trainable low-rank matrices into attention layers. The base model weights remain frozen while adapters learn task-specific modifications, reducing overfitting risk and computational cost.

### Mechanism 3
Multi-task instruction tuning dramatically improves zero-shot and few-shot transfer to unseen datasets. Training on multiple datasets with varied schemas forces the model to learn generalizable extraction patterns rather than dataset-specific heuristics. The shared instruction format enables cross-task knowledge transfer.

## Foundational Learning

- **Encoder-only vs Decoder-only Transformer Architectures**: Why needed here - The paper's central comparison requires understanding that encoder models use bidirectional attention with classification heads, while decoder models use autoregressive generation guided by prompts. Quick check - Can you explain why a decoder model can handle both CE and RE without architecture changes while an encoder model requires different classification heads?

- **LoRA (Low-Rank Adaptation)**: Why needed here - PEFT is the recommended fine-tuning strategy; understanding LoRA's rank decomposition is essential for configuration. Quick check - What happens to the LoRA adapter weights during inference, and why doesn't this add latency?

- **Instruction Tuning and Zero/Few-Shot Learning**: Why needed here - The multi-task instruction tuning experiment is the key generalization mechanism; understanding leave-one-dataset-out evaluation is critical. Quick check - If a model trained on datasets A, B, C achieves F1=0.35 on held-out dataset D in zero-shot setting, what does this indicate about transfer learning?

## Architecture Onboarding

- Component map: Clinical Note Input → Instruction Template + Prompt Engineering → Decoder-based LLM (Llama 3.1-8B or GatorTronLlama-8B recommended) → LoRA Adapters (rank=256, dropout=0.2, ~0.8% trainable params) → Text Generation Output → Post-processing for structured extraction

- Critical path:
  1. Prepare data in unified instruction format (see Supplementary Table S1 for templates)
  2. Select GatorTronLlama-8B for clinical domain or Llama 3.1-8B for general use
  3. Configure LoRA with rank=256, dropout=0.2, learning_rate=1e-4
  4. For multi-task: combine 3+ datasets with mixed task instructions
  5. Evaluate using leave-one-dataset-out with 5/10/20/50 shot samples

- Design tradeoffs:
  - **Full fine-tuning vs PEFT**: PEFT preferred for models >1B parameters; full fine-tuning may work better for smaller models with abundant data
  - **Single-task vs Multi-task**: Multi-task sacrifices ~1-2% on in-distribution performance for 10-35% gains on out-of-distribution
  - **General vs Clinical pre-training**: GatorTronLlama (clinical) showed 0.2-0.5% improvement over Llama 3.1; consider cost of continued pre-training

- Failure signatures:
  - Zero-shot F1 near 0 without instruction tuning (expected baseline ~0.01-0.02)
  - Boundary errors on nested entities (e.g., "Currently unemployed" vs "unemployed")
  - Duplicate span generation in decoder output
  - Relation extraction errors on RadGraph dataset (consistently lowest scores across all models, 0.70-0.85)
  - Domain shift errors when annotation schemas differ significantly

- First 3 experiments:
  1. **Baseline replication**: Fine-tune GatorTronLlama-8B with LoRA on 2018 n2c2 for CE only; target F1 >0.90
  2. **PEFT vs Full comparison**: Compare LoRA vs full fine-tuning on GatorTron-base (345M); verify paper finding that smaller models may favor full fine-tuning
  3. **Multi-task transfer test**: Train on 2018 n2c2 + 2022 n2c2 + RadGraph; evaluate zero-shot and 20-shot on UFHealth held-out set; target zero-shot F1 >0.30

## Open Questions the Paper Calls Out
None

## Limitations

- Architecture transfer gap: The comparison conflates architecture choice with task formulation strategy, making it difficult to isolate whether decoder models would maintain advantages under identical classification-based formulations.
- Generalizability concerns: Performance on datasets with substantially different annotation schemas or nested entity structures remains unknown, with RadGraph showing consistently lower scores (0.70-0.85 F1).
- PEFT configuration dependency: The claimed superiority of LoRA-based PEFT may be configuration-dependent, with smaller models (<500M parameters) potentially favoring full fine-tuning.

## Confidence

**High confidence**: Decoder-based LLMs with prompt-based PEFT outperform encoder-based models with traditional fine-tuning on benchmark clinical IE tasks.

**Medium confidence**: Multi-task instruction tuning provides consistent zero-shot and few-shot transfer gains across diverse clinical datasets.

**Low confidence**: The proposed architecture represents the optimal approach for all clinical IE scenarios.

## Next Checks

**Validation Check 1**: Evaluate the decoder-based LLM approach on a clinical dataset with fundamentally different annotation schema (e.g., pathology reports with nested cellular structures) to test true cross-domain transfer capability.

**Validation Check 2**: Systematically vary LoRA rank and dropout parameters across model scales (345M, 1B, 8B parameters) on multiple clinical datasets to determine if the claimed PEFT superiority holds across the full parameter space.

**Validation Check 3**: Conduct ablation studies removing instruction templates from the multi-task training pipeline to isolate whether architectural advantages or instruction-based formulation drives the performance gains.