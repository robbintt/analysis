---
ver: rpa2
title: Multi-Record Web Page Information Extraction From News Websites
arxiv_id: '2502.14625'
source_url: https://arxiv.org/abs/2502.14625
tags:
- pages
- page
- dataset
- information
- record
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of extracting structured information
  from multi-record web pages, particularly news list pages, which are underrepresented
  in existing research and datasets. The authors created the first large-scale dataset
  for this task in Russian, comprising 13,120 web pages from 278 news websites with
  257,595 records.
---

# Multi-Record Web Page Information Extraction From News Websites

## Quick Facts
- arXiv ID: 2502.14625
- Source URL: https://arxiv.org/abs/2502.14625
- Reference count: 10
- Dataset: 13,120 pages from 278 Russian news websites with 257,595 records

## Executive Summary
This paper introduces the first large-scale dataset for multi-record web page information extraction, focusing on news list pages. The authors propose a three-stage pipeline using MarkupLM for segmentation, classification, and matching of record attributes. Experimental results demonstrate that the sequential pipeline with record context achieves high precision and recall, with F1 scores of 0.916 for title, 0.82 for tag, and 0.85 for date.

## Method Summary
The method employs a sequential pipeline using MarkupLM, a neural model designed for markup-language-based documents. First, segmentation identifies record boundaries by classifying DOM nodes as BEGIN or OUT labels. Then, classification assigns attribute labels (title, tag, date) to nodes within each segmented record. Finally, matching associates attributes with their parent records using XPath prefix alignment. The pipeline processes HTML content after removing script tags and translating Russian text to English for compatibility with pre-trained models.

## Key Results
- Sequential pipeline with record context achieves F1 scores of 0.916 (title), 0.82 (tag), and 0.85 (date)
- Record context significantly improves classification accuracy over page context (title F1: 0.99 vs. 0.79)
- MarkupLM segmentation outperforms classical MDR methods (F1: 0.925 vs. 0.465)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constrained classification scope (record context vs. full page context) improves attribute extraction accuracy.
- Mechanism: By first segmenting the page into discrete record boundaries, subsequent classification operates only on relevant DOM subtrees, reducing false positives from similar content in adjacent records.
- Core assumption: Record boundaries can be reliably detected before classification, and attributes are contained within their respective record's DOM subtree.
- Evidence anchors:
  - [abstract]: "Experimental results show that the sequential pipeline with record context achieves high precision and recall"
  - [section V.D, Table IV]: Record context F1 for title = 0.99 vs. page context F1 = 0.79; date F1 = 0.88 vs. 0.73
  - [corpus]: Weak direct corpus support—neighbors focus on LLM agents and multilingual extraction, not context window effects.
- Break condition: If record boundaries frequently cross-cut DOM subtrees (e.g., shared footer elements contain dates), segmentation errors will cascade into classification failures.

### Mechanism 2
- Claim: MarkupLM's joint encoding of text content and DOM structure enables superior segmentation over heuristic tree alignment methods.
- Mechanism: MarkupLM encodes both node text and positional XPath information, allowing the model to learn structural patterns that indicate record boundaries (BEGIN labels) versus non-boundary nodes (OUT labels).
- Core assumption: The pre-trained MarkupLM model's understanding of HTML structure transfers to the news list page domain with sufficient fine-tuning data.
- Evidence anchors:
  - [section V.C, Table III]: MarkupLM segmentation F1 = 0.925, ARI = 0.802; MDR F1 = 0.465, ARI = 0.437
  - [section II]: "The main feature of this model is encoding text and location of node together"
  - [corpus]: ArXiv:2110.08518 (original MarkupLM paper) is cited; corpus neighbors do not contest this finding.
- Break condition: If target websites use radically different DOM structures than training data (e.g., single-page application frameworks with dynamically generated class names), segmentation quality may degrade significantly.

### Mechanism 3
- Claim: Sequential pipeline architecture achieves higher recall than parallel pipeline by eliminating the independent matching stage.
- Mechanism: In the sequential pipeline, classification is performed directly within segmented record boundaries, so attributes are inherently associated with their parent record.
- Core assumption: Segmentation errors are less costly than matching errors, and the sequential dependency does not create unacceptable latency for production use.
- Evidence anchors:
  - [section V.G, Table V]: Sequential pipeline recall for title = 0.974 vs. parallel = 0.889; sequential recall for date = 0.893 vs. parallel = 0.826
  - [section V.G]: "The sequential pipeline outperforms the parallel pipeline in recall... making it the preferred method due to its robustness and consistency"
  - [corpus]: No direct corpus comparison; WebLists (ArXiv:2504.12682) uses LLM agents rather than pipeline comparison.
- Break condition: If segmentation misses records entirely (FN in segmentation), those records cannot be recovered—parallel pipeline's classification stage might still detect attributes that matching could partially salvage.

## Foundational Learning

- Concept: **Token classification with BIO-style labeling (BEGIN/OUT)**
  - Why needed here: Segmentation is formulated as node-level classification where each DOM node receives a label indicating whether it starts a new record.
  - Quick check question: Given a DOM subtree with 3 sibling `<div>` elements each containing a news item, which nodes should receive BEGIN vs. OUT labels?

- Concept: **XPath expressions and DOM tree traversal**
  - Why needed here: The matching algorithm relies on positional XPath to associate classified attributes with segmented records via prefix matching.
  - Quick check question: If an attribute node has xpath `/html/body/div[2]/span[3]/text()` and the nearest BEGIN node has xpath `/html/body/div[2]`, will the prefix matching algorithm correctly associate them?

- Concept: **MarkupLM architecture (text + markup joint encoding)**
  - Why needed here: Understanding how MarkupLM differs from standard BERT is necessary for debugging extraction failures and determining if fine-tuning data is sufficient.
  - Quick check question: What additional input features does MarkupLM receive beyond token embeddings, and how might these features help distinguish a news title from a navigation menu item?

## Architecture Onboarding

- Component map:
  - Raw HTML -> Clean HTML (remove script tags) -> DOM tree extraction -> MarkupLM tokenization with XPath encoding -> Segmentation inference (BEGIN/OUT labels) -> Record segmentation -> Classification inference (title/tag/date labels) -> (Parallel pipeline only) XPath prefix matching -> Structured output

- Critical path:
  1. Raw HTML → Clean HTML (remove `<script>` tags)
  2. Clean HTML → DOM tree extraction → MarkupLM tokenization with XPath encoding
  3. Segmentation inference → Record boundary nodes marked BEGIN
  4. For each record segment → Classification inference → Attribute labels
  5. (Parallel only) Attribute XPaths → Prefix match to record boundaries → Structured output (JSON)

- Design tradeoffs:
  - **Sequential vs. Parallel**: Sequential offers higher recall (0.974 vs. 0.889 for title) but requires segmentation before classification; parallel allows independent stage development and potential parallelization
  - **Record vs. Page context**: Record context improves classification (title F1: 0.99 vs. 0.79) but requires accurate segmentation first
  - **Translation dependency**: Russian pages were translated to English for pre-trained model compatibility—this adds latency and potential translation artifacts

- Failure signatures:
  - Low segmentation F1 with high classification F1 → Segmentation model underfitting or DOM structure mismatch with training distribution
  - High precision/low recall in final output → Overly conservative segmentation thresholds or matching algorithm rejecting valid associations
  - Poor tag extraction relative to title/date → Multi-valued attribute handling issues; tags often span multiple nodes per record

- First 3 experiments:
  1. **Baseline reproduction**: Run the provided sequential pipeline on the test split (25% of dataset); verify segmentation F1 ≈ 0.925 and title F1 ≈ 0.916
  2. **Cross-domain transfer test**: Apply the trained model to e-commerce list pages (e.g., from PLATE dataset) without fine-tuning; measure segmentation degradation to assess generalization claim from Section III ("methods should generalize beyond the news domain")
  3. **Ablation on translation**: For a subset of Russian pages, compare extraction quality with and without English translation preprocessing; quantify translation-induced errors vs. model vocabulary limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the extraction performance differ for low-frequency attributes like "author" and "short text" compared to the high-frequency attributes (title, tag, date) tested in this study?
- Basis in paper: [explicit] The authors state in the Conclusion: "In future work, we are going to study the extraction of additional attributes from our dataset, such as short text, short title, author, and time."
- Why unresolved: The current experimental evaluation explicitly limited scope to the three most frequent attributes (title, tag, date) to validate the pipeline, leaving the efficacy on sparse data unconfirmed.
- What evidence would resolve it: Reporting F1 scores for the "author" and "time" attributes using the proposed sequential pipeline on the existing test set.

### Open Question 2
- Question: To what extent does the reliance on machine translation (Russian to English) impact the accuracy of MarkupLM feature extraction compared to processing native English HTML?
- Basis in paper: [inferred] The methodology notes that pages were translated into English "to be able to use pre-trained models," implying a potential dependency or loss of semantic nuance inherent to the translation step.
- Why unresolved: It is unclear if translation errors introduce noise into the node text features that degrade the model's classification capabilities compared to a natively English dataset.
- What evidence would resolve it: A comparative analysis of extraction results on a natively English dataset versus the translated Russian dataset using the same model architecture.

### Open Question 3
- Question: Can the proposed sequential pipeline generalize to other domains, such as e-commerce or real estate, which possess different HTML structural patterns than news websites?
- Basis in paper: [inferred] Section III (Problem Definition) sets a constraint that "methods should generalize beyond the news domain," but the dataset and experiments are restricted exclusively to Russian news sites.
- Why unresolved: While the authors claim the method provides a basis for "other forms of semi-structured web content," they provide no empirical evidence of performance outside the news vertical.
- What evidence would resolve it: Zero-shot or fine-tuning evaluation results of the model on a multi-record dataset from a distinct domain, such as the PLATE (e-commerce) dataset.

## Limitations
- Translation-induced artifacts: The pipeline relies on translating Russian HTML content to English, but the paper does not specify the translation method or quantify translation errors.
- Domain specificity of segmentation model: The high segmentation performance may not transfer to structurally different list pages outside the news domain.
- XPath dependency fragility: The matching algorithm uses XPath prefix matching, which assumes stable XPath structures and may fail on dynamically generated content.

## Confidence
- **High confidence** in sequential pipeline superiority claims: Experimental results show clear, consistent improvements in recall and F1 scores when using record context over page context.
- **Medium confidence** in generalization claims: While the paper asserts methods generalize beyond news, this is based on theoretical argument rather than empirical validation on non-news domains.
- **Low confidence** in translation neutrality: The paper does not address whether translation affects attribute extraction quality or whether the model would perform differently on native English news pages.

## Next Checks
1. **Cross-domain segmentation transfer test**: Apply the trained segmentation model to list pages from non-news domains (e-commerce, directories, academic publications) and measure F1 degradation. This validates the generalization claim beyond theoretical assertion.
2. **Translation impact quantification**: Extract attributes from a subset of Russian pages both with and without English translation (using multilingual MarkupLM if available). Compare extraction quality to measure translation-induced error rates.
3. **XPath stability stress test**: Test the matching algorithm on pages with dynamic class names and JavaScript-generated DOM structures. Measure how often xpath prefix matching fails versus attribute association accuracy on static HTML.