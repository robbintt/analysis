---
ver: rpa2
title: LM Agents for Coordinating Multi-User Information Gathering
arxiv_id: '2502.12328'
source_url: https://arxiv.org/abs/2502.12328
tags:
- information
- user
- about
- have
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PEOPLE JOIN, a benchmark for evaluating LM-mediated
  collaborative problem solving. Given a user request, PEOPLE JOIN agents must identify
  teammates who might be able to assist, converse with these teammates to gather information,
  and finally compile a useful answer or summary for the original user.
---

# LM Agents for Coordinating Multi-User Information Gathering

## Quick Facts
- arXiv ID: 2502.12328
- Source URL: https://arxiv.org/abs/2502.12328
- Reference count: 23
- Primary result: LM-powered agents achieve Match scores of 24.4-54.8 across QA and summarization tasks, struggling with multi-user coordination

## Executive Summary
This paper introduces PEOPLE JOIN, a benchmark for evaluating LM-mediated collaborative problem solving where agents must identify teammates, converse to gather distributed information, and compile answers for users. The benchmark includes two domains: PEOPLE JOIN-QA for questions about tabular data and PEOPLE JOIN-DocCreation for document creation tasks, both adapted from existing NLP benchmarks but distributed across synthetic organizations of 2-20 users. Reference agents using Phi-3-medium, GPT-4-turbo, and GPT-4o show that while LM-powered agents can coordinate across multiple users, they struggle with precision in contact selection and query formulation, achieving Match scores ranging from 24.4 to 54.8.

## Method Summary
The benchmark uses synthetic organizations derived from SPIDER (200 databases → organizations of 2-20 users) and MultiNews (67 organizations, 200 test instances). Agents operate via an event-based reactive loop using ReAct-style prompting with tools for document search (BM25, top-3), people search (BM25, top-10), messaging, and reflection. Four hand-annotated exemplars per domain guide the agents. Evaluation uses LLM-based Match scoring (0/50/100) for QA and ROUGE-L/G-Eval for summarization, with efficiency metrics like message count and people contacted. User simulators are implemented using GPT-4-turbo prompted with each collaborator's documents and persona descriptions.

## Key Results
- Agents achieve Match scores of 24.4-54.8 across different models and tasks
- People-Precision averages 0.61, indicating challenges in correctly identifying relevant collaborators
- Reflection improves QA performance (54.8 vs 48.0) but not summarization
- 25% of failures stem from overly-specific queries causing users to incorrectly deny having information
- Human evaluation of simulator fidelity shows similar performance to LLM-based evaluation (Match 50 vs 44)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReAct-style interleaved reasoning and action enables adaptive multi-turn information gathering
- Mechanism: The agent operates in an event-triggered loop where each user message initiates a sequence of: (1) reflection/chain-of-thought reasoning about current state and next steps, (2) action selection (document search, people search, or messaging), (3) observation incorporation, repeating until task completion. This allows dynamic replanning based on collaborator responses.
- Core assumption: The language model can maintain coherent multi-step plans across conversation turns and adapt when initial contacts prove unproductive.
- Evidence anchors:
  - [abstract] "PeopleJoin agents must identify teammates who might be able to assist, converse with these teammates to gather information, and finally compile a useful answer"
  - [section 4] "We consider an event-based reactive agent... following ReAct-style prompting loop (Yao et al., 2023), taking actions, making observations, and performing reflection"
  - [corpus] Related work on task decomposition (Khot et al., 2022) supports componentized problem-solving, though corpus lacks direct replications of this specific multi-user coordination pattern.
- Break condition: When reflection steps increase token usage without improving person-selection precision (P-Prec stayed at 0.61 even with reflection), or when orchestration errors occur (10% of failures in analysis).

### Mechanism 2
- Claim: Two-stage retrieval (document search → people search → messaging) reduces communication overhead while maintaining information coverage
- Mechanism: Agents first check the initiating user's documents via BM25 search. If insufficient, they query a people search interface with imprecise expertise descriptions, then selectively message identified collaborators. This filters contacts before engaging humans.
- Core assumption: BM25 indices on document content and expertise profiles provide sufficient signal to identify relevant information holders, even with incomplete descriptions.
- Evidence anchors:
  - [abstract] "Agents have direct access to the initiating user's documents, and can engage in conversations with other users to gather relevant information"
  - [section 4.1] "People Retrieval: agents can search through a repository of employee profiles... descriptions are retrieved using a standard BM25 index"
  - [corpus] Multi-hop QA literature (Welbl et al., 2018) supports iterative retrieval, but corpus shows no direct evidence for BM25 efficacy in organizational expertise matching.
- Break condition: When expertise descriptions are too imprecise (the paper notes they "by design sometimes result in imprecise or incomplete descriptions"), causing low P-Prec (0.61) or missed redirects (38% accuracy on redirection category).

### Mechanism 3
- Claim: Synthetic user simulators enable scalable evaluation while preserving task validity
- Mechanism: Each collaborator is simulated by prompting an LLM with their assigned documents and persona description. The simulator responds to agent queries based only on information in their document set, enabling automated evaluation of multi-turn coordination.
- Core assumption: GPT-4-turbo prompted with document access can adequately simulate human collaborator responses, including appropriate refusals and redirects.
- Evidence anchors:
  - [section 6] "All experiments use a gpt-4-turbo model... prompted with each collaborator's description and document collection, to implement these simulators"
  - [section 6.3] Human evaluation (n=100) showed similar performance: Match 50 vs 44, MsgCnt 10.0 vs 9.3, suggesting simulation validity
  - [corpus] Corpus lacks independent validation of LLM-based user simulation fidelity; this remains an open methodological question.
- Break condition: When simulators produce responses that humans would not (e.g., overly terse answers, missing clarification questions), potentially inflating or deflating agent performance metrics.

## Foundational Learning

- Concept: **ReAct prompting (Reasoning + Acting)**
  - Why needed here: The core agent loop requires interleaving "thought" steps with tool calls—agents must reason about whom to contact before messaging, and interpret responses before next actions.
  - Quick check question: Given a partial observation "Bhushan says: I don't have pet types, only pet IDs," can you write a reflection step that identifies the missing information and next action?

- Concept: **Multi-hop information synthesis**
  - Why needed here: Tasks require joining information across users (e.g., pet IDs from one user → pet types from another → student names from a third), analogous to database joins but via natural language queries.
  - Quick check question: If User A has {student_id: 00158, pet_id: PET027} and User B has {pet_id: PET027, type: Dog}, what question should the agent ask User C to get student names?

- Concept: **Organizational navigation under partial observability**
  - Why needed here: Agents receive only hints about who knows what ("Chen might know about student demographics"), requiring probabilistic contact selection and handling of redirects.
  - Quick check question: An agent's people search returns three users with overlapping but incomplete expertise descriptions. What strategy should determine contact order?

## Architecture Onboarding

- Component map: User message → Agent Core (ReAct loop) → Tools (search_documents, search_relevant_people, send_message, resolve_person, finish, send_session_completed, reflection) → User simulators → LLM evaluator → Performance metrics
- Critical path:
  1. Parse incoming user message → trigger agent loop
  2. Resolve primary user → search their documents
  3. If insufficient, search people → resolve person IDs → send messages
  4. Collect responses → synthesize answer → send to initiator → mark session complete
  5. Each step may include reflection before action
- Design tradeoffs:
  - **Exemplar count vs. generalization**: Paper uses only 4 exemplars per domain; more might improve performance but risk overfitting to specific patterns
  - **Reflection overhead**: Reflection improved QA (54.8 vs 48.0) but not summarization—costs tokens without universal benefit
  - **Simulator fidelity vs. reproducibility**: GPT-4 simulators enable scalable eval but may not capture human response variability
  - **Message granularity**: Asking precise questions reduces back-and-forth but risks being "overly-specific" (25% of failures)
- Failure signatures:
  - **Contact undershoot**: Agent stops after partial information, tells user it couldn't find everything (20% of failures)
  - **Query malformation**: Questions too specific ("Do you have information about students with dogs?") cause false negatives (25% of failures)
  - **Orchestration break**: Agent predicts invalid tool calls or skips search entirely (10% of failures)
  - **Redirect mishandling**: Agent doesn't follow suggested contacts, or loops without progressing (visible in 38% accuracy on redirection category)
- First 3 experiments:
  1. **Baseline replication**: Run Reactive agent on 50 PeopleJoin-QA instances with GPT-4-turbo; verify Match ~54.8, P-Prec ~0.61, analyze failure mode distribution
  2. **Ablation: People search quality**: Replace BM25 people search with oracle retrieval (ground-truth relevant users always in top-3); measure upper bound on P-Prec and Match improvement
  3. **Intervention: Query reformulation**: Add a step where, if a user says "I don't have that," the agent generates 2 alternative phrasings before moving to next contact; measure reduction in "poorly worded query" failures

## Open Questions the Paper Calls Out

- How can agents learn to adapt to an organization's structure over time to improve efficiency and accuracy? [explicit] The conclusion states, "Future work could consider AI agents that learn over time from interactions for improving their performance over time."
- How can agents optimally determine which people to contact and in what order? [explicit] The introduction and abstract highlight that "major research questions remain around how to optimally determine which people to contact and in what order."
- What privacy-centric evaluation frameworks and information access models are needed to mitigate data exposure risks? [explicit] The future directions section notes that "privacy risks emerge when agents access personal documents" and calls for "privacy-centric evaluations."
- How can agents be trained to formulate precise questions that do not mislead collaborators into withholding information? [inferred] The failure analysis notes that 25% of errors were caused by "poorly worded or overly-specific queries" causing other users to conclude they lacked relevant information.

## Limitations

- BM25 effectiveness in expertise matching: With P-Prec at only 0.61, the people search mechanism shows clear limitations when expertise descriptions are intentionally "imprecise or incomplete"
- Simulator fidelity for real-world deployment: While human evaluation showed similar performance to LLM simulators, the paper doesn't validate whether synthetic users capture the full range of human communication patterns
- Generalization beyond structured domains: Results come from SPIDER (structured tables) and MultiNews (news articles), with unknown effectiveness on unstructured business documents or dynamic organizational knowledge

## Confidence

- High confidence: The ReAct-style agent architecture is technically sound and the benchmark construction methodology is well-documented
- Medium confidence: The reported performance metrics (Match scores of 24.4-54.8) appear reproducible given the detailed specification
- Low confidence: Claims about real-world applicability and sufficiency of current agent designs for complex organizational information gathering

## Next Checks

1. **Oracle People Search Ablation**: Replace the BM25 people search with an oracle that always returns the true relevant users. Measure the resulting upper bound on Match scores to quantify how much performance is bottlenecked by expertise discovery versus query formulation and conversation management.

2. **Query Reformulation Intervention**: Implement an automatic query paraphrasing step that generates 2-3 alternative phrasings whenever a user responds "I don't have that." Compare failure rates on "poorly worded query" category before and after this intervention to quantify the impact of communication style on success.

3. **Real-User Pilot Study**: Deploy the best-performing agent configuration (likely GPT-4-turbo with reflection) on 20 tasks with actual human participants recruited from within the organization. Compare Match scores, MsgCnt, and qualitative feedback against the synthetic user results to validate simulator fidelity and identify gaps in the evaluation methodology.