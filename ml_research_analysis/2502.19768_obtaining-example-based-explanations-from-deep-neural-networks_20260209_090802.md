---
ver: rpa2
title: Obtaining Example-Based Explanations from Deep Neural Networks
arxiv_id: '2502.19768'
source_url: https://arxiv.org/abs/2502.19768
tags:
- example
- examples
- test
- layers
- attributions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EBE-DNN, a method for generating example-based
  explanations from deep neural networks by combining DNN feature extraction with
  k-nearest neighbor classification. The approach uses embeddings from specific DNN
  layers as input to KNN, enabling the retrieval of training examples that most influence
  a test example's prediction.
---

# Obtaining Example-Based Explanations from Deep Neural Networks

## Quick Facts
- **arXiv ID:** 2502.19768
- **Source URL:** https://arxiv.org/abs/2502.19768
- **Reference count:** 20
- **Primary result:** Example-based explanations from DNNs using KNN on embeddings provide interpretable, concentrated attributions while maintaining or improving accuracy

## Executive Summary
This paper introduces EBE-DNN, a method for generating example-based explanations from deep neural networks by combining DNN feature extraction with k-nearest neighbor classification. The approach uses embeddings from specific DNN layers as input to KNN, enabling the retrieval of training examples that most influence a test example's prediction. Empirical results on MNIST, Fashion-MNIST, and CIFAR-10 datasets demonstrate that EBE-DNN provides highly concentrated example attributions while maintaining or improving predictive accuracy compared to the original ResNet18 model. The choice of embedding layer significantly impacts both accuracy and attribution quality, with middle layers often performing best.

## Method Summary
EBE-DNN works by extracting embeddings from specific layers of a pre-trained ResNet18 model, then using these embeddings as input to a k-nearest neighbor classifier with cosine similarity. For each test example, the model retrieves the k most similar training examples from the embedding space, predicts the class by majority vote, and provides these neighbors as the explanation. The method involves training ResNet18 on the target dataset, extracting and storing embeddings from candidate layers for all training data, and using KNN to find nearest neighbors for each test example. The paper recommends k=10 as a reasonable tradeoff between interpretability and accuracy.

## Key Results
- EBE-DNN provides highly concentrated example attributions while maintaining or improving predictive accuracy compared to the original ResNet18 model
- The choice of embedding layer significantly impacts both accuracy and attribution quality, with middle layers often performing best
- The method successfully identifies meaningful visual properties (color, texture, structure) in the example attributions
- Using 10 examples per attribution provides an optimal balance between interpretability and predictive performance

## Why This Works (Mechanism)

### Mechanism 1: Embedding-Based Nearest Neighbor Retrieval
DNN embeddings enable semantically meaningful example retrieval that raw input features cannot provide. A pre-trained DNN transforms high-dimensional inputs into learned feature representations. These embeddings are indexed, and a test example's embedding queries this index via k-nearest neighbors using cosine similarity. The retrieved neighbors constitute the example attribution. The core assumption is that the embedding space encodes task-relevant similarity such that proximity correlates with shared class membership and visual properties.

### Mechanism 2: Layer-Dependent Feature Hierarchy
The choice of embedding layer determines which visual properties drive attribution—early layers capture color/texture, middle layers capture structure, deeper layers capture category-level abstractions. CNN layers build hierarchical representations where shallow layers encode local patterns, mid-layers integrate these into parts and structures, and deep layers abstract to class-level semantics. The core assumption is that there exists an optimal intermediate abstraction level where features are discriminative yet not over-generalized.

### Mechanism 3: Concentrated Attribution via Small k
A small number of training examples (k ≈ 10) can explain predictions without sacrificing—and sometimes improving—accuracy. KNN voting with small k forces the model to rely on the most similar neighbors. If embeddings are well-structured, these neighbors are highly representative, yielding concentrated attributions. The core assumption is that the embedding space has sufficient class separation that the top-k neighbors predominantly belong to the correct class.

## Foundational Learning

- **Concept: k-Nearest Neighbors (KNN) with cosine similarity**
  - Why needed here: The entire attribution mechanism relies on retrieving nearest neighbors in embedding space. Understanding distance metrics (cosine vs. Euclidean) and their behavior in high dimensions is essential.
  - Quick check question: Why might cosine similarity outperform Euclidean distance for normalized deep network embeddings?

- **Concept: Convolutional Neural Network feature hierarchies**
  - Why needed here: Layer selection is a critical hyperparameter. You must understand what different layers encode to choose appropriately.
  - Quick check question: If early layers capture edges and textures while deeper layers capture object parts and semantics, which layer would you expect to work best for distinguishing similar clothing items (e.g., shirts vs. t-shirts)?

- **Concept: Example attribution vs. feature attribution**
  - Why needed here: This method provides a fundamentally different explanation paradigm. Understanding the distinction clarifies what insights each offers.
  - Quick check question: For a medical imaging model, would you prioritize explaining which pixels mattered (feature attribution) or which training cases influenced the decision (example attribution)? When would each be more actionable?

## Architecture Onboarding

- **Component map:** ResNet18 (frozen weights) -> Embedding extractor (specific convolutional layer) -> KNN retriever (cosine similarity) -> Aggregator (mode voting)
- **Critical path:** 1) Train or load pre-trained ResNet18 on target dataset 2) Extract and store embeddings from candidate layers for all training data 3) For each test example: extract embedding → query KNN → retrieve top-k neighbors → return prediction and attribution
- **Design tradeoffs:** Layer depth vs. specificity (shallow retain detail but lack discriminative power; deep generalize but may lose instance-specific cues); k vs. interpretability (smaller k yields more concentrated explanations but risks instability; larger k smooths predictions but dilutes attribution); Accuracy vs. transparency (paper claims no tradeoff is necessary, but depends on finding right layer)
- **Failure signatures:** Cross-class retrievals (if retrieved neighbors span multiple classes, layer is over-generalizing); Accuracy drop vs. backbone (if EBE-DNN underperforms base DNN, layer selection or k is misconfigured); Visually incoherent attributions (if retrieved examples share no discernible properties with query, embeddings lack semantic structure for that layer)
- **First 3 experiments:** 1) Layer sweep: For target dataset, extract embeddings from layers 1 through final, measure accuracy with k=10. Identify peak-performing layer(s). 2) k sensitivity analysis: At best layer(s), vary k ∈ {1, 3, 5, 10, 20, 50} and plot accuracy. Confirm k=10 is stable operating point. 3) Attribution sanity check: Manually inspect retrieved neighbors for 10-20 test examples. Verify they share meaningful properties (color, texture, structure) with queries.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can an adaptive mechanism be developed to automatically select the optimal DNN layer for embeddings without manual hyperparameter tuning? (Section 4 notes developing automated or adaptive strategies could enhance usability and efficiency, addressing current dependency on treating layer choice as tunable hyperparameter)
- **Open Question 2:** How can the computational complexity of the KNN step be reduced to allow EBE-DNN to scale to high-dimensional datasets and large-scale real-world applications? (Section 4 notes KNN can become computationally expensive for high-dimensional embeddings or large-scale datasets, suggesting exploring scalable implementations or alternative similarity-based approaches)
- **Open Question 3:** Is the EBE-DNN framework equally effective in non-image domains, such as natural language processing or time-series analysis? (Conclusion explicitly lists further exploration needed to determine whether framework is equally effective in other domains, such as text or time-series data)
- **Open Question 4:** Does combining EBE-DNN with feature attribution methods yield a more comprehensive explanation than either method provides alone? (Section 4 suggests complementing example attributions with feature attributions to more clearly determine factors influencing model's predictions)

## Limitations
- Dependence on specific ResNet18 architecture without addressing how architectural modifications (needed for small input images) might affect embedding quality
- Claim that k=10 provides optimal interpretability-accuracy balance is dataset-specific and may not generalize to more complex or imbalanced datasets
- Method's interpretability gains come at the cost of requiring storage of all training embeddings, creating scalability concerns for large datasets

## Confidence

- **Layer-dependent attribution mechanism**: High confidence - results consistently show layer selection impacts both accuracy and attribution quality across all tested datasets
- **k=10 as optimal balance**: Medium confidence - while empirically supported for the three tested datasets, the optimal k likely varies with dataset complexity and class distribution
- **Visual property identification in attributions**: Medium confidence - qualitative observations are convincing, but systematic quantification of "meaningful" visual properties would strengthen this claim

## Next Checks

1. **Architecture sensitivity test**: Implement and compare EBE-DNN using modified ResNet architectures optimized for small images versus standard ResNet18 with padding/truncation to isolate architectural effects on embedding quality
2. **Dataset complexity scaling**: Apply EBE-DNN to progressively more complex datasets (e.g., CIFAR-100, ImageNet subsets) to test whether middle layers consistently provide optimal performance or if the optimal layer depth shifts with task complexity
3. **Attribution quality quantification**: Develop a quantitative metric (e.g., consistency of retrieved neighbors' visual features with the query across multiple human raters) to complement the qualitative observations about visual property identification