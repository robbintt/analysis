---
ver: rpa2
title: 'Doing Things with Words: Rethinking Theory of Mind Simulation in Large Language
  Models'
arxiv_id: '2510.13395'
source_url: https://arxiv.org/abs/2510.13395
tags:
- actions
- agents
- agent
- action
- beliefs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines whether large language models (LLMs) can simulate
  Theory of Mind (ToM) by making genuine inferences about others' beliefs in social
  contexts. Using the Generative Agent-Based Model Concordia, the researchers embedded
  utterances in complex scenarios and assessed whether GPT-4 could select actions
  based on belief attribution rather than linguistic memorization.
---

# Doing Things with Words: Rethinking Theory of Mind Simulation in Large Language Models

## Quick Facts
- arXiv ID: 2510.13395
- Source URL: https://arxiv.org/abs/2510.13395
- Reference count: 40
- GPT-4 consistently failed to choose context-appropriate actions based on belief attribution in social simulations, suggesting ToM-like performance stems from pattern recognition rather than genuine mentalizing

## Executive Summary
This study examines whether large language models (LLMs) can simulate Theory of Mind (ToM) by making genuine inferences about others' beliefs in social contexts. Using the Generative Agent-Based Model Concordia, researchers embedded utterances in complex scenarios and assessed whether GPT-4 could select actions based on belief attribution rather than linguistic memorization. The results show that GPT-4 consistently failed to choose context-appropriate actions, suggesting its ToM-like performance stems from shallow statistical associations rather than true reasoning. Additionally, the model struggled to generate coherent causal effects from agent actions, with average coherence ratings of only 2.11/5. These findings challenge claims of emergent ToM capabilities in LLMs and highlight the need for more rigorous, action-based evaluation frameworks to distinguish genuine mentalizing from pattern recognition.

## Method Summary
The researchers used Concordia, a Generative Agent-Based Model framework, to embed utterances in complex social scenarios. They created 200 simulations across 5 tasks, each with 40 stimuli covering 7 linguistic phenomena (Indirect Requests, Suggestions, Declinations, Threats, and Verbal Irony variants). Two agents with randomized Big Five traits and first/second-order beliefs were placed in scenarios where an ambiguous utterance occurred. GPT-4o-mini was tasked with selecting context-appropriate actions and generating coherent causal effects. Performance was evaluated through multiple-choice action selection accuracy and coherence ratings (1-5 scale) for generated effects, assessed by both LLM-as-a-Judge and human annotators.

## Key Results
- GPT-4 consistently failed to select context-appropriate actions based on belief attribution across all 7 linguistic phenomena
- Average coherence ratings for generated causal effects were only 2.11/5, indicating poor alignment between actions and belief states
- No significant difference in performance between first-order and second-order belief tasks, despite explicit provision of belief information

## Why This Works (Mechanism)
The study leverages Concordia's ability to create controlled social scenarios where belief attribution directly influences action selection. By manipulating what agents know about obstacles and their beliefs about others' knowledge, the researchers create scenarios where only genuine mentalizing can lead to correct action choices. The Chain-of-Thought prompting and LLM-as-a-Judge coherence evaluation provide a structured framework for assessing whether the model maintains belief states across reasoning steps.

## Foundational Learning
- **Theory of Mind (ToM)**: The ability to attribute mental states (beliefs, intents, desires) to others and understand they differ from one's own. *Why needed*: The study's core question is whether LLMs possess genuine ToM capabilities. *Quick check*: Can the model predict behavior based on false beliefs?
- **First-order vs Second-order beliefs**: First-order beliefs are what an agent believes; second-order beliefs are what an agent believes another agent believes. *Why needed*: The study manipulates both to test depth of mentalizing. *Quick check*: Does performance improve when second-order beliefs are explicitly provided?
- **Pragmatic phenomena**: Indirect speech acts, irony, and other non-literal language uses. *Why needed*: These require understanding speaker intent beyond literal meaning. *Quick check*: Can the model distinguish between literal and intended meanings?
- **Coherence evaluation**: Assessing how well generated effects align with actions and beliefs. *Why needed*: To determine if the model maintains consistent mental states. *Quick check*: Do generated effects reference relevant beliefs and actions?

## Architecture Onboarding

**Component map:**
Concordia Game Master -> Agent Memory/Observations -> GPT-4o-mini API calls -> MCQA action selection -> CoT event generation -> Coherence rating

**Critical path:**
Scenario setup → Agent memory initialization → Utterance presentation → Action selection → Effect generation → Coherence evaluation

**Design tradeoffs:**
- Action-based evaluation vs. verbal ToM tasks (more diagnostic but more complex)
- First-order vs. second-order belief manipulation (increased cognitive demand)
- LLM-as-a-Judge vs. human-only evaluation (scalability vs. reliability)

**Failure signatures:**
- Model defaults to common action associations regardless of belief context
- Generated effects lose coherence from earlier CoT steps
- No performance difference between belief complexity levels

**First experiments:**
1. Run MCQA phase with default prompting to establish baseline accuracy
2. Test second-order belief scenarios to verify no improvement over first-order
3. Generate effects with Chain-of-Thought and evaluate coherence ratings

## Open Questions the Paper Calls Out
**Open Question 1:** Can evaluation tasks be designed that effectively isolate genuine ToM reasoning in LLMs from pattern matching on memorized linguistic constructions? The authors note the challenge of designing tasks that isolate ToM-like abilities from confounding variables, questioning whether models are genuinely reasoning about beliefs or simply learning how to solve familiar tasks through exposure. This remains unresolved as the study's action-based framework still showed poor performance.

**Open Question 2:** Does explicitly providing second-order belief representations to LLMs improve ToM task performance compared to first-order beliefs alone? The study explored this by adding explicit second-order beliefs and contextual information, finding no clear pattern or significant difference between first-order and second-order belief tasks. The model failed to leverage explicitly provided second-order beliefs.

**Open Question 3:** What internal representational mechanisms do LLMs employ during ToM-related tasks, and do these resemble belief attribution processes? From the limitations section, the study evaluated outputs only without investigating internal mechanisms underlying the model's ToM-related reasoning. Output-level analysis cannot distinguish between genuine belief representation and superficial pattern completion.

**Open Question 4:** How can "functional ToM" benchmarks be developed that assess mentalizing through action selection rather than metalinguistic judgment? The authors conclude that to isolate mentalizing processes, we should rely on more complex scenarios focusing on assessing functional ToM rather than merely literal ToM. Current action-based evaluations showed poor coherence.

## Limitations
- Evaluation framework may not effectively isolate genuine ToM from pattern matching
- Low coherence ratings (2.11/5) could reflect conversational context maintenance issues rather than absence of mental state reasoning
- Study only examined outputs without investigating internal mechanisms of ToM-related reasoning

## Confidence

**Major claim clusters confidence:**
- GPT-4 fails to perform genuine ToM inference in action selection: **Medium**
- Poor coherence in generated effects indicates lack of ToM: **Medium**
- ToM-like performance in LLMs reflects pattern matching rather than reasoning: **Low-Medium**

## Next Checks
1. Replicate the study using alternative prompting strategies (e.g., role-play instructions, explicit belief-state reminders) to determine if the observed failures persist across different approaches
2. Compare GPT-4 performance on the action-based tasks versus traditional ToM benchmarks to assess whether different evaluation methods yield different conclusions about ToM capabilities
3. Conduct ablation studies removing linguistic context while preserving social dynamics to isolate whether failures stem from language processing versus social reasoning limitations