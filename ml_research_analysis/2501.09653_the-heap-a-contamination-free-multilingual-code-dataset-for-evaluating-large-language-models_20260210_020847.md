---
ver: rpa2
title: 'The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating Large
  Language Models'
arxiv_id: '2501.09653'
source_url: https://arxiv.org/abs/2501.09653
tags:
- dataset
- code
- datasets
- language
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces The Heap, a large-scale multilingual code
  dataset containing 57 programming languages designed to enable fair evaluation of
  large language models without data contamination. The dataset addresses the challenge
  of limited fresh code available for downstream LLM evaluation by focusing on non-permissively
  licensed code and performing extensive deduplication against commonly used training
  datasets like The Stack.
---

# The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating Large Language Models

## Quick Facts
- arXiv ID: 2501.09653
- Source URL: https://arxiv.org/abs/2501.09653
- Reference count: 22
- Contains 32.7 million unique files across 733,663 repositories in 57 programming languages

## Executive Summary
The Heap addresses a critical challenge in LLM evaluation: data contamination from training-test overlap. The authors create a large-scale multilingual code dataset by collecting non-permissively licensed GitHub repositories and performing extensive deduplication against commonly used training datasets. By focusing on copyleft-licensed code and flagging duplicates rather than removing them, The Heap provides researchers with a contamination-resistant evaluation resource that preserves maximum flexibility for diverse use cases.

## Method Summary
The authors collect up to 50,000 repositories per language from GitHub, filtering for non-permissive licenses (GPL, AGPL, LGPL, MPL, etc.) and repositories created between 2008-2024. They extract files by extension, apply size and word count filters, then deduplicate both exactly using SHA-256 hashing and near-exactly using MinHash LSH with 128 permutations and Jaccard similarity threshold of 0.7. The dataset is annotated with metadata including quality indicators and Boolean flags for duplicates against The Stack v1/v2, Red Pajama, GitHub Code, and CodeParrot, while preserving all files for post-hoc filtering flexibility.

## Key Results
- 32.7 million unique files across 733,663 repositories
- Coverage of 57 programming languages
- Extensive deduplication against 4 major training datasets
- Boolean flagging approach preserves maximum data flexibility
- Non-permissive licensing creates structural contamination barrier

## Why This Works (Mechanism)

### Mechanism 1
Non-permissive licensing creates a structural barrier against inclusion in mainstream training corpora. Copyleft licenses require derivative works to be released under the same terms, making this code legally unattractive for commercial or permissively-licensed model training pipelines that prioritize unconstrained redistribution. This works if most large-scale code LLM trainers systematically exclude non-permissively licensed code to avoid licensing obligations.

### Mechanism 2
Pre-computed deduplication against known training datasets eliminates the most common contamination vectors. Files are normalized and compared against The Stack v1/v2, Red Pajama, GitHub Code, and CodeParrot using SHA-256 for exact matches and MinHash LSH (128 permutations, Jaccard â‰¥0.7) for near-duplicates. Results are stored as Boolean flags rather than removal, preserving flexibility. This works if the reference datasets used for deduplication represent the majority of code seen during pre-training of target models.

### Mechanism 3
Boolean flagging rather than hard removal maximizes utility across heterogeneous evaluation scenarios. Each file retains metadata flags indicating exact/near-duplicate status against each reference dataset. Users filter post-hoc based on which training corpora are relevant to their specific model under test. This works if evaluators know which datasets their target model was trained on and can select appropriate filters.

## Foundational Learning

- **Data contamination in LLM evaluation**: Why needed here - The entire premise rests on understanding why train/test overlap inflates performance metrics and undermines research conclusions. Quick check question: If a model achieves 95% accuracy on a benchmark, what minimum evidence would you need before attributing this to genuine capability rather than memorization?

- **Locality-Sensitive Hashing (LSH) for near-duplicate detection**: Why needed here - MinHash LSH with Jaccard similarity is the core technical mechanism for catching paraphrased or lightly-modified code duplicates. Quick check question: Why would exact hash matching alone be insufficient for code deduplication, and what tradeoff does the 0.7 Jaccard threshold represent?

- **License compatibility and copyleft propagation**: Why needed here - The dataset's contamination resistance depends on understanding why GPL-family licenses create downstream obligations that discourage inclusion. Quick check question: If you fine-tune a model on GPL-3.0 code and distribute it, what license obligations might apply to your model weights?

## Architecture Onboarding

- **Component map**: GitHub API scraping -> star-ranked selection -> file extraction by extension -> size/word-count filtering -> in-corpus exact deduplication -> comment/whitespace normalization -> SHA-256 exact matching -> MinHash LSH near-matching -> Boolean flag assignment -> per-language subsets with unified schema -> HuggingFace distribution

- **Critical path**: 1) Identify which training datasets your target model used 2) Load The Heap subset for your programming language(s) of interest 3) Filter to exact_duplicates_[dataset]=False AND near_duplicates_[dataset]=False for all relevant datasets 4) Apply quality filters as needed 5) Construct evaluation tasks from remaining files

- **Design tradeoffs**: Boolean flags vs. hard removal preserves data for edge cases but shifts burden to evaluator; 7-character shingles optimized for code's limited character set but may miss semantic duplicates with identifier renaming; 0.7 Jaccard threshold favors recall at cost of false positives; comment stripping reduces false negatives but loses provenance signals

- **Failure signatures**: Silent contamination from model trained on undisclosed dataset not in deduplication list; over-aggressive filtering from MinHash threshold removing semantically distinct but syntactically similar files; language misattribution bypassing comment stripping; temporal drift from repositories created after August 2024 not represented

- **First 3 experiments**: 1) Contamination audit: Compare model performance on The Heap (filtered for that model's known training data) vs. unfiltered subset; divergence indicates genuine capability vs. memorization 2) Cross-dataset consistency: Run identical evaluation using files flagged as duplicates in The Stack but not in Red Pajama; consistent performance suggests robustness to specific contamination sources 3) Language-specific baseline: Select 3 languages with high repo counts and 3 with low counts; compare model performance delta to assess generalization vs. resource dependence

## Open Questions the Paper Calls Out

### Open Question 1
How does the presence of multiple natural languages within code files impact the performance and evaluation of code-focused Large Language Models? The authors identify the intersection of natural languages and code as an "under-explored research area" and propose developing a tagging system for natural languages present in files. Current code datasets often lack annotations for the natural language used in comments and identifiers, making it difficult to isolate performance differences between English-focused and non-English code LLMs.

### Open Question 2
What constitutes an effective deduplication strategy for code datasets beyond file-level exact and near-duplication? The authors note there is "limited research on what constitutes an effective deduplication strategy" and suggest issues may exist at "lower granularity level than file-based deduplication." While The Heap uses SHA-256 and MinHash LSH, function-level or code-fragment-level duplicates may persist, potentially allowing for data leakage despite file-level deduplication.

### Open Question 3
Can topic modeling techniques be effectively adapted for code datasets to identify specializations and imbalances across different programming languages? The authors suggest that "Adopting the FineWeb topic modeling approach for code datasets would create interesting annotations" to address the limitation that many languages are used across multiple specializations. It is currently difficult to select code based on domain-specific functionality rather than just programming language syntax, potentially hiding topic-based biases in evaluations.

### Open Question 4
Does the removal of common phrases, boilerplate code, and autogenerated content significantly improve the quality of code datasets for downstream evaluation? The authors state that "there is still the question of file 'quality'" and hypothesize that "languages that rely heavily on boiler plating... may benefit from removing certain common phrases." The current dataset includes all non-duplicate files, but the impact of "lorem ipsum" text or excessive boilerplate on the robustness of LLM evaluation metrics remains unknown.

## Limitations
- Unknown training datasets not included in deduplication may still cause contamination
- MinHash LSH with Jaccard 0.7 threshold may either miss semantically similar code or remove unique files
- Comment-stripping regex patterns may fail on polyglot code or files with incorrect extensions
- Non-permissive licensing creates structural barriers but not absolute prevention of contamination

## Confidence

- **High Confidence**: The dataset contains 32.7 million unique files across 733,663 repositories with proper metadata annotation and deduplication flags
- **Medium Confidence**: Non-permissive licensing significantly reduces contamination probability; Boolean flagging approach preserves data flexibility
- **Low Confidence**: The Heap is completely contamination-free for all potential LLM training scenarios; MinHash LSH with Jaccard 0.7 threshold captures all relevant near-duplicates

## Next Checks

1. **Contamination Audit**: Compare model performance on The Heap (filtered for that model's known training data) vs. unfiltered subset; significant performance differences would validate contamination resistance claims

2. **Cross-Dataset Validation**: Run identical evaluations using files flagged as duplicates in The Stack but not in Red Pajama; consistent performance across these subsets would indicate robustness to specific contamination sources

3. **Temporal Consistency Test**: Split The Heap by repository creation date and evaluate whether model performance degrades on newer vs. older code, which would indicate genuine capability rather than memorization of older patterns