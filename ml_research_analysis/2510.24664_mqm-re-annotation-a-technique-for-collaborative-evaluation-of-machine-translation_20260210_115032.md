---
ver: rpa2
title: 'MQM Re-Annotation: A Technique for Collaborative Evaluation of Machine Translation'
arxiv_id: '2510.24664'
source_url: https://arxiv.org/abs/2510.24664
tags:
- human
- translation
- annotations
- quality
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MQM re-annotation as a technique to improve
  the quality of human evaluation for machine translation. The method involves having
  an expert rater review and edit pre-existing MQM annotations, which can come from
  the same rater, a different human rater, or an automatic system.
---

# MQM Re-Annotation: A Technique for Collaborative Evaluation of Machine Translation

## Quick Facts
- arXiv ID: 2510.24664
- Source URL: https://arxiv.org/abs/2510.24664
- Reference count: 11
- Authors propose MQM re-annotation to improve machine translation evaluation quality through expert review of existing annotations

## Executive Summary
This paper introduces MQM re-annotation as a method to enhance the reliability of human evaluation in machine translation. The technique involves having expert raters review and edit pre-existing MQM annotations, which can originate from the same rater, different human annotators, or automatic systems. The authors find that re-annotation improves cross-group agreement and helps identify previously missed errors, with automatic annotations serving as effective priors without additional human annotation costs.

## Method Summary
The MQM re-annotation method involves expert raters reviewing and editing existing MQM annotations. Raters can work with their own previous annotations, annotations from other human raters, or automatically generated annotations. The process focuses on identifying and correcting errors that may have been missed in the initial annotation pass, with the goal of improving overall annotation quality and agreement across different groups of raters.

## Key Results
- Re-annotators make fewer changes to their own annotations compared to others' or automatic annotations
- Re-annotation of human annotations improves quality by increasing cross-group agreement
- Providing raters with automatic prior annotations improves quality without additional human annotation cost
- Primary benefit is identification of previously missed errors

## Why This Works (Mechanism)
The effectiveness of MQM re-annotation stems from the cognitive benefits of reviewing existing work rather than starting from scratch. When raters examine pre-existing annotations, they can focus on verification and error detection rather than initial judgment formation. This second-pass review allows for more critical evaluation, as the cognitive load of making initial decisions has already been handled. The presence of prior annotations serves as a cognitive scaffold that helps raters identify inconsistencies and overlooked issues.

## Foundational Learning

**Machine Translation Quality Evaluation**
Why needed: Forms the foundation for understanding why better annotation methods matter
Quick check: Can identify at least two major challenges in MT evaluation

**MQM Annotation Framework**
Why needed: Understanding the specific annotation scheme being re-used
Quick check: Can explain what MQM stands for and its basic annotation categories

**Human Annotation Psychology**
Why needed: Critical for understanding why re-annotation improves quality
Quick check: Can articulate at least one cognitive bias that affects initial annotation

## Architecture Onboarding

Component map: MQM Framework -> Initial Annotation -> Re-annotation Review -> Quality Assessment

Critical path: The most important sequence is Initial Annotation â†’ Re-annotation Review, as this is where quality improvements are realized. The review process must maintain fidelity to the original MQM framework while allowing for corrections and improvements.

Design tradeoffs: The method balances quality improvement against annotation time and cost. While re-annotation adds overhead, it provides quality gains without requiring entirely new human annotations. The use of automatic priors further optimizes this tradeoff by reducing human workload.

Failure signatures: Poor quality improvements occur when re-annotators are not sufficiently expert, when the initial annotations are of very low quality, or when the MQM framework is not consistently applied across annotation passes.

First experiments:
1. Compare re-annotation changes across self, other human, and automatic priors
2. Measure cross-group agreement before and after re-annotation
3. Analyze types of errors identified during re-annotation versus initial pass

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several emerge from the analysis: How does re-annotation performance vary across different language pairs and domains? What is the optimal balance between initial annotation quality and re-annotation effort? How do different automatic annotation systems compare as priors for human re-annotation?

## Limitations
- Study primarily focuses on English-to-German translation, limiting generalizability
- Analysis based on relatively small sample size of annotators
- Does not fully explore whether improved agreement translates to better alignment with actual translation quality

## Confidence

High confidence: Re-annotation increases cross-group agreement is well-supported by data
Medium confidence: Re-annotators make fewer changes to their own annotations needs larger-scale validation
Medium confidence: Automatic annotations serve as effective prior annotations requires further testing across different systems

## Next Checks

1. Conduct a multi-domain study with diverse language pairs to test whether re-annotation benefits generalize beyond the English-to-German setting
2. Implement a longitudinal study tracking whether re-annotation improves evaluation stability over multiple rounds of assessment
3. Compare re-annotation results with independent quality assessments to verify that improved agreement correlates with better quality judgments