---
ver: rpa2
title: 'Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for
  Real-Time Strategy Games'
arxiv_id: '2510.18395'
source_url: https://arxiv.org/abs/2510.18395
tags:
- state
- masmp
- machine
- agents
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Memory-Augmented State Machine Prompting (MASMP),
  a novel framework that integrates state machine prompting with memory mechanisms
  to address key challenges in LLM-based agents for real-time strategy games. MASMP
  guides LLMs to emulate finite state machines and behavior trees through natural
  language prompts while maintaining long-term tactical coherence via a lightweight
  memory module.
---

# Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games

## Quick Facts
- arXiv ID: 2510.18395
- Source URL: https://arxiv.org/abs/2510.18395
- Reference count: 13
- Key outcome: MASMP achieves 60% win rate against StarCraft II Lv7 AI, outperforming baseline LLM agents at 0%

## Executive Summary
This paper introduces Memory-Augmented State Machine Prompting (MASMP), a novel framework that integrates state machine prompting with memory mechanisms to address key challenges in LLM-based agents for real-time strategy games. MASMP guides LLMs to emulate finite state machines and behavior trees through natural language prompts while maintaining long-term tactical coherence via a lightweight memory module. Experiments in StarCraft II demonstrate MASMP's 60% win rate against the hardest built-in AI (Lv7), significantly outperforming baselines at 0%. The framework resolves the "Knowing-Doing Gap" by combining LLMs' semantic comprehension with strict state-action mapping, achieving both interpretability and reliability.

## Method Summary
MASMP framework comprises three core components: (1) Macro-Strategic State Machine with natural language-driven tactical states and transition conditions, (2) Action Implementation Behavior Tree for hierarchical decision-making, and (3) Strategic Memory module preserving tactical variables across decision cycles. The system processes game observations through a prompt that combines current state, retrieved memory, and structured state machine definitions, with outputs parsed to extract strategy variables for memory updating. Experiments use DeepSeek-V3 LLM with Easy Build/Control Mode enabled for micromanagement, tested on StarCraft II Simple64 map against built-in AI at difficulty levels 1-7.

## Key Results
- MASMP achieves 60% win rate against StarCraft II Lv7 AI, compared to 0% for baseline LLM agents
- Framework demonstrates effective state transitions (defensive→aggressive→defensive) with coherent tactical reasoning
- Memory module successfully preserves strategic variables across decision cycles, preventing fragmented execution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining LLM outputs through a natural language state machine architecture reduces hallucinations and enforces valid action generation.
- Mechanism: The framework defines explicit tactical states (e.g., `<aggressive>`, `<defensive>`), natural-language transition conditions, and hierarchical behavior tree structures (selector, sequence, condition, action nodes) that the LLM must follow. This channels the LLM's generation into pre-validated action spaces.
- Core assumption: LLMs can reliably parse and adhere to structured state machine logic presented as natural language prompts across diverse game states.
- Evidence anchors: [abstract] "natural language-driven state machine architecture that guides LLMs to emulate finite state machines and behavior trees through prompts"; [section 3.1] "framework comprises three key components: Macro-Strategic State Machine... Action Implementation Behavior Tree... Supplementary Atomic Rules"; [corpus] Weak direct validation; neighboring papers on memory-augmented agents do not specifically test state machine prompting for action validity.
- Break condition: If natural language state definitions become ambiguous, or the LLM fails to consistently parse transition conditions under novel inputs, FSM-like reliability degrades toward baseline LLM performance.

### Mechanism 2
- Claim: External memory maintaining strategic variables across decision cycles prevents fragmented execution and enables long-term tactical coherence.
- Mechanism: A lightweight memory module stores extracted state variables (e.g., `[Tactic]:<defensive>`, `[PriorityUnit]`). At each timestep, the last strategy is retrieved and concatenated with the current observation and state machine prompt, extending the decision formulation from `at ~ LLM_Generate(ot, prompt)` to `(st, at) ~ LLM_Generate(ot, Mt-1, promptsm)`.
- Core assumption: Strategic variables can be reliably extracted via regex and remain relevant across multi-step game progression.
- Evidence anchors: [abstract] "lightweight memory module preserving strategic variables (e.g., tactics, priority units) across decision cycles"; [section 3.2] Equations (2)-(3); [section 4.2] Table 1 shows 60% vs 0% win rate at Lv7; [corpus] "From Experience to Strategy" and "UserCentrix" propose memory-augmented agent frameworks but do not validate this specific strategic variable persistence mechanism.
- Break condition: If StrategyExtractor regex fails to capture relevant strategy tokens, or if stored variables become stale as game state evolves rapidly, memory provides no benefit or actively misleads decisions.

### Mechanism 3
- Claim: Natural language transition conditions enable fuzzy reasoning and generalization without exhaustive rule enumeration.
- Mechanism: Instead of hard-coded numeric thresholds (e.g., `if units > 50`), the prompt uses semantic conditions (e.g., "when achieving force advantage"). The LLM's semantic comprehension maps observations to state transitions adaptively.
- Core assumption: LLMs can perform consistent semantic-to-action mapping across unseen game states without explicit hard-coded rules.
- Evidence anchors: [section 3.1] "Unlike traditional FSMs requiring exhaustive rule enumeration, our approach uses natural language conditions... leveraging LLMs' ability to generalize from partial specifications"; [section 4.3] Fig. 3 shows dynamic state transitions: defensive→aggressive→defensive with coherent tactical reasoning at steps 75-78; [corpus] Absent—neighboring papers do not specifically test fuzzy natural-language transition conditions in state machine contexts.
- Break condition: If the LLM's semantic interpretation becomes inconsistent across similar states, or if edge cases require precise threshold judgments the LLM cannot reliably estimate, the system loses FSM-like reliability.

## Foundational Learning

- Concept: **Finite State Machines (FSM) and Behavior Trees**
  - Why needed here: MASMP explicitly emulates FSM patterns and behavior tree node types (selector, sequence, condition, action). Understanding these structures is prerequisite to interpreting how prompts constrain LLM behavior.
  - Quick check question: Can you explain how a behavior tree's selector node differs from a sequence node in failure handling?

- Concept: **Markov vs Non-Markovian Decision Processes**
  - Why needed here: The paper argues RTS games with fog of war and temporal dependencies violate Markov assumptions. Understanding why memory is required for non-Markovian settings clarifies the motivation for the memory module.
  - Quick check question: In StarCraft II with fog of war, why would treating each observation as independent lead to strategically incoherent behavior?

- Concept: **Prompt Engineering and Chain-of-Thought**
  - Why needed here: State machine prompting builds on prompt engineering foundations; understanding how prompts structure LLM reasoning is essential.
  - Quick check question: How does chain-of-thought prompting differ from direct instruction in terms of intermediate reasoning visibility?

## Architecture Onboarding

- Component map: Observation (ot) → Memory Retrieval (Mt-1) → Prompt Concatenation → LLM Generation → Strategy Extraction → Memory Update (Mt) + Action Execution
- Critical path: Text-based API providing natural language interface to StarCraft II observations and actions → Macro-Strategic State Machine definitions → Action Implementation Behavior Tree → Supplementary Atomic Rules → Strategy Extraction → Memory Update
- Design tradeoffs: Natural language flexibility vs. FSM reliability (more interpretable and generalizable, but probabilistic rather than deterministic); Lightweight memory vs. comprehensive state history (lower overhead, but may miss relevant historical context); Easy Build Mode enabled (rules handle low-level execution while LLM focuses on strategic decisions)
- Failure signatures: Repeated invalid actions → State machine prompt not properly constraining LLM output format; Strategy flip-flopping each timestep → Memory retrieval failing or LLM ignoring retrieved context; Only low-tier units produced → `[PriorityUnit]` variable not being set or parsed correctly
- First 3 experiments: 1) Validate prompt parsing: Pass a single game observation through the LLM with state machine prompt; verify output contains valid state tag and executable actions. 2) Test memory persistence: Run two consecutive decision cycles; confirm strategy variable from cycle 1 appears in cycle 2's concatenated LLM input. 3) Baseline comparison at Lv4-5: Run 10 games each for MASMP vs baseline; verify win rate differential emerges as reported (100% vs 40%).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the MASMP framework be extended to handle multi-agent coordination?
- Basis in paper: [explicit] The conclusion explicitly lists "exploring multi-agent coordination" as a direction for future work.
- Why unresolved: The current implementation focuses on a single agent's decision cycle (observation-memory-action) and does not define protocols for communication or shared state between multiple agents.
- What evidence would resolve it: A demonstration of MASMP managing distinct agent roles with cooperative state transitions in a multi-unit scenario.

### Open Question 2
- Question: Does MASMP maintain its performance and reliability across different backbone LLMs?
- Basis in paper: [inferred] The experimental setup restricts testing to the DeepSeek-V3 model, leaving the framework's dependency on specific model capabilities (e.g., instruction following) unverified.
- Why unresolved: Different LLMs may interpret the natural language state machine constraints with varying degrees of adherence, potentially affecting the "Knowing-Doing Gap."
- What evidence would resolve it: Comparative benchmarks showing win rates against Lv7 AI when using alternative LLMs (e.g., GPT-4, Llama) with the same prompts.

### Open Question 3
- Question: Can MASMP transfer effectively to other complex domains without manual re-engineering of the state machine definitions?
- Basis in paper: [explicit] The conclusion identifies "cross-domain applications" as a key area for future research.
- Why unresolved: While the prompts are natural language, the state variables (e.g., "aggressive") and transition logic were likely tailored to StarCraft II mechanics.
- What evidence would resolve it: Successful application of the MASMP architecture to a different decision-making domain (e.g., autonomous driving or military simulations) using a similar prompt structure.

## Limitations

- Core mechanisms rely on unstated prompt templates, regex patterns, and behavior tree structures that fundamentally determine whether MASMP can reliably constrain LLM outputs
- 60% win rate against Lv7 AI represents a single data point without statistical significance reporting
- Comparison assumes baseline agents truly achieve 0% win rate without showing evidence

## Confidence

- **High Confidence**: MASMP framework design concept (combining state machine prompting with memory) is well-articulated and technically coherent. The theoretical argument for addressing "Knowing-Doing Gap" through hybrid symbolic-neural approaches is sound.
- **Medium Confidence**: Experimental win rates (60% vs 0%) appear internally consistent but lack statistical validation, full experimental protocols, or baseline agent details needed for external verification.
- **Low Confidence**: Mechanism 3 (fuzzy reasoning via natural language conditions) has minimal empirical support and no comparison to hard-coded threshold approaches to demonstrate its superiority.

## Next Checks

1. **Prompt Structure Validation**: Request and examine the complete state machine prompt template (prompt_sm) to verify it contains explicit state definitions, transition conditions, and action mappings that could actually constrain LLM output format rather than just guide reasoning.

2. **Statistical Significance Verification**: Obtain the number of games played per difficulty level and calculate confidence intervals for win rate claims, particularly the 60% vs 0% comparison at Lv7 where small sample sizes could produce misleading results.

3. **Baseline Agent Documentation**: Review the exact implementation of baseline agents used for comparison to confirm they represent current state-of-the-art LLM-based RTS agents and that their 0% win rate claim is empirically supported through controlled experiments.