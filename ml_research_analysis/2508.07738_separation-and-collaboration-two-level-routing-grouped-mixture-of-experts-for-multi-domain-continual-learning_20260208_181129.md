---
ver: rpa2
title: 'Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts
  for Multi-Domain Continual Learning'
arxiv_id: '2508.07738'
source_url: https://arxiv.org/abs/2508.07738
tags:
- task
- learning
- knowledge
- expert
- forgetting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting and forward forgetting
  in multi-domain continual learning (MDCL) where tasks vary in both class sets and
  domains. To address these issues, the authors propose a Two-Level Routing Grouped
  Mixture-of-Experts (TRGE) method that dynamically expands a pre-trained CLIP model
  by assigning a specific expert group for each task, thereby mitigating catastrophic
  forgetting.
---

# Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning

## Quick Facts
- **arXiv ID:** 2508.07738
- **Source URL:** https://arxiv.org/abs/2508.07738
- **Reference count:** 14
- **Primary result:** Outperforms state-of-the-art methods with 23M parameters, achieving 1.35% to 3.20% average improvements on MTIL and MCIL benchmarks.

## Executive Summary
This paper addresses catastrophic forgetting and forward forgetting in multi-domain continual learning by proposing a Two-Level Routing Grouped Mixture-of-Experts (TRGE) method. The approach dynamically expands a pre-trained CLIP model by assigning specific expert groups to each task, freezing them after training to prevent forgetting. It uses hierarchical routing (intra-group and inter-group) to manage complexity and enable cross-task collaboration, while a Semantics-based Task Recognition (STR) module leverages MLLMs for accurate task identification. Experiments demonstrate superior performance with fewer trainable parameters compared to existing methods.

## Method Summary
TRGE works by dynamically expanding a frozen CLIP backbone with LoRA-based expert groups, one per task. Each group contains a fixed number of experts (N_e=3) and uses an intra-group router to select the top-k (k=2) experts. An inter-group router selects relevant groups based on task prototypes and identifiers. The STR module uses MLLMs to recognize task IDs, enabling the system to distinguish between seen and unseen tasks. During inference, outputs from CLIP and TRGE are dynamically fused using a scaling factor (α=0.025) for unseen samples, preserving zero-shot capabilities while leveraging learned knowledge.

## Key Results
- Achieves 1.35% to 3.20% average improvements over state-of-the-art methods on MTIL and MCIL benchmarks.
- Uses only 23M trainable parameters compared to larger baselines.
- Successfully mitigates both catastrophic forgetting (through task-specific isolation) and forward forgetting (through dynamic fusion).

## Why This Works (Mechanism)

### Mechanism 1: Task-Specific Knowledge Isolation via Expert Grouping
- **Claim:** Allocating distinct, fixed-size parameter groups to specific tasks minimizes catastrophic forgetting by preventing parameter updates from overwriting previously learned weights.
- **Mechanism:** The system expands the architecture dynamically by instantiating a new expert group G_t for each incoming task t. Crucially, once training for task t is complete, the parameters in G_t are frozen. This isolates the learned knowledge, ensuring that gradient updates from future tasks cannot modify the feature representations of past tasks.
- **Core assumption:** Tasks are sufficiently distinct that storing them in separate parameter spaces is more efficient than interference-heavy joint training, and that the capacity of a single group is sufficient for one task.
- **Evidence anchors:**
  - [abstract] "...assigning specific expert group for each task to mitigate catastrophic forgetting."
  - [method] "To prevent the task knowledge from being corrupted by subsequent task training, only the group corresponded to current task will be trained while previous groups are frozen..."
  - [corpus] This aligns with findings in related work like LLaVA-CMoE, which uses specific experts for continual multimodal learning to handle expansion.

### Mechanism 2: Hierarchical Routing for Stability and Transfer
- **Claim:** Decoupling routing into "intra-group" (fixed complexity) and "inter-group" (dynamic selection) prevents routing overfitting while enabling cross-task collaboration.
- **Mechanism:** As the total number of experts grows, standard MoE routers struggle with complexity. TRGE fixes the number of experts N_e within any group G_t, keeping the intra-group router complexity constant. An inter-group router then selects relevant groups based on task identity and prototype distance. This allows the model to scale in capacity (more groups) without destabilizing the internal routing logic of those groups.
- **Core assumption:** The semantic distance between an input sample and a stored task prototype correlates with the relevance of that task's expert group.
- **Evidence anchors:**
  - [method] "...TRGE maintains the static experts count within the group and introduces the intra-group router to alleviate routing overfitting..."
  - [method] "...we design an inter-group routing policy based on task identifiers and task prototype distance..."
  - [corpus] Little By Little highlights issues with ambiguous routing in LoRA-based MoE; the hierarchical approach here attempts to structurally enforce stability.

### Mechanism 3: Semantic-Based Task Recognition (STR) for Zero-Shot Preservation
- **Claim:** Using Multimodal Large Language Models (MLLMs) to identify task identities preserves the zero-shot capabilities of the pre-trained backbone better than feature-based clustering.
- **Mechanism:** Instead of relying on statistical feature boundaries (which drift and degrade), the system uses an MLLM to parse class names into semantic descriptions and match input images to these descriptions. If the MLLM identifies a sample as "unseen" (ID = -1), the system dynamically fuses the output using a scaling factor α, prioritizing the frozen pre-trained model initially and gradually increasing the adapter's weight as training progresses.
- **Core assumption:** The MLLM has sufficient visual-semantic alignment to distinguish between learned and unseen domains without requiring gradient updates.
- **Evidence anchors:**
  - [abstract] "...leverage Multimodal Large Language Models (MLLMs)... to generate semantic task descriptions and recognize the correct task identifier."
  - [results] "...STR achieves superior results... proving its high recognition accuracy both in seen and unseen tasks."
  - [corpus] While specific MLLM-routing papers are sparse in the immediate corpus, HiCL emphasizes separation mechanisms, contrasting with the semantic integration approach here.

## Foundational Learning

- **Concept: Mixture of Experts (MoE) & Sparse Gating**
  - **Why needed here:** The core architecture relies on MoE to scale capacity without blowing up computation. Understanding how a "router" selects specific "experts" (sparse activation) is essential to grasp how TRGE manages to isolate tasks while keeping inference costs manageable.
  - **Quick check question:** Can you explain why a Top-k gating function is used in Equation 1, rather than a Softmax over all experts?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT) / LoRA**
  - **Why needed here:** The "experts" in this architecture are not full neural networks but LoRA (Low-Rank Adaptation) adapters. You must understand low-rank matrix factorization to see why this method allows "dynamic expansion" with only ~23M parameters.
  - **Quick check question:** If the expert networks were full fine-tuning layers instead of LoRA, how would that impact the "Parameter Count" column in Table 1?

- **Concept: Catastrophic vs. Forward Forgetting**
  - **Why needed here:** The paper distinguishes between losing knowledge of *past* tasks (catastrophic) and losing the zero-shot ability for *future/unseen* tasks (forward). The dual-mechanism solution (Isolation for past, Dynamic Fusion for future) addresses these distinct failure modes differently.
  - **Quick check question:** Looking at Figure 3, why does the "Zero-shot" accuracy generally increase, and how does the dynamic fusion mechanism in Equation 10 exploit this?

## Architecture Onboarding

- **Component map:**
  1. **Backbone:** Frozen CLIP ViT-B/16 (Visual Encoder).
  2. **TRGE Adapter:**
     * **Expert Groups:** Sequence of LoRA adapters (G_1, G_2, ... G_t).
     * **Intra-group Router:** Small linear layer + Top-k Softmax (fixed size per group).
     * **Inter-group Router:** Prototype distance calculator (L_2 norm) + Thresholding (θ).
  3. **STR Module:** External MLLM (e.g., Qwen-VL) used for inference-time task ID prediction.
  4. **Fusion Layer:** Weighted sum of CLIP logits and TRGE logits based on training phase t.

- **Critical path:**
  Input Image → CLIP Encoder → [CLS] token → STR (decides ID) → Inter-group Router (selects Main + Assistants) → Intra-group Router (selects Top-k experts) → LoRA Experts → Logits → Dynamic Fusion → Prediction.

- **Design tradeoffs:**
  * **Inference Speed vs. Accuracy:** The STR module requires an external MLLM pass, which adds significant latency compared to simple feature extraction.
  * **Stability vs. Plasticity:** Freezing groups (Stability) prevents forgetting but limits adaptation if the task definition drifts over time.
  * **Hyperparameter θ:** Setting the assistant group threshold too high ignores helpful cross-task knowledge; setting it too low introduces noise (see Figure 5a).

- **Failure signatures:**
  * **Routing Collapse:** The inter-group router defaults to always selecting the same "generalist" group, rendering the expansion useless.
  * **STR Hallucination:** The MLLM consistently misidentifies unseen samples as belonging to a learned task, causing the fallback fusion (Equation 10) to fail.
  * **Overfitting on Small Tasks:** If N_e (experts per group) is too high for a simple task (Figure 4b), the intra-group router overfits.

- **First 3 experiments:**
  1. **Sanity Check (MTIL Baseline):** Run TRGE on the MTIL benchmark with the STR module disabled (using ground truth task IDs). Verify that performance matches the "Ours" row in Table 1 to ensure the adapter groups are training correctly.
  2. **Ablation on Group Size (N_e):** Replicate Figure 4(b) by varying N_e ∈ {1, 3, 5, 7}. Confirm that performance peaks at 3 and drops due to redundancy/overfitting at 7.
  3. **Forward Forgetting Test:** Isolate the dynamic fusion mechanism. Run inference on "unseen" datasets (e.g., SUN397 in early phases) with α=0 vs. α=0.025 to verify that the fused approach outperforms the frozen CLIP baseline.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- **External MLLM Dependency:** The STR module's reliance on an external MLLM introduces inference latency and sensitivity to prompt engineering that isn't quantified.
- **Task Overlap Handling:** The strict isolation of task-specific groups may limit positive transfer when tasks have high semantic overlap or share fine-grained features.
- **Scalability Concerns:** The linear expansion of expert groups may face memory and prototype congestion issues with significantly more tasks (>50) than the 11 datasets tested.

## Confidence
- **High Confidence:** The mechanism of task-specific expert isolation (Mechanism 1) is clearly described and logically sound. The hierarchical routing design (Mechanism 2) is well-motivated and addresses a known limitation in MoE scaling.
- **Medium Confidence:** The STR module's effectiveness (Mechanism 3) is claimed but relies on an external component (MLLM) whose performance characteristics are not fully controlled or reported. The quantitative improvements (Table 1) are credible but may be sensitive to unreported hyperparameters.
- **Low Confidence:** The claim of "cross-task collaboration" through assistant groups lacks direct ablation evidence. The forward forgetting mitigation via dynamic fusion is demonstrated but the optimal α value (0.025) appears arbitrary without sensitivity analysis.

## Next Checks
1. **Ablation on Inter-group Routing:** Remove the assistant group selection (only use the main group based on task ID) and measure performance drop. This directly tests whether the claimed "cross-task collaboration" provides measurable benefit beyond simple task isolation.
2. **STR Module Sensitivity Analysis:** Vary the MLLM prompt templates and measure task ID recognition accuracy on seen vs. unseen tasks. Correlate STR accuracy with final model performance to quantify its contribution and identify potential failure modes.
3. **Forward Forgetting Tuning Curve:** Systematically vary the fusion factor α ∈ {0.0, 0.01, 0.05, 0.1, 0.2} across multiple tasks and plot the Transfer vs. Last trade-off curve. This validates whether 0.025 is truly optimal or just a reasonable default.