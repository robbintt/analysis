---
ver: rpa2
title: 'CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images'
arxiv_id: '2510.11718'
source_url: https://arxiv.org/abs/2510.11718
tags:
- reasoning
- visual
- mathematical
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CodePlot-CoT, a code-driven chain-of-thought
  paradigm that enables Vision Language Models to engage in visual reasoning for mathematical
  problem solving. The approach leverages the VLM to generate executable plotting
  code representing visual thoughts, which are then rendered into images and fed back
  into the model.
---

# CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images

## Quick Facts
- **arXiv ID:** 2510.11718
- **Source URL:** https://arxiv.org/abs/2510.11718
- **Reference count:** 38
- **Primary result:** CodePlot-CoT achieves up to 21% improvement over base model on Math-VR benchmark for mathematical visual reasoning

## Executive Summary
CodePlot-CoT introduces a code-driven chain-of-thought paradigm for Vision Language Models to perform visual reasoning in mathematical problem solving. The approach replaces pixel-level image generation with executable plotting code, which is then rendered into images and fed back into the reasoning chain. This overcomes the limitations of current multimodal models that struggle with precise geometric constructions. The authors construct Math-VR, a large-scale bilingual dataset of 178K mathematical problems requiring visual reasoning, and develop MatplotCode, a high-fidelity image-to-code converter for mathematical figures. Experiments demonstrate that CodePlot-CoT achieves significant improvements on the Math-VR benchmark, validating the effectiveness of the code-driven visual reasoning paradigm.

## Method Summary
CodePlot-CoT leverages a Vision Language Model to generate executable plotting code representing visual thoughts, which are rendered into images and fed back into the reasoning chain. The method involves two-stage training: first, training MatplotCode (a fine-tuned image-to-code converter) on ImgCode-8.6M data to enable high-fidelity mathematical figure reconstruction; second, initializing CodePlot-CoT from MatplotCode and fine-tuning on SFT data constructed from Math-VR using the converter. The pipeline uses Qwen2.5VL-32B-Instruct as the base model, with training involving interleaved text, code, and rendered image sequences. Evaluation uses Answer Correctness (AC) and Process Score (PS) metrics, with GPT-4.1 for grading.

## Key Results
- CodePlot-CoT achieves up to 21% improvement over the base model on the Math-VR benchmark
- The model outperforms Qwen2.5VL-72B, demonstrating that task-specific training matters more than model scale
- MatplotCode achieves 100% execution success rate and is preferred in 554/1000 cases vs next-best commercial models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing pixel-level image generation with code generation improves geometric precision in mathematical visual reasoning
- Mechanism: Mathematical figures require accurate representation of structured geometric properties rather than pixel-level textures. By generating executable plotting code instead of directly generating images, the VLM operates in the text modality where it is inherently proficient. The code is then deterministically rendered into images, bypassing the difficulty of modeling high-dimensional pixel distributions where diffusion models struggle with strict geometric constraints
- Core assumption: Mathematical visual reasoning primarily depends on structural/spatial accuracy rather than visual fidelity; code representations can capture this structure more reliably than pixel generation
- Evidence anchors: [abstract] states code generation overcomes limitations with precise geometric constructions; [page 2, Section 1] emphasizes structural properties over pixel details; ChartReasoner uses similar code-driven modality bridging

### Mechanism 2
- Claim: Visual feedback via rendered code execution enables grounded, self-correcting reasoning chains
- Mechanism: The model generates code → code executes and renders image → rendered image is fed back as visual input → model conditions subsequent reasoning on this concrete visual evidence. This creates a bidirectional mapping where visual thoughts become inspectable, verifiable intermediate products rather than latent abstractions, enabling the model to catch and correct reasoning errors through visual inspection of its own generated figures
- Core assumption: VLMs can effectively interpret their own generated visual output; the rendering pipeline preserves sufficient geometric fidelity for the model to extract meaningful information
- Evidence anchors: [abstract] mentions rendered images fed back into reasoning chain; [page 7, Figure 5] shows complete pipeline; no direct corpus evidence for this specific feedback-loop mechanism in mathematical domains

### Mechanism 3
- Claim: Specialized image-to-code conversion enables high-quality training data curation at scale
- Mechanism: Existing mathematical resources rarely provide paired code annotations for figures. MatplotCode converts existing mathematical figures to executable code, creating the bidirectional image-code mapping needed for supervised training. This allows curating 178K samples where visual reasoning steps have corresponding executable code, teaching the model to represent visual thoughts as code rather than needing human-annotated code for every sample
- Core assumption: The image-to-code converter achieves sufficient fidelity that generated training pairs are not misleading; fine-tuning on this data transfers to novel problems
- Evidence anchors: [page 3, Section 4.2] describes development of MatplotCode for scalable creation of code-image pairs; [page 9, Table 3] shows MatplotCode preferred in 554/1000 cases vs next-best with 100% execution success; GeoThought addresses geometry reasoning but focuses on dataset creation rather than code-based representation

## Foundational Learning

- **Vision-Language Model Architecture** (ViT encoder + projection + LLM decoder)
  - Why needed here: CodePlot-CoT builds on Qwen2.5VL-32B; understanding how images are encoded, projected, and interleaved with text tokens is prerequisite for modifying training procedures and understanding where code/image inputs enter the sequence
  - Quick check question: Can you explain how a VLM processes an interleaved sequence of text tokens and image patches during both training and inference?

- **Supervised Fine-Tuning with Mixed-Modality Sequences**
  - Why needed here: Training involves sequences containing text reasoning, code blocks, and rendered images. Loss is applied to text and code but not to rendered images. Understanding loss masking and sequence construction is essential
  - Quick check question: In a training sequence `[question, text_reasoning, code_block, rendered_image, continuation]`, which segments typically receive loss, and how would you mask the rendered image tokens?

- **Code Generation as Structured Output**
  - Why needed here: The core innovation is generating executable plotting code (matplotlib/Python). This differs from free-form text generation because syntax errors cause execution failures. Understanding constrained decoding or code-specific training approaches helps
  - Quick check question: What failure modes occur when an LLM generates code that is syntactically invalid vs. semantically incorrect, and how might training data quality affect each?

## Architecture Onboarding

- **Component map:**
  Input Layer (text + images) → Base VLM (Qwen2.5VL-32B-Instruct) → MatplotCode Converter → Code Execution Environment → Rendered Images → CodePlot-CoT (fine-tuned from MatplotCode) → Evaluation Pipeline (GPT-4.1 grading)

- **Critical path:**
  1. Train MatplotCode on ImgCode-8.6M filtered subset (Stage 1: ViT+projector alignment, Stage 2: full fine-tuning)
  2. Use MatplotCode to convert Math-VR figures → code, then GPT-4.1 selects optimal code representation
  3. Construct SFT dataset with question → text reasoning → code → rendered image → continuation sequences
  4. Initialize CodePlot-CoT from stage 1 converter weights
  5. Fine-tune CodePlot-CoT on SFT data (5000 steps, loss on text+code, no loss on rendered images)
  6. At inference: model generates code → execute → feed rendered image back → continue reasoning

- **Design tradeoffs:**
  - Code vs. direct image generation: Code is precise and verifiable but requires execution environment; pixel generation is end-to-end differentiable but geometrically imprecise
  - Model scale (32B vs. 72B): CodePlot-CoT-32B outperforms Qwen2.5VL-72B, suggesting task-specific training matters more than scale for this paradigm
  - Inference cost: Averages 1,691.8 output tokens/problem vs. 3,847.3 for baseline, but requires code execution latency (~1 second per image)
  - Converter fidelity vs. training data scale: Higher-fidelity conversion enables better training but requires more compute; current 55.4% preference rate leaves room for noise in training data

- **Failure signatures:**
  - Execution errors: Generated code fails to run (0% for MatplotCode vs. 13.8-20.4% for commercial models)
  - Geometric drift in rendered images: Points slightly misplaced (Figure 14 shows point H not on edge AD despite correct reasoning), potentially misleading subsequent reasoning
  - Over-reliance on visual feedback: If rendered image is wrong, model may compound errors rather than detect inconsistency
  - Text-only fallback: Model may revert to text-only reasoning without generating code when uncertain

- **First 3 experiments:**
  1. Ablate visual feedback: Run CodePlot-CoT with code generation but WITHOUT feeding rendered images back (text-only code generation). Compare to full pipeline to isolate visual feedback contribution vs. code-generation-as-structured-reasoning benefit
  2. Converter fidelity impact: Train separate CodePlot-CoT variants using (a) MatplotCode-converted data, (b) GPT-o3-converted data, (c) Gemini-converted data. Measure how converter quality affects final Math-VR performance
  3. Inference-time rendering variations: Test whether rendering code at different resolutions (224px vs. 448px vs. 560px as used in training) affects answer correctness, probing visual encoding robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the fidelity of the MatplotCode image-to-code converter be improved to prevent subtle geometric misalignments in visual reasoning?
- Basis in paper: [explicit] Appendix E states that due to data scale and model size limits, the converter has not achieved 100% fidelity, noting a specific example where a generated point was displaced
- Why unresolved: The authors identify that current limitations result in slightly imperfect visual reasoning, which risks misleading the model during the "thinking with images" process
- What evidence would resolve it: Demonstration of higher reconstruction accuracy on the Math-VR test set or a qualitative analysis showing correct placement of auxiliary points in complex diagrams

### Open Question 2
- Question: Can the CodePlot-CoT paradigm generalize to scientific domains beyond secondary-school mathematics that require 3D or physics-based simulations?
- Basis in paper: [inferred] The method is specialized for "mathematical figures" using Matplotlib and the Math-VR dataset (mostly geometry/calculus)
- Why unresolved: The pipeline relies on a specialized image-to-code converter and dataset tuned for standard 2D mathematical plotting, which may not capture the constraints of physics diagrams or 3D objects
- What evidence would resolve it: Successful application of the code-driven reasoning framework on benchmarks involving physics problems or 3D spatial reasoning without retraining the core converter from scratch

### Open Question 3
- Question: How robust is the reasoning chain when the generated code executes successfully but contains logical errors (e.g., wrong coordinates) compared to syntax errors?
- Basis in paper: [inferred] The paper measures "Execution Success Rate" but focuses less on the impact of semantically incorrect code that renders valid but wrong images
- Why unresolved: A code snippet that runs without error but visualizes a wrong hypothesis creates a "visual hallucination" that the model accepts as ground truth, potentially derailing the solution more than a text error
- What evidence would resolve it: An ablation study measuring performance degradation when logical bugs are intentionally injected into the code generation step

## Limitations
- Converter fidelity limitations: MatplotCode achieves only 55.4% preference rate, introducing potential noise in training data that could teach incorrect code-visual mappings
- Reproducibility challenges: Multiple black-box components (GPT-4.1 code selection) and unspecified prompt templates make faithful reproduction difficult
- Domain specificity: Method specialized for structured mathematical figures and may not generalize to domains requiring photorealistic textures or non-geometric visual reasoning

## Confidence
- **High confidence:** Code-driven visual reasoning improves geometric precision over pixel-level generation (supported by direct evidence and mechanism alignment)
- **Medium confidence:** Visual feedback enables self-correcting reasoning chains (novel mechanism with limited direct evidence)
- **Medium confidence:** Specialized image-to-code conversion enables scalable high-quality training data (converter shows good