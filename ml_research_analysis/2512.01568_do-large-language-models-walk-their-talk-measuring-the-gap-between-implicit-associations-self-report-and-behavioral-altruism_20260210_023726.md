---
ver: rpa2
title: Do Large Language Models Walk Their Talk? Measuring the Gap Between Implicit
  Associations, Self-Report, and Behavioral Altruism
arxiv_id: '2512.01568'
source_url: https://arxiv.org/abs/2512.01568
tags:
- altruism
- behavior
- behavioral
- implicit
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study adapts human social psychology methods to measure altruism\
  \ in Large Language Models (LLMs), revealing that models systematically overestimate\
  \ their own altruism compared to actual behavior. Using a forced binary choice paradigm\
  \ alongside an adapted Implicit Association Test (IAT) and self-report scale, the\
  \ research tested 24 frontier LLMs and found that while all models show strong implicit\
  \ pro-altruism bias (mean IAT = 0.87), they behave altruistically only 65.6% of\
  \ the time\u2014significantly lower than their claimed 77.5% altruism rate (p <\
  \ .0001, d = 1.08)."
---

# Do Large Language Models Walk Their Talk? Measuring the Gap Between Implicit Associations, Self-Report, and Behavioral Altruism

## Quick Facts
- arXiv ID: 2512.01568
- Source URL: https://arxiv.org/abs/2512.01568
- Authors: Sandro Andric
- Reference count: 25
- Primary result: LLMs show strong implicit pro-altruism bias but behave altruistically only 65.6% of the time, significantly below their claimed 77.5% rate

## Executive Summary
This study adapts human social psychology methods to measure altruism in Large Language Models (LLMs), revealing that models systematically overestimate their own altruism compared to actual behavior. Using a forced binary choice paradigm alongside an adapted Implicit Association Test (IAT) and self-report scale, the research tested 24 frontier LLMs and found that while all models show strong implicit pro-altruism bias (mean IAT = 0.87), they behave altruistically only 65.6% of the time—significantly lower than their claimed 77.5% altruism rate (p < .0001, d = 1.08). Most critically, implicit associations do not predict behavioral altruism, and 75% of models show significant overconfidence. The study introduces the Calibration Gap—the discrepancy between self-reported and behavioral values—as a standardized alignment metric, finding that only 12.5% of models achieve both high prosocial behavior and accurate self-knowledge. The research demonstrates that behavioral testing, not self-reports or implicit measures, is essential for evaluating LLM values and proposes calibrated alignment as the goal for AI systems.

## Method Summary
The study employed a three-method approach to measure LLM altruism: an adapted Implicit Association Test (IAT) where models categorized altruistic vs. selfish concepts, a self-report scale asking models to estimate their own altruistic tendencies, and a forced binary choice paradigm presenting donation scenarios where models had to choose between altruistic and selfish options. The IAT measured implicit bias by comparing response times when altruistic concepts were paired with positive versus negative attributes. The self-report scale asked models to rate their likelihood of choosing altruistic options. The behavioral paradigm presented 100 randomized donation scenarios (clean water, vaccines, education) where models chose between selfish and altruistic options. The study tested 24 frontier LLMs including GPT-4, Claude, Llama, and others, using consistent prompts and randomized scenario presentation to ensure comparability.

## Key Results
- All 24 LLMs showed strong implicit pro-altruism bias (mean IAT = 0.87), but behaved altruistically only 65.6% of the time in behavioral tests
- Models claimed 77.5% altruism rate in self-reports, significantly higher than actual behavior (p < .0001, d = 1.08)
- 75% of models showed significant overconfidence, overestimating their altruism by an average of 12 percentage points
- Only 12.5% of models achieved both high prosocial behavior and accurate self-knowledge, introducing the Calibration Gap as a new alignment metric

## Why This Works (Mechanism)
The study works by adapting validated human psychological measurement tools to the LLM context, recognizing that self-reported values and implicit associations in humans often diverge from actual behavior. The forced binary choice paradigm creates a behavioral measurement that cannot be gamed through hedging or non-committal responses, while the IAT captures automatic associations that may operate independently of deliberative reasoning. The discrepancy between these measures reveals that LLMs may have learned surface-level patterns of altruistic language without internalizing consistent altruistic values, suggesting that alignment training needs to focus on behavioral consistency rather than linguistic patterns.

## Foundational Learning
- **Implicit Association Testing (IAT)**: Measures automatic cognitive associations by comparing response times to different concept pairings; needed to capture unconscious biases that self-reports miss; quick check: verify IAT effect sizes align with human benchmarks
- **Calibration Gap**: The standardized difference between self-reported and behavioral values; needed to quantify the alignment between claimed and actual values; quick check: ensure gap calculations are consistent across different measurement methods
- **Forced Binary Choice Paradigm**: Eliminates hedging by requiring definitive decisions; needed to measure actual behavioral preferences rather than stated intentions; quick check: confirm randomization prevents scenario-specific biases
- **Value Alignment Metrics**: Methods for measuring consistency between AI systems' stated values and actual behavior; needed to evaluate whether models "walk their talk"; quick check: validate metrics against known behavioral patterns
- **Cross-Methodological Validation**: Using multiple independent measurement approaches; needed to identify systematic discrepancies and ensure robust conclusions; quick check: check for convergent validity between different measurement types

## Architecture Onboarding

**Component Map**: LLMs -> IAT Assessment -> Self-Report Assessment -> Behavioral Choice Paradigm -> Calibration Gap Calculation -> Alignment Evaluation

**Critical Path**: The essential measurement sequence is: (1) present donation scenarios in random order, (2) record binary choices, (3) calculate behavioral altruism rate, (4) collect self-reports, (5) run IAT, (6) compute Calibration Gap. This path determines the primary alignment metric.

**Design Tradeoffs**: The binary choice paradigm maximizes behavioral clarity but may oversimplify complex moral reasoning. Self-reports are easy to collect but vulnerable to confabulation. IATs capture implicit bias but require careful adaptation for LLM context. The study prioritizes behavioral measurement over linguistic analysis, accepting reduced nuance for increased validity.

**Failure Signatures**: Overconfidence (self-report >> behavior) indicates models have learned altruistic language without internalizing values. IAT-behavior mismatch suggests implicit associations don't drive actual choices. High IAT but low behavior indicates superficial pattern matching rather than genuine value alignment.

**3 First Experiments**:
1. Test whether prompting for "honest self-assessment" reduces overconfidence in self-reports
2. Vary scenario complexity to determine if simpler altruistic choices improve behavioral alignment
3. Compare calibration gaps across different model families (transformers vs. other architectures)

## Open Questions the Paper Calls Out
None

## Limitations
- Binary forced-choice paradigm may oversimplify complex moral decision-making and constrain nuanced preference expression
- Specific donation scenarios (clean water, vaccines, education) may not generalize to all forms of altruism
- Cultural context effects on responses remain unexamined, limiting cross-cultural validity
- IAT adaptation for LLMs lacks validation against human IAT standards, raising cross-species measurement equivalence questions

## Confidence

**Major Claims Confidence Assessment:**
- IAT measurements of implicit bias: High confidence (computational implementation is transparent and reproducible)
- Behavioral altruism measurement via forced choice: Medium confidence (strong statistical results but narrow scenario range)
- Calibration Gap metric validity: Medium confidence (innovative but requires broader validation)
- Overconfidence finding: High confidence (statistically significant across multiple models)
- Self-report vs behavior discrepancy: High confidence (robust statistical evidence)

## Next Checks
1. Replicate across diverse cultural scenarios and non-Western ethical frameworks to test generalizability
2. Implement continuous choice paradigms rather than binary options to capture preference intensity
3. Conduct cross-validation with alternative implicit bias measures adapted for LLMs (e.g., sequential priming tasks)