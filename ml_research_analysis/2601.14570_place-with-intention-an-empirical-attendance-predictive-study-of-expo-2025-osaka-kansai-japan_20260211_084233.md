---
ver: rpa2
title: 'Place with Intention: An Empirical Attendance Predictive Study of Expo 2025
  Osaka, Kansai, Japan'
arxiv_id: '2601.14570'
source_url: https://arxiv.org/abs/2601.14570
tags:
- attendance
- reservation
- data
- forecasting
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tackles the challenge of forecasting daily attendance
  at large-scale international events, exemplified by Expo 2025 Osaka, Kansai, Japan.
  Traditional approaches often depend on multi-source external data (e.g., weather,
  traffic), but these require extensive historical records and complex integration,
  which can be unreliable or impractical.
---

# Place with Intention: An Empirical Attendance Predictive Study of Expo 2025 Osaka, Kansai, Japan

## Quick Facts
- arXiv ID: 2601.14570
- Source URL: https://arxiv.org/abs/2601.14570
- Reference count: 34
- Primary result: Transformer-based framework using reservation dynamics achieves MAE 745.86 and MAPE 1944.82 for Expo 2025 Osaka attendance forecasting

## Executive Summary
This study addresses the challenge of forecasting daily attendance at large-scale international events by proposing a Transformer-based framework that leverages reservation dynamics as a proxy for visitor attendance intentions. Traditional forecasting approaches rely on multi-source external data integration, which requires extensive historical records and complex processing. The proposed approach implicitly captures external influences through reservation booking patterns without explicit multi-source fusion, making it more practical for real-world deployment. Experiments with Expo 2025 Osaka data demonstrate that modeling East and West gates separately improves accuracy, with the model outperforming strong baselines like TCN.

## Method Summary
The framework uses a Transformer architecture to process reservation data, treating booking and update patterns as signals of attendance intentions. The model employs an encoder-decoder structure with inverse-style embedding and an adaptive fusion module to capture temporal patterns and correlations in reservation dynamics. The approach models East and West gates separately to account for their distinct attendance patterns. Input consists of 7 days of reservation data, with a forecasting horizon of 5 days. The framework is evaluated against baseline models using real-world data from Expo 2025 Osaka, demonstrating superior performance in terms of MAE and MAPE metrics.

## Key Results
- Achieved MAE of 745.86 and MAPE of 1944.82 under two-channel setting with 7-day input and 5-day horizon
- East/West gate separation improved forecasting accuracy for short- and medium-term horizons
- Outperformed TCN baseline (MAE 779.10, MAPE 3273.98) in attendance prediction
- Ablation studies confirmed importance of encoder-decoder structure, inverse-style embedding, and adaptive fusion module

## Why This Works (Mechanism)
The approach works by using reservation dynamics as a proxy for attendance intentions, capturing implicit external influences through booking and update patterns rather than explicit multi-source data integration. The Transformer architecture effectively processes sequential reservation data to identify temporal patterns and correlations that precede actual attendance. Separate modeling of East and West gates accounts for distinct attendance behaviors at different venue entrances. The encoder-decoder structure with adaptive fusion captures both local temporal patterns and global trends in reservation data, while inverse-style embedding helps the model learn attendance-related features from reservation dynamics.

## Foundational Learning
- **Transformer Architecture**: Why needed - to capture complex temporal dependencies in sequential reservation data; Quick check - verify attention mechanisms properly weight different time steps
- **Reservation Dynamics as Proxy**: Why needed - provides implicit capture of external influences without requiring explicit multi-source data fusion; Quick check - validate reservation patterns correlate with actual attendance
- **Separate Gate Modeling**: Why needed - East and West gates exhibit distinct attendance patterns requiring independent modeling; Quick check - compare joint vs. separate gate modeling performance
- **Encoder-Decoder Structure**: Why needed - enables learning of both local temporal patterns and global trends; Quick check - verify temporal receptive fields capture relevant patterns
- **Adaptive Fusion Module**: Why needed - combines multiple information streams effectively for attendance prediction; Quick check - test fusion module performance with different combination strategies
- **Inverse-Style Embedding**: Why needed - helps learn attendance-related features from reservation dynamics; Quick check - compare with standard embedding approaches

## Architecture Onboarding

Component Map: Reservation Data -> Encoder -> Adaptive Fusion -> Decoder -> Attendance Forecast

Critical Path: The critical path runs from reservation data through the encoder to capture temporal patterns, through the adaptive fusion module to combine information streams, and finally through the decoder to generate the attendance forecast. The inverse-style embedding serves as a crucial preprocessing step that transforms raw reservation data into a format suitable for the Transformer architecture.

Design Tradeoffs: The framework trades explicit external data integration (weather, traffic) for implicit capture through reservation dynamics, reducing data complexity but potentially missing direct external influences. Separate gate modeling increases architectural complexity but improves accuracy. The Transformer architecture provides powerful temporal modeling but requires substantial computational resources compared to simpler baselines like TCN.

Failure Signatures: High MAPE values may indicate poor generalization to different attendance patterns or reservation systems. Performance degradation could occur when reservation update frequencies differ significantly from training data. The model may fail to capture sudden attendance changes not reflected in reservation patterns, such as weather-related cancellations or transportation disruptions.

First Experiments:
1. Test model sensitivity to different input window lengths (3, 7, 14 days) to find optimal temporal context
2. Compare performance with and without inverse-style embedding to validate its contribution
3. Evaluate separate vs. joint gate modeling on held-out test data to quantify accuracy gains

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on single real-world dataset from Expo 2025 Osaka limits generalizability to other events
- Extremely high MAPE value (1944.82%) raises questions about practical utility of forecasts
- Lack of detailed reservation data characteristics (booking lead times, cancellation rates) makes it difficult to assess whether reservation dynamics truly capture attendance intentions
- Limited baseline comparisons - missing relevant architectures like LSTMs, Prophet, or other attention-based models
- Insufficient ablation results showing individual contributions of architectural components

## Confidence

| Claim | Label |
|-------|-------|
| Transformer-based framework effectiveness with reservation data | High |
| East/West gate separation benefits for forecasting accuracy | Medium |
| Generalizability to other large-scale events and reservation systems | Low |

## Next Checks
1. Test the model on historical data from at least two other major international events with different attendance patterns and reservation systems
2. Conduct sensitivity analysis showing how different reservation update frequencies affect forecast accuracy
3. Perform ablation studies isolating the contributions of each architectural component (encoder-decoder structure, inverse-style embedding, adaptive fusion module)