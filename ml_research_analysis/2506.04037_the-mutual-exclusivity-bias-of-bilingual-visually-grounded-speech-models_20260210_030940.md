---
ver: rpa2
title: The mutual exclusivity bias of bilingual visually grounded speech models
arxiv_id: '2506.04037'
source_url: https://arxiv.org/abs/2506.04037
tags:
- bilingual
- familiar
- bias
- novel
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the mutual exclusivity (ME) bias\u2014\
  a language learning heuristic\u2014in bilingual visually grounded speech (VGS) models\
  \ trained on speech-image pairs from multiple languages. We improve upon prior work\
  \ by using modern self-supervised feature extractors (WavLM for audio, DINO for\
  \ images) and a simpler contrastive architecture, resulting in stronger monolingual\
  \ ME performance (~66% accuracy)."
---

# The mutual exclusivity bias of bilingual visually grounded speech models

## Quick Facts
- arXiv ID: 2506.04037
- Source URL: https://arxiv.org/abs/2506.04037
- Reference count: 0
- This study finds that bilingual visually grounded speech models generally exhibit weaker mutual exclusivity (ME) bias compared to monolingual models, with ~66% ME accuracy in monolingual vs. lower in bilingual settings.

## Executive Summary
This paper investigates the mutual exclusivity (ME) bias—a language learning heuristic where learners associate novel words with novel objects—in bilingual visually grounded speech (VGS) models. Using modern self-supervised feature extractors (WavLM for audio, DINO for images) and a simpler contrastive architecture, the authors find that monolingual models achieve ~66% ME accuracy while bilingual models generally show weaker ME bias, aligning with findings from child language studies. The analysis reveals that both mono- and bilingual models place novel concepts between familiar ones in the embedding space, but bilingual models pack familiar image embeddings more tightly, contributing to increased confusion between novel and familiar concepts. The work lays the foundation for future large-scale multilingual studies of ME in computational models.

## Method Summary
The study uses a two-tower contrastive architecture with frozen WavLM (audio) and DINO (image) feature extractors. Learnable transformer pooling layers project both modalities to 256-dimensional embeddings, which are L2-normalized and compared via dot product. The model is trained on speech-image pairs from multiple languages using a bidirectional contrastive loss. The familiar test evaluates class discrimination on 13 known concepts, while the ME test presents novel audio with both novel and familiar images to measure the preference for novel-novel pairings. Models are trained for 24 epochs with a linear warmup to 2e-4 learning rate followed by cosine annealing.

## Key Results
- Monolingual models achieve ~66% ME accuracy, while bilingual models generally show weaker ME bias
- Bilingual models pack familiar image embeddings more tightly in the embedding space, increasing confusion between novel and familiar concepts
- Cross-lingual translation is achieved implicitly through shared visual grounding, with >97% accuracy for all language pairs
- Both mono- and bilingual models place novel concepts between familiar ones in the embedding space, but bilingual models show tighter clustering of familiar visual embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mutual exclusivity (ME) bias emerges because novel audio embeddings cluster closer to novel image embeddings than to familiar ones in the joint embedding space.
- Mechanism: During contrastive training on familiar classes, the model spreads familiar audio-image pairs throughout the representation space. When tested on novel inputs, both novel audio and novel images occupy an "in-between" region where samples from different novel classes overlap, making novel-novel pairings score higher than novel-familiar pairings.
- Core assumption: The embedding geometry learned during familiar-class training generalizes predictably to unseen classes.
- Evidence anchors:
  - [abstract] "Analysis of the embedding spaces reveals that while both mono- and bilingual models show a modality gap and place novel concepts between familiar ones"
  - [section 3.2] "the overall variance of the novel data is much smaller than that of the familiar data...novel classes are placed in an in-between region"
  - [corpus] Weak direct support; no corpus papers address ME in VGS models specifically.
- Break condition: If novel classes were more widely dispersed than familiar ones, ME bias would weaken or reverse.

### Mechanism 2
- Claim: Bilingual models exhibit weaker ME bias partly because familiar image embeddings have reduced variance (tighter packing), increasing confusion between novel and familiar concepts.
- Mechanism: Adding a second language during training compresses the visual embedding space for familiar items. This tighter packing reduces the relative distance advantage of novel-to-novel comparisons, causing more novel items to be misidentified as familiar.
- Core assumption: The variance reduction in visual embeddings causally contributes to (rather than merely correlates with) weaker ME.
- Evidence anchors:
  - [abstract] "bilingual models pack familiar image embeddings more tightly, contributing to increased confusion between novel and familiar concepts"
  - [section 3.3] "A consistent change when adding either French or Dutch to the English model is that the spread across samples in the visual modality becomes tighter, in particular for familiar samples"
  - [corpus] No corpus papers examine bilingual vs. monolingual embedding variance in this context.
- Break condition: If image variance increased rather than decreased in bilingual models, ME should strengthen (all else equal).

### Mechanism 3
- Claim: Bilingual VGS models implicitly achieve cross-lingual translation by aligning audio embeddings from both languages to shared visual referents.
- Mechanism: Since both languages map to the same image space during training, audio embeddings for translation-equivalent words (e.g., "clock"/"klok") converge regardless of phonetic similarity. A nearest-centroid classifier in audio space can then "translate" between languages.
- Core assumption: Visual grounding provides sufficient constraint for cross-lingual alignment.
- Evidence anchors:
  - [section 3.4] "We quantify this translation performance...With this approach, we achieve translation accuracies over 97% for all language pairs"
  - [section 3.4] "This happens regardless of whether the two words sound similar (clock–klok) or not (horse–paard)"
  - [corpus] Weak support; corpus papers on multilingual speech models don't address this specific mechanism.
- Break condition: If visual embeddings were language-conditioned (separate spaces per language), translation accuracy would drop to near-random.

## Foundational Learning

- Concept: **Contrastive learning objective (InfoNCE-style)**
  - Why needed here: The model learns by maximizing similarity for matched audio-image pairs while minimizing it for negative samples. Understanding Eq. (1) is essential for interpreting why familiar classes spread apart and why novel classes cluster in-between.
  - Quick check question: Given a batch with 1 positive and 11 negative images per audio sample, what would happen to the loss if temperature τ increased dramatically?

- Concept: **Modality gap in multimodal embedding spaces**
  - Why needed here: Audio and image embeddings occupy different regions of the unit sphere despite alignment. This gap is a known property of contrastive learning and affects how cross-modal similarity scores should be interpreted.
  - Quick check question: Why might L2-normalized embeddings from different modalities end up in separate "cones" even when trained to maximize alignment?

- Concept: **Self-supervised pre-trained feature extractors (frozen backbones)**
  - Why needed here: WavLM and DINO provide fixed perceptual representations, simulating pre-linguistic perceptual abilities. Only the pooling layers are trained, which constrains what the model can learn and affects embedding geometry.
  - Quick check question: If DINO features were replaced with random projections, would the model still learn meaningful audio-image associations?

## Architecture Onboarding

- Component map:
  Audio waveform -> WavLM base-plus (768-dim, frozen) -> Transformer pooling (256-dim) -> L2 normalize
  Image -> DINO ResNet-50 (2048-dim, frozen) -> Transformer pooling (256-dim) -> L2 normalize
  Dot product with learnable temperature τ -> bidirectional contrastive loss

- Critical path:
  1. Audio and image pass through frozen encoders (WavLM/DINO)
  2. Each modality's sequence features are pooled via a transformer with learnable CLS token
  3. L2-normalized embeddings are compared via dot product
  4. Contrastive loss pulls positive pairs together, pushes negatives apart
  5. At test time, novel audio is scored against both novel and familiar images

- Design tradeoffs:
  - **Frozen vs. fine-tuned encoders**: Freezing improves efficiency and simulates fixed perceptual systems, but limits adaptability to domain-specific features.
  - **Shared vs. modality-specific pooling architectures**: Same pooling design for both modalities simplifies analysis but may not capture modality-specific structure optimally.
  - **Batch size/negative samples**: 11 negatives per positive; fewer negatives would weaken contrastive pressure to separate familiar classes.

- Failure signatures:
  - **High familiar test accuracy but ME ≈50%**: Model learned familiar associations but embedding geometry doesn't separate novel/familiar (check variance analysis).