---
ver: rpa2
title: Uncertainty Quantification for Multi-fidelity Simulations
arxiv_id: '2503.08408'
source_url: https://arxiv.org/abs/2503.08408
tags:
- neural
- data
- network
- deep
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The research addresses the challenge of uncertainty quantification
  in computational fluid dynamics by integrating high-fidelity and low-fidelity simulations.
  It uses Nektar++ for high-fidelity data and XFOIL for low-fidelity data to model
  aerodynamic properties like lift and drag coefficients.
---

# Uncertainty Quantification for Multi-fidelity Simulations

## Quick Facts
- **arXiv ID:** 2503.08408
- **Source URL:** https://arxiv.org/abs/2503.08408
- **Reference count:** 0
- **Primary result:** Multi-fidelity Deep Neural Network (MF-DNN) outperforms Co-kriging and RBF models in UQ for aerodynamic simulations, handling 100D problems efficiently.

## Executive Summary
This research addresses uncertainty quantification in computational fluid dynamics by integrating high-fidelity and low-fidelity simulations. The method uses Nektar++ for high-fidelity data and XFOIL for low-fidelity data to model aerodynamic properties like lift and drag coefficients. A multi-fidelity deep neural network (MF-DNN) is developed to combine these datasets, reducing reliance on expensive high-fidelity simulations. The MF-DNN approach, which includes linear and nonlinear corrections, demonstrates superior accuracy and efficiency compared to traditional methods like Co-kriging and Radial Basis Function models, particularly in high-dimensional problems.

## Method Summary
The method employs a sequential training approach with two neural networks: a Low-Fidelity Deep Neural Network (LF-DNN) that learns from abundant cheap data, followed by a Correction Deep Neural Network that maps the LF predictions to high-fidelity outputs using sparse expensive data. The correction formulation is unified under a single sub-network with ReLU activation, allowing it to autonomously select between linear and non-linear correction strategies. Bayesian optimization determines hyperparameters, and training uses ADAM followed by L-BFGS optimizers with MSE + L2 regularization loss. The framework is implemented in TensorFlow and validated on synthetic benchmark functions (1D, 32D, 100D) and CFD data for NACA0012 airfoil.

## Key Results
- MF-DNN accurately predicts probability density distributions and statistical moments for UQ across 1D, 32D, and 100D functions
- The method significantly lowers computational costs while maintaining precision compared to traditional Co-kriging approaches
- MF-DNN demonstrates capability in predicting aerodynamic properties with reduced reliance on expensive high-fidelity simulations

## Why This Works (Mechanism)

### Mechanism 1
The MF-DNN reduces sample efficiency burden by isolating general trend learning from error correction. The system decouples learning into two sequential stages: first, a Low-Fidelity DNN learns the general functional shape using abundant, cheap data. Second, a Correction DNN takes pre-trained LF predictions and sparse High-Fidelity data to learn a mapping function that bridges the accuracy gap, rather than learning the full physics from scratch using only expensive data.

### Mechanism 2
Using a unified sub-network with ReLU activation allows the model to autonomously select between linear and non-linear correction strategies without manual architecture switching. The ReLU activation function provides expressiveness to capture complex non-linear correlations while maintaining the ability to represent linear relationships, simplifying the training pipeline.

### Mechanism 3
MF-DNN enables high-dimensional Uncertainty Quantification by replacing matrix-inversion limits with scalable weight optimization. Traditional Co-Kriging models require inversion and storage of large covariance matrices, failing at 32D due to memory constraints. MF-DNN bypasses this by optimizing neural network weights via gradient descent, which scales efficiently to 100-dimensional inputs and massive datasets without requiring dense matrix storage.

## Foundational Learning

- **Concept: Spectral/hp Element Method**
  - **Why needed here:** This is the source of the High-Fidelity (HF) "ground truth" data. Higher polynomial orders yield higher accuracy at exponential computational cost.
  - **Quick check question:** Why does the paper use Polynomial Order 6 for training data instead of Order 2, and what is the computational tradeoff?

- **Concept: Co-Kriging (Multi-fidelity Kriging)**
  - **Why needed here:** This is the baseline algorithm the paper aims to defeat. Understanding that Co-Kriging fuses data using covariance matrices is essential to understanding why it crashes in high dimensions.
  - **Quick check question:** In Co-Kriging, how is the "cheap" data related to the "expensive" data according to the scaling parameter ρ?

- **Concept: Regularization (L2 Loss)**
  - **Why needed here:** The MF-DNN trains on very sparse HF data. Without L2 regularization, the network would overfit the few HF points available.
  - **Quick check question:** What component of the loss function prevents the Correction DNN from memorizing the noise in the small High-Fidelity dataset?

## Architecture Onboarding

- **Component map:** X -> LF-DNN -> Correction DNN -> ŷ_HF
- **Critical path:** You must train the LF-DNN to convergence first. Once trained, its weights are frozen (static). The Correction DNN is then trained on the error residuals between the frozen LF predictions and the sparse HF ground truth.
- **Design tradeoffs:**
  - Accuracy vs. Stability: ReLU is used for speed and linear approximation capability, trading off smoothness guarantees sometimes preferred in fluid dynamics.
  - Memory vs. Resolution: Co-Kriging provides statistical confidence intervals naturally but crashes on memory >32D. MF-DNN handles >100D but requires explicit training loops to approximate probability distributions.
- **Failure signatures:**
  - Co-Kriging Crash: System memory error (OOM) when attempting 32D+ matrix operations
  - MF-DNN Underfitting: If the Correction DNN is too small, MSE will plateau high, failing to bridge the gap between LF and HF data
  - Divergence: If LF data is fundamentally uncorrelated to HF data, the Correction network loss will not decrease
- **First 3 experiments:**
  1. Replicate 1D Linear Correlation: Implement using 21 LF points and 4 HF points. Verify that the Correction network learns the linear scaling ρ.
  2. Dimensionality Stress Test: Run the 32D function on both Co-Kriging and MF-DNN to confirm the memory crash in Co-Kriging and successful prediction in MF-DNN.
  3. CFD Data Fusion: Train the LF-DNN on XFOIL data and the Correction DNN on Nektar++ data. Plot the lift curve to see if the model interpolates between the known HF angles of attack.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proposed MF-DNN methodology be effectively scaled to handle 3-dimensional Large Eddy Simulations (LES) and Direct Numerical Simulations (DNS) at higher angles of attack?
- **Basis:** Future research encompasses 3-dimensional Large Eddy & Direct Numerical Simulations at higher angles of attack for NACA airfoil
- **Why unresolved:** The current study validated the methodology primarily using 2-dimensional incompressible flow solvers and synthetic benchmark functions
- **What evidence would resolve it:** Successful reconstruction of probability density distributions and statistical moments from high-dimensional 3D LES/DNS datasets

### Open Question 2
- **Question:** How does the MF-DNN model perform relative to Co-Kriging when conducting Uncertainty Quantification on high-dimensional physical CFD outputs, rather than synthetic benchmark functions?
- **Basis:** The study demonstrates MF-DNN superiority over Co-Kriging in 32D/100D using synthetic functions but only validates the aerodynamic data fusion using Co-Kriging in lower dimensions
- **Why unresolved:** It is unclear if the memory advantages of MF-DNN in high dimensions transfer directly to the specific data structures and noise profiles of actual CFD simulation outputs
- **What evidence would resolve it:** A comparison of MSE and memory consumption between MF-DNN and Co-Kriging when applied to a 32+ dimensional aerodynamic dataset

### Open Question 3
- **Question:** Can the framework accurately predict uncertainty propagation in complex internal turbomachinery flows, as distinct from the external aerodynamics tested?
- **Basis:** The problem statement claims this is the "first application of multi-fidelity deep neural network in solving the UQ problem for the turbomachinery flows"
- **Why unresolved:** The paper validates the method on external flow and mathematical benchmarks, leaving the specific application to internal turbomachinery flows unverified in the results
- **What evidence would resolve it:** Validation of the MF-DNN surrogate model against high-fidelity data from turbine vane or blade configurations

## Limitations
- The paper lacks explicit error bounds for multi-fidelity correction in high-dimensional settings
- The assumption of correlation between low-fidelity and high-fidelity data is not systematically tested across different flow regimes or airfoil geometries
- Computational cost comparison is limited to memory usage rather than wall-clock time

## Confidence

- **High Confidence:** The sequential training methodology and its implementation using TensorFlow are well-documented and reproducible. Superior performance of MF-DNN over Co-Kriging in high-dimensional problems is clearly demonstrated.
- **Medium Confidence:** The claim that ReLU activation sufficiently captures both linear and non-linear correction functions is supported by results but relies on empirical validation rather than theoretical guarantees.
- **Medium Confidence:** The application to NACA0012 aerodynamics is well-validated within specific conditions, but generalization to other flight conditions or geometries is not explicitly tested.

## Next Checks

1. **Dimensionality Robustness Test:** Implement the 32D analytical function and systematically increase dimensions beyond 100D to identify practical limits of the MF-DNN approach and compare error growth with theoretical predictions.
2. **Cross-Airfoil Validation:** Apply the trained MF-DNN (trained on NACA0012) to predict lift/drag coefficients for a different airfoil under the same flow conditions to test generalization capabilities and quantify transfer error.
3. **Computational Time Benchmarking:** Measure the complete wall-clock time for training the MF-DNN versus a single high-fidelity neural network trained only on sparse HF data, to validate claimed efficiency gains in practical deployment scenarios.