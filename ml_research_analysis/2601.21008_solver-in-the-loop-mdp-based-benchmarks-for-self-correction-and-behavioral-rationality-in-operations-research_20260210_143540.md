---
ver: rpa2
title: 'Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and Behavioral
  Rationality in Operations Research'
arxiv_id: '2601.21008'
source_url: https://arxiv.org/abs/2601.21008
tags:
- training
- type
- table
- arxiv
- constraint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two benchmarks that evaluate iterative self-correction
  and behavioral rationality in operations research (OR) by placing the solver in
  the evaluation loop, rather than treating OR as one-shot code generation. OR-Debug-Bench
  evaluates iterative debugging through 5,000+ problems with deterministic solver
  feedback via Irreducible Infeasible Subsystems (IIS), while OR-Bias-Bench evaluates
  inventory decision-making against closed-form optimal policies across 2,000 newsvendor
  instances.
---

# Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and Behavioral Rationality in Operations Research

## Quick Facts
- arXiv ID: 2601.21008
- Source URL: https://arxiv.org/abs/2601.21008
- Reference count: 40
- Primary result: 8B model outperforms frontier APIs on OR self-correction (95.3% vs 86.2% recovery) and achieves negative ID→OOD bias drift (-9.6%) via curriculum learning

## Executive Summary
This paper introduces two novel benchmarks that evaluate iterative self-correction and behavioral rationality in operations research by placing the solver in the evaluation loop. OR-Debug-Bench tests iterative debugging through 5,000+ problems with deterministic solver feedback via IIS, while OR-Bias-Bench evaluates inventory decision-making against closed-form optimal policies across 2,000 newsvendor instances. Domain-specific RLVR training with process rewards enables an 8B model to surpass frontier APIs in both tasks, demonstrating that process-level evaluation with verifiable oracles enables targeted training that outperforms scale.

## Method Summary
The method formalizes OR debugging as an MDP with hierarchical action spaces (Diagnostic/Repair/Meta) and uses Gurobi's IIS computation as a deterministic oracle. Composite reward-guided policy optimization (GRPO) trains with rewards for outcome, diagnosis, and efficiency. For behavioral rationality, three-stage curriculum learning exposes models to extreme cases before generalization. Both benchmarks use verifiable oracles (IIS computation, closed-form optimal policies) enabling process-level evaluation beyond one-shot code generation.

## Key Results
- 8B model achieves 95.3% recovery rate vs 86.2% for frontier APIs (+9.1%)
- Diagnostic accuracy improves from 47.8% to 62.4% (+14.6%) with domain-specific GRPO
- Three-stage curriculum uniquely achieves negative ID→OOD bias drift (-9.6%), reducing systematic bias by 48%
- Process-level evaluation reveals "lucky fixes" (high RR@5, low DA) invisible to standard metrics

## Why This Works (Mechanism)

### Mechanism 1: Composite Reward-Guided Policy Optimization with Verifiable Oracles
Domain-specific GRPO training with composite rewards (R = 0.5·R_outcome + 0.3·R_diagnosis + 0.2·R_efficiency) enables small models to outperform frontier APIs. The deterministic IIS oracle provides verifiable feedback for learning diagnostic reasoning rather than trial-and-error. Faithfulness penalty (-20) discourages repairs targeting non-IIS constraints.

### Mechanism 2: Staged Curriculum for Bias Mitigation in Decision-Making
Three-stage curriculum (extreme CR values → boundary refinement → full distribution) achieves negative ID→OOD bias drift (-9.6%). This prevents memorization of ID patterns by explicitly exposing models to bias-triggering extremes before gradual generalization.

### Mechanism 3: MDP Formulation with Hierarchical Action Spaces
Formalizing debugging as an MDP with structured action space (Diagnostic/Repair/Meta) enables systematic evaluation of reasoning quality. The state space and verifiable DA metric distinguish correct reasoning from lucky fixes.

## Foundational Learning

- **Irreducible Infeasible Subsystem (IIS)**: The minimal certificate of infeasibility that serves as the core diagnostic signal. Without IIS understanding, one cannot interpret reward signals or evaluate diagnostic accuracy.
  - Quick check: Given an infeasible LP with constraints {c1, c2, c3, c4} where IIS = {c1, c3}, which constraint should be diagnosed as the root cause?

- **Markov Decision Process (MDP) with Partial Observability**: The debugging task is formalized as an MDP where the agent infers hidden state (root cause) from observable feedback (IIS, status). Understanding MDP structure is essential for implementing GRPO.
  - Quick check: In OR-Debug-Bench MDP, does "Get IIS" modify the state? What about "Relax(c, δ)"?

- **Pull-to-Center Bias (Behavioral OR)**: Systematic deviation where models over-order when optimal is low and under-order when optimal is high. Understanding this bias is necessary to interpret bias drift metrics.
  - Quick check: If CR = 0.1, optimal Q* = 74, and a model orders Q = 95, does this exhibit pull-to-center bias? What is the bias magnitude?

## Architecture Onboarding

- **Component map**: Environment (Gurobi 11.0 + IIS oracle) -> Agent (Qwen3-8B with LoRA) -> Trainer (GRPO with composite reward) -> PRM (optional Qwen3-8B-LoRA for step supervision) -> Curriculum Controller (three-stage sampler for OR-Bias-Bench)

- **Critical path**: 1) SFT on 696 expert trajectories (success=True ∧ steps≤5 ∧ DA≥0.5) → 2) GRPO training with solver-in-the-loop evaluation (4 epochs, group size 4) → 3) Optional PRM integration (trades +4.7% DA for -3% RR@5) → 4) For OR-Bias-Bench: apply three-stage curriculum during SFT

- **Design tradeoffs**: DA weight: 30% vs 40% (40% yields higher DA but slower convergence); PRM usage: +4.7% DA vs -3% RR@5; Curriculum vs direct SFT: Curriculum improves OOD (-9.6% drift) but higher ID bias (20.0% vs 4.9%)

- **Failure signatures**: High RR@5 + low DA (>85% RR@5, <50% DA): trial-and-error dominance, needs increased diagnostic weight; Positive ID→OOD drift (+6% or higher): memorization, add curriculum; Token explosion (>5000 tokens/episode): inefficient strategies, add efficiency penalty

- **First 3 experiments**: 1) Baseline calibration: Run Qwen3-8B-SFT on 50 OR-Debug-Bench samples (target: RR@5 >90%, DA >55%); 2) Curriculum ablation: Train three variants (no curriculum / Stage 1 only / full) on OR-Bias-Bench and compare ID→OOD drift; 3) PRM impact test: Compare GRPO vs GRPO+PRM on 200-sample validation set

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation framework relies heavily on Gurobi's IIS computation as ground truth oracle, creating potential blind spots when IIS detection fails
- Behavioral rationality evaluation uses narrow parameter space (uniformly distributed CR values) that may not capture real-world demand distributions
- Curriculum learning results showing negative ID→OOD bias drift rely on specific three-stage design that may not generalize to other cognitive biases or OR domains

## Confidence
- **High confidence**: Solver-in-the-loop MDP formulation works as described (RR@5=95.3% vs 86.2%, DA=62.4% vs 47.8%)
- **Medium confidence**: 48% bias reduction claim relies on internal comparisons and specific OR-Bias-Bench setup
- **Low confidence**: Claims about process-level evaluation outperforming scale extrapolate from one 8B model to broader assertions

## Next Checks
1. **Oracle robustness test**: Systematically inject IIS computation errors (10-30% corruption) into validation set and measure degradation in DA and RR@5
2. **Domain transfer experiment**: Apply full curriculum pipeline to vehicle routing or scheduling domain and measure whether negative ID→OOD bias drift (-9.6%) replicates
3. **Scaling sensitivity analysis**: Train same pipeline with 2B, 4B, 16B, and 32B parameter models using identical reward structures and curriculum design to test if 8B advantage persists across scales