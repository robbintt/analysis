---
ver: rpa2
title: 'Hierarchical Deep Research with Local-Web RAG: Toward Automated System-Level
  Materials Discovery'
arxiv_id: '2511.18303'
source_url: https://arxiv.org/abs/2511.18303
tags:
- surface
- pfas
- page
- across
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a hierarchical deep research agent with local-first
  retrieval-augmented generation and adaptive tree-structured planning (DToR) for
  complex materials discovery problems. The system combines local RAG with web search,
  expanding and pruning research branches to maximize coverage, depth, and coherence
  while operating within explicit resource budgets.
---

# Hierarchical Deep Research with Local-Web RAG: Toward Automated System-Level Materials Discovery

## Quick Facts
- arXiv ID: 2511.18303
- Source URL: https://arxiv.org/abs/2511.18303
- Authors: Rui Ding; Rodrigo Pires Ferreira; Yuxin Chen; Junhong Chen
- Reference count: 40
- One-line result: DToR with open-source LLMs achieved 8.57/10 average score, ranking first overall in materials discovery tasks while reducing cost versus commercial solutions

## Executive Summary
This work introduces a hierarchical deep research agent with local-first retrieval-augmented generation and adaptive tree-structured planning (DToR) for complex materials discovery problems. The system combines local RAG with web search, expanding and pruning research branches to maximize coverage, depth, and coherence while operating within explicit resource budgets. Evaluated across 27 nanomaterials/device topics using LLM-as-judge rubrics and dry-lab simulations, DToR with open-source LLMs (gpt-oss120B) achieved an average score of 8.57/10, ranking first overall and outperforming commercial solutions at substantially lower cost. The framework demonstrates robust performance improvements over single-instance deep research across different model backbones and local corpus budgets, while enabling on-prem integration with local data and tools.

## Method Summary
DToR is a hierarchical deep research framework that combines local-first RAG with web search and adaptive tree-structured planning. The system uses LangGraph to orchestrate a tree controller with specialized roles (diversifier, router, analyst, knowledge gap explorer, synthesizer) that expand and prune research branches based on resource budgets. Local retrieval is performed via Chroma vector store using BAAI/bge-m3 embeddings over a 1.1M-paper corpus spanning sensors, batteries, catalysis, and semiconductors. Web search via DuckDuckGo provides complementary coverage. The framework operates with explicit budgets (max depth 3, max branches 3, nodes per branch 100, total cap 500) and supports both single DR instances and multi-branch DToR mode. Evaluation uses an LLM-as-judge rubric across five dimensions (relevance, depth, clarity, applicability, novelty) with pairwise A/B duels, plus dry-lab DFT/AIMD validation on five tasks.

## Key Results
- DToR with open-source LLMs achieved average score of 8.57/10 across 27 materials discovery topics
- System ranked first overall in comparative evaluation against commercial solutions
- Achieved substantial cost reduction compared to commercial alternatives while maintaining superior performance
- Demonstrated robust improvements over single-instance deep research across different model backbones and corpus budgets

## Why This Works (Mechanism)
The hierarchical approach with local-first RAG enables comprehensive materials discovery by leveraging both domain-specific local knowledge and complementary web information. The adaptive tree-structured planning allows systematic exploration of research spaces while maintaining coherence through explicit resource budgeting. The local corpus provides deep domain expertise and reduces hallucination risk, while web search fills knowledge gaps and ensures currency. The multi-role controller optimizes branch expansion and pruning to maximize coverage within computational constraints.

## Foundational Learning
- **Local-first RAG with adaptive tree planning**: Combines domain-specific local knowledge with web search for comprehensive coverage while controlling computational costs
- **Multi-role tree controller**: Orchestrates specialized agents (diversifier, router, analyst, knowledge gap explorer, synthesizer) to optimize research branch expansion and pruning
- **LLM-as-judge evaluation**: Uses automated scoring across five dimensions (relevance, depth, clarity, applicability, novelty) with pairwise comparisons for robust assessment
- **Dry-lab simulation validation**: Employs DFT and AIMD to validate material proposals, with explicit-solvent simulations preferred for aqueous systems
- **Resource-aware hierarchical planning**: Operates within explicit budgets (depth, branches, nodes, total cap) to balance exploration and computational efficiency
- **Local corpus construction**: Builds domain-specific vector stores from large paper collections using embeddings and section-aware processing

## Architecture Onboarding
**Component map**: User Query -> Tree Controller -> (Diversifier/Router/Analyst/Gap Explorer/Synthesizer) -> Local RAG (Chroma + BAAI/bge-m3) -> Web Search (DuckDuckGo) -> LLM Outputs -> LLM-as-judge Evaluation -> Dry-lab Validation

**Critical path**: Query reception → Tree controller planning → Local RAG retrieval → Branch expansion/pruning → Web search augmentation → Synthesis and validation

**Design tradeoffs**: Local-first RAG prioritizes domain expertise and reduces hallucination but requires substantial corpus construction; web search ensures currency but may introduce noise; hierarchical planning maximizes coverage but increases complexity

**Failure signatures**: Hallucinated materials proposals (incompatible phase combinations); static DFT rankings reversing under AIMD; insufficient local corpus coverage leading to over-reliance on web search

**First experiments**:
1. Construct minimal local corpus (100 papers) with BAAI/bge-m3 embeddings and test retrieval precision on sample queries
2. Run single DR instance on one topic with local RAG only, evaluate with LLM-as-judge rubric
3. Compare single DR vs DToR mode on one topic, measuring coverage and coherence scores

## Open Questions the Paper Calls Out
None

## Limitations
- Reproducibility constraints due to unspecified "gpt-oss120B/20B" model weights and incomplete agent prompt documentation
- Evaluation scope limited to 27 nanomaterials/device topics with validation on only five dry-lab cases
- Computational transparency lacking - no absolute compute requirements or scaling analysis provided
- Claims about cost-effectiveness depend on undisclosed commercial baseline implementations

## Confidence
- **High confidence**: Hierarchical architecture, local-first RAG integration, and tree-structured planning methodology are clearly specified and implementable
- **Medium confidence**: LLM-as-judge evaluation framework is internally consistent but lacks external expert validation
- **Low confidence**: Cost-effectiveness claims and absolute performance superiority require access to commercial baselines and detailed compute metrics

## Next Checks
1. Reproduce the local corpus pipeline using domain-scoping templates from Appendix D, creating a 500k-1M paper vector store with BAAI/bge-m3 embeddings, and evaluate retrieval precision on held-out materials discovery queries

2. Benchmark against alternative deep research frameworks (AutoRAG, Elicit, SciAgents) on a held-out test set of materials discovery problems, using both LLM-as-judge and expert human evaluation on a subset of 5-10 tasks

3. Conduct computational scaling analysis measuring wall-clock time, token usage, and inference costs across different local corpus sizes (50k, 500k, 1M papers) and model scales (7B, 32B, 70B parameters) on standardized hardware