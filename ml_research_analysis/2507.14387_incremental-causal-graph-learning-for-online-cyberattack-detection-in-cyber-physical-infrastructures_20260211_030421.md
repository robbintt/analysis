---
ver: rpa2
title: Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical
  Infrastructures
arxiv_id: '2507.14387'
source_url: https://arxiv.org/abs/2507.14387
tags:
- causal
- graph
- detection
- data
- incremental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: INCADET introduces an incremental causal graph learning framework
  for real-time cyberattack detection in critical infrastructures. It addresses the
  limitations of static causal methods by dynamically updating causal graphs to adapt
  to evolving attack patterns and distribution shifts.
---

# Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures

## Quick Facts
- arXiv ID: 2507.14387
- Source URL: https://arxiv.org/abs/2507.14387
- Reference count: 40
- INCADET achieves superior F1-scores, ROC-AUC, and PRC-AUC on SWaT, WADI, TE, and SMD datasets compared to static causal and deep learning baselines.

## Executive Summary
INCADET introduces an incremental causal graph learning framework for real-time cyberattack detection in critical cyber-physical infrastructures. The method addresses limitations of static causal approaches by dynamically updating causal graphs to adapt to evolving attack patterns and distribution shifts. By combining early symptom detection via edge weight divergence, incremental causal graph learning with experience replay and causal edge reinforcement, and graph convolutional networks for classification, INCADET demonstrates significant performance improvements over traditional static methods across multiple real-world datasets.

## Method Summary
The framework operates in three main stages: (1) Early Symptom Detection using DYNOTEARS to construct static causal graphs for consecutive time windows, computing Jensen-Shannon Divergence of edge weight distributions to trigger incremental learning when divergence exceeds threshold τ℘; (2) Incremental Causal Graph Construction that activates upon trigger, incorporating experience replay with causal edge reinforcement to preserve attack knowledge while adapting to distribution shifts; (3) Graph Classification using Graph Convolutional Networks trained on the updated causal graphs to distinguish attack from normal states. The approach processes streaming data segmented into fixed time windows, with DYNOTEARS capturing time-lagged dependencies and DGCNN performing binary classification via Laplacian convolution.

## Key Results
- Achieves higher F1-scores, ROC-AUC, and PRC-AUC compared to static causal and deep learning baselines across SWaT, WADI, TE, and SMD datasets
- Demonstrates improved memory efficiency through replay buffer pruning while maintaining classification accuracy
- Shows reduced missed alarm rates in detecting attacks across diverse cyber-physical infrastructure scenarios
- Ablation studies confirm critical role of replay buffer in preventing catastrophic forgetting during incremental updates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Divergence in causal edge weight distributions signals system state transitions before attacks fully manifest.
- Mechanism: Static causal graphs are constructed for consecutive time windows using DYNOTEARS. Edge weight distributions are discretized into bins and normalized. Jensen-Shannon Divergence quantifies structural divergence; when similarity ℘G falls below threshold τ℘, incremental learning is triggered.
- Core assumption: Attack precursors measurably alter causal structure before cascading effects become observable in raw sensor data.
- Evidence anchors:
  - [abstract] "Detects transitions in system status using divergence in edge-weight distributions across sequential causal graphs."
  - [section III.A] "℘Gt+1|t = 1 − JS((ω(Gt)||(ω(Gt+1))... T = Gtk |℘Gtk|tk−1 > τ℘"
  - [corpus] Related work on structural divergence (arXiv:2508.09504) suggests causal graph profiling for anomaly detection, supporting the structural-change premise, though direct empirical validation of early warning timing remains limited.
- Break condition: If edge weight distributions are stable under attack (e.g., slow-onset attacks with gradual drift), early symptom detection may miss precursors.

### Mechanism 2
- Claim: Experience replay with causal edge reinforcement preserves attack knowledge while adapting to distribution shifts.
- Mechanism: A prior-knowledge-pruned replay buffer stores attack-impact subgraphs (attack/impact node pairs with edge weights). During incremental updates, Causal Edge Reinforcement (CER) multiplies weights of recurring edges by ω while adding non-recurring buffered edges to prevent forgetting. Symmetric Laplacian normalization maintains spectral properties.
- Core assumption: Attack patterns recur; causal relationships from past attacks remain partially valid under distribution shift.
- Evidence anchors:
  - [abstract] "Leverages experience replay and edge reinforcement to continually refine causal structures while preserving prior knowledge."
  - [section III.B] "For edges E(i,j) extracted from Ek, we apply: Ã(Gk+1) = wki,j × ω, ∀Ei,j ∈ Ek"
  - [section IV.B2, ablation] "The replay buffer is critical in preventing catastrophic forgetting... In the absence of the buffer, performance deteriorates significantly."
  - [corpus] No direct corpus validation of CER specifically; related incremental learning work (arXiv:2504.04374) uses meta-learning for adaptation, but causal edge reinforcement is a novel contribution here.
- Break condition: If attacks exhibit non-repeating causal signatures or if prior knowledge is noisy/incorrect, reinforcement amplifies spurious edges.

### Mechanism 3
- Claim: Graph Convolutional Networks trained on causal graphs distinguish attack from normal states by leveraging topological patterns.
- Mechanism: Incremental causal graphs serve as ground-truth representations. DGCNN processes Laplacian representations through three convolutional layers with ReLU activation and dropout, outputting binary classification via sigmoid.
- Core assumption: Normal and attack system states exhibit separable causal graph topologies.
- Evidence anchors:
  - [abstract] "Employs Graph Convolutional Networks (GCNs) to classify system status using the learned causal graphs."
  - [section III.C] "H(i+1) = σ(Lagg ∗ H(i) ∗ W(i))... We use Binary Cross Entropy (BCE) Loss"
  - [section IV.B1] INCADET outperforms baselines in F1, ROC-AUC, PRC-AUC across SWaT, WADI, TE, SMD datasets.
  - [corpus] arXiv:2507.08177 highlights causality-driven cybersecurity for spatio-temporal anomaly detection, consistent with the graph classification approach.
- Break condition: If causal graphs for normal and attack states are topologically similar (e.g., subtle attacks preserving causal structure), GCN classification accuracy degrades.

## Foundational Learning

- Concept: **Causal Structure Learning (DYNOTEARS)**
  - Why needed here: Learns directed acyclic graphs from time-series data with time-lagged dependencies, capturing delayed causal effects in cyber-physical systems.
  - Quick check question: Can you explain how time-lag parameter τ affects detection of cascading attack impacts?

- Concept: **Continual/Incremental Learning & Catastrophic Forgetting**
  - Why needed here: Understanding why neural systems forget prior knowledge when learning new distributions explains the need for replay buffers and regularization.
  - Quick check question: What happens to F1-score if the replay buffer is removed, and why?

- Concept: **Graph Neural Networks for Classification**
  - Why needed here: GCNs aggregate neighborhood information through Laplacian convolution; understanding this enables debugging of graph classification failures.
  - Quick check question: How does symmetric Laplacian normalization prevent bias from high-degree attack nodes?

## Architecture Onboarding

- Component map:
  - Streaming data segmented into k-second windows -> Static causal graphs built per window -> JSD divergence computed between consecutive graphs -> If divergence > threshold, incremental learning triggered -> Replay buffer supplies prior attack edges; CER reinforces recurring patterns -> Normalized causal graph fed to trained DGCNN for classification

- Critical path:
  1. Streaming data segmented into k-second windows
  2. Static causal graphs built per window
  3. JSD divergence computed between consecutive graphs
  4. If divergence > threshold, incremental learning triggered
  5. Replay buffer supplies prior attack edges; CER reinforces recurring patterns
  6. Converged causal graph fed to trained DGCNN for classification

- Design tradeoffs:
  - **Time-lag τ**: Small τ misses delayed impacts; large τ increases computation and risks overfitting (see Fig. 3 sensitivity analysis).
  - **Reinforcement weight ω**: Higher ω prioritizes historical attack edges but may suppress novel attack signatures.
  - **Buffer size vs. memory**: Storing all past edges preserves knowledge but incurs overhead; subgraph pruning mitigates this.

- Failure signatures:
  - High false negatives during novel attacks: Replay buffer may not contain relevant causal patterns.
  - Excessive false positives after concept drift: Threshold τ℘ may be too sensitive; recalibration needed.
  - Degraded performance on high-dimensional systems (e.g., WADI): Curse of dimensionality affects causal discovery; consider feature selection.

- First 3 experiments:
  1. **Ablation on Replay Buffer**: Run INCADET with replay buffer disabled on SWaT; expect ~30-40% F1 drop (see Table 3).
  2. **Time-lag Sensitivity**: Vary τ from 1 to 8 on WADI; plot F1/ROC-AUC to identify optimal lag (Fig. 3 pattern).
  3. **Threshold Calibration**: Systematically vary τ℘ (e.g., 0.85-0.95) and measure trigger timing vs. ground-truth attack onset to validate early warning capability.

## Open Questions the Paper Calls Out

- Question: How can Large Language Models (LLMs) be effectively integrated with incremental causal frameworks to better utilize textual descriptions of causal knowledge in cybersecurity systems?
  - Basis in paper: [explicit] The conclusion explicitly identifies "exploring the use of large language models to harness the rich textual descriptions of causal knowledge" as a promising direction for future work.
  - Why unresolved: The current INCADET framework relies on structured multivariate time-series data and a Replay Knowledge Buffer fed by domain expertise, but does not currently process unstructured textual data often found in system logs or manuals.
  - What evidence would resolve it: An extension of the INCADET architecture that successfully ingests textual logs to automatically populate or refine the causal graph structures.

- Question: What theoretical guarantees can be established for robust causal structure learning within this incremental framework?
  - Basis in paper: [explicit] The conclusion lists "theoretical guarantees for robust causal structure learning" as a challenge that must be addressed for real-world deployment.
  - Why unresolved: While the paper demonstrates empirical success on specific datasets, it does not provide formal mathematical proofs regarding the convergence properties or error bounds of the incremental graph updates under concept drift.
  - What evidence would resolve it: A theoretical analysis providing bounds on the stability and accuracy of the causal graph updates as the number of time windows approaches infinity.

- Question: How sensitive is the framework to domain expert bias or inaccuracy when defining the Replay Knowledge Buffer?
  - Basis in paper: [explicit] The conclusion highlights "causal knowledge accuracy" and "domain expert interruption bias" as specific challenges that need addressing for deployment.
  - Why unresolved: The methodology assumes the prior knowledge (attack and impact nodes) used to prune the replay buffer is correct; the paper does not quantify performance degradation when this expert input is noisy or biased.
  - What evidence would resolve it: A sensitivity analysis measuring the drop in F1-score and ROC-AUC when the "ground truth" causal edges in the Replay Buffer are intentionally corrupted or skewed.

## Limitations
- Causal graph stability under high-dimensional data (WADI: 123 sensors) is not empirically validated beyond reported performance metrics
- No ablation on the prior-knowledge pruning mechanism itself—how much does it contribute to avoiding spurious edges?
- Fixed 15-minute window size may not adapt to varying attack onset times or system dynamics

## Confidence
- High Confidence: Early symptom detection via structural divergence (mechanistic clarity, consistent with related work)
- Medium Confidence: Replay buffer + CER effectiveness (supported by ablation but not fully validated against alternative incremental learning methods)
- Medium Confidence: GCN classification performance (competitive metrics but no cross-validation on hyperparameter sensitivity)

## Next Checks
1. **Ablation on Prior Knowledge Pruning**: Disable pruning and compare edge precision, recall, and classification F1 to quantify pruning's contribution
2. **Dynamic Threshold Calibration**: Implement an adaptive τ℘ that updates based on false alarm rate over time, rather than fixed per dataset
3. **Multi-step Ahead Prediction**: Measure whether early symptom detection predicts attack onset 1-3 time windows ahead (ground truth alignment)