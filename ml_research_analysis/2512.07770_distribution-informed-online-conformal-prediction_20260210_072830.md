---
ver: rpa2
title: Distribution-informed Online Conformal Prediction
arxiv_id: '2512.07770'
source_url: https://arxiv.org/abs/2512.07770
tags:
- coverage
- prediction
- learning
- width
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Conformal Optimistic Prediction (COP), a
  novel online conformal prediction algorithm designed to handle data distribution
  shifts in adversarial environments. Unlike traditional methods that rely solely
  on past errors, COP incorporates an estimated cumulative distribution function (CDF)
  of non-conformity scores into its update rule, enabling more efficient prediction
  sets when predictable patterns exist.
---

# Distribution-informed Online Conformal Prediction

## Quick Facts
- arXiv ID: 2512.07770
- Source URL: https://arxiv.org/abs/2512.07770
- Reference count: 40
- Primary result: COP achieves tighter prediction intervals than state-of-the-art baselines while maintaining valid coverage under distribution shifts in adversarial environments.

## Executive Summary
This paper introduces Conformal Optimistic Prediction (COP), a novel online conformal prediction algorithm designed to handle data distribution shifts in adversarial environments. Unlike traditional methods that rely solely on past errors, COP incorporates an estimated cumulative distribution function (CDF) of non-conformity scores into its update rule, enabling more efficient prediction sets when predictable patterns exist. The method is connected to online optimistic gradient descent, with a distribution-informed hint, and establishes a joint regret–coverage bound. Theoretically, COP achieves distribution-free, finite-sample coverage under arbitrary learning rates and converges with i.i.d. scores. Empirically, COP outperforms state-of-the-art baselines on simulation datasets under changepoints, distribution drift, and real-world time series from finance, energy, and climate domains, maintaining target coverage while producing tighter prediction intervals.

## Method Summary
COP operates by treating the conformal threshold update as an Optimistic Online Gradient Descent (OOGD) step, combining a standard gradient descent update on quantile loss with a refinement step using an estimated CDF of past non-conformity scores. The method maintains a sliding window of scores to compute the empirical CDF, then uses this to proactively adjust prediction intervals. The algorithm is parameterized by a learning rate η, a scale factor λ controlling trust in the CDF estimate, and a window size w for the CDF computation. The core innovation is the refinement step that uses the estimated CDF to shrink prediction intervals when the current threshold covers more than the target proportion of data.

## Key Results
- COP maintains valid coverage (target 90%) while producing significantly tighter prediction intervals than baselines across simulation and real-world datasets
- Theoretical guarantees show COP achieves distribution-free, finite-sample coverage under arbitrary learning rates
- The method converges with i.i.d. scores and establishes a joint bound on regret and coverage
- COP demonstrates superior performance on changepoint datasets, distribution drift scenarios, and real-world time series from finance, energy, and climate domains

## Why This Works (Mechanism)

### Mechanism 1
Incorporating an estimated Cumulative Distribution Function (CDF) as a "hint" allows the algorithm to tighten prediction sets proactively rather than reactively, while maintaining coverage guarantees. COP treats the threshold update as an Optimistic Online Gradient Descent (OOGD) step. Standard methods update based on past errors (gradient of quantile loss). COP adds a refinement step using the estimated CDF $\hat{F}_{t+1}$. If $\hat{F}_{t+1}$ suggests the current threshold covers more than the target $(1-\alpha)$, the threshold is reduced to shrink the set size before the next step occurs. This mechanism assumes that the estimated CDF deviation and the true CDF deviation have the same sign relative to the target quantile. The core assumption is that $[\hat{F}_{t+1}(\hat{q}_{t+1}) - (1-\alpha)]$ and $[F_{t+1}(\hat{q}_{t+1}) - (1-\alpha)]$ share sign.

### Mechanism 2
Long-term coverage validity is preserved distribution-free, meaning the algorithm remains robust even if the estimated CDF is inaccurate. The algorithm decouples the "validity" update (standard gradient descent on quantile loss) from the "efficiency" update (the optimistic refinement). Theorem 2 treats the optimistic term $M_t$ as a bounded perturbation. As long as the optimistic term is bounded, the long-term coverage error converges to the target $\alpha$, regardless of the true data distribution. The core assumption is that non-conformity scores $s_t$ and optimistic terms $M_t$ must be bounded ($s_t \in [0, B]$ and $M_t \in [-M, M]$).

### Mechanism 3
A joint bound on regret and coverage theoretically justifies the specific choice of the distribution-informed hint. By framing the problem as OOGD, the authors derive a bound involving both regret (efficiency) and coverage (validity) in Theorem 1. The optimal hint $M_t$ that minimizes the regret term is shown to align with the true CDF deviation $F_t(\hat{q}_t) - (1-\alpha)$. This provides a theoretical motivation for using the estimated CDF as the hint. The analysis relies on the convexity of the quantile loss function to derive the Bregman proximal inequality.

## Foundational Learning

- **Concept: Quantile Loss & Online Gradient Descent (OGD)**
  - Why needed here: The paper frames the conformal threshold update as a gradient descent step on the quantile loss $\ell_{1-\alpha}$. Minimizing $\ell$ corresponds to finding the correct quantile.
  - Quick check question: If the miscoverage rate is higher than $\alpha$, should the threshold $q_t$ increase or decrease? (Answer: Increase, to capture more data).

- **Concept: Optimistic Online Learning (OOGD)**
  - Why needed here: This is the theoretical engine of COP. It assumes the algorithm has a "hint" about the next gradient. If the hint is good, the algorithm converges faster; if bad, it falls back to standard robustness.
  - Quick check question: In OOGD, if the "hint" predicts a steep loss gradient, how does the learner adjust its parameter? (Answer: It preemptively adjusts to minimize the predicted loss).

- **Concept: Non-conformity Scores**
  - Why needed here: COP operates on the distribution of these scores $s_t$, not the raw data. The quality of the CDF estimation depends entirely on how $s_t$ is constructed.
  - Quick check question: If the base model improves drastically, what happens to the magnitude of $s_t$ and consequently the prediction set width? (Answer: $s_t$ shrinks, leading to tighter prediction sets, assuming the estimator adapts).

## Architecture Onboarding

- **Component map:** Base Predictor $\hat{f}_t$ -> Scorer (computes $s_t = |Y_t - \hat{Y}_t|$) -> CDF Estimator (sliding window of scores) -> COP Core (OGD update + Optimistic Refinement)

- **Critical path:** The Refinement Step (Line 7 in Algorithm 1). This is where the distribution information is injected. Failure to compute $\hat{F}$ efficiently or choosing a bad scale factor $\lambda$ directly impacts efficiency.

- **Design tradeoffs:**
  - **Scale Factor $\lambda$:** Controls trust in the estimated CDF.
    - $\lambda \approx 1$: High trust. Tighter sets if CDF is accurate, potentially jagged updates if CDF is noisy.
    - $\lambda \approx 0$: Low trust. Reverts to standard ACI/OGD behavior (robust but conservative).
  - **Window Size $w$:** Trade-off between adaptivity to recent shifts (small $w$) and stability of the CDF estimate (large $w$).

- **Failure signatures:**
  - **Infinite Width (ACI failure):** COP avoids this via the refinement step, but if the score variance explodes, intervals will widen significantly.
  - **Coverage Lag:** If the CDF estimator is stale, the "optimistic hint" may be wrong, causing the base OGD to do all the work, resulting in standard OGD performance.

- **First 3 experiments:**
  1. **Vanilla ECDF Test:** Run COP on a stationary dataset with $\lambda=0.5$. Verify that $q_t$ converges to the static quantile faster than OGD.
  2. **Ablation on $\lambda$:** Test with $\lambda \in \{0, 0.5, 1.0\}$ on a "Changepoint" dataset. Confirm that $\lambda=0$ is slow to adapt while $\lambda=1.0$ might be volatile but efficient.
  3. **Corrupted CDF Test (Adversarial):** Replace $\hat{F}$ with uniform noise to verify that coverage remains valid ($\approx 1-\alpha$) even when the "hint" is garbage, proving the safety mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
How can the scale factor λt+1/η be adaptively selected in a principled manner based on the reliability of the estimated CDF and base model performance? The paper sets λ = 0.5 as default but provides no principled selection method. The scale factor critically governs the refinement magnitude, yet current selection relies on heuristics or manual tuning without theoretical grounding.

### Open Question 2
Can COP achieve stronger theoretical guarantees (e.g., rate of convergence) under structured non-i.i.d. assumptions beyond the adversarial setting? Theorem 3 proves convergence "when scores are i.i.d.," but the paper only establishes long-term coverage without convergence guarantees for the general non-stationary case. Real time-series often exhibit structured dependencies that fall between i.i.d. and fully adversarial.

### Open Question 3
What are principled approaches for selecting the estimated CDF function ˆFt+1 across different data regimes? The paper uses ECDF and kernel-based estimation with comparable results, but notes that existing methods' selection of its scorecaster model is arbitrary and lacks principled guidance. Different estimators may excel under different temporal patterns (sharp changepoints vs. smooth drift).

## Limitations
- The boundedness assumption for scores and optimistic terms may fail in high-variance regimes
- The "distribution-informed" advantage assumes identifiable patterns, which may not exist in truly chaotic shifts
- Hyperparameter sensitivity to λ and window size is acknowledged but not extensively explored across domains

## Confidence
- **High confidence** in the core mechanism: the refinement step's ability to proactively shrink intervals when CDF estimates are accurate, as validated by Proposition 1 and Theorem 2
- **Medium confidence** in the empirical superiority, given the results hinge on specific baseline implementations and hyperparameter choices not fully detailed
- **Low confidence** in the robustness under extreme adversarial CDF corruption, as the safety mechanism is theoretically proven but not stress-tested beyond the stated simulation

## Next Checks
1. **Adversarial CDF Test**: Replace the estimated CDF with deliberately misleading estimates (e.g., inverted or uniform noise) to confirm long-term coverage remains valid even when the "hint" is maximally uninformative.
2. **Hyperparameter Sweep**: Systematically test λ ∈ {0.1, 0.3, 0.5, 0.7, 0.9} and window sizes w ∈ {50, 100, 200} across all three real-world datasets to quantify the efficiency-coverage tradeoff.
3. **Extreme Drift Benchmark**: Construct a synthetic dataset with abrupt, large-magnitude shifts (e.g., β_t jumps from (2,2,2,2) to (-2,-2,-2,-2)) to stress-test whether COP's regret-coverage bound prevents catastrophic interval inflation.