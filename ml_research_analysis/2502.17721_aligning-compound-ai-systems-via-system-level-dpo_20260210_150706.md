---
ver: rpa2
title: Aligning Compound AI Systems via System-level DPO
arxiv_id: '2502.17721'
source_url: https://arxiv.org/abs/2502.17721
tags:
- system
- preference
- alignment
- compound
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the challenge of aligning compound AI systems\u2014\
  architectures combining multiple interacting components like LLMs and diffusion\
  \ models\u2014where traditional single-model alignment techniques fall short. The\
  \ core problem stems from non-differentiable interactions between components and\
  \ the inability to directly map system-level preferences into component-level objectives."
---

# Aligning Compound AI Systems via System-level DPO

## Quick Facts
- arXiv ID: 2502.17721
- Source URL: https://arxiv.org/abs/2502.17721
- Reference count: 40
- This paper introduces SysDPO, a framework for aligning compound AI systems by extending Direct Preference Optimization to system-level objectives

## Executive Summary
This paper addresses the challenge of aligning compound AI systems—architectures combining multiple interacting components like LLMs and diffusion models—where traditional single-model alignment techniques fall short. The core problem stems from non-differentiable interactions between components and the inability to directly map system-level preferences into component-level objectives. To address this, the authors model compound systems as Directed Acyclic Graphs (DAGs) and propose SysDPO, a Direct Preference Optimization (DPO)-based framework with two variants: SysDPO-Direct (requires intermediate outputs) and SysDPO-Sampling (uses diverse candidate sampling). Theoretical analysis shows both variants achieve β-perfect alignment in the population setting. Empirical results demonstrate SysDPO-Direct improves a language model + diffusion model system's order consistency from 32% to 73% and preference score from -0.20 to 0.25. SysDPO-Sampling improves a two-LLM collaboration system's win rate against human preferences from 12.8% to 19.8%, outperforming both separate component alignment and unaligned prompting baselines. These results validate that joint system-level alignment enables better coordination and performance than optimizing components independently.

## Method Summary
SysDPO extends Direct Preference Optimization to compound AI systems by modeling them as Directed Acyclic Graphs (DAGs). The framework decomposes the joint probability distribution of system outputs into component-level conditional probabilities, enabling gradient-based optimization of system-level preferences. Two variants are proposed: SysDPO-Direct requires intermediate outputs and optimizes the exact joint likelihood, while SysDPO-Sampling approximates the optimization using diverse candidate sampling when intermediate states are unobserved. The method is theoretically grounded with convergence guarantees and empirically validated on two compound systems: an LLM + diffusion model for image generation and a two-LLM collaboration system. Training uses standard DPO objectives applied to the decomposed DAG structure, with hyperparameters tuned for each specific system setup.

## Key Results
- SysDPO-Direct improves LLM + diffusion system's order consistency from 32% to 73% and preference score from -0.20 to 0.25
- SysDPO-Sampling improves two-LLM collaboration system's win rate against human preferences from 12.8% to 19.8%
- Joint system-level alignment outperforms separate component alignment by 19% relative improvement in LLM+LLM system
- Diverse Beam Search with k=2 achieves 68.5% win rate vs. Monte Carlo sampling's 66.0-67.0% with k=8

## Why This Works (Mechanism)

### Mechanism 1: DAG-Based Probability Decomposition
- Claim: Modeling compound AI systems as Directed Acyclic Graphs enables tractable optimization of otherwise non-differentiable component interactions.
- Mechanism: The DAG structure allows factorization of the joint probability distribution pθ(s|x) into a product of conditional probabilities for each component. For example, in an LLM + diffusion system: p(s|x) = pψ(y|x) · ∏i pϕ(zi|yi), where y represents intermediate outputs (captions) and z represents final outputs (images). This decomposition transforms a single intractable optimization problem into multiple tractable sub-problems while preserving the dependency structure.
- Core assumption: Component interactions can be modeled as directed acyclic dependencies without feedback loops; the conditional independence assumptions implied by the DAG structure hold for the system.
- Evidence anchors:
  - [section 2.1]: "We model a compound AI system as a Directed Acyclic Graph (DAG), where nodes represent variables and edges capture the flow of information between components... This formulation with two examples... involves an LLM generating image captions, followed by a diffusion model."
  - [section 3.2]: Theorem 1 requires Assumption 1 (positive probability for any intermediate output), which is necessary for the decomposition to preserve alignment properties.
  - [corpus]: Related work on DPO variants (Mix- and MoE-DPO) also uses decomposition strategies, but for model capacity rather than system architecture. FMR scores (0.58-0.62) suggest this is an active research direction with moderate validation.
- Break condition: The DAG assumption fails when systems have cyclic dependencies, iterative refinement loops, or stateful interactions that require memory across time steps. The paper explicitly notes this limitation for "dynamic routing, feedback loops, or interactive collaboration."

### Mechanism 2: System-Level Preference Signal Distribution
- Claim: Optimizing the entire system with a unified preference objective produces better coordination than independently aligning components, even when individual components are well-aligned.
- Mechanism: SysDPO extends the DPO loss from single-model outputs to system-level outputs by replacing the single output probability pθ(z|x) with the joint probability of all generated variables pθ(s|x). The preference signal at the system level (e.g., "image sequence shows coherent progression") implicitly trains intermediate components (e.g., the LLM generating captions) to produce outputs that enable downstream components to succeed. This creates credit assignment across components without requiring explicit intermediate preferences.
- Core assumption: System-level preferences can be decomposed into learnable component-level behaviors through gradient-based optimization; the Bradley-Terry preference model extends to multi-component outputs.
- Evidence anchors:
  - [abstract]: "System-level preferences cannot be directly transformed into component-level preferences... We introduce SysDPO, a framework that extends Direct Preference Optimization (DPO) to enable joint system-level alignment."
  - [section 5.2.1]: Separate-DPO (independent alignment) achieves 16.6% WR-chosen vs. SysDPO-Sampling's 19.8%—a 19% relative improvement, demonstrating that joint optimization captures coordination patterns missed by independent training.
  - [corpus]: Distributionally Robust DPO (arxiv:2502.01930) addresses preference heterogeneity across populations, which is complementary but distinct from SysDPO's focus on heterogeneity across system components.
- Break condition: When intermediate outputs are high-dimensional or latent (e.g., vision encodings, multimodal embeddings), constructing preference datasets becomes prohibitively expensive. The paper notes this limitation for systems "with high-dimensional or latent outputs."

### Mechanism 3: Diverse Sampling for Intermediate State Approximation
- Claim: Diverse Beam Search with small sample sizes (k=2-4) can effectively approximate the intractable summation over all possible intermediate outputs when intermediate states are unobserved.
- Mechanism: When intermediate outputs y are unavailable, SysDPO-Sampling approximates pθ(z|x) = Σy pθ(s|x) by sampling k diverse, high-probability candidates {yα} using Diverse Beam Search. The diversity penalty ensures candidates span the output space, making the approximation robust even with few samples. This can be viewed as coreset selection—finding a representative subset that minimizes approximation error. Theoretically, coreset methods achieve O(1/√k) error bounds.
- Core assumption: The distribution over intermediate outputs is sufficiently peaked that high-probability samples dominate the summation; diverse sampling captures the essential variation needed for gradient estimation.
- Evidence anchors:
  - [section 5.2.3]: DBS with k=2 achieves 68.5% WR-Prompted vs. MC sampling's 66.0-67.0%, despite using fewer samples. The paper notes MC sampling "often produces near-duplicate candidates."
  - [section 2.3]: "We focus only on a small number of highly probable, distinct samples yαi, given that the less probable samples contribute little to the summation."
  - [corpus]: Related DPO work (BPO, CTR-Guided DPO) does not address the intermediate state problem; SysDPO's sampling approach appears novel for compound systems. Corpus lacks direct comparisons to alternative approximation methods.
- Break condition: When the intermediate output space is flat (many equally probable outputs) or when diversity penalties cause mode-dropping, the approximation degrades. The paper does not evaluate failure modes of the sampling strategy.

## Foundational Learning

- Concept: **Bradley-Terry Preference Model**
  - Why needed here: SysDPO builds on DPO, which assumes preferences follow the Bradley-Terry model: pref(z≻z'|x) = exp(r*(x,z)) / (exp(r*(x,z)) + exp(r*(x,z'))). Understanding this is essential for grasping how system-level preferences are modeled and optimized.
  - Quick check question: Given two system outputs with reward scores r1=2 and r2=1, what is the probability that output 1 is preferred? (Answer: e²/(e²+e¹) ≈ 0.73)

- Concept: **Direct Preference Optimization (DPO) Loss Function**
  - Why needed here: SysDPO extends the standard DPO loss L(θ) = -E[log σ(β log(pθ(zw|x)/pθ̄(zw|x)) - β log(pθ(zl|x)/pθ̄(zl|x)))] to compound systems. The paper modifies this by replacing single-output probabilities with system-wide joint probabilities.
  - Quick check question: In DPO, what role does the β parameter play? (Answer: It controls the strength of KL regularization, balancing preference optimization against deviation from the reference model.)

- Concept: **Jensen's Inequality and Convex Optimization Bounds**
  - Why needed here: The paper uses Jensen's inequality to derive an upper bound on the SysDPO loss for diffusion models, where likelihoods are intractable. Understanding this explains why the method works for non-differentiable components.
  - Quick check question: Why does Jensen's inequality allow us to optimize an upper bound instead of the exact loss? (Answer: Minimizing an upper bound guarantees the true loss decreases, provided the bound is tight enough.)

## Architecture Onboarding

- Component map:
```
[Input x] → [Component θ1 (e.g., LLM)] → [Intermediate y]
                                              ↓
         [Component θ2 (e.g., Diffusion)] ← [y propagated]
              ↓
         [Final Output z]
              ↓
    [Preference Oracle] → [Preference Signal]
              ↓
    [SysDPO Loss Computation]
              ↓
    [Gradient Updates to θ1, θ2 jointly]
```

- Critical path:
  1. **Define DAG structure**: Identify all components (nodes), their inputs/outputs (edges), and data flow dependencies. The paper uses Figure 2's examples as templates.
  2. **Construct preference dataset**: For SysDPO-Direct, collect (x, sw, sl) tuples where sw/sl contain both intermediate and final outputs. For SysDPO-Sampling, standard (x, zw, zl) pairs suffice.
  3. **Compute decomposed probabilities**: For each component, compute log-likelihoods pθi(yi|Pa(yi)) using the model's probability outputs.
  4. **Aggregate into system loss**: Sum component log-likelihoods according to DAG factorization, then apply DPO formula.
  5. **Backpropagate jointly**: Update all component parameters simultaneously using gradients from the unified loss.

- Design tradeoffs:
  - **SysDPO-Direct vs. SysDPO-Sampling**: Direct requires intermediate outputs (higher data collection cost) but is more precise; Sampling works with standard datasets but requires diverse sampling strategy and may have higher variance.
  - **Sample size k**: Paper finds k=2 sufficient with DBS; larger k increases computation linearly with diminishing returns. MC sampling requires larger k to match DBS performance.
  - **Diversity penalty**: Set to 20 in experiments. Higher values increase diversity but may drop high-probability modes; lower values risk duplicate samples.
  - **Training separate vs. joint**: Stage-wise training (freezing one component) converges faster but achieves lower final performance (Table 2: SysDPO-ψ1 peaks at ~step 10, joint at ~step 55).

- Failure signatures:
  - **Order Consistency remains low (<40%)**: Indicates LLM is not learning to generate structured prompts that guide downstream components. Check: is preference signal sufficiently capturing order, or is metric too noisy?
  - **Training instability after peak**: Paper observes performance decline after optimal point (step ~55 for joint training), attributed to DPO overoptimization. Mitigation: early stopping, learning rate decay.
  - **MC sampling underperforms DBS significantly**: Suggests intermediate output distribution is multi-modal; MC samples cluster around modes, missing diversity. Switch to DBS or increase k.
  - **Component-wise training outperforms joint**: May indicate preference signal is too noisy for effective credit assignment across components, or DAG structure doesn't match true dependencies.

- First 3 experiments:
  1. **Validate on minimal compound system**: Implement a two-stage text pipeline (summarizer → rewriter) on an existing DPO dataset (e.g., Intel/orca-dpo-pairs). Compare: (a) no alignment, (b) separate DPO per component, (c) SysDPO-Sampling. Expected: (c) > (b) > (a) on preference win rate. This reproduces Section 5.2 with minimal compute.
  2. **Ablate sampling strategy**: On the same pipeline, compare DBS with k∈{2,4,8} vs. MC sampling with k∈{2,4,8,16}. Measure: WR-Prompted, training time, and sample diversity (e.g., distinct n-gram ratio). Expected: DBS matches MC with 2-4x fewer samples.
  3. **Test break condition—cyclic system**: Implement a simple iterative refinement loop (generator → critic → generator with feedback). Apply SysDPO naively (ignoring cycle). Compare against a baseline that unrolls the loop as a DAG. Expected: Naive SysDPO degrades or fails to converge; this validates the DAG assumption and identifies scope for future work.

## Open Questions the Paper Calls Out
None

## Limitations
- The DAG assumption fails for systems with feedback loops or iterative refinement, limiting applicability to certain compound architectures
- Intermediate outputs must be either directly observable or approximable via diverse sampling, excluding systems with high-dimensional or latent intermediate states
- The sampling approximation lacks theoretical error bounds, and the paper does not compare against alternative approximation methods like importance sampling

## Confidence
- **High confidence** in the DAG-based probability decomposition mechanism (supported by clear mathematical formulation and theoretical analysis)
- **Medium confidence** in the system-level preference signal distribution claim (empirical results show improvement but rely on specific preference datasets)
- **Low confidence** in the diverse sampling approximation claims (limited ablation studies, no theoretical error bounds)

## Next Checks
1. Test SysDPO on a compound system with known feedback loops to quantify performance degradation when DAG assumptions are violated
2. Implement alternative sampling strategies (importance sampling, Monte Carlo with larger k) to benchmark against Diverse Beam Search approximation quality
3. Conduct adversarial preference testing where intermediate outputs are intentionally designed to mislead downstream components, measuring system robustness to preference signal manipulation