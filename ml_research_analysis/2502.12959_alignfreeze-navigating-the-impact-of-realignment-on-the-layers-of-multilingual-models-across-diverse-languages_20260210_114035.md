---
ver: rpa2
title: 'AlignFreeze: Navigating the Impact of Realignment on the Layers of Multilingual
  Models Across Diverse Languages'
arxiv_id: '2502.12959'
source_url: https://arxiv.org/abs/2502.12959
tags:
- realignment
- languages
- average
- language
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AlignFreeze addresses the challenge of degraded cross-lingual performance
  in multilingual language models after realignment, especially for languages differing
  from the source language. It introduces a method that freezes either the lower or
  upper half of the model layers during realignment, aiming to preserve important
  linguistic features while improving alignment.
---

# AlignFreeze: Navigating the Impact of Realignment on the Layers of Multilingual Models Across Diverse Languages

## Quick Facts
- **arXiv ID**: 2502.12959
- **Source URL**: https://arxiv.org/abs/2502.12959
- **Reference count**: 25
- **Primary result**: Front-freezing lower layers during realignment significantly improves Part-of-Speech tagging in typologically distant languages where full realignment fails

## Executive Summary
AlignFreeze addresses the challenge of degraded cross-lingual performance in multilingual language models after realignment, especially for languages differing from the source language. It introduces a method that freezes either the lower or upper half of the model layers during realignment, aiming to preserve important linguistic features while improving alignment. Controlled experiments across 4 tasks, 3 models, and 35 languages show that realignment affects all layers but is most detrimental to the lower ones. Freezing the lower layers during realignment significantly improves Part-of-Speech tagging performance in languages where full realignment fails, with XLM-R showing improvements of more than one standard deviation in accuracy for seven additional languages compared to full realignment.

## Method Summary
AlignFreeze modifies the multilingual model realignment process by selectively freezing layers during the alignment phase. The method takes a pre-trained multilingual language model (XLM-R, mBERT, or DistilMBERT) and applies one of two strategies: front-freezing (freezing lower layers) or back-freezing (freezing upper layers) before performing contrastive realignment using bilingual dictionary-aligned word pairs from the OPUS-100 corpus. Only the unfrozen layers are updated during realignment. After realignment, all layers are unfrozen and the model is fine-tuned on downstream tasks. The approach aims to preserve foundational linguistic features in frozen layers while still enabling cross-lingual alignment improvements in the unfrozen layers.

## Key Results
- Realignment impacts all layers but is most detrimental to lower layers, contradicting previous hypotheses that it only affects upper layers
- Front-freezing (freezing lower layers) significantly improves Part-of-Speech tagging accuracy in languages where full realignment fails
- With XLM-R, front-freezing provides improvements of more than one standard deviation in accuracy for seven additional languages compared to full realignment
- For semantic tasks like NLI and QA, realignment effects are mixed and less consistent across language pairs

## Why This Works (Mechanism)

### Mechanism 1: Lower-Layer Preservation During Realignment
Freezing lower layers during realignment preserves foundational linguistic features while still enabling cross-lingual alignment improvements. Lower layers of transformer models encode basic linguistic features (syntax, morphology). Realignment applied to all layers can corrupt these representations. Front-freezing allows the contrastive alignment loss to operate only on upper layers, preserving foundational language understanding while improving cross-lingual transfer.

### Mechanism 2: Catastrophic Forgetting Mitigation via Partial Freezing
Full-model realignment induces catastrophic forgetting of monolingual and cross-lingual features; partial freezing acts as regularizer. The contrastive realignment loss optimizes for multilingual embedding similarity but has no constraint to preserve existing useful representations. Freezing half the parameters limits the optimization space, implicitly regularizing against catastrophic forgetting.

### Mechanism 3: Task-Type Interaction with Layer Freezing Strategy
Front-freezing benefits low-level syntactic tasks (PoS tagging) more than high-level semantic tasks (NLI, QA). PoS tagging relies on morphological and syntactic features encoded in lower layers. NLI/QA require semantic reasoning in upper layers. Front-freezing preserves syntax-relevant weights while still allowing alignment optimization where semantic features reside.

## Foundational Learning

- **Concept: Cross-Lingual Realignment (Contrastive Loss)**
  - Why needed here: AlignFreeze modifies the realignment process. You must understand what realignment optimizes (maximizing similarity between aligned word pairs vs. negative samples) to understand why freezing matters.
  - Quick check question: Can you explain why the contrastive loss (Equation 1) might harm lower-layer representations when applied globally?

- **Concept: Layer-Wise Feature Distribution in Transformers**
  - Why needed here: The entire premise rests on lower layers encoding syntax and upper layers encoding semantics. Without this mental model, the freezing strategy appears arbitrary.
  - Quick check question: In BERT-family models, would you expect part-of-speech information to be more extractable from layer 2 or layer 10? Why?

- **Concept: Catastrophic Forgetting in Fine-Tuning**
  - Why needed here: The paper frames realignment failure as a forgetting problem. Understanding how gradient updates can overwrite useful pre-trained knowledge explains why partial freezing helps.
  - Quick check question: If you fine-tune a pre-trained model on a small dataset with a high learning rate, what risk emerges regarding previously learned representations?

## Architecture Onboarding

- **Component map:**
  Pre-trained mLM (XLM-R/mBERT/DistilMBERT) -> [Freeze Strategy Selection: Front-half OR Back-half] -> Realignment Phase (contrastive loss on unfrozen layers only) -> [Unfreeze all layers] -> Task Fine-tuning Phase (full model, standard cross-entropy) -> Cross-lingual Evaluation

- **Critical path:** The freezing is applied only during realignment (Step 2-3), not during downstream fine-tuning. If you freeze during fine-tuning, you will not recover the intended benefits.

- **Design tradeoffs:**
  - Front-freezing vs. Back-freezing: Paper shows front-freezing (lower layers frozen) consistently outperforms back-freezing for PoS. Back-freezing sometimes helps QA but results are noisy.
  - Half-layer split vs. granular: Appendix C.3 shows granular strategies (freezing single layers) provide minimal additional benefit over half-split.
  - Aligner choice: Bilingual dictionaries performed best in prior work; FastAlign/AwesomeAlign tested but not primary focus.

- **Failure signatures:**
  - Full realignment degrades performance on typologically distant languages (Farsi, Hebrew in XLM-R PoS).
  - Back-freezing often worse than no realignment for NLI (Table 2: XLM-R back-freezing = 72.9% vs. fine-tuning only = 73.9%).
  - High variance in QA results (Â±1.0+ F1) suggests the method is unreliable for semantic tasks.

- **First 3 experiments:**
  1. Baseline establishment: Run fine-tuning-only (no realignment) on your target task/language pair to establish a reference point. Compare against full realignment to confirm whether realignment helps or hurts your setting.
  2. Front-freezing validation: Apply AlignFreeze with front-freezing using bilingual dictionary aligner on OPUS-100 or equivalent parallel corpus. Measure per-language accuracy changes; identify which languages show >1 std deviation improvement.
  3. Ablation on language distance: Split evaluation languages into "typologically close to source" vs. "typologically distant from source" (use lang2vec distances). Verify that AlignFreeze helps more for distant languages, as the paper suggests.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the AlignFreeze methodology transfer effectively to recent decoder-only generative multilingual models?
- Basis in paper: Section 7.1 explicitly notes that realignment could be extended to "more recent decoder-only generative multilingual models like Bloom or XGLM."
- Why unresolved: The current study focused exclusively on encoder-only models (DistilMBERT, mBERT, XLM-R) due to their prevalence in the realignment literature.
- What evidence would resolve it: Experiments applying partial freezing strategies to large generative architectures like Bloom or Llama.

### Open Question 2
- Question: Can cross-lingual transfer success be accurately predicted based on linguistic features and realignment metrics?
- Basis in paper: Section 6 and 7.1 state that "Further research is needed to better understand what makes realignment fail under some conditions" because current methods are hard to predict.
- Why unresolved: The authors' regression analysis overfitted on language-specific features and failed to generalize to unseen languages.
- What evidence would resolve it: A robust predictive model that generalizes across languages to forecast realignment utility.

### Open Question 3
- Question: Can more granular freezing strategies improve performance over the half-layer approach?
- Basis in paper: Section 7.1 suggests that "More granular freezing strategies could be designed to better understand the role of each layer."
- Why unresolved: The authors employed a simple half-split for experimental control, and their preliminary granular tests were inconclusive.
- What evidence would resolve it: Systematic experiments freezing specific, non-contiguous layers (e.g., every other layer) to observe performance impacts.

## Limitations
- The benefits of AlignFreeze are primarily demonstrated for syntactic tasks like PoS tagging, with weaker and more inconsistent effects for semantic tasks like NLI and QA.
- The analysis focuses on English as the source language, leaving questions about performance when using non-English source languages or for different typological distances.
- The catastrophic forgetting mechanism is hypothesized but not directly measured, lacking empirical validation of representation degradation.

## Confidence

**High confidence**: The finding that realignment affects all layers (not just upper layers as previously hypothesized) and that front-freezing preserves lower-layer linguistic features while enabling alignment improvements. This is supported by controlled experiments across multiple models and tasks.

**Medium confidence**: The claim that front-freezing provides consistent benefits across all language pairs. While statistically significant for PoS tagging, the effect sizes for NLI and QA are small and variable, suggesting task and language pair dependencies.

**Low confidence**: The catastrophic forgetting mechanism explanation, which is hypothesized but not directly measured. The paper references this as a motivation but lacks empirical evidence showing representation degradation in unfrozen layers.

## Next Checks

1. **Layer-wise analysis validation**: Replicate the layer-wise realignment impact analysis by measuring average activation similarity between pre-trained and realigned models for each layer independently, confirming that lower layers show greater degradation than upper layers.

2. **Cross-source language experiment**: Repeat the front-freezing experiments using a non-English source language (e.g., French or German) to test whether the benefits generalize beyond English-centric alignment.

3. **Granular freezing ablation**: Test freezing individual critical layers (e.g., only layer 0 or layers 0-2) rather than half-layer splits to determine if selective freezing of specific foundational layers provides comparable benefits with less parameter restriction.