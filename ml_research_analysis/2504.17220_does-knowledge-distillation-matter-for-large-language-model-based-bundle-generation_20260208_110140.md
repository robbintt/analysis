---
ver: rpa2
title: Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?
arxiv_id: '2504.17220'
source_url: https://arxiv.org/abs/2504.17220
tags:
- knowledge
- bundle
- generation
- performance
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates knowledge distillation (KD) for efficient
  bundle generation using large language models (LLMs). The research addresses the
  high computational costs of deploying large-scale LLMs by transferring expertise
  from teacher models to compact student models.
---

# Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?

## Quick Facts
- arXiv ID: 2504.17220
- Source URL: https://arxiv.org/abs/2504.17220
- Reference count: 40
- Knowledge distillation effectively reduces computational requirements while maintaining or surpassing teacher model performance for LLM-based bundle generation

## Executive Summary
This study investigates knowledge distillation (KD) for efficient bundle generation using large language models (LLMs). The research addresses the high computational costs of deploying large-scale LLMs by transferring expertise from teacher models to compact student models. A comprehensive KD framework is proposed, extracting knowledge progressively from raw data in forms ranging from frequent patterns to formalized rules and deep thoughts. The framework employs various sampling strategies, domain/format accumulation, and complementary LLM adaptation techniques (in-context learning and supervised fine-tuning) to investigate how knowledge format, quantity, and utilization methods influence bundle generation performance. Extensive experiments on three real-world datasets demonstrate that knowledge distillation effectively reduces computational requirements while maintaining or surpassing teacher model performance, with student models achieving superior precision and coverage.

## Method Summary
The paper proposes a progressive knowledge distillation framework that extracts expertise from teacher LLMs in three forms: frequent patterns (via Apriori mining), formalized rules (via self-reflection), and deep thoughts (via chain-of-thought reasoning). The framework controls knowledge quantity through sampling strategies (random, length, diversity, difficulty) and multi-domain/format aggregation. Knowledge is utilized through in-context learning (ICL) with retrieval, supervised fine-tuning (SFT) with QLoRA, or their combination. The student model (Llama3.1-8B) is trained on bundle generation tasks and evaluated against baselines on three real-world datasets (Electronic, Clothing, Food) using session-level precision/recall and bundle-level coverage metrics.

## Key Results
- Student models achieve superior precision and coverage while reducing computational requirements
- Knowledge distillation maintains or surpasses teacher model performance across all three datasets
- Utilization method significantly impacts performance, with SFT+ICL combination proving most effective
- Different knowledge formats (patterns, rules, thoughts) capture complementary abstraction levels of bundling expertise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressive knowledge extraction (patterns → rules → thoughts) enables more effective transfer than single-format distillation because different knowledge formats capture complementary abstraction levels of bundling expertise.
- Mechanism: Frequent patterns provide statistical co-occurrence heuristics at category level; formalized rules capture conditional logic and causal relationships for why items bundle; deep thoughts encode context-sensitive reasoning chains. When properly integrated, these formats address different aspects of the bundle generation task—patterns improve recall through global trends, rules provide explicit guidelines, and thoughts enhance precision through dynamic reasoning.
- Core assumption: The effectiveness of each format depends on the utilization method (ICL vs. SFT); patterns may oversimplify when used alone, thoughts may misalign in ICL due to imperfect retrieval.
- Evidence anchors:
  - [abstract] "progressively extracts knowledge from raw data in increasingly sophisticated forms, i.e., frequent patterns → formalized rules → deep thoughts"
  - [section 4.2] "For Llama3.1-SFT, Thought achieves the highest Precision... while Pattern yields the best Recall... Rule demonstrates superior performance only in terms of Coverage"
  - [corpus] Limited direct corpus evidence on format-specific effects; related work focuses on routing mechanisms rather than format analysis.
- Break condition: When all three formats are naively combined in ICL, performance degrades due to context length burden and conflicting signals (Section 4.3.3).

### Mechanism 2
- Claim: Knowledge quantity impacts ICL and SFT differently—ICL scales monotonically with more data (reliance on retrieval pool), while SFT exhibits peak-then-plateau behavior and benefits more from diversity than raw volume.
- Mechanism: ICL retrieves relevant examples at inference time; larger knowledge pools increase retrieval quality. SFT internalizes knowledge through parameter updates; beyond optimal sampling ratio, additional data provides diminishing returns or introduces noise. Cross-domain and multi-format aggregation provides complementary insights that prevent overfitting in SFT.
- Core assumption: The relationship between sampling ratio and actual knowledge quantity is approximately linear (verified in Figure 3), and diversity-based strategies capture more representative distributions.
- Evidence anchors:
  - [abstract] "captures varying quantities of distilled knowledge through different sampling strategies, multi-domain accumulation, and multi-format aggregation"
  - [section 4.3.1] "For Llama3.1-ICL... performance consistently scales with the sampling ratio... For Llama3.1-SFT... reaches its peak with a certain ratio (e.g., 0.7) and then shows a slight drop"
  - [corpus] Neighboring paper on "Routing Distilled Knowledge via Mixture of LoRA Experts" suggests quantity-management through expert routing, aligning with diversity benefits.
- Break condition: For ICL with rule/thought knowledge in high-diversity domains (Electronic, Food), increased sampling ratios can reduce Recall by causing overfitting to specific bundle types.

### Mechanism 3
- Claim: The utilization method (ICL vs. SFT vs. combined) is the most impactful factor among the three investigated dimensions, with SFT+ICL combination achieving best results when knowledge types are carefully aligned across stages.
- Mechanism: SFT permanently embeds knowledge in model parameters, enabling internalized task understanding. ICL provides flexible, context-specific guidance at inference. Combined approach leverages both: SFT establishes foundational capability, ICL refines for specific sessions. Misalignment between knowledge used in SFT vs. ICL stages creates conflicting signals that degrade performance.
- Core assumption: The student model can effectively integrate parametric knowledge (from SFT) and non-parametric knowledge (from ICL) without catastrophic interference.
- Evidence anchors:
  - [abstract] "utilization method significantly impacts performance, with supervised fine-tuning combined with in-context learning proving most effective"
  - [section 4.4.1, Table 7] "Among the three factors... the utilization method generally has the greatest overall impact... RQ3: 39.62%"
  - [corpus] Self-distillation approaches (Self-Distilled Reasoner paper) show on-policy distillation benefits, supporting SFT effectiveness; limited corpus evidence on SFT+ICL combinations specifically.
- Break condition: When divergent knowledge types are used across SFT and ICL stages (e.g., SFT+Thought combined with ICL+Thought), negative transfer occurs (Figure 13, negative y-values for purple bars in certain configurations).

## Foundational Learning

- **Knowledge Distillation (KD) in Recommendation Systems**
  - Why needed here: The paper's core contribution is applying explicit KD to bundle generation, requiring understanding of how teacher-student transfer differs from traditional KD in vision/NLP.
  - Quick check question: Can you explain why explicit KD (output-based) is used instead of implicit KD (feature-based) for LLM-to-LLM transfer in this work?

- **Bundle Generation vs. Bundle Recommendation**
  - Why needed here: The paper addresses generation (constructing new bundles) not recommendation (ranking existing bundles); this distinction affects evaluation metrics and knowledge requirements.
  - Quick check question: What are the key differences in task formulation between identifying bundles within a session (generation) versus ranking pre-existing bundles (recommendation)?

- **In-Context Learning (ICL) vs. Supervised Fine-Tuning (SFT) for LLM Adaptation**
  - Why needed here: The paper systematically compares these two adaptation paradigms; understanding their mechanisms is essential for interpreting RQ3 results.
  - Quick check question: Why does ICL benefit more from larger knowledge quantities while SFT peaks at moderate amounts?

## Architecture Onboarding

- **Component map**: Raw session data → Pattern mining (Apriori) → Rule generation (teacher LLM self-reflection) → Thought generation (teacher LLM CoT) → Sampling strategy application → Student model training (SFT) / Inference preparation (ICL retrieval) → Bundle generation → Metric evaluation

- **Critical path**: Raw session data → Pattern mining (Apriori) → Rule generation (teacher LLM self-reflection) → Thought generation (teacher LLM CoT) → Sampling strategy application → Student model training (SFT) / Inference preparation (ICL retrieval) → Bundle generation → Metric evaluation

- **Design tradeoffs**:
  - **ICL vs. SFT**: ICL requires no training but needs large knowledge pool; SFT requires training but internalizes knowledge more robustly
  - **Knowledge format selection**: Patterns improve recall but may miss fine-grained item relationships; thoughts improve precision but may misalign in ICL retrieval
  - **Sampling strategy choice**: Random is simplest; difficulty-based targets hard cases but requires preliminary teacher inference; diversity-based captures distribution but may include noise
  - **Single-domain vs. cross-domain**: Single-domain knowledge is more targeted; cross-domain improves generalization but may introduce irrelevant patterns

- **Failure signatures**:
  1. **Low Recall despite high Precision**: Likely caused by knowledge that over-constrains the model (e.g., overly specific rules in ICL)
  2. **Performance degradation at high sampling ratios (SFT)**: Indicates data noise or overfitting; reduce ratio or switch to diversity-based sampling
  3. **Negative transfer in SFT+ICL combination**: Knowledge types misaligned across stages; ensure consistency (e.g., SFT+Pattern with ICL+Pattern)
  4. **ICL underperforming SFT significantly**: Knowledge pool too small or retrieval mechanism failing; increase sampling ratio or improve retrieval features

- **First 3 experiments**:
  1. **Establish baseline with format comparison**: Train Llama3.1-SFT with each knowledge format (Pattern, Rule, Thought) separately using 100% sampling ratio on Electronic domain; evaluate Precision, Recall, Coverage to validate format-specific effects claimed in Section 4.2.
  2. **Validate quantity sensitivity**: Run Llama3.1-ICL with Rule knowledge at sampling ratios {0.1, 0.3, 0.5, 0.7, 1.0} using random and diversity-based strategies on Clothing domain; verify monotonic scaling for ICL vs. peak-then-plateau for SFT (Figure 4 vs. Figure 7).
  3. **Test combined utilization with aligned knowledge**: Configure Llama3.1-SFT with Pattern knowledge and ICL with Pattern knowledge (aligned configuration); compare against misaligned configuration (SFT+Pattern with ICL+Thought) on Food domain to quantify alignment benefit shown in Figure 13.

## Open Questions the Paper Calls Out

- **Implicit Knowledge Distillation**: The study focuses on explicit knowledge distillation, which "may not fully capture the rich, implicit knowledge residing within the internal states" of the teacher LLM. Future work should explore implicit techniques to align student processing with the teacher's.

- **Adaptive Knowledge Selection**: The paper notes that "simply using all available knowledge is not always optimal" and performance is highly sensitive to the specific combination of knowledge used in SFT and ICL. The authors call for "adaptive or model-driven methods" to determine beneficial subsets automatically.

- **Multi-Modal Data Integration**: The authors explicitly state that "extending these knowledge distillation techniques to incorporate multi-modal data (e.g., item images) common in bundle generation scenarios remains an important avenue."

## Limitations

- Dependency on specific teacher LLM capabilities (GPT-3.5-turbo) for knowledge extraction quality
- Potential dataset-specific effects not validated across diverse domains
- Computational overhead of the knowledge extraction pipeline itself

## Confidence

- Primary KD framework effectiveness: **High** (validated across 3 datasets with multiple baselines)
- Format-specific knowledge effects: **Medium** (limited ablation studies, potential confounding with dataset characteristics)
- Combined utilization superiority: **Medium** (strong empirical support but complex interaction effects)
- Quantity-sensitivity patterns: **Medium** (clear trends but threshold values likely dataset-dependent)

## Next Checks

1. **Dataset Diversity Test**: Apply the complete KD pipeline to a substantially different bundle dataset (e.g., from a different domain like books or electronics with different purchasing patterns) to verify that format-specific effects and quantity sensitivity patterns hold beyond the original three datasets.

2. **Teacher Model Robustness**: Replace GPT-3.5-turbo with an open-source teacher LLM (e.g., Llama3.1-70B) for knowledge extraction and compare the quality of extracted patterns/rules/thoughts and downstream student performance to assess dependency on proprietary model capabilities.

3. **Knowledge Extraction Cost Analysis**: Measure and report the wall-clock time and API costs for extracting knowledge across all three formats for the largest dataset, and compare against the computational savings from using student models to provide a complete efficiency picture.