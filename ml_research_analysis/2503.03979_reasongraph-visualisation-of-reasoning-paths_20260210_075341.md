---
ver: rpa2
title: 'ReasonGraph: Visualisation of Reasoning Paths'
arxiv_id: '2503.03979'
source_url: https://arxiv.org/abs/2503.03979
tags:
- reasoning
- visualization
- platform
- reasongraph
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReasonGraph is a web-based platform for visualizing and analyzing
  LLM reasoning processes, supporting six sequential and tree-based reasoning methods
  across over 50 models. It addresses the challenge of analyzing complex LLM reasoning
  processes by providing real-time graphical visualization through a modular framework.
---

# ReasonGraph: Visualisation of Reasoning Paths

## Quick Facts
- arXiv ID: 2503.03979
- Source URL: https://arxiv.org/abs/2503.03979
- Authors: Zongqian Li; Ehsan Shareghi; Nigel Collier
- Reference count: 4
- One-line primary result: Web-based platform for visualizing and analyzing LLM reasoning processes across 6 methods and 50+ models

## Executive Summary
ReasonGraph is a web-based platform that visualizes and analyzes LLM reasoning processes across six sequential and tree-based methods (Chain-of-Thought, Self-consistency, Least-to-Most, Self-refine, Tree-of-Thoughts, Beam Search) with support for over 50 models from major providers. The platform addresses the challenge of analyzing complex LLM reasoning by providing real-time graphical visualization through a modular Flask backend and Mermaid.js frontend. It achieves nearly 100% parsing accuracy for properly formatted outputs with negligible visualization generation time, demonstrating strong usability with approximately 90% of users successfully using the platform without assistance.

## Method Summary
ReasonGraph employs a three-module Flask backend architecture consisting of Configuration Manager, API Factory, and Reasoning Methods modules that work together to process user queries and generate structured reasoning outputs. The system uses rule-based XML parsing to extract reasoning paths from LLM responses, which are then visualized in real-time using Mermaid.js on the frontend. The platform supports RESTful API communication between components and allows users to configure API keys, select reasoning methods, and compare outputs across different models through a web interface. The approach achieves high parsing accuracy when LLMs produce properly formatted XML outputs, though this depends on the models' ability to follow specific instruction formats.

## Key Results
- Nearly 100% parsing accuracy for properly formatted LLM outputs with negligible visualization generation time
- Approximately 90% of users successfully used the platform without assistance
- Supports 6 reasoning methods across 50+ models from Anthropic, OpenAI, Google, and Together.AI

## Why This Works (Mechanism)
ReasonGraph works by standardizing reasoning process visualization through a modular framework that separates configuration, API interaction, and reasoning method logic. The XML parsing architecture provides near-perfect accuracy when models produce properly formatted outputs, while Mermaid.js enables real-time, interactive visualization of complex reasoning paths. The platform's ability to compare reasoning methods across multiple models simultaneously allows users to identify optimal approaches for specific tasks, and the graphical interface reduces cognitive load compared to analyzing raw text outputs.

## Foundational Learning
- **LLM reasoning methods**: Understanding Chain-of-Thought, Tree-of-Thoughts, and other reasoning approaches is essential to interpret visualized reasoning paths correctly. Why needed: Different methods produce structurally distinct outputs that affect visualization interpretation. Quick check: Can you identify which reasoning method produced a given visualization based on its structure?
- **XML parsing for structured outputs**: Rule-based parsing converts LLM text outputs into structured data for visualization. Why needed: Enables systematic extraction of reasoning steps from unstructured text. Quick check: Does the parser correctly handle nested reasoning structures in test XML samples?
- **Mermaid.js visualization**: JavaScript library that generates flowcharts and diagrams from text-based graph descriptions. Why needed: Provides interactive, real-time visualization without complex rendering pipelines. Quick check: Can you create a simple flowchart using Mermaid syntax?

## Architecture Onboarding

**Component map**: User Interface -> Flask Backend (Configuration Manager -> API Factory -> Reasoning Methods) -> LLM APIs -> XML Parser -> Mermaid.js Frontend

**Critical path**: User query submission → API Factory selects model → Reasoning Methods generates prompt → LLM API response → XML Parser extracts reasoning → Mermaid.js visualization → Frontend display

**Design tradeoffs**: The platform prioritizes parsing accuracy and real-time visualization over bidirectional editing capabilities, choosing rule-based XML parsing for reliability rather than more flexible but potentially error-prone natural language processing approaches.

**Failure signatures**: Parsing errors manifest as empty or malformed visualizations, API failures appear as connection error messages, and unsupported reasoning methods show configuration errors in the settings panel.

**First experiments**:
1. Configure a supported LLM API key and test a simple Chain-of-Thought reasoning task
2. Compare visualization outputs between two different reasoning methods on the same query
3. Test the XML parser with intentionally malformed outputs to observe error handling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does implementing editable nodes in the visualization flowchart enable more effective correction of reasoning paths compared to re-prompting?
- Basis in paper: [explicit] The authors state they "aim to implement editable nodes in the visualization flowcharts, enabling direct modification of reasoning processes through the graph workspace."
- Why unresolved: The current platform focuses on visualization and analysis; it does not yet support bidirectional modification where edits in the graph update the reasoning state.
- What evidence would resolve it: A comparative study measuring the time and accuracy of fixing reasoning errors via direct graph manipulation versus iterative text prompting.

### Open Question 2
- Question: How robust is the rule-based XML parsing architecture when processing outputs from models with high reasoning capacity but low instruction-following fidelity?
- Basis in paper: [inferred] The evaluation claims "nearly 100% accuracy" but qualifies it as applying only to "properly formatted outputs," noting that performance depends on the "LLM's ability to follow the specified instructions."
- Why unresolved: The paper does not quantify parsing failure rates for "improperly formatted" responses, which are common in smaller or less instruction-tuned models.
- What evidence would resolve it: Stress-testing the parser on a benchmark of open-source models known for erratic formatting to measure the frequency of visualization errors.

### Open Question 3
- Question: Does graphical visualization quantitatively reduce cognitive load and improve error detection speed compared to reading structured raw text?
- Basis in paper: [inferred] The paper claims the framework "reduces cognitive load" and "improves error detection," but the evaluation only provides usability metrics (90% success rate) rather than comparative cognitive measurements.
- Why unresolved: Without a controlled user study comparing the tool against raw text analysis, the claimed benefits regarding cognitive load remain theoretical.
- What evidence would resolve it: A controlled experiment measuring user time-to-error-detection and NASA-TLX scores for complex reasoning tasks using ReasonGraph versus standard text interfaces.

## Limitations
- Parsing accuracy depends entirely on LLMs producing properly formatted XML outputs, with no data on failure rates for malformed responses
- Usability claims lack detailed methodology, sample size, and participant diversity information
- Applications in education and prompt engineering optimization are speculative without empirical validation
- Scalability under concurrent usage and with large reasoning graphs is not addressed

## Confidence
- High confidence in visualization framework architecture and basic functionality claims
- Medium confidence in parsing accuracy claims (methodology unclear)
- Low confidence in usability statistics and application claims (insufficient detail)

## Next Checks
1. Request and test the exact prompt templates used to elicit XML-formatted reasoning outputs from each supported LLM model
2. Verify parsing accuracy by feeding intentionally malformed outputs to the XML parser and measuring failure rates
3. Conduct a controlled usability study with at least 30 participants across different technical backgrounds to validate the ~90% unassisted usage claim