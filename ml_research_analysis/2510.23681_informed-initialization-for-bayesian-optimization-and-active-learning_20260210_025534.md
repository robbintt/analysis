---
ver: rpa2
title: Informed Initialization for Bayesian Optimization and Active Learning
arxiv_id: '2510.23681'
source_url: https://arxiv.org/abs/2510.23681
tags:
- hipe
- optimization
- batch
- learning
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces HIPE, a novel acquisition function for initializing
  Bayesian Optimization and Active Learning. HIPE addresses the limitation of traditional
  space-filling designs, which often fail to balance coverage of the input space with
  effective hyperparameter learning.
---

# Informed Initialization for Bayesian Optimization and Active Learning

## Quick Facts
- arXiv ID: 2510.23681
- Source URL: https://arxiv.org/abs/2510.23681
- Authors: Carl Hvarfner; David Eriksson; Eytan Bakshy; Max Balandat
- Reference count: 35
- Key outcome: HIPE outperforms standard initialization strategies, particularly in large-batch, few-shot settings, achieving better predictive accuracy, hyperparameter identification, and subsequent optimization performance.

## Executive Summary
This paper addresses the critical "cold start" problem in Bayesian Optimization and Active Learning by introducing HIPE, a novel information-theoretic acquisition function for initializing surrogate models. Traditional space-filling designs like Sobol sequences often fail to balance input space coverage with effective hyperparameter learning in GPs. HIPE resolves this by maximizing the joint information gain over both test function values and model hyperparameters, using a closed-form expression for Gaussian processes and a practical Monte Carlo approximation for batched optimization. Extensive experiments demonstrate HIPE consistently outperforms standard methods across synthetic and real-world tasks, particularly excelling in high-dimensional settings and when accurate model calibration is critical.

## Method Summary
HIPE is derived from information-theoretic principles to address the cold-start problem in BO and AL by optimizing for both predictive uncertainty reduction and hyperparameter identification. The method combines Expected Predictive Information Gain (EPIG) and Bayesian Active Learning by Disagreement (BALD) through joint information gain maximization. For GPs, this yields a closed-form expression that can be efficiently approximated using Monte Carlo sampling. The acquisition function automatically weights the two objectives through β = EIG(y(x*); θ|D), eliminating the need for manual hyperparameter tuning. HIPE is optimized jointly over all batch points using gradient-based methods, making it suitable for large-batch, few-shot scenarios where initialization quality critically impacts downstream performance.

## Key Results
- HIPE consistently achieves better predictive accuracy than Sobol, Random, BALD, and NIPV across multiple benchmarks
- The method excels in high-dimensional settings and accurately identifies relevant input dimensions through lengthscale recovery
- HIPE demonstrates superior hyperparameter identification while maintaining effective space coverage, unlike pure space-filling or pure exploration strategies
- Automatic β weighting eliminates the need for manual hyperparameter tuning while balancing predictive and hyperparameter objectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HIPE produces initial designs that simultaneously reduce predictive uncertainty and improve hyperparameter identification.
- Mechanism: The acquisition function combines Expected Predictive Information Gain (EPIG) with Bayesian Active Learning by Disagreement (BALD). EPIG targets reduction in predictive entropy over a test distribution, while BALD targets information gain about model hyperparameters. At β=1, Proposition 1 shows this is mathematically equivalent to maximizing joint information gain over both test function values and hyperparameters.
- Core assumption: The surrogate model is a Gaussian Process with hyperparameters (lengthscales, noise variance, signal variance) that influence predictive quality across the search space.
- Evidence anchors:
  - [abstract]: "HIPE maximizes the joint information gain over test function values and model hyperparameters, using a closed-form expression for Gaussian processes."
  - [Section 4.1]: "arg max HIPE_β=1(X) = arg max E_{x*~p*}[EIG(y(x*), θ; X)]" (Eq. 8)
  - [corpus]: OASI paper similarly addresses surrogate initialization for BO, suggesting initialization quality is a recognized problem, though with different objectives.
- Break condition: If the GP kernel structure is severely misspecified or hyperparameter priors are wildly incorrect, the information gain estimates may not reflect true uncertainty reduction.

### Mechanism 2
- Claim: The automatic weighting β = EIG(y(x*); θ|D) balances predictive and hyperparameter objectives proportional to their downstream relevance.
- Mechanism: Rather than treating β as a tunable hyperparameter, HIPE sets β equal to the mutual information between hyperparameters and test labels. This quantifies how much knowing θ reduces uncertainty about y(x*). When hyperparameters are informative about predictions, β increases and BALD gets more weight; when hyperparameters have little predictive impact, β decreases and EPIG dominates.
- Core assumption: The test distribution p*(x) accurately reflects regions where prediction quality matters for downstream optimization.
- Evidence anchors:
  - [Section 4.1]: "β = EIG(y(x*); θ|D) ... does not depend on the candidate set X and can thus be pre-computed."
  - [Section 4.1]: "Setting β = EIG(y(x*); θ|D) balances the two competing objectives... without introducing any additional hyperparameters."
  - [corpus]: BioBO uses biology-informed priors for BO, analogous to incorporating domain knowledge, but doesn't address the initialization weighting problem.
- Break condition: If the test distribution is poorly chosen (e.g., doesn't cover the optimum region), the automatic weighting may misallocate effort between coverage and hyperparameter learning.

### Mechanism 3
- Claim: Monte Carlo approximation enables practical batched optimization of the HIPE objective.
- Mechanism: The acquisition function uses M hyperparameter samples from the prior, T uniformly sampled test points, and N posterior samples to estimate both BALD and EPIG components. The resulting estimator is deterministic and differentiable via the reparameterization trick, allowing joint optimization over all q batch points simultaneously in a qD-dimensional space using standard gradient-based methods (L-BFGS-B).
- Core assumption: M samples sufficiently approximate the hyperparameter posterior; T test points adequately represent p*(x).
- Evidence anchors:
  - [Section 4.2]: "Using Sample Average Approximation... the HIPE objective is deterministic and auto-differentiable."
  - [Section 5, Setup]: "Monte Carlo estimators use M=12 hyperparameter samples, T=1024 test points... and N=128 predictive posterior samples."
  - [corpus]: Related work on batch BO (Pretrained Joint Predictions paper) also relies on MC sampling but for different acquisition functions.
- Break condition: Computational cost scales with M×T×N; for very high dimensions or large batches, runtime may become prohibitive for real-time applications.

## Foundational Learning

- Concept: Mutual Information and Entropy in Bayesian Models
  - Why needed here: HIPE is derived entirely from information-theoretic principles. Understanding that I(A;B) = H(A) - H(A|B) and how conditioning reduces entropy is essential to grasp why combining EPIG and BALD works.
  - Quick check question: If observing a point x reduces H(θ) by 2 nats but H(y(x*)) by only 0.5 nats, what does β tell you about the relative weight of each term?

- Concept: Gaussian Process Hyperparameters and ARD Kernels
  - Why needed here: The entire motivation stems from lengthscales affecting which dimensions are "important." Without understanding how ℓ_i controls smoothness in dimension i, the value of hyperparameter learning is opaque.
  - Quick check question: A GP has lengthscales [10.0, 0.1] in a 2D problem. What does this imply about the relative importance of each input dimension?

- Concept: Batch Bayesian Optimization with q-Point Joint Acquisition
  - Why needed here: HIPE is designed for large-batch, few-shot settings where all q points are selected jointly rather than sequentially. Understanding that batch acquisition differs from q iterations of single-point acquisition is critical.
  - Quick check question: Why can't you just run a single-point acquisition function q times sequentially to get a batch?

## Architecture Onboarding

- Component map: Hyperparameter Sampler -> Test Point Generator -> EPIG Estimator -> BALD Estimator -> β Computer -> Batch Optimizer
- Critical path:
  1. Define GP model with hyperpriors on lengthscales, noise, signal variance
  2. Pre-compute β using M hyperparameter samples and T test points (one-time cost)
  3. For each optimization restart: initialize candidate batch X, compute MC estimate of HIPE(X), gradient-step to maximize
  4. Return best batch across all restarts

- Design tradeoffs:
  - Larger M → better hyperparameter posterior approximation but O(M) compute increase
  - Larger T → better test distribution coverage but O(T) compute increase
  - Larger q (batch size) → more parallelism but optimization in qD dimensions becomes harder
  - Uniform p*(x) vs. informed test distribution: uniform requires no domain knowledge but may waste queries on irrelevant regions

- Failure signatures:
  - **All queries cluster on one axis**: β may be too large (BALD dominates); check if hyperparameters are highly uncertain but predictive impact is low
  - **Queries concentrate at boundaries**: May indicate p*(x) is not uniform or gradient optimization is stuck in local optima; increase restarts
  - **RMSE improves but NLL stagnates**: Model is overfitting to observed points; BALD component may not be engaging (β ≈ 0)
  - **Runtime exceeds function evaluation time**: HIPE is too expensive for the application; reduce M, T, or consider NIPV/Sobol

- First 3 experiments:
  1. **Sanity check on 2D Branin or Hartmann**: Run HIPE vs. Sobol vs. NIPV with q=8, B=2. Verify HIPE selects points that are neither fully space-filling (Sobol) nor axis-clustered (BALD alone). Plot acquisition surfaces as in Figure 1.
  2. **Ablation on β settings**: Compare β=1 (joint information gain) vs. β=EIG(...) (automatic) vs. β→∞ (BALD only) vs. β→0 (EPIG only) on Hartmann 6D. Quantify tradeoffs in RMSE vs. NLL vs. lengthscale identification error.
  3. **Scale test on SVM 40D**: Replicate the high-dimensional experiment from Section 5.4. Track both optimization performance and lengthscale recovery for relevant vs. dummy dimensions. This stress-tests whether HIPE can identify structure when D ≫ q.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can HIPE be effectively extended to the transfer learning setting using multi-task Gaussian Process surrogates?
- Basis in paper: [explicit] The Future Work section states, "This motivates an extension of HIPE to the transfer learning setting, e.g., by means of using a multi-task GP surrogate."
- Why unresolved: The current method addresses the "cold start" problem, but practical scenarios often involve prior data from related tasks that could inform initialization, a mechanism not yet integrated into the HIPE framework.
- What evidence would resolve it: A derivation of HIPE that incorporates multi-task kernel structures and a demonstration of improved sample efficiency on related tasks compared to single-task HIPE.

### Open Question 2
- Question: How does HIPE performance scale in multi-objective optimization settings where different surrogates model different objectives?
- Basis in paper: [explicit] The authors explicitly express interest in "studying the multi-objective setting in which different surrogates of potentially different form... model different objectives but share observations at the same input locations."
- Why unresolved: The current acquisition function maximizes information gain for a single model; balancing hyperparameter learning across multiple, potentially conflicting objectives with different sensitivities remains undefined.
- What evidence would resolve it: A multi-objective HIPE variant that jointly optimizes information gain across different surrogates and outperforms standard multi-objective initialization strategies on synthetic benchmarks.

### Open Question 3
- Question: Can the HIPE acquisition function be derived and practically implemented for non-Gaussian Process Bayesian models, such as Bayesian Neural Networks?
- Basis in paper: [inferred] The authors note in the Limitations section that while the approach applies conceptually to other models, "the computation of the acquisition function would have to be re-derived" and is currently limited to GPs.
- Why unresolved: The paper relies on the closed-form predictive entropy available in GPs; replicating this efficiency for models lacking analytical posterior variances is a non-trivial theoretical and computational challenge.
- What evidence would resolve it: An approximation of the HIPE criterion for a Bayesian Neural Network and experimental results showing comparable benefits in model calibration and optimization performance.

## Limitations
- The method's effectiveness depends critically on the choice of test distribution p*(x), which defaults to uniform sampling but may be suboptimal when domain knowledge suggests a non-uniform distribution of interest.
- The computational overhead of HIPE is substantial compared to simple baselines like Sobol, particularly for large batch sizes or high-dimensional problems.
- The theoretical guarantees are limited to the Gaussian Process setting, and the method's behavior with other surrogate models (e.g., neural networks) remains unexplored.
- The experiments focus on few-shot, large-batch scenarios where initialization quality has outsized impact, leaving open questions about performance in sequential or long-horizon settings.

## Confidence
- **High confidence**: The core mechanism of combining EPIG and BALD through joint information gain is mathematically sound and well-supported by the derivation in Section 4.1. The empirical superiority of HIPE over baselines in the presented experiments is clearly demonstrated.
- **Medium confidence**: The automatic β weighting scheme is theoretically motivated but its practical robustness across diverse problem domains warrants further validation. The computational scalability claims assume reasonable parameter choices (M=12, T=1024) that may need adjustment for specific applications.
- **Low confidence**: The method's performance in sequential/iterative settings beyond initialization, and its behavior with non-GP surrogates, cannot be assessed from the current results.

## Next Checks
1. **Domain-informed test distribution**: Replace the uniform p*(x) with a problem-specific distribution (e.g., Gaussian centered at known optima regions) and compare HIPE performance against the baseline uniform test distribution across all experimental tasks.
2. **Sequential extension validation**: Design a sequential BO experiment where HIPE is used for initial batch selection followed by standard sequential acquisition, comparing against fully sequential methods from the start. Measure both final optimization performance and query efficiency.
3. **Non-GP surrogate stress test**: Implement HIPE with a Bayesian Neural Network surrogate instead of GP, evaluating whether the information-theoretic framework translates effectively to this different model class while maintaining the observed performance advantages.