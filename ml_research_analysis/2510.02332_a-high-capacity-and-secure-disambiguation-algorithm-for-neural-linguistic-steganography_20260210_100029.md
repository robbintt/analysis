---
ver: rpa2
title: A High-Capacity and Secure Disambiguation Algorithm for Neural Linguistic Steganography
arxiv_id: '2510.02332'
source_url: https://arxiv.org/abs/2510.02332
tags:
- capacity
- look-ahead
- security
- sync
- secure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of tokenization ambiguity in
  neural linguistic steganography, which can lead to catastrophic decoding failures.
  The authors propose a method called look-ahead Sync that resolves ambiguity while
  preserving statistical undetectability.
---

# A High-Capacity and Secure Disambiguation Algorithm for Neural Linguistic Steganography

## Quick Facts
- arXiv ID: 2510.02332
- Source URL: https://arxiv.org/abs/2510.02332
- Reference count: 34
- Authors: Yapei Feng; Feng Jiang; Shanhao Wu; Hua Zhong
- Key outcome: A disambiguation algorithm that resolves tokenization ambiguity while preserving statistical undetectability, outperforming state-of-the-art by 160% in English and 25% in Chinese

## Executive Summary
This paper addresses a critical challenge in neural linguistic steganography: tokenization ambiguity in subword tokenizers can cause catastrophic decoding failures when the same visible string maps to different token sequences. The authors propose look-ahead Sync, which resolves ambiguity by minimal synchronized sampling on truly indistinguishable sequences while preserving all other discernible paths to maximize embedding capacity. The method maintains computational zero-KL security through careful probability distribution management and CSPRNG-based synchronization. Experiments on Llama 3 (English) and Qwen 2.5 (Chinese) benchmarks demonstrate significant capacity improvements over existing methods, particularly in settings with larger candidate pools.

## Method Summary
The method operates through a three-phase iterative process: (1) PARTITIONBYPREFIX groups candidate token sequences by their visible string prefixes, isolating ambiguity into disjoint pools; (2) Inter-group entropy coding selects exactly one pool using the visible prefix as payload-bearing information; (3) LOOKAHEAD resolves remaining ambiguity within the selected pool by CSPRNG-seeded sampling on truly indistinguishable sequences, then performs LLM forward passes to generate new continuations. The algorithm maintains zero-KL security by preserving computational indistinguishability of probability distributions at each step through careful expectation management and CSPRNG guarantees. The process continues recursively until reaching EOS with no remaining ambiguous sequences.

## Key Results
- Look-ahead Sync consistently approaches the theoretical capacity upper bound while maintaining zero-KL security (KL divergence = 0.00)
- Performance improvements exceed 160% in English (Llama 3) and 25% in Chinese (Qwen 2.5) over state-of-the-art SyncPool method
- Capacity gains are particularly pronounced in settings with larger candidate pools (top-k ∈ {16, 32, 128})
- Computational overhead remains manageable with Tok/Call ratios below 1.0 due to minimal synchronization sampling

## Why This Works (Mechanism)

### Mechanism 1: Prefix-Based Pool Partitioning
- Grouping token sequences by visible string prefixes isolates ambiguity into disjoint pools, enabling unambiguous payload embedding at the pool level
- Candidates sharing a visible prefix are placed into the same intra-group pool; inter-group probabilities are aggregated sums
- The entropy encoder selects exactly one pool—this choice is payload-bearing and decodable from the visible text alone
- Core assumption: detokenization function φ is deterministic; sorting by visible strings places shorter prefixes before longer continuations
- Break condition: If φ is non-deterministic or if tokenization is lossy, pool membership becomes ambiguous and decoding fails

### Mechanism 2: Minimal Synchronized Sampling on Indistinguishable Sequences
- Only sequences that decode exactly to the current visible prefix require non-payload-bearing synchronization; all others preserve entropy for future embedding
- The prefix set S_prefix (sequences with φ(s)=v_m) is sampled via CSPRNG-seeded SyncSample; the partial set S_partial is preserved untouched
- This limits synchronization loss to the conditional mutual information I(S_prefix; V_end | v_m)
- Core assumption: The Linguistic Smoothness Hypothesis—semantically equivalent token sequences yield similar future conditional distributions
- Break condition: If token-distinct but semantically equivalent prefixes produce divergent future distributions, synchronization loss grows and capacity drops

### Mechanism 3: Recursive Distribution-Preserving State Update
- The algorithm maintains computational zero-KL security by ensuring expected weights E[P_t(s)] remain computationally indistinguishable from model probabilities P_θ(s|c) at every step
- Induction on state (S_t, P_t): preserved partials retain expectation directly; newly generated expansions inherit expectation via law of total expectation
- Core assumption: CSPRNG output is computationally indistinguishable from uniform random to any PPT adversary
- Break condition: If the entropy coder or CSPRNG introduces bias, or if LLM forward passes are non-deterministic across runs, the invariant breaks

## Foundational Learning

- **Tokenization Ambiguity in Subword Tokenizers**: BPE-style tokenizers can map the same visible string to multiple token sequences (e.g., "mistrust" → [278] or [377, 245]); without resolution, sender and receiver deserialize to different contexts and desynchronize. Quick check: Given a vocabulary with tokens "mis", "trust", and "mistrust", what are two tokenizations of the string "mistrustful"?

- **Zero KL Divergence Security**: The strongest security standard requires the stegotext distribution to equal the covertext distribution; any KL > 0 admits a distinguisher. Quick check: If a stegosystem always avoids low-probability tokens that the base model would occasionally emit, does KL remain zero?

- **Conditional Mutual Information and Synchronization Loss**: Capacity gap arises when non-payload-bearing choices reveal information about future visible outcomes; quantifying I(S_prefix; V_end | v_m) explains why minimal sync preserves capacity. Quick check: If all prefix-set sequences produce identical next-token distributions, what is I(S_prefix; V_end | v_m)?

## Architecture Onboarding

- **Component map**: Initialization -> PartitionByPrefix -> Inter-group Entropy Coding -> Look-ahead Resolution -> Loop Termination
- **Critical path**: Inter-group encoding is the only payload-bearing step; any bug in partition or prefix matching corrupts decoding. SyncSample must use identical CSPRNG seed/key at both ends
- **Design tradeoffs**: Larger top-k increases ambiguity frequency → more look-ahead calls → lower Tok/Call but higher BPT; coarse-grained (SyncPool) wastes entropy; fine-grained (Look-ahead Sync) preserves more but requires recursive LLM calls
- **Failure signatures**: Decoding mismatch → check visible prefix matching; KL > 0 → verify no pruning and SyncSample uses normalized P_prefix; Infinite loop → check termination condition; Capacity collapse → inspect ambiguity pool sizes
- **First 3 experiments**: (1) Unit test partition correctness on small vocabulary with known ambiguities; (2) End-to-end roundtrip: embed bitstream, decode from visible text, confirm recovered bits match; (3) KL divergence measurement: compare empirical token distribution from stego generation vs. base model sampling

## Open Questions the Paper Calls Out

- **Adaptive Look-ahead Strategies**: Can heuristics (e.g., prefix set entropy) optimize the trade-off between embedding capacity and computational overhead? The current fixed recursive implementation recovers capacity but incurs computational overhead (Tok/Call < 1.0) due to additional LLM forward passes.

- **Linguistic Smoothness Hypothesis Across Domains**: Does the hypothesis hold for low-resource languages or specialized domains (e.g., code) where token ambiguity differs statistically? If ambiguous paths in other domains result in divergent future distributions, the synchronization loss (mutual information) would increase, potentially reducing capacity gains.

- **Eliminating Synchronization Loss**: Can the residual capacity gap attributed to synchronization loss be mathematically eliminated without compromising the zero-KL security guarantee? The algorithm relies on discarding the entropy of the synchronized choice to maintain security, creating a theoretical limit on efficiency.

## Limitations

- The Linguistic Smoothness Hypothesis lacks direct empirical validation and assumes semantically equivalent token sequences yield similar future conditional distributions, which may not hold across all domains
- The method depends critically on CSPRNG-seeded synchronization between sender and receiver, creating a potential failure point if seeding is not properly synchronized
- The approach assumes deterministic tokenization and forward passes, which may not hold across different hardware or software configurations

## Confidence

**High Confidence Claims:**
- Prefix-based disambiguation mechanism is formally proven and straightforward to implement
- Zero-KL security maintenance follows standard cryptographic arguments about CSPRNG indistinguishability

**Medium Confidence Claims:**
- Capacity improvements over SyncPool are well-demonstrated empirically
- Practical implementation details (entropy coder choice, CSPRNG implementation) are assumed to work as specified

**Low Confidence Claims:**
- The Linguistic Smoothness Hypothesis is the weakest link, lacking external validation or controlled experiments

## Next Checks

1. **Linguistic Smoothness Hypothesis Validation**: Design a controlled experiment that isolates synchronization loss by comparing look-ahead Sync performance against a variant that synchronizes on all ambiguous sequences. Measure the actual capacity difference and verify it matches the theoretical conditional mutual information calculation.

2. **CSPRNG Synchronization Robustness**: Implement a fault injection test where the CSPRNG seed or state becomes slightly desynchronized between encoder and decoder. Measure the exact failure point and determine if the system fails gracefully or catastrophically.

3. **Tokenization Ambiguity Baseline**: Create a minimal implementation of the disambiguation problem using a simple vocabulary with known ambiguities (e.g., "mis", "trust", "mistrust"). Verify that the prefix partitioning correctly isolates ambiguous sequences and that the look-ahead resolution preserves capacity as claimed.