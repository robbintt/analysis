---
ver: rpa2
title: Serve Programs, Not Prompts
arxiv_id: '2510.25412'
source_url: https://arxiv.org/abs/2510.25412
tags:
- system
- serving
- cache
- generation
- symphony
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies fundamental inefficiencies in current LLM\
  \ serving systems, which are optimized for simple text completion but struggle with\
  \ complex AI workflows due to their inflexible prompt-based architecture. To address\
  \ this, the authors propose a new architecture where LLM serving systems process\
  \ user-defined programs\u2014called LLM Inference Programs (LIPs)\u2014rather than\
  \ just prompts."
---

# Serve Programs, Not Prompts

## Quick Facts
- arXiv ID: 2510.25412
- Source URL: https://arxiv.org/abs/2510.25412
- Reference count: 40
- One-line primary result: Identifies inefficiencies in LLM serving systems and proposes a new architecture where LLM serving systems process user-defined programs (LIPs) instead of just prompts, enabling application-specific KV cache management and achieving up to 7× higher throughput.

## Executive Summary
Current LLM serving systems are optimized for simple text completion but struggle with complex AI workflows due to their inflexible prompt-based architecture. This paper identifies fundamental inefficiencies in these systems and proposes a new architecture where LLM serving systems process user-defined programs called LLM Inference Programs (LIPs) rather than just prompts. This shift enables applications to manage KV cache explicitly, control token generation, and handle external interactions directly within the serving system. The authors demonstrate this concept through Symphony, a prototype system that exposes model computations as system calls and virtualizes KV cache as a file system, allowing users to define custom optimization strategies.

## Method Summary
The paper proposes Symphony, a prototype LLM serving system that processes user-defined LLM Inference Programs (LIPs) instead of simple prompts. The system exposes model computations as system calls (pred()) and virtualizes KV cache as a file system (KVFS). LIPs can explicitly manage KV cache, implement custom decoding strategies, and execute external functions directly within the serving system. The architecture includes a two-level scheduler (CPU thread scheduler and GPU batch inference scheduler) to manage LIP execution. Preliminary experiments demonstrate up to 7× higher throughput than vLLM by enabling application-specific KV cache management policies, particularly for workloads with skewed request distributions.

## Key Results
- Symphony prototype achieves up to 7× higher throughput than vLLM through application-specific KV cache management
- Enables custom decoding strategies and constrained generation without server modifications
- Co-locates function execution and external I/O within the serving system to reduce network roundtrips
- Virtualizes KV cache as a file system (KVFS) with POSIX-like operations for explicit management

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit, application-controlled KV cache management can improve throughput for workloads with predictable reuse patterns.
- Mechanism: Symphony virtualizes KV cache as a file system (KVFS), exposing POSIX-like operations (open, fork, close, extract, merge). This allows LIPs to persist cache across requests, share it between parallel generations, and implement custom eviction policies.
- Core assumption: Applications have better knowledge of their own reuse patterns than a generic system-wide policy.
- Evidence anchors: 7× throughput improvement demonstrated for RAG workloads with skewed request distributions; results show improvement "when the Pareto index is small."
- Break condition: If request patterns are uniform or adversarial, custom policies may not outperform LRU. If KV cache manipulation overhead exceeds reuse benefits, throughput degrades.

### Mechanism 2
- Claim: Decoupling token generation logic from model computation enables arbitrary decoding strategies without server modifications.
- Mechanism: The `pred()` system call accepts tokens and positions, performs one forward pass, and returns the full next-token distribution (~200KB for GPT-4). LIPs can then implement constrained decoding, speculative decoding, or custom sampling as user code.
- Core assumption: Returning full distributions is practical and the overhead does not negate flexibility gains.
- Evidence anchors: LIPs can implement various decoding algorithms including constrained decoding and speculative decoding through full access to token distribution.
- Break condition: If distribution transfer becomes a bottleneck at high batch sizes, or if most users only need standard autoregressive generation, complexity may not justify flexibility.

### Mechanism 3
- Claim: Co-locating function execution and external I/O within the serving system reduces network roundtrips for tool-augmented workflows.
- Mechanism: LIPs run as OS-style threads that can directly call external APIs or execute code without client mediation. When threads block on I/O, Symphony swaps their KV cache to CPU memory to free GPU resources.
- Core assumption: Most function calls do not require user-environment access and can be executed in a sandboxed environment.
- Evidence anchors: Enables elimination of client-as-intermediary pattern for function calls, reducing round trips from prompt → server → function spec → client → execute → client → server.
- Break condition: If functions require user-side resources, co-location fails. If security sandboxing adds prohibitive latency, benefits diminish.

## Foundational Learning

- Concept: **KV Cache and PagedAttention**
  - Why needed here: The entire architecture hinges on understanding that KV tensors can be persisted, shared, and paged. Without this, KVFS makes no sense.
  - Quick check question: Why can the KV cache for token position N be reused when generating token N+1? What does it mean to "fork" a KV cache?

- Concept: **System Calls vs. Library Calls**
  - Why needed here: Symphony's `pred()` is a system call, meaning it crosses a protection boundary. This has scheduling and batching implications distinct from calling a local function.
  - Quick check question: What is the semantic difference between calling `model.forward()` in Python versus invoking a `pred()` syscall handled by a serving OS?

- Concept: **Two-Level Scheduling**
  - Why needed here: Symphony has a thread scheduler (CPU) and a batch inference scheduler (GPU). Understanding why these are separate is critical to understanding throughput tradeoffs.
  - Quick check question: Why can't a single scheduler handle both CPU threads and GPU batches efficiently? What information does each scheduler have that the other lacks?

## Architecture Onboarding

- Component map: LIPs (user-defined programs) -> KVFS (virtual file system for KV cache) -> pred() syscall (model computation) -> Thread Scheduler (CPU) -> Batch Inference Scheduler (GPU)

- Critical path:
  1. LIP calls `kv_open()` to get a KV cache handle
  2. LIP calls `pred(kv, tokens, positions)` → thread blocks, moves to inference pool
  3. Batch scheduler collects pending `pred()` calls, forms batch
  4. GPU executes batch, updates KV files, returns distributions
  5. LIP receives distributions, samples next token, loops or terminates
  6. On completion, LIP calls `kv_close()` or `kv_remove()` to release resources

- Design tradeoffs:
  - **Flexibility vs. optimization**: Decentralized control enables custom strategies but loses global optimization opportunities (e.g., pipelining sampling with model execution).
  - **API granularity**: Current `pred()` is coarse (one forward pass). Finer access (attention layers, intermediate activations) would enable more optimizations but complicate batching.
  - **Security vs. openness**: Accepting user code (not just data) fundamentally shifts trust boundaries. Sandboxing is essential but adds overhead.

- Failure signatures:
  - **GPU underutilization**: If `pred()` calls are sparse or highly variable, batch scheduler may execute prematurely (low utilization) or delay excessively (high latency).
  - **KV cache thrashing**: If LIPs fork aggressively without cleanup, KVFS pages exhaust GPU memory.
  - **Sandbox escape**: User-submitted LIPs could attempt resource exhaustion or model parameter extraction via distribution analysis.
  - **Heterogeneous LIP interference**: Mixing simple and complex LIPs in one system may cause head-of-line blocking or unfair resource allocation.

- First 3 experiments:
  1. **Reproduce the RAG caching experiment** with varying Pareto indices to confirm the 7× throughput claim and identify where it breaks down.
  2. **Measure `pred()` overhead** by benchmarking distribution transfer time at different batch sizes; compare against the theoretical ~200KB transfer cost.
  3. **Test a custom constrained decoding strategy** by implementing a simple grammar (e.g., JSON-only output) in a LIP and comparing correctness and latency against vLLM's built-in JSON mode.

## Open Questions the Paper Calls Out

- Question: What is the optimal level of API granularity for model computation system calls (e.g., single forward pass vs. layer-level access) to balance application flexibility with efficient system batching?
  - Basis in paper: The authors state, "Finding the right interface granularity... remains an open question," noting that while finer-grained access enables optimizations, it complicates batch scheduling.
  - Why unresolved: The current prototype treats a single forward pass as atomic; exploring sub-model granularity requires solving complex scheduling conflicts.
  - What evidence would resolve it: A study comparing GPU utilization rates and scheduling latency when exposing attention mechanisms or layer outputs versus a monolithic `pred` call.

- Question: How can a serving system effectively mitigate the "opportunity costs" of decentralized control, where heterogeneous LIPs hinder global optimization strategies like GPU pipelining?
  - Basis in paper: The authors ask how to design "intelligent scheduling algorithms and potentially new co-design approaches" to address the performance overhead of moving control logic to user programs.
  - Why unresolved: Decoupling sampling from model execution makes it difficult for the system to maintain a complete view of the workload for global optimization.
  - What evidence would resolve it: Demonstration of a two-level scheduler that maintains >90% GPU utilization despite diverse, user-defined sampling strategies.

- Question: What specific sandboxing mechanisms (e.g., WASM, seccomp) are required to securely execute arbitrary user-defined LIPs without introducing prohibitive performance overhead?
  - Basis in paper: The paper notes that accepting code instead of prompts shifts the trust boundary, creating vulnerabilities like resource exhaustion that necessitate "robust sandboxing."
  - Why unresolved: The paper outlines the risk but does not implement or evaluate specific security frameworks for LIPs.
  - What evidence would resolve it: Security penetration testing and overhead benchmarks of a Symphony deployment utilizing lightweight VMs or sandboxed runtimes.

## Limitations
- The 7× throughput improvement is demonstrated only in a simulated environment and depends critically on skewed request distributions.
- No empirical validation exists for the claimed reduction in network roundtrips through co-located function execution.
- The security model for user-submitted LIPs is hand-waved with no implementation details or performance impact analysis.
- No comparison to existing programmable serving systems like Pie, leaving the novelty unclear.

## Confidence

**High confidence**: The fundamental insight that current LLM serving systems struggle with complex AI workflows is well-established. The architectural vision of moving from prompt-based to program-based serving is internally consistent and builds on proven concepts like KV cache virtualization.

**Medium confidence**: The mechanisms for explicit KV cache management and co-located function execution are theoretically sound, but their practical performance benefits depend heavily on workload characteristics that aren't fully characterized in the paper.

**Low confidence**: The security model for user-submitted LIPs is hand-waved. The paper acknowledges the need for sandboxing but provides no implementation details or performance impact analysis, making this a critical unknown for real-world deployment.

## Next Checks
1. **Reproduce the RAG caching experiment** with varying Pareto indices to confirm the 7× throughput claim and identify where it breaks down in practice.
2. **Measure `pred()` overhead** by benchmarking distribution transfer time at different batch sizes; compare against the theoretical ~200KB transfer cost to quantify the practical impact of API granularity.
3. **Implement and benchmark a security sandbox** for LIPs to measure the overhead of isolating user-submitted code while preventing resource exhaustion and model parameter extraction attacks.