---
ver: rpa2
title: Separated Contrastive Learning for Matching in Cross-domain Recommendation
  with Curriculum Scheduling
arxiv_id: '2502.16239'
source_url: https://arxiv.org/abs/2502.16239
tags:
- learning
- domain
- sccdr
- contrastive
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses training instability in cross-domain recommendation
  (CDR) caused by directly mixing intra-domain and inter-domain contrastive learning
  tasks. The authors propose SCCDR, a framework that separates intra-CL and inter-CL
  tasks into distinct stages, with an inter-domain curriculum scheduling strategy
  that orders negative samples by difficulty (measured via Katz centrality).
---

# Separated Contrastive Learning for Matching in Cross-domain Recommendation with Curriculum Scheduling

## Quick Facts
- arXiv ID: 2502.16239
- Source URL: https://arxiv.org/abs/2502.16239
- Reference count: 40
- Primary result: SCCDR achieves >2% HIT@100 improvement over CCDR in half of cross-domain settings, with 1.92% CTR and 3.65% duration gains in online A/B tests.

## Executive Summary
This paper addresses training instability in cross-domain recommendation caused by mixing intra-domain and inter-domain contrastive learning tasks. The authors propose SCCDR, a framework that separates these tasks into distinct stages with curriculum scheduling based on Katz centrality. The method stabilizes training and improves knowledge transfer from source to target domains, particularly in cold-start scenarios. SCCDR is evaluated across 8 Amazon cross-domain scenarios and a proprietary industrial dataset, demonstrating significant improvements over state-of-the-art baselines.

## Method Summary
SCCDR is a two-stage contrastive learning framework for cross-domain recommendation. Stage 1 trains intra-domain contrastive learning (intra-CL) separately for source and target domains using BCELoss on neighbor pairs. Stage 2 performs inter-domain contrastive learning (inter-CL) using InfoNCE loss to align overlapping users, with stop-gradient protection on source embeddings and curriculum scheduling of negative samples based on Katz centrality. The curriculum scheduler orders negatives by difficulty (easy-to-hard) and gradually introduces harder samples during training. GraphSAGE with JK-Net serves as the encoder, with embedding size 64 and output dimension 128.

## Key Results
- SCCDR achieves >2% HIT@100 improvement over CCDR in half of cross-domain settings on Amazon datasets
- Online A/B tests show 1.92% CTR and 3.65% duration increases compared to GraphDR+ on Music2Videos
- Stop-gradient operation contributes ~0.02 absolute HIT@100 improvement; curriculum scheduling adds ~0.01-0.02 improvement
- Performance is consistent across different GNN architectures (GraphSAGE, LightGCN, GCN, GAT) and evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1: Separated Intra-CL and Inter-CL Training Stages
Directly mixing intra-domain and inter-domain contrastive learning tasks ignores the difficulty differential between learning from homogeneous intra-domain preferences versus heterogeneous cross-domain patterns. SCCDR separates these into distinct stages, allowing stable intra-CL learning first, then leveraging those stabilized representations for inter-CL knowledge transfer. The paper observes inter-domain learning is harder because overlapped users are sparse and cross-domain preferences are more heterogeneous.

### Mechanism 2: Stop-Gradient Protection for Source Domain
During inter-CL, the source encoder receives `stopgrad()` on its outputs, protecting source embeddings already learned from rich interactions. This prevents source domain representations from collapsing or over-aligning with the target domain during knowledge transfer. The stop-gradient operation breaks the interdependency between user and item embedding distributions that can cause dimensional collapse.

### Mechanism 3: Curriculum Scheduling Based on Katz Centrality
The inter-CL stage orders negative samples by difficulty using Katz centrality, starting with the easiest 50% and progressively adding harder samples. Nodes with higher Katz centrality are considered easier negatives because their patterns are more learnable—popular/central items have more consistent interaction patterns. This easy-to-hard progression improves inter-CL representation calibration.

## Foundational Learning

- **Contrastive Learning (InfoNCE and BCE Loss)**: Both intra-CL and inter-CL use contrastive objectives. Intra-CL uses BCELoss for neighbor-similarity; inter-CL uses InfoNCE for aligned-user and neighbor-based contrasts. Quick check: Can you explain why InfoNCE uses a temperature parameter and how it affects negative sample weighting?

- **Graph Neural Networks (Message Passing)**: SCCDR uses GraphSAGE with JK-Net as encoders. Equation 3 shows aggregation from sampled neighbors; understanding neighborhood sampling is critical for debugging embedding quality. Quick check: What happens to a cold-start item's embedding if it has no neighbors in the subgraph?

- **Curriculum Learning**: The inter-CL stage requires understanding difficulty measurement and progressive training. N_step calculation determines when harder negatives are introduced. Quick check: Given N_epoch=100 and N_neg=20, what is N_step and when are all negatives used?

## Architecture Onboarding

- **Component map**: Intra-CL Stage (Source GraphSAGE -> BCELoss) + (Target GraphSAGE -> BCELoss) → Inter-CL Stage (Stopgrad Source GraphSAGE) + (Target GraphSAGE -> InfoNCE with Curriculum Scheduler)

- **Critical path**: 1) Pre-compute Katz centrality for all nodes (NetworkX, α=0.1, β=1.0) 2) Train intra-CL until convergence (monitor loss stability) 3) Switch to inter-CL with curriculum (monitor HIT@N on validation)

- **Design tradeoffs**: GraphSAGE+JK-Net outperforms LightGCN on sampled subgraphs; BCELoss reduces computation vs InfoNCE for intra-CL; static pre-computed Katz curriculum is cheaper than dynamic re-computation.

- **Failure signatures**: Training loss oscillation in early epochs indicates mixed intra+inter CL instead of sequential; target domain shows no improvement suggests empty overlapping user set; source domain embeddings collapse indicates missing stop-gradient.

- **First 3 experiments**: 1) Reproduce Figure 1 on small dataset: mixed vs separated training loss curves to verify instability claim 2) Ablate stop-gradient: SCCDR## vs SCCDR# vs SCCDR on one Amazon scenario (expect ~0.02 HIT@100 difference) 3) Sweep loss weights: λ_intra=1.0, λ_inter=0.5 grid search to confirm paper's findings

## Open Questions the Paper Calls Out
1. Can dynamic or adaptive curriculum scheduling strategies outperform the static "easy-to-hard" implementation used in SCCDR? The current implementation uses fixed, pre-computed schedule based on Katz centrality rather than feedback-driven adaptive mechanism.

2. Can the separated contrastive learning paradigm be effectively adapted for the ranking stage of recommender systems? The paper validates on retrieval (HIT@N) but doesn't explore ranking-specific metrics like NDCG.

3. Does the "stop-gradient" operation retain its effectiveness when integrated with more complex, heterogeneous graph neural network architectures? The evaluation uses standard GNNs but doesn't test on heterogeneous graphs or advanced architectures.

## Limitations
- Hyperparameter sensitivity not fully explored—critical parameters (N_neg, N_epoch, batch size) are fixed without justification
- Katz centrality pre-computation uses fixed α=0.1, β=1.0 without sensitivity analysis across different graph structures
- Industrial dataset (Music2Videos) lacks transparency—no description of data distribution, domain similarity, or overlap ratio

## Confidence
- **High**: Separated intra-CL and inter-CL stages stabilize training and improve performance over mixed training (supported by Figure 1 loss curves and ablation studies)
- **Medium**: Stop-gradient operation protects source domain representations and contributes ~0.02 absolute HIT@100 improvement (supported by Table 5 ablation)
- **Medium**: Curriculum scheduling with Katz centrality improves calibration (supported by Table 5, but mechanism relies on untested assumption that centrality correlates with learning difficulty)

## Next Checks
1. Reproduce Figure 1 instability claim: Train SCCDR variant with mixed intra+inter CL on a small Amazon dataset; compare loss curves to separated training. Verify ~10% relative performance drop when mixing.

2. Ablate curriculum scheduler: Run SCCDR with and without curriculum scheduling on Books-Videos scenario. Expect ~0.01-0.02 HIT@100 difference to validate contribution.

3. Sweep stop-gradient sensitivity: Test SCCDR with varying stop-gradient aggressiveness (full stop vs partial vs none) across 3 Amazon scenarios. Monitor source domain embedding variance to detect collapse.