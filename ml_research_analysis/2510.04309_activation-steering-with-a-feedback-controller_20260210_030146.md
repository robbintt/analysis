---
ver: rpa2
title: Activation Steering with a Feedback Controller
arxiv_id: '2510.04309'
source_url: https://arxiv.org/abs/2510.04309
tags:
- steering
- control
- activation
- error
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PID Steering, a control-theoretic framework
  for activation steering in large language models that addresses the steady-state
  error and overshoot limitations of existing methods. The key insight is interpreting
  activation steering through the lens of feedback control systems, where popular
  steering methods correspond to proportional (P) controllers.
---

# Activation Steering with a Feedback Controller

## Quick Facts
- arXiv ID: 2510.04309
- Source URL: https://arxiv.org/abs/2510.04309
- Reference count: 40
- Primary result: PID Steering achieves up to 8× reduction in toxicity, 92.7-94.9% reduction in jailbreak success rates on larger models, and effective style control in diffusion models

## Executive Summary
This paper presents PID Steering, a control-theoretic framework for activation steering in large language models that addresses the steady-state error and overshoot limitations of existing methods. The key insight is interpreting activation steering through the lens of feedback control systems, where popular steering methods correspond to proportional (P) controllers. The proposed PID Steering extends this by adding integral and derivative terms, modeled as a state-feedback PID controller operating layer-wise on the model's activations. The method uses a PID controller to compute steering vectors that align activations with target concepts while reducing residual errors and oscillations.

## Method Summary
PID Steering computes difference-in-means vectors r(k) = μ_target(k) - μ_source(k) from contrastive datasets at each layer. A PID controller (per layer) computes control signals u(k) = Kp·r(k) + Ki·Σr(j) + Kd·(r(k)-r(k-1)), which are applied to activations via steering functions like ActAdd or DirAblate. The method operates sequentially across layers at inference time, with error signals recomputed after each intervention to respect causal dependencies.

## Key Results
- Up to 8× reduction in toxicity compared to baselines on RealToxicityPrompts
- 92.7-94.9% reduction in jailbreak success rates on larger models (Qwen2.5-7B, Llama-3-8B)
- Effective style control in diffusion models (FLUX) while maintaining utility with minimal performance degradation

## Why This Works (Mechanism)

### Mechanism 1: Proportional Control Foundations
Existing steering methods (ActAdd, DirAblate, Mean-AcT) implement proportional controllers, which explains their steady-state error limitation. P-control applies corrections proportional to current error e(k). The difference-in-means vector r(k) serves as the error signal. Layer dynamics follow x(k) = f^(k)(ρ_steer(x(k-1), K_p·r(k-1))), producing bounded but non-zero residual error under persistent disturbance w(k).

### Mechanism 2: Integral Term Eliminates Steady-State Bias
Adding integral action (PI) drives average error to zero for matched disturbance components. The integral term s(k) = s(k-1) + e(k) accumulates error history. When combined with proportional feedback, the accumulated integral generates control signals that cancel disturbances w_∥ in the Jacobian image, leaving only unmatched w_⊥.

### Mechanism 3: Derivative Term Dampens Oscillations
The derivative term reduces overshoot amplitude while preserving PI's bias-removal property. D-term responds to error rate of change: K_d·(r(k) - r(k-1)). This counteracts rapid error growth near the setpoint, preventing the integral from pushing the trajectory beyond target.

## Foundational Learning

- **Concept: Residual Stream and Layer-wise Transformations**
  - Why needed here: PID operates on the activation trajectory across layers; understanding how f^(k) transforms residual streams is essential for computing error signals r(k) at each layer.
  - Quick check question: Can you explain why ActAdd modifies x^(k) before the layer transformation f^(k), and how this relates to sequential vs. non-sequential mapping?

- **Concept: Input-to-State Stability (ISS)**
  - Why needed here: The paper's theoretical guarantees rely on ISS—understanding that bounded inputs produce bounded states with decaying transients is necessary to interpret Propositions 1-3.
  - Quick check question: Given a discrete system e(k+1) = Ae(k) + w(k) with ||A|| < 1, what happens to e(k) as k → ∞ when w(k) is bounded?

- **Concept: Jacobians and Linearization**
  - Why needed here: Error dynamics (Equation 20) depend on Ā(k), the mean local Jacobian. Stability conditions involve spectral properties of these matrices.
  - Quick check question: How would you estimate Ā(k) for a transformer layer f^(k) given activations x_i^(k)?

## Architecture Onboarding

- **Component map:**
  - Data preparation -> difference-in-means extraction -> PID controller (per layer) -> steering function application -> sequential propagation
  - Data preparation: contrastive datasets (source/target) -> forward passes -> activation collection
  - PID controller: r(k) extraction -> u(k) computation (Kp·r(k) + Ki·Σr(j) + Kd·Δr(k)) -> sequential update
  - Steering function: ρ_steer(x, u) (ActAdd: x + αu, DirAblate: x - uu^Tx)
  - Sequential propagator: x(k) = f^(k)(ρ_steer(x(k-1), u(k-1)))

- **Critical path:** Data preparation → per-layer r(k) extraction → PID gain tuning (K_p, K_i, K_d) → layer-wise u(k) computation → intervention at inference

- **Design tradeoffs:**
  - Sequential vs. non-sequential mapping: Sequential respects causality but requires full forward passes per layer; non-sequential is faster but neglects intervention effects on downstream layers
  - Gain tuning: Paper uses "stability-first, one-gain-at-a-time" strategy—conservative but may miss optimal regions; numerical methods (LMI) suggested as future work
  - Model utility vs. steering strength: Higher gains reduce toxicity/ASR but increase perplexity and may degrade MMLU

- **Failure signatures:**
  - Persistent error (P-only): ⟨e(0), e(k)⟩ plateaus above zero
  - Overshoot (PI with large K_i): Error crosses zero and oscillates before settling
  - Instability (excessive K_d): Divergent trajectories, degraded fluency

- **First 3 experiments:**
  1. Reproduce Figure 3: Apply P, PI, PID to randomly initialized transformer; plot ⟨e(0), e(k)⟩ vs. layer index to validate steady-state error and overshoot behavior
  2. Toxicity mitigation sweep: On Gemma2-2B with RealToxicityPrompts, vary K_i (fixing K_p=1, K_d=0) and measure toxicity score vs. perplexity tradeoff curve
  3. Jailbreak ASR validation: Replace DIM in Angular Steering with PID on Qwen2.5-3B-Instruct using ADVBENCH split; confirm ASR improvement over baseline while monitoring TinyBenchmarks utility

## Open Questions the Paper Calls Out

1. Can LMI-based or other numerical optimization methods identify controller gains that outperform the sequential "one-gain-at-a-time" tuning strategy?
2. What is the practical impact of the unmatched disturbance component \(w_\perp\) that cannot be compensated by the integral term?
3. Does PID Steering generalize to architectures beyond decoder-only transformers, such as encoder-decoder models or mixture-of-experts systems?
4. How robust is PID Steering to violations of the smoothness condition required for derivative gain effectiveness?

## Limitations

- The ISS-based theoretical guarantees assume locally linear layer dynamics and bounded disturbances, which may not hold for highly non-smooth activation functions
- Gain tuning sensitivity: the "stability-first" strategy may leave performance on the table and lacks specific values for reproduction
- The integral term's effectiveness relies on disturbance decomposition assumptions that lack empirical validation in LLM contexts
- Limited testing on extremely large models (70B+) and significantly different architectures

## Confidence

**High Confidence** (empirically validated, theory-grounded):
- PID Steering consistently outperforms P-only methods on toxicity mitigation, jailbreak prevention, and style control tasks
- The derivative term effectively reduces overshoot amplitude without degrading PI's bias-removal property
- Model utility preservation is maintained at reasonable steering strengths

**Medium Confidence** (theoretically sound but limited empirical validation):
- The ISS-based theoretical guarantees for P, PI, and PID controllers in LLM activation dynamics
- The matched/unmatched disturbance decomposition assumption for integral term effectiveness
- Layer-wise sequential mapping superiority over non-sequential alternatives

**Low Confidence** (largely theoretical, minimal empirical support):
- The optimal gain tuning strategy and performance bounds achievable with systematic numerical methods
- Behavior on extremely large models (70B+) and significantly different architectures
- Generalization to domains beyond text and image generation

## Next Checks

1. Systematically sweep Kp, Ki, Kd values across a grid for toxicity mitigation on Gemma2-2B and plot performance vs. utility degradation curves to identify optimal operating regions.
2. Apply PID Steering to a significantly different architecture (e.g., LLaMA-2-7B with RMSNorm or a MoE variant) to test framework generalizability.
3. Test PID Steering on a 70B+ parameter model (e.g., Llama-2-70B) for jailbreak prevention to validate scalability and stability conditions.