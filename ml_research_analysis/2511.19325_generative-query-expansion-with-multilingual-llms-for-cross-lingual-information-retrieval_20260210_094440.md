---
ver: rpa2
title: Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information
  Retrieval
arxiv_id: '2511.19325'
source_url: https://arxiv.org/abs/2511.19325
tags:
- query
- expansion
- retrieval
- language
- cross-lingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines the use of multilingual large language models
  for cross-lingual query expansion in information retrieval. It compares three open-source
  models (Aya Expanse 8B, Gemma 3 4B, and Gemma 3 12B) across four prompting strategies
  (zero-shot, Chain-of-Thought, Rephrase and Respond, and few-shot) on two datasets:
  CLIRMatrix and mMARCO.'
---

# Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval

## Quick Facts
- arXiv ID: 2511.19325
- Source URL: https://arxiv.org/abs/2511.19325
- Reference count: 40
- Key result: Query length determines optimal prompting strategy (zero-shot for short, few-shot for long queries) in cross-lingual query expansion

## Executive Summary
This paper investigates the use of multilingual large language models for cross-lingual query expansion in information retrieval. The authors compare three open-source models (Aya Expanse 8B, Gemma 3 4B, and Gemma 3 12B) across four prompting strategies (zero-shot, Chain-of-Thought, Rephrase and Respond, and few-shot) on two datasets: CLIRMatrix and mMARCO. Results show that query length is the primary determinant of prompting effectiveness, with zero-shot prompting excelling for short queries and few-shot prompting for longer queries. Cross-lingual expansion improves retrieval performance, particularly for languages with weaker baselines, but performance degrades for non-Latin scripts. Fine-tuning on similar-style data yields modest gains, while mismatched fine-tuning data causes losses.

## Method Summary
The paper evaluates cross-lingual query expansion using multilingual LLMs to generate pseudo-documents that improve retrieval performance. Three models (Aya Expanse 8B, Gemma 3 4B/12B) are tested with four prompting strategies (zero-shot, CoT, RaR, few-shot with 5 examples) across two datasets (CLIRMatrix with 8 languages, mMARCO with 14 languages). The pipeline involves translating source queries to target languages, generating pseudo-documents via mLLM, and retrieving documents using BM25 with either pseudo-documents only or concatenated with original queries. Fine-tuning is applied to Aya Expanse 8B on CLIRMatrix monolingual subsets to assess format-specific transfer effects.

## Key Results
- Query length is the primary determinant of prompting effectiveness: zero-shot excels for short queries, few-shot for longer queries
- Cross-lingual expansion improves retrieval performance, especially for languages with weaker baselines
- Performance degrades for non-Latin scripts, with European language pairs showing highest effectiveness
- Fine-tuning improves performance only when training and test data share similar query format

## Why This Works (Mechanism)

### Mechanism 1: Query Length Determines Prompting Effectiveness
Short queries (1-2 words) are easily degraded by elaborate prompts that introduce irrelevant meta-text, which harms BM25's lexical matching. Longer queries have more context, so few-shot examples help the model generate relevant pseudo-documents without concept drift. This explains why zero-shot prompting achieves highest scores for CLIRMatrix short queries while few-shot excels for longer mMARCO queries.

### Mechanism 2: Script and Resource-Level Linguistic Disparities
Higher-resource languages have better multilingual representation in mLLM training data, enabling more accurate translation and semantically relevant pseudo-document generation. Non-Latin scripts suffer from weaker cross-lingual alignment and more translation errors, causing the 10-15% Hit@10 performance gap between Latin and non-Latin script pairs.

### Mechanism 3: Format-Sensitive Fine-Tuning Transfer
Fine-tuning on CLIRMatrix (short title-style queries) optimizes the model for that format; applying it to mMARCO (longer question-style queries) introduces distribution shift that degrades pseudo-document quality. This explains why fine-tuning improves performance on CLIRMatrix but hurts performance on mMARCO.

## Foundational Learning

- Concept: Query Expansion vs. Pseudo-Document Generation
  - Why needed: The paper shifts from traditional term-based expansion to LLM-generated pseudo-documents
  - Quick check: How does a pseudo-document differ from a synonym-augmented query, and why does this help dense retrieval?

- Concept: Sparse vs. Dense Retrieval Tradeoffs
  - Why needed: The paper uses BM25 (sparse) for evaluation; mechanism effectiveness may differ for embedding-based dense retrieval
  - Quick check: Would the negative effect of meta-text in CoT/RaR prompts persist with a dense retriever like mE5?

- Concept: Cross-Lingual Transfer in mLLMs
  - Why needed: Language-specific performance disparities stem from training data imbalances
  - Quick check: Why might a model trained on 23 languages (Aya) perform differently than one trained on 140+ languages (Gemma 3) on the same task?

## Architecture Onboarding

- Component map: Input query -> Translation stage -> Expansion stage (prompting strategy) -> Pseudo-document generation -> Retrieval stage (BM25) -> Ranked document output

- Critical path: Prompting strategy selection based on query length is the highest-leverage decision. Misalignment here (e.g., using CoT for 2-word queries) can degrade performance by 5-10 percentage points on Hit@10.

- Design tradeoffs:
  - Zero-shot vs. few-shot: Zero-shot is simpler and faster; few-shot requires example selection but helps longer queries
  - Pre-translation expansion vs. post-translation expansion: Paper finds pre-translation expansion + query concatenation generally best for recall
  - Pseudo-document only vs. query + pseudo-document: Concatenation helps longer queries more; short queries are already captured in pseudo-document

- Failure signatures:
  - Meta-text contamination: CoT/RaR prompts generate phrases like "I will answer by..." which harm BM25 matching
  - Concept drift in few-shot: Examples influence pseudo-document content, introducing irrelevant terms for short queries
  - Cross-script degradation: Chinese/Arabic/Japanese queries show 10-15% lower Hit@10 than European language pairs

- First 3 experiments:
  1. Baseline comparison: Run retrieval with original queries (no expansion) on your target language pair to establish baseline Hit@10 and Recall@50
  2. Query length stratification: Categorize your queries by length (short <5 words vs. long >=5 words) and test zero-shot vs. few-shot separately on each category
  3. Script-aware evaluation: If your use case involves non-Latin scripts, explicitly measure the performance gap between same-script and cross-script pairs to set deployment expectations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the optimal prompting strategies for cross-lingual query expansion transfer to dense retrieval settings?
- Basis: The authors note that BM25 was used for evaluation but dense retrieval methods are increasingly common
- Why unresolved: The study only evaluated sparse retrieval; dense methods may interact differently with generated pseudo-documents
- What evidence would resolve it: Comparative experiments using the same models and strategies with dense retrievers alongside BM25

### Open Question 2
- Question: Does fine-tuning mLLMs on mixed datasets combining multiple query styles lead to cross-domain generalization gains?
- Basis: The authors state that future work could look at fine-tuning on mixed datasets
- Why unresolved: Fine-tuning on CLIRMatrix helped on CLIRMatrix but hurt performance on mMARCO
- What evidence would resolve it: Fine-tune models on combined datasets with varied query formats and evaluate on held-out domains

### Open Question 3
- Question: Can zero-shot cross-lingual generation (directly producing target-language pseudo-documents from source-language queries) outperform the two-stage translation-then-expansion approach?
- Basis: The authors write that future research may consider zero-shot cross-lingual generation
- Why unresolved: The study always separated translation and expansion into sequential steps
- What evidence would resolve it: Experiments comparing direct cross-lingual generation versus the translation+expansion pipeline

## Limitations
- Findings may not generalize to dense retrieval architectures where irrelevant terms have less impact
- Performance disparities for non-Latin scripts are documented but not fully explained mechanistically
- Fine-tuning sensitivity to query format is demonstrated but only tested on two specific fine-tuning scenarios

## Confidence

**High confidence**:
- Query length determines prompting effectiveness
- Cross-lingual expansion improves retrieval for languages with weaker baselines
- Fine-tuning improves performance only when training and test data share similar format

**Medium confidence**:
- Script and resource-level linguistic disparities explain performance differences
- Pre-translation expansion + query concatenation is generally optimal for recall
- Fine-tuned models benefit from larger model sizes

**Low confidence**:
- Specific impact of prompt meta-text contamination across different retrieval systems
- Generalization of findings to languages outside the tested 8-language set
- Optimal few-shot example selection strategy (random sampling details not specified)

## Next Checks

1. **Retrieval architecture validation**: Replicate key findings using a dense retriever (e.g., mE5) instead of BM25 to test whether query length-prompting strategy relationship holds across retrieval paradigms.

2. **Script performance diagnostics**: For a non-Latin script pair (e.g., Chineseâ†’English), analyze specific translation and pseudo-document generation failures by comparing generated outputs to reference translations and examining concept drift patterns.

3. **Fine-tuning format generalization**: Test whether fine-tuning on a mixed-format dataset (combining CLIRMatrix and mMARCO styles) can achieve good performance across both query formats, or if curriculum learning across query types provides better generalization than single-format fine-tuning.