---
ver: rpa2
title: 'Learning More from Less: Unlocking Internal Representations for Benchmark
  Compression'
arxiv_id: '2602.00710'
source_url: https://arxiv.org/abs/2602.00710
tags:
- benchmark
- source
- qwen2
- item
- gp-irt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REPCORE introduces a benchmark compression framework that leverages
  internal hidden states to overcome source model scarcity. Unlike prior methods that
  rely on sparse output labels, REPCORE aligns heterogeneous hidden states into a
  unified latent space, enabling robust coreset selection and performance extrapolation
  even with as few as ten source models.
---

# Learning More from Less: Unlocking Internal Representations for Benchmark Compression

## Quick Facts
- **arXiv ID**: 2602.00710
- **Source URL**: https://arxiv.org/abs/2602.00710
- **Reference count**: 40
- **One-line result**: REPCORE improves benchmark compression using aligned hidden states, achieving up to ρ = 0.913 and MAE as low as 0.045 with as few as ten source models.

## Executive Summary
REPCORE introduces a benchmark compression framework that leverages internal hidden states to overcome source model scarcity. Unlike prior methods that rely on sparse output labels, REPCORE aligns heterogeneous hidden states into a unified latent space, enabling robust coreset selection and performance extrapolation even with as few as ten source models. Experiments on five benchmarks (BBH, GSM8K, SEED-Bench-2-Plus, ARC-Challenge, and MMLU-Pro) with over 200 models show consistent improvements over output-based baselines in ranking correlation (up to 0.913) and mean absolute error (as low as 0.045). Spectral analysis reveals that aligned representations encode separable components reflecting both broad response tendencies and task-specific reasoning patterns. Ablation studies confirm that continuous embeddings are essential for preserving item structure, while source model diversity and capability range further enhance performance. REPCORE remains effective under temporal distribution shifts and scales to larger coreset budgets, demonstrating stability and generalizability.

## Method Summary
REPCORE addresses benchmark compression by extracting final-layer final-token hidden states from diverse source models, aligning them via model-specific linear projections into a shared 32-dimensional bottleneck with a shared MLP, and training with cross-entropy to predict correctness. Consensus embeddings are formed by averaging and L2-normalizing across models, then clustered to select representative coresets. Ridge regression on scalar average-difficulty features extrapolates target model performance from the coreset to the full benchmark.

## Key Results
- **Ranking correlation**: Achieves up to ρ = 0.913 on ARC-Challenge, outperforming output-based baselines.
- **Mean absolute error**: Reaches as low as 0.045 on GSM8K, demonstrating precise performance extrapolation.
- **Robustness**: Maintains effectiveness under temporal distribution shifts and scales to coreset budgets up to 200 items.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligned hidden states encode task-specific structure beyond scalar difficulty.
- Mechanism: Model-specific linear projections followed by a shared MLP create aligned 32-dim embeddings trained via cross-entropy on correctness prediction. The bottleneck forces retention of prediction-relevant features while discarding model-specific noise.
- Core assumption: Hidden states from the final layer and final token contain sufficient signal about reasoning patterns and item characteristics, and this signal can be linearly aligned across architectures.
- Evidence anchors:
  - [abstract] "Spectral analysis further indicates that the aligned representations contain separable components reflecting broad response tendencies and task-specific reasoning patterns."
  - [section 4.2] After difficulty stratification, subtask clustering persists with ε² ≈ 0.54, "confirming that the aligned representations encode fine-grained functional identities that transcend mere performance-based scalar signals."
  - [corpus] Limited external validation; neighboring papers on hidden states for assessment exist (e.g., arXiv:2501.12934 on code correctness via internal representations) but don't directly replicate this alignment mechanism.
- Break condition: If hidden states from different architectures lack shared predictive structure for correctness, alignment will fail to improve over random. If task types don't differ systematically in reasoning patterns, clustering gains will disappear.

### Mechanism 2
- Claim: Averaging aligned embeddings across diverse source models yields stable consensus representations.
- Mechanism: L2-normalized consensus embeddings ẽᵢ = eᵢ/‖eᵢ‖₂ are computed where eᵢ = (1/|S|)Σₘ zₘ,ᵢ. Clustering on these selects one anchor per cluster (item closest to centroid), maximizing directional similarity.
- Core assumption: Model-specific noise averages out while task-relevant structure persists; diversity in source model families reduces architectural bias.
- Evidence anchors:
  - [section 2.3] "This normalization emphasizes directional agreement and captures the geometric structure of the item space independent of vector magnitude."
  - [section 5, Table 5] Single-family source pools show degraded performance; diverse composition yields ρ = 0.868 vs. 0.686–0.848 for homogeneous pools.
  - [corpus] No direct external evidence; mechanism is specific to this work.
- Break condition: If source models share systematic biases (same training data, architecture), consensus will amplify bias rather than cancel noise. With <5 diverse models, averaging may not stabilize.

### Mechanism 3
- Claim: Ridge regression on scalar average-difficulty features extrapolates accurately from small coresets.
- Mechanism: Each item gets feature xᵢ = average correctness across sources. Ridge regressor gₜ fitted on coreset predicts target model scores on remaining items: ĝₜ = argmin Σᵢ∈C(g(xᵢ) − yₜ,ᵢ)². Full accuracy combines coreset ground-truth with predictions on I\C.
- Core assumption: Target model performance correlates linearly with source-averaged item difficulty; coreset anchors span the difficulty distribution.
- Evidence anchors:
  - [section 2.4] "This aggregated score serves as a robust input feature" for extrapolation.
  - [section 4.3, Table 2] PC1 alone (aligned with difficulty ρ ≈ 0.46) provides competitive baseline; adding PC2/PC3 improves performance, confirming difficulty alone is insufficient.
  - [corpus] GP-IRT (Polo et al., 2024) uses similar extrapolation from response patterns but requires more source models; REPCORE's innovation is the input representation, not the regressor.
- Break condition: If target model has qualitatively different difficulty calibration than sources (e.g., much stronger/weaker), linear extrapolation will be biased. If coreset lacks coverage of hard or easy items, predictions extrapolate poorly.

## Foundational Learning

- **Hidden States in LLMs**:
  - Why needed here: Understanding what information final-layer hidden states encode is prerequisite to trusting them as item representations.
  - Quick check question: Can you explain why the final token's hidden state might encode information about model confidence and reasoning beyond the output token itself?

- **Representation Alignment Across Models**:
  - Why needed here: The core technical challenge is mapping heterogeneous hidden dimensions to a shared space without introducing systematic bias.
  - Quick check question: Why might model-specific projections + shared MLP be preferable to a single projection for all models?

- **Coreset Selection via Clustering**:
  - Why needed here: Selecting representative items requires understanding how clustering in embedding space relates to benchmark coverage.
  - Quick check question: If two items have similar difficulty but different task types, should they end up in the same cluster? How would you verify this?

## Architecture Onboarding

- **Component map**:
  Hidden State Extraction (final layer, final token) -> Model-Specific Linear Projection Projₘ -> Shared MLP f_θ (bottleneck to 32-dim) -> Linear Classifier g_φ -> Cross-Entropy Loss -> Consensus Embedding (average + L2 normalize) -> K-Means Clustering -> Select K anchors -> Ridge Regressor (scalar difficulty features) -> Full Benchmark Prediction

- **Critical path**: The alignment quality (Projₘ + shared MLP training) determines everything downstream. Monitor classifier AUC on held-out items; paper reports ~0.9. If AUC < 0.75, bottleneck may be too narrow or hidden states lack predictive signal.

- **Design tradeoffs**:
  - Bottleneck dimension d_z = 32: Paper claims effective rank <20 captures 99% variance. Larger d_z adds noise; smaller loses structure.
  - Single scalar feature for extrapolation: Robust to overfitting on small coresets but discards inter-model variance information. Alternative: per-model difficulty features (|S| features) risks high variance.
  - K clusters = coreset size: Direct mapping. Paper tests K ∈ {10, 20, 30, 40, 50, 100, 150, 200}.

- **Failure signatures**:
  - Low classifier AUC (<0.7): Hidden states don't predict correctness for this task/model combination.
  - Stratified analysis shows ε² ≈ 0 for task type: Embeddings collapsed to difficulty-only; check source model diversity.
  - MAE improves but Agreement (item-level match) stays low: Regressor shrinking to mean; coreset may not span difficulty range.

- **First 3 experiments**:
  1. **Sanity check**: Train alignment on one benchmark (e.g., BBH), verify classifier AUC >0.8 on held-out items. If below threshold, try larger d_z or different hidden layer.
  2. **Ablation on clustering input**: Compare consensus embeddings vs. binary correctness vectors (Table 4). Expect Δρ ≈ 0.02–0.05 improvement with embeddings.
  3. **Source diversity test**: Run with 10 sources from single family vs. diverse families. Expect 5–15% ρ degradation with homogeneous sources (Table 5).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can aligned representations learned from open-source models transfer to enable efficient coreset selection for closed-source proprietary systems without requiring hidden state access?
- Basis in paper: [explicit] Authors state: "For future directions, we aim to extend this paradigm to closed-source models, thereby establishing a unified standard for open and proprietary systems."
- Why unresolved: The current framework requires extracting hidden states from source models, which is impossible for closed-source APIs. No proxy mechanism or transfer learning approach has been proposed to bridge this gap.
- What evidence would resolve it: A method that trains alignment projections on open-source models and successfully applies them to closed-source model evaluation, or an alternative signal that can substitute for hidden states in the alignment process.

### Open Question 2
- Question: Do aligned representations exhibit cross-domain transferability, enabling coreset selection for specialized domains without task-specific retraining?
- Basis in paper: [explicit] Authors state: "Furthermore, exploring the cross-domain transferability of aligned representations promises to enable efficient coreset selection for specialized domains without task-specific retraining."
- Why unresolved: Experiments were conducted on five specific benchmarks, but whether representations learned on one benchmark can generalize to structurally different domains (e.g., code, legal reasoning, medical diagnostics) remains untested.
- What evidence would resolve it: Experiments showing that aligned embeddings trained on general benchmarks can guide coreset selection on specialized benchmarks without retraining the alignment module.

### Open Question 3
- Question: What is the optimal bottleneck dimension for the aligned embedding space, and how does it vary across benchmarks with different structural complexity?
- Basis in paper: [inferred] The paper sets d_z = 32 empirically, noting "the effective rank of the aligned embedding matrix (capturing 99% variance) remains below 20," but provides no systematic study of dimension sensitivity or optimal capacity.
- Why unresolved: The choice appears heuristic; compressed representations could lose task-specific structural signals if too small, or introduce noise if too large, particularly for benchmarks with richer hierarchies.
- What evidence would resolve it: A systematic ablation varying d_z across a range (e.g., 8–128) on all benchmarks, reporting both predictive performance and structural preservation metrics.

## Limitations
- Relies on final-layer final-token hidden states, which may not capture optimal task-relevant structure across all architectures.
- Requires hidden state access, making it inapplicable to closed-source models without additional transfer mechanisms.
- Assumes linear relationships between source-averaged difficulty and target model performance, which may break for extreme capability gaps.

## Confidence

- **High confidence**: The empirical demonstration that aligned hidden states outperform output-based baselines on all five benchmarks, with consistent improvements in both ranking correlation (up to ρ = 0.913) and mean absolute error (as low as 0.045). The ablation showing continuous embeddings outperform binary vectors is robust across datasets.
- **Medium confidence**: The spectral analysis showing separable components in aligned representations reflects both broad response tendencies and task-specific reasoning patterns. While statistically demonstrated (ε² ≈ 0.54 for task clustering), the interpretation that this captures "reasoning patterns" is inferential rather than directly validated.
- **Low confidence**: The claim that the final-layer final-token hidden state specifically is optimal for this task. The paper does not compare against alternative layer positions or token positions, nor explain why this particular state captures the relevant information.

## Next Checks
1. **Hidden state ablation**: Test REPCORE with hidden states from different layers (e.g., middle layer, first layer) and token positions (e.g., [CLS] token, average pooling) to determine if final-layer final-token states are uniquely effective or simply sufficient.
2. **Temporal shift robustness**: Evaluate REPCORE on benchmark subsets from different time periods or with known distribution shifts to quantify degradation and compare against temporal adaptation baselines.
3. **Scalability validation**: Test the framework with larger coreset sizes (K > 200) and evaluate whether the linear extrapolation remains effective or if more sophisticated models (e.g., nonlinear regressors) become necessary.