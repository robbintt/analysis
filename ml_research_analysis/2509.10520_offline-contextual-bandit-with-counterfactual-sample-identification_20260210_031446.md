---
ver: rpa2
title: Offline Contextual Bandit with Counterfactual Sample Identification
arxiv_id: '2509.10520'
source_url: https://arxiv.org/abs/2509.10520
tags:
- policy
- action
- context
- learning
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses confounding in offline contextual bandit
  learning, where context features can bias the estimation of action rewards. The
  proposed Counterfactual Sample Identification (CSI) method reframes the problem:
  instead of directly predicting rewards, it learns to identify which action led to
  a successful outcome by comparing it to a counterfactual action sampled from the
  logging policy under the same context.'
---

# Offline Contextual Bandit with Counterfactual Sample Identification
## Quick Facts
- arXiv ID: 2509.10520
- Source URL: https://arxiv.org/abs/2509.10520
- Authors: Alexandre Gilotte; Otmane Sakhi; Imad Aouali; Benjamin Heymann
- Reference count: 27
- This paper introduces Counterfactual Sample Identification (CSI) for offline contextual bandits, showing improved performance over direct reward modeling in both synthetic and production environments.

## Executive Summary
This paper addresses confounding in offline contextual bandit learning, where context features can bias the estimation of action rewards. The proposed Counterfactual Sample Identification (CSI) method reframes the problem: instead of directly predicting rewards, it learns to identify which action led to a successful outcome by comparing it to a counterfactual action sampled from the logging policy under the same context. This approach effectively models the multiplicative advantage of actions while reducing confounding effects from context. Experiments show CSI consistently outperforms direct reward modeling (DM) in synthetic environments, particularly as sample size increases. On a large-scale production system for banner design optimization, CSI improved click-through rates by 0.5-1% over the DM baseline, with similar performance even when using a reduced feature set. The method provides a practical alternative to IPS-based approaches while maintaining theoretical grounding.

## Method Summary
The CSI method reframes offline contextual bandit learning by predicting which action led to a successful outcome rather than directly predicting rewards. For each logged interaction, the method samples a counterfactual action from the logging policy distribution under the same context. A binary classifier is then trained to predict whether the logged action (rather than the counterfactual) led to the observed outcome. This creates a learning signal that captures the relative advantage of the logged action while reducing confounding from context features. The approach can be implemented using any binary classification algorithm and scales to large action spaces. During inference, the method estimates the probability that each candidate action would be chosen as the successful one, effectively ranking actions by their expected reward.

## Key Results
- CSI consistently outperforms direct reward modeling in synthetic environments, with performance gap widening as sample size increases
- On a production banner optimization system, CSI achieved 0.5-1% higher click-through rates compared to the direct modeling baseline
- CSI maintained similar performance levels even when using a reduced feature set, demonstrating robustness to feature selection

## Why This Works (Mechanism)
The method works by transforming the reward prediction problem into a counterfactual comparison task. By training a classifier to distinguish between the logged action and a sampled counterfactual under the same context, the model learns to identify the relative advantage of each action independent of confounding context effects. This effectively estimates the multiplicative advantage function that relates reward to context, avoiding the direct modeling of potentially confounded reward predictions.

## Foundational Learning
- **Counterfactual reasoning**: Understanding how to reason about alternative outcomes under different actions is crucial for this method. The approach explicitly constructs counterfactual samples to create a learning signal.
- **Multiplicative advantage modeling**: Instead of directly predicting rewards, the method models how much better one action is compared to another under the same context, which helps avoid confounding.
- **Offline policy evaluation**: The method provides an alternative to importance sampling approaches for evaluating policies from logged data, with different bias-variance tradeoffs.
- **Binary classification as reward proxy**: The approach demonstrates how a binary classification task can be used to approximate ranking or selection tasks in contextual bandits.

## Architecture Onboarding
- **Component map**: Logging policy → Counterfactual sampler → Binary classifier → Action selector
- **Critical path**: Logged data → Context-action pairs → Counterfactual sampling → Classification training → Inference ranking
- **Design tradeoffs**: Simplicity and scalability vs. potential loss of information compared to direct reward modeling; avoids importance sampling variance but may require more samples
- **Failure signatures**: Poor performance when logging policy has insufficient exploration; degraded results when context-action correlations are too strong; sensitivity to counterfactual sampling quality
- **First experiments**: 1) Verify counterfactual sampling correctly matches logging policy distribution, 2) Test binary classifier performance on distinguishing logged vs counterfactual actions, 3) Compare ranking quality against direct reward modeling baseline

## Open Questions the Paper Calls Out
None

## Limitations
- The approach assumes the logging policy's behavior is known and stable, which may not hold in real-world dynamic environments
- The method's effectiveness relies on sufficient exploration in the logged data - if the logging policy consistently selects suboptimal actions for certain contexts, the counterfactual sampling may be uninformative
- The paper doesn't thoroughly address computational complexity, particularly for high-dimensional action spaces where sampling from the logging policy distribution could become expensive

## Confidence
- **High Confidence**: The theoretical foundation connecting CSI to IPS estimators and the mathematical derivation of the advantage-based formulation are sound and well-justified
- **Medium Confidence**: The synthetic experiments demonstrate clear advantages over direct reward modeling, though the controlled nature of these environments may not fully capture real-world complexity
- **Medium Confidence**: The production system results showing 0.5-1% CTR improvement are promising but based on a single application domain, limiting generalizability claims

## Next Checks
1. Test CSI on environments with non-stationary logging policies to assess robustness to policy shifts over time
2. Evaluate performance degradation when reducing the proportion of logged data from the optimal action to simulate limited exploration scenarios
3. Conduct ablation studies comparing CSI against hybrid approaches that combine it with direct reward modeling or other variance reduction techniques