---
ver: rpa2
title: 'Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis
  Design Framework and Empirical Trends'
arxiv_id: '2601.00536'
source_url: https://arxiv.org/abs/2601.00536
tags:
- retrieval
- reasoning
- multi-hop
- arxiv
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper surveys the procedural design space of retrieval\u2013\
  reasoning in multi-hop question answering (QA). It introduces a four-axis framework\u2014\
  overall execution plan, index structure, next-step control, and stop/continue criteria\u2014\
  to make explicit the procedural choices that couple retrieval and reasoning."
---

# Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis Design Framework and Empirical Trends

## Quick Facts
- **arXiv ID**: 2601.00536
- **Source URL**: https://arxiv.org/abs/2601.00536
- **Reference count**: 40
- **One-line primary result**: Introduces a four-axis framework to organize procedural design choices in multi-hop QA, showing empirical trends favoring interleaved execution and learned control.

## Executive Summary
This survey systematically maps the procedural design space of retrieval–reasoning in multi-hop question answering by proposing a four-axis framework: overall execution plan, index structure, next-step control, and stop/continue criteria. By categorizing 104 representative systems along these axes, the authors reveal that most systems interleave retrieval and reasoning, employ graph- or KG-based indices, and use budget-based stopping. Empirical trends indicate that moving from static retrieve–then–read to interleaved or plan-then-execute execution plans, and from rule-based to policy-based control, is associated with improved accuracy and faithfulness, especially under noisy retrieval and long reasoning chains.

## Method Summary
The authors reviewed 104 multi-hop QA systems and abstracted their procedural design choices onto four orthogonal axes: (1) execution plan (retrieve–then–read, interleaved, plan-then-execute), (2) index structure (raw text, passage graph, knowledge graph), (3) next-step control (rule-based, learned, hybrid), and (4) stopping criteria (budget, certainty, human-in-the-loop). Systems were classified based on published descriptions, and empirical trends were synthesized by aggregating reported performance and design rationales across studies. The framework aims to make explicit the coupling between retrieval and reasoning processes, and to guide future system design.

## Key Results
- Most systems use interleaved execution plans and graph- or KG-based indices, suggesting these choices improve accuracy and robustness.
- Policy-based (learned) control is increasingly adopted over rule-based approaches, correlating with gains in accuracy and faithfulness.
- Budget-based stopping dominates, though it may limit reasoning depth and adaptability under distribution shift.

## Why This Works (Mechanism)
The four-axis framework exposes the procedural coupling between retrieval and reasoning, enabling systematic comparison and identification of effective design choices. Interleaving retrieval and reasoning allows the system to adaptively refine queries based on intermediate evidence, mitigating noise and improving precision. Policy-based control leverages learned strategies to navigate complex reasoning spaces, generalizing better than hand-crafted rules. Graph- or KG-based indices facilitate structured traversal of multi-hop evidence paths, supporting efficient and interpretable reasoning.

## Foundational Learning
- **Multi-hop QA**: Requires aggregating evidence across multiple documents; needed to model complex reasoning chains.
  - *Quick check*: Can the system answer questions requiring evidence from at least two documents?
- **Graph/KG indices**: Enable structured navigation of document or entity relationships; needed for efficient multi-hop traversal.
  - *Quick check*: Does the index support path queries or entity linking?
- **Policy-based control**: Learns next-step actions from data; needed for adaptive reasoning under uncertainty.
  - *Quick check*: Is the control policy learned end-to-end or via reinforcement learning?
- **Interleaved execution**: Alternates retrieval and reasoning steps; needed to refine queries dynamically.
  - *Quick check*: Does retrieval happen after every reasoning step?
- **Budget-based stopping**: Halts reasoning after a fixed number of steps; needed to bound computation.
  - *Quick check*: Is stopping triggered by step count or uncertainty threshold?

## Architecture Onboarding
- **Component map**: Query → Index (Text/G- Graph/KG) → Control (Rule/Learned/Hybrid) → Stop/Continue → Answer
- **Critical path**: Query → Control → Index → Evidence → Reasoning → Stop/Continue → Answer
- **Design tradeoffs**: Interleaving improves accuracy but increases latency; policy-based control boosts adaptability but requires training data; budget-based stopping ensures tractability but may truncate reasoning.
- **Failure signatures**: Over-reliance on budget stopping can yield incomplete answers; rule-based control may fail on novel reasoning patterns; noisy retrieval can derail interleaved reasoning.
- **First experiments**:
  1. Compare retrieve–then–read vs. interleaved execution on a noisy retrieval benchmark.
  2. Ablate policy-based vs. rule-based control to measure robustness.
  3. Vary stopping criteria (budget vs. certainty) to assess answer completeness.

## Open Questions the Paper Calls Out
- How to align execution plans with index structures for optimal performance?
- Can generalisable control policies be learned across diverse multi-hop QA datasets?
- How to achieve robust stopping under distribution shift and noisy retrieval?

## Limitations
- The survey may overrepresent successful systems, underrepresenting failed or incremental approaches.
- The four-axis mapping assumes clear documentation and separability of design choices, which may not hold in practice.
- Empirical trends aggregate results across heterogeneous datasets and metrics, limiting causal attribution to individual axes.

## Confidence
- **High**: The four-axis framework is a useful organizing lens and accurately captures the major procedural dimensions across surveyed systems.
- **Medium**: Empirical trends suggesting benefits of interleaved execution and learned control are plausible but not rigorously validated across controlled experiments.
- **Medium**: The characterization of stopping criteria as predominantly budget-based is supported but may miss nuanced or implicit strategies in some systems.

## Next Checks
1. Conduct controlled ablation studies varying only one axis at a time (e.g., execution plan or control policy) to measure direct impact on accuracy and faithfulness.
2. Systematically audit a sample of surveyed papers for undocumented or implicit design choices to assess completeness of the four-axis mapping.
3. Replicate the trend analyses on a unified evaluation benchmark (same dataset, metric, and noise level) to isolate the effect of procedural design choices.