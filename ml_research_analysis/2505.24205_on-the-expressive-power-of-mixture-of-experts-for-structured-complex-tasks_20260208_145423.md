---
ver: rpa2
title: On the Expressive Power of Mixture-of-Experts for Structured Complex Tasks
arxiv_id: '2505.24205'
source_url: https://arxiv.org/abs/2505.24205
tags:
- networks
- layer
- network
- expert
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical analysis of Mixture-of-Experts
  (MoE) networks for modeling complex tasks with structural priors. The authors prove
  that shallow MoE networks can efficiently approximate functions on low-dimensional
  manifolds, overcoming the curse of dimensionality.
---

# On the Expressive Power of Mixture-of-Experts for Structured Complex Tasks

## Quick Facts
- arXiv ID: 2505.24205
- Source URL: https://arxiv.org/abs/2505.24205
- Authors: Mingze Wang; Weinan E
- Reference count: 40
- One-line primary result: Shallow MoE networks can efficiently approximate functions on low-dimensional manifolds, overcoming the curse of dimensionality.

## Executive Summary
This paper provides a theoretical analysis of Mixture-of-Experts (MoE) networks for modeling complex tasks with structural priors. The authors prove that shallow MoE networks can efficiently approximate functions on low-dimensional manifolds, overcoming the curse of dimensionality. For deep MoE networks, they show that O(L)-layer MoEs with E experts per layer can approximate piecewise functions comprising E^L pieces with compositional sparsity. The theoretical analysis reveals how MoE components work together - expert networks approximate localized subfunctions while the gating mechanism ensures correct input-to-expert assignment.

## Method Summary
The paper analyzes MoE expressive power through constructive approximation proofs. For shallow MoEs, they show that depth-2 MoE networks with E experts per layer can approximate functions on d-dimensional manifolds embedded in R^D with error bounds scaling with intrinsic dimension d rather than ambient dimension D. For deep MoEs, they prove that depth-2L MoE networks with E experts per layer can approximate piecewise functions comprising E^L distinct pieces when the target exhibits compositional sparsity. The proofs construct explicit network architectures where expert networks approximate local subfunctions and gating mechanisms ensure correct input routing.

## Key Results
- Shallow MoE networks overcome the curse of dimensionality by efficiently approximating functions on low-dimensional manifolds with error bounds depending on intrinsic dimension d rather than ambient dimension D.
- Deep MoE networks with O(L) layers and E experts per layer can approximate piecewise functions comprising E^L distinct pieces when the target exhibits compositional sparsity.
- The exponential expressivity gain requires compositional structure - without it, MoE networks are limited to O(LE) distinct regions.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Shallow MoE networks can efficiently approximate functions supported on low-dimensional manifolds, overcoming the curse of dimensionality.
- **Mechanism:** The approximation problem decomposes into (1) localized subproblems on low-dimensional subregions, each solved by a specialized expert, and (2) an assignment problem mapping inputs to correct regions via gating. Expert networks approximate local functions $f|_{U_i} \circ \phi_i^{-1}$ in intrinsic dimension $d$, while the gating mechanism ensures correct input-to-expert assignment through partition-function approximation.
- **Core assumption:** The target function lives on a compact, $d$-dimensional smooth manifold $\mathcal{M} \subset \mathbb{R}^D$ with a regular atlas (smooth coordinate maps $\phi_i$ with $\kappa(\phi_i) > \frac{D+3}{2}$).
- **Evidence anchors:**
  - [abstract] "For shallow MoEs, we prove that they can efficiently approximate functions supported on low-dimensional manifolds, overcoming the curse of dimensionality."
  - [Section 4.2, Theorem 4.8] Error bound $\tilde{O}(m^{-\frac{\kappa(f|_{U_i})}{d} \wedge \frac{1}{2}})$ depends on intrinsic dimension $d$, not ambient $D$.
  - [corpus] Limited direct corpus support; related work (Shaham et al. 2018, Chen et al. 2019) shows dense networks can also approximate manifold functions but require explicit coordinate constructions and activate all parameters.
- **Break condition:** If the manifold lacks regularity (e.g., non-smooth or no finite atlas) or if gating cannot approximate partition functions $\rho_i$ accurately, the decomposition fails.

### Mechanism 2
- **Claim:** Depth-$O(L)$ MoE networks with $E$ experts per layer can approximate piecewise functions comprising $E^L$ distinct pieces when the target exhibits compositional sparsity.
- **Mechanism:** Each pair of MoE layers implements $E$ subtasks via selective expert activation. Stacking $L$ blocks enables hierarchical composition: the $l$-th block approximates subfunctions $f_{l,i}(x_l)$ depending only on coordinate subset $x_l$, and outputs compose hierarchically to form the full function $f(x) = f_{out}(f_{1,i_1}(x_1), \ldots, f_{L,i_L}(x_L))$.
- **Core assumption:** The target function exhibits compositional sparsity—each subtask depends on a small subset of input coordinates, and the overall function is a hierarchical composition of these subtasks (Eq. 5).
- **Evidence anchors:**
  - [abstract] "For deep MoEs, we show that $\mathcal{O}(L)$-layer MoEs with $E$ experts per layer can approximate piecewise functions comprising $E^L$ pieces with compositional sparsity."
  - [Section 5.2, Theorem 5.2] Formal statement with error bound $\max_{l,i} \tilde{O}(m^{-\frac{\kappa(f_{l,i})}{d_l} \wedge \frac{1}{2}})$.
  - [corpus] Corpus papers on MoE sparsity (e.g., "Sparsity and Superposition in Mixture of Experts") discuss feature sparsity but do not directly address compositional structure.
- **Break condition:** If subtasks have dense dependencies (depend on many coordinates) or lack hierarchical composability, the exponential expressivity gain does not materialize.

### Mechanism 3
- **Claim:** MoE architectural components have distinct, complementary roles—gating handles input-to-expert assignment; experts approximate localized subfunctions.
- **Mechanism:** Layer 1 (or odd layers in deep MoE) approximates smooth partition functions $\rho_i$ using dense computation. The subsequent gating network $g^{(2)}(x) = W_R^{(2)} h^{(1)}(x)$ selects the expert corresponding to the region where $x \in U_{i_x}$. The proof constructs exact assignment via $\tau_{i_x}(x) \geq \frac{3}{4E}$ ensuring $\rho_{i_x}(x) > 0$.
- **Core assumption:** Partition functions $\rho_i$ are smooth enough to be approximated by neural networks; linear gating with appropriate $W_R$ can implement the selection.
- **Evidence anchors:**
  - [Section 4.2] "Expert networks approximate localized subfunctions, while the gating mechanism ensures correct input-to-expert assignment."
  - [Appendix A.2, Proof of Theorem 4.8] Explicit construction showing $\|\rho_i - \tau_i\| \leq \frac{1}{4E}$ guarantees correct routing.
  - [corpus] No direct corpus validation of this decomposition mechanism.
- **Break condition:** If partition boundaries are sharp/discontinuous or gating weights cannot linearly separate expert regions, assignment errors propagate.

## Foundational Learning

- **Concept: Manifold Hypothesis**
  - **Why needed here:** The core theoretical contribution assumes high-dimensional data lies on low-dimensional manifolds; understanding this is essential to grasp why MoE can avoid the curse of dimensionality.
  - **Quick check question:** Can you explain why approximation rate $\kappa(f)/D$ (dense) is worse than $\kappa(f|_{U_i})/d$ (MoE) when $d \ll D$?

- **Concept: Partition of Unity**
  - **Why needed here:** The routing mechanism relies on smooth partition functions $\rho_i$ that sum to 1 and localize inputs; this mathematical tool underpins the assignment construction.
  - **Quick check question:** How would failure to approximate $\rho_i$ affect expert selection in the proof construction?

- **Concept: Compositional Sparsity**
  - **Why needed here:** Deep MoE's exponential expressivity depends on this structural prior; understanding it clarifies when deep MoE provides advantage over shallow.
  - **Quick check question:** Given a function $f(x_1, x_2, x_3) = g(h_1(x_1), h_2(x_2), h_3(x_3))$, does it exhibit compositional sparsity? How many subfunctions define it?

## Architecture Onboarding

- **Component map:** Input -> Layer 1 (dense-like, approximates $\rho_i$) -> Layer 2 gating (reads $\tau(x)$, selects expert) -> Layer 2 expert (computes $g_i \circ \psi_i$) -> output. For deep MoE, repeat this block $L$ times with compositional aggregation.

- **Critical path:** Input $\to$ Layer 1 (dense-like, approximates $\rho_i$) $\to$ Layer 2 gating (reads $\tau(x)$, selects expert) $\to$ Layer 2 expert (computes $g_i \circ \psi_i$) $\to$ output. For deep MoE, repeat this block $L$ times with compositional aggregation.

- **Design tradeoffs:**
  - **Depth vs. experts:** Depth $L$ enables hierarchical composition (exponential task capacity); expert count $E$ enables subtask specialization. Increasing $E$ alone yields only $O(LE)$ regions without compositional structure.
  - **Linear vs. nonlinear gating:** Paper suggests nonlinear gating could eliminate the need for a preceding dense layer, improving efficiency (consistent with empirical findings in corpus: Akbarian et al. 2024).
  - **Shared vs. routed experts:** Shared expert per layer (as in Qwen2, DeepSeek) has equivalent expressive power to alternating MoE-dense architectures.
  - **Low-dimensional experts:** Autoencoder-expert composition ($f_{\text{low}} \circ \text{Enc}$) reduces parameters from $O(D^2)$ to $\#(\text{Enc}) + O(d^2)$ when $d \ll D$.

- **Failure signatures:**
  - Routing collapse: All inputs assigned to few experts (gating fails to separate regions).
  - Manifold mismatch: If data doesn't lie on low-$d$ manifold, theoretical guarantees don't apply; approximation may not improve over dense.
  - Missing compositional structure: Deep MoE doesn't gain exponential capacity if subtasks have dense coordinate dependencies.
  - Load imbalance: Top-K routing with auxiliary losses (not analyzed here) may introduce training dynamics issues (noted as future work).

- **First 3 experiments:**
  1. **Synthetic manifold validation:** Generate data on a known low-dimensional manifold (e.g., Swiss roll in $\mathbb{R}^3$) with smooth target functions. Compare shallow MoE vs. dense network approximation error as a function of expert width $m$ and ambient dimension $D$. Verify error scales with intrinsic $d$.
  2. **Compositional sparsity stress test:** Construct piecewise functions with explicit compositional structure (Eq. 4-5) over $E^L$ regions. Train depth-$2L$ MoE with $E$ experts/layer and measure whether all $E^L$ regions are approximated. Compare to naive $O(LE)$ capacity baseline.
  3. **Nonlinear gating ablation:** Replace linear gating $g(x) = W_R x$ with a small MLP gating function. Measure whether a single MoE layer with nonlinear gating achieves comparable approximation to the 2-layer construction in Theorem 4.8 (which uses Layer 1 for partition approximation).

## Open Questions the Paper Calls Out

- **Question:** Can the expressive MoE solutions described in the paper be efficiently learned via standard training algorithms like stochastic gradient descent (SGD)?
  - **Basis in paper:** [explicit] Section 6 states that analyzing "whether such expressive solutions can be found via training algorithms such as stochastic gradient descent" is a "critical open question."
  - **Why unresolved:** The paper focuses solely on approximation theory (existence of weights) rather than optimization dynamics, which involves non-differentiable Top-K routing and load-balancing objectives.
  - **What evidence would resolve it:** A theoretical analysis of the optimization landscape for MoEs or empirical proofs-of-concept demonstrating that SGD consistently converges to the constructed compositional solutions.

- **Question:** How does the expressive power of deep MoE networks change when approximating functions with non-compositional sparsity structures, such as group or graph sparsity?
  - **Basis in paper:** [explicit] Section 6 identifies "characterizing the expressive power of deep MoE networks under these alternative structures" (specifically group, graph, and temporal sparsity) as an important direction for future work.
  - **Why unresolved:** The main theoretical results (Theorem 5.2) rely heavily on the assumption of compositional sparsity (hierarchical decomposition) to prove exponential capacity; it is unclear if this efficiency holds for other sparse structures.
  - **What evidence would resolve it:** Deriving approximation rates for MoEs on function classes defined by group or graph sparsity, showing whether the exponential expressivity extends to these domains.

- **Question:** Does the theoretically motivated "low-dimensional expert" architecture (using an autoencoder flow) provide empirical efficiency gains over standard dense experts?
  - **Basis in paper:** [explicit] Section 4.3 proposes a specific architecture reducing parameters via an encoder $Enc$ and flow $flow$, but notes "We leave empirical validation of this theoretically motivated architecture for future work."
  - **Why unresolved:** While the theory suggests parameter reduction from $O(D^2)$ to $O(d^2)$, the practical trainability and performance of this specific decomposition on real-world data remain unverified.
  - **What evidence would resolve it:** Empirical experiments comparing standard MoE experts against the proposed encoder-flow experts on tasks with low-dimensional manifold priors, measuring both approximation error and parameter efficiency.

## Limitations

- The theoretical guarantees rely on strong structural priors (low-dimensional manifolds or compositional sparsity) that may not hold in real-world datasets.
- The proofs provide existence constructions but don't demonstrate these can be learned via gradient descent from data.
- The analysis assumes either a linear gating function or a two-layer construction, leaving gaps between theory and practical MoE architectures.

## Confidence

- **High Confidence:** The theoretical framework for shallow MoE approximation on manifolds (Mechanism 1) is mathematically rigorous and the proof construction is complete. The error bounds scaling with intrinsic dimension d are clearly derived.
- **Medium Confidence:** The deep MoE compositional sparsity analysis (Mechanism 2) provides formal guarantees, but the assumption of hierarchical composability may be restrictive. The exponential expressivity claim depends heavily on this structural assumption.
- **Low Confidence:** The practical implications of these theoretical results are uncertain. While the paper proves what MoE networks *can* represent, it doesn't address what they *will* learn in practice, particularly regarding routing dynamics and training stability.

## Next Checks

1. **Synthetic Manifold Experiment:** Generate data on a known low-dimensional manifold (e.g., Swiss roll in $\mathbb{R}^3$) with smooth target functions. Compare shallow MoE vs. dense network approximation error as a function of expert width $m$ and ambient dimension $D$. Verify error scales with intrinsic $d$, not ambient $D$.

2. **Compositional Sparsity Stress Test:** Construct piecewise functions with explicit compositional structure over $E^L$ regions. Train depth-$2L$ MoE with $E$ experts/layer and measure whether all $E^L$ regions are approximated. Compare to naive $O(LE)$ capacity baseline.

3. **Nonlinear Gating Ablation:** Replace linear gating $g(x) = W_R x$ with a small MLP gating function. Measure whether a single MoE layer with nonlinear gating achieves comparable approximation to the 2-layer construction in Theorem 4.8, which uses Layer 1 for partition approximation.