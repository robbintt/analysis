---
ver: rpa2
title: Dialect Normalization using Large Language Models and Morphological Rules
arxiv_id: '2506.08907'
source_url: https://arxiv.org/abs/2506.08907
tags:
- data
- greek
- thrace
- dialectal
- standard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a dialect-to-standard normalization method
  for Greek dialects that combines rule-based linguistic transformations with large
  language models (LLMs) using targeted few-shot prompting. The method applies morphological
  rules to preprocess dialectal text, then uses GPT-4o or Llama 3.1-70B with dialect-specific
  examples to produce normalized output.
---

# Dialect Normalization using Large Language Models and Morphological Rules

## Quick Facts
- arXiv ID: 2506.08907
- Source URL: https://arxiv.org/abs/2506.08907
- Reference count: 40
- Key outcome: Rule-based normalization + LLM few-shot prompting achieves 4.68 form and 4.62 meaning scores on Greek dialect proverbs

## Executive Summary
This paper introduces a method for dialect-to-standard normalization of Greek dialects that combines rule-based morphological transformations with large language models using targeted few-shot prompting. The approach applies 14 dialect-specific string replacement rules to handle systematic phonological patterns, then uses GPT-4o or Llama 3.1-70B with 3 examples per dialect group to handle facultative features and lexical items. Human evaluation shows GPT-4o with the full pipeline achieves average form score of 4.68/5 and meaning score of 4.62/5, with annotators preferring it 90% of the time. Downstream experiments demonstrate that normalization successfully removes superficial orthographic features that previously allowed accurate geolocation based on dialect rather than semantics.

## Method Summary
The method uses a two-step pipeline: first, rule-based normalization (RBN) applies dialect-group-specific string replacements based on comparative grammar to reverse systematic phonological transformations; second, an LLM processes the RBN output with few-shot prompting, using 3 dialect-specific examples and explicit instructions to preserve style. The approach handles three Greek dialect groups (Northern, Southern, Pontic) with 14 total rules covering phenomena like Northern vocalism and velar palatalization. GPT-4o with RBN achieved the highest human evaluation scores, while normalization reduced geolocation accuracy from F1=0.33 to F1=0.13, confirming previous methods relied on superficial features rather than semantics.

## Key Results
- GPT-4o with RBN achieved 4.68 form and 4.62 meaning scores vs. GPT-4o alone at 4.46 and 4.26
- Annotators preferred GPT-4o+RBN 88.3% (form) and 91.5% (meaning) of the time
- Downstream geolocation accuracy dropped from F1=0.33 to F1=0.13 after normalization
- K-means clustering produced more meaningful geographic groupings with normalized data
- Llama 3.1-70B achieved only 3.10 form score, significantly lower than GPT-4o

## Why This Works (Mechanism)

### Mechanism 1: Rule-Based Normalization for Systematic Phonological Patterns
Deterministic string replacements reliably reverse predictable dialectal transformations following regular phonological rules. RBN applies dialect-group-specific character sequence replacements (14 rules total) based on comparative grammar knowledge, handling patterns like Northern vocalism where unstressed mid-vowels raise to high vowels. This works when phonological changes are sufficiently regular and predictable for deterministic reversal without context-dependent disambiguation.

### Mechanism 2: Few-Shot Prompting for Facultative and Lexical Features
LLMs with carefully constructed prompts and minimal dialect-specific examples handle normalization features that resist rule-based approaches. The prompt provides region name, explicit style-preservation instructions, and three parallel examples showcasing non-rule-covered dialectal features. The LLM uses in-context learning to generalize from these examples, which is effective when the model has encountered sufficient dialectal text during pre-training.

### Mechanism 3: Sequential Pipeline Architecture (RBN Preprocessing Before LLM)
Applying rule-based normalization before LLM processing creates cleaner input and improves overall quality by reducing LLM burden. RBN handles systematic transformations first, producing intermediate text with reduced dialectal surface variation. The LLM then focuses on facultative features and lexical decisions that rules cannot address, preventing error propagation from ambiguous rule applications.

## Foundational Learning

- Concept: Phonological and morphological variation in dialectology
  - Why needed here: To design appropriate RBN rules, you must understand which dialectal features are systematic versus facultative. The paper classifies Greek dialects into three groups based on Trudgill (2003), each with distinct rule sets
  - Quick check question: In Figure 1, why does "ου" (/u/) in "Ου Θεός" need to be reversed to "ο" (/o/), and why would incorrectly reversing this to "ούτε" (neither) change the meaning entirely?

- Concept: Few-shot in-context learning with instruction following
  - Why needed here: The method's second stage relies on the LLM generalizing from 3 examples while following explicit style constraints. Understanding prompt engineering—particularly the role of negative constraints—is essential for replication
  - Quick check question: What would likely happen if you removed the instruction "Use words with the same etymology if and only if they exist in standard Greek, otherwise use different words"?

- Concept: Inter-annotator agreement metrics (ICC)
  - Why needed here: The paper uses ICC (2,k) to validate that human evaluation scores are reliable across three annotators. Understanding what constitutes "good" agreement (0.75-0.90) versus "excellent" (>0.90) helps interpret evaluation methodology
  - Quick check question: Why does the meaning ICC for GPT 3s+RBN (0.783) fall in the "good" range while form ICC (0.884) approaches "excellent"—what does this suggest about the relative subjectivity of these two evaluation axes?

## Architecture Onboarding

- Component map: Input: Raw dialectal proverb -> [Dialect Group Classifier] -> [RBN Engine] -> [Prompt Constructor] -> [LLM API] -> Output: Standard Modern Greek proverb

- Critical path:
  1. Classify input sentence to dialect group (based on source region metadata)
  2. Execute RBN rules: deterministic string replacements run in milliseconds on CPU
  3. Construct prompt with dialect-group-specific template and 3 examples
  4. Call LLM API (primary latency and cost driver)
  5. Return normalized text

- Design tradeoffs:
  - Model selection: GPT-4o costs more but achieves ~50% higher form scores than Llama 3.1-70B (4.68 vs 3.10)
  - Rule coverage vs. false positives: The 14 rules target high-confidence reversals only
  - Example selection: The 3 examples per group are manually selected to showcase features not covered by RBN
  - Geographic grouping: Three groups simplify rule design but may not capture sub-regional variation

- Failure signatures:
  - Form score < 3.0 with good meaning score: Indicates LLM is too weak or RBN not applied
  - Meaning score substantially lower than form score: Over-normalization—LLM removing semantic content
  - Specific vocabulary left unchanged or incorrectly translated: Dialectal lexical items with no standard cognate
  - Downstream geolocation still works too well after normalization: Normalization failed to remove superficial features
  - Clustering produces geographically incoherent groups: Normalized data still contains dialectal artifacts

- First 3 experiments:
  1. Pipeline ablation on your target language: Prepare 30 dialectal sentences with human-created reference normalizations. Run three conditions: (a) LLM-only with 3-shot, (b) RBN-only, (c) RBN+LLM. Have native speakers rate form and meaning on 1-5 scale.
  2. Rule coverage audit: Before implementing RBN, manually annotate 50 sentences marking each dialectal feature as systematic/rule-addressable, facultative/requiring LLM, or lexical/requiring dictionary.
  3. Downstream task validation: Train a simple classifier (e.g., logistic regression) on both original dialectal and normalized data for a semantic task (sentiment, topic). If accuracy is similar or improves on normalized data while geolocation accuracy drops substantially, normalization successfully removed superficial features while preserving semantics.

## Open Questions the Paper Calls Out
- Can the integration of a specialized dialectal-to-standard lexicon improve the pipeline's ability to handle rare vocabulary with no standard cognates?
- Can this rule-based plus few-shot prompting pipeline generalize effectively to language families with morphological complexity significantly different from Greek?
- Does the normalization of dialectal text lead to measurable performance improvements in general NLU tasks (e.g., sentiment analysis) rather than just geolocation?
- To what extent does the pre-training data composition of the underlying LLM determine the success of the few-shot normalization step?

## Limitations
- The 14 morphological rules are incompletely specified in the paper, making exact reproduction difficult
- Performance with smaller models (Llama 3.1-70B) is substantially lower than GPT-4o, raising questions about scalability
- The approach depends on the LLM having encountered sufficient dialectal text during pre-training
- Three examples may not be sufficient for all dialectal features across different languages

## Confidence
- High confidence: The pipeline architecture combining RBN preprocessing with LLM processing improves form and meaning scores over LLM alone (statistically significant p-values reported)
- Medium confidence: The dialectal features are sufficiently systematic that deterministic rule-based reversal works reliably; the approach generalizes beyond Greek to other language pairs with similar phonological variation
- Low confidence: Three examples are sufficient for LLM generalization across all dialectal features; RBN rules are complete and correct as implemented

## Next Checks
1. **Rule completeness audit**: Manually annotate 50 dialectal sentences to identify all systematic phonological features and verify the 14 rules cover them. Document any systematic features the rules miss.
2. **Example sufficiency test**: Vary the number of few-shot examples (1, 3, 5, 10) in the LLM prompt and measure impact on form/meaning scores to determine optimal trade-off between prompt length and performance.
3. **Downstream task validation**: Train a semantic classification model (e.g., topic or sentiment) on normalized vs. original data. If normalized data achieves similar or better performance while geolocation accuracy drops (as reported), this confirms normalization successfully removed superficial features while preserving semantics.