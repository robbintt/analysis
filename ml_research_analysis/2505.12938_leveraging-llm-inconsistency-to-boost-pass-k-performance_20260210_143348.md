---
ver: rpa2
title: Leveraging LLM Inconsistency to Boost Pass@k Performance
arxiv_id: '2505.12938'
source_url: https://arxiv.org/abs/2505.12938
tags:
- variants
- success
- challenge
- variant
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models exhibit inconsistent performance on equivalent
  task variants, which is typically viewed as a drawback. This paper introduces a
  novel "Variator" agent that leverages this inconsistency to improve Pass@k performance.
---

# Leveraging LLM Inconsistency to Boost Pass@k Performance

## Quick Facts
- **arXiv ID**: 2505.12938
- **Source URL**: https://arxiv.org/abs/2505.12938
- **Reference count**: 40
- **Primary result**: Variator agent leverages LLM inconsistency across task variants to improve Pass@k performance on coding and cybersecurity challenges

## Executive Summary
Large language models exhibit inconsistent performance on semantically equivalent task variants, a limitation typically viewed as a drawback. This paper introduces the Variator agent, which exploits this inconsistency by generating k task variants and solving each once, outperforming the baseline approach of generating k solutions for the original task. The method leverages the convexity of the Pass@k success function, where small performance gains on hard tasks are exponentially amplified while losses on easy tasks are capped. Experiments demonstrate consistent improvements across coding benchmarks and cybersecurity challenges, even with frontier reasoning models like OpenAI o3-mini and Claude 3.7 Sonnet.

## Method Summary
The Variator agent generates k semantic variants of a problem prompt using an LLM, then generates one solution per variant. This contrasts with the Repeater baseline that generates k solutions for the original prompt. The method preserves input/output specifications while modifying surface features like backstory and wording. For coding challenges, the agent uses APPS dataset problems with ≥60 test cases. Solutions are evaluated against the original test suite to calculate Pass@k performance.

## Key Results
- Variator consistently outperforms Repeater across coding benchmarks and cybersecurity challenges
- Performance gains are most pronounced for "hard" tasks where success rates are low
- Improvements persist even with frontier reasoning models (o3-mini, Claude 3.7 Sonnet)
- The inconsistency effect is robust across domains and model types

## Why This Works (Mechanism)

### Mechanism 1: Non-Linear Amplification via Pass@k
The Pass@k success function u_k(p) = 1 - (1-p)^k is convex. For hard tasks with low success rates p_o, variants often have slightly higher success rates due to random chance/wording alignment. These small gains are amplified exponentially by u_k, while losses on easy tasks (where p_o is already high) are capped because probabilities saturate at 1. This asymmetric gain results in higher mean Pass@k scores.

### Mechanism 2: Pathway Diversification (Tree Traversal)
LLM generation is conceptualized as tree traversal where early tokens act as critical decision points. A specific prompt variant may bias the model away from a "failure pathway" that the original prompt triggers. By generating variants, the system bets on at least one variant "steering" the model toward a robust solution path.

### Mechanism 3: Mitigation of Memorization Bias
Baseline performance is artificially inflated on public benchmarks due to training data memorization. By rewriting problems, Variator forces models to rely on reasoning rather than recall, allowing it to significantly outperform baseline on private datasets where memorization is absent.

## Foundational Learning

- **Concept: Pass@k Metric**
  - Why needed: This is the optimization target. Unlike accuracy (Pass@1), Pass@k allows for "lottery ticket" strategies where only one success among k attempts is required.
  - Quick check: If k=10 and a model has a 20% success rate on a task, how does generating 10 variants improve the odds compared to retrying the original task 10 times? (Answer: It depends on the variance of the variants; if variance is high, it exploits the convexity of the success function).

- **Concept: Semantic Equivalence**
  - Why needed: The Variator only works if the solution to a variant is also a solution to the original problem.
  - Quick check: If a variant changes the input/output format or adds a constraint not present in the original, does the mechanism hold? (Answer: No, the verifier would reject the solution or the solution would be invalid).

- **Concept: Prompt Sensitivity (Brittleness)**
  - Why needed: This "flaw" in LLMs is the resource the Variator consumes. One must understand that synonymous prompts yield different probability distributions over outputs.
  - Quick check: Why does the paper argue that larger/reasoning models (o3-mini) do not eliminate the need for this method?

## Architecture Onboarding

- **Component map**: Original Task -> Variator (LLM) -> k Variants -> Solver (LLM) -> k Solutions -> Verifier -> Aggregator (checks if ≥1 solution passes)

- **Critical path**: The Variant Generation Prompt. It must instruct the LLM to change "surface" features (wording, theme) while strictly locking "functional" features (constraints, I/O). The prompt provided in Appendix C is the reference implementation.

- **Design tradeoffs**:
  - Latency/Cost vs. Accuracy: Variator requires k variant generations + k solution generations (vs just k solutions), significantly increasing token usage.
  - Public vs. Private Data: On public benchmarks, the baseline has an unfair advantage (memorization). Variator shines on private/novel data.

- **Failure signatures**:
  - Non-equivalence: Generated variants accidentally change the logical constraints of the problem, making the original test suite invalid.
  - Low Variance: If the model is too robust or the generation prompt too weak, variants will look identical to the original, reducing Variator to Repeater.

- **First 3 experiments**:
  1. Sanity Check (Pass@1): Verify that on average, solving one variant is roughly as hard as solving the original (p_v ≈ p_o) to rule out that variants are just "easier."
  2. Variance Distribution: Generate 30 variants of a single task, solve each 50 times. Plot the histogram of success rates. Confirm it does not match the Null Hypothesis (Binomial distribution).
  3. Ablation on Memorization: Run Variator vs. Repeater on a known public benchmark (e.g., APPS) vs. a paraphrased "private" version of the same benchmark to measure the memorization headwind.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic or structural features in a task variant determine the selection of a specific reasoning pathway that leads to success or failure?
- Basis: The Discussion section states, "as yet we do not know what causes the model to choose a particular pathway for each variant."
- Why unresolved: The authors hypothesize that early decision points in the generation tree dictate the success rate, but they have not isolated the causal input features that trigger these specific paths.
- What evidence would resolve it: An interpretability analysis mapping specific semantic or syntactic attributes of variants to the model's internal state or early token generation choices that correlate with successful outcomes.

### Open Question 2
- Question: Can incorporating synthetically generated task variants into training datasets improve model robustness or generalization capabilities?
- Basis: Section 6 notes, "An interesting research direction beyond this work's scope is the use of our variant generation method to enrich models' training sets."
- Why unresolved: The current study leverages variants solely at inference time to boost Pass@k scores; the potential for these variants to serve as data augmentation for training has not been investigated.
- What evidence would resolve it: A comparative study where models are fine-tuned on datasets augmented with Variator-generated variants, followed by an evaluation of performance consistency and robustness on perturbed prompts.

### Open Question 3
- Question: To what extent does adding an automatic verification step for generated variants improve the success rate of the Variator agent?
- Basis: Section 6 suggests future work may include an "automatic verification step for the variants generated by the Variator to test equivalence."
- Why unresolved: The current implementation accepts a small percentage of non-equivalent variants (approx. 6%) which may lower the effective success rate, but the impact of filtering these out remains unquantified.
- What evidence would resolve it: An empirical benchmark comparing the current Variator agent against a modified version that utilizes a verifier to exclude non-equivalent variants before solution generation.

## Limitations
- The empirical evidence for mechanism 2 (pathway diversification) is primarily theoretical and anecdotal, with limited direct experimental validation.
- The magnitude of the inconsistency effect across different model families and scales remains unclear.
- The computational cost-benefit tradeoff is not fully quantified.

## Confidence
- **High confidence** in the core empirical finding: Variator consistently outperforms Repeater on verifiable tasks across multiple benchmarks and models.
- **Medium confidence** in mechanism 1 (Pass@k convexity amplification): The theoretical framework is sound and matches empirical observations.
- **Low confidence** in mechanism 2 (pathway diversification): This remains largely a theoretical hypothesis without direct experimental validation.

## Next Checks
1. **Ablation on Memorization**: Run Variator vs. Repeater on a known public benchmark (e.g., APPS) vs. a paraphrased "private" version of the same benchmark to measure the memorization headwind and quantify its contribution to performance differences.
2. **Variant Equivalence Analysis**: Systematically analyze the 6% non-equivalence rate mentioned in Section 3.2. What types of transformations most commonly break equivalence? Can this be reduced through prompt engineering?
3. **Cost-Performance Frontier**: Measure the Pass@k improvement per additional token cost for different values of k. Determine the breakeven point where Variator's benefits no longer justify its computational overhead.