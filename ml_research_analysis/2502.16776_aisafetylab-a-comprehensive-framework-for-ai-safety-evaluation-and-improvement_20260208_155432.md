---
ver: rpa2
title: 'AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement'
arxiv_id: '2502.16776'
source_url: https://arxiv.org/abs/2502.16776
tags:
- safety
- arxiv
- attack
- defense
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AISafetyLab is a comprehensive framework that integrates attack,
  defense, and evaluation methods for AI safety. It implements 13 attack methods (white-box,
  gray-box, and black-box), 16 defense methods (13 inference-time and 3 training-time),
  and 7 evaluation methods.
---

# AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement

## Quick Facts
- arXiv ID: 2502.16776
- Source URL: https://arxiv.org/abs/2502.16776
- Reference count: 18
- Primary result: Comprehensive AI safety framework with 13 attacks, 16 defenses, and 7 evaluators; AutoDAN achieves 92% ASR on Vicuna-7B-v1.5

## Executive Summary
AISafetyLab is a comprehensive framework that integrates attack, defense, and evaluation methods for AI safety. It implements 13 attack methods (white-box, gray-box, and black-box), 16 defense methods (13 inference-time and 3 training-time), and 7 evaluation methods. The framework features a modular design with unified interfaces for ease of use and extensibility. Experiments on Vicuna-7B-v1.5 using the HarmBench dataset show attack success rates ranging from 0% to 94%, with AutoDAN being the most effective attack method. Defense methods like Prompt Guard, Robust Aligned, and Safe Unlearning demonstrate strong performance, reducing attack success rates significantly. However, some defenses introduce high over-refusal rates, highlighting the trade-off between security and usability. The framework aims to advance AI safety research by providing a standardized toolkit for systematic evaluation and improvement.

## Method Summary
AISafetyLab implements 13 attack methods (white-box, gray-box, black-box), 16 defense methods (13 inference-time, 3 training-time), and 7 evaluation scorers. The framework uses a modular design with four attack components (Init, Mutate, Select, Feedback) and defense stages (preprocess, intraprocess, postprocess). It supports both local HuggingFace models and OpenAI-compatible APIs, with unified interfaces for configuration via YAML files. The framework includes comprehensive logging and supports concurrent deployment of multiple defenses. Training-time defenses were trained on approximately 1,000 samples, while experiments used a 50-sample HarmBench subset and Vicuna-7B-v1.5 as the target model.

## Key Results
- Attack success rates range from 0% to 94%, with AutoDAN achieving 92% ASR as the most effective method
- Prompt Guard defense completely neutralizes all attacks with 0% ASR but may cause over-refusal
- Erase Check and Robust Aligned defenses achieve low ASR but introduce over-refusal rates of 99.6% and 93.2% respectively
- Some attacks (DeepInception) achieve high ASR while generating fictional narratives rather than actionable harm, highlighting evaluation challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular attack decomposition enables rapid composition and comparison of jailbreak strategies.
- Mechanism: The Attack module separates concerns into four reusable components—Init (environment setup), Mutate (prompt transformation strategies), Select (ranking adversarial queries), and Feedback (optimization signals). This decomposition allows new attacks to be constructed by recombining existing modules rather than reimplementing from scratch.
- Core assumption: Attack methods share sufficient structural similarity that a common abstraction can capture their workflows without loss of effectiveness.
- Evidence anchors:
  - [section 3.2.1] "The modular design provides two key advantages: 1. User-Friendly... 2. Customizability: Developers can extend or modify the attack flow by adapting individual modules, facilitating the creation of new attack strategies using the provided building blocks."
  - [abstract] "AISafetyLab features an intuitive interface that enables developers to seamlessly apply various techniques while maintaining a well-structured and extensible codebase."
  - [corpus] Related toolkit OET similarly uses optimization-based modular design for prompt injection evaluation, suggesting this pattern is generalizable.
- Break condition: If an attack method requires tight coupling between mutation and selection logic (e.g., gradient-based optimization where selection is implicit), the modular decomposition may introduce abstraction overhead without benefit.

### Mechanism 2
- Claim: Staged inference-time defense provides defense-in-depth but introduces a security-usability trade-off.
- Mechanism: Defense methods operate at three stages—preprocessing (input modification), intraprocessing (decoding guidance), and postprocessing (output monitoring). The framework supports concurrent deployment of multiple defenses. Prompt Guard (preprocessing) achieves 0% ASR by classifier-based input rejection; however, Erase Check and Robust Aligned exhibit over-refusal rates of 99.6% and 93.2% respectively.
- Core assumption: Defense stages are sufficiently independent that composition does not create interference or adversarial blind spots.
- Evidence anchors:
  - [section 5.2, Table 1] "Prompt Guard completely neutralizes all attacks by employing a classifier on input queries. However, some defenses, such as Erase Check and Robust Aligned, while highly effective, introduce significant over-refusal rates, highlighting a trade-off in overall usability."
  - [section 3.3] "Inference-time defenses allow the concurrent deployment of multiple strategies to enhance robustness."
  - [corpus] PandaGuard and related frameworks similarly observe that aggressive defenses cause false positives; corpus evidence is limited on optimal composition strategies.
- Break condition: If composed defenses have conflicting objectives (e.g., one removes adversarial suffixes while another relies on their presence for detection), the combined system may underperform individual components.

### Mechanism 3
- Claim: Unified scorer interface enables standardized comparison but evaluation inconsistencies persist.
- Mechanism: All scorers inherit from `BaseScorer` and implement a `score(query, response)` interface returning judgment and metadata. This allows attack/defense comparisons using consistent evaluation criteria. However, the paper notes that DeepInception achieves high ASR under some defenses while generating fictional narratives rather than actionable harm, suggesting scorers may not capture nuanced harm.
- Core assumption: Safety can be adequately captured by binary or categorical judgments on query-response pairs, and different scoring methods will agree on clear-cut cases.
- Evidence anchors:
  - [section 3.4] "All the scorers utilize the same interface score to conduct safety evaluation, which takes a query-response pair as input and returns the judgment from the scorer."
  - [section 5.2] "The evaluation of robustness still poses significant challenges, often resulting in unfair comparisons between methods... responses often consist of fictional narratives or simple repetitions of the question, without providing precise or potentially harmful information."
  - [corpus] Neighbor papers (HarmBench, SALAD-Bench) similarly struggle with evaluation consistency; no corpus evidence resolves this limitation.
- Break condition: If scorers diverge significantly on edge cases (fictional harm vs. refusal vs. partial compliance), cross-method comparisons become unreliable.

## Foundational Learning

- Concept: **Jailbreak Attack Taxonomy (White-box vs. Gray-box vs. Black-box)**
  - Why needed here: The framework organizes 13 attack methods by access level; understanding this taxonomy is prerequisite for selecting appropriate attacks for a given threat model.
  - Quick check question: If you have only API access to a model (no gradients, no logits), which attack category applies?

- Concept: **Defense Stage Separation (Preprocess/Intraprocess/Postprocess)**
  - Why needed here: The defense module categorizes 13 inference-time methods by when they intervene; this determines composition rules and latency impact.
  - Quick check question: Which defense stage would be appropriate for a method that modifies token probabilities during generation?

- Concept: **Over-refusal Trade-off in Safety Alignment**
  - Why needed here: Table 1 shows defenses like Erase Check achieve low ASR but 99.6% over-refusal; understanding this trade-off is essential for selecting defenses that match deployment constraints.
  - Quick check question: A defense achieves 0% ASR but refuses 50% of benign requests—is this acceptable for a customer-facing chatbot?

## Architecture Onboarding

- Component map:
  ```
  AISafetyLab/
  ├── attack/           # 13 methods, organized by access level
  │   ├── attackers/    # Individual attack implementations (e.g., AutoDANManager)
  │   └── [Init|Mutate|Select|Feedback] modules
  ├── defense/
  │   ├── inference_defense/  # 13 methods: PPL, PromptGuard, SelfReminder, etc.
  │   └── training_defense/   # 3 methods: SafeTuning, SafeRLHF, SafeUnlearning
  ├── evaluation/
  │   └── scorers/      # 7 scorers inheriting from BaseScorer
  └── [Models|Dataset|Utils|Logging]  # Auxiliary modules
  ```

- Critical path:
  1. Define target model via Models module (local HuggingFace or OpenAI-compatible API)
  2. Load dataset via Dataset module (local files or HuggingFace Datasets)
  3. Configure attack via YAML/ConfigManager → `Attacker.from_config()`
  4. Optionally compose defenses via `create_defender_from_yaml()` and `chat(model, query, defenders)`
  5. Evaluate outputs via `Scorer.score(query, response)`

- Design tradeoffs:
  - **Unified interface vs. method-specific optimization**: The framework prioritizes ease of use and extensibility; some methods (e.g., GCG) required careful tokenization handling to work within the abstraction.
  - **Comprehensiveness vs. maintenance burden**: 36 total methods (13+16+7) require ongoing maintenance; the paper commits to continuous updates.
  - **Standardized evaluation vs. nuanced harm assessment**: Binary scorer outputs enable comparison but may miss context-dependent harm (see DeepInception issue).

- Failure signatures:
  - High over-refusal (>90%) when using Erase Check or Robust Aligned defenses on benign inputs
  - Attack success rate does not decrease despite defense deployment → check if defense is applicable to attack type (e.g., PPL Filter only effective against unreadable adversarial prompts)
  - Scorer disagreement on edge cases → use multiple scorers and inspect metadata

- First 3 experiments:
  1. **Baseline attack survey**: Run all 13 attacks on Vicuna-7B-v1.5 with HarmBench subset (50 samples) using LlamaGuard3Scorer; identify highest-ASR attacks (expect AutoDAN ~92%).
  2. **Single-defense effectiveness**: Apply Prompt Guard as sole defense; verify 0% ASR and measure over-refusal on XSTest benign set.
  3. **Defense composition test**: Combine Prompt Guard (preprocess) + Self Evaluation (postprocess); measure whether ASR remains at 0% and whether latency increases significantly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can evaluation frameworks be refined to accurately distinguish between genuinely harmful jailbreaks and benign fictional narratives to ensure fair comparisons?
- Basis in paper: [explicit] The authors note that methods like DeepInception often produce "fictional narratives" without "precise or potentially harmful information," which "underscores the necessity for more dependable evaluation frameworks."
- Why unresolved: Current evaluation methods may rely on pattern matching or classifiers that trigger false positives on creative but non-malicious text, leading to inflated Attack Success Rates (ASR).
- What evidence would resolve it: The development of a scorer that specifically filters for actionable harmful content rather than simple refusal absence, validated against human judgment.

### Open Question 2
- Question: How can defense mechanisms be optimized to minimize the trade-off between high security (low Attack Success Rate) and model usability (low over-refusal rate)?
- Basis in paper: [inferred] The paper reports that while methods like Erase Check and Robust Aligned are effective defenses, they introduce "significant over-refusal rates," highlighting a critical "trade-off in overall usability."
- Why unresolved: It remains difficult to harden models against adversarial prompts without causing them to incorrectly refuse benign or safe instructions.
- What evidence would resolve it: A defense strategy that maintains a low ASR (e.g., <10%) while keeping the over-refusal rate comparable to an undefended model.

### Open Question 3
- Question: What specific architectural or methodological adaptations are required to extend unified safety frameworks from LLMs to multimodal and agentic AI systems?
- Basis in paper: [explicit] The authors list "implementing methods for multimodal safety" and "agent safety" as key items in their future work plans.
- Why unresolved: The current framework and codebase are designed primarily for text-based Large Language Models; agents and multimodal models introduce complex state interactions and non-textual inputs.
- What evidence would resolve it: The successful integration of attack and defense modules for image/audio inputs or tool-using agents within the AISafetyLab framework.

## Limitations

- **Hyperparameter sensitivity**: The paper does not specify exact hyperparameters for individual attack methods, making it difficult to determine whether reported ASR values are representative or optimized.
- **Evaluation consistency challenges**: While the framework provides unified scorer interfaces, the paper acknowledges that evaluation remains a significant challenge, with some attacks achieving high ASR while generating fictional rather than harmful content.
- **Training-time defense generalization**: The three training-time defenses were trained on only ~1,000 samples, raising questions about their robustness to diverse attack patterns.

## Confidence

**High Confidence**: The framework's modular architecture and unified interfaces are well-documented and functional. The separation of attack methods by access level (white-box, gray-box, black-box) and defense methods by stage (preprocess, intraprocess, postprocess) is clearly implemented and experimentally validated.

**Medium Confidence**: The reported attack success rates and defense effectiveness are reproducible given the experimental setup (Vicuna-7B-v1.5, HarmBench subset). However, the lack of hyperparameter specifications and limited dataset size (~50 samples) reduces confidence in generalization.

**Low Confidence**: Claims about the framework's extensibility to new methods and its ability to resolve the "unsystematic and fragmented" nature of existing safety research are aspirational rather than empirically validated. The paper does not demonstrate integration of novel methods beyond the 36 included implementations.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary key hyperparameters (e.g., optimization steps, population size, temperature) for the top-3 attack methods and measure impact on ASR across different defenses. This will determine whether reported results are robust or sensitive to specific configurations.

2. **Cross-Scoring Agreement Study**: Run the same query-response pairs through all 7 scorers and compute inter-rater agreement (e.g., Cohen's kappa). Identify cases where scorers disagree and analyze whether these represent legitimate safety ambiguities or scorer limitations.

3. **Defense Composition Optimization**: Experiment with different defense stage combinations beyond the paper's examples. Systematically test all 2^13 possible inference-time defense combinations on a held-out attack set to identify synergistic combinations that minimize both ASR and over-refusal rate.