---
ver: rpa2
title: Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective
  and Efficient Reasoners
arxiv_id: '2509.26226'
source_url: https://arxiv.org/abs/2509.26226
tags:
- tfpi
- arxiv
- training
- reasoning
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Thinking-Free Policy Initialization (TFPI) addresses the high computational
  cost of RLVR training for reasoning models, which demands extremely long context
  lengths and leads to substantial resource usage. The core idea is to introduce a
  simple "ThinkFree" operation that explicitly discards thinking content during inference
  and training, enabling more efficient reasoning without specialized rewards or complex
  training designs.
---

# Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners

## Quick Facts
- arXiv ID: 2509.26226
- Source URL: https://arxiv.org/abs/2509.26226
- Reference count: 33
- A "ThinkFree" operation that discards thinking content enables 70%+ token reduction while improving reasoning performance

## Executive Summary
Thinking-Free Policy Initialization (TFPI) addresses the high computational cost of RLVR training for reasoning models by introducing a "ThinkFree" operation that explicitly discards thinking content during both inference and training. TFPI serves as an initialization stage before standard RLVR, bridging long Chain-of-Thought distillation and RLVR. The method achieves significant computational savings while improving reasoning performance, with experiments showing a 4B model reaching 89.0% accuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours.

## Method Summary
TFPI introduces a simple "ThinkFree" operation that explicitly discards thinking content during inference and training, enabling more efficient reasoning without specialized rewards or complex training designs. The method serves as an initialization stage before standard RLVR, bridging long Chain-of-Thought distillation and RLVR. TFPI accelerates RL convergence, achieves higher performance ceilings, and produces more token-efficient models. The approach addresses the computational challenges of RLVR training, which demands extremely long context lengths and leads to substantial resource usage.

## Key Results
- 4B model reached 89.0% accuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours
- TFPI consistently outperforms direct RL training and other efficient reasoning baselines across multiple benchmarks
- Reduces output token usage by 70% or more while maintaining or improving reasoning performance

## Why This Works (Mechanism)
The "ThinkFree" operation works by explicitly discarding thinking content during both training and inference phases. This approach leverages the observation that not all intermediate reasoning steps are necessary for the final answer, allowing the model to focus on the most relevant information. By initializing with this approach before standard RLVR, the model learns to reason more efficiently from the start, accelerating convergence and achieving better performance ceilings.

## Foundational Learning
- **Chain-of-Thought (CoT) reasoning**: Why needed - provides interpretable intermediate steps for complex reasoning; Quick check - verify models can generate coherent reasoning traces
- **Reinforcement Learning from Verifiable Rewards (RLVR)**: Why needed - enables learning from correctness feedback without manual labels; Quick check - confirm reward signals are properly shaped and bounded
- **Distillation from larger models**: Why needed - transfers reasoning capabilities to smaller, more efficient models; Quick check - measure knowledge transfer effectiveness

## Architecture Onboarding

**Component Map**
- Long CoT Distillation -> TFPI Initialization -> Standard RLVR

**Critical Path**
The critical path involves: (1) initial distillation from long CoT examples, (2) TFPI initialization with ThinkFree operation, (3) fine-tuning with standard RLVR, and (4) final evaluation on reasoning benchmarks.

**Design Tradeoffs**
TFPI trades off the interpretability and potential robustness of full Chain-of-Thought reasoning for computational efficiency and improved performance. The approach assumes that thinking content can be reliably discarded without losing essential reasoning information, which may not hold for all types of reasoning tasks.

**Failure Signatures**
- Degradation in performance on tasks requiring detailed intermediate reasoning steps
- Inability to handle complex multi-step problems that benefit from visible reasoning traces
- Potential loss of reasoning robustness when the ThinkFree operation removes critical intermediate information

**First 3 Experiments**
1. Compare TFPI-initialized models against direct RL training on AIME24 benchmark
2. Measure token reduction and accuracy trade-offs across different model sizes
3. Evaluate generalization to non-mathematical reasoning tasks beyond AIME24 and LiveCodeBench

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on two reasoning benchmarks (AIME24 and LiveCodeBench), potentially limiting generalizability
- Computational savings measured only in H2O hours and token reduction, without considering reasoning quality trade-offs
- Effectiveness depends on assumption that thinking content can be reliably discarded without losing essential information

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Performance improvements on specific benchmarks | High |
| Computational efficiency gains | High |
| General applicability to other reasoning domains | Medium |
| Long-term stability and robustness | Medium |

## Next Checks
1. Test TFPI on a broader range of reasoning benchmarks, including mathematical, logical, and commonsense reasoning tasks
2. Conduct ablation studies to quantify the impact of the "ThinkFree" operation on reasoning quality and information retention
3. Evaluate computational efficiency gains across different model sizes and hardware configurations to assess scalability