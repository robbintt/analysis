---
ver: rpa2
title: Error Detection and Constraint Recovery in Hierarchical Multi-Label Classification
  without Prior Knowledge
arxiv_id: '2407.15192'
source_url: https://arxiv.org/abs/2407.15192
tags:
- error
- learning
- constraints
- class
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a neurosymbolic approach to hierarchical
  multi-label classification (HMC) that learns to detect model errors and recover
  hierarchical constraints without prior knowledge of the hierarchy. The authors extend
  the Error Detection Rules (EDR) framework to the HMC setting and propose Focused
  EDR, which optimizes for error F1-score rather than precision by leveraging the
  ratio of submodular functions.
---

# Error Detection and Constraint Recovery in Hierarchical Multi-Label Classification without Prior Knowledge

## Quick Facts
- arXiv ID: 2407.15192
- Source URL: https://arxiv.org/abs/2407.15192
- Reference count: 39
- Primary result: A neurosymbolic method that learns to detect model errors and recover hierarchical constraints in HMC without prior knowledge, outperforming black-box detectors and recovering explainable constraints.

## Executive Summary
This paper introduces a neurosymbolic approach to hierarchical multi-label classification that learns error detection rules and recovers hierarchical constraints without requiring prior knowledge of the hierarchy. The authors extend the Error Detection Rules (EDR) framework to the HMC setting and propose Focused EDR, which optimizes for error F1-score rather than precision by leveraging submodular function ratios. The method learns rules that both detect when a model has made an error and recovers the underlying hierarchical constraints. Experiments on three datasets demonstrate that Focused EDR significantly outperforms both black-box neural error detectors and the original EDR approach in error detection accuracy, while also recovering hierarchical constraints and improving model consistency when integrated with Logic Tensor Networks.

## Method Summary
The method extends EDR to hierarchical multi-label classification by learning error detection rules that leverage predictions from complementary granularities. Given a base HMC model, the approach constructs condition sets using predictions from both fine and coarse granularity levels, along with secondary model predictions and binary classifiers. It then learns error detection rules per class by optimizing the F1-score of the error class through submodular ratio maximization. The recovered rules can be integrated with Logic Tensor Networks to improve model consistency by penalizing violations of the discovered constraints during training.

## Key Results
- Focused EDR achieves significantly higher error detection F1-scores compared to black-box neural detectors and original EDR on three datasets
- The method successfully recovers hierarchical constraints even when some classes are omitted from training data, with graceful degradation in performance
- When integrated with Logic Tensor Networks, the recovered constraints improve model performance and reduce inconsistencies by 2.4-3.4 percentage points

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical constraints can be implicitly recovered by detecting inconsistencies between predictions at different levels of granularity.
- **Mechanism:** The method constructs error detection rules where conditions include predictions from other hierarchy levels. Violations (e.g., predicting coarse label "Tank" but failing to assign valid sub-class) map the hierarchical structure.
- **Core assumption:** Base model errors correlate with hierarchical inconsistencies, and sufficient cross-granularity predictions establish these correlations.
- **Evidence anchors:** Section 3 describes using complementary granularity predictions; Table 1 shows specific rules with violating sets as complements of valid child classes.
- **Break condition:** If base model has high accuracy or noise is so high that predictions are random, the correlation signal is lost.

### Mechanism 2
- **Claim:** Optimizing for F1-score rather than precision prevents learning overly narrow rules.
- **Mechanism:** Reformulates objective as maximizing error class F1-score by treating optimization as ratio of submodular functions (POS and BOD+FP) and applying greedy approximation.
- **Core assumption:** Submodular ratio reliably approximates F1 landscape and greedy approximation finds global optimum.
- **Evidence anchors:** Section 3 explains F1 optimization via submodular ratio; Theorem 3.1 proves equivalence between maximization objective and F1-score.
- **Break condition:** If training set contains too few false positives, optimization may become unstable due to zero denominator.

### Mechanism 3
- **Claim:** Recovered error rules act as functional constraints that improve neurosymbolic model consistency.
- **Mechanism:** Learned logical rules are converted to constraints for Logic Tensor Networks, penalizing base model during training to align latent space with discovered hierarchy.
- **Core assumption:** Recovered rules reflect ground-truth structure rather than spurious correlations.
- **Evidence anchors:** Abstract mentions leveraging rules as constraints; Table 2 shows reduced inconsistency (7.77% to 5.37%) when integrated with LTN.
- **Break condition:** If rule conditions are derived from noisy secondary models, recovered constraints may propagate noise into LTN.

## Foundational Learning

- **Concept:** Submodular Function Optimization
  - **Why needed here:** Core algorithm maximizes ratio of submodular functions to find best error detection rules.
  - **Quick check question:** Can you explain why greedy algorithm works well for submodular maximization but might struggle with ratios of submodular functions?

- **Concept:** Hierarchical Multi-Label Classification (HMC)
  - **Why needed here:** Labels exist at different granularities (e.g., "Tank" vs. "T-72") and must satisfy parent-child relationships.
  - **Quick check question:** How does loss function differ between standard classification and HMC?

- **Concept:** Logic Tensor Networks (LTN)
  - **Why needed here:** Downstream consumer of recovered rules; combines neural perception with logical reasoning.
  - **Quick check question:** How does LTN incorporate logical rule like $A \implies B$ into gradient descent process?

## Architecture Onboarding

- **Component map:** Base Model ($\hat{f}_\theta$) -> Condition Generators (secondary models, binary classifiers) -> Rule Learner (Algorithm 1) -> Neurosymbolic Integrator (LTN)

- **Critical path:** Definition of Condition Set ($C_g$). If condition set doesn't include complementary granularity predictions, system cannot learn hierarchical constraints.

- **Design tradeoffs:**
  - Interpretability vs. Accuracy: Explicit rules offer explainability but may have lower raw accuracy than black-box detectors in low-noise regimes
  - Precision vs. F1: Explicitly trades high precision of prior work for better F1 (recall) to catch more errors

- **Failure signatures:**
  - High Noise: Constraint recovery F1 drops gracefully but rapidly when specific classes are removed from training
  - Model Collapse: Perfectly accurate base model results in zero POS and FP, leaving no signal for rule learner

- **First 3 experiments:**
  1. Ablate secondary model/binary classifier conditions from $C_g$ to measure drop in constraint recovery F1
  2. Replicate noise ratio experiment on custom dataset to verify graceful degradation holds for non-visual data
  3. Implement LTN loss function with recovered rules and measure if inconsistency decreases without sacrificing base accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Focused-EDR effectively learn error detection rules and recover constraints in HMC problems with more than two levels of granularity ($G > 2$)?
- **Basis in paper:** Section 3 states framework can theoretically extend to $G > 2$ but leaves evaluation to future work.
- **Why unresolved:** All experimental datasets had only two granularity levels (coarse and fine).
- **What evidence would resolve it:** Experimental results on deep hierarchies demonstrating maintained error F1-scores and constraint recovery as depth increases.

### Open Question 2
- **Question:** How does performance degrade if complementary secondary model provides low-quality or noisy condition sets?
- **Basis in paper:** Section 3 describes constructing condition sets using secondary model assuming availability of diverse conditions, but doesn't test sensitivity to secondary model failure.
- **Why unresolved:** Method relies on secondary model conditions to detect errors in main model; if secondary model is uncorrelated or significantly worse, rule learning might fail.
- **What evidence would resolve it:** Ablation study showing error F1-score trends while systematically varying secondary model accuracy.

### Open Question 3
- **Question:** Is submodular optimization algorithm scalable to large-scale hierarchies containing thousands of labels without computational bottlenecks?
- **Basis in paper:** Section 3 claims $O(|C_g|^2)$ worst-case time complexity which authors call "efficient," but may become prohibitive as classes scale up significantly.
- **Why unresolved:** Experiments used subsets of classes (e.g., 50 classes for ImageNet), leaving scalability to full-scale taxonomies unverified.
- **What evidence would resolve it:** Runtime and memory usage analysis on datasets with $|Y| > 1000$ compared to current benchmarks.

## Limitations
- Method's effectiveness depends critically on quality and coverage of condition sets; too accurate or too noisy base models prevent meaningful constraint learning
- Assumes hierarchical violations manifest as consistent patterns across granularity levels, which may not hold for all taxonomies
- LTN integration assumes recovered rules accurately reflect ground-truth structure rather than spurious correlations

## Confidence
- Error Detection Claims (High): Strong evidence from submodular optimization framework and experimental results on three datasets
- Constraint Recovery Claims (Medium): Successfully recovers many constraints but evaluation relies on known ground-truth hierarchies with performance degrading under noise
- LTN Integration Claims (Medium): Promising reduction in inconsistency rates but modest improvement in downstream task performance

## Next Checks
1. Conduct ablation study removing secondary model conditions from C_g and measure drop in constraint recovery F1 to verify importance of cross-granularity signals
2. Replicate noise ratio experiment on non-visual data (e.g., text classification) to confirm graceful degradation generalizes beyond presented domains
3. Perform human evaluation of learned rules to assess alignment with domain expert understanding of hierarchical relationships