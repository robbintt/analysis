---
ver: rpa2
title: 'Family Matters: Language Transfer and Merging for Adapting Small LLMs to Faroese'
arxiv_id: '2510.00810'
source_url: https://arxiv.org/abs/2510.00810
tags:
- language
- faroese
- languages
- lora
- full
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting small language models
  to Faroese, a low-resource North Germanic language, by leveraging transfer from
  related Scandinavian languages. The authors investigate the effects of continued
  pre-training on individual or merged Scandinavian languages before fine-tuning on
  Faroese, comparing full fine-tuning and LoRA (parameter-efficient fine-tuning).
---

# Family Matters: Language Transfer and Merging for Adapting Small LLMs to Faroese

## Quick Facts
- **arXiv ID**: 2510.00810
- **Source URL**: https://arxiv.org/abs/2510.00810
- **Reference count**: 22
- **Primary result**: Transfer from related Scandinavian languages improves Faroese LLM adaptation, with Icelandic excelling in linguistic accuracy and mainland Scandinavian languages enhancing comprehension; full fine-tuning generally outperforms LoRA on comprehension tasks.

## Executive Summary
This paper tackles the challenge of adapting small language models to Faroese, a low-resource North Germanic language, by leveraging transfer from related Scandinavian languages. The authors systematically investigate continued pre-training on individual or merged Scandinavian languages before fine-tuning on Faroese, comparing full fine-tuning and LoRA (parameter-efficient fine-tuning) approaches. They introduce two new minimal-pair evaluation benchmarks (FoBLiMP for linguistic acceptability and FoBCoMP for text comprehension) and conduct human evaluation by Faroese linguists. The results demonstrate that transfer from related languages is crucial, with Icelandic providing the best linguistic accuracy and mainland Scandinavian languages enhancing text comprehension. Model merging shows promise but with inconsistent benefits, while full fine-tuning generally outperforms LoRA on comprehension tasks and downstream summarization.

## Method Summary
The study adapts SmolLM2 models (135M and 360M parameters) to Faroese through a two-phase approach: continued pre-training (CPT) on Scandinavian languages followed by Faroese fine-tuning. For transfer, models undergo 1 epoch of CPT on individual languages (Icelandic, Danish, Swedish, Norwegian-Bokmål, Norwegian-Nynorsk) or merged combinations, then 5 epochs of fine-tuning on Faroese. The experiments compare full fine-tuning and LoRA (rank 256, alpha 512) parameter-efficient fine-tuning. Model merging is performed using TIES with density 0.5. Evaluation uses perplexity on Fineweb-2 validation data and accuracy on newly created minimal-pair benchmarks FoBLiMP and FoBCoMP, supplemented by human evaluation for linguistic accuracy.

## Key Results
- Transfer from related Scandinavian languages is crucial, with Icelandic providing the best linguistic accuracy and mainland Scandinavian languages enhancing text comprehension
- Full fine-tuning generally outperformed LoRA on comprehension tasks and downstream summarization, while LoRA showed advantages in linguistic acceptability
- Model merging showed promise, especially when combining Icelandic and Danish, but benefits were inconsistent across different merging strategies
- The 360M parameter model outperformed the 135M model across all tasks, confirming the importance of model capacity for comprehension

## Why This Works (Mechanism)
The success of cross-linguistic transfer stems from the shared typological features among North Germanic languages, particularly their rich morphological systems and syntactic structures. Icelandic's conservative grammar (preserving case systems and verb conjugations) provides strong linguistic supervision, while Danish's extensive corpus and mainland Scandinavian languages' syntactic similarities enhance text comprehension capabilities. The merging strategy allows the model to learn complementary features from multiple source languages simultaneously, though interference effects can occur when combining languages with conflicting grammatical patterns.

## Foundational Learning
- **Continued Pre-training (CPT)**: Further training on domain-specific or related language data before task-specific fine-tuning; needed to adapt general-purpose models to language-specific features; quick check: monitor perplexity drop on target language validation set
- **Parameter-Efficient Fine-Tuning (LoRA)**: Technique that freezes base model weights and trains small adapter matrices; needed to reduce computational cost while maintaining performance; quick check: compare performance degradation vs full fine-tuning
- **Model Merging**: Combining checkpoints from different training runs to leverage complementary strengths; needed to integrate benefits from multiple source languages; quick check: verify merged model perplexity doesn't exceed worst individual component
- **Minimal-Pair Evaluation**: Testing model sensitivity to linguistic acceptability through contrastive examples; needed for precise measurement of grammatical understanding; quick check: ensure benchmark covers diverse linguistic phenomena
- **Cross-Linguistic Transfer**: Leveraging similarities between related languages for adaptation; needed when target language resources are limited; quick check: validate transfer benefits through ablation studies
- **Morphological Complexity**: Rich inflectional systems requiring substantial grammatical knowledge; needed to understand why certain transfer languages work better; quick check: analyze model performance on case marking and agreement tasks

## Architecture Onboarding

**Component Map**
SmolLM2 Base Model -> Continued Pre-training (1 epoch) -> Fine-tuning (5 epochs) -> Evaluation (FoBLiMP/FoBCoMP/Human)

**Critical Path**
Base Model Selection → Continued Pre-training → Fine-tuning → Evaluation → Analysis

**Design Tradeoffs**
- Full Fine-Tuning vs LoRA: Full FT provides better comprehension but requires more parameters; LoRA is parameter-efficient but may underperform on complex tasks
- Individual vs Merged Transfer: Individual transfers are stable but limited; merging can combine strengths but introduces interference
- Model Size: Larger models (360M) perform better but increase computational costs

**Failure Signatures**
- Exploded perplexity in merged models (especially LoRA merges)
- Language mixing or gibberish in generated text
- Poor LoRA performance on comprehension tasks
- Inconsistent merging benefits across different language combinations

**First 3 Experiments to Run**
1. Transfer CPT on Icelandic only, then fine-tune on Faroese with full fine-tuning
2. Transfer CPT on Danish only, then fine-tune on Faroese with LoRA
3. Merge Icelandic and Danish checkpoints, then fine-tune on Faroese with full fine-tuning

## Open Questions the Paper Calls Out
### Open Question 1
How do the comparative dynamics of full fine-tuning versus LoRA, and the efficacy of model merging, change when adapting larger LLMs or applying instruction tuning? The study was constrained to small parameter sizes (135M and 360M) and continued pre-training only; the interaction of these methods with the emergent capabilities of larger models or instruction-following datasets remains unknown.

### Open Question 2
Do the specific transfer patterns identified for Faroese—where Icelandic aids linguistic accuracy and Danish aids comprehension—generalize to low-resource languages in other language families? Faroese has a unique typological profile (sharing morphology with one neighbor but syntax with others); it is unclear if this "split" transfer learning benefit is a universal phenomenon or specific to the North Germanic context.

### Open Question 3
Can adaptive or data-driven merging strategies improve the consistency of combining related languages, particularly for LoRA-based adaptations? The study used static merging approaches which resulted in unstable perplexities for LoRA merges; it is unknown if calculating merge weights dynamically based on data characteristics would resolve this interference.

## Limitations
- Evaluation focuses on linguistic acceptability and comprehension rather than fluency or generation quality
- Model merging experiments limited to two language combinations and one merging technique (TIES)
- Poor performance of 135M model on comprehension tasks may reflect model capacity limitations rather than fundamental constraints
- Computational costs and training efficiency metrics not reported

## Confidence
- **High Confidence**: Transfer from related languages improves Faroese adaptation; Icelandic excels in linguistic tasks while mainland Scandinavian languages enhance comprehension
- **Medium Confidence**: Icelandic+Danish merging combinations outperform individual languages; inconsistent benefits across different LoRA merging strategies
- **Low Confidence**: LoRA being "better suited for linguistic acceptability" versus full fine-tuning for "text comprehension" based on single dataset comparisons

## Next Checks
1. Extend merging experiments to include additional Scandinavian language combinations (e.g., Swedish+Norwegian, three-way merges) and alternative merging techniques
2. Evaluate generation quality and fluency through human evaluation on open-ended generation tasks (story completion, dialogue)
3. Test transfer from non-Scandinavian languages such as German or Dutch to establish generalizability beyond North Germanic languages