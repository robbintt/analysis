---
ver: rpa2
title: Automatic Joint Structured Pruning and Quantization for Efficient Neural Network
  Training and Compression
arxiv_id: '2502.16638'
source_url: https://arxiv.org/abs/2502.16638
tags:
- size
- bert
- layer
- features
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GETA is a framework that automatically performs joint structured
  pruning and quantization-aware training for deep neural networks. It addresses engineering
  difficulties, black-box optimization, and insufficient architecture generalization
  in existing joint pruning and quantization methods.
---

# Automatic Joint Structured Pruning and Quantization for Efficient Neural Network Training and Compression

## Quick Facts
- arXiv ID: 2502.16638
- Source URL: https://arxiv.org/abs/2502.16638
- Reference count: 40
- Primary result: GETA achieves competitive or superior performance compared to state-of-the-art methods in terms of accuracy and bit operations for joint structured pruning and quantization

## Executive Summary
GETA is a framework that automatically performs joint structured pruning and quantization-aware training for deep neural networks. It addresses engineering difficulties, black-box optimization, and insufficient architecture generalization in existing joint pruning and quantization methods. GETA introduces three key innovations: a quantization-aware dependency graph (QADG) for constructing pruning search spaces in quantization-aware DNNs, a partially projected stochastic gradient method (PPSG) for satisfying layerwise bit constraints, and a joint learning strategy for managing conflicts between pruning and quantization. Experiments on CNNs (ResNet, VGG) and transformer architectures (BERT, ViT, Phi2) demonstrate that GETA achieves competitive or superior performance compared to state-of-the-art methods in terms of accuracy and bit operations.

## Method Summary
GETA uses a four-stage QASSO (Quantization-Aware Structured Sparsity Optimization) optimizer to jointly train and compress neural networks. The framework first constructs a quantization-aware dependency graph (QADG) to identify pruning groups that maintain valid tensor shapes despite quantization layers. During training, it uses a partially projected stochastic gradient (PPSG) method to satisfy bit-width constraints by projecting only the quantization step size d, avoiding instability from exponential terms in gradients of other quantization parameters. The joint learning strategy uses angle-based rules to select pruning rates and step sizes that guarantee descent direction despite conflicts between pruning and quantization objectives.

## Key Results
- GETA achieves higher accuracy with fewer BOPs compared to state-of-the-art methods on ResNet, VGG, BERT, ViT, and Phi2 architectures
- ResNet56-CIFAR10: 60% sparsity at 32-bit weight quantization (10% BOPs reduction) and 8-bit quantization (40% BOPs reduction) with accuracy loss <0.5%
- Phi2-2.7B: 85% BOPs reduction at 8-bit quantization with F1 score loss of only 1.3 points on SQuAD 2.0
- GETA maintains competitive performance even at high sparsity levels (>60%) where most methods fail

## Why This Works (Mechanism)

### Mechanism 1: Quantization-Aware Dependency Graph (QADG) for Architecture-Agnostic Search Space Construction
QADG enables automatic construction of pruning search spaces for any quantization-aware DNN by consolidating complex quantization-induced graph structures into single entities. The framework traces the computational graph and identifies two patterns unique to quantization layers: attached branches from weight quantization and inserted branches from activation quantization. QADG merges each branch into a single vertex, eliminating weight-sharing and shape-ambiguous operators, then applies standard dependency graph analysis to derive minimally removable structures.

### Mechanism 2: Partially Projected Stochastic Gradient (PPSG) for Stable Bit-Width Constraint Satisfaction
PPSG projects only the quantization step size d (not qm or t) onto feasible intervals to guarantee bit-width constraints while avoiding gradient instability from exponential terms in the gradient. The bit width b depends on (qm)^t / d. Gradients w.r.t. qm and t contain exponential terms (qm)^t and (qm)^(t-1). Abrupt projection of these variables can cause large value changes → gradient explosion/collapse. The gradient of d lacks such exponentials. PPSG projects only d onto the feasible range [d_min, d_max] computed from current (qm, t) and target bit-width bounds.

### Mechanism 3: Angle-Based Forget Rate (γ) and Step Size (d) Selection for Descent-Guaranteed Joint Updates
Computing γ and d based on angles between gradient direction and quantized/clipped value directions guarantees the joint update is a descent direction for the loss, resolving pruning-quantization conflicts. For redundant parameter groups GR, the update is: x ← x - α∇f - γx_Q, where x_Q is the quantized value. This decomposes into clipped values and residual terms. The angle θγ between −∇f and −clipped_values, and angle θd between −∇f and −residual, determine γ and d selection. When these angles favor descent, large γ/d are used; when they oppose descent, γ/d are bounded to maintain descent.

## Foundational Learning

- **Concept: Quantization-Aware Training (QAT) with Learnable Bit Widths**
  - Why needed here: GETA uses parameterized quantization layers where bit width is computed from learned (d, q_m, t) rather than fixed. Understanding how straight-through estimators (STE) enable gradient flow through rounding is essential.
  - Quick check question: Given the quantization x_Q = d · ⌊|x|^t / d⌉ and bit width b = log₂((q_m)^t / d + 1) + 1, how would increasing d affect the bit width while holding (q_m, t) constant?

- **Concept: Structured Pruning via Dependency Graphs**
  - Why needed here: GETA extends dependency graph analysis to quantization-aware networks. The core idea is that certain parameters (e.g., consecutive conv layers' channels) must be removed together to maintain valid tensor shapes.
  - Quick check question: In a ResNet residual block with Conv→BN→ReLU→Conv, what parameter groups must be pruned together to maintain valid shapes for both the main path and skip connection?

- **Concept: Projected Gradient Descent for Constrained Optimization**
  - Why needed here: PPSG is a variant where projection is partial (only on d). Understanding why standard projection fails (no closed-form for bit-width constraint) motivates the partial approach.
  - Quick check question: For constraint b ∈ [b_l, b_u] where b = f(d, q_m, t), why is there no closed-form projection operator, and what alternative approaches exist (penalty methods, ADMM)?

## Architecture Onboarding

- **Component map:** Input Model → Parameterized Quantization Layer Insertion → QADG Construction → Four-Stage QASSO Training (Warmup → Projection → Joint → Cooldown) → construct_subnet() → Compressed Model

- **Critical path:**
  1. Ensure QADG correctly identifies attached/inserted branches (debug by visualizing merged vertices)
  2. Monitor bit-width convergence during projection stage—should reach [b_l, b_u] smoothly
  3. Verify γ > 0 during joint stage (pruning active) and descent property holds (loss decreasing)

- **Design tradeoffs:**
  - PPSG on d vs. qm or t: Projecting d is stable but requires progressive bit-width reduction schedule; projecting qm/t would be more direct but causes instability
  - Four-stage vs. end-to-end: Stages add complexity but ablation shows each contributes; removing joint or cooldown stages causes significant accuracy drops (3-5%)
  - Structured vs. unstructured pruning: Structured pruning is hardware-friendly but achieves lower sparsity (60% vs. 80% for ResNet56-CIFAR10 under joint setup)

- **Failure signatures:**
  - Bit-width divergence: If projection stage doesn't converge to [b_l, b_u], check that b_r (reduction factor) and B (periods) satisfy b_r · B ≤ b_u - b_l
  - Gradient explosion during projection: Verify PPSG is projecting only d, not qm or t
  - QADG shape errors: Weight-sharing or ambiguous operators not merged correctly; inspect trace graph for residual detached branches

- **First 3 experiments:**
  1. Validation on ResNet20-CIFAR10 (reproduce Tab. 2): Only weight quantization, target ~4.5% relative BOPs. Compare against baseline to verify framework correctness.
  2. Ablation of projection schedule: Vary B (projection periods) and b_r (bit-width reduction factor) to understand sensitivity of bit-width convergence.
  3. Architecture generalization test: Apply to a transformer (e.g., ViT-Small on ImageNet per Tab. 6) to verify QADG handles attention mechanisms correctly without manual dependency specification.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can GETA be adapted to leverage specialized hardware architectures (e.g., FPGAs, ASICs) for real-world deployment?
- Basis in paper: [explicit] The Conclusion states, "In the future, it will be interesting to explore adapting GETA for specialized hardware to improve real-world deployment on different platforms."
- Why unresolved: The current framework focuses on algorithmic optimization and software-level compression metrics (BOPs), but does not address hardware-specific constraints or compiler integration.
- What evidence would resolve it: Successful deployment of GETA-compressed models on hardware accelerators demonstrating latency and energy efficiency improvements over standard CPU/GPU baselines.

### Open Question 2
- Question: Can the quantization parameters q_m (maximum value) and t (exponent) be stabilized for direct projection during training?
- Basis in paper: [inferred] Section 5.1 states that projecting q_m or t causes training instability (gradient explosion/vanishing) due to exponential dependencies, so PPSG only projects d.
- Why unresolved: Restricting projection to d constrains the optimization search space; it is unclear if the instability is inherent to the parameters or a limitation of the current projection method.
- What evidence would resolve it: A modified projection operator or parameterization that allows q_m and t to be updated alongside d without causing training collapse.

### Open Question 3
- Question: Can the QASSO optimizer be extended to support ultra-large language models where full gradient computation is infeasible?
- Basis in paper: [inferred] Section 6.2 notes that Phi2-2.7B was selected specifically to ensure "computational feasibility on a single A100 GPU" because GETA leverages "full gradient information."
- Why unresolved: The current white-box optimization approach requires backpropagation through the full model, which creates a memory bottleneck for models significantly larger than 3B parameters.
- What evidence would resolve it: An extension of the framework that uses approximate gradients or memory-efficient fine-tuning (e.g., gradient checkpointing/LoRA) to compress models in the 7B–70B parameter range.

## Limitations

- Theoretical grounding of angle-based descent guarantee in high-dimensional stochastic settings remains uncertain, with no analysis of gradient variance effects
- QADG's handling of operators with dynamic shapes or control flow remains untested, limiting architecture generality claims
- PPSG's stability depends on the progressive bit-width reduction schedule; no analysis shows what happens if this schedule is violated or if initial quantization parameters are poorly chosen

## Confidence

- **High**: QADG effectively consolidates quantization branches into single vertices for dependency analysis (supported by Algorithm 1 and trace graph examples)
- **Medium**: PPSG with partial projection on d prevents gradient instability (supported by numerical testing in Section 5.1 but lacks theoretical bounds on convergence rate)
- **Medium**: Four-stage training strategy consistently improves accuracy over end-to-end approaches (supported by ablation but stage-specific contributions not isolated)

## Next Checks

1. **Convergence sensitivity**: Systematically vary projection periods B and bit-width reduction factor b_r to identify breaking points where PPSG fails to maintain feasible bit-widths
2. **Architecture stress test**: Apply GETA to a network with dynamic control flow (e.g., transformer with adaptive computation steps) to test QADG's dependency resolution limits
3. **Stochastic gradient validation**: Run controlled experiments comparing deterministic vs. stochastic gradients in the joint stage to measure degradation of the angle-based descent guarantee