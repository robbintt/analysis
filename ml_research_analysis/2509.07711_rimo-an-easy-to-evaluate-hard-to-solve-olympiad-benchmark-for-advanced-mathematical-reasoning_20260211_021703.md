---
ver: rpa2
title: 'RIMO: An Easy-to-Evaluate, Hard-to-Solve Olympiad Benchmark for Advanced Mathematical
  Reasoning'
arxiv_id: '2509.07711'
source_url: https://arxiv.org/abs/2509.07711
tags:
- answer
- rimo
- reasoning
- problems
- olympiad
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RIMO introduces a two-track benchmark to address the challenge
  of reliably evaluating Olympiad-level mathematical reasoning in large language models.
  The first track, RIMO-N, remakes 335 IMO problems to have unique integer answers,
  enabling deterministic grading, while the second track, RIMO-P, uses decomposed
  proof problems to assess step-by-step reasoning.
---

# RIMO: An Easy-to-Evaluate, Hard-to-Solve Olympiad Benchmark for Advanced Mathematical Reasoning

## Quick Facts
- arXiv ID: 2509.07711
- Source URL: https://arxiv.org/abs/2509.07711
- Reference count: 12
- Primary result: Benchmark reveals significant gap between current LLM capabilities and Olympiad-level reasoning

## Executive Summary
RIMO introduces a two-track benchmark designed to reliably evaluate Olympiad-level mathematical reasoning in large language models. The benchmark addresses the challenge of deterministic grading in complex mathematical problems by offering RIMO-N with single-integer answers and RIMO-P with decomposed proof problems. Testing ten frontier models reveals a significant performance drop compared to established benchmarks, demonstrating that current LLMs struggle with true Olympiad-level reasoning. The results show that explicit reasoning optimization and problem formulation are more critical than model scale or recency for advancing mathematical reasoning capabilities.

## Method Summary
The benchmark consists of two tracks: RIMO-N with 335 remade IMO problems featuring unique integer answers for deterministic grading, and RIMO-P with decomposed proof problems to assess step-by-step reasoning. The evaluation tests ten frontier models on RIMO-N, revealing performance patterns that highlight the gap between current LLM capabilities and advanced mathematical reasoning. The methodology emphasizes transparent reporting and clear performance metrics across multiple dimensions.

## Key Results
- Current models show significant performance drop on RIMO compared to established benchmarks like GSM8K and MATH
- Even best-performing models achieve only moderate accuracy on Olympiad-level problems
- Explicit reasoning optimization and problem formulation are more critical than scale or recency for mathematical reasoning

## Why This Works (Mechanism)
The benchmark works by creating a reliable evaluation framework for complex mathematical reasoning through two complementary approaches: deterministic grading via single-integer answers and step-by-step assessment via decomposed proofs. This dual-track design addresses practical constraints in benchmark evaluation while maintaining the rigor of Olympiad-level problem solving.

## Foundational Learning
- **Mathematical Olympiad problem structure**: Why needed - to create authentic yet evaluable problems; Quick check - verify problems match original IMO difficulty and style
- **Deterministic grading systems**: Why needed - to ensure reliable automated evaluation; Quick check - test grading consistency across multiple runs
- **Problem decomposition techniques**: Why needed - to assess reasoning steps in complex proofs; Quick check - validate decomposition preserves problem integrity
- **Benchmark design principles**: Why needed - to create meaningful comparisons across models; Quick check - ensure problems are diverse and representative
- **LLM mathematical reasoning capabilities**: Why needed - to understand current limitations; Quick check - compare performance against established benchmarks
- **Evaluation metric design**: Why needed - to measure progress meaningfully; Quick check - validate metrics correlate with human judgment

## Architecture Onboarding

Component map:
- Problem Collection -> Problem Remaking -> RIMO-N Track Creation
- Problem Collection -> Proof Decomposition -> RIMO-P Track Creation
- Both tracks -> Model Evaluation -> Performance Analysis

Critical path:
Problem collection and remaking for RIMO-N must be completed before model evaluation can begin, as this track forms the core assessment framework.

Design tradeoffs:
Single-integer answers enable deterministic grading but may oversimplify complex reasoning; proof decomposition provides detailed assessment but increases evaluation complexity.

Failure signatures:
Poor performance may indicate either fundamental reasoning limitations or issues with problem formulation rather than model capabilities.

First experiments:
1. Run small sample of problems through multiple models to validate benchmark design
2. Test grading system consistency across different model outputs
3. Evaluate problem difficulty calibration against human expert performance

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to 10 specific frontier models, potentially missing broader performance patterns
- Focus on IMO problems may limit generalizability to other mathematical domains
- Does not explore potential biases from single-integer answer format
- Missing analysis of different decomposition strategy impacts

## Confidence
- High confidence in benchmark design and implementation
- High confidence in relative performance rankings across models
- Medium confidence in interpretation of why certain models perform better
- Medium confidence in generalizability to broader mathematical reasoning tasks

## Next Checks
1. Evaluate broader range of models with different architectural approaches to validate performance patterns
2. Conduct cross-domain validation testing same models on other mathematical reasoning benchmarks
3. Implement ablation studies to quantify impact of different problem decomposition strategies on proof track performance