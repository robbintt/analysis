---
ver: rpa2
title: Teaching a Language Model to Speak the Language of Tools
arxiv_id: '2506.23394'
source_url: https://arxiv.org/abs/2506.23394
tags:
- language
- tool
- function-calling
- function
- tools
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The research addresses the challenge of enabling reliable tool
  use in non-English language models, particularly for Bulgarian. The core method
  involves fine-tuning BgGPT models (2.6B, 9B, 27B parameters) on a novel bilingual
  dataset of 10,035 function-calling examples using parameter-efficient LoRA techniques.
---

# Teaching a Language Model to Speak the Language of Tools

## Quick Facts
- **arXiv ID:** 2506.23394
- **Source URL:** https://arxiv.org/abs/2506.23394
- **Reference count:** 0
- **Primary result:** Fine-tuning BgGPT models (2.6B, 9B, 27B) on a bilingual function-calling dataset using LoRA improves function-calling accuracy by up to 28.75% while preserving Bulgarian language understanding.

## Executive Summary
This research addresses the challenge of enabling reliable tool use in non-English language models, particularly for Bulgarian. The core method involves fine-tuning BgGPT models (2.6B, 9B, 27B parameters) on a novel bilingual dataset of 10,035 function-calling examples using parameter-efficient LoRA techniques. Results show up to 28.75% improvement in function-calling accuracy (Tucan-2.6B: 78.75% vs 50.00% base; Tucan-9B: 86.67% vs 78.33% base; Tucan-27B: 87.50% vs 86.67% base) while preserving core language understanding on Bulgarian benchmarks. The TUCAN models produce clean, parseable function calls suitable for production deployment, contrasting with the verbose outputs of base models. All models, datasets, and evaluation frameworks are released as open-source contributions.

## Method Summary
The study fine-tunes BgGPT-Gemma-2 models (2.6B, 9B, 27B) using LoRA with parameter-efficient adaptation. Models are trained on a bilingual dataset (10,035 conversations) with English function definitions and Bulgarian conversations. LoRA rank/alpha settings: 16/16 for 2.6B, 32/32 for larger models. Training uses 4-bit quantization, cosine learning rate scheduling with 0.1 warmup, adamw_8bit optimizer (0.01 weight decay), and eager attention for Gemma-2. Models are evaluated on a custom Tucan-Eval framework (120 test cases) and knowledge retention on Bulgarian benchmarks.

## Key Results
- Function-calling accuracy improved by 28.75pp (2.6B), 8.34pp (9B), and 0.83pp (27B) over base models
- Tucan models produce clean, parseable function calls vs. verbose outputs from base models
- No catastrophic forgetting: knowledge retention within 0.04 points on HellaSwagBG, WinograndeBG, ARC-Easy-BG, ARC-Challenge-BG
- Smallest model (2.6B) shows greatest relative improvement, suggesting capability injection vs. amplification effects

## Why This Works (Mechanism)

### Mechanism 1: Parameter-efficient LoRA adaptation
- Claim: LoRA can add function-calling capabilities without catastrophic forgetting
- Mechanism: Updates only 0.79-1.2% of parameters in low-rank subspace, constraining adaptation to preserve distributed linguistic knowledge
- Evidence: Tucan models show maximum deviation of 0.0382 points on HellaSwagBG; Chen and Chen 2024 shows LoRA effectiveness with 0.28% tuned parameters
- Break condition: If base model lacks foundational reasoning capability, LoRA alone may be insufficient

### Mechanism 2: Bilingual structured training data
- Claim: Bilingual dataset with XML-style tags teaches tool-use context discrimination
- Mechanism: Combines explicit behavioral signals (72.57% tool calls, 16.54% rejections, 67.69% clarifications) with consistent formatting
- Evidence: Tucan-2.6B improved from 0% to 65% on "Function Call Required" and 0% to 80% on "Multiple Functions Selection"
- Break condition: Generalization may degrade if target domain has different conversational patterns

### Mechanism 3: Capability injection for smaller models
- Claim: Smaller models benefit disproportionately from specialized training
- Mechanism: Fine-tuning appears to inject missing reasoning patterns into smaller models while calibrating existing capabilities in larger models
- Evidence: Relative improvement decreases with size (28.75% → 8.34% → 0.83%); compressed scaling in Tucan (8.75pp range vs. 36.67pp in base)
- Break condition: This pattern is specific to BgGPT/Gemma-2 architecture

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**:
  - Why needed here: Core adaptation technique for function-calling capability injection
  - Quick check question: Can you explain why higher LoRA rank caused instability in the 2.6B model but worked well for 27B?

- **Catastrophic Forgetting**:
  - Why needed here: Central claim depends on preserving language understanding while adding tool-use
  - Quick check question: What evaluation strategy would you use to detect catastrophic forgetting in a domain-adapted model?

- **Function-Calling Evaluation Paradigms**:
  - Why needed here: Paper introduces custom framework with 6 scenario types and 5 error categories
  - Quick check question: Why is "NO_CALL_WHEN_EXPECTED" a more informative error than simply tracking accuracy?

## Architecture Onboarding

- **Component map**: BgGPT-Gemma-2 models (2.6B/9B/27B) → 4-bit quantization → LoRA adapters → Tucan-BG-v1.0 dataset (XML-style tags) → Tucan-Eval framework → Structured prompt template

- **Critical path**: Base model → 4-bit quantization → LoRA adapter loading → Structured prompt formatting → Function-calling inference → Response parsing via tool_call blocks

- **Design tradeoffs**:
  - Rank selection: Higher rank = more capacity but instability risk for smaller models
  - Dataset language split: English functions (86.7%) vs. Bulgarian conversations—optimizes for deployment but may limit code-switching
  - 4-bit quantization: Enables accessible deployment but may introduce precision-related degradation

- **Failure signatures**:
  - NO_CALL_WHEN_EXPECTED: Model generates conversational text instead of function calls
  - UNEXPECTED_CALL: Model invokes tools for general knowledge queries
  - WRONG_PARAMETERS: Correct function selection but incorrect arguments
  - MALFORMED_JSON: None observed, indicates format learning failure

- **First 3 experiments**:
  1. Baseline reproduction: Load BgGPT-2.6B-IT-v1.0, run Tucan-Eval, verify ~50% accuracy with verbose outputs
  2. LoRA application: Load Tucan-2.6B-v1.0-LoRA, run same evaluation, confirm ~78.75% accuracy with clean outputs
  3. Knowledge retention: Run lm-evaluation-harness-bg on both models for HellaSwagBG/WinograndeBG, verify <0.04 point deviation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does specialized fine-tuning compare to complex prompt engineering or inherently multilingual tool-use models for Bulgarian function-calling?
- Basis: Section 6.7 calls for comparison against alternative approaches including prompt engineering and multilingual models
- Why unresolved: Study only compared against BgGPT base models
- What evidence: Head-to-head benchmarking against GPT-4o or BgGPT with expert-crafted chain-of-thought prompts

### Open Question 2
- Question: Can the methodology generalize to languages with distinct morphological structures or scripts outside Bulgarian?
- Basis: Abstract claims methodology enables robust tool use in "any target language" but validation is restricted to Bulgarian
- Why unresolved: "Case study" approach confirms efficacy for one language but universality unproven
- What evidence: Successful replication for non-Indo-European or low-resource language using released framework

### Open Question 3
- Question: What mechanisms can reduce WRONG_PARAMETERS errors in smaller models?
- Basis: Section 6.3 identifies "WRONG_PARAMETERS" as most common error among successful TUCAN models
- Why unresolved: Paper quantifies error rates but doesn't investigate specific causes
- What evidence: Ablation studies varying parameter complexity or data augmentation targeting argument extraction

### Open Question 4
- Question: Do performance gains hold in open-ended, multi-turn production environments?
- Basis: Section 6.7 notes dataset "represents controlled laboratory conditions" and calls for "larger-scale evaluation protocols"
- Why unresolved: Fixed evaluation set may not capture real-world user behavior variability
- What evidence: Deployment in live environment with human-in-the-loop evaluation or dynamic tool definition testing

## Limitations
- Synthetic dataset generation introduces potential bias with limited transparency on prompt engineering
- 0.83% absolute improvement in 27B model suggests diminishing returns for larger models
- Bilingual approach (English functions, Bulgarian conversations) may not generalize to code-switched scenarios
- 4-bit quantization requirement introduces potential precision degradation not fully characterized

## Confidence

*High confidence:*
- LoRA fine-tuning effectively adds function-calling capabilities without catastrophic forgetting
- TUCAN models produce significantly cleaner, parseable function calls
- 2.6B model shows substantial relative improvement (28.75pp)

*Medium confidence:*
- Scaling pattern of diminishing relative improvements generalizes beyond BgGPT/Gemma-2
- Bilingual training with English functions adequately prepares for real-world deployment
- Tucan-Eval framework comprehensively captures function-calling competency

*Low confidence:*
- Synthetic dataset produces representative real-world tool-use patterns
- 10,035 conversation dataset size is optimal (could be over/under-fitting)
- XML-style tag format will remain optimal as standards evolve

## Next Checks
1. **Cross-linguistic generalization test**: Evaluate TUCAN models on function-calling tasks in language pairs beyond Bulgarian-English (e.g., Spanish, Romanian) to assess bilingual training approach's generalizability.

2. **Real-world deployment pilot**: Deploy TUCAN models in controlled production environment with actual API tool integration to measure live function-calling performance and identify distribution shifts.

3. **Architecture transfer validation**: Apply identical fine-tuning methodology to non-Gemma architecture (e.g., Llama, Mistral) to determine whether observed scaling patterns are architecture-specific.