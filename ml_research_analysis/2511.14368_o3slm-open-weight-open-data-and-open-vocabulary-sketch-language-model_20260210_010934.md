---
ver: rpa2
title: 'O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model'
arxiv_id: '2511.14368'
source_url: https://arxiv.org/abs/2511.14368
tags:
- sketch
- sketches
- image
- object
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of enabling large vision-language
  models to understand and reason with hand-drawn sketches, which are abstract and
  variable visual inputs that current models struggle with. The authors propose a
  large-scale dataset called SketchVCL, containing image-sketch-instruction triplets,
  and train a new model, O3SLM, on this dataset.
---

# O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model

## Quick Facts
- **arXiv ID**: 2511.14368
- **Source URL**: https://arxiv.org/abs/2511.14368
- **Reference count**: 22
- **Primary result**: Achieves state-of-the-art performance on sketch-based object localization, counting, image retrieval, and VQA compared to open-weight LVLMs

## Executive Summary
O3SLM addresses the challenge of enabling large vision-language models to understand and reason with hand-drawn sketches, which are abstract and variable visual inputs that current models struggle with. The authors propose a large-scale dataset called SketchVCL, containing image-sketch-instruction triplets, and train a new model on this dataset. The model is evaluated on sketch-based object localization, counting, image retrieval, and visual question answering, achieving state-of-the-art performance compared to existing open-weight LVLMs. Notably, O3SLM also generalizes to unseen sketch styles and can handle fine-grained queries combining sketches and text.

## Method Summary
O3SLM uses a two-stage training approach: large-scale sketch alignment pretraining followed by instruction tuning across four tasks. The model architecture consists of a CLIP ViT-L/336 backbone, a two-layer MLP multimodal connector, and a Vicuna v1.5 LLM backbone initialized from LLaVA-1.5 weights. Training uses LoRA with rank=64, learning rate=2e-5, and cosine decay schedule. The model processes concatenated sketch, image, and text tokens through self-attention, learning implicit cross-modal alignment. SketchVCL pretraining data combines Objects365 and OpenImages with synthetic sketches, while instruction tuning uses SketchMIX combining QuickDraw!, Sketchy, and other datasets.

## Key Results
- Achieves state-of-the-art performance across four sketch-language tasks compared to open-weight LVLMs
- Successfully generalizes to unseen sketch styles (TU-Berlin) with zero-shot performance
- Demonstrates emergent capability for fine-grained sketch-text queries combining sketches and text descriptions
- Shows superior performance on sketch-based image retrieval (SBIR) and object localization tasks

## Why This Works (Mechanism)

### Mechanism 1
Sequential two-stage training enables sketch-to-image alignment before task specialization. Stage I (pretraining on 600K image-sketch-instruction triplets) establishes three-way correspondence between sketches, natural images, and text. Stage II (instruction tuning on 215K samples) then specializes this aligned representation for four downstream tasks using task-specific prefixes. This sequential approach is critical because the domain gap between abstract sketches and natural images cannot be bridged by task tuning alone.

### Mechanism 2
Fine-tuning the multimodal connector (projector) is critical for sketch-image feature alignment. The two-layer MLP connector projects CLIP ViT-L/336 embeddings into the LLM's input space. When frozen, the connector preserves pretrained image-text alignment but cannot adapt to the abstract sketch modality. Fine-tuning enables the connector to learn a shared projection space for both natural images and sketches.

### Mechanism 3
Token concatenation with self-attention enables implicit cross-modal alignment without explicit fusion modules. Sketch tokens, image tokens, and text tokens are concatenated and passed through the LLM's self-attention layers. The LLM's capacity learns to align modalities implicitly through next-token prediction, rather than requiring explicit cross-attention or handcrafted feature fusion.

## Foundational Learning

- **Vision-Language Alignment**: Understanding how CLIP-based vision encoders map visual inputs to shared embedding spaces is prerequisite for O3SLM's three-way alignment (sketch-image-text). Quick check: Can you explain how a contrastive objective aligns image and text embeddings, and why CLIP struggles with abstract sketches despite strong performance on natural images?

- **Instruction Tuning for LVLMs**: Stage II relies on task-specific prefixes (COUNT, BBOX, VQA, SBIR) to disambiguate output formats. Understanding instruction tuning is necessary to interpret why this works. Quick check: How does instruction tuning differ from standard supervised fine-tuning, and why might task-specific prefixes improve output consistency without modifying the tokenizer?

- **Sketch Abstraction Levels**: The paper uses SketchMIX (QuickDraw!, Sketchy, SketchVCL-C) to cover low-to-high abstraction. Understanding this spectrum is essential for interpreting generalization results to held-out TU-Berlin. Quick check: QuickDraw! sketches are highly abstract (often incomplete outlines); Sketchy sketches are more detailed. How might abstraction level affect cross-modal retrieval accuracy?

## Architecture Onboarding

- **Component map**: CLIP ViT-L/336 (shared encoder for sketches and images) -> 2-layer MLP projector (projects visual tokens to LLM input space) -> Vicuna v1.5 LLM (reasoning backbone) -> Task-specific prefixes (COUNT, BBOX, VQA, SBIR)

- **Critical path**: Load LLaVA-1.5 weights → Generate SketchVCL pretraining data via SAM2 + Pix2Pix pipeline → Stage I pretraining (600K samples, sketch alignment) → Stage II instruction tuning (215K samples, multi-task) → Evaluate on held-out sketch styles (TU-Berlin)

- **Design tradeoffs**: Synthetic sketch generation vs. hand-drawn sketches (scale vs. realism), freezing vs. fine-tuning projector (preservation vs. adaptation), open-vocabulary pretraining vs. task-specific datasets (generalization vs. per-class coverage)

- **Failure signatures**: Low SBIR accuracy with frozen projector (sketch features not usable), good performance on Sketchy but poor on TU-Berlin (overfitting to training styles), high VQA but low detection accuracy (semantic vs. spatial grounding), hallucinated bounding boxes on crowded scenes (lacks NMS-style suppression)

- **First 3 experiments**: 1) Reproduce Stage I vs. Stage II ablation on SBIR Acc@1 (expect ~40% drop without pretraining), 2) Projector tuning sweep (frozen vs. LoRA vs. full tuning on detection), 3) Sketch style generalization test on TU-Berlin across all four tasks

## Open Questions the Paper Calls Out

**Open Question 1**: How can sketch-based object detection be improved in crowded scenes with highly overlapping objects? The model struggles with duplicate bounding boxes in crowded scenes and lacks non-maximal suppression capabilities.

**Open Question 2**: What mechanisms enable emergent fine-grained sketch-text understanding without explicit combined-query training? The capability emerges from multitask pretraining but the specific mechanisms are not analyzed.

**Open Question 3**: How does the domain gap between synthetic pretraining sketches and hand-drawn evaluation sketches affect model performance? The impact of synthetic-to-real sketch domain shift on alignment quality and downstream generalization is not measured.

## Limitations

- Struggles with crowded scenes containing highly overlapping objects, often generating duplicate bounding boxes without non-maximal suppression
- Generalization to truly novel sketch styles beyond controlled SketchMIX construction is not thoroughly validated
- Synthetic sketch generation pipeline may not fully capture cognitive and stylistic variations present in human hand-drawn sketches

## Confidence

**High Confidence**: Projector fine-tuning effectiveness (direct comparisons show ~10% absolute improvement in detection accuracy)

**Medium Confidence**: State-of-the-art claims across all four tasks (supported by comparisons but limited to open-weight models)

**Low Confidence**: Implicit cross-modal alignment sufficiency (lacks direct comparison with explicit cross-attention mechanisms)

## Next Checks

1. **Cross-style generalization stress test**: Evaluate O3SLM on sketches from multiple held-out datasets (TU-Berlin, QuickDraw! test set, and cultural variant dataset) across all four tasks. Measure performance degradation as a function of sketch abstraction level and stylistic distance from training data.

2. **Architecture ablation study**: Compare O3SLM's token concatenation approach against a cross-attention variant where sketch and image tokens interact through explicit cross-attention layers before concatenation with text. Evaluate on SBIR and detection tasks.

3. **Pretraining strategy comparison**: Implement an alternative pretraining regime using joint training of all 600K samples with contrastive objectives between sketch-image and image-text pairs. Compare final task performance and training efficiency to assess whether the sequential approach is optimal.