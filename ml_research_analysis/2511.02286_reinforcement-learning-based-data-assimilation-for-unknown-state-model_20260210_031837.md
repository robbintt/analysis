---
ver: rpa2
title: Reinforcement learning based data assimilation for unknown state model
arxiv_id: '2511.02286'
source_url: https://arxiv.org/abs/2511.02286
tags:
- state
- observation
- estimation
- methods
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of data assimilation (DA) for
  state estimation when the governing equations of the underlying dynamics are unknown.
  Traditional DA methods require explicit knowledge of the state transition model,
  which is often infeasible to obtain for complex systems.
---

# Reinforcement learning based data assimilation for unknown state model

## Quick Facts
- arXiv ID: 2511.02286
- Source URL: https://arxiv.org/abs/2511.02286
- Authors: Ziyi Wang; Lijian Jiang
- Reference count: 40
- Addresses data assimilation when governing equations are unknown using reinforcement learning

## Executive Summary
This paper addresses the fundamental challenge of data assimilation when the governing equations of a dynamical system are unknown. Traditional data assimilation methods require explicit knowledge of state transition models, which is often infeasible for complex systems. The authors propose a novel framework that integrates reinforcement learning with ensemble-based Bayesian filtering methods, enabling state estimation directly from noisy observations without requiring true state trajectories.

The key innovation lies in reformulating maximum likelihood estimation of surrogate state transition model parameters as a sequential decision-making problem modeled as a Markov decision process. By finding an optimal policy using reinforcement learning techniques, the framework learns the surrogate model directly from observations. The method includes two specific implementations (PPO-EnKF and PPO-PF) and demonstrates superior accuracy and robustness in high-dimensional settings compared to existing approaches.

## Method Summary
The proposed framework integrates reinforcement learning with ensemble-based Bayesian filtering methods to address data assimilation when state transition models are unknown. The method reformulates MLE of surrogate model parameters as a sequential decision-making problem modeled as an MDP. An optimal policy for this MDP is found using RL techniques, enabling direct learning of the surrogate model from noisy observations without requiring true state trajectories. Two specific implementations are developed: PPO-EnKF combines proximal policy optimization with ensemble Kalman filtering, while PPO-PF integrates PPO with particle filtering. The framework demonstrates superior accuracy and robustness in high-dimensional settings compared to existing methods through numerical examples on various dynamical systems.

## Key Results
- Demonstrates superior accuracy and robustness in high-dimensional settings compared to existing data assimilation methods
- Successfully learns surrogate state transition models directly from noisy observations without requiring true state trajectories
- Shows effective performance across various dynamical systems including linear and non-linear examples

## Why This Works (Mechanism)
The framework works by transforming the parameter estimation problem in data assimilation into a sequential decision-making problem. When traditional methods fail due to unknown governing equations, the RL approach learns the optimal mapping from observations to state estimates through interaction with the environment. The MDP formulation captures the temporal dependencies in sequential observations, while the policy learned through RL effectively approximates the unknown state transition dynamics. This allows the system to adapt to the true underlying dynamics without explicit mathematical models.

## Foundational Learning
- Markov Decision Process (MDP): Why needed - to model sequential decision-making in parameter estimation; Quick check - verify state transitions follow Markov property
- Proximal Policy Optimization (PPO): Why needed - stable RL algorithm for learning optimal policies; Quick check - monitor KL divergence during training
- Ensemble Kalman Filter (EnKF): Why needed - efficient Bayesian filtering for high-dimensional systems; Quick check - verify ensemble spread matches observation uncertainty
- Particle Filter (PF): Why needed - non-parametric Bayesian filtering without Gaussian assumptions; Quick check - ensure effective sample size remains sufficient
- Maximum Likelihood Estimation (MLE): Why needed - parameter estimation framework for surrogate models; Quick check - verify convergence of likelihood maximization

## Architecture Onboarding

Component Map:
Observations -> RL Policy -> Surrogate Model -> Ensemble Filter -> State Estimate

Critical Path:
Observations are fed into the RL policy, which outputs parameters for the surrogate state transition model. The ensemble filter uses this surrogate model along with observations to generate state estimates. The policy is trained to maximize the likelihood of observations given the state estimates.

Design Tradeoffs:
The framework trades computational complexity of RL training for the ability to work without explicit state transition models. While traditional methods require mathematical models but are computationally efficient, this approach requires significant training time but can handle systems where models are unavailable. The ensemble size in the filter represents another tradeoff between accuracy and computational cost.

Failure Signatures:
Poor performance may manifest as divergence of the ensemble, high variance in policy outputs, or systematic bias in state estimates. If the RL policy fails to learn meaningful patterns, the surrogate model will produce unrealistic state transitions. Insufficient exploration during RL training can lead to local optima and poor generalization.

First 3 Experiments:
1. Test on a simple linear dynamical system with known analytical solution to verify basic functionality
2. Evaluate performance under varying observation noise levels to assess robustness
3. Compare computational cost against traditional methods for systems of increasing dimension

## Open Questions the Paper Calls Out
None

## Limitations
- Performance heavily depends on observation quality and frequency
- Computational complexity of RL training may be prohibitive for very high-dimensional systems
- Robustness in scenarios with highly sparse or corrupted data is not thoroughly evaluated
- Limited testing on complex, real-world systems beyond specific dynamical examples

## Confidence
- Superior accuracy in high-dimensional settings: Medium
- Learning surrogate model from noisy observations without true trajectories: High
- Generalizability to complex real-world systems: Low

## Next Checks
1. Test the framework on a broader range of dynamical systems, including those with non-linear and chaotic behavior, to assess generalizability
2. Evaluate the method's performance under varying observation conditions, such as sparse, noisy, or irregularly sampled data, to understand its robustness
3. Conduct a scalability analysis to determine the computational feasibility of the approach for high-dimensional systems and compare it with traditional DA methods in terms of both accuracy and efficiency