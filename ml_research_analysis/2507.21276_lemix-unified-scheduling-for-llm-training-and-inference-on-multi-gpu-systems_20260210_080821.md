---
ver: rpa2
title: 'LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems'
arxiv_id: '2507.21276'
source_url: https://arxiv.org/abs/2507.21276
tags:
- node
- training
- inference
- task
- serving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces L EMIX, a unified system for scheduling LLM
  training and inference workloads on multi-GPU systems. The core idea is to co-locate
  and dynamically manage concurrent training and inference tasks using offline profiling,
  execution prediction, and runtime scheduling to maximize resource utilization while
  meeting response time SLOs.
---

# LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems

## Quick Facts
- **arXiv ID:** 2507.21276
- **Source URL:** https://arxiv.org/abs/2507.21276
- **Reference count:** 40
- **Primary result:** Unified scheduling improves throughput by up to 3.53× and SLO attainment by up to 2.12× versus separate setups

## Executive Summary
LeMix introduces a unified scheduling system for co-locating LLM training and inference on multi-GPU nodes. By leveraging offline profiling, execution prediction, and runtime scheduling, LeMix dynamically manages concurrent workloads to maximize GPU utilization while meeting strict response time SLOs. The system intelligently allocates tasks, adjusts priorities, and handles memory constraints to address inefficiencies from dynamic request arrivals and pipeline idleness. Evaluations demonstrate significant performance gains over traditional separate deployments.

## Method Summary
LeMix co-locates LLM training and inference on shared multi-GPU nodes, using offline profiling to estimate task execution metrics (idleness increased, response time) and hierarchical scheduling to allocate resources. Tasks are assigned to nodes via priority scoring that balances utilization and latency, with runtime memory awareness to prevent OOM issues. The system builds on vLLM and DeepSpeed, using algorithms to compute idleness, prioritize tasks, and handle memory overruns through wait-or-offload policies.

## Key Results
- Throughput improves by up to 3.53× compared to separate training and inference setups
- Inference loss reduced by up to 0.61× under mixed workloads
- SLO attainment increases by up to 2.12×, especially under high training rates

## Why This Works (Mechanism)

### Mechanism 1
LeMix co-locates training and inference on shared nodes, filling idle pipeline periods by anticipating execution interference. It estimates idleness increased (II) and response time (R) per task per node using offline-profiled latency coefficients and forecasted execution paths. The quadratic scaling of latency with query length and linear scaling with batch size underpins these predictions.

### Mechanism 2
Hierarchical resource allocation combines local node scoring with global queue deprioritization. Task-level scoring uses idleness profit (IP), length consistency (LC), and response time (R), while queue-level deprioritization protects SLOs by delaying training when inference risks violation. This enables workload consolidation while maintaining responsiveness.

### Mechanism 3
Runtime memory-aware scheduling prevents memory overruns via a wait-or-offload policy. Before each stage's forward pass, LeMix checks memory against a threshold; if constrained, it waits up to T_max, then offloads KV cache/activations to CPU if needed, recalibrating forecasted paths with actual traces.

## Foundational Learning

- **Pipeline Parallelism (PP) & Pipeline Bubbles**
  - **Why needed:** LeMix relies on understanding how micro-batches flow through stages and where "far dependencies" create idle gaps that co-located tasks can fill.
  - **Quick check:** Given a 4-stage pipeline with micro-batches of varying lengths, can you sketch where forward-backward interference creates gaps?

- **Model Parallelism (MP) & Stage Sharding**
  - **Why needed:** Each node hosts multiple stages (sharded model layers); LeMix schedules at stage granularity within nodes.
  - **Quick check:** If a model is split into 4 stages across 2 GPUs per node, what is the dependency order for forward and backward passes?

- **Continuous Batching for Inference**
  - **Why needed:** LeMix uses iteration-level FCFS batching to form inference mini-batches; understanding prefill vs. decode latency tradeoffs is essential for interpreting R predictions.
  - **Quick check:** How does batching requests with heterogeneous prompt lengths affect time-to-first-token (TTFT) vs. throughput?

## Architecture Onboarding

- **Component map:** Global Scheduler → Per-Node Queues → Per-Stage Threads ← Memory Checker → CPU Offload → Runtime Calibration → Queue-level Deprioritization

- **Critical path:** Task arrival → COMPUTE IDLENESS (per node) → priority scoring (Eq. 3) → node assignment → per-stage wait-or-offload (Algorithm 2) → execution → trace calibration → queue-level deprioritization check (Eq. 4)

- **Design tradeoffs:**
  - Higher λ₁ (response weight) → lower latency but lower utilization (less consolidation)
  - Higher λ₂ (length consistency) → better training convergence but potentially skewed load distribution
  - Larger τ (idleness tolerance) → more consolidation, higher throughput, but increased queueing delay

- **Failure signatures:**
  - **SLO collapse under burst traffic:** Suggests deprioritization threshold τ_R too permissive or T_max too long
  - **OOM despite memory awareness:** Indicates M_threshold (κ·M_peak) set too aggressively or offload bandwidth insufficient
  - **Stagnant training loss:** May indicate excessive deprioritization starving training tasks

- **First 3 experiments:**
  1. **Profile latency coefficients (η_F, η_B):** Run synthetic workloads with varying ℓ and C; confirm quadratic scaling holds on your hardware.
  2. **Stress test SLO compliance:** Replay LMSYS traces at escalating request rates (50→150 rps) with 50% training rate; measure SLO attainment and identify where deprioritization activates.
  3. **Ablate memory awareness:** Disable offloading and observe SLO/throughput degradation under memory-constrained scenarios (long sequences, large KV cache).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does LeMix performance degrade when scheduling dependent tasks, such as multi-turn conversations or agentic chains?
- **Basis in paper:** [explicit] Section II-A states the system model assumes tasks are independent with no inter-task communication.
- **Why unresolved:** The current scheduling algorithms rely on independent arrival and execution planning; dependencies would introduce blocking periods not currently factored into "idleness increased" (II) or response time (R) calculations.
- **What evidence would resolve it:** Evaluation on traces with explicit dependency graphs (e.g., agentic workflows or multi-turn dialogue) measuring the accuracy of the execution predictor and SLO attainment under blocking conditions.

### Open Question 2
- **Question:** What are the long-term convergence implications of LeMix's decentralized weight update strategy compared to centralized synchronization?
- **Basis in paper:** [inferred] Section V-B notes that LeMix uses a decentralized strategy "similar to federated learning" for weight updates to resolve privacy concerns, but the evaluation focuses on immediate inference loss rather than convergence.
- **Why unresolved:** While the paper demonstrates short-term inference loss reduction via local updates, it does not analyze if the lack of global aggregation leads to model divergence or suboptimal convergence over extended training periods.
- **What evidence would resolve it:** Long-term convergence analysis tracking global model accuracy metrics (e.g., validation loss on a hold-out set) over thousands of decentralized updates versus periodic global checkpointing.

### Open Question 3
- **Question:** Can the scheduling heuristics effectively optimize utilization in clusters with heterogeneous GPU capabilities?
- **Basis in paper:** [inferred] Section VI-A describes testbeds using homogeneous hardware (all A6000 or all A100), implying the latency coefficients (η) and heuristics are calibrated for uniform compute capability.
- **Why unresolved:** In heterogeneous clusters, assigning tasks based on length consistency (LC) or idleness profit (IP) without adjusting for specific GPU throughput could lead to significant load imbalance or straggler effects.
- **What evidence would resolve it:** Experiments on a mixed cluster (e.g., combining A100s and T4s) measuring SLO attainment and throughput balance to verify if the offline profiler generalizes to heterogeneous nodes.

## Limitations
- Evaluation relies on simulated workloads rather than real production clusters, potentially missing operational robustness gaps.
- Quadratic latency model may not hold under hardware-specific optimizations (FlashAttention v2, quantization) or non-deterministic behaviors like memory fragmentation.
- Deprioritization mechanism may require per-workload tuning of τ_R to avoid training starvation under bursty traffic.

## Confidence

- **High Confidence:** The core mechanism of task-level idleness profit calculation and node consolidation is well-specified, with clear algorithmic definitions (Algorithm 1) and supporting ablation results (Fig. 12). The quadratic latency assumption is empirically reasonable given its prevalence in the literature.
- **Medium Confidence:** The hierarchical scheduling approach (task + queue level) is logically sound, but the interaction between λ₁/λ₂ tuning and real-world SLO adherence needs further validation. The paper's simulation framework may not fully capture production-grade tail latency behaviors.
- **Low Confidence:** The memory-aware wait-or-offload policy lacks extensive validation on heterogeneous hardware (e.g., systems with limited CPU-GPU bandwidth), and its impact on SLO under sustained memory pressure is not thoroughly explored.

## Next Checks
1. **Hardware Sensitivity Test:** Evaluate LeMix on GPUs with varying memory bandwidths and CPU-GPU interconnect speeds (e.g., A100 vs. H100 vs. older GPUs) to quantify offloading overhead and identify hardware-specific bottlenecks.
2. **Long-Horizon Stability:** Run multi-day simulations with realistic traffic patterns (e.g., LMSYS traces) to assess training convergence stability and SLO adherence under sustained mixed workloads, particularly focusing on deprioritization-induced starvation.
3. **Robustness to Model Optimizations:** Test LeMix with models using FlashAttention v2, quantization (e.g., 8-bit), or sparse attention to validate whether the quadratic latency model and forecasted paths remain accurate under these optimizations.