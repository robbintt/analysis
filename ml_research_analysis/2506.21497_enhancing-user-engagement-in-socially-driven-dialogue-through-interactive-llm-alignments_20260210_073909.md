---
ver: rpa2
title: Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM
  Alignments
arxiv_id: '2506.21497'
source_url: https://arxiv.org/abs/2506.21497
tags:
- user
- engagement
- interactive
- simulator
- conversation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the challenge of enhancing user engagement\
  \ in interactive LLM applications like emotional support and persuasive dialogues.\
  \ Instead of focusing on knowledge or dialogue act planning, the authors propose\
  \ aligning LLMs using a direct engagement signal\u2014the user's future reaction\
  \ tied to the dialogue's intended goal."
---

# Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments

## Quick Facts
- arXiv ID: 2506.21497
- Source URL: https://arxiv.org/abs/2506.21497
- Reference count: 19
- Primary result: Alignment via i×MCTS and DPO increases emotional support engagement from 64.06% to 80.47% and doubles persuasion donation amount from $0.58 to $1.29.

## Executive Summary
This paper addresses the challenge of enhancing user engagement in socially-driven dialogues such as emotional support and persuasive conversations. Rather than focusing on knowledge retrieval or dialogue act planning, the authors propose aligning LLMs using the user's future reaction—specifically the terminal engagement signal tied to the dialogue's intended goal. They introduce an i×MCTS method to explore conversation trajectories with a user simulator, construct a preference dataset, and apply Direct Preference Optimization to fine-tune the LLM. Experiments on two tasks show significant engagement improvements while maintaining conversation efficiency.

## Method Summary
The framework trains user simulators and interactive LLMs via SFT with LoRA on task-specific datasets (emotional support and persuasion). An i×MCTS method explores dialogue trajectories between the simulator and LLM, using UCB selection, expansion width e=3, pruning to K=81 nodes, and rollout rewards based on terminal engagement signals. Chosen-rejected response pairs are extracted to form Dp, augmented with Dt (pairs from training contexts ranked by a reward model trained on Dp). The interactive LLM is then aligned using DPO on the combined preference dataset.

## Key Results
- Emotional support model engagement rate increased from 64.06% to 80.47%
- Persuasion model average donation amount doubled from $0.58 to $1.29
- Both improvements achieved while maintaining similar conversation lengths
- Ablation studies confirm the necessity of i×MCTS and DPO over SFT alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Future user reactions serve as more reliable engagement signals than intermediate dialogue acts or knowledge retrieval.
- Mechanism: The system uses the user's final state (emotional expression completeness or donation amount) as a terminal reward, which propagates backward through the conversation tree. This captures cumulative conversational effects rather than turn-level heuristics.
- Core assumption: User engagement is shaped by the full conversation trajectory, not individual turns; simulator behavior approximates real user decision patterns.
- Evidence anchors:
  - [abstract] "we adopt a more direct and relevant indicator of user engagement, i.e., the user's reaction related to dialogue intention after the interaction"
  - [section 1] "user engagement is shaped by the cumulative effect of the entire conversation"
  - [corpus] Weak direct validation; neighbor papers focus on proactive retrieval and persona but do not test future-reward signals specifically.
- Break condition: If real users' engagement decisions diverge significantly from simulator patterns, the terminal reward signal becomes misaligned with actual outcomes.

### Mechanism 2
- Claim: i×MCTS generates higher-quality preference pairs by exploring multiple dialogue trajectories and selecting based on engagement outcomes.
- Mechanism: The tree alternates between model nodes and user simulator nodes. Selection uses UCB bandit; expansion generates multiple responses; pruning retains top-K trajectories based on similarity (emotional support) or sentiment accumulation (persuasion). Backpropagation updates visit counts and rewards, allowing the tree to identify which responses lead to successful engagement.
- Core assumption: Pruning criteria (cosine similarity between predicted and actual user state, or sentiment scores) correlate with trajectories that lead to higher engagement.
- Evidence anchors:
  - [section 3.2] "we explore K trajectories up to a certain depth T or until an engagement signal is reached"
  - [section 4.5] Pruning equations (4) and (5) define similarity and sentiment-based retention
  - [corpus] No direct external validation of this specific pruning approach; related work uses MCTS for dialogue act planning but not direct engagement optimization.
- Break condition: If pruning eliminates viable trajectories that would have led to engagement (false negatives), the preference dataset becomes biased toward easily discoverable paths.

### Mechanism 3
- Claim: DPO on the constructed preference dataset shifts the model toward responses that historically led to higher engagement.
- Mechanism: The preference dataset D combines Dp (pairs from i×MCTS) and Dt (pairs from training contexts ranked by a reward model trained on Dp). DPO optimizes the likelihood of chosen responses while decreasing rejected ones, using the old policy as reference.
- Core assumption: The reward model trained on Dp generalizes to rank responses from training contexts accurately; engagement patterns transfer across user conditions.
- Evidence anchors:
  - [section 3.3] "We incorporate the second part to mitigate the risk of LLM forgetting previously learned knowledge"
  - [table 2] Aligned model achieves 80.47% engagement vs 64.06% SFT baseline; donation doubles from $0.58 to $1.29
  - [corpus] DPO is well-established, but corpus lacks validation of engagement-specific reward models in dialogue.
- Break condition: If the reward model overfits to simulator patterns or preference pairs are noisy, DPO amplifies spurious correlations rather than true engagement drivers.

## Foundational Learning

- Concept: Monte Carlo Tree Search (MCTS)
  - Why needed here: i×MCTS is the core exploration mechanism for generating diverse trajectories and identifying engagement-optimizing paths.
  - Quick check question: Can you explain how UCB balances exploration vs. exploitation, and why backpropagation updates all ancestor nodes?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO replaces explicit reward modeling by directly optimizing from preference pairs; understanding the loss function is critical for debugging alignment.
  - Quick check question: Why does DPO use the ratio πθ/πθold instead of raw probabilities, and what role does β play?

- Concept: User Simulation in Dialogue
  - Why needed here: The entire pipeline depends on simulator fidelity; if the simulator doesn't approximate real users, the preference signal is invalid.
  - Quick check question: What are the tradeoffs between rule-based vs. LLM-based user simulators, and how would you validate simulator-to-human consistency?

## Architecture Onboarding

- Component map: User Simulator (SFT on domain data) -> i×MCTS (selection/expansion/rollout/backprop with pruning) -> Preference Dataset (Dp + Dt) -> Reward Model -> DPO Alignment -> Deployed Interactive LLM

- Critical path:
  1. Train user simulator on task-specific data with explicit user conditions
  2. Run i×MCTS with pruning to collect engagement-labeled trajectories
  3. Extract chosen-rejected pairs and train reward model
  4. Generate Dt from training set contexts using reward model
  5. Run DPO on combined dataset D

- Design tradeoffs:
  - Expansion width (e=3) vs. computational cost: wider exploration finds better trajectories but increases tree size exponentially
  - Pruning threshold K (81): higher K preserves diversity but slows search
  - Max depth T (25 emotional, 15 persuasion): deeper trees capture longer-horizon effects but increase rollout cost and simulator drift

- Failure signatures:
  - Low engagement rates after alignment: check if pruning eliminates viable paths; inspect preference pair quality via human annotation
  - Reward model disagrees with human judgment: Dp may be noisy; increase annotation or recalibrate engagement detection rules
  - Aligned model forgets general capabilities: Dt component insufficient; increase mixing ratio or augment with broader preference data

- First 3 experiments:
  1. **Simulator validation**: Before running i×MCTS, compare simulator responses to held-out human conversations using the 50-pair discrimination test described in Appendix A.3. Target: ≤50% identification rate.
  2. **Ablation on pruning criteria**: Run i×MCTS with different pruning strategies (no pruning, similarity-only, sentiment-only) on a small subset (100 user conditions) and measure final engagement rate. Hypothesis: pruning improves efficiency without sacrificing engagement quality.
  3. **DPO dataset composition**: Train separate models on Dp-only vs. Dp+Dt and evaluate on held-out user conditions. Expect Dp+Dt to maintain general fluency while Dp-only may overfit to specific engagement patterns.

## Open Questions the Paper Calls Out

- How can the framework be adapted to support personalized engagement strategies rather than a general user model?
  - Basis in paper: [explicit] The authors explicitly state in the Limitations section that developing a "more personalized and engaging interactive LLM" is a direction for future work, noting challenges in data collection and few-shot training.
  - Why unresolved: The current method optimizes for a general engagement signal, whereas personalization requires handling unique user traits and sparse data not addressed in this study.
  - What evidence would resolve it: Successful application of the i×MCTS framework using personal profiles, showing improved engagement metrics tailored to specific user demographics or psychological types.

- To what extent does the "simulator-to-real" gap affect the transferability of the aligned LLM to actual human interactions?
  - Basis in paper: [inferred] The methodology relies entirely on an SFT-based user simulator for exploration and preference collection, which the appendix admits "could still bring additional variances."
  - Why unresolved: The model optimizes against a synthetic user; discrepancies between the simulator's behavior and real human complexity could lead to hallucinated engagement signals.
  - What evidence would resolve it: A study correlating simulator-based performance gains with statistically equivalent gains in large-scale, real-user A/B testing.

- How does the computational overhead of i×MCTS limit its scalability for broader application or real-time alignment?
  - Basis in paper: [inferred] The appendix notes that generating 64 interactions took approximately 8 hours on a single A100 GPU, suggesting a bottleneck.
  - Why unresolved: The high inference cost of MCTS makes it difficult to scale the dataset construction to cover the vast diversity of potential dialogue states.
  - What evidence would resolve it: Demonstrating an optimized search variant or distillation method that reduces generation time significantly without degrading the quality of the preference dataset.

## Limitations

- The framework's effectiveness depends critically on user simulator fidelity, but validation against real human behavior is limited to a discrimination test on held-out data
- Engagement detection relies on rule-based regex matching and sentiment scoring, which may miss nuanced user reactions
- Computational cost of i×MCTS limits scalability, with 64 interactions taking ~8 hours on a single A100 GPU
- DPO hyperparameters and reward model architecture details are not specified, making it difficult to reproduce exact results

## Confidence

- **High confidence** in the core methodology: MCTS-based exploration with future reward signals is a well-established technique, and DPO on preference data is empirically validated across multiple domains. The significant performance improvements (engagement rate +16.41%, donation amount +123%) are internally consistent and unlikely to be random variation.
- **Medium confidence** in mechanism claims: While the future-reward signal and trajectory pruning are theoretically sound, the paper does not provide ablation evidence showing these components are necessary for the observed gains. The simulator fidelity validation is suggestive but not definitive proof that the learned preferences transfer to real users.
- **Low confidence** in generalizability: The framework is demonstrated only on two specific tasks (emotional support and persuasion for donations) using Chinese and English datasets respectively. The pruning criteria are task-specific, and no experiments test whether the same approach works for other dialogue types or languages.

## Next Checks

1. **Simulator-to-Human Transfer Test**: Deploy the aligned models with real human users on Mechanical Turk or similar platform. Measure actual engagement rates and donation amounts, then compare to simulator predictions. If performance drops >15% relative to simulator results, the simulator fidelity assumption is violated.

2. **Ablation on Pruning Strategy**: Run i×MCTS with three pruning variants (no pruning, cosine-similarity-only, sentiment-only) on 200 randomly sampled user conditions. Measure both computational efficiency (tree size, search time) and final engagement/donation outcomes. If differences are <5%, pruning may be unnecessary overhead.

3. **Reward Model Robustness**: Create a test set of 100 LLM-generated response pairs from training contexts. Have three human annotators rate which response would lead to higher engagement. Compare human preference agreement with the reward model's rankings. If agreement <70%, the reward model is unreliable and may introduce bias into DPO training.