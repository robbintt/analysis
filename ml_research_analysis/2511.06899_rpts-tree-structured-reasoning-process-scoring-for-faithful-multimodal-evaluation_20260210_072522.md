---
ver: rpa2
title: 'RPTS: Tree-Structured Reasoning Process Scoring for Faithful Multimodal Evaluation'
arxiv_id: '2511.06899'
source_url: https://arxiv.org/abs/2511.06899
tags:
- reasoning
- rpts
- multimodal
- rpts-eval
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the need for a more nuanced evaluation of
  multimodal reasoning in Large Vision-Language Models (LVLMs). The authors introduce
  RPTS-Eval, a benchmark comprising 390 reasoning instances, each with reliable visual-textual
  clues serving as leaf nodes in a reasoning tree.
---

# RPTS: Tree-Structured Reasoning Process Scoring for Faithful Multimodal Evaluation

## Quick Facts
- **arXiv ID:** 2511.06899
- **Source URL:** https://arxiv.org/abs/2511.06899
- **Reference count:** 10
- **Key outcome:** RPTS-Eval benchmark and RPTS metric reveal significant limitations in current LVLMs' multimodal reasoning capabilities, with open-source models particularly struggling to derive conclusions from images for further inference.

## Executive Summary
This paper addresses the need for faithful evaluation of multimodal reasoning in Large Vision-Language Models (LVLMs). The authors introduce RPTS-Eval, a benchmark with 390 reasoning instances featuring reliable visual-textual clues as leaf nodes in a reasoning tree. They propose RPTS, a tree-structured metric that assigns weighted scores to each reasoning step based on hierarchical position, allowing evaluation of both global coherence and localized failures. Experiments with representative LVLMs reveal significant limitations in their multimodal reasoning capabilities, particularly in deriving visual information for subsequent inference steps, with notable performance gaps between English and Chinese contexts.

## Method Summary
The RPTS framework evaluates reasoning faithfulness by parsing model outputs into a tree structure where leaf nodes represent atomic evidence (visual/textual clues) and non-leaf nodes represent inference steps. Each step receives a weight based on its height in the tree using the formula w_i = λ^|hf - h|, where h is node height and hf is focus height. The final RPTS score is the weighted average of individual step scores (0-1) obtained from an LLM scorer. The method includes a re-evaluation mechanism for steps scoring below 0.5, using broader context with a 0.8 penalty. The approach uses Chain-of-Thought prompting, GPT-4 reformatting to structured "[PREMISE] + [PREMISE] → [CONCLUSION]" steps, and individual step scoring to prevent contamination from adjacent reasoning elements.

## Key Results
- Current open-source LVLMs struggle to derive conclusions from images for further inference, showing significantly lower RPTS scores at initial inference steps (hf=1) compared to textual clues
- Open-source models exhibit varying performance across languages, with Llava-Next-7B accuracy dropping from 0.62 in English to 0.13 in Chinese contexts
- Models with correct final answers but low RPTS scores indicate "right answers for wrong reasons" scenarios, detectable through the filtering mechanism
- GPT-4 as scorer achieved lowest mean absolute error (0.095) compared to human scores among tested models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical tree-structured scoring captures non-linear multimodal reasoning better than linear chain evaluation
- **Mechanism:** Reasoning is parsed into a tree where leaf nodes = atomic evidence (visual/textual clues) and non-leaf nodes = inference steps. Each step receives a weight: w_i = λ^|hf - h|, where h = node height and hf = focus height. The final RPTS = Σ(w_i × s_i) / Σ(w_i). By adjusting λ (decay factor) and hf, evaluators can emphasize global coherence (λ ≈ 1) or localize failures (small λ, specific hf)
- **Core assumption:** Reasoning quality varies by position in the inference chain, and weighting by tree structure reflects logical dependency depth
- **Evidence anchors:** [abstract] "organize the reasoning steps into a reasoning tree and leverage its hierarchical information to assign weighted faithfulness scores"; [page 3] Formula definitions for weight calculation and overall RPTS score

### Mechanism 2
- **Claim:** Step-isolated LLM scoring prevents score contamination from adjacent correct/incorrect reasoning
- **Mechanism:** After CoT prompting and GPT-4 reformatting to "[PREMISE] + [PREMISE] → [CONCLUSION]", each step is scored independently (0-1). For image-derived conclusions: semantic similarity vs. ground truth. For logical steps: LLM assesses premise-conclusion coherence without seeing other steps
- **Core assumption:** LLMs can reliably judge single-step logical validity without context from the full reasoning chain
- **Evidence anchors:** [page 3] "we only evaluate individual reasoning steps, not the entire process. This method allows for more precise evaluations by preventing the influence of other reasoning elements"; [page 6] Table 4 shows GPT-4 achieved lowest mean absolute error (0.095) vs. human scores among tested models

### Mechanism 3
- **Claim:** Re-evaluation with broadened context catches reasoning that appears flawed in isolation but is valid given full information
- **Mechanism:** If initial step score < 0.5, re-score using ALL textual clues + prior conclusions as premises. Apply 0.8 penalty if original premises were incorrect. Take max(initial, re-evaluated) score
- **Core assumption:** Some valid reasoning uses premises not explicitly stated in the immediate step but available in the broader context
- **Evidence anchors:** [page 3] "instances where the model's selected premises may not directly support the given conclusion, though they may be justified within the broader reasoning context"; [page 3] Explicit re-evaluation procedure with 0.8 penalty

## Foundational Learning

- **Chain-of-Thought (CoT) Prompting**
  - Why needed here: RPTS requires models to output explicit step-by-step reasoning for tree parsing. Without CoT, models produce direct answers without intermediate steps
  - Quick check question: Can you write a prompt that elicits explicit "[PREMISE] → [CONCLUSION]" format from an LVLM?

- **Tree Data Structures (Height, Depth, Weighted Traversal)**
  - Why needed here: Understanding node height, decay weighting, and focus parameters is essential for computing RPTS scores and interpreting λ/hf settings
  - Quick check question: Given a tree with max depth 5, if hf=2 and λ=0.9, which nodes receive highest weights?

- **Multimodal Reasoning Taxonomy**
  - Why needed here: RPTS-Eval categorizes instances into guided, adversarial, and independent modality relationships—essential for error analysis
  - Quick check question: If text says "the red box contains the key" but the image shows a blue box, which relationship type is this?

## Architecture Onboarding

- **Component map:** Input image+query → CoT generation → Parsing → Step scoring → Tree construction → RPTS calculation → Accuracy comparison
- **Critical path:** Input image+query → CoT generation → Parsing → Step scoring → Tree construction → RPTS calculation → Accuracy comparison
- **Design tradeoffs:**
  - λ=1 (uniform weights) vs. λ<1 (localized focus): λ=1 gives global assessment; λ=0.9, hf=1 emphasizes early reasoning steps
  - GPT-4 as scorer: Highest human alignment (Table 4) but introduces dependency on closed-source model
  - Re-evaluation threshold 0.5: Balances catching false negatives vs. computational cost
- **Failure signatures:**
  - High accuracy, low RPTS → "right answer, wrong reasoning" (target detection)
  - Low accuracy, high RPTS → logically coherent but off-target conclusions (open-source models in Table 3)
  - RPTS drop at hf=1 vs. hf=3 → early-step visual processing failures (Figure 6 pattern)
- **First 3 experiments:**
  1. **Reproduce Table 3 on a single open-source model (e.g., Llava-v1.5-7B)**: Compare accuracy, RPTS, and filtered accuracy between English and Chinese to validate the language transfer gap
  2. **Ablate λ values (0.7, 0.8, 0.9, 1.0) with fixed hf=1**: Measure sensitivity of filtered accuracy percentage (replicate Table 6 pattern) to confirm localization capability
  3. **Manual inspection of 20 "correct answer, RPTS<0.5" cases**: Verify that re-evaluation mechanism correctly identifies premise issues vs. parser failures

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific architectural or training modifications are required to enable open-source LVLMs to reliably derive intermediate conclusions from visual inputs for multi-step inference?
- **Basis in paper:** [explicit] The authors state in the "Results and Analysis" section that "open-source models still lack sufficient capabilities in image processing. They fail to derive necessary information from images for subsequent reasoning tasks," specifically noting low RPTS scores at the initial inference step ($h_f=1$) compared to textual clues
- **Why unresolved:** The paper identifies the failure mode (visual-to-conclusion derivation) but does not propose or test methods to improve this specific capability in open-source models
- **What evidence would resolve it:** A study demonstrating that fine-tuning open-source LVLMs on datasets rich in intermediate visual reasoning steps significantly improves their RPTS scores at lower tree heights

### Open Question 2
- **Question:** How can training methodologies be adapted to minimize the performance disparity between English and Chinese contexts in multimodal reasoning tasks?
- **Basis in paper:** [explicit] The Conclusion notes "a significant disparity in the capabilities of models between Chinese and English contexts, suggesting that existing training methodologies fall short in transferring multimodal abilities from English to other languages"
- **Why unresolved:** The paper highlights the gap (e.g., Llava-Next-7B accuracy dropping from 0.62 to 0.13) but leaves the underlying cause—whether data scarcity, tokenization, or alignment—as an open inference
- **What evidence would resolve it:** Experiments showing that language-specific visual instruction tuning or balanced multilingual multimodal pre-training closes the RPTS score gap between English and Chinese evaluations

### Open Question 3
- **Question:** Does the reliance on GPT-4 as an automated scorer introduce systematic bias against the reasoning styles of smaller, open-source models?
- **Basis in paper:** [inferred] While the authors justify using GPT-4 as a scorer based on minimal Mean Absolute Error (0.095) compared to humans (Table 4), the methodology assumes a proprietary model is a universal "ground truth" for logic, which may penalize valid reasoning patterns of smaller models that differ from GPT-4's style
- **Why unresolved:** The paper validates the scorer's correlation with humans generally but does not analyze if the "flawed reasoning" detected in open-source models is actually stylistic divergence rather than logical error
- **What evidence would resolve it:** A human evaluation specifically comparing the "filtered" (low RPTS) reasoning paths of smaller models to determine the false positive rate of the GPT-4 scorer

## Limitations

- The framework relies heavily on GPT-4 for both reasoning reformatting and scoring, creating dependency on a closed-source model and limiting accessibility
- The semantic similarity metric for image-derived conclusions is not explicitly defined, leaving implementation details ambiguous
- The benchmark's relatively small size (390 instances) may not fully capture the diversity of multimodal reasoning challenges

## Confidence

- **High Confidence:** The hierarchical tree-structured scoring mechanism (Mechanism 1) is well-grounded with clear mathematical formulation and reasonable assumptions about reasoning dependency depth
- **Medium Confidence:** The effectiveness of the re-evaluation mechanism (Mechanism 3) has moderate support from the paper's description but lacks direct empirical validation
- **Low Confidence:** The complete reproducibility of results depends heavily on undisclosed prompt templates and exact API configurations, particularly for the GPT-4 scorer and CoT generation steps

## Next Checks

1. **Parameter Sensitivity Analysis:** Systematically vary λ (0.7, 0.8, 0.9, 1.0) and hf (1, 2, 3) on a single open-source model to empirically validate the localization capability claims and identify optimal parameter settings for different reasoning patterns

2. **Cross-Scorer Validation:** Replace GPT-4 scorer with an open-source alternative (e.g., GPT-3.5-turbo or a fine-tuned model) to assess the dependency on closed-source models and evaluate consistency of RPTS scores across different scoring systems

3. **Manual Verification Study:** Conduct detailed manual analysis of 50 "correct answer, RPTS<0.5" cases to validate whether the parser correctly identifies flawed reasoning versus cases where the tree structure fails to capture valid non-linear reasoning patterns