---
ver: rpa2
title: 'GTPO: Stabilizing Group Relative Policy Optimization via Gradient and Entropy
  Control'
arxiv_id: '2508.03772'
source_url: https://arxiv.org/abs/2508.03772
tags:
- grpo
- gtpo
- entropy
- training
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two key limitations in GRPO for LLM alignment:
  token-level penalization, where shared tokens receive conflicting gradients, and
  policy collapse, where negative rewards destabilize confident predictions. The authors
  propose GTPO, which corrects gradient conflicts via a masking scheme that suppresses
  negative updates on shared tokens while amplifying positive ones, and controls policy
  collapse through entropy-based regularization and filtering.'
---

# GTPO: Stabilizing Group Relative Policy Optimization via Gradient and Entropy Control

## Quick Facts
- arXiv ID: 2508.03772
- Source URL: https://arxiv.org/abs/2508.03772
- Authors: Marco Simoni; Aleksandar Fontana; Giulio Rossolini; Andrea Saracino; Paolo Mori
- Reference count: 40
- One-line primary result: GTPO improves Pass@64 and Maj@64 scores by up to 75 and 42.5 points over GRPO on mathematical reasoning tasks

## Executive Summary
This paper addresses two key limitations in GRPO for LLM alignment: token-level penalization, where shared tokens receive conflicting gradients, and policy collapse, where negative rewards destabilize confident predictions. The authors propose GTPO, which corrects gradient conflicts via a masking scheme that suppresses negative updates on shared tokens while amplifying positive ones, and controls policy collapse through entropy-based regularization and filtering. GTPO eliminates the KL divergence term used in GRPO, simplifying training and removing the need for a reference model. Extensive experiments on GSM8K, MATH, AIME 2024/2025, and AMC 2023 show GTPO consistently outperforms GRPO and SFT in both in-distribution and out-of-distribution settings, with improvements of up to 75 points on Pass@64 and 42.5 points on Maj@64 over GRPO, and 32.5 and 42.5 points over SFT.

## Method Summary
GTPO modifies GRPO by replacing KL divergence regularization with (i) conflict-aware gradient correction that masks negative updates to shared prefix/suffix tokens while amplifying positive ones, and (ii) entropy control that filters high-entropy completions and adds entropy regularization. The method uses LLaMA 8B and Qwen 2.5 3B with LoRA fine-tuning, setting the entropy threshold at ln(2) for low-entropy models and using γ=0.1 for entropy regularization. Training employs group size G∈{8,12}, temperature 1.0, learning rate 1e-6, and one PPO iteration per step with cosine learning rate schedule.

## Key Results
- GTPO achieves up to 75 points higher Pass@64 and 42.5 points higher Maj@64 than GRPO on mathematical reasoning benchmarks
- Performance improvements are consistent across in-distribution (GSM8K, MATH) and out-of-distribution (AIME 2024/2025, AMC 2023) tasks
- GTPO shows 32.5 and 42.5 point improvements over SFT in Pass@64 and Maj@64 respectively
- Ablation studies confirm both conflict-aware gradient correction and entropy control are essential for GTPO's superior stability and performance

## Why This Works (Mechanism)

### Mechanism 1: Conflict-Aware Gradient Masking
Filtering negative gradients on tokens shared by both high- and low-reward completions prevents token-level penalization inherent in GRPO. The system identifies "conflict tokens" appearing at the same position in completions with opposite advantage signs, applying masks that set gradient weight to 0 for these tokens in negative completions and 2 in positive ones. This prevents punishment of structural tokens that appear in failed reasoning chains.

### Mechanism 2: Entropy-Based Filtering
Discarding completions with entropy exceeding a specific threshold prevents policy collapse caused by training on uncertain, low-quality outputs. The method calculates average Shannon entropy for each completion and zeroes out gradient contributions when entropy exceeds ln(2) for low-entropy models, tracking instability in real-time.

### Mechanism 3: Reference-Free Regularization
Replacing KL-divergence with direct entropy regularization stabilizes training more effectively. GTPO removes the β·D_KL term and reference model, instead minimizing γ·⟨H⟩_i directly in the loss, lowering memory overhead and reacting faster to instability than KL's delayed corrective signal.

## Foundational Learning

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed: GTPO is a direct modification of GRPO. Understand that GRPO generates a group of outputs per prompt and normalizes their rewards to compute advantages, eliminating the need for a value model.
  - Quick check: How does GRPO estimate the baseline for advantage calculation without a critic network?

- **Concept: Token-Level Credit Assignment**
  - Why needed: The paper's central critique is that GRPO applies sequence-level rewards equally to all tokens, causing "gradient conflicts" on shared tokens.
  - Quick check: Why does applying a scalar reward to a whole sequence cause specific issues for tokens that appear in both correct and incorrect answers?

- **Concept: Policy Entropy vs. KL Divergence**
  - Why needed: The paper argues these are distinct stability signals. Entropy measures the randomness of the current policy, while KL measures the drift from the reference policy.
  - Quick check: Why might entropy be a more immediate signal of "policy collapse" than KL divergence?

## Architecture Onboarding

- **Component map:** Sampler -> Advantage Calculator -> Conflict Detector -> Entropy Filter -> Loss Engine
- **Critical path:** The correct identification of "conflict tokens" is vital. If the alignment logic (left-to-right/right-to-left) is off, the masking mechanism will suppress the wrong gradients.
- **Design tradeoffs:** Removing the reference model (KL) saves significant VRAM but removes the safety anchor of pretrained weights. Setting the entropy threshold too low may filter valid data; setting it too high invites collapse.
- **Failure signatures:** Collapse shows formatting reward drops steeply while average entropy rises. Stagnation occurs if γ is too high, causing policy to become overconfident too quickly.
- **First 3 experiments:** 1) Sanity Check: Replicate the "Token-level Penalization" experiment (Figure 3) to verify that conflict masking alone boosts performance over vanilla GRPO. 2) Ablation: Disable the entropy filter on a low-entropy model to observe the timing of policy collapse. 3) Threshold Sweep: Test entropy thresholds on a validation set to find the boundary between "healthy exploration" and "collapse."

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical entropy bounds that guarantee effective exploration without compromising training stability in GTPO?
- Basis in paper: [explicit] Conclusion states: "For future work, we plan to further identify theoretical entropy bounds to ensure effective exploration without compromising stability."
- Why unresolved: Current entropy threshold (⟨H⟩_i < ln 2) and regularization weight (γ = 0.1) are empirically determined with no formal guarantees for why these values work or how to set them theoretically.
- What evidence would resolve it: A formal derivation connecting entropy bounds to convergence properties, validated across multiple model architectures and tasks with provable stability guarantees.

### Open Question 2
- Question: Can the conflict-aware gradient correction mechanism be extended to middle tokens without destabilizing training?
- Basis in paper: [explicit] Section 5.1 states: "masking isolated conflict tokens in the middle could harm stability and learning, as their meaning often depends on surrounding context, and altering their gradients may be counterproductive."
- Why unresolved: The current masking scheme deliberately excludes middle tokens, potentially leaving gradient conflicts unaddressed in substantial portions of completions.
- What evidence would resolve it: Ablation experiments with middle-token masking strategies showing either improved performance or empirical evidence of the predicted destabilization.

### Open Question 3
- Question: Why does GTPO prevent policy collapse in low-entropy models (LLaMA) but show similar stability to GRPO in high-entropy models (Qwen)?
- Basis in paper: [inferred] Section 6.1 notes Qwen training curves "are not affected by a policy collapse, as it shows high-entropy behavior," suggesting GTPO's benefits may depend on initial model entropy characteristics.
- Why unresolved: The paper demonstrates differential behavior but does not isolate whether GTPO's entropy filtering or gradient correction drives the model-specific improvements.
- What evidence would resolve it: Systematic experiments varying initial model entropy while controlling for other factors, or theoretical analysis of the interaction between baseline entropy and GTPO mechanisms.

## Limitations
- Performance comparisons are limited to GRPO and SFT baselines without testing against other modern RLHF/RLAIF methods like DPO or PPO with KL penalties
- The conflict token detection algorithm's exact implementation details are sparse, particularly regarding longest common prefix/suffix vs. first contiguous span selection
- The entropy threshold of ln(2) may be task-dependent and might not generalize across different model families or domains

## Confidence

- **High Confidence:** The existence of token-level penalization in GRPO and its mitigation through gradient masking is well-supported by both theoretical reasoning and empirical results (Figure 3). The policy collapse mechanism and entropy-based filtering are also convincingly demonstrated (Figure 4).
- **Medium Confidence:** The claim that removing KL divergence while maintaining stability is novel and effective. While the paper provides ablation studies, the broader applicability of this design choice without a reference model anchor needs further validation across diverse tasks and model architectures.
- **Low Confidence:** The assertion that GTPO will generalize to non-mathematical tasks or models with inherently higher entropy distributions (e.g., creative writing models) is not substantiated. The specific entropy threshold (ln(2)) may be task-dependent.

## Next Checks

1. **Conflict Token Detection Robustness:** Implement multiple variants of the conflict token detection algorithm (longest common prefix/suffix vs. first contiguous span) and measure the variance in GTPO's performance across these implementations on a held-out validation set.

2. **Entropy Threshold Sensitivity:** Conduct a systematic sweep of entropy thresholds (e.g., [0.5, 1.0, 1.5, 2.0, 2.5]) across different model families (LLaMA, Qwen, Mistral) to determine if ln(2) is universally optimal or task/model-specific.

3. **Generalization to Non-Mathematical Tasks:** Evaluate GTPO on a non-mathematical task with a different reward structure (e.g., summarization with ROUGE scores or instruction following with human feedback) to test if the gradient masking and entropy control mechanisms are broadly applicable or task-specific.