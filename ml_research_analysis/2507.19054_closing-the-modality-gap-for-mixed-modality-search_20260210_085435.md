---
ver: rpa2
title: Closing the Modality Gap for Mixed Modality Search
arxiv_id: '2507.19054'
source_url: https://arxiv.org/abs/2507.19054
tags:
- modality
- text
- image
- clip
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mixed modality search requires retrieving semantically relevant
  content across heterogeneous documents containing text, images, and multimodal combinations.
  Current contrastive models like CLIP suffer from a modality gap, where image and
  text embeddings form distinct clusters, causing intra-modal ranking bias and inter-modal
  fusion failure.
---

# Closing the Modality Gap for Mixed Modality Search

## Quick Facts
- arXiv ID: 2507.19054
- Source URL: https://arxiv.org/abs/2507.19054
- Reference count: 40
- Mixed modality search improved by 26 percentage points NDCG@10 using lightweight mean-centering calibration

## Executive Summary
Mixed modality search requires retrieving semantically relevant content across heterogeneous documents containing text, images, and multimodal combinations. Current contrastive models like CLIP suffer from a modality gap, where image and text embeddings form distinct clusters, causing intra-modal ranking bias and inter-modal fusion failure. To address this, we propose GR-CLIP, a lightweight post-hoc calibration method that removes the modality gap by mean-centering embeddings for each modality. Evaluated on MixBench—the first benchmark for mixed modality search—GR-CLIP improves NDCG@10 by up to 26 percentage points over CLIP and surpasses VLM2Vec by 4 percentage points while using 75x less compute. This demonstrates that closing the modality gap is both effective and efficient for unified multimodal retrieval.

## Method Summary
GR-CLIP addresses the modality gap in mixed modality search by applying a post-hoc mean-centering calibration to CLIP embeddings. The method computes the mean vector for each modality (text and image) on a calibration set, then subtracts these means from all embeddings before indexing and retrieval. For multimodal documents, GR-CLIP uses linear interpolation between calibrated image and text embeddings with a tunable weight α. This lightweight approach requires no model retraining and minimal additional computation, achieving significant performance improvements across multiple datasets while maintaining the efficiency advantages of pre-trained CLIP models.

## Key Results
- GR-CLIP improves NDCG@10 by up to 26 percentage points over CLIP on mixed modality search tasks
- Outperforms VLM2Vec by 4 percentage points while using 75x less compute
- Eliminates the U-shaped performance curve observed when varying screenshot proportion in documents
- Fusion with intermediate α values outperforms unimodal baselines once modality gap is removed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing the modality gap via mean-centering improves cross-modal retrieval by eliminating systematic intra-modal ranking bias.
- Mechanism: CLIP embeddings form separate clusters for images and text due to initialization and contrastive optimization dynamics. This gap causes same-modality pairs to have spuriously higher cosine similarity than cross-modal pairs with equivalent semantics. Mean-centering subtracts the modality-specific mean vector (approximately orthogonal to the semantic subspace) from each embedding, reducing this structural bias.
- Core assumption: The modality gap can be approximated as a constant offset vector, as characterized in prior work [35, 36].
- Evidence anchors:
  - [abstract] "image and text embeddings form distinct clusters, leading to intra-modal ranking bias"
  - [section 3.2] "We attribute the U-shaped performance to modality gap... text and image clusters remain separate, resulting in systematically higher intra-modal similarity scores"
  - [corpus] Limited direct corpus support; "Closing the Modality Gap Aligns Group-Wise Semantics" (FMR=0.52) addresses similar gap issues but with different methodology.
- Break condition: If the gap is not approximately constant (varies significantly across semantic subspaces), mean-centering alone will be insufficient.

### Mechanism 2
- Claim: Linear interpolation of multimodal embeddings fails without gap removal because fused representations land in semantically weak regions.
- Mechanism: When image and text embeddings occupy separate regions, their convex combinations fall in the "void" between clusters—far from both modalities' semantic neighborhoods. After mean-centering aligns both modalities around the origin, interpolation produces representations that combine complementary information rather than averaging toward semantic noise.
- Core assumption: Linear interpolation is a reasonable fusion strategy when modalities share a unified space.
- Evidence anchors:
  - [section 4.2] "performance typically peaks at one of the unimodal endpoints... fusion with intermediate α values fails to outperform"
  - [section 4.2] "Once the modality gap is removed... performance peaks at intermediate α values—surpassing both unimodal baselines"
  - [corpus] "e5-omni" explores omni-modal embeddings but does not directly validate this fusion mechanism.
- Break condition: If non-linear fusion (e.g., attention-based) is required for complex interleaved documents, linear interpolation benefits may not transfer.

### Mechanism 3
- Claim: The calibration generalizes across CLIP variants and modalities because the gap is a structural property of contrastive training, not dataset-specific.
- Mechanism: Since the gap arises from initialization and contrastive loss geometry, it persists across model scales and training data distributions. Computing unified mean vectors from diverse calibration sets captures this invariant structure, enabling transfer without task-specific fine-tuning.
- Core assumption: A single global mean per modality sufficiently approximates the gap; local variations are negligible.
- Evidence anchors:
  - [section 3.4] "U-shaped curve is observed across three CLIP variants... GR-CLIP consistently flattens the curve"
  - [appendix A.1] Results extend to video-text (ViCLIP) and audio-text (CLAP) with similar pattern
  - [corpus] "Post-pre-training for Modality Alignment" (FMR=0.55) confirms gap persistence across CLIP variants but proposes retraining rather than post-hoc calibration.
- Break condition: If downstream domains have dramatically different embedding distributions (e.g., medical images vs. natural images), unified means may not capture domain-specific gaps.

## Foundational Learning

- Concept: **Contrastive Learning Objective**
  - Why needed here: CLIP's training aligns paired image-text samples but does not explicitly minimize the global modality gap. Understanding this explains why gap persists despite alignment.
  - Quick check question: Why does contrastive loss on matched pairs not guarantee unified embedding clusters?

- Concept: **Cosine Similarity Geometry in High Dimensions**
  - Why needed here: The modality gap manifests as systematically different cosine similarities for intra- vs. cross-modal pairs. Mean-centering affects angular relationships.
  - Quick check question: If all text embeddings are shifted by a constant vector, what happens to text-text and text-image cosine similarities?

- Concept: **NDCG@k Metric**
  - Why needed here: Evaluation uses NDCG@10, which penalizes ranking errors at top positions. The 26 percentage point gain reflects corrected ranking order.
  - Quick check question: Why does the U-shaped curve in Figure 2b indicate a ranking problem rather than just a similarity scale issue?

## Architecture Onboarding

- Component map:
  - CLIP text encoder f^T -> Calibration subtraction (ē_q) -> Retrieval
  - CLIP image encoder f^I -> Calibration subtraction (ē_I) -> Retrieval
  - Fusion Layer: Weighted interpolation α·e^T + (1-α)·e^I for multimodal documents

- Critical path:
  1. Encode all corpus documents → compute per-modality means on calibration set (one-time, O(n))
  2. Subtract means from all embeddings before indexing
  3. At query time: encode query, subtract query mean, compute similarity against calibrated corpus

- Design tradeoffs:
  - **Unified vs. dataset-specific means**: Paper uses unified means for generalization; dataset-specific means may improve slightly but risk overfitting.
  - **Fusion weight α**: Paper uses α∈[0,1]; optimal α varies by dataset (see Figure 3b). Default α=0.5 is reasonable starting point.
  - **Calibration set size**: Paper uses ~10k samples per modality; smaller sets may introduce noise.

- Failure signatures:
  - Performance still U-shaped after calibration → means not representative of test distribution
  - Multimodal fusion still worse than unimodal → gap not fully removed (verify mean computation)
  - Cross-modal retrieval accuracy degrades on specific domains → consider domain-specific recalibration

- First 3 experiments:
  1. Replicate U-shaped curve on NFCorpus with screenshot replacement (p=0 to p=1) using vanilla CLIP; verify GR-CLIP flattens it.
  2. Compute means on MSCOCO training split, evaluate on OVEN test set to test generalization.
  3. Sweep α∈{0.0, 0.25, 0.5, 0.75, 1.0} on multimodal documents; confirm peak shifts from endpoint (CLIP) to interior (GR-CLIP).

## Open Questions the Paper Calls Out
None

## Limitations
- The method assumes the modality gap is a constant offset vector, which may not hold for all semantic subspaces or model architectures
- Linear interpolation for multimodal fusion may be suboptimal for complex interleaved documents requiring non-linear combination strategies
- Limited evaluation on downstream task performance beyond standard retrieval metrics

## Confidence
- **High Confidence**: The experimental demonstration that mean-centering eliminates the U-shaped performance curve on NFCorpus, MixModS, and OVEN datasets is reproducible and directly supported by ablation results. The 26 percentage point improvement over CLIP is a measurable, objective outcome.
- **Medium Confidence**: The claim that a single unified mean vector per modality suffices for cross-domain generalization (MSCOCO → OVEN) is supported but limited to two datasets. The mechanism's reliance on constant gap approximation is theoretically sound but untested on models with different contrastive training objectives.
- **Low Confidence**: The assertion that linear interpolation is optimal for multimodal fusion lacks direct comparison to attention-based or non-linear fusion methods, leaving open the possibility that gap removal alone is insufficient for peak performance on interleaved documents.

## Next Checks
1. **Cross-Model Validation**: Apply GR-CLIP to a non-OpenAI contrastive model (e.g., OpenCLIP) and verify if the U-shaped curve flattens and if performance gains persist. This tests whether the gap is truly structural rather than model-specific.
2. **Domain-Specific Recalibration**: Train separate mean vectors on medical imaging data and evaluate on a medical retrieval task (e.g., PubVQA). If performance degrades without domain-specific calibration, it challenges the unified mean assumption.
3. **Non-Linear Fusion Benchmark**: Replace linear interpolation with a simple attention mechanism for multimodal documents and compare against GR-CLIP + linear interpolation. If attention outperforms, it suggests fusion strategy, not just gap removal, is critical.