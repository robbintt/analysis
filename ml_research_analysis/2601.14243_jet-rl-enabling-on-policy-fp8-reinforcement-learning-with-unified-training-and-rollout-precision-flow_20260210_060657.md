---
ver: rpa2
title: 'Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training
  and Rollout Precision Flow'
arxiv_id: '2601.14243'
source_url: https://arxiv.org/abs/2601.14243
tags:
- training
- rollout
- uni00000013
- bf16
- jet-rl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency and instability of existing
  reinforcement learning (RL) pipelines, where the rollout phase consumes over 70%
  of total training time. The authors identify that the commonly used BF16-train +
  FP8-rollout strategy suffers from severe training instability and accuracy collapse,
  especially under long-horizon rollouts and challenging tasks.
---

# Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow

## Quick Facts
- **arXiv ID**: 2601.14243
- **Source URL**: https://arxiv.org/abs/2601.14243
- **Reference count**: 40
- **One-line primary result**: Achieves 33% rollout speedup, 41% training speedup, and 16% end-to-end speedup over BF16 training while maintaining stable convergence and <1% accuracy degradation.

## Executive Summary
This paper addresses the inefficiency and instability of existing RL pipelines where the rollout phase consumes over 70% of training time. The commonly used BF16-train + FP8-rollout strategy suffers from severe training instability and accuracy collapse, especially under long-horizon rollouts and challenging tasks, due to numerical mismatches between training and inference precision that introduce off-policy effects. Jet-RL proposes a unified FP8 precision flow for both training and rollout, enforcing on-policy consistency by ensuring identical quantization behavior across phases. This eliminates the need for inter-step calibration and minimizes numerical discrepancies, achieving significant speedups while maintaining stable convergence.

## Method Summary
Jet-RL implements a unified FP8 precision flow where both training and rollout phases use identical FP8 quantization configurations. The approach uses per-group (1Ã—128) activation quantization and per-block (128Ã—128) weight quantization, with master weights stored in BF16 for stability. Forward activations are quantized to FP8 and fused with preceding operators, while gradients are transported in BF16 to preserve numerical fidelity. All GEMM operations (FProp, WGrad, DGrad) are performed in FP8. The system integrates vLLM for inference (rollout) and VeRL for training (evaluation and update phases), with custom DeepGEMM kernels and Triton implementations supporting the required matmul configurations.

## Key Results
- Achieves 33% speedup in rollout phase, 41% in training phase, and 16% end-to-end speedup over BF16 training
- Maintains stable convergence and <1% accuracy degradation on reasoning tasks (GSM8K, MATH, DeepMATH)
- Eliminates training divergence observed in BF16-train-FP8-rollout approach at long rollout lengths (>8K tokens)
- Shows consistent performance across different model sizes (8B, 14B, 32B) and tensor parallelism configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Enforcing a unified FP8 precision flow between training and rollout preserves on-policy consistency, stabilizing RL optimization.
- Mechanism: The paper models precision propagation as a directed graph ð’¢ = (ð’±, â„°). In BF16-train-FP8-rollout, training uses ð’¢_fwd_train (BF16) while rollout uses ð’¢_infer (FP8), creating two distinct forward graphs. This mismatch means the actor learns from logits that differ numerically from what it actually generates during rolloutâ€”violating the on-policy assumption. Jet-RL forces ð’¢_infer to be a subgraph of ð’¢_fwd_train, ensuring identical quantization behavior. The only difference is a BF16 master weight copy for stability.
- Core assumption: The RL algorithm (e.g., PPO, GRPO) requires strict on-policy consistency; even small numerical drift between training and rollout distributions accumulates across long sequences, degrading convergence.
- Evidence anchors:
  - [abstract]: "These failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference."
  - [section 4]: "In Jet-RL, we propose solving this problem by forcing ð’¢_infer to be a subgraph of ð’¢_fwd_train."
  - [corpus]: FP8-RL paper (arXiv:2601.18150) also addresses low-precision RL but does not explicitly unify precision flow; corpus confirms this is an emerging problem space with no prior unified-flow solution.
- Break condition: If the RL algorithm is explicitly off-policy (e.g., using importance sampling with stale data), the unified precision flow benefit diminishes; the mechanism targets on-policy methods like PPO/GRPO.

### Mechanism 2
- Claim: Per-group (1Ã—128) activation quantization combined with per-block (128Ã—128) weight quantization stabilizes FP8 GEMM computation during training.
- Mechanism: Per-tensor FP8 quantization is unstable for LLM training (evidenced by prior work). The paper adopts finer granularity: activations/gradients use 1Ã—128 per-group quantization (one scale per 128 elements along the token dimension), while weights use 128Ã—128 per-block quantization. For FProp and DGrad, this enables efficient rowÃ—column GEMM layouts. For WGrad, the second matrix is quantized as 128Ã—1 to match gradient layout requirements. This balances dynamic range coverage (finer on activations) with memory efficiency (coarser on weights).
- Core assumption: Activation distributions vary more across tokens than weight distributions do across channels; thus activations require finer granularity to avoid quantization-induced drift.
- Evidence anchors:
  - [section 4.2]: "We quantize the weights using 128Ã—128 per-block quantization and quantize the activations and gradients using 1Ã—128 per-group quantization."
  - [section 4.2]: "Per-tensor quantization of FP8 has been shown to be unstable in training large language models."
  - [corpus]: COAT (arXiv:2410.19313, cited in paper) and DeepSeek-V3 technical report establish per-block quantization as a stabilizing technique; the corpus supports but does not independently validate the specific 1Ã—128/128Ã—128 hybrid.
- Break condition: If activation outliers are extreme (beyond E4M3 dynamic range) and not handled by RMSNorm/fused operations, finer granularity alone may not prevent underflow/overflow.

### Mechanism 3
- Claim: Storing activations in FP8 for backward pass and gradients in BF16 for inter-operator transport preserves both memory efficiency and gradient precision.
- Mechanism: During forward pass, activations are quantized to FP8 and fused with preceding operators (e.g., RMSNorm). These FP8 activations are saved for backward computation, avoiding dequantization overhead. However, gradients transported between operators during backprop remain in BF16 to prevent gradient underflow and quantization noise. The GEMM operations (DGrad, WGrad) internally quantize inputs to FP8 for accelerated computation, then output BF16 results. This asymmetric design prioritizes memory bandwidth reduction where possible (activations) while preserving numerical fidelity where critical (gradients).
- Core assumption: Gradient noise from FP8 quantization is more harmful to convergence than activation quantization noise, because gradients directly drive weight updates with smaller magnitudes.
- Evidence anchors:
  - [section 4.1]: "We retain the gradients transported between operators during the backward pass in BF16 precision to preserve model accuracy... they often introduce gradient underflow or quantization noise."
  - [section 4.1]: "We elect to store the activations for the backward pass also in FP8 precision."
  - [corpus]: No direct corpus evidence on this specific asymmetric storage strategy; labeled as assumption based on paper's internal reasoning.
- Break condition: If the model has very deep layers or low learning rates where gradients are near-underflow even in BF16, retaining BF16 gradients may still be insufficient; mixed-precision gradient scaling would be needed.

## Foundational Learning

- Concept: **On-Policy vs. Off-Policy RL**
  - Why needed here: Jet-RL's core motivation is that BF16-train-FP8-rollout is "effectively off-policy" because precision mismatch causes the training distribution to diverge from the rollout distribution. Understanding on-policy constraints clarifies why unified precision matters.
  - Quick check question: In PPO, what happens if the policy used to collect data differs from the policy being updated?

- Concept: **FP8 (E4M3) Format and Dynamic Range**
  - Why needed here: The paper uses E4M3 FP8 (max value 448), which has limited dynamic range compared to BF16. This motivates per-group/per-block quantization to handle outliers.
  - Quick check question: Why does E4M3 have a max of 448 rather than 2^8 = 256?

- Concept: **Quantization Granularity (Per-Tensor, Per-Block, Per-Group)**
  - Why needed here: The paper's stability mechanism hinges on choosing correct granularity for different tensor types. Per-tensor is too coarse; per-group/block provides finer control.
  - Quick check question: If you quantize a 4096Ã—4096 weight matrix per-tensor, how many scale factors do you have? What if you use 128Ã—128 per-block?

## Architecture Onboarding

- Component map: vLLM (inference) -> VeRL (training) -> DeepGEMM kernels (FP8 GEMM) -> Triton kernels (fused operations) -> BF16 master weights

- Critical path:
  1. **Rollout Phase** (vLLM): Actor generates responses using FP8 weights â†’ produces token sequences
  2. **Evaluation Phase** (VeRL): Reference/Reward/Critic models compute KL divergence, rewards, and value estimates (prefill workload)
  3. **Update Phase** (VeRL): Actor forward+backward with unified FP8 precision flow â†’ weight update in BF16 master copy
  4. **Synchronization**: Quantize updated weights to FP8, sync to vLLM for next rollout

- Design tradeoffs:
  - **Speedup vs. TP Degree**: Higher tensor parallelism reduces FP8 speedup (e.g., 32B model: 1.33Ã— at TP=2 vs. 1.10Ã— at TP=4) due to communication overhead
  - **Accuracy vs. Rollout Length**: Longer rollouts (>8K tokens) amplify precision mismatch; Jet-RL mitigates this but BF16-train-FP8-rollout fails catastrophically
  - **Memory vs. Granularity**: Finer quantization granularity increases scale factor storage but stabilizes training

- Failure signatures:
  - **Training divergence after 20-50 steps on long rollouts**: Indicates off-policy mismatch (likely using BF16-train-FP8-rollout)
  - **Convergence on easy tasks but failure on hard tasks**: Model confidence is low; quantization noise distorts trajectories (observed in Qwen3-8B-Base on MATH)
  - **Gradual accuracy decline with increasing rollout length**: Cumulative numerical drift between training and rollout distributions
  - **Speedup lower than expected at high TP**: Communication dominates; consider reducing TP or using larger batch sizes

- First 3 experiments:
  1. **Ablation on rollout length**: Train Qwen3-8B-Base on MATH with rollout lengths 4K, 8K, 16K comparing BF16, BF16-train-FP8-rollout, and Jet-RL. Verify Jet-RL maintains <1% degradation at 16K while baseline collapses
  2. **Granularity sensitivity**: Test per-tensor vs. per-group (1Ã—128) vs. per-block (128Ã—128) quantization for activations and weights independently. Measure training loss curve stability and final accuracy
  3. **End-to-end throughput profiling**: Profile each phase (rollout, evaluation, update, sync) on 8B, 14B, 32B models. Measure speedup breakdown: FP8 GEMM acceleration, memory bandwidth reduction, and communication overhead at different TP settings

## Open Questions the Paper Calls Out
- How does Jet-RL scale to larger model sizes (14Bâ€“70B+) in full end-to-end RL training, and how does tensor parallelism degree affect FP8 speedup gains?
- Can the BF16 master weight requirement be eliminated to achieve fully FP8 training without sacrificing stability?
- Does the unified FP8 precision flow generalize to other RL algorithms (e.g., DPO, DAPO, GSPO) beyond PPO and GRPO?
- What is the precise mechanism by which task difficulty amplifies quantization-induced errors during RL training?

## Limitations
- Lack of open-source implementation prevents independent verification of claimed speedup and stability benefits
- Per-group/per-block quantization strategy may not generalize to other model architectures or data distributions
- Does not address potential degradation in edge cases with extreme activation outliers or very low learning rates
- Resource constraints limited full training experiments to 8B models; larger models only benchmarked for offline generation

## Confidence
- **High Confidence**: The mechanism of unified FP8 precision flow eliminating numerical mismatch is well-founded. The theoretical argument for on-policy consistency is robust, and the problem of BF16-train-FP8-rollout instability is well-documented in the RL literature.
- **Medium Confidence**: The per-group (1Ã—128) activation and per-block (128Ã—128) weight quantization strategy is supported by prior work (COAT, DeepSeek-V3) but the specific configuration's optimality for RL training has not been independently verified.
- **Medium Confidence**: The asymmetric storage strategy (FP8 activations, BF16 gradients) is logically sound but lacks external validation beyond the paper's internal experiments.

## Next Checks
1. **Numerical Drift Analysis**: Implement instrumentation to track KL divergence between training and rollout distributions across different precision configurations (BF16, BF16-train-FP8-rollout, Jet-RL) on a fixed task. Quantify the accumulated numerical drift at different rollout lengths.
2. **Cross-Model Generalization**: Test Jet-RL on non-LLM architectures (e.g., transformer-based vision models with RL fine-tuning) to verify that the unified precision flow and quantization strategy generalize beyond the original use case.
3. **Communication Overhead Characterization**: Profile the actual speedup breakdown at different tensor parallelism degrees, separating FP8 GEMM acceleration from memory bandwidth gains and communication overhead to identify scaling bottlenecks.