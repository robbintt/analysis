---
ver: rpa2
title: Diagnosing and Mitigating Modality Interference in Multimodal Large Language
  Models
arxiv_id: '2505.19616'
source_url: https://arxiv.org/abs/2505.19616
tags:
- modality
- multimodal
- interference
- language
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Modality interference in MLLMs occurs when spurious signals from
  non-essential modalities distort model decisions, particularly in unimodal diagnostic
  settings. The core issue stems from MLLMs' inability to fairly evaluate and integrate
  information across modalities, leading to inappropriate reliance on irrelevant modality
  signals.
---

# Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models

## Quick Facts
- **arXiv ID**: 2505.19616
- **Source URL**: https://arxiv.org/abs/2505.19616
- **Reference count**: 40
- **One-line result**: Achieves Pareto-optimal performance by improving both unimodal robustness and multimodal generalization via unified fine-tuning framework

## Executive Summary
Modality interference in Multimodal Large Language Models (MLLMs) occurs when spurious signals from non-essential modalities distort model decisions, particularly in unimodal diagnostic settings. This work proposes a unified fine-tuning framework that combines heuristic and adversarial perturbation-based data augmentation with output-level consistency regularization between original and perturbed inputs. The method consistently improves both unimodal robustness and multimodal generalization across multiple MLLM architectures and scales, achieving over 14% improvement in VQA accuracy while maintaining near-clean performance under modality interference.

## Method Summary
The framework addresses modality interference through a unified fine-tuning approach combining three key components: heuristic data augmentation (adding unrelated facts or misleading descriptions to prompts), adversarial training in embedding space using raw gradients with modality-specific masking, and consistency regularization via JS divergence between original and perturbed output distributions. The total loss combines supervised fine-tuning, adversarial loss, and consistency regularization with a tunable parameter λ. Training uses modest computational overhead (1 epoch, batch size 8, LR 2e-5) and demonstrates effectiveness across multiple architectures including LLaVA and InstructBLIP.

## Key Results
- Achieves over 14% improvement in VQA accuracy while maintaining near-clean performance under modality interference
- Boosts Caltech-101 accuracy from 98.6% to 98.4% under misleading text perturbations
- Outperforms baselines including chain-of-thought prompting and reasoning-enhanced models
- Maintains consistent performance across multiple MLLM architectures and scales

## Why This Works (Mechanism)
The method works by simultaneously training the model to be robust to perturbations in non-relevant modalities while maintaining consistency in predictions. The adversarial training with modality-specific masking prevents noise from destroying semantic content in task-relevant modalities, while consistency regularization ensures stable outputs across original and perturbed inputs. This dual approach addresses the core issue of MLLMs' inability to fairly evaluate and integrate information across modalities, preventing inappropriate reliance on irrelevant modality signals.

## Foundational Learning
- **Adversarial training in embedding space**: Needed to generate realistic perturbations that challenge the model without destroying task-relevant information. Quick check: Verify perturbation magnitude ε is appropriate for model scale.
- **Modality-specific masking**: Essential for restricting adversarial noise to task-irrelevant tokens only. Quick check: Confirm mask M correctly identifies modality boundaries in unified embedding sequences.
- **Consistency regularization via JS divergence**: Provides stability across original and perturbed inputs. Quick check: Monitor JS divergence values during training to ensure they remain meaningful.
- **Data augmentation with heuristic perturbations**: Creates challenging training scenarios that expose modality interference. Quick check: Validate perturbed samples actually contain the intended misleading information.
- **Multi-task learning across modality-heavy datasets**: Ensures framework generalizes across different modality dominance patterns. Quick check: Balance image-heavy and text-heavy samples in each training batch.

## Architecture Onboarding

**Component Map**: Input → Embedding Layer → Modality Mask Application → Adversarial Perturbation → Consistency Loss → Total Loss → Backpropagation

**Critical Path**: The adversarial training step with modality-specific masking is critical - incorrect mask implementation leads to degraded VQA performance (85.9% in No Mask ablation).

**Design Tradeoffs**: The framework trades increased training complexity for robustness gains. Using raw gradients (not signed) provides more nuanced perturbations but requires careful tuning of step size α and perturbation budget ε.

**Failure Signatures**: If VQA accuracy drops significantly under adversarial conditions, the mask M is likely allowing noise into task-relevant tokens. If robustness to misleading text is weak, consistency loss may be insufficient or adversarial step size too small.

**First Experiments**:
1. Verify binary mask M correctly isolates image tokens from text tokens in LLaVA's unified embedding space
2. Test different prompt templates for converting Mini-ImageNet to VQA format and measure impact on performance
3. Conduct sensitivity analysis of λ_cons parameter across different perturbation types

## Open Questions the Paper Calls Out
- **Open Question 1**: Can dynamic attention tracking reveal fine-grained modality reliance patterns beyond the paper's coarse-grained "image-heavy" and "text-heavy" categorization? (Section F explicitly suggests this as a path for deeper insights)
- **Open Question 2**: Is it possible to develop perturbation-agnostic methods that resolve modality interference without relying on bounded adversarial noise or exhaustive heuristic enumeration? (Section F lists this as an "ultimate goal")
- **Open Question 3**: Does the proposed masking and consistency regularization framework extend effectively to settings with more than two modalities (e.g., audio-video-language)? (Framework currently defined only for Vision and Text modalities)

## Limitations
- Exact prompt templates for converting classification datasets to VQA format are not provided, impacting reproducibility
- Binary mask implementation details for identifying modality-specific tokens across different MLLM architectures remain underspecified
- Computational efficiency claims (1 epoch training) may be architecture-dependent and not scale to larger models

## Confidence
- **High Confidence**: Core problem formulation and overall framework design are well-supported
- **Medium Confidence**: Experimental methodology is thorough but implementation details are unclear
- **Low Confidence**: Generalization claims and computational efficiency assessment may be overly optimistic

## Next Checks
1. **Implementation Validation**: Reproduce the binary mask M implementation to ensure it correctly identifies and isolates task-irrelevant modality tokens across both LLaVA and InstructBLIP architectures
2. **Prompt Template Verification**: Test multiple prompt template variants for converting classification datasets to VQA format to confirm reported performance gains are robust to reasonable template variations
3. **Ablation Study Replication**: Independently verify the No Mask ablation result (85.9% accuracy) and systematically explore the sensitivity of consistency regularization to λ_cons across different perturbation types