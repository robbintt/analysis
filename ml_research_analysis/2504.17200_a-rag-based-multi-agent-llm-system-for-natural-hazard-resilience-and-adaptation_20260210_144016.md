---
ver: rpa2
title: A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation
arxiv_id: '2504.17200'
source_url: https://arxiv.org/abs/2504.17200
tags:
- data
- wildfire
- fire
- wildfiregpt
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents WildfireGPT, a retrieval-augmented generation
  (RAG)-based multi-agent large language model (LLM) system designed to support decision-making
  in wildfire risk management. The system uses a user-centered, multi-agent architecture
  to deliver tailored risk insights by integrating natural hazard projections, observational
  datasets, and scientific literature.
---

# A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation

## Quick Facts
- arXiv ID: 2504.17200
- Source URL: https://arxiv.org/abs/2504.17200
- Reference count: 40
- Authors: Yangxinyu Xie, Bowen Jiang, Tanwi Mallick, Joshua David Bergerson, John K. Hutchison, Duane R. Verner, Jordan Branham, M. Ross Alexander, Robert B. Ross, Yan Feng, Leslie-Anne Levy, Weijie Su, Camillo J. Taylor

## Executive Summary
This paper presents WildfireGPT, a retrieval-augmented generation (RAG)-based multi-agent large language model (LLM) system designed to support decision-making in wildfire risk management. The system uses a user-centered, multi-agent architecture to deliver tailored risk insights by integrating natural hazard projections, observational datasets, and scientific literature. Evaluation across ten expert-led case studies showed that WildfireGPT significantly outperformed baseline alternatives, achieving high fidelity in data analysis (98.94% precision) and literature review (100% precision), with strong contextual relevance (96.67–100% success rate across categories). The results highlight WildfireGPT’s potential to bridge the gap between scientific knowledge and actionable, location-specific strategies for diverse stakeholders in natural hazard resilience.

## Method Summary
WildfireGPT employs a three-stage conversational flow: a User Profile Agent extracts structured user constraints (profession, location, timeline), a Planning Agent formulates a step-by-step analysis plan based on the profile, and an Analyst Agent executes the plan by retrieving geospatial data (Fire Weather Index, Census) and literature via semantic search on a curated corpus. The system uses GPT-4 Turbo with function calling for tool integration, FAISS for vector similarity search on literature embeddings, and a Streamlit interface. Evaluation compared performance against ChatClimate and Perplexity across ten case studies, measuring precision, relevance, and contextualization.

## Key Results
- WildfireGPT achieved 98.94% precision in data analysis and 100% precision in literature review.
- Expert-rated contextual relevance was 96.67–100% across five professional categories.
- Ablation study showed profile granularity directly enhanced recommendation specificity for different user roles.
- Outperformed baselines in location-specific data retrieval (9/10 prompts vs. ChatClimate 5/10).

## Why This Works (Mechanism)

### Mechanism 1
A sequential, multi-agent architecture creates a stateful "user context" that significantly improves the relevance of subsequent retrieval and generation compared to single-turn zero-shot prompting. The system decouples context gathering from execution. A User Profile Agent first extracts structured constraints (profession, location, timeline). A Planning Agent then uses this profile to formulate a specific analysis plan. Finally, an Analyst Agent executes this plan. This ensures the retrieval queries are pre-filtered by user intent before the LLM attempts to answer.

### Mechanism 2
Integrating quantitative geospatial data (grid-based projections) directly into the RAG context yields higher factual fidelity than relying on text-based training data or generic search. The Analyst Agent uses functional tools to map user coordinates to specific grid indices (Crossmodel references) to retrieve Fire Weather Index (FWI) data. It retrieves precise numerical values rather than descriptive summaries. The LLM then synthesizes these hard numbers with qualitative literature, grounding its textual recommendations in observed or projected data points.

### Mechanism 3
Dynamically generating literature search queries based on professional identity ("persona-based retrieval") retrieves more actionable evidence than generic semantic search. The Analyst Agent injects professional keywords derived from the user profile into the vector search query (e.g., adding "transmission lines" for a power grid manager vs. "marketable species" for a homeowner). This steers the vector database (FAISS) toward a specific sub-corpus of the literature, ensuring the retrieved abstracts align with the user's domain language.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: The system relies on external data (climate projections, papers) that exceed the LLM's training cutoff and context window. Understanding RAG explains how the system avoids hallucinating fire statistics.
  - Quick check question: Does the system generate the answer from its weights, or does it retrieve a document and ask the LLM to summarize it?

- **Concept: Geospatial Resolution (Grid vs. Point)**
  - Why needed here: The paper highlights the failure of baselines to handle "grid-structured data." Understanding that climate data is often raster (grid) rather than vector (points) explains the need for the specific coordinate-mapping function in the Analyst Agent.
  - Quick check question: How does the system convert a user's "City Name" into a specific set of climate model indices?

- **Concept: Multi-Agent Orchestration**
  - Why needed here: The system isn't one prompt; it's a workflow. Understanding how agents hand off state (Profile -> Plan -> Analysis) is critical to debugging why the system might ask clarification questions before answering.
  - Quick check question: Which agent is responsible for deciding *which* dataset to query: the Profiler or the Planner?

## Architecture Onboarding

- **Component map:**
  User Interface -> Task Orchestrator -> User Profile Agent -> Planning Agent -> Analyst Agent -> Final Synthesis

- **Critical path:**
  User Input -> Profile Extraction (5 turns) -> Planning (User confirms plan) -> Data Retrieval (FWI/Census) -> Literature Retrieval (Vector Search) -> Final Synthesis

- **Design tradeoffs:**
  - Granular Profiling vs. Speed: The system forces a 5-turn profiling phase. This improves relevance (96.67% success) but increases time-to-first-answer compared to zero-shot baselines.
  - Custom Database vs. Live Search: The system uses a curated corpus (Argonne/CIACC) and static datasets (ClimRR). This ensures high fidelity (98.94%) but limits discovery of *new* papers or real-time events compared to Perplexity AI.

- **Failure signatures:**
  - "Could be better" Relevance: Occurs when retrieved data covers the wrong timeline (e.g., RCP 4.5 instead of requested RCP 8.5) or when the literature corpus lacks papers for a specific location (e.g., Virginia vs. West Coast).
  - Tool Hallucination: GPT-4 may generate invalid function calls (typos in JSON) during the Analyst phase, causing silent retrieval failures.
  - Geographic Generalization: If data is missing, the LLM may default to generic regional trends (e.g., "Southwest patterns") instead of admitting ignorance.

- **First 3 experiments:**
  1. Ablation on Profile Depth: Run the same prompt (e.g., "Protect my home from fire") with Profile Agent disabled vs. enabled. Measure the change in specificity of the recommended literature (checking if "defensible space" citations appear).
  2. Grid Mapping Accuracy: Query 10 distinct locations with known FWI values. Verify the `retrieve_fwi_data` function returns the correct Crossmodel indices and values (Ground Truth validation).
  3. Stress Test on Data Gaps: Query a location with known sparse data (e.g., specific RCP scenarios). Analyze if the system acknowledges the gap or extrapolates incorrectly (measuring "Hallucination" vs "Honesty").

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can prediction-powered inference effectively quantify uncertainty in LLM-as-a-judge evaluations to provide statistically valid rankings compared to limited human data?
- Basis in paper: [explicit] The authors note moderate agreement between GPT-4 and human evaluators and suggest that "prediction-powered inference... could quantify uncertainty in model-based evaluations and provide confidence intervals."
- Why unresolved: Current automated evaluation methods (LLM-as-a-judge) struggle to capture the nuanced, utility-driven considerations of domain experts without statistically rigorous validation mechanisms.
- What evidence would resolve it: A statistical framework showing that combining limited expert labels with LLM predictions produces confidence intervals for system performance that narrow and align with ground truth as prediction accuracy improves.

### Open Question 2
- Question: Does integrating spatially informed knowledge graphs (e.g., KnowWhereGraph) into the retrieval process mitigate geographic contextualization failures better than semantic search alone?
- Basis in paper: [explicit] The discussion identifies "geographic contextualization failures" when applying strategies from ecologically dissimilar regions and suggests that "constructing and integrating spatially informed knowledge graphs... would help enable explicit spatial reasoning."
- Why unresolved: The current reliance on general-purpose language models for semantic search leads to mismatches where studies from ecologically distinct regions (e.g., West Coast vs. Virginia) are incorrectly applied to user queries.
- What evidence would resolve it: Comparative tests showing that retrieval augmented with knowledge graphs yields higher location specificity and ecological relevance scores than the current semantic-only approach.

### Open Question 3
- Question: Can implementing a Human-Robot Teaming Framework (HRT-ML) architecture successfully modulate technical depth and detail to improve relevance for diverse stakeholders?
- Basis in paper: [explicit] The paper states that the HRT-ML framework "offers a promising architectural template" to address current gaps where responses may be overly generic for homeowners or insufficiently rigorous for scientists.
- Why unresolved: Current personalization relies heavily on user profiles but lacks a dynamic mechanism to adjust communication styles (e.g., technical jargon vs. detailed explanations) based on inferred user intent and expertise levels.
- What evidence would resolve it: Ablation studies showing that an HRT-ML-enabled agent achieves higher "Accessibility" and "Relevance" scores across diverse professional profiles (e.g., climate scientists vs. urban planners) compared to the baseline agent.

### Open Question 4
- Question: How can the alignment of WildfireGPT’s recommendations with existing expert knowledge be quantified to measure its direct impact on decision-making quality?
- Basis in paper: [explicit] The authors state that while experts noted alignment with their knowledge, "quantifying this alignment and its impact on decision-making remains an open research question."
- Why unresolved: Current evaluation metrics focus on fidelity and relevance, but lack a standardized measure for how effectively the system accelerates decision-making or introduces novel, actionable insights.
- What evidence would resolve it: The development and validation of new metrics that correlate system usage with measurable outcomes, such as the speed of strategy formulation or the adoption rate of recommended mitigation measures in real-world planning.

## Limitations

- User Profile Quality Dependency: The system's high performance hinges on the quality of the structured user profile extracted by the User Profile Agent. If users provide vague or conflicting constraints, the Planning Agent's strategy will be misaligned, degrading subsequent retrieval relevance.
- Generalizability Beyond Wildfire Domain: All empirical results are specific to wildfire risk assessment. The assumption that this sequential multi-agent workflow will generalize to other natural hazards (floods, hurricanes) without retraining or corpus adaptation is untested.
- Static Corpus Constraint: The reliance on a curated, pre-indexed literature corpus (Argonne/CIACC) ensures high fidelity but creates a blind spot for emerging research or real-time events.

## Confidence

- **High Confidence**: The ablation study showing that profile granularity directly enhances recommendation specificity (96.67-100% success rate) and the comparison of precision metrics (98.94% vs. ChatClimate 5/10) are directly supported by the reported experimental data.
- **Medium Confidence**: The claim that the multi-agent architecture creates a stateful "user context" improving relevance over single-turn prompting is logically sound based on the described mechanism, but the paper does not provide quantitative ablation data comparing a single-agent vs. multi-agent version of the same system.
- **Low Confidence**: The assertion that the system will "bridge the gap between scientific knowledge and actionable strategies" for all stakeholder groups is aspirational. The paper only validates this for the five specific professional roles listed in the ablation study, not for the full diversity of potential users in natural hazard resilience.

## Next Checks

1. **Profile Extraction Robustness**: Run the system with intentionally ambiguous user inputs (e.g., "I live near the woods") and measure the frequency of profile extraction failures or the generation of irrelevant analysis plans.
2. **Cross-Hazard Generalization**: Adapt the literature corpus and vector embeddings for a different natural hazard (e.g., coastal flooding) and rerun the ten case studies to assess if the same multi-agent architecture maintains comparable precision and relevance scores.
3. **Real-Time Knowledge Integration**: Implement a mechanism to query live academic databases (e.g., Semantic Scholar API) alongside the static corpus, and measure the change in precision and the latency of responses for time-sensitive queries.