---
ver: rpa2
title: Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs
arxiv_id: '2509.13664'
source_url: https://arxiv.org/abs/2509.13664
tags:
- question
- aens
- neurons
- ambiguity
- steering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work identifies a sparse set of neurons that encode question
  ambiguity signals in large language models. Using linear probes, the authors find
  that ambiguity is linearly separable in model activations, often concentrated in
  as few as one neuron per model.
---

# Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs

## Quick Facts
- arXiv ID: 2509.13664
- Source URL: https://arxiv.org/abs/2509.13664
- Reference count: 40
- Key outcome: Identifies sparse neurons that encode question ambiguity in LLMs; enables high-accuracy ambiguity detection and causal steering toward abstention.

## Executive Summary
This work identifies a sparse set of neurons that encode question ambiguity signals in large language models. Using linear probes, the authors find that ambiguity is linearly separable in model activations, often concentrated in as few as one neuron per model. These "Ambiguity-Encoding Neurons" (AENs) generalize across datasets and models, and outperform prompting-based baselines in ambiguity detection. Activation steering on AENs enables causal control over model behavior, shifting responses from direct answers to abstentions. Layerwise analysis shows AENs emerge in early transformer layers. The approach achieves strong empirical results with high probe accuracy and efficient abstention control, offering a sparse, interpretable mechanism for detecting and steering ambiguity-aware behavior in LLMs.

## Method Summary
The method uses linear probing to identify whether ambiguity is linearly separable in hidden states, then isolates the most salient neurons via probe weight magnitude and noise-injection validation. Sparse probes using only these neurons confirm functional sufficiency. Activation steering constructs a contrastive direction between abstention and answering behaviors, masked to AENs, and adds it to hidden states at a chosen layer to induce behavioral shifts. Evaluation uses an LLM-as-judge to classify responses as abstention or direct answer.

## Key Results
- Question ambiguity is linearly separable in hidden states with probe accuracy 90-97% across models and datasets.
- As few as one neuron per model can encode ambiguity signals, confirmed via noise-injection validation.
- AEN-masked steering achieves higher abstention rates than full-vector steering baselines.
- AENs emerge in early transformer layers (as early as layer 2) with accuracy saturating before deeper layers.

## Why This Works (Mechanism)

### Mechanism 1: Linear Separability of Semantic Features
- Claim: Question ambiguity is linearly accessible in hidden state representations, enabling detection with simple classifiers.
- Mechanism: LLMs organize conceptual distinctions (ambiguous vs. clear) along learnable linear directions in activation space. A logistic regression probe on mean-pooled hidden states achieves high accuracy (90–97%), indicating that a hyperplane can separate these categories.
- Core assumption: Ambiguity constitutes a coherent, model-internal semantic feature rather than a collection of unrelated patterns.
- Evidence anchors:
  - [abstract] "question ambiguity is linearly encoded in the internal representations of LLMs"
  - [section 4.2] Table 1 reports macro accuracy 90.65–97.10% across models and datasets
  - [corpus] Prior work (Gurnee et al., 2023; Geva et al., 2022) supports the linear abstraction hypothesis for high-level features
- Break condition: If ambiguity were distributed nonlinearly (e.g., requiring polynomial probes), linear probes would fail and AENs would not localize cleanly.

### Mechanism 2: Sparse Neuron-Level Encoding
- Claim: Ambiguity signals concentrate in a tiny subset of neurons (1–3), not diffusely across dimensions.
- Mechanism: Probe weight magnitudes rank neuron salience; Gaussian noise injection on top-k neurons causes sharp accuracy drops (see Figure 3), indicating functional concentration. Sparse probes using only these neurons recover most of the full-probe performance.
- Core assumption: Individual neurons or small subspaces can causally encode specific high-level features.
- Evidence anchors:
  - [abstract] "a small number of neurons, as few as one, encode question ambiguity information"
  - [section 4.2] Figure 3 shows perturbing top-1 to top-3 neurons drops accuracy sharply
  - [corpus] Weak direct support; related work focuses on distributed features or different tasks
- Break condition: If perturbing top neurons had minimal effect or sparse probes performed poorly, the sparsity claim would be undermined.

### Mechanism 3: Early-Layer Semantic Abstraction
- Claim: Ambiguity signals emerge in shallow transformer layers (before layer 5), enabling early intervention.
- Mechanism: Layerwise probing shows accuracy saturates in early layers (Figure 7), suggesting the model abstracts ambiguity before deeper processing and generation.
- Core assumption: Early layers capture semantic-level distinctions, not just syntactic patterns.
- Evidence anchors:
  - [abstract] "AENs emerge from shallow layers, suggesting early encoding of ambiguity signals"
  - [section 4.4.1] "AENs probe accuracy surpasses 90% as early as Layer 2" in Gemma 7B IT
  - [corpus] Related work does not directly address early emergence for ambiguity; evidence is paper-specific
- Break condition: If ambiguity required deep-layer integration or late-layer context, early-layer probes would show low accuracy.

## Foundational Learning

- Concept: Linear Probing
  - Why needed here: Determines whether a feature is linearly accessible in activations; foundational to localizing AENs.
  - Quick check question: If you train a logistic regression probe on layer-14 mean-pooled hidden states and achieve 93% accuracy, what does that imply about where the feature is encoded?

- Concept: Activation Steering
  - Why needed here: Tests causality—can modifying specific neurons change behavior (e.g., from answering to abstention)?
  - Quick check question: Given a steering vector v and scaling α, write the intervention: what operation modifies the hidden state h at layer ℓ?

- Concept: Sparsity via Noise Injection
  - Why needed here: Validates that top-ranked neurons are functionally critical, not just correlated features.
  - Quick check question: If adding Gaussian noise to 3 neurons drops probe accuracy by 40 points, but noise on 50 other neurons drops it by 5 points, what inference can you draw?

## Architecture Onboarding

- Component map:
  - Probing pipeline: extract hidden states → mean-pool → train logistic regression → rank neurons by |w_i| → identify top-k as AENs.
  - Steering pipeline: construct contrastive activations (abstention vs. answer) → mean-center → PCA for direction → mask to AENs → add α·(mask ⊙ Δ) to hidden states at layer ℓ.
  - Evaluation: LLM-as-judge to classify response as abstention or direct answer; compute abstention rate change.

- Critical path:
  1. Train linear probe on ambiguous vs. clear examples.
  2. Identify AENs via noise-injection validation (k where ∆acc(k) is maximized).
  3. Train sparse probe using only AENs to confirm sufficiency.
  4. Construct steering vector via contrastive PCA on abstention vs. answering examples.
  5. Apply AEN-masked steering at layer ℓ (default 14) and measure abstention rate shift.

- Design tradeoffs:
  - Layer choice: Earlier layers (≤5) capture ambiguity but may lack context; later layers (≈14) integrate more information but include noise from other features.
  - k selection: Smaller k yields higher per-neuron efficiency but lower absolute effect; larger k increases absolute abstention rate with diminishing returns (Table 3).
  - Steering strength α: Higher α increases abstention rate but risks oversteering and response degradation.

- Failure signatures:
  - Low probe accuracy (<70%): feature may not be linearly accessible or layer is wrong.
  - Sparse probe underperforms full probe by >10%: AENs do not capture sufficient signal.
  - Steering causes nonsensical outputs or excessive refusals: α is too high or steering vector is noisy.
  - Cross-dataset steering fails: AENs may be dataset-specific artifacts rather than generalizable features.

- First 3 experiments:
  1. Replicate linear probing at layer 14 on AmbigQA for one model (e.g., LLaMA 3.1 8B); verify accuracy matches paper (~90%).
  2. Perform noise injection on top-k neurons; confirm accuracy drops sharply for k=1–3, identifying AENs.
  3. Apply AEN-masked steering with α=1–3 on a held-out ambiguous set; measure abstention rate increase and compare to full-vector steering baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does question ambiguity emerge and evolve at the token level during model processing?
- Basis in paper: [explicit] "Looking ahead, an important direction for future work is to extend this analysis to the token level to see how ambiguity arises within a question and how it influences model uncertainty."
- Why unresolved: Current analysis uses mean-pooled activations over the full sequence, obscuring how individual tokens contribute to ambiguity signals and when within a question the model recognizes ambiguity.
- What evidence would resolve it: Token-level probing across sequence positions, tracking when ambiguity signals first emerge and which tokens most influence AEN activations.

### Open Question 2
- Question: Do Ambiguity-Encoding Neurons generalize across diverse architectures beyond the three instruction-tuned models tested?
- Basis in paper: [explicit] "broader validation on diverse architectures and tasks is needed to assess generality"
- Why unresolved: Only LLaMA 3.1 8B, Mistral 7B, and Gemma 7B were evaluated; findings may not transfer to larger models, different architectures, or non-instruction-tuned models.
- What evidence would resolve it: AEN identification and steering experiments on additional architectures (e.g., GPT-style, encoder-decoder models) and scale ranges (1B–100B+ parameters).

### Open Question 3
- Question: How do AENs interact with or compete against other encoded semantic features (e.g., topic, sentiment, factual knowledge)?
- Basis in paper: [inferred] The paper demonstrates AEN sparsity and causal efficacy but does not investigate whether steering ambiguity signals disrupts other functional representations.
- Why unresolved: Noise injection and steering experiments focus narrowly on ambiguity classification; potential interference with co-occurring semantic representations remains untested.
- What evidence would resolve it: Multi-task probing after AEN steering to assess preservation of other semantic features, and analysis of neuron overlap between AENs and other identified functional neurons.

## Limitations

- Sparsity claim depends on unspecified noise injection variance; results may vary with different σ values.
- Steering scale α is left to empirical tuning without reported optimal values, affecting reproducibility.
- Cross-dataset generalization is shown but potential dataset overlap is not discussed, raising concerns about true abstraction.

## Confidence

- **High**: Linear separability of ambiguity (supported by >90% probe accuracy); early-layer emergence (confirmed by Figure 7); steering causality (AEN-masked steering increases abstention over full-vector baseline).
- **Medium**: Sparsity of encoding (noise-injection validation shows functional concentration but lacks reported variance and statistical tests); cross-dataset generalization (qualitative overlap shown, no quantitative comparison).
- **Low**: Interpretability of individual neurons (no semantic analysis of neuron activations beyond rank weights); biological plausibility of sparsity (no comparison to known sparse coding theories in neuroscience).

## Next Checks

1. Re-run noise injection with multiple σ values (e.g., 0.1, 1.0, 2.0) and plot accuracy drop curves to verify that the top-1 neuron consistently dominates functional importance.
2. Perform ablation: compare AEN-masked steering with random neuron masking at same k; confirm that only AEN masking produces large abstention shifts.
3. Test steering generalization by training on AmbigQA but evaluating abstention control on a third, unseen ambiguity dataset (e.g., a held-out subset of SQuAD with ambiguous questions).