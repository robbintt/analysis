---
ver: rpa2
title: 'CompanionCast: A Multi-Agent Conversational AI Framework with Spatial Audio
  for Social Co-Viewing Experiences'
arxiv_id: '2512.10918'
source_url: https://arxiv.org/abs/2512.10918
tags:
- agent
- agents
- viewing
- multi-agent
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CompanionCast, a multi-agent conversational
  AI framework with spatial audio for enhancing social co-viewing experiences. The
  system orchestrates role-specialized AI agents that respond to video content using
  multimodal inputs and provides real-time companionship during media consumption.
---

# CompanionCast: A Multi-Agent Conversational AI Framework with Spatial Audio for Social Co-Viewing Experiences

## Quick Facts
- arXiv ID: 2512.10918
- Source URL: https://arxiv.org/abs/2512.10918
- Reference count: 10
- Primary result: Multi-agent conversational AI framework with spatial audio and LLM-as-Judge quality control for enhanced social co-viewing experiences

## Executive Summary
This paper introduces CompanionCast, a multi-agent conversational AI framework designed to recreate social co-viewing experiences through role-specialized AI agents that respond to video content in real-time. The system orchestrates three distinct personas (enthusiastic supporter, analytical commentator, humorous antagonist) that engage viewers during sports events using multimodal inputs and spatial audio positioning. A key innovation is the LLM-as-a-Judge module that iteratively scores and refines conversations across five dimensions before presentation to users. The framework was validated through a pilot study with soccer fans, demonstrating improved perceived social presence compared to solo viewing, though technical limitations like response latency and speech recognition errors affected user experience.

## Method Summary
The framework processes video content using SoccerNet datasets for captions and event detection, triggering conversations during important moments and replays. Three role-specialized agents (Claude Sonnet 4, temp=0.7) generate responses using a 60-second rolling context window, with conversations refined through 1-3 rounds of LLM-as-Judge evaluation (GPT-4o, temp=0.2) across relevance, authenticity, engagement, diversity, and personality consistency dimensions. Text-to-speech synthesis via ElevenLabs with spatial audio positioning creates auditory separation between agents. The system includes conversation frequency constraints (30-second minimum separation, 15s for high-intensity moments) and was evaluated with 15 soccer fans using a 5-point Likert questionnaire measuring enjoyment, immersion, and social presence.

## Key Results
- Multi-agent interaction enhanced engagement compared to solo viewing, with users reporting improved social presence (3-4 on 5-point scales)
- The LLM-as-Judge iterative refinement pipeline improved conversation quality across all five evaluation dimensions
- Spatial audio positioning contributed to perceived co-presence, though technical limitations like response latency (5-10+ seconds) and speech recognition errors affected immersion

## Why This Works (Mechanism)

### Mechanism 1
Role-specialized multi-agent orchestration may recreate diverse social dynamics found in natural group co-viewing. Multiple agents with distinct personas provide complementary social roles that a single agent cannot fulfill, creating conversational tension and interpretive variety. The core assumption is that users perceive greater social presence when agent personalities are differentiated and behaviorally consistent, rather than homogenized. Evidence anchors include the abstract's description of "multiple role-specialized AI agents" and section 2.2's discussion of complementary agent roles. If personalities blur or responses become repetitive, the illusion of group dynamics collapses.

### Mechanism 2
The LLM-as-Judge iterative refinement pipeline appears to improve conversation quality across multiple dimensions before user presentation. An evaluator agent (GPT-4o with temperature=0.2) scores conversations on relevance, authenticity, engagement, diversity, and personality consistency (0-10 scale), providing qualitative feedback that guides 1-3 rounds of agent refinement before output. The core assumption is that lower temperature and structured evaluation criteria produce more reliable quality assessments than unguided generation. Evidence anchors include the abstract's mention of "LLM-as-a-Judge module that iteratively scores and refines conversations" and section 3.2's description of multi-round conversation protocols. If evaluator feedback is too generic or refinement loops introduce unacceptable latency, the mechanism degrades.

### Mechanism 3
Spatial audio positioning with distinct voice profiles likely enhances perceived co-presence by simulating physical distribution of companions. Each agent is assigned a unique voice (via ElevenLabs) and spatial position around the user, creating auditory separation that mirrors real-world watch party dynamics. The core assumption is that spatial differentiation increases the sense of shared physical space, even when visual avatars are absent. Evidence anchors include the abstract's mention of "spatial audio" and section 2.1's discussion of spatial audio in video meetings. If spatial cues are poorly calibrated or voices sound similar, the effect diminishes.

## Foundational Learning

- Concept: **Multi-agent LLM orchestration (Autogen/CAMEL-style frameworks)**
  - Why needed here: The system coordinates 3+ agents with shared context but independent generation; understanding message passing, context windows, and turn-taking protocols is essential.
  - Quick check question: Can you explain how a rolling context cache is shared across agents while maintaining persona isolation?

- Concept: **LLM-as-judge evaluation paradigms**
  - Why needed here: Quality control depends on structured evaluation across five dimensions; understanding prompt design for evaluators and feedback integration is critical.
  - Quick check question: How would you design an evaluator prompt that scores "personality consistency" objectively?

- Concept: **Spatial audio rendering basics**
  - Why needed here: Agent voices must be spatially positioned; understanding head-related transfer functions (HRTFs) or binaural rendering helps debug presence issues.
  - Quick check question: What happens to spatial perception if two agents are assigned overlapping spatial positions?

## Architecture Onboarding

- Component map: Content Processing (Video → Captions → Event detection → Context cache) → Agent Orchestration (3 agents → Conversation generation) → Evaluator Pipeline (GPT-4o → Scoring → Refinement) → Audio Rendering (TTS → Spatial positioning → Output)

- Critical path: Event detection latency → Agent generation → Evaluator refinement rounds → TTS synthesis → Spatial rendering. Total pipeline latency determines whether responses feel real-time or delayed.

- Design tradeoffs:
  - More refinement rounds improve quality but increase latency (paper notes response lag as a limitation)
  - More agents increase social richness but raise coordination complexity and token costs
  - Higher conversation frequency increases engagement but risks interrupting viewing flow (30-second minimum separation constraint)

- Failure signatures:
  - Timing desync: Agent responses arrive 5-10+ seconds after the event they reference (noted by participants)
  - Speech recognition errors: Domain-specific terms (player/team names) misrecognized (noted by participants)
  - Personality drift: Agents converge in tone or forget assigned personas over long sessions
  - Evaluator feedback loops: Refinement produces generic responses rather than persona-specific improvements

- First 3 experiments:
  1. Latency profiling: Instrument the full pipeline (event detection → audio output) with timestamps at each stage; identify the bottleneck (likely LLM generation or TTS synthesis).
  2. Evaluator calibration: Compare evaluator scores against human ratings for a fixed set of conversations; check correlation across all 5 dimensions.
  3. Ablation study: Run the system with 1, 2, and 3 agents to measure the marginal contribution of agent count to perceived social presence (using the questionnaire from Appendix A).

## Open Questions the Paper Calls Out

### Open Question 1
Can multi-agent conversational systems recreate the social presence of shared viewing experiences? Stated as a fundamental research question on page 2, this remains unresolved as the pilot study with only 2 participants showed moderate social presence ratings (3-4 on 5-point scales), indicating partial success but not full replication of human co-viewing. Larger-scale user studies with diverse participant populations showing social presence ratings comparable to human co-viewing conditions would resolve this question.

### Open Question 2
Does the LLM-as-judge pipeline improve conversational quality across different content domains beyond sports? Stated as fundamental research question on page 2, the framework was designed for generalizability but only validated in sports viewing. The evaluator-agent pipeline was tested only on soccer content; its effectiveness for movies, documentaries, education, or other domains remains unverified. Comparative studies evaluating conversation quality scores across multiple content domains with and without the evaluator pipeline would resolve this question.

### Open Question 3
What is the optimal number and configuration of AI agents for different viewing contexts and user preferences? Inferred from the paper's use of 3 agents with fixed roles (DieHard_fan, Analyst_fan, Comedian_fan) without investigating whether this is optimal or how to adapt configurations. No systematic comparison of agent count, role combinations, or personality balance was conducted; participants requested "greater user control over agent conversation frequency and personality balance." Ablation studies varying agent number and role configurations, measuring user engagement and social presence outcomes would resolve this question.

## Limitations

- Small pilot sample (n=15) with self-selected soccer fans limits generalizability across demographics and content types
- Response latency (5-10+ seconds) and speech recognition errors on domain-specific terms detract from immersion and real-time co-viewing parity
- LLM-as-Judge pipeline lacks independent validation—evaluator scores are not compared against human ratings
- Absence of visual avatars limits multimodal social presence cues, potentially reducing effectiveness compared to embodied co-viewing systems

## Confidence

- **High Confidence**: The architectural feasibility of role-specialized multi-agent orchestration with spatial audio (demonstrated by working prototype and alignment with established frameworks like Autogen)
- **Medium Confidence**: The LLM-as-Judge iterative refinement improves conversation quality (supported by mechanism description but lacks independent evaluator validation)
- **Medium Confidence**: Spatial audio positioning enhances perceived co-presence (supported by related work on auditory embodiment but not directly validated in this study)
- **Low Confidence**: The framework generalizes to non-sport content and diverse demographics (extrapolated from pilot but not tested)

## Next Checks

1. **Evaluator Calibration Study**: Compare GPT-4o evaluator scores against human ratings for 50+ conversations across all five dimensions; calculate inter-rater reliability and identify scoring biases.

2. **Latency Impact Analysis**: Conduct A/B testing with real-time vs. delayed agent responses (0s vs. 5s vs. 10s latency); measure effects on immersion, social presence, and conversation relevance using validated presence scales.

3. **Cross-Content Generalization**: Deploy the framework with three content types (sports, drama series, documentary); compare engagement metrics and user feedback across content categories to assess domain adaptability.