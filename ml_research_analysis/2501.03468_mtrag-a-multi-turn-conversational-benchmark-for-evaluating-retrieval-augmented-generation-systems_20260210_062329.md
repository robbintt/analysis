---
ver: rpa2
title: 'MTRAG: A Multi-Turn Conversational Benchmark for Evaluating Retrieval-Augmented
  Generation Systems'
arxiv_id: '2501.03468'
source_url: https://arxiv.org/abs/2501.03468
tags:
- question
- retrieval
- questions
- turn
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MTRAG, a multi-turn conversational benchmark
  for evaluating retrieval-augmented generation (RAG) systems. The benchmark addresses
  the challenge of evaluating RAG systems in multi-turn conversations, which is often
  overlooked but presents additional complexities.
---

# MTRAG: A Multi-Turn Conversational Benchmark for Evaluating Retrieval-Augmented Generation Systems

## Quick Facts
- arXiv ID: 2501.03468
- Source URL: https://arxiv.org/abs/2501.03468
- Reference count: 23
- Benchmark evaluates multi-turn RAG systems across 110 human-generated conversations spanning four domains

## Executive Summary
This paper introduces MTRAG, a multi-turn conversational benchmark for evaluating retrieval-augmented generation (RAG) systems. The benchmark addresses the challenge of evaluating RAG systems in multi-turn conversations, which is often overlooked but presents additional complexities. MTRAG contains 110 human-generated conversations across four domains, with an average of 7.7 turns each, totaling 842 tasks. The benchmark is designed to be diverse across several dimensions, including question types, multi-turn patterns, and answerability. The authors evaluate both retrieval and generation components of RAG systems using MTRAG. Their results show that even state-of-the-art LLMs struggle on this benchmark, especially on unanswerable questions and in later turns. They also explore automation paths via synthetic data and LLM-as-a-Judge evaluation.

## Method Summary
MTRAG is constructed through human annotation of conversations across four domains, with each conversation containing an average of 7.7 turns. The benchmark includes 842 tasks derived from these conversations. The evaluation framework assesses both retrieval (using metrics like Recall@5) and generation (using FANC scores: Faithfulness, Appropriateness, Naturalness, Completeness) components. The authors employ an IDK ("I Don't Know") judge to handle unanswerable questions, and explore synthetic data generation as an automation path. The benchmark specifically tests active retrieval (retrieving at every turn) versus static retrieval, and evaluates the effectiveness of query rewriting for non-standalone questions.

## Key Results
- State-of-the-art LLMs struggle significantly on MTRAG, especially on unanswerable questions and later conversation turns
- Retrieval performance degrades substantially in later turns (Recall@5 drops from 0.89 on Turn 1 to 0.47 on later turns)
- Query rewriting dramatically improves retrieval performance compared to using raw conversation history or last turn alone
- Models exhibit high hallucination rates on unanswerable questions, with GPT-4o's faithfulness score dropping from 0.66 to 0.20 when IDK conditioning is applied

## Why This Works (Mechanism)

### Mechanism 1: Contextual Query Rewriting
If a multi-turn query is rewritten to be standalone before retrieval, retrieval performance improves significantly compared to using the raw conversation history or the last turn alone. Query rewriting uses an LLM to resolve coreferences and dependencies, converting "it" or "he" into explicit entities, thereby aligning the query embedding with the correct document chunks. Core assumption: The LLM performing the rewriting can accurately infer the user's intent and missing context from the conversation history. Evidence: Using the full conversation under-performed; the most effective strategy was just using the last user turn with query rewriting. Break condition: If the conversation context exceeds the rewriting model's window, or if the rewriting model hallucinates context, retrieval accuracy will degrade.

### Mechanism 2: IDK-Conditioned Evaluation
Standard evaluation metrics fail in multi-turn RAG unless they explicitly condition scores on whether the model admits inability to answer (IDK). Unanswerable questions often trigger hallucinations in LLMs. Without a specific "IDK judge" to detect refusals, standard metrics may penalize a correct refusal ("I don't know") because it lacks lexical overlap with a non-existent answer, or reward a fluent hallucination. The mechanism isolates "unanswerable" cases and scores them 1 if the model refuses and 0 otherwise. Core assumption: The "IDK judge" classifier is highly accurate at distinguishing between a refusal and a valid answer. Evidence: We employ an IDK judge... words used to indicate not knowing the answer may not match the context. Break condition: If the IDK judge misclassifies a partial answer as a full refusal, or vice versa, the aggregate metric scores become unreliable.

### Mechanism 3: Active Retrieval Induces Passage Diversity
Forcing the system to retrieve at every turn (active retrieval) creates a challenging benchmark where passage diversity increases, exposing the generator's inability to synthesize information across disjoint contexts. Unlike static retrieval (one search per conversation), active retrieval updates the context window with new passages at every turn. This prevents the generator from relying on "memorized" answers or a single static context, forcing it to ground responses in potentially shifting evidence. Core assumption: The user's questions shift sufficiently in topic or focus to trigger new retrieval results, rather than re-using the same top passages. Evidence: Annotators modify the passage set to improve relevance and diversity, resulting in 16.9 unique passages per conversation. Break condition: If the retriever fails to find new relevant passages for follow-up questions, the "active" mechanism reverts to a static context problem.

## Foundational Learning

- **Concept:** Contextual Query Rewriting (CQR)
  - **Why needed here:** Multi-turn queries like "What about his powers?" are non-standalone. Without CQR, a retriever searching for "powers" might miss the entity "Doctor Strange" if that context isn't injected into the query vector.
  - **Quick check question:** Does your retriever performance drop significantly when comparing Turn 1 (standalone) to Turn > 1 (context-dependent)?

- **Concept:** Hallucination vs. Refusal Trade-off
  - **Why needed here:** The paper highlights that SOTA models often hallucinate rather than admit ignorance (low IDK accuracy). Evaluating RAG requires distinguishing between "wrong answer" and "refusal to answer."
  - **Quick check question:** How does your evaluation metric handle a model response of "I do not have specific information" for a question that is actually unanswerable?

- **Concept:** FANC Properties (Faithfulness, Appropriateness, Naturalness, Completeness)
  - **Why needed here:** This is the rubric used to grade the "quality" of a response. It moves beyond simple token overlap (ROUGE) to semantic correctness.
  - **Quick check question:** Can a model be "Faithful" (grounds in passages) but "Incomplete" (misses a detail)? (Yes).

## Architecture Onboarding

- **Component map:** Query Rewriter -> Retriever -> Generator -> Evaluator
- **Critical path:** User Query (Turn N) -> Query Rewriter (History + Current Turn -> Standalone Query) -> Retriever (Index -> Top-5 Passages) -> Passages + History -> Generator -> Response -> Response + Ground Truth -> Evaluator -> Performance Score
- **Design tradeoffs:**
  - Using a smaller model (e.g., Mixtral 8x7B) for rewriting is faster but might miss complex context compared to a larger model
  - `RBalg` (algorithmic) is cheaper and interpretable but brittle; `RBllm` (LLM-as-judge) correlates better with humans but is expensive and potentially biased
  - Human-generated data (MTRAG) is high quality but expensive/static. Synthetic data (MTRAG-S) is scalable but tends to be shorter and less diverse
- **Failure signatures:**
  - Retrieval Recall@5 drops below 0.50 in later turns
  - Models generate detailed answers for "Unanswerable" questions instead of refusing
  - Models generate plausible-sounding but unsupported claims
- **First 3 experiments:**
  1. Run the benchmark using "Last Turn Only" vs. "Query Rewrite" to measure the retrieval lift on later turns
  2. Evaluate your best generator on the "Unanswerable" subset only to measure the hallucination rate (IDK accuracy)
  3. Compare `RBalg` scores against human preference (Win-Rate) to see if algorithmic metrics suffice for your domain or if you need an LLM Judge

## Open Questions the Paper Calls Out
None

## Limitations
- Human-generated conversation data covers only four domains and may not generalize to specialized or technical fields
- Synthetic data generation produces notably shorter and less diverse conversations than human-annotated ones
- LLM-as-a-judge evaluation introduces potential bias from the judge model itself
- The benchmark's focus on open-domain conversations may not fully represent task-oriented dialogue systems

## Confidence
- **High confidence** in the retrieval performance degradation observation (Recall@5 dropping from 0.89 to 0.47 across turns)
- **Medium confidence** in the query rewriting effectiveness, as gains depend heavily on the rewriting model's capabilities
- **Medium confidence** in the IDK-conditioned evaluation framework, as the approach relies on potentially brittle classification boundaries
- **Low confidence** in the synthetic data as a complete substitute for human annotations, given the substantial quality gap

## Next Checks
1. Apply MTRAG evaluation to conversations from specialized domains (medical, legal, technical support) to assess whether observed performance patterns hold
2. Conduct a fine-grained error analysis categorizing hallucinations by type (entity confabulation, relationship invention, attribute hallucination)
3. Test the IDK detection mechanism across multiple judge models with varying sizes and training approaches to quantify variance in IDK classification