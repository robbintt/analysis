---
ver: rpa2
title: Faster MoE LLM Inference for Extremely Large Models
arxiv_id: '2505.03531'
source_url: https://arxiv.org/abs/2505.03531
tags:
- experts
- zhang
- wang
- expert
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores inference efficiency in fine-grained Mixture-of-Experts
  (MoE) language models. It identifies that while MoE reduces compute compared to
  dense models, it faces greater memory I/O bottlenecks, particularly under low and
  high concurrency.
---

# Faster MoE LLM Inference for Extremely Large Models

## Quick Facts
- arXiv ID: 2505.03531
- Source URL: https://arxiv.org/abs/2505.03531
- Reference count: 40
- Reducing activated experts improves throughput with minimal performance loss

## Executive Summary
This work investigates inference efficiency optimization for fine-grained Mixture-of-Experts (MoE) language models, identifying memory I/O bottlenecks as a key constraint. The study evaluates expert skipping (reducing activated experts) and expert pruning (reducing total experts) strategies. Expert skipping yields significant throughput gains—up to 50% acceleration at low concurrency—with only minor performance degradation, while expert pruning provides moderate acceleration but causes severe performance loss. The authors conclude that fine-grained MoE models present substantial opportunities for inference optimization through selective expert activation.

## Method Summary
The paper evaluates two strategies for MoE inference optimization: expert skipping and expert pruning. Expert skipping reduces the number of activated experts (na) per layer while keeping total experts (ne) constant, using layer-wise patterns defined by (b,h,e,p) tuples for linear interpolation. Expert pruning reduces the total number of experts before inference using strategies like random removal, structured selection (odd/even/half), and soft/hard count methods based on expert usage frequency. The study tests DeepSeek-V2-Lite and DeepSeek-V3 models across various concurrency levels (2-784) and evaluates performance on benchmark suites including ARC-Easy, ARC-Challenge, BoolQ, OpenBookQA, RTE, and Winogrande.

## Key Results
- Reducing activated experts from 8 to 2 in DeepSeek-V3 improves throughput by at least 10% with no performance loss
- Expert skipping provides up to 50% acceleration at low concurrency with minimal degradation
- Expert pruning yields up to 2.3× speedup but causes severe performance degradation
- Optimal expert skipping strategy differs between softmax-based (DeepSeek-V2) and sigmoid-based (DeepSeek-V3) routers

## Why This Works (Mechanism)

### Mechanism 1: Expert Skipping Reduces Memory I/O Bottlenecks at Low Concurrency
- Claim: Reducing activated experts increases throughput at low concurrency by lowering memory I/O pressure
- Mechanism: At low concurrency, fewer active experts mean fewer expert parameter loads per token, directly reducing memory I/O pressure and latency
- Core assumption: System bottleneck at low concurrency is memory bandwidth, not compute capacity
- Evidence: Reducing na yields significant throughput gains—up to 50% acceleration at low concurrency
- Break condition: If system is compute-bound rather than I/O-bound at low concurrency, throughput gains will be smaller

### Mechanism 2: Expert Skipping Improves Compute Efficiency at High Concurrency
- Claim: Reducing na increases throughput at high concurrency by decreasing computational load
- Mechanism: At high concurrency, inference shifts to compute-bound regime; fewer active experts reduce FLOPs per token
- Core assumption: System transitions from I/O-bound to compute-bound as concurrency increases
- Evidence: Fine-grained MoE models present optimization opportunities through selective expert activation
- Break condition: If system remains I/O-bound even at high concurrency, compute reduction won't translate to throughput gains

### Mechanism 3: Expert Pruning Improves Throughput but Severely Degrades Performance
- Claim: Reducing total experts increases throughput but causes substantial performance degradation
- Mechanism: Fewer experts increase computational intensity per expert, improving hardware utilization, but removing critical experts leads to significant model quality loss
- Core assumption: Expert importance is uneven; some experts are far more critical than others
- Evidence: Reducing total experts provides moderate acceleration but causes severe performance degradation
- Break condition: If experts are homogeneous and interchangeable, pruning could be effective with minimal loss

## Foundational Learning

- **Mixture-of-Experts (MoE) Architecture**
  - Why needed: Paper manipulates MoE components (activated experts, total experts); understanding MoE structure is prerequisite
  - Quick check: What is the key difference between coarse-grained and fine-grained MoE architectures?

- **Memory I/O vs. Compute-Bound Inference**
  - Why needed: Paper's claims depend on whether inference is limited by memory bandwidth or compute, which varies with concurrency
  - Quick check: How does increasing concurrency shift a system from I/O-bound to compute-bound?

- **Expert Routing and Activation**
  - Why needed: Paper modifies number of activated experts; understanding routing is necessary to predict impact
  - Quick check: In an MoE model, how does the router determine which experts are activated for a given token?

## Architecture Onboarding

- Component map: Input -> Embedding -> Stacked Transformer Layers (each with Attention, Shared Expert, Routed Experts via Router) -> Output Head
- Critical path: MoE layer is often bottleneck due to parameter loading and expert dispatch; at low concurrency, I/O for expert weights dominates; at high concurrency, compute across many experts dominates
- Design tradeoffs: Expert skipping offers throughput gains with minor performance loss, especially at low/high concurrency, but less effective at moderate concurrency; expert pruning reduces memory footprint and can boost throughput but severely risks model performance
- Failure signatures: No throughput gain from expert skipping suggests system not in expected I/O-bound or compute-bound state; severe performance collapse after pruning indicates critical experts were removed
- First 3 experiments:
  1. On fine-grained MoE model, measure throughput and latency at low, medium, and high concurrency while varying na, keeping ne fixed
  2. For each na setting, evaluate performance on standard benchmarks to quantify performance degradation
  3. For fixed concurrency, apply expert pruning using structured method versus random removal, then measure throughput and performance to compare trade-offs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a universal, transferable expert skipping strategy be developed, or is the optimal strategy strictly dependent on specific model architectures?
- **Basis:** Authors observe opposite trends between DeepSeek-V2 (descending strategy) and V3 (ascending strategy), suggesting "a universal skipping strategy may not exist"
- **Why unresolved:** Study empirically tested various static shapes but didn't find single pattern optimizing both architectures, nor establish theoretical rule for predicting optimal shape
- **What evidence would resolve it:** Theoretical framework linking router activation functions to layer-wise expert importance, or adaptive algorithm that automatically identifies optimal allocation

### Open Question 2
- **Question:** How can expert skipping be effectively applied to sigmoid-gated fine-grained MoE models without inducing performance instability?
- **Basis:** DeepSeek-V3 exhibits "greater instability" during skipping compared to softmax-based models because weights tend to polarize toward 0 or 1
- **Why unresolved:** Current method of reducing TopK count fails to account for weight magnitude distribution in sigmoid routers, leading to unpredictable degradation
- **What evidence would resolve it:** Skipping mechanism incorporating weight magnitude thresholds rather than just rank-based selection, demonstrating stable performance on sigmoid-gated models

### Open Question 3
- **Question:** Is there a pruning method for fine-grained MoE models that maintains model performance while significantly reducing expert pool?
- **Basis:** Current pruning methods result in "severe performance degradation" and "total failure" at high sparsity
- **Why unresolved:** Fine-grained experts are initialized randomly and lack homogeneity of coarse-grained experts, making standard importance metrics unreliable without retraining
- **What evidence would resolve it:** Pruning technique retaining >90% of baseline average score while removing ≥50% of total experts, potentially involving expert merging or post-pruning distillation

## Limitations
- System bottleneck characterization (I/O-bound vs compute-bound) is theoretical rather than empirically validated through profiling
- Expert pruning methodology lacks crucial implementation details for soft count/hard count methods and sigmoid router handling
- Benchmark evaluation uses fixed input/output token length which may not represent typical usage patterns

## Confidence

**High confidence**: Reducing activated experts improves throughput with minimal performance degradation is well-supported by empirical results across multiple models and benchmarks

**Medium confidence**: Expert pruning provides throughput gains but causes severe performance degradation is supported, but magnitude varies significantly based on pruning strategy

**Low confidence**: Specific mechanisms explaining why low concurrency is I/O-bound and high concurrency is compute-bound are not empirically validated

## Next Checks
1. Profile memory bandwidth utilization and compute utilization across low, medium, and high concurrency settings to validate system transitions from I/O-bound to compute-bound
2. Analyze expert weight distributions in DeepSeek-V3's sigmoid-based router before and after expert skipping to confirm weight polarization behavior
3. Implement and compare multiple expert importance scoring methods for soft count pruning to validate which best preserves performance while maximizing throughput gains