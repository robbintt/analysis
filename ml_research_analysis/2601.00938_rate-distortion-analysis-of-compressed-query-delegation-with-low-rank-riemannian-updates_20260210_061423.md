---
ver: rpa2
title: Rate-Distortion Analysis of Compressed Query Delegation with Low-Rank Riemannian
  Updates
arxiv_id: '2601.00938'
source_url: https://arxiv.org/abs/2601.00938
tags:
- tensor
- torch
- mode
- grad
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces compressed query delegation (CQD) for bounded-context
  reasoning, where a high-dimensional latent reasoning state is compressed into a
  low-rank tensor query, delegated to an external oracle, and then updated via Riemannian
  optimization on fixed-rank manifolds. The method connects CQD to rate-distortion
  and information bottleneck principles, showing spectral hard-thresholding is optimal
  for a constrained quadratic distortion problem.
---

# Rate-Distortion Analysis of Compressed Query Delegation with Low-Rank Riemannian Updates

## Quick Facts
- arXiv ID: 2601.00938
- Source URL: https://arxiv.org/abs/2601.00938
- Reference count: 40
- One-line primary result: CQD achieves 94.7% accuracy with 0.18× query ratio vs GPT-4o CoT 72.4%

## Executive Summary
This paper introduces Compressed Query Delegation (CQD) for bounded-context reasoning, where high-dimensional latent reasoning states are compressed into low-rank tensor queries, delegated to external oracles, and updated via Riemannian optimization on fixed-rank manifolds. The method connects CQD to rate-distortion and information bottleneck principles, showing spectral hard-thresholding via truncated SVD/HOSVD is optimal for a constrained quadratic distortion problem. Experiments on a 2,500-item reasoning suite demonstrate CQD outperforms chain-of-thought baselines while maintaining controlled semantic drift across modern oracles.

## Method Summary
CQD compresses latent reasoning states (tensors) using Adaptive Spectral Masking (ASM) based on Tucker/HOSVD decomposition with spectral hard-thresholding. The compressed query contains a masked core tensor and factor metadata, enabling sub-linear communication via rank-constrained projection. Updates occur via Riemannian stochastic gradient descent on the manifold of fixed multilinear rank, with convergence guarantees under bounded oracle noise assumptions. The approach treats the oracle as a noisy operator and optimizes the trade-off between query budget and reasoning fidelity through spectral masking thresholds.

## Key Results
- CQD achieves 94.7% accuracy on reasoning suite vs GPT-4o CoT baseline of 72.4%
- Query ratio reduced to 0.18× compared to baseline approaches
- Human benchmark (N=200) demonstrates strong epistemic gain and controlled semantic drift
- Convergence guarantees established for Riemannian stochastic approximation under oracle noise

## Why This Works (Mechanism)

### Mechanism 1
Spectral hard-thresholding via truncated SVD/HOSVD minimizes quadratic distortion under rank constraints. The Adaptive Spectral Masking (ASM) operator projects each mode-n unfolding of the latent tensor onto its top-rₙ singular subspace. By Theorem 5.2 (Eckart-Young-Mirsky) and Proposition 5.3, this projector is optimal for Frobenius-norm distortion at fixed rank. Core assumption: intermediate reasoning states exhibit structured redundancy (low effective multilinear rank), so dominant spectral components preserve task-relevant information.

### Mechanism 2
Riemannian stochastic gradient descent on fixed-rank manifolds converges to stationary points under bounded oracle noise. The update X_{k+1} = Retr_{X_k}(-η_k grad f̃_k(X_k)) stays on the manifold M_r of fixed multilinear rank. Under L-smoothness and bounded variance, Theorem 6.2 guarantees lim inf E[‖grad F(X_k)‖²] = 0 for standard step-size schedules. Core assumption: oracle noise ξ(Q) has zero conditional mean and bounded second moment.

### Mechanism 3
Query budget scales with the product of retained multilinear ranks (r₁r₂r₃), enabling sub-linear communication vs raw traces. The encoder transmits only the masked core tensor Ĝ and factor metadata. Per Section 5.5, B(Q) ∝ r₁r₂r₃; for rₙ ≪ Iₙ, this yields orders-of-magnitude compression. Core assumption: the core tensor captures sufficient statistics for the oracle.

## Foundational Learning

- Concept: Tucker decomposition and Higher-Order SVD (HOSVD)
  - Why needed here: The latent state is parameterized as X = G ×₁ U⁽¹⁾ ×₂ U⁽²⁾ ×₃ U⁽³⁾; understanding mode-n unfoldings and multilinear rank is essential for ASM.
  - Quick check question: Given a 3-way tensor X ∈ R^{I×J×K}, what is the mode-1 unfolding X_{(1)} and how does mode-1 projection via P affect it?

- Concept: Riemannian optimization on Stiefel/fixed-rank manifolds
  - Why needed here: Updates must respect orthonormality of factor matrices; retractions (e.g., QR-based) keep iterates feasible.
  - Quick check question: Why can't we use standard Euclidean gradient descent directly on Stiefel manifold constraints, and what role does the retraction play?

- Concept: Rate–distortion and Information Bottleneck (IB) principles
  - Why needed here: CQD is framed as a Lagrangian trade-off between distortion and query budget; IB intuition explains why preserving dominant spectral content is principled.
  - Quick check question: In the IB formulation min I(X; Q) s.t. I(Q; Y) ≥ constraint, what do X, Q, and Y correspond to in the CQD setting?

## Architecture Onboarding

- Component map: Latent State X -> ASMCompressor -> Enc(Encoder) -> Q (Query) -> Oracle O -> R (Response) -> Updater -> X_{k+1}

- Critical path:
  1. Initialize X₀ on M_r (random or task-specific prior)
  2. Compress: X̂_k ← Ψ_ASM(X_k; ε)
  3. Encode: Q_k ← Enc(X̂_k)
  4. Query oracle: R_k ← O.infer(Q_k)
  5. Riemannian step: X_{k+1} ← Retr_{X_k}(-η_k grad f(X_k; R_k))
  6. Repeat until convergence or budget exhausted

- Design tradeoffs:
  - **Threshold ε vs. rank vs. budget**: Lower ε → higher retained ranks → higher fidelity but larger query. Tuning ε controls the rank–distortion Pareto frontier.
  - **Oracle ensemble size m**: Larger m reduces variance but increases latency and cost.
  - **Retraction choice**: QR-based is standard; approximations may introduce drift if poorly conditioned.

- Failure signatures:
  - **Rank collapse**: All rₙ → 1; query becomes uninformative, epistemic gain G_E drops.
  - **Semantic drift**: D_sem spikes (cosine distance > 0.9) when oracle noise is high and ε too aggressive.
  - **Non-convergence**: ‖grad F(X_k)‖ does not approach 0; check step-size schedule or oracle bias.

- First 3 experiments:
  1. **Sanity check on synthetic data**: Generate low-rank tensor + known noise; verify ASM recovers ground-truth rank and that Riemannian updates converge to a fixed point matching oracle response.
  2. **ε-sweep on BBH subset**: Run CQD on 100 BBH tasks varying ε ∈ {0.05, 0.1, 0.2, 0.4}; plot accuracy vs. query ratio to validate Pareto curve.
  3. **Ablation: random vs. spectral projection**: Replace ASM with random rank-r projection; expect accuracy drop and report Δ accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
Does the zero-mean bounded-variance oracle noise model (Eq. 9) accurately capture systematic biases in real LLM oracles? Basis in paper: Assumption 6.1 assumes E[ξ(Q)|Q] = 0 and E‖ξ(Q)‖² ≤ σ². Why unresolved: Real oracles may exhibit state-dependent or directional biases that violate zero-mean assumptions, potentially affecting convergence guarantees. What evidence would resolve it: Empirical analysis of residual distributions from oracle responses across diverse query types, testing for systematic bias and state-dependence.

### Open Question 2
Is spectral hard-thresholding optimal for non-quadratic distortion measures aligned with task-specific semantics? Basis in paper: Proposition 5.3 establishes optimality specifically for quadratic/Frobenius distortion. Why unresolved: Semantic drift and epistemic gain metrics measured in Benchmark B may not correspond to quadratic objectives, yet the paper uses spectral truncation throughout. What evidence would resolve it: Comparative experiments with alternative compression schemes (e.g., learned encoders) optimized directly for task-relevant distortion measures.

### Open Question 3
How should the spectral threshold ε be adapted online to maintain budget constraints while maximizing performance? Basis in paper: Appendix B.3 briefly mentions adapting εₖ online but provides no principled rule or convergence analysis for dual ascent on the budget constraint. Why unresolved: The Lagrangian formulation introduces λ, but no adaptive scheme for ε or λ is analyzed theoretically or evaluated empirically. What evidence would resolve it: Ablation studies on adaptive vs. fixed ε, plus convergence analysis for dual variable updates under manifold constraints.

### Open Question 4
Can the drift–gain tradeoff observed in Benchmark B be theoretically characterized and optimized? Basis in paper: Figure 1 shows empirical drift–gain scaling across oracles, but no theoretical bound or optimal operating point is derived. Why unresolved: The paper demonstrates the tradeoff exists but does not connect it to the rate-distortion framework or provide guidance on oracle selection. What evidence would resolve it: Deriving theoretical bounds on semantic drift as a function of compression rate and oracle noise, validated against human benchmark data.

## Limitations

- The precise mapping from text-based reasoning traces to high-dimensional latent tensors is not specified, making exact reproduction challenging without architectural assumptions.
- The interaction between oracle responses and Riemannian gradient updates relies on a query/response protocol that is abstracted in the paper.
- Empirical evaluation combines multiple datasets without full specification of task diversity and difficulty distribution.

## Confidence

- **High confidence**: Theorem 6.2 (convergence under bounded noise) — standard RSGD analysis with clear assumptions.
- **Medium confidence**: Proposition 5.3 (spectral hard-thresholding optimality) — Eckart-Young-Mirsky theorem applies, but proof details are terse.
- **Medium confidence**: Empirical accuracy and query ratio improvements — strong absolute numbers, but comparison baselines lack full hyperparameter transparency.
- **Low confidence**: Exact implementation of latent state construction and query encoding — these are black boxes in the paper.

## Next Checks

1. Implement ASM compression on synthetic tensors and verify that the quadratic distortion bound in Lemma 5.4 holds empirically for varying ε.
2. Test Algorithm 1 with a mock oracle on a toy manifold; confirm convergence to a fixed point and that step-size schedules achieve diminishing gradients.
3. Perform an ablation replacing ASM with random projection; measure accuracy drop to validate the claimed optimality of spectral masking.