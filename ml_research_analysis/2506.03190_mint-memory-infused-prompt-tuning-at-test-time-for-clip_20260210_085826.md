---
ver: rpa2
title: 'MINT: Memory-Infused Prompt Tuning at Test-time for CLIP'
arxiv_id: '2506.03190'
source_url: https://arxiv.org/abs/2506.03190
tags:
- prompt
- mint
- prompts
- test-time
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MINT: Memory-Infused Prompt Tuning at Test-time for CLIP MINT
  addresses the challenge of improving Vision-Language Pre-trained Models'' (VLMs)
  generalization under test-time distribution shifts. The core method introduces a
  Memory Prompt Bank (MPB) that stores learnable key-value prompt pairs, functioning
  as a memory of previously seen samples.'
---

# MINT: Memory-Infused Prompt Tuning at Test-time for CLIP

## Quick Facts
- arXiv ID: 2506.03190
- Source URL: https://arxiv.org/abs/2506.03190
- Authors: Jiaming Yi; Ruirui Pan; Jishen Yang; Xiulong Yang
- Reference count: 30
- Key outcome: MINT achieves 63.12% average Top-1 accuracy across four ImageNet benchmarks, with 78.68% on ImageNet-R

## Executive Summary
MINT addresses the challenge of improving Vision-Language Pre-trained Models' (VLMs) generalization under test-time distribution shifts. The core method introduces a Memory Prompt Bank (MPB) that stores learnable key-value prompt pairs, functioning as a memory of previously seen samples. During test-time, hierarchical visual features from incoming images dynamically retrieve and combine relevant prompt pairs from the MPB to create Associative Prompts, which are then injected into the image encoder. MINT also employs learnable text prompts for joint adaptation. The method leverages associative memory theory to enable rapid, precise adaptation without source data or retraining. Experimental results demonstrate that MINT significantly outperforms existing test-time adaptation methods.

## Method Summary
MINT operates entirely post-deployment on unlabeled test data without access to source data or labels. The method extracts hierarchical [CLS] token features from multiple encoder layers as queries, which retrieve relevant prompt pairs from a learnable Memory Prompt Bank (MPB) via cosine similarity. Retrieved prompts are aggregated into an Associative Prompt and injected at the image encoder input. Jointly, learnable text prompts are prepended to class embeddings. The system optimizes both components using entropy minimization on high-confidence augmented views, filtered by a confidence-based selection step. Updates occur batch-wise using AdamW with a learning rate of 5×10⁻³.

## Key Results
- MINT achieves 63.12% average Top-1 accuracy across ImageNet-R, ImageNet-A, ImageNet-V2, and ImageNet-Sketch
- Breakthrough performance of 78.68% on ImageNet-R, significantly exceeding existing TTA methods
- Ablation shows MPB provides clear advantage over single prompt baselines (49.25% → 59.83% on ImageNet-A)
- 512 memory entries identified as optimal MPB capacity

## Why This Works (Mechanism)

### Mechanism 1: Memory-Augmented Compositional Prompting
A learnable Memory Prompt Bank (MPB) enables test-time storage and retrieval of visual-semantic patterns, allowing compositional reuse across distribution shifts. The MPB stores 512 key-value pairs, with query features retrieving top-3 similar entries per layer via cosine similarity. Retrieved prompts are averaged into an Associative Prompt injected at the image encoder input. This works under the assumption that test samples share latent visual-semantic patterns that can be captured, stored, and recombined. Entropy minimization with similarity regularization may mitigate prompt collapse risks observed in similar dynamic retrieval systems.

### Mechanism 2: Hierarchical Multi-Scale Querying
Extracting query features from multiple encoder layers captures both low-level textures and high-level semantics for finer-grained memory retrieval. [CLS] tokens from distinct levels serve as queries, each retrieving different memory entries, encoding multi-scale information. This assumes different layers encode distinct abstraction levels relevant to different domain shifts. Experimental evidence shows layer-1 injection performs best, suggesting early integration of multi-scale information is critical. However, direct corpus evidence on hierarchical querying is limited.

### Mechanism 3: Confidence-Filtered Entropy Minimization
Jointly optimizing text prompts and MPB via entropy minimization on high-confidence augmented views enables stable unsupervised adaptation. For each test image, 64 augmented views are generated, with top 10% lowest-entropy samples retained for optimization. This assumes low-entropy predictions correlate with correct labels, improving generalization without labels. Confidence filtering reduces noise but risks discarding valid samples if too aggressive, or including noisy samples if too loose.

## Foundational Learning

- **Concept: Test-Time Adaptation (TTA)**
  - Why needed: MINT operates entirely post-deployment without source data or labels. Understanding TTA constraints is prerequisite.
  - Quick check: Why does TPT optimize only text prompts while MINT also updates a visual memory bank? What tradeoff does this introduce?

- **Concept: Vision Transformer (ViT) Token Semantics**
  - Why needed: MINT extracts [CLS] tokens from intermediate layers as queries. Understanding what ViT layers encode informs retrieval quality.
  - Quick check: What does the [CLS] token at layer 3 vs. layer 10 typically represent in a ViT? How might this affect which memory entries are retrieved?

- **Concept: Content-Addressable Memory / Key-Value Retrieval**
  - Why needed: MPB is structurally similar to associative memory systems. Query-key matching via cosine similarity is the core retrieval operation.
  - Quick check: If all memory keys drift to similar vectors during training, what failure mode occurs? How does the similarity regularization term help?

## Architecture Onboarding

- **Component map:** Input image → B=64 augmented views → hierarchical queries → cosine similarity retrieval from MPB → aggregate P_a → prepend to image encoder input → encode → v' → text encoder with learnable prompts → predictions → entropy filter → L → backprop to θ_MINT

- **Critical path:** 1) Input image x → B=64 augmented views 2) Each view → hierarchical queries {q^(l)} via frozen encoder 3) Queries retrieve prompts from MPB → aggregate P_a 4) Prepend P_a to patch embeddings → encode → v' 5) Text encoder with learnable prompts → {t_k} 6) Compute P(y|x) = softmax(cos(v', t_k)/τ) 7) Filter by entropy → compute L → backprop to θ_MINT = {P_t, {k_i, v_i}}

- **Design tradeoffs:** MPB size 512 balances capacity vs. retrieval cost (O(N_MPB × N_layers × D_I)); prompt length L_m=2 balances expressiveness vs. parameter count; layer-1 injection ensures stability but may not leverage deep features; 10% confidence threshold balances noise reduction vs. sample retention

- **Failure signatures:** Prompt collapse (all v_i converge); memory underutilization (few keys receive >90% of queries); filter starvation (<5% samples pass); retrieval noise (high variance in P_a across similar images)

- **First 3 experiments:** 1) Ablate MPB: Replace with single learnable visual prompt (49.25% → 59.83% on ImageNet-A) 2) Sweep N_MPB: Test {128, 256, 512, 1024} (512 optimal) 3) Layer injection study: Compare layers {1, 3, 6, 9, 12} (layer 1 best)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the memory retrieval and composition mechanisms be optimized to reduce computational overhead in high-frequency test-time scenarios?
- Basis: Future Directions section states exploring efficient retrieval is key to reducing costs
- Why unresolved: Current dynamic querying introduces latency hindering real-time deployment
- What evidence would resolve it: Modified retrieval strategy (e.g., approximate nearest neighbor) lowering latency without degrading accuracy

### Open Question 2
- Question: Can MINT be effectively extended to other pre-trained architectures and multimodal tasks, such as video understanding or Visual Question Answering?
- Basis: Future Directions suggests extending MINT to other models and tasks
- Why unresolved: Current experiments limited to image classification using CLIP; interaction with video or QA tasks unknown
- What evidence would resolve it: Successful implementation on video transformers or VQA models showing consistent improvements

### Open Question 3
- Question: How can MINT mitigate sensitivity to Memory Prompt Bank initialization and hyperparameters to avoid scenario-specific tuning?
- Basis: Limitations note initialization and hyperparameters "may require scenario-specific tuning"
- Why unresolved: Dataset-specific tuning compromises universality and ease of deployment
- What evidence would resolve it: Experiments showing stable performance across all four ImageNet benchmarks using fixed hyperparameters and random initialization

### Open Question 4
- Question: Would injecting Associative Prompts into multiple layers improve performance if stability issues were addressed?
- Basis: Ablation notes single-layer injection "to ensure stability... considering that only one sample is available"
- Why unresolved: Unclear if single-layer is optimal or merely constrained by noise in single-instance entropy minimization
- What evidence would resolve it: Stabilization technique enabling robust multi-layer prompt injection outperforming single-layer baseline

## Limitations
- Architecture specification gaps: N_layers and text prompt module details not specified, preventing exact reproduction
- Scalability constraints: O(N_MPB × N_layers × D_I) retrieval complexity becomes prohibitive as MPB size grows
- Generalization scope: Experiments limited to ImageNet-based benchmarks; performance on truly out-of-distribution scenarios untested

## Confidence
- Test-time adaptation effectiveness: High - multiple baselines show consistent outperformance across four benchmarks
- Hierarchical querying benefit: Medium - ablation shows layer-1 optimal but compositional benefit not fully justified
- Memory bank compositional learning: Medium - MPB shows advantage but compositional reuse mechanism needs more direct validation

## Next Checks
1. Memory Bank Pattern Analysis: Visualize learned MPB keys across different domain shifts to validate compositional reuse hypothesis
2. Retrieval Robustness Under Noise: Systematically corrupt query features and measure MPB retrieval stability
3. Sequential Adaptation Testing: Evaluate MINT on sequence of progressively different domains to measure catastrophic forgetting and multi-domain adaptation capability