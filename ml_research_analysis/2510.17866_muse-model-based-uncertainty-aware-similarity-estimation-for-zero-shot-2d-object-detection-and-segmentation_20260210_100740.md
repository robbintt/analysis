---
ver: rpa2
title: 'MUSE: Model-based Uncertainty-aware Similarity Estimation for zero-shot 2D
  Object Detection and Segmentation'
arxiv_id: '2510.17866'
source_url: https://arxiv.org/abs/2510.17866
tags:
- similarity
- object
- muse
- score
- patch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MUSE is a training-free framework for model-based zero-shot 2D
  object detection and segmentation that achieves state-of-the-art performance on
  the BOP Challenge 2025. The key innovation lies in integrating class and patch embeddings
  using generalized mean pooling (GeM) to create compact yet discriminative feature
  representations.
---

# MUSE: Model-based Uncertainty-aware Similarity Estimation for zero-shot 2D Object Detection and Segmentation

## Quick Facts
- arXiv ID: 2510.17866
- Source URL: https://arxiv.org/abs/2510.17866
- Authors: Sungmin Cho; Sungbum Park; Insoo Oh
- Reference count: 40
- Key outcome: MUSE achieves state-of-the-art performance on BOP Challenge 2025 with mAP of 0.533 for detection and 0.525 for segmentation on Classic Core benchmark

## Executive Summary
MUSE introduces a training-free framework for zero-shot 2D object detection and segmentation that achieves first place across all tracks of the BOP Challenge 2025. The framework leverages generalized mean pooling (GeM) to integrate class and patch embeddings, creating compact yet discriminative feature representations. By combining absolute and relative similarity metrics through a joint scoring mechanism and incorporating uncertainty-aware objectness priors via a Bayesian framework, MUSE demonstrates superior performance particularly in cluttered scenes and scenarios with visually similar objects.

## Method Summary
MUSE is a training-free framework that integrates class and patch embeddings using generalized mean pooling (GeM) to create discriminative feature representations for zero-shot object detection and segmentation. The framework introduces a joint similarity score combining absolute and relative metrics to improve discrimination in challenging scenarios. An uncertainty-aware objectness prior is incorporated through Bayesian frameworks, leveraging proposal confidence scores to refine predictions. The method achieves state-of-the-art performance without any additional training or fine-tuning, ranking first across Classic Core, H3, and Industrial tracks of the BOP Challenge 2025.

## Key Results
- Achieved first place across all tracks of BOP Challenge 2025 (Classic Core, H3, and Industrial)
- Obtained mAP of 0.533 for detection and 0.525 for segmentation on Classic Core benchmark
- Demonstrated particular effectiveness in cluttered scenes and scenarios with visually similar objects

## Why This Works (Mechanism)
The framework's success stems from its innovative integration of multiple complementary approaches. The GeM pooling mechanism creates compact yet discriminative feature representations by effectively combining class and patch embeddings. The joint similarity score addresses the challenge of distinguishing between visually similar objects by incorporating both absolute and relative similarity metrics. The uncertainty-aware objectness prior refines predictions by leveraging Bayesian frameworks to account for proposal confidence scores, reducing false positives in ambiguous scenarios.

## Foundational Learning

**Generalized Mean Pooling (GeM)**: A pooling operation that generalizes max and average pooling by introducing a learnable parameter. Why needed: Creates flexible feature aggregation that adapts to different object characteristics. Quick check: Verify that GeM parameters are optimized for the specific object categories in the target domain.

**Joint Similarity Scoring**: Combines absolute similarity (object-to-class) and relative similarity (object-to-other-classes) metrics. Why needed: Improves discrimination between visually similar objects by considering multiple perspectives. Quick check: Ensure the weighting between absolute and relative components is balanced for the specific dataset characteristics.

**Bayesian Objectness Priors**: Incorporates uncertainty estimation into object detection through probabilistic frameworks. Why needed: Reduces false positives by accounting for prediction confidence and ambiguity. Quick check: Validate that uncertainty estimates correlate with actual detection accuracy across different scene complexities.

## Architecture Onboarding

**Component Map**: Input Images -> Feature Extraction -> GeM Pooling -> Class Embedding Integration -> Joint Similarity Scoring -> Uncertainty-Aware Filtering -> Detection/Segmentation Output

**Critical Path**: Feature extraction and GeM pooling form the foundation, followed by joint similarity scoring as the discrimination mechanism, with uncertainty-aware filtering serving as the final refinement step.

**Design Tradeoffs**: The framework trades computational efficiency for accuracy by using multiple similarity metrics and uncertainty estimation, which may impact real-time deployment scenarios.

**Failure Signatures**: Poor performance may manifest in scenarios with extreme occlusion, unusual lighting conditions, or when training data distribution significantly differs from the target domain.

**First Experiments**:
1. Validate GeM pooling parameter optimization across different object categories
2. Test joint similarity scoring performance with varying weight ratios between absolute and relative metrics
3. Assess uncertainty-aware prior effectiveness by comparing with and without Bayesian filtering

## Open Questions the Paper Calls Out
None

## Limitations
- Limited ablation studies to demonstrate individual component contributions
- Reliance on pre-trained models without fine-tuning may limit domain-specific adaptability
- Performance claims based solely on BOP Challenge metrics without cross-dataset validation

## Confidence
**High Confidence**: GeM pooling methodology is technically sound and well-established in literature.
**Medium Confidence**: Performance metrics are credible but lack comparative analysis and statistical significance testing.
**Low Confidence**: Generalization capability beyond BOP Challenge remains uncertain due to limited cross-dataset validation.

## Next Checks
1. Conduct comprehensive ablation studies to quantify individual contributions of GeM pooling, joint similarity scoring, and uncertainty-aware priors.
2. Validate MUSE's performance on alternative benchmark datasets (e.g., COCO, LVIS) to assess generalization.
3. Perform real-time inference analysis to measure computational efficiency and identify deployment bottlenecks.