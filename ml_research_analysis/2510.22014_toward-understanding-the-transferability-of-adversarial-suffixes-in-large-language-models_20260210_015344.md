---
ver: rpa2
title: Toward Understanding the Transferability of Adversarial Suffixes in Large Language
  Models
arxiv_id: '2510.22014'
source_url: https://arxiv.org/abs/2510.22014
tags:
- suffix
- refusal
- transfer
- prompt
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a systematic statistical and interventional
  analysis of why and when adversarial suffixes transfer across prompts and models
  in large language models. The authors focus on suffix-based jailbreak attacks, which
  append short, nonsensical suffixes to harmful prompts to elicit disallowed responses.
---

# Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models

## Quick Facts
- **arXiv ID:** 2510.22014
- **Source URL:** https://arxiv.org/abs/2510.22014
- **Reference count:** 16
- **Key outcome:** Geometric activation-space factors (refusal connectivity, suffix push, orthogonal shift) predict jailbreak suffix transfer success across prompts and models

## Executive Summary
This paper provides a systematic statistical and interventional analysis of why and when adversarial suffixes transfer across prompts and models in large language models. The authors focus on suffix-based jailbreak attacks, which append short, nonsensical suffixes to harmful prompts to elicit disallowed responses. They identify three key geometric factors in activation space that correlate with transfer success: the prompt's alignment with the model's internal refusal direction (lower alignment increases transfer likelihood), the suffix's push away from the refusal direction (larger antiparallel shifts increase transfer), and the suffix's orthogonal shift relative to refusal (larger shifts increase transfer). Semantic similarity between prompts shows only weak correlation with transfer. These findings are supported by large-scale experiments optimizing 10,000 suffixes per model and validated through interventional experiments that modify attack algorithms to improve success rates. The work offers practical insights for designing more transferable attacks and potential defenses.

## Method Summary
The authors analyze transferability of adversarial suffixes (generated via GCG) by computing geometric features in activation space. They extract a "refusal direction" from contrastive harmful/harmless prompt pairs at an optimal layer, then measure three features for each prompt-suffix pair: refusal connectivity (prompt's alignment with refusal direction), suffix push (suffix-induced antiparallel shift), and orthogonal shift (suffix-induced perpendicular shift). Using logistic regression, they predict transfer success from these features across multiple model pairs. They validate findings through interventional experiments modifying the GCG loss function with regularization terms for push and orthogonal shift.

## Key Results
- Suffix push away from refusal direction is the strongest predictor of transfer success (standardized coefficients 1.12-2.46)
- Prompts with lower refusal connectivity (further from refusal direction) show higher transfer likelihood
- Larger orthogonal shifts relative to refusal direction increase transfer probability
- Semantic similarity between prompts shows only weak correlation with transfer success

## Why This Works (Mechanism)

### Mechanism 1: Refusal Connectivity of the Prompt
- **Claim:** Prompts whose activations are less aligned with the model's internal refusal direction are more susceptible to suffix-based jailbreak transfer.
- **Mechanism:** Each prompt has a baseline activation vector. The dot product and cosine similarity with the refusal direction measure "refusal connectivity." Lower values indicate the prompt sits further from the model's learned refusal representation, making it easier for a suffix to push it past the refusal threshold.
- **Core assumption:** The refusal direction (computed from harmful vs. harmless prompt contrastive pairs) reliably captures the model's internal harm representation.
- **Evidence anchors:**
  - [abstract] "how much a prompt without a suffix activates a model's internal refusal direction"
  - [Section 5.4, Table 2] Negative standardized coefficients (-0.12 to -0.28) across Qwen, Vicuna, Llama 3.2, Llama 2, statistically significant (p<0.001)
  - [corpus] Weak direct support; related work focuses on attack generation rather than transfer mechanisms
- **Break condition:** If refusal direction extraction fails (e.g., insufficient harmful/harmless contrast pairs), connectivity measurements become unreliable.

### Mechanism 2: Suffix Push (Antiparallel Shift)
- **Claim:** Suffixes that induce larger shifts away from (antiparallel to) the refusal direction are more likely to transfer successfully.
- **Mechanism:** When a suffix is appended, it modifies the prompt's residual stream activation. The suffix push Δpush measures how much this shifts the activation *away* from the refusal direction. Larger positive values indicate stronger suppression of refusal-related activation.
- **Core assumption:** Successful jailbreaking requires reducing activation alignment with the refusal direction; this geometric property generalizes across prompts and models.
- **Evidence anchors:**
  - [abstract] "how strongly a suffix induces a push away from this direction"
  - [Section 5.5, Table 3] Largest effect sizes among all predictors (1.12 to 2.46 standardized coefficients)
  - [Section 5.6, Table 4] Interventional validation: adding suffix push regularization to GCG loss improves ASR from 0.0138 to 0.0214
  - [corpus] Neighbor papers (e.g., "Universal Adversarial Suffixes Using Calibrated Gumbel-Softmax Relaxation") mention universal suffixes but don't analyze this geometric mechanism
- **Break condition:** If suffix push doesn't generalize across layers (i.e., only affects late layers without propagating), transfer may fail.

### Mechanism 3: Orthogonal Shift
- **Claim:** Suffixes that induce larger shifts in directions orthogonal to the refusal direction increase transfer probability.
- **Mechanism:** Beyond moving away from refusal, successful suffixes also shift activations in perpendicular directions (δ⊥). This orthogonal component may disrupt refusal circuitry or move the representation into regions where refusal is less well-defined.
- **Core assumption:** The refusal direction is one-dimensional; orthogonal perturbations access dimensions where safety training is weaker.
- **Evidence anchors:**
  - [abstract] "how large these shifts are in directions orthogonal to refusal"
  - [Section 5.4, Table 2] Positive coefficients (0.29 to 2.00) across all models, highly significant
  - [Section 5.6, Table 5] Small positive regularization coefficients (0.00001) improve ASR, though larger coefficients hurt
  - [corpus] No direct corpus support; orthogonal dynamics unexplored in related work
- **Break condition:** Excessive orthogonal shift may push representations into out-of-distribution regions where the model's behavior is unpredictable.

## Foundational Learning

- **Refusal Direction (Arditi et al., 2024)**
  - Why needed: Central to all three mechanisms; a vector extracted by contrasting harmful vs. harmless prompt activations at a specific layer.
  - Quick check question: Given two activation vectors, can you compute their difference to estimate a refusal direction?

- **Residual Stream Activations**
  - Why needed: All geometric features (push, shift, connectivity) are computed from activations at the end-of-instruction token at the "optimal layer."
  - Quick check question: Where in the forward pass would you hook to extract the activation for computing refusal alignment?

- **GCG (Greedy Coordinate Gradient) Attack**
  - Why needed: The paper modifies GCG's loss function to test interventional hypotheses; understanding the base algorithm clarifies how regularization terms integrate.
  - Quick check question: How does GCG update individual tokens in the adversarial suffix?

## Architecture Onboarding

- **Component map:**
  - JailbreakBench dataset (100 harmful prompts) -> Refusal direction extraction -> GCG suffix generation -> Geometric feature computation -> Logistic regression prediction -> Interventional validation

- **Critical path:**
  1. Extract refusal direction from contrastive harmful/harmless pairs at optimal layer (Definition 2, Appendix A)
  2. Generate suffixes via GCG (potentially with modified loss)
  3. Compute geometric features: s_base (refusal connectivity), Δpush (suffix push), δ⊥ (orthogonal shift)
  4. Fit logistic regression to predict transfer success; examine standardized coefficients
  5. Validate with interventional experiments (rephrasing prompts, altering GCG loss)

- **Design tradeoffs:**
  - Multi-seed suffix generation (100 per prompt) improves statistical power but increases compute; single-seed limits variance for larger models
  - Optimal layer selection balances signal strength vs. generalization; early layers may not capture semantic content, late layers may be too task-specific
  - Llama-3-70B judge provides consistent evaluation but introduces dependency on a separate model

- **Failure signatures:**
  - Very low transfer success rate (e.g., Qwen→Llama 3.2) yields insufficient variance for statistical analysis (Section 5.5)
  - Excessive regularization coefficients in modified GCG loss degrade ASR (Tables 4-5)
  - Vicuna shows anomalous negative suffix push coefficient (Table 2), suggesting model-specific deviations

- **First 3 experiments:**
  1. **Replicate refusal direction extraction:** Compute v_refusal for a new model using 50 harmful + 50 harmless prompts; validate by adding/subtracting from activations and observing behavior change
  2. **Correlation baseline:** Fit logistic regression predicting transfer success from refusal connectivity alone; expect negative coefficient
  3. **Intervention test:** Modify GCG loss with suffix push regularization (coefficient=0.001); compare ASR against vanilla GCG on held-out prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the identified geometric factors (refusal connectivity, suffix push, orthogonal shift) predict transferability for jailbreak methods beyond GCG, such as AutoDAN or PAIR?
- Basis in paper: [explicit] The authors state: "Because attacks from this family are all structurally similar, in this paper, we focus on the most frequently used, well-studied variant: Greedy Coordinate Gradient (GCG)."
- Why unresolved: The study restricts itself to suffix-based GCG attacks, leaving open whether the mechanisms generalize to structurally different jailbreak strategies with different optimization procedures.
- What evidence would resolve it: Replicate the statistical analysis on transfer success using AutoDAN, PAIR, or other jailbreak methods across the same model suite.

### Open Question 2
- Question: How can the identified geometric factors be leveraged to construct principled defenses against suffix-based jailbreaks?
- Basis in paper: [explicit] Conclusion states: "we also demonstrate that these insights can be used to design stronger attacks and hope they can be used for developing stronger defenses."
- Why unresolved: The paper demonstrates offensive applications (altered GCG loss) but does not implement or test any defensive interventions; the defensive direction remains aspirational.
- What evidence would resolve it: Design and evaluate defense mechanisms (e.g., detecting large orthogonal shifts or abnormal suffix push patterns) that reduce attack success rates without degrading model utility.

### Open Question 3
- Question: What explains the asymmetry in inter-model transfer, where suffixes transfer better from more aligned models to less aligned ones?
- Basis in paper: [explicit] Section 5.1 notes: "suffixes optimized on a more aligned model (Llama 3.2) transfer better to a less aligned one (Qwen) than vice versa."
- Why unresolved: The paper observes this asymmetry but does not provide a mechanistic account of why transfer direction matters; the regression for Qwen→Llama 3.2 found no statistically significant effects due to low transfer success.
- What evidence would resolve it: Controlled experiments varying model alignment strength systematically, combined with analysis of how refusal directions differ across model pairs.

## Limitations
- Very low transfer success rates (e.g., Qwen→Llama 3.2) yield insufficient variance for statistical analysis
- Anomalous negative coefficient for suffix push in Vicuna suggests model-specific deviations not explained by current framework
- Limited corpus support for orthogonal dynamics mechanism; minimal related work exploring this phenomenon

## Confidence

**High confidence:** The core finding that suffix push is the strongest predictor of transfer success, supported by large effect sizes (1.12-2.46 standardized coefficients) and interventional validation showing improved ASR when adding push regularization to GCG.

**Medium confidence:** The geometric interpretation of suffix push and orthogonal shift as antiparallel and perpendicular movements in activation space. While the statistical correlations are strong, the causal mechanism (why these specific geometric properties transfer) remains incompletely explained.

**Low confidence:** The claim that semantic similarity between prompts shows only weak correlation with transfer. The paper mentions this finding but provides limited quantitative support, and related work on universal suffixes suggests semantic content may play a larger role than acknowledged.

## Next Checks

1. **Interventional experiment on orthogonal shift:** Modify GCG to include orthogonal shift regularization with varying coefficients (0.00001-0.1) on a held-out prompt set, measuring ASR and comparing against push-only regularization to isolate orthogonal effects.

2. **Cross-layer analysis:** Repeat the geometric feature computation and regression analysis at multiple layers (e.g., 10, 20, 30) to determine if the optimal layer selection is critical or if these mechanisms operate consistently across the network.

3. **Synthetic prompt study:** Generate synthetic prompts with controlled properties (varying refusal connectivity, semantic similarity) to test whether the geometric predictors hold independent of natural language variation, and whether semantic similarity becomes more predictive in controlled settings.