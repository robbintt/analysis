---
ver: rpa2
title: 'M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language
  Models with Momentum-Anchored Policy Optimization'
arxiv_id: '2512.13070'
source_url: https://arxiv.org/abs/2512.13070
tags:
- policy
- training
- m-grpo
- learning
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a critical instability in self-supervised
  reinforcement learning for large language models, where policies collapse during
  long-horizon training. The authors propose M-GRPO, a momentum-anchored policy optimization
  framework that uses a slowly evolving momentum model to provide stable training
  targets, combined with an IQR-based trajectory entropy filter to prevent premature
  convergence.
---

# M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization

## Quick Facts
- arXiv ID: 2512.13070
- Source URL: https://arxiv.org/abs/2512.13070
- Reference count: 6
- M-GRPO achieves 79.75% accuracy on MATH500 versus 47.50% baseline

## Executive Summary
This paper addresses a critical instability in self-supervised reinforcement learning for large language models, where policies collapse during extended training. The authors propose M-GRPO, a momentum-anchored policy optimization framework that uses a slowly evolving momentum model to provide stable training targets. Combined with an IQR-based trajectory entropy filter to prevent premature convergence, M-GRPO demonstrates superior stability and effectiveness on multiple reasoning benchmarks, achieving state-of-the-art performance.

## Method Summary
The authors identify that self-supervised RL for LLMs suffers from policy collapse during long-horizon training due to unstable reward maximization. M-GRPO introduces a momentum-anchored policy optimization framework where a slowly evolving momentum model provides stable training targets. The method also incorporates an IQR-based trajectory entropy filter to prevent premature convergence. The approach is tested on mathematical reasoning benchmarks including MATH500, demonstrating that the momentum anchoring prevents the policy collapse observed in baseline methods while achieving higher accuracy scores.

## Key Results
- M-GRPO achieves 79.75% accuracy on MATH500 versus 47.50% baseline
- Prevents policy collapse during extended training horizons
- Demonstrates state-of-the-art performance on multiple reasoning benchmarks

## Why This Works (Mechanism)
M-GRPO works by providing stable training targets through a momentum model that evolves slowly over time, preventing the rapid policy shifts that cause collapse. The IQR-based trajectory entropy filter maintains exploration by preventing premature convergence to suboptimal solutions. This combination addresses the fundamental instability in self-supervised RL where policies tend to collapse when maximizing rewards over long horizons without proper stabilization mechanisms.

## Foundational Learning
- **Policy Gradient Methods**: Used to update LLM policies based on reward signals; needed because standard RL approaches fail in self-supervised settings due to instability
- **Momentum Models in RL**: Provide moving averages of policy parameters to stabilize training; quick check: monitor parameter variance between updates
- **Trajectory Entropy**: Measures policy uncertainty; needed to detect and prevent premature convergence; quick check: track entropy trends during training
- **Interquartile Range (IQR) Filtering**: Statistical method for outlier detection; used to identify high-quality trajectories; quick check: verify filter thresholds on validation data
- **Self-Supervised RL**: Trains policies without explicit reward labels; needed for LLMs that generate their own supervision; quick check: ensure self-generated rewards remain meaningful
- **Policy Collapse**: Phenomenon where policies degenerate to trivial solutions; needed to understand failure modes; quick check: monitor reward plateauing or degradation

## Architecture Onboarding
**Component Map**: LLM -> Momentum Model -> Policy Update -> Entropy Filter -> Reward Calculation
**Critical Path**: Forward pass through LLM → Momentum-anchored gradient computation → Entropy-filtered trajectory selection → Policy parameter update
**Design Tradeoffs**: Momentum speed vs responsiveness; entropy threshold selection vs exploration; model complexity vs training stability
**Failure Signatures**: Reward plateauing, entropy collapse, parameter oscillation, gradient explosion
**First Experiments**: 1) Verify momentum model stabilizes parameter updates compared to vanilla GRPO, 2) Test entropy filter prevents premature convergence on simple tasks, 3) Compare training stability curves between M-GRPO and baseline across different horizon lengths

## Open Questions the Paper Calls Out
None

## Limitations
- Uncertainty whether momentum-anchored approach generalizes beyond tested task distributions
- Long-term stability of momentum model remains unclear
- IQR-based filter threshold selection may require task-specific tuning not fully characterized

## Confidence
- High: Empirical demonstration that policy collapse occurs in self-supervised RL for LLMs during extended training
- Medium: Claim that M-GRPO prevents collapse more effectively than alternatives, based on limited comparison methods
- Low: Claims about mechanism by which momentum anchoring provides stability, as theoretical analysis remains heuristic

## Next Checks
1. Test M-GRPO's performance on non-mathematical reasoning tasks to assess generality
2. Conduct ablation studies removing either the momentum model or entropy filter to quantify individual contributions
3. Implement long-term stability tests extending beyond current training horizons to identify potential delayed failure modes