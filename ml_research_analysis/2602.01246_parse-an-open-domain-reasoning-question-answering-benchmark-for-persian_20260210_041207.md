---
ver: rpa2
title: 'PARSE: An Open-Domain Reasoning Question Answering Benchmark for Persian'
arxiv_id: '2602.01246'
source_url: https://arxiv.org/abs/2602.01246
tags:
- persian
- reasoning
- question
- questions
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PARSE introduces the first open-domain Persian reasoning QA benchmark,
  featuring 10,800 questions across Boolean, multiple-choice, and factoid formats
  with diverse reasoning types and difficulty levels. The benchmark is generated via
  a controlled LLM-based pipeline and validated through human evaluation for linguistic
  and factual quality.
---

# PARSE: An Open-Domain Reasoning Question Answering Benchmark for Persian

## Quick Facts
- arXiv ID: 2602.01246
- Source URL: https://arxiv.org/abs/2602.01246
- Reference count: 40
- Introduces the first open-domain Persian reasoning QA benchmark with 10,800 questions across 18 configurations

## Executive Summary
PARSE introduces the first open-domain Persian reasoning QA benchmark, featuring 10,800 questions across Boolean, multiple-choice, and factoid formats with diverse reasoning types and difficulty levels. The benchmark is generated via a controlled LLM-based pipeline and validated through human evaluation for linguistic and factual quality. Experiments show that Persian prompts and structured prompting (Chain-of-Thought for Boolean/multiple-choice, few-shot for factoid) significantly improve LLM performance. Fine-tuning on PARSE further boosts results, especially for Persian-specialized models. These findings demonstrate PARSE's effectiveness as both an evaluation benchmark and training resource for developing reasoning-capable LLMs in low-resource settings.

## Method Summary
The benchmark uses GPT-4o to generate 10,800 questions across 18 configuration families (3 QA types × 2 approach dimensions × 3 subtypes), with 600 questions each. A train-test split of 8,640/2,160 is used (120 per configuration). Evaluation is performed on the Together AI platform with temperature 0.7, using Persian prompts with CoT for Boolean/MCQ and few-shot for factoid. Models tested include Qwen-2.5, LLaMA-3, Mistral, Gemma-2, and Dorna. Metrics are matched to answer types: Accuracy for Boolean, Jaccard for multi-answer, and Contains for simple factoid answers.

## Key Results
- Persian prompts consistently outperform English prompts across all task types and model architectures
- Chain-of-Thought prompting provides strongest performance for Boolean and multiple-choice reasoning
- Few-shot prompting performs best for factoid questions, particularly non-answerable detection
- Fine-tuning on PARSE yields substantial performance gains, especially for Persian-specialized models like Dorna

## Why This Works (Mechanism)

### Mechanism 1: Language-Aligned Prompting Improves Reasoning QA Performance
Using Persian-language prompts with Persian questions yields higher accuracy than English prompts, conditional on model architecture and task type. Questions in Persian contain morphological and syntactic cues that are preserved when prompts are also in Persian, reducing cross-lingual information loss during attention and decoding.

### Mechanism 2: Task-Type Dependent Prompting Strategy
Chain-of-thought (CoT) prompting improves Boolean and multiple-choice reasoning QA, while few-shot prompting is more effective for factoid QA, conditional on question complexity and answer format. CoT decomposes reasoning chains, benefiting questions requiring logical inference, while factoid questions benefit more from pattern matching against exemplars.

### Mechanism 3: Language-Specific Fine-Tuning Amplifies Benchmark Utility
Fine-tuning on PARSE yields substantial gains for Persian-specialized models beyond improvements seen in general multilingual models, conditional on training data quality and configuration coverage. Persian-specialized models have prior exposure to Persian morphology and vocabulary, consolidating task-specific adapters without catastrophic forgetting.

## Foundational Learning

- Concept: **Multi-hop Reasoning in QA**
  - Why needed here: PARSE explicitly includes multi-hop configurations where answers require chaining evidence across multiple facts.
  - Quick check question: Given "Which two planets are closer to the Sun than Earth?", what intermediate facts must be retrieved before answering?

- Concept: **Prompting Strategy Selection (Zero-shot vs. Few-shot vs. CoT)**
  - Why needed here: Experimental results show task-type dependent optimal strategies.
  - Quick check question: For a Boolean question requiring negation reasoning, which prompting strategy is expected to outperform zero-shot baseline?

- Concept: **Low-Resource Language Benchmarking**
  - Why needed here: Persian has limited high-quality QA resources; PARSE addresses this gap.
  - Quick check question: Why might a multilingual model fine-tuned primarily on English data underperform on Persian reasoning tasks?

## Architecture Onboarding

- Component map: Generation Pipeline (GPT-4o with configuration-specific prompts → CSV output → JSON normalization) → Quality Control (structural validation, de-duplication, semantic checks, difficulty calibration) → Taxonomy (3 QA types × 2 dimensions × 3 subtypes = 18 configurations) → Evaluation Suite (accuracy, Jaccard, Contains metrics)

- Critical path: 1) Understand the 18-configuration taxonomy before designing experiments, 2) Use Persian prompts with CoT for Boolean/MC, few-shot for factoid, 3) If fine-tuning, split 8,640 train / 2,160 test uniformly across configurations, 4) Evaluate with metrics matched to answer type

- Design tradeoffs: LLM-generated questions enable scale and diversity but require human validation; tradeoff between cost and linguistic authenticity. 600 items per configuration provides statistical power but limits granularity within subtypes.

- Failure signatures: English prompts with Persian questions: expect ~5-15% accuracy drop. Zero-shot on hard multi-hop questions: expect near-random performance for 7-8B models. Few-shot on factoid non-answerable: high false positive rate if examples lack "None" pattern.

- First 3 experiments: 1) Baseline establishment: Run Qwen-2.5-7B and LLaMA-3-8B on all 18 configurations with Persian prompts, zero-shot. 2) Prompting strategy ablation: For the 3 lowest-performing configurations, compare zero-shot, few-shot (3 examples), and CoT. 3) Fine-tuning delta: Fine-tune Dorna-8B on the 8,640-item training split; evaluate on the held-out 2,160 test items.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Retrieval-Augmented Generation (RAG) significantly improve performance on the PARSE benchmark compared to parametric-only approaches?
- Basis in paper: [explicit] The authors explicitly state in the conclusion: "As future directions, we aim to explore retrieval-augmented generation (RAG)..."
- Why unresolved: The current experiments are limited to zero-shot, few-shot, and CoT prompting without external knowledge retrieval.

### Open Question 2
- Question: Can advanced reasoning-oriented architectures (e.g., DeepSeek-R1) outperform standard instruction-tuned models on the multihop reasoning sections of PARSE?
- Basis in paper: [explicit] The conclusion identifies the need to test "more advanced reasoning-oriented models such as DeepSeek" as a future direction.
- Why unresolved: The study primarily evaluates general-purpose LLMs rather than models specifically fine-tuned for complex reasoning chains.

### Open Question 3
- Question: Why do models exhibit near-zero accuracy on non-answerable multiple-choice questions, and can specific training regimes mitigate this abstention failure?
- Basis in paper: [inferred] Table 5 shows drastic performance drops on "Non-Ans" subtypes, suggesting models cannot reliably detect unanswerable premises.
- Why unresolved: The paper reports the low scores but does not investigate whether this is due to training data bias or linguistic nuances in Persian negation.

## Limitations
- Benchmark validity depends on quality of LLM-generated questions, though human validation is reported
- Evaluation protocol uses single temperature setting (0.7) without exploring sensitivity
- Study focuses on Persian-specific prompting but doesn't compare against cross-lingual transfer learning approaches

## Confidence

**High Confidence**: The core contribution of introducing a Persian reasoning QA benchmark is well-established. The 10,800-question scale, controlled generation pipeline, and human validation provide strong empirical grounding.

**Medium Confidence**: The task-type dependent prompting strategy claims are supported by experimental results but lack external validation from independent studies.

**Low Confidence**: The fine-tuning effectiveness claims, particularly the assertion that Persian-specialized models benefit most, are based on limited model comparisons without controlling for other factors.

## Next Checks

1. **Cross-lingual Transfer Experiment**: Evaluate whether English-prompted models with Persian fine-tuning can match Persian-prompted performance to test if language-aligned prompting provides reasoning improvements versus mere token-level familiarity.

2. **Prompt Sensitivity Analysis**: Systematically vary temperature (0.1, 0.5, 0.7, 1.0) and prompt phrasing for each task type to establish performance bounds and reveal whether reported differences reflect robust reasoning capability.

3. **Configuration Coverage Validation**: Conduct inter-rater reliability testing on a subset of questions across all 18 configurations to verify that human annotators consistently agree on difficulty levels and reasoning type classification.