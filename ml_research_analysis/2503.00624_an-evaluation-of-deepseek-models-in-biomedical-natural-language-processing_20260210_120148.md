---
ver: rpa2
title: An evaluation of DeepSeek Models in Biomedical Natural Language Processing
arxiv_id: '2503.00624'
source_url: https://arxiv.org/abs/2503.00624
tags:
- biomedical
- language
- extraction
- deepseek
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates DeepSeek models (Distilled-DeepSeek-R1 series
  and Deepseek-LLMs) on four biomedical NLP tasks across 12 datasets, benchmarking
  them against state-of-the-art models including Llama3-8B, Qwen2.5-7B, and Mistral-7B.
  The models were assessed on event extraction, relation extraction, named entity
  recognition, and text classification using precision, recall, and F1-score metrics.
---

# An evaluation of DeepSeek Models in Biomedical Natural Language Processing

## Quick Facts
- arXiv ID: 2503.00624
- Source URL: https://arxiv.org/abs/2503.00624
- Reference count: 40
- This study evaluates DeepSeek models on four biomedical NLP tasks across 12 datasets, benchmarking them against state-of-the-art models including Llama3-8B, Qwen2.5-7B, and Mistral-7B.

## Executive Summary
This study benchmarks DeepSeek models (Distilled-DeepSeek-R1 series and Deepseek-LLMs) against state-of-the-art models on four biomedical NLP tasks across 12 datasets. The models were evaluated on event extraction, relation extraction, named entity recognition, and text classification using precision, recall, and F1-score metrics. Results show DeepSeek models perform competitively in named entity recognition and text classification, with F1 scores consistently above 0.95 and 0.91 respectively, but face challenges in event and relation extraction due to precision-recall trade-offs. The study provides task-specific model recommendations and highlights future research directions including retrieval-augmented generation and chain-of-thought reasoning to improve performance.

## Method Summary
The study evaluated 12 model variants (7 DeepSeek models + 5 baseline models) on 12 biomedical datasets using LoRA fine-tuning with rank=64, alpha=32, dropout=0.1, and AdamW optimizer (lr=1e-5). Models were fine-tuned for 5,000 steps with batch size 4 per A100 40GB GPU, evaluated every 1,000 steps, and tested using exact-match precision/recall/F1 metrics. The four task types included event extraction (PHEE, Genia2011, Genia2013), relation extraction (DDI, GIT, BioRED), named entity recognition (BC5CDR, BC2GM, BC4Chemd), and text classification (ADE, PubMed20k RCT, HealthAdvice).

## Key Results
- DeepSeek models achieved F1 >0.95 in named entity recognition and F1 >0.91 in text classification
- Event extraction F1 ranged from 0.1419 to 0.9571, with highest scores from DeepSeek-R1-Distill-Llama-70B (0.9571) and Mistral-7B-Instruct-v0.2 (0.9469)
- Relation extraction F1 ranged from 0.4654 to 0.7684, with DeepSeek-R1-Distill-Qwen-32B achieving highest scores (0.7609, 0.7684)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distilled DeepSeek-R1 models retain competitive biomedical task performance through knowledge compression that preserves reasoning patterns while reducing parameter footprint.
- **Mechanism:** The distillation process transfers teacher model capabilities into smaller student architectures. The paper notes these models "effectively compress the knowledge and capabilities of larger models into more compact forms, resulting in improved computational efficiency without significant loss in performance."
- **Core assumption:** Distillation quality depends on teacher model's biomedical knowledge encoding; limited exposure to biomedical corpora during pre-training may bound transfer effectiveness.
- **Evidence anchors:**
  - [abstract] "DeepSeek models perform competitively in named entity recognition and text classification"
  - [section: Methods] "The distillation process employed in these models effectively compresses the knowledge and capabilities of larger models"
  - [corpus] Weak direct evidence; neighboring papers on distillation (KoGNER) address knowledge graph approaches, not DeepSeek-specific distillation
- **Break condition:** If distilled models show >15% F1 degradation versus teacher on novel biomedical datasets, distillation may have compressed domain knowledge excessively.

### Mechanism 2
- **Claim:** Precision-recall trade-offs in event and relation extraction emerge from models' semantic reasoning limitations when handling implicit or multi-hop relationships.
- **Mechanism:** Complex extraction requires understanding implied relationships versus explicit mentions. The paper shows Llama3-8B achieved 0.9595 recall but only 0.3711 precision on BioRED, suggesting "certain models prioritize exhaustive relation discovery" but generate false positives.
- **Core assumption:** High recall with low precision indicates model over-generates predictions rather than missing relationships; this reflects uncertainty calibration issues, not architecture failure.
- **Evidence anchors:**
  - [abstract] "challenges persist in event and relation extraction due to precision-recall trade-offs"
  - [section: Discussion] "relation extraction remains a challenging task for LLMs, particularly in complex biomedical literature, where relationships are often implied rather than explicitly stated"
  - [corpus] Weak evidence; no neighboring papers directly analyze precision-recall dynamics in biomedical extraction
- **Break condition:** If tuning decision thresholds or adding verification steps fails to improve precision without catastrophic recall loss, the issue may be fundamental representation inadequacy rather than calibration.

### Mechanism 3
- **Claim:** LoRA fine-tuning enables parameter-efficient adaptation to biomedical tasks with limited labeled data, but effectiveness varies by task complexity.
- **Mechanism:** Low-rank adaptation (rank=64, alpha=32, dropout=0.1) modifies only adapter weights while freezing base model. This preserves general capabilities while specializing for biomedical patterns.
- **Core assumption:** LoRA rank of 64 is sufficient to capture biomedical domain shifts; insufficient rank may explain performance gaps on complex tasks.
- **Evidence anchors:**
  - [section: Methods] "We configured LoRA with a rank of 64, an alpha value of 32, and a dropout rate of 0.1"
  - [section: Results] NER and classification show stable high F1 (>0.91), suggesting LoRA captures surface patterns well
  - [corpus] Weak evidence; neighboring papers do not evaluate LoRA specifically on DeepSeek architectures
- **Break condition:** If full fine-tuning yields >10% improvement on event/relation extraction over LoRA, adapter rank or architecture may be the bottleneck.

## Foundational Learning

- **Concept:** Named Entity Recognition (NER) vs. Relation/Event Extraction
  - **Why needed here:** Results show fundamentally different difficulty profiles—NER F1 consistently >0.95 while event extraction ranges 0.14–0.96 depending on dataset. Understanding why distinguishes surface pattern matching from semantic reasoning.
  - **Quick check question:** Can you explain why identifying "aspirin" as a chemical is easier than extracting "aspirin causes bleeding" as an adverse event?

- **Concept:** Precision-Recall Trade-off in Extraction Tasks
  - **Why needed here:** The paper documents extreme cases (Llama3-8B: recall 0.96, precision 0.37 on BioRED). Interpreting these trade-offs determines whether a model suits discovery vs. clinical decision support.
  - **Quick check question:** If a model has 0.95 recall and 0.40 precision on relation extraction, would you deploy it for hypothesis generation or safety-critical alerts?

- **Concept:** Knowledge Distillation and Model Compression
  - **Why needed here:** The DeepSeek-R1-Distill series claims to maintain performance while reducing parameters. Understanding this helps evaluate cost-accuracy trade-offs for deployment.
  - **Quick check question:** What information might be lost when distilling a 70B parameter model to 7B parameters, and how would you detect that loss?

## Architecture Onboarding

- **Component map:** DeepSeek-LLM-7B/67B -> LoRA (rank=64) -> Fine-tuned model -> Exact-match evaluation
- **Critical path:**
  1. Select appropriate model variant based on task (70B for complex extraction, 7B–14B for NER/classification)
  2. Apply LoRA fine-tuning on task-specific biomedical dataset (5K steps recommended)
  3. Evaluate precision-recall balance; adjust decision thresholds if precision is critical
- **Design tradeoffs:**
  - **Model size vs. deployment cost:** 70B models achieve highest F1 but require A100 GPUs; 7B–14B models viable on smaller hardware
  - **High recall vs. high precision:** Llama3-8B for exploratory tasks, DeepSeek-R1-Distill-Qwen-32B for structured extraction
  - **Base vs. distilled:** Base models underperform on some tasks (DeepSeek-LLM-7B: F1 0.673 on ADE); distilled models more consistent
- **Failure signatures:**
  - F1 <0.35 on event extraction (Genia2013 pattern): Model struggles with complex event structures; consider CoT prompting
  - Precision <0.50 with recall >0.90: Model over-generating; calibrate thresholds or add verification step
  - Base model underperforming distilled variant by >15%: Pre-training corpus likely lacks biomedical exposure
- **First 3 experiments:**
  1. Replicate NER evaluation on BC5CDR with DeepSeek-R1-Distill-Llama-8B to validate LoRA configuration (rank=64, alpha=32) before scaling to larger models.
  2. Compare DeepSeek-R1-Distill-Qwen-32B vs. Llama3-8B on BioRED relation extraction to quantify precision-recall trade-off; establish threshold for your precision requirement.
  3. Test DeepSeek-R1-Distill-Llama-70B on Genia2013 event extraction with CoT prompting to determine if structured reasoning improves performance on complex events.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Retrieval-Augmented Generation (RAG) mitigate the precision-recall trade-offs observed in DeepSeek models during biomedical event and relation extraction?
- **Basis in paper:** [explicit] The authors state, "Future research could explore retrieval-augmented generation (RAG)... to enhance accuracy, robustness, and interpretability," specifically noting its potential to ground predictions in factual data for extraction tasks.
- **Why unresolved:** The current study focused on benchmarking fine-tuned models without external retrieval mechanisms, leaving the specific impact of dynamic knowledge retrieval on precision-recall balance untested.
- **What evidence would resolve it:** A comparative evaluation of DeepSeek models equipped with RAG pipelines against the current baseline on the Genia and BioRED datasets, specifically analyzing shifts in precision and recall metrics.

### Open Question 2
- **Question:** Does Chain-of-Thought (CoT) reasoning improve the extraction of complex, multi-step biomedical events where standard fine-tuning currently fails?
- **Basis in paper:** [explicit] The paper highlights CoT as a future direction to help "models break down complex tasks," explicitly linking it to the need for better "causality understanding in event and relation extraction."
- **Why unresolved:** The evaluation utilized standard LoRA fine-tuning, which treats event extraction as a direct mapping task rather than a multi-step reasoning process, failing to capture complex structures like those in Genia2013.
- **What evidence would resolve it:** An analysis of performance delta on the Genia2013 dataset when applying CoT prompting strategies compared to the standard inference methods reported in the paper.

### Open Question 3
- **Question:** What specific dataset characteristics of Genia2013 cause state-of-the-art LLMs to underperform significantly (max F1 0.3348) compared to PHEE (F1 > 0.94)?
- **Basis in paper:** [inferred] The discussion notes a "substantial variation in performance" and suggests "dataset-specific factors, such as annotation style, complexity of event structures, or entity ambiguity" impact results, yet the exact cause remains unidentified.
- **Why unresolved:** The study reports the performance drop but does not conduct an error analysis capable of distinguishing whether the failure is driven by the refinement of event types (12 types), annotation granularity, or sentence complexity.
- **What evidence would resolve it:** An ablation study or qualitative error analysis contrasting model errors on Genia2013 versus PHEE, specifically isolating variables such as event nestedness and entity density.

## Limitations
- The study lacks prompt engineering exploration, which could substantially impact extraction task performance
- The evaluation methodology does not account for multi-hop reasoning requirements in real-world clinical applications
- LoRA configuration (rank=64) may be suboptimal for complex biomedical reasoning tasks

## Confidence
- **High confidence:** NER and text classification results (F1 consistently >0.91)
- **Medium confidence:** Relation extraction findings (F1 0.4654–0.7684, varies by dataset complexity)
- **Low confidence:** Event extraction results (extreme variance 0.1419–0.9571, may reflect annotation inconsistencies)

## Next Checks
1. Replicate event extraction results on Genia2013 with alternative prompt engineering approaches (chain-of-thought, structured output formats) to determine if performance gaps are task-dependent versus model-architecture limitations.
2. Conduct ablation studies comparing LoRA fine-tuning (rank=64) against full fine-tuning and higher-rank adapters (rank=128, 256) on relation extraction tasks to quantify parameter efficiency trade-offs.
3. Perform cross-dataset generalization testing where models trained on one biomedical corpus are evaluated on another to assess true domain adaptation versus dataset memorization.