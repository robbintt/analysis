---
ver: rpa2
title: 'Toward Reproducible Cross-Backend Compatibility for Deep Learning: A Configuration-First
  Framework with Three-Tier Verification'
arxiv_id: '2509.06977'
source_url: https://arxiv.org/abs/2509.06977
tags:
- path
- torch
- tensor
- atol
- return
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a configuration-first framework for evaluating
  cross-backend compatibility in deep learning systems across CPU, GPU, and compiled
  runtimes. The method decouples experiments from code using YAML configurations and
  employs a three-tier verification protocol: tensor closeness, activation alignment,
  and task-level metrics.'
---

# Toward Reproducible Cross-Backend Compatibility for Deep Learning: A Configuration-First Framework with Three-Tier Verification

## Quick Facts
- **arXiv ID**: 2509.06977
- **Source URL**: https://arxiv.org/abs/2509.06977
- **Reference count**: 40
- **Primary result**: Configuration-driven framework achieves 72% cross-backend compatibility pass rate across 672 checks, with detection models and compiled backends showing highest drift.

## Executive Summary
This paper presents a framework for evaluating cross-backend compatibility in deep learning systems, addressing numerical drift between CPU, GPU, and compiled runtime deployments. The method decouples experiments from code using YAML configurations and employs a three-tier verification protocol comparing tensor closeness, activation alignment, and task-level metrics. Across 672 checks over four tolerance settings, the framework achieved a 72.0% pass rate overall, with discrepancies concentrated at tighter tolerances. The work demonstrates that deterministic adapters and selective fallbacks can substantially improve agreement without significant performance loss, providing a reproducible methodology for dependable deployment across heterogeneous runtimes.

## Method Summary
The framework uses a configuration-first approach where YAML files specify models, inputs, backends, and tolerances. A three-tier verification protocol compares outputs: tensor-level closeness using infinity norm with absolute and relative tolerances, activation alignment through selective probing, and task-level metrics (Top-1/Top-5 accuracy, mAP, mIoU). The system runs reference and target backends with global seed control and deterministic flags, logging results to JSONL. Detection models require special handling through deterministic pre-NMS sorting adapters to address nondeterminism in box selection.

## Key Results
- 72.0% aggregate pass rate across 672 cross-backend compatibility checks
- Detection models and compiled backends showed the most drift, often due to nondeterministic post-processing
- Tensor-level agreement often high even when task metrics diverged, suggesting numerical noise can be benign
- Deterministic adapters and selective fallbacks improved compatibility without significant performance loss

## Why This Works (Mechanism)
The framework works by systematically isolating numerical discrepancies between different execution backends through controlled comparisons. By decoupling configuration from code, it enables reproducible experiments across diverse hardware and software stacks. The three-tier verification protocol captures both benign numerical noise and consequential drift affecting final task performance. Deterministic adapters address known sources of nondeterminism (like NMS sorting), while tolerance sweeps help distinguish meaningful failures from acceptable variation. The approach reveals that many backend differences are numerical artifacts rather than semantic failures.

## Foundational Learning
- **Numerical Precision and Floating-Point Drift**: Why needed: The framework detects discrepancies from different hardware kernels and compiler optimizations producing slightly different floating-point results (e.g., 1e-6 vs 1e-5). Quick check: If you run matrix multiplication on CPU and GPU, will results be bitwise identical? No, due to different rounding behaviors and instruction scheduling.
- **Deep Learning Model zoos and Backends**: Why needed: The framework evaluates "cross-backend compatibility" where models execute on different software backends (PyTorch eager, torch.compile, ONNX Runtime) with potential behavioral differences. Quick check: Name three backends for PyTorch deployment? Eager mode, torch.compile, ONNX Runtime. Key difference: Eager executes ops immediately while compiled optimizes the execution graph.
- **Non-Maximum Suppression (NMS) in Object Detection**: Why needed: The primary case study focuses on making NMS deterministic since small numerical differences can change final output through box selection. Quick check: What problem does NMS solve? It eliminates redundant overlapping bounding boxes. If two boxes have identical scores, non-deterministic sorting could arbitrarily select different boxes.

## Architecture Onboarding
- **Component map**: YAML Config → Loader (Library/Repo) → Preprocessor → Execution Engine (reference & target backends) → Three-Tier Verifier → Reporter (JSONL). Adapter Registry injects deterministic logic into execution flow.
- **Critical path**: 1) YAML Specification correctly defines model/backend/tolerances, 2) Execution Engine runs both backends with seeds/deterministic flags, 3) Three-Tier Verification compares outputs, with task-level often being the critical check.
- **Design tradeoffs**: Strictness vs. pass rate (tight tolerances yield failures from benign noise), activation probing vs. cost (expensive, used selectively), latency vs. compatibility (compiled backends faster but more drift, allowing selective fallbacks).
- **Failure signatures**: Detection model failure manifests as different final bounding boxes despite high tensor agreement; compiled backend failure passes on eager but fails when compiled; high tensor error with low task impact suggests false alarms.
- **First 3 experiments**: 1) Baseline classification run (ResNet18) across CPU/GPU with default tolerance, 2) Tolerance sensitivity sweep (1e-6 to 1e-3) to observe pass rate curve, 3) Mitigation validation on detection model with deterministic adapter enabled.

## Open Questions the Paper Calls Out
- **Open Question 1**: Does early convolutional layer drift in classification models generalize to other architectures? Basis: Authors plan systematic activation-level survey across architectures; current findings rely on selective probes. Unresolved due to computational cost of large-scale sweeps. Evidence needed: Layer-wise divergence data across wider model range and backend pairs.
- **Open Question 2**: Can deterministic adapters effectively resolve drift in generative and multimodal models? Basis: Future work lists extending framework to generative/multimodal models. Unresolved as current study validates only classification, detection, segmentation. Evidence needed: Application to generative architectures showing high pass rates without latency degradation.
- **Open Question 3**: To what degree do hardware autotuning and mixed-precision modes contribute to 28% failure rate under strict tolerances? Basis: Threats to validity note hardware/driver autotuning may introduce residual nondeterminism. Unresolved as framework doesn't isolate these factors. Evidence needed: Ablation studies controlling for autotuning and precision settings.

## Limitations
- Applicability to extremely large models (GPT-scale) and non-vision domains remains untested, as experiments focused on 19 models under 2B parameters.
- Detection model results are particularly sensitive to NMS post-processing nondeterminism, requiring domain-specific adapters.
- Three-tier verification protocol's effectiveness depends heavily on appropriate tolerance calibration, which may vary across model families and hardware generations.

## Confidence
- **High Confidence**: Cross-backend numerical drift exists and impacts task-level metrics (validated across 672 checks with measurable pass rate variations).
- **Medium Confidence**: Deterministic adapters meaningfully improve compatibility for detection models (demonstrated through targeted mitigation but requires broader validation).
- **Medium Confidence**: Three-tier verification protocol effectively isolates failure modes (proven across experiments but edge cases may exist).

## Next Checks
1. Test framework on large language models (1B+ parameters) to assess scalability and whether tensor-level discrepancies cascade to semantic-level failures.
2. Validate detection model mitigations across additional architectures (SSD, EfficientDet) and datasets to ensure adapter generalizability.
3. Evaluate framework performance on mobile/edge devices with hardware-specific backends (TensorFlow Lite, CoreML) to test cross-platform portability.