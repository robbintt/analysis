---
ver: rpa2
title: 'OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model Hallucinations
  in Ontology Matching'
arxiv_id: '2409.14038'
source_url: https://arxiv.org/abs/2409.14038
tags:
- matching
- ontology
- hallucinations
- llms
- oaei
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OAEI-LLM, a benchmark dataset designed to
  understand and quantify hallucinations in large language models (LLMs) when used
  for ontology matching (OM). The dataset extends the OAEI reference alignments by
  identifying LLM-specific errors, including missing true mappings, incorrect mappings,
  and misclassifications.
---

# OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching

## Quick Facts
- arXiv ID: 2409.14038
- Source URL: https://arxiv.org/abs/2409.14038
- Authors: Zhangcheng Qiang; Kerry Taylor; Weiqing Wang; Jing Jiang
- Reference count: 18
- Primary result: Benchmark dataset that extends OAEI alignments to identify and categorize LLM hallucinations in ontology matching tasks

## Executive Summary
OAEI-LLM introduces a benchmark dataset designed to understand and quantify hallucinations in large language models when used for ontology matching. The dataset extends the OAEI reference alignments by identifying LLM-specific mapping errors including missing true mappings, generating false mappings, and incorrect alignments. Three types of errors are defined: missing from LLM, missing from OAEI, and incorrect mappings (with subtypes like false, disputed, align-up, and align-down). The dataset schema is extended to record these errors for each LLM, enabling benchmarking and fine-tuning applications.

## Method Summary
The benchmark construction takes source ontology (Os), target ontology (Ot), and their OAEI Reference (Roaei) as inputs. An LLM-based OM system generates LLM Alignment (Allm). The matching assessment procedure compares Roaei with Allm to identify discrepancies: mappings in Roaei but missing from Allm, mappings in Allm but not in Roaei, and partial matches where one entity aligns but the other maps incorrectly. An LLM-based evaluator categorizes incorrect mappings based on relevance and semantic hierarchy relationships. The results are serialized in an extended EDOAL schema with per-LLM hallucination metadata.

## Key Results
- Extends OAEI reference alignments to capture LLM-specific hallucination patterns in ontology matching
- Defines three error categories: missing from LLM, missing from OAEI, and incorrect mappings with four subtypes
- Provides extended EDOAL schema for recording per-LLM hallucination metadata in single benchmark file
- Identifies potential use cases for benchmarking LLMs and generating fine-tuning training data

## Why This Works (Mechanism)

### Mechanism 1
Comparing LLM-generated alignments against human-curated reference alignments reveals specific hallucination patterns in ontology matching tasks. The benchmark construction takes source ontology (Os), target ontology (Ot), and their OAEI Reference (Roaei) as inputs. An LLM-based OM system generates LLM Alignment (Allm). The matching assessment procedure identifies discrepancies by checking: (1) mappings in Roaei but missing from Allm, (2) mappings in Allm but not in Roaei, and (3) partial matches where one entity aligns but the other maps incorrectly.

Core assumption: The OAEI reference alignments represent ground truth for valid mappings, though the paper acknowledges this may not always hold (hence "disputed-mapping" category).

Evidence anchors:
- [abstract] "The dataset extends the OAEI reference alignments by identifying and categorizing LLM-specific mapping errors including missing true mappings, generating false mappings, and incorrect alignments."
- [section 2.1] "For each mapping (e1, e2) ∈ Roaei, we consider that it is not an LLM hallucination if there exists (e1', e2') ∈ Allm such that e1 = e1' and e2 = e2'."
- [corpus] Related work (OAEI-LLM-T) extends this approach to TBox datasets, suggesting the comparative mechanism generalizes across matching categories.

Break condition: If LLM-based matchers achieve near-perfect alignment, or if reference alignments contain substantial errors not captured by the "disputed" category, the comparative signal degrades.

### Mechanism 2
Fine-grained categorization of incorrect mappings distinguishes between different hallucination causes (irrelevant associations, semantic hierarchy confusion, and legitimate disagreements with reference data). When the LLM maps entity e1 to e2' instead of the reference entity e2, an LLM-based evaluator determines: (1) relevance of e2' to the domain, (2) whether e2' is a superclass (align-up) or subclass (align-down) of e2, or (3) whether the mapping is simply irrelevant (false-mapping) or potentially more precise than the reference (disputed-mapping).

Core assumption: An LLM-based evaluator can reliably assess semantic relationships (relevance, superclass/subclass) between entities. The paper explicitly states: "At this time we cannot recommend a particular LLM to be used."

Evidence anchors:
- [section 2.1] "False-mapping: LLMs map the entity to an irrelevant entity. Disputed-mapping: LLMs map the entity to a relevant entity, but it does not align with the OAEI Reference."
- [section 1] "LLMs may consider 'chair' in the context of a research conference to have a similar meaning to 'conference chair'."
- [corpus] Limited direct evidence; corpus focuses on hallucination detection broadly rather than this specific categorization schema.

Break condition: If the evaluator LLM has comparable hallucination issues, categorization reliability cascades into noise.

### Mechanism 3
Extending the EDOAL schema to embed per-LLM hallucination metadata enables downstream fine-tuning and comparative benchmarking without requiring multiple separate alignment files. The standard EDOAL `<Cell>` element is extended with `<hallucination>` child elements, each containing: the LLM identifier (`<llm>`), error category (`<category>`), source of error (`<source>`), and for incorrect mappings, the erroneous entity and error type. This allows a single benchmark file to encode alignment quality across multiple LLMs.

Core assumption: Consumers of the benchmark can parse extended EDOAL and that tooling supports optional extension elements without breaking compatibility.

Evidence anchors:
- [section 2.2] "We extend the current EDOAL mapping schema to record the new information related to LLM hallucinations."
- [section 2.2] Code Snippet 1 demonstrates recording hallucinations for claude-3-sonnet (missing) and llama-3-8b (incorrect/disputed) within the same Cell element.
- [corpus] No corpus papers reference this specific schema extension; novelty is high but adoption evidence is pending.

Break condition: If SSSOM (mentioned as an alternative) becomes the dominant standard, or if schema extensions cause parser compatibility issues in downstream tools.

## Foundational Learning

- Concept: Ontology Matching (OM)
  - Why needed here: The entire benchmark assumes understanding that OM involves finding semantic correspondences between entities across different ontologies, typically represented as (entity1, entity2, relation, confidence) tuples.
  - Quick check question: Given two ontologies describing conferences, what would a mapping between "http://cmt#Chairman" and "http://conference#Chair" represent?

- Concept: LLM Hallucination in Domain Tasks
  - Why needed here: The paper's core premise is that LLMs generate synthesized answers when lacking sufficient or having biased domain knowledge, which manifests differently in structured tasks like OM than in open-ended generation.
  - Quick check question: Why might an LLM incorrectly match "Chair" (person) to "ConferenceChair" even though both exist in the target ontology?

- Concept: EDOAL/Alignment File Formats
  - Why needed here: Understanding the baseline format (Cell, entity1, entity2, measure, relation tags) is prerequisite to appreciating how the extension mechanism works.
  - Quick check question: In a standard EDOAL alignment, what does the `<measure>` element represent and how does the OAEI-LLM extension preserve this while adding hallucination metadata?

## Architecture Onboarding

- Component map: Input Layer (Os, Ot, Roaei) -> LLM Matcher (produces Allm) -> Matching Assessment (compares Roaei vs Allm, categorizes discrepancies) -> Schema Extension Layer (embeds hallucination metadata into extended EDOAL) -> Output (OAEI-LLM benchmark file)

- Critical path:
  1. Run LLM-based matcher on ontology pairs → generate Allm
  2. Compare Allm against Roaei → identify missing, extra, and incorrect mappings
  3. For incorrect mappings, query evaluator LLM for relevance and hierarchy relationships
  4. Serialize results in extended EDOAL with hallucination tags per LLM
  5. Validate schema compliance and that original EDOAL semantics are preserved

- Design tradeoffs:
  - One-to-one mapping constraint: Simplifies evaluation but excludes complex/subsumption matches
  - LLM-based evaluator: Scalable but introduces second-order hallucination risk; paper suggests future voting or human expert alternatives
  - EDOAL vs SSSOM extension: EDOAL chosen for OAEI compatibility; SSSOM noted as alternative for broader adoption

- Failure signatures:
  - High "disputed-mapping" rate: May indicate reference alignment quality issues rather than LLM problems
  - Evaluator LLM inconsistencies: Same mapping categorized differently across runs → temperature/stochasticity issue
  - Schema validation errors: Extended elements rejected by strict EDOAL parsers → need fallback to standard format

- First 3 experiments:
  1. Baseline calibration: Run the matching assessment on a small ontology pair manually to verify categorization matches human judgment (especially disputed vs false-mapping distinction)
  2. Evaluator consistency check: Have the same LLM evaluator categorize the same incorrect mappings multiple times to measure categorization stability
  3. Multi-LLM benchmark: Generate Allm using 3+ different LLMs on the same ontology pair to verify the schema correctly captures per-LLM hallucination profiles in a single file

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can user interaction be effectively utilized to mitigate LLM hallucinations in ontology matching, and what is the required level of interaction?
- Basis in paper: [explicit] Section 5 outlines OAEI-LLM-I, a proposed variant to understand "how user interaction could be used to mitigate LLM hallucinations."
- Why unresolved: The interactive track is listed as future work; the current dataset and methodology do not address interactive scenarios.
- What evidence would resolve it: Results from an interactive track measuring hallucination reduction relative to the volume and type of user input.

### Open Question 2
- Question: What novel hallucination types emerge when LLMs perform complex or one-to-many mappings compared to the simple one-to-one equivalence mappings currently benchmarked?
- Basis in paper: [inferred] Section 4 notes the current work is limited to simple mappings and that "sophisticated cases may cause additional types of LLM hallucinations."
- Why unresolved: The OAEI-LLM taxonomy was constructed using only one-to-one equivalence mappings, leaving complex relationship errors undefined.
- What evidence would resolve it: An extension of the benchmark to complex matching scenarios with a categorized analysis of observed errors.

### Open Question 3
- Question: How does the choice of LLM evaluator impact the reliability of categorizing mapping errors (e.g., "disputed" vs. "false")?
- Basis in paper: [inferred] Section 4 states the assessment varies by LLM and suggests future work could use voting or human experts for "a more reliable" benchmark.
- Why unresolved: The current method relies on a "moderately simple" LLM-based evaluator without a validated standard for subjective categorizations.
- What evidence would resolve it: A comparative study aligning LLM evaluator outputs against human expert judgments for specific error categories.

## Limitations
- Benchmark effectiveness depends critically on quality and completeness of OAEI reference alignments
- LLM-based evaluator introduces second-order hallucination risk without validation of evaluator reliability
- One-to-one mapping constraint excludes many valid ontology relationships, potentially underestimating true LLM performance

## Confidence

**High confidence**: The schema extension mechanism for embedding hallucination metadata in EDOAL format is clearly specified and technically sound

**Medium confidence**: The comparative methodology for identifying LLM hallucinations relative to reference alignments is valid, though dependent on reference quality

**Medium confidence**: The categorization scheme for incorrect mappings is theoretically justified, but practical implementation may vary significantly based on evaluator LLM choice and prompting

## Next Checks
1. Run a small-scale manual validation where human experts verify the categorization of 50 randomly selected hallucination instances to establish baseline accuracy
2. Compare evaluator consistency by having multiple LLM evaluators categorize the same set of incorrect mappings and measure inter-rater agreement
3. Test schema interoperability by attempting to parse OAEI-LLM files with both extended EDOAL and SSSOM parsers to verify backward compatibility