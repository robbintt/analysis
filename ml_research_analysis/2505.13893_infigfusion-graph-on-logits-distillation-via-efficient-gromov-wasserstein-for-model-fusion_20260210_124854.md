---
ver: rpa2
title: 'InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein
  for Model Fusion'
arxiv_id: '2505.13893'
source_url: https://arxiv.org/abs/2505.13893
tags:
- infigfusion
- reasoning
- fusion
- mmlu
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InfiGFusion, a structure-aware model fusion
  framework that addresses the limitations of existing logit-based fusion methods,
  which align vocabulary dimensions independently and miss semantic dependencies encoded
  by cross-dimension interactions. The core method, Graph-on-Logits Distillation (GLD),
  models model outputs as feature-level graphs by retaining top-k logits and aggregating
  their outer products to form co-activation graphs, then aligns these graphs using
  a novel sorting-based approximation to Gromov-Wasserstein distance that reduces
  complexity from O(n^4) to O(n log n) with provable error bounds.
---

# InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion

## Quick Facts
- **arXiv ID**: 2505.13893
- **Source URL**: https://arxiv.org/abs/2505.13893
- **Reference count**: 40
- **Primary result**: Structure-aware fusion framework using Graph-on-Logits Distillation consistently outperforms state-of-the-art models and fusion baselines on 11 benchmarks spanning reasoning, coding, and mathematics

## Executive Summary
InfiGFusion addresses the limitations of existing logit-based model fusion methods by capturing cross-dimensional semantic dependencies through graph structures. The framework models model outputs as feature-level graphs by retaining top-k logits and aggregating their outer products to form co-activation graphs, then aligns these graphs using a novel sorting-based approximation to Gromov-Wasserstein distance. This approach preserves the intrinsic correlations between semantic dimensions that are lost in independent token-level alignment, resulting in improved multi-step reasoning transfer across heterogeneous LLMs. Experiments demonstrate consistent improvements over SFT, ULD, and other fusion baselines on diverse reasoning, coding, and mathematics benchmarks.

## Method Summary
The method constructs a global co-activation graph by computing the outer product of the top-k logit vectors (C = Z_top^T · Z_top) for each source and pivot model. Node features are extracted as row means of the adjacency matrix, and a sorting-based Gromov-Wasserstein approximation computes the loss between sorted feature vectors. The fusion loss combines three components: SFT loss for token-level alignment, ULD (sorting-based Wasserstein-1) for coarse token alignment, and GLD for structure-level alignment. Training uses C-AdamW optimizer with cosine decay scheduling, optimizing over a multi-task dataset spanning reasoning, mathematics, and coding domains.

## Key Results
- Consistently outperforms SFT, ULD, and other fusion baselines on 11 benchmarks
- Shows particular strength in complex reasoning tasks (+35.6 improvement on Multistep Arithmetic and +37.06 on Causal Judgement over SFT)
- The GLD component specifically boosts reasoning scores while ULD stabilizes coarse alignment
- Proven error bounds for the sorting-based GW approximation (O(1/n) error)

## Why This Works (Mechanism)

### Mechanism 1: Cross-Dimensional Dependency Capture via Logit Graphs
The method constructs a global co-activation graph C = Z_top^T · Z_top by computing the outer product of the top-k logit vectors. In this graph, nodes represent vocabulary channels and edge weights represent the joint activation strength across sequence positions. This captures semantic dependencies between tokens that are lost in independent token-level alignment.

### Mechanism 2: Structure-Aware Alignment via Gromov-Wasserstein
Instead of matching tokens directly, the method minimizes the Gromov-Wasserstein (GW) distance between the source and pivot graphs. This measures the discrepancy in pairwise relational structures rather than node locations, aligning the "reasoning capability" encoded in the internal geometry of logit space.

### Mechanism 3: Computational Efficiency via Sorting-Based Approximation
The method avoids solving the complex optimal transport problem of GW directly by compressing graph structure into node-level scalar features (node degrees/row sums) and computing the distance between sorted feature vectors. This reduces complexity from O(n^4) to O(n log n) with provable error bounds.

## Foundational Learning

- **Knowledge Distillation (Logit-based)**: Standard distillation uses KL divergence to align marginal token distributions. Understanding this baseline is essential to appreciate how InfiGFusion improves upon it by preserving cross-dimensional relationships.

- **Gromov-Wasserstein (GW) Distance**: This mathematical tool aligns graphs without explicit node correspondence, working when points live in different spaces but have similar internal structures. It's the core tool for aligning logit graph structures.

- **Outer Product as Similarity**: The paper constructs graphs using Z_top^T · Z_top. This operation effectively correlates dimensions by measuring how often two tokens are co-activated across sequence positions.

## Architecture Onboarding

- **Component map**: Logit Extractors -> Sparse Selection (top-k) -> Graph Builder (C = Z_top^T · Z_top) -> Feature Compressor (row means) -> Alignment Loss (sorted L1 distance) -> Fusion Optimizer

- **Critical path**: The Top-k selection and the Sorting operation are critical. The stability of the gradient depends on the smoothness of the sorting operation, and the information bottleneck is at the Top-k mask.

- **Design tradeoffs**: Accuracy vs. Speed (approximation trades exact GW precision for speed), Stability vs. Precision (tighter generalization bounds but may lose some precision), Cross-dimensional vs. Token-level alignment.

- **Failure signatures**: Degradation on simple tasks like factoid QA where relational structure is irrelevant, sensitivity to k parameter (too small loses semantics, too large introduces noise), potential instability if sorting operation is not differentiable.

- **First 3 experiments**:
  1. Ablation on k: Run fusion with k ∈ {5, 10, 30} to find the optimal sparsity balance for your specific domain
  2. Loss Component Analysis: Train with (ULD only) vs. (GLD only) vs. (ULD + GLD) to verify ULD stabilizes coarse alignment while GLD boosts reasoning scores
  3. Stability Test: Compare training loss curves of standard KL distillation vs. InfiGFusion to validate tighter generalization bounds and stability

## Open Questions the Paper Calls Out

- **Conflict Resolution**: The paper lacks explicit mechanisms for factual conflict resolution when source models provide contradictory knowledge, motivating future work on source reliability estimation.

- **Alternative Feature Extractors**: While the paper implements degree-based features, it notes that alternative extractors like row norm or diffusion-based features could potentially improve approximation quality.

- **Vocabulary Size Scaling**: The error bound decays with larger vocabularies, but the paper doesn't explore how performance scales with vocabulary size or at what point the O(1/n) approximation error becomes practically significant.

## Limitations
- The method relies heavily on top-k logit sparsification, introducing a critical hyperparameter dependency that may not generalize across all domains
- Assumes source and pivot models share the same vocabulary or have compatible projections, which may not hold for all cross-model alignment scenarios
- May degrade performance on literal token matching tasks (factoid QA) by optimizing for relational structure over exact token correspondence

## Confidence

**High Confidence** (experimental results strongly support):
- InfiGFusion outperforms SFT, ULD, and other fusion baselines across 11 benchmarks
- The sorting-based approximation provides computational efficiency with provable error bounds
- GLD component specifically improves reasoning task performance

**Medium Confidence** (theoretical claims supported but practical limits exist):
- Cross-dimensional dependencies are meaningfully captured by top-k logit graphs
- Gromov-Wasserstein alignment transfers reasoning structures between models
- Structure-aware fusion is superior for multi-step reasoning tasks

**Low Confidence** (limited validation or significant assumptions):
- The error bound (O(1/n)) provides tight approximation in practice for LLM vocabularies
- Vocabulary projection handles all cross-model alignment scenarios
- The trade-off between relational structure preservation and exact token matching is well-characterized

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically evaluate GLD performance across k ∈ [5, 10, 15, 20, 30] on a reasoning task subset to quantify the optimal sparsity trade-off and robustness.

2. **Vocabulary Projection Validation**: Test cross-model alignment with deliberately misaligned vocabularies to quantify the impact of vocabulary projection quality on GLD effectiveness.

3. **Structure vs. Token Trade-off**: Design experiments comparing GLD against token-level distillation on mixed task sets to quantify performance degradation on memorization tasks and characterize the structure-token trade-off curve.