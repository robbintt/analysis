---
ver: rpa2
title: Unbiased Gradient Low-Rank Projection
arxiv_id: '2510.17802'
source_url: https://arxiv.org/abs/2510.17802
tags:
- arxiv
- galore
- low-rank
- training
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GaLore Unbiased with Muon (GUM), a memory-efficient
  optimization algorithm for large language model training that addresses the bias
  issue in gradient low-rank projection methods. GUM combines GaLore's gradient low-rank
  projection mechanism with layerwise sampling debiasing and the Muon optimizer to
  achieve unbiased gradient updates while preserving memory efficiency.
---

# Unbiased Gradient Low-Rank Projection

## Quick Facts
- **arXiv ID:** 2510.17802
- **Source URL:** https://arxiv.org/abs/2510.17802
- **Reference count:** 40
- **Primary result:** Introduces GUM, an unbiased gradient low-rank projection method that achieves full-parameter convergence guarantees while using less memory than standard low-rank methods.

## Executive Summary
This paper proposes GUM (GaLore Unbiased with Muon), a memory-efficient optimization algorithm for large language model training that addresses the bias issue in gradient low-rank projection methods. GUM combines GaLore's gradient low-rank projection mechanism with layerwise sampling debiasing and the Muon optimizer to achieve unbiased gradient updates while preserving memory efficiency. Theoretically, GUM matches the convergence guarantees of full-parameter Muon training. Empirically, GUM consistently outperforms GaLore in LLM fine-tuning across instruction-following, mathematical reasoning, and commonsense reasoning tasks. Notably, in LLM pretraining, GUM even outperforms full-parameter AdamW training by 0.3%-1.1% overall accuracy while matching or exceeding performance in 6 out of 7 tasks.

## Method Summary
GUM is a memory-efficient optimization algorithm that combines GaLore's low-rank gradient projection with layerwise sampling debiasing and the Muon optimizer. The key innovation is using Bernoulli sampling to select layers that receive full-rank updates (compensated by scaling factors) while other layers receive low-rank updates, ensuring the overall update remains unbiased in expectation. The method operates in periodic intervals, updating the projection matrix via SVD and resetting momentum buffers at the start of each period. By carefully balancing the sampling probability and projection rank, GUM achieves memory efficiency comparable to GaLore while maintaining the convergence guarantees of full-parameter Muon training.

## Key Results
- GUM consistently outperforms GaLore in LLM fine-tuning across instruction-following, mathematical reasoning, and commonsense reasoning tasks.
- In LLM pretraining, GUM outperforms full-parameter AdamW training by 0.3%-1.1% overall accuracy while matching or exceeding performance in 6 out of 7 tasks.
- GUM achieves memory efficiency comparable to GaLore (40GB for LLaMA3-8B) while providing unbiased gradient updates.
- GUM demonstrates more uniform knowledge distribution within layers, leading to better utilization of the model parameter space and improved memorization.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GUM removes the directional bias inherent in low-rank gradient projection methods by stochastically sampling the full-rank residual update.
- **Mechanism:** Standard low-rank projection ($P P^\top G$) discards gradient components orthogonal to the projector $P$, creating bias. GUM introduces a Bernoulli random variable per layer: with probability $q$, it computes the full-rank update using the residual $(I - P P^\top)G$ (scaled by $1/q$); otherwise, it uses the low-rank projection (scaled by $1/(1-q)$). In expectation, the scaled sum of these two mutually exclusive updates equals the original full gradient $G$.
- **Core assumption:** The gradient noise and sampling variables are independent such that the expectation of the scaled update equals the true gradient.
- **Evidence anchors:** [abstract], [section 3.1], [corpus] PLUMAGE corroborating probabilistic unbiased estimators.
- **Break condition:** If sampling probability $q$ is too low, variance may explode; if too high, memory efficiency is lost.

### Mechanism 2
- **Claim:** The inclusion of high-rank residual updates improves the "stable rank" of the model weights, leading to better parameter utilization and memorization.
- **Mechanism:** Pure low-rank updates tend to drive weight matrices toward low-rank structures (rapid decay of tail singular values). By occasionally applying the full-rank residual $(I - P P^\top)G$, GUM injects energy into the orthogonal subspace. This maintains a more uniform (long-tailed) singular value distribution in the weights, which correlates with better performance on memorization-heavy tasks.
- **Core assumption:** The performance gap in LLM pretraining is partly caused by the inability of low-rank methods to utilize the full parameter space for storing knowledge.
- **Evidence anchors:** [section 5.4], [figure 3], [corpus] weak evidence regarding specific "stable rank" mechanism in related papers.
- **Break condition:** If projection rank $r$ is too small relative to the intrinsic rank of the problem, residual updates may be insufficient to correct spectral decay.

### Mechanism 3
- **Claim:** The Muon optimizer's structural properties (Newton-Schulz iteration) allow it to commute with the projection operation, preserving the theoretical convergence guarantees of the base optimizer.
- **Mechanism:** The theoretical proof relies on the update step being commutable with the projector ($P \cdot \text{OptUpdate}(G) = \text{OptUpdate}(P \cdot G)$). Muon uses matrix-valued updates (Newton-Schulz) that satisfy this property, unlike standard Adam which applies element-wise scaling. This allows GUM to inherit Muon's convergence rate.
- **Core assumption:** The Newton-Schulz iteration accurately approximates the spectral decomposition required for the optimizer's logic.
- **Evidence anchors:** [section 4], [appendix b.1], [corpus] On the Convergence Analysis of Muon supporting theoretical foundation.
- **Break condition:** If base optimizer doesn't support matrix-structured updates, commutativity property breaks and unbiasedness is not theoretically guaranteed by this specific proof method.

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD) & Low-Rank Projection**
  - **Why needed here:** The entire method relies on projecting gradients onto the top-$r$ singular vectors ($P$) to save memory.
  - **Quick check question:** If a gradient matrix $G$ has rank 100, and you project it onto rank 10, what information is lost? (Answer: The components corresponding to the 90 smallest singular values).

- **Concept: Unbiased Estimators**
  - **Why needed here:** The core contribution is fixing the "bias" of GaLore. One must understand that an estimator $\hat{x}$ is unbiased for $x$ if $E[\hat{x}] = x$.
  - **Quick check question:** Does a high-variance estimator necessarily have high bias? (Answer: No. GUM trades potentially higher variance for zero bias to ensure convergence).

- **Concept: Spectral Norm and Stable Rank**
  - **Why needed here:** The paper explains performance gains through "stable rank" ($\|M\|_F^2 / \|M\|_2^2$).
  - **Quick check question:** Does a higher stable rank imply the matrix is more "complex" or "utilizing more dimensions"? (Answer: Yes, it implies energy is spread across more singular values rather than concentrated in a few).

## Architecture Onboarding

- **Component map:** Compute Gradient $G$ -> Period Start? (Update Projector $P$, Reset Momentum $R$) -> Sample Layer -> (Full-Rank path) OR (Low-Rank path) -> Update Momentum -> Apply Muon/Newton-Schulz -> Update Weights.

- **Critical path:**
  1. Compute Gradient $G$.
  2. **Period Start?** -> If yes, update Projector $P$ via SVD and **Reset** Momentum $R$.
  3. Sample Layer -> (Full-Rank path) OR (Low-Rank path).
  4. Update Momentum -> Apply Muon/Newton-Schulz -> Update Weights.

- **Design tradeoffs:**
  - **Rank ($r$) vs. Sampling Rate ($q$):** Increasing $r$ improves low-rank approximation but costs memory. Increasing $q$ improves unbiasedness but costs memory (full-rank states). The paper suggests reducing $r$ (e.g., from 512 to 128) to "pay for" the memory cost of the full-rank sampling.
  - **Period Length ($K$):** Longer periods reduce SVD frequency (computationally cheaper) but allow momentum to drift further from the current subspace.

- **Failure signatures:**
  - **OOM (Out of Memory):** Likely caused by setting the number of sampled full-rank layers ($\gamma$) or the sampling probability ($q$) too high.
  - **Non-convergence/Oscillation:** Occurs if $q$ is effectively 0 (reverting to biased GaLore-Muon in noisy settings) or if the momentum reset interval $K$ is misaligned with the learning rate schedule.

- **First 3 experiments:**
  1. **Synthetic Convergence Check:** Reproduce Figure 1 (Linear Regression with Noise) to verify that GUM converges while biased GaLore diverges under high noise. This validates the "unbiased" implementation.
  2. **Memory/Performance Sweep:** Fine-tune a small model (e.g., LLaMA-60M) sweeping $r \in \{64, 128, 256\}$ and $\gamma \in \{1, 2, 4\}$ to find the Pareto optimal point where memory equals GaLore but accuracy is higher (Table 3 analysis).
  3. **Spectral Analysis:** After a short pretraining run, plot the singular value distribution of the weights (Figure 3/5 style) to confirm that GUM produces a "longer tail" compared to GaLore.

## Open Questions the Paper Calls Out
- Can standard variance reduction techniques be integrated into GUM to stabilize training instability caused by the inherent high variance of sampled high-rank updates?
- How does GUM perform empirically and computationally when applied to non-Transformer architectures, such as Vision Transformers (ViT), Mamba models, or Diffusion models?
- What are the theoretical and practical implications of combining GUM with acceleration techniques (e.g., learning rate warmup) or generalization methods (e.g., Sharpness-Aware Minimization)?

## Limitations
- The specific implementation details of the Muon optimizer's Newton-Schulz iteration (number of iterations, coefficients) are not fully specified in the main text.
- The definition of a "block" in the context of layerwise sampling is ambiguous (single linear layer vs. entire Transformer block).
- The exact modification to Algorithm 2 referenced in Appendix C.1 for full-parameter Muon recovery is unclear.

## Confidence
- **High Confidence:** The core theoretical claim that GUM is an unbiased estimator of the full gradient, and that it inherits Muon's convergence guarantees under the stated commutativity assumptions.
- **Medium Confidence:** The empirical performance improvements reported across all tasks (fine-tuning and pretraining). The results are consistently positive but rely on specific hyperparameter choices and datasets.
- **Medium Confidence:** The mechanistic explanation for pretraining gains (more uniform knowledge distribution, improved stable rank). While supported by spectral analysis, the causal link to memorization and generalization is less directly proven.

## Next Checks
1. **Reproduce Synthetic Convergence (Figure 1):** Run the linear regression with noise experiment to verify that GUM converges under high noise while biased GaLore diverges. This directly validates the unbiasedness claim.
2. **Memory-Performance Sweep:** Conduct a hyperparameter sweep (r ∈ {64, 128, 256}, γ ∈ {1, 2, 4}) on a small LLaMA-60M fine-tuning task to find the Pareto optimal point and verify the memory savings claimed in Table 3.
3. **Spectral Distribution Analysis:** After a short pretraining run (e.g., 1-2% of total steps), plot the singular value distribution of the model weights. Confirm that GUM produces a longer-tailed distribution compared to GaLore, supporting the mechanistic explanation for its gains.