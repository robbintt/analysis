---
ver: rpa2
title: 'MALT: Mechanistic Ablation of Lossy Translation in LLMs for a Low-Resource
  Language: Urdu'
arxiv_id: '2502.00041'
source_url: https://arxiv.org/abs/2502.00041
tags:
- llms
- translation
- languages
- language
- low-resource
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of poor performance of large
  language models (LLMs) on low-resource languages like Urdu, which are underrepresented
  in training data. The core idea is that LLMs primarily reason in English even when
  prompted in another language, with final layers acting as lossy translators to the
  target language.
---

# MALT: Mechanistic Ablation of Lossy Translation in LLMs for a Low-Resource Language: Urdu

## Quick Facts
- arXiv ID: 2502.00041
- Source URL: https://arxiv.org/abs/2502.00041
- Authors: Taaha Saleem Bajwa
- Reference count: 11
- Key outcome: MALT improves LLM performance on Urdu from 11.6% to 55% accuracy by mechanistically removing lossy translation features

## Executive Summary
This paper addresses the challenge of poor performance of large language models (LLMs) on low-resource languages like Urdu, which are underrepresented in training data. The core idea is that LLMs primarily reason in English even when prompted in another language, with final layers acting as lossy translators to the target language. The proposed method, MALT, mechanistically removes these translation features from the final layers and uses a dedicated machine translation model instead. Experiments on Gemma-2-2b and Llama-3.2-3b show significant improvements: Llama-3.2-3b accuracy increased from 11.6% to 55%, while Gemma-2-2b improved from 0% to 15.6%. The approach preserves cultural nuances of the input language while producing more coherent responses.

## Method Summary
MALT operates by identifying and removing translation-specific features from the residual stream of LLM final layers. The method computes mean residual activations for English vs. Urdu prompts at a target layer, subtracts to find the translation direction, normalizes it, and projects it out from all activations during inference. The resulting English latent response is then translated to Urdu using a dedicated machine translation model. This avoids the lossy translation that occurs when LLMs handle low-resource languages directly, while preserving cultural context from the original input.

## Key Results
- Llama-3.2-3b accuracy improved from 11.6% to 55% on Urdu questions
- Gemma-2-2b accuracy improved from 0% to 15.6% on Urdu questions
- Human evaluation showed improved fluency and relevance of English latent responses
- Cultural nuances appear preserved in the latent representation before translation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs reason internally in English even when prompted in low-resource languages, with final layers functioning as translators to the target language.
- **Mechanism:** The model processes Urdu input → computes latent English representation → final layers project to Urdu output. The translation projection is lossy for low-resource languages.
- **Core assumption:** The English latent response is coherent; performance degradation occurs primarily at the translation stage.
- **Evidence anchors:**
  - [abstract] "LLMs primarily reason in English when prompted in another language, with the final layers acting as translators"
  - [section 1] "internal latent response of LLMs in English is quite coherent; however, the translation features are lossy"
  - [corpus] Wendler et al. (2024) cited as prior evidence for English-centric latent reasoning
- **Break condition:** If the model's latent English response is itself incoherent (due to poor comprehension of Urdu input), ablation will not help.

### Mechanism 2
- **Claim:** Low-resource language generation is mediated by a single direction in the residual stream of final layers, which can be mechanistically identified and ablated.
- **Mechanism:** Compute mean residual activations for English vs. Urdu prompts at a target layer → subtract to find translation direction → normalize → project out this direction from all activations.
- **Core assumption:** Translation features localize to a specific layer and direction (layer 24 for Gemma-2-2b, layer 25 for Llama-3.2-3b).
- **Evidence anchors:**
  - [section 2.3] Formula: dℓ,norm = (murd,ℓ − meng,ℓ) / ||murd,ℓ − meng,ℓ||
  - [section 2.4] Ablation via: Rablation = R − ((R · dℓ,norm) · dℓ,norm)
  - [corpus] Limited direct corpus evidence; related work on language-specific neurons (Tang et al., 2024) cited but not directly replicated
- **Break condition:** If translation features are distributed across multiple layers (likely in larger models), single-direction ablation will be incomplete or cause collateral damage.

### Mechanism 3
- **Claim:** Preserving input in the original language maintains cultural context that would be lost via pre-translation pipelines.
- **Mechanism:** Urdu input → model processes with cultural context intact → ablate translation → extract English latent response → external MT translates back to Urdu.
- **Core assumption:** The model encodes cultural nuances from Urdu input into its latent English representation.
- **Evidence anchors:**
  - [section 3.3] "cultural nuances are seem to be somewhat preserved"
  - [figure 3] Example: "gene" interpreted as "ghost" due to Urdu script similarity, with Quranic references reflecting Muslim cultural context
  - [corpus] No corpus validation; explicitly noted as not quantitatively verified
- **Break condition:** If cultural context is lost during encoding (not just translation), the approach provides no advantage over translate-then-process pipelines.

## Foundational Learning

- **Concept: Residual stream and activation steering**
  - Why needed here: MALT operates by computing directions in residual space and projecting them out; requires understanding how activations flow through layers.
  - Quick check question: Can you explain why subtracting a normalized direction from activations removes specific features without retraining?

- **Concept: Polysemantic neurons**
  - Why needed here: The paper attributes error types (fluency, repetition, non-relevance) to collateral ablation of non-translation features.
  - Quick check question: Why might ablating a "translation direction" also affect unrelated model capabilities?

- **Concept: Low-resource vs. high-resource language behavior in LLMs**
  - Why needed here: The approach is motivated by the specific failure mode of low-resource languages where translation features are undertrained.
  - Quick check question: Why would translation features for Urdu be "lossier" than for French in an English-dominant model?

## Architecture Onboarding

- **Component map:**
  - Target LLMs: Gemma-2-2b (26 layers, intervention at layer 24) → Llama-3.2-3b (28 layers, intervention at layer 25)
  - Translation model: mBART fine-tuned for Urdu (abdulwaheed1/english-to-urdu-translation-mbart)
  - Intervention library: TransformerLens for activation caching and ablation
  - Evaluation: Human review of English latent responses (223 questions)

- **Critical path:**
  1. Cache residual activations for N=16 English and Urdu prompts at target layer
  2. Compute translation direction via mean difference and normalization
  3. During inference, ablate direction at each token position
  4. Extract English output and translate externally

- **Design tradeoffs:**
  - Single-layer vs. multi-layer ablation: Paper finds single layer sufficient for 2-3B models; larger models may require distributed intervention
  - Ablation aggressiveness: Stronger removal risks polysemantic collateral damage (observed as fluency/repetition errors)
  - Translation model choice: Better MT improves final output but adds latency and dependency

- **Failure signatures:**
  - Fluency error: Garbage output suggests over-ablation or wrong layer
  - Repetition error: Model echoes query without answering—may indicate comprehension failure, not translation failure
  - Non-relevant error: Coherent but off-topic—suggests input understanding was compromised

- **First 3 experiments:**
  1. Validate direction identification: Ablate translation direction and verify output switches to English without quality degradation on English prompts.
  2. Layer sweep: Test ablation at layers 20-28 to confirm optimal intervention point; compare accuracy and error profiles.
  3. Cross-language generalization: Apply same method to another low-resource language (e.g., Swahili) to test whether the approach transfers or requires language-specific calibration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the MALT methodology generalize effectively to low-resource languages other than Urdu, or is it language-specific?
- Basis in paper: [explicit] The limitations section states, "It remains to be seen how MALT generalizes to other low resource languages and does this technique have significant performance gaps for different low resource languages."
- Why unresolved: The study exclusively used Urdu as a use case for low-resource languages.
- What evidence would resolve it: Applying MALT to a diverse set of low-resource languages (e.g., Swahili, Yoruba) and comparing the performance delta against the baseline.

### Open Question 2
- Question: Do translation features become distributed across multiple layers in larger LLMs, making mechanistic ablation significantly more complex?
- Basis in paper: [explicit] The paper notes, "As the size of LLMs increases, the translation features are expected to become more distributed across multiple final layers... it remains to be seen how challenging this process is for low-resource languages."
- Why unresolved: Computational constraints limited the study to smaller models (2-3B parameters), where features were localized to a single direction.
- What evidence would resolve it: Replicating the ablation experiment on larger models (e.g., 7B or 70B parameters) and analyzing the layer-wise distribution of translation features.

### Open Question 3
- Question: To what extent are cultural nuances quantitatively preserved in the latent English representation compared to the original input?
- Basis in paper: [explicit] The paper states, "Upon close observation of outputs, it seems that cultural nuances are seem to be