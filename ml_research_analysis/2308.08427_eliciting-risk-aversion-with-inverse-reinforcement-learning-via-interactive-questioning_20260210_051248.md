---
ver: rpa2
title: Eliciting Risk Aversion with Inverse Reinforcement Learning via Interactive
  Questioning
arxiv_id: '2308.08427'
source_url: https://arxiv.org/abs/2308.08427
tags:
- risk
- aversion
- learning
- lemma
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes an interactive inverse reinforcement learning\
  \ framework to elicit non-expert clients' risk aversion using binary-choice questionnaires.\
  \ The method models risk aversion with cost functions and spectral risk measures,\
  \ proving finite-sample identifiability and a \u221AN convergence rate (up to logarithmic\
  \ factors)."
---

# Eliciting Risk Aversion with Inverse Reinforcement Learning via Interactive Questioning

## Quick Facts
- arXiv ID: 2308.08427
- Source URL: https://arxiv.org/abs/2308.08427
- Reference count: 40
- Primary result: Interactive IRL framework elicits risk aversion via binary-choice questionnaires with <50 questions

## Executive Summary
This paper presents an interactive inverse reinforcement learning framework to elicit non-expert clients' risk aversion parameters using adaptive binary-choice questionnaires. The method models risk aversion with cost functions and spectral risk measures, proving finite-sample identifiability and convergence rates. By maximizing distinguishing power between candidate risk profiles, the approach achieves accurate estimation with fewer than 50 questions. The framework is validated through simulated experiments comparing different question design strategies, demonstrating superior efficiency over random sampling approaches.

## Method Summary
The approach uses a particle-based optimization algorithm to elicit risk aversion parameters $(c_0, \mu_0)$. It maintains a set of $K=1000$ particles representing candidate risk profiles, where each particle has a cost parameter $c$ and spectral density $\mu$ defined on a grid. The system selects binary-choice questions by maximizing expected distinguishing power across particle pairs, then updates particles via gradient descent based on client responses. The algorithm iteratively eliminates underperforming particles and spawns new ones to maintain coverage of the hypothesis space.

## Key Results
- Proved finite-sample identifiability of risk profiles through adaptive questioning
- Achieved estimation accuracy with fewer than 50 questions in simulated experiments
- Demonstrated superior efficiency compared to random question selection strategies
- Extended framework to infinite-horizon settings with qualitative identifiability

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Environment Design for Identifiability
The system constructs binary-choice questions that function as "separating environments" to resolve the identifiability problem in IRL. By observing client choices, it eliminates incompatible risk profiles, ensuring the hypothesis space shrinks with every interaction. The paper proves that for any distinct risk profiles, a specific question exists that forces divergent choices, guaranteeing progress.

### Mechanism 2: Maximizing Distinguishing Power ($\Psi$)
The system calculates $\Psi$ for candidate questions based on how differently two leading risk hypotheses would answer them. By maximizing $\Psi$, it selects questions that maximize the optimality gap between options, thereby maximizing information gain per round. This information-theoretic approach accelerates convergence compared to random questioning.

### Mechanism 3: Particle-Based Approximation
The infinite-dimensional space of risk measures is approximated using a finite set of discrete particles. The algorithm maintains candidate risk profiles as particles and uses gradient-descent-inspired updates based on binary choices. Particles with high loss are eliminated while new ones are spawned to maintain coverage, enabling efficient exploration of the hypothesis space.

## Foundational Learning

- **Spectral Risk Measures (SRMs)**: Weighted integral of distribution tails allowing non-parametric risk aversion modeling. *Quick check*: Can you explain how $\rho_\mu$ differs from a simple expectation in terms of outcome weighting?
- **Inverse Reinforcement Learning (IRL) Identifiability**: Recovering an agent's internal objective from behavior. *Quick check*: Why is it generally impossible to recover a unique reward function from static demonstrations without additional constraints?
- **Wasserstein Distance**: Metric defining distance between probability measures for convergence analysis. *Quick check*: How does Wasserstein distance handle "distance" between distributions compared to Total Variation?

## Architecture Onboarding

- **Component map**: Question Generator -> Client Interface -> Loss Computation -> Particle System -> Updater
- **Critical path**: Question Design (maximize $\Psi$) → Client Query → Loss Computation → Particle Update → (Repeat until $\Psi \approx 0$)
- **Design tradeoffs**: 
  - Grid Size ($J$): Larger $J$ enables higher fidelity but increases computational cost
  - Particle Count ($K$): More particles improve coverage but slow optimization
  - Learning Rate ($\nu$): Aggressive rates might miss true profile; conservative rates require more questions
- **Failure signatures**: 
  - Oscillation: Particles fail to converge, indicating low distinguishing power or high noise
  - Early Collapse: All particles except one eliminated prematurely, suggesting overly aggressive elimination
  - Stagnation: Error metrics plateau, suggesting model misspecification
- **First 3 experiments**:
  1. Reproduce Experiment I using small fixed candidate profiles to verify distinguishing power logic
  2. Run Experiment II varying grid density ($J$) to visualize estimation error vs computational time tradeoff
  3. Simulate client with risk profile outside candidate grid to observe misspecification handling

## Open Questions the Paper Calls Out

- **Can a single multiple-choice question distinguish between different risk aversion profiles when the cost function is not identity ($C(x) \neq x$)?** The current proof only covers $C(x)=x$, and generalization remains open.
- **What is the theoretical convergence rate in the infinite-horizon dynamic setting?** The static case has derived rates, but the dynamic case lacks quantitative analysis.
- **Is it possible to establish identifiability in a non-parametric Bayesian framework?** Formulating the framework requires working with probability measures on probability measures, presenting significant technical complexity.

## Limitations

- Theoretical guarantees assume deterministic, error-free client behavior, which may not hold in practice
- Particle-based approximation may converge to suboptimal solutions if true risk profile lies outside discrete grid representation
- Brute-force question design optimization becomes computationally prohibitive for higher-dimensional problems

## Confidence

- **High Confidence**: Theoretical framework for interactive question design and finite-sample identifiability proofs
- **Medium Confidence**: Convergence rate theorem and practical effectiveness of particle-based algorithm
- **Low Confidence**: Extension to infinite-horizon settings is preliminary without quantitative validation

## Next Checks

1. Implement stochastic client model with 5-10% error rate to measure impact on convergence rates compared to deterministic case
2. Design experiments with true risk profile outside candidate particle space to quantify approximation error and identify failure modes
3. Extend question design optimization to 10-state MDP and compare computational cost of brute-force search against gradient-based approaches