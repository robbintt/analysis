---
ver: rpa2
title: Developing synthetic microdata through machine learning for firm-level business
  surveys
arxiv_id: '2512.05948'
source_url: https://arxiv.org/abs/2512.05948
tags:
- data
- synthetic
- business
- pums
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study demonstrates how synthetic microdata can be generated\
  \ using machine learning for firm-level business surveys while preserving privacy\
  \ and statistical fidelity. The approach uses CART-based synthesizers\u2014specifically\
  \ CenSyn and synthpop\u2014to model the distribution of sensitive survey data and\
  \ generate synthetic datasets that maintain the original data's statistical properties\
  \ without exposing individual records."
---

# Developing synthetic microdata through machine learning for firm-level business surveys

## Quick Facts
- arXiv ID: 2512.05948
- Source URL: https://arxiv.org/abs/2512.05948
- Reference count: 20
- Primary result: Synthetic microdata generation using CART-based synthesizers achieves high statistical fidelity while preserving privacy

## Executive Summary
This study demonstrates how synthetic microdata can be generated using machine learning for firm-level business surveys while preserving privacy and statistical fidelity. The approach uses CART-based synthesizers—specifically CenSyn and synthpop—to model the distribution of sensitive survey data and generate synthetic datasets that maintain the original data's statistical properties without exposing individual records. Quality evaluation was performed using the 2007 Survey of Business Owners public-use file, treating it as the "ground truth." Two synthetic datasets were generated with approximately 1 million firms each, achieving k-marginal scores of 992 (baseline: 994). Principal component analysis showed strong alignment between the synthetic and original data distributions across the top five principal components, which together captured about 25% of the total variance.

The synthetic datasets were further validated by replicating a high-impact econometric analysis on immigrant-owned firm transnational activities from Small Business Economics. The results from synthetic data closely matched the original findings across logistic and ordinary least squares models, with odds ratios and coefficient estimates showing qualitative consistency and most confidence intervals overlapping. Some quantitative differences emerged, particularly for rare events and multi-variable interactions, indicating areas for future refinement. Overall, the results suggest synthetic data can serve as a viable alternative to restricted-use data for exploratory and specification-testing research while preserving privacy.

## Method Summary
The study employs CART-based synthesizers (CenSyn and synthpop) to generate synthetic firm-level business survey data. The 2007 Survey of Business Owners public-use file serves as the ground truth for validation. The synthetic generation process involves modeling the joint distribution of survey variables using classification and regression trees, then sampling from this distribution to create synthetic records. Two synthetic datasets of approximately 1 million firms each were created. Quality assessment includes k-marginal distance metrics comparing marginal distributions, principal component analysis to evaluate multivariate structure preservation, and replication of published econometric analyses using the synthetic data to test substantive validity.

## Key Results
- Synthetic datasets achieved k-marginal scores of 992 versus 994 for baseline (original data)
- PCA analysis showed strong alignment across top five principal components capturing ~25% of variance
- Replicated econometric analysis on immigrant-owned firm activities with qualitative consistency in odds ratios and coefficient estimates
- Most confidence intervals overlapped between synthetic and original results, with some quantitative differences for rare events

## Why This Works (Mechanism)
The CART-based synthesizers work by recursively partitioning the feature space and modeling conditional distributions at each node. This approach captures complex, non-linear relationships and interactions between variables without requiring parametric assumptions. The synthesizers learn the joint distribution of sensitive survey variables and generate synthetic records by sampling from these learned distributions, effectively preserving statistical properties while ensuring individual records cannot be reverse-engineered. The recursive partitioning naturally handles mixed data types and missing values, making it well-suited for complex survey data structures.

## Foundational Learning
- CART (Classification and Regression Trees): Recursive partitioning algorithm for modeling conditional distributions; needed to capture non-linear relationships in survey data; quick check: visualize tree depth and node purity
- k-marginal distance metrics: Statistical measure comparing marginal distributions between original and synthetic data; needed to quantify synthesis quality; quick check: compute k-marginal for individual variables
- Principal Component Analysis: Dimensionality reduction technique for evaluating multivariate structure preservation; needed to assess whether synthetic data maintains joint distribution patterns; quick check: compare explained variance ratios
- Logistic regression and OLS model replication: Substantive validation through econometric analysis replication; needed to demonstrate practical utility beyond statistical metrics; quick check: compare coefficient estimates and confidence intervals
- Synthetic data generation from learned distributions: Sampling methodology that preserves privacy while maintaining statistical properties; needed to create usable datasets without exposing sensitive information; quick check: verify synthetic records don't exactly match original records

## Architecture Onboarding
**Component map:** Survey data -> CART synthesizer (CenSyn/synthpop) -> Learned distribution model -> Synthetic data generation -> Quality validation (k-marginal, PCA) -> Econometric replication

**Critical path:** The core synthesis pipeline runs: data preprocessing → CART model training → synthetic record generation → validation through statistical metrics → substantive validation through model replication

**Design tradeoffs:** CART-based approaches balance model complexity with interpretability and computational efficiency. While they can capture non-linear relationships effectively, they may struggle with very rare events or require careful tuning of tree depth and pruning parameters. The choice between CenSyn and synthpop involves tradeoffs between different CART implementations and their handling of specific data characteristics.

**Failure signatures:** Poor k-marginal scores indicate failure to preserve marginal distributions; misalignment in PCA suggests loss of multivariate structure; inconsistent econometric results point to substantive validity issues; exact matches between synthetic and original records indicate privacy risks.

**First experiments:** 1) Generate synthetic data with varying tree depths to assess sensitivity; 2) Compare k-marginal scores across different synthesizer parameters; 3) Test synthetic data on alternative econometric specifications not used in validation

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis based on single year of survey data (2007), limiting temporal generalizability
- Performance uncertainty for different survey types and industry sectors
- Quantitative discrepancies for rare events and multi-variable interactions suggest limitations for extreme value analysis
- Single survey focus leaves uncertainty about broader applicability

## Confidence
High: Core methodology and general applicability to exploratory research
Medium: Specific quantitative precision in complex econometric models and rare event analysis
Low: Generalizability to other survey types and economic contexts

## Next Checks
1. Replicate synthetic generation across multiple years of SBO data to assess temporal stability and robustness
2. Test synthesizers on alternative firm-level surveys with different structures to evaluate broader applicability
3. Conduct systematic sensitivity analysis on synthetic sample sizes and synthesizer parameters, particularly for rare events and multi-variable interactions