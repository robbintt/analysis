---
ver: rpa2
title: Towards Assessing Medical Ethics from Knowledge to Practice
arxiv_id: '2508.05132'
source_url: https://arxiv.org/abs/2508.05132
tags:
- medical
- ethical
- ethics
- knowledge
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PrinciplismQA addresses the gap in assessing LLMs\u2019 ethical\
  \ reasoning by introducing a comprehensive benchmark with 3,648 questions grounded\
  \ in medical ethics principles. The dataset combines multiple-choice questions from\
  \ authoritative textbooks and open-ended questions from real clinical ethics cases,\
  \ all validated by medical experts."
---

# Towards Assessing Medical Ethics from Knowledge to Practice

## Quick Facts
- arXiv ID: 2508.05132
- Source URL: https://arxiv.org/abs/2508.05132
- Reference count: 12
- PrinciplismQA reveals significant knowledge-practice gap in LLM medical ethics reasoning, with models struggling most in Beneficence application.

## Executive Summary
PrinciplismQA introduces a comprehensive benchmark to assess LLMs' medical ethics reasoning across four principles: Autonomy, Beneficence, Non-maleficence, and Justice. The benchmark combines 2,182 multiple-choice questions from authoritative textbooks with 1,466 open-ended clinical ethics cases, all validated by medical experts. Experiments reveal a significant knowledge-practice gap, with models performing substantially better on factual recall than real-world ethical application. Medical domain fine-tuning improves practical competence but risks forgetting core ethical knowledge, while reasoning-focused models show superior performance in complex scenarios.

## Method Summary
PrinciplismQA evaluates LLMs through two tasks: Knowledge (MCQA) and Practice (open-ended case analysis). Knowledge questions are curated from 350+ medical ethics textbooks (2010+), while Practice cases are extracted from AMA Journal of Ethics "Case and Commentary" articles. Each Practice question includes expert-validated keypoint checklists as reference answers. Evaluation uses GPT-4o as an LLM-as-a-Judge with a three-level rubric (0.0/0.5/1.0) per keypoint, achieving human-comparable reliability (ICC=0.71). Models are tested across various sizes and domains, with temperature=0.1 and max length=2048 tokens.

## Key Results
- Models show significant Knowledge-Practice gap, scoring higher on factual recall than real-world application
- Beneficence is the weakest principle across all models, with most LLMs over-emphasizing other principles
- Medical domain fine-tuning improves Practice scores but risks Knowledge score degradation
- Reasoning-focused models (e.g., o3) outperform chat-oriented variants in complex ethical scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Medical domain fine-tuning improves practical ethical reasoning in LLMs.
- Mechanism: Exposure to clinical data and expert annotations emphasizing patient well-being increases model sensitivity to beneficence-oriented reasoning in open-ended scenarios.
- Core assumption: The medical training corpora contain ethical reasoning patterns that transfer to novel cases.
- Evidence anchors:
  - [abstract] "Notably, medical domain fine-tuning can enhance models' overall ethical competence."
  - [Table 6] Medical LLMs consistently show Practice score gains.
  - [corpus] Weak external validation; MedEthicsQA also finds domain-specific evaluation valuable but does not test fine-tuning effects directly.
- Break condition: If the medical training data lacks ethically annotated examples, fine-tuning may not improve and could degrade ethical reasoning.

### Mechanism 2
- Claim: Stronger general reasoning capability improves ethical application more than domain knowledge alone.
- Mechanism: Models with extended reasoning chains can better navigate principle conflicts and generate nuanced justifications in ambiguous scenarios.
- Core assumption: Ethical dilemmas require multi-step reasoning that benefits from general reasoning capacity.
- Evidence anchors:
  - [section 4.2] "Reasoning-focused variants...consistently surpassed their chat-oriented counterparts in Practice scenarios."
  - [Table 6] o3 achieves highest Practice score vs. GPT-4.1, despite similar Knowledge scores.
  - [corpus] Limited direct evidence; no neighbor papers specifically compare reasoning vs. chat model variants on ethics benchmarks.
- Break condition: If reasoning models over-analyze and generate verbose but irrelevant justifications, Practice scores could decrease.

### Mechanism 3
- Claim: LLM-as-a-Judge evaluation achieves human-comparable reliability for structured ethical assessments.
- Mechanism: GPT-4o scores keypoints using a three-level rubric, with consistency driven by the structured prompt and explicit rubric criteria.
- Core assumption: Ethical reasoning can be decomposed into discrete, independently scorable keypoints.
- Evidence anchors:
  - [section 4.4] "ICC between GPT-4o and the human mean was 0.71," compared to human-human ICC of 0.67.
  - [Figure 2] Stepwise evaluation shows per-keypoint scoring with justifications.
  - [corpus] No neighbor papers validate LLM-as-a-Judge specifically for medical ethics; this is a novel contribution.
- Break condition: If model responses are adversarially crafted or keypoints are ambiguously defined, judge reliability may degrade.

## Foundational Learning

- Concept: **Principlism framework (Autonomy, Beneficence, Non-maleficence, Justice)**
  - Why needed here: The benchmark's structure and scoring depend on classifying questions by principle; understanding these four pillars is prerequisite to interpreting results.
  - Quick check question: Given a scenario where a patient refuses life-saving treatment, which principle(s) are in tension?

- Concept: **Knowledge-Practice Gap in ethical reasoning**
  - Why needed here: The paper's central finding; models score higher on MCQA (factual recall) than open-ended application.
  - Quick check question: Why might a model correctly identify the definition of beneficence but fail to recommend actions promoting patient well-being?

- Concept: **LLM-as-a-Judge paradigm**
  - Why needed here: Evaluates open-ended responses at scale; requires understanding of rubric-based scoring and inter-rater reliability metrics (ICC).
  - Quick check question: What safeguards prevent an LLM judge from systematically favoring responses from its own model family?

## Architecture Onboarding

- Component map:
  Knowledge MCQA pipeline: Textbook extraction (GPT-4o) → COI segmentation → MCQA generation → SOTA filtering → Expert validation → 2,182 questions
  Practice Open-ended pipeline: AMA case extraction → Keypoint decomposition → ACGME competency tagging → Expert validation → 1,466 questions with 6,555 keypoints
  Evaluation layer: LLM-as-a-Judge (GPT-4o) with 3-level rubric per keypoint → Aggregated Practice scores (0-1)

- Critical path: Data curation quality → Keypoint precision → Judge calibration. Weakness in any stage compounds downstream; expert validation retention rates (87.3% MCQA, 96.4% open-ended) indicate high pipeline quality.

- Design tradeoffs:
  Breadth vs. depth: 3,648 questions cover four principles broadly, but single-case-multiple-questions design may not capture principle interaction complexity fully.
  Automation vs. validity: LLM-as-a-Judge enables scalability (ICC=0.71) but may miss subtle reasoning errors a human expert would catch.
  General vs. medical LLMs: Medical fine-tuning improves Practice but risks Knowledge forgetting (e.g., MedGemma-27B: 65.5% → 64.4% Knowledge).

- Failure signatures:
  Beneficence underperformance: Models prioritize autonomy/justice over patient well-being.
  Practice-Based Learning competency collapse: Lowest scores across models, suggesting inability to adapt reasoning contextually.
  Knowledge drop after medical fine-tuning: Indicates catastrophic forgetting of ethical principles not reinforced in medical corpora.

- First 3 experiments:
  1. Baseline diagnostic: Run a sample model on 50 MCQAs and 20 open-ended cases across all four principles; identify which principle shows the largest Knowledge-Practice gap.
  2. Judge calibration check: Manually score 20 model responses using the keypoint rubric; compare to LLM-as-a-Judge scores to verify alignment (target: <15% discrepancy).
  3. Ablation on reasoning: Compare performance of a model with and without chain-of-thought prompting on 30 ambiguous Practice cases; quantify reasoning benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can medical domain fine-tuning be optimized to improve practical ethical reasoning without causing the observed "catastrophic forgetting" of theoretical ethical knowledge?
- Basis in paper: [explicit] The conclusion states that medical fine-tuning enhances practical competence but risks "forgetting core ethical knowledge," calling for future work to prioritize alignment to prevent this.
- Why unresolved: The paper identifies the trade-off but does not propose or test specific training methodologies to mitigate this specific type of forgetting.
- What evidence would resolve it: Experiments comparing standard fine-tuning against techniques like mixture-of-experts or replay buffers that simultaneously maintain or improve Knowledge accuracy while boosting Practice scores.

### Open Question 2
- Question: Does the persistent underperformance in *Beneficence* (compared to Autonomy and Justice) stem from an over-alignment towards safety/refusal (Non-maleficence) in general LLM training data?
- Basis in paper: [inferred] The authors note that models "struggle most with beneficence" and often prioritize other principles. This suggests a bias in the base models toward avoiding harm rather than actively pursuing good.
- Why unresolved: The paper quantifies the performance gap but does not isolate the specific data distributions or RLHF objectives that cause models to de-prioritize active well-being.
- What evidence would resolve it: An ablation study varying the ratio of "safety" vs. "proactive care" examples in the training data to observe the causal impact on Beneficence scores.

### Open Question 3
- Question: Does the "Knowledge-Practice Gap" persist uniformly across different cultural contexts, or is it exacerbated by the Western-centric nature of the "Principlism" framework?
- Basis in paper: [inferred] The dataset relies on "authoritative textbooks" and JAMA cases (Western consensus). The paper does not evaluate if models trained on non-Western medical data exhibit the same gap when applying different ethical frameworks.
- Why unresolved: The benchmark assumes a universal applicability of the four principles, leaving the interaction between the model's implicit cultural biases and the benchmark's explicit cultural grounding unexplored.
- What evidence would resolve it: Evaluating the same models on a translated dataset derived from non-Western medical ethics case studies to see if the Practice scores degrade further or if the "Beneficence" imbalance shifts.

## Limitations
- Dataset Availability: The PrinciplismQA benchmark dataset is not yet publicly available, preventing independent validation and reproduction.
- Generalizability: Findings may not extend beyond the specific medical ethics domains covered (AMA Journal of Ethics cases).
- LLM-as-a-Judge Reliability: While ICC=0.71 suggests good agreement, reliability for nuanced ethical reasoning in novel contexts requires further validation.

## Confidence
- High: The core finding that LLMs exhibit a Knowledge-Practice gap in medical ethics is well-supported by empirical results and expert validation rates.
- Medium: The claim that medical domain fine-tuning improves practical ethical reasoning is supported but requires further validation due to potential confounding factors.
- Medium: The reliability of LLM-as-a-Judge (ICC=0.71) is promising but limited by the lack of external validation and potential model-specific biases.

## Next Checks
1. **Dataset Release and Independent Reproduction**: Once PrinciplismQA is publicly available, independently reproduce key results (e.g., Knowledge-Practice gap, Beneficence underperformance) using a diverse set of LLMs and medical ethics scenarios.
2. **Cross-Cultural and Cross-Domain Validation**: Test the benchmark's applicability to medical ethics cases from non-Western contexts and other healthcare domains to assess generalizability.
3. **Ablation Study on Fine-tuning Data**: Conduct an ablation study to isolate the impact of ethically annotated examples in medical corpora on LLM performance, addressing concerns about catastrophic forgetting and domain-specific biases.