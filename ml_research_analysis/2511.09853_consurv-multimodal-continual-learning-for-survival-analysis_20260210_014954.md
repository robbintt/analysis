---
ver: rpa2
title: 'ConSurv: Multimodal Continual Learning for Survival Analysis'
arxiv_id: '2511.09853'
source_url: https://arxiv.org/abs/2511.09853
tags:
- learning
- survival
- datasets
- data
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in multimodal continual
  learning for cancer survival prediction, where models struggle to retain knowledge
  from prior cancer datasets while learning new ones. The authors propose ConSurv,
  the first method to integrate WSIs and genomic data across multiple cancer types.
---

# ConSurv: Multimodal Continual Learning for Survival Analysis

## Quick Facts
- arXiv ID: 2511.09853
- Source URL: https://arxiv.org/abs/2511.09853
- Reference count: 40
- First method to integrate WSIs and genomic data across multiple cancer types for multimodal continual learning in survival analysis

## Executive Summary
ConSurv addresses catastrophic forgetting in multimodal continual learning for cancer survival prediction, where models struggle to retain knowledge from prior cancer datasets while learning new ones. The authors propose ConSurv, the first method to integrate WSIs and genomic data across multiple cancer types. ConSurv employs Multi-staged Mixture of Experts (MS-MoE) to capture shared and task-specific knowledge at different network stages, and Feature Constrained Replay (FCR) to preserve learned features through distillation. A new benchmark, MSAIL, integrating four TCGA cancer datasets, is introduced for evaluation.

## Method Summary
ConSurv tackles catastrophic forgetting in multimodal continual learning for cancer survival analysis by introducing a novel framework that integrates whole-slide images (WSIs) and genomic data across multiple cancer types. The method employs Multi-staged Mixture of Experts (MS-MoE) to capture both shared and task-specific knowledge at different network stages, while Feature Constrained Replay (FCR) preserves learned features through distillation. The approach is evaluated on a new benchmark called MSAIL, which integrates four TCGA cancer datasets, demonstrating superior performance compared to state-of-the-art methods.

## Key Results
- ConSurv outperforms state-of-the-art methods with higher average C-index (0.601) and C-index IPCW (0.597)
- Better backward and forward transfer compared to baseline methods
- Effectively mitigates catastrophic forgetting while balancing stability-plasticity trade-offs

## Why This Works (Mechanism)
ConSurv addresses catastrophic forgetting by combining two key mechanisms: Multi-staged Mixture of Experts (MS-MoE) captures both shared and task-specific knowledge at different network stages, while Feature Constrained Replay (FCR) preserves learned features through distillation. This dual approach allows the model to maintain previously learned information while adapting to new cancer types and modalities.

## Foundational Learning
- **Catastrophic forgetting**: When neural networks learn new tasks, they tend to overwrite previously learned knowledge. Critical for understanding why traditional continual learning approaches fail in multimodal cancer survival analysis.
- **Mixture of Experts (MoE)**: A technique where multiple specialized networks (experts) are combined, with a gating network deciding which expert to use for each input. Quick check: Verify that the gating mechanism effectively balances between shared and task-specific knowledge.
- **Knowledge distillation**: A method to transfer knowledge from a larger model to a smaller one by matching output distributions. Quick check: Ensure the distillation loss is properly weighted to prevent interference with new learning.

## Architecture Onboarding

**Component Map:**
WSI + Genomic Data -> MS-MoE (Shared + Task-specific layers) -> FCR (Distillation) -> Survival Prediction

**Critical Path:**
The critical path involves processing multimodal inputs through MS-MoE to extract shared and task-specific features, followed by FCR to preserve learned representations, ultimately leading to survival prediction.

**Design Tradeoffs:**
The use of MS-MoE allows for flexible adaptation to new cancer types while maintaining shared knowledge, but increases model complexity. FCR helps preserve learned features but requires careful tuning of distillation weights to balance stability and plasticity.

**Failure Signatures:**
Potential failure modes include poor gating decisions in MS-MoE leading to inadequate feature extraction, or overly aggressive distillation in FCR causing interference with new learning. Monitor performance degradation on previously learned tasks as an indicator.

**First Experiments:**
1. Test MS-MoE with different numbers of experts to find optimal balance between shared and task-specific knowledge.
2. Evaluate FCR with varying distillation weights to optimize the stability-plasticity tradeoff.
3. Assess performance on held-out tasks to verify backward transfer capabilities.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation on a relatively small number of cancer types (four) in the MSAIL benchmark may not fully represent real-world clinical data diversity.
- Performance metrics show modest improvements (C-index values around 0.6), indicating room for further enhancement.
- Focus on two specific modalities (WSIs and genomic data) without exploring scalability to additional data types.

## Confidence
- **High confidence** in the novel integration of multimodal continual learning with survival analysis, as this is the first method to address catastrophic forgetting in this specific context.
- **Medium confidence** in the effectiveness of the MS-MoE and FCR components, as the ablation studies support their contributions but do not fully isolate their individual impacts.
- **Low confidence** in the generalizability of results to broader clinical settings, given the limited number of cancer types and modalities tested.

## Next Checks
1. Evaluate ConSurv on a larger and more diverse set of cancer types and modalities to assess scalability and robustness.
2. Conduct ablation studies to isolate the contributions of MS-MoE and FCR components, and test alternative architectural choices.
3. Perform cross-institutional validation using external datasets to ensure the method's applicability beyond the TCGA data used in this study.