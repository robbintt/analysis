---
ver: rpa2
title: Mind Reading or Misreading? LLMs on the Big Five Personality Test
arxiv_id: '2511.23101'
source_url: https://arxiv.org/abs/2511.23101
tags:
- complex
- essays
- trait
- prompt
- personality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates five large language models (LLMs) for binary\
  \ personality prediction from text using the Five Factor Model (Big Five) under\
  \ zero-shot conditions. The authors test two prompting strategies\u2014minimal and\
  \ enriched with linguistic and psychological cues\u2014across three datasets (Essays,\
  \ MyPersonality, Pandora) and five personality traits."
---

# Mind Reading or Misreading? LLMs on the Big Five Personality Test

## Quick Facts
- **arXiv ID:** 2511.23101
- **Source URL:** https://arxiv.org/abs/2511.23101
- **Reference count:** 40
- **Primary result:** Enriched prompts improve LLM output validity and class balance but introduce systematic positive bias toward predicting trait presence; no configuration yields consistently reliable predictions.

## Executive Summary
This study evaluates five large language models for zero-shot binary personality prediction using the Five Factor Model across three datasets. Two prompting strategies—minimal and enriched with psychological descriptors—are tested. While enriched prompts reduce invalid outputs and can improve class balance, they introduce a systematic positive bias toward predicting trait presence. Per-class recall proves more informative than aggregate metrics, revealing significant asymmetries masked by accuracy. The study concludes that current out-of-the-box LLMs are not yet suitable for reliable automatic personality prediction from text.

## Method Summary
The study performs zero-shot binary classification of Big Five personality traits (OCEAN) from text across three datasets: Essays (2,467 texts), MyPersonality (255 aggregated Facebook posts), and Pandora (14,221 Reddit posts). Two prompting strategies are tested: Simple (minimal instructions) and Complex (enriched with trait descriptions, IPIP facets, and Goldberg adjectives). Five models are evaluated: GPT-4, LLaMA 3.1, Phi-3, Gemma, and Mistral. Outputs are filtered to retain only valid binary responses (0 or 1), and evaluation focuses on per-class precision, recall, and F1 rather than aggregate accuracy.

## Key Results
- Enriched prompts reduce invalid outputs and improve class balance but introduce systematic positive bias toward predicting trait presence.
- Per-class recall is more informative than aggregate metrics like accuracy or macro-F1, which mask significant asymmetries in model performance.
- No configuration yields consistently reliable predictions; while lightweight open-source models can approach GPT-4's performance under enriched prompting, the results remain inconsistent.

## Why This Works (Mechanism)

### Mechanism 1: Semantic Priming and Positive Bias Induction
Providing detailed linguistic descriptors and facet-level cues acts as a strong semantic prime, lowering the decision threshold for the positive class. The model prioritizes pattern matching against provided positive descriptors over strict inference regarding trait absence, causing it to interpret ambiguous text as evidence of the trait.

### Mechanism 2: Metric Asymmetry in Evaluation
Aggregate metrics like accuracy fail to detect class-specific failures in personality prediction. A model can achieve high accuracy by predicting the majority or "primed" class, masking total failure to detect the minority class. Per-class recall reveals these asymmetries that aggregate measures conceal.

### Mechanism 3: Safety Alignment Interference
Proprietary models (GPT-4) fail via refusal due to safety alignment, treating psychological assessment tasks as sensitive. Open-source models lack this specific alignment and instead fail through incoherence or task misunderstanding, creating different failure modes across model families.

## Foundational Learning

- **Precision-Recall Trade-off & Class Imbalance**: Understanding why a model predicting "Trait Present" for 95% of inputs might have high accuracy but near-zero Recall_0 (negative class) is crucial for interpreting results.
- **Zero-Shot Prompting vs. Fine-Tuning**: Zero-shot performance establishes a lower bound for APPT capability without training data, serving as a stress test of baseline model understanding.
- **The "Positivity Bias" in LLMs**: Recognizing that richer prompts make models more liberal in guessing "present" rather than more conservative is essential for interpreting systematic prediction patterns.

## Architecture Onboarding

- **Component map:** Text data (Essays, MyPersonality, Pandora) → Prompting Interface (Simple vs. Complex) → Model Layer (5 LLMs) → Filter (removes invalid outputs) → Evaluation Engine (calculates per-class metrics)
- **Critical path:** Constructing Complex prompt using IPIP/Goldberg dictionaries, filtering invalid outputs before evaluation, calculating Recall_0 vs. Recall_1 to detect positive bias
- **Design tradeoffs:** Validity vs. Bias (Complex prompts ensure clean outputs but introduce positive bias), Model Size vs. Compliance (larger models refuse, smaller models hallucinate)
- **Failure signatures:** Refusal/Verbose (GPT-4), Positive Bias (all Complex), Incoherence (Phi-3 Simple)
- **First 3 experiments:**
  1. Baseline Validity Check: Run Simple Prompt across all models on Pandora subset to establish formatting compliance
  2. Bias Injection Test: Run Complex Prompt on MyPersonality to quantify positive bias by comparing "1" vs "0" prediction ratios
  3. Metric Sensitivity Analysis: Calculate both Accuracy and Per-Class Recall for Openness to verify if high accuracy masks low Recall_0

## Open Questions the Paper Calls Out
None

## Limitations
- The study cannot isolate whether observed biases stem from pretraining distribution, prompt engineering, or specific trait formulations used.
- Binarization of continuous personality scores introduces measurement uncertainty that affects "true" labels.
- The study does not explore in-context learning effects from prompt structure itself.

## Confidence

- **High Confidence:** Enriched prompts systematically increase positive class predictions; per-class recall is more informative than aggregate metrics.
- **Medium Confidence:** No configuration yields "reliably accurate" predictions (subjective threshold), open-source models can approach GPT-4 under enriched prompting (context-dependent).
- **Low Confidence:** Specific mechanism of "semantic priming" as cause of bias (not directly tested), safety alignment interference claim (based on observed refusals without ablation studies).

## Next Checks

1. **Bias Source Isolation:** Re-run with prompts including both positive and negative trait descriptors to determine if bias stems from cue asymmetry.
2. **In-Context Learning Probe:** Create prompts with balanced in-context demonstrations to measure if models can learn to correct biases within the prompt.
3. **Continuous vs. Binary Evaluation:** Use Pandora's continuous scores with correlation metrics to test if models capture personality gradients rather than just binary presence/absence.