---
ver: rpa2
title: 'KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity'
arxiv_id: '2512.05916'
source_url: https://arxiv.org/abs/2512.05916
tags:
- attention
- cache
- matrix
- arxiv
- kq-svd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces KQ-SVD, a cache compression method for transformer-based
  LLMs that targets the fundamental attention matrix directly rather than compressing
  keys or values separately. The key insight is that attention fidelity depends on
  key-query inner products, which prior methods neglect by compressing keys alone
  or treating queries and keys independently.
---

# KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity

## Quick Facts
- **arXiv ID:** 2512.05916
- **Source URL:** https://arxiv.org/abs/2512.05916
- **Reference count:** 20
- **Key outcome:** Introduces KQ-SVD method that directly compresses the attention matrix KQ^T rather than keys or values separately, achieving provably better approximation fidelity with closed-form optimal projections.

## Executive Summary
This paper introduces KQ-SVD, a novel approach to compressing transformer KV caches by targeting the fundamental attention matrix directly. Unlike prior methods that compress keys alone (K-SVD) or treat queries and keys independently (Eigen), KQ-SVD computes the optimal low-rank approximation of KQ^T under Frobenius norm. The method provides closed-form projection matrices A and B that provably minimize attention approximation error. Evaluated on Llama2 and Mistral models using the C4 dataset, KQ-SVD consistently outperforms existing approaches in attention output fidelity while being invariant to query/key norm imbalances.

## Method Summary
KQ-SVD formulates KV cache compression as a low-rank approximation problem on the attention matrix KQ^T. During calibration, the method collects key, query, and value caches from multiple sequences, concatenates them into large matrices, and computes the truncated SVD of KQ^T. The optimal projection matrices A = K^+Û and B = K^TÛ are derived from the top-R left singular vectors Û of KQ^T. At inference, compressed caches KA* are stored and queries are projected via B before reconstruction. Rank R is selected per layer based on spectral energy threshold ε=0.1. The method also extends to value-output compression and grouped-query attention settings.

## Key Results
- KQ-SVD achieves lower attention output approximation error than K-SVD and Eigen across all tested models (Llama2-7B/13B, Llama3-8B, Mistral-7B) and layers
- The method is provably optimal for standard multi-head attention and extends to grouped-query attention with stacked query matrices
- KQ-SVD is invariant to query/key norm imbalances, while Eigen degrades to K-SVD performance under such conditions
- The approach maintains high fidelity even at aggressive compression ratios, with relative Frobenius errors consistently below competing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct low-rank approximation of KQ^T preserves attention outputs more accurately than compressing keys alone
- Mechanism: KQ-SVD computes SVD of KQ^T and derives projections A=K^+Û and B=K^TÛ, ensuring compressed attention matrix is optimal rank-R approximation
- Core assumption: Calibration keys span the query-relevant subspace (R(K) contains R(Û))
- Evidence: Theorem 2 provides closed-form solution; experiments show consistently lower error than K-SVD/Eigen across models

### Mechanism 2
- Claim: K-SVD incurs provably higher attention approximation error than KQ-SVD
- Mechanism: K-SVD minimizes ∥K−Ḱ∥_F but ignores query interactions; Theorem 3 shows quantifiable error gap
- Core assumption: Singular value spectra of K and KQ^T differ (typical when queries emphasize specific key dimensions)
- Evidence: Theorem 3 provides exact error formula; Figure 1 shows K-SVD high attention error despite low key error

### Mechanism 3
- Claim: Eigen method degrades to K-SVD when key/query norms are unbalanced
- Mechanism: With ∥Q∥_F/∥K∥_F → 0, top-R singular vectors of [K; Q] converge to K's alone, ignoring query information
- Core assumption: Practical deployments may have unbalanced query/key matrices due to scaling/normalization
- Evidence: Theorem 4 proves convergence; Figure 2 shows Eigen error converging to K-SVD under imbalance

## Foundational Learning

- **Concept:** Singular Value Decomposition (SVD) and Low-Rank Approximation
  - Why needed: KQ-SVD relies on Eckart-Young-Mirsky theorem for optimality of truncated SVD under Frobenius norm
  - Quick check: Given matrix M with singular values [10, 5, 1, 0.1], what rank-2 approximation error (in Frobenius norm squared) do you expect?

- **Concept:** Multi-Head Attention and KV Cache
  - Why needed: Method targets attention matrices QK^T/√d and caches K, V to compress
  - Quick check: In autoregressive decoding with sequence length T, why does KV cache grow as O(T) and why does per-token computation remain O(T)?

- **Concept:** Grouped-Query Attention (GQA)
  - Why needed: Theorem 5 proves KQ-SVD optimality extends to GQA by stacking query matrices
  - Quick check: If 8 query heads share 2 key heads (group size m=4), how does KQ-SVD compute projections for shared key cache?

## Architecture Onboarding

- **Component map:** Calibration data → K,Q,V cache extraction → SVD computation → Projection matrices A,B → Compressed cache storage → Runtime projection via B → Attention reconstruction
- **Critical path:** 1) Calibration data selection (128 sequences × 2048 tokens from C4) → 2) Per-layer rank selection using spectral energy (ε=0.1) → 3) SVD computation for KQ^T (efficient O(Td²) method) → 4) Store projections A,B → 5) Apply to all inference
- **Design tradeoffs:** Rank R (compression vs accuracy), calibration data size (128 sequences sufficient), value compression (independent enable/disable)
- **Failure signatures:** High attention output error despite low key error (K-SVD mismatch), degraded performance under norm imbalance (Eigen failure), layer-specific error spikes (check rank selection)
- **First 3 experiments:** 1) Replicate Figure 1 for Llama2-7B comparing K-SVD, Eigen, KQ-SVD on attention error across layers → 2) Verify Theorem 4: scale K by β∈{1,2,5,10}, divide Q by β, plot errors → 3) Test GQA extension (Theorem 5) on Mistral-7B with grouped query heads

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important research directions emerge from the work. The evaluation focuses exclusively on attention fidelity metrics rather than downstream task performance, leaving the relationship between approximation error and model quality unexplored. The computational overhead of projection operations during inference is not measured, nor is the sensitivity to calibration data distribution and domain shift analyzed. Additionally, the potential for combining KQ-SVD with quantization-based methods remains untested despite being theoretically complementary.

## Limitations

- SVD-based projection requires substantial calibration data (128 sequences × 2048 tokens) and computational overhead during setup, though this is a one-time cost
- The method assumes low-rank structure holds across diverse inputs, which may not generalize to domains very different from C4 training data
- While theoretical analysis is rigorous for standard multi-head attention, the GQA extension relies on concatenating query matrices without capturing all architectural nuances

## Confidence

- **High confidence:** Core mechanism and optimality guarantee (Theorem 2) are well-supported by theoretical derivation and empirical results
- **Medium confidence:** Comparative advantage over K-SVD and Eigen is demonstrated empirically, but theoretical gaps assume specific spectral conditions
- **Medium confidence:** Value-output compression extension follows the same framework but is less detailed in main text

## Next Checks

1. Test KQ-SVD on out-of-domain data (code or specialized technical text) to verify low-rank assumption beyond C4-style web text
2. Measure inference latency overhead of projection computation (B·Q) at runtime to confirm claimed efficiency gains
3. Conduct ablation studies varying spectral energy threshold ε to quantify compression-fidelity tradeoff across different model layers