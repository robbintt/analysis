---
ver: rpa2
title: Geospatial Question Answering on Historical Maps Using Spatio-Temporal Knowledge
  Graphs and Large Language Models
arxiv_id: '2508.21491'
source_url: https://arxiv.org/abs/2508.21491
tags:
- questions
- historical
- geospatial
- factual
- descriptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a GeoQA system for historical maps that integrates
  a spatio-temporal knowledge graph (KG) with large language models (LLMs) to enable
  natural language querying of historical map data. The system addresses the challenge
  of retrieving structured information from vectorized historical map features through
  both factual and descriptive question answering.
---

# Geospatial Question Answering on Historical Maps Using Spatio-Temporal Knowledge Graphs and Large Language Models

## Quick Facts
- arXiv ID: 2508.21491
- Source URL: https://arxiv.org/abs/2508.21491
- Reference count: 28
- This work presents a GeoQA system for historical maps that integrates a spatio-temporal knowledge graph (KG) with large language models (LLMs) to enable natural language querying of historical map data.

## Executive Summary
This paper presents a novel GeoQA system that enables natural language querying of historical map data through integration of spatio-temporal knowledge graphs with large language models. The system addresses the challenge of retrieving structured information from vectorized historical map features by developing two distinct workflows: one for factual questions using SPARQL query generation and validation, and another for descriptive questions that incorporates additional context sources like map images and internet search. The approach effectively handles the uncertainty inherent in historical map vectorization by pre-computing spatial relations with buffer zones, while maintaining high semantic accuracy through LLM-guided query validation. Evaluation shows the system achieves high delivery and semantic accuracy rates, with DeepSeek-Reasoner and GPT-4o outperforming Claude 3.7 Sonnet for factual QA, and map image integration yielding the best content quality for descriptive questions.

## Method Summary
The system constructs a spatio-temporal knowledge graph from vectorized historical map features using GraphDB, incorporating precomputed spatial relations to handle geometric uncertainty from vectorization errors. For factual questions, natural language queries are converted to SPARQL using structured LLM prompts containing ontology constraints and few-shot examples, with a second LLM validating query semantics before execution. Descriptive questions are decomposed into factual sub-questions, processed against the KG, and enriched with additional context from map images and internet search before LLM synthesis of final answers. The system was evaluated on Siegfried historical maps of Switzerland covering Aarberg, Bargen, Seedorf, and Radelfingen regions, with 100 test questions covering Yes/No, numerical, and overview question types.

## Key Results
- SPARQL generation accuracy (manual validation) achieved 72-84% across different LLMs, with DeepSeek-Reasoner and GPT-4o outperforming Claude 3.7 Sonnet
- Delivery rate for factual questions reached 95-100% when using auto-validated SPARQL
- Descriptive QA with KG+Map+Internet Search achieved perfect fact accuracy (1.00) while KG+Map+Semantic Map provided best content quality (Fluency 0.94, Informativeness 0.97)
- Pre-computed spatial relations with buffer zones effectively handled geometric uncertainty from historical map vectorization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured SPARQL generation with LLM-guided validation improves factual QA delivery rates on geospatial knowledge graphs.
- Mechanism: Natural language questions are converted to SPARQL queries via an LLM using structured prompts containing ontology constraints, few-shot examples, and validation rules. A second LLM validates query semantics before execution, reducing malformed queries.
- Core assumption: The ontology definitions and property lists provided in the prompt sufficiently constrain LLM output to generate syntactically and semantically valid SPARQL.
- Evidence anchors:
  - [abstract]: "workflows of two different types of GeoQA: factual and descriptive...developed a GeoQA system by integrating a spatio-temporal knowledge graph (KG) constructed from historical map data with large language models"
  - [section 2.5]: "For factual questions, they are first converted into SPARQL queries using an LLM, guided by a structured prompt...Before executing, the generated query is passed through a second LLM to perform a validation check"
  - [corpus]: "Question Answering Over Spatio-Temporal Knowledge Graph" confirms STKGQA remains limited, validating the novelty of this pipeline approach
- Break condition: If the KG schema changes significantly without prompt updates, query generation accuracy degrades. Table 1 shows SPARQL accuracy (manual) at 0.72-0.84, indicating ~15-28% of generated queries remain semantically incorrect.

### Mechanism 2
- Claim: Pre-computing spatial relations with buffer zones mitigates geometric uncertainty from historical map vectorization errors.
- Mechanism: Spatial relations (topological, proximity, cardinal directions) are precomputed using defined metrics and buffer zones rather than computed on-the-fly. This normalizes positional inaccuracies inherent in extracted feature geometries before KG construction.
- Core assumption: Buffer zone parameters adequately capture true spatial relationships despite extraction noise.
- Evidence anchors:
  - [section 2.3]: "Relying strictly on feature geometries may lead to incorrect interpretations of their relationships. To eliminate the influence of positional inaccuracy, the most relevant relations are precomputed by defined metrics and buffer zones"
  - [section 4]: "While pre-computing feature relations helps address geometric uncertainty, it would reduce QA's dynamic reasoning flexibility"
  - [corpus]: No direct corpus evidence on pre-computation strategies for spatial KGs
- Break condition: If users require ad-hoc spatial queries not covered by precomputed relations, the system cannot dynamically compute them without schema extension.

### Mechanism 3
- Claim: Multi-source context integration (KG + map images + internet search) improves descriptive QA fact accuracy and content quality differentially.
- Mechanism: Descriptive questions are decomposed into factual sub-questions processed against the KG. Additional context from map images (via GPT visual interpretation) and internet search (via Tavily/LangChain) enriches answer generation. Each source contributes differently: map images improve content quality metrics, while internet search achieves highest fact accuracy.
- Core assumption: LLMs can synthesize heterogeneous context sources without introducing hallucinations beyond the KG grounding.
- Evidence anchors:
  - [section 2.5]: "Additional contextual information such as map image patches and internet search results are incorporated to generate the final answer"
  - [Table 2]: KG+Map+Internet Search achieves Fact Accuracy (Manual) = 1.00, while KG+Map+Semantic Map achieves best Fluency (0.94) and Informativeness (0.97)
  - [corpus]: "GeoRAG" supports geographic perspective in QA; "Spatial-Agent" confirms agentic approaches improve geospatial reasoning
- Break condition: When internet search dominates, KG-related facts decrease (Table 2 shows fewer factual questions: 2.0 vs 9.4 for KG-only), potentially reducing answer grounding in the historical map data.

## Foundational Learning

- Concept: SPARQL and GeoSPARQL query languages
  - Why needed here: The system translates natural language to SPARQL queries against a geospatial knowledge graph. Understanding query structure, triple patterns, and spatial filters is essential for debugging failed queries.
  - Quick check question: Can you write a SPARQL query that retrieves all features within a buffer zone of a given polygon using GeoSPARQL?

- Concept: Knowledge Graph ontology design
  - Why needed here: The ontology defines feature types, properties (semantic, geospatial), and relations (spatial, temporal). The LLM prompt includes this schema to constrain query generation.
  - Quick check question: Given a feature class "Building" with properties "year" and "geometry", how would you define a temporal relation "transformed_from" to another feature type?

- Concept: LLM prompt engineering for structured output
  - Why needed here: Factual QA relies on prompts containing ontology constraints, few-shot examples, and validation rules. Prompt quality directly impacts SPARQL accuracy (0.63-0.72 auto-validated).
  - Quick check question: How would you structure a prompt to constrain LLM output to valid property names from a predefined list while handling ambiguous place names?

## Architecture Onboarding

- Component map:
  Knowledge Graph -> SPARQL Generator (LLM) -> Query Validator (LLM) -> Answer Generator (LLM) -> Context Enrichment (Map images, Internet search) -> Web Interface

- Critical path:
  1. User submits natural language question
  2. Question classified as factual or descriptive
  3. (Factual) LLM generates SPARQL → Validator checks → GraphDB executes → LLM formulates answer
  4. (Descriptive) Decompose to sub-questions → Process each as factual → Enrich with map images/search → LLM synthesizes final answer

- Design tradeoffs:
  - Pre-computed spatial relations vs. dynamic computation: Pre-computation handles uncertainty but reduces flexibility
  - Multi-source context vs. KG grounding: More sources improve fact accuracy but reduce KG-specific content
  - Auto vs. manual SPARQL validation: Auto (GPT) is stricter but rejects valid structural variations

- Failure signatures:
  - Delivery rate < 0.95: Check SPARQL syntax errors or missing properties in prompt
  - SPARQL accuracy gap (auto < manual): GPT validator being overly rigid; consider manual review
  - Descriptive answers lack KG facts: Internet search dominating context; reduce search weight or filter
  - Ambiguity in municipality/place constraints: Rules in prompt not resolving geographic scope

- First 3 experiments:
  1. Replicate the SPARQL generation pipeline with a smaller LLM (e.g., Llama-3) on a subset of 20 factual questions. Measure delivery rate and accuracy. This establishes baseline performance for open-weight models.
  2. Vary buffer zone sizes (e.g., 10m, 50m, 100m) for spatial relation pre-computation on a held-out map region. Measure impact on relation accuracy against manually labeled ground truth. This validates the uncertainty handling assumption.
  3. Ablate context sources for descriptive QA: Test KG-only vs. KG+Map vs. KG+Search on the 10 overview questions. Measure fact accuracy and informativeness. This quantifies the tradeoff between grounding and richness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the integration of external internet search with the Knowledge Graph be optimized to maintain specific KG-derived facts while maximizing general fact accuracy?
- Basis in paper: [explicit] The conclusion states that "the combination of extra contextual sources with KG should be carefully investigated," while the results section notes that the "KG+Map+Internet Search" configuration generated answers containing fewer KG-related facts than other setups.
- Why unresolved: The current system exhibits a trade-off where adding internet search improves overall fact accuracy to 1.00 but reduces the utilization of specific knowledge graph content compared to using "KG+Map+Semantic Map."
- What evidence would resolve it: Experimental results from a modified weighting or fusion strategy that achieves high general fact accuracy while retaining a high density of KG-specific attributes (e.g., specific area measurements) in the final answers.

### Open Question 2
- Question: Can an automated evaluation framework be developed that accurately validates descriptive answers and SPARQL semantic correctness without the need for manual review?
- Basis in paper: [explicit] The authors state that "more automated evaluation regarding the descriptive questions should be investigated" and admit that the current evaluation "still requires manual check."
- Why unresolved: The study found that automated GPT-based checks were too rigid ("rigid regarding logical or structural variations") to accurately assess SPARQL validity, necessitating human intervention to distinguish between valid syntactic variations and actual errors.
- What evidence would resolve it: A new evaluation metric or model that demonstrates high correlation with human manual review scores when assessing the semantic correctness of generated SPARQL queries and the factual consistency of descriptive text.

### Open Question 3
- Question: To what extent does the system's output align with user expectations for usefulness and reliability in practical applications?
- Basis in paper: [explicit] The conclusion explicitly proposes that "the usefulness and reliability of the answers can be potentially evaluated through user studies."
- Why unresolved: The current evaluation relies on technical metrics (delivery rate, accuracy, perplexity) and expert manual review, but has not yet validated if the natural language answers are actually useful or reliable for the target audience of non-domain experts.
- What evidence would resolve it: Qualitative and quantitative data from a user study involving non-experts performing specific information retrieval tasks using the web interface, measuring task success rates and user satisfaction.

## Limitations
- The system's performance is heavily dependent on precomputed spatial relations, limiting dynamic reasoning flexibility for novel spatial queries.
- SPARQL generation accuracy remains at 72-84% even with manual validation, indicating significant room for improvement in query generation.
- Evaluation was conducted on a single map region, potentially limiting generalizability to different geographic areas or map styles.

## Confidence
- **High Confidence**: The overall system architecture (KG + LLM pipeline) is sound and follows established patterns for question answering systems. The integration of multiple context sources for descriptive QA is well-supported by the evaluation results.
- **Medium Confidence**: The specific prompt engineering approaches and buffer zone parameters used for spatial relation precomputation are likely effective for the tested Swiss map dataset but may require tuning for different geographic regions or map styles.
- **Low Confidence**: The long-term reliability of the system given that historical map data quality varies significantly across regions and time periods. The evaluation on a single map region may not capture this variability.

## Next Checks
1. **Generalization Testing**: Apply the system to a different historical map region with distinct geographic features (e.g., mountainous vs. flat terrain) and compare SPARQL generation accuracy and spatial relation effectiveness.
2. **Dynamic Spatial Query Validation**: Implement on-the-fly spatial computation alongside pre-computed relations and measure accuracy trade-offs for questions requiring dynamic geometric reasoning not covered by pre-computed relations.
3. **Context Source Impact Analysis**: Systematically vary the weighting and presence of KG results, map images, and internet search in descriptive QA across different question types to quantify the optimal balance for fact accuracy versus KG grounding.