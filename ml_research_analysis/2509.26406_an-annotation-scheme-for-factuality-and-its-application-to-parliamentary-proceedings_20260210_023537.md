---
ver: rpa2
title: An Annotation Scheme for Factuality and its Application to Parliamentary Proceedings
arxiv_id: '2509.26406'
source_url: https://arxiv.org/abs/2509.26406
tags:
- factuality
- type
- claim
- sentences
- annotation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a multi-faceted annotation scheme for factuality,
  developed for Hebrew and applied to parliamentary proceedings. The scheme combines
  concepts from prior works, covering aspects like check-worthiness, event selecting
  predicates, agency, stance, hedging, quantities, named entities, and time expressions.
---

# An Annotation Scheme for Factuality and its Application to Parliamentary Proceedings

## Quick Facts
- arXiv ID: 2509.26406
- Source URL: https://arxiv.org/abs/2509.26406
- Reference count: 31
- Primary result: Multi-faceted annotation scheme for factuality in Hebrew parliamentary proceedings, achieving moderate inter-annotator agreement (Kappa 0.5-0.63) and 77.26% accuracy on check-worthiness prediction with domain-specific fine-tuning

## Executive Summary
This paper presents a comprehensive annotation scheme for factuality designed specifically for Hebrew parliamentary proceedings. The scheme integrates multiple linguistic features including check-worthiness, event selecting predicates, agency, stance, hedging, quantities, named entities, and time expressions. Applied to 4,987 sentences from Knesset proceedings, the annotation achieved moderate agreement among annotators. The authors developed domain-specific Hebrew models (Knesset-DictaBERT) that significantly outperformed off-the-shelf GPT models for check-worthiness prediction, demonstrating the value of fine-tuning on parliamentary domain data. The annotated corpus and models are publicly released to support fact-checking applications.

## Method Summary
The authors developed a multi-faceted annotation scheme covering eight linguistic features relevant to factuality assessment in parliamentary discourse. They manually annotated 4,987 Hebrew sentences from Knesset proceedings, with 100 sentences receiving triple annotation for agreement evaluation. The primary task was check-worthiness prediction (3-way classification: "worth checking," "not worth checking," "not a factual proposition"). For model development, they fine-tuned Hebrew BERT models (AlephBertGimmel, DictaBERT, Knesset-DictaBERT) on the annotated data using Hugging Face Transformers, training for 3 epochs with batch size 8 and weight decay 0.01. The best model achieved 77.26% accuracy on held-out test data.

## Key Results
- Moderate inter-annotator agreement (Kappa 0.5-0.63) on complex multi-layer annotation
- Domain-specific fine-tuning achieved 77.26% accuracy vs. 30.4-60.7% for GPT models
- Annotation scheme successfully captures factuality features in Hebrew parliamentary discourse
- Public release of annotated corpus and models under CC BY-SA 4.0 license

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-adapted fine-tuning improves check-worthiness prediction accuracy.
- Mechanism: Pre-trained Hebrew BERT models fine-tuned on parliamentary proceedings data encode domain-specific lexical patterns, discourse structures, and stylistic cues that off-the-shelf models lack, enabling more accurate identification of check-worthy claims.
- Core assumption: Parliamentary discourse contains distinctive linguistic features (e.g., formal register, political terminology, procedural language) that systematically differ from general-domain training data.
- Evidence anchors:
  - [abstract]: "fine-tuned Hebrew models (Knesset-DictaBERT) achieved significantly higher accuracy (77.26%), demonstrating the value of domain-specific fine-tuning"
  - [section 6.2]: "Knesset-DictaBERT achieved the highest accuracy. This is unsurprising, as this model underwent domain adaptation specifically tailored to Knesset data"
  - [corpus]: Related parliamentary corpora (ParlaSpeech, GerParCor) similarly leverage domain-specific data for improved NLP performance, supporting domain adaptation as a general mechanism
- Break condition: If the test domain shifts dramatically from parliamentary discourse (e.g., to social media), domain-specific benefits may diminish.

### Mechanism 2
- Claim: Annotation scheme complexity with multiple interacting layers limits inter-annotator agreement.
- Mechanism: The multi-layered annotation scheme (check-worthiness, ESPs, agency, stance, hedging, quantities, time expressions) creates cognitive load and interpretation variability, as annotators must simultaneously track 4-10 features per layer, with disagreements on even one feature counting as full disagreement.
- Core assumption: Annotator disagreement reflects genuine ambiguity in factuality judgments rather than poor guidelines.
- Evidence anchors:
  - [section 5]: "for the strictest agreement score (3), the results are relatively low for some of the layers, such as Check-worthiness, Stance and Agency and ESP"
  - [section 5]: "mean pairwise Kappa between each of the annotators and the other two ranged between 0.5 and 0.63"
  - [corpus]: Weak corpus signal on annotation complexity mechanisms; related parliamentary corpora focus on speech/alignment rather than multi-layer annotation schemes
- Break condition: If annotation were simplified to single features per sentence, agreement would likely increase but at cost of annotation richness.

### Mechanism 3
- Claim: Off-the-shelf GPT models underperform on specialized linguistic classification without explicit task-specific training.
- Mechanism: GPT models, despite strong general capabilities, lack exposure to the specific annotation conventions and linguistic distinctions required for factuality assessment, and zero-shot/few-shot prompting cannot fully compensate for this gap.
- Core assumption: Check-worthiness judgment requires learning implicit annotation criteria beyond surface-level definitions.
- Evidence anchors:
  - [abstract]: "Experiments with GPT models for check-worthiness prediction yielded low accuracy (30.4-60.7%)"
  - [section 6.1]: "few-shot prompting... led to a significant improvement... Kappa scores are 0.25, better than the previous approaches but still far below human agreement"
  - [corpus]: Weak corpus signal on GPT failure modes for specialized tasks; related work focuses on corpus creation rather than model comparison
- Break condition: If GPT models were given substantially more in-context examples or fine-tuned on the annotation scheme, performance gap might narrow.

## Foundational Learning

- Concept: **Factuality vs. Veracity Distinction**
  - Why needed here: The annotation scheme explicitly measures whether utterances *can* be verified (factuality), not whether they are *true* (veracity); conflating these leads to annotation errors.
  - Quick check question: If a speaker says "The moon is made of green cheese," what is the factuality score? (Answer: High factuality—check-worthy claim—despite being false)

- Concept: **Event Selecting Predicates (ESPs)**
  - Why needed here: ESPs (e.g., "say," "know," "want") modify the factuality of embedded events; identifying SIPs vs. NSIPs is critical for accurate factuality profiling.
  - Quick check question: In "He believes the budget was approved," does "believes" introduce a new source? (Answer: Yes—SIP type, source = "He")

- Concept: **Cohen's Kappa for Agreement**
  - Why needed here: The paper reports Kappa scores (0.5-0.63) to measure annotation reliability; understanding Kappa accounts for chance agreement is essential for interpreting these results.
  - Quick check question: If annotators agree 80% of the time but the task is easy with 70% base agreement, is Kappa high? (Answer: No—Kappa would be low because agreement barely exceeds chance)

## Architecture Onboarding

- Component map:
  - **Input Layer**: Raw Hebrew sentences from Knesset Corpus (plenary + committee proceedings)
  - **Annotation Scheme Layers** (8 components): (1) Check-worthiness (score, claim type, factuality profile), (2) Event Selecting Predicates (ESP type, source), (3) Agency (agent, experiencer, animacy, morphology), (4) Stance (confidence level, type, polarity, reference), (5) Hedging (hedge lexical items), (6) Quantities (expression, quantifier, accuracy), (7) Named Entities (from existing NER), (8) Time Expressions (time ranges, relative times)
  - **Model Layer**: Knesset-DictaBERT (fine-tuned) for automatic check-worthiness prediction; other layers require future model development
  - **Output**: Annotated sentences with structured factuality tags

- Critical path:
  1. Manual annotation of 5K sentences using the scheme (completed)
  2. Agreement evaluation on 100 triple-annotated sentences (completed, Kappa 0.5-0.63)
  3. Model training for check-worthiness (completed, 77.26% accuracy)
  4. Automatic annotation of full corpus for check-worthiness (in progress)
  5. Model development for remaining 7 annotation layers (future work)

- Design tradeoffs:
  - **Sentence vs. claim granularity**: Scheme supports multiple claims per sentence but complicates model training; current model uses sentence-level labels derived from claim-level annotations
  - **Annotation depth vs. agreement**: Rich multi-layer annotation provides utility but lowers inter-annotator agreement; simplified schemes would improve agreement but lose information
  - **GPT vs. fine-tuned models**: GPT models offer flexibility without training data but underperform; fine-tuned models require annotated data but achieve higher accuracy
  - **Hebrew-specific vs. language-agnostic design**: Scheme developed for Hebrew's morphology/orthography; authors claim adaptability to other languages but not yet demonstrated

- Failure signatures:
  - **Low GPT accuracy (30-60%)**: Indicates model reliance on surface patterns rather than annotation criteria; few-shot prompting provides only marginal improvement
  - **Annotator disagreement on claim count**: Suggests sentence segmentation ambiguities; resolved by focusing evaluation on sentences with agreed claim sets
  - **Low strict agreement (score=3) on complex layers**: Indicates subjectivity in stance/agency annotations; mitigated by using majority vote for ground truth
  - **Domain shift risk**: Models trained on parliamentary data may fail on informal genres (social media, news commentary)

- First 3 experiments:
  1. **Baseline validation**: Test Knesset-DictaBERT on held-out single-claim sentences (56-sentence evaluation set) to confirm reported 78.57% accuracy; compare against random baseline and majority-class baseline.
  2. **Feature ablation**: Train separate models for each annotation layer (e.g., stance-only, hedging-only) to identify which features are most predictable and which require more annotated data.
  3. **Cross-domain robustness**: Test the fine-tuned model on non-parliamentary Hebrew text (e.g., news articles, if available) to measure domain sensitivity and identify failure modes before scaling to full corpus annotation.

## Open Questions the Paper Calls Out

- **Can the full, complex factuality annotation scheme be automated effectively, or are features like stance and agency too subjective for reliable prediction?**
  - Basis in paper: [explicit] The authors note that current experiments are "limited to a single feature" (check-worthiness) and list predicting "full factuality structures" as the primary goal for future work.
  - Why unresolved: While check-worthiness achieved 77% accuracy, the more complex layers (e.g., Stance, Agency) showed only moderate human agreement (Kappa 0.5–0.63), suggesting high subjectivity that may limit automation.
  - What evidence would resolve it: Training multi-task models on the remaining layers and evaluating if their performance metrics approach the check-worthiness baseline.

- **Is the proposed scheme linguistically universal enough to be adapted to other morphologically-rich languages?**
  - Basis in paper: [explicit] The conclusion explicitly lists plans for "adaptation of our scheme to other languages, with a focus on morphologically-rich languages."
  - Why unresolved: The scheme was tailored specifically for Hebrew's "complex morphology and deficient orthography," and it is unclear if the defined features translate effectively to languages with different structural constraints.
  - What evidence would resolve it: Applying the annotation scheme to a non-Hebrew parliamentary corpus and achieving comparable inter-annotator agreement scores.

- **To what extent does factuality manifestation (e.g., hedging, agency) differ between government and opposition speakers?**
  - Basis in paper: [explicit] The authors explicitly plan to "explore how factuality is manifested in different groups of the parliament (e.g., government vs. opposition)."
  - Why unresolved: The current study focused on scheme validation and aggregated annotation, without performing a sociolinguistic analysis based on the political roles of the speakers.
  - What evidence would resolve it: A statistical analysis of the annotated corpus comparing the frequency of specific factuality features across different political factions.

## Limitations

- Moderate inter-annotator agreement (Kappa 0.5-0.63) suggests inherent subjectivity in factuality judgments
- Hebrew-specific focus limits immediate cross-linguistic applicability
- GPT models performed poorly (30.4-60.7% accuracy) even with few-shot prompting

## Confidence

- **High Confidence**: Domain adaptation mechanism, GPT models underperforming specialized models, core annotation scheme design
- **Medium Confidence**: Inter-annotator agreement interpretation, label aggregation logic, cross-linguistic generalizability claims
- **Low Confidence**: Model hyperparameter choices, exact random seed effects, long-term stability on full corpus

## Next Checks

1. **Held-out single-claim evaluation**: Replicate the 56-sentence single-claim evaluation to verify the reported 78.57% accuracy, comparing against random (33%) and majority-class (baseline) baselines to confirm model superiority.

2. **Feature-level ablation study**: Train separate models for individual annotation layers (e.g., stance-only, hedging-only) to identify which features are most predictable and which require additional annotated data, validating the multi-layer approach's practical utility.

3. **Cross-domain robustness test**: Evaluate the fine-tuned Knesset-DictaBERT model on non-parliamentary Hebrew text (if available, e.g., news articles) to measure domain sensitivity and identify systematic failure modes before full corpus annotation deployment.