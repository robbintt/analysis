---
ver: rpa2
title: 'Intelli-Planner: Towards Customized Urban Planning via Large Language Model
  Empowered Reinforcement Learning'
arxiv_id: '2601.21212'
source_url: https://arxiv.org/abs/2601.21212
tags:
- planning
- urban
- area
- functional
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Intelli-Planner, a novel framework integrating
  Large Language Models (LLMs) with Deep Reinforcement Learning (DRL) for customized
  urban planning. The framework uses LLMs to formulate planning objectives based on
  demographic and geographic data, then employs DRL to generate functional area schemes.
---

# Intelli-Planner: Towards Customized Urban Planning via Large Language Model Empowered Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.21212
- Source URL: https://arxiv.org/abs/2601.21212
- Reference count: 40
- Primary result: Framework integrates LLMs with DRL for customized urban planning, achieving comparable performance to state-of-the-art methods while enhancing stakeholder satisfaction and convergence speed.

## Executive Summary
Intelli-Planner is a novel framework that integrates Large Language Models (LLMs) with Deep Reinforcement Learning (DRL) to address customized urban planning challenges. The system uses LLMs to formulate planning objectives based on demographic and geographic data, then employs DRL to generate functional area schemes optimized for multiple criteria. The framework introduces a five-dimensional evaluation system and uses LLM-based stakeholders to provide subjective satisfaction scores, creating a hybrid approach that balances objective metrics with diverse stakeholder interests.

## Method Summary
The framework combines LLM-based objective formulation, knowledge enhancement, and satisfaction scoring with a DRL agent trained via Proximal Policy Optimization (PPO). The LLM analyzes local area features to output recommended functional types, which guide the DRL policy network by modifying action probabilities. The system evaluates outcomes across five dimensions: service (15-minute living circle), ecology (park coverage), economy (business density), equity (distance fairness), and satisfaction (LLM-based stakeholder scoring). The approach was tested on three diverse urban communities using OpenStreetMap data and demonstrated superior performance compared to traditional baselines.

## Key Results
- Intelli-Planner outperforms traditional urban planning baselines across all five evaluation metrics
- The framework achieves comparable performance to state-of-the-art DRL methods while enhancing stakeholder satisfaction
- Knowledge enhancement mechanism accelerates DRL convergence by guiding exploration with LLM recommendations
- LLM-based stakeholders provide reliable satisfaction scores that supplement objective metrics

## Why This Works (Mechanism)

### Mechanism 1: Knowledge-Enhanced Policy Guidance
Integrating LLM recommendations into the DRL policy network accelerates convergence by pruning the exploration space using domain priors. The LLM analyzes local area features and outputs recommended functional types, which the DRL agent uses to modify action probability distributions by multiplying recommended actions by a factor λ > 1.

### Mechanism 2: Semantic-to-Spatial Translation
LLMs bridge the gap between abstract natural language planning styles and concrete numerical targets required for DRL reward calculation. The framework inputs high-level demographic data and target styles into an LLM, which outputs JSON structures defining precise coverage rates and facility counts used as target variables in reward functions.

### Mechanism 3: Role-Based Subjective Reward Shaping
Simulating diverse stakeholders via role-playing LLMs allows optimization for subjective equity and satisfaction metrics. Three distinct LLM agents with different personas (Resident, Developer, Government) review planning schemes and provide discrete satisfaction scores that supplement objective metrics in the reward signal.

## Foundational Learning

- **Concept: Markov Decision Process (MDP)**
  - Why needed here: Treats urban planning as a sequential decision problem where each decision changes the neighborhood state
  - Quick check: How does the state representation capture spatial dependencies required for a 15-minute living circle?

- **Concept: Actor-Critic Methods (PPO)**
  - Why needed here: Uses Proximal Policy Optimization to train the planning agent, balancing Actor (policy) and Critic (value) networks
  - Quick check: How does the clipping function prevent the policy from changing too drastically during updates?

- **Concept: Prompt Engineering & Role-Play**
  - Why needed here: LLM output quality depends entirely on how context and persona are framed in prompts
  - Quick check: Why does the paper enforce discrete scoring rather than continuous scalar outputs for stakeholder satisfaction?

## Architecture Onboarding

- **Component map:** Data Ingestion → LLM Objective Formulation → DRL Training Loop (Actor selects type → LLM Knowledge Enhance → Env Step → Calculate Objective + LLM Subjective Reward → Update Networks)
- **Critical path:** Data Ingestion → LLM Objective Formulation (Generate Target Ratios) → DRL Training Loop (Actor selects type → LLM Knowledge Enhance → Env Step → Calculate Objective + LLM Subjective Reward → Update Networks)
- **Design tradeoffs:**
  - Cost vs. Guidance: LLM API calls for every training step are expensive; implementation must manage API rate limits
  - Flexibility vs. Constraints: Allows flexible natural language goals but relies on strict planning rules for physical feasibility
- **Failure signatures:**
  - Metric Collapse: Agent optimizes one metric at expense of others if reward weights are unbalanced
  - LLM Inconsistency: Satisfaction reward fluctuates wildly between epochs, preventing Critic convergence
- **First 3 experiments:**
  1. Ablation on Knowledge Enhancement: Run DRL without LLM bias (λ=1) to verify convergence speedup
  2. Lambda Sensitivity: Tune enhancement factor λ to check for local optima trapping
  3. Hallucination Check: Validate LLM objective formulation outputs against ground truth human expert plans

## Open Questions the Paper Calls Out

- How does the framework's latency and API cost scale when applied to city-wide planning compared to traditional heuristic methods?
- To what extent does the Knowledge Enhancement module propagate factual errors into the DRL policy when the LLM lacks training data for specific regions?
- How sensitive is the planning outcome to the weighting of the five reward dimensions (Service, Ecology, Economy, Equity, Satisfaction)?
- Can LLM-based stakeholder agents reliably simulate satisfaction across diverse cultural and demographic groups not well-represented in LLM training data?

## Limitations

- Reliance on LLM consistency and alignment with human planning logic for subjective reward shaping
- Dependency on LLM API availability and cost, limiting scalability to larger urban areas
- Limited validation of LLM-as-stakeholder against actual human feedback in diverse cultural contexts

## Confidence

- **High Confidence:** Objective metrics and their mathematical formulations are well-defined and grounded in urban planning principles; PPO training setup is reproducible
- **Medium Confidence:** LLM-based subjective reward shaping is theoretically sound but relies on assumptions about simulated stakeholder accuracy; prompt engineering quality is crucial
- **Low Confidence:** Scalability and generalization to vastly different urban contexts beyond tested regions remains unproven; framework's handling of complex, contradictory requirements is untested

## Next Checks

1. Conduct human validation study comparing LLM satisfaction scores against ratings from actual human planners and residents
2. Apply trained model from one city to a new, unseen city with different demographics to evaluate generalization capability
3. Systematically test LLM's objective formulation module with contradictory or extreme requirements to identify failure modes