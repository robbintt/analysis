---
ver: rpa2
title: 'Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code Generation
  Capabilities'
arxiv_id: '2502.11829'
source_url: https://arxiv.org/abs/2502.11829
tags:
- code
- mllms
- test
- flowchart
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Code-Vision, a benchmark designed to evaluate
  the logical understanding and code generation capabilities of Multimodal Large Language
  Models (MLLMs). It challenges MLLMs to generate correct programs based on given
  flowcharts, which visually represent desired algorithms or processes.
---

# Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code Generation Capabilities

## Quick Facts
- arXiv ID: 2502.11829
- Source URL: https://arxiv.org/abs/2502.11829
- Reference count: 15
- Open-source models achieve only 15% on Hard problems while GPT-4o reaches 79.3% pass@1

## Executive Summary
This paper introduces Code-Vision, a benchmark that challenges Multimodal Large Language Models (MLLMs) to generate Python code from visual flowcharts. Unlike existing benchmarks where images are supplementary, Code-Vision makes the flowchart the primary source of logic, forcing models to translate visual graph structures into executable code. The benchmark reveals a fundamental capability gap: while proprietary models like GPT-4o achieve strong performance (79.3% pass@1 on Hard problems), open-source models struggle significantly (15% at best), with failures primarily in visual-to-syntax translation.

## Method Summary
Code-Vision comprises three subsets (HumanEval-V, Algorithm, MATH) with 438 total problems. Each problem presents a flowchart image alongside a function signature and docstring. Models generate Python code which is executed against unit tests to determine correctness. The evaluation uses Pass@1 metric with temperature=0.2, top_p=0.95, max_tokens=1024. Both proprietary (GPT-4o, Claude 3.5, Gemini 1.5) and open-source (Llama-3.2-Vision, Phi-3-Vision, MiniCPM-V 2.6, Qwen-VL-Plus) models are evaluated. The benchmark design forces visual grounding by showing that performance collapses when images are removed.

## Key Results
- GPT-4o achieves 79.3% pass@1 on Hard Algorithm problems while the best open-source model reaches only 15%
- Performance drops dramatically when images are removed (GPT-4o from 87.9% to 24.8%), proving visual signal is essential
- Open-source models show ~20% improvement on Easy tasks when given Mermaid text instead of flowchart images
- Error analysis reveals open-source models primarily fail with syntax errors while proprietary models fail with logical assertion errors

## Why This Works (Mechanism)

### Mechanism 1: Visual-Logic Entanglement
By enforcing dependency between visual structure and code syntax, the benchmark isolates visual reasoning from text-only pattern matching. The input flowchart contains logic encoded spatially, requiring models to parse spatial relationships and serialize them into syntactic control flow. This is proven by GPT-4o's performance collapse from 87.9% (Image+Text) to 24.8% (Text Only), unlike MMCode where text-only performance is similar to multimodal.

### Mechanism 2: Representation Translation Gap
Current MLLMs exhibit significant fidelity loss when translating pixel-based graphs into logic compared to text-based graph descriptions. Open-source models show ~20% performance increase on Easy tasks when given Mermaid text instead of flowchart images, suggesting the vision encoder is the primary bottleneck for visual feature extraction.

### Mechanism 3: Syntactic vs. Semantic Failure Bifurcation
The benchmark reveals capability ceilings where open-source models fail at code syntax generation while proprietary models fail at logical correctness. Open-source models have >10% SyntaxError rates, while proprietary models have >90% AssertionError rates (code runs but logic is wrong), indicating different failure modes at generation vs. reasoning stages.

## Foundational Learning

- **Concept: Control Flow Graphs (CFGs) & Mermaid**
  - Why needed here: Flowcharts represent algorithms using shapes that map to control structures. Understanding that diamond shapes map to conditional branches and rectangles to statements is the core translation task.
  - Quick check question: Can you manually convert a simple "Is X > 0?" diamond node into a Python `if` statement?

- **Concept: Pass@k Metric**
  - Why needed here: The results use Pass@1 (probability of solving in one go), which is stricter than "best of k" and measures model reliability.
  - Quick check question: If a model generates 10 samples and 1 is correct, is the Pass@1 score 10%? (Yes, roughly, based on the formula).

- **Concept: Multimodal Grounding**
  - Why needed here: The model must "ground" text generation in visual input. Removing the image causes GPT-4o to drop by 60+ points in this benchmark but not in others because logical prerequisites are exclusively visual here.
  - Quick check question: Why does removing the image cause the performance of GPT-4o to drop by 60+ points in this benchmark but not in others?

## Architecture Onboarding

- **Component map:** Vision Encoder (ViT) -> Projector/Adapter -> LLM (Llama/Qwen) -> Evaluator (Python Sandbox)
- **Critical path:** The flow of information from Vision Encoder to Projector is the likely bottleneck for open-source models. If the encoder fails to detect arrow direction between nodes, the LLM receives garbled logic.
- **Design tradeoffs:** Using rendered images tests "true" vision capabilities but Mermaid text significantly boosts performance. An engineer might trade realism for reliability by using text-based graph inputs if accuracy is the goal. The paper limits complexity to avoid hallucination, trading depth of reasoning for eval stability.
- **Failure signatures:**
  - Visual Hallucination: Model invents variables not present in flowchart
  - Syntax Dropout: Model generates conceptually correct but invalid Python syntax
  - RecursionError: Model follows visual loop but fails to define base case
- **First 3 experiments:**
  1. Ablation (Input Modality): Run benchmark in "Text-Only" vs. "Image+Text" mode to confirm image usage
  2. Representation Swap: Feed Mermaid text definition instead of rendered image to quantify "Vision Tax"
  3. Error Classification: Log whether failures are syntactic (code won't run) or semantic (code runs but wrong output)

## Open Questions the Paper Calls Out
The paper explicitly states future work to "build more and harder problems that contain more complex logic and flowcharts to challenge proprietary models," indicating current benchmark difficulty is insufficient for stress-testing top models like GPT-4o.

## Limitations
- Benchmark may conflate visual perception failures with actual code generation deficiencies
- Small dataset sizes (438 total problems) raise questions about statistical significance, especially for "Hard" subsets with 0% success rates for some models
- Evaluation focuses on correctness but doesn't adequately evaluate code quality metrics like efficiency, readability, or style

## Confidence
- **High Confidence:** Visual-logic entanglement mechanism is valid (performance collapse when images removed); error type bifurcation between open-source and proprietary models is reliably observed
- **Medium Confidence:** Interpretation that open-source models fail at visual-to-syntax translation while proprietary models fail at logical correctness; assumes error types are clean proxies for failure modes
- **Low Confidence:** Claims about "fundamental gaps in open-source models' code generation abilities" may conflate visual parsing limitations with actual code generation deficiencies

## Next Checks
1. **Vision Tax Quantification:** Systematically test each model with Mermaid text input vs. rendered image to precisely measure performance penalty from pixel processing
2. **Error Attribution Study:** Manually classify failures from 50 Algorithm-Hard samples into visual hallucination, syntax dropout, or logical reasoning errors
3. **Cross-Benchmark Transfer:** Test whether models performing well on Code-Vision also excel at ChartCoder or other chart-to-code tasks to determine if Code-Vision measures a distinct capability dimension