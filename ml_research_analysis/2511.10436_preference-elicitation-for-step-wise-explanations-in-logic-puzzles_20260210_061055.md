---
ver: rpa2
title: Preference Elicitation for Step-Wise Explanations in Logic Puzzles
arxiv_id: '2511.10436'
source_url: https://arxiv.org/abs/2511.10436
tags:
- facts
- explanation
- constraints
- explanations
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MACHOP, a novel interactive preference elicitation
  method for generating step-wise explanations in logic puzzles like Sudoku and Logic-Grid
  problems. The core challenge is to learn user preferences over multiple sub-objectives
  (e.g., number of constraints, facts) that define explanation quality.
---

# Preference Elicitation for Step-Wise Explanations in Logic Puzzles

## Quick Facts
- **arXiv ID**: 2511.10436
- **Source URL**: https://arxiv.org/abs/2511.10436
- **Reference count**: 24
- **Primary result**: MACHOP reduces relative regret by up to 80% vs. baseline, with real-user Sudoku evaluation showing 70.7% preference for MACHOP explanations after 30 queries.

## Executive Summary
This paper introduces MACHOP, a novel interactive preference elicitation method for generating step-wise explanations in logic puzzles like Sudoku and Logic-Grid problems. The core challenge is learning user preferences over multiple sub-objectives (e.g., number of constraints, facts) that define explanation quality. MACHOP extends Constructive Preference Elicitation by introducing a non-domination constraint to prevent trivial comparisons, a local normalization strategy to stabilize learning, and an Upper Confidence Bound-based weighting scheme to guide diversification toward both important and unexplored objectives. Experiments show MACHOP reduces relative regret by up to 80% compared to the baseline Choice Perceptron, with real-user evaluations demonstrating better alignment with human preferences.

## Method Summary
MACHOP implements a preference elicitation loop where users compare pairs of explanation steps for CSP facts. The system learns a weighted linear utility function over explanation features through Choice Perceptron updates. MACHOP introduces three key innovations: local normalization of features per query pair to handle scale variations, a non-domination constraint ensuring the second option is strictly better on at least one objective, and UCB-based diversification weights that balance exploiting known important objectives with exploring under-sampled ones. The OCUS solver generates optimal explanation pairs subject to these constraints, while instance selection can be online (selecting the next fact to explain) or offline (pre-determined fact sequences).

## Key Results
- MACHOP reduces relative regret by up to 80% compared to baseline Choice Perceptron across Sudoku and Logic-Grid puzzles
- Real-user evaluation shows MACHOP explanations preferred over baseline in 70.7% of cases after 30 queries
- Local normalization and UCB diversification consistently outperform default methods, with regret reductions of 40-80%
- Offline-SES instance selection offers favorable runtime-quality tradeoff, enabling real-time learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Local normalization of sub-objectives stabilizes preference learning when feature scales vary widely.
- **Mechanism:** Dynamically rescaling each sub-objective based on the maximum value of the most recent comparison pair ensures weight updates operate on comparable feature magnitudes.
- **Core assumption:** Relative importance of sub-objectives is invariant to absolute scale, but learning algorithm stability is not.
- **Evidence anchors:** Results show local normalization performs best on average with significantly lower regret than default methods.

### Mechanism 2
- **Claim:** A non-domination constraint ensures pairwise comparisons provide non-trivial information for learning.
- **Mechanism:** Forces the second query option to be better than the first on at least one sub-objective, preventing trivial comparisons where choice is obvious.
- **Core assumption:** Comparing a dominated solution to a non-dominated one provides little learning signal about user preferences.
- **Evidence anchors:** Adding disjunctive constraints yields significantly lower regret in 7 out of 8 setups.

### Mechanism 3
- **Claim:** A UCB-based weighting scheme for diversification efficiently balances exploiting known important objectives and exploring under-sampled ones.
- **Mechanism:** Calculates weights combining estimated importance from past selections and exploration bonuses based on how often each objective's trade-off has been explored.
- **Core assumption:** Importance can be inferred from selection frequency, and under-explored objectives may contain valuable preference information.
- **Evidence anchors:** UCB weights consistently reduce regret by about 40% for both Sudoku and Logic-Grid problems.

## Foundational Learning

- **Concept: Constraint Satisfaction Problem (CSP) & Explanation Step**
  - **Why needed here:** The framework is built on explaining facts derived from CSPs. An explanation step `E ∧ S ⇒ N` shows how existing facts `E` and constraints `S` logically imply a new fact `N`.
  - **Quick check question:** Given a Sudoku grid, can you define what constitutes an "explanation step" for a newly filled cell in terms of the paper's `E`, `S`, `N` notation?

- **Concept: Linear Utility Function & Weight Update (Preference Perceptron)**
  - **Why needed here:** The system learns user preferences represented as a weighted sum of sub-objectives through the learning rule `wt+1 = wt + η(ϕ(y−) − ϕ(y+))`.
  - **Quick check question:** If a user prefers solution A over B (`y+ = A`, `y- = B`), and feature vector of B is `[2, 10]` while A's is `[1, 5]`, how would the weight vector `w = [1, 1]` be updated with learning rate `η=0.1`?

- **Concept: Domination in Multi-Objective Optimization**
  - **Why needed here:** The non-domination constraint is based on Pareto domination - a solution is dominated if another is at least as good on all objectives and strictly better on at least one.
  - **Quick check question:** Consider two explanation steps with feature vectors `ϕ1 = [5, 2]` and `ϕ2 = [4, 1]` (assuming lower is better for both). Does `ϕ2` dominate `ϕ1`? Explain why or why not.

## Architecture Onboarding

**Component Map:**
- Instance Selector: Chooses next CSP state (puzzle grid and remaining explainable facts) for query generation
- Query Generator: Core optimization engine using OCUS solver to produce explanation step pairs with MACHOP constraints
- Weight Updater: Applies Preference Perceptron update rule with locally normalized features

**Critical Path:**
1. Start with initial CSP and equal weights
2. Instance Selection: Pick puzzle and fact to explain
3. Query Generation: Find two optimal explanation steps with MACHOP constraints
4. User Interaction: Present pair, get choice
5. Weight Update: Adjust weights, loop back to step 2

**Design Tradeoffs:**
- Online vs. Offline Fact Selection: Online yields higher-quality explanations but increases user wait time significantly; Offline is faster with slightly higher regret
- Runtime vs. Explanation Quality: Primary tradeoff governed by Instance Selector between seconds response (Offline-SES) vs. minute wait (Online) with better learning

**Failure Signatures:**
- Stagnant Weights / High Regret: Check non-domination constraint, normalization stability
- Excessive Query Time: Switch to Offline-SES for complex puzzles or check solver configuration
- Overfitting: Performance degrades with more queries; may need hyperparameter tuning

**First 3 Experiments:**
1. Baseline Reproduction: Implement Choice Perceptron only, compare normalization strategies on simulated users for Sudoku
2. Query Strategy Ablation: Compare standard generation, Learned Weights diversification, and MACHOP UCB weights
3. Latency vs. Quality Benchmark: Measure query generation time and regret for Online, Offline-Random, and Offline-SES modes

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can non-linear utility functions be effectively integrated into the elicitation framework to capture complex preference structures missed by linear models?
- **Basis in paper:** Future work could explore "learning preferences as a non-linear utility function"
- **Why unresolved:** Non-linear functions are computationally expensive and current solvers primarily support linear objectives
- **What evidence would resolve it:** Study comparing accuracy and computational overhead of learned non-linear models versus linear baselines

### Open Question 2
- **Question:** To what extent does actively selecting the next problem instance for querying accelerate the preference learning process?
- **Basis in paper:** "Learning could be further sped up by actively choosing which instance to generate a query for next"
- **Why unresolved:** Current experiments rely on random or pre-computed instance selection strategies
- **What evidence would resolve it:** Empirical results showing convergence rate and query complexity of active instance selection vs. offline methods

### Open Question 3
- **Question:** How can queries where users express no preference be utilized to improve the efficiency and robustness of the weight learning process?
- **Basis in paper:** "Using queries where users express no preference could accelerate learning too"
- **Why unresolved:** Current framework updates weights based on distinct preferences, not indifference labels
- **What evidence would resolve it:** Extension of MACHOP algorithm incorporating indifference constraints with evaluation of convergence speed

## Limitations
- Evaluation relies entirely on simulated users with fixed Bradley-Terry models, leaving real human preference performance untested
- Feature engineering is domain-specific to Sudoku and Logic-Grid puzzles, limiting cross-domain transferability
- Runtime analysis focuses on query generation time without accounting for full system latency including CSP state selection

## Confidence
- **High**: Core MACHOP mechanisms (non-domination constraint, local normalization, UCB diversification) show consistent improvements in controlled experiments
- **Medium**: Relative regret improvements and user preference win-rates are robust within simulated user framework but require real user validation
- **Low**: Scalability analysis to larger puzzles is limited, and method's behavior under highly non-linear or multi-modal preferences remains unknown

## Next Checks
1. **Real User Validation**: Conduct controlled study with human participants solving Sudoku puzzles, comparing MACHOP against baseline methods using actual preference feedback
2. **Cross-Domain Transfer**: Apply MACHOP to different CSP domain (e.g., crossword puzzle explanations or graph coloring) to assess normalization and diversification generalization
3. **Robustness to Preference Complexity**: Test MACHOP with simulated users exhibiting non-linear preferences or multiple "sweet spots" for objectives to evaluate linear utility assumption effectiveness