---
ver: rpa2
title: Optimal Power Allocation and Sub-Optimal Channel Assignment for Downlink NOMA
  Systems Using Deep Reinforcement Learning
arxiv_id: '2601.12242'
source_url: https://arxiv.org/abs/2601.12242
tags:
- channel
- rate
- noma
- learning
- power
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the channel assignment and power allocation
  problem in downlink NOMA systems, which is NP-hard due to the dynamic nature of
  wireless environments and complexity of successive interference cancellation. The
  authors propose a deep reinforcement learning framework that combines experience
  replay memory with a policy gradient method to achieve near-optimal resource allocation.
---

# Optimal Power Allocation and Sub-Optimal Channel Assignment for Downlink NOMA Systems Using Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.12242
- Source URL: https://arxiv.org/abs/2601.12242
- Authors: WooSeok Kim; Jeonghoon Lee; Sangho Kim; Taesun An; WonMin Lee; Dowon Kim; Kyungseop Shin
- Reference count: 18
- Primary result: DRL-based channel assignment + optimal JRA power allocation achieves near-optimal sum rates in NOMA systems

## Executive Summary
This paper addresses the NP-hard problem of joint channel assignment and power allocation in downlink NOMA systems. The authors propose a deep reinforcement learning framework that decomposes the problem into channel assignment (learned via DRL) and power allocation (solved analytically via Joint Resource Allocation). By combining experience replay memory with a policy gradient method, the framework achieves near-optimal sum rates with tractable complexity. Extensive simulations demonstrate robust performance across various network configurations, learning rates, and batch sizes.

## Method Summary
The framework uses Joint Resource Allocation (JRA) to solve the optimal power allocation problem via closed-form water-filling equations (8-9), while a deep reinforcement learning agent learns sub-optimal channel assignments. The DRL agent employs REINFORCE with baseline and experience replay memory to improve generalization. The state space includes Channel-to-Noise Ratio (CNR), distance, and channel assignment status features. The agent outputs probability distributions over user-channel assignments, samples actions, and receives rewards based on sum rates computed by JRA. Training continues until validation error drops below threshold and loss converges.

## Key Results
- Proposed method achieves sum rates very close to exhaustive search (near-optimal) across various network configurations
- Experience replay memory improves generalization compared to pure on-policy learning
- Three-feature state representation (CNR + distance + channel status) provides best performance despite longer convergence time
- Method shows robust performance across different learning rates (0.001-0.0001) and batch sizes (20-80)

## Why This Works (Mechanism)

### Mechanism 1: Problem Decomposition
The NP-hard joint allocation problem is decomposed into channel assignment (learned via DRL) and power allocation (solved analytically via JRA). This enables near-optimal solutions with tractable complexity by avoiding combinatorial explosion. The core assumption is that channel assignment and power allocation are sufficiently separable. Evidence shows the framework achieves near-optimal sum rates while being computationally tractable. Break condition: If channel assignment strongly couples with power constraints, the decomposition may yield infeasible or highly suboptimal solutions.

### Mechanism 2: Experience Replay for Generalization
Combining experience replay memory with REINFORCE improves generalization to unseen NOMA configurations. Replay memory stores diverse experience tuples and batch sampling breaks temporal correlation, exposing the policy to broader distributions. The core assumption is that past experiences remain partially relevant for improving the current policy. Evidence shows JRA-DRL with replay memory outperforms JRA-DRL without replay across all tested state sizes. Break condition: If the policy changes rapidly, old replay experiences become misleading, causing instability or divergence.

### Mechanism 3: Feature Enrichment
State feature enrichment (CNR + distance + channel assignment status) improves sum rate performance at the cost of longer convergence time. These features enable the agent to learn allocation heuristics analogous to "pair strong-near users with weak-far users on same channel" which is optimal for NOMA SIC ordering. The core assumption is that the neural network can learn non-obvious interactions between these features and optimal channel pairing. Evidence shows larger state sizes improve sum rate performance. Break condition: If feature dimensions scale without corresponding network capacity or training data, the agent may underfit or overfit.

## Foundational Learning

- **Concept**: Non-Orthogonal Multiple Access (NOMA) and Successive Interference Cancellation (SIC)
  - Why needed here: The problem assumes users share the same time-frequency resource block, differentiated only by power levels. SIC decoding order depends on CNR-based power ordering.
  - Quick check question: If two users on the same channel have nearly equal CNR, what happens to SIC decoding reliability and sum rate?

- **Concept**: Policy Gradient (REINFORCE) with Baseline
  - Why needed here: The channel assignment is a sequential decision problem with stochastic policy. The gradient estimator uses the difference between online and baseline model rewards to reduce variance.
  - Quick check question: Why does the baseline model (using argmax rather than sampling) provide a stable reference without being updated every episode?

- **Concept**: Experience Replay Memory
  - Why needed here: Breaks temporal correlation of sequential experiences, enabling mini-batch training and improved generalization despite being theoretically mismatched with on-policy learning.
  - Quick check question: What happens to gradient bias if the replay buffer contains mostly experiences from very old policies?

## Architecture Onboarding

- **Component map**: Environment -> State tensor (N×K×F) -> Online Model (p_θ) -> Action sequence (ζ) -> Baseline Model (p_θ_bl) -> JRA Module -> Sum rates (R, R_bl) -> Replay Memory -> Policy gradient update

- **Critical path**:
  1. Environment generates N random users with CNR/distance features → State tensor S (N×K×F)
  2. Online model samples assignment sequence ζ (N actions)
  3. Baseline model selects argmax assignment ζ_bl
  4. JRA computes optimal power and sum rates R, R_bl for both assignments
  5. Store (ζ, R, R_bl) in replay memory; sample batch δ
  6. Compute policy gradient (Equation 14); update θ via Adam
  7. Periodically validate; if error < threshold and loss stable, save θ_bl and stop

- **Design tradeoffs**:
  - Learning rate: 0.001 converges fast (280s) but lower sum rate; 0.0001 achieves highest sum rate but slow (3990s). Recommendation: start at 0.0005.
  - Batch size: 20 is fastest but lowest performance; 80 is best but slow. Recommendation: 40 as practical middle ground.
  - Model type: FCNN offers stable convergence; ANN converges fast but with oscillating loss; CNN is slowest. Recommendation: FCNN for stability.
  - State features: F=3 best performance but longest convergence; F=1 unstable. Recommendation: Use F=3 with patience for convergence.

- **Failure signatures**:
  - Loss oscillates without convergence → learning rate too high or replay memory too small
  - Sum rate plateaus far below ES → insufficient state features or model capacity
  - Baseline model updates too frequently → threshold too loose; increase validation patience
  - Training diverges mid-episode → check CNR ordering assumptions or SIC stability constraints

- **First 3 experiments**:
  1. Baseline reproduction: Set N=6, K=3, F=3, batch=40, lr=0.0005, FCNN. Verify sum rate approaches ES on validation seeds within 10-15 minutes.
  2. Ablation on replay memory: Run same configuration with replay disabled. Compare sum rate degradation to quantify generalization loss.
  3. Scalability test: Increase to N=20, K=10, F=3. Monitor training time and sum rate gap vs. ES. Identify if network capacity or batch size needs adjustment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the framework's performance and convergence scale when the constraint of two users per channel (Nk=2) is relaxed to support higher multiplexing orders?
- Basis in paper: Section 2 explicitly fixes the number of users per channel to two (Nk=2) to limit hardware complexity and processing time.
- Why unresolved: Increasing users per channel exponentially increases SIC complexity and the action space for the DRL agent.
- What evidence would resolve it: Simulation results analyzing sum rate and convergence speed in scenarios where Nk > 2.

### Open Question 2
- Question: Can the framework maintain stability and near-optimal performance in multi-cell environments with inter-cell interference (ICI)?
- Basis in paper: The system model in Section 2 assumes a single Base Station, ignoring neighboring cells which is critical in realistic deployments.
- Why unresolved: ICI introduces non-stationary dynamics into CNR, which may disrupt learning stability provided by experience replay memory.
- What evidence would resolve it: Evaluation of the trained model in a multi-cell simulation setup measuring robustness against dynamic interference profiles.

### Open Question 3
- Question: What architectural modifications are required to stabilize Attention-based Neural Networks (ANN) within this specific DRL framework?
- Basis in paper: Section 4 notes that the ANN model exhibited oscillating loss values and unstable learning due to frequent changes in the baseline model.
- Why unresolved: The attention mechanism's sensitivity to baseline policy updates suggests a mismatch between the architecture and the policy gradient method used.
- What evidence would resolve it: Demonstration of an ANN-based agent achieving stable loss convergence and sum rates comparable to the FCNN model.

## Limitations
- Neural network architecture details (layer counts, hidden dimensions, activation functions) are not specified for the FCNN, CNN, and ANN variants, limiting precise reproduction.
- Replay memory capacity and validation threshold values are unspecified, making it difficult to replicate stopping criteria exactly.
- The framework assumes two users per channel and single-cell environments, limiting applicability to more complex scenarios.

## Confidence
- **High confidence**: The core mechanism of decomposing channel assignment (DRL) and power allocation (JRA) is well-supported by simulation results showing near-optimal sum rates.
- **Medium confidence**: The benefit of experience replay for generalization is demonstrated, but lacks direct ablation studies against other DRL variants in NOMA literature.
- **Low confidence**: Feature enrichment claims are based on limited ablation; the precise contribution of each feature dimension is not isolated.

## Next Checks
1. **Architecture Sensitivity**: Systematically vary neural network depth/width to identify sensitivity of convergence speed and final performance to model capacity.
2. **Replay Memory Size**: Conduct ablation studies on replay buffer size to quantify trade-off between generalization and gradient bias from stale experiences.
3. **Feature Contribution Analysis**: Train separate agents with only one feature at a time (CNR, distance, channel status) to isolate which features drive performance gains.