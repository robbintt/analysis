---
ver: rpa2
title: 'The Hidden Width of Deep ResNets: Tight Error Bounds and Phase Diagrams'
arxiv_id: '2509.10167'
source_url: https://arxiv.org/abs/2509.10167
tags:
- have
- error
- dynamics
- limit
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the training dynamics of deep residual networks
  (ResNets) as the depth L goes to infinity, establishing that for arbitrary hidden
  width M, the training dynamics converges to a "Neural Mean ODE" limit model. The
  key insight is that due to random initialization, forward and backward passes through
  ResNets behave as stochastic approximations of certain mean ODEs, and by propagation
  of chaos, this behavior is preserved through training dynamics.
---

# The Hidden Width of Deep ResNets: Tight Error Bounds and Phase Diagrams

## Quick Facts
- arXiv ID: 2509.10167
- Source URL: https://arxiv.org/abs/2509.10167
- Reference count: 8
- Primary result: Establishes that deep ResNets with arbitrary hidden width M converge to a Neural Mean ODE limit, with error bounds O(1/L + α/√(LM)) and reveals phase diagrams for feature learning behaviors.

## Executive Summary
This paper analyzes the training dynamics of deep residual networks (ResNets) as depth L approaches infinity, establishing that for arbitrary hidden width M, the dynamics converge to a "Neural Mean ODE" limit model. The key insight is that random initialization causes forward and backward passes through ResNets to behave as stochastic approximations of certain mean ODEs, with this behavior preserved through training via propagation of chaos. The main result provides tight error bounds showing how discretization error (1/L) and sampling error (α/√(LM)) combine, where α is a residual scale parameter that determines whether the model exhibits complete feature learning or operates in a lazy linear regime.

## Method Summary
The paper establishes convergence of deep ResNets to continuous limit models by showing that randomly initialized networks behave as stochastic approximations of Neural Mean ODEs. The analysis uses propagation of chaos to show that independence of hidden units is preserved during training, allowing the collective behavior to be described by deterministic ODEs. Error bounds are derived by quantifying both discretization error from finite depth and sampling error from finite width. The framework is extended to ResNets with two-layer perceptron blocks, where a critical residual scale Θ(√D/LM) determines feature learning behavior, leading to error bounds O(1/L + √D/√(LM)) and revealing phase diagrams of different learning regimes.

## Key Results
- Proves ResNets with arbitrary width M converge to Neural Mean ODE limit with error O(1/L + α/√(LM))
- Shows phase transition behavior: α = Θ(1) yields complete feature learning, α → ∞ yields lazy ODE regime
- For 2LP blocks, critical residual scale is Θ(√D/LM), giving error bound O(1/L + √D/√(LM))
- Reveals phase diagram showing three regimes: semi-complete, complete, and lazy feature learning
- Provides tight theoretical predictions validated experimentally across various settings

## Why This Works (Mechanism)

### Mechanism 1: Stochastic Approximation of Mean ODEs
Randomly initialized deep ResNets approximate continuous limit models via Monte Carlo sampling over depth and width. The forward and backward passes sum contributions over L (depth) and M (width), which statistically behaves like integral approximation. The dynamics converge to a deterministic Neural Mean ODE with error rate O(1/√(LM)), treating the product of depth and width as effective sample size. This bound fails if LM doesn't grow sufficiently to suppress sampling variance.

### Mechanism 2: Propagation of Chaos (Asymptotic Independence)
Independence of hidden units is preserved throughout training, allowing mean-field approximation to hold. In high dimensions, randomly initialized units remain nearly independent even as they interact through layers and gradient updates. This allows collective behavior of LM units to remain close to the distribution of the limit stochastic process. The independence assumption fails if initialization is non-i.i.d. or width/depth is too small.

### Mechanism 3: Residual Scale Phase Transition
A single hyperparameter (residual scale) determines whether the network operates in "complete feature learning" or "lazy" linear regime. The paper defines residual scale dependent on initialization variance σ_v. When scale is Θ(1), the limit Mean ODE is non-linearly parameterized (feature learning). When scale α → ∞, the dynamics linearize (Lazy ODE), similar to Neural Tangent Kernel regime. If residual scale grows too fast, initialization fluctuations may dominate.

## Foundational Learning

**Concept: Mean-Field Limits**
Why needed: The paper maps discrete neural network layers to continuous distributions (ODEs). Understanding that infinite particles (neurons) can be described by a density is crucial for Section 2.2.
Quick check: How does increasing the number of particles in a system allow us to describe it using a deterministic differential equation?

**Concept: Stochastic Approximation**
Why needed: The core result interprets ResNet depth/width as sampling steps approximating an integral.
Quick check: Why does the error of a Monte Carlo estimate decrease at a rate of 1/√N?

**Concept: Linearization (Taylor Expansion)**
Why needed: The "Lazy ODE" regime is defined by linearizing the dynamics around initialization.
Quick check: If you approximate a function f(x) by its tangent line at x₀, what is the order of the error term?

## Architecture Onboarding

**Component map:**
Inputs (D, n) -> ResNet (L layers, M width) with blocks φ -> Limit Model (Neural Mean ODE with stochastic process Z(s))
Hyperparameters: Learning rates (η_u, η_v), Initialization scales (σ_u, σ_v)

**Critical path:**
1. Determine embedding dimension D and target model size
2. Select Width M and Depth L to ensure effective width LM ≫ D (specifically M ≥ D recommended)
3. Set Initialization Scale: For 2LP blocks, set σ_v ≈ √D and σ_u ≈ √D for complete feature learning
4. Set Learning Rates: Scale LRs as η_u, η_v ∝ D to match balanced regime

**Design tradeoffs:**
- Depth vs. Width: Error bound O(1/L + √D/√(LM)) implies you can trade width for depth as long as LM is constant
- Stability vs. Feature Learning: Choosing σ_v ≫ √D pushes model into lazy regime; choosing σ_v ≪ √D risks semi-complete regime with limited feature diversity

**Failure signatures:**
- High Output Variance: If √D/√(LM) is large, output will have high variance from initialization noise
- No Feature Movement: If α is too large, weight displacement ‖ΔU‖ will remain small (linear regime)

**First 3 experiments:**
1. Verify Convergence Rate: Train ResNets with fixed LM but varying L; plot error vs. 1/L to confirm discretization term dominance
2. Phase Diagram Scan: Vary initialization scale σ_v over range (D^{1/4} to D); monitor weight evolution to identify regime transitions
3. Dimensional Scaling: Fix LM and vary D; verify sampling error scales as √D by measuring output fluctuations at initialization

## Open Questions the Paper Calls Out

### Open Question 1
Can convergence results for generic ResNet blocks be extended to activation functions lacking global Lipschitzness (e.g., ReLU) or to architectures where initialization doesn't satisfy E[D₁φ(x, Z₀)] = 0?
Basis: Section 2.3 states the "assumed regularity on φ is quite restrictive," and Section 3.1 notes the lazy regime requires centered initial forward/backward passes, which fails for blocks like φ(x, W) = Wρ(x) if ρ is not odd.
Why unresolved: Proofs rely on controlling propagation of errors via uniform Lipschitz constants and variance proxies which may not exist for non-smooth or non-centered blocks.
What evidence would resolve it: A convergence proof using local Lipschitz analysis or generalized sub-gaussian bounds, or empirical verification that error bounds hold despite assumption violations.

### Open Question 2
Do derived error bounds and phase diagrams remain identical for SGD or Adam, or does mini-batching noise fundamentally alter the "effective width" scaling?
Basis: Section 2.1 notes focus is on GD "to fix ideas" and suggests technique would apply to SGD/Adam if update rule is Lipschitz, but provides no formal proofs.
Why unresolved: Analysis depends on deterministic or "propagation of chaos" arguments for full dataset; mini-batch noise introduces stochasticity that might compound with forward pass approximation error.
What evidence would resolve it: Theoretical derivation of error bounds specifically for SGD with explicit batch-size dependencies, or experiments demonstrating O(1/L + √D/√(LM)) scaling holds for Adam/SGD across various batch sizes.

### Open Question 3
Can explicit D-dependency error bound in Theorem 4 be rigorously proven without assuming hidden width M scales linearly with embedding dimension D (i.e., M = Ω(D))?
Basis: Section 4.3 states for convenience the proof limits itself to M = Ω(D), but notes hypothesis is likely unnecessary and D = O(LM) might be sufficient.
Why unresolved: Proof techniques likely utilize matrix concentration inequalities that simplify or behave more predictably when M ≫ D.
What evidence would resolve it: A generalized proof of Theorem 4 valid for M < D, or empirical analysis showing error scaling breaks down in bottleneck layers where M ≪ D.

## Limitations

- Dimensional Scaling Ambiguity: Error bounds depend on specific scaling of learning rates η_u, η_v ∝ D, with unclear practical implications in finite-width regimes
- Initialization Sensitivity: Analysis critically depends on i.i.d. initialization assumptions; propagation of chaos may break down for non-standard schemes
- Computational Complexity: Error bounds suggest increasing LM product improves approximation quality but at significant computational cost

## Confidence

**High Confidence** (Mechanistic Claims):
- The stochastic approximation framework for ResNet dynamics
- The phase transition behavior based on residual scale

**Medium Confidence** (Theoretical Bounds):
- The O(1/L + α/√(LM)) error bound for general ResNets
- The O(1/L + √D/√(LM)) bound for 2LP blocks

**Low Confidence** (Practical Implications):
- Direct applicability of bounds to practical architectures
- Exact phase transition boundaries in finite-width regimes

## Next Checks

1. **Finite-Width Validation**: Systematically test error bounds for ResNets with varying width-to-depth ratios, explicitly measuring sampling error term α/√(LM) across different initialization scales to verify theoretical predictions hold beyond asymptotic regimes.

2. **Non-I.I.D. Initialization Study**: Evaluate propagation of chaos assumption under correlated initialization schemes (orthogonal initialization, pretrained weights) to quantify how deviations from i.i.d. assumptions affect mean-field approximation quality.

3. **Computational Efficiency Benchmark**: Compare practical performance of ResNets operating at different points in width-depth tradeoff space, measuring both approximation accuracy and training/inference efficiency to provide concrete guidance on optimal architecture scaling for real-world applications.