---
ver: rpa2
title: 'Prompts to Proxies: Emulating Human Preferences via a Compact LLM Ensemble'
arxiv_id: '2509.11311'
source_url: https://arxiv.org/abs/2509.11311
tags:
- preference
- agents
- test
- survey
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Prompts to Proxies (P2P), a two-stage system
  for aligning large language models with population-level preferences from survey
  data. Stage 1 uses entropy-based adaptive sampling to construct a diverse agent
  pool spanning the latent preference space, while Stage 2 employs L1-regularized
  regression to select a compact ensemble whose aggregate responses match target populations.
---

# Prompts to Proxies: Emulating Human Preferences via a Compact LLM Ensemble

## Quick Facts
- arXiv ID: 2509.11311
- Source URL: https://arxiv.org/abs/2509.11311
- Reference count: 40
- Primary result: 43% MSE improvement over prompting baselines at ~$0.8 per survey

## Executive Summary
This paper introduces Prompts to Proxies (P2P), a two-stage system for aligning large language models with population-level preferences from survey data. Stage 1 uses entropy-based adaptive sampling to construct a diverse agent pool spanning the latent preference space, while Stage 2 employs L1-regularized regression to select a compact ensemble whose aggregate responses match target populations. P2P achieves an average test MSE of 0.014 across 14 American Trends Panel waves at approximately $0.8 per survey, representing a 43% improvement over prompting baselines. The system requires no fine-tuning or demographic data, relying only on API inference costs.

## Method Summary
P2P operates through a two-stage process that first generates a diverse pool of LLM agents and then selects a compact ensemble to match population preferences. In Stage 1, entropy-based adaptive sampling creates an agent pool that spans the latent preference space by iteratively selecting prompts that maximize response diversity. Stage 2 applies L1-regularized regression to identify a minimal subset of agents whose weighted combination best reproduces target population responses. The system achieves alignment without fine-tuning or demographic data, using only API inference costs for training.

## Key Results
- Average test MSE of 0.014 across 14 American Trends Panel waves
- 43% improvement over prompting baselines
- $0.8 per survey cost with competitive performance using less than 3% of training data compared to SFT-aligned baselines

## Why This Works (Mechanism)
The system works by creating a diverse agent pool that spans the latent preference space through entropy-based adaptive sampling, then using L1-regularized regression to select a compact ensemble whose weighted responses match population-level preferences. This approach avoids the need for fine-tuning while achieving population alignment through careful agent selection and combination.

## Foundational Learning
- Entropy-based adaptive sampling: Why needed - to ensure diverse agent pool spanning preference space; Quick check - response diversity metrics should increase monotonically
- L1-regularized regression: Why needed - to select compact ensemble while avoiding overfitting; Quick check - coefficient sparsity should reflect meaningful agent selection
- Population-level preference modeling: Why needed - to capture aggregate behavioral patterns without individual targeting; Quick check - ensemble predictions should match survey distributions

## Architecture Onboarding

Component Map: Survey Prompts -> Entropy Sampling -> Agent Pool -> L1 Regression -> Compact Ensemble -> Population Alignment

Critical Path: Survey prompts flow through entropy-based sampling to create agent pool, which is then processed by L1 regression to select ensemble that produces population-aligned outputs.

Design Tradeoffs: Avoids demographic data for privacy but may sacrifice fine-grained targeting; uses API inference only (no fine-tuning) for cost efficiency but may limit adaptability; selects compact ensembles for efficiency but may miss nuanced preferences.

Failure Signatures: Poor performance indicates insufficient diversity in agent pool (entropy sampling failure), overfitting in ensemble selection (L1 regularization too weak), or misalignment between survey prompts and target population.

First Experiments:
1. Verify entropy-based sampling increases response diversity across initial prompt sets
2. Test L1 regression coefficient sparsity and ensemble selection stability
3. Validate population alignment accuracy on held-out survey waves

## Open Questions the Paper Calls Out
None

## Limitations
- Performance in settings requiring fine-grained demographic targeting remains untested
- Cross-locale generalization claims limited by restricted diversity of validation datasets
- Entropy-based sampling strategy lacks formal theoretical guarantees

## Confidence

**Confidence Labels:**
- High confidence in system architecture and methodology description
- Medium confidence in reported MSE improvements and cost efficiency
- Medium confidence in cross-locale generalization claims
- Low confidence in robustness of entropy-based sampling strategy

## Next Checks

1. Evaluate P2P on additional international survey datasets with greater linguistic and cultural diversity to confirm cross-locale generalization
2. Conduct ablation studies on the entropy-based sampling and L1-regularized regression components to quantify their individual contributions to performance
3. Benchmark P2P against contemporary alignment methods that use fine-tuning or direct preference learning, ensuring fair comparison on both cost and data efficiency