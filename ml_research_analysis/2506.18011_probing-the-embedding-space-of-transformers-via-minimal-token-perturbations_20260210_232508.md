---
ver: rpa2
title: Probing the Embedding Space of Transformers via Minimal Token Perturbations
arxiv_id: '2506.18011'
source_url: https://arxiv.org/abs/2506.18011
tags:
- token
- layers
- tokens
- embedding
- perturbations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the effects of minimal token perturbations\
  \ on the embedding space of Transformer models to understand information propagation\
  \ and support interpretability. The authors replace each token with its nearest\
  \ neighbor in the embedding space (by cosine similarity) and measure the resulting\
  \ shift in the embedding space using various norms (l1, l2, l\u221E)."
---

# Probing the Embedding Space of Transformers via Minimal Token Perturbations

## Quick Facts
- **arXiv ID**: 2506.18011
- **Source URL**: https://arxiv.org/abs/2506.18011
- **Reference count**: 6
- **Primary result**: Minimal token perturbations (nearest-neighbor substitutions) show rare tokens cause larger embedding shifts than common tokens, with perturbations amplifying through Transformer layers except the final layer.

## Executive Summary
This paper investigates how minimal token perturbations affect the embedding space of Transformer models to understand information propagation and support interpretability. The authors systematically replace each token with its nearest neighbor in the embedding space (by cosine similarity) and measure the resulting shift in the embedding space using various norms (l1, l2, l∞). They find that common tokens (e.g., punctuation) yield minimal shifts while rare tokens cause larger perturbations, and that perturbations propagate increasingly through deeper layers except for the final layer which is more task-specific. The consistency of minimal shifts across norms is confirmed by high Pearson (r ≈ 0.99) and Spearman (ρ ≈ 0.95) correlations.

## Method Summary
The method involves no training—only analysis of a fine-tuned BERT model on IMDb Movie Reviews. For each token in a sequence, the authors find its nearest neighbor in the embedding matrix by cosine similarity (excluding itself), substitute it, and compute the embedding shift using l1, l2, and l∞ norms. They analyze 500 sequences for layer-wise propagation and bin tokens by commonness (10 bins, 50 sentences each) to study frequency effects. The analysis measures how perturbations propagate through all 12 encoder layers, with results aggregated across the dataset to establish statistical significance.

## Key Results
- Rare tokens induce significantly larger embedding shifts than common tokens when replaced with nearest neighbors
- Perturbations amplify through deeper Transformer layers, with mean difference increasing with depth
- Final layer shows inversion in perturbation pattern, attributed to task-specific fine-tuning
- Choice of norm (l1, l2, l∞) minimally affects which tokens are identified as least perturbing (Pearson r ≈ 0.99, Spearman ρ ≈ 0.95)

## Why This Works (Mechanism)

### Mechanism 1
Common tokens induce smaller embedding shifts than rare tokens under minimal perturbation because common tokens have denser semantic neighborhoods in embedding space; their nearest neighbors are more likely to preserve overall sequence meaning, producing smaller post-substitution shifts. Rare tokens are underrepresented, so their replacements diverge more sharply from original semantics.

### Mechanism 2
Perturbations propagate and amplify through deeper Transformer layers, except the final layer which behaves task-specifically, because early layers retain input-aligned representations; each successive attention and residual block increasingly intermixes token information, amplifying initial perturbations. The final layer, shaped by fine-tuning, compresses representations toward task outputs rather than preserving input structure.

### Mechanism 3
Choice of norm (l1, l2, l∞) minimally affects which tokens are identified as least perturbing because in finite-dimensional embedding spaces, all norms are topologically equivalent; for small perturbations within semantically meaningful regions, distance rankings remain consistent.

## Foundational Learning

- **Embedding space geometry and cosine similarity**: Why needed here: The entire perturbation framework relies on finding nearest neighbors via cosine similarity and measuring shifts via vector norms. Quick check question: Given two token embeddings, can you compute their cosine similarity and l2 distance? Do you understand why cosine similarity may be preferred for semantic closeness?

- **Transformer encoder layer structure (attention, residuals, layer norm)**: Why needed here: Propagation analysis assumes you know how information flows and mixes across layers. Quick check question: Can you sketch one encoder block and explain how a perturbation at the input could affect the hidden state at layer N?

- **Token frequency vs. information content**: Why needed here: The paper's core claim links frequency to semantic load and perturbation impact. Quick check question: Why might a rare word like "defenestration" cause a larger embedding shift than a common word like "the" when replaced by its nearest neighbor?

## Architecture Onboarding

- **Component map**: Input sequence -> tokenizer -> embedding lookup (E) -> 12 encoder layers (hi for i=1..12) -> task head (classification) -> Perturbation module: For each token position, find argmax cosine-similarity neighbor in vocabulary; substitute; compute ||E(x) − E(x′)|| and ||hi(x) − hi(x′)|| across layers

- **Critical path**: 1) Load fine-tuned BERT model and tokenizer, 2) For each input sequence, identify top-k tokens whose substitution yields minimal embedding shift (compute for each token, rank by norm), 3) Propagate perturbed sequence through all layers; record per-layer distances, 4) Aggregate statistics across dataset (frequency analysis, commonness bins, layer-wise trends)

- **Design tradeoffs**:
  - Single-token vs. multi-token perturbation: Single-token keeps analysis tractable and avoids OOD; but misses interaction effects
  - Cosine similarity vs. other metrics: Cosine emphasizes direction over magnitude; may miss magnitude-based structure
  - Norm choice: L2 is smooth and differentiable; L1 is robust to outliers; L∞ captures worst-case dimension. Paper shows they agree in practice for small perturbations

- **Failure signatures**:
  - Anisotropic embeddings: If embeddings cluster narrowly, nearest neighbors may be nearly identical regardless of semantics; perturbations become uninformative
  - Overly large vocabulary: Rare tokens may have no meaningful neighbors; substitutions become near-random
  - Task mismatch: On structured output tasks (e.g., QA, translation), minimal perturbation tokens may differ from sentiment classification; early-layer proxy assumption may not hold

- **First 3 experiments**:
  1. Reproduce frequency-shift relationship: On a new dataset (e.g., SST-2), bin tokens by commonness and plot embedding distance vs. commonness. Confirm downward trend
  2. Layer-wise propagation on different architectures: Run the same perturbation propagation analysis on RoBERTa or a decoder-only model. Compare amplification curves and final-layer inversion
  3. Multi-token perturbation pilot: Extend to substituting two tokens simultaneously (top-2 least impactful). Measure whether shifts are additive or super-additive; test if early-layer proxy assumption degrades

## Open Questions the Paper Calls Out
None

## Limitations
- Semantic validity of cosine-similarity-based nearest neighbors is assumed but not empirically verified
- Final-layer inversion explanation relies heavily on task-specific fine-tuning assumption without ruling out architectural artifacts
- Commonness metric depends on unspecified corpus frequency data

## Confidence
- **High Confidence**: The norm-consistency finding (r ≈ 0.99, ρ ≈ 0.95) is directly supported by the reported correlation analysis and is theoretically expected given norm equivalence in finite-dimensional spaces for small perturbations
- **Medium Confidence**: The frequency-shift relationship is supported by regression analysis showing decreasing distance with increasing commonness, but the semantic interpretation (that common tokens have denser neighborhoods) is inferred rather than directly tested
- **Medium Confidence**: The layer-wise propagation claim is supported by observed distance increases through layers, but the attribution to "information intermixing" versus other factors is not definitively established
- **Low Confidence**: The final-layer inversion explanation relies heavily on the assumption that task-specific fine-tuning causes the observed behavior, without ruling out alternative explanations

## Next Checks
1. **Semantic Validation of Nearest Neighbors**: For a sample of top-ranked "minimal shift" tokens, manually verify whether their nearest neighbors (by cosine similarity) are semantically related or merely orthographically/positionally similar. This would validate whether cosine similarity in this embedding space approximates semantic similarity as assumed.

2. **Alternative Architectures and Tasks**: Replicate the layer-wise propagation analysis on a decoder-only model (e.g., GPT-2) and a non-text task (e.g., image classification with ViT). Compare whether the amplification-through-layers pattern holds and whether the final-layer inversion appears, which would test if these are general architectural phenomena or BERT-specific.

3. **Controlled Semantic Drift Measurement**: Instead of measuring only embedding distance, measure downstream task performance degradation when substituting minimal-shift tokens. This would directly test whether "minimal shift" corresponds to "minimal semantic impact" rather than just minimal vector displacement.