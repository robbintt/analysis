---
ver: rpa2
title: Ordering-based Conditions for Global Convergence of Policy Gradient Methods
arxiv_id: '2504.02130'
source_url: https://arxiv.org/abs/2504.02130
tags:
- convergence
- global
- approximation
- softmax
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the global convergence of policy gradient
  (PG) methods under linear function approximation. The authors establish that global
  convergence depends on inter-related properties between the policy update and the
  representation.
---

# Ordering-based Conditions for Global Convergence of Policy Gradient Methods

## Quick Facts
- arXiv ID: 2504.02130
- Source URL: https://arxiv.org/abs/2504.02130
- Authors: Jincheng Mei; Bo Dai; Alekh Agarwal; Mohammad Ghavamzadeh; Csaba Szepesvari; Dale Schuurmans
- Reference count: 40
- Key outcome: Global convergence of policy gradient methods depends on order preservation conditions, not approximation error, under linear function approximation.

## Executive Summary
This paper establishes new global convergence conditions for policy gradient methods under linear function approximation. The authors demonstrate that convergence depends on whether the representation preserves the ranking of rewards rather than the magnitude of approximation error. For Softmax PG, global convergence requires non-domination conditions and existence of a linear function that orders actions according to ground-truth rewards. For NPG, convergence occurs if and only if the least-squares projection of rewards preserves the optimal action's rank. The results challenge previous analysis approaches that focused on approximation error as the key convergence determinant.

## Method Summary
The paper analyzes two policy gradient algorithms: Softmax PG and Natural Policy Gradient (NPG) with linear function approximation. The methods operate in a multi-armed bandit setting where policies are parameterized as π_θ = softmax(Xθ) with feature matrix X ∈ R^{K×d}. The algorithms use constant learning rates and aim to maximize expected reward through gradient ascent. The key innovation is shifting focus from approximation error to ordering conditions between the feature representation and ground-truth rewards. The authors provide theoretical proofs for convergence conditions and validate them through concrete examples and simulation studies.

## Key Results
- Softmax PG achieves global convergence when representation preserves reward ranking and satisfies non-domination condition
- NPG achieves global convergence if and only if the projected reward preserves optimal action's rank
- Approximation error is insufficient for characterizing global convergence - problems with similar approximation errors can have different convergence behaviors
- Neither Softmax PG nor NPG dominates the other in terms of convergence conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Softmax PG achieves global convergence when the feature matrix satisfies non-domination and preserves reward ordering.
- Mechanism: When a weight vector w exists such that Xw preserves the same ranking as true rewards r, the gradient updates maintain a monotonic improvement path to the optimal action. The covariance structure Cov_π(r', r) ≥ 0 (Lemma 2) ensures that updates in direction w are non-decreasing, preventing entrapment in suboptimal plateaus.
- Core assumption: Non-domination condition where x_i^T x_i > x_i^T x_j for all j ≠ i ensures each action has a unique feature representation.
- Evidence anchors:
  - [abstract]: "For the standard Softmax PG method, global convergence is achieved when the representation preserves the ranking of rewards (non-domination condition) and there exists a linear function that orders actions according to the ground-truth rewards."
  - [Theorem 1]: Formal statement of sufficient conditions for Softmax PG convergence.
  - [corpus]: Limited direct corpus support; most related work focuses on approximation error frameworks rather than ordering conditions.
- Break condition: If features violate non-domination or no w preserves reward ordering, gradient ascent can converge to suboptimal local maxima (Example 2 demonstrates this failure).

### Mechanism 2
- Claim: NPG achieves global convergence if and only if the least-squares projection of rewards preserves the optimal action's top ranking.
- Mechanism: NPG's update Xθ_{t+1} = Xθ_t + η·r̂ where r̂ = X(X^T X)^{-1}X^T r. When r̂(a*) > r̂(a) for all suboptimal actions, the optimal action receives the largest score update per iteration, causing π_{θ_t}(a*) to grow exponentially toward 1.
- Core assumption: Constant learning rate η > 0 and full column rank feature matrix X.
- Evidence anchors:
  - [abstract]: "For the natural policy gradient (NPG), global convergence occurs if and only if the projection of the reward onto the representable space preserves the optimal action's rank."
  - [Theorem 2]: "Necessary and sufficient condition for Algorithm 2 to achieve global convergence... is that r̂(a*) > r̂(a) for all a ≠ a*"
  - [corpus]: No direct corpus papers address this specific optimal-action-preservation condition.
- Break condition: If r̂(a*) ≤ r̂(a) for any suboptimal action a, the probability ratio π_{θ_t}(a*)/π_{θ_t}(a) cannot grow beyond its initial value (Example 4, Example 3).

### Mechanism 3
- Claim: Approximation error is fundamentally inadequate for characterizing PG convergence under linear function approximation.
- Mechanism: Approximation error measures magnitude of representation mismatch but ignores ordinal structure. Two problems with identical approximation errors can have opposite convergence outcomes depending on whether ordering is preserved (Examples 1-3 all have ϵ_approx ≈ 14.2-14.6 yet different convergence behaviors).
- Core assumption: The paper's analysis is limited to finite-arm bandits (single-state MDPs); generalization to full MDPs remains open.
- Evidence anchors:
  - [abstract]: "Approximation error is not a key quantity for characterizing global convergence in either algorithm."
  - [Section 3.2]: "Examples 1, 2 and 3 all have similar approximation errors, yet Softmax PG achieves global convergence on Example 1 but reaches a bad local maxima on Example 2."
  - [corpus]: Related paper "Rethinking the Global Convergence of Softmax Policy Gradient with Linear Function Approximation" appears to explore similar themes but abstract was truncated.
- Break condition: Assumption: This finding may not extend to multi-state MDPs where value functions depend on policy-dependent state distributions.

## Foundational Learning

- Concept: **Policy Gradient with Softmax Parameterization**
  - Why needed here: The paper analyzes convergence of π_θ = softmax(Xθ) where X is a feature matrix. Understanding how gradients flow through softmax is essential to follow the proof sketches.
  - Quick check question: Can you derive why dπ_θ^T r/dθ = X^T(diag(π_θ) - π_θπ_θ^T)r?

- Concept: **Linear Function Approximation and Realizability**
  - Why needed here: The paper's central contribution distinguishes between realizability (exact representation) and the weaker ordering conditions that suffice for convergence.
  - Quick check question: If reward r ∈ R^K cannot be expressed as Xw for any w ∈ R^d, does this prevent global convergence? (Answer: Not necessarily—see Example 1.)

- Concept: **Fisher Information Matrix and Natural Gradient**
  - Why needed here: NPG preconditioning with (X^T X)^{-1} X^T is related to the Fisher information matrix in the log-linear policy class.
  - Quick check question: Why does NPG's update involve a least-squares projection rather than the raw gradient?

## Architecture Onboarding

- Component map:
  - **Feature Matrix X ∈ R^{K×d}**: Maps actions to d-dimensional representations; must satisfy non-domination (diagonal dominance of X X^T)
  - **Reward Vector r ∈ R^K**: Ground-truth action values; ordering must be checkable against features
  - **Policy π_θ = softmax(Xθ)**: Log-linear policy; parameter space is R^d with d < K
  - **NPG Projection r̂ = X(X^T X)^{-1}X^T r**: Precomputed once; determines NPG convergence eligibility

- Critical path:
  1. Verify X has full column rank and satisfies non-domination condition
  2. For Softmax PG: Check linear feasibility of reward order preservation (solve LP with constraints x_i^T w ≥ x_j^T w whenever r(i) > r(j))
  3. For NPG: Compute r̂ and verify r̂(a*) > max_{a≠a*} r̂(a)

- Design tradeoffs:
  - **Softmax PG vs NPG**: Different convergence conditions—neither dominates the other. Softmax PG can converge where NPG fails (Example 3) and vice versa (Example 2)
  - **Feature richness vs overfitting**: Higher d reduces approximation error but doesn't guarantee convergence; ordinal structure matters more
  - **Learning rate**: Softmax PG requires η < 4/(9·||r||_∞·λ_max(X^T X)) for monotonic improvement; NPG is more tolerant

- Failure signatures:
  - **Softmax PG stuck at suboptimal action**: Check if feature matrix violates non-domination or reward ordering cannot be preserved
  - **NPG probability ratio stagnant**: Verify r̂(a*) > r̂(a) for all suboptimal a; if ties exist, convergence fails (Example 4)
  - **High approximation error but convergence**: Expected—this paper proves approximation error is not diagnostic

- First 3 experiments:
  1. Replicate Examples 1-3: Train both Softmax PG and NPG on identical feature matrices with permuted columns; verify that convergence behavior depends on ordering preservation, not approximation error magnitude
  2. Stress-test ordering conditions: Generate random feature matrices with controlled approximation error but varying order preservation; plot convergence rate vs r̂ gap (for NPG) and LP feasibility (for Softmax PG)
  3. MDP generalization probe: Extend to a 2-state MDP with shared features; test whether preserving Q*(s,·) ordering per state is sufficient (as suggested in Appendix C)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the exact necessary and sufficient conditions for the global convergence of the standard Softmax Policy Gradient method under linear function approximation?
- Basis in paper: [explicit] The authors state in Section 7 that "Identifying exact necessary and sufficient conditions for the global convergence of Softmax PG remains future work."
- Why unresolved: The paper establishes that reward order preservation is sufficient (Theorem 1), but Example 5 demonstrates that global convergence can occur even when this condition is violated, proving the current condition is not necessary.
- What evidence would resolve it: A theoretical characterization of the convergence boundary that holds for all representations, specifically explaining the convergence observed in Example 5.

### Open Question 2
- Question: Can the ordering-based global convergence conditions be extended from the single-state (bandit) setting to general Markov Decision Processes (MDPs)?
- Basis in paper: [explicit] Section 7 lists "Extending the results and techniques to general MDPs" as a "challenging next step." Appendix C also discusses research plans for this generalization.
- Why unresolved: The proofs rely on static reward vectors $r$, whereas MDPs involve value functions $Q^{\pi}$ that change dynamically as the policy is updated, complicating the order preservation analysis.
- What evidence would resolve it: Convergence guarantees for Softmax PG or NPG in multi-state MDPs that utilize modified ordering conditions based on action-value functions rather than static rewards.

### Open Question 3
- Question: Can the proposed ordering-based conditions be utilized to design algorithms for better representation learning?
- Basis in paper: [explicit] Section 7 states, "Investigating whether these new global convergence conditions might be used to achieve better representation learning is of great interest for algorithm design."
- Why unresolved: The paper assumes a fixed representation $X$ and analyzes the convergence properties given that $X$; it does not explore how to algorithmically construct or learn $X$ to satisfy these conditions.
- What evidence would resolve it: A new representation learning objective derived from order preservation or non-domination conditions that empirically improves PG convergence compared to standard approximation-error minimization.

## Limitations

- Analysis restricted to finite-arm bandits (single-state MDPs); generalization to multi-state MDPs remains open
- Results may not directly apply to value-based PG methods (e.g., actor-critic) which are more common in practice
- Paper provides concrete examples but general applicability of non-domination condition and its relationship to feature design principles remains unclear

## Confidence

- **High confidence**: The theoretical proofs establishing ordering conditions for Softmax PG and NPG are rigorous and well-supported by the bandit examples. The mechanism showing approximation error is insufficient for convergence characterization is convincing.
- **Medium confidence**: The simulation results are illustrative but limited in scope (K=4, d=2). The extension to MDPs via per-state Q*-ordering preservation is proposed but not empirically validated.
- **Low confidence**: Claims about the broader implications for RL practice (e.g., how to design features that satisfy ordering conditions) are speculative without systematic evaluation on standard RL benchmarks.

## Next Checks

1. **MDP Generalization Test**: Implement a 2-state MDP with shared features and verify whether per-state Q*-ordering preservation suffices for convergence, as suggested in Appendix C.
2. **Feature Design Study**: Generate random feature matrices with controlled approximation error but varying order preservation; plot convergence rates against LP feasibility (Softmax PG) and r̂ gap (NPG).
3. **Larger-Scale Validation**: Test the ordering conditions on a standard RL benchmark (e.g., CartPole with linear features) to assess practical relevance beyond the bandit case.