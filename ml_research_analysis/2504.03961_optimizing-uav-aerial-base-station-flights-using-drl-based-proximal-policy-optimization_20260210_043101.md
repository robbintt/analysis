---
ver: rpa2
title: Optimizing UAV Aerial Base Station Flights Using DRL-based Proximal Policy
  Optimization
arxiv_id: '2504.03961'
source_url: https://arxiv.org/abs/2504.03961
tags:
- algorithm
- policy
- movement
- where
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a deep reinforcement learning approach for
  optimizing UAV-based base station positioning in emergency communication scenarios.
  The method employs proximal policy optimization (PPO) with continuous action spaces,
  using reference signals and angle of arrival measurements rather than GPS data to
  determine UAV positioning.
---

# Optimizing UAV Aerial Base Station Flights Using DRL-based Proximal Policy Optimization

## Quick Facts
- **arXiv ID**: 2504.03961
- **Source URL**: https://arxiv.org/abs/2504.03961
- **Reference count**: 24
- **Primary result**: DRL-based PPO approach for UAV positioning achieves >7 Mbps throughput with 17.64% improvement over static positioning

## Executive Summary
This paper presents a deep reinforcement learning approach using Proximal Policy Optimization (PPO) to dynamically position UAV-based aerial base stations in emergency communication scenarios. The system uses reference signals and angle of arrival measurements rather than GPS data to determine optimal UAV positioning, making it suitable for GPS-denied environments. The approach demonstrates significant performance improvements over static positioning methods across various user equipment mobility patterns including linear, circular, and mixed movements.

The proposed method shows particular strength in complex scenarios where UAV clusters move in perpendicular or opposite directions, achieving 17.64% and 11.30% improvements respectively. The algorithm maintains high throughput even under real-world imperfections, successfully operating with significant noise in angle of arrival estimation. This makes it particularly valuable for emergency response situations where communication infrastructure is damaged and traditional positioning methods are unavailable.

## Method Summary
The approach employs proximal policy optimization (PPO) with continuous action spaces to optimize UAV positioning in real-time. Instead of relying on GPS data, the system uses reference signals received from user equipment and angle of arrival measurements to determine UAV positioning. The PPO algorithm learns to adapt UAV positions based on these signals, continuously optimizing coverage and throughput. The training process incorporates various UE mobility patterns including static, linear, circular, and mixed movements, allowing the agent to develop robust positioning strategies across different scenarios.

## Key Results
- Achieved throughput exceeding 7 Mbps across various UE mobility scenarios
- Demonstrated 17.64% improvement over static positioning in perpendicular UAV movement scenarios
- Maintained high performance with significant noise in angle of arrival estimation
- Showed 11.30% improvement in opposite direction UAV movement scenarios

## Why This Works (Mechanism)
The PPO-based approach succeeds because it learns continuous positioning policies that adapt in real-time to UE movements without requiring GPS data. By using reference signals and angle of arrival measurements, the UAV can maintain accurate positioning even in GPS-denied environments. The proximal policy optimization algorithm provides stable learning by limiting policy updates to a trust region, preventing catastrophic performance drops during training. This enables the UAV to learn complex positioning strategies that balance coverage, throughput, and energy efficiency across diverse mobility patterns.

## Foundational Learning
- **Proximal Policy Optimization (PPO)**: A policy gradient method that optimizes policies by limiting updates to a trust region. Why needed: Prevents large policy updates that could destabilize training in continuous control problems.
- **Angle of Arrival (AoA) Estimation**: Technique to determine signal direction using multiple antennas. Why needed: Enables positioning without GPS by triangulating UE locations from signal angles.
- **Continuous Action Spaces**: Policy outputs that can take any value within a range rather than discrete choices. Why needed: Allows smooth UAV movement control rather than jerky discrete jumps.
- **Reference Signal Processing**: Method to extract channel state information from pilot signals. Why needed: Provides the feedback mechanism for learning optimal positioning policies.
- **Mobile User Equipment Patterns**: Different movement trajectories including linear, circular, and mixed motions. Why needed: Represents realistic user behaviors in emergency scenarios for training robust policies.

## Architecture Onboarding

Component Map:
PPO Agent -> Action Generator -> UAV Controller -> Environment -> State Observer -> Reward Calculator -> PPO Agent

Critical Path:
The PPO agent receives state observations (reference signals, AoA measurements) from the environment, computes actions for UAV positioning, and updates its policy based on rewards calculated from achieved throughput. The critical path involves continuous state-action-reward loops where the agent learns to maximize throughput while adapting to UE movements.

Design Tradeoffs:
The choice of PPO over other RL algorithms balances sample efficiency with stable learning, crucial for real-time deployment. Using AoA instead of GPS trades positioning accuracy for robustness in GPS-denied environments. Continuous action spaces enable smooth UAV movement but increase the complexity of the learning problem compared to discrete actions.

Failure Signatures:
Performance degradation occurs when AoA estimation becomes too noisy, when UE mobility patterns differ significantly from training data, or when the reward function doesn't properly capture the desired optimization objectives. The system may also fail to converge if the learning rate is too high or if the state space representation loses critical information.

First Experiments:
1. Test basic UAV movement control using simple PID controllers with simulated UEs
2. Implement AoA estimation with varying noise levels to characterize performance bounds
3. Train PPO agent on single UE linear movement scenario before scaling to multiple UEs

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Assumes perfect knowledge of UE trajectories for benchmarking, which may not hold in real emergency scenarios
- Simulation uses ideal channel conditions with only free-space path loss, potentially underestimating real-world interference
- Simplified mobility models (linear, circular, mixed) may not capture complexity of actual emergency situations
- Limited robustness testing with noise in AoA estimation, leaving uncertainty about extreme condition performance

## Confidence

| Claim | Confidence |
|-------|------------|
| Algorithm performance claims (throughput > 7 Mbps, 17.64% improvement) | High |
| Comparison to static positioning | High |
| Robustness to AoA noise | Medium |
| Real-world applicability in emergency scenarios | Low |

## Next Checks

1. Test the algorithm with more realistic mobility patterns based on actual emergency evacuation data or real-world traffic patterns
2. Implement the system with real UAV hardware to validate simulation results under actual hardware constraints and environmental conditions
3. Conduct extensive testing with varying levels of environmental noise, interference, and obstacles to assess performance degradation limits