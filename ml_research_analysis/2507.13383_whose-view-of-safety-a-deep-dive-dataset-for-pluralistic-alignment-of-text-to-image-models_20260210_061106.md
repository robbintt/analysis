---
ver: rpa2
title: Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image
  Models
arxiv_id: '2507.13383'
source_url: https://arxiv.org/abs/2507.13383
tags:
- raters
- safety
- demographic
- diverse
- rater
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the need for pluralistic alignment in text-to-image
  (T2I) models, recognizing that safety perceptions vary across demographic groups
  due to different lived experiences. The authors introduce DIVE, the first multimodal
  dataset specifically designed for pluralistic alignment, featuring 1000 adversarial
  prompt-image pairs evaluated by 637 demographically diverse raters across 30 intersectional
  groups (gender, age, ethnicity).
---

# Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image Models

## Quick Facts
- arXiv ID: 2507.13383
- Source URL: https://arxiv.org/abs/2507.13383
- Reference count: 40
- Introduces DIVE, first multimodal dataset for pluralistic alignment in text-to-image safety evaluation across intersectional demographic groups

## Executive Summary
This paper addresses the challenge of pluralistic alignment in text-to-image (T2I) models, recognizing that safety perceptions vary across demographic groups due to different lived experiences. The authors introduce DIVE, a dataset of 1000 adversarial prompt-image pairs evaluated by 637 demographically diverse raters across 30 intersectional groups. The dataset captures nuanced safety perceptions through graded harm scales and qualitative explanations. Empirical analysis confirms that demographic attributes significantly influence harm perception, with intersectional groups showing higher internal agreement than broader demographic categories. The study reveals that diverse raters identify different safety issues compared to conventional policy-based evaluations, particularly for bias-related content, and explores LLM steerability toward diverse perspectives.

## Method Summary
The DIVE dataset was created by sampling 1000 prompt-image pairs from the Adversarial Nibbler dataset, prioritizing pairs with disagreement among policy raters across three violation types and 12 topics. These pairs were evaluated by 637 raters recruited through Prolific, stratified across 30 intersectional groups (2 gender × 3 age × 5 ethnicity). Raters provided 5-point Likert scales for harm-to-self and harm-to-others, along with free-text explanations. The dataset was analyzed using Group Association Index (GAI) to measure demographic influence on harm perception, compared against policy-based safety classifiers, and used to evaluate LLM steerability through demographic prompting.

## Key Results
- Intersectional demographic groups show higher internal agreement (GAI=1.29-1.38) than broader demographic categories (GAI=1.04)
- Diverse raters identify safety issues systematically missed by conventional policy-based evaluations, especially for bias-related content
- LLM steerability shows modest improvements in alignment with diverse raters when demographic context is injected into prompts (~0.23 Kendall's Tau vs ~0.024 without)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Intersectional demographic groupings predict systematic differences in harm perception for T2I content, with higher internal cohesion than single-dimension demographics.
- **Mechanism**: Lived experiences, proxied by demographic attributes, shape how individuals perceive harm in visual content. Overlapping identities create more cohesive perspective clusters than broad demographic categories because shared experiences compound at intersections.
- **Core assumption**: Demographics reliably proxy for the lived experiences that influence harm perception.
- **Evidence anchors**:
  - [abstract]: "intersectional groups showing higher internal agreement than broader demographic categories"
  - [section 4.2, Table 1]: GenZ-Black raters show GAI=1.38, Millennial-Black GAI=1.29 (highest values), while broad GenZ group shows only GAI=1.04
  - [corpus]: PRISM and DICES datasets (text-based) validate demographics-as-proxy approach; corpus evidence for T2I multimodal domain is limited

### Mechanism 2
- **Claim**: Demographically diverse rater pools identify safety issues systematically missed by conventional policy-based evaluations, especially for bias-related content requiring cultural/contextual knowledge.
- **Mechanism**: Policy raters apply standardized guidelines that may not encode culturally-specific harm recognition. Diverse raters surface blind spots by bringing varied lived experiences to evaluation, particularly where implicit stereotypes or context-dependent harms exist.
- **Core assumption**: Diverse raters flag genuinely missed harms rather than simply producing noisier annotations.
- **Evidence anchors**:
  - [abstract]: "diverse raters identify different safety issues compared to conventional policy-based evaluations, particularly for bias-related content"
  - [section 5.1, Figure 4]: Policy raters, LlavaGuard, and ShieldGemma all show high false negative rates on 'Bias' violations compared to diverse raters; lower rates on 'Explicit'/'Violent'
  - [corpus]: Limited direct corpus evidence for this mechanism in T2I; related text-safety work notes similar patterns

### Mechanism 3
- **Claim**: Injecting demographic context into LLM prompts produces modest improvements in aligning LLM-as-judge ratings with target demographic groups.
- **Mechanism**: LLMs encode statistical associations between demographic identities and perspectives. Explicit demographic prompting activates relevant associations, and few-shot examples provide additional grounding, enabling limited perspective-taking without fine-tuning.
- **Core assumption**: LLMs' encoded demographic associations reflect genuine perspective differences rather than stereotypes.
- **Evidence anchors**:
  - [abstract]: "exploring LLM steerability toward diverse perspectives"
  - [section 5.2, Table 2]: Average Kendall's Tau correlation with diverse raters: ~0.23 with demographics in prompt vs. ~0.024 without; minimal improvement from few-shot examples
  - [corpus]: Related work on persona-prompting and character simulation exists but explicitly notes limitations in reproducing authentic human experiences of harm

## Foundational Learning

- **Concept: Pluralistic vs. Monolithic Alignment**
  - **Why needed here**: The entire DIVE framework rejects the premise of a single "ground truth" safety label. Without understanding this, you'll misinterpret disagreement as noise rather than signal.
  - **Quick check question**: If 60% of raters call an image "safe" and 40% call it "harmful," should you aggregate to "safe"? (Answer: No—this discards valid minority perspectives that DIVE is designed to preserve.)

- **Concept: Group Association Index (GAI)**
  - **Why needed here**: The paper uses GAI (ratio of within-group agreement to cross-group agreement) to validate that intersectional groups are more cohesive. You need this to interpret their claims about demographic proxy validity.
  - **Quick check question**: If a group has GAI=1.0, what does that mean? (Answer: Within-group agreement equals cross-group agreement—group membership provides no predictive signal for perspectives.)

- **Concept: Subjectivity Prioritization via Disagreement Signals**
  - **Why needed here**: DIVE's prompt-image sampling explicitly prioritized pairs with split policy-rater opinions (U=3 prioritized over U=6). This design choice shapes what the dataset can teach.
  - **Quick check question**: Why would you sample ambiguous cases rather than clear violations? (Answer: To capture where subjective interpretation matters most—clear cases don't reveal perspective diversity.)

## Architecture Onboarding

- **Component map**: Adversarial Nibbler dataset -> Sampling layer (disagreement prioritization) -> 1000 PI pairs -> Rater pool (30 trisections × ~22 raters) -> Annotation schema (harm scales + explanations) -> Evaluation metrics (GAI/IRR/XRR/Kendall's Tau)

- **Critical path**: 1) PI pair sampling using disagreement signals 2) Rater recruitment with intersectional stratification 3) Annotation collection (20-30 ratings per PI pair) 4) Analysis: GAI computation → demographic influence testing → safety classifier comparison → LLM steerability experiments

- **Design tradeoffs**:
  - Binary gender (Men/Women) vs. broader spectrum: Pragmatic choice for recruitment feasibility; limits generalizability
  - US/UK raters only: Cultural homogeneity within ethnicity groups; may not generalize globally
  - Adversarial prompts only: Over-represents harmful edge cases; may underrepresent subtle everyday harms
  - 20-30 ratings per pair: High replication enables reliable insights but increases cost

- **Failure signatures**:
  - Low IRR within intersectional groups (GAI ≈ 1.0): Demographics not predictive for this domain
  - Policy raters and diverse raters show high agreement on 'Bias' content: Blind-spot mechanism not operating
  - LLM steerability shows no improvement with demographic prompting: Base model lacks relevant associations

- **First 3 experiments**:
  1. Replicate GAI analysis on held-out PI pairs: Verify intersectional cohesion holds beyond the 1000-pair sample
  2. Permutation test on demographic groupings: Shuffle demographic labels and recompute GAI distribution to establish null baseline
  3. Fine-tuning experiment (not just prompting): Train a small multimodal model on DIVE ratings to compare against in-context steering baseline—determine if task-specific training substantially improves demographic alignment over prompting alone

## Open Questions the Paper Calls Out

- Can individual value profiles provide better predictive power for T2I safety perceptions than demographic groupings? (Basis: Future Work section suggests investigating individual value profiles as alternative to demographic proxies)
- How can the independent and interactive effects of prompt safety versus image safety on harm perception be disentangled? (Basis: Limitations section notes inability to isolate effects of text vs. visual content)
- How can larger or fine-tuned LLMs accurately represent diverse demographic perspectives in safety evaluations compared to the small models tested? (Basis: Limitations section notes only evaluated small open-sourced LLMs)
- How do expert policy knowledge and cultural/lived experiences independently contribute to safety rating differences between policy raters and diverse raters? (Basis: Limitations section notes policy raters' demographics were not collected)

## Limitations

- Demographic-as-proxy assumption remains unproven for multimodal T2I domains, despite validation in text-based safety research
- Binary gender framework excludes non-binary perspectives and limits generalizability of findings
- Adversarial sampling strategy may overrepresent extreme cases while missing subtle everyday harms

## Confidence

- **High confidence**: Intersectional group cohesion findings (GAI metrics showing 1.29-1.38 vs 1.04 for broader groups); safety classifier blind-spot analysis (clear quantitative differences in false negative rates for bias content)
- **Medium confidence**: Demographic-driven perspective diversity claims (demographic-as-proxy assumption not proven for T2I domain); mechanism explaining why diverse raters flag genuinely missed harms (lacks direct validation)
- **Medium confidence**: LLM steerability results showing modest improvements (~0.2 correlation gain), suggesting limited practical utility for perspective alignment

## Next Checks

1. **Cross-cultural validation**: Replicate the DIVE evaluation protocol with raters from at least three non-Western cultural contexts to test whether demographic-proxy relationships hold across different cultural frameworks for harm perception.

2. **Fine-tuning vs. prompting comparison**: Train a small multimodal model specifically on DIVE ratings from target demographic groups and compare performance against in-context steering baselines to determine if task-specific adaptation substantially outperforms prompting alone.

3. **Natural distribution sampling**: Apply DIVE's intersectional evaluation framework to a naturally-distributed T2I dataset (not adversarial sampling) to test whether demographic perspective diversity is equally pronounced for everyday content versus edge cases.