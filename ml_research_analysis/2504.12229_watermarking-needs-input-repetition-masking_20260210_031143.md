---
ver: rpa2
title: Watermarking Needs Input Repetition Masking
arxiv_id: '2504.12229'
source_url: https://arxiv.org/abs/2504.12229
tags:
- watermarked
- watermark
- aaronson
- response
- conversations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how linguistic adaptation\u2014specifically,\
  \ mimicry\u2014occurs when interacting with watermarked language models (LMs). The\
  \ authors show that unwatermarked LMs and humans begin to reproduce watermarked\
  \ language patterns when engaging in extended conversations with watermarked models,\
  \ leading to false positives in watermark detection."
---

# Watermarking Needs Input Repetition Masking

## Quick Facts
- arXiv ID: 2504.12229
- Source URL: https://arxiv.org/abs/2504.12229
- Reference count: 40
- Key outcome: Unwatermarked models and humans adapt to watermarked language, producing false positives in watermark detection during extended interaction.

## Executive Summary
This paper investigates how linguistic adaptation—specifically, mimicry—occurs when interacting with watermarked language models (LMs). The authors show that unwatermarked LMs and humans begin to reproduce watermarked language patterns when engaging in extended conversations with watermarked models, leading to false positives in watermark detection. Using two popular watermarking schemes, they demonstrate that even with low detection probabilities, unwatermarked models can mimic watermarks at rates up to 12.9%. Experiments with human participants confirm similar adaptation in real conversations. The study concludes that for long-term watermark reliability, significantly lower false positive rates and longer n-grams are needed.

## Method Summary
The authors conduct experiments with two popular watermarking schemes: distributed watermarking (DM) and signature watermarking (SM). They use unwatermarked models to interact with watermarked models, prompting them to continue text in a way that naturally leads to watermark mimicry. They also conduct human experiments where participants converse with watermarked models for up to 10 exchanges. The mimicry of watermark patterns is then evaluated by measuring false positive detection rates when unwatermarked models or humans produce watermarked text. The experiments focus on how the length of interaction and n-gram size affect adaptation and detection.

## Key Results
- Unwatermarked models mimic watermarked patterns at up to 12.9% false positive rate when interacting with watermarked models.
- Human participants show similar mimicry after as few as 3 exchanges with watermarked models.
- Current watermarking schemes are vulnerable to adaptation in extended interactions, necessitating lower false positive rates and longer n-grams.

## Why This Works (Mechanism)
The adaptation occurs because repeated exposure to watermarked language during extended interaction leads both models and humans to internalize and reproduce these patterns, even when generating new content. This mimicry results in text that appears watermarked to detectors, creating false positives.

## Foundational Learning
- **Watermarking schemes (DM/SM):** Methods for embedding detectable patterns in generated text; needed to understand how LMs can be marked and how detection works.
- **Linguistic mimicry:** The tendency of speakers (human or model) to unconsciously adopt the language patterns of their conversation partner; quick check: observe repetition of phrasing or structure in dialogue.
- **False positive rates:** The probability that unwatermarked text is incorrectly identified as watermarked; quick check: measure detection accuracy on known unwatermarked samples.
- **N-gram length:** The number of consecutive words used as a unit for watermark detection; longer n-grams are harder to mimic accidentally.
- **Adaptation over conversation length:** The effect of extended dialogue on increasing mimicry; quick check: compare mimicry rates at different conversation lengths.

## Architecture Onboarding
- **Component map:** Watermarked LM -> Unwatermarked LM/Human -> Detection System
- **Critical path:** Conversation generation with watermarked LM -> Mimicry adoption -> Text generation by unwatermarked LM/human -> Watermark detection
- **Design tradeoffs:** Shorter n-grams are easier to mimic (higher false positives) but more robust to watermarking; longer n-grams reduce false positives but may reduce watermark robustness.
- **Failure signatures:** High false positive rates in unwatermarked models after interaction; humans producing watermarked text after short conversations.
- **3 first experiments:** 1) Test mimicry in longer conversations (>10 exchanges). 2) Evaluate different watermarking schemes beyond DM/SM. 3) Assess the impact of input repetition masking on reducing adaptation.

## Open Questions the Paper Calls Out
None

## Limitations
- The longest human experiments were capped at 10 exchanges, potentially underrepresenting real-world, longer conversations.
- The study focuses on two specific watermarking schemes, leaving uncertainty about generalizability to other methods.
- The human participant pool is limited in size and demographic diversity, constraining generalizability.

## Confidence
- Confidence in the core claim—that unwatermarked LMs and humans adapt to and reproduce watermark patterns during interaction—is **Medium**, given strong experimental support but limited ecological validity.
- Confidence in the assertion that this adaptation leads to high false positive rates in watermark detection is **High**, as the results clearly demonstrate significant mimicry effects across tested models and humans.
- Confidence in the recommendation that longer n-grams and lower false positive rates are necessary for robust watermarking is **Medium**, as this is inferred from the data but not exhaustively validated across all potential scenarios.

## Next Checks
1. Test adaptation effects in extended, naturalistic dialogues exceeding 10 exchanges to assess the stability and magnitude of mimicry over time.
2. Evaluate the impact of input repetition masking and other countermeasures on reducing adaptation-induced false positives across diverse watermarking schemes.
3. Conduct experiments with a broader and more diverse set of human participants and model architectures to generalize findings and rule out context-specific biases.