---
ver: rpa2
title: Exploring the Feasibility of End-to-End Large Language Model as a Compiler
arxiv_id: '2511.04132'
source_url: https://arxiv.org/abs/2511.04132
tags:
- code
- llms
- compilation
- assembly
- compiler
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using end-to-end Large Language Models (LLMs)
  as compilers, directly transforming source code into assembly code. To evaluate
  this concept, the authors created CompilerEval, a dataset and framework with 20
  test cases across domains like linear algebra and image processing, and evaluated
  four mainstream LLMs (GPT-4o, Claude-3.5-Sonnet, Gemini-2.0-Flash-Exp, LLaMA-3.1-405B)
  against traditional compilers (gcc, clang).
---

# Exploring the Feasibility of End-to-End Large Language Model as a Compiler

## Quick Facts
- **arXiv ID:** 2511.04132
- **Source URL:** https://arxiv.org/abs/2511.04132
- **Reference count:** 40
- **Primary result:** LLMs can generate basic assembly code but achieve low compilation success rates, with only simple programs compiling correctly

## Executive Summary
This paper explores using end-to-end Large Language Models (LLMs) as compilers to directly transform source code into assembly code. The authors created CompilerEval, a dataset and framework with 20 test cases across domains like linear algebra and image processing, and evaluated four mainstream LLMs against traditional compilers (gcc, clang). The evaluation revealed that while LLMs possess basic assembly code generation capabilities, they achieve low compilation success rates, with only simple programs being correctly compiled and executed. The study found that applying prompt engineering, scaling up models, and incorporating reasoning methods significantly improved the quality of LLM-generated assembly code. Additionally, LLMs demonstrated the ability to generate cross-platform assembly code, with better performance on ARM and RISC-V architectures compared to x86.

## Method Summary
The authors evaluated four LLMs (GPT-4o, Claude-3.5-Sonnet, Gemini-2.0-Flash-Exp, LLaMA-3.1-405B) using the CompilerEval dataset containing 20 C kernel test cases. The LLMs generated assembly code for x86, ARM, and RISC-V architectures, which was then inserted into predefined templates, compiled, and executed. The evaluation measured Compilation Success Rate, Executable Samples Count, and Executable Samples Correctness Rate. The study tested prompt engineering variants targeting specific error categories, model scaling (Llama 8B/70B/405B), and reasoning methods (GPT-4o vs GPT-o1).

## Key Results
- LLMs show basic capability to generate executable assembly code from source code but are limited to simple programs
- Prompt engineering significantly improves compilation success, with Claude-3.5-Sonnet showing the largest improvement (7.5 percentage points)
- GPT-o1's reasoning method achieved superior performance, with improvements exceeding 30 percentage points on certain test cases
- ARM and RISC-V architectures achieved higher compilation success rates than x86 (35.02% vs 32.30% vs 27.85%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can perform end-to-end source-to-assembly translation by leveraging learned patterns from training on code corpora containing both high-level languages and assembly.
- Mechanism: The LLM maps source code tokens directly to target architecture instruction sequences, bypassing traditional IR-based transformations. This relies on the model's ability to internalize compilation patterns rather than explicit compiler logic.
- Core assumption: The training data contains sufficient aligned source-assembly pairs for the model to learn transformation patterns; this alignment quality is not verified in the paper.
- Evidence anchors:
  - [abstract] "LLMs exhibit basic capabilities as compilers but currently achieve low compilation success rates"
  - [Section IV-A] "LLMs possess the basic capability to generate executable assembly code from source code. However, they are limited to correctly compiling and executing simple programs"
  - [corpus] LEGO-Compiler paper addresses the same transformation problem but decomposes it for better composability, suggesting pure end-to-end has limitations
- Break condition: Complex programs with nested control flow, indirect memory access, or optimization requirements exceeding the model's pattern-matching capacity (evidenced by 0% success on jacobi-2d across all models).

### Mechanism 2
- Claim: Error-type-specific prompts can reduce compilation failures by constraining the output space and providing corrective guidance.
- Mechanism: By categorizing observed errors (instruction errors: 7269 occurrences, invalid register usage: 2052, symbol redefinition: 2556) and designing prompts that address each category, the model receives targeted feedback to avoid specific failure modes.
- Core assumption: The model can interpret and apply prompt constraints during generation; effectiveness varies significantly by model architecture.
- Evidence anchors:
  - [abstract] "By optimizing prompts... the quality of assembly code generated by LLMs can be significantly enhanced"
  - [Section IV-B] "Claude demonstrating the largest increase of 7.5 percentage points, while Llama showed minimal gains... some negative effects were observed, particularly in Gemini's performance"
  - [corpus] Weak direct corpus support for error-guided prompting specifically in compilation; related work focuses on IR-level optimization
- Break condition: When prompt token overhead consumes context window needed for complex programs, or when error categories overlap in ways prompts cannot disambiguate.

### Mechanism 3
- Claim: Reasoning-augmented models (e.g., GPT-o1) improve compilation success by decomposing the transformation into intermediate reasoning steps analogous to compiler passes.
- Mechanism: Chain-of-thought reasoning enables the model to perform implicit analysis phases (semantic understanding, register allocation planning, instruction scheduling) before generating final assembly, rather than direct token-to-token mapping.
- Core assumption: The reasoning process can learn to approximate traditional compiler phases without explicit phase structure; Assumption: reinforcement learning on compilation tasks internalizes these patterns.
- Evidence anchors:
  - [abstract] "incorporating reasoning methods, the quality of assembly code generated by LLMs can be significantly enhanced"
  - [Section IV-B] "GPT-o1 achieved superior performance across most test cases. Significant improvements exceeding 30 percentage points were observed in rotate, resize, and saxpy kernels"
  - [corpus] Compiler-R1 uses RL for compiler tuning; REASONING COMPILER uses LLM-guided optimization—suggests reasoning + compilation is an active direction but not yet established
- Break condition: Programs requiring precise register allocation across long code sequences where intermediate reasoning states exceed context limits.

### Mechanism 4
- Claim: RISC architectures (ARM, RISC-V) yield higher compilation success than CISC (x86) due to simpler, more regular instruction patterns that better match LLM sequence prediction.
- Mechanism: Reduced instruction sets have fewer instruction variants, more consistent operand formats, and less complex addressing modes—reducing the output vocabulary complexity and pattern diversity the model must learn.
- Core assumption: Training data contains proportionally more or better-quality ARM/RISC-V assembly; this distribution is not analyzed in the paper.
- Evidence anchors:
  - [Section IV-C] "ARM and RISC-V architectures achieved higher rates than x86... 35.02% (ARM), 32.30% (RISC-V), 27.85% (x86)"
  - [Section IV-C] "We attribute this result to the characteristics of ARM and RISC-V as reduced instruction set architectures, where instructions are relatively simple for LLMs to understand"
  - [corpus] No corpus papers directly compare cross-architecture LLM compilation performance
- Break condition: Long programs on RISC architectures where increased instruction count exceeds context length (acknowledged in Section IV-C).

## Foundational Learning

- Concept: Three-phase compiler architecture (frontend → middle-end → backend)
  - Why needed here: LaaC proposes replacing this entire pipeline with a single LLM; understanding what each phase does (lexical/syntax analysis, IR optimization, instruction selection/register allocation) is prerequisite to evaluating what the LLM must learn.
  - Quick check question: Can you explain what information is lost when a traditional compiler lowers from source to IR, and why the authors claim LLMs might avoid this loss?

- Concept: Instruction set architecture characteristics (RISC vs. CISC)
  - Why needed here: The paper reports significant performance differences across x86, ARM, and RISC-V; understanding register counts, instruction regularity, and addressing modes explains why.
  - Quick check question: Why would a sequence prediction model perform better on a fixed-length instruction encoding versus variable-length encodings with complex addressing modes?

- Concept: Assembly-level correctness (lexical, syntactic, semantic)
  - Why needed here: The evaluation distinguishes executable samples (syntactically valid) from correct execution (semantically valid); the gap reveals where LLMs fail.
  - Quick check question: If an LLM generates assembly that assembles without errors but produces wrong output at runtime, what category of error is this, and which Table II error types might be responsible?

## Architecture Onboarding

- Component map: Source Code (C) -> [Prompts + Knowledge Base] -> LLM Core -> [Reasoning Module] -> Assembly Code (.s) -> [Template Integration] -> [Assembler + Linker] -> Executable -> [Evaluation Module]

- Critical path:
  1. Prompt design (includes target architecture specification, error-type guidance)
  2. LLM inference (source → assembly generation)
  3. Template insertion (kernel assembly + main program scaffold)
  4. Compilation via system assembler
  5. Execution and output comparison against reference compiler output

- Design tradeoffs:
  - Prompt detail vs. context consumption: More detailed prompts (error corrections, ISA specs) improve success rates but consume tokens that could hold program context
  - Model scale vs. latency: 405B models show better results but are impractically slow for compilation workflows
  - Single-stage vs. reasoning: GPT-o1's reasoning improves quality but adds latency and cost
  - Architecture target selection: ARM/RISC-V yield better results but may not match deployment targets

- Failure signatures:
  - Compilation errors (largest category): Unrecognized characters, instruction errors, invalid register usage, symbol redefinition—indicates the LLM generates syntactically invalid assembly
  - Execution errors (second category): Segmentation faults, illegal instructions—indicates syntactically valid but semantically incorrect code (wrong register binding, incorrect memory access patterns)
  - Wrong results (smallest category): Program runs but produces incorrect output—indicates logic errors in the generated instruction sequence

- First 3 experiments:
  1. Replicate the basic evaluation: Run CompilerEval on a single model (suggest Claude-3.5-Sonnet or GPT-4o) for all 20 test cases on x86, categorizing failures into compilation errors vs. execution errors vs. wrong results. This establishes baseline familiarity with the failure modes.
  2. Ablate prompt components: Starting from the best-performing prompt configuration, selectively remove error-type guidance and measure the per-category error increase. This reveals which prompt elements are actually causal vs. coincidental.
  3. Cross-architecture comparison on a single kernel: Take `saxpy` (the only case where all models produced executable samples) and generate ARM, RISC-V, and x86 assembly. Manually inspect the generated code to understand why x86 shows lower quality—hypothesis: look for addressing mode complexity and register pressure differences.

## Open Questions the Paper Calls Out

- **How can an LLM be trained to simultaneously meet the strict constraints of compilation accuracy, computational cost, and context length?**
  - Basis in paper: [explicit] Section V.B explicitly asks, "How can LaaC train an LLM that meets compilation scenario constraints?"
  - Why unresolved: Current models require excessive resources and fail to handle large programs; specialized training methods like knowledge distillation for compilation are proposed but untested.
  - Evidence: A fine-tuned model that achieves compilation accuracy comparable to gcc/clang with latency and resource usage suitable for standard developer workflows.

- **How can LaaC effectively decouple core compilation reasoning from specific language and hardware specifications to support many-to-many mappings?**
  - Basis in paper: [explicit] Section V.B asks, "How can LaaC effectively bridge diverse programming languages and multiple hardware instruction sets?"
  - Why unresolved: The paper proposes using knowledge bases and prompt engineering to avoid retraining, but notes the challenge of context window consumption by these additional inputs.
  - Evidence: A system successfully compiling a new source language to a new architecture using only a prompt update and spec sheet, without modifying the model weights.

- **How can LaaC integrate with debugging infrastructure to generate accurate debugging information and enable intelligent error analysis?**
  - Basis in paper: [explicit] Section V.B explicitly asks, "How can LaaC collaborate effectively with debuggers?"
  - Why unresolved: LLMs currently struggle to generate syntactically correct assembly, making the generation of accurate symbol tables and debug metadata a significant challenge.
  - Evidence: An LaaC system that generates standard debugging information (e.g., DWARF format) allowing debuggers to correctly map machine states to source variables.

## Limitations

- The evaluation relies on fixed prompt templates without ablation studies isolating the impact of individual prompt components on error categories
- Cross-architecture performance differences may reflect training data bias rather than inherent architectural advantages
- The CompilerEval dataset, while comprehensive, may not represent real-world compilation complexity including optimization requirements and edge cases
- Performance metrics combine syntactic correctness (executable samples) with semantic correctness (correct execution), obscuring where LLMs fail

## Confidence

- High confidence: Basic end-to-end assembly generation capability exists but remains limited to simple programs
- Medium confidence: Prompt engineering and reasoning methods significantly improve quality, though effects vary by model architecture
- Medium confidence: RISC architectures (ARM, RISC-V) show better performance than x86 due to instruction set complexity
- Low confidence: Long-term viability of pure end-to-end compilation without IR intermediate representations

## Next Checks

1. **Error Category Ablation**: Systematically remove prompt components addressing specific error types (instruction errors, register usage, symbol redefinition) to measure individual contribution to compilation success rates
2. **Training Data Analysis**: Analyze the proportion and quality of ARM/RISC-V vs x86 assembly in the training corpora of evaluated models to verify architectural performance claims
3. **Complexity Scaling Test**: Generate progressively more complex versions of simple kernels (saxpy, conv2d) by adding nested loops and conditional logic to identify the complexity threshold where LLM compilation consistently fails