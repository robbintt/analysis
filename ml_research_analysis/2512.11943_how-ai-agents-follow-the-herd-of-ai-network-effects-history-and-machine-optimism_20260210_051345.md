---
ver: rpa2
title: How AI Agents Follow the Herd of AI? Network Effects, History, and Machine
  Optimism
arxiv_id: '2512.11943'
source_url: https://arxiv.org/abs/2512.11943
tags:
- agents
- network
- participation
- historical
- effects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how large language model (LLM)-based agents\
  \ navigate network-effect games where individual payoffs depend on peer participation.\
  \ Using a conference-attendance framework, we test three Qwen models (max, turbo,\
  \ 1.5B) across static and repeated game settings with four price trajectories (fixed,\
  \ ascending, descending, random) and two network-effect strengths (\u03B2=0.25,\
  \ \u03B2=0.75)."
---

# How AI Agents Follow the Herd of AI? Network Effects, History, and Machine Optimism

## Quick Facts
- arXiv ID: 2512.11943
- Source URL: https://arxiv.org/abs/2512.11943
- Reference count: 2
- Agents fail to converge toward theoretical equilibrium without historical data

## Executive Summary
This study investigates how large language model (LLM)-based agents navigate network-effect games where individual payoffs depend on peer participation. Using a conference-attendance framework, we test three Qwen models across static and repeated game settings with four price trajectories and two network-effect strengths. Without historical data, agents fail to converge toward theoretical equilibrium, showing significant expectation dispersion. Under weak network effects, ordered price trajectories enable partial convergence to theoretical participation levels. However, strong network effects trigger persistent "AI optimism" - agents overestimate participation despite contradictory historical evidence.

## Method Summary
The study employs a 6-player conference-attendance game where agents decide whether to participate based on expected peer attendance and a payoff function U_j = θ_j + βN - p_j ≥ 0. Three Qwen models (Qwen-max, Qwen-turbo, Qwen-2.5-1.5B) are tested across static games (10 iterations per price, no history) and repeated games (6 price adjustments per trajectory, 10 independent repetitions). The study tests four price trajectories (fixed, ascending, descending, random) and two network-effect strengths (β=0.25, β=0.75), comparing agent expectations to theoretical Fulfilled Expectation Equilibrium (FEE) participation levels.

## Key Results
- Without historical data, agents show significant expectation dispersion and fail to converge toward theoretical equilibrium
- Weak network effects (β=0.25) with ordered price trajectories enable partial convergence to theoretical participation levels
- Strong network effects (β=0.75) trigger persistent "AI optimism" where agents systematically overestimate participation despite contradictory evidence

## Why This Works (Mechanism)
None

## Foundational Learning
- **Network effects**: How individual utility depends on total participation - needed to understand payoff structure; check by verifying U_j = θ_j + βN - p_j formulation
- **Fulfilled Expectation Equilibrium (FEE)**: Theoretical benchmark where agents' expectations match actual participation - needed to measure convergence; check by computing FEE for given β and price values
- **Temporal coherence**: How ordered vs. random historical data affects reasoning - needed to explain convergence patterns; check by comparing ascending/descending vs. random trajectories
- **AI optimism**: Systematic overestimation of participation under strong network effects - needed to characterize behavioral pattern; check by measuring deviation from FEE under β=0.75
- **Price trajectory design**: Fixed, ascending, descending, and random price sequences - needed to test environmental stability; check by mapping each to theoretical participation thresholds
- **Historical data injection**: How past participation counts are presented to agents - needed for repeated game dynamics; check by validating format and timing of historical information delivery

## Architecture Onboarding
- **Component map**: Manager agent sets prices -> collects participation -> provides historical counts -> scholar agents update expectations
- **Critical path**: Price selection → participation decision → historical data update → expectation formation → next round
- **Design tradeoffs**: Fixed price (stable but unrealistic) vs. varying price (dynamic but potentially confusing); strong network effects (amplified optimism) vs. weak effects (partial convergence)
- **Failure signatures**: Expectation dispersion without history; persistent optimism despite negative feedback; complete failure under random price sequences
- **First experiments**: 1) Test convergence with clean ascending price trajectory and weak network effects; 2) Validate "AI optimism" by measuring overestimation under strong network effects with descending prices; 3) Compare expectation formation with and without historical data injection

## Open Questions the Paper Calls Out
- How do strategic dynamics change in hybrid human-AI networks compared to pure AI systems? [explicit] The conclusion explicitly states, "Future work should explore hybrid human-AI networks."
- Does the reliance on temporal coherence persist under real-world data noise? [explicit] The authors identify "real-world data noise" as a necessary extension for future research.
- Is "AI optimism" an artifact of training data or a structural reasoning failure? [inferred] Section 5.3 and 5.4 note that agents prioritize collective gains over evidence, but the paper does not determine if this is a model weighting issue or a logic deficit.

## Limitations
- The "AI optimism" phenomenon and failure to converge depend critically on specific prompt structures and historical data formatting not fully specified
- Price trajectory design may not fully capture real-world network-effect scenarios, potentially limiting generalizability
- Study isolates AI agents in controlled settings, excluding human behavioral factors that would exist in hybrid networks

## Confidence
- **High confidence**: Empirical observation that LLM agents fail to converge toward theoretical equilibrium without historical data, and that convergence improves under weak network effects with ordered price trajectories
- **Medium confidence**: Characterization of "AI optimism" as systematic overestimation under strong network effects, as this relies heavily on specific historical data formatting
- **Low confidence**: Claim that "temporal coherence in historical data critically shapes LLM reasoning" - requires validation across different prompt architectures and model variants

## Next Checks
1. Test alternative prompt formats (different historical data presentation, explicit reasoning steps, output constraints) to determine if convergence patterns persist across implementations
2. Validate findings across multiple LLM architectures beyond Qwen (e.g., GPT, Claude) to assess whether observed behaviors are model-specific or generalizable
3. Systematically vary price step sizes and range boundaries to identify whether convergence failures are robust to trajectory parameterization or stem from specific price-point choices