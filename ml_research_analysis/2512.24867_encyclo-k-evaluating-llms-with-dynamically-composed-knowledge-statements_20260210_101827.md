---
ver: rpa2
title: 'Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements'
arxiv_id: '2512.24867'
source_url: https://arxiv.org/abs/2512.24867
tags:
- statements
- knowledge
- zhang
- wang
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Encyclo-K, a novel benchmark for evaluating
  large language models (LLMs) that addresses three limitations of existing benchmarks:
  vulnerability to data contamination, restriction to single-knowledge-point assessment,
  and reliance on costly domain expert annotation. Encyclo-K rethinks benchmark construction
  by treating individual knowledge statements, extracted from authoritative textbooks,
  as the unit of curation rather than questions.'
---

# Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements

## Quick Facts
- **arXiv ID:** 2512.24867
- **Source URL:** https://arxiv.org/abs/2512.24867
- **Reference count:** 40
- **Primary result:** Even top model (OpenAI-GPT-5.1) achieves only 62.07% accuracy on this novel LLM evaluation benchmark

## Executive Summary
This paper introduces Encyclo-K, a novel benchmark for evaluating large language models (LLMs) that addresses three limitations of existing benchmarks: vulnerability to data contamination, restriction to single-knowledge-point assessment, and reliance on costly domain expert annotation. Encyclo-K rethinks benchmark construction by treating individual knowledge statements, extracted from authoritative textbooks, as the unit of curation rather than questions. These statements are dynamically composed into evaluation questions through random sampling at test time.

The benchmark's design ensures contamination resistance through combinatorial statement composition, comprehensive assessment via multi-statement questions, and reduced annotation costs by requiring only formatting verification. Experiments on over 50 LLMs demonstrate Encyclo-K's substantial challenges: even the top-performing OpenAI-GPT-5.1 achieves only 62.07% accuracy, with clear performance gradients across reasoning models (16.04%-62.07%) and chat models (9.71%-50.40%). The benchmark effectively differentiates capabilities while validating the effectiveness of dynamic evaluation and multi-statement comprehensive understanding.

## Method Summary
Encyclo-K introduces a fundamentally different approach to benchmark construction by using knowledge statements as atomic units rather than pre-formed questions. The method extracts authoritative knowledge statements from textbooks in biomedical and history domains, then dynamically composes these statements into evaluation questions through random sampling at test time. This combinatorial approach creates 16,380 possible questions from a base of 260 statements, ensuring contamination resistance since models cannot memorize specific question-answer pairs. The evaluation requires models to synthesize information across multiple statements, testing comprehensive understanding rather than isolated knowledge retrieval. Annotation costs are minimized because domain experts only need to verify statement formatting rather than create and validate entire questions.

## Key Results
- Top-performing model (OpenAI-GPT-5.1) achieves only 62.07% accuracy, demonstrating benchmark difficulty
- Clear performance gradient between reasoning models (16.04%-62.07%) and chat models (9.71%-50.40%)
- 50+ models tested, showing effective differentiation across model capabilities
- Dynamic composition approach successfully prevents data contamination while enabling comprehensive assessment

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its combinatorial design that creates fundamentally different evaluation questions at test time. By treating knowledge statements as atomic units and randomly composing them into questions, the approach ensures that even if models have seen individual statements during training, they cannot have encountered the specific multi-statement combinations used for evaluation. This dynamic composition forces models to demonstrate genuine comprehension and reasoning capabilities rather than pattern matching or memorization. The multi-statement questions also require models to integrate knowledge across different domains, providing a more comprehensive assessment of their understanding capabilities.

## Foundational Learning
- **Knowledge statement extraction:** Required to create atomic units of evaluation that can be safely combined without contamination risk
- **Combinatorial question generation:** Enables creation of vast question spaces from limited statement sets while maintaining evaluation integrity
- **Dynamic evaluation methodology:** Allows testing models on novel question combinations they couldn't have memorized
- **Multi-domain knowledge integration:** Tests comprehensive understanding rather than isolated fact retrieval
- **Cost-effective annotation:** Reduces expert time requirements from full question creation to simple format verification
- **Contamination resistance principles:** Critical for ensuring benchmark validity in an era of widespread data leakage

Quick check: Verify that the combinatorial space (16,380 questions) is sufficiently large relative to the knowledge base size (260 statements) to prevent memorization patterns.

## Architecture Onboarding

**Component map:** Knowledge Statement Database -> Dynamic Question Composer -> LLM Evaluation Interface -> Performance Metrics

**Critical path:** The system extracts statements from authoritative sources, stores them in a knowledge base, then randomly samples and composes these statements into evaluation questions at test time. The LLM processes these dynamically generated questions, and performance metrics are calculated based on accuracy across the question set.

**Design tradeoffs:** The approach trades the simplicity of static benchmarks for contamination resistance and comprehensive assessment. While this increases implementation complexity and requires careful statement curation, it provides more reliable evaluation of genuine understanding versus memorization.

**Failure signatures:** Models performing poorly despite high training performance on static benchmarks, suggesting the dynamic composition reveals limitations in comprehensive understanding. High variance in performance across different question compositions may indicate sensitivity to specific knowledge combinations.

**First experiments:** 1) Test contamination resistance by evaluating models on both dynamically composed and static versions of identical knowledge content. 2) Measure annotation time savings by comparing expert time requirements for traditional question creation versus format verification. 3) Evaluate performance degradation as the number of statements per question increases to identify comprehension limits.

## Open Questions the Paper Calls Out
None

## Limitations
- Relatively small knowledge base (260 statements) may not capture full diversity of required knowledge
- Focus on biomedical and history domains limits generalizability to other knowledge areas
- Benchmark shows only 62.07% maximum accuracy even for top models, raising questions about potential systematic issues
- Single-dataset evaluation may not fully validate contamination resistance claims

## Confidence
- **High:** Benchmark construction methodology and contamination resistance claims are mathematically sound
- **Medium:** Performance differentiation claims across model types are supported but based on single-dataset evaluation
- **Medium-Low:** Annotation cost reduction claims lack detailed time/cost analysis compared to traditional methods

## Next Checks
1. Expand the knowledge base to include additional domains and measure impact on model performance distribution
2. Conduct cross-dataset validation using Encyclo-K questions to evaluate contamination resistance claims empirically
3. Perform ablation studies testing different statement composition strategies (e.g., semantic similarity vs. random sampling) and their effect on benchmark difficulty and model differentiation