---
ver: rpa2
title: Are LLMs reliable? An exploration of the reliability of large language models
  in clinical note generation
arxiv_id: '2505.17095'
source_url: https://arxiv.org/abs/2505.17095
tags:
- clinical
- note
- llms
- consistency
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the reliability of 12 open-weight and proprietary
  large language models (LLMs) in clinical note generation by measuring consistency
  and correctness across multiple iterations using the same prompt. Meta's Llama 70B
  and Mistral's Small model achieved perfect semantic consistency and outperformed
  proprietary models, making them the most reliable.
---

# Are LLMs reliable? An exploration of the reliability of large language models in clinical note generation

## Quick Facts
- arXiv ID: 2505.17095
- Source URL: https://arxiv.org/abs/2505.17095
- Reference count: 10
- Key outcome: Meta's Llama 70B and Mistral's Small model achieved perfect semantic consistency and outperformed proprietary models in clinical note generation.

## Executive Summary
This study evaluates the reliability of 12 open-weight and proprietary large language models (LLMs) in clinical note generation by measuring consistency and correctness across multiple iterations using the same prompt. The research found that mid-sized open-weight models like Meta's Llama 3.1-70B and Mistral's Small-2402 22B achieved perfect semantic consistency and outperformed larger proprietary models like GPT-4o and Claude Sonnet. By configuring LLMs with deterministic sampling parameters (temperature=0, top_p=0, top_k=1), the study demonstrated that open-weight models can generate highly reliable clinical notes while offering better data privacy compliance through local deployment.

## Method Summary
The study evaluated 12 LLMs using the aci-bench dataset of simulated clinical transcripts. Models were configured with deterministic sampling parameters to maximize consistency, and generated notes were assessed using BERTScore for semantic similarity. The evaluation measured three metrics: consistency rate (CR) based on string equivalence, semantic consistency (SC) based on embedding similarity, and semantic similarity (SS) comparing generated notes to ground truth. Each model processed 90 transcripts with 5 iterations per transcript, generating 450 notes per model for comparison.

## Key Results
- Meta's Llama 3.1-70B and Mistral's Small-2402 22B achieved 100% semantic consistency and outperformed proprietary models
- Open-weight models generally demonstrated better reliability than proprietary alternatives when configured for determinism
- Most models showed stable semantic consistency despite varying outputs, with Llama 3.1-70B and Mistral Small ranking highest in reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Configuring LLM sampling parameters to maximize determinism improves semantic consistency in clinical note generation without inherently degrading correctness.
- Mechanism: By setting `temperature=0`, `top_p=0`, and `top_k=1`, token sampling randomness is minimized, compelling the model to select the highest-probability next token. For constrained tasks like summarization, this path more reliably aligns with the training data's dominant patterns for medical language.
- Core assumption: The model possesses a sufficiently stable internal representation of medical language; reducing sampling variance only sharpens retrieval of this representation rather than introducing systematic bias.
- Evidence anchors:
  - [abstract] "...evaluates the reliability of 12...LLMs...configured to maximize determinism."
  - [section] Table 1 shows all temperature/top_p/top_k parameters set to 0 or 1. Meta Llama 3.1-70B and Mistral Small-2402 22B achieve 100% semantic consistency.
  - [corpus] Corpus papers do not directly analyze the effect of these specific parameters on consistency.
- Break condition: If the model has not been sufficiently pre-trained on clinical text, minimizing sampling variance may cause repetitive, low-quality, or hallucinated outputs by locking onto an incorrect high-probability path.

### Mechanism 2
- Claim: Embedding-based semantic similarity metrics (e.g., BERTScore) are more appropriate for measuring reliability in long-form clinical notes than strict string equivalence.
- Mechanism: Clinical notes can convey identical meaning with varied phrasing. String equivalence (consistency rate) fails to capture this. BERTScore measures cosine similarity in a contextual embedding space, detecting semantic alignment despite stylistic differences.
- Core assumption: The underlying embeddings are sufficiently sensitive to medical semantics to distinguish clinically meaningful variations from superficial ones.
- Evidence anchors:
  - [abstract] "have the same meaning (semantic consistency) and are correct (semantic similarity)..."
  - [section] "String equivalence was noted as a strict measure of reliability while evaluating whether responses contextually mean the same is specifically important in CNG due to stylistic differences..."
  - [corpus] Related work (e.g., DENSE, arXiv:2507.14079) also utilizes embedding-based metrics for clinical text evaluation.
- Break condition: If the embedding model cannot distinguish between a correct note and one containing a clinically significant hallucination (e.g., wrong medication dosage), the metric will overstate reliability.

### Mechanism 3
- Claim: Mid-sized open-weight models can achieve superior reliability over larger proprietary models for clinical note generation from transcripts.
- Mechanism: The study finds models like Llama 3.1-70B and Mistral Small-2402 outperform GPT-4o and Claude Sonnet on combined measures of semantic consistency and correctness. This suggests a favorable balance of model capacity (for modeling clinical patterns) and instruction-following alignment, without the complexity that can introduce variability in much larger systems. Open weights further enable local deployment for data privacy.
- Core assumption: The `aci-bench` dataset of simulated transcripts is sufficiently representative of real clinical conversations to generalize these performance rankings.
- Evidence anchors:
  - [abstract] "Meta's Llama 70B and Mistral's Small model achieved perfect semantic consistency and outperformed proprietary models"
  - [section] Figure 4 and Table 2 position Llama 3.1-70B and Mistral Small in the top-right quadrant for reliability.
  - [corpus] Other papers (e.g., arXiv:2501.12106) evaluate open-source LLMs on medical tasks but do not offer direct comparative confirmation of this specific ranking.
- Break condition: Generalization may fail if performance is highly sensitive to prompt formatting or if real clinical notes require a level of nuance not present in the simulated benchmark.

## Foundational Learning

- Concept: **Intra-prompt Stability**
  - Why needed here: This is the study's core definition of reliability, measuring whether an LLM produces consistent outputs when given the exact same input multiple times. It underpins the Consistency Rate (CR) and Semantic Consistency (SC) metrics.
  - Quick check question: If an LLM generates three different notes from the same transcript, but all three have the same clinical meaning, would its consistency rate be high or low? What about its semantic consistency?

- Concept: **BERTScore**
  - Why needed here: The paper uses BERTScore as its primary evaluation metric. Understanding that it compares texts in an embedding space rather than by surface-form matching is essential to interpreting the reported results on semantic consistency and similarity.
  - Quick check question: Why is BERTScore considered more appropriate for evaluating clinical notes than a metric based on exact string matching (like ROUGE)?

- Concept: **Model Parameters for Determinism**
  - Why needed here: The paper explicitly manipulates `temperature`, `top_p`, and `top_k`. An engineer must understand that these parameters control the sampling strategy and that setting them to zero forces the model to select the single most probable next token.
  - Quick check question: If you set `temperature=1.0`, what is the expected effect on the consistency rate (CR) of the model's outputs, compared to `temperature=0`?

## Architecture Onboarding

- Component map: Transcript -> User Prompt Template -> LLM (configured with temperature=0, top_p=0, top_k=1) -> Generated Clinical Note -> BERTScore Evaluation

- Critical path: **Prompt Template Design → Model Configuration → BERTScore Evaluation**. An error in template formatting leads to malformed notes. Incorrect model configuration (e.g., non-zero temperature) destroys consistency. An inappropriate metric provides misleading signals.

- Design tradeoffs: **Accessibility vs. Privacy/Control**. Proprietary models are easy to access but pose data privacy risks. Open-weight models require local compute but guarantee data privacy. The study's finding—that open-weight models can outperform proprietary ones on reliability—helps resolve this tradeoff in favor of local deployment.

- Failure signatures: 1. **Low CR, Low SC**: Indicates high randomness or a model incapable of the task. 2. **High SC, Low SS**: The model is *consistently wrong*; its stable outputs do not match ground truth. 3. **High CR, Low SS**: The model is deterministically producing an incorrect note.

- First 3 experiments:
  1. **Baseline Replication**: Run Llama 3.1-70B on the `aci-bench` subset with `temperature=0`. Calculate the three metrics to establish a baseline for your infrastructure.
  2. **Parameter Sensitivity Test**: Run the same experiment while varying `temperature` (e.g., 0.0, 0.3, 0.7, 1.0). Plot the degradation of Consistency Rate to quantify the reliability-diversity tradeoff.
  3. **Prompt Robustness Test**: Create 3 variations of the `User Prompt Template` (e.g., changing instruction phrasing). Measure if Semantic Similarity remains stable across variations to assess prompt brittleness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust are LLM-generated clinical notes across different prompt formulations and prompt engineering strategies?
- Basis in paper: [explicit] "As this study did not include prompt optimization, future work could involve comparing the same measures across various prompts to check for robustness"
- Why unresolved: Only a single prompt template was used across all experiments without any prompt optimization or variation testing.
- What evidence would resolve it: Multi-prompt experiments measuring consistency and correctness across varied prompt formulations for identical transcripts.

### Open Question 2
- Question: What is the correlation between BERTScore automatic metrics and human expert evaluation for clinical note quality?
- Basis in paper: [explicit] "Metrics that utilize knowledge graphs and sentence parsers can also be used, along with an evaluation by human experts" (listed in Limitations)
- Why unresolved: The study relied solely on automatic evaluation metrics; no human expert validation was conducted despite human evaluation being the acknowledged gold standard.
- What evidence would resolve it: Parallel evaluation using both automatic metrics and blinded human expert ratings with statistical correlation analysis.

### Open Question 3
- Question: Do these reliability findings generalize to real clinical encounters rather than simulated role-play conversations?
- Basis in paper: [explicit] "Our work only used one publicly available dataset that includes data gathered from simulations in English. We believe that it is necessary to conduct clinical validation and utility studies to capture and address contextual nuances"
- Why unresolved: The aci-bench dataset uses role-played patient-doctor conversations, not authentic clinical encounters with real patients.
- What evidence would resolve it: Comparative validation study using transcripts from actual clinical encounters with appropriate ethical approvals and patient consent.

### Open Question 4
- Question: How do these LLMs perform on inter-prompt stability for clinical note generation when subjected to varied prompts designed to elicit equivalent outputs?
- Basis in paper: [inferred] The study evaluated only intra-prompt stability (same prompt, multiple iterations). Inter-prompt stability is mentioned in Related Work as an alternative reliability assessment approach but was not tested.
- Why unresolved: No experiments were conducted using different prompt formulations for the same clinical transcript.
- What evidence would resolve it: Experiments varying prompts for identical transcripts and measuring consistency of generated outputs across prompt variations.

## Limitations

- The study used simulated clinical transcripts rather than real-world clinical encounters, which may not capture the full complexity of actual medical conversations.
- BERTScore-based evaluation, while more appropriate than string matching, may still fail to detect clinically significant hallucinations in the generated notes.
- The study did not include human expert validation to verify the quality and clinical accuracy of the generated notes.

## Confidence

**High Confidence**: The finding that open-weight models can achieve perfect semantic consistency (100%) under deterministic sampling conditions is strongly supported by the experimental setup and metrics.

**Medium Confidence**: The claim that open-weight models outperform proprietary models for clinical note generation is supported by the current evaluation on the benchmark dataset, but requires validation on real clinical transcripts to confirm generalizability.

**Medium Confidence**: The recommendation to deploy open-weight models locally for data privacy compliance is sound from a theoretical perspective, but practical implementation considerations are not addressed in the study.

## Next Checks

1. **Real Clinical Transcript Validation**: Test the top-performing models (Llama 3.1-70B, Mistral Small) on a dataset of real, de-identified clinical transcripts rather than simulated data to verify if the reliability rankings hold under actual clinical conditions with natural conversational complexity.

2. **Ground Truth Accuracy Verification**: Implement a secondary evaluation layer where domain experts review model outputs to verify that high semantic consistency does not mask systematic errors or hallucinations, particularly in critical medical information such as medication dosages and diagnoses.

3. **Prompt Sensitivity Analysis**: Conduct a systematic evaluation of how minor variations in the user prompt template affect the consistency and correctness metrics across all models, measuring the brittleness of each model's performance to prompt engineering variations that might occur in real-world deployment.