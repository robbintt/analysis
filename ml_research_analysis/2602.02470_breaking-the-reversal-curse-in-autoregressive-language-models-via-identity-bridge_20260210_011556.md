---
ver: rpa2
title: Breaking the Reversal Curse in Autoregressive Language Models via Identity
  Bridge
arxiv_id: '2602.02470'
source_url: https://arxiv.org/abs/2602.02470
tags:
- reversal
- training
- identity
- test
- curse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the \"reversal curse\" problem in autoregressive\
  \ language models, where models fail to deduce reversed relations (e.g., if trained\
  \ on \"Alice's husband is Bob,\" they cannot answer \"Who is Bob's wife?\"). The\
  \ authors propose a simple data regularization method called the Identity Bridge,\
  \ which adds statements of the form \"A \u2192 A\" (e.g., \"The name of Alice is\
  \ Alice\") to training data."
---

# Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge

## Quick Facts
- arXiv ID: 2602.02470
- Source URL: https://arxiv.org/abs/2602.02470
- Authors: Xutao Ma; Yixiao Huang; Hanlin Zhu; Somayeh Sojoudi
- Reference count: 40
- Primary result: 40% success rate on real-world reversal tasks using Identity Bridge data augmentation

## Executive Summary
This paper addresses the "reversal curse" in autoregressive language models, where models fail to deduce reversed relations (e.g., if trained on "Alice's husband is Bob," they cannot answer "Who is Bob's wife?"). The authors propose a simple data regularization method called the Identity Bridge, which adds statements of the form "A â†’ A" (e.g., "The name of Alice is Alice") to training data. Theoretically, they prove that even a one-layer transformer can break the reversal curse using this method through analysis of gradient descent's implicit bias. Empirically, fine-tuning a 1B parameter pretrained model with Identity Bridge data achieves a 40% success rate on real-world reversal tasks, compared to near-zero success without this regularization.

## Method Summary
The Identity Bridge approach adds synthetic training examples of the form "The [relation] of [entity] is [entity]" (e.g., "The name of Alice is Alice") to the training corpus. This creates a bridge that helps the model learn bidirectional associations between entities. The theoretical analysis shows that gradient descent with Identity Bridge data induces a bias that makes the model capable of deducing reversed relations. The method works by transforming the reversal task into an out-of-context reasoning problem, where the model learns to map from one entity to another through the identity bridge, then apply the reversed relation.

## Key Results
- Fine-tuning a 1B parameter model with Identity Bridge data achieves 40% success rate on real-world reversal tasks
- Without Identity Bridge regularization, models achieve near-zero success on reversal tasks
- The method works with a one-layer transformer, demonstrating that architectural complexity is not required
- Theoretical proof shows gradient descent's implicit bias with Identity Bridge data can break the reversal curse

## Why This Works (Mechanism)
The Identity Bridge method works by creating explicit self-referential mappings in the training data that help the model learn entity associations bidirectionally. When the model sees "The name of Alice is Alice," it learns to associate the entity "Alice" with itself in a way that generalizes to other relations. During inference, when asked about a reversed relation (e.g., "Who is Bob's wife?"), the model can use the learned identity bridge pattern to map from one entity to another and apply the reversed relation. The theoretical analysis shows that gradient descent with Identity Bridge data creates an implicit bias toward learning these bidirectional mappings, even in simple one-layer transformers.

## Foundational Learning
- Reversal Curse: Language models struggle with deducing reversed relations (e.g., if "A is B's X" is in training data, the model fails to answer "Who is B's X?"). This is a fundamental limitation of autoregressive models.
- Identity Bridge: A data augmentation technique that adds statements of the form "The [relation] of [entity] is [entity]" to training data, creating self-referential mappings.
- Implicit Bias of Gradient Descent: The tendency of gradient descent optimization to converge to certain types of solutions based on the initialization and data distribution, which the paper leverages to learn bidirectional mappings.

Why needed: Understanding these concepts is crucial for grasping how the Identity Bridge method addresses the reversal curse through data augmentation rather than architectural changes.
Quick check: Can you explain why a model trained on "Alice's husband is Bob" cannot answer "Who is Bob's wife?" without Identity Bridge data?

## Architecture Onboarding
Component map: Training data -> Identity Bridge augmentation -> One-layer Transformer -> Reversed relation inference
Critical path: The Identity Bridge augmentation is the critical component that enables the model to learn bidirectional entity associations.
Design tradeoffs: The method trades increased training data size (due to Identity Bridge additions) for improved reversal task performance without requiring architectural modifications.
Failure signatures: Without Identity Bridge, models show near-zero success on reversed relations; with it, success rates improve to 40% but still leave room for improvement.
First experiments: 1) Test Identity Bridge on a one-layer transformer with synthetic data, 2) Evaluate performance on symmetric vs. asymmetric relations, 3) Measure the effect of Identity Bridge data quantity on reversal task success.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several emerge from the work:
- How does the Identity Bridge method generalize to different relation types and languages?
- What is the optimal amount of Identity Bridge data needed for maximum effectiveness?
- Can the Identity Bridge concept be extended to multi-hop reasoning tasks?

## Limitations
- The 40% success rate, while significantly better than near-zero, still indicates substantial failure modes
- The method's effectiveness on diverse relation types and languages has not been established
- The theoretical analysis relies on idealized assumptions about gradient descent dynamics

## Confidence
- Theoretical proof of reversal curse breaking: Medium - The analysis is mathematically sound but depends on idealized assumptions about gradient descent and model capacity
- Empirical effectiveness of Identity Bridge: Medium - Results show clear improvement but are limited to a specific model size and task set
- Generalizability to other relation types: Low - The paper does not thoroughly explore performance across diverse relation patterns

## Next Checks
1. Test the Identity Bridge method on multiple model sizes (from 100M to 10B parameters) to establish scaling behavior and identify any threshold effects
2. Evaluate performance on reversed relations with varying complexity (symmetric, asymmetric, transitive) and across multiple languages to assess generalizability
3. Compare Identity Bridge against alternative simple data augmentation methods (e.g., symmetric relation duplication, negative sampling) to establish its relative effectiveness and identify potential synergies