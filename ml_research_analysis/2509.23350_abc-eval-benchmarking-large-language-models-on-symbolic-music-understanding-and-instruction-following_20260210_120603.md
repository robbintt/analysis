---
ver: rpa2
title: 'ABC-Eval: Benchmarking Large Language Models on Symbolic Music Understanding
  and Instruction Following'
arxiv_id: '2509.23350'
source_url: https://arxiv.org/abs/2509.23350
tags:
- music
- tasks
- symbolic
- understanding
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ABC-Eval is the first open-source benchmark designed to evaluate
  large language models' understanding and instruction-following capabilities for
  text-based symbolic music in ABC notation. It includes 1,086 test samples across
  10 sub-tasks spanning basic syntax parsing to complex sequence-level reasoning.
---

# ABC-Eval: Benchmarking Large Language Models on Symbolic Music Understanding and Instruction Following

## Quick Facts
- arXiv ID: 2509.23350
- Source URL: https://arxiv.org/abs/2509.23350
- Reference count: 0
- ABC-Eval is the first open-source benchmark for evaluating LLMs on text-based symbolic music understanding and instruction following

## Executive Summary
ABC-Eval is a pioneering open-source benchmark designed to evaluate large language models' capabilities in understanding and processing symbolic music represented in ABC notation. The benchmark comprises 1,086 test samples across 10 sub-tasks ranging from basic syntax parsing to complex sequence-level reasoning. When tested with seven state-of-the-art LLMs, the benchmark revealed significant limitations in symbolic music processing, with top-performing models achieving only 40-55% overall accuracy. While basic syntax tasks were handled well (>90% accuracy by some models), more complex segment-level and sequence-level tasks proved challenging, with emotion/composer/genre recognition tasks performing close to random guessing.

## Method Summary
ABC-Eval systematically evaluates LLMs across three task categories: basic syntax tasks (note/pitch/measure/bar parsing), segment-level tasks (melody/chord identification), and sequence-level tasks (emotion/composer/genre recognition). The benchmark uses ABC notation as its symbolic music representation format and employs prompt-based evaluation with both zero-shot and few-shot settings. Seven state-of-the-art LLMs were tested, including GPT-4o, Claude-3.5-Sonnet, and Qwen2.5-1.5B. The evaluation methodology includes diverse sampling strategies for generating test cases and human annotation for answer validation.

## Key Results
- Top-performing models achieved only 40-55% overall accuracy on ABC-Eval
- Basic syntax tasks were handled well (>90% accuracy by some models)
- Complex tasks like emotion/composer/genre recognition performed close to random guessing (25%)

## Why This Works (Mechanism)
ABC-Eval works by systematically breaking down symbolic music understanding into discrete, evaluable tasks that reveal specific capabilities and limitations of LLMs. The benchmark's structure allows for precise identification of where models succeed and fail, providing clear diagnostic information about symbolic music processing abilities. The use of ABC notation as a standardized, text-based format enables consistent evaluation across different models and avoids the complexity of audio processing.

## Foundational Learning
- ABC notation fundamentals: why needed - provides the symbolic music representation format; quick check - can the model correctly parse basic ABC syntax
- Symbolic music structure: why needed - understanding how musical elements map to textual representations; quick check - can the model identify measures, bars, and notes from ABC input
- Music theory basics: why needed - necessary for tasks involving melody, harmony, and rhythm; quick check - can the model recognize chord progressions and rhythmic patterns
- Prompt engineering techniques: why needed - critical for effective model evaluation; quick check - can the model follow instructions consistently across different prompt formats
- Evaluation metrics for symbolic music: why needed - ensures meaningful assessment of model performance; quick check - can the model achieve above-random performance on basic syntax tasks

## Architecture Onboarding
**Component Map:** Input ABC notation -> Prompt engineering -> LLM processing -> Output evaluation -> Performance metrics
**Critical Path:** ABC input generation → prompt formulation → model inference → answer validation → accuracy calculation
**Design Tradeoffs:** Standardized ABC format enables consistent evaluation but may not capture all musical nuances; synthetic data ensures controlled testing but may lack real-world complexity
**Failure Signatures:** Random guessing on complex tasks indicates lack of genuine understanding; high variance across similar tasks suggests inconsistent reasoning capabilities
**First 3 Experiments:** 1) Test basic syntax parsing across all models to establish baseline capabilities; 2) Evaluate segment-level tasks to identify reasoning limitations; 3) Assess sequence-level tasks to measure complex pattern recognition

## Open Questions the Paper Calls Out
The paper calls out several important open questions that need addressing:

**Unknown: Model Architecture Impact**
What specific architectural features of LLMs (transformer depth, attention mechanisms, etc.) contribute most significantly to their symbolic music understanding capabilities? This question remains unanswered as the benchmark doesn't systematically vary model architectures.

**Assumption: Transfer Learning Potential**
How well can models trained on symbolic music tasks transfer to other music-related domains like audio processing or performance? The benchmark focuses exclusively on ABC notation, leaving this transfer capability unexplored.

**Unknown: Human-Machine Performance Gap**
What is the true performance gap between LLMs and human experts on these symbolic music tasks? The benchmark provides human-annotated validation but doesn't directly compare human versus machine performance.

**Assumption: Task Difficulty Calibration**
Are the difficulty levels of different task categories appropriately calibrated? The significant performance drop from basic to complex tasks suggests possible calibration issues, but this needs systematic validation.

## Limitations
- Benchmark focuses exclusively on ABC notation, limiting generalizability to other symbolic music formats
- Evaluation methodology depends on specific prompt engineering strategies that may not generalize
- Dataset relies on synthetic and human-annotated data that may not capture real-world musical diversity

## Confidence
- High confidence in the overall finding that current LLMs show significant limitations in symbolic music processing
- Medium confidence in the specific accuracy percentages reported
- Low confidence in the interpretation of the "substantial gap" between theoretical knowledge and practical reasoning

## Next Checks
1. Test the benchmark with additional symbolic music formats (MIDI, MusicXML) to assess format-specific versus general symbolic music understanding capabilities
2. Conduct ablation studies on prompt engineering to determine the sensitivity of results to specific prompting strategies
3. Validate findings with additional, independently developed datasets of symbolic music to assess generalizability across different data sources