---
ver: rpa2
title: 'BUILDA: A Thermal Building Data Generation Framework for Transfer Learning'
arxiv_id: '2508.12703'
source_url: https://arxiv.org/abs/2508.12703
tags:
- uni00000044
- uni00000055
- building
- uni00000013
- uni00000046
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces BuilDa, a framework for generating synthetic
  thermal building data suitable for transfer learning research. It addresses the
  challenge of acquiring high-variance, high-quality building thermal data needed
  to explore transfer learning scenarios, such as selecting optimal source models.
---

# BUILDA: A Thermal Building Data Generation Framework for Transfer Learning

## Quick Facts
- arXiv ID: 2508.12703
- Source URL: https://arxiv.org/abs/2508.12703
- Reference count: 40
- Primary result: Transfer learning models trained on BuilDa synthetic data achieved 0.05-0.16 RMSE vs 1.03 from scratch for thermal forecasting

## Executive Summary
BuilDa is a framework for generating synthetic thermal building data suitable for transfer learning research. It addresses the challenge of acquiring high-variance, high-quality building thermal data needed to explore transfer learning scenarios, such as selecting optimal source models. BuilDa uses a validated single-zone Modelica building model exported as an FMU and simulates it via Python, allowing flexible parameter variation without requiring deep building simulation expertise. Users can adjust building characteristics, weather, occupancy, and control settings through a configuration file. The framework supports parallelized data generation and outputs customizable multivariate time-series data.

## Method Summary
BuilDa uses a validated single-zone Modelica building model exported as an FMU, simulated via FMPy in Python. Users configure building parameters (U-value, heat capacity, floor area), weather, and control settings through a YAML-like configuration file. A converter layer maps simplified user parameters to dependent FMU parameters, handling dependencies automatically. The framework supports parallel execution for generating multiple parameter variations. Data is output as multivariate time-series (zone temperature, weather, control signals) at user-specified intervals. For validation, 27 source buildings with varying thermal parameters were generated, along with a target building. LSTM models were pretrained on each source and fine-tuned to the target, demonstrating transfer learning effectiveness.

## Key Results
- Transfer learning models achieved 0.05-0.16 RMSE for 4-step ahead temperature prediction vs 1.03 from scratch
- U-value parameter dominated heat capacity and floor area in determining transfer learning effectiveness
- Parallelized data generation achieved 7.5 seconds per simulation for 100 variations

## Why This Works (Mechanism)

### Mechanism 1
Physics-based simulation via Modelica-to-FMU preserves thermal dynamics fidelity, enabling generated data to serve as effective TL pretraining material. The base model uses R-C element discretization (3 nodes per component) following VDI6007 Part 1 standards, capturing conductive, convective, and radiative heat transfer. This yields physically plausible time-series that encode realistic thermal inertia relationships, which neural networks can learn as transferable representations. Validated single-zone models generalize sufficiently to support TL research, even if multi-zone dynamics differ.

### Mechanism 2
Converter layer abstraction enables parameter-space exploration without simulation expertise, directly enabling large-scale dataset generation. The converter layer maps simplified user-facing parameters (e.g., floor dimensions) to dependent FMU parameters (e.g., volume, wall area), reducing configuration complexity from O(n²) parameter dependencies to O(n) user inputs. Parallelized FMU execution then scales data generation linearly with compute cores. Users primarily need to vary high-level parameters; fine-grained Modelica-level control is unnecessary for TL research questions.

### Mechanism 3
Source-target parameter similarity correlates with TL effectiveness, with U-value dominating heat capacity and floor area in the demonstrated regime. LSTM models pretrained on source buildings learn implicit representations of thermal response characteristics. When source U-values (insulation quality) approach target values, the learned temperature-response dynamics require less adjustment during fine-tuning, yielding lower RMSE. The observed U-value dominance generalizes beyond the tested parameter ranges and weather conditions.

## Foundational Learning

- **Concept: Functional Mock-up Interface (FMI/FMU)**: Why needed here: BuilDa's entire architecture depends on understanding that an FMU is a compiled, tool-agnostic simulation unit exported from Modelica that Python can execute via FMPy. Quick check question: Can you explain why the FMU approach avoids requiring users to install Modelica tooling?

- **Concept: Transfer Learning (Pretrain-Finetune Paradigm)**: Why needed here: The framework's value proposition assumes you understand that pretraining on source data learns generalizable features, and fine-tuning adapts them with limited target data. Quick check question: Given a model pretrained on 27 buildings, why does fine-tuning with only 30 days of target data outperform training from scratch on those same 30 days?

- **Concept: R-C Thermal Modeling (Resistance-Capacitance Networks)**: Why needed here: The building model's physical fidelity comes from R-C element discretization; understanding this helps interpret why certain parameters (U-value = 1/R, heat capacity = C) dominate TL outcomes. Quick check question: If you increase wall heat capacity while keeping U-value constant, would you expect faster or slower temperature response to heating signals?

## Architecture Onboarding

- **Component map**: Configuration File (YAML-like) → Converter Layer (parameter expansion, dependency resolution) → FMPy Python Interface → FMU (Modelica Buildings Library) → Parallel Simulation Executor → CSV Output (multivariate time-series: zone_temp, weather, control signals)

- **Critical path**: Configuration → Converter → FMU execution. The converter layer is the hardest to modify; it's coupled to the specific FMU's parameter schema. If you swap the FMU, you must adapt this layer first.

- **Design tradeoffs**:
  - Internal vs. external controller: Internal P-controller is 4x faster (7.5s vs 29s per sim) but less flexible for custom control logic
  - Single-zone vs. multi-zone: Current model limits realism but maximizes simulation speed and parameter interpretability
  - Fixed FMU vs. recompilable Modelica: Trading customization for usability; users cannot modify physics without Modelica expertise

- **Failure signatures**:
  - Simulations hang: Likely external controller update frequency too high; FMU recalculates on each control signal
  - RMSE doesn't improve over scratch: Source-target parameter distance too large; check U-value ranges overlap with target
  - Parameter variation silently ignored: Converter layer may be overriding user input with calculated dependencies; verify in output metadata

- **First 3 experiments**:
  1. **Baseline replication**: Generate 27 source buildings per Section 4.2 parameters, train LSTMs, verify RMSE distribution (0.05-0.16) matches paper before extending
  2. **Climate sensitivity**: Keep building parameters fixed, vary weather files (e.g., Prague, London, Zurich per Figure 4), measure TL performance degradation across climate zones
  3. **Parameter sweep for your target**: Define your own target building, generate a 3³ factorial design around it, identify which parameter dimension yields steepest RMSE gradient—this reveals which source characteristics matter most for your use case

## Open Questions the Paper Calls Out

- **Which specific building parameters and value combinations are most critical for selecting the optimal source model for transfer learning?**: The authors state in Section 4.2 that "It is unclear which parameters are most important to consider for selecting the right source... and which parameter value combinations lead to the best performance after TL." The demonstration showed U-value dominance, but the interaction effects with heat capacity and floor area were complex, preventing the formation of general selection rules. A comprehensive sensitivity analysis using BuilDa to map specific parameter variations against transfer learning performance gains across diverse target buildings would resolve this.

- **Can the framework be effectively extended to simulate multi-zone buildings and diverse building types while maintaining its ease of use?**: Section 5 notes the plan to "provide other base models representing different building types and want to incorporate multi-zone buildings." The current validated model assumes a single zone with uniform indoor air temperature, limiting the framework's applicability to more complex residential or commercial structures. The integration of a multi-zone FMU into the BuilDa framework that supports the same configuration-file-based parameter variation without requiring expert simulation knowledge would resolve this.

- **How effective are generalized Transformer architectures pre-trained on BuilDa data for forecasting building thermal dynamics compared to standard models like LSTMs?**: The authors suggest "pretraining of generalized Transformer architectures" as a future research application enabled by the framework in Section 5. The paper only validated the synthetic data's utility using a standard LSTM model, leaving the potential performance of more advanced, generalized architectures untested. Benchmarks demonstrating that Transformers pre-trained on BuilDa data outperform LSTMs and scratch-trained models on downstream tasks like Model Predictive Control would resolve this.

## Limitations

- The framework's effectiveness is demonstrated only for single-zone heating scenarios in one climate zone; performance in multi-zone buildings, cooling-dominated climates, or with complex occupancy patterns remains untested.
- Parameter importance conclusions (U-value dominance) are based on a limited 3³ factorial design and may not generalize to different weather patterns or control strategies.
- The converter layer abstraction, while simplifying usage, creates a rigid dependency on the specific FMU parameter schema, making adaptation to different building physics models non-trivial.

## Confidence

- **High**: The FMU-based simulation approach preserves thermal dynamics fidelity; parallel execution achieves linear scaling; generated data is suitable for transfer learning pretraining.
- **Medium**: Parameter similarity correlates with TL effectiveness in the tested regime; U-value dominates heat capacity and floor area for the Munich heating scenario.
- **Low**: The converter layer abstraction significantly reduces user expertise requirements; BuilDa's usability advantage over Synconn_build/eplusr generalizes to other simulation tasks.

## Next Checks

1. **Climate sensitivity test**: Generate data using the same building parameters but different weather files (e.g., Prague, London, Zurich) to verify whether parameter importance ordering holds across climate zones.

2. **Multi-zone extension validation**: Modify the base Modelica model to include inter-zone heat transfer, regenerate source data, and measure TL performance degradation to quantify single-zone assumption limits.

3. **Controller flexibility test**: Replace the internal P-controller with an external controller that implements setback schedules or occupancy-based controls, measure simulation speed impact, and evaluate TL model performance with more realistic control signals.