---
ver: rpa2
title: 'Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking,
  and System Design'
arxiv_id: '2511.17127'
source_url: https://arxiv.org/abs/2511.17127
tags:
- training
- attention
- each
- which
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first large-scale pretraining study on
  a pure AMD hardware stack, demonstrating the maturity of MI300X GPUs and Pollara
  networking for frontier LLM development. The authors introduce ZAYA1-base, an 8.3B
  parameter MoE model with 760M active parameters, achieving competitive performance
  with much larger dense models like Qwen3-4B and Gemma3-12B while significantly outperforming
  models like Llama-3-8B and OLMoE.
---

# Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design

## Quick Facts
- arXiv ID: 2511.17127
- Source URL: https://arxiv.org/abs/2511.17127
- Reference count: 17
- First large-scale LLM pretraining study on pure AMD hardware stack with MI300X GPUs and Pollara networking

## Executive Summary
This paper presents the first large-scale pretraining study on a pure AMD hardware stack, demonstrating the maturity of MI300X GPUs and Pollara networking for frontier LLM development. The authors introduce ZAYA1-base, an 8.3B parameter MoE model with 760M active parameters, achieving competitive performance with much larger dense models like Qwen3-4B and Gemma3-12B while significantly outperforming models like Llama-3-8B and OLMoE. The work provides comprehensive networking benchmarks for Pollara across all major collectives at scale, MI300X-specific transformer sizing guidelines, and detailed characterization of memory bandwidth and collective communication performance.

## Method Summary
The study introduces ZAYA1-base, an 8.3B parameter MoE model with 760M active parameters, trained on a pure AMD hardware stack consisting of MI300X GPUs and Pollara networking. The architecture features novel innovations including Compressed Convolutional Attention (CCA) for efficient attention computation, an expressive MLP-based router with exponential depth averaging for expert specialization, and lightweight residual scaling for information flow control. The training stack includes custom kernels for Muon optimizer and LayerNorm, a fault-tolerance system (Aegis), and efficient checkpointing mechanisms. The model was pretrained on a dataset with a 5:1 ratio of code to non-code data and evaluated on standard benchmarks including MMLU, HumanEval, and BBH.

## Key Results
- ZAYA1-base achieves competitive performance with much larger dense models (Qwen3-4B, Gemma3-12B) while being significantly more efficient
- Comprehensive networking benchmarks demonstrate Pollara's strong performance across all major collectives at scale
- MI300X memory bandwidth and collective communication performance characterized across various batch sizes and sequence lengths
- Model shows strong general knowledge, mathematics, and coding capabilities while being particularly efficient for inference on low-end consumer GPUs

## Why This Works (Mechanism)
The ZAYA1 architecture achieves its performance through three key innovations: Compressed Convolutional Attention (CCA) provides efficient attention computation by using depth-wise separable convolutions that learn hierarchical spatial relationships, the MLP-based router with exponential depth averaging enables effective expert specialization while maintaining stability, and lightweight residual scaling controls information flow without introducing significant computational overhead. The combination of these techniques with the AMD hardware stack's high memory bandwidth and efficient networking creates a system capable of training competitive models at scale.

## Foundational Learning
- **Mixture-of-Experts (MoE)**: Why needed - Enables sparse activation where only a subset of parameters are used per token, dramatically reducing compute and memory requirements. Quick check - Verify that the router selects different experts for different input types.
- **Collective Communication**: Why needed - Essential for synchronizing gradients and model parameters across multiple GPUs in distributed training. Quick check - Ensure all-reduce operations complete within expected time bounds for given batch sizes.
- **Memory Bandwidth Optimization**: Why needed - Critical for feeding data to compute units efficiently, especially for attention mechanisms that require large intermediate activations. Quick check - Monitor achieved bandwidth vs theoretical maximum during training.

## Architecture Onboarding

Component Map:
Preprocessing -> Data Loader -> Model Forward Pass -> CCA -> MLP Router -> Expert Layers -> Residual Scaling -> Loss Computation -> Backward Pass -> Optimizer -> Checkpointing

Critical Path:
The most time-consuming operations are the attention mechanism (handled by CCA), all-reduce operations for gradient synchronization across GPUs, and the router computation which determines expert selection. These operations must be carefully optimized to achieve good scaling efficiency.

Design Tradeoffs:
The authors chose MoE over dense architectures to achieve better parameter efficiency, accepting the complexity of routing and increased communication overhead. They opted for CCA over standard attention to reduce computational complexity, trading off some modeling capacity for efficiency. The lightweight residual scaling provides better control over information flow compared to standard scaling techniques while adding minimal overhead.

Failure Signatures:
Training instability often manifests as exploding or vanishing gradients, which can be detected through monitoring gradient norms. Router collapse (where only a few experts are selected) can be identified by examining expert usage statistics. Communication bottlenecks typically appear as reduced scaling efficiency when adding more GPUs.

First Experiments:
1. Verify that the CCA implementation matches theoretical computational complexity reduction compared to standard attention
2. Test the router's ability to maintain balanced expert usage across different input types and batch sizes
3. Measure scaling efficiency of all-reduce operations across different numbers of GPUs to identify optimal configurations

## Open Questions the Paper Calls Out
The authors acknowledge they haven't yet explored larger model sizes beyond the 8.3B parameter MoE, leaving uncertainty about scalability to truly frontier-scale models. They also note that the focus on single-node configurations for MI300X memory bandwidth analysis, while practical, doesn't fully characterize multi-node scaling behavior.

## Limitations
- Evaluation methodology limitations affect confidence in absolute performance claims due to potential differences in evaluation protocols and data preprocessing
- Lack of ablation studies on novel architectural components prevents quantification of individual contributions to performance
- Inference efficiency claims on low-end consumer GPUs lack empirical validation on actual hardware

## Confidence
- **High** - AMD hardware stack maturity for LLM pretraining (based on comprehensive benchmarking and successful training)
- **Medium** - Model architecture innovations (lacks ablation studies to quantify individual contributions)
- **High** - Comprehensive networking benchmarks (provides valuable reference data)
- **Medium** - Inference efficiency claims (based on parameter counts but lacks empirical validation)

## Next Checks
1. Conduct ablation studies on the three novel architectural components (CCA, MLP router with exponential depth averaging, and residual scaling) to quantify their individual contributions to model performance
2. Perform direct comparisons with other models using identical evaluation protocols and data preprocessing to ensure fair benchmarking
3. Validate the inference efficiency claims by testing the model on actual low-end consumer GPUs and measuring real-world latency and throughput metrics