---
ver: rpa2
title: 'DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning'
arxiv_id: '2506.14827'
source_url: https://arxiv.org/abs/2506.14827
tags:
- video
- ai-generated
- arxiv
- reasoning
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of explainable detection of\
  \ AI-generated videos, which current methods often treat as opaque binary classification.\
  \ To overcome this, the authors introduce DAVID-X, the first dataset providing fine-grained\
  \ defect-level annotations\u2014including defect categories, temporal-spatial localization,\
  \ and natural language explanations\u2014for AI-generated videos, along with corresponding\
  \ real videos."
---

# DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning

## Quick Facts
- arXiv ID: 2506.14827
- Source URL: https://arxiv.org/abs/2506.14827
- Reference count: 40
- Primary result: Introduces DAVID-X dataset with fine-grained defect annotations and DAVID-XR1 model achieving 76.7% out-of-domain detection accuracy

## Executive Summary
This paper addresses the challenge of explainable detection of AI-generated videos, which current methods often treat as opaque binary classification. The authors introduce DAVID-X, the first dataset providing fine-grained defect-level annotations—including defect categories, temporal-spatial localization, and natural language explanations—for AI-generated videos, along with corresponding real videos. Leveraging these annotations, they present DAVID-XR1, a vision-language model fine-tuned to produce interpretable chains of visual reasoning. DAVID-XR1 demonstrates strong generalization to unseen generators and improves backbone accuracy from 26.7% to 76.7% on out-of-domain detection.

## Method Summary
The authors developed DAVID-X, a comprehensive dataset featuring detailed annotations for AI-generated video defects, including defect categories, temporal-spatial localization, and natural language explanations paired with real video counterparts. Building on this foundation, they created DAVID-XR1, a vision-language model fine-tuned specifically for generating interpretable chains of visual reasoning about detected defects. The model leverages the annotated defect information to provide explainable detection results rather than simple binary classification outputs.

## Key Results
- DAVID-XR1 achieves 76.7% accuracy on out-of-domain detection, significantly improving from 26.7% baseline
- Manual evaluation shows explanation precision of 54.7%, indicating robust reasoning quality
- Strong generalization demonstrated across unseen AI video generators
- First dataset providing fine-grained defect-level annotations for AI-generated video detection

## Why This Works (Mechanism)
The approach works by combining detailed defect annotations with vision-language modeling to create interpretable reasoning chains. The DAVID-X dataset provides granular information about specific defects, their locations, and explanations, which the DAVID-XR1 model learns to reproduce and apply to new videos. This structured reasoning approach allows the model to not only detect AI-generated content but also explain why it made that determination, making the detection process transparent and trustworthy.

## Foundational Learning

**Vision-Language Models**: Combine visual understanding with natural language processing to generate explanations. Needed for bridging visual defect detection with interpretable reasoning. Quick check: Can the model correctly identify and describe defects in new videos?

**Fine-grained Defect Annotations**: Detailed labeling of specific artifact types, locations, and explanations in videos. Needed to train models on specific generative flaws rather than generic patterns. Quick check: Are annotations comprehensive enough to cover diverse generator artifacts?

**Explainable AI Detection**: Moving beyond binary classification to provide reasoning for detection decisions. Needed for building trust and enabling human oversight in AI detection systems. Quick check: Do explanations align with human judgment of video authenticity?

## Architecture Onboarding

**Component Map**: Raw Video -> Vision Encoder -> Reasoning Module -> Defect Localization -> Natural Language Explanation -> Detection Output

**Critical Path**: Video input flows through vision encoder to extract visual features, which are processed by the reasoning module that generates defect localization and explanations, culminating in the final detection output. The explanation generation is the critical component that distinguishes this approach from standard binary classifiers.

**Design Tradeoffs**: The approach trades computational efficiency for interpretability and generalization. While simpler binary classifiers might be faster, the reasoning chains provide transparency and better cross-domain performance at the cost of increased model complexity and inference time.

**Failure Signatures**: The model may struggle with high-quality AI-generated videos lacking obvious artifacts, produce incomplete or inaccurate explanations for subtle defects, or fail to generalize to novel generator architectures not represented in training data.

**First Experiments**:
1. Test DAVID-XR1 on videos from generators not included in the training dataset to verify generalization claims
2. Compare explanation quality between DAVID-XR1 and baseline binary classifiers on identical test sets
3. Evaluate detection accuracy on high-quality AI-generated videos with minimal visible artifacts

## Open Questions the Paper Calls Out
None

## Limitations
- Moderate explanation precision (54.7%) raises reliability concerns for high-stakes applications
- Dataset focus on defect-level annotations may limit effectiveness on videos without obvious artifacts
- Limited evaluation of generator diversity in out-of-domain testing raises questions about true generalization capability

## Confidence
- High confidence in the dataset contribution and its annotation framework
- Medium confidence in the generalization claims due to limited detail on generator diversity
- Medium confidence in the reasoning quality based on moderate manual evaluation scores
- Low confidence in the scalability of the approach to real-world deployment scenarios

## Next Checks
1. Conduct extensive testing across a broader range of AI generators, including those not represented in the training data, to verify true generalization capabilities
2. Perform large-scale human evaluation studies with domain experts to assess the practical utility and accuracy of the reasoning chains in real-world detection scenarios
3. Evaluate the system's performance on high-quality AI-generated videos with minimal artifacts to determine its effectiveness beyond obvious defect detection