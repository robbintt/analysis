---
ver: rpa2
title: 'Predicting Human Chess Moves: An AI Assisted Analysis of Chess Games Using
  Skill-group Specific n-gram Language Models'
arxiv_id: '2512.01880'
source_url: https://arxiv.org/abs/2512.01880
tags:
- move
- accuracy
- game
- chess
- moves
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of predicting human chess moves
  across different skill levels, rather than optimal computer moves. The core method
  treats chess as a language modeling task, using n-gram language models trained on
  move sequences from Lichess players grouped into seven rating-based skill levels.
---

# Predicting Human Chess Moves: An AI Assisted Analysis of Chess Games Using Skill-group Specific n-gram Language Models

## Quick Facts
- arXiv ID: 2512.01880
- Source URL: https://arxiv.org/abs/2512.01880
- Authors: Daren Zhong; Dingcheng Huang; Clayton Greenberg
- Reference count: 2
- Primary result: Skill-specific n-gram models improve Top-3 move prediction accuracy by up to 39.1% over global benchmark

## Executive Summary
This work treats chess move prediction as a language modeling task, training separate n-gram models on games from seven rating-based skill levels. The framework includes a model selector that classifies player skill using early-game move patterns, then constrains predictions to the selected skill-appropriate model. Experimental results show significant improvements in both skill classification (up to 31.7% accuracy with 16 half-moves) and move prediction accuracy (up to 39.1% better than global selection), demonstrating that human chess behavior exhibits distinct statistical patterns across skill levels that can be captured by n-gram language models.

## Method Summary
The framework trains seven 5-gram KenLM language models on games from seven rating bands (L1: ≤1000 to L7: ≥2250), then uses a model selector that computes cumulative surprisal over early moves to classify player skill level. For move prediction, it queries only the selected model's probability distribution rather than choosing the highest-probability move across all models. The approach treats chess as a sequential prediction problem where skill manifests in statistically distinct move patterns that can be captured within 4-move contexts, with early-game positions providing cleaner discrimination signals than middle-game complexity.

## Key Results
- Model selector achieves 31.7% accuracy in classifying skill levels using 16 half-moves (vs 14.3% random baseline)
- Selector Assisted Accuracy improves Top-3 move prediction by up to 39.1% over global benchmark
- Early-game classification (16 half-moves) outperforms longer windows for L1 and L7 players, but shows inverted trends for intermediate levels
- Perplexity heatmap shows lowest values on diagonal where model level matches game level, confirming skill-specific pattern capture

## Why This Works (Mechanism)

### Mechanism 1
Players at different skill levels exhibit statistically distinct move sequence patterns that can be captured by n-gram language models. The framework trains separate 5-gram language models on games from seven rating bands (L1: ≤1000 to L7: ≥2250). Each model learns probability distributions over move sequences characteristic of its skill level. When a model trained on level X evaluates games from level X, it produces lower perplexity than when evaluating other levels.

Core assumption: Skill level manifests in recurring local move patterns (within 4 moves of context) that differ across rating bands.

Evidence anchors:
- [abstract] "n-gram language models to capture move patterns characteristic of specific player skill levels"
- [section 5.1] Figure 3 heatmap shows lowest perplexities on diagonal where model level matches game level
- [corpus] Related work "A Behavior-Based Knowledge Representation Improves Prediction of Players' Moves in Chess by 25%" similarly uses behavioral patterns for move prediction

### Mechanism 2
Early-game move sequences provide sufficient signal to classify player skill level via surprisal comparison across models. The model selector computes cumulative surprisal (negative log probability) for each game under all seven models using the first N half-moves. The model yielding lowest total surprisal is selected as the predicted skill level.

Core assumption: Opening play reflects skill-correlated tendencies (e.g., novices play irregular moves; experts follow theory) that persist across games.

Evidence anchors:
- [abstract] "model selector module...can classify skill levels with an accuracy of up to 31.7% when utilizing early game information (16 half-moves)"
- [section 5.2] Table 2 shows 16 half-move accuracy exceeds 100 half-move accuracy for L1 (37.2% vs 22.3%) and L7 (43.3% vs 15.3%), supporting early-game discrimination

### Mechanism 3
Constraining prediction to a skill-appropriate model improves accuracy over globally selecting across all models. Rather than choosing the highest-probability move across all seven models (benchmark), the framework uses only the selected model's probability distribution. This prevents contamination from skill-mismatched patterns.

Core assumption: Skill-specific models encode priors that are more accurate for their target population than an uncalibrated ensemble.

Evidence anchors:
- [abstract] "Selector Assisted Accuracy being up to 39.1% more accurate than our benchmark accuracy"
- [section 5.4] Figure 6 and discussion show substantial Top-3 improvement over benchmark

## Foundational Learning

- **N-gram language models and perplexity**
  - Why needed here: The entire framework builds on estimating P(move_n | move_{n-4}...move_{n-1}) and comparing models via perplexity/surprisal.
  - Quick check: Given a sequence "e4 e6 d4 d5", what does a 3-gram model condition on when predicting the next move?

- **Surprisal as negative log-likelihood**
  - Why needed here: The model selector aggregates surprisal values; understanding why lower surprisal indicates better model fit is essential.
  - Quick check: If a model assigns probability 0.01 to an observed move, what is the surprisal? What if probability is 0.5?

- **Chess algebraic notation and PGN format**
  - Why needed here: The preprocessing pipeline extracts move tokens from PGN. Understanding what "e4", "Nf3", "O-O" represent clarifies what the model predicts.
  - Quick check: In Standard Algebraic Notation, does "Nxf7+" capture the piece, destination, capture, and check all in one token?

## Architecture Onboarding

- **Component map**: Raw PGN files → preprocessing script (metadata removal, evaluation stripping) → skill-binned corpora (7 training sets, 7 test sets) → KenLM `lmplz` trains 7 independent 5-gram LMs → `query` generates surprisal per move and perplexity per game → Model selector aggregates surprisal over N half-moves, selects argmin across 7 models → Move predictor queries selected model's probability distribution, returns Top-1 or Top-3 moves

- **Critical path**: 1) Obtain Lichess PGN data for July 2024 (training) and August 2024 (testing) 2) Assign games to L1-L7 by averaging WhiteRating and BlackRating 3) Strip PGN to space-separated move tokens only 4) Train 7 KenLM models (5-gram, Tuning for smoothing) 5) For each test game, compute surprisal under all 7 models for first N moves 6) Select model with lowest cumulative surprisal; query its Top-K predictions 7) Compare against actual moves; compute accuracy

- **Design tradeoffs**: N-gram vs neural LMs: authors chose KenLM for computational efficiency and low latency, but limited to 4-token history and no board state awareness. 16 vs 100 half-moves for selection: early-game selection yields better accuracy for L1/L7 but worse for L2-L5. Top-1 vs Top-3 evaluation: Top-1 shows modest improvement (~6.6%); Top-3 shows large improvement (~39.1%).

- **Failure signatures**: Illegal move predictions: n-gram models lack game state; may predict moves that are not currently legal. Middle-game accuracy drop: Figure 5-6 show minimum accuracy at ~50 half-moves. L1/L7 prediction failures: novices are erratic and experts deviate from common patterns.

- **First 3 experiments**:
  1. Reproduce perplexity heatmap: Train 7 KenLM models on described data splits; verify diagonal dominance in perplexity matrix
  2. Ablate selection window: Compare classification accuracy at 8, 16, 32, 50, 100 half-moves to identify optimal early-game cutoff per skill level
  3. Add legality filter: Post-process predictions through python-chess to filter illegal moves; measure impact on Top-1/Top-3 accuracy

## Open Questions the Paper Calls Out

### Open Question 1
Can capturing the sequential nature of moves enable the classification of player openings into distinct styles, such as aggressive or defensive?
Basis in paper: [explicit] Section 8 states future work could explore classifying openings or early-game strategies into distinct styles.
Why unresolved: The current n-gram implementation focuses on local probability rather than modeling the long-term strategic intent required to define stylistic archetypes.
What evidence would resolve it: A model extension that successfully clusters games by style labels (e.g., aggressive vs. defensive) with high predictive validity.

### Open Question 2
How can the model architecture be modified to capture long-term context beyond the current 4-token history?
Basis in paper: [explicit] Section 7 notes the models are unable to take into account more than 4 tokens of history, limiting the capture of long-term context.
Why unresolved: The use of 5-gram models imposes a hard limit on the historical window, preventing the learning of longer strategic dependencies.
What evidence would resolve it: Experiments using architectures with extended memory (e.g., RNNs or Transformers) showing improved accuracy on moves requiring long-range planning.

### Open Question 3
How can game state awareness be integrated into the language model to prevent the generation of illegal moves?
Basis in paper: [explicit] Section 7 identifies that the models are oblivious to the game state and unaware of the rules of chess, leading to illegal move predictions.
Why unresolved: The pure language modeling approach treats chess moves as text tokens without an internal representation of the board's legality constraints.
What evidence would resolve it: A hybrid system that integrates board state validation, reducing the rate of illegal predictions to zero without degrading move matching accuracy.

## Limitations

- **Unmodeled board state**: The n-gram models predict moves based solely on prior move sequences without encoding current piece positions, which can lead to predicting illegal moves or moves inconsistent with the actual game state.
- **Skill classification accuracy**: While 31.7% skill classification accuracy shows significant improvement over random baseline (14.3%), it remains imperfect and may lead to incorrect model selection for borderline skill levels.
- **Generalization across time**: The study uses games from July-August 2024, so performance on earlier or future data remains unknown as chess opening theory and player behavior may evolve.

## Confidence

- **High Confidence**: The framework design, data processing pipeline, and Top-3 prediction improvements are well-specified and reproducible.
- **Medium Confidence**: The claim of 39.1% improvement over benchmark accuracy is supported but depends on the specific benchmark choice.
- **Low Confidence**: The paper doesn't specify KenLM smoothing parameters, logarithm base, or random seed for data sampling, making exact reproduction difficult.

## Next Checks

1. **Reproduce perplexity heatmap**: Train the 7 KenLM models on the described data splits and verify that perplexity is lowest on the diagonal (model i evaluated on test set i), confirming the skill-specific pattern hypothesis.

2. **Ablate selection window**: Compare classification accuracy at multiple half-move cutoffs (8, 16, 32, 50, 100) to identify optimal early-game window sizes per skill level, and test whether adaptive window selection improves overall performance.

3. **Add legality filter**: Post-process predictions through a chess legality checker (e.g., python-chess) to filter illegal moves, then measure the impact on Top-1/Top-3 accuracy and report the rate of illegal predictions before filtering.