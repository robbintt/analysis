---
ver: rpa2
title: 'RFG: Test-Time Scaling for Diffusion Large Language Model Reasoning with Reward-Free
  Guidance'
arxiv_id: '2509.25604'
source_url: https://arxiv.org/abs/2509.25604
tags:
- diffusion
- arxiv
- language
- reasoning
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Reward-Free Guidance (RFG), a method for guiding
  diffusion language model (dLLM) reasoning without requiring explicit process reward
  models. The key innovation is parameterizing the reward as the log-likelihood ratio
  of a policy and reference dLLM, allowing step-wise guidance to be derived without
  additional training.
---

# RFG: Test-Time Scaling for Diffusion Large Language Model Reasoning with Reward-Free Guidance

## Quick Facts
- arXiv ID: 2509.25604
- Source URL: https://arxiv.org/abs/2509.25604
- Reference count: 40
- Primary result: RFG achieves up to 9.2% accuracy gains over baselines in zero-shot reasoning tasks.

## Executive Summary
RFG introduces a training-free method for guiding diffusion language model reasoning by parameterizing rewards as log-likelihood ratios between policy and reference models. This enables step-wise guidance without explicit reward models or additional training. The method shows consistent improvements across mathematical reasoning and code generation benchmarks using diverse dLLM families.

## Method Summary
The method computes guided logits at each denoising step by combining policy and reference model outputs: log π_RFG = (1+w) log π_θ − w log π_ref. This reward-free approach leverages the log-likelihood ratio as a proxy for reasoning quality, eliminating the need for process reward models. The framework is model-agnostic and works with both block diffusion (LLaDA) and standard diffusion (Dream) architectures, requiring only access to paired base/reference and policy checkpoints.

## Key Results
- Up to 9.2% accuracy gains over naive ensemble baselines on GSM8K and MATH-500
- Consistent improvements across all tested benchmarks (GSM8K, MATH-500, HumanEval, MBPP)
- Performance gains observed across multiple dLLM families (LLaDA, Dream) and post-training methods
- Robust performance across a wide range of guidance strengths w

## Why This Works (Mechanism)
The reward-free guidance works by exploiting the log-likelihood ratio between policy and reference models as a proxy for reasoning quality. This creates a reward-reweighted target distribution that guides the denoising process toward better reasoning trajectories without requiring explicit reward modeling or additional training.

## Foundational Learning
1. Diffusion Language Models (dLLMs): Generative models that denoise text through iterative refinement steps
   - Why needed: RFG operates specifically on the denoising dynamics of dLLMs
   - Quick check: Verify understanding of forward/backward diffusion processes

2. Log-likelihood ratio parameterization: Using the difference between log probabilities as a reward signal
   - Why needed: Forms the theoretical foundation for reward-free guidance
   - Quick check: Confirm understanding of how log π_θ - log π_ref approximates a reward function

3. Block diffusion vs standard diffusion: Different masking/unmasking schedules
   - Why needed: RFG must integrate with model-specific denoising procedures
   - Quick check: Distinguish between LLaDA's block diffusion and Dream's nucleus sampling approach

## Architecture Onboarding

Component map: Policy dLLM <- Logit combination -> Reference dLLM -> Guided denoising

Critical path: Load both models → Compute logits at each step → Combine using w → Apply to masking schedule → Generate output

Design tradeoffs: Training-free vs potential performance ceiling; Model-agnostic vs need for paired checkpoints; Simple implementation vs hyperparameter sensitivity

Failure signatures: Performance degradation vs ensemble baseline; Inconsistent improvements across w values; Logit misalignment due to tokenization mismatches

First experiments:
1. Implement RFG logit combination and verify it produces valid probability distributions
2. Test on single-step denoising with synthetic data to confirm reward direction
3. Run ablation comparing different w values on a small benchmark subset

## Open Questions the Paper Calls Out
1. Can RFG be extended to multimodal diffusion models or agentic reasoning systems? (explicitly called out in conclusion)

2. How does reference model choice impact guidance stability and effectiveness? (inferred from method's reliance on reference model)

3. Can optimal guidance strength w be determined adaptively during decoding? (inferred from discussion of wide plateau and over-optimization)

## Limitations
- Requires access to both policy and reference model checkpoints
- Guidance strength w needs manual tuning for optimal performance
- Performance depends on quality of the policy-reference pairing
- Limited evaluation to text-based reasoning and code generation tasks

## Confidence
- Theoretical foundation: High
- Empirical robustness: High
- Exact numerical replication: Medium
- Hyperparameter specification: Low

## Next Checks
1. Sweep guidance strength w across a wide range (-2 to +2) for at least one model-benchmark pair to confirm the existence of a robust performance plateau and identify optimal values.

2. Verify the exact unmask/denoising schedule implementation by comparing outputs to baseline behavior, particularly for block diffusion and low-confidence remasking.

3. Run ablations comparing RFG against both the naive ensemble and the original policy model using matched compute and generation lengths to confirm reported accuracy gains are reproducible.