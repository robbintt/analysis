---
ver: rpa2
title: Improving the Inclusivity of Dutch Speech Recognition by Fine-tuning Whisper
  on the JASMIN-CGN Corpus
arxiv_id: '2502.17284'
source_url: https://arxiv.org/abs/2502.17284
tags:
- speech
- whisper
- recognition
- non-native
- children
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated fine-tuned Whisper models for Dutch ASR across
  diverse speaker groups (children, elderly, non-native) using the JASMIN-CGN corpus.
  The primary goal was to assess how age and linguistic background influence ASR performance.
---

# Improving the Inclusivity of Dutch Speech Recognition by Fine-tuning Whisper on the JASMIN-CGN Corpus

## Quick Facts
- arXiv ID: 2502.17284
- Source URL: https://arxiv.org/abs/2502.17284
- Reference count: 7
- Primary result: Fine-tuning Whisper on subpopulation-specific Dutch speech data achieves 65-81% relative WER reduction across children, elderly, and non-native speakers

## Executive Summary
This study demonstrates that fine-tuning the Whisper large-v2 ASR model on the JASMIN-CGN corpus significantly improves Dutch speech recognition accuracy for underrepresented speaker groups. The research addresses a critical gap in ASR inclusivity by focusing on children, elderly, and non-native speakers, who typically experience poor recognition performance with zero-shot models. Through 10-fold cross-validation experiments, the authors show that fine-tuning on demographic-specific data yields substantial WER improvements (65-81% relative reduction), with best results achieved by training on the comprehensive dataset containing all four speaker groups. The findings suggest that ASR models can be made substantially more inclusive through targeted adaptation to diverse speech patterns.

## Method Summary
The study fine-tunes Whisper large-v2 on the JASMIN-CGN corpus, which contains approximately 60 hours of Dutch speech from 300 speakers across four demographic groups: native children, non-native children, non-native adults, and native elderly. The corpus includes both read speech and interactive conversations. The authors perform five fine-tuning experiments: four group-specific (one per demographic) and one combined model trained on all groups. Training uses 10-fold cross-validation with 80/10/10 train/validation/test splits, learning rate 3×10⁻⁵ for 5 epochs, and checkpoint selection based on lowest validation WER. All data cleaning involved removing special annotation codes and converting transcriptions from Latin-1 to UTF-8.

## Key Results
- Zero-shot Whisper performance was poor across all groups (WER: 26.12-42.07%)
- Fine-tuning achieved 81% relative WER reduction for native children (WER: 5.45%)
- Fine-tuning achieved 72% relative WER reduction for non-native children (WER: 10.90%)
- Fine-tuning achieved 67% relative WER reduction for non-native adults (WER: 13.95%)
- Fine-tuning achieved 65% relative WER reduction for native elderly (WER: 9.96%)
- Combined fine-tuning on all groups performed at least as well as group-specific fine-tuning for each group

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a large pre-trained ASR model on subpopulation-specific data yields substantial accuracy improvements for that subpopulation. The Whisper large-v2 model has latent representational capacity for diverse speech patterns, and fine-tuning adjusts the encoder-decoder weights to better align acoustic features with transcription targets for underrepresented speaker characteristics.

### Mechanism 2
Fine-tuning on one demographic group moderately improves recognition for other groups, particularly when training data shares structural properties like spontaneous speech and short sentences. Cross-group transfer occurs through shared corpus characteristics rather than demographic similarity.

### Mechanism 3
Training on a comprehensive dataset with all subpopulations yields equal or better per-group performance than group-specific training alone. A unified training set exposes the model to greater acoustic variability, encouraging learning of robust features that generalize within and across groups.

## Foundational Learning

- **Concept: Word Error Rate (WER)**
  - **Why needed here:** Primary evaluation metric; understanding substitution/deletion/insertion trade-offs is essential for interpreting the 65-81% relative improvements.
  - **Quick check question:** If a model transcribes "the cat sat" as "the cat sat down," what is the WER?

- **Concept: k-Fold Cross-Validation**
  - **Why needed here:** Study uses k=10 cross-validation to estimate performance variance on limited data (~12 hours per group); essential for understanding the reported standard deviations.
  - **Quick check question:** Why use k-fold rather than a single train/test split when dataset size is limited?

- **Concept: Mel Spectrogram Representation**
  - **Why needed here:** Whisper's input representation; understanding how audio is transformed helps diagnose preprocessing failures.
  - **Quick check question:** What does a Mel spectrogram represent dimensionally (time × frequency vs. raw waveform)?

## Architecture Onboarding

- **Component map:** Audio → Mel spectrogram → Encoder transformer blocks → Decoder transformer blocks → Token sequence → Text transcription
- **Critical path:** 1. Segment audio >30s using timestamp alignment from .ort files, 2. Convert Latin-1 → UTF-8 transcriptions, 3. Clean transcriptions (remove *v, *n, *a prefixes), 4. Fine-tune: 5 epochs, LR 3×10⁻⁵, checkpoint every 0.1 epoch, 5. Select checkpoint by lowest validation WER, 6. Evaluate on held-out fold
- **Design tradeoffs:** Full fine-tuning vs. LoRA (this study uses full fine-tuning); group-specific vs. combined training (combined is simpler and performs equally well); learning rate 3×10⁻⁵ follows Whisper authors' guidance
- **Failure signatures:** Hallucinated Unicode characters (U+FFFD) when language specification missing; high variance across folds (std >2%) indicating data sparsity; zero-shot WER >40% expected for non-native speech
- **First 3 experiments:** 1. Reproduce zero-shot baseline: Run Whisper large-v2 on JASMIN-CGN test sets without fine-tuning, 2. Single-group fine-tuning: Fine-tune on native children data only (12h), evaluate on all four test sets, 3. Combined fine-tuning: Merge all four groups, fine-tune once, evaluate per-group

## Open Questions the Paper Calls Out

### Open Question 1
Does fine-tuning on age-varied speech yield better generalization than fine-tuning on nativeness-varied speech? The authors observe indications that age variation improves generalizability more than nativeness variation, but this was a post-hoc interpretation rather than a controlled variable in the experimental design.

### Open Question 2
Would a pairwise fold-merging strategy eliminate the bias toward native children found in the unified model? The authors propose merging folds pairwise (100 experiments) as a more effective approach but lacked time to execute this combinatorial training strategy.

### Open Question 3
Does retaining specific annotations for mispronunciations and disfluencies improve the recognition of spontaneous speech? The authors removed special codes during data cleaning to ensure compatibility, but it's unclear if this strips the model of necessary signal to handle real-life speech features.

## Limitations

- **Data size constraints**: Study relies on k-fold cross-validation due to limited data (~12 hours per group), introducing variance in performance estimates
- **Transfer mechanism ambiguity**: Paper attributes cross-group improvements to corpus-level effects rather than demographic similarity, but relative contributions are not quantified
- **Generalization uncertainty**: Results demonstrate effectiveness for Dutch, but extent to which mechanisms apply to languages with different phonological systems remains untested

## Confidence

- **High Confidence**: Fine-tuning on subpopulation-specific data significantly improves ASR accuracy for that subpopulation
- **Medium Confidence**: Fine-tuning on one demographic group moderately improves recognition for other groups due to shared corpus characteristics
- **Medium Confidence**: Training on a comprehensive dataset yields equal or better per-group performance than group-specific training

## Next Checks

1. **Mechanism isolation experiment**: Design a controlled study where you fine-tune on matched speech styles (spontaneous vs. read) across different demographics to quantify whether acoustic-phonetic transfer or corpus-level effects drive cross-group improvements.

2. **Scaling experiment**: Fine-tune the same models on progressively larger subsets of each demographic group (e.g., 12h → 24h → 48h) to determine data requirements for diminishing returns and identify optimal training sizes for each subpopulation.

3. **Linguistic transfer analysis**: Evaluate fine-tuned models on controlled test sets that isolate specific linguistic phenomena (e.g., non-native phoneme substitutions, child pitch ranges, elderly speech rate variations) to determine which acoustic or linguistic features are most effectively adapted through fine-tuning.