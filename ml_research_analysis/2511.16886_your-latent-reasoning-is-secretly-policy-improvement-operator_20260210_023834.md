---
ver: rpa2
title: Your Latent Reasoning is Secretly Policy Improvement Operator
arxiv_id: '2511.16886'
source_url: https://arxiv.org/abs/2511.16886
tags:
- policy
- improvement
- reasoning
- step
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Deep Improvement Supervision (DIS), a training
  method for Tiny Recursive Models (TRMs) that provides step-wise intermediate targets
  during recursion. DIS uses a monotonic discrete corruption schedule of the ground-truth
  output, turning each recursion step into a supervised sub-goal.
---

# Your Latent Reasoning is Secretly Policy Improvement Operator

## Quick Facts
- arXiv ID: 2511.16886
- Source URL: https://arxiv.org/abs/2511.16886
- Authors: Arip Asadulaev; Rayan Banerjee; Fakhri Karray; Martin Takac
- Reference count: 17
- Primary result: DIS achieves 24% accuracy on ARC-AGI-1 with 0.8M parameters, outperforming most open-source LLM models without external knowledge

## Executive Summary
This paper introduces Deep Improvement Supervision (DIS), a training method for Tiny Recursive Models (TRMs) that provides step-wise intermediate targets during recursion. The approach addresses the common problem of "dead compute steps" in recursive models by using a monotonic discrete corruption schedule of ground-truth output, ensuring each recursion step contributes meaningfully to the final output. DIS achieves competitive performance on complex reasoning benchmarks, including ARC-AGI 1 and ARC-AGI 2, using a simpler architecture than traditional TRMs while significantly reducing computational requirements.

## Method Summary
DIS transforms recursive reasoning into a supervised sub-goal learning problem by providing intermediate targets at each recursion step. The method uses a monotonic discrete corruption schedule that gradually increases the difficulty of the ground-truth output, creating a curriculum where each step must meaningfully improve upon the previous one. This supervision strategy eliminates the need for training a separate halting mechanism and reduces the total number of forward passes by 18x while maintaining performance. The approach achieves this efficiency through 3x fewer supervision steps and 8x fewer latent reasoning steps compared to traditional TRMs.

## Key Results
- Achieves 24% accuracy on ARC-AGI-1 with only 0.8 million parameters
- Outperforms most open-source LLM models on ARC-AGI benchmarks without external knowledge
- Reduces total forward passes by 18x while maintaining competitive performance
- Uses 3x fewer supervision steps and 8x fewer latent reasoning steps compared to TRM

## Why This Works (Mechanism)
DIS works by converting each recursion step into a supervised learning problem with increasingly corrupted targets. This forces the model to learn meaningful intermediate representations at each step rather than potentially "dead" computations. The monotonic corruption schedule creates a natural curriculum where early steps handle easier, less corrupted inputs while later steps handle more complex transformations. By providing explicit supervision for each intermediate state, DIS ensures that every computational step contributes to the final solution rather than some steps being redundant or ineffective.

## Foundational Learning

**Recursive Model Training**: Why needed - Traditional TRMs suffer from dead compute steps where some recursion iterations don't contribute to final output. Quick check - Compare gradient flow across recursion steps with and without DIS supervision.

**Intermediate Supervision**: Why needed - Provides explicit training signals for each reasoning step rather than only final output. Quick check - Measure performance with varying numbers of intermediate supervision points.

**Monotonic Corruption Schedules**: Why needed - Creates curriculum learning effect that gradually increases task difficulty. Quick check - Test different corruption schedule patterns (linear, exponential, step-wise).

## Architecture Onboarding

**Component Map**: Input -> Corruption Scheduler -> Recursive Model -> Output, where Corruption Scheduler generates progressively corrupted ground-truth targets for each recursion step.

**Critical Path**: The recursive computation loop where each iteration receives corrupted target supervision and must produce improved output relative to that target.

**Design Tradeoffs**: Simpler architecture versus traditional TRM, reduced computational cost versus potential loss of flexibility, explicit intermediate supervision versus learned halting mechanisms.

**Failure Signatures**: Performance degradation when corruption schedule is too aggressive, dead compute steps re-emerging with inappropriate supervision granularity, training instability with rapid corruption changes.

**3 First Experiments**:
1. Vary corruption schedule aggressiveness and measure impact on convergence speed and final accuracy
2. Compare DIS performance with different numbers of recursion steps (2, 4, 8, 16)
3. Test DIS on non-ARC reasoning tasks to evaluate generalization beyond visual reasoning

## Open Questions the Paper Calls Out
None

## Limitations
- Performance claims come from a single research group without independent validation
- Results are limited to ARC-AGI benchmarks, with unclear generalization to other reasoning tasks
- The discrete corruption schedule's effectiveness across different problem types remains unexplored
- Limited comparison with established models makes it difficult to contextualize the performance improvements

## Confidence
- ARC-AGI-1 24% accuracy claim: Medium
- 18x reduction in forward passes: Medium
- Superior performance to open-source LLMs: Medium
- Generalization to other reasoning tasks: Low

## Next Checks
1. Test DIS on multiple reasoning benchmark suites (GSM8K, MATH, BBH) to assess generalization beyond ARC-AGI
2. Conduct ablation studies varying the corruption schedule parameters and comparing against different intermediate supervision strategies
3. Perform controlled experiments measuring the actual compute savings across different problem complexities and comparing with TRM using identical model sizes