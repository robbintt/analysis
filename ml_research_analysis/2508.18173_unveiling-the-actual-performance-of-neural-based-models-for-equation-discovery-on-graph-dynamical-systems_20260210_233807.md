---
ver: rpa2
title: Unveiling the Actual Performance of Neural-based Models for Equation Discovery
  on Graph Dynamical Systems
arxiv_id: '2508.18173'
source_url: https://arxiv.org/abs/2508.18173
tags:
- symbolic
- dynamics
- systems
- data
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study rigorously evaluates neural-based models for discovering
  governing equations of graph dynamical systems, addressing the challenge of balancing
  interpretability with predictive accuracy. The authors benchmark sparse regression,
  MLP-based architectures, and a novel Graph Kolmogorov-Arnold Network (GKAN-ODE)
  framework on synthetic and real-world datasets.
---

# Unveiling the Actual Performance of Neural-based Models for Equation Discovery on Graph Dynamical Systems

## Quick Facts
- arXiv ID: 2508.18173
- Source URL: https://arxiv.org/abs/2508.18173
- Reference count: 32
- Primary result: GKAN-ODE with learnable spline activations achieves lowest long-term trajectory errors while being more parameter-efficient than sparse regression baselines

## Executive Summary
This study rigorously evaluates neural-based models for discovering governing equations of graph dynamical systems, addressing the challenge of balancing interpretability with predictive accuracy. The authors benchmark sparse regression, MLP-based architectures, and a novel Graph Kolmogorov-Arnold Network (GKAN-ODE) framework on synthetic and real-world datasets. GKAN-ODE uses learnable spline activations with an optional multiplicative enhancement to better capture physical interactions. Results show neural models significantly outperform sparse regression baselines, with GKAN-ODE achieving the lowest long-term trajectory errors while being more parameter-efficient. The proposed Spline-Wise symbolic regression extracts transparent symbolic equations directly from KANs, providing faithful representations of learned dynamics.

## Method Summary
The framework learns symbolic ODEs governing graph dynamical systems by training separate neural networks to predict numerical derivatives (computed via five-point stencil) of node states. The GKAN-ODE architecture decouples intrinsic dynamics (H) and interaction dynamics (G), parameterizing them with learnable spline activations and aggregating interactions via the adjacency matrix. The model is trained to minimize MAE between predicted and numerical derivatives, with optional L1/entropy regularization. Symbolic equations are extracted using either PySR (genetic programming) or Spline-Wise fitting, which treats each trained spline as a 1D regression problem against a library of symbolic primitives. The approach is validated on synthetic systems (Kuramoto oscillators, epidemic spreading, biochemical, population dynamics) and real-world epidemic data.

## Key Results
- GKAN-ODE with multiplicative nodes achieves lowest MAE_traj (long-term trajectory error) across all synthetic and real-world datasets
- Spline-Wise symbolic regression successfully recovers exact symbolic forms (e.g., sine for Kuramoto) on synthetic systems with high precision
- Neural models demonstrate robust long-term stability and generalization, while sparse regression suffers from error accumulation on epidemic data
- GKAN-ODE architecture is more parameter-efficient than MLP baselines while maintaining superior performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling intrinsic dynamics from interaction dynamics improves data efficiency and interpretability for graph systems.
- **Mechanism:** The architecture separates the learning problem into two distinct functions: H (self-dynamics) and G (pairwise interaction). By parameterizing these with separate networks (GKAN or MLP) and aggregating interactions via the adjacency matrix A, the model enforces the physical structure of network dynamics rather than learning a monolithic state-transition function.
- **Core assumption:** The system follows the network universality principle where dynamics decompose into local (H) and interaction (G) terms.
- **Evidence anchors:** Methods explicitly define decoupled parameterization in Eq. 3; abstract highlights topological structure affects behavior; corpus supports constraining discovery with physical priors.
- **Break condition:** If underlying dynamics cannot be expressed as sum of self and pairwise terms, the decoupled model will underfit or learn a "ghost" interaction term to compensate.

### Mechanism 2
- **Claim:** Learnable univariate spline activations allow for direct extraction of symbolic functional forms.
- **Mechanism:** Unlike MLPs with fixed activations, KANs place learnable B-splines on edges. The "Spline-Wise" fitting algorithm treats each trained spline as a 1D regression problem, fitting it to a library of symbolic primitives (sin, exp, etc.), bypassing the need to interpret high-dimensional weight matrices.
- **Core assumption:** Constituent functions of dynamical system are smooth and can be approximated by low-order splines and subsequently mapped to simple symbolic primitives.
- **Evidence anchors:** Abstract states KANs achieve performance with greater parsimony; results show SW successfully identifying specific functional forms like sine for Kuramoto dynamics.
- **Break condition:** If true dynamics involve discontinuous or highly complex non-linearities requiring deep compositions not captured by spline grid resolution, symbolic regression yields overly complex or "noisy" expressions.

### Mechanism 3
- **Claim:** Integrating multiplicative nodes within layers effectively captures physical interaction terms (e.g., x_i x_j) without manual feature engineering.
- **Mechanism:** Standard neural aggregations are often additive. By designating half the nodes as multiplicative (h_mult = ∏φ(x)), the architecture explicitly builds in capacity for product-based interactions common in physics (mass-action, epidemiology).
- **Core assumption:** Target dynamical laws rely significantly on multiplicative interactions (e.g., (1-x_i)x_j in epidemics), which are otherwise difficult for additive-only networks to approximate efficiently.
- **Evidence anchors:** Methods state enhancement with hyperparameter-free multiplicative nodes; results show without multiplicative nodes, model struggles to recognize interaction terms in EPID/BIO dynamics.
- **Break condition:** If data is purely additive or linear, multiplicative nodes might introduce instability or overfitting, though authors claim sparsity regularization prunes these nodes effectively.

## Foundational Learning

- **Concept:** Numerical Differentiation (Five-Point Stencil)
  - **Why needed here:** Models act as "Neural ODEs," learning derivative rather than next state directly. Must understand how ground-truth derivatives are approximated from discrete trajectory data to train the network.
  - **Quick check question:** If time-series data has high-frequency observation noise, how would five-point stencil method perform compared to simple difference quotient?

- **Concept:** Graph Inductive Biases
  - **Why needed here:** Model assumes locality and permutation invariance via adjacency matrix A. Understanding this helps distinguish why this architecture works for networks but might fail on non-graph data.
  - **Quick check question:** In equation ĥẋ_i = Ĥ(x_i) + ΣA_ijĜ(x_i, x_j), if graph is fully connected, does model reduce to standard Neural ODE? Why or why not?

- **Concept:** Pareto Optimality in Symbolic Regression
  - **Why needed here:** Spline-Wise algorithm selects symbolic expressions based on trade-off between accuracy (MSE) and complexity (operator count). Need this to tune "interpretability vs. fit" trade-off.
  - **Quick check question:** If regularization parameter γ in Eq. 7 is set too high, what is likely visual appearance of spline φ vs. selected symbolic function f*?

## Architecture Onboarding

- **Component map:** Input Layer (x_i(t), A) -> Derivative Estimator (5-point stencil) -> GKAN-ODE Core (Ĥ Branch, Ĝ Branch, Layers with Spline activations + Multiplicative Nodes) -> Symbolic Distillation (PySR or Spline-Wise fitting)

- **Critical path:** 1. Data Prep: Generate trajectories → Numerical differentiation (5-point) → Dataset of {(x, ẋ)} 2. Training: Fit Ĥ and Ĝ to minimize MAE between predicted and numerical derivatives 3. Extraction: Run Spline-Wise fitting on trained KAN splines to get symbolic coefficients 4. Validation: Integrate symbolic equation over long horizon to check for divergence

- **Design tradeoffs:**
  - GKAN vs. MLP: GKAN more parameter-efficient and interpretable (via Spline-Wise) but may be slower to train due to spline optimization
  - GP vs. SW Extraction: "Black-box" GP finds simpler, more parsimonious laws; "White-box" SW more faithful to neural network's actual logic but can produce unwieldy, complex expressions
  - Multiplicative Nodes: Improves performance on interacting systems (EPID) but adds architectural complexity

- **Failure signatures:**
  - Exploding Gradients/Loss: Check spline grid ranges; splines can grow unbounded if not regularized
  - High Trajectory Error despite low Training Loss: Model overfitted to local derivatives but failed to capture global stability (common in chaotic systems)
  - Symbolic Regression Convergence Failure: Spline learned non-symbolic shape. Check library F in Spline-Wise or increase grid size

- **First 3 experiments:**
  1. Sanity Check (Kuramoto): Train on Kuramoto oscillators. Verify Ĥ approximates constant and Ĝ approximates sine function, confirming decoupling works
  2. Ablation on Multiplicative Nodes: Run EPID model with and without multiplicative node enhancement. Compare MAE_traj to quantify gain from inductive bias
  3. Noise Robustness: Add Gaussian noise (SNR 50dB) to training data. Compare trajectory stability of TPSINDy baseline vs. GKAN-ODE to verify denoising capacity

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the GKAN-ODE framework be adapted to handle Partial Differential Equations (PDEs) and multivariate dynamics on graphs?
- **Basis in paper:** Conclusion explicitly states future research should focus on extending these frameworks to PDEs and multivariate dynamics.
- **Why unresolved:** Current study restricted to ODEs where state derivative depends on current state and graph neighbors. PDEs require modeling spatial gradients and more complex boundary conditions, which current architecture does not support.
- **What evidence would resolve it:** Demonstration of GKAN-ODE discovering symbolic solution to benchmark PDE (e.g., diffusion or wave equation) discretized on mesh graph, successfully capturing spatial derivatives.

### Open Question 2
- **Question:** Can framework's robustness be improved for datasets with high noise levels (SNR < 50 dB) without relying on explicit numerical differentiation?
- **Basis in paper:** Technical Appendix notes models "tend to degenerate" at SNR ≤ 50 dB because five-point stencil used for derivative estimation amplifies noise.
- **Why unresolved:** Current training pipeline requires pre-computing target derivatives (ẋ) from noisy observations, creating bottleneck. Paper does not explore implicit differentiation or physics-informed loss functions that could bypass this noisy pre-processing step.
- **What evidence would resolve it:** Modifying loss function to integrate ODE directly against data (collocation point approach) and showing successful equation recovery on synthetic datasets with SNR = 20 dB.

### Open Question 3
- **Question:** How can trade-off between granular complexity of Spline-Wise (SW) fitting and parsimony of Genetic Programming (GP) be automated for unknown systems?
- **Basis in paper:** Results show for real-world epidemic data, SW fitting produces formula with complexity 30, while GP produces one with complexity 5, leaving choice of "scientific plausibility" to human expert.
- **Why unresolved:** No quantitative metric or automated heuristic provided to determine when high-complexity SW model is "overfitting" versus capturing essential nuances that simpler GP model misses, particularly when ground truth unavailable.
- **What evidence would resolve it:** Automated selection metric that successfully identifies correct model complexity on validation set of synthetic systems with varying noise and non-linearity, applied subsequently to real-world dataset.

## Limitations
- Spline-Wise symbolic regression can produce overly complex expressions that may not be practically useful, particularly for real-world systems involving discontinuities or non-smooth terms not well-captured by learnable splines
- Numerical differentiation step using five-point stencil is sensitive to noise in observational data, which could degrade derivative estimation accuracy and propagate through entire discovery pipeline
- Framework focuses on pairwise interaction dynamics through adjacency matrix, which may limit applicability to systems requiring higher-order interactions or non-local dynamics beyond immediate neighbor effects

## Confidence
- **High Confidence**: Empirical superiority of GKAN-ODE over sparse regression baselines for long-term trajectory prediction, supported by quantitative results across multiple synthetic and real-world datasets with statistically significant margins
- **Medium Confidence**: Interpretability advantage of learnable spline activations and Spline-Wise extraction method, as results show successful symbolic recovery for synthetic systems but produce complex, less parsimonious expressions for real-world epidemic data
- **Medium Confidence**: Architectural benefit of multiplicative nodes for capturing physical interaction terms, validated on synthetic epidemic models but with limited testing on real-world systems where interaction patterns may differ

## Next Checks
1. **Noise Robustness Validation**: Systematically test five-point stencil numerical differentiation under varying noise levels (SNR 20-80 dB) and compare trajectory stability of GKAN-ODE versus TPSINDy baseline to quantify neural architecture's denoising capacity
2. **Higher-Order Interaction Test**: Design synthetic graph dynamical systems with triplet or higher-order interactions and evaluate whether current pairwise aggregation via adjacency matrix captures these dynamics or requires architectural modifications
3. **Real-World Dataset Verification**: Obtain exact COVID-19/H1N1/SARS epidemic datasets with full preprocessing details and validate reported results, particularly focusing on discrepancy between simpler PySR-derived and more complex Spline-Wise expressions for these systems