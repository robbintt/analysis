---
ver: rpa2
title: Effective Damage Data Generation by Fusing Imagery with Human Knowledge Using
  Vision-Language Models
arxiv_id: '2508.01380'
source_url: https://arxiv.org/abs/2508.01380
tags:
- damage
- data
- fusion
- image
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to generate synthetic damage imagery
  for humanitarian assistance and disaster response by fusing satellite imagery with
  human knowledge using vision-language models. The approach addresses class imbalance
  and data scarcity issues in damage assessment by employing techniques such as prompt
  engineering, in-context learning, chain-of-thought prompting, and retrieval-augmented
  generation within a VLM framework.
---

# Effective Damage Data Generation by Fusing Imagery with Human Knowledge Using Vision-Language Models

## Quick Facts
- arXiv ID: 2508.01380
- Source URL: https://arxiv.org/abs/2508.01380
- Reference count: 37
- Generated damage imagery achieves F1=0.64 (mean) when classified by CLIFGAN, statistically comparable to real xView2 data (F1=0.67)

## Executive Summary
This paper presents a method to generate synthetic damage imagery for humanitarian assistance and disaster response by fusing satellite imagery with human knowledge using vision-language models. The approach addresses class imbalance and data scarcity issues in damage assessment by employing techniques such as prompt engineering, in-context learning, chain-of-thought prompting, and retrieval-augmented generation within a VLM framework. Generated images were evaluated using a damage classification system, showing F1 scores of 0.64—statistically comparable to real xView2 data (F1 0.67) and significantly higher than those from two GAN baselines (~0.50). Qualitative results demonstrated realistic seasonal and damage-level variations. This suggests that VLMs can produce high-quality synthetic data for training and augmenting damage assessment models.

## Method Summary
The method uses Gemini 2.0/2.5-flash VLMs to generate synthetic satellite imagery showing building damage at four severity levels. The system employs prompt engineering with xView2 damage definitions, in-context learning with sample image examples, chain-of-thought prompting, and retrieval-augmented generation using a local HADR knowledge base. Generated images are evaluated using a pre-trained CLIFGAN classifier. The approach contrasts with slower GAN-based methods, achieving comparable F1 scores while requiring only seconds per image versus 12+ hours for GAN training.

## Key Results
- HK-VLM achieved F1=0.64 (mean) and 0.65 (median) when evaluated by CLIFGAN classifier
- Performance is statistically comparable to real xView2 data (F1=0.67) and significantly better than GAN baselines (~0.50)
- Generated images show realistic seasonal and damage-level variations across different disaster contexts
- VLM generation is substantially faster than GAN training (seconds vs 12+ hours on RTX 3090)

## Why This Works (Mechanism)

### Mechanism 1
Natural language prompts encoding expert damage definitions can guide VLMs to generate damage imagery with class-relevant visual features. The VLM receives system prompts with explicit damage level definitions which condition the generative process toward domain-appropriate outputs rather than generic image synthesis. Core assumption: The pre-trained VLM has sufficient visual-semantic grounding to translate textual descriptions of structural damage into pixel-level manifestations.

### Mechanism 2
In-context learning with few-shot image examples improves damage generation quality by providing multimodal grounding for damage concepts. Providing sample images with known damage levels as input-output pairs allows the VLM to learn visual correspondences that complement natural language descriptions, reducing ambiguity in damage level interpretation. Core assumption: VLMs can generalize from limited examples to novel scenes while maintaining damage-level consistency.

### Mechanism 3
Retrieval-Augmented Generation (RAG) with a local HADR knowledge base reduces hallucinations and improves domain-specific generation quality. A vector database stores specialized HADR expertise unlikely to be in VLM pre-training; retrieval injects relevant context at generation time, grounding outputs in validated domain knowledge. Core assumption: HADR damage patterns contain specialized knowledge gaps in general-purpose VLMs that local retrieval can address.

## Foundational Learning

- **Vision-Language Models (VLMs)**: Why needed here: The core approach relies on multimodal models (Gemini 2.0/2.5-flash) that process both images and text to generate synthetic damage imagery. Quick check question: Can you explain how a VLM differs from a text-only LLM in terms of input/output modalities and typical architectures?

- **xView2 Dataset and Damage Classification Schema**: Why needed here: The paper uses xView2 damage levels (no damage, minor, major, total) as the target taxonomy; understanding this structure is essential for prompt design and evaluation. Quick check question: What are the four damage severity levels in xView2, and what class imbalance challenge does the dataset present?

- **Information Fusion Levels (JDL Model)**: Why needed here: The paper situates its approach within Level 1-5 information fusion, particularly Level 5 "user refinement" where human knowledge guides assessment. Quick check question: What is Level 5 information fusion, and how does it relate to human-in-the-loop systems?

## Architecture Onboarding

- **Component map**: Expert Knowledge → Prompt Engineering → VLM (Gemini 2.0/2.5) → Grounding DINO/YOLO/SAM → Object Masks → CLIFGAN Classifier → F1 Evaluation

- **Critical path**: System prompt definition → VLM generation → CLIFGAN classification → F1 score comparison against real xView2 baseline (0.67)

- **Design tradeoffs**: VLM generation (fast, ~seconds) vs. GAN training (slow, 12+ hrs on RTX 3090) — VLM offers efficiency gains; Precision vs. recall: HK-VLM shows precision (0.81 mean) significantly higher than recall (0.55 mean), suggesting conservative damage generation; Qualitative control vs. scale: Paper focused on small, controlled scenes; large geographically coherent areas remain future work.

- **Failure signatures**: Low recall with high precision: Generated damage may miss subtle or rare damage patterns; Visual plausibility without semantic correctness: Images may look realistic but misrepresent damage levels; Hallucination in novel contexts: Without adequate RAG or in-context examples, VLM may generate non-existent damage features.

- **First 3 experiments**: 1) Baseline prompt ablation: Generate images with damage definitions only (no in-context examples) and compare F1 scores against full system to isolate in-context learning contribution; 2) RAG coverage test: Vary the size/quality of the local HADR knowledge base and measure impact on hallucination rates (using expert visual inspection or LLM-as-judge); 3) Cross-disaster generalization: Train CLIFGAN on hurricane imagery, test on generated earthquake/fire imagery to assess whether VLM-generated data transfers across disaster types.

## Open Questions the Paper Calls Out

- Can HK-VLM synthetic data effectively augment real datasets to improve damage assessment model generalization? The conclusion states that "systematic studies will be performed to evaluate the effectiveness of these generated images as augmented data." The current study only assessed the statistical similarity of generated images to real data, not their utility in training downstream models.

- Can this method generate large, geographically coherent areas with consistent damage patterns? The conclusion identifies the need to move beyond "small, controlled scenes" to "generate large, geographically coherent areas" for wide-area assessment. The current proof-of-concept is limited to small image patches, whereas real-world HADR operations require spatially consistent wide-area imagery.

- Does "LLM-as-a-judge" provide a more reliable quality assessment than the current CLIFGAN baseline? The conclusion lists exploring "LLM as a judge" as a next step to "shed more light on the quality of the generated images." Current evaluation relies solely on the CLIFGAN classifier, which may introduce specific biases or limitations in assessing VLM outputs.

## Limitations

- Unverified damage semantics: While F1 scores are statistically comparable to real data, there's no qualitative validation that generated images truly reflect damage patterns in new disaster contexts beyond the test set.
- Unknown prompt engineering specifics: Exact system/user prompts, RAG knowledge base contents, and LoRA fine-tuning parameters are not specified, creating significant reproducibility gaps.
- Hallucination risk unaddressed: The paper claims RAG reduces hallucinations but provides no quantitative measurement of hallucination rates or expert validation of semantic correctness.

## Confidence

- **High confidence**: VLM generation is faster than GAN training (~seconds vs 12+ hours), and F1 scores show statistical parity with real data.
- **Medium confidence**: Generated images show plausible damage patterns and seasonal variations, but semantic correctness across novel disaster types remains unverified.
- **Low confidence**: Claims about RAG reducing hallucinations are qualitative without supporting quantitative metrics.

## Next Checks

1. Cross-disaster generalization test: Generate images for disaster types not in xView2 (e.g., flood, wildfire) and validate semantic correctness through expert inspection or LLM-as-judge.
2. Hallucination quantification: Implement automated hallucination detection by comparing generated damage features against known damage patterns in the local knowledge base, reporting hallucination rates.
3. Ablation study on RAG contribution: Generate images with and without RAG retrieval, measuring impact on both F1 scores and hallucination rates to isolate RAG's contribution to quality.