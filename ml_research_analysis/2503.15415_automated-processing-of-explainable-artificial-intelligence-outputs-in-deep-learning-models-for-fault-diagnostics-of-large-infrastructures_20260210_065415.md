---
ver: rpa2
title: Automated Processing of eXplainable Artificial Intelligence Outputs in Deep
  Learning Models for Fault Diagnostics of Large Infrastructures
arxiv_id: '2503.15415'
source_url: https://arxiv.org/abs/2503.15415
tags:
- explanations
- images
- deep
- class
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of validating deep learning
  (DL) model outputs for fault diagnostics in large infrastructures by automating
  the analysis of model explanations. Manual review of these explanations is time-consuming
  and error-prone, necessitating automated tools.
---

# Automated Processing of eXplainable Artificial Intelligence Outputs in Deep Learning Models for Fault Diagnostics of Large Infrastructures

## Quick Facts
- arXiv ID: 2503.15415
- Source URL: https://arxiv.org/abs/2503.15415
- Reference count: 0
- Primary result: Automated framework improves DL model reliability by 8% accuracy on faulty classes through analysis of model explanations.

## Executive Summary
This paper addresses the challenge of validating deep learning (DL) model outputs for fault diagnostics in large infrastructures by automating the analysis of model explanations. Manual review of these explanations is time-consuming and error-prone, necessitating automated tools. The proposed framework combines post-hoc explanations with semi-supervised anomaly detection to identify anomalous explanations that may indicate model abnormal behaviors, such as misclassifications or reliance on non-causal shortcuts. The framework was applied to drone-collected images of insulator shells for power grid monitoring, using two CNN classifiers (MobileNetV3 Small and EfficientNet-B0), GradCAM for explanations, and Deep Semi-Supervised Anomaly Detection (Deep SAD) for anomaly identification.

## Method Summary
The framework processes GradCAM explanations to identify anomalous explanations indicating misclassifications or non-causal shortcuts in fault diagnostics of insulator shells. It uses a pre-trained CNN classifier to predict classes, generates GradCAM heatmaps as explanations, and trains class-specific Deep SAD models to detect anomalies. The method employs 5-fold cross-validation with 60/20/20 train/validation/test splits within each class, optimizing anomaly detection thresholds to maximize precision (β=0.1). The approach was tested on the IDID dataset with 13,336 healthy, 2,564 flashover, and 1,204 broken insulator shell images.

## Key Results
- Average classification accuracy improvement of 8% on faulty classes after manual reclassification of flagged anomalies
- Maintenance operators required to manually reclassify only 15% of the images
- Outperformed state-of-the-art faithfulness-based approaches with consistently higher F1 scores
- Successfully identified correct classifications resulting from non-causal shortcuts, such as reliance on ID tags

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The framework detects non-causal shortcuts by identifying deviations in the spatial coherence of feature attribution maps relative to a learned "normal" cluster.
- **Mechanism:** GradCAM generates a heatmap (explanation) for a predicted class. Deep SAD maps these heatmaps into an embedding space. If the model uses a shortcut (e.g., an ID tag instead of a fault), the heatmap spatially diverges from the cluster of heatmaps generated by correctly classified images, resulting in a high distance from the center.
- **Core assumption:** Correctly classified images produce explanation heatmaps that are visually and spatially similar to one another, whereas misclassifications and shortcut-driven classifications produce spatially distinct explanations.
- **Evidence anchors:** [Abstract]: "...successfully identified correct classifications that resulted from non-causal shortcuts, such as the presence of ID tags printed on insulator shells." [Page 16, Section 7.2]: "Explanation 6d illustrates a correct classification for the wrong reason... Deep SAD maps this explanation far from the center, as it significantly differs from the typical flashover explanations..."

### Mechanism 2
- **Claim:** Semi-supervised learning on explanations allows the system to generalize the definition of an "anomaly" beyond the specific error types seen during training.
- **Mechanism:** Deep SAD is trained using a loss function that minimizes the distance of unlabeled (mostly correct) explanations to a center point while maximizing the distance for known misclassifications (labeled anomalies). Unlike supervised classifiers, this forces the model to learn a compact boundary around "normal" reasoning, flagging any reasoning that falls outside, even if the specific error mode was never seen before.
- **Core assumption:** Known misclassifications available during training are representative of the broader phenomenon of "abnormal reasoning," allowing the model to push away all non-normal explanations, not just the specific types seen.
- **Evidence anchors:** [Page 3, Section 1]: "...Deep SAD is selected... [as] it does not impose constraints on the compactness of clusters formed by anomalous explanations, which is essential for accommodating rare or previously unseen types of explanations." [Page 8, Section 4.3]: Eq. (1) shows the loss function balancing unlabeled data compactness against labeled anomaly separation.

### Mechanism 3
- **Claim:** Class-specific anomaly detection models are required because the visual signature of a valid explanation varies significantly across different fault types.
- **Mechanism:** Instead of one global detector, the architecture trains Z separate Deep SAD models (φz), one for each predicted class. A "flashover" explanation looks different from a "broken" explanation; separate models ensure that a valid "broken" explanation isn't flagged as anomalous just because it looks different from a "flashover" explanation.
- **Core assumption:** Explanations for different classes form distinct, non-overlapping clusters in the embedding space.
- **Evidence anchors:** [Page 10, Section 5]: "The decision to develop Z distinct models... is driven by the observation that explanations of images correctly classified into different classes often form distinct clusters." [Page 11, Algorithm 3]: Explicitly details the step "Divide the dataset Dz... for each class z."

## Foundational Learning

### Concept: GradCAM (Gradient-weighted Class Activation Mapping)
- **Why needed here:** This is the source of the "explanation" data. You must understand that GradCAM highlights *spatial regions* rather than pixel-level color intensity. It uses gradients flowing into the final convolutional layer to assign importance weights to feature maps.
- **Quick check question:** If the model focuses on the background sky instead of the insulator, will GradCAM highlight the sky?

### Concept: Deep SAD (Deep Semi-Supervised Anomaly Detection)
- **Why needed here:** This is the core logic for flagging errors. You need to distinguish it from standard classification. It maps inputs to a hypersphere (compact cluster) rather than a hyperplane decision boundary.
- **Quick check question:** In the loss function, do we push labeled anomalies *towards* or *away* from the center c?

### Concept: Transfer Learning (CNN Backbones)
- **Why needed here:** The framework assumes the existence of a pre-trained classifier (EfficientNet/MobileNet). Understanding that these models have pre-learned feature extractors (edges, textures) is crucial for debugging why GradCAM might highlight irrelevant features.
- **Quick check question:** Why is the final fully connected layer of the pre-trained model modified in this framework?

## Architecture Onboarding

### Component map:
Input image → Classifier (Φ) → GradCAM explainer (g) → Class-specific Deep SAD (Ψ) → Anomaly decision (distance from center)

### Critical path:
The configuration of the **Autoencoder pre-training** for Deep SAD. The paper notes that Deep SAD is initialized via an autoencoder to learn a robust representation of the explanation data before applying the anomaly loss. Failure to pre-train or reconstruct explanations accurately will likely result in a collapsed embedding space.

### Design tradeoffs:
- **Precision vs. Recall (β in F-score):** The paper sets β=0.1 to prioritize precision (minimizing manual review work). This means the system will miss some misclassifications (low recall) to ensure that the flagged images are highly likely to be actual errors.
- **Heatmap Resolution:** GradCAM produces coarse outputs (low spatial resolution). This reduces noise but might miss fine-grained faults.

### Failure signatures:
- **Trivial Solution:** The Deep SAD loss collapses to zero because all explanations map to the exact same point (center). This happens if the learning rate is too high or labeled anomaly weight η is too low.
- **High Standard Deviation:** Due to small validation sets, the anomaly threshold Thz may fluctuate wildly between folds (Page 17).
- **Noisy Explanations:** Using methods like CartoonX on classes where it produces jittery outputs (Page 25) causes the framework to fail because the "normal" cluster becomes too diffuse.

### First 3 experiments:
1. **Sanity Check GradCAM:** Feed images of known faults into Φ and visualize GradCAM. Does it highlight the fault or the background (shortcut)? This validates the input data for the Anomaly Detector.
2. **Autoencoder Reconstruction:** Train the autoencoder on explanations and check reconstruction error. If it cannot reconstruct heatmaps, the subsequent Deep SAD embedding will be meaningless.
3. **Threshold Sensitivity:** Run the Deep SAD model on a validation set. Plot the distance distribution for "Correct" vs. "Incorrect" classifications to see if a clear separation threshold exists before locking in the Fβ optimization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating multiple XAI techniques (e.g., combining GradCAM and CartoonX) into a single input improve the ability of the Deep SAD anomaly detector to distinguish anomalous explanations compared to single-method inputs?
- Basis in paper: [explicit] The conclusion states, "Another improvement of the framework could concern the integration of multiple XAI techniques to obtain richer explanations... combining several XAI methods could extend the information on how the DL model is reasoning and potentially improve Deep SAD’s... ability to distinguish anomalous explanations."
- Why unresolved: The study evaluated GradCAM and CartoonX separately, finding that CartoonX struggled with the "flashover" class (producing jittery, uninformative explanations), but did not test a fused approach to mitigate individual method weaknesses.
- What evidence would resolve it: Experimental results comparing the F1 scores and precision/recall metrics of the Deep SAD model when processing concatenated explanations from multiple XAI methods versus single-method baselines on the same dataset.

### Open Question 2
- Question: How can the framework be systematically optimized to define an objective function that balances the financial costs of misclassification against the operational costs of manual review by maintenance operators?
- Basis in paper: [explicit] The authors note that "systematic optimization will require a considering the interactions between the models and the definition of an objective function considering several factors, like the costs of misclassifications, manual classifications and model development."
- Why unresolved: The current study prioritized minimizing manual reclassification (precision) via the β hyperparameter heuristically, rather than optimizing for a holistic economic or operational utility function.
- What evidence would resolve it: A proposed cost-sensitive optimization framework where hyperparameters (such as the anomaly detection threshold η) are tuned to minimize a total cost function, demonstrating superior economic efficiency compared to the standard F-score optimization used in the paper.

### Open Question 3
- Question: Does retraining the diagnostic model using a loss function that explicitly penalizes deviations from expert-provided explanations effectively reduce the occurrence of non-causal shortcuts (such as the ID tag bias observed in the case study)?
- Basis in paper: [explicit] The conclusion proposes that "future research will explore strategies to minimize misclassifications and non-causal shortcuts during model training. One potential direction... is to involve experts in providing explanations and retraining the diagnostic model using a loss function that explicitly incorporates desired explanation characteristics."
- Why unresolved: The current framework acts post-hoc to identify errors after the model has made a prediction; it does not correct the underlying model weights to prevent the model from learning spurious correlations (like ID tags) in the first place.
- What evidence would resolve it: A comparative study showing that a classifier trained with this explanation-based loss function relies less on non-causal features (verified by XAI outputs) and achieves higher out-of-sample accuracy than the original baseline model.

## Limitations
- **Data Dependency**: The framework's effectiveness is highly dependent on the quality and diversity of the initial classifier's predictions. If the classifier has high baseline error rates, the semi-supervised anomaly detection assumption breaks down.
- **XAI Method Sensitivity**: The framework's performance is sensitive to the choice of XAI method. While GradCAM works well for this application, other methods like CartoonX may produce noisy explanations that prevent effective clustering.
- **Class-Specific Thresholding**: The need for separate anomaly detection models per class adds complexity and may not scale well to applications with many classes or highly similar classes.

## Confidence
- **High Confidence**: The framework's ability to identify non-causal shortcuts (e.g., ID tags) is well-supported by the experimental results showing 8% accuracy improvement on faulty classes.
- **Medium Confidence**: The semi-supervised learning mechanism's generalizability to unseen error types is theoretically sound but would benefit from validation on a broader range of error modes.
- **Medium Confidence**: The class-specific anomaly detection approach is well-justified by the distinct visual signatures of different fault types, but the scalability to many classes remains untested.

## Next Checks
1. **Robustness to Classifier Error**: Test the framework's performance when the initial classifier has varying levels of accuracy (e.g., 70% vs. 95%) to validate the semi-supervised learning assumption.
2. **Cross-Dataset Generalization**: Apply the framework to a different infrastructure fault detection dataset (e.g., bridge crack detection) to assess its generalizability beyond insulator shells.
3. **Alternative XAI Methods**: Compare the framework's performance using different XAI methods (e.g., LIME, SHAP) to determine the optimal choice for various fault types and imaging conditions.