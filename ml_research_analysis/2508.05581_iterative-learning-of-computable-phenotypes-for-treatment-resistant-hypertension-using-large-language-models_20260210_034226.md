---
ver: rpa2
title: Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension
  using Large Language Models
arxiv_id: '2508.05581'
source_url: https://arxiv.org/abs/2508.05581
tags:
- features
- performance
- sedi
- prompt
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) can generate clinically meaningful,
  interpretable computable phenotypes (CPs) for hypertension-related conditions through
  iterative refinement. The study demonstrates that GPT-4o with a synthesize-execute-debug-instruct
  (SEDI) strategy can produce CPs achieving 0.85 AUPRC and 0.94 AUROC for apparent
  treatment-resistant hypertension, comparable to state-of-the-art ML methods while
  requiring significantly fewer training examples.
---

# Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models

## Quick Facts
- arXiv ID: 2508.05581
- Source URL: https://arxiv.org/abs/2508.05581
- Reference count: 40
- Primary result: LLMs generate clinically meaningful computable phenotypes for hypertension with 0.85 AUPRC and 0.94 AUROC

## Executive Summary
This study demonstrates that large language models can generate interpretable computable phenotypes (CPs) for treatment-resistant hypertension through an iterative refinement process. Using GPT-4o with a synthesize-execute-debug-instruct (SEDI) strategy, the researchers produced CPs that achieve performance metrics comparable to state-of-the-art machine learning methods while requiring significantly fewer training examples. The approach produces Python programs that are both accurate and interpretable, offering a scalable alternative to traditional CP development that typically demands extensive manual expert effort.

## Method Summary
The study employs an iterative learning approach where GPT-4o generates initial CPs based on phenotype descriptions and feature sets, then refines them through a cycle of synthesis, execution, debugging, and instruction. The method uses a diverse set of features including ICD codes, medications, laboratory tests, and vital signs. CPs are evaluated using apparent treatment-resistant hypertension as a case study, with performance measured through AUPRC and AUROC metrics. The iterative process continues until performance plateaus or a target threshold is reached, with each iteration producing more refined and accurate phenotype definitions.

## Key Results
- GPT-4o-generated CPs achieved 0.85 AUPRC and 0.94 AUROC for apparent treatment-resistant hypertension
- Performance matches state-of-the-art ML methods while requiring fewer training examples
- Rich phenotype descriptions and expert-curated feature sets improve performance, though SEDI enables competitive results even with minimal prompts
- Generated CPs are interpretable Python programs balancing accuracy and transparency

## Why This Works (Mechanism)
The success of this approach stems from LLMs' ability to understand clinical context and translate it into executable code. The iterative refinement process allows the model to learn from its mistakes, gradually improving the precision of phenotype definitions. By leveraging existing clinical knowledge encoded in the model's training data, the system can generate clinically meaningful phenotypes without requiring extensive labeled training data. The interpretability of the generated Python code ensures that clinicians can review and validate the phenotype logic, maintaining transparency in the development process.

## Foundational Learning
- Clinical Phenotype Development: Understanding how clinical conditions are defined and operationalized in healthcare data (needed to create meaningful phenotype definitions; quick check: can the model generate accurate CPs for well-established conditions)
- Iterative Refinement Process: Knowledge of how to systematically improve outputs through repeated cycles of execution and correction (needed to optimize phenotype accuracy; quick check: does performance improve with each iteration)
- Python Programming for Clinical Data: Ability to translate clinical logic into executable code using healthcare data structures (needed to create usable CPs; quick check: can the generated code run without syntax errors on real data)
- Performance Metric Interpretation: Understanding AUPRC and AUROC in the context of phenotype validation (needed to evaluate CP quality; quick check: do metrics align with clinical expectations)
- EHR Data Structure Knowledge: Familiarity with how clinical data is organized and stored in electronic health records (needed to access and process relevant features; quick check: can the model identify relevant features from EHR schemas)
- Large Language Model Prompt Engineering: Skills in crafting effective prompts to elicit desired outputs from LLMs (needed to initiate and guide the phenotype generation process; quick check: does prompt quality affect initial CP performance)

## Architecture Onboarding

**Component Map**: Clinical Description -> GPT-4o LLM -> Initial CP -> Execution Environment -> Performance Metrics -> Feedback Loop -> Refined CP

**Critical Path**: The SEDI cycle (Synthesize → Execute → Debug → Instruct) represents the core workflow where phenotype definitions are iteratively improved through execution and feedback.

**Design Tradeoffs**: Accuracy vs. interpretability (more complex phenotypes may perform better but be harder to understand), automation vs. expert oversight (fully automated approaches may miss clinical nuances), and computational efficiency vs. refinement depth (more iterations improve quality but increase resource requirements).

**Failure Signatures**: Poor initial phenotype descriptions leading to irrelevant CPs, insufficient feature sets resulting in incomplete phenotype definitions, and model hallucinations generating non-existent clinical concepts or data elements.

**First Experiments**:
1. Test CP generation on a well-defined condition with established clinical criteria to establish baseline performance
2. Vary prompt complexity and feature set richness to quantify their impact on CP quality
3. Implement a human-in-the-loop validation step where clinicians review and provide feedback on generated CPs

## Open Questions the Paper Calls Out
None

## Limitations
- Single-institution dataset limits generalizability to other healthcare settings with different EHR systems and patient populations
- Performance metrics require validation in external datasets before broader clinical adoption
- Quality of generated CPs depends heavily on the LLM's current capabilities, which may evolve over time

## Confidence
- High Confidence: LLMs can generate interpretable Python code for computable phenotypes achieving competitive performance metrics
- Medium Confidence: LLMs can produce clinically meaningful phenotypes comparable to state-of-the-art ML methods
- Low Confidence: This approach significantly reduces manual expert effort compared to traditional CP development

## Next Checks
1. Test generated CPs on datasets from at least two additional healthcare systems with different EHR platforms to assess generalizability
2. Evaluate CP performance stability over time by applying phenotypes to patient data collected at least 12 months apart
3. Conduct a prospective study measuring whether LLM-generated CPs improve treatment decision-making accuracy or patient outcomes compared to standard clinical practice