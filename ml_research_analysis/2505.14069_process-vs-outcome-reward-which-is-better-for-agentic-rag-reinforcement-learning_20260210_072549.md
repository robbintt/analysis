---
ver: rpa2
title: 'Process vs. Outcome Reward: Which is Better for Agentic RAG Reinforcement
  Learning'
arxiv_id: '2505.14069'
source_url: https://arxiv.org/abs/2505.14069
tags:
- reasoning
- query
- evidence
- answer
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReasonRAG, a process-supervised reinforcement
  learning method for improving agentic retrieval-augmented generation (RAG). The
  approach addresses challenges in existing outcome-supervised RL methods, including
  low exploration efficiency, gradient conflicts, and sparse rewards.
---

# Process vs. Outcome Reward: Which is Better for Agentic RAG Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.14069
- Source URL: https://arxiv.org/abs/2505.14069
- Reference count: 40
- Primary result: ReasonRAG achieves state-of-the-art performance on multi-hop reasoning tasks using only 5k training instances compared to 90k for baseline methods

## Executive Summary
This paper introduces ReasonRAG, a process-supervised reinforcement learning method for improving agentic retrieval-augmented generation (RAG). The approach addresses fundamental challenges in outcome-supervised RL methods including low exploration efficiency, gradient conflicts, and sparse rewards. By using Monte Carlo Tree Search for reasoning path exploration and Shortest Path Reward Estimation for fine-grained process-level rewards, ReasonRAG demonstrates superior performance on multi-hop reasoning tasks while requiring significantly fewer training instances than existing methods.

## Method Summary
ReasonRAG employs a hybrid reinforcement learning approach that combines Monte Carlo Tree Search (MCTS) for exploration with a novel Shortest Path Reward Estimation mechanism for process supervision. The method breaks down the RAG pipeline into three key stages - query generation, evidence extraction, and answer generation - each receiving fine-grained rewards during training. The system automatically constructs the RAG-ProGuide dataset through a bootstrapping process, creating 5k high-quality queries and 13k preference pairs without expensive human annotation. The MCTS explores multiple reasoning paths while the reward estimation provides immediate feedback at each stage, enabling more efficient learning compared to traditional outcome-only supervision.

## Key Results
- Achieves state-of-the-art performance on five benchmark datasets for multi-hop reasoning tasks
- Requires only 5k training instances versus 90k needed by baseline Search-R1 method
- Demonstrates superior exploration efficiency through process-level reward signals
- Shows effectiveness across diverse reasoning scenarios including HotpotQA and 2WikiMultiHopQA

## Why This Works (Mechanism)
ReasonRAG's effectiveness stems from addressing the fundamental limitations of outcome-supervised RL in RAG systems. Traditional outcome-only rewards create sparse feedback signals that make learning inefficient, while process supervision provides immediate guidance at each reasoning stage. The Monte Carlo Tree Search enables systematic exploration of reasoning paths rather than random exploration, while the Shortest Path Reward Estimation mechanism provides differentiable, fine-grained feedback that reduces gradient conflicts. This combination allows the agent to learn which reasoning steps are most valuable for successful retrieval and generation.

## Foundational Learning

1. **Monte Carlo Tree Search (MCTS)** - Why needed: Systematic exploration of reasoning paths; Quick check: Verify tree depth and branching factor match reasoning complexity
2. **Shortest Path Reward Estimation** - Why needed: Provide immediate, differentiable feedback; Quick check: Confirm reward signals align with final outcomes
3. **Process vs. Outcome Supervision** - Why needed: Address sparse rewards in RL; Quick check: Compare learning curves with/without process supervision
4. **Multi-hop Reasoning** - Why needed: Complex queries requiring chained evidence; Quick check: Validate retrieval accuracy at each reasoning step
5. **RAG Pipeline Decomposition** - Why needed: Isolate learning at each stage; Quick check: Measure individual stage performance improvements

## Architecture Onboarding

Component Map: Query Generator -> Evidence Extractor -> Answer Generator -> MCTS Planner -> Reward Estimator

Critical Path: User Query → Query Generation → Evidence Retrieval → Answer Generation → Reward Calculation → Policy Update

Design Tradeoffs: Process supervision provides richer feedback but requires more complex reward engineering; MCTS exploration is computationally expensive but improves sample efficiency

Failure Signatures: 
- Low exploration diversity indicates MCTS parameters need adjustment
- Reward signal misalignment suggests reward estimation needs refinement
- Stage-specific performance drops indicate decomposition issues

First Experiments:
1. Baseline comparison with outcome-only supervised RL on single-hop queries
2. Ablation study removing MCTS to test exploration impact
3. Reward sensitivity analysis varying process reward weights

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations

- The claim that process supervision is universally better than outcome supervision may be overgeneralized and context-dependent
- Automatic dataset construction quality is not thoroughly validated, raising potential bias concerns
- Performance on non-multi-hop reasoning tasks and domain-specific knowledge is not extensively explored

## Confidence

High Confidence:
- Technical implementation of MCTS and Shortest Path Reward Estimation is sound
- Experimental methodology and benchmark comparisons are rigorous
- Performance improvements over baseline are statistically significant

Medium Confidence:
- Efficiency claims (5k vs 90k training instances) are supported but require broader validation
- RAG-ProGuide dataset quality assessment needs more detailed methodology
- Generalizability to different domains and task types requires further testing

Low Confidence:
- Universal superiority of process supervision over outcome supervision is not established
- Claims about effectiveness across all RAG scenarios are overstated

## Next Checks

1. Conduct cross-domain experiments testing ReasonRAG on non-factoid, creative, or subjective reasoning tasks to validate whether process supervision maintains its advantages beyond structured, multi-hop reasoning scenarios.

2. Perform ablation studies isolating the contributions of Monte Carlo Tree Search versus Shortest Path Reward Estimation to determine which component drives the performance improvements and whether both are necessary.

3. Test ReasonRAG's performance when trained on smaller datasets (e.g., 1k or 2k instances) to establish the minimum effective dataset size and validate whether the 5k training claim represents a true efficiency breakthrough or a domain-specific finding.