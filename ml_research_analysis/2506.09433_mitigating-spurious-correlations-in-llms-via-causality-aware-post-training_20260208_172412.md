---
ver: rpa2
title: Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training
arxiv_id: '2506.09433'
source_url: https://arxiv.org/abs/2506.09433
tags:
- symbol
- reasoning
- description
- 'null'
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of spurious correlations in large
  language models (LLMs) during pre-training and fine-tuning, which degrade out-of-distribution
  (OOD) generalization in reasoning tasks. The proposed causality-aware post-training
  (CAPT) method decomposes biased predictions into two steps: event estimation and
  event intervention.'
---

# Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training

## Quick Facts
- arXiv ID: 2506.09433
- Source URL: https://arxiv.org/abs/2506.09433
- Reference count: 38
- Key outcome: CAPT enables 3B models to outperform larger LLMs on ID/OOD reasoning tasks using only 100 fine-tuning samples

## Executive Summary
This paper addresses spurious correlations in LLMs during pre-training and fine-tuning that degrade out-of-distribution generalization in reasoning tasks. The proposed Causality-Aware Post-Training (CAPT) method decomposes biased predictions into event estimation and event intervention steps. By estimating events from prompts and replacing them with random symbolic placeholders, CAPT blocks both pre-training and fine-tuning biases without requiring additional data. Experiments on CLadder and PrOntoQA show that CAPT enables 3B models to outperform both larger LLMs and standard fine-tuning approaches on ID and OOD tasks using only 100 fine-tuning samples, demonstrating strong effectiveness, robustness, and sample efficiency.

## Method Summary
CAPT applies causal intervention to LLMs by first estimating events from prompts using a strong teacher model (GPT-4o-mini), then replacing these events with random symbolic placeholders during fine-tuning. This breaks spurious correlations by forcing the model to rely on invariant logical structure rather than surface-level event associations. The method is applied to binary classification reasoning tasks using 100-200 in-distribution training samples, with evaluation on both in-distribution and out-of-distribution test sets including anti-sense perturbations and non-sense random strings.

## Key Results
- CAPT enables 3B models to outperform GPT-4o and standard SFT on both ID and OOD reasoning tasks
- Demonstrates strong sample efficiency, achieving superior performance with only 100 fine-tuning samples
- Shows robust generalization with performance drops consistently within 3% when event estimation is slightly imperfect
- Ablation study confirms randomization is critical: CAPT=order converges faster but achieves lower final OOD performance

## Why This Works (Mechanism)

### Mechanism 1: Symbolic Intervention Blocks Spurious Paths
The paper models data generation as $E \rightarrow X \leftarrow S \rightarrow Y$ where standard training induces spurious correlation between events $E$ and answers $Y$. Replacing specific event entities with randomized symbols forces the model to rely on latent logical structure ($S$) rather than surface-level correlations ($E$). This performs a causal intervention that cuts the "backdoor" path, ensuring $P(Y|X)$ depends only on the invariant structure $S$. Evidence shows this blocks both pre-training and fine-tuning biases through consistent OOD performance gains.

### Mechanism 2: Transferable Event Estimation
Large pre-trained models possess a "transferable" capability to identify and extract events ($E$) from prompts ($X$) even when they fail to reason correctly about them. Pre-training data is diverse enough to dilute specific event-level collider biases, making event recognition robust while reasoning remains brittle. CAPT uses GPT-4o-mini to perform this estimation reliably, with experiments showing performance drops are consistently within 3%, indicating the estimation step only slightly affects original in-domain results.

### Mechanism 3: Prevention of Fine-Tuning Shortcut Injection
Randomizing symbol assignments during fine-tuning prevents the model from memorizing new spurious associations between synthetic symbols and answers. If "Husband" is always mapped to "Symbol A", the model might learn "Symbol A $\rightarrow$ Yes". By randomly assigning symbols (e.g., "A", "C", "Z") for each sample generation, the model cannot form a static statistical shortcut between symbol identity and label. It must learn the *process* of resolving variables rather than memorizing surface patterns.

## Foundational Learning

- **Structural Causal Models (SCMs) & Collider Bias**: The paper frames LLM failures as specific causal error structures (collider bias) where $X$ opens spurious paths between $E$ and $S$. Understanding the directed acyclic graph $E \rightarrow X \leftarrow S$ is essential to why intervention works. Quick check: In $A \rightarrow C \leftarrow B$, why does conditioning on $C$ create spurious association between $A$ and $B$?

- **Backdoor Adjustment / Intervention**: CAPT performs a "do-calculus" intervention on variable $E$ to estimate true effect of $X$ on $Y$ by blocking backdoor paths. This is distinct from simply regressing out variables. Quick check: Why is "regressing out" a variable statistically different from a causal intervention?

- **Chain-of-Thought as Latent Structure Approximation**: The paper treats CoT as an approximation of unobserved logic structure $S$. CAPT aims to force reliance on this $S$ rather than event tokens $E$. Quick check: How does the paper argue Answer-only supervision fails to capture "ensemble over reasoning traces"?

## Architecture Onboarding

- **Component map**: Raw prompt $X$ -> Event Estimator (GPT-4o-mini) -> Symbolic Mapping -> Transformer (Qwen2.5-3B) -> Fine-tuned Model
- **Critical path**: The Event Estimator's accuracy is the bottleneck. If it fails to identify an event, the bias remains in the data and intervention fails. The student model is strictly dependent on the quality of symbolic abstraction.
- **Design tradeoffs**: Converting events to symbols removes semantic bias but also removes world knowledge (e.g., "Husbands are typically adults"). This is safe for formal logic but risky for commonsense tasks. The approach requires expensive inference passes over the entire training dataset.
- **Failure signatures**: Symbol Overfitting (model reasons about "{symbol_1}" but fails when symbol count exceeds training distribution) and Semantic Drift (Event Estimator hallucinates events in OOD data, creating garbage symbolic prompts).
- **First 3 experiments**: 1) Replicate Appendix D.3 comparing "CAPT=order" vs. "CAPT=random" on OOD accuracy. 2) Measure Teacher Model's extraction F1 score on "Anti-sense" prompts. 3) Plot accuracy vs. training samples (100, 200, 500) for SFT vs. CAPT.

## Open Questions the Paper Calls Out

### Open Question 1
Can CAPT be effectively extended to natural language generation tasks and multi-hop reasoning scenarios beyond binary classification? The paper currently focuses on binary classification reasoning tasks; extending it to generation or multi-hop reasoning requires further exploration. This remains unresolved because generation tasks involve open-ended outputs with different event dependencies, and multi-hop reasoning may require reasoning across multiple interconnected events that the current single-intervention approach may not capture.

### Open Question 2
How can CAPT be adapted to handle domains where event boundaries are ambiguous or context-dependent? In domains where event boundaries are ambiguous or context-dependent, more in-context examples may be required. This is unresolved because the current approach assumes clear event definitions extractable via few-shot prompting, but when events are not cleanly separable or their identification depends on contextual interpretation, the event estimation step may become unreliable.

### Open Question 3
How does the symbolic abstraction in CAPT affect performance on tasks that genuinely require contextual or semantic understanding of event names? While the method helps reduce spurious correlations, it may also suppress useful contextual cues in some settings, potentially affecting performance on tasks requiring textual understanding. This remains uncharacterized because by design CAPT replaces event names with abstract symbols to break spurious correlations, but some tasks may legitimately rely on semantic relationships between events.

### Open Question 4
Can the fine-tuned model itself perform reliable event estimation, or does CAPT inherently require a separate, larger model for this preprocessing step? The implementation uses GPT-4o-mini for event estimation while fine-tuning Qwen2.5-3B. The paper states "event estimation is treated as a transferable capability of LLMs" but does not test whether smaller fine-tuned models can perform this estimation without external assistance. This remains unresolved because if preprocessing cannot be performed by smaller models or must remain external, CAPT's practical applicability in resource-constrained settings may be limited.

## Limitations

- Method assumes reasoning logic $S$ is invariant and independent of event identity $E$, which may not hold for commonsense reasoning
- Event estimation reliability depends heavily on teacher model's robustness, with limited quantification of hallucination rates
- Experimental scope is narrow, focusing only on binary classification tasks with limited sample sizes
- The approach may suppress legitimate contextual cues when semantic relationships between events are logically necessary

## Confidence

**High Confidence**: Symbolic intervention blocking backdoor paths is well-grounded in causal inference literature. Ablation study comparing random vs. deterministic symbol assignment provides strong empirical support.

**Medium Confidence**: Sample efficiency claims are supported by experimental results but require validation across more diverse tasks and datasets. The assertion that event estimation is "transferable" is plausible but under-specified.

**Low Confidence**: The claim that CAPT prevents fine-tuning shortcut injection is theoretically sound but lacks direct evidence showing how symbol memorization would manifest in model weights or attention patterns.

## Next Checks

1. **Estimator Robustness Test**: Measure GPT-4o-mini's event extraction accuracy on "Anti-sense" prompts across multiple domains to verify the <3% performance drop claim and assess hallucination rates.

2. **Symbol Memorization Analysis**: Train CAPT with fixed random seeds across multiple runs and examine attention patterns or embedding distances to detect potential symbol-level memorization.

3. **Generalization Stress Test**: Apply CAPT to a multi-class reasoning dataset (e.g., StrategyQA) with 1000+ samples to test scalability beyond binary classification and small sample sizes.