---
ver: rpa2
title: Defense against Prompt Injection Attacks via Mixture of Encodings
arxiv_id: '2504.07467'
source_url: https://arxiv.org/abs/2504.07467
tags:
- prompt
- defense
- base64
- injection
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses prompt injection attacks, where malicious instructions
  embedded in external content manipulate LLM outputs. It proposes a Mixture of Encodings
  defense that encodes external data using multiple character encodings (Base64 and
  Caesar cipher), generates separate LLM responses for each encoding, and aggregates
  the final output.
---

# Defense against Prompt Injection Attacks via Mixture of Encodings

## Quick Facts
- arXiv ID: 2504.07467
- Source URL: https://arxiv.org/abs/2504.07467
- Reference count: 17
- The paper proposes a Mixture of Encodings defense that uses multiple character encodings to protect LLMs from prompt injection attacks while maintaining NLP task performance.

## Executive Summary
This paper addresses the critical security vulnerability of prompt injection attacks, where malicious instructions embedded in external content manipulate large language model (LLM) outputs. The proposed solution employs a "Mixture of Encodings" approach that encodes external data using multiple character encodings (Base64 and Caesar cipher), generates separate LLM responses for each encoding, and aggregates the final output. This method effectively balances safety (reducing attack success rate) and helpfulness (maintaining NLP task performance) by making it harder for attackers to inject malicious prompts while preserving the model's ability to perform standard NLP tasks.

## Method Summary
The Mixture of Encodings defense works by transforming external data through multiple character encoding schemes before passing it to the LLM. The system applies Base64 encoding and Caesar cipher (with configurable shift values) to the external content, generating multiple encoded versions of the same input. Each encoded version is then processed independently by the LLM, producing separate responses. The final output is obtained by aggregating these responses, typically using a mean or majority vote approach. This multi-encoding strategy creates redundancy and makes it difficult for attackers to craft successful injection payloads that work across all encoding schemes simultaneously.

## Key Results
- Achieves one of the lowest attack success rates across four prompt injection attack datasets
- Maintains high performance across nine diverse NLP tasks including text classification, summarization, and question answering
- Outperforms existing character encoding-based defense methods in balancing security and utility
- Shows robustness across different types of prompt injection attacks including direct instruction injection and jailbreak attempts

## Why This Works (Mechanism)
The defense works by exploiting the fact that prompt injection attacks rely on specific textual patterns that trigger malicious behavior in LLMs. By encoding external content through multiple schemes, the attack patterns become obfuscated and harder to recognize. The Base64 encoding transforms readable text into a different character set, while the Caesar cipher shifts characters by a fixed amount. When these encoded inputs are processed separately and aggregated, any single encoding scheme is unlikely to preserve the attack pattern effectively. The aggregation step helps filter out responses that may have been influenced by partially successful attacks, as clean responses from different encodings tend to agree while malicious responses diverge.

## Foundational Learning
- **Character Encoding Schemes** (Base64, Caesar cipher): Essential for understanding how text transformation can obscure attack patterns while preserving semantic content for legitimate tasks
- **Prompt Injection Attack Types**: Direct instruction injection, jailbreak attempts, and context manipulation techniques that the defense must counter
- **Response Aggregation Strategies**: Mean, majority vote, and confidence-weighted methods for combining outputs from multiple encoded inputs
- **LLM Safety vs. Utility Trade-offs**: The fundamental challenge of maintaining model helpfulness while preventing malicious exploitation
- **Multi-modal Input Processing**: Handling the same content through different transformations to create redundancy and resilience

## Architecture Onboarding

**Component Map**: External Content -> Multiple Encoders (Base64, Caesar) -> Parallel LLM Calls -> Response Aggregator -> Final Output

**Critical Path**: The critical path involves encoding external content, making parallel LLM calls for each encoding, and aggregating responses. The encoding step must be fast and deterministic, while the aggregation must effectively combine potentially conflicting responses.

**Design Tradeoffs**: The main tradeoff is between security strength (more encodings provide better protection) and computational cost (more encodings require more LLM calls). The choice of aggregation strategy also involves tradeoffs between simplicity (mean) and sophistication (confidence-weighted voting). Additionally, there's a balance between encoding strength (harder to reverse) and maintaining task performance (easier to understand).

**Failure Signatures**: The defense may fail when attackers can identify and optimize payloads that work across multiple encodings simultaneously, when the aggregation strategy incorrectly weights malicious responses, or when the encoding process introduces noise that degrades NLP task performance. Performance degradation may manifest as reduced accuracy on standard tasks or increased latency.

**3 First Experiments**:
1. Baseline measurement: Run standard NLP tasks without any defense to establish performance benchmarks
2. Single encoding test: Apply only Base64 or only Caesar cipher encoding to measure individual effectiveness
3. Multi-encoding ablation: Test with different numbers of encodings (2, 3, 4) to find the optimal balance of security vs. performance

## Open Questions the Paper Calls Out
None

## Limitations
- The defense adds computational overhead by requiring multiple LLM calls for each input, potentially limiting real-time applications
- The evaluation focuses on synthetic datasets and may not capture all real-world attack variations
- The Caesar cipher with fixed shift values may provide limited security against determined attackers who can attempt to reverse-engineer the encoding parameters

## Confidence

**High confidence**: The empirical evaluation methodology and performance metrics are sound and transparently reported

**Medium confidence**: The claim that this approach achieves "one of the lowest attack success rates" is supported by comparative results but lacks statistical significance testing across multiple runs

**Medium confidence**: The generalizability to production environments and other LLM models is plausible but not empirically validated

## Next Checks
1. Conduct runtime efficiency analysis comparing single-call vs. multi-encoding approaches across different hardware configurations and input sizes
2. Test the defense against adaptive attackers who know the encoding scheme and attempt to optimize injection payloads accordingly
3. Evaluate the approach with different LLM architectures (smaller models, open-source alternatives) and measure performance degradation compared to the GPT-4 baseline