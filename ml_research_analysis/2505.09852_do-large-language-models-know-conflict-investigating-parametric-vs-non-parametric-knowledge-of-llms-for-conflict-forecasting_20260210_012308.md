---
ver: rpa2
title: Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric
  Knowledge of LLMs for Conflict Forecasting
arxiv_id: '2505.09852'
source_url: https://arxiv.org/abs/2505.09852
tags:
- conflict
- llms
- macro
- forecasting
- parametric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether large language models can forecast
  violent conflict using only their pretrained knowledge (parametric) or when augmented
  with external context via retrieval-augmented generation (RAG). Experiments compare
  GPT-4 and LLaMA-2 across conflict-prone regions from 2020-2024, evaluating categorical
  trend prediction (escalate, de-escalate, stable, peace) and fatality estimates.
---

# Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting

## Quick Facts
- arXiv ID: 2505.09852
- Source URL: https://arxiv.org/abs/2505.09852
- Reference count: 10
- Primary result: GPT-4 outperforms LLaMA-2 in conflict forecasting, with RAG augmentation improving accuracy for both models

## Executive Summary
This study investigates whether large language models can forecast violent conflict using either their pretrained knowledge alone or when augmented with external context through retrieval-augmented generation (RAG). The research compares GPT-4 and LLaMA-2 across conflict-prone regions from 2020-2024, evaluating both categorical trend prediction (escalate, de-escalate, stable, peace) and fatality estimates. The experiments demonstrate that GPT-4 consistently outperforms LLaMA-2 in all configurations, while RAG augmentation improves performance for both models, particularly in classification tasks. The findings suggest that LLMs can capture conflict patterns when provided with structured external data, with open-source models showing promise for resource-constrained applications.

## Method Summary
The study employs a comparative experimental design where GPT-4 and LLaMA-2 are tested on their ability to forecast conflict trends and fatality estimates in various regions. The evaluation spans from 2020 to 2024, using both parametric knowledge (pretrained model capabilities) and non-parametric knowledge (RAG-augmented context). Models are assessed on categorical trend prediction tasks (escalate, de-escalate, stable, peace) and quantitative fatality estimation. Performance is measured using macro F1 scores for classification tasks and mean absolute error for fatality predictions, allowing for direct comparison between the two models and their RAG-augmented variants.

## Key Results
- GPT-4 achieved macro F1 scores up to 0.67 for categorical prediction and MAE around 93-177 for fatality estimates
- LLaMA-2 showed limited gains from RAG augmentation compared to GPT-4
- RAG augmentation improved accuracy for both models, especially in classification tasks

## Why This Works (Mechanism)
The superior performance of GPT-4 can be attributed to its larger model capacity and more extensive pretraining on diverse conflict-related data. RAG augmentation works by providing models with access to up-to-date external information that supplements their static knowledge base, allowing them to capture recent developments and contextual factors that influence conflict dynamics. The mechanism appears to involve better pattern recognition when combining pretrained knowledge with retrieved evidence, particularly for categorical predictions where contextual understanding is crucial.

## Foundational Learning
- Conflict pattern recognition: Understanding how LLMs identify temporal and spatial patterns in conflict data is crucial for interpreting forecasting capabilities
- RAG augmentation: The ability to retrieve and incorporate external context enhances model performance by providing up-to-date information beyond pretraining data
- Macro F1 score interpretation: This metric balances precision and recall across multiple classes, essential for evaluating multi-class conflict trend prediction

## Architecture Onboarding
Component map: RAG retrieval -> Context integration -> Model inference -> Output generation
Critical path: External data retrieval → Context embedding → Model processing → Forecast generation
Design tradeoffs: Parametric knowledge offers speed and consistency, while RAG augmentation provides adaptability but introduces retrieval latency and potential noise
Failure signatures: Over-reliance on parametric knowledge may miss recent developments; RAG augmentation can introduce irrelevant or conflicting information
First experiments: 1) Test baseline performance without RAG augmentation, 2) Evaluate different retrieval strategies and data sources, 3) Compare performance across different temporal windows

## Open Questions the Paper Calls Out
- How do different conflict types (civil wars, international disputes, terrorism) affect model performance and forecasting accuracy?
- What is the optimal balance between parametric knowledge and RAG-augmented information for different forecasting tasks?
- How can models be adapted to handle the inherent uncertainty and complexity of conflict dynamics?
- What are the ethical implications of using LLMs for conflict forecasting, particularly regarding bias and accountability?

## Limitations
- Restricted temporal scope (2020-2024) may not capture full complexity of global conflict dynamics
- Limited geographic coverage of conflict regions affects generalizability of findings
- Evaluation metrics may not fully represent real-world forecasting utility
- Potential data quality issues in retrieved information affecting RAG performance
- Model biases from pretraining data may influence conflict pattern recognition

## Confidence
- High confidence in GPT-4 outperforming LLaMA-2 across all configurations
- Medium confidence in overall improvement from RAG augmentation
- Medium confidence in fatality prediction results showing variability
- Low confidence in model generalizability to regions and time periods outside the evaluation scope

## Next Checks
1. Test models across a longer temporal range (pre-2020) to evaluate temporal generalization and potential overfitting to recent conflict patterns
2. Conduct ablation studies isolating effects of RAG augmentation by testing different retrieval strategies and data quality thresholds
3. Validate results with additional open-source models of varying sizes to better understand relationship between model scale and conflict forecasting capability
4. Implement cross-validation across different conflict types to assess model robustness and identify specific failure modes