---
ver: rpa2
title: Securing Large Language Models (LLMs) from Prompt Injection Attacks
arxiv_id: '2512.01326'
source_url: https://arxiv.org/abs/2512.01326
tags:
- prompt
- injection
- jatmo
- adversarial
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated whether JATMO-style fine-tuning can harden
  LLMs against prompt injection attacks by training non-instruction-tuned base models
  on a single summarization task. Models (LLaMA 2-7B, Qwen 1.5-4B, Qwen 1.5-0.5B)
  were fine-tuned using LoRA and tested against a modified HOUYI genetic attack framework
  with content manipulation and information gathering objectives.
---

# Securing Large Language Models (LLMs) from Prompt Injection Attacks

## Quick Facts
- **arXiv ID:** 2512.01326
- **Source URL:** https://arxiv.org/abs/2512.01326
- **Reference count:** 12
- **Primary result:** Task-specific fine-tuning reduces prompt injection success rates by 4-10x, but models remain vulnerable to multilingual and code-triggered attacks.

## Executive Summary
This study evaluates whether JATMO-style fine-tuning can harden LLMs against prompt injection attacks. The authors fine-tune non-instruction-tuned base models (LLaMA 2-7B, Qwen 1.5-4B/0.5B) on a single summarization task using LoRA, then test against a modified HOUYI genetic attack framework. Results show that while JATMO fine-tuned models reduce attack success rates significantly compared to GPT-3.5-Turbo (9.68-25.00% vs 100%), they are not fully immune. A key finding is that higher summarization quality (ROUGE-L) correlates with increased injection vulnerability, revealing a fundamental trade-off between task performance and security. The study demonstrates that while task-specific fine-tuning improves robustness, it does not eliminate injection vulnerabilities, suggesting the need for layered defense strategies.

## Method Summary
The study fine-tuned non-instruction-tuned base models on Amazon All-Beauty reviews using a task-only dataset (no instruction templates). LLaMA 2-7B, Qwen1.5-4B, and Qwen1.5-0.5B were fine-tuned via LoRA, while GPT-3.5-Turbo served as baseline. The HOUYI genetic attack framework was modified to target two attack intents: content manipulation (appending "pwned") and information gathering (revealing current date). Attackers generated 72 adversarial prompts per attack type using mutation/crossover across generations, with custom fitness scoring based on token detection. Model robustness was evaluated using Attack Success Rate (ASR) and summarization quality via ROUGE-L.

## Key Results
- JATMO fine-tuned models reduced attack success rates by 4-10x compared to GPT-3.5-Turbo (100% vulnerable)
- Attack success rates ranged from 9.68% to 25.00% across models and attack types
- Clear trade-off emerged: higher ROUGE-L summarization quality (0.29-0.88) correlated with increased injection susceptibility
- Residual vulnerabilities persisted to multilingual disruptors and code-triggered shifts

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning non-instruction-tuned base models on task-only data reduces their susceptibility to adversarial commands. By training exclusively on input-output pairs containing task content without explicit instructional phrasing, the model learns to map inputs to outputs without developing generalized instruction-following circuits. Injected prompts are then processed as data rather than commands.

### Mechanism 2
A trade-off exists where higher generation quality (ROUGE-L) correlates with increased injection vulnerability. Models that better learn task patterns also develop stronger implicit instruction-following from pretraining priors. Improved task fidelity suggests the model attends more to input semantics—including embedded adversarial commands.

### Mechanism 3
Residual instruction-following persists in non-instruction-tuned models due to pretraining on web-scale corpora containing imperative language patterns. Pretraining data (StackOverflow, wikiHow, tutorials) embeds behavioral priors (e.g., responding to "append," "ignore"). Task-specific fine-tuning does not fully erase these patterns, leaving exploitable surfaces—especially for multilingual or code-triggered disruptors.

## Foundational Learning

- **Concept: Prompt Injection Attack Structure**
  - **Why needed here:** Understanding the Framework-Separator-Disruptor composition (HOUYI's attack template) is essential to interpret why certain disruptors bypass JATMO defenses.
  - **Quick check question:** Can you sketch how an adversarial "ignore previous instructions" prompt would be structured within a summarization task input?

- **Concept: Instruction-Tuned vs. Base Models**
  - **Why needed here:** JATMO's core hypothesis depends on the distinction: base models lack supervised instruction-following training, which is hypothesized to reduce injection susceptibility.
  - **Quick check question:** What is the expected behavioral difference when a base model versus an instruction-tuned model receives "Summarize this text:" as a prefix?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA)**
  - **Why needed here:** The study used LoRA for all fine-tuning due to hardware constraints; understanding this is critical for reproducing or adapting the approach.
  - **Quick check question:** Why does LoRA keep base model weights frozen, and what implication does this have for retaining (or erasing) pretraining behaviors?

## Architecture Onboarding

- **Component map:** Dataset Preparation Pipeline -> JATMO Fine-Tuning Module -> HOUYI Attack Engine -> Evaluation Layer
- **Critical path:** Prepare instruction-free task dataset -> Fine-tune base model with LoRA -> Generate 72 adversarial prompts via HOUYI -> Run attacks and compute ASR/ROUGE-L
- **Design tradeoffs:** Task specificity vs. flexibility (JATMO models locked to single task); Quality vs. robustness (higher ROUGE-L increases vulnerability); LoRA vs. full fine-tuning (LoRA preserves pretraining priors)
- **Failure signatures:** Multilingual disruptors can bypass learned task patterns; code-triggered shifts activate code-generation mode; imperative phrasing residuals trigger compliance despite no instruction fine-tuning
- **First 3 experiments:** 1) Baseline replication with 10 manually crafted adversarial prompts to confirm reduced vulnerability; 2) Ablation on dataset instruction contamination comparing instruction-free vs. instruction-prefixed data; 3) Probe residual instruction-following with targeted test set of imperative phrasings and code-trigger keywords

## Open Questions the Paper Calls Out
None

## Limitations
- Exact LoRA hyperparameters and HOUYI genetic algorithm parameters were not specified, making precise replication challenging
- The correlation between ROUGE-L quality and injection vulnerability lacks independent validation beyond this single study
- Effectiveness against real-world adversarial strategies beyond the controlled HOUYI framework remains untested

## Confidence
- **High confidence**: Experimental methodology is sound and ASR reduction results (9.68-25.00% vs 100% baseline) are reproducible with proper implementation
- **Medium confidence**: Mechanism explaining why task-specific fine-tuning reduces injection vulnerability is plausible but not definitively proven
- **Medium confidence**: Residual vulnerability analysis is supported by observations but requires systematic testing across more diverse attack vectors

## Next Checks
1. Independent replication of the ROUGE-L/vulnerability trade-off across different base models and task domains
2. Systematic probe of residual instruction-following with comprehensive test battery covering multilingual imperatives and code-related disruptors
3. Ablation on pretraining data filtering to quantify contribution of instructional patterns to residual vulnerabilities