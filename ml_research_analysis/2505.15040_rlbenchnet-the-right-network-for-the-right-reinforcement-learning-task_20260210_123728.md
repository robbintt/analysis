---
ver: rpa2
title: 'RLBenchNet: The Right Network for the Right Reinforcement Learning Task'
arxiv_id: '2505.15040'
source_url: https://arxiv.org/abs/2505.15040
tags:
- mamba
- memory
- tasks
- lstm
- mamba-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates the performance of various
  neural network architectures in reinforcement learning tasks using PPO as the base
  algorithm. The study compares MLP, LSTM, GRU, Transformer-XL, GTrXL, Mamba, and
  Mamba-2 across continuous control, discrete decision-making, and memory-intensive
  environments.
---

# RLBenchNet: The Right Network for the Right Reinforcement Learning Task

## Quick Facts
- arXiv ID: 2505.15040
- Source URL: https://arxiv.org/abs/2505.15040
- Authors: Ivan Smirnov; Shangding Gu
- Reference count: 34
- Primary result: Mamba models achieve 4.5× higher throughput than LSTM and 3.9× higher than GRU while maintaining comparable performance

## Executive Summary
This paper systematically evaluates the performance of various neural network architectures in reinforcement learning tasks using PPO as the base algorithm. The study compares MLP, LSTM, GRU, Transformer-XL, GTrXL, Mamba, and Mamba-2 across continuous control, discrete decision-making, and memory-intensive environments. Key findings include: Mamba models achieve 4.5× higher throughput than LSTM and 3.9× higher than GRU while maintaining comparable performance; MLP excels in fully observable continuous control tasks; recurrent architectures like LSTM and GRU perform well in partially observable environments; and only Transformer-XL, GTrXL, and Mamba-2 successfully solve the most challenging memory-intensive tasks, with Mamba-2 requiring 8× less memory than Transformer-XL. The study provides practical guidelines for selecting neural architectures based on task characteristics and computational constraints.

## Method Summary
The study benchmarks PPO with seven neural architectures (MLP, LSTM, GRU, Transformer-XL, GTrXL, Mamba, Mamba-2) across four environment categories: MuJoCo continuous control, Atari discrete decision-making, masked classic control, and MiniGrid memory tasks. Architectures were scaled to approximately equal parameters and trained using CleanRL implementations. The evaluation measured average episode return, training throughput (steps-per-second), inference latency, and GPU memory usage. Domain-specific hyperparameters were tuned for each architecture, with MiniGrid observation windows reduced to 3×3 and classic control tasks using masked velocities to test partial observability.

## Key Results
- Mamba models achieve 4.5× higher throughput compared to LSTM and 3.9× higher than GRU while maintaining comparable performance
- MLP provides optimal balance of efficiency and performance in fully observable continuous control tasks
- Only Transformer-XL, GTrXL, and Mamba-2 successfully solve the most challenging memory-intensive tasks
- Mamba-2 requires 8× less memory than Transformer-XL for long-horizon tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Mamba and Mamba-2 achieve higher training throughput than recurrent and transformer baselines by leveraging hardware-aware selective state-space models (SSMs) rather than attention or standard recurrence.
- **Mechanism**: SSMs utilize a selective scan mechanism that processes sequences in linear time (O(L)) and maintains a constant memory state, avoiding the quadratic complexity of self-attention and the sequential bottleneck of standard RNN unrolling.
- **Core assumption**: The efficiency gains hold under the specific hardware configuration tested (NVIDIA RTX A5000) and the specific PPO implementation (CleanRL).
- **Evidence anchors**: Section 4.5 notes Mamba achieves 2734 SPS vs. LSTM at 604 SPS, attributing this to the efficiency of the selective scan mechanism.
- **Break condition**: Performance advantages may degrade if the sequence length is extremely short (overhead dominates) or if the implementation forces frequent synchronization.

### Mechanism 2
- **Claim**: Advanced memory architectures (Transformer-XL, Mamba-2) outperform recurrent baselines (LSTM/GRU) in long-horizon credit assignment by maintaining longer effective context windows.
- **Mechanism**: Transformer-XL uses segment-level recurrence with relative positional encoding, while Mamba-2 uses an expanded state dimension ($d\_state$). This allows the policy to reference states from significantly earlier timesteps than the limited hidden state of a standard LSTM can retain.
- **Core assumption**: The task requires correlating current rewards with actions taken many steps prior (e.g., finding a key then a door), which exceeds the capacity of standard recurrence.
- **Evidence anchors**: Section 4.4 shows LSTM/GRU failing to learn in MiniGrid-Memory-S11 (score ~0.49), while Mamba-2 reaches ~0.96.
- **Break condition**: If the environment requires strict adherence to recent history over ancient history (short-term dependency), these models may overfit or suffer from noise dilution compared to simpler RNNs.

### Mechanism 3
- **Claim**: MLPs provide the optimal balance of efficiency and performance in fully observable continuous control because the Markov property of the state renders explicit memory mechanisms redundant.
- **Mechanism**: In MuJoCo tasks (e.g., Walker2d), the observation vector typically contains velocities and positions sufficient to determine the optimal action. MLPs map this static state directly to action distributions without the computational overhead of maintaining or updating temporal states.
- **Core assumption**: The environment state is fully observable or can be approximated as Markovian by the policy.
- **Evidence anchors**: Figure 1 and Section 4.1 show MLP matching or beating complex architectures in Walker2d and HalfCheetah.
- **Break condition**: If partial observability is introduced (e.g., masked velocities as in LunarLander Masked), MLP performance collapses, as seen in Section 4.3.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: This is the base algorithm used for all benchmarks. Understanding the balance between the clip objective, value function loss, and entropy bonus is critical to interpreting why certain architectures stabilize faster than others.
  - Quick check question: How does the PPO clipping mechanism prevent the policy from changing too drastically during an update?

- **Concept: Partial Observability & POMDPs**
  - Why needed here: The paper categorizes tasks by observability. You must understand the difference between an MDP (where current state implies optimal action) and a POMDP (where history is required) to select the correct architecture.
  - Quick check question: In a POMDP, why is feeding only the current observation to an MLP insufficient for optimal control?

- **Concept: State Space Models (SSMs) vs. Attention**
  - Why needed here: Mamba and Mamba-2 are benchmarked as efficient alternatives to Transformers. Understanding that SSMs compress history into a fixed-size state (like RNNs) but allow parallel training (like Transformers) explains their high throughput.
  - Quick check question: Why does Mamba's inference cost remain constant with sequence length, unlike a Transformer?

## Architecture Onboarding

- **Component map**: Input Layer -> Trunk (MLP/LSTM/GRU/TrXL/Mamba) -> Policy Head and Value Head
- **Critical path**: 
  1. Profile the target environment (Observability & Horizon)
  2. If Markovian -> Select MLP
  3. If Partially Observable + Short Horizon -> Select LSTM/GRU
  4. If Long Horizon Memory -> Select Mamba-2 or GTrXL
  5. Train using PPO with architecture-specific learning rates (lower for Mamba)
- **Design tradeoffs**:
  - **Mamba vs. LSTM**: Mamba offers 4.5x speed but risks state leakage across episodes (implementation detail noted in Section 3.1)
  - **Transformer-XL vs. Mamba-2**: Transformer-XL is highly stable but memory-intensive; Mamba-2 is 8x more memory efficient but requires careful tuning of state size ($d\_state$)
  - **Stacked MLP (PPO-4)**: A cheap alternative to recurrence for short-term history, but lacks the theoretical capacity of RNNs for long dependencies
- **Failure signatures**:
  - **Mamba State Leakage**: If Mamba fails to reset hidden states between episodes, it may overfit to training sequence orders or show instability in episodic tasks
  - **Transformer Overfitting**: GTrXL/TrXL may converge slowly or overfit to noise in simple environments (e.g., CartPole) due to excessive capacity
  - **Recurrent Gradient Issues**: LSTM/GRU may plateau in long-horizon tasks (MiniGrid Memory-S11) due to vanishing gradients over long sequences
- **First 3 experiments**:
  1. **Baseline Validation**: Train PPO-MLP on Walker2d (Markovian) to establish a reference for stability and throughput on your hardware
  2. **Memory Stress Test**: Compare LSTM vs. Mamba-2 on MiniGrid-Memory-S11 to verify the "long-horizon" advantage and measure the GPU memory gap (expected 8x reduction for Mamba)
  3. **Throughput Profiling**: Run a fixed step count (e.g., 1M steps) on a complex Atari game (e.g., Breakout) using GRU vs. Mamba to validate the 3.9x throughput claim in a vision-based context

## Open Questions the Paper Calls Out

- **Open Question 1**: Does implementing proper hidden state resets in Mamba architectures degrade their sample efficiency or asymptotic performance in episodic memory tasks?
  - **Basis in paper**: Section 4.8 notes the current implementation allows information leakage between episodes and requires technical improvement to add proper resets.
  - **Why unresolved**: The authors acknowledge that the current lack of resets may confound performance results, as the model might be leveraging information it should not have access to across episode boundaries.
  - **What evidence would resolve it**: A comparative benchmark of "leaky" versus strictly episodic Mamba implementations on the MiniGrid Memory-S11 and DoorKey tasks.

- **Open Question 2**: Do the relative performance and efficiency rankings of Mamba-2 over Transformers persist when applied to off-policy reinforcement learning algorithms like SAC or TD3?
  - **Basis in paper**: Section 4.8 suggests extending the evaluation beyond PPO to other algorithms like SAC and TD3.
  - **Why unresolved**: PPO's on-policy nature interacts differently with recurrent and state-space mechanisms than the replay buffers and off-policy gradients used in algorithms like SAC.
  - **What evidence would resolve it**: Benchmark results showing training throughput and final returns for Mamba, LSTM, and Transformer-XL within an SAC or TD3 framework on continuous control tasks.

- **Open Question 3**: Can the performance gap between Transformer-XL and Mamba-2 in long-horizon tasks be closed through specific hyperparameter tuning, or are the observed deficits structural?
  - **Basis in paper**: Section 4.8 states that a more thorough hyperparameter optimization process beyond default CleanRL settings could provide a fairer comparison.
  - **Why unresolved**: The authors relied on standard hyperparameters to ensure reproducibility, but Transformer-based models are often highly sensitive to learning rates and context lengths, potentially understating their performance.
  - **What evidence would resolve it**: A study utilizing grid search or Bayesian optimization specifically for Transformer-XL in the MiniGrid-Memory environments to find optimal settings.

## Limitations

- The Mamba state leakage issue suggests potential instability in episodic tasks that wasn't fully resolved
- The comparison assumes equal parameter counts, but architectural differences in how parameters are utilized may affect real-world deployment decisions
- The hardware-specific performance metrics (NVIDIA RTX A5000) may not translate directly to other GPU architectures or consumer-grade hardware

## Confidence

- **High Confidence**: MLP performance in fully observable continuous control tasks, Mamba throughput advantages over LSTM/GRU, and the general architectural guidelines for task categorization
- **Medium Confidence**: Mamba-2's memory efficiency advantage over Transformer-XL, and the failure of LSTM/GRU on long-horizon memory tasks
- **Low Confidence**: The relative ranking of Transformer-XL vs. GTrXL vs. Mamba-2 for memory-intensive tasks beyond the MiniGrid benchmark, and the specific hyperparameter sensitivity of Mamba models

## Next Checks

1. Implement and test proper hidden state reset mechanisms for Mamba across episodic boundaries to verify claims about state leakage and episodic performance
2. Reproduce the MiniGrid-Memory-S11 benchmark on a different GPU architecture (e.g., NVIDIA A100 or consumer RTX 4090) to validate memory usage claims and throughput measurements
3. Conduct ablation studies varying observation window sizes in MiniGrid to determine the minimum requirements for Mamba-2 to maintain its performance advantage over recurrent baselines