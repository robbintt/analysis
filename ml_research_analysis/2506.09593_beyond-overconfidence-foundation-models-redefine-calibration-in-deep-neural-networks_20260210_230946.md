---
ver: rpa2
title: 'Beyond Overconfidence: Foundation Models Redefine Calibration in Deep Neural
  Networks'
arxiv_id: '2506.09593'
source_url: https://arxiv.org/abs/2506.09593
tags:
- calibration
- distribution
- datasets
- post-hoc
- shift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study challenges the conventional understanding of neural
  network calibration by demonstrating that current-generation models (ConvNeXt, EVA,
  BEiT) exhibit systematic underconfidence in in-distribution predictions, contrasting
  with the overconfidence reported for earlier architectures. Through comprehensive
  evaluation across ImageNet, ImageNet-C, ImageNet-V2, ImageNet-A, and four biomedical
  imaging datasets, the research reveals that these modern models show reduced calibration
  error under distribution shifts but are highly responsive to post-hoc calibration
  techniques in in-distribution settings.
---

# Beyond Overconfidence: Foundation Models Redefine Calibration in Deep Neural Networks

## Quick Facts
- **arXiv ID:** 2506.09593
- **Source URL:** https://arxiv.org/abs/2506.09593
- **Reference count:** 40
- **Primary result:** Modern vision models exhibit systematic underconfidence in-distribution, with distribution shifts partially compensating this miscalibration

## Executive Summary
This study challenges the conventional understanding of neural network calibration by demonstrating that current-generation models (ConvNeXt, EVA, BEiT) exhibit systematic underconfidence in in-distribution predictions, contrasting with the overconfidence reported for earlier architectures. Through comprehensive evaluation across ImageNet, ImageNet-C, ImageNet-V2, ImageNet-A, and four biomedical imaging datasets, the research reveals that these modern models show reduced calibration error under distribution shifts but are highly responsive to post-hoc calibration techniques in in-distribution settings. However, the effectiveness of such techniques diminishes under severe distribution shifts and can even become counterproductive. Critically, calibration insights derived from web-scraped benchmarks like ImageNet show limited transferability to specialized biomedical domains, where convolutional architectures consistently outperform transformer-based counterparts. These findings highlight the complex, non-monotonic effects of architectural and training innovations on calibration, challenging simple narratives of continuous improvement.

## Method Summary
The study evaluates calibration properties of foundation models across five model families (ResNet, ViT, Swin Transformer, BEiT, EVA, ConvNeXt) using Expected Calibration Error (ECE) with 15 equal-mass bins. Models are sourced from the timm library with specified pretraining configurations. Post-hoc calibration methods (Temperature Scaling, Ensemble Temperature Scaling, Isotonic Regression, Spline-based) are fit on a 10% calibration split and applied to test data. Evaluation spans ImageNet in-distribution and distribution-shifted datasets (ImageNet-C with 19 corruptions × 5 severity levels, ImageNet-V2, ImageNet-A), plus four biomedical imaging datasets (BUSI, HAM10000, OCT, Pneumonia) using two-stage transfer learning with frozen feature extraction followed by progressive fine-tuning.

## Key Results
- Current-generation models exhibit systematic underconfidence in in-distribution settings, with ECE ranging from 0.081-0.147
- Distribution shifts partially compensate for underconfidence, reducing ECE on ImageNet-V2 by 0.022-0.037 for modern models
- Post-hoc calibration methods are highly effective in-distribution but can degrade performance under severe distribution shifts
- Convolutional architectures consistently outperform transformers in biomedical imaging calibration tasks
- Reliability diagrams reveal calibration error patterns are consistent across datasets but vary in magnitude by domain

## Why This Works (Mechanism)

### Mechanism 1: Underconfidence-Shift Compensation Effect
- **Claim:** Current-generation models exhibit systematic underconfidence in-distribution, which distribution shifts partially offset by increasing confidence relative to accuracy, thereby reducing ECE.
- **Mechanism:** Distribution shifts typically increase model confidence relative to accuracy. For models already biased toward underconfidence, this shift moves them closer to calibration rather than further away.
- **Evidence anchors:** [Page 7] distribution shift increases confidence relative to accuracy; [Page 8, Figure 4] Current-generation models show reduced ECE on ImageNet-V2 (△ECE = -0.022 to -0.037) while traditional models show increased ECE (△ECE = +0.016 to +0.051).

### Mechanism 2: Post-hoc Calibration Distribution Mismatch
- **Claim:** Post-hoc calibration methods calibrated on in-distribution validation data become unreliable or counterproductive under severe distribution shifts because the learned transformation does not generalize to shifted confidence-accuracy mappings.
- **Mechanism:** Methods like Temperature Scaling learn a mapping from confidence scores to empirical accuracy on a validation set. Under severe covariate shift, this relationship changes, so the calibration transformation systematically misaligns.
- **Evidence anchors:** [Page 10] under severe distribution shifts, post-hoc calibration methods can degrade calibration performance to levels worse than those of uncalibrated models; EVA at severity 5 shows TS produces △ECE = +0.034, exceeding uncalibrated baseline.

### Mechanism 3: Architectural Inductive Bias–Domain Alignment
- **Claim:** Convolutional architectures outperform transformer-based counterparts in biomedical imaging calibration because convolutional inductive biases (local pattern recognition, translation equivariance) align better with the structure of standardized medical images.
- **Mechanism:** Biomedical images are acquired under standardized protocols. Convolutional architectures' inherent bias toward local features, edge detection, and texture analysis transfers more effectively from ImageNet pretraining to these constrained imaging contexts than transformers' tokenized global representations.
- **Evidence anchors:** [Page 12] architectures founded on convolutional principles demonstrate consistent superiority in both accuracy and calibration metrics across all biomedical tasks; attributed to multi-scale feature extraction, transferable edge detection and texture analysis, and inherent bias toward local pattern recognition.

## Foundational Learning

- **Concept: Expected Calibration Error (ECE)**
  - **Why needed here:** Primary metric throughout the paper; interprets whether confidence matches accuracy.
  - **Quick check question:** If a model has ECE = 0.09, what does that mean in plain terms?

- **Concept: Reliability Diagrams**
  - **Why needed here:** Visual diagnostic for overconfidence vs. underconfidence; used to identify systematic bias direction.
  - **Quick check question:** In a reliability diagram, what does it mean if the curve lies below the diagonal?

- **Concept: Post-hoc Calibration (Temperature Scaling, Isotonic Regression, Spline)**
  - **Why needed here:** Paper evaluates these methods; understanding their input-output transformations clarifies why they fail under shift.
  - **Quick check question:** Why does Temperature Scaling use only a single scalar parameter, and what are the tradeoffs?

## Architecture Onboarding

- **Component map:** Input -> Pretrained backbone (ConvNeXt / BEiT / EVA / ResNet / ViT) -> Softmax logits -> Post-hoc calibrator (optional) -> Calibrated probabilities -> ECE computation via confidence binning

- **Critical path:** 1) Select architecture based on deployment domain (convolutional for biomedical, transformer acceptable for natural images) 2) Apply post-hoc calibration on held-out in-distribution validation split 3) Evaluate on both in-distribution test and shifted datasets 4) If severe shift expected, prefer architectural robustness over post-hoc calibration reliance

- **Design tradeoffs:**
  - **ConvNeXt:** Best robustness under shift; higher in-distribution ECE but highly responsive to TS; recommended for deployment
  - **ViT/Swin:** Better inherent in-distribution calibration; Swin shows limited responsiveness to post-hoc methods
  - **BEiT/EVA:** Strong accuracy but underconfidence requires correction; calibration degrades sharply under severe shift

- **Failure signatures:**
  - Post-hoc calibration increasing ECE on shifted data (△ECE > 0) -> severe distribution mismatch
  - Persistent high ECE after TS on in-distribution data (Swin Transformer pattern) -> architectural calibration rigidity
  - Transformer underperforming CNN on biomedical tasks -> inductive bias misalignment

- **First 3 experiments:**
  1. **Baseline calibration audit:** Compute ECE and reliability diagrams for uncalibrated model on in-distribution validation; identify overconfidence vs. underconfidence bias.
  2. **Post-hoc method sweep:** Apply TS, ETS, IRM, SPL on calibration split; report ECE reduction and method stability; select simplest effective method.
  3. **Shift robustness probe:** Evaluate uncalibrated and calibrated models on at least two shift severities (e.g., ImageNet-C levels 1 and 5); confirm whether calibration remains beneficial or becomes counterproductive.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific roles do architectural design, pretraining scale, and regularization play in causing the systematic underconfidence observed in current-generation models?
- **Basis in paper:** [explicit] The authors state that "disentangling their individual contributions would require a comprehensive follow-up study" and that the interaction between these factors remains "unexplored."
- **Why unresolved:** The current study characterizes the underconfidence phenomenon but lacks the large-scale controlled ablations necessary to isolate the specific cause (e.g., masked image modeling vs. model size).
- **What evidence would resolve it:** Ablation studies training models from scratch while independently varying architecture types, dataset scales, and regularization strategies to observe the isolated effect on calibration error.

### Open Question 2
- **Question:** How can post-hoc calibration techniques be redesigned to remain robust or non-detrimental under severe distribution shifts?
- **Basis in paper:** [inferred] The paper demonstrates that methods like Temperature Scaling become "counterproductive" and "degrade calibration performance" under severe shifts (e.g., ImageNet-A), creating a need for methods that do not fail in these scenarios.
- **Why unresolved:** Existing methods optimize for in-distribution validation sets, which results in negative transfer when the test distribution differs significantly from the calibration set.
- **What evidence would resolve it:** The development of calibration algorithms that enforce constraints based on corruption invariance or utilize worst-case optimization, showing stable or improved ECE on ImageNet-A/C severity 5.

### Open Question 3
- **Question:** Why do convolutional architectures consistently yield superior calibration compared to transformers in biomedical transfer learning scenarios?
- **Basis in paper:** [explicit] The authors note the "consistent superiority" of convolutional models in medical contexts is likely due to "inherent bias toward local pattern recognition," but contrast this with transformer success in natural images, leaving the exact transferability limits undefined.
- **Why unresolved:** The paper identifies the performance gap but does not conduct the mechanistic analysis required to confirm if tokenization or global attention mechanisms are inherently ill-suited for the standardized nature of medical imaging.
- **What evidence would resolve it:** Mechanistic interpretability studies comparing feature attribution and uncertainty estimation between CNNs and Vision Transformers on standardized medical imaging datasets.

## Limitations
- Findings based on standard ImageNet benchmarks may not generalize to other high-performing architectures trained with different objectives
- Domain-specific calibration advantages of convolutional architectures in biomedical imaging require validation across more diverse medical imaging modalities
- Mechanisms explaining calibration behavior under distribution shift are theoretically grounded but lack direct empirical dissection
- Alternative calibration formulations (e.g., distribution-aware or adaptive methods) were not explored

## Confidence
- **High:** Empirical observations of systematic underconfidence in current-generation models on ImageNet and shift-induced ECE reduction patterns
- **Medium:** Domain-specific calibration advantages of convolutional architectures in biomedical imaging, based on a limited set of four datasets
- **Medium:** Mechanisms linking underconfidence and shift compensation, supported by indirect evidence but requiring deeper theoretical and empirical validation
- **Low:** Generalizability of calibration insights across broader model families and domain shifts beyond those tested

## Next Checks
1. Test whether underconfidence persists in models trained with alternative objectives (e.g., self-supervised or contrastive methods) or on non-ImageNet data
2. Evaluate calibration robustness under synthetic and real-world covariate shifts in biomedical domains to confirm convolutional advantages
3. Investigate adaptive calibration methods (e.g., domain-aware temperature scaling) to quantify the feasibility of mitigating shift-induced degradation