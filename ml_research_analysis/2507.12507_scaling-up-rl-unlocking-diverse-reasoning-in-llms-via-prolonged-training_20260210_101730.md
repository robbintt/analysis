---
ver: rpa2
title: 'Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training'
arxiv_id: '2507.12507'
source_url: https://arxiv.org/abs/2507.12507
tags:
- training
- reasoning
- entropy
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates prolonged reinforcement learning for reasoning-focused
  language models, addressing key challenges such as entropy collapse, exploration-exploitation
  balance, and performance plateaus. The authors introduce several critical components:
  decoupled clipping and dynamic sampling from DAPO to maintain exploration, controlled
  KL regularization to stabilize training from a pretrained model, and periodic reference
  policy resets to overcome stagnation.'
---

# Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training

## Quick Facts
- arXiv ID: 2507.12507
- Source URL: https://arxiv.org/abs/2507.12507
- Reference count: 35
- Extended RL training with DAPO enhancements yields +14.7% on math, +13.9% on coding, +54.8% on logic puzzles

## Executive Summary
This work demonstrates that prolonged reinforcement learning training with carefully designed stabilization techniques can significantly improve reasoning capabilities in small language models. The authors address key challenges including entropy collapse, exploration-exploitation balance, and performance plateaus through decoupled clipping, dynamic sampling, KL regularization, and periodic reference policy resets. Their approach yields substantial improvements across diverse reasoning tasks, showing that even 1.5B parameter models can achieve strong performance through extended training and better RL design.

## Method Summary
The authors employ GRPO with DAPO enhancements to train a 1.5B parameter model (DeepSeek-R1-Distill-Qwen-1.5B) on diverse verifiable reasoning tasks. Key innovations include decoupled clipping with asymmetric bounds (ϵ_low=0.2, ϵ_high=0.4), dynamic sampling that filters trivial/impossible prompts, KL regularization (β=1e-4), and periodic reference policy resets. Training uses n=16 rollouts per prompt at temperature 1.2, with context window gradually extended from 8k to 16k tokens. The process spans 8 sequential runs with hard resets triggered by validation degradation or KL spikes.

## Key Results
- +14.7% improvement on math reasoning (AIME, AMC, MATH benchmarks)
- +13.9% improvement on coding tasks (Codeforces, LiveCodeBench)
- +54.8% improvement on logic puzzles (graph_color, sudoku, hitori)

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Combining a small KL divergence penalty with periodic reference policy resets enables stable prolonged training that would otherwise plateau or degrade.
**Mechanism:** The KL penalty (β=0.0001) prevents the online policy from drifting too far from a stable reference, mitigating overfitting to spurious reward signals. However, over time this penalty dominates and restricts learning. Resetting the reference policy π_ref to a recent snapshot of π_θ releases this constraint and restores learning dynamics.
**Core assumption:** The base model is already well-initialized and capable of coherent chain-of-thought outputs; KL regularization is less beneficial for training from raw pretrained models.
**Evidence anchors:**
- [Section 3.3.1] "Periodically, we hard-reset the reference policy π_ref to a more recent snapshot of the online policy π_θ, and reinitialize the optimizer states."
- [Section 5.3, Figure 6] Shows sharp Codeforces degradation in extended Run 1, recovered via hard reset.
- [corpus] ProRL (2505.24864) similarly finds prolonged RL expands reasoning boundaries, supporting extended training validity.

### Mechanism 2
**Claim:** Decoupled clipping with asymmetric bounds (ϵ_low=0.2, ϵ_high=0.4) preserves entropy and delays mode collapse compared to symmetric PPO clipping.
**Mechanism:** Standard PPO clips the importance sampling ratio symmetrically, which can suppress exploration. Setting a higher upper bound (ϵ_high=0.4) enables "clip-higher," allowing the policy to upweight previously low-probability tokens. This maintains output diversity critical for GRPO's group-based advantage estimation.
**Core assumption:** Entropy collapse is the primary bottleneck; simply increasing sampling temperature delays but does not prevent it.
**Evidence anchors:**
- [Section 3.3] "By setting a higher value for ϵ_high, the algorithm promotes 'clip-higher', uplifting the probabilities of previously unlikely tokens."
- [Section 5.2, Figure 5b] Shows higher entropy maintained with decoupled clipping compared to standard settings.

### Mechanism 3
**Claim:** Dynamic sampling that filters trivial (accuracy=1) and impossible (accuracy=0) prompts increases effective sample efficiency.
**Mechanism:** Prompts where the model always succeeds or fails provide zero advantage signal under GRPO's group-relative formulation. Filtering these focuses gradient updates on intermediate-difficulty examples where relative advantages are meaningful, increasing signal density per batch.
**Core assumption:** The verifier reliably distinguishes correct from incorrect outputs; reward noise is minimal.
**Evidence anchors:**
- [Section 3.3] "DAPO employs dynamic sampling, filtering out prompts for which the model consistently succeeds or fails (i.e., accuracy 1 or 0), as these provide no learning signal."
- [Section 5.2, Figure 5c] Shows faster validation improvement with dynamic vs. static sampling.

## Foundational Learning

- **Concept: GRPO (Group Relative Policy Optimization)**
  - **Why needed here:** This is the core RL algorithm replacing PPO's value model with group-based advantage estimation. Understanding how advantages are computed from group rewards is essential for debugging training dynamics.
  - **Quick check question:** Can you explain why GRPO samples multiple responses per prompt and how the advantage A(τ) is normalized?

- **Concept: Entropy Collapse**
  - **Why needed here:** The primary training instability this paper addresses. Without intervention, output distributions sharpen prematurely, destroying exploration.
  - **Quick check question:** What happens to the advantage estimates in GRPO when all sampled responses become nearly identical?

- **Concept: KL Divergence as Regularization**
  - **Why needed here:** The paper explicitly contradicts recent work recommending KL removal; understanding the trade-off is critical for hyperparameter decisions.
  - **Quick check question:** Why might a KL penalty be beneficial when starting from a strong distilled model but not from a raw pretrained model?

## Architecture Onboarding

- **Component map:** verl framework -> GRPO implementation -> DeepSeek-R1-Distill-Qwen-1.5B -> Sandboxed reward servers (math-verify, code execution, GPT-4o judge, Reasoning Gym verifiers, IFEval) -> Validation benchmarks (AIME, Codeforces, GPQA, IFEval, graph_color)

- **Critical path:**
  1. Deploy sandboxed reward servers with multiprocessing (critical for code execution safety)
  2. Configure GRPO with decoupled clipping (ϵ_low=0.2, ϵ_high=0.4) and KL penalty (β=1e-4)
  3. Set rollout temperature=1.2, n=16 samples per prompt, context window=8k
  4. Monitor KL divergence, entropy, response length, and validation scores per domain
  5. Trigger reference policy reset when KL spikes or validation plateaus

- **Design tradeoffs:**
  - Higher temperature (1.2) improves exploration but yields lower initial rewards; necessary early-stage
  - Small KL penalty stabilizes but must be reset periodically to avoid constraining learning
  - 8k context window limits reasoning chain length but enables faster iteration; extend to 16k late-stage

- **Failure signatures:**
  - Sudden validation drop on specific domain (e.g., Codeforces) → likely overfitting, trigger reset
  - Response length explosion with repeated text → add termination penalty (reward shaping)
  - Entropy near zero with flat validation → entropy collapse, increase ϵ_high or temperature

- **First 3 experiments:**
  1. **Temperature ablation:** Compare T=0.6 vs T=1.2 on early-stage training (reward trajectory, AIME2024 score). Expect T=1.2 to start lower but overtake.
  2. **Clipping configuration ablation:** Test symmetric (0.2/0.2) vs decoupled (0.2/0.4) clipping on entropy preservation and final validation scores.
  3. **Reference reset timing:** Train until validation plateaus, then trigger hard reset vs. continue without reset. Compare recovery dynamics on held-out benchmarks.

## Open Questions the Paper Calls Out

- **Does the efficacy of high rollout sampling temperatures (1.2) generalize to larger model architectures?**
  - The paper notes this observation "may be model-dependent" and requires further analysis, as the ablation was restricted to 1.5B parameters.

- **What data composition is required to overcome the "cold start" limitations in complex reasoning domains like ARC or games?**
  - The paper states failures in tasks like ARC stem from lack of "core reasoning skills" or background knowledge, leaving data enhancements for future work.

- **Can the timing of reference policy resets be automated or theoretically grounded rather than relying on manual heuristics?**
  - While the paper validates reset utility, it describes resets as manual interventions based on observing validation declines or KL spikes, without proposing a formal automated trigger.

## Limitations

- The reference policy reset mechanism lacks direct ablation studies to show whether resets provide net benefit or merely enable recovery from suboptimal policies.
- The 8-run training schedule appears somewhat arbitrary with unknown criteria for when exactly to trigger each reset.
- Comparison to baseline RL model is limited to only 1B parameters, preventing conclusions about whether these techniques scale to larger models.

## Confidence

- **High Confidence:** The decoupled clipping mechanism (ϵ_low=0.2, ϵ_high=0.4) maintains entropy and improves exploration, directly demonstrated in Figure 5b with clear quantitative comparisons.
- **Medium Confidence:** The KL regularization benefit for prolonged training is supported by recovery results but lacks direct ablation studies.
- **Medium Confidence:** Dynamic sampling improves sample efficiency, with Figure 5c showing faster initial improvement but unclear long-term impact.

## Next Checks

1. **Reference Reset Ablation:** Train a parallel model for the same total steps without any reference policy resets. Compare validation trajectories and final scores to determine if resets provide net benefit or merely enable recovery from suboptimal policies.

2. **Clipping Bound Sensitivity:** Systematically vary ϵ_low and ϵ_high values (e.g., [0.1,0.3] vs [0.3,0.5]) to identify optimal ranges and test whether the benefits persist beyond the specific 0.2/0.4 configuration used.

3. **Temperature vs Entropy Trade-off:** Conduct controlled experiments at multiple temperatures (0.6, 1.0, 1.2, 1.5) measuring not just final rewards but also entropy trajectories, advantage variance, and sample efficiency to quantify the exploration-exploitation balance more precisely.