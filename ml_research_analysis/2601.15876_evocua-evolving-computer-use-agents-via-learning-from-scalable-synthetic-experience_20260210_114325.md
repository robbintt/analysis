---
ver: rpa2
title: 'EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic
  Experience'
arxiv_id: '2601.15876'
source_url: https://arxiv.org/abs/2601.15876
tags:
- learning
- experience
- evocua
- agent
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EvoCUA, a computer-use agent that evolves
  through verifiable synthetic experience rather than static imitation. The key innovation
  is integrating verifiable task synthesis, large-scale sandbox rollouts, and iterative
  learning into a self-sustaining cycle.
---

# EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience

## Quick Facts
- arXiv ID: 2601.15876
- Source URL: https://arxiv.org/abs/2601.15876
- Reference count: 40
- Key outcome: EvoCUA-32B achieves 56.7% success rate on OSWorld, setting new open-source state-of-the-art and outperforming larger models like OpenCUA-72B (45.0%)

## Executive Summary
EvoCUA introduces a novel approach to computer-use agents that evolves through verifiable synthetic experience rather than static imitation. The system integrates verifiable task synthesis, large-scale sandbox rollouts, and iterative learning into a self-sustaining cycle. By co-generating tasks with executable validators and orchestrating tens of thousands of concurrent rollouts, EvoCUA achieves state-of-the-art performance with a 32B model on the OSWorld benchmark, demonstrating a scalable path for advancing native agent capabilities.

## Method Summary
EvoCUA employs a dual-stage learning process combining rejection sampling fine-tuning to consolidate successful trajectories and step-level preference optimization to correct failures. The system generates verifiable synthetic tasks with executable validators to ensure strict environmental grounding. An asynchronous infrastructure manages tens of thousands of concurrent sandbox rollouts, creating a scalable synthetic experience pipeline. This evolving paradigm consistently improves performance across different model scales, establishing a new benchmark for open-source computer use agents.

## Key Results
- EvoCUA-32B achieves 56.7% success rate on OSWorld benchmark
- Outperforms OpenCUA-72B (45.0%) and closed-weights UI-TARS-2 (53.1%)
- Demonstrates state-of-the-art performance among open-source models
- Shows consistent performance improvement across model scales

## Why This Works (Mechanism)
The system's effectiveness stems from its self-evolving nature, where verifiable synthetic experience replaces static imitation learning. The combination of task co-generation with executable validators ensures environmental grounding, while the dual-stage learning process (rejection sampling + preference optimization) enables precise capability refinement. The asynchronous infrastructure enables massive parallel rollouts, creating a scalable feedback loop that continuously improves agent performance through verifiable experience.

## Foundational Learning
1. **Verifiable Task Synthesis**: Generating tasks with executable validators ensures strict environmental grounding
   - Why needed: Prevents distribution shift between training and deployment environments
   - Quick check: Can the generated tasks be validated automatically without human intervention?

2. **Rejection Sampling Fine-tuning**: Consolidates successful trajectories while filtering out failures
   - Why needed: Focuses learning on productive behaviors rather than random exploration
   - Quick check: What percentage of rollouts survive the rejection sampling threshold?

3. **Step-level Preference Optimization**: Corrects specific failure modes at granular level
   - Why needed: Enables targeted refinement of agent capabilities
   - Quick check: Can the system identify and prioritize the most critical failure types?

4. **Asynchronous Rollout Orchestration**: Enables concurrent execution of thousands of sandbox environments
   - Why needed: Scales synthetic experience generation to practical levels
   - Quick check: What is the throughput limit for concurrent rollouts on target hardware?

## Architecture Onboarding
**Component Map**: Task Generator -> Sandbox Executor -> Validator -> Learning Pipeline -> Updated Agent

**Critical Path**: Synthetic task generation → Concurrent sandbox execution → Validation → Dual-stage learning → Performance evaluation

**Design Tradeoffs**: The system trades computational overhead for improved generalization through massive synthetic experience generation, rather than relying on curated human demonstrations.

**Failure Signatures**: 
- Insufficient task diversity leading to overfitting
- Validator mismatch with real-world environments
- Bottlenecks in asynchronous infrastructure limiting rollout throughput

**First Experiments**:
1. Validate task generation diversity across different application domains
2. Test validator accuracy by comparing synthetic vs real-world performance
3. Benchmark computational efficiency at different concurrency levels

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic experience pipeline may not capture real-world edge cases and user variability
- Generalizability to open-domain tasks beyond structured benchmarks remains uncertain
- Computational overhead and cost-effectiveness compared to static imitation approaches warrant further investigation

## Confidence
- High: EvoCUA-32B achieves 56.7% success rate vs OpenCUA-72B at 45.0% (direct benchmark comparison)
- Medium: Generalizability to open-domain tasks beyond curated benchmarks
- Medium: Scalability assertion across model families (limited ablation studies)

## Next Checks
1. Conduct robustness testing on out-of-distribution tasks and adversarial scenarios to assess generalization beyond benchmark performance
2. Perform ablation studies isolating the impact of verifiable task synthesis versus large-scale rollouts on final performance
3. Evaluate computational efficiency metrics (training time, resource utilization) compared to static imitation baselines across multiple hardware configurations