---
ver: rpa2
title: Mixture-of-Experts Operator Transformer for Large-Scale PDE Pre-Training
arxiv_id: '2510.25803'
source_url: https://arxiv.org/abs/2510.25803
tags:
- uni00000013
- uni00000014
- datasets
- uni00000048
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of heterogeneous PDE datasets
  causing interference in neural operator pre-training and the high computational
  cost of dense models. To solve this, it proposes MoE-POT, a sparse Mixture-of-Experts
  architecture that uses a router-gating network to dynamically select 4 routed experts
  from 16, plus 2 shared experts, for efficient specialization and generalization.
---

# Mixture-of-Experts Operator Transformer for Large-Scale PDE Pre-Training

## Quick Facts
- arXiv ID: 2510.25803
- Source URL: https://arxiv.org/abs/2510.25803
- Authors: Hong Wang; Haiyang Xin; Jie Wang; Xuanze Yang; Fei Zha; Huanshuo Dong; Yan Jiang
- Reference count: 40
- One-line primary result: Sparse MoE-POT achieves up to 40% lower zero-shot error than dense models with 90M activated parameters vs 120M.

## Executive Summary
This paper addresses the challenge of heterogeneous PDE datasets causing interference in neural operator pre-training and the high computational cost of dense models. To solve this, it proposes MoE-POT, a sparse Mixture-of-Experts architecture that uses a router-gating network to dynamically select 4 routed experts from 16, plus 2 shared experts, for efficient specialization and generalization. Pre-trained models with 90M activated parameters achieve up to 40% lower zero-shot error than dense models with 120M activated parameters on 6 public PDE datasets. The router network also achieves 98% accuracy in classifying PDE types, validating the interpretability and effectiveness of the MoE design.

## Method Summary
MoE-POT uses a Mixture-of-Experts architecture where a router-gating network dynamically selects 4 routed experts from 16, plus 2 shared experts, to process heterogeneous PDE datasets. The model employs auto-regressive denoising training with noise injection and includes a load balancing loss to prevent expert collapse. It uses Fourier Neural Operator layers mixed with MoE layers, trained on 6 datasets (FNO-NS 1e-5/1e-3, PDEBench-CNS/SWE/DR, CFDBench) with resolution standardized to H=128. The training objective minimizes one-step prediction loss plus load balancing loss over 1000 epochs.

## Key Results
- MoE-POT with 90M activated parameters achieves up to 40% lower zero-shot error than dense models with 120M activated parameters
- Router network achieves 98% accuracy in classifying PDE types
- Inference time of MoE-POT-M (489M total) is comparable to DPOT-M (158M dense), validating efficient compute-parameter decoupling

## Why This Works (Mechanism)

### Mechanism 1: Interference Reduction via Feature Specialization
If the router successfully partitions inputs based on PDE characteristics, then gradient updates from heterogeneous datasets update disjoint sets of parameters, reducing negative transfer. The architecture separates experts into 16 "routed" experts and 2 "shared" experts. The router selects Top-4 routed experts per input. Shared experts capture universal dynamics (e.g., conservation laws), while routed experts specialize in equation-specific features (e.g., compressibility vs. incompressibility). Distinct PDE types possess learnable feature differences that a linear projection (the router) can distinguish from tokenized inputs.

### Mechanism 2: Compute-Parameter Decoupling
Increasing total parameter count (experts) while keeping activated parameter count (Top-K) constant improves model capacity without increasing inference latency. The MoE layer replaces a dense Feed-Forward Network. While the total parameters scale with the number of experts ($N_r$), the FLOPs scale only with the number of selected experts ($K$). This allows the model to memorize diverse physical dynamics without the quadratic cost of dense scaling. The GPU memory overhead of storing inactive experts does not bottleneck the system.

### Mechanism 3: Emergent Router Semantics
The router weights act as an implicit classifier for physical systems, validating that the network learns physically meaningful representations rather than overfitting to noise. The router minimizes the prediction loss ($L2$), not a classification loss. To minimize error across heterogeneous data, it must route similar physics to the same experts. This results in distinct gating distributions for different PDEs (e.g., Navier-Stokes vs. Shallow Water). There exists a correlation between input distribution statistics and the optimal expert for that distribution.

## Foundational Learning

- **Concept: Neural Operators (Fourier Layer)**
  - Why needed: The backbone of MoE-POT is not a standard Transformer MLP but a Fourier Neural Operator (FNO) layer mixed with MoE. You must understand spectral convolution (Eq. 6) to grasp how spatial derivatives are processed.
  - Quick check: Can you explain why the Fourier layer uses a low-pass filter (modes) and how Eq. 7 approximates the integral kernel?

- **Concept: Auto-regressive Denoising**
  - Why needed: The training objective (Sec 2.2) is predicting the next time-step $u_T$ from $u_{<T}$ with noise injection. This differs from standard next-token prediction.
  - Quick check: Why does injecting noise $\epsilon$ into the input frames improve robustness (Sec 2.2)?

- **Concept: Load Balancing Loss (CV Loss)**
  - Why needed: MoE models suffer from "router collapse" where one expert handles everything. The paper uses a specific Coefficient of Variation (CV) loss (Eq. 12) to prevent this.
  - Quick check: How does the CV loss term specifically penalize a distribution where Expert 1 has 99% importance and others 0%?

## Architecture Onboarding

- **Component map:** Input (spatiotemporal tensor) -> Encoder (Patchify + Positional Embedding + Temporal Aggregation) -> Trunk (N Blocks of [Fourier Layer -> MoE Layer]) -> Output (Projection layer)

- **Critical path:**
  1. Router Gating: G(x) outputs logits
  2. Top-K Selection: Select indices and weights for top 4 logits
  3. Expert Compute: Pass input through 2 Shared + 4 Selected Experts
  4. Aggregation: Weighted sum (Eq. 10)

- **Design tradeoffs:**
  - Patch Size (P=8): Larger patches reduce token count (faster) but lose high-frequency spatial details
  - Expert Count (N_r=16): More experts allow finer specialization but increase VRAM usage without increasing inference speed
  - Shared vs. Routed: Removing shared experts caused instability; they provide a "safe baseline" for ambiguous inputs

- **Failure signatures:**
  - Training Collapse: Loss diverges or plateaus high early on. Usually caused by router collapse (check expert usage histogram)
  - Negative Transfer: Zero-shot error is higher on a specific dataset when trained on "Mixed" data vs. "Single" data (Sec 3.1)
  - Instability: Using MLPs instead of CNNs for experts failed completely on this data type

- **First 3 experiments:**
  1. Router Behavior Audit: Run inference on 2 distinct datasets (e.g., NS vs. SWE). Extract the router indices from Block 4. Verify they are different (Fig 2)
  2. Ablation on Top-K: Train MoE-POT-Tiny with Top-K=1 vs Top-K=4 (Table 4) to verify the performance/efficiency tradeoff
  3. Inference Speed Benchmark: Compare forward pass time of MoE-POT-S (90M active) vs a dense DPOT-M (122M active) to validate the sparse advantage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the mathematical essence of the router-gating network's ability to classify PDE types?
- Basis: The conclusion states "we have yet to analyze the mathematical essence of this classification mechanism"
- Why unresolved: While the paper empirically demonstrates that the router can classify datasets with 98% accuracy, it lacks a theoretical explanation of how the gating weights mathematically correlate with specific physical properties or equation parameters
- What evidence would resolve it: A theoretical analysis or visualization linking specific dimensions of the router's logit space to physical parameters (e.g., viscosity, boundary conditions) rather than just dataset labels

### Open Question 2
- Question: How can the router's implicit PDE classification guide the construction of more effective pre-training datasets?
- Basis: The conclusion identifies "exploring how PDE classification can guide the construction of more effective pre-training datasets" as an important future direction
- Why unresolved: The current work focuses on training on a fixed set of 6 datasets; it does not investigate how to use the model's clustering of experts to inform data selection, such as identifying under-represented physical regimes
- What evidence would resolve it: Experiments demonstrating that using router-based data sampling or dataset augmentation improves scaling laws or convergence speed compared to random sampling

### Open Question 3
- Question: How does varying the number of shared experts (fixed at 2 in this study) affect the trade-off between generalization and specialization?
- Basis: The method section fixes the number of shared experts at 2 (N_s=2) and the ablation study varies routed experts and Top-K, but never varies the number of shared experts
- Why unresolved: It is unclear if 2 shared experts are sufficient to capture all "universal physical principles" or if increasing them would further stabilize training on highly heterogeneous data
- What evidence would resolve it: An ablation study showing model performance and router stability when varying shared experts (e.g., 0, 1, 2, 4) while keeping total parameters constant

## Limitations

- The exact CNN architecture for experts is unspecified, requiring assumptions for reproduction
- The noise injection scale Ïµ is not provided, with only the distribution form given
- The paper does not directly measure gradient interference between experts, though it claims MoE reduces interference

## Confidence

- **High confidence**: Efficiency gains of MoE scaling and router's PDE classification ability (98% accuracy), supported by ablation studies and comparative benchmarks
- **Medium confidence**: MoE-POT reduces interference - while the router achieves high classification accuracy and the model outperforms dense baselines, the paper does not directly measure gradient interference between experts
- **Medium-Low confidence**: Specific mechanism of shared vs. routed expert specialization - the paper provides limited empirical analysis of what each expert type learns

## Next Checks

1. **Interference Validation**: Train a dense model on mixed PDE data and measure gradient cosine similarity between updates from different datasets. Compare this to MoE-POT's expert-specific gradients to directly verify interference reduction.

2. **Expert Function Analysis**: Extract and analyze the learned weights of individual experts by passing synthetic inputs with known PDE characteristics through each expert separately. Visualize their outputs to validate that routed experts specialize in equation-specific features while shared experts capture universal dynamics.

3. **Router Robustness Test**: Create adversarial inputs that are weighted combinations of two different PDE types. Measure whether the router's Top-4 selection smoothly interpolates between the expert combinations used for each pure PDE type, or if it collapses to a single combination.