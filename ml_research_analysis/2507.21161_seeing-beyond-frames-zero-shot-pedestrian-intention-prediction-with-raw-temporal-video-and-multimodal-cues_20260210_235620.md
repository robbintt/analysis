---
ver: rpa2
title: 'Seeing Beyond Frames: Zero-Shot Pedestrian Intention Prediction with Raw Temporal
  Video and Multimodal Cues'
arxiv_id: '2507.21161'
source_url: https://arxiv.org/abs/2507.21161
tags:
- pedestrian
- video
- prediction
- intention
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BF-PIP introduces a zero-shot pedestrian crossing intention prediction
  framework that processes continuous video clips with structured metadata using Gemini
  2.5 Pro. Unlike prior methods relying on static frame sequences or extensive training,
  BF-PIP leverages multimodal prompts incorporating short video segments, bounding
  box annotations, and ego-vehicle speed.
---

# Seeing Beyond Frames: Zero-Shot Pedestrian Intention Prediction with Raw Temporal Video and Multimodal Cues

## Quick Facts
- arXiv ID: 2507.21161
- Source URL: https://arxiv.org/abs/2507.21161
- Reference count: 20
- Primary result: 73% accuracy on JAAD dataset, outperforming GPT-4V baseline by 18% and leading MLLM approach by 6%

## Executive Summary
BF-PIP introduces a zero-shot pedestrian crossing intention prediction framework that processes continuous video clips with structured metadata using Gemini 2.5 Pro. Unlike prior methods relying on static frame sequences or extensive training, BF-PIP leverages multimodal prompts incorporating short video segments, bounding box annotations, and ego-vehicle speed. Evaluated on the JAAD dataset, BF-PIP achieves 73% accuracy, outperforming the GPT-4V baseline by 18% and the leading MLLM approach by 6%. Ablation studies confirm the importance of annotated video and vehicle speed metadata. The approach demonstrates strong generalization and robust intent prediction in diverse traffic scenarios without retraining.

## Method Summary
BF-PIP employs a two-stage multimodal prompt structure to query Gemini 2.5 Pro for pedestrian crossing intention prediction. The framework ingests 16-frame video clips (approximately 0.5 seconds) ending 30 frames before a crossing event, along with bounding box coordinates and ego-vehicle speed metadata. Videos are uploaded to Google Cloud Storage and processed through the Vertex AI API. The prompt combines role-play framing (model as "autonomous vehicle") with chain-of-thought reasoning instructions, requiring explicit analysis of pedestrian posture, gaze, and environmental context before outputting a single-word binary prediction. The system operates in zero-shot mode without gradient updates, relying entirely on the pre-trained capabilities of Gemini 2.5 Pro.

## Key Results
- Achieves 73% accuracy on JAAD-beh test set, outperforming GPT-4V baseline (55%) by 18% and OmniPredict (67%) by 6%
- Annotated Video + Speed configuration yields highest performance, demonstrating the value of visual bounding box overlays and vehicle speed metadata
- Ablation studies confirm that unannotated video drops accuracy to 65%, while adding raw bounding box coordinates to unannotated video further reduces performance to 60%
- Zero-shot approach generalizes across diverse traffic scenarios without dataset-specific fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
Processing continuous video clips enables superior capture of motion dynamics compared to discrete frame sequences. The framework leverages Gemini 2.5 Pro's native video processing to ingest a 16-frame observation window as a continuous temporal stream, preserving motion continuity and allowing detection of subtle behavioral cues like hesitation and gaze changes that are often lost when sampling static keyframes.

### Mechanism 2
Integrating ego-vehicle speed with visual data grounds the prediction in physical context, disambiguating crossing intent. The prompt structure fuses visual inputs with heterogeneous metadata—specifically ego-vehicle speed and bounding boxes—forcing the model to reason about the dynamic relationship between the pedestrian and the moving vehicle.

### Mechanism 3
Structured role-play prompting enforces consistent behavioral reasoning and output formatting. The system employs a two-stage prompt defining the model's role as an "autonomous vehicle" and enforcing explicit reasoning steps before the binary output, channeling the model's generative capacity into a specific observational strategy rather than open-ended description.

## Foundational Learning

- **Concept: Zero-Shot Generalization**
  - Why needed here: BF-PIP relies entirely on pre-trained knowledge without gradient updates, so performance depends on the pre-training corpus covering similar traffic scenarios
  - Quick check question: If the model encounters a traffic sign or pedestrian behavior not present in its pre-training data, will it fail gracefully or hallucinate?

- **Concept: Time-To-Event (TTE)**
  - Why needed here: The task is strictly defined as predicting intent 30 frames (1 second) before the event, critical for slicing the JAAD dataset correctly
  - Quick check question: Does extracting the video segment at TTE=20 instead of TTE=30 change the fundamental nature of the prediction task from "intent" to "action recognition"?

- **Concept: Multimodal Context Fusion**
  - Why needed here: The system combines video (high-dimensional pixels), bounding boxes (spatial coordinates), and speed (scalar/categorical value), with the primary engineering challenge being serialization into a single prompt
  - Quick check question: Should ego-vehicle speed be provided as a raw number or a semantic description for best MLLM performance?

## Architecture Onboarding

- **Component map:** Data Loader -> Prompt Constructor -> Gemini 2.5 Pro API -> Output Parser
- **Critical path:** The formatting of the "Annotated Video" is critical. The paper suggests rendering bounding boxes into the video pixels (visual overlay) rather than just providing coordinates in text.
- **Design tradeoffs:**
  - Annotated vs. Unannotated Video: Rendering boxes (AV) improves spatial grounding but bakes in detector errors permanently
  - Raw Coordinates vs. Visual Overlay: Providing raw text coordinates alongside unannotated video actually hurt performance in some ablation cases
- **Failure signatures:**
  - Hallucinated intent: High confidence "Crossing" prediction when the pedestrian is clearly static but the ego-vehicle is moving fast
  - Metadata ignoring: Model describes the scene accurately but outputs a prediction contradictory to the vehicle speed trend
  - Format break: Model generates an explanation instead of the required JSON/single-word output
- **First 3 experiments:**
  1. Reproduce AV+S Baseline: Render bounding boxes onto 16-frame clips, include ego-speed in the prompt, verify ~73% accuracy
  2. Input Sensitivity Test (Ablation): Run "UV + BB" configuration to confirm performance drop validating model preference for visual overlays
  3. TTE Robustness Check: Shift observation window to TTE=15 frames to see if "zero-shot" performance degrades

## Open Questions the Paper Calls Out
- How does BF-PIP performance generalize to geographically diverse datasets like PIE or to adverse environmental conditions not present in JAAD?
- Can the cloud-dependent inference pipeline satisfy the strict real-time latency requirements necessary for autonomous vehicle safety systems?
- Why does the inclusion of raw bounding box coordinates degrade performance when paired with unannotated video?
- Does the model's reliance on role-play prompting and chain-of-thought introduce instability or hallucination in edge cases with ambiguous pedestrian behavior?

## Limitations
- Exact prompt template and bounding box rendering parameters are unspecified, making exact replication difficult
- Performance boost from ego-vehicle speed is validated but encoding method and handling contradictory cues are not detailed
- Zero-shot generalization claim is strong but robustness to unseen scenarios or detector failures is not tested

## Confidence
- **High Confidence:** Ablation study results (AV+S = 73% accuracy) and general superiority of annotated video over unannotated
- **Medium Confidence:** Mechanisms for why continuous video processing and metadata fusion work are logically sound but lack deep empirical validation
- **Low Confidence:** Efficacy of role-play prompting strategy is asserted but lacks direct citations or ablation testing

## Next Checks
1. Reproduce the AV+S Baseline: Render bounding boxes onto 16-frame clips, include ego-speed in the prompt, verify the claimed ~73% accuracy on the JAAD-beh test set using the specified GCP setup
2. Input Sensitivity Test (Ablation): Run the "UV + BB" configuration to confirm the performance drop (ACC 0.60) reported in Table II
3. TTE Robustness Check: Shift the observation window to TTE=15 frames (0.5s) to see if the "zero-shot" performance degrades