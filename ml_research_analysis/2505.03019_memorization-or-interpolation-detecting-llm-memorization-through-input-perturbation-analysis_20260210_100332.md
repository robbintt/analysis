---
ver: rpa2
title: Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation
  Analysis
arxiv_id: '2505.03019'
source_url: https://arxiv.org/abs/2505.03019
tags:
- memorization
- data
- training
- humaneval
- sensitivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PEARL, a framework that detects memorization
  in Large Language Models (LLMs) by measuring sensitivity to input perturbations.
  The core idea is that memorized content exhibits high sensitivity to small input
  changes, leading to drastic performance drops, while generalization maintains stable
  performance.
---

# Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis

## Quick Facts
- arXiv ID: 2505.03019
- Source URL: https://arxiv.org/abs/2505.03019
- Reference count: 16
- Primary result: Introduces PEARL, a framework detecting LLM memorization via input perturbation sensitivity, achieving 60/100 memorized instances in HumanEval and 42/100 in Bible dataset

## Executive Summary
This paper presents PEARL (Perturbation Sensitivity for Memorization Detection), a black-box framework that detects memorization in Large Language Models by measuring sensitivity to input perturbations. The core insight is that memorized content exhibits high sensitivity to small input changes, leading to drastic performance drops, while generalization maintains stable performance. Applied to GPT-4o and Pythia models, PEARL successfully identified memorization across multiple datasets, with sensitivity thresholds controlling false positives. The approach offers a practical method for assessing memorization risks without requiring access to model internals or training data.

## Method Summary
PEARL works by generating perturbed versions of inputs through bit-flipping at varying intensities (0-5%), prompting the LLM multiple times per perturbation level, and measuring performance degradation using distance metrics like Normalized Compression Distance (NCD) for completion tasks or ROUGE-L for summarization. The framework quantifies sensitivity by computing the maximum performance falloff between consecutive perturbation intensities. Memorization is flagged when sensitivity exceeds a calibrated threshold α, which is set using data known to be outside the training set to control false positive rates.

## Key Results
- PEARL identified 60 memorized instances out of 100 HumanEval samples in GPT-4o
- Successfully detected 42 memorized Bible instances and 5 NY Times articles
- Task-dependence confirmed: only 6 Bible instances detected during summarization vs. 42 in completion
- False positive rate of 0.04 achieved with threshold α=0.2 on calibration data

## Why This Works (Mechanism)

### Mechanism 1
Memorized training data exhibits high sensitivity to small input perturbations, producing abrupt performance drops beyond a threshold perturbation intensity. Memorized sequences are stored as precise input-output mappings, so when perturbed, the exact retrieval cue is disrupted, forcing reliance on generalization the model may lack. This causes nonlinear performance falloff rather than gradual degradation. Evidence shows abrupt decreases at ~2% perturbation for memorized content versus regular degradation for non-training text.

### Mechanism 2
Completion tasks reveal memorization more reliably than summarization tasks. Completion requires reproducing exact token sequences while summarization requires semantic abstraction. Memorization is more detectable when the task demands verbatim reproduction rather than transformation. Evidence shows only 6/100 Bible instances detected in summarization versus 42 in completion.

### Mechanism 3
Sensitivity threshold α can be calibrated on known non-training data to control false positive rates. By running PEARL on data confirmed outside the training set, practitioners set α where false positives are acceptably low, then apply this threshold to suspect data. Evidence shows α=0.2 achieves FPR=0.04 on calibration data.

## Foundational Learning

- **Concept: Memorization vs. Interpolation**
  - Why needed here: The entire framework distinguishes these two behaviors; without understanding the difference, PEARL's purpose is opaque.
  - Quick check question: Given a model that completes "To be or not to..." with "...that is the question," how would you determine if this is memorization or interpolation?

- **Concept: Perturbation-based Robustness Testing**
  - Why needed here: PEARL applies controlled input corruptions (bit flips) and measures output degradation; understanding this paradigm is prerequisite to implementation.
  - Quick check question: Why might bit-flip perturbations at the token-encoding level be more diagnostic than synonym substitutions for detecting memorization?

- **Concept: Membership Inference and Black-box Detection**
  - Why needed here: PEARL operates without model internals or confirmed training data labels; it's a black-box membership inference proxy.
  - Quick check question: What information does PEARL require that traditional white-box membership inference attacks do not?

## Architecture Onboarding

- **Component map:** Input → Bit-flip perturbation → Multi-sample prompting → Distance metric calculation → Sensitivity aggregation → Threshold comparison
- **Critical path:** Input → Perturbation injection → Multi-sample prompting → Distance computation → Sensitivity aggregation → Threshold decision
- **Design tradeoffs:** Higher α reduces false positives but may miss true memorization; lower α catches more cases but increases noise. Bit-flip perturbations are task-agnostic but may not be optimal for all content types; semantic perturbations could improve detection but require more compute. Completion tasks are more diagnostic but not applicable to all use cases; summarization is broadly applicable but less sensitive.
- **Failure signatures:** High sensitivity on calibration (non-training) data → α is too low or calibration data is contaminated. Near-zero sensitivity on known training data → perturbation intensity range may be insufficient. High variance across i samples → model temperature too high or task is inherently stochastic.
- **First 3 experiments:** 1) Replicate Pythia-410m validation: Run PEARL on 100 Pile samples and 100 RefinedWeb samples with α=0.2; confirm order-of-magnitude difference in detection rates. 2) Threshold sweep: Vary α from 0.1 to 0.5 on calibration data; plot FPR curve to select task-specific thresholds. 3) Cross-task comparison: Run both completion and summarization tasks on the same 50 Bible samples; quantify overlap in detected memorization instances.

## Open Questions the Paper Calls Out

### Open Question 1
How can PEARL's sensitivity threshold be calibrated for closed-source models when reliable data known to be outside the training set is unavailable? The current methodology relies on assumptions about unseen datasets, which may not hold for all proprietary models. A calibration technique using synthetic data or intrinsic model statistics that does not rely on external "known negative" datasets would resolve this.

### Open Question 2
Can the framework be adapted to detect memorization in tasks where performance is robust to input perturbations, such as summarization? The current metric relies on performance falloffs associated with verbatim reproduction, failing to capture memorization that aids semantic tasks. A modified metric that detects memorization through semantic consistency or internal activation patterns rather than output edit distance would resolve this.

### Open Question 3
To what extent does the specific perturbation strategy (e.g., bit-flips vs. synonym replacement) affect the accuracy of memorization detection? Different perturbation types may trigger different failure modes in memorized vs. generalized learning. Ablation studies comparing the true positive/false positive rates of PEARL using various perturbation functions on the same model and dataset would resolve this.

## Limitations

- The framework assumes perturbation sensitivity cleanly separates memorization from generalization, which may not hold for all data distributions
- Calibration requires representative non-training data, yet distributional shifts could invalidate thresholds
- The approach has not been validated on instruction-tuned or multimodal models, limiting generalizability

## Confidence

- **High Confidence**: Memorization detection works for completion tasks on text models (validated across Pythia, GPT-4o, multiple datasets)
- **Medium Confidence**: Task-dependence is real but mechanism not fully explained (only 6/100 Bible instances detected in summarization)
- **Medium Confidence**: Threshold calibration approach is sound but requires careful validation on representative non-training data
- **Low Confidence**: Bit-flip perturbations are optimal for all content types (not tested against semantic perturbations)
- **Low Confidence**: Method generalizes to non-text modalities or instruction-tuned models (no validation)

## Next Checks

1. **Distributional Robustness**: Apply PEARL to datasets with varying degrees of overlap with training data (e.g., C4, RealNews, and post-training news articles) to quantify false positive/negative rates across realistic scenarios.

2. **Perturbation Mechanism Comparison**: Implement semantic perturbations (synonym replacement, sentence reordering) alongside bit-flips on the same memorized instances to determine which perturbation type provides superior discrimination.

3. **Cross-Modality Validation**: Apply PEARL to code models (e.g., CodeLlama) and multimodal models (e.g., GPT-4V) to assess whether the perturbation-sensitivity framework extends beyond text-only LLMs.