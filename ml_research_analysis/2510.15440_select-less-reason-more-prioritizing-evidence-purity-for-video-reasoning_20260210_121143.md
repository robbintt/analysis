---
ver: rpa2
title: 'Select Less, Reason More: Prioritizing Evidence Purity for Video Reasoning'
arxiv_id: '2510.15440'
source_url: https://arxiv.org/abs/2510.15440
tags:
- arxiv
- video
- reasoning
- frames
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of long-form video reasoning in
  Video Large Language Models, where static uniform frame sampling leads to information
  dilution and obscures critical evidence. The authors propose a novel evidence-prioritized
  adaptive framework called EARL (Evidence-Aware Reinforcement Learning), built on
  the core philosophy of "Select Less, Reason More." The method dynamically selects
  the most relevant frames and performs localized re-sampling around key frames to
  access fine-grained temporal detail, guided by a multi-component reward system that
  enforces evidence purity.
---

# Select Less, Reason More: Prioritizing Evidence Purity for Video Reasoning

## Quick Facts
- **arXiv ID:** 2510.15440
- **Source URL:** https://arxiv.org/abs/2510.15440
- **Reference count:** 40
- **Primary result:** EARL achieves state-of-the-art performance on long-form video reasoning benchmarks with a 7B model

## Executive Summary
This paper addresses the challenge of long-form video reasoning in Video Large Language Models, where static uniform frame sampling leads to information dilution and obscures critical evidence. The authors propose EARL (Evidence-Aware Reinforcement Learning), a novel framework that dynamically selects the most relevant frames and performs localized re-sampling around key frames to access fine-grained temporal detail. Built on the core philosophy of "Select Less, Reason More," EARL uses a multi-component reward system enforcing evidence purity to transform the model into an active interrogator of evidence. Extensive experiments on five demanding video reasoning benchmarks demonstrate significant performance improvements over existing approaches.

## Method Summary
EARL tackles long-form video reasoning by implementing a dynamic evidence selection strategy that prioritizes frame relevance over uniform coverage. The framework uses reinforcement learning with IoU-based rewards to ensure selected frames are highly relevant to the query, while performing localized re-sampling around key frames to capture fine-grained temporal details. The multi-component reward system enforces evidence purity, guiding the model to focus on the most informative visual evidence rather than processing all available frames indiscriminately. This approach addresses the fundamental limitation of static uniform sampling, which often dilutes important information across too many frames.

## Key Results
- Achieves 59.8% on LongVideoBench, 69.0% on MVBench, and 64.9% on VideoMME with a 7B model
- Demonstrates state-of-the-art performance among open-source Video LLMs on five demanding video reasoning benchmarks
- Successfully learns an effective and high-purity visual evidence selection policy

## Why This Works (Mechanism)
The EARL framework works by fundamentally changing how video evidence is selected and processed. Instead of uniformly sampling frames across the entire video duration, EARL actively interrogates the video content by dynamically selecting frames most relevant to the query. The reinforcement learning approach allows the model to learn which frames contain the most critical evidence for answering questions. The IoU-based reward system ensures that selected frames have high overlap with relevant visual content, while the localized re-sampling around key frames provides access to temporal details that might be missed with single-frame selection. This evidence-prioritized approach reduces information dilution and allows the model to focus computational resources on the most informative portions of the video.

## Foundational Learning
- **Reinforcement Learning for Frame Selection**: Needed to enable the model to learn optimal evidence selection policies; Quick check: Does the reward function properly incentivize selecting frames with high relevance?
- **IoU-based Relevance Scoring**: Required to quantify frame relevance to queries; Quick check: Are the IoU thresholds appropriately calibrated for different video domains?
- **Adaptive Temporal Re-sampling**: Essential for capturing fine-grained details around key moments; Quick check: Does re-sampling maintain temporal coherence and avoid redundancy?
- **Evidence Purity Metrics**: Critical for measuring the quality of selected frames; Quick check: How does evidence purity correlate with downstream reasoning performance?
- **Long-form Video Processing**: Necessary to handle extended temporal contexts; Quick check: What are the computational limits for video length?

## Architecture Onboarding

**Component Map**
Evidence Selection Module -> Reward Computation Module -> Frame Re-sampling Module -> Reasoning Module

**Critical Path**
Query -> Evidence Selection -> Frame Re-sampling -> Temporal Reasoning -> Answer Generation

**Design Tradeoffs**
The framework prioritizes evidence purity over comprehensive coverage, which may miss some relevant information but reduces noise and computational overhead. The reinforcement learning approach requires careful reward engineering to avoid suboptimal policies, while the adaptive re-sampling adds complexity to the pipeline.

**Failure Signatures**
The system may struggle with queries requiring cross-scene reasoning if the evidence selection becomes too localized. Ambiguous queries with multiple valid interpretations could lead to inconsistent frame selection. Videos with poor visual quality or unclear scene boundaries may produce unreliable IoU scores.

**First Experiments**
1. Test frame selection performance on a simple video with clear, isolated events to validate the basic mechanism
2. Evaluate the impact of different reward function weights on evidence purity and reasoning accuracy
3. Measure performance degradation when removing the adaptive re-sampling component to assess its contribution

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation focuses primarily on open-source Video LLMs, leaving unclear whether gains would translate to proprietary models like GPT-4V or Gemini
- The frame selection mechanism relies on IoU-based rewards but doesn't adequately address potential failure modes when scenes lack clear visual boundaries
- The adaptive re-sampling strategy may introduce computational overhead that could limit real-time deployment, though this is not quantified in the results

## Confidence
- **High confidence:** The empirical results demonstrating state-of-the-art performance on the three primary benchmarks appear robust, with clear numerical improvements over baselines
- **Medium confidence:** The claim that EARL "learns an effective and high-purity visual evidence selection policy" is supported by the results but lacks qualitative analysis of what the policy actually learns or how it handles edge cases
- **Low confidence:** The assertion that the "core philosophy of 'Select Less, Reason More'" is universally optimal for video reasoning is not rigorously tested against alternative sampling strategies beyond what's reported

## Next Checks
1. Test EARL's transferability by applying the learned evidence selection policy to a completely different video domain (e.g., from instructional videos to surveillance footage) to assess generalization
2. Conduct ablation studies specifically isolating the contribution of the adaptive re-sampling component versus the frame selection mechanism to determine which aspect drives most of the performance gains
3. Measure the computational overhead introduced by EARL's dynamic selection and re-sampling pipeline compared to static uniform sampling, particularly for videos approaching the upper length limits tested