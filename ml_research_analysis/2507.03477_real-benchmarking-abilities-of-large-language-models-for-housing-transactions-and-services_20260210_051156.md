---
ver: rpa2
title: 'REAL: Benchmarking Abilities of Large Language Models for Housing Transactions
  and Services'
arxiv_id: '2507.03477'
source_url: https://arxiv.org/abs/2507.03477
tags:
- block
- real
- names
- llms
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces REAL, the first benchmark suite designed
  to evaluate large language models (LLMs) in housing transactions and services. REAL
  contains 5,316 high-quality questions across 4 topics: memory, comprehension, reasoning,
  and hallucination, organized into 14 categories.'
---

# REAL: Benchmarking Abilities of Large Language Models for Housing Transactions and Services

## Quick Facts
- arXiv ID: 2507.03477
- Source URL: https://arxiv.org/abs/2507.03477
- Reference count: 29
- 12 mainstream LLMs tested; average accuracy of 50% on memory, comprehension, and reasoning tasks

## Executive Summary
REAL is the first benchmark suite designed to evaluate large language models on housing transactions and services, containing 5,316 high-quality questions across four topics: memory, comprehension, reasoning, and hallucination. The benchmark covers real estate properties, natural language descriptions, client behavior reasoning, and hallucination detection (inventing non-existent entities). Experiments show current LLMs achieve only 50% accuracy on core tasks and exhibit 40% hallucination rates, significantly lagging behind human real estate agents.

## Method Summary
REAL evaluates LLMs using a structured pipeline: domain experts define 14 categories, data is collected from corporate and internet sources with privacy filtering, questions are verified manually, and archived as JSON entries. The benchmark uses multiple-choice formats for memory, comprehension, and reasoning tasks (6, 3, and 1 categories respectively), while hallucination detection employs a GPT-4 judge model using natural language inference methods. All models are evaluated zero-shot with temperature=0.1, and responses are parsed for first uppercase letter selection. The evaluation covers 12 mainstream LLMs including various GPT-4, Qwen, and Yi-1.5 variants.

## Key Results
- LLMs achieve average 50% accuracy on memory, comprehension, and reasoning tasks, with reasoning accuracy below 50% for all models
- Hallucination rates average around 40%, spiking higher when fictional entities are present in queries
- GPT-4 models show strongest performance overall, but even they struggle with reasoning tasks and hallucination detection
- Models perform better on comprehension (matching descriptions) than on memory recall or reasoning about client behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured data pipeline with domain expert oversight produces reliable benchmark entries.
- Mechanism: Domain experts define 14 categories → data collection from corporate and internet sources → privacy filtering and quality checks → manual annotator verification → archived JSON entries. This sequence filters out invalid data (e.g., unrealistic grassland ratios) while preserving task diversity.
- Core assumption: Expert-curated data and multi-stage verification are necessary for domain-specific benchmark validity.
- Evidence anchors:
  - [section 3.3, Figure 4]: "All entries in the REAL are produced by this pipeline strictly."
  - [section 3.3]: "All questions and options are verified by annotators manually before archived."
  - [corpus]: Weak direct support; related papers focus on other domains (e-gaming, drug safety, homelessness).

### Mechanism 2
- Claim: Four-topic evaluation structure (memory, comprehension, reasoning, hallucination) enables granular capability assessment.
- Mechanism: Topics are derived from expert ability analysis → each topic uses distinct task formats (multiple-choice for first three, NLI-judged for hallucination) → scores aggregated per topic and overall. This isolates specific weaknesses (e.g., low reasoning accuracy across all models).
- Core assumption: The four topics comprehensively reflect real estate agent requirements and hallucination is a distinct failure mode.
- Evidence anchors:
  - [abstract]: "REAL comprises 5,316 high-quality evaluation entries across 4 topics: memory, comprehension, reasoning and hallucination."
  - [section 3.2]: "We summarize various abilities that an expert should possess and divide them into topics."
  - [corpus]: No direct mechanism support; Pet-Bench similarly defines ability categories for e-pets.

### Mechanism 3
- Claim: Fictional entity injection + NLI-based judge model quantifies hallucination propensity.
- Mechanism: Fictional names (blocks, areas, facilities) are generated → injected into queries → model responses collected → judge model (GPT-4) compares responses against premises (e.g., "Block X does not exist") → hallucination rate calculated. Agreement testing validates judge-human correlation (Pearson r > 0.9).
- Core assumption: The judge model accurately represents human judgment for hallucination detection.
- Evidence anchors:
  - [section 3.2, Hallucination Topic]: "Hallucination is evaluated based on the natural language inference (NLI) methods."
  - [section 4.1]: "We use GPT-4 as the judge model... Pearson Correlation coefficients are all higher than 0.9."
  - [corpus]: No direct support; other papers don't address NLI-based hallucination evaluation in this domain.

## Foundational Learning

- **Concept: Domain-specific benchmarks**
  - Why needed here: REAL argues general benchmarks (C-Eval, CMMLU) lack professional real estate knowledge, requiring custom evaluation.
  - Quick check question: Why can't C-Eval's 52 disciplines assess real estate agent capabilities?

- **Concept: Natural Language Inference (NLI) for hallucination detection**
  - Why needed here: The paper uses NLI to classify model responses as entailed, contradictory, or neutral relative to premises about fictional entities.
  - Quick check question: In REAL's hallucination evaluation, what would a "contradiction" decision indicate?

- **Concept: Multi-topic evaluation aggregation**
  - Why needed here: REAL computes per-topic accuracies and an overall average, revealing uneven performance (e.g., high comprehension but low reasoning).
  - Quick check question: If a model scores 80% on memory but 30% on reasoning, what does the overall average conceal?

## Architecture Onboarding

- **Component map**: Data pipeline (collection → classification → manipulation → verification) → Benchmark construction (memory/comprehension/reasoning/hallucination tasks) → Evaluation framework (model inference → judge model for hallucination → metric calculation) → Models tested (GPT-4 variants, Qwen, internal models, etc.)

- **Critical path**: Domain expert definition → data pipeline execution → benchmark construction → model inference → judge evaluation → metric aggregation. The hallucination evaluation depends on judge model alignment with human annotators.

- **Design tradeoffs**: Scope limited to Beijing (simplifies data but reduces generalizability); internal models have access to private data (performance not directly comparable); fictional name generation relies on LLM creativity (potential bias toward certain naming styles).

- **Failure signatures**: Average accuracy ~50% (Section 4.2); reasoning accuracy below 50% for all models; hallucination rates spike when fictional entities are present (Table 5); judge-human agreement drops for certain models (e.g., Yi-1.5-34B-Chat at 95.33% accuracy).

- **First 3 experiments**:
  1. Replicate evaluation on a new open-source model (e.g., Llama 3) using provided prompts to validate REAL's discriminative power.
  2. Test hallucination robustness by increasing fictional name diversity (e.g., combining styles from Appendix B.2–B.6) to see if hallucination rates change.
  3. Extend benchmark to another city (e.g., Shanghai) using the same pipeline to assess geographic transferability and identify scope limitations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLM performance on REAL transfer to housing markets with different geographic or regulatory characteristics outside of Beijing?
- Basis in paper: [explicit] The conclusion states, "Up to now, the REAL only focus on the information and knowledge related to Beijing. We will... expand the scope to other cities."
- Why unresolved: The current benchmark is strictly monolingual (Chinese) and localized to Beijing's specific real estate entities, limiting the understanding of model generalizability.
- What evidence would resolve it: Evaluation results from a multi-city or cross-lingual version of the REAL benchmark.

### Open Question 2
- Question: Can Retrieval-Augmented Generation (RAG) effectively mitigate the high hallucination rates triggered by fictional entity names?
- Basis in paper: [inferred] The hallucination evaluation (Section 4.3) shows models often fail to correct fictional names. The current methodology tests internal knowledge reliability without external tool use.
- Why unresolved: It is unclear if the hallucination issue stems from a lack of internal knowledge boundaries or a fundamental failure to recognize unknown entities.
- What evidence would resolve it: A comparative study evaluating hallucination rates on the same queries with and without an external knowledge retrieval component.

### Open Question 3
- Question: Do advanced prompting strategies (e.g., Chain-of-Thought) significantly improve the low accuracy observed in the Reasoning topic?
- Basis in paper: [inferred] Table 4 shows all models scored below 50% on reasoning. Section 4.1 notes the evaluation used a zero-shot setting, leaving the potential of few-shot or CoT reasoning unexplored.
- Why unresolved: The current results may reflect a failure of the zero-shot prompting style rather than a fundamental lack of reasoning capability in the models.
- What evidence would resolve it: Experiment results comparing zero-shot accuracy against few-shot and Chain-of-Thought performance on the Reasoning category.

## Limitations
- Geographic scope limited to Beijing, restricting generalizability to other markets
- Proprietary Beike data prevents independent validation and exact reproduction
- Internal models trained on private data make direct performance comparisons problematic
- Fictional name generation may introduce naming convention biases

## Confidence

- **High Confidence**: The benchmark construction methodology (data pipeline with expert oversight) and the four-topic evaluation structure are well-documented and reproducible in principle. The hallucination detection mechanism using NLI-based judge models is clearly specified with validation metrics.
- **Medium Confidence**: The claim that current LLMs achieve only 50% accuracy on core tasks is supported by experimental results, but the exact distribution of performance across different model families and question types could benefit from deeper analysis.
- **Medium Confidence**: The finding that hallucination rates increase with fictional entity presence is robust, but the absolute rates (around 40%) may be influenced by the specific judge model used (GPT-4) and the particular fictional name generation strategies employed.

## Next Checks
1. **Geographic Transferability Test**: Replicate the REAL evaluation framework for a different major city (e.g., Shanghai or New York) using publicly available real estate data to assess whether the benchmark's discriminative power and the observed performance limitations hold across different markets.

2. **Judge Model Robustness Analysis**: Systematically vary the judge model (using different GPT-4 variants, Claude, or human annotators) to determine the stability of hallucination rate measurements and identify potential judge-specific biases in the NLI evaluation process.

3. **Task Decomposition Study**: Conduct a detailed error analysis breaking down the 50% accuracy into specific question types within each topic to identify whether certain real estate concepts (e.g., spatial reasoning, facility matching) consistently challenge all LLMs, revealing fundamental capability gaps versus implementation artifacts.