---
ver: rpa2
title: 'Language, Culture, and Ideology: Personalizing Offensiveness Detection in
  Political Tweets with Reasoning LLMs'
arxiv_id: '2510.02351'
source_url: https://arxiv.org/abs/2510.02351
tags:
- language
- conservative
- reasoning
- left
- political
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated whether reasoning-enhanced large language
  models (LLMs) could personalize offensive language detection by simulating diverse
  political and cultural perspectives. Using a multilingual dataset of political tweets,
  models were prompted to classify content as offensive or non-offensive from the
  viewpoints of various ideological personas (far-right, conservative, progressive,
  centrist) across English, Polish, and Russian contexts.
---

# Language, Culture, and Ideology: Personalizing Offensiveness Detection in Political Tweets with Reasoning LLMs

## Quick Facts
- arXiv ID: 2510.02351
- Source URL: https://arxiv.org/abs/2510.02351
- Reference count: 40
- Large reasoning models achieved higher cross-language consistency and better ideological differentiation than non-reasoning models in offensive language detection

## Executive Summary
This study investigated whether reasoning-enhanced large language models could personalize offensive language detection by simulating diverse political and cultural perspectives. Using a multilingual dataset of political tweets, models were prompted to classify content as offensive or non-offensive from the viewpoints of various ideological personas (far-right, conservative, progressive, centrist) across English, Polish, and Russian contexts. Results showed that large reasoning-capable models (e.g., DeepSeek-R1, o4-mini) achieved higher cross-language consistency and better ideological differentiation than non-reasoning models, which produced more uniform outputs.

## Method Summary
The researchers used a 300-tweet subset from the MD-Agreement dataset, translated to Polish and Russian via DeepL API with manual correction. They constructed detailed persona profiles for 4 political groups across 3 nationalities (12 configurations total) and prompted LLMs to classify tweets as offensive or non-offensive from each perspective. Reasoning models were sampled 5 times per prompt, while non-reasoning models used token probabilities. Responses were filtered using Wald CIs (excluding p̂∈{0.4,0.6}) before binary classification. Performance was evaluated using Cross-Language Consistency (CLC) and Inter-Group Differentiation (IGD) metrics, with correlation matrices showing ideological differentiation.

## Key Results
- DeepSeek-R1 achieved IGD=100.03 vs DeepSeek-V3's IGD=1.58, showing far-right had lowest cross-group correlations (-0.37≤r≤0.01) to progressive left
- Intra-group correlations remained stable across languages (EN, PL, RU), suggesting model judgments are consistent for a given ideology regardless of language
- Non-reasoning models produced uniformly high correlations across all persona combinations with minimal ideological differentiation

## Why This Works (Mechanism)

### Mechanism 1
Explicit reasoning traces enable models to maintain coherent ideological perspectives across classification decisions. When models generate intermediate reasoning steps before classification, these traces appear to constrain outputs toward internally consistent persona-aligned judgments, reducing random variation and improving inter-group differentiation.

### Mechanism 2
Cross-linguistic ideological consistency emerges from language-invariant prompt structures combined with persona specifications. When political persona definitions remain structurally consistent across language variants, models transfer learned ideological associations to translated content despite linguistic variation.

### Mechanism 3
Model scale alone does not produce ideological differentiation without reasoning capabilities. Large non-reasoning models converge toward consensus judgments that minimize variation across persona prompts, while reasoning-enabled models explore persona-specific reasoning paths.

## Foundational Learning

- **Persona-based prompting with ideological specification**: The paper constructs detailed profiles combining political affiliation with traits like "values civility and tradition, dislikes extreme rhetoric from both sides" - more nuanced than simple label-based prompting
  - Quick check: Can you explain why the paper includes nationality alongside political affiliation in persona definitions?

- **Binary classification correlation interpretation**: The paper warns that "high agreement can coincide with low correlation" for binary variables - Pearson correlations near zero don't necessarily indicate disagreement
  - Quick check: Why does Figure 3 show 54% agreement despite r=0.27 correlation between Moderate Conservative PL and Progressive Left EN?

- **Reasoning trace language bias**: DeepSeek-R1 produced English reasoning for 86% of prompts regardless of input language; o4-mini showed 100% English reasoning - critical for multilingual deployment
  - Quick check: What implications does English-dominant reasoning have for non-English users seeking interpretable outputs?

## Architecture Onboarding

- **Component map**: Prompt construction (persona definitions + tweet text) → Model inference (reasoning-enabled vs disabled) → Response aggregation (5 samples → probability estimation → Wald CI filtering → binary classification) → Evaluation (correlation matrices, CLC/IGD scoring)

- **Critical path**: Persona design → prompt formatting → reasoning model selection → multi-sample collection (5×) → confidence filtering (exclude p̂∈{0.4,0.6}) → correlation computation → CLC/IGD scoring

- **Design tradeoffs**: Sample size (5) vs computational cost; reasoning depth vs interpretability; language fidelity (manual translation correction vs automated approaches)

- **Failure signatures**: Uniformly high correlations across all persona combinations; English-only reasoning despite non-English prompts; low valid response rates

- **First 3 experiments**: 1) Replicate DeepSeek-R1 vs DeepSeek-V3 comparison on 50-tweet subset to verify reasoning effect with controlled model family; 2) Test prompt language vs reasoning language alignment: Force reasoning output language specification and measure CLC/IGD changes; 3) Ablate persona detail level: Compare full persona descriptions vs label-only prompts to quantify detail contribution to IGD

## Open Questions the Paper Calls Out
- Can instruction-tuning or alignment methods be developed to ensure reasoning outputs match the prompt language in multilingual settings?
- Do reasoning-enabled models maintain ideological differentiation when evaluated on larger, more diverse datasets spanning multiple political domains?
- Can empirically grounded or dynamically generated persona representations achieve stronger ideological differentiation than manually constructed profiles?
- What benchmarks effectively evaluate reasoning quality for personalized, multilingual offensiveness detection?

## Limitations
- Dataset consisted of a relatively small sample – 300 tweets from a single political domain, which may constrain generalizability
- Personas were manually constructed and necessarily idealized, rather than empirically grounded or dynamically generated
- Limited evidence for how results translate to non-Western political contexts beyond Polish and Russian translations

## Confidence
- **High confidence**: The differential performance between reasoning and non-reasoning models (CLC and IGD metrics) is well-supported by systematic comparisons across multiple model families and sizes
- **Medium confidence**: The claim that reasoning traces reflect actual perspective-taking rather than post-hoc rationalization is plausible but not directly verified
- **Medium confidence**: Cross-linguistic consistency findings are robust for the tested language pairs, but generalizability to other language combinations remains uncertain

## Next Checks
1. Force reasoning models to output traces in the same language as prompts and measure whether CLC/IGD scores change significantly
2. Conduct expert review of whether American political persona descriptions meaningfully correspond to Polish and Russian political identities
3. Systematically examine reasoning traces for evidence of perspective-taking versus template-following, particularly for o4-mini's "brief, often absent" summaries