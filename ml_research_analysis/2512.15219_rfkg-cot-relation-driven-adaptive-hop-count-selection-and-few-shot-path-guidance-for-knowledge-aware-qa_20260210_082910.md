---
ver: rpa2
title: 'RFKG-CoT: Relation-Driven Adaptive Hop-count Selection and Few-Shot Path Guidance
  for Knowledge-Aware QA'
arxiv_id: '2512.15219'
source_url: https://arxiv.org/abs/2512.15219
tags:
- reasoning
- paths
- path
- knowledge
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RFKG-CoT improves knowledge-intensive QA by addressing two limitations
  in KG-CoT: rigid hop-count selection and underutilization of reasoning paths. It
  introduces a relation-driven adaptive hop-count selector that dynamically adjusts
  reasoning steps based on KG relations (e.g., 1-hop for direct relations, 2-hop for
  indirect chains), formalized via a relation mask.'
---

# RFKG-CoT: Relation-Driven Adaptive Hop-count Selection and Few-Shot Path Guidance for Knowledge-Aware QA

## Quick Facts
- arXiv ID: 2512.15219
- Source URL: https://arxiv.org/abs/2512.15219
- Authors: Chao Zhang; Minghan Li; Tianrui Lv; Guodong Zhou
- Reference count: 9
- Key outcome: RFKG-CoT improves knowledge-intensive QA by addressing two limitations in KG-CoT: rigid hop-count selection and underutilization of reasoning paths. It introduces a relation-driven adaptive hop-count selector that dynamically adjusts reasoning steps based on KG relations (e.g., 1-hop for direct relations, 2-hop for indirect chains), formalized via a relation mask. Additionally, it incorporates few-shot in-context learning path guidance with CoT to teach LLMs how to interpret reasoning paths. Experiments on four KGQA benchmarks show RFKG-CoT improves accuracy by up to 14.7 percentage points (Llama2-7B on WebQSP) over KG-CoT. Ablations confirm the hop-count selector and path prompt are complementary, jointly transforming KG evidence into more faithful answers.

## Executive Summary
RFKG-CoT addresses critical limitations in KG-CoT by introducing a relation-driven adaptive hop-count selector and few-shot path guidance for knowledge-aware QA. The method dynamically adjusts reasoning steps based on KG relation types using a relation mask, and teaches LLMs to interpret reasoning paths through in-context learning. Experimental results demonstrate significant accuracy improvements across four benchmarks, with the two proposed components working synergistically to transform KG evidence into more faithful answers.

## Method Summary
RFKG-CoT introduces two key innovations to improve KG-CoT's performance. First, it implements a relation-driven adaptive hop-count selector that dynamically determines the number of reasoning steps based on KG relation types - using 1-hop for direct relations and 2-hop for indirect chains, formalized through a relation mask. Second, it incorporates few-shot in-context learning path guidance that teaches LLMs how to interpret and utilize reasoning paths through carefully curated examples. These components work together to address KG-CoT's rigid hop-count selection and underutilization of reasoning paths, transforming KG evidence into more faithful answers.

## Key Results
- RFKG-CoT improves accuracy by up to 14.7 percentage points on WebQSP using Llama2-7B
- The relation-driven hop-count selector and few-shot path prompt are confirmed as complementary components through ablation studies
- Significant improvements demonstrated across four KGQA benchmarks
- Method transforms KG evidence into more faithful answers compared to KG-CoT

## Why This Works (Mechanism)
The relation-driven adaptive hop-count selector works by analyzing KG relation types and dynamically adjusting the number of reasoning steps. For direct relations, it uses 1-hop traversal, while for indirect relations requiring chains, it employs 2-hop traversal. This is formalized through a relation mask that encodes the relevance of each relation type. The few-shot path guidance component teaches LLMs to interpret reasoning paths by providing curated in-context examples, helping the model understand how to extract and utilize path information effectively.

## Foundational Learning
- **Relation Masks**: Binary indicators that encode whether a KG relation should use 1-hop or 2-hop traversal - needed to dynamically adjust reasoning steps based on relation types, verified by checking mask values against relation categories.
- **Knowledge Graph Traversal**: Systematic exploration of KG nodes and edges to find answer paths - needed to navigate structured knowledge for QA, verified by tracing traversal paths from question to answer.
- **In-Context Learning**: Teaching models through curated examples within prompts rather than fine-tuning - needed to guide LLM reasoning without additional training, verified by testing model performance with and without guidance examples.
- **Chain-of-Thought Reasoning**: Step-by-step reasoning process for complex problems - needed to break down multi-hop KGQA into interpretable steps, verified by examining intermediate reasoning outputs.
- **Relation Types**: Categorization of KG relationships (direct vs. indirect) - needed to determine appropriate hop counts, verified by mapping relations to their traversal requirements.

## Architecture Onboarding

**Component Map**: Input Question -> Relation Mask Generator -> Adaptive Hop-count Selector -> Few-shot Path Guidance -> LLM Reasoning -> Output Answer

**Critical Path**: Question and KG relations → Relation mask generation → Hop-count determination → Path guidance prompt construction → LLM reasoning with adapted hop-count and path examples → Final answer generation

**Design Tradeoffs**: Adaptive hop-count selection trades computational overhead for more precise reasoning, while few-shot path guidance requires curated examples but enables zero-shot adaptation. The relation mask approach assumes binary relevance, potentially oversimplifying complex KG structures.

**Failure Signatures**: Incorrect relation categorization leading to wrong hop-count selection, poorly curated path examples causing confusion rather than guidance, or LLM failure to interpret the adaptive hop-count instructions properly.

**First 3 Experiments**: 1) Test relation mask accuracy by comparing predicted hop-counts against ground truth for various relation types, 2) Evaluate path guidance effectiveness by measuring performance with different numbers/quality of in-context examples, 3) Benchmark computational overhead of adaptive selection versus fixed hop-count approaches.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation limited to English-language benchmarks, restricting generalizability to multilingual settings
- Relation mask formulation assumes binary relevance without handling ambiguous relation types or quantifying partial relevance
- Few-shot path guidance sensitivity to example quality and selection criteria not explored
- Computational overhead of adaptive components not measured or reported
- Performance gains vary significantly across datasets (0.3-14.7 points), suggesting context-dependent rather than universal improvements

## Confidence

**High Confidence**: The core methodology of relation-driven adaptive hop-count selection and few-shot path guidance is clearly defined and technically sound. The experimental setup is rigorous, with proper ablation studies confirming the complementarity of the two proposed components.

**Medium Confidence**: The magnitude of improvements, particularly the 14.7-point gain on WebQSP, warrants scrutiny. While statistically significant, such large gains may be influenced by specific dataset characteristics or KG-CoT's limitations. The generalization of these improvements across diverse KGQA scenarios remains to be fully established.

**Low Confidence**: Claims about the mechanism by which relation masks improve reasoning (e.g., "one-hop relations capture most entities in KGs") are presented without empirical validation or theoretical justification. The paper assumes these patterns without testing alternative formulations.

## Next Checks
1. **Robustness Testing**: Evaluate RFKG-CoT across diverse KG schemas and relation types, including cases with ambiguous or multi-hop indirect relations, to assess whether the relation mask adapts appropriately beyond the studied benchmarks.

2. **Computational Overhead Analysis**: Measure and report the latency and resource consumption introduced by the adaptive hop-count selector and path prompt generation, comparing against KG-CoT to quantify practical deployment trade-offs.

3. **Generalization to Multilingual KGQA**: Test RFKG-CoT on non-English KGQA datasets to determine whether the relation-driven approach and path guidance generalize across languages and cultural contexts.