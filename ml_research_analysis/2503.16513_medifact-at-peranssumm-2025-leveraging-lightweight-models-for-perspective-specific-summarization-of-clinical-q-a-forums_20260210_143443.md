---
ver: rpa2
title: 'Medifact at PerAnsSumm 2025: Leveraging Lightweight Models for Perspective-Specific
  Summarization of Clinical Q&A Forums'
arxiv_id: '2503.16513'
source_url: https://arxiv.org/abs/2503.16513
tags:
- classification
- summarization
- snorkel
- task
- weak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses perspective-aware healthcare answer summarization
  by combining weak supervision with lightweight models. The proposed Snorkel-BART-SVM
  pipeline classifies responses into predefined perspectives (experience, information,
  cause, suggestion, question) using rule-based labeling, SVM with sentence embeddings,
  and zero-shot learning fallback.
---

# Medifact at PerAnsSumm 2025: Leveraging Lightweight Models for Perspective-Specific Summarization of Clinical Q&A Forums

## Quick Facts
- arXiv ID: 2503.16513
- Source URL: https://arxiv.org/abs/2503.16513
- Reference count: 6
- Key outcome: Achieved 12th place among 100 teams in PerAnsSumm 2025 with Weighted F1: 0.8361 and ROUGE-1: 0.3485

## Executive Summary
This work presents a perspective-aware healthcare answer summarization system for clinical question-answering forums. The approach combines weak supervision with lightweight models to classify responses into five predefined perspectives (experience, information, cause, suggestion, question) and generate coherent summaries. The pipeline uses Snorkel for programmatic labeling, SVM with sentence embeddings for classification, and a two-stage summarization approach (extractive with BART followed by abstractive refinement with Pegasus). The system achieved competitive performance while maintaining computational efficiency through fine-tuning rather than training from scratch.

## Method Summary
The method employs a hierarchical classification pipeline: Snorkel's labeling functions apply regex-based pattern matching for initial labeling, followed by an SVM classifier using sentence embeddings, and zero-shot learning fallback via BART-large-MNLI for uncertain cases. For summarization, BART-large-CNN extracts key sentences (max 150 tokens), which Pegasus-XSum then refines for coherence (max 100 tokens). The approach leverages fine-tuning of pre-trained models rather than training from scratch to reduce resource demands while maintaining competitive performance on the PerAnsSumm 2025 task.

## Key Results
- Achieved 12th place among 100 teams in the PerAnsSumm 2025 shared task
- Classification performance: Weighted F1 score of 0.8361
- Summarization quality: ROUGE-1 score of 0.3485
- Maintained computational efficiency through model fine-tuning
- Demonstrated competitive performance across multiple evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Classification with Cascading Fallback
Combining weak supervision, supervised learning, and zero-shot classification in sequence improves perspective labeling coverage when labeled data is scarce. The Snorkel Label Model applies regex-based heuristics for initial labeling, SVM classifies using sentence embeddings when confidence is low, and BART-large-MNLI performs zero-shot inference as a final fallback. This cascading approach ensures robust classification across diverse input patterns.

### Mechanism 2: Two-Stage Summarization for Content Preservation
Extractive summarization followed by abstractive refinement preserves factual content while improving fluency. BART-large-CNN first extracts salient sentences to reduce input noise, then Pegasus-XSum rewrites for coherence and readability. This approach maintains factual accuracy from the extractive stage while enhancing the output's linguistic quality through abstractive refinement.

### Mechanism 3: Lightweight Model Selection for Computational Efficiency
Fine-tuning pre-trained models rather than training from scratch achieves competitive performance with reduced resource demands. By adapting existing BART and Pegasus models through fine-tuning, the system avoids the computational costs of training large models from scratch while maintaining strong performance on the medical CQA domain.

## Foundational Learning

- **Concept: Weak supervision with labeling functions**
  - Why needed here: Manual annotation of perspective spans is expensive; Snorkel enables programmatic label generation from heuristics.
  - Quick check question: Can you write a regex that matches sentences containing "I experienced" or "in my case" to label EXPERIENCE perspective?

- **Concept: Sentence embeddings for classification**
  - Why needed here: SVM requires fixed-dimensional input; sentence transformers map variable-length text to dense vectors capturing semantic meaning.
  - Quick check question: Why might all-MiniLM-L6-v2 embeddings struggle with medical terminology compared to BioBERT embeddings?

- **Concept: Zero-shot classification with NLI models**
  - Why needed here: Fallback mechanism for unseen or ambiguous inputs where rule-based and supervised methods fail.
  - Quick check question: How does BART-large-MNLI perform zero-shot classificationâ€”what does it compare the input against?

## Architecture Onboarding

- **Component map**: Input (healthcare CQA responses) -> Snorkel labeling functions -> Label Model -> SVM classifier (sentence embeddings) -> Zero-shot BART-large-MNLI fallback -> BART-large-CNN (extractive summarization) -> Pegasus-XSum (abstractive refinement) -> Output (perspective-labeled summaries)

- **Critical path**: 1) Define regex-based labeling functions for each perspective category, 2) Train Snorkel Label Model for 500 epochs, 3) Train SVM on sentence embeddings from labeled subset, 4) Configure BART and Pegasus generation parameters, 5) Evaluate on held-out test set using Weighted F1 and ROUGE metrics

- **Design tradeoffs**: Regex-based labeling offers speed but brittleness; SVM provides faster inference than deep classifiers but may underfit complex patterns; two-stage summarization increases compute but improves factuality; open-source models reduce cost but may have lower performance ceilings

- **Failure signatures**: Low Strict Matching Precision (0.1383) indicates over-prediction of spans; low ROUGE scores (R1: 0.3485) suggest summaries lack reference content; Macro F1 (0.8361) < Weighted F1 (0.8887) reveals class imbalance effects; low AlignScore (0.3121) indicates factual consistency issues

- **First 3 experiments**: 1) Ablate each classification stage to measure individual component contributions, 2) Replace MiniLM embeddings with BioBERT to test domain-specific impact, 3) Skip extractive stage to quantify its contribution to factuality vs ROUGE

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can efficient fine-tuning methods like Low-Rank Adaptation (LoRA) or quantization improve the balance between computational efficiency and performance compared to the current full fine-tuning approach?
- **Basis in paper**: Explicit mention in conclusion suggesting exploration of LoRA and quantization
- **Why unresolved**: Current study used standard fine-tuning without testing parameter-efficient fine-tuning techniques
- **What evidence would resolve it**: Comparative experiments showing LoRA/quantization metrics relative to training time and memory usage

### Open Question 2
- **Question**: Does Retrieval-Augmented Generation (RAG) enhance the system's ability to handle unseen or ambiguous perspectives better than the current zero-shot learning fallback?
- **Basis in paper**: Explicit suggestion to explore RAG for handling unseen perspectives
- **Why unresolved**: Current pipeline relies on zero-shot transformer model, but efficacy on novel perspectives not analyzed in detail
- **What evidence would resolve it**: Ablation studies replacing zero-shot component with RAG module, evaluated on out-of-domain perspectives

### Open Question 3
- **Question**: How can the pipeline be modified to improve factual alignment scores (AlignScore/Factuality) without compromising semantic relevance (BERTScore)?
- **Basis in paper**: Discussion notes factual consistency metrics indicate improvement areas and calls for strengthening factual alignment
- **Why unresolved**: High BERTScore (0.8336) contrasts with low AlignScore (0.3121), showing discrepancy between semantic similarity and factual consistency
- **What evidence would resolve it**: Integration of factual consistency loss function or fact-checking reward model into Pegasus fine-tuning, resulting in higher AlignScore values

## Limitations
- Regex-based weak supervision may not capture medical discourse complexity, particularly for nuanced perspectives
- General-domain sentence embeddings (MiniLM) may struggle with specialized medical terminology
- ROUGE scores (R1: 0.3485) suggest generated summaries may not fully preserve reference content
- Factual consistency remains challenging despite competitive semantic similarity scores

## Confidence
- **Hierarchical Classification Effectiveness**: High confidence - well-supported by explicit methodology and competitive F1 scores
- **Two-Stage Summarization Benefits**: Medium confidence - claimed improvements based on limited metric comparison
- **Computational Efficiency Claims**: High confidence - clearly articulated rationale aligned with established transfer learning practices

## Next Checks
1. **Domain-Specific Embedding Impact**: Replace general-domain all-MiniLM-L6-v2 embeddings with BioBERT or PubMedBERT in the SVM classifier and measure classification F1 for each perspective category.

2. **Zero-Shot Fallback Threshold Analysis**: Systematically vary the confidence threshold triggering the zero-shot BART-large-MNLI fallback and evaluate trade-offs between classification coverage and accuracy.

3. **Extractive Stage Ablation Study**: Remove the BART extractive summarization stage and feed raw classified spans directly to Pegasus-XSum, comparing factuality metrics and ROUGE scores between full pipeline and direct abstractive generation.