---
ver: rpa2
title: 'FedHL: Federated Learning for Heterogeneous Low-Rank Adaptation via Unbiased
  Aggregation'
arxiv_id: '2505.18494'
source_url: https://arxiv.org/abs/2505.18494
tags:
- lora
- fedhl
- aggregation
- client
- rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses convergence issues in federated learning with
  heterogeneous low-rank adaptation (LoRA) modules. Existing approaches suffer from
  truncation bias and gradient drift when clients have different rank constraints,
  leading to degraded performance and unreliable convergence guarantees.
---

# FedHL: Federated Learning for Heterogeneous Low-Rank Adaptation via Unbiased Aggregation

## Quick Facts
- arXiv ID: 2505.18494
- Source URL: https://arxiv.org/abs/2505.18494
- Reference count: 40
- Primary result: Eliminates truncation bias in federated LoRA with O(1/√T) convergence guarantee

## Executive Summary
FedHL addresses convergence issues in federated learning with heterogeneous LoRA modules by eliminating truncation bias and gradient drift. The framework uses the full-rank global model as a stable aggregation baseline instead of truncated approximations, and derives optimal aggregation weights to minimize gradient drift. Experiments show 1-3% performance improvements over state-of-the-art methods across multiple datasets and heterogeneity levels.

## Method Summary
FedHL maintains a full-rank global adapter on the server, which is SVD-truncated to each client's specific rank constraint. Clients train only low-rank LoRA matrices locally, then return updates. The server aggregates using the full-rank baseline with optimal weights inversely proportional to truncation error. This eliminates model truncation bias and minimizes gradient descent drift, achieving O(1/√T) convergence rate with practical applicability across cross-silo and cross-device settings.

## Key Results
- Outperforms state-of-the-art methods by 1-3% in evaluation metrics across Fed-GSM8K, Fed-CodeAlpaca, and Fed-Dolly datasets
- Demonstrates superior stability and performance across different heterogeneity levels
- Scales effectively from cross-silo (10 clients) to cross-device (100 clients) settings
- Performs well even under homogeneous rank configurations

## Why This Works (Mechanism)

### Mechanism 1: Unified Entity Aggregation (Eliminating Cross-Term Noise)
Instead of averaging low-rank matrices B and A separately (introducing "cross-term" noise like B₁A₂), FedHL reconstructs the full update matrix W = BA for each client before aggregation. This ensures cross-terms don't pollute the global update direction.

### Mechanism 2: Unbiased Full-Rank Aggregation Baseline
FedHL changes the aggregation rule from Wₜ₊₁ = Σpᵢ(Wᵣᵢₜ + ΔWᵢ) to Wₜ₊₁ = Σpᵢ(Wₜ + ΔWᵢ), using the server's stored full-rank global model Wₜ as the baseline for all clients. This mathematically cancels out initialization truncation error in the update step.

### Mechanism 3: Drift-Minimizing Adaptive Weighting
The framework computes truncation error r̂ᵢ = ||Wₜ - Wᵣᵢₜ||² for each client and sets aggregation weights p*ᵢ ∝ 1/(r̂²ᵢ + ε) inversely proportional to this error. This minimizes accumulated gradient descent drift by downweighting clients with larger truncation errors.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed: FedHL operates on the premise that updates ΔW can be decomposed into low-rank matrices B and A, where W = W₀ + BA
  - Quick check: If a global LoRA update has rank 64 and a client supports rank 8, how does SVD help in truncation and what information is lost?

- **Concept: Federated Averaging (FedAvg)**
  - Why needed: FedHL modifies standard FedAvg, changing from averaging resulting models to using full-rank baseline for deltas
  - Quick check: In standard FedAvg, do we aggregate gradients or model weights?

- **Concept: Singular Value Decomposition (SVD) for Truncation**
  - Why needed: The mechanism relies on truncating global full-rank model Wₜ to client-specific ranks rᵢ using SVD
  - Quick check: Does SVD truncation preserve exact update direction or approximate best low-rank subspace?

## Architecture Onboarding

- **Component map:** Global Server -> Clients -> Global Server (aggregation)
- **Critical path:** Server performs SVD truncation for each client → Clients train LoRA matrices → Server reconstructs Wᵢₜ₊₁ = BᵢAᵢ → Server aggregates using optimal weights
- **Design tradeoffs:** Server computation burden shifts to SVD initialization phase; server must store full-rank Wₜ; communication remains efficient (only low-rank factors transmitted)
- **Failure signatures:** SVD collapse if Wₜ becomes rank-deficient; weight explosion if truncation errors approach zero; stagnation if truncation errors are uniformly high
- **First 3 experiments:**
  1. Verify bias elimination by running FedHL vs. FlexLoRA on synthetic dataset with known full-rank optimum
  2. Test weight sensitivity under high heterogeneity (ranks 4-64) comparing adaptive vs. uniform vs. data-proportional weights
  3. Measure server-side SVD bottleneck scaling from 10 to 1000 clients for cross-device feasibility

## Open Questions the Paper Calls Out
- Future work will optimize rank assignments to better suit diverse client conditions, as current static distributions don't adapt to evolving conditions
- Generalization to other foundation model architectures beyond Llama-2 7B remains unexplored despite theoretical analysis assuming general neural network properties
- Optimal selection strategy for the constant ε in aggregation weight formula needs investigation, as current treatment provides no guidance on hyperparameter setting

## Limitations
- Computational overhead from server-side SVD operations may limit scalability in large-scale cross-device settings
- Adaptive weighting mechanism assumes truncation error reliably proxies update quality, which may fail with highly heterogeneous data distributions
- Theoretical guarantees assume full participation or specific sampling schemes that may not reflect practical federated learning scenarios

## Confidence
- **High Confidence:** Mechanism for eliminating truncation bias using full-rank baseline is mathematically sound with robust convergence guarantees
- **Medium Confidence:** Adaptive weighting based on truncation error shows promise but depends on assumption that truncation error correlates with update quality
- **Low Confidence:** Practical scalability of server-side SVD computations for large client populations remains uncertain without empirical evidence

## Next Checks
1. Measure server-side SVD computation time scaling as client count increases from 10 to 1000
2. Test FedHL under highly non-IID data distributions where low-rank clients have critical unique data
3. Create scenarios with differing local learning rates for A and B matrices to test unified aggregation robustness