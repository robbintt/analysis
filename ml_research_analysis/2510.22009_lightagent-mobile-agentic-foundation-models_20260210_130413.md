---
ver: rpa2
title: 'LightAgent: Mobile Agentic Foundation Models'
arxiv_id: '2510.22009'
source_url: https://arxiv.org/abs/2510.22009
tags:
- task
- reasoning
- agent
- cloud
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of building effective mobile GUI
  agents that can run on-device under tight computational and memory constraints,
  while still matching the performance of larger cloud-based models. The authors propose
  LightAgent, a mobile agentic foundation model that combines a lightweight on-device
  MLLM (Qwen2.5-VL-3B) with a cloud-based fallback through dynamic orchestration.
---

# LightAgent: Mobile Agentic Foundation Models

## Quick Facts
- **arXiv ID**: 2510.22009
- **Source URL**: https://arxiv.org/abs/2510.22009
- **Reference count**: 40
- **Key outcome**: On-device MLLM (3B) with SFT→GRPO training + device-cloud orchestration achieves 15.2% SR standalone, 47.1% SR with Gemini-2.5-Pro support, cutting cloud calls by ~10%

## Executive Summary
This paper tackles the problem of building effective mobile GUI agents that can run on-device under tight computational and memory constraints, while still matching the performance of larger cloud-based models. The authors propose LightAgent, a mobile agentic foundation model that combines a lightweight on-device MLLM (Qwen2.5-VL-3B) with a cloud-based fallback through dynamic orchestration. The on-device agent is enhanced with a two-stage training pipeline (SFT→GRPO) on synthetic GUI data and an efficient long-reasoning mechanism that summarizes historical interactions to fit limited context. The device-cloud collaboration dynamically switches between on-device and cloud models based on real-time complexity assessment, minimizing cloud calls while maintaining high task completion rates. Experiments on the AndroidLab benchmark show LightAgent matches or nears larger models, significantly reducing cloud costs while achieving competitive success rates.

## Method Summary
The authors propose LightAgent, a mobile agentic foundation model built on Qwen2.5-VL-3B with a two-stage training pipeline (SFT→GRPO) on synthetic GUI reasoning data. The model uses text-based state summarization to maintain long interaction histories within limited context, and employs dynamic device-cloud orchestration that assesses task complexity before execution and monitors for failure patterns during execution. The framework uses complexity assessment (γ, ω) to determine when to enable monitoring and how frequently to check for intervention triggers. When failure patterns are detected, the system switches to a cloud model (e.g., Gemini-2.5-Pro) for the remainder of the task.

## Key Results
- LightAgent achieves 15.2% success rate on AndroidLab benchmark when running purely on-device
- With device-cloud collaboration, success rate increases to 47.1% using Gemini-2.5-Pro as cloud fallback
- Device-cloud framework reduces cloud calls by approximately 10% compared to cloud-only approaches
- OpenPhone model with state history achieves 15.2% SR versus 4.3% SR without history on custom tasks
- Cloud model performs approximately 65% of steps even with collaboration, reflecting on-device capacity limits

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Training Pipeline (SFT→GRPO) for Compact GUI Agents
- Claim: A small MLLM (3B parameters) can achieve competitive GUI task performance when trained via supervised fine-tuning followed by group relative policy optimization, conditional on access to high-quality synthetic reasoning data.
- Mechanism: SFT injects foundational GUI knowledge and reasoning patterns from synthetic chain-of-thought data; GRPO then aligns the model's outputs with task success using accuracy and format rewards, enabling test-time scaling through long-horizon reasoning.
- Core assumption: The synthetic data generation pipeline produces reasoning chains that transfer to real GUI tasks; GRPO's group-relative advantage calculation provides stable optimization signals for small models.
- Evidence anchors: [abstract] "enhances Qwen2.5-VL-3B via two-stage SFT→GRPO training on synthetic GUI data for strong decision-making"; [section 2.3.2] "GRPO eliminates the need for additional value function approximation... and instead utilizes the average reward from multiple sampled outputs"; [corpus] Weak direct validation—Ferret-UI Lite (arXiv:2509.26539) explores small on-device GUI agents but uses different training approaches; no direct comparison to SFT→GRPO in corpus.

### Mechanism 2: Text-Based State Summarization for Efficient Context Management
- Claim: Compressing screenshot history into structured textual state assessments enables longer-horizon reasoning under tight context budgets, conditional on the summarization preserving task-relevant information.
- Mechanism: At each step, the model generates a `<STATE_ASSESSMENT>` block capturing current interface state, task progress, next action, expected outcome, and potential issues—replacing high-token screenshots with compact text that can be retained across 10–20 steps.
- Core assumption: Textual summaries retain sufficient information for future decision-making; the model can reliably generate accurate self-assessments.
- Evidence anchors: [section 2.1.2] "textual summaries use far fewer tokens than images, enabling long-term history retention (e.g., 10–20 steps)"; [section 3.3] "OpenPhone w/o History" drops to 4.3% SR vs. 15.2% with history; [corpus] No direct corpus validation for this specific summarization approach.

### Mechanism 3: Task Complexity Assessment and Dynamic Device-Cloud Orchestration
- Claim: Real-time complexity assessment combined with dynamic switching reduces cloud invocations by ~10% while maintaining task success rates, conditional on accurate failure pattern detection.
- Mechanism: Pre-task assessment determines monitoring start step (γ) and frequency (ω); runtime evaluation checks for repetitive actions, trajectory deviation, or low-quality actions to trigger cloud handoff once per task.
- Core assumption: Historical performance data reliably predicts task difficulty; failure patterns (repetition, deviation) are detectable before irreversible errors.
- Evidence anchors: [section 3.4] "device-cloud framework cuts cloud calls by roughly 10%"; [figure 6a] Cloud performs ~65% of steps even with collaboration, reflecting on-device capacity limits; [corpus] CHORD (arXiv:2510.03038) explores device-cloud collaboration for recommendation models but addresses different domain; no direct validation of orchestration logic for GUI tasks.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: Enables reinforcement learning for small MLLMs without a separate value function, using group-relative advantages for stability.
  - Quick check question: Can you explain why GRPO's group-relative advantage calculation (Ai = (ri - mean) / std) might stabilize training compared to absolute reward values?

- **Concept: Test-Time Scaling via Chain-of-Thought**
  - Why needed here: Small models lack inherent capability; extended reasoning at inference time compensates through deliberate step-by-step analysis.
  - Quick check question: Why might prompting a weaker model (e.g., GPT-5-nano) for reasoning *degrade* performance while helping stronger models?

- **Concept: Hierarchical Agent Orchestration**
  - Why needed here: Single-agent systems struggle with both high-level planning and low-level execution; device-cloud separation enables specialization.
  - Quick check question: What information must the device agent pass to the cloud agent during handoff to ensure seamless continuation?

## Architecture Onboarding

- **Component map:** On-device agent (Qwen2.5-VL-3B) → Complexity assessor → Dynamic orchestrator → Cloud fallback (Gemini-2.5-Pro)
- **Critical path:** 1. Task received → Complexity assessment → (γ, ω) determined; 2. On-device agent executes with history summarization; 3. At step γ and every ω steps: orchestrator evaluates switching criteria; 4. If triggered: cloud model takes over until completion; 5. If never triggered: on-device agent completes task
- **Design tradeoffs:** Monitoring frequency vs. overhead (more frequent checks catch failures earlier but increase orchestration cost); Summarization granularity vs. context length (more detailed assessments preserve information but consume context budget); Cloud model selection (more capable models improve success rates but increase per-call cost)
- **Failure signatures:** Repetitive actions without progress (same tap 2+ times); Navigation to wrong app/screen; State misunderstanding (e.g., misinterpreting toggle positions); Form field struggles after reaching correct screen
- **First 3 experiments:** 1. Ablate training stages: Compare SFT-only, GRPO-only, and SFT→GRPO to isolate contribution of each stage (expected: GRPO-only learns slower but steadier; SFT→GRPO achieves best final performance); 2. Vary monitoring frequency: Test ω ∈ {2, 5, 10} on held-out tasks to characterize cost-accuracy frontier (expected: more frequent monitoring reduces cloud calls but may miss late-stage failures); 3. Stress-test summarization: Run tasks requiring long-horizon memory (10+ steps) with and without state assessment to quantify information loss (expected: tasks requiring early-screen details fail without history)

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the size of task-specific GUI models be reduced to a truly on-device scale (e.g., 4B parameters or smaller) while maintaining acceptable performance on complex mobile tasks?
  - Basis in paper: [explicit] The authors explicitly list this as "Question 1" on page 2, questioning if models can be made small enough for smartphones without losing efficacy.
  - Why unresolved: While the proposed OpenPhone (3B) runs on-device, its standalone success rate (15.2%) indicates a significant performance gap compared to cloud models, suggesting "acceptable performance" remains elusive for purely local execution.
  - What evidence would resolve it: A sub-4B model achieving comparable success rates (e.g., >40%) to 7B+ models on benchmarks like AndroidLab without external assistance.

- **Open Question 2:** To what extent can the usage costs of proprietary cloud-based models be reduced through device-cloud collaboration before task completion rates degrade?
  - Basis in paper: [explicit] The authors explicitly list this as "Question 2" on page 2, aiming to mitigate the high costs of systems like GPT-5 or Gemini.
  - Why unresolved: Although the framework reduces cloud steps, Figure 6 shows the cloud model still executes ~65% of steps, implying significant costs persist; the optimal balance remains unclear.
  - What evidence would resolve it: A framework demonstrating a >50% reduction in cloud token usage relative to a cloud-only baseline while maintaining a negligible drop (<2%) in task success rate.

- **Open Question 3:** What is the minimal model capability threshold required for chain-of-thought (CoT) reasoning to improve, rather than impair, performance in GUI agents?
  - Basis in paper: [inferred] Page 9 observes that CoT reasoning improved Gemini-2.5-Flash but severely degraded GPT-5-nano (18.1% to 2.9% SR), suggesting a capability floor exists.
  - Why unresolved: The paper notes this dependency on "baseline capability" but does not define the specific metrics or thresholds that predict whether reasoning will be beneficial or detrimental.
  - What evidence would resolve it: A correlative study identifying a specific benchmark score (e.g., specific MMLU or grounding thresholds) above which CoT yields positive returns.

- **Open Question 4:** Does the reliance on synthetic data generated by large models for training small GUI agents introduce a specific ceiling on generalization or reasoning fidelity?
  - Basis in paper: [inferred] Section 2.3.1 describes using Gemini-2.5-Pro to generate reasoning chains for the 3B model, assuming the teacher's reasoning is distillable.
  - Why unresolved: The paper does not analyze if the student model fails because it cannot replicate the teacher's logic, potentially limiting the effectiveness of the SFT→GRPO pipeline.
  - What evidence would resolve it: An error analysis comparing failure modes of synthetic-trained vs. human-demonstration-trained agents, specifically looking for hallucinations in reasoning steps.

## Limitations

- **Synthetic Data Quality**: The entire training pipeline depends on synthetic GUI reasoning data generated by Gemini-2.5-Pro and formatted by Qwen3-32B. No empirical validation of data quality or domain coverage is provided beyond training results.
- **GRPO Hyperparameters**: Critical values for ε, β, group size G, and learning rate are unspecified, making it difficult to assess whether reported improvements stem from algorithmic advantages or hyperparameter tuning.
- **Device-Cloud Thresholds**: The complexity assessment (γ, ω) appears task-specific but lacks systematic analysis of how thresholds were determined or their sensitivity to task characteristics.
- **Closed-Source Dependencies**: Heavy reliance on Gemini-2.5-Pro for both data generation and cloud fallback creates reproducibility barriers and obscures where performance gains originate.

## Confidence

- **High Confidence**: The two-stage training approach (SFT→GRPO) produces measurable improvements over single-stage baselines. The text-based state summarization demonstrably reduces context consumption while preserving task-relevant information.
- **Medium Confidence**: The dynamic device-cloud orchestration achieves the claimed ~10% reduction in cloud calls while maintaining success rates, though the exact mechanism for complexity assessment remains underspecified.
- **Low Confidence**: Claims about the general applicability of the training pipeline to other small MLLMs or GUI domains beyond Android, as no cross-domain validation is presented.

## Next Checks

1. **Ablation on Synthetic Data Quality**: Generate two synthetic datasets with varying reasoning quality (e.g., shallow vs. deep CoT chains) and compare SFT→GRPO performance to isolate data quality impact from training method.
2. **Cloud Intervention Timing Analysis**: For tasks where cloud intervention occurs, analyze whether intervention timing (early vs. late) correlates with success rate to validate the switching criteria's effectiveness.
3. **On-Device-Only Stress Test**: Run LightAgent on a subset of AndroidLab tasks with cloud disabled to quantify the true capability gap between on-device and cloud models, validating the orchestration's necessity.