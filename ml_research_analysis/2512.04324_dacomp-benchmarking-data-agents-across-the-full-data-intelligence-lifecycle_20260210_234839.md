---
ver: rpa2
title: 'DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle'
arxiv_id: '2512.04324'
source_url: https://arxiv.org/abs/2512.04324
tags:
- data
- score
- tasks
- agent
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DAComp introduces the first benchmark evaluating data agents across
  the full intelligence lifecycle, combining repository-level engineering with open-ended
  analysis. It features 210 tasks spanning architecture design, multi-stage SQL pipeline
  construction and evolution, and strategic data analysis over enterprise schemas.
---

# DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle

## Quick Facts
- **arXiv ID:** 2512.04324
- **Source URL:** https://arxiv.org/abs/2512.04324
- **Reference count:** 40
- **Primary result:** First benchmark evaluating data agents across full intelligence lifecycle with 210 tasks spanning architecture design, SQL pipeline construction/evolution, and strategic data analysis.

## Executive Summary
DAComp introduces the first comprehensive benchmark for evaluating data agents across the complete data intelligence lifecycle. It features 210 tasks combining repository-level engineering with open-ended analysis over enterprise schemas. The benchmark exposes critical gaps in autonomous enterprise data agents, with state-of-the-art models achieving under 20% success on engineering tasks and under 40% on analysis tasks. DAComp provides rigorous evaluation through execution-based metrics for deterministic tasks and hierarchical rubric-guided LLM judging for open-ended analysis.

## Method Summary
DAComp evaluates data agents through two main categories: DAComp-DE for repository-level data engineering (Architecture, Implementation, Evolution subtasks) and DAComp-DA for open-ended data analysis across five categories. DE tasks use execution-based metrics including Component Score (per-node evaluation), Cascading Failure Score (end-to-end DAG evaluation), and Success Rate (all-or-nothing). DA tasks combine hierarchical rubric scores with Good-Same-Bad LLM judging using a weighted formula. Agents are evaluated via OpenHands framework or custom DA-Agent implementations with 200 interaction round limits and 120s timeouts per action.

## Key Results
- State-of-the-art models achieve under 20% success on engineering tasks and under 40% on analysis tasks
- Cascading failure scores reveal orchestration bottlenecks, with CS→CFS→SR degradation patterns exposing pipeline-level weaknesses
- Open-ended analysis execution failures dominate (~59% of errors), not planning or interpretation issues
- Multi-agent DE-Impl evaluation shows significant variance across runs, suggesting sensitivity to initialization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cascading failure scoring reveals orchestration bottlenecks that component-level metrics miss
- Mechanism: CFS evaluates nodes sequentially along DAG and nullifies downstream scores when upstream dependencies fail, distinguishing isolated code correctness from end-to-end pipeline integrity
- Core assumption: Real-world data engineering failures propagate through dependencies, making local correctness insufficient
- Evidence anchors: Abstract states CFS exposes "pipeline-level orchestration bottleneck beyond single-file correctness"; section 3.3 shows pronounced drop from CS to CFS

### Mechanism 2
- Claim: Hierarchical rubrics with multiple solution paths enable fair evaluation of open-ended analysis without penalizing valid methodological diversity
- Mechanism: Rubrics decompose tasks into requirements and sub-requirements, enumerate distinct methodological paths per sub-requirement, and apply only best-matching path's scoring items
- Core assumption: Open-ended analytical questions admit multiple valid solution strategies with convergent conclusions
- Evidence anchors: Abstract mentions "guided by hierarchical, meticulously crafted rubrics"; section 2.2 explains path enumeration accommodates diverse correct approaches

### Mechanism 3
- Claim: Layered pipeline complexity creates non-linear difficulty escalation from staging to marts
- Mechanism: Staging handles basic cleaning with fewer errors; intermediate layer contains complex business logic generating most errors; marts inherits upstream failures while adding aggregation complexity
- Core assumption: Business logic complexity compounds with dependency depth
- Evidence anchors: Section 3.3 shows staging has fewest local errors while intermediate layer challenge "intensifies dramatically"

## Foundational Learning

- Concept: **DAG-based data pipeline architecture** (staging → intermediate/core → marts)
  - Why needed here: DAComp-DE requires understanding how data flows through transformation layers with dependencies; evaluation metrics assume this structure
  - Quick check question: Can you explain why a correct intermediate node might still score zero under CFS if a staging node fails?

- Concept: **Open-ended evaluation via rubric decomposition**
  - Why needed here: DA tasks have no single correct answer; understanding hierarchical rubrics is essential for interpreting DA scores
  - Quick check question: If an agent uses an unenumerated but valid analytical method, how should the rubric scoring handle it according to the paper's principle-based assessment?

- Concept: **Execution-based vs. judge-based evaluation tradeoffs**
  - Why needed here: DE tasks use deterministic execution metrics; DA/Arch tasks use LLM judges; understanding when each applies is critical
  - Quick check question: Why does the paper validate the LLM judge through human-model agreement, cross-judge consistency, and stochastic stability?

## Architecture Onboarding

- Component map: DAComp-DE (Architecture/Implementation/Evolution) → Execution metrics (CS/CFS/SR) | DAComp-DA (5 analysis categories) → Hierarchical rubric + GSB judge

- Critical path:
  1. Identify task type → select evaluation protocol
  2. For DE: Run pipeline in DuckDB → compute CS → CFS → SR
  3. For DA/Arch: Apply rubric path selection → aggregate item scores → combine with GSB comparison
  4. Validate judge alignment before drawing conclusions

- Design tradeoffs:
  - Strictness vs. diagnostic value: CS gives credit for isolated correctness; CFS reveals orchestration gaps; SR is binary but harsh
  - Rubric specificity vs. path coverage: More enumerated paths reduce false negatives but increase annotation cost
  - Judge model selection: Gemini-2.5-Flash chosen for cost/stability over potentially higher-accuracy alternatives

- Failure signatures:
  - High CS + low CFS: Orchestration bottleneck—correct code but fails dependency management
  - Cascading upstream errors: GPT-5 shows ~3x upstream vs. intrinsic errors in calculation logic
  - Missing dependencies bias in Evolution: Weaker models disproportionately fail to identify downstream impact
  - DA execution failures dominate: ~59% of errors from calculation/execution, not planning

- First 3 experiments:
  1. Run baseline evaluation on single DE-Impl task with all three metrics to observe CS→CFS→SR degradation
  2. Annotate simple DA task's rubric paths to understand how path selection affects scoring fairness
  3. Compare judge outputs across 2-3 LLM judges on fixed DA responses to measure cross-judge consistency

## Open Questions the Paper Calls Out
None

## Limitations
- GSB scoring mechanism lacks transparency in baseline report generation and selection criteria
- Cascading failure evaluation assumes deterministic execution paths, missing conditional branching and dynamic dependencies
- Multi-agent DE-Impl evaluation shows significant variance across runs, suggesting sensitivity to agent initialization

## Confidence
- **High confidence**: CS vs CFS comparison revealing orchestration bottlenecks; quantitative results showing model performance gaps
- **Medium confidence**: Hierarchical rubric mechanism for fair open-ended evaluation; layered pipeline complexity findings
- **Low confidence**: Cross-task generality of GSB judge reliability; long-term stability of agent performance across diverse schemas

## Next Checks
1. Conduct cross-schema generalization study: evaluate same agent across 10+ distinct enterprise schemas to test performance pattern stability
2. Implement conditional branching support in DAG evaluation: modify CFS to handle if-else logic and dynamic dependencies, then re-run core experiments
3. Perform ablation study on GSB components: systematically remove either hierarchical rubric or GSB comparison in DA scoring to quantify each component's contribution to final rankings