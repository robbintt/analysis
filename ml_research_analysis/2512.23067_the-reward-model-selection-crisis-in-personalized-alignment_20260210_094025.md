---
ver: rpa2
title: The Reward Model Selection Crisis in Personalized Alignment
arxiv_id: '2512.23067'
source_url: https://arxiv.org/abs/2512.23067
tags:
- reward
- accuracy
- policy
- alignment
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses a critical evaluation gap in personalized
  alignment research, demonstrating that standard reward model accuracy metrics fail
  to predict deployment performance under inference-time adaptation constraints. Through
  systematic evaluation across three datasets and four model scales, the authors show
  that upstream reward model accuracy correlates weakly with downstream policy accuracy
  (Kendall's tau = 0.08-0.31) when using reward-guided decoding.
---

# The Reward Model Selection Crisis in Personalized Alignment

## Quick Facts
- arXiv ID: 2512.23067
- Source URL: https://arxiv.org/abs/2512.23067
- Reference count: 40
- Primary result: Standard reward model accuracy metrics fail to predict deployment performance under inference-time adaptation constraints

## Executive Summary
This paper reveals a fundamental evaluation crisis in personalized alignment research. Through systematic experiments across three datasets and four model scales, the authors demonstrate that standard reward model (RM) accuracy metrics correlate weakly with deployment performance under reward-guided decoding (RGD). They introduce Pref-LaMP, the first personalized alignment benchmark with ground-truth user completions, enabling direct behavioral evaluation. The results show that methods with large RM accuracy differences produce nearly identical output quality, and that simple in-context learning (ICL) dominates all reward-guided methods for models > 3B parameters, achieving 3-5 point ROUGE-1 gains over the best reward method at 7B scale.

## Method Summary
The paper evaluates personalized alignment methods under realistic deployment constraints where only inference-time adaptation is possible. It introduces Pref-LaMP, a benchmark constructed from LaMP-5 by mining hard negative demonstrations using Qwen3-Embedding-0.6B. The framework evaluates both discriminative ranking (RM accuracy) and generation quality (behavioral alignment via ROUGE/BERTScore against ground truth). Four main methods are compared: Global RM (non-personalized), LoRE, PReF, and ICL-RAG (retrieved demonstrations). All RMs use LoRA rank-8 adaptation. RGD employs the scoring function `log π + λ·r` where λ is tuned to maximize policy accuracy. Evaluation spans model scales from 135M to 7B parameters across TLDR, PRISM, and Pref-LaMP datasets.

## Key Results
- RM accuracy correlates only weakly with policy-level discrimination ability (Kendall's τ = 0.08–0.31)
- Methods with 20-point RM accuracy differences produce nearly identical output quality
- Simple ICL-RAG dominates all reward-guided methods at 3B+ scales, achieving 3-5 point ROUGE-1 gains at 7B
- High win rates (>95%) from circular evaluation do not translate to behavioral alignment improvements

## Why This Works (Mechanism)

### Mechanism 1: RM-Policy Decoupling Under RGD
Reward model ranking accuracy does not predict whether that reward model can guide token-level generation effectively. RGD uses the reward model to score candidate tokens at each generation step via `score(v|x,y_{<t}; z_k, λ) = log π(v|x,y_{<t}) + λ·r_{θ,z_k}(v|x,y_{<t})`. This requires reward models to provide meaningful local guidance, not just correct final rankings. A model that ranks complete responses correctly may give misleading token-level signals.

### Mechanism 2: Circular Evaluation Inflation via Reward Hacking
Evaluating RGD outputs using the same reward model that guided generation produces artificially high win rates that don't reflect behavioral alignment. When the same RM guides decoding and judges outputs, policies can exploit reward model artifacts rather than capturing true preferences. Methods with 20-point RM accuracy differences achieve nearly identical ROUGE scores despite vastly different claimed win rates.

### Mechanism 3: ICL-RAG Scaling Advantage
In-context learning with retrieved demonstrations outperforms all reward-guided methods at 3B+ scale. Larger models effectively leverage retrieved preference demonstrations directly in context, avoiding the information loss from compressing preferences into parametric reward functions. ICL-RAG achieved ~49 ROUGE-1 at 7B with 8 shots versus ~46 for best RM method.

## Foundational Learning

- **Reward-Guided Decoding (ARGS)**: The scoring function `log π + λ·r` combines base policy with reward guidance. Needed to understand why sequence-level RM accuracy doesn't predict token-level guidance quality.
  - Quick check: Can you explain why a reward model might correctly rank "response A > response B" but still provide poor token-level guidance during RGD?

- **Policy Accuracy vs. RM Accuracy**: Policy accuracy measures whether RGD scoring function correctly discriminates preferred/dispreferred responses, while RM accuracy measures standalone ranking. Needed to understand the key contribution distinguishing these metrics.
  - Quick check: Given a reward model with 85% RM accuracy but 55% policy accuracy under RGD, would you deploy it? What additional evidence would you need?

- **Ground-Truth Behavioral Evaluation**: Using actual user completions for evaluation rather than reward scores to avoid circular evaluation. Needed to understand why circular evaluation creates "reward hacking" vulnerability.
  - Quick check: Why does using the same reward model for both guiding generation and evaluating outputs create a "reward hacking" vulnerability?

## Architecture Onboarding

- **Component map**: Preference data → RM training on U_train → User adaptation via `z_k = A(D_support; θ)` → RGD deployment with personalized reward → Behavioral evaluation against ground truth

- **Critical path**: Preference dataset partitioned into `U_train` (shared structure learning) and `U_adapt` (few-shot personalization with support/query splits) → Reward Model with shared parameters θ and user-specific z_k trained via LoRA rank-8 → RGD scoring combining base policy log-probability with weighted reward → ICL-RAG alternative retrieving user demonstrations via embedding similarity → Evaluation using RM accuracy, policy accuracy, and behavioral alignment metrics

- **Design tradeoffs**: ARGS λ selection maximizes policy accuracy on train users but higher λ increases reward influence while risking incoherence; LoRA rank-8 balances adaptation capacity with overfitting risk; ICL vs RM tradeoff between simplicity/demonstration dependency versus amortized preference learning

- **Failure signatures**: High RM accuracy + low policy accuracy indicates reward model provides poor token-level signals despite good final rankings; near-100% win rate + flat ROUGE indicates circular evaluation artifact; personal RMs underperforming Global RM suggests personalization failing to capture user-specific signal

- **First 3 experiments**:
  1. Train multiple RM architectures on TLDR/PRISM, compute both RM accuracy and policy accuracy across scales (135M-7B). Expect weak correlation (τ < 0.3) as shown in Table 2.
  2. Compare win rates (RM judging its own outputs) against ROUGE scores on Pref-LaMP. Expect 95%+ win rates with negligible ROUGE improvements.
  3. At 3B+ scale, compare ICL-RAG with varying shot counts (2, 4, 8) against best RM+RGD method. Expect ICL-RAG to match or exceed RM methods at ≥4 shots per Figure 3.

## Open Questions the Paper Calls Out

- Does the weak correlation between reward model accuracy and policy performance persist across non-stylistic domains like reasoning or code generation? The authors note "dataset dependence" and call for "larger multi-dataset benchmark suites" but it's unclear if complex reasoning tasks suffer the same discrimination-generation decoupling.

- Can alternative reward model factorizations overcome the token-wise guidance errors inherent in current RGD methods? Even GenARM, designed for autoregressive guidance, failed to bridge the gap, suggesting standard factorizations are insufficient.

- Is the decoupling of preference discrimination and generation capability fundamental to the inference-time adaptation paradigm? The authors state "the disconnect... may be fundamental to the inference-time adaptation paradigm" as simple ICL outperformed complex RGD with all reward-based methods showing 20-point accuracy differences yielding identical outputs.

## Limitations

- Ground-truth evaluation scarcity: Pref-LaMP's ground-truth user completions are rare in practice, limiting generalizability as most personalization scenarios lack reference completions for behavioral evaluation
- Inference-time optimization assumption: Findings may not hold under alternative adaptation strategies like per-user RL fine-tuning or direct parameter adaptation
- Single model family bias: All experiments use Qwen2.5 variants, potentially limiting generalizability across different architectures

## Confidence

**High confidence**: RM accuracy correlates weakly with policy accuracy under RGD (τ = 0.08-0.31); Circular evaluation inflates win rates without improving behavioral alignment; ICL-RAG outperforms reward methods at 3B+ scales when demonstrations are available

**Medium confidence**: Simple ICL-RAG dominates all reward-guided methods at scale; Personal RMs fail to generate behaviorally aligned responses despite high discrimination; Pref-LaMP reveals decoupling between discriminative ranking and generation quality

**Low confidence**: The proposed framework generalizes to all personalization scenarios; ICL-RAG's advantage holds across all preference types and domains; The RM-policy decoupling represents a fundamental limitation of reward modeling

## Next Checks

1. Cross-architecture validation: Reproduce the RM-policy correlation analysis using different model families (e.g., Llama, Mistral) and training approaches to assess architectural dependence

2. Alternative deployment path comparison: Compare RGD performance against per-user RL fine-tuning or direct parameter adaptation on the same datasets to determine if the decoupling is specific to RGD

3. Sparse demonstration evaluation: Systematically reduce demonstration availability in ICL-RAG to identify the break-even point where reward models become more competitive, mapping the boundary conditions for each approach