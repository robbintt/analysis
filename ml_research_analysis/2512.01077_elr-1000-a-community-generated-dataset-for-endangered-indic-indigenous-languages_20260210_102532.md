---
ver: rpa2
title: 'ELR-1000: A Community-Generated Dataset for Endangered Indic Indigenous Languages'
arxiv_id: '2512.01077'
source_url: https://arxiv.org/abs/2512.01077
tags:
- cultural
- translation
- languages
- language
- recipes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ELR-1000, a dataset of 1,060 traditional
  recipes in 10 endangered Indic languages collected from rural communities using
  a mobile interface designed for low digital literacy. The dataset includes multimodal
  data (text, images, audio) and parallel English translations for evaluation.
---

# ELR-1000: A Community-Generated Dataset for Endangered Indic Indigenous Languages

## Quick Facts
- **arXiv ID**: 2512.01077
- **Source URL**: https://arxiv.org/abs/2512.01077
- **Reference count**: 37
- **Primary result**: Context injection dramatically improves LLM translation quality for endangered languages, but models still struggle with cultural nuances and often substitute indigenous concepts with Western equivalents.

## Executive Summary
ELR-1000 introduces a dataset of 1,060 traditional recipes in 10 endangered Eastern Indic languages, collected directly from rural communities using a mobile interface designed for low digital literacy users. The dataset includes multimodal data (text, images, audio) with parallel English translations for evaluation. Experiments with six state-of-the-art LLMs reveal that without cultural and linguistic context, translations are often unusable and generate "fluent falsehoods"—grammatically correct but factually incorrect content. Context injection significantly improves translation quality across all models, though challenges remain with preserving cultural authenticity and indigenous concepts.

## Method Summary
The study translates traditional recipes from 10 endangered Eastern Indic languages (Bodo, Assamese, Meitei, Kaman-Mishmi, Khortha, Santhali, Ho, Sadri, Mundari, Khasi) into English using six LLMs under two conditions: no-context (direct translation) and contextual (with language background, cultural context, and few-shot examples). Evaluation uses four 5-point Likert scale metrics—Adequacy, Fluency, Comprehensibility, and Cultural and Contextual Appropriateness—with LLM-as-judge methodology using Gemini 2.5 Pro and GPT-5, plus human oversight on score discrepancies. The dataset was collected through a mobile app designed for low-literacy users, with recipes authored natively by community members.

## Key Results
- Without context, LLMs consistently generate fluent but factually incorrect translations, often hallucinating entirely different recipes
- Context injection dramatically improves translation quality across all six models, with some models scoring 4.0+ on adequacy metrics
- Even with context, models struggle with cultural nuances and frequently substitute indigenous concepts with Western equivalents (e.g., replacing silkworms with mushrooms)
- Community-sourced data preserves cultural-epistemic knowledge that translation-derived benchmarks cannot capture

## Why This Works (Mechanism)

### Mechanism 1: Context Injection Enables Low-Resource Translation
LLMs lack sufficient training data for endangered languages. Injecting language background, cultural context, and few-shot examples into prompts provides external scaffolding that compensates for missing pre-training knowledge, enabling cross-lingual transfer. This works because models possess transferable linguistic reasoning capabilities that can be activated with appropriate context even without direct language exposure. Evidence shows context dramatically improves translation quality from nonsensical output (scoring 1.0) to usable translations (scoring 4.0+).

### Mechanism 2: Community-Sourced Data Captures Cultural-Epistemic Knowledge
Data authored by native speakers in natural contexts preserves cultural and ecological knowledge that translation-derived benchmarks cannot capture. When speakers document practices in their own language without translation mediation, they retain indigenous terminology, traditional methods, seasonal knowledge, and material culture references that are typically lost when data is created by translating from high-resource languages.

### Mechanism 3: Fluent Falsehood Emerges from Training Distribution Bias
LLMs generate grammatically fluent but factually incorrect translations for endangered languages because they prioritize output plausibility over factual accuracy when training data is sparse. Models trained predominantly on high-resource languages learn to produce fluent outputs as a default. When faced with low-resource language inputs they cannot reliably interpret, they generate plausible-seeming content based on dominant-language priors rather than source-text meaning.

## Foundational Learning

- **Low-Resource Language Translation Gap**
  - Why needed here: Understanding why standard NMT fails for languages without parallel corpora explains why context injection is necessary.
  - Quick check question: Can you explain why a model trained on 101 languages still fails on languages represented in that set?

- **Cultural Grounding in NLP**
  - Why needed here: The paper's central contribution is demonstrating that culturally grounded data differs fundamentally from translated benchmarks.
  - Quick check question: What information is lost when a recipe is translated from English to an endangered language versus authored natively?

- **Human-in-the-Loop Evaluation**
  - Why needed here: Automated metrics (BLEU, chrF) failed to capture the "fluent falsehood" problem; human evaluation was essential.
  - Quick check question: Why would a translation score high on BLEU but be completely wrong in meaning?

## Architecture Onboarding

- **Component map**: Data Collection Layer (Karya mobile app) → Data Storage Layer (modular array structure) → Translation Layer (LLM inference with context) → Evaluation Layer (human translations + LLM judge ensemble)

- **Critical path**: 1) Community trust-building via local coordinators (weeks) → 2) Participant training on app usage (in-person, iterative) → 3) Recipe documentation with multimodal capture → 4) Two-layer validation (local + cross-regional) → 5) Subset translation by bilingual native speakers → 6) LLM evaluation under context/no-context conditions

- **Design tradeoffs**: 
  - Flexibility vs. Standardization: Allowing free-form documentation captured authentic cultural content but created heterogeneous data requiring extensive normalization
  - Fixed Payment vs. Effort-Based Compensation: Equal pay per recipe was culturally appropriate but didn't reflect varying complexity
  - Context Length vs. Model Limits: Detailed cultural context improves translation but risks exceeding context windows

- **Failure signatures**:
  - Hallucinated recipes: Models generate plausible but entirely unrelated dishes (e.g., bamboo shoots substituted for star fruit)
  - Tool normalization: Models insert Western implements (chopping boards) absent in indigenous kitchens
  - Ingredient substitution: Culturally specific items (silkworms, forest greens) replaced with generic alternatives (mushrooms, chicken)
  - Fluency-accuracy gap: High grammatical correctness scores with low meaning preservation

- **First 3 experiments**:
  1. Baseline translation test: Run all 6 models on held-out recipes with no context to establish failure modes
  2. Context ablation study: Systematically remove components of the context prompt to identify driving factors
  3. Error taxonomy construction: Manually categorize translation errors to distinguish linguistic from cultural-epistemic failures

## Open Questions the Paper Calls Out

### Open Question 1
How do cultural concepts from endangered languages propagate within LLM embeddings, and can internal probing methods reveal where cultural grounding breaks down in the translation pipeline? The paper suggests moving beyond surface-level evaluation to explicitly probe LLMs' cultural awareness through internal probing methods that examine how cultural concepts are represented in model embeddings.

### Open Question 2
What is the minimum amount of parallel data required to meaningfully improve LLM translation quality for endangered languages with virtually no digital presence? The dataset is designed for evaluation, not training, and the threshold for effective fine-tuning on endangered languages remains unknown.

### Open Question 3
Can automated evaluation metrics be developed that reliably distinguish "fluent falsehoods" from faithful translations in culturally-specific content? Standard metrics reward surface fluency over factual accuracy, and current LLM-based evaluators may share the same cultural blind spots as translation models.

### Open Question 4
Does culturally grounded knowledge transfer across domains (e.g., from recipes to agricultural practices) within the same endangered language community? The dataset is domain-specific, and it's unclear whether models trained on culturally-grounded recipes would generalize to other culturally-rooted knowledge domains.

## Limitations
- The dataset contains only 3 human-translated recipes per language for evaluation, which may not be representative of linguistic and cultural diversity
- Evaluation relies entirely on LLM-as-judge methodology with human oversight only on score discrepancies, rather than direct human evaluation of all translations
- Context injection was tested with a specific prompt template that may not generalize to other endangered language contexts or prompt engineering approaches

## Confidence
- **High Confidence**: The core finding that LLMs without context generate fluent but factually incorrect translations is well-supported by systematic error patterns across multiple models and languages
- **Medium Confidence**: The claim that context injection dramatically improves translation quality is supported by the data, but the specific mechanisms by which different context components contribute to improvement remain unseparated
- **Medium Confidence**: The assertion that community-sourced data preserves cultural-epistemic knowledge better than translation-derived benchmarks is theoretically sound and supported by qualitative examples
- **Low Confidence**: Claims about generalizability to other endangered language domains or different types of cultural knowledge preservation are speculative and not directly tested

## Next Checks
1. **Error Type Distribution Analysis**: Manually categorize translation errors across the 30 evaluation recipes to quantify the relative frequency of linguistic failures versus cultural-epistemic failures
2. **Context Component Ablation Study**: Systematically remove individual context components (language background, few-shot examples, cultural guidelines) from the context prompt to identify which elements drive the observed improvements
3. **Cross-Domain Generalization Test**: Apply the same translation and evaluation methodology to a different domain of indigenous knowledge (e.g., traditional medicine, agricultural practices) to assess whether the "fluent falsehood" phenomenon and context injection benefits generalize beyond culinary knowledge