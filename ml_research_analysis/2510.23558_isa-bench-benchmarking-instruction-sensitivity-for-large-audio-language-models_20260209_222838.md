---
ver: rpa2
title: 'ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language Models'
arxiv_id: '2510.23558'
source_url: https://arxiv.org/abs/2510.23558
tags:
- instruction
- audio
- lalms
- sensitivity
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ISA-Bench is a benchmark that evaluates instruction sensitivity
  in large audio language models (LALMs) across three dimensions: instruction description,
  output format, and task composition. The benchmark reveals that even state-of-the-art
  LALMs struggle with instruction sensitivity, showing significant performance degradation
  when instructions vary.'
---

# ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language Models

## Quick Facts
- **arXiv ID:** 2510.23558
- **Source URL:** https://arxiv.org/abs/2510.23558
- **Reference count:** 0
- **Key outcome:** ISA-Bench evaluates instruction sensitivity in LALMs across three dimensions, revealing significant performance degradation when instructions vary and catastrophic forgetting during mitigation fine-tuning.

## Executive Summary
ISA-Bench introduces a comprehensive benchmark for evaluating instruction sensitivity in large audio language models (LALMs) across three dimensions: instruction description, output format, and task composition. The benchmark reveals that even state-of-the-art LALMs struggle with instruction sensitivity, showing significant performance degradation when instructions vary. Experiments with nine LALMs on five tasks show average instruction-following rates of 70-85% and relative performance scores of 50-80%, depending on the dimension and model. While supervised fine-tuning on diverse instruction variants improves compliance by up to 100%, it also causes catastrophic forgetting, where models lose previously mastered task capabilities. This indicates that current LALMs are not robust to instruction variations, and more sophisticated solutions are needed to improve instruction-following robustness in real-world deployment scenarios.

## Method Summary
ISA-Bench evaluates nine LALMs across five public audio datasets (LibriSpeech, CoV oST2, IEMOCAP, AudioCaps) using three instruction sensitivity dimensions: D (description variations like case changes and semantic complexity), F (output format constraints like JSON and case sensitivity), and N (task composition permutations). The benchmark employs compliance-aware metrics (MetricIF) that credit task performance only when outputs meet required formats, alongside instruction-following rate (IFR) and relative performance score (RPS). Instruction variants are generated via GPT-4 rewriting of default prompts. A mitigation experiment applies supervised fine-tuning on Qwen2-Audio with diverse instruction variants, revealing improved compliance but catastrophic forgetting of previously mastered capabilities.

## Key Results
- Instruction-following rates across nine LALMs range from 70-85% on average, with significant degradation when instruction variants vary
- Relative Performance Scores show 50-80% degradation across models depending on instruction dimension and task
- Supervised fine-tuning improves instruction-following by up to 100% but causes catastrophic forgetting of mastered task capabilities
- No single model consistently excels across all tasks and dimensions, with Qwen2-Audio performing best on ASR but worst on format constraints

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Varying instruction descriptions degrades LALM task performance even when task intent remains unchanged.
- **Mechanism:** LALMs trained on specific instruction phrasings during SFT develop overfitting to those surface forms; when deployment instructions diverge (case changes, syntax errors, semantic complexity shifts), the model's learned instruction-to-behavior mapping fails to generalize, producing lower compliance and accuracy.
- **Core assumption:** Instruction-following capability in LALMs is partly memorized rather than abstractly understood.
- **Evidence anchors:**
  - [abstract] "existing LALMs are highly sensitive to how instructions are phrased, affecting both (i) instruction-following rates and (ii) task performance"
  - [section 1] "published evaluations of LALMs mostly use instruction forms that are seen during supervised fine-tuning (SFT), giving an upper-bound of performance estimate"
  - [corpus] AHAMask paper confirms "they usually suffer from prompt sensitivity, where different instructions of the same intention can yield drastically different outcomes"

### Mechanism 2
- **Claim:** Output format constraints create a dual-objective burden where models must simultaneously satisfy structural compliance and acoustic task accuracy.
- **Mechanism:** LALMs must allocate representational capacity to both (a) parsing and executing format requirements (JSON structure, prefix/suffix tags, case constraints) and (b) performing the core audio understanding task. When format complexity increases, models with limited instruction-following training fail to maintain both objectives, resulting in either format-compliant but inaccurate outputs or accurate but non-compliant responses.
- **Core assumption:** Format compliance and task accuracy compete for model capacity during inference.
- **Evidence anchors:**
  - [section 3.4] "MetricIF credits task performance only when the response satisfies the required format"
  - [section 4.2] "Gemini-2.5-Pro handles most variations in the D and F dimensions, but fails under JSON-style formatting constraints"
  - [corpus] Limited direct corpus evidence on format-constraint mechanisms; primarily inferred from ISA-Bench results.

### Mechanism 3
- **Claim:** Supervised fine-tuning on diverse instruction variants improves compliance but triggers catastrophic forgetting of previously mastered task capabilities.
- **Mechanism:** When LALMs are fine-tuned on new instruction style variants, gradient updates modify shared weights that also encode previously learned task competencies. Without explicit retention mechanisms (replay data, regularization, or parameter-isolation strategies), these updates overwrite task-specific knowledge, causing models to either reproduce training-like responses or refuse to answer on out-of-distribution samples.
- **Core assumption:** Task knowledge and instruction-style knowledge share overlapping parameters without isolation.
- **Evidence anchors:**
  - [abstract] "supervised fine-tuning on diverse instruction variants improves compliance by up to 100%, it also causes catastrophic forgetting"
  - [section 4.3] "models might suffer catastrophic forgetting cases: they lose previously mastered capabilities when fine-tuned on new instruction variants, only reproducing a few responses similar to those seen during training"
  - [corpus] Speech-IFEval paper (cited as [8]) addresses "quantifying catastrophic forgetting in speech-aware language models"

## Foundational Learning

- **Concept: LALM Architecture (Audio Encoder → Projector → LLM)**
  - **Why needed here:** ISA-Bench evaluates end-to-end systems; understanding where instruction processing occurs (LLM backbone) versus where acoustic features are extracted (encoder) clarifies why instruction sensitivity manifests despite robust audio perception.
  - **Quick check question:** If you froze the audio encoder and only fine-tuned the LLM backbone on instruction variants, would you expect D-dimension sensitivity to change?

- **Concept: Compliance-Aware Metrics (MetricIF)**
  - **Why needed here:** The benchmark doesn't just measure task accuracy—it penalizes non-compliant outputs by scoring them as empty responses. This dual evaluation reveals where models fail (format vs. content).
  - **Quick check question:** A model achieves 90% WER accuracy but only 60% format compliance—what is its likely MetricIF score relative to a model with 80% accuracy and 95% compliance?

- **Concept: Catastrophic Forgetting in Sequential Learning**
  - **Why needed here:** The paper's mitigation experiments show SFT improves instruction-following but degrades prior capabilities. Recognizing this trade-off is essential before proposing solutions.
  - **Quick check question:** Why might training on diverse instruction variants overwrite task-specific weights rather than augmenting them?

## Architecture Onboarding

- **Component map:** Audio Input → [Audio Encoder] → Acoustic Representations → [Projector Layer] → LLM Embedding Space ← Text Instruction → [LLM Backbone] → Text Output
- **Critical path:**
  1. Instruction tokenization and embedding (LLM backbone handles all text processing)
  2. Audio encoding and projection (parallel pathway)
  3. Cross-modal fusion within LLM attention layers
  4. Output generation with format constraints applied via instruction conditioning

- **Design tradeoffs:**
  - **End-to-end fine-tuning vs. frozen encoder:** Full fine-tuning improves task performance but increases forgetting risk; frozen encoder preserves acoustic capabilities but limits adaptation.
  - **Diverse instruction training vs. stability:** More instruction variants improve robustness (D/F-dimensions) but exacerbate catastrophic forgetting without regularization.
  - **MetricIF vs. raw task metrics:** Compliance-aware scoring better reflects deployment reality but may underrepresent acoustic capability in research comparisons.

- **Failure signatures:**
  - **Format non-compliance:** Model outputs correct content but ignores JSON structure, case constraints, or prefix requirements → check F-dimension training coverage.
  - **Task performance collapse after SFT:** Model refuses to answer or produces generic responses → catastrophic forgetting; investigate replay data or parameter isolation.
  - **Inconsistent performance across instruction paraphrases:** Large variance in accuracy across D-dimension variants → instruction diversity insufficient during training.

- **First 3 experiments:**
  1. **Baseline profiling:** Run ISA-Bench D/F/N dimensions on your target LALM; identify weakest dimension and compare against Qwen2-Audio and DeSTA2.5-Audio baselines reported in Figure 1.
  2. **Instruction diversity ablation:** Fine-tune on 3 levels of instruction variant diversity (baseline, 2× variants, 5× variants) and measure both IFR improvement and task metric degradation to quantify the compliance-forgetting trade-off.
  3. **Format-constraint isolation:** Test your model on F-dimension variants only (holding task and description constant) to determine if format-following is the bottleneck; if JSON failures dominate, consider pre-training format compliance as a separate objective.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What training strategies beyond simple supervised fine-tuning can improve instruction-following robustness in LALMs without inducing catastrophic forgetting of previously mastered task capabilities?
- **Basis in paper:** [explicit] The authors state that "SFT remains insufficient: fine-tuning with diverse instructions can improve instruction following ability, but it often leads to catastrophic forgetting on mastered tasks. This finding highlights the inherent difficulty of the instruction sensitivity problem and suggests that more sophisticated solutions are required."
- **Why unresolved:** The paper demonstrates the trade-off empirically—SFT improved instruction-following rates by up to 100% in the N-dimension, but caused models to lose previously mastered capabilities—but offers no solution.
- **What evidence would resolve it:** A training methodology that achieves high instruction-following rates across D, F, and N dimensions while maintaining or improving baseline task performance metrics (WER, BLEU, ACC, METEOR) on held-out test sets.

### Open Question 2
- **Question:** Can scaling to significantly larger and more diverse instruction-variant datasets resolve instruction sensitivity, or are architectural modifications to LALMs necessary?
- **Basis in paper:** [explicit] The authors suggest "Strategies such as those employed in DeSTA2.5-Audio or scaling to much larger, diverse data sets may be considered for instruction sensitivity improvement" but do not test either approach systematically.
- **Why unresolved:** The paper only experiments with limited SFT data constructed from training subsets of test sets; the relationship between data scale/diversity and instruction robustness remains unexplored.
- **What evidence would resolve it:** Controlled experiments varying instruction dataset size and diversity, measuring the resulting instruction-following rates and RPS scores across all three dimensions, compared against architectural interventions.

### Open Question 3
- **Question:** Why do state-of-the-art LALMs exhibit inconsistent performance patterns across different tasks and instruction dimensions, with no single model excelling universally?
- **Basis in paper:** [inferred] The paper reports that "no model consistently excels across all three tasks and both dimensions" and shows that Qwen2-Audio performs best on ASR in the D-dimension but worst in the F-dimension, yet does not investigate the underlying causes.
- **Why unresolved:** The benchmark reveals performance inconsistencies but the paper does not analyze whether these stem from training data distribution, model architecture, attention mechanisms, or other factors.
- **What evidence would resolve it:** Ablation studies correlating specific training data characteristics or architectural components with performance on each dimension, potentially using probing tasks to identify where instruction processing fails.

## Limitations
- Instruction variant generation methodology and compliance regex patterns are not fully specified, limiting exact reproduction
- Catastrophic forgetting mitigation is demonstrated only on Qwen2-Audio, not across the nine evaluated models
- Compliance-aware metrics may underrepresent true acoustic capabilities when format constraints dominate scoring

## Confidence
- **High confidence** in core observation of instruction sensitivity across all three dimensions with systematic evaluation of nine models
- **Medium confidence** in catastrophic forgetting mechanism during SFT mitigation, demonstrated on single model without ablation studies
- **Low confidence** in specific instruction variant generation methodology and compliance regex patterns essential for exact reproduction

## Next Checks
1. **Instruction variant ablation study:** Replicate benchmark with systematically varied instruction diversity levels (baseline, 2×, 5× variants) to quantify compliance-forgetting trade-off
2. **Format constraint isolation:** Evaluate models on F-dimension variants only while holding task and description constant to determine if format-following is primary bottleneck
3. **Cross-model forgetting analysis:** Apply SFT mitigation across multiple LALMs with identical hyperparameters to assess consistency of catastrophic forgetting patterns