---
ver: rpa2
title: Towards Tensor Network Models for Low-Latency Jet Tagging on FPGAs
arxiv_id: '2601.10801'
source_url: https://arxiv.org/abs/2601.10801
tags:
- tensor
- latency
- each
- trigger
- particle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores Tensor Network (TN) models as low-latency alternatives
  to deep neural networks for jet tagging in high-energy physics. Motivated by the
  HL-LHC Level-1 trigger's strict timing and resource constraints, the authors implement
  and deploy Matrix Product States (MPS) and Tree Tensor Networks (TTN) on FPGAs for
  real-time jet substructure classification.
---

# Towards Tensor Network Models for Low-Latency Jet Tagging on FPGAs

## Quick Facts
- arXiv ID: 2601.10801
- Source URL: https://arxiv.org/abs/2601.10801
- Reference count: 0
- Primary result: Tensor Network models achieve competitive jet tagging accuracy (66-77%) with sub-microsecond FPGA inference latency

## Executive Summary
This work explores Tensor Network (TN) models as low-latency alternatives to deep neural networks for jet tagging in high-energy physics. Motivated by the HL-LHC Level-1 trigger's strict timing and resource constraints, the authors implement and deploy Matrix Product States (MPS) and Tree Tensor Networks (TTN) on FPGAs for real-time jet substructure classification. Jets are represented using low-level particle features, and tensor network models are trained to classify jets originating from light quarks, gluons, W/Z bosons, or top quarks. The study systematically evaluates model performance under quantization and compares it to state-of-the-art deep learning classifiers, achieving competitive results. Both architectures are synthesized for FPGA deployment, demonstrating sub-microsecond inference latency and efficient resource usage. Post-training quantization further reduces hardware demands without significant accuracy loss. Overall, the study demonstrates the feasibility and advantages of TN-based models for ultra-fast, interpretable, and resource-efficient inference in low-latency environments.

## Method Summary
The authors develop Tensor Network models (MPS and TTN) for jet tagging using low-level particle features. Jets are represented by their constituents with features (pT, E_rel, ΔR) that are polynomially embedded into higher dimensions. MPS arranges tensors in a chain while TTN uses a hierarchical tree structure, both with tunable bond dimensions to control model capacity. Models are trained on jet substructure data to classify jet origins. The architectures are then synthesized for FPGA deployment using HLS for MPS and VHDL for TTN, with post-training quantization to reduce resource usage while maintaining accuracy.

## Key Results
- TN models achieve 66-77% accuracy across jet types (quark, gluon, W, Z, top) with 8-32 particles
- FPGA synthesis achieves sub-microsecond inference latency with efficient DSP/LUT usage
- Post-training quantization reduces DSP usage by up to 69% with minimal accuracy loss
- TTN implementation achieves zero DSP usage while MPS requires DSPs (129% utilization for N=32)
- Hardware implementation uses up to 22% LUTs, 1% BRAMs, and 5% FFs on VU7P FPGA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Polynomial feature embedding preserves tensor product structure while capturing second-order feature interactions
- Mechanism: Each particle's features (pT, E_rel, ΔR) are expanded via polynomial map φ(xi) to dimension d=7, then combined as tensor product Φ(x) = ⨂i=1N φ(xi). This transforms the problem from O(dN) to O((nd)N) where polynomial degree controls non-linearity.
- Core assumption: Jet classification depends on low-order polynomial relationships between kinematic features
- Evidence anchors:
  - [abstract] "Using low-level jet constituent features, our models achieve competitive performance"
  - [section] Eq. 1 defines polynomial embedding: φ(xi) = 1/C{1, pT, E_rel, ΔR, pT², E_rel², ΔR²}
  - [corpus] MLP-Mixer paper achieves similar performance with constituent-level processing, suggesting low-level features suffice
- Break condition: If jet classification requires higher-order interactions (degree >2) or non-polynomial feature relationships

### Mechanism 2
- Claim: Bond dimension constrains expressivity to capture only relevant correlations, enabling compact models
- Mechanism: MPS limits correlations via linear chain with bond dimension D (parameters scale O(NdD²) vs O(dN)). TTN uses hierarchical tree structure with max bond dimension χ. Quantum Mutual Information analysis reveals which feature correlations require larger bonds—permuting features to group correlated ones nearby reduces required D.
- Core assumption: Jet physics correlations are approximately local or hierarchically factorizable
- Evidence anchors:
  - [abstract] "TNs as compact and interpretable alternatives to deep neural networks"
  - [section] Figure 2 shows QMI analysis: unpermuted features show long-range correlations (2a), permuted features show localized correlations (2b), enabling smaller bond dimension
  - [section] Table I: Bond dimension D=10, χ=10 achieves 66-77% accuracy across N={8,16,32}
  - [corpus] Limited direct evidence—neighboring papers use DNNs/Transformers without bond dimension analysis
- Break condition: If correlations require bond dimension approaching D ≈ dN/2 (exact representation), negating compression benefit

### Mechanism 3
- Claim: Post-training quantization exploits TN linearity for hardware efficiency with graceful accuracy degradation
- Mechanism: Weights quantized to fixed-point (2-bit integer + variable fractional bits). TN's linear tensor contractions avoid gradient-related quantization sensitivity in DNNs. TTN shows stronger robustness (plateau at FB≥6) vs MPS (plateau at FB≥8).
- Core assumption: Quantization noise distributes uniformly and model is not critically sensitive to weight precision
- Evidence anchors:
  - [abstract] "Post-training quantization further reduces hardware demands without significant accuracy loss"
  - [section] Figure 6-7: Accuracy curves show plateau then degradation; TTN more robust than MPS
  - [section] Table II: Reducing FB from 14 to optimal values cuts DSP usage dramatically (e.g., MPS N=32: 129%→60% DSP)
  - [corpus] da4ml paper addresses distributed arithmetic for neural networks, suggesting quantization is active research area but specific TN quantization evidence limited
- Break condition: If accumulated quantization error in deep contraction chains exceeds decision margin

## Foundational Learning

- **Tensor Network Fundamentals (MPS/TTN)**
  - Why needed: TNs factorize high-dimensional tensors into low-rank structures. MPS arranges tensors in a chain; TTN in a binary tree. Bond dimension controls how much "entanglement" (correlation) the network can represent.
  - Quick check question: Given a 32-particle jet with 7-dimensional embedding per particle, what's the full Hilbert space dimension vs. MPS parameter count with D=10?

- **FPGA Resource Types (DSP, LUT, BRAM, FF)**
  - Why needed: Hardware deployment requires understanding resource budgets. DSPs handle multiplication; LUTs implement logic; BRAM stores weights; FF holds registers. Quantization trades DSP for LUT.
  - Quick check question: Why does reducing fractional bit width decrease DSP usage but may increase LUT usage?

- **Jet Physics Basics (Constituents, pT, ΔR)**
  - Why needed: Jet tagging classifies origin (quark, gluon, W, Z, top) using substructure. pT is transverse momentum; ΔR is angular distance from jet axis. Heavy particles (top, W/Z) produce distinct substructure.
  - Quick check question: Why might top quark jets have larger ΔR spread than gluon jets?

## Architecture Onboarding

- **Component map:**
  Input particles → Polynomial embedding → MPS/TTN tensor network → Contraction → Output vector (5 classes) → Softmax

- **Critical path:** Embedded input contraction dominates latency. MPS uses bidirectional parallel contractions toward output tensor. TTN uses layer-by-layer parallel contractions up the tree.

- **Design tradeoffs:**
  - MPS vs TTN: MPS simpler (HLS-friendly), TTN more parallelizable (lower latency in VHDL)
  - Bond dimension: Higher D/χ → better accuracy but O(D³) operations
  - Particle count N: More constituents → better accuracy (66%→77%) but O(N) parameters
  - Quantization: Lower FB → fewer DSPs but accuracy cliff below threshold

- **Failure signatures:**
  - Accuracy cliff at FB<8 (MPS) or FB<6 (TTN)—check quantization study curves
  - DSP overflow (>100% utilization)—reduce FB or model size
  - Latency exceeds budget (>1µs)—may need shallower tree or smaller bond dimension
  - Training divergence—check canonical form, loss function choice (cross-entropy for MPS, MSE for TTN)

- **First 3 experiments:**
  1. Replicate baseline: Train MPS with N=8, D=10, evaluate on hls4ml jet dataset, target ~66% accuracy
  2. Quantization sweep: Vary FB from 14 down to 4, plot accuracy to find optimal point before cliff
  3. Hardware synthesis: Synthesize quantized model on target FPGA (or XCVU13P), verify sub-microsecond latency and resource fit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ad hoc post-training quantization methods that exploit the inherent gauge freedom of tensor network architectures achieve higher compression rates or accuracy retention than standard techniques?
- Basis in paper: [explicit] The authors state that "future studies may investigate... the development of ad hoc post-training quantization methods exploiting the gauge freedom inherent in TN architectures."
- Why unresolved: The current study employs standard post-training quantization but does not utilize the specific mathematical structure (gauge freedom) of Tensor Networks to optimize the quantization process itself.
- What evidence would resolve it: A comparative study showing that gauge-aware quantization algorithms maintain classification accuracy at lower bit-widths or with fewer parameters than the uniform quantization methods presented in the paper.

### Open Question 2
- Question: Does combining quantum-inspired feature importance measures with quantization methods enable further compression of tensor network models for jet tagging?
- Basis in paper: [explicit] The conclusion suggests that "A quantum-inspired measure of feature importance, combined with quantization methods, has the potential to compress these models further."
- Why unresolved: While the paper uses Quantum Mutual Information (QMI) to analyze input correlations and aid embedding design, it has not yet integrated these measures directly into the model compression or quantization pipeline.
- What evidence would resolve it: Demonstrating a pruning or mixed-precision quantization algorithm guided by QMI that results in a smaller memory footprint or lower DSP usage without degrading the AUC scores.

### Open Question 3
- Question: Would a low-level VHDL implementation of the Matrix Product State (MPS) architecture eliminate the DSP usage and latency scaling overhead observed in the current HLS implementation?
- Basis in paper: [inferred] The paper notes that TTN (VHDL) uses zero DSPs, while MPS (HLS) consumes significant DSP resources (up to 129%). It also mentions that HLS infers register usage (nreg) which scales latency, a constraint not present in the direct VHDL control used for TTN.
- Why unresolved: The comparison between MPS and TTN is confounded by the implementation language (HLS vs. VHDL), leaving the intrinsic hardware efficiency of the MPS architecture itself unverified compared to TTN.
- What evidence would resolve it: Synthesizing the MPS model using a VHDL firmware flow to verify if it can match the TTN's zero-DSP utilization and deterministic timing, thereby isolating architectural efficiency from toolchain overhead.

### Open Question 4
- Question: What is the latency and resource overhead for performing the full inference pipeline entirely on-chip, including the input feature embedding and the final argmax classification decision?
- Basis in paper: [inferred] The authors explicitly state that "input embedding is therefore not implemented in hardware" and the "final classification decision is performed off-chip" to simplify the firmware test.
- Why unresolved: The reported sub-microsecond latencies apply only to the tensor contractions. For a real-world Level-1 trigger deployment, the preprocessing (embedding) and post-processing (argmax) must also occur on the FPGA within the strict 12.5 μs budget.
- What evidence would resolve it: Synthesizing the complete firmware including the polynomial feature map logic and the argmax layer to determine the total end-to-end latency and resource occupancy.

## Limitations

- Hardware verification limited to specific FPGA models; performance may vary across different FPGA families or ASIC implementations
- HLS-based MPS implementation may not achieve optimal resource utilization compared to hand-optimized VHDL/Verilog
- Generalization to other HEP ML tasks beyond jet tagging remains unexplored
- QMI-based feature permutation analysis considers only three features per particle and may not generalize to more complex feature sets

## Confidence

**High Confidence:** TN models achieve competitive jet tagging accuracy (66-77%) compared to DNNs; post-training quantization reduces DSP usage without catastrophic accuracy loss; hardware synthesis achieves sub-microsecond latency within FPGA resource constraints

**Medium Confidence:** Bond dimension optimization based on QMI analysis effectively reduces model size; TTN architecture provides better latency than MPS for same model capacity; polynomial embedding captures sufficient feature interactions for jet classification

**Low Confidence:** TN interpretability advantages over DNNs are demonstrated through correlation analysis but not formally quantified; the approach generalizes to other low-latency HEP applications beyond jet tagging; QMI-based feature permutation strategies transfer across different physics datasets

## Next Checks

1. **Cross-Validation on Independent Datasets:** Train and evaluate the TN models on jet datasets from different collision energies or detector configurations to verify generalization. Compare performance degradation relative to DNN baselines.

2. **Ablation Study on Polynomial Embedding:** Systematically vary the polynomial degree (n>2) and test non-polynomial feature transformations to quantify the embedding's contribution to accuracy. Compare with learned embeddings or attention-based alternatives.

3. **Resource-Accuracy Pareto Analysis:** Generate comprehensive curves showing accuracy vs. FPGA resource utilization (DSP, LUT, BRAM) across multiple model configurations (N, D, χ, FB). Identify optimal operating points for different trigger latency budgets and identify the accuracy cliff for each configuration.