---
ver: rpa2
title: 'From Observations to Causations: A GNN-based Probabilistic Prediction Framework
  for Causal Discovery'
arxiv_id: '2507.20349'
source_url: https://arxiv.org/abs/2507.20349
tags:
- causal
- graph
- methods
- edge
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a graph neural network (GNN)-based probabilistic
  framework for causal discovery that learns a probability distribution over causal
  graphs, addressing limitations of traditional methods that produce only a single
  deterministic graph. The approach uses a fully connected graph where nodes represent
  variables and edges encode statistical and information-theoretic features (114 per
  edge), including mutual information, conditional entropy, and polynomial fit metrics.
---

# From Observations to Causations: A GNN-based Probabilistic Prediction Framework for Causal Discovery

## Quick Facts
- arXiv ID: 2507.20349
- Source URL: https://arxiv.org/abs/2507.20349
- Authors: Rezaur Rashid; Gabriel Terejanu
- Reference count: 7
- One-line primary result: GNN-based framework learns probability distribution over causal graphs, outperforming traditional and recent methods on synthetic and real-world datasets

## Executive Summary
This paper introduces a graph neural network (GNN)-based probabilistic framework for causal discovery that learns a probability distribution over causal graphs, addressing limitations of traditional methods that produce only a single deterministic graph. The approach uses a fully connected graph where nodes represent variables and edges encode statistical and information-theoretic features (114 per edge), including mutual information, conditional entropy, and polynomial fit metrics. A GraphSAGE model predicts edge direction probabilities by integrating node and edge features through message passing. The framework generalizes from synthetic training data to real-world datasets without retraining, outperforming traditional methods (PC, GES), recent approaches (NOTEARS, DAG-GNN), and a GNN-based baseline (DAGMA) on synthetic and real-world data.

## Method Summary
The framework transforms tabular observational data into a fully connected graph where nodes represent variables and edges carry 114 statistical and information-theoretic features (e.g., mutual information, conditional entropy, polynomial fit errors). A GraphSAGE model performs message passing to update node embeddings by aggregating neighbor information, learning to predict edge direction probabilities (forward, reverse, or no edge). The model is trained on synthetic data generated from Structural Equation Models (SEMs) with ground truth DAGs. Four inference methods are implemented: GNN-PG (sample from edge probabilities), GNN-MLG (argmax per edge), GNN-PDAG (sample conditioned on Maximum Spanning DAG topological ordering), and GNN-MLDAG (greedy highest-prob edges respecting acyclicity). The framework generalizes from synthetic training data to real-world datasets without retraining.

## Key Results
- Lower Structural Hamming Distance (SHD), higher True Positive Rate (TPR), and reduced False Positive Rate (FPR) on synthetic datasets compared to traditional methods (PC, GES), recent approaches (NOTEARS, DAG-GNN), and DAGMA
- Consistently superior performance on Microsoft CSuite benchmarks and the Sachs protein network using probabilistic inference methods (GNN-PDAG and GNN-MLDAG)
- GNN-PDAG and GNN-MLDAG outperform raw probability thresholding (GNN-PG, GNN-MLG) by enforcing acyclicity while capturing uncertainty in causal relationships

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Encoding observational data into a fully connected graph with high-dimensional statistical features allows the model to approximate complex causal dependencies that raw correlation cannot capture.
- **Mechanism:** The framework transforms tabular data into a graph where edges carry 114 features (mutual information, conditional entropy, polynomial fit errors). This creates a rich latent space where causal direction becomes a separable classification problem. The GNN then learns to map these statistical fingerprints to edge probabilities ($i \to j$, $j \to i$, or none).
- **Core assumption:** The selected statistical and information-theoretic features (e.g., kurtosis, HSIC) are sufficient statistics for identifying causal direction across diverse datasets.
- **Evidence anchors:**
  - [Abstract]: "augmented with statistical and information-theoretic measures... capturing both local and global data properties."
  - [Section 3.1]: "We then extract statistical and information-theoretic measures... to represent each node with 13 features and each edge with 114 features..."
  - [Corpus]: Weak direct evidence; neighbor papers suggest GNNs are gaining traction for causal structure in bio-data, but do not validate this specific feature set.
- **Break condition:** If the data exhibits causal mechanisms that do not leave statistical footprints in the selected 114 metrics (e.g., complex dynamic systems requiring temporal features not present here), the model may fail to distinguish direction.

### Mechanism 2
- **Claim:** Message passing in the GNN allows the model to leverage global graph structure to resolve local ambiguities in edge direction.
- **Mechanism:** A GraphSAGE model aggregates information from neighbors. By updating a node's representation based on its incoming and outgoing edges, the model effectively contextualizes a potential edge $A \to B$ based on the broader structure surrounding $A$ and $B$. This helps identify v-structures (colliders) and chains which are statistically indistinguishable in pairwise analysis.
- **Core assumption:** Local neighborhoods (defined by the fully connected graph) provide the necessary context to enforce conditional independence constraints required for valid causal graphs.
- **Evidence anchors:**
  - [Abstract]: "encodes both node and edge attributes into a unified graph representation, enabling the model to learn complex causal structures directly from data."
  - [Section 3.2]: "This model captures both local and global dependencies... enhancing the accuracy of inferred causal relations between nodes considering their relationships with neighbors."
  - [Corpus]: General consensus in GNN literature supports GNNs' ability to aggregate neighborhood info, though specific validation for *causal* discovery is less cited.
- **Break condition:** If the training graphs are structurally dissimilar to the test graphs (e.g., vastly different node degrees), the learned aggregation weights may not generalize, leading to spurious edge predictions.

### Mechanism 3
- **Claim:** Enforcing acyclicity constraints *after* probabilistic prediction significantly improves structural validity compared to raw probability thresholding.
- **Mechanism:** The GNN outputs edge probabilities independently. The "Probabilistic Inference" methods (PDAG, MLDAG) apply a post-processing step—approximating the maximum likelihood topological ordering and constructing a Maximum Spanning DAG (MSDAG)—to prune cycles. This decouples the learning problem (edge prediction) from the constraint satisfaction problem (DAG validity).
- **Core assumption:** The approximation of the maximum likelihood topological ordering via MSDAG is sufficiently robust to correct the cycles inherent in the raw probability distribution.
- **Evidence anchors:**
  - [Section 3.3]: "The transition from PG/MLG to PDAG/MLDAG is crucial... the latter two explicitly enforce the acyclicity assumption required for valid causal graphs."
  - [Results]: "Enforcing DAG constraints in GNN-PDAG and GNN-MLDAG improves performance metrics relative to GNN-PG and GNN-MLG..."
  - [Corpus]: Not explicitly validated in corpus.
- **Break condition:** If the ground truth contains feedback loops (cycles), this mechanism will erroneously remove correct edges to satisfy the DAG constraint.

## Foundational Learning

- **Concept:** Graph Neural Networks (GNNs) & Message Passing
  - **Why needed here:** The paper relies on GraphSAGE to process the fully connected graph. Understanding how node features are updated via neighbor aggregation is critical to understanding how the model captures "global" causal context.
  - **Quick check question:** How does a GNN update a node's feature vector using its neighbors? (Answer: By aggregating neighbor features and transforming them, typically via a learned weight matrix).

- **Concept:** Structural Equation Models (SEMs)
  - **Why needed here:** The model is trained on synthetic data generated from SEMs. Understanding that $X_j = f(PA_j) + noise$ defines the ground truth is essential for interpreting the "Supervised Learning" aspect.
  - **Quick check question:** In a synthetic SEM, what determines the value of a child node? (Answer: A deterministic or stochastic function of its parent nodes plus independent noise).

- **Concept:** Topological Sorting
  - **Why needed here:** The post-processing step for GNN-MLDAG relies on finding a topological order to enforce acyclicity.
  - **Quick check question:** What property must a graph have for a topological sort to exist? (Answer: It must be a Directed Acyclic Graph (DAG)).

## Architecture Onboarding

- **Component map:** Raw Data -> Statistical Feature Computation -> GNN Message Passing -> Probability Prediction -> Topological Ordering -> Final DAG
- **Critical path:** Raw Data -> Statistical Feature Computation -> GNN Message Passing -> Probability Prediction -> Topological Ordering -> Final DAG
- **Design tradeoffs:**
  - **Supervised on Synthetic vs. Unsupervised:** The model trains once on synthetic data to generalize to real data. This avoids per-dataset optimization but assumes the synthetic SEMs cover the distribution of real-world causal mechanisms.
  - **Full Connectivity vs. Sparsity:** The input is a dense graph ($d^2$ edges). This is computationally expensive for high $d$ but ensures no potential relationship is ignored a priori.
  - **Feature Engineering vs. End-to-End:** The model relies on hand-crafted statistical features (Mutual Info, HSIC) rather than learning features from raw data.
- **Failure signatures:**
  - **High False Positive Rate (FPR):** If the "DAG Enforcer" is too permissive or the GNN probabilities are noisy, the graph will be densely connected with spurious edges (seen in GNN-PG).
  - **Low True Positive Rate (TPR):** If the model is over-regularized or the synthetic training data is insufficient, it may default to predicting "no edge" (seen in GNN-MLG).
  - **Sensitivity to Scaling:** While the paper claims robustness, check if input features (skewness, etc.) degrade if raw data is not standardized, despite paper claims.
- **First 3 experiments:**
  1. **Validation of Inference Methods:** Compare GNN-PG (probabilistic) vs. GNN-MLDAG (deterministic DAG) on the Sachs dataset to quantify the trade-off between TPR and FPR specifically introduced by the acyclicity constraint.
  2. **Feature Ablation:** Remove the "Causal-pairs" probability features (3 of the 114 edge features) to determine if the GNN is learning from raw statistics or heavily relying on the pre-computed pairwise causal baseline.
  3. **Generalization Test:** Train on Scale-Free (SF) graphs and test on Erdos-Renyi (ER) graphs (and vice-versa) to verify if the model learns universal causal mechanics or overfits to graph topology types.

## Open Questions the Paper Calls Out
The paper explicitly identifies future research directions including: explicitly incorporating acyclicity constraints into the GNN framework during training rather than enforcing them post-hoc, adapting the framework to learn causal structures directly from raw data to reduce reliance on hand-crafted feature engineering, and investigating advanced GNN architectures such as Graph Attention Networks (GAT) to improve differentiation between direct causal parents and spurious correlations.

## Limitations
- Heavy reliance on hand-crafted statistical features assumes these 114 metrics are universally sufficient for causal discovery across diverse domains
- Architecture details (hidden dimensions, layers, hyperparameters) are underspecified, potentially affecting reproducibility
- Supervised synthetic training assumes real-world causal mechanisms follow similar distributions, which may not hold

## Confidence
- **High:** The GraphSAGE-based probabilistic framework produces lower SHD than traditional methods (PC, GES) and recent approaches (NOTEARS, DAG-GNN) on synthetic data
- **Medium:** Generalization from synthetic to real-world datasets (Microsoft CSuite, Sachs) without retraining, given the corpus only weakly supports this claim
- **Low:** The sufficiency of the 114 statistical features for capturing complex causal dependencies across all tested domains

## Next Checks
1. Conduct feature ablation studies removing specific edge feature categories (e.g., causal-pairs, polynomial fits) to determine which metrics drive performance gains
2. Test domain transfer by training on one synthetic graph type (ER) and evaluating on structurally distinct types (Scale-Free) to quantify generalization limits
3. Compare performance when using learned features from raw data versus hand-crafted statistical features to validate the feature engineering assumption