---
ver: rpa2
title: 'COMET: Codebook-based Online-adaptive Multi-scale Embedding for Time-series
  Anomaly Detection'
arxiv_id: '2602.01635'
source_url: https://arxiv.org/abs/2602.01635
tags:
- anomaly
- time
- detection
- codebook
- comet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COMET, a time series anomaly detection framework
  that addresses the challenges of capturing temporal dependencies, multivariate correlations,
  and distribution shifts. The key innovation is Multi-scale Patch Encoding, which
  captures patterns across different temporal scales while modeling inter-variable
  correlations, combined with a Vector-Quantized Coreset that learns representative
  normal patterns and detects anomalies using both quantization error and memory distance.
---

# COMET: Codebook-based Online-adaptive Multi-scale Embedding for Time-series Anomaly Detection

## Quick Facts
- arXiv ID: 2602.01635
- Source URL: https://arxiv.org/abs/2602.01635
- Reference count: 40
- State-of-the-art performance on 5 benchmarks, achieving best results in 36 out of 45 evaluation metrics

## Executive Summary
This paper introduces COMET, a time series anomaly detection framework that addresses the challenges of capturing temporal dependencies, multivariate correlations, and distribution shifts. The key innovation is Multi-scale Patch Encoding, which captures patterns across different temporal scales while modeling inter-variable correlations, combined with a Vector-Quantized Coreset that learns representative normal patterns and detects anomalies using both quantization error and memory distance. The framework also incorporates Online Codebook Adaptation that dynamically adapts to distribution shifts at inference time through pseudo-labeling and contrastive learning. Experiments on five benchmark datasets demonstrate state-of-the-art performance, achieving the best results in 36 out of 45 evaluation metrics while maintaining parameter efficiency with only 567K parameters.

## Method Summary
COMET operates through three main stages during training: Multi-scale Patch Encoding extracts temporal patterns at different scales (patch sizes {2,4,6} with strides {1,2,3}), Vector-Quantized Coreset maps these embeddings to discrete codebook entries to learn normal patterns, and reconstruction ensures the model can recover normal sequences. At inference, dual anomaly scoring combines quantization error and memory distance, while Online Codebook Adaptation updates the model using pseudo-labels and contrastive learning to handle distribution shifts.

## Key Results
- Achieves best performance in 36 out of 45 evaluation metrics across five benchmark datasets
- Outperforms existing methods by significant margins on range-based metrics (Affiliation F1, VUS-ROC)
- Maintains parameter efficiency with only 567K parameters while achieving superior performance
- Ablation studies confirm the importance of both multi-scale encoding and online adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-scale patch encoding captures temporal dependencies and inter-variable correlations across diverse temporal ranges, enabling detection of anomalies that manifest at different scales.
- Mechanism: Input time series are segmented using K different patch sizes (e.g., {2, 4, 6}) with corresponding strides (e.g., {1, 2, 3}). At each scale, variable-wise encoders capture individual series characteristics, while a shared core encoder models inter-variable correlations. These are fused to create embeddings that represent both local and broader temporal contexts.
- Core assumption: Time series anomalies can be point-based, contextual, or collective, requiring representations at multiple resolutions for effective detection.
- Evidence anchors:
  - [abstract]: "Multi-scale Patch Encoding captures temporal dependencies and inter-variable correlations across multiple patch scales."
  - [section 3.1]: Describes the architecture of Multi-scale Patching, Series Patch-specific Encoder, Core Encoder, and Feature Fusion.
  - [corpus]: "CrossAD" (arXiv:2510.12489) and "NeuroRVQ" (arXiv:2510.13068) both employ multi-scale tokenization to capture latent patterns across granularities.
- Break condition: If the time series data is uniformly sampled and anomalies are exclusively point anomalies, multi-scale encoding may introduce unnecessary computational overhead without significant gain.

### Mechanism 2
- Claim: A Vector-Quantized Coreset provides an explicit, memory-efficient representation of normal patterns, enabling anomaly detection through quantization error and memory distance.
- Mechanism: Continuous patch embeddings are mapped to a finite codebook of M entries (e.g., 128), trained on normal data. The reconstruction loss forces the model to represent normal patterns well. During inference, anomalies are flagged via a dual scoring system: (1) a quantization score measuring the distance between an embedding and its nearest codebook entry, and (2) a memory score using a local scaling distance to the coreset memory bank, which handles density variations.
- Core assumption: Normal data in unsupervised training can be compactly represented by a finite set of prototypes, and deviations from these prototypes indicate anomalies.
- Evidence anchors:
  - [abstract]: "Vector-Quantized Coreset learns representative normal patterns via codebook and detects anomalies with a dual-score combining quantization error and memory distance."
  - [section 3.2]: Details the Vector Quantization process, training objectives (L_cb, L_cm, L_rec), and Coreset Memory Bank construction.
  - [corpus]: Weak or missing direct corpus evidence for VQ-based coreset anomaly scoring.
- Break condition: If the "normal" training data is heavily contaminated with anomalies, the codebook will learn anomalous patterns as normal, leading to high false negatives.

### Mechanism 3
- Claim: Online Codebook Adaptation with codebook activation-based pseudo-labeling and contrastive learning allows the model to adapt to distribution shifts at inference time without model contamination.
- Mechanism: At inference, the codebook is dynamically updated. Pseudo-labels are generated by checking if a test sample's quantization index corresponds to a codebook entry activated during training (`C_seen`). Samples mapping to seen entries are labeled normal; others are labeled abnormal. Contrastive loss (`L_con`) is applied to separate normal and abnormal representations. Only normal samples update the reconstruction loss (`L_total`) to prevent assimilating anomalies.
- Core assumption: Distribution shifts occur gradually, and samples quantizing to previously seen normal prototypes are likely normal, allowing for safe adaptation.
- Evidence anchors:
  - [abstract]: "Online Codebook Adaptation generates pseudo-labels based on codebook entries and dynamically adapts the model at inference through contrastive learning."
  - [section 3.4]: Describes Codebook Activation Set, Activation-based Pseudo-labeling, Contrastive Codebook Adaptation, and Index-wise Coreset Update.
  - [corpus]: "Segmented Confidence Sequences..." (arXiv:2508.06638) and "Cluster-Aware Causal Mixer..." (arXiv:2506.00188) address adaptation in nonstationary or online settings, though using different mechanisms.
- Break condition: If a sudden, drastic distribution shift causes all normal samples to map to unseen codebook entries, pseudo-labeling will fail, preventing adaptation.

## Foundational Learning

- **Vector Quantization (VQ) and Codebooks**:
  - Why needed here: This is the core representation learning mechanism. The entire anomaly detection premise relies on mapping inputs to a discrete set of learned "normal" prototypes.
  - Quick check question: Can you explain how a continuous vector is mapped to a discrete codebook entry and how the codebook is updated during training?

- **Test-Time Adaptation (TTA)**:
  - Why needed here: The model's ability to handle non-stationary data depends entirely on this inference-time mechanism. Understanding its risks (e.g., model contamination) is critical.
  - Quick check question: What is the primary risk of adapting a model on unlabeled test data, and how does COMET attempt to mitigate it?

- **Anomaly Scoring Metrics**:
  - Why needed here: The paper evaluates on a mix of point-wise (F1, AUC-ROC) and range-based (Affiliation F1, VUS-ROC) metrics. Understanding the difference is key to interpreting the results.
  - Quick check question: Why might a model achieve a high F1(K=0) score but a low Affiliation F1 score?

## Architecture Onboarding
- Component map: The COMET architecture consists of three main stages during training: 1) Multi-scale Patch Encoding (variable-wise + core encoders), 2) Vector Quantization (quantizing embeddings to a codebook), and 3) Reconstruction (decoder). At inference, an additional 4) Anomaly Scoring (memory score + quantization score) and 5) Online Adaptation (pseudo-labeling + contrastive loss) stage is active.
- Critical path: The **Coreset Memory Bank** construction (Algorithm 1, Phase 2) is the most critical non-standard step. If this is not populated correctly with the activated codebook entries and their local scales from the training data, the memory-based anomaly score will fail at inference.
- Design tradeoffs:
  - **Codebook Size (M)**: A smaller M may not capture the diversity of normal patterns (underfitting), while a larger M may lead to over-fragmentation and noise sensitivity (overfitting). The paper suggests tuning this per dataset (e.g., 128 for PSM, 256 for SWaT).
  - **Multi-Scale Patch Config**: Using more scales (e.g., {2,4,6,8}) adds computational cost and may dilute fine-grained signals. The paper finds {2,4,6} to be a robust default.
  - **TTA vs. Stability**: Online adaptation improves performance but introduces non-determinism in inference. The "inference-then-train" strategy prevents test data leakage but means results can vary based on the order of batches.
- Failure signatures:
  - **High False Negatives on Drifted Normal Data**: This indicates TTA is not activating or the pseudo-labeling is too strict (no new entries are being added to `C_seen`).
  - **High False Positives in Sparse Regions**: This suggests the local scaling distance is not effectively normalizing for density, possibly due to an incorrect k-NN parameter.
  - **Performance Collapse Over Time**: This suggests model contamination, where anomalous samples are being incorrectly pseudo-labeled as normal and updating the model.
- First 3 experiments:
  1. **Baselines vs. COMET (w/o TTA) on a single dataset**: Run the provided baseline models (e.g., LSTM-AE, Anomaly Transformer) and COMET without TTA on the PSM dataset. Reproduce Table 1 (subset) to validate the core architecture's performance and your evaluation pipeline.
  2. **Ablation Study on Anomaly Scoring**: On the SWaT dataset, run COMET without the memory score, then without the quantization score, and then without local scaling distance. Compare the Affiliation F1 and AUC-PR scores to understand the contribution of each scoring component (as in Table 2).
  3. **Hyperparameter Sensitivity on Codebook Size**: Train COMET on the SMAP dataset using codebook sizes M = {16, 64, 128, 256}. Plot the Affiliation F1 score to observe the trade-off between representational capacity and robustness, confirming the findings in Table 11.

## Open Questions the Paper Calls Out
- The paper acknowledges that the streaming nature of inference data constrains information available for adaptation but doesn't address how to handle novel normal patterns that emerge during inference.

## Limitations
- The TTA mechanism assumes distribution shifts are gradual and pseudo-labels based on seen codebook entries are reliable, which may fail under sudden concept drift or heavy contamination.
- Some hyperparameters (contrastive temperature τ, variable selection percentile ρ) are not fully specified, potentially affecting reproducibility.
- The ablation study lacks analysis of memory score vs quantization score trade-offs across datasets.

## Confidence
- **Multi-scale Patch Encoding effectiveness**: High - well-grounded in related work and ablation results show consistent improvement
- **VQ Coreset anomaly detection**: Medium - mechanism is sound but lacks extensive corpus validation for the memory-based scoring component
- **Online Codebook Adaptation**: Medium - innovative but depends heavily on assumptions about pseudo-label reliability that may not hold in practice
- **State-of-the-art claims**: High - comprehensive evaluation across 5 datasets with 45 metrics shows consistent superiority

## Next Checks
1. **Robustness to Training Contamination**: Intentionally inject anomalies (5-20%) into training data and evaluate COMET's performance degradation to test the core assumption that training data is normal
2. **TTA Failure Modes**: Create scenarios with sudden distribution shifts and evaluate when pseudo-labeling fails - compare against a static baseline to quantify the cost of incorrect adaptation
3. **Memory Score Necessity**: Systematically remove the memory score component across all datasets and measure performance impact to quantify its contribution beyond quantization error alone