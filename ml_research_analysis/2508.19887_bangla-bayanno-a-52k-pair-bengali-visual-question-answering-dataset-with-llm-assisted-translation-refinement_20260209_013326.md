---
ver: rpa2
title: 'Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with
  LLM-Assisted Translation Refinement'
arxiv_id: '2508.19887'
source_url: https://arxiv.org/abs/2508.19887
tags:
- dataset
- translation
- bangla-bayanno
- question
- bangla
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Bangla-Bayanno, a 52K-pair Bengali Visual Question
  Answering (VQA) dataset built using a multilingual LLM-assisted translation refinement
  pipeline. Addressing the scarcity of high-quality, large-scale VQA datasets for
  low-resource languages, the authors enhanced raw translations from VQA v2 into fluent,
  context-preserving Bengali, mitigating common issues like literal wording and syntactic
  awkwardness.
---

# Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement

## Quick Facts
- arXiv ID: 2508.19887
- Source URL: https://arxiv.org/abs/2508.19887
- Reference count: 26
- Bangla-Bayanno is the largest Bengali VQA dataset at 52K QA pairs across 4,750+ images

## Executive Summary
This paper presents Bangla-Bayanno, a 52K-pair Bengali Visual Question Answering (VQA) dataset built using a multilingual LLM-assisted translation refinement pipeline. Addressing the scarcity of high-quality, large-scale VQA datasets for low-resource languages, the authors enhanced raw translations from VQA v2 into fluent, context-preserving Bengali, mitigating common issues like literal wording and syntactic awkwardness. The dataset contains 52,650 QA pairs across 4,750+ images, categorized into nominal, quantitative, and polar answer types. Translation quality was validated through expert scoring (mean 5.0), outperforming baseline MT and manual Excel-style approaches. Compared to prior Bengali VQA efforts, Bangla-Bayanno is the largest in scale and diversity, enabling broader research in low-resource multimodal learning and inclusive AI system development.

## Method Summary
The authors constructed Bangla-Bayanno by translating English QA pairs from the VQA v2 dataset into Bengali using GPT-4 with a structured prompt that preserves JSON schema. Each of the 52,650 QA pairs was processed individually through Azure OpenAI's GPT-4 API with instructions to produce fluent, contextually appropriate Bengali while maintaining structural integrity. The translated pairs were validated for consistency and categorized into three answer types (nominal, quantitative, polar) before being distributed as four configurations on Hugging Face Hub. The approach specifically addresses the challenge of low-resource language translation quality by using LLM refinement to overcome literal translation artifacts and syntactic awkwardness common in direct machine translation.

## Key Results
- Bangla-Bayanno contains 52,650 QA pairs across 4,750+ images, making it the largest Bengali VQA dataset to date
- Expert validation of 20 randomly sampled QA pairs achieved a mean score of 5.0 on a 1-5 scale, demonstrating high translation quality
- The dataset provides three answer type subsets (nominal: 49.9%, quantitative: 12.4%, polar: 37.6%) plus full corpus for targeted benchmarking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-assisted translation refinement produces higher-quality Bengali VQA annotations than direct machine translation or manual Excel-style entry.
- Mechanism: GPT-4 processes each English QA pair individually with explicit instructions to produce fluent, contextually appropriate Bengali; the model applies contextual reasoning from pretraining to resolve syntactic ambiguity and literal phrasing artifacts.
- Core assumption: GPT-4's translation quality for Bengali generalizes across the full 52,650 QA pairs based on limited expert validation (20-question sample).
- Evidence anchors:
  - [abstract]: "implemented a multilingual LLM-assisted translation refinement pipeline... overcomes the issues of low-quality translations"
  - [section]: "Bangla-Bayanno showcases superior semantic precision and syntactic fluency" in 20-question evaluation; mean expert score 5.0 vs. lower scores for baseline MT and Excel-style
  - [corpus]: No direct corpus validation of LLM-assisted translation for Bengali VQA; related papers do not address this pipeline
- Break condition: If GPT-4 translation quality degrades significantly on domain-specific vocabulary or complex compositional questions not represented in the validation sample.

### Mechanism 2
- Claim: Sourcing from VQA v2 provides diverse visual reasoning coverage that transfers to Bengali through translation.
- Mechanism: VQA v2's balanced design (real, fake, is, isn't pairings) compels models to use both visual and linguistic inputs; translation preserves this structure, enabling low-resource models to inherit robust multimodal grounding signals.
- Core assumption: Visual reasoning patterns in Bengali sufficiently overlap with English such that translated questions elicit equivalent reasoning demands.
- Evidence anchors:
  - [section]: "VQA v2 addresses language bias by associating each question with corresponding visuals, compelling models to depend on both visual and linguistic inputs"
  - [section]: Dataset includes recognition, counting, and reasoning tasks; answer type distribution (49.9% nominal, 37.6% polar, 12.4% quantitative)
  - [corpus]: VQA v2 is established benchmark (corpus references confirm); no corpus evidence on cross-lingual reasoning transfer
- Break condition: If Bengali syntactic structures require fundamentally different visual attention patterns than English.

### Mechanism 3
- Claim: Modular JSON schema with categorical answer types enables targeted benchmarking and downstream task specialization.
- Mechanism: Each QA entry embeds structured fields (identifier, image reference, question, answer, answer type); three category-specific subsets plus full corpus allow researchers to isolate polar, quantitative, or nominal reasoning.
- Core assumption: Answer type categorization is sufficiently consistent to support meaningful subset-level evaluation.
- Evidence anchors:
  - [abstract]: "Questions are classified into three distinct answer types: nominal (short descriptive), quantitative (numeric), and polar (yes/no)"
  - [section]: Table II shows subset sizes; Fig. 6-7 show distribution; average question length 6.21 tokens, answer length 1.09 tokens
  - [corpus]: No corpus papers validate this specific categorization scheme
- Break condition: If categorization errors propagate into subset benchmarks, skewing evaluation metrics.

## Foundational Learning

- Concept: Visual Question Answering (VQA) task definition
  - Why needed here: Core task structure; must understand that VQA requires joint vision-language reasoning, not separate processing
  - Quick check question: Can you explain why VQA v2's balanced pairs reduce language prior bias compared to VQA v1?

- Concept: Low-resource language translation challenges
  - Why needed here: Motivates the LLM refinement approach; baseline MT produces literal, contextually inappropriate outputs
  - Quick check question: What two failure modes does the paper identify for traditional MT in low-resource Bengali contexts?

- Concept: LLM prompting for structured output
  - Why needed here: The pipeline relies on prompt engineering to produce valid JSON translations; poor prompts yield inconsistent schema
  - Quick check question: What output format constraints should appear in a prompt to ensure parseable JSON QA pairs?

## Architecture Onboarding

- Component map:
  - Source Layer: VQA v2 subset (4,750+ images, English QA pairs)
  - Translation Layer: Azure OpenAI GPT-4 API with prompt template (Fig. 4)
  - Validation Layer: Automated consistency checks + expert sampling
  - Storage Layer: JSON schema (Fig. 3, 5) with QA ID, image ref, question, answer, type
  - Distribution Layer: Hugging Face Hub with 4 configs (polar, numeric, descriptive, full)

- Critical path:
  1. Extract and serialize VQA v2 QA pairs to JSON
  2. Send each entry to GPT-4 with translation/refinement prompt
  3. Parse LLM response into Bengali JSON structure
  4. Run consistency validation (image linkage, type categorization)
  5. Aggregate into categorical subsets and full dataset
  6. Deploy to Hugging Face Hub

- Design tradeoffs:
  - Individual vs. batch translation: Individual processing reduces batch noise but increases API cost (~$110-132 for 52,650 calls)
  - LLM vs. human annotation: LLM scales efficiently; human-in-the-loop deferred to future work
  - VQA v2 vs. custom images: Leveraging established benchmark ensures diversity; limits cultural specificity of visual content

- Failure signatures:
  - Incomplete JSON parsing (missing fields, malformed structure)
  - Untranslated English terms in Bengali output (code-mixing artifacts)
  - Numerical answer corruption during translation
  - Image-QA misalignment after serialization

- First 3 experiments:
  1. **Baseline quality check**: Sample 100 QA pairs across all three answer types; manually verify translation accuracy, fluency, and JSON validity against source English.
  2. **API cost profiling**: Run 1,000 translations with token logging to validate the $110-132 cost estimate before full dataset generation.
  3. **Answer type consistency**: Automatically verify that categorized subsets contain only their intended answer types (e.g., polar subset has only yes/no answers).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does integrating human-in-the-loop validation improve linguistic accuracy over the current automated pipeline?
- Basis in paper: [explicit] The authors state future work will focus on "human-in-the-loop validation to guarantee linguistic accuracy," acknowledging that some translations currently preserve source-language terminology.
- Why unresolved: The current dataset relies primarily on automated LLM refinement without comprehensive post-hoc human verification.
- What evidence would resolve it: A comparative error analysis between the current automated outputs and a human-validated subset.

### Open Question 2
- Question: Can the Bangla-Bayanno pipeline be effectively adapted for cross-lingual visual question answering tasks?
- Basis in paper: [explicit] The authors list expanding the dataset "to encompass cross-lingual visual question answering" as a key objective for future endeavors.
- Why unresolved: The current dataset and methodology are designed for monolingual (Bangla) outputs rather than cross-lingual alignment.
- What evidence would resolve it: Successful application of the translation refinement pipeline to generate aligned parallel corpora for cross-lingual model training.

### Open Question 3
- Question: Does the high translation quality observed in the small evaluation sample generalize across the full dataset?
- Basis in paper: [inferred] The methodology validates translation quality using only 20 manually assessed examples (Section IV-B), leaving the consistency of the remaining ~52,630 pairs unverified.
- Why unresolved: A sample size of 20 is statistically insufficient to guarantee that the LLM avoided hallucinations or syntactic errors in the full corpus.
- What evidence would resolve it: A large-scale, randomized audit (e.g., N>500) of the dataset by independent linguists.

## Limitations

- The translation quality validation sample of 20 questions is too small to guarantee consistent quality across all 52,650 QA pairs
- The pipeline relies on GPT-4's generalization capabilities without comprehensive full-dataset quality verification
- The answer type categorization scheme lacks corpus-level validation to confirm consistent application across the dataset

## Confidence

- **High Confidence**: The dataset construction methodology (JSON schema, categorical subsets, Hugging Face distribution) is clearly specified and reproducible
- **Medium Confidence**: The LLM-assisted translation refinement approach is theoretically sound and shows promising results in limited validation, but full-dataset quality remains unverified
- **Low Confidence**: Claims about cross-lingual reasoning transfer from English VQA v2 patterns to Bengali are speculative without empirical validation of whether Bengali syntactic structures require different visual attention patterns

## Next Checks

1. **Full Dataset Quality Audit**: Implement automated quality checks across all 52,650 pairs to detect translation inconsistencies, code-mixing artifacts, and answer corruption, followed by expert review of a stratified random sample.

2. **Cross-Lingual Reasoning Validation**: Conduct controlled experiments comparing model performance on Bengali vs. English versions of identical visual questions to quantify reasoning transfer effectiveness.

3. **Cultural Adaptation Assessment**: Evaluate whether the English-sourced visual content adequately represents Bengali cultural contexts and whether translation quality varies across different visual domains.