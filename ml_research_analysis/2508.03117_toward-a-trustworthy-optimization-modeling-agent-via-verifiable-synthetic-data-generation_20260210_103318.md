---
ver: rpa2
title: Toward a Trustworthy Optimization Modeling Agent via Verifiable Synthetic Data
  Generation
arxiv_id: '2508.03117'
source_url: https://arxiv.org/abs/2508.03117
tags:
- optimization
- problem
- description
- formulation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OptiTrust, a framework for training trustworthy
  large language model (LLM) agents for optimization modeling. The approach uses a
  verifiable synthetic data generation pipeline to create structured symbolic representations
  of linear and mixed-integer linear programming problems, which are then systematically
  converted into natural language descriptions, mathematical formulations, and solver-executable
  code.
---

# Toward a Trustworthy Optimization Modeling Agent via Verifiable Synthetic Data Generation

## Quick Facts
- arXiv ID: 2508.03117
- Source URL: https://arxiv.org/abs/2508.03117
- Reference count: 40
- This paper introduces OptiTrust, a framework for training trustworthy large language model (LLM) agents for optimization modeling, achieving state-of-the-art performance on seven benchmark datasets.

## Executive Summary
This paper introduces OptiTrust, a framework for training trustworthy large language model (LLM) agents for optimization modeling. The approach uses a verifiable synthetic data generation pipeline to create structured symbolic representations of linear and mixed-integer linear programming problems, which are then systematically converted into natural language descriptions, mathematical formulations, and solver-executable code. Each generated instance includes a verified optimal solution, enabling automatic filtering of low-quality demonstrations and ensuring data quality. The OptiTrust agent performs multi-stage translation from natural language to solver-ready code using stepwise demonstrations, multi-language inference, and majority-vote cross-validation. Experiments show that OptiTrust achieves state-of-the-art performance on seven benchmark datasets, obtaining the highest accuracy on six and outperforming the next-best method by at least 8% on three datasets.

## Method Summary
OptiTrust uses a verifiable synthetic data generation (SDG) pipeline to create training data with known optimal solutions for linear and mixed-integer linear programming problems. The pipeline constructs symbolic MILP representations, generates natural language descriptions, and uses teacher models to create reasoning traces. Only trajectories producing solutions matching the verified optimal value are retained. The framework employs a three-agent architecture (decomposition, formulation, code) with multi-language inference and majority-vote cross-validation. The model is fine-tuned on verified trajectories using Granite 3.2 8B Instruct in two stages, achieving state-of-the-art accuracy on seven optimization modeling benchmarks.

## Key Results
- OptiTrust achieves state-of-the-art accuracy on seven benchmark datasets, obtaining the highest accuracy on six.
- Outperforms the next-best method by at least 8% on three datasets (NL4Opt, NLP4LP, ReSocratic).
- Identifies and corrects errors in existing datasets, improving their reliability.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Verifiable synthetic data generation enables automatic quality filtering by grounding training examples in known-optimal solutions.
- Mechanism: The pipeline constructs optimization problems symbolically first, then generates natural language descriptions. Since the symbolic form has a provably correct optimal solution, any teacher-model trajectory that produces a mismatching solution can be automatically discarded, ensuring training data contains only verified reasoning paths.
- Core assumption: Symbolic-to-natural-language generation preserves sufficient semantic fidelity that the reverse translation task remains learnable.
- Evidence anchors:
  - [abstract] "By programmatically constructing each instance with known optimal solutions, the pipeline ensures full verifiability and enables automatic filtering of low-quality demonstrations generated by teacher models."
  - [section: Verifiable Synthetic Data Generation Pipeline] "If (any of) the solutions found by the teacher model match the expected ground truth, we collect reasoning traces for each step of the workflow and save those for fine-tuning."
  - [corpus] Limited direct corpus support; neighboring papers focus on verification frameworks but not this specific symbolic-to-verified-trajectory mechanism.
- Break condition: If generated natural language descriptions become too divorced from symbolic structure (e.g., excessive paraphrasing introduces ambiguity), the verification loop may accept incorrect trajectories or reject correct ones.

### Mechanism 2
- Claim: Multi-stage agent decomposition with redundant information access improves error recovery compared to single-stage translation.
- Mechanism: The three sub-agents (decomposition, formulation, code) each receive the original problem description alongside intermediate outputs. If formulation fails, the code agent can still retrieve correct information from the source, reducing cascade failure probability.
- Core assumption: Errors are detectable at each stage; agents can recognize when to fall back to original description.
- Evidence anchors:
  - [section: Multi-Agent Architecture for OptiTrust] "The key novelty of our agent is that it incrementally evolves the components of the modeling trajectory, so if a mistake occurs at an intermediate stage, there remains an opportunity to recover the correct information from the original problem description."
  - [section: Multi-Agent Architecture for OptiTrust] "That is primarily designed to allow the coding agent to retrieve information directly from the problem description in case any of the previous steps fails."
  - [corpus] A-LAMP (arXiv:2512.11270) uses staged MDP modeling with error correction but focuses on RL environments rather than optimization.
- Break condition: If errors compound silently (e.g., decomposition extracts wrong variables but formulation accepts them), the recovery path is not triggered.

### Mechanism 3
- Claim: Multi-language majority voting improves solution reliability by leveraging implementation diversity to cross-validate correctness.
- Mechanism: The code agent generates implementations in five modeling languages (Pyomo, Gurobipy, DOcplex, CVXPY, PySCIPOpt), each executed with different solvers. Solutions are compared via majority vote; inconsistent outputs flag potential errors for review.
- Core assumption: Solver implementations are sufficiently independent that errors are uncorrelated across languages; the correct solution will dominate.
- Evidence anchors:
  - [abstract] "OptiTrust...leveraging stepwise demonstrations, multi-language inference, and majority-vote cross-validation."
  - [section: Multi-Agent Architecture for OptiTrust] "We find that incorporating this consistency check into the optimization workflow significantly improves model performance across both baseline and fine-tuned models."
  - [corpus] Weak corpus support; neighboring papers on LLM verification do not specifically address multi-language cross-validation for optimization.
- Break condition: If training data is biased toward one language's syntax, the model may systematically favor that language even when wrong, breaking majority-vote assumptions.

## Foundational Learning

- Concept: Mixed-Integer Linear Programming (MILP) formulation
  - Why needed here: The entire SDG pipeline is built on MILP representations; understanding decision variables, objective functions, and constraints is prerequisite to evaluating agent correctness.
  - Quick check question: Given a production planning problem with discrete batch sizes, can you identify whether it requires integer or continuous variables?

- Concept: Supervised Fine-Tuning (SFT) of LLMs
  - Why needed here: OptiTrust fine-tunes Granite 3.2 using multi-instruction pairs; understanding loss functions and data formatting is essential for reproducing training.
  - Quick check question: What is the difference between full fine-tuning and parameter-efficient fine-tuning (e.g., LoRA), and which would you choose for a domain-specific optimization agent?

- Concept: Chain-of-Thought (CoT) reasoning and self-consistency
  - Why needed here: The agent uses stepwise demonstrations and majority voting, which are grounded in CoT and self-consistency literature.
  - Quick check question: How does self-consistency differ from simple ensemble methods, and what assumption does it make about reasoning path diversity?

## Architecture Onboarding

- Component map: Natural Language Description -> Decomposition Agent -> Formulation Agent -> Code Agent -> Solver Execution -> Majority Vote

- Critical path:
  1. Symbolic MILP generation with coefficient sampling
  2. Teacher model generates NL description + reasoning traces
  3. Verify optimal solution matches ground truth
  4. If verified, save trajectory as training pair (pairDA, pairFA, pairCA)
  5. Fine-tune student model on verified trajectories
  6. Deploy agent with multi-language majority voting at inference

- Design tradeoffs:
  - Complexity vs. reliability: Three-agent system increases debuggability but adds latency
  - Synthetic vs. real data: Synthetic enables verification but may not cover real-world edge cases
  - Language coverage: Five languages improve robustness but require maintaining five code-generation prompts

- Failure signatures:
  - Execution errors: Syntax or runtime errors in generated code → triggers debugging loop (up to 6 iterations)
  - Infeasibility: Model returns no solution → prompts infeasibility debugging
  - Solution mismatch: Majority vote fails → flags instance for manual review
  - Ground truth errors in benchmarks: Agent identifies incorrect labels in existing datasets (paper reports correcting errors in 5/7 benchmarks)

- First 3 experiments:
  1. Replicate baseline comparison: Run non-fine-tuned Granite on NL4Opt with majority voting disabled to isolate the contribution of each modeling language.
  2. Ablate verification: Train a model on unfiltered teacher trajectories (skip ground-truth matching) and compare accuracy to verify filtering impact.
  3. Stress-test recovery: Intentionally corrupt decomposition outputs and measure whether code agent successfully retrieves correct information from original descriptions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the verifiable synthetic data generation pipeline be extended to nonlinear, stochastic, or robust optimization problems beyond LP and MILP?
- Basis in paper: [explicit] The paper states it is "Focusing on linear and mixed-integer linear programming" and acknowledges that some benchmark datasets contain nonlinear problems (ReSocratic, ComplexLP) where performance remains limited.
- Why unresolved: The entire SDG pipeline and symbolic representation are designed around linear constraint structures; nonlinear problems require fundamentally different verification mechanisms.
- What evidence would resolve it: Successful application of the pipeline to standard nonlinear programming benchmarks with comparable accuracy and verifiability.

### Open Question 2
- Question: How can LLM agents be improved to reliably model optimization problems with lengthy, complex natural language descriptions where current accuracy remains below 65%?
- Basis in paper: [explicit] The paper explicitly notes that "optimization problems characterized by complex and lengthy descriptions—such as ComplexOR, IndustryOR, and ComplexLP—remain particularly challenging for all evaluated methods" and that "substantial scope exists for further research into enhancing LLM agents' capability to interpret and accurately model highly complex optimization scenarios."
- Why unresolved: Current modular decomposition may lose coherence across extended contexts; the relationship between description length and modeling accuracy is not analyzed.
- What evidence would resolve it: Systematic ablation of context length and architectural modifications demonstrating improved accuracy above 80% on ComplexOR and IndustryOR.

### Open Question 3
- Question: How should the framework handle infeasible or unbounded optimization problems, which are currently discarded?
- Basis in paper: [explicit] The paper states: "we discard infeasible instances (where the feasible set is empty) and unbounded instances (where the feasible set is non-empty, but the objective value can be made arbitrarily good)."
- Why unresolved: The SDG pipeline requires known optimal solutions for verification; detecting and correctly communicating infeasibility/unboundedness to users requires different reasoning traces and training signals.
- What evidence would resolve it: Extended pipeline generating verified infeasible/unbounded instances and agent achieving high accuracy in classifying problem feasibility status.

### Open Question 4
- Question: Does training on synthetically generated, well-specified problems transfer to real-world scenarios with ambiguous or under-specified problem statements?
- Basis in paper: [inferred] The introduction states "natural language problem statements are frequently ambiguous or under-specified," yet the SDG pipeline generates problems from structured symbolic representations with complete specifications, potentially creating a distribution shift.
- Why unresolved: No evaluation on intentionally ambiguous or incomplete real-world problem descriptions; the paper does not analyze whether synthetic data captures the linguistic variability of practical deployment.
- What evidence would resolve it: Evaluation on a benchmark of under-specified industrial optimization problems with multiple valid interpretations, measuring whether the agent identifies ambiguity or makes reasonable assumptions.

## Limitations

- The framework is limited to linear and mixed-integer linear programming, with performance on nonlinear optimization problems remaining below baseline.
- The synthetic data generation pipeline may not capture the ambiguity and under-specification common in real-world problem statements.
- The verification mechanism discards infeasible and unbounded instances, which are important problem classes in practical optimization.

## Confidence

- **High**: Multi-language majority voting improves solution reliability through solver diversity
- **Medium**: Verifiable synthetic data generation enables automatic quality filtering without introducing bias
- **Medium**: Three-agent decomposition architecture improves error recovery compared to single-stage translation

## Next Checks

1. **Generalization stress test**: Evaluate OptiTrust on optimization problems with non-standard structures (non-linear constraints, stochastic elements, hierarchical objectives) to assess synthetic data coverage limitations.

2. **Filter bias analysis**: Quantify false-negative rates in the verification pipeline by comparing filtered vs. unfiltered teacher trajectories on a held-out validation set, measuring impact on training data diversity.

3. **Recovery mechanism audit**: Conduct ablation studies removing the recovery feature (forcing code agent to rely only on formulation outputs) and measure specific failure rates where decomposition errors propagate to final solutions.