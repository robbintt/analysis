---
ver: rpa2
title: Real-Time Procedural Learning From Experience for AI Agents
arxiv_id: '2511.22074'
source_url: https://arxiv.org/abs/2511.22074
tags:
- memory
- agents
- arxiv
- procedural
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PRAXIS, a state-dependent memory mechanism
  that enables AI agents to learn procedures from experience in real time. The method
  stores action-outcome traces indexed by both environmental and internal agent states,
  then retrieves relevant memories during decision-making.
---

# Real-Time Procedural Learning From Experience for AI Agents

## Quick Facts
- arXiv ID: 2511.22074
- Source URL: https://arxiv.org/abs/2511.22074
- Reference count: 23
- Improved task completion accuracy from 40.3% to 44.1% across multiple VLM backbones

## Executive Summary
This paper introduces PRAXIS, a state-dependent memory mechanism enabling AI agents to learn procedures from experience in real time. By storing action-outcome traces indexed by both environmental and internal agent states, then retrieving relevant memories during decision-making, PRAXIS improves agent accuracy, reliability, and efficiency. Evaluated on the REAL web browsing benchmark, the method demonstrates that procedural memory can enhance agent performance in stateful environments without requiring additional model training.

## Method Summary
PRAXIS implements a post-training procedural learning system that stores experience tuples (environment state before, internal state, action, environment state after) in real time. During inference, it retrieves memories by jointly matching current environmental and internal states using Intersection-over-Union (IoU) for environment similarity and embedding similarity for internal state. Retrieved exemplars are injected into the action selection context to bias decisions toward previously successful trajectories. The system uses a dual-retrieval mechanism: top-k environmental matches re-ranked by internal state similarity and filtered by threshold τ.

## Key Results
- Improved task completion accuracy from 40.3% to 44.1% across multiple VLM backbones
- Increased reliability (mean success rate over 5 runs) from 74.5% to 79.0%
- Reduced average steps-to-completion from 25.2 to 20.2
- Performance scaled with retrieval breadth, plateauing around k=8-10

## Why This Works (Mechanism)

### Mechanism 1: Dual-State Retrieval Alignment
Matching both environmental AND internal states yields more action-relevant exemplars than either alone. Uses IoU with length overlap normalization for environmental similarity, ranks by internal state embedding similarity, then filters by threshold τ. Assumes actions succeeding under similar environmental conditions AND similar goal contexts will transfer. Evidence shows performance improvement over baselines; could break if environments have high variance with few patterns or internal embeddings fail to capture goal similarity.

### Mechanism 2: Exemplar-Guided Variance Reduction
Injecting retrieved state-action-result traces into action selection context reduces stochastic variance in VLM decisions. Augments context with concrete procedural exemplars, biasing the model toward previously successful trajectories under similar states. Assumes VLMs can extract and generalize procedural patterns from exemplars without explicit abstraction. Evidence from accuracy improvements; could break if exemplars conflict with current goal or context crowding degrades reasoning.

### Mechanism 3: Retrieval Breadth Scaling with Diminishing Returns
Performance increases with retrieval breadth k but converges to a plateau. Larger k provides diverse relevant exemplars enabling better generalization, but local context crowding causes slight intra-step decreases before plateau. Assumes retrieval relevance is maintained as k increases. Evidence shows accuracy increasing from ~45% at k≈1 to ~51% plateau at k≈8-10; could break if memory store is small or low-quality.

## Foundational Learning

- Concept: **State-dependent memory** (cognitive psychology)
  - Why needed here: Entire PRAXIS design inspired by encoding specificity—recall improves when retrieval context matches encoding context
  - Quick check question: Why might matching BOTH environmental state (DOM/visual) AND internal state (goal/directive) retrieve more useful memories than matching either alone?

- Concept: **Intersection over Union (IoU)** for set similarity
  - Why needed here: Environmental state matching uses IoU over feature sets
  - Quick check question: Given two sets of DOM features A and B, IoU = |A ∩ B| / |A ∪ B|. What does IoU = 0.1 vs. IoU = 0.8 tell you about state similarity?

- Concept: **Non-parametric memory vs. weight-based learning**
  - Why needed here: PRAXIS is explicitly post-training and lightweight—adds no model parameters, only retrieval infrastructure
  - Quick check question: What are the tradeoffs of storing experiences verbatim vs. learning procedural abstractions into model weights?

## Architecture Onboarding

- Component map: Memory Store -> State Encoder -> Retrieval Engine -> Action Selection Node -> Memory Writer
- Critical path: Observe state → Encode (env + internal) → Retrieve top-k via Alg 1 → Inject into action selection context → Execute action → Write outcome to memory
- Design tradeoffs:
  - **k (breadth) vs. context budget**: Higher k gives more exemplars but consumes tokens; paper shows plateau around k=8-10
  - **τ (threshold) vs. recall**: Higher τ improves precision but may return empty results for novel states
  - **Real-time vs. filtered storage**: Paper stores immediately; no quality filtering discussed (potential noise accumulation)
- Failure signatures:
  - Empty retrievals on novel states (τ too high or memory store empty)
  - Performance degradation after initial gains (noise accumulation, memory store pollution)
  - High variance persists despite PRAXIS (retrieval returning irrelevant exemplars—check encoding quality)
  - Slow inference (retrieval overhead not benchmarked in paper)
- First 3 experiments:
  1. **Retrieval quality audit**: Sample 50 query states, manually label top-5 retrieved memories for relevance. Target: >70% relevant; if lower, adjust τ or improve state encoding
  2. **k-sweep replication**: Replicate Figure 3 on your target domain. Identify plateau point and optimal k for your context window budget
  3. **Component ablation**: Disable environmental matching (use internal only) and vice versa. Paper does not report this—establish relative contribution of each signal

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can richer state encoders improve retrieval quality and invariance to superficial changes compared to the current basic visual and DOM feature overlap method?
- Basis: Authors state in "Future directions" that their "proof-of-concept implementation... uses basic visual and DOM feature overlap" and suggest "A richer encoder can improve both retrieval quality and invariance to superficial changes"
- Why unresolved: Current implementation relies on basic IoU and length overlap metrics which may lack semantic depth
- What evidence would resolve it: Benchmarking PRAXIS using neural state encoders or semantic embeddings against current IoU baseline on tasks with significant visual noise or DOM variations

### Open Question 2
- Question: Can an adaptive retrieval mechanism based on real-time uncertainty outperform the current fixed retrieval breadth (k) approach?
- Basis: Authors propose in "Future directions" that "Rather than a fixed retrieval based on state similarity heuristics, the retrieval mechanism can account for real-time factors such as uncertainty and compute budget"
- Why unresolved: Current system uses static hyperparameter (k) for retrieval breadth which may not be optimal for all states
- What evidence would resolve it: Study comparing fixed-k retrieval against dynamic mechanism that adjusts k based on agent's confidence or state ambiguity

### Open Question 3
- Question: Can PRAXIS effectively learn subjective user preferences (alignment) in scenarios where objective task success is undefined?
- Basis: Authors suggest "From action agents to alignment agents," hypothesizing that PRAXIS could "use user preference as a training signal... converging to a procedural memory that encodes the user's preferences"
- Why unresolved: Current study relies entirely on objective binary signals (task success/failure) from REAL benchmark
- What evidence would resolve it: Evaluating agent in open-ended tasks where "correctness" is defined by human feedback or preference rankings rather than programmatic checks

### Open Question 4
- Question: Does performance of retrieval breadth scaling (k) observed in Gemini 2.5 Flash generalize to other foundation model backbones?
- Basis: Authors note in Section 4.4 that "Due to resource limitations, we only tested the effects of this ablation on the agent with a Gemini 2.5 Flash backbone"
- Why unresolved: Unknown if other models (e.g., Llama 4, Claude) exhibit same plateau behavior or suffer from context crowding differently
- What evidence would resolve it: Repeating ablation study on retrieval breadth across other distinct VLM backbones mentioned in main results

## Limitations
- State representation format and embedding function f for M_int are unspecified, critical for retrieval quality
- Memory storage policy unclear (all experiences vs. successful trajectories only), potential for noise accumulation
- No ablation separating environmental vs. internal state matching contributions
- Context budget constraints and token limits not discussed

## Confidence
- **High confidence**: Core mechanism of dual-state retrieval alignment is well-specified and theoretically sound; general trend of performance improvement with retrieval breadth is well-supported
- **Medium confidence**: Specific performance numbers (44.1% accuracy, 79.0% reliability) are likely reproducible with careful state representation implementation, but depend heavily on unspecified encoding details
- **Low confidence**: Scalability claims and long-term behavior after thousands of stored experiences are not validated and could break down due to memory pollution or context crowding

## Next Checks
1. **State representation validation**: Implement memory system with multiple state encoding strategies and measure performance variation. Target: identify encodings achieving >70% retrieval precision on held-out states
2. **Ablation study**: Systematically disable environmental matching (use internal only) and disable internal state matching (use environmental only). Target: quantify each component's contribution to 3.8% accuracy improvement
3. **Long-term scalability test**: Run agent for extended periods with continuous experience storage (100+ episodes). Monitor performance trends, retrieval precision decay, and memory store growth. Target: establish whether performance plateaus or degrades, and at what storage threshold