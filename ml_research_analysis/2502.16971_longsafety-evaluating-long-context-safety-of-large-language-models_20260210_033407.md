---
ver: rpa2
title: 'LongSafety: Evaluating Long-Context Safety of Large Language Models'
arxiv_id: '2502.16971'
source_url: https://arxiv.org/abs/2502.16971
tags:
- safety
- context
- long
- long-context
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LONG SAFETY is the first comprehensive benchmark for evaluating
  LLM safety in open-ended long-context tasks. It includes 1,543 test cases averaging
  5,424 words, covering 7 safety issues and 6 task types.
---

# LongSafety: Evaluating Long-Context Safety of Large Language Models

## Quick Facts
- arXiv ID: 2502.16971
- Source URL: https://arxiv.org/abs/2502.16971
- Reference count: 32
- Most models achieve safety rates below 55% in long-context tasks

## Executive Summary
LongSafety is the first comprehensive benchmark for evaluating LLM safety in open-ended long-context tasks. The benchmark includes 1,543 test cases averaging 5,424 words, covering 7 safety issues and 6 task types. Results reveal that most models achieve safety rates below 55%, showing significant vulnerabilities in long-context safety. The study finds that long-context safety performance is misaligned with short-context safety, with models struggling particularly with sensitive topics and generation-oriented tasks.

## Method Summary
The LongSafety benchmark was developed through a systematic three-phase pipeline. First, task types and safety issues were defined based on prior literature and potential risks in long-context scenarios. Second, test cases were created with diverse sources including human-written, human-written with model-assisted creation, and purely model-generated content. Each test case underwent careful safety label annotation by experts. Third, a multi-agent evaluator was implemented using self-instruct models to assess safety, achieving 92% accuracy validated against human annotations.

## Key Results
- Most models achieve safety rates below 55% on long-context tasks
- Long-context safety performance is misaligned with short-context safety
- Relevant context and longer input sequences exacerbate safety risks

## Why This Works (Mechanism)
LongSafety addresses the critical gap in evaluating safety for long-context language models, where existing benchmarks focus primarily on short-context scenarios. The benchmark's effectiveness stems from its comprehensive coverage of both task diversity and safety issue types, combined with robust evaluation methodology using multiple independent judges.

## Foundational Learning
- **Long-context understanding**: Why needed - LLMs process increasingly long sequences; quick check - measure attention span across input lengths
- **Safety alignment**: Why needed - prevents harmful outputs; quick check - test against known safety triggers
- **Multi-agent evaluation**: Why needed - reduces individual judge bias; quick check - compare consensus vs individual judgments
- **Cross-cultural safety**: Why needed - safety perceptions vary by region; quick check - test with diverse cultural contexts
- **Context sensitivity**: Why needed - relevant information affects safety decisions; quick check - vary context relevance systematically

## Architecture Onboarding

Component map: Task Type Definition -> Test Case Generation -> Safety Label Annotation -> Multi-Agent Evaluation -> Benchmark Analysis

Critical path: Test Case Generation -> Safety Label Annotation -> Multi-Agent Evaluation

Design tradeoffs: The benchmark balances between comprehensive safety coverage and practical evaluation efficiency, using self-instruct models as judges while maintaining human validation for accuracy.

Failure signatures: Models show particular vulnerability to sensitive topics, generation-oriented tasks, and when relevant context amplifies problematic content.

First experiments:
1. Test safety rates across different model sizes to identify scaling effects
2. Evaluate the impact of context length on safety performance
3. Compare safety rates for different task types to identify high-risk scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark focuses on English language tasks and specific cultural contexts (US, India, China, Japan, Europe)
- Safety rate metrics below 55% may reflect evaluation methodology limitations rather than fundamental model vulnerabilities
- Specific mechanisms driving long-context safety divergence from short-context remain incompletely understood

## Confidence
- Benchmark findings overall: High confidence (multiple independent evaluators, comprehensive human validation)
- Multi-agent evaluator accuracy: High confidence (92% accuracy validated against human annotations)
- Long-context vs short-context misalignment: High confidence (well-supported by data)
- Context length and relevance effects: Medium confidence (correlational evidence)
- Cultural generalizability: Limited confidence (focus on 5 regions only)

## Next Checks
1. Test benchmark across diverse linguistic and cultural contexts to assess generalizability of safety findings
2. Conduct ablation studies to isolate which aspects of long-context (length, relevance, task type) most strongly influence safety failures
3. Evaluate whether fine-tuning on LongSafety improves actual safety behavior in real-world long-context deployments, not just benchmark performance