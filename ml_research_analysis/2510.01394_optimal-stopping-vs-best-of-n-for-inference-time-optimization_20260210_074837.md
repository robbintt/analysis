---
ver: rpa2
title: Optimal Stopping vs Best-of-$N$ for Inference Time Optimization
arxiv_id: '2510.01394'
source_url: https://arxiv.org/abs/2510.01394
tags:
- reward
- algorithm
- adaptive
- arxiv
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of optimizing inference-time compute
  allocation in large language models by deciding when to stop generating responses
  based on quality and cost trade-offs. It introduces a principled approach based
  on the Pandora's Box problem from optimal stopping theory, treating each generation
  as opening a costly box with random reward.
---

# Optimal Stopping vs Best-of-$N$ for Inference Time Optimization

## Quick Facts
- arXiv ID: 2510.01394
- Source URL: https://arxiv.org/abs/2510.01394
- Reference count: 40
- Key result: Adaptive UCB algorithm matches Best-of-N performance with 15-35% fewer LLM generations

## Executive Summary
This paper addresses the problem of optimizing inference-time compute allocation in large language models by deciding when to stop generating responses based on quality and cost trade-offs. The authors introduce a principled approach based on the Pandora's Box problem from optimal stopping theory, treating each generation as opening a costly box with random reward. They develop a UCB-style algorithm that adaptively learns stopping thresholds without prior knowledge of reward distributions, using confidence bounds on fair-cap values. Experiments on AlpacaFarm and HH-RLHF datasets with multiple LLM-reward model pairs show the adaptive strategy achieves the same performance as non-adaptive Best-of-N sampling while requiring 15-35% fewer generations on average.

## Method Summary
The authors frame inference-time optimization as a Pandora's Box problem where each LLM generation has an associated cost and random quality (reward). They introduce a UCB-based algorithm that maintains confidence bounds on the fair-cap value - the threshold at which the expected gain from another generation equals its cost. To handle reward scaling across different prompts, they employ a Bradley-Terry transformation that normalizes rewards and enables learning of prompt-specific stopping thresholds. The method adaptively decides whether to continue generating or stop based on the upper confidence bound of the current best reward compared to the fair-cap threshold. This approach eliminates the need for pre-specified generation budgets while maintaining theoretical performance guarantees.

## Key Results
- Adaptive UCB algorithm achieves same performance as Best-of-N sampling
- 15-35% reduction in average number of generations needed
- Validated on AlpacaFarm synthetic dataset and HH-RLHF real-world dataset
- Works across multiple LLM-reward model pairs (LLaMA-3.1-8B/70B, GPT-4o, various reward models)

## Why This Works (Mechanism)
The method works by framing inference-time optimization as a sequential decision problem where each generation has an uncertain reward and known cost. The UCB algorithm maintains statistical confidence bounds on the fair-cap value, which represents the maximum reward threshold where another generation is worthwhile. By adaptively updating these bounds based on observed rewards, the algorithm learns when additional generations are unlikely to improve the best response sufficiently to justify their cost. The Bradley-Terry transformation normalizes rewards across different prompts, enabling the algorithm to learn effective stopping thresholds that generalize across diverse tasks.

## Foundational Learning
**Optimal Stopping Theory**: Framework for sequential decision-making where choices must be made without full information about future options. Needed to model the trade-off between generation costs and uncertain rewards. Quick check: Verify that the expected reward distribution satisfies the assumptions of the theory.

**Upper Confidence Bound (UCB) Algorithms**: Statistical method for balancing exploration and exploitation by maintaining confidence intervals around estimates. Needed to adaptively learn fair-cap thresholds without prior knowledge. Quick check: Confirm confidence bounds properly capture uncertainty in reward estimates.

**Bradley-Terry Model**: Pairwise comparison model for ranking items based on relative preferences. Needed to normalize rewards across different prompts with varying scales. Quick check: Validate that transformed rewards maintain meaningful ordinal relationships.

## Architecture Onboarding
**Component Map**: LLM Generator -> Reward Model -> Bradley-Terry Transformer -> UCB Threshold Calculator -> Stopping Decision
**Critical Path**: Each generation flows through the reward model, gets transformed via Bradley-Terry, updates UCB statistics, and triggers stopping decision when UCB-bound exceeds fair-cap.
**Design Tradeoffs**: Adaptive stopping vs. fixed budget - adaptive provides efficiency but requires maintaining statistics; Bradley-Terry vs. direct normalization - Bradley-Terry handles relative comparisons better but adds complexity.
**Failure Signatures**: High false positive stopping (premature termination), high false negative stopping (excessive generations), or unstable thresholds across similar prompts.
**First Experiments**: 1) Test stopping behavior on synthetic rewards with known distributions, 2) Validate Bradley-Terry normalization across prompts with different reward scales, 3) Compare UCB confidence bounds with empirical reward distributions.

## Open Questions the Paper Calls Out
The paper acknowledges limitations around synthetic reward functions and limited real-world validation scope. The authors note that i.i.d. assumptions may not hold for diverse prompts and that reward model quality significantly impacts performance. They call for evaluation on more diverse human judgment datasets and testing under non-stationary reward conditions.

## Limitations
- Validation relies on synthetic reward functions rather than human judgments
- i.i.d. generation reward assumption may not hold for diverse prompts
- Performance guarantees depend on reward model quality and Bradley-Terry transformation effectiveness
- Limited scope of real-world validation datasets

## Confidence
**Theoretical Framework**: High confidence - optimal stopping theory is well-established
**Algorithmic Approach**: High confidence - UCB adaptation is logically sound
**Empirical Results**: Medium confidence - synthetic evaluation limits generalizability
**Real-world Applicability**: Medium confidence - limited validation scope

## Next Checks
1. Evaluate on real human judgment datasets across diverse task domains (reasoning, coding, creative writing) to assess robustness to varying prompt characteristics.
2. Test performance when reward distributions are non-stationary or exhibit prompt-dependent properties, measuring degradation in theoretical bounds.
3. Compare against more sophisticated inference-time methods like adaptive tree search or Monte Carlo Tree Search approaches on complex reasoning tasks.