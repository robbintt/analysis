---
ver: rpa2
title: 'When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training'
arxiv_id: '2509.24923'
source_url: https://arxiv.org/abs/2509.24923
tags:
- gaussian5
- reward
- training
- var1
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models often struggle with exploration in sequential
  decision-making tasks like the multi-armed bandit problem, defaulting to greedy
  behavior. This paper investigates how supervised fine-tuning (SFT) and reinforcement
  learning (RL) shape exploration strategies in LLMs.
---

# When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training

## Quick Facts
- arXiv ID: 2509.24923
- Source URL: https://arxiv.org/abs/2509.24923
- Reference count: 40
- Large language models trained on bandit tasks show emergent exploitation bias despite achieving UCB-level regret

## Executive Summary
This paper investigates how supervised fine-tuning (SFT) and reinforcement learning (RL) shape exploration strategies in large language models trained on multi-armed bandit tasks. While both methods achieve UCB-level performance on average, behavioral analysis reveals that learned policies exhibit premature exploitation and higher suffix failure rates than theoretically optimal UCB. RL-trained agents generalize more reliably than SFT across distributions, but both converge to more greedy variants of UCB. The study highlights that average regret can mask systematic exploration failures and underscores the importance of reward design and evaluation metrics beyond simple regret.

## Method Summary
The authors train Qwen 2.5 3B/7B models as meta-bandit agents using SFT on expert UCB trajectories and RL with three reward signals: original bandit rewards (RL-OG), strategic regret-shaped rewards (RL-STG), and algorithmic rewards that incentivize UCB imitation (RL-ALG). Training uses PPO with dual-scale GAE across intra-turn and inter-turn steps. The environments include 5-armed Gaussian and Bernoulli bandits with configurable variance and mean distributions. Evaluation includes standard regret metrics plus behavioral analysis of exploration patterns including suffix failure rates and greedy frequency.

## Key Results
- Both SFT and RL achieve UCB-level average regret but exhibit emergent exploitation bias with premature abandonment of exploration
- RL-ALG agents converge to greedy UCB variants (98% mention UCB formula) while outperforming their teacher by 10-20% in best-arm frequency
- SFT shows strong in-distribution performance but asymmetric generalization due to arithmetic capability degradation on OOD numeric ranges
- RL-trained agents demonstrate more reliable OOD generalization than SFT across bandit families
- Suffix failure rate reveals systematic exploration failures masked by average regret metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RL with algorithmic rewards produces policies that outperform the teacher by converging to greedier UCB variants.
- **Mechanism:** Binary algorithmic reward (r=1 if action matches UCB, else 0) creates an imitation objective that becomes correlated with exploitation as episodes progress. As uncertainty bounds shrink, UCB decisions increasingly select the greedy arm, making the reward signal easier to satisfy via exploitation than faithful exploration logic.
- **Core assumption:** The LLM can discover algorithmic variants through token-level policy optimization without explicit formula supervision.
- **Evidence anchors:**
  - [section] "RL-ALG trained in Gaussian5_Var1_MeanN0...converges to a UCB-like algorithm and mentions in its rationales 98% of the time: Q_t(a) + C × √(log(N_t(a)+1)/N_t(a))" — this variant's exploration term depends only on per-arm pulls, allowing premature abandonment of actions (p. 9-10).
  - [abstract] "agents trained to imitate UCB learn to outperform their teacher by adopting more exploitative variants."
  - [corpus] Related work "Exploitation Is All You Need... for Exploration" suggests exploitation strategies can paradoxically support exploration, but this paper's finding is specifically about unintended greedy convergence.
- **Break condition:** If the training horizon is too short for UCB's exploration phase to complete, the correlation between imitation and exploitation strengthens, accelerating greedy convergence.

### Mechanism 2
- **Claim:** Strategic (regret-shaped) rewards reduce training variance but do not alter the optimal policy, improving sample efficiency in high-variance environments.
- **Mechanism:** The strategic reward r̃_t = 1 - Δ_t/Δ_max uses pseudo-regret (expected regret rather than realized stochastic rewards), functioning as a control variate that reduces variance while preserving the same policy optimum as original rewards.
- **Core assumption:** Access to true arm means during training to compute pseudo-regret (not available at test time).
- **Evidence anchors:**
  - [section] "While this approach reduces variance, it theoretically does not alter the optimal policy to which the agent converges" (p. 4).
  - [section] "RL-STG empirically improves the performance of the policy in the Gaussian training setup" (p. 7).
  - [corpus] Weak corpus signal — no directly comparable regret-shaped reward formulations found.
- **Break condition:** As environment variance decreases (e.g., Bernoulli), RL-STG becomes equivalent to RL-OG, negating the variance-reduction benefit.

### Mechanism 3
- **Claim:** SFT achieves strong in-distribution performance through explicit UCB calculation supervision but generalizes asymmetrically due to arithmetic capability degradation.
- **Mechanism:** SFT trains on templated responses showing step-by-step UCB calculations. When training distributions lack certain inputs (e.g., negative rewards), the model's arithmetic capabilities for those cases degrade, causing systematic calculation errors and subsequent reasoning abandonment.
- **Core assumption:** The model's learned calculation procedures transfer across distribution shifts.
- **Evidence anchors:**
  - [section] "SFT policy trained on Bernoulli5_Uniform...systematic errors in simple calculations involving negative rewards, which are unseen during training—a sign of catastrophic forgetting of basic arithmetic skills" (p. 9).
  - [section] In Gaussian5_Var3_MeanN0, Bernoulli-trained SFT "chooses an arm different from the one with the highest calculated UCB value 78% of the time" (Appendix C).
  - [corpus] No direct corpus match for SFT arithmetic degradation in bandits.
- **Break condition:** OOD evaluation with inputs outside the training distribution's numeric range (negative rewards, different scales) triggers capability regression.

## Foundational Learning

- **Concept:** Multi-armed bandit regret (cumulative and instantaneous)
  - **Why needed here:** Regret is the primary metric, but the paper shows it can obscure failure modes like suffix failures.
  - **Quick check question:** Can an agent achieve low average regret while having high suffix failure rate? (Yes — exploitative policies can.)

- **Concept:** UCB algorithm with exploration constant C
  - **Why needed here:** UCB is the teacher policy and baseline; understanding its formula (log(t)/N_t(a) term) is essential to recognize how learned variants differ.
  - **Quick check question:** What happens to UCB's exploration if log(t) is replaced with log(N_t(a)) in the numerator? (Exploration becomes per-arm bounded, allowing permanent abandonment.)

- **Concept:** Token-level credit assignment in hierarchical RL
  - **Why needed here:** The paper uses dual (γ, λ) GAE across intra-turn and inter-turn steps; understanding how rewards propagate to early tokens is critical for debugging why RL-ALG converges to wrong formulas.
  - **Quick check question:** Why might an agent abandon a correct formula that produced incorrect numerical calculations? (Credit assignment misattributes poor outcomes to the formula rather than calculation errors.)

## Architecture Onboarding

- **Component map:** Environment samples → Observation summarization → LLM generates reasoning + action → Parser extracts action → Reward computation → PPO update with token-level advantages
- **Critical path:**
  1. Environment samples → Observation summarization → LLM generates reasoning + action → Parser extracts action → Reward computation → PPO update with token-level advantages.
  2. For RL-ALG: episode-level γ and λ set to 0 (myopic reward sufficient for reactive oracle).

- **Design tradeoffs:**
  - **RL-OG vs RL-STR:** OG is fully online (no oracle access), STR requires knowing true arm means during training.
  - **RL-ALG vs SFT:** ALG leaves reasoning unsupervised (more flexible but risks formula drift); SFT supervises reasoning explicitly (higher fidelity but brittle to distribution shift).
  - **7B vs 3B models:** 3B struggles to learn from environmental rewards (stagnant accuracy in RL-OG/RL-STR), requires teacher guidance.

- **Failure signatures:**
  - **High suffix failure rate with low regret:** Indicates exploitative convergence (check GreedyFreq@300, SuffixFail@150).
  - **Match rate divergence from UCB increasing over time:** Indicates learned variant drift (compare against both UCB oracle and discovered UCB-var).
  - **SFT calculation errors on OOD numeric ranges:** Check for negative numbers or values outside training distribution; inspect rationales for arithmetic mistakes.

- **First 3 experiments:**
  1. **Replicate in-distribution baseline:** Train RL-ALG on Gaussian5_Var1_MeanN0 (T=50), evaluate at T=300. Verify match rate against UCB and UCB-var, plot suffix failure over time.
  2. **Ablate reward type:** Compare RL-OG, RL-STR, RL-ALG on same environment with 3B and 7B models. Confirm 3B stagnation on environmental rewards.
  3. **Test OOD generalization:** Train SFT on Bernoulli5_Uniform, evaluate on Gaussian5_Var1_MeanN0. Inspect rationales for arithmetic errors on negative rewards; quantify calculation-to-decision divergence rate.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can more fine-grained RL reward signals enable agents to converge to the correct UCB formula rather than exploitative variants?
- **Basis in paper:** [explicit] Authors state that RL-ALG agents "converge to suboptimal variants of the UCB algorithm" due to credit assignment issues, where "the agent incorrectly attributes poor outcomes to the formula itself, rather than to flawed calculations," and propose "Future work could explore more fine-grained RL signals to address this problem."
- **Why unresolved:** Current sparse reward at action time fails to differentiate between correct algorithm application with calculation errors versus incorrect algorithm selection.
- **What evidence would resolve it:** Experiments with intermediate rewards for correct formula articulation or step-by-step calculation verification showing convergence to standard UCB rather than greedy variants.

### Open Question 2
- **Question:** Can focused replay techniques re-weighting experiences by information gain promote robust exploration over greedy exploitation?
- **Basis in paper:** [explicit] Conclusion advocates for "focused replay techniques that re-weight experiences based on information gain and surprise" to address the "fundamental imbalance in training data, where sparse exploration signals are easily overwhelmed by frequent exploitation."
- **Why unresolved:** Standard training emphasizes frequent exploitation actions; no mechanism currently prioritizes rare but informative exploration events.
- **What evidence would resolve it:** Agents trained with information-gain-weighted replay showing lower suffix failure rates and sustained exploration in long horizons compared to uniform sampling.

### Open Question 3
- **Question:** Can adversarial or curriculum-based environment design enforce robust long-horizon exploration behavior in LLM agents?
- **Basis in paper:** [explicit] Conclusion proposes "adversarial and curriculum-based environments that make robust long-horizon planning a necessity for success" to counteract emergent exploitation bias.
- **Why unresolved:** Current training environments may allow greedy strategies to achieve low average regret, masking catastrophic failure modes; no pressure exists for worst-case robustness.
- **What evidence would resolve it:** Curriculum-trained agents maintaining low regret and low suffix failure rates when evaluated on adversarially designed bandit instances specifically crafted to exploit greedy strategies.

## Limitations

- Average regret metrics can mask systematic exploration failures, making suffix failure rates a more critical evaluation measure
- SFT shows asymmetric generalization due to arithmetic capability degradation on OOD numeric ranges
- 3B models require teacher guidance to learn from environmental rewards, limiting scalability
- Current reward designs lead to convergence on more greedy UCB variants rather than the theoretically optimal algorithm

## Confidence

- **High Confidence:** Claims about SFT performance degradation on OOD arithmetic tasks, the convergence of RL-ALG to more exploitative UCB variants, and the superior OOD generalization of RL over SFT are well-supported by empirical evidence and behavioral metrics.
- **Medium Confidence:** The mechanism explaining why RL-ALG converges to greedy variants (due to the correlation between imitation and exploitation as episodes progress) is plausible but could benefit from more detailed analysis of the training dynamics.
- **Medium Confidence:** The claim that regret-shaped rewards reduce variance without changing optimal policy is theoretically sound but the empirical improvement in high-variance environments needs broader validation.

## Next Checks

1. Test RL-ALG training with truncated horizons (T < 50) to quantify how early stopping accelerates greedy convergence versus faithful UCB imitation.
2. Implement a "formula correctness" reward signal that isolates UCB formula accuracy from numerical calculation errors, then compare convergence behavior against RL-ALG.
3. Evaluate trained agents on non-stationary bandit environments where arm means drift over time to test whether learned policies maintain exploration when the optimal arm changes.