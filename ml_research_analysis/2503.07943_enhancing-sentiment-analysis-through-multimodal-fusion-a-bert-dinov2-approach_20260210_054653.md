---
ver: rpa2
title: 'Enhancing Sentiment Analysis through Multimodal Fusion: A BERT-DINOv2 Approach'
arxiv_id: '2503.07943'
source_url: https://arxiv.org/abs/2503.07943
tags:
- fusion
- sentiment
- multimodal
- bert
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes a novel multimodal sentiment analysis architecture
  that integrates text and image data to enhance sentiment understanding. It employs
  BERT for text feature extraction and DINOv2 for image feature extraction, then fuses
  these features using three proposed methods: Basic Fusion Model, Self-Attention
  Fusion Model, and Dual-Attention Fusion Model.'
---

# Enhancing Sentiment Analysis through Multimodal Fusion: A BERT-DINOv2 Approach

## Quick Facts
- **arXiv ID**: 2503.07943
- **Source URL**: https://arxiv.org/abs/2503.07943
- **Reference count**: 28
- **Primary Result**: Proposes multimodal sentiment analysis using BERT-DINOv2 with three fusion methods achieving SOTA on MVSA-single (accuracy 0.73, F1 0.71)

## Executive Summary
This paper introduces a novel multimodal sentiment analysis architecture that integrates text and image data to enhance sentiment understanding. The approach employs BERT for text feature extraction and DINOv2 for image feature extraction, then fuses these features using three proposed methods: Basic Fusion Model, Self-Attention Fusion Model, and Dual-Attention Fusion Model. The model is evaluated on three datasets (Memotion 7k, MVSA-single, and MVSA-multi) and achieves state-of-the-art performance on the MVSA-single dataset. The study demonstrates the effectiveness of combining BERT and DINOv2 through attention-based fusion techniques for multimodal sentiment analysis.

## Method Summary
The proposed method uses BERT to extract contextual text features and DINOv2 to extract visual features from images. These features are then combined using three different fusion approaches: Basic Fusion (simple concatenation), Self-Attention Fusion (text-to-text and image-to-image attention), and Dual-Attention Fusion (cross-modal attention between text and image features). The fused features are passed through fully connected layers for sentiment classification. The model is trained end-to-end on multimodal sentiment datasets, with separate evaluations for single-label and multi-label sentiment classification tasks.

## Key Results
- Achieves state-of-the-art accuracy of 0.73 and F1 score of 0.71 on MVSA-single dataset
- Dual-Attention Fusion Model achieves macro F1 score of 0.3552 on Memotion 7k dataset
- Demonstrates effectiveness of combining BERT and DINOv2 through attention-based fusion techniques

## Why This Works (Mechanism)
The paper doesn't explicitly explain the mechanism behind why this approach works. However, the integration of BERT's contextual text understanding with DINOv2's visual feature extraction, combined with attention-based fusion methods, likely captures complementary information from both modalities that improves sentiment classification accuracy.

## Foundational Learning
- **BERT for text feature extraction**: BERT provides contextual embeddings that capture semantic meaning and relationships in text. This is needed because raw text needs to be converted into numerical representations that capture meaning for machine learning models.
- **DINOv2 for image feature extraction**: DINOv2 is a self-supervised vision transformer that provides rich visual features. This is needed because images contain visual cues that complement textual sentiment information.
- **Attention mechanisms**: Self-attention and cross-attention help the model focus on relevant parts of both text and image features. This is needed because not all parts of the input contribute equally to sentiment understanding.
- **Multimodal fusion**: Combining information from different modalities can provide more comprehensive understanding than single modalities alone. This is needed because sentiment often manifests through both visual and textual cues.
- **Dual-Attention Fusion**: The cross-modal attention between text and image features allows the model to learn relationships between visual and textual elements. This is needed because sentiment expression often involves interactions between what is said and what is shown.
- **State-of-the-art evaluation**: Comparing against previous methods on established benchmarks validates the approach's effectiveness. This is needed to demonstrate that the proposed method provides tangible improvements over existing solutions.

## Architecture Onboarding

**Component Map**: Text -> BERT -> Text Features; Image -> DINOv2 -> Image Features; Features -> Fusion (Basic/Self-Attention/Dual-Attention) -> FC Layers -> Sentiment Classification

**Critical Path**: Input text and image → BERT and DINOv2 feature extraction → Attention-based fusion → Fully connected layers → Sentiment classification output

**Design Tradeoffs**: The choice between three fusion methods (Basic, Self-Attention, Dual-Attention) represents a tradeoff between model complexity and performance. Dual-Attention provides the best performance but adds computational overhead compared to Basic Fusion.

**Failure Signatures**: Poor performance on datasets with high sarcasm or complex multimodal sentiment expressions, as evidenced by the modest F1 score (0.3552) on Memotion 7k despite claims of state-of-the-art performance.

**First Experiments**:
1. Train Basic Fusion model on MVSA-single dataset and compare performance with Dual-Attention Fusion
2. Evaluate each fusion method's performance on Memotion 7k to understand their relative strengths
3. Perform ablation study by removing BERT or DINOv2 features to quantify their individual contributions

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The macro F1 score of 0.3552 on Memotion 7k appears modest despite claims of state-of-the-art performance, raising questions about effectiveness for complex multimodal sentiment tasks
- Lacks detailed ablation studies to quantify individual contributions of BERT versus DINOv2 features and the specific impact of each fusion method
- Evaluation focuses primarily on accuracy and F1 metrics without exploring other relevant measures such as precision-recall trade-offs or confusion matrices

## Confidence

**High confidence**: The technical implementation of using BERT for text and DINOv2 for image feature extraction, along with the basic fusion methodology, appears sound and follows established practices in multimodal learning.

**Medium confidence**: The claims of achieving state-of-the-art performance, particularly on MVSA-single (accuracy 0.73, F1 0.71) and Memotion 7k (macro F1 0.3552), are supported by reported results but lack sufficient comparative analysis and statistical significance testing against baseline models.

**Low confidence**: The assertion that the Dual-Attention Fusion Model significantly outperforms previous approaches on complex multimodal sentiment analysis tasks, given the relatively modest F1 score on Memotion 7k and absence of comprehensive error analysis or qualitative examples demonstrating model superiority.

## Next Checks

1. Conduct statistical significance testing comparing the proposed model's performance against all reported baselines on each dataset to verify claims of state-of-the-art achievement, including confidence intervals and p-values for the reported metrics.

2. Perform detailed ablation studies that systematically remove or replace components (BERT features, DINOv2 features, each attention mechanism) to quantify their individual contributions to overall performance and understand which aspects of the architecture drive improvements.

3. Generate and analyze confusion matrices and per-class performance metrics across all sentiment categories to identify specific strengths and weaknesses of the model, particularly for challenging cases involving sarcasm, humor, or mixed sentiments that characterize real-world multimodal content.