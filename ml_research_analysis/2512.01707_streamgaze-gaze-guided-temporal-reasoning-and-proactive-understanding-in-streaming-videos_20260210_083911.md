---
ver: rpa2
title: 'StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in
  Streaming Videos'
arxiv_id: '2512.01707'
source_url: https://arxiv.org/abs/2512.01707
tags:
- gaze
- object
- video
- fixation
- streaming
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STREAMGAZE is the first benchmark for gaze-guided streaming video
  understanding, addressing the gap in evaluating how multimodal large language models
  (MLLMs) use human gaze signals for temporal reasoning and proactive prediction.
  It introduces a semi-automatic pipeline that aligns egocentric videos with raw gaze
  trajectories via fixation extraction, region-specific visual prompting, and scanpath
  construction to generate spatio-temporally grounded QA pairs.
---

# StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos

## Quick Facts
- arXiv ID: 2512.01707
- Source URL: https://arxiv.org/abs/2512.01707
- Reference count: 40
- First benchmark for gaze-guided streaming video understanding

## Executive Summary
STREAMGAZE introduces the first benchmark specifically designed to evaluate how multimodal large language models (MLLMs) leverage human gaze signals for temporal reasoning and proactive understanding in streaming videos. The benchmark addresses a critical gap in current video understanding research by requiring models to interpret real-time gaze patterns, track attention shifts, and infer intentions from only past and currently observed frames. Through a semi-automatic pipeline that aligns egocentric videos with raw gaze trajectories, STREAMGAZE generates spatio-temporally grounded QA pairs across 10 tasks covering past, present, and proactive reasoning scenarios.

The benchmark reveals substantial performance gaps between state-of-the-art MLLMs (including GPT-4o and InternVL-3.5) and human performance, highlighting fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction capabilities. Detailed analyses of gaze-prompting strategies and task-specific behaviors provide insights into why current models struggle with these tasks and what capabilities future models must develop to bridge this gap.

## Method Summary
STREAMGAZE employs a semi-automatic pipeline that aligns egocentric videos with raw gaze trajectories through fixation extraction, region-specific visual prompting, and scanpath construction. The pipeline converts gaze data into visual prompts that capture spatio-temporal attention patterns, then generates QA pairs by analyzing the relationship between gaze movements and video content. The benchmark includes 8,521 QA pairs distributed across 10 distinct tasks that require models to perform various reasoning operations, from basic temporal tracking to complex proactive understanding of viewer intentions. Each task is designed to test specific aspects of gaze-guided video comprehension, with questions requiring interpretation of real-time gaze patterns in conjunction with video frame analysis.

## Key Results
- Substantial performance gaps between MLLMs (GPT-4o, InternVL-3.5) and human performance across all tasks
- Current MLLMs struggle with gaze-based temporal reasoning, intention modeling, and proactive prediction
- Detailed analysis reveals specific limitations in how models interpret gaze patterns and integrate them with video content
- Performance varies significantly across different task types, with proactive reasoning being particularly challenging

## Why This Works (Mechanism)
STREAMGAZE works by creating a structured evaluation framework that forces models to integrate two challenging modalities: temporal video understanding and human gaze interpretation. The semi-automatic pipeline ensures that gaze signals are properly aligned with video content through fixation extraction and scanpath construction, creating temporally coherent attention patterns. By generating QA pairs that require both past and present frame analysis, the benchmark captures the dynamic nature of streaming video comprehension where models must reason about temporal dependencies while simultaneously interpreting viewer attention.

## Foundational Learning
- Egocentric video processing: Understanding first-person perspective video content and its unique characteristics
- Gaze trajectory analysis: Interpreting raw gaze data to extract meaningful attention patterns and fixations
- Temporal reasoning: Tracking and reasoning about events and changes across video time sequences
- Proactive prediction: Inferring future actions or intentions based on current observations and gaze patterns
- Visual prompting techniques: Converting complex gaze trajectories into effective visual representations for MLLMs
- Multimodal integration: Combining visual, temporal, and gaze information into coherent understanding

These concepts are needed because gaze-guided video understanding requires simultaneous processing of multiple dynamic information streams. Quick checks include verifying gaze-video alignment accuracy, testing temporal consistency of generated QA pairs, and validating that visual prompts effectively capture gaze attention patterns.

## Architecture Onboarding
Component map: Video frames -> Gaze trajectory extraction -> Fixation detection -> Visual prompt generation -> QA pair creation -> MLLM evaluation

Critical path: The pipeline processes egocentric videos through gaze alignment to create spatio-temporally grounded prompts that are then used to generate QA pairs. This path is critical because any error in gaze-video alignment propagates through to the final evaluation.

Design tradeoffs: The semi-automatic approach balances scalability with quality control, but introduces potential biases in how gaze trajectories are converted to prompts. The choice to focus on single-gaze tracking simplifies the problem but limits ecological validity.

Failure signatures: Poor gaze-video alignment manifests as temporally inconsistent attention patterns. Inadequate visual prompting results in models ignoring gaze signals. Overly complex QA pairs that require reasoning beyond current MLLM capabilities.

First experiments:
1. Validate gaze-video alignment accuracy across different video types
2. Test visual prompt effectiveness with simple attention tracking tasks
3. Evaluate baseline MLLM performance on individual task types before full benchmark deployment

## Open Questions the Paper Calls Out
None

## Limitations
- Relatively small scale with 8,521 QA pairs may not comprehensively evaluate all aspects
- Semi-automatic pipeline introduces potential biases in gaze-to-prompt conversion
- Focus on egocentric videos limits generalizability to other video domains
- Single-gaze tracking may not capture collaborative viewing scenarios

## Confidence
High confidence: STREAMGAZE is the first benchmark specifically designed for gaze-guided streaming video understanding, with clear differentiation from existing video understanding benchmarks.

Medium confidence: Performance gaps between MLLMs and humans are reported, but human evaluation methodology details are limited in the abstract.

Low confidence: Claims about future model capabilities are forward-looking and not empirically validated in the abstract.

## Next Checks
1. Conduct inter-annotator reliability tests on QA pair generation to quantify consistency and identify biases
2. Perform ablation studies comparing MLLM performance with and without gaze guidance across video genres
3. Extend the benchmark with additional QA pairs and tasks to test scalability and coverage, especially for proactive prediction scenarios