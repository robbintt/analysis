---
ver: rpa2
title: 'InSQuAD: In-Context Learning for Efficient Retrieval via Submodular Mutual
  Information to Enforce Quality and Diversity'
arxiv_id: '2508.21003'
source_url: https://arxiv.org/abs/2508.21003
tags:
- learning
- diversity
- in-context
- retrieval
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving In-Context Learning
  (ICL) by enhancing the quality and diversity of in-context exemplars through a unified
  combinatorial approach. The authors introduce InSQuaD, which leverages Submodular
  Mutual Information (SMI) functions to jointly model quality, diversity, and order
  in exemplar selection.
---

# InSQuAD: In-Context Learning for Efficient Retrieval via Submodular Mutual Information to Enforce Quality and Diversity

## Quick Facts
- arXiv ID: 2508.21003
- Source URL: https://arxiv.org/abs/2508.21003
- Reference count: 40
- One-line primary result: Unified SMI-based exemplar selection achieves up to 21.6% gains on classification, 16.4% on multi-choice, and 7% on generation tasks while reducing inference time.

## Executive Summary
This paper introduces InSQuaD, a unified framework that leverages Submodular Mutual Information (SMI) functions to improve In-Context Learning (ICL) exemplar selection by jointly optimizing for quality, diversity, and order. The approach addresses the key challenge that exemplars in ICL must be both relevant to the query (quality) and sufficiently diverse to improve generalization (diversity), while also being ordered appropriately. InSQuaD comprises two components: InSQuaD-RETRIEVE for targeted exemplar selection using SMI maximization, and InSQuaD-LEARN for training a retrieval model that inherently captures quality and diversity through a novel likelihood-based loss derived from SMI functions. Evaluated across nine benchmark datasets, InSQuaD demonstrates significant improvements over existing baselines while reducing inference times compared to iterative selection methods.

## Method Summary
InSQuaD addresses ICL exemplar selection through a unified SMI-based approach. The framework operates in two phases: InSQuaD-LEARN fine-tunes a retrieval model (SBERT/MPNet) using a novel likelihood-based loss that enforces both quality (query-relevant documents vs. distractors) and diversity (query-relevant documents vs. paraphrases), with the trade-off controlled by hyperparameter λ. InSQuaD-RETRIEVE then performs targeted exemplar selection at inference time using greedy SMI maximization to select a diverse yet relevant set of exemplars. The method is trained on augmented HotpotQA data with GPT-3.5-generated paraphrases and evaluated on nine downstream ICL tasks, demonstrating superior performance to baseline selection strategies while maintaining computational efficiency through single-pass greedy optimization.

## Key Results
- Achieves up to 21.6% performance gains on classification tasks compared to random exemplar selection
- Demonstrates 16.4% improvements on multi-choice tasks and up to 7% on generation-based tasks
- Reduces inference times by using a combinatorial approach compared to iterative selection methods
- Outperforms state-of-the-art baselines including Vote-K and IDEAL across all nine benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Submodular Mutual Information (SMI) functions unify quality, diversity, and ordering in a single-stage selection formulation.
- Mechanism: SMI functions $I_f(A; Q)$ measure shared information between candidate set $A$ and query $Q$. Maximizing SMI via greedy selection yields exemplars with high query relevance (quality) while the diminishing returns property of submodular $f$ naturally promotes diversity. Greedy optimization produces an implicit ordering based on incremental information gains, addressing exemplar order sensitivity observed in ICL.
- Core assumption: Embedding similarity in the pretrained retrieval model meaningfully captures task-relevance and diversity signals for downstream ICL tasks.
- Evidence anchors: [abstract] "introduce a unified selection strategy based on SMIs which mines relevant yet diverse in-context examples encapsulating the notions of quality and diversity"; [section III-C1] Eq. 2-3 and Algorithm 1 describe targeted selection with SMI maximization; "adopting greedy optimization in maximizing SMI models an implicit ordering of exemplars"; [corpus] KITE (arXiv 2509.15676) similarly uses information-theoretic exemplar selection.

### Mechanism 2
- Claim: Likelihood-based SMI training objectives bake quality and diversity into retrieval model parameters.
- Mechanism: InSQuaD-LEARN frames retrieval as a Submodular Point Process (SPP). Given positive set $S^+$ (relevant docs), distractors $S^-$, and paraphrases $S_p$, it minimizes $\mathcal{L} = \log(I_f(S^-; Q)) - \log(I_f(S^+; Q))$. Quality loss $\mathcal{L}_q$ pushes query-relevant docs higher; diversity loss $\mathcal{L}_d$ using paraphrases discourages near-duplicates. Joint loss $\mathcal{L}_{InSQuAD} = \exp((1-\lambda)\mathcal{L}_q + \lambda \mathcal{L}_d)$ interpolates both.
- Core assumption: Paraphrases adequately proxy diversity requirements in downstream ICL; the SPP likelihood accurately reflects retrieval desirability.
- Evidence anchors: [abstract] "learns the parameters of an SMI function to enforce both quality and diversity in the retrieval model through a novel likelihood-based loss"; [section III-C2] Eq. 5-7 and Table I define $\mathcal{L}_q$, $\mathcal{L}_d$, and joint loss with FL/GC/LD instantiations; [corpus] Related work on submodular data selection (PRISM, SCORE) supports combinatorial losses for representation learning.

### Mechanism 3
- Claim: Greedy combinatorial selection reduces inference time vs. iterative methods while preserving joint quality-diversity optimization.
- Mechanism: Instead of multi-stage pipelines (retrieve→rerank→select) or iterative influence maximization, InSQuaD performs single-pass greedy SMI maximization with $(1-e^{-1})$ approximation guarantee. The similarity kernel $S_{ab}$ is precomputed; selection requires $O(k|V|)$ kernel lookups per query.
- Core assumption: Precomputed embeddings and kernel are sufficiently stable; approximation guarantee translates to empirical performance gains.
- Evidence anchors: [abstract] "reduces inference times by using a combinatorial approach, making it more efficient than iterative selection methods"; [section IV-D] Figure 4 shows log-scale inference time reductions vs. Vote-K and IDEAL; [corpus] Submodular Context Partitioning (arXiv 2510.05130) also addresses ICL efficiency but via partitioning.

## Foundational Learning

- Concept: **Submodular Functions and Diminishing Returns**
  - Why needed here: Core mathematical property enabling diversity through greedy maximization with approximation guarantees.
  - Quick check question: Given set function $f$, if $f(A \cup \{v\}) - f(A) \geq f(B \cup \{v\}) - f(B)$ for $A \subseteq B$, what does this imply about adding $v$ to larger sets?

- Concept: **Mutual Information for Targeted Selection**
  - Why needed here: SMI extends submodular selection to query-conditioned (targeted) scenarios, essential for query-relevant exemplar retrieval.
  - Quick check question: How does $I_f(A; Q)$ differ from maximizing $f(A)$ alone when $Q$ is a query embedding?

- Concept: **Probabilistic Interpretation via Point Processes**
  - Why needed here: Bridges combinatorial objectives to differentiable loss functions for neural retrieval model training.
  - Quick check question: In Eq. 4, why does the denominator normalize over all subsets, and what simplification makes the loss tractable?

## Architecture Onboarding

- Component map:
  - Retrieval Model $R(\cdot, \theta)$ -> SMI Selector -> Training Data Generator -> Loss Composer

- Critical path:
  1. Prepare augmented training data (paraphrase generation)
  2. Train $R$ with $\mathcal{L}_{InSQuAD}$ (7 epochs, lr=3e-5)
  3. Precompute kernel $S_{ab}$ on annotated pool $V_{labeled}$
  4. At inference: embed query → greedy SMI selection → construct prompt

- Design tradeoffs:
  - FL vs. GC vs. LD: Paper finds GC best on average (rank 3.6), but FL/LD may excel on specific tasks (Table II).
  - $\lambda \in [0,1]$: Controls quality-diversity tradeoff; Table IV shows task-specific optimal $\lambda$, requiring calibration.
  - Annotation budget $B$: Figure 3c shows diminishing returns beyond $B=18$; larger budgets increase selection complexity without proportional gains.

- Failure signatures:
  - If selected exemplars are semantically similar despite diversity objective: check $\lambda$ (may be too low), verify paraphrase quality in training.
  - If inference is slow despite combinatorial approach: profile kernel precomputation vs. on-the-fly embedding; ensure kernel is cached.
  - If performance degrades on domain-shifted queries: $R$ may have overfit to HotpotQA distribution; consider mixed-domain training data.

- First 3 experiments:
  1. Reproduce Table II entry for MRPC with InSQuaD-GC: train $R$ on augmented HotpotQA, run greedy selection with $k=4$, verify $\sim$0.58 accuracy.
  2. Ablate $\lambda \in \{0, 0.25, 0.5, 0.75, 1.0\}$ on 2 classification + 1 generation task (Table IV partial reproduction) to find task-specific optima.
  3. Measure inference latency (Figure 4 partial): compare InSQuaD-GC vs. random vs. top-k similarity on 100 queries, report mean/std time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of InSQuaD-LEARN vary when trained on multi-hop QA datasets other than HotpotQA?
- Basis in paper: [explicit] The authors state in the conclusion that the current model adopts only the HotpotQA dataset, leaving other multi-hop datasets for future research.
- Why unresolved: The experimental scope was restricted to a single dataset source to leverage its popularity, without validating generalization to contemporaneous datasets like MuSiQue or ConcurrentQA.
- What evidence would resolve it: Benchmarking the retrieval model trained on alternative multi-hop QA datasets using the same ICL evaluation framework.

### Open Question 2
- Question: Can the quality-diversity trade-off hyperparameter $\lambda$ be automated to avoid task-specific calibration?
- Basis in paper: [inferred] The ablation study (Section V) notes that the optimal value of $\lambda$ varies based on the downstream task, requiring the user to perform task-specific calibration.
- Why unresolved: While the paper demonstrates the importance of the trade-off, it does not offer a mechanism to determine the optimal $\lambda$ dynamically without manual tuning.
- What evidence would resolve it: A proposed adaptive mechanism for $\lambda$ that achieves comparable performance to the manually tuned baseline without human intervention.

### Open Question 3
- Question: What methods can effectively mitigate selection biases during the exemplar annotation and retrieval stages?
- Basis in paper: [explicit] The conclusion identifies addressing selection biases during exemplar annotation and retrieval as a potential future research direction.
- Why unresolved: The current framework utilizes greedy optimization on an unlabeled pool, which may inherit or amplify biases present in the source data without correction.
- What evidence would resolve it: Experiments incorporating debiasing techniques into the SMI objective function, measuring fairness metrics alongside accuracy.

## Limitations

- Performance depends on the quality of the retrieval model's embeddings, which may not generalize across all domains or task types
- The quality-diversity tradeoff controlled by λ requires manual task-specific tuning, suggesting the unified formulation may not fully generalize
- Computational efficiency gains depend heavily on kernel precomputation, which may not scale to dynamic or streaming scenarios

## Confidence

- **High confidence** in the core SMI-based selection mechanism (Mechanism 1) due to clear mathematical foundations and consistent approximation guarantees
- **Medium confidence** in the SPP-based training objective (Mechanism 2) as it extends established submodular selection theory to differentiable training, though empirical validation across diverse domains is limited
- **Medium confidence** in the practical efficiency claims (Mechanism 3) based on the reported log-scale improvements, though absolute runtime measurements would strengthen this claim

## Next Checks

1. Conduct domain transfer experiments using InSQuaD-LEARN trained on HotpotQA but evaluated on out-of-domain queries to test embedding generalization
2. Perform ablation studies varying annotation budget B beyond the reported 18-100 range to identify scaling patterns and practical limits
3. Implement and compare alternative diversity proxies (e.g., semantic clusters, lexical diversity) against the paraphrase-based L_d to test robustness of the diversity objective