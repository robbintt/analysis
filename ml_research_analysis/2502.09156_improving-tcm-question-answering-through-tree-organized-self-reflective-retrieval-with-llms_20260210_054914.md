---
ver: rpa2
title: Improving TCM Question Answering through Tree-Organized Self-Reflective Retrieval
  with LLMs
arxiv_id: '2502.09156'
source_url: https://arxiv.org/abs/2502.09156
tags:
- knowledge
- chinese
- medicine
- treatment
- traditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Tree-Organized Self-Reflective Retrieval
  (TOSRR) framework for Traditional Chinese Medicine (TCM) question answering, addressing
  the challenge of insufficient knowledge representation and retrieval in TCM domains.
  The method constructs a tree-structured knowledge base using Subject-Predicate-Object-Text
  (SPO-T) triples and integrates a self-reflective mechanism for iterative retrieval
  and validation.
---

# Improving TCM Question Answering through Tree-Organized Self-Reflective Retrieval with LLMs

## Quick Facts
- arXiv ID: 2502.09156
- Source URL: https://arxiv.org/abs/2502.09156
- Reference count: 0
- GPT-4 accuracy on TCM MLE improved by 19.85% absolute with TOSRR framework

## Executive Summary
This paper introduces a Tree-Organized Self-Reflective Retrieval (TOSRR) framework for Traditional Chinese Medicine question answering. The method addresses insufficient knowledge representation in TCM domains by constructing a tree-structured knowledge base using SPO-T (Subject-Predicate-Object-Text) triples and integrating a self-reflective mechanism for iterative retrieval and validation. Experiments show TOSRR improves GPT-4 performance by 19.85% absolute accuracy on TCM Medical Licensing Examination and increases recall accuracy from 27% to 38% on Classics Course Exam.

## Method Summary
The TOSRR framework constructs a tree-structured knowledge base from 33 TCM textbooks using SPO-T triples, combining structured knowledge graphs with rich semantic context. The system employs multi-way recall with keyword-based SPO matching and dense vector retrieval, followed by a self-reflective loop where the model iteratively validates and refines answers. The knowledge base contains 28,599 SPO-T triples and 8,460 Q&A pairs, processed through ERNIE-Layout for document analysis, GPT-4 for extraction, and HNSWLib for vector storage.

## Key Results
- TOSRR achieved 75.67% accuracy on TCM Medical Licensing Examination vs. 55.83% for GPT-4 alone
- SPO-T structure improved recall accuracy from 27% to 38% on Classics Course Exam
- Manual evaluation showed 18.52 point improvement across safety, consistency, explainability, compliance, and coherence dimensions
- Self-reflection component added 5.5 percentage points to accuracy over SPO-T RAG alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SPO-T structure improves retrieval relevance by integrating structured knowledge graphs with semantic context
- Mechanism: Tree-organized knowledge base where SPO triples serve as structural backbone with text chunks as leaves, allowing retrieval to balance relational hints with explanatory context
- Core assumption: TCM QA requires both precise relational knowledge and deep semantic understanding that a hybrid structure can serve better than either alone
- Evidence: SPO-T RAG improved recall by 0.11 (27% to 38%); internal ablation shows incremental gains over pure vector RAG

### Mechanism 2
- Claim: Self-reflective retrieval loop improves accuracy through iterative validation
- Mechanism: After answer generation, model assesses if retrieved SPO-Ts support the answer and if the answer is helpful, reformulating query if needed
- Core assumption: GPT-4 possesses sufficient meta-cognitive ability to judge its own output's groundedness in specialized domains
- Evidence: TOSRR (75.67%) outperformed SPO-T RAG (70.17%) on TCM MLE, showing additive gain from self-reflection

### Mechanism 3
- Claim: Multi-way recall captures broader relevant knowledge than single-method retrieval
- Mechanism: Combines keyword matching for SPO triples with dense vector similarity for text summaries and chunks
- Core assumption: Relevant TCM knowledge can be signaled by both explicit terminology and implicit semantic similarity
- Evidence: Recall evaluation shows SPO-T RAG outperformed pure RAG; fusion strategy captures complementary signals

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**: Why needed - TOSRR is specialized RAG framework; Quick check - Explain how RAG differs from fine-tuning an LLM on domain data. What is the primary advantage of RAG for rapidly updating knowledge?

- **Knowledge Graphs (KGs) & SPO Triples**: Why needed - SPO-T structure is hybrid of KG principles and text; Quick check - Provide an example of an SPO triple for a TCM concept. How does adding "Text" component change its utility for an LLM?

- **Self-Reflection in LLMs**: Why needed - Core innovation includes reflection loop; Quick check - Design a simple two-step prompt that first asks an LLM to answer a question and then asks it to verify if its answer is supported by provided context

## Architecture Onboarding

- **Component Map**: Knowledge Base Constructor -> Vector & Index Store -> Multi-way Recall Engine -> Self-Reflective RAG Controller -> LLM Interface
- **Critical Path**: System performance hinges on quality of SPO-T knowledge base and reliability of self-reflection loop
- **Design Tradeoffs**: SPO-T structure and reflection loop add complexity vs. accuracy gains; retrieval breadth vs. noise in fused results
- **Failure Signatures**: Low recall indicates knowledge base lacks depth; high latency from multiple LLM calls; "I don't know" loops from unreliable self-assessment
- **First 3 Experiments**:
  1. Ablation on knowledge structure: Compare pure vector RAG, SPO-only RAG, and SPO-T RAG to quantify text component contribution
  2. Reflection loop analysis: Log iteration counts per question and correlate with difficulty types
  3. Recall-answer correlation: Manually score top-15 recalled items for sample questions and correlate with final accuracy

## Open Questions the Paper Calls Out

- **Question 1**: How does TOSRR perform in actual clinical assistance and TCM teaching scenarios compared to controlled exam benchmarks? The authors note application in real scenarios "awaits further research and evaluation" - current study limited to MLE and CCE datasets.

- **Question 2**: Can automated evaluation metrics like RAGAs scoring effectively replace or augment manual expert review? Current methodology relied entirely on resource-intensive manual scoring by 10 TCM experts without scalability.

- **Question 3**: To what extent does integrating real user data improve safety and accuracy of TOSRR? Authors identify "integrating real user data to optimize response" as future research focus - current knowledge base from static textbooks only.

## Limitations
- Exact prompt templates for SPO extraction, self-reflection, and knowledge base construction are not provided
- Expert evaluation lacks inter-rater reliability metrics and methodological transparency
- Self-reflection implementation details are vague regarding whether it uses specialized critic tokens
- Knowledge base quality depends on GPT-4's SPO extraction accuracy, which isn't validated against ground truth

## Confidence
- **High Confidence**: General framework design clearly specified; ablation studies well-documented
- **Medium Confidence**: Absolute performance numbers reported with statistical rigor but lack significance testing
- **Low Confidence**: Qualitative evaluation scores lack transparency in rating scales and inter-rater reliability

## Next Checks
1. **Prompt Template Validation**: Request exact prompts for SPO extraction, self-reflection, and knowledge base construction; run small-scale experiment comparing TOSRR with and without specific prompts
2. **Inter-Rater Reliability for Expert Evaluation**: Replicate expert evaluation with 3-5 independent raters; calculate inter-rater reliability for each dimension
3. **Knowledge Base Accuracy Audit**: Manually validate random sample of 50 SPO triples extracted from textbooks; compare against ground truth to quantify extraction error rates