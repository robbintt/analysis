---
ver: rpa2
title: 'Backward Oversmoothing: why is it hard to train deep Graph Neural Networks?'
arxiv_id: '2505.16736'
source_url: https://arxiv.org/abs/2505.16736
tags:
- oversmoothing
- gnns
- backward
- graph
- stationary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes a fundamental optimization challenge in deep
  Graph Neural Networks (GNNs) called backward oversmoothing. While forward oversmoothing
  (where node representations become indistinguishable as layers increase) has been
  extensively studied, the authors examine how backpropagated errors are also subject
  to oversmoothing from output to input layers.
---

# Backward Oversmoothing: why is it hard to train deep Graph Neural Networks?

## Quick Facts
- arXiv ID: 2505.16736
- Source URL: https://arxiv.org/abs/2505.16736
- Reference count: 40
- Primary result: Backward oversmoothing creates spurious stationary points where deep GNNs get stuck with near-zero gradients despite high loss

## Executive Summary
This paper identifies a fundamental optimization challenge in deep Graph Neural Networks called backward oversmoothing, where backpropagated errors become oversmoothed from output to input layers. While forward oversmoothing has been extensively studied, this work shows that backward errors undergo a similar smoothing process, creating many spurious stationary points. The core insight is that when forward signals are oversmoothed, the backpropagation equations become approximately linear, allowing characterization of the constant oversmoothed limit of backward errors. This phenomenon is specific to GNNs and does not occur in regular MLPs.

## Method Summary
The paper analyzes vanilla GNNs with the form X^(k+1) = ρ(P·X^(k)·W^(k)) on symmetric bi-stochastic propagation matrices P = I - (D-A)/d_max. The main theoretical analysis relies on bounded weights (s·λ < 1) and smooth activation functions. Experiments use CSBM synthetic graphs with λ ≈ 0.16 and compare shallow (L=5) vs deep (L=40) GNNs against MLPs. The key measurements are layer-wise gradient norms and training loss dynamics to demonstrate the stationary-point trapping behavior.

## Key Results
- Backward errors in deep GNNs undergo oversmoothing similar to forward signals, creating approximately linear smoothing equations
- Backward oversmoothing peaks at middle layers where both forward oversmoothing and sufficient backward propagation steps have occurred
- Every stationary point in the output layer becomes a global stationary point across all layers, trapping the network despite high loss
- This phenomenon is specific to GNNs and does not occur in MLPs with the same depth

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Backpropagated errors in deep GNNs undergo smoothing from output to input layers, similar to forward oversmoothing but with a key difference in how non-linearities propagate.
- Mechanism: The backward signal B(k) = ∂L/∂H(k) follows a recursion that multiplies by the propagation matrix P at each layer. When the forward signal X(k) is already oversmoothed (near constant across nodes), the activation derivative ρ'(H(k)) becomes approximately constant across nodes, making the backward smoothing equation effectively linear: B(k) ≈ c·P^T·B(k+1)·W(k+1)^T.
- Core assumption: Weights remain bounded such that s·λ < 1, where s is the spectral norm of weights and λ < 1 is the second-largest eigenvalue of P (Assumption 1).
- Evidence anchors:
  - [abstract] "backward errors are subjected to a linear oversmoothing even in the presence of non-linear activation function, such that the average of the output error plays a key role"
  - [section 4, sketch of proof] "When the forward signal is oversmoothed (near the output layer), the term ρ'(H(k)) is approximately constant, and the smoothing equation is approximately linear"
  - [corpus] Related work on gradient oversmoothing exists (Park & Kim, 2024) but only for linear GNNs; this paper extends to non-linear cases.
- Break condition: If weights expand sufficiently (s > 1/λ), forward oversmoothing may not occur, breaking the precondition for backward oversmoothing at middle layers.

### Mechanism 2
- Claim: Backward oversmoothing peaks at middle layers, where both forward signal oversmoothing AND sufficient backward propagation steps have occurred.
- Mechanism: The bound on pairwise differences E(B(k)) contains two competing terms: one decreasing exponentially in k (forward contribution), another in L-k (backward contribution). Middle layers maximize both effects.
- Core assumption: Assumption 4 requires ρ' to be D_ρ-Lipschitz (excludes vanilla ReLU but includes smooth variants like softplus).
- Evidence anchors:
  - [section 3.2] "backward oversmoothing happens at the middle layers (Fig. 2, right), when both k and L-k are high"
  - [theorem 2] E(B(k)) ≤ bound with term (λs)^(L-k) showing exponential decay from output
  - [corpus] Corpus papers discuss forward oversmoothing extensively but do not address backward dynamics; this represents a gap in existing literature.
- Break condition: Shallow networks (small L) may not have enough layers for the middle-layer interaction to dominate.

### Mechanism 3
- Claim: Once the output layer reaches a stationary point, the entire GNN becomes trapped at a near-global stationary point with potentially high loss—this is specific to GNNs and does not occur in MLPs.
- Mechanism: When the last layer is trained (gradient small), the average output error ‖1_n^T B(L)‖_2 becomes small. Through backward oversmoothing, this propagates to make gradients small at ALL layers. The oversmoothed forward signal F(k) ≈ 1_n·v^T means F(k)^T·B(k) depends on the average of B(k), which connects to the average of B(L).
- Core assumption: Theorem 3 requires either bounded output features (‖F(L)‖_F ≥ D_F·√n) OR probabilistic assumptions on label distribution (centered for regression, balanced classes for classification).
- Evidence anchors:
  - [theorem 3] "as soon as the last layer is trained, the whole GNN is in a near stationary-point"
  - [corollary 2] Proves existence of δ-stationary points with loss ≥ σ_y² - δ (near random performance)
  - [proposition 2] Explicit counterexample showing MLPs with ∂L/∂W(L)=0 can have ∂L/∂W(k)=1 at other layers
  - [corpus] "Residual connections provably mitigate oversmoothing" suggests skip connections as one mitigation, though this paper analyzes vanilla GNNs without such modifications.
- Break condition: Architectural modifications like skip connections, normalization (PairNorm, etc.), or non-vanilla message-passing schemes may alter this behavior—the paper explicitly notes these are outside scope.

## Foundational Learning

- Concept: Spectral properties of stochastic matrices (eigenvalue decomposition, role of λ_2 < 1)
  - Why needed here: Oversmoothing rate depends on λ·s < 1; understanding how P shrinks non-constant directions is central to all bounds.
  - Quick check question: Given a symmetric bi-stochastic matrix P with second eigenvalue λ=0.9 and weight spectral norm s=1.1, will forward oversmoothing occur?

- Concept: Backpropagation through message-passing layers (computing ∂L/∂W(k) = F(k)^T·B(k))
  - Why needed here: The gradient is a product of forward and backward signals; understanding this coupling is essential to see why both must be non-smooth for learning.
  - Quick check question: Why does the Hadamard product with ρ'(H(k)) in the backward pass become approximately constant when H(k) is oversmoothed?

- Concept: Stationary points vs. vanishing gradients (δ-stationary points, local minima, saddle points)
  - Why needed here: The paper distinguishes "vanishing gradients" (different magnitudes across layers) from "stationary points" (near-zero gradients everywhere with high loss).
  - Quick check question: If gradients are near-zero at all layers but loss is high, is this necessarily a local minimum?

## Architecture Onboarding

- Component map:
  - Input: Node features X^(0) ∈ R^(n×d_0), propagation matrix P ∈ R^(n×n), labels Y
  - Forward pass: X^(k+1) = ρ(P·X^(k)·W^(k)) for k=0,...,L-1; output = H^(L)
  - Backward pass: B(k) = ρ'(H(k)) ⊙ (P^T·B(k+1)·W(k+1)^T) with B(L) = ∂L/∂H(L)
  - Gradients: ∂L/∂W(k) = F(k)^T·B(k) where F(k) = P·X^(k)

- Critical path: The interaction happens at gradients = F(k)^T·B(k). Both signals must remain non-constant across nodes for gradients to carry meaningful information about per-node differences.

- Design tradeoffs:
  - Depth L vs. oversmoothing: More layers enable longer-range communication but increase both forward and backward oversmoothing
  - Weight initialization scale: Larger spectral norm s can counteract oversmoothing (s > 1/λ) but risks feature/gradients explosion
  - Non-linearity choice: Smooth activations (softplus) satisfy Assumption 4; vanilla ReLU does not

- Failure signatures:
  - Gradients rapidly decrease to near-zero at all layers within first few epochs (Fig. 1, left)
  - Loss remains high despite near-zero gradients
  - Deeper GNNs (L=40) perform worse than shallow (L=5) on same task
  - MLP baselines of same depth do NOT show this pattern (Fig. 1, center)

- First 3 experiments:
  1. Reproduce Figure 1: Train shallow (L=5) vs. deep (L=40) GNN and MLP on CSBM node classification; plot gradient norms per layer and loss curves to confirm the stationary-point trapping in GNNs only.
  2. Measure E(B(k)) across layers: At initialization, compute the pairwise difference metric (Eq. 6) for both forward signal X(k) and backward signal B(k) to verify the middle-layer peak predicted by Theorem 2.
  3. Vary spectral norm s: Initialize weights with different spectral norms (s=0.5, 1.0, 1.5 for fixed λ≈0.84) and measure whether larger s delays or prevents the gradient collapse, testing the s·λ < 1 condition.

## Open Questions the Paper Calls Out
None

## Limitations
- Bounded weights assumption: The theory relies on weights remaining bounded (s·λ < 1), but optimization algorithms may cause weight expansion
- Activation function specifics: Exact activation function used in experiments is not clearly stated, though smooth variants are implied
- Generalizability beyond symmetric bi-stochastic P: Analysis focuses on specific propagation matrix, extension to other graph structures unexplored

## Confidence
- High confidence: The characterization of backward oversmoothing as a linear process when forward signals are oversmoothed and the existence of spurious stationary points
- Medium confidence: The peak of backward oversmoothing at middle layers follows logically but requires empirical validation across diverse graph topologies
- Medium confidence: The claim that this phenomenon is specific to GNNs is supported by Proposition 2 but would benefit from testing against other architectures

## Next Checks
1. **Spectral norm monitoring**: During training, track the spectral norm of weights to empirically verify they remain bounded (s < 1/λ) throughout optimization
2. **Cross-architecture comparison**: Test the backward oversmoothing phenomenon on Graph Attention Networks (GATs) and Graph Transformers to determine whether it extends beyond vanilla GNNs
3. **Transfer to real-world graphs**: Validate the theory on benchmark datasets (Cora, Citeseer, Pubmed) with various graph structures to assess generalizability beyond synthetic CSBM data