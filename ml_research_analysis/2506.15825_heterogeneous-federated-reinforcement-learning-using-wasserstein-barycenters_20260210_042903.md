---
ver: rpa2
title: Heterogeneous Federated Reinforcement Learning Using Wasserstein Barycenters
arxiv_id: '2506.15825'
source_url: https://arxiv.org/abs/2506.15825
tags:
- learning
- global
- training
- agents
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FedWB, a federated learning algorithm that
  uses Wasserstein barycenters for model fusion in distributed deep learning and reinforcement
  learning. The method addresses the challenge of training a global model across heterogeneous
  environments without centralizing data.
---

# Heterogeneous Federated Reinforcement Learning Using Wasserstein Barycenters

## Quick Facts
- arXiv ID: 2506.15825
- Source URL: https://arxiv.org/abs/2506.15825
- Reference count: 29
- Primary result: FedWB achieves faster early-epoch convergence than FedAvg in both distributed MNIST classification and heterogeneous federated RL on CartPole

## Executive Summary
This paper introduces FedWB, a federated learning algorithm that uses Wasserstein barycenters for model fusion in distributed deep learning and reinforcement learning. The method addresses the challenge of training a global model across heterogeneous environments without centralizing data. Instead of averaging weights (as in FedAvg), FedWB uses Wasserstein barycenters to preserve the geometric structure of weight distributions during aggregation. The approach is demonstrated on MNIST classification and extended to heterogeneous federated reinforcement learning using the CartPole environment.

## Method Summary
FedWB aggregates local neural network weights using Wasserstein barycenters rather than arithmetic averaging. For MNIST classification, agents train a single hidden layer network (256 nodes) using SGD, with WB aggregation performed after each epoch. The algorithm flattens weight matrices, normalizes them as probability distributions, computes the WB, then rescales back to original magnitudes. For heterogeneous federated RL, agents train DQNs in environments with varying pole lengths, with periodic WB aggregation of Q-network weights. The method produces a global model that performs well across all heterogeneous environments, showing superior early performance compared to FedAvg.

## Key Results
- FedWB achieves 95% test accuracy within 25 epochs on MNIST, with faster early convergence than FedAvg
- Performance improves as the number of agents increases, though requiring more epochs for convergence
- In CartPole experiments, FedWB produces a global DQN that performs well across all heterogeneous environments
- FedWB shows superior early performance compared to FedAvg in both classification and RL settings, with both methods eventually converging to similar control capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Wasserstein barycenters preserve geometric structure of weight distributions better than arithmetic averaging, yielding faster early-epoch convergence
- Mechanism: WB computes a geodesic-aware average that retains mass geometry rather than naively overlaying distributions
- Core assumption: Neural network weights, when normalized as probability distributions, contain geometric structure that correlates with functional performance
- Evidence anchors: Abstract shows FedWB achieves higher early-epoch accuracy than traditional FedAvg averaging; section 5.1 demonstrates FedWB has more rapid increase in test accuracy but flattens out quickly

### Mechanism 2
- Claim: Intermittent global aggregation of local Q-networks produces a policy that generalizes across heterogeneous environments
- Mechanism: Each agent trains a DQN in its local environment (varying pole lengths). Periodic aggregation via WB fuses weight parameters, creating a global model that averages across environmental variations
- Core assumption: Heterogeneity across environments can be captured in shared network parameters without requiring architectural changes per environment
- Evidence anchors: Abstract states "the end outcome is a global DQN that functions across all environments"; section 4.2 describes instantiating environments with varying pole lengths

### Mechanism 3
- Claim: The accuracy-speed tradeoff between FedWB and FedAvg suggests a hybrid approach optimizes both metrics
- Mechanism: FedWB provides better initial convergence through geometry-preserving aggregation; FedAvg provides faster per-epoch computation. Switching from FedWB to FedAvg mid-training could capture both benefits
- Core assumption: The trajectory established by early WB aggregation positions the model in a region where arithmetic averaging is sufficient
- Evidence anchors: Section 5.1 suggests hybrid formulation yields optimal solution where first few epochs use FedWB then switch to FedAvg; section 6 proposes using FedWB initially then switching to FedAvg

## Foundational Learning

- **Wasserstein Distance and Barycenters**: Why needed here - Core aggregation mechanism replaces arithmetic averaging. Understanding why WB preserves geometry vs. naive averaging is essential. Quick check question: Given two 1D Gaussian distributions at positions 0 and 10, what does arithmetic mean vs. Wasserstein barycenter produce?

- **Deep Q-Networks (DQN) with Experience Replay**: Why needed here - The HFRL extension applies FedWB to DQN training. Understanding target/online networks and Bellman updates is required. Quick check question: Why does DQN use separate target and online networks, and how does aggregation interact with this?

- **Federated Learning Synchronization**: Why needed here - The distributed architecture requires understanding communication overhead, aggregation frequency, and local vs. global model relationships. Quick check question: How does increasing the number of agents affect epochs-to-convergence and wall-clock time differently?

## Architecture Onboarding

- **Component map**: Local agents -> Central server -> Aggregation module -> Global model -> Broadcast to local agents
- **Critical path**: Initialize identical models across all agents → Run local training epochs → Collect weights to central server → Execute Model Fusion (flatten → normalize → WB → rescale) → Broadcast global model → Repeat until convergence criteria met
- **Design tradeoffs**: More agents → faster parallel wall-clock time but more epochs required; WB vs. arithmetic averaging → faster convergence vs. faster per-aggregation computation; Aggregation frequency → not extensively tested
- **Failure signatures**: Communication bottleneck when agent count exceeds parallel capacity; WB computation dominates wall-clock time for large networks; Local model divergence if aggregation frequency too low; Unexpected epoch spike (4→5 agents showed worse performance than 10 agents)
- **First 3 experiments**: 1) Single-agent baseline: Train NN on full MNIST dataset, measure epochs to 90% accuracy; 2) FedWB vs. FedAvg comparison (5 agents): Run both algorithms with identical data partitions, plot accuracy per epoch; 3) CartPole heterogeneity test: Create 3 environments with different pole lengths, train global DQN with FedWB aggregation every C steps, measure duration per episode

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal switching point from FedWB to FedAvg in a hybrid aggregation scheme to maximize both convergence speed and final accuracy?
- Basis in paper: [explicit] The authors state: "It is possible a hybrid formulation yields the optimal solution, where for the first few epochs FedWB yields a better global model which is then swapped out for the speed of FedAvg, to reach potentially higher accuracy in a faster time frame."
- Why unresolved: The paper only compares FedWB and FedAvg independently; no hybrid experiments were conducted to identify when the tradeoff between Wasserstein barycenter computation cost and accuracy gain becomes unfavorable
- What evidence would resolve it: Empirical results from experiments systematically varying the switching epoch across multiple datasets and network architectures, measuring both wall-clock time and final accuracy

### Open Question 2
- Question: Why does the required number of epochs increase from 4 to 5 agents, then rapidly decrease when scaling to 10 agents in the distributed MNIST setting?
- Basis in paper: [explicit] The authors note: "The interesting, not easily explained, behavior is the increase in the number of epochs from 4 to 5 agents, followed by the rapid decrease when we jump to 10 agents. This phenomenon was neither expected, nor will it be studied or explored in this paper."
- Why unresolved: The authors provide only a conjecture about data distribution patterns without empirical validation
- What evidence would resolve it: Controlled experiments analyzing the composition of data partitions across agent counts, coupled with convergence dynamics analysis at the gradient trajectory level

### Open Question 3
- Question: How does the 1D flattening of weight matrices before computing Wasserstein barycenters affect performance on deeper or more complex neural network architectures?
- Basis in paper: [inferred] The methodology flattens each weight matrix into a 1D vector, treating it as a probability distribution. This approach was tested only on a single-hidden-layer network for MNIST and a simple DQN for CartPole
- Why unresolved: Flattening discards spatial and structural relationships within weight matrices that may be important for deeper architectures, yet no experiments validate the approach beyond shallow networks
- What evidence would resolve it: Experiments applying FedWB to CNNs on image datasets and to deeper feedforward networks, comparing against layer-wise or structure-preserving optimal transport methods

## Limitations

- Computational efficiency remains a concern, with WB aggregation being more expensive than FedAvg averaging, particularly for large networks
- The optimal switching point from FedWB to FedAvg in hybrid approaches is proposed but not empirically validated
- The limits of environmental heterogeneity that can be accommodated by a single global policy in the HFRL extension are not explored

## Confidence

- **High confidence**: FedWB achieves faster early-epoch convergence than FedAvg on MNIST classification
- **Medium confidence**: WB aggregation preserves geometric structure better than arithmetic averaging
- **Medium confidence**: HFRL extension produces globally functional policies across heterogeneous environments

## Next Checks

1. **Scaling experiment**: Test FedWB with deeper networks (2+ hidden layers) and larger agent counts to quantify computational overhead and identify the breakeven point where FedAvg becomes preferable

2. **Hybrid approach validation**: Implement and compare the proposed FedWB→FedAvg switching strategy against pure FedWB and pure FedAvg baselines across multiple runs

3. **Heterogeneity stress test**: Systematically vary the degree of environmental heterogeneity in CartPole (extreme pole lengths, different dynamics) to identify when global model performance degrades beyond acceptable thresholds