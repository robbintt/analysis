---
ver: rpa2
title: Interpreting the Repeated Token Phenomenon in Large Language Models
arxiv_id: '2503.08908'
source_url: https://arxiv.org/abs/2503.08908
tags:
- poem
- company
- token
- attention
- norm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explains why large language models (LLMs) fail to repeat
  tokens when prompted to do so, a vulnerability that allows models to diverge from
  intended behavior. The authors link this "repeated token divergence" phenomenon
  to "attention sinks," an emergent behavior where the initial token receives disproportionately
  high attention, crucial for fluency.
---

# Interpreting the Repeated Token Phenomenon in Large Language Models

## Quick Facts
- **arXiv ID:** 2503.08908
- **Source URL:** https://arxiv.org/abs/2503.08908
- **Reference count:** 40
- **Primary result:** Mechanistic explanation for why LLMs diverge when prompted to repeat tokens, linking this to attention sinks and proposing a targeted patch

## Executive Summary
This paper provides the first mechanistic explanation for a critical LLM vulnerability where models fail to repeat tokens when prompted, instead producing unrelated text or training data. The authors identify this "repeated token divergence" as an emergent behavior tied to attention sinks - a mechanism where the first token receives disproportionately high attention crucial for model fluency. Through systematic analysis, they trace this to a two-stage neural circuit and demonstrate how this same mechanism can be exploited through a novel "cluster attack" without exact repetition. A simple, targeted patch effectively mitigates the vulnerability while preserving model performance.

## Method Summary
The authors use mechanistic interpretability to analyze attention patterns in transformer models, focusing on how repeated tokens trigger attention sinks. They extract attention scores across layers, identify sink neurons through contribution analysis, and perform ablation studies by zeroing these neurons. The cluster attack is constructed by analyzing token clustering in the first attention layer's heads. The mitigation patch is implemented as a hook on the MLP up_proj layer that preserves the first token's hidden state during prefill and broadcasts it to all positions.

## Key Results
- Identified a two-stage neural circuit responsible for attention sinks: first attention layer marks first token, then sink neurons amplify its hidden state
- Demonstrated repeated token sequences are misclassified as "first tokens" due to RoPE convergence, triggering attention sinks
- Developed cluster attack that induces attention sinks without exact token repetition
- Created targeted patch that mitigates vulnerability without harming model performance

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Attention Sink Circuit
The attention sink emerges from a specific neural circuit: the first attention layer marks the first token by projecting tokens into linearly separable subspaces, and then a sparse set of sink neurons in an early MLP layer detect this mark and add high-magnitude values to the first token's hidden state, causing it to attract disproportionate attention in subsequent layers. This mechanism serves a functional role in maintaining model fluency by creating a stable attention target.

### Mechanism 2: Repeated Token Misclassification
Long sequences of repeated tokens are falsely classified as "first tokens" by the attention sink circuit due to convergence in Rotary Position Embeddings. With RoPE and causal masking, the first attention layer identifies single-token sequences rather than explicitly detecting position. As repetitions increase, the representation converges to that of a singleton sequence, making repeated tokens indistinguishable from a true first token and triggering sink neuron amplification.

### Mechanism 3: Cluster Attack via Token Similarity
Tokens that share attention head clustering properties can trigger attention sinks without exact repetition. The first attention layer uses different heads to cluster tokens into sets. Tokens within the same cluster activate the same "attend to others" head, which projects them toward the "first token subspace." Repeating tokens from a shared cluster induces high norms similar to attention sinks.

## Foundational Learning

- **Concept:** Attention Sinks
  - **Why needed here:** The entire vulnerability stems from how LLMs use attention sinks for fluency. Without understanding that the first token receives disproportionately high attention as a feature (not a bug), the repeated token phenomenon seems inexplicable.
  - **Quick check question:** If you ablated the sink neurons in layer 1, would the model's fluency on normal sentences increase, decrease, or stay the same?

- **Concept:** Rotary Position Embeddings (RoPE)
  - **Why needed here:** RoPE encodes position through rotation rather than absolute indices, which creates the mathematical conditions for repeated token convergence. The paper notes RoPE's isometric property bounds the positional signal.
  - **Quick check question:** Why does RoPE make it harder for the model to distinguish position 1 from position 500 when all tokens are identical?

- **Concept:** Causal Masking in Transformers
  - **Why needed here:** The first attention layer uses causal masking to force the first token to self-attend, which is how it "marks" the beginning. This design choice directly enables the vulnerability.
  - **Quick check question:** In a causally masked attention layer, which tokens can position 5 attend to?

## Architecture Onboarding

- **Component map:** Input tokens → Attention Layer 0 (first-token detection via causal masking + near-orthogonal Q/K) → MLP Layer 1 (sink neurons amplify marked tokens) → Subsequent layers (attend to high-norm tokens) → Output

- **Critical path:** The vulnerability requires: (1) sufficient repetitions to trigger convergence (~100-4000 depending on model), (2) a token that activates the relevant sink neuron, (3) the first attention layer's "attend to others" behavior misclassifying repeated tokens.

- **Design tradeoffs:** Attention sinks improve fluency but create security vulnerability. Simple repetition detection fails against cluster attacks. Neuron ablation reduces vulnerability but may affect other computations (though benchmarks show negligible impact in Table 2).

- **Failure signatures:** Hidden state norms at sink layer exceeding ~6.0 (BoS-typical) vs. normal ~2.4; high attention scores on non-first tokens in early layers; model outputting unrelated text or training data when prompted to repeat; convergence visible in L2 difference between singleton and repeated sequences (Figure 5).

- **First 3 experiments:**
  1. Reproduce the norm measurement: Run 500 repetitions of "the" through LLaMa-2-7B, measure hidden state norms at layer 1, compare to BoS norm (~6.93) and typical token norm (~2.41).
  2. Ablate sink neurons: Zero-ablate neurons 7890 and 10411 in layer 1, re-run repeated token prompt, verify norm reduction and behavior change.
  3. Test cluster attack: Extract Head 4's cluster tokens from Appendix B, construct 2-token repeated sequences (e.g., "Sch Com"), measure MLP1 norms and model output divergence.

## Open Questions the Paper Calls Out

- **Open Question 1:** What is the precise causal mechanism linking the activation of attention sinks (via repeated or clustered tokens) to the extraction of memorized training data? The authors state the mechanism by which training data leakage occurs through attention sinks remains unclear.

- **Open Question 2:** Why do only certain tokens induce attention sinks when repeated, while others (e.g., "Another", "bit", "dust" in LLaMa2) fail to do so? The paper notes that not all tokens in LLaMa2 induce sinks, suggesting other factors are at play.

- **Open Question 3:** Does the identified "sink circuit" (first attention layer marking + sink neuron amplification) generalize to closed-source models or architectures without Rotary Position Embeddings (RoPE)? The authors mention they intend to investigate broader generalizability in future work.

## Limitations

- The cluster attack mechanism is demonstrated but incompletely specified - the clustering methodology is not fully provided, making replication challenging.
- The mechanistic explanation relies heavily on correlations rather than causal interventions, with potential intermediate factors not fully characterized.
- The theoretical convergence proof assumes ideal conditions that may not hold in practice, with empirical results diverging from theoretical bounds.

## Confidence

**High Confidence:** The existence of attention sinks and their correlation with model fluency is well-established. The two-stage circuit mechanism has strong empirical support through ablation studies and norm measurements. The repeated token vulnerability itself is directly observable and reproducible.

**Medium Confidence:** The cluster attack mechanism is demonstrated but incompletely specified. The generalizability to other models and tokens requires further validation. The relationship between cluster attacks and practical exploitation scenarios needs more exploration.

**Low Confidence:** The theoretical convergence proof has limited practical alignment with empirical results. The behavioral consequences of attention sinks on normal model operation beyond the specific vulnerability are not fully characterized. The long-term stability of the mitigation patch across different contexts is unknown.

## Next Checks

1. **Cluster Attack Reproducibility:** Extract the clustering methodology from the paper's code or contact authors for clarification. Apply the method to a new model (e.g., Mistral) and attempt to construct cluster attacks using different token sets. Measure whether the induced norms match those shown for "Sch Com" and whether model divergence occurs.

2. **Behavioral Impact Characterization:** Design experiments to measure the impact of attention sinks on normal model behavior. Compare model performance on fluency-sensitive tasks (story continuation, dialogue) with and without sink neuron ablation. Characterize whether sink ablation degrades performance in subtle ways not captured by standard benchmarks.

3. **Convergence Dynamics Verification:** Systematically vary repetition count from 10 to 10000 tokens and measure the L2 norm convergence rate. Compare empirical results with the theoretical bound, identifying the conditions under which the proof's assumptions break down. Test whether the convergence behavior changes with different RoPE scales or positional embedding schemes.