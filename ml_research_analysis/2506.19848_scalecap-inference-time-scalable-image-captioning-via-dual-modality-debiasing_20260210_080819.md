---
ver: rpa2
title: 'ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality Debiasing'
arxiv_id: '2506.19848'
source_url: https://arxiv.org/abs/2506.19848
tags:
- image
- scalecap
- caption
- captions
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ScaleCap, an inference-time scalable image
  captioning framework that addresses two major biases in large vision-language models:
  multimodal bias leading to imbalanced descriptive granularity and linguistic bias
  causing hallucinations. The core method involves a two-stage approach: heuristic
  question answering generates content-specific questions based on initial captions
  to extract fine-grained visual details, while contrastive sentence rating uses offline
  contrastive decoding to filter out hallucinated content by comparing token probabilities
  with and without image input.'
---

# ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality Debiasing

## Quick Facts
- arXiv ID: 2506.19848
- Source URL: https://arxiv.org/abs/2506.19848
- Reference count: 40
- Primary result: 4.3% improvement on InfoVQA over ShareGPT4V-450k and 2.4% over DenseFusion-450k in Qwen2.5-7B setting

## Executive Summary
This paper introduces ScaleCap, an inference-time scalable image captioning framework that addresses two major biases in large vision-language models: multimodal bias leading to imbalanced descriptive granularity and linguistic bias causing hallucinations. The core method involves a two-stage approach: heuristic question answering generates content-specific questions based on initial captions to extract fine-grained visual details, while contrastive sentence rating uses offline contrastive decoding to filter out hallucinated content by comparing token probabilities with and without image input. ScaleCap is used to annotate 450K images, and models pretrained on this dataset achieve the best performance on 11 widely used benchmarks. For example, ScaleCap-450k improves InfoVQA scores by 4.3% over ShareGPT4V-450k and 2.4% over DenseFusion-450k in Qwen2.5-7B setting. Additional evaluations via downstream VQA tasks and image reconstruction confirm the superior richness and accuracy of captions generated by ScaleCap compared to both open-source models and GPT-4o.

## Method Summary
ScaleCap operates on top of standard LVLM stacks, using a text-only LLM to analyze initial captions and generate "Describe more details about X" instructions. The LVLM answers these questions, yielding fine-grained details it would otherwise omit. Contrastive sentence rating filters hallucinations by comparing token probabilities with and without image input, removing sentences with low probability differences. The process is iterative, with the number of questions N as a controllable budget. The framework uses Qwen2-VL-7B for visual answering and Qwen2-72B for instruction generation and summarization, producing detailed, balanced, and hallucination-free descriptions for LVLM pretraining.

## Key Results
- ScaleCap-450k improves InfoVQA scores by 4.3% over ShareGPT4V-450k and 2.4% over DenseFusion-450k in Qwen2.5-7B setting
- Captions achieve superior richness and accuracy verified through downstream VQA tasks and image reconstruction
- Performance plateaus after ~20 heuristic questions, aligning with diminishing returns from information saturation
- Contrastive sentence rating reduces CHAIR hallucination metrics from 48.8 to 33.6 for LLaVA-v1.5

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Guided questioning extracts latent visual knowledge that LVLMs fail to emit spontaneously.
- **Mechanism:** A text-only LLM analyzes the initial caption, identifies under-described objects via in-context prompting, and generates "Describe more details about X" instructions. The LVLM answers these, yielding fine-grained details it would otherwise omit. This exploits the observation that 7B and 72B LVLMs have similar perceptual capacity but differ in reasoning-driven output organization.
- **Core assumption:** The LVLM already perceives details but fails to report them without explicit prompting; the LLM can reliably identify missing elements from text alone.
- **Evidence anchors:**
  - [abstract]: "heuristic question answering... generates content-specific questions... to progressively inject relevant information"
  - [Figure 2]: "The reason for certain object detail omissions... is mainly due to the absence of guiding heuristic questions rather than insufficient perceptual capability"
  - [corpus]: Related work on inference-time feedback (arxiv:2501.04513) supports iterative refinement, but does not validate the specific heuristic QA approach.
- **Break condition:** If the LLM fails to generate relevant questions (e.g., for highly specialized domains) or if the LVLM genuinely lacks perceptual capacity for certain attributes, the enrichment loop stalls.

### Mechanism 2
- **Claim:** Sentence-level contrastive probability analysis filters hallucinations while preserving fluency.
- **Mechanism:** For each generated sentence, compute token probabilities conditioned on the image (P) and without the image (P'). The difference ∆P = P - P' quantifies visual grounding. Sentences with low max(∆p) are likely hallucinated and removed. This is done offline after generation, avoiding the coherence issues of token-level online decoding.
- **Core assumption:** Hallucinated content arises from linguistic priors and will show low ∆P; visually grounded content will show high ∆P. Sentence-level filtering preserves fluency better than token-level intervention.
- **Evidence anchors:**
  - [Section 2.2]: "A high ∆pk indicates that token ck benefits significantly from the visual context... In contrast, a low ∆pk suggests that the token is generated primarily based on language priors"
  - [Table 7]: Golden Sentence Selection reduces CHAIR metrics (e.g., 48.8 → 33.6 for LLaVA-v1.5)
  - [corpus]: No direct corpus validation for this specific offline contrastive sentence rating; related work (OPERA, VCD) uses online decoding.
- **Break condition:** If hallucinated content coincidentally has high ∆P (false positive grounding) or if valid abstract descriptions have low ∆P (false negative), filtering becomes unreliable.

### Mechanism 3
- **Claim:** Progressive enrichment scales caption quality with inference budget via iterative question-answer cycles.
- **Mechanism:** The number of heuristic questions N is a controllable budget. More questions → more object/position details → richer captions. Performance plateaus after ~20 questions (Figure 7), aligning with diminishing returns from information saturation.
- **Core assumption:** Visual information in typical images can be exhausted by finite questions; the relationship between N and quality is monotonic but saturating.
- **Evidence anchors:**
  - [Figure 7a]: "initially increasing the number of heuristic questions leads to a sharp rise... when the number exceeds 20, the performance curve begins to plateau"
  - [Section 2]: "ScaleCap's scalable refinement strategy flexibly balances caption quality and computational cost"
  - [corpus]: Inference-time scaling concept supported by related work on test-time compute, but not specifically for captioning.
- **Break condition:** If images contain unusually high object density or if questions become redundant rather than complementary, additional budget yields negligible gains.

## Foundational Learning

- **Concept: Vision-Language Model Architecture (Vision Encoder → Projector → LLM)**
  - **Why needed here:** ScaleCap operates on top of this standard LVLM stack, assuming separate visual perception (encoder) and reasoning/generation (LLM).
  - **Quick check question:** Can you explain why the projector is needed between the vision encoder and the LLM?

- **Concept: Contrastive Decoding**
  - **Why needed here:** The paper adapts this idea (comparing output distributions under different conditions) to detect hallucinations via probability differences with/without image input.
  - **Quick check question:** How does contrastive decoding differ from standard decoding, and why might it reduce bias?

- **Concept: Inference-Time Scaling**
  - **Why needed here:** ScaleCap explicitly trades off quality vs. cost by controlling the number of refinement iterations, a departure from fixed-compute inference.
  - **Quick check question:** What factors determine the optimal inference budget for a given image complexity?

## Architecture Onboarding

- **Component map:** Image I → LVLM (Qwen2-VL-7B) generates initial caption C → Contrastive Sentence Rating filters C → Golden Sentences SG → LLM (Qwen2-72B) generates object/position instructions from SG → LVLM answers instructions → Detail sets Do, Dp → LLM integrates SG + Do + Dp → Final caption Fc
- **Critical path:** 1. Golden Sentence extraction (hallucination filtering gates all downstream quality) 2. Instruction generation relevance (LLM must identify meaningful gaps) 3. Final integration (LLM must synthesize ~20k token context without loss)
- **Design tradeoffs:**
  - LVLM size: 7B sufficient for perception; larger models add world knowledge but marginal perceptual gain (Table 6)
  - LLM size: 72B needed for integration; 7B fails with long context (Table 5)
  - Budget N: Higher N improves quality to saturation point; practical ceiling ~20 questions
  - Sentence vs. token filtering: Sentence-level preserves fluency but may miss subtle token-level hallucinations
- **Failure signatures:**
  - Persistent hallucinations → Contrastive threshold τ too low or probability analysis fails
  - Missing object details → LLM instruction generation irrelevant or LVLM perception limited
  - Incoherent final caption → LLM summarization capacity exceeded (use larger model or chunk integration)
  - Stagnant quality with increased N → Question redundancy; refine instruction generation prompt
- **First 3 experiments:**
  1. **Ablate Contrastive Sentence Rating:** Run ScaleCap without hallucination filtering; measure CHAIR scores to validate the component's contribution (expect significant degradation).
  2. **Vary Scale Budget N:** Test N ∈ {2, 6, 10, 15, 20, all} on a held-out set; plot quality vs. cost to identify practical operating point before saturation.
  3. **Swap LVLM/LLM scales:** Run with Qwen2-VL-72B + Qwen2-7B and Qwen2-VL-2B + Qwen2-72B to confirm the finding that LVLM scale saturates at 7B while LLM scale is critical for integration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can external knowledge or content-level supervision be integrated to detect harmful content that the current probability-based contrastive decoding misses?
- Basis in paper: [explicit] Section 6 states the method lacks semantic supervision and cannot suppress biased or harmful content arising from the model.
- Why unresolved: The current mechanism filters sentences based on visual grounding (probability differences), not semantic safety or toxicity.
- What evidence would resolve it: A safety layer or external knowledge integration that reduces toxic outputs without degrading the detailed richness of ScaleCap captions.

### Open Question 2
- Question: Can the caption integration phase be optimized for smaller (e.g., 7B) LLMs to avoid the dependency on 72B models for summarization?
- Basis in paper: [inferred] Section 4.3 and Table 5 show 7B LLMs struggle with integration due to long context lengths (up to 20k tokens), necessitating the 72B model.
- Why unresolved: The high token count of fragmented visual details overwhelms the context window or reasoning capacity of smaller models.
- What evidence would resolve it: A hierarchical or truncated summarization method allowing a 7B model to match the integration quality of the 72B baseline.

### Open Question 3
- Question: Does the Heuristic Question Answering module inadvertently introduce "world knowledge" hallucinations not present in the image?
- Basis in paper: [inferred] Figure 7b shows larger models introduce more world knowledge; LLM-generated questions might prompt the LVLM to generate ungrounded common-sense details.
- Why unresolved: The paper focuses on filtering object hallucinations but does not deeply analyze if the *answers* to heuristic questions include hallucinated external knowledge.
- What evidence would resolve it: An ablation study measuring the factuality of "world knowledge" details in the final caption against strict ground truth.

## Limitations
- Contrastive threshold τ is not specified, limiting reproducibility of hallucination filtering
- Complexity scoring tool [17] (IC9600) for LAION filtering lacks detailed implementation description
- Claims of being "best" on 11 benchmarks rely on downstream performance on models trained on ScaleCap-450k rather than direct captioning quality comparisons

## Confidence

- **High confidence:** The LVLM perceptual ceiling at 7B scale (Table 6) is well-supported by direct comparison and aligns with existing literature on LVLM scaling laws.
- **Medium confidence:** The hallucination reduction via contrastive sentence rating is demonstrated via CHAIR but lacks ablation and cross-domain validation.
- **Medium confidence:** The inference-time scalability mechanism is theoretically sound and supported by Figure 7, but the practical value depends on the quality of the heuristic QA loop, which is not independently verified.
- **Low confidence:** The claim of being the "best" on 11 benchmarks is based on downstream performance on models trained on ScaleCap-450k, not direct captioning quality comparisons, and lacks independent replication.

## Next Checks

1. **Ablate Heuristic QA:** Generate captions with and without the iterative question-answer refinement loop on a held-out set; measure the marginal gain in detail and downstream benchmark performance to isolate the contribution of this component.

2. **Vary Contrastive Threshold:** Systematically sweep τ in the sentence rating module and report CHAIR, Prism, and downstream VQA scores to identify the optimal operating point and assess robustness.

3. **Cross-Dataset Generalization:** Apply ScaleCap to a different base dataset (e.g., CC3M or YFCC100M) and evaluate whether the scaling behavior and hallucination reduction transfer, or if performance is tightly coupled to the ShareGPT4V-450k distribution.