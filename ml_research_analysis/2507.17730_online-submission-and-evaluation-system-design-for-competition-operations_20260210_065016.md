---
ver: rpa2
title: Online Submission and Evaluation System Design for Competition Operations
arxiv_id: '2507.17730'
source_url: https://arxiv.org/abs/2507.17730
tags:
- competition
- evaluation
- system
- participants
- submission
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an online competition system designed to automate
  submission and evaluation processes for algorithmic competitions. The system addresses
  the operational burden of managing large volumes of submissions and compatibility
  issues arising from participants developing solutions in diverse environments.
---

# Online Submission and Evaluation System Design for Competition Operations

## Quick Facts
- arXiv ID: 2507.17730
- Source URL: https://arxiv.org/abs/2507.17730
- Authors: Zhe Chen; Daniel Harabor; Ryan Hechnenberger; Nathan R. Sturtevant
- Reference count: 6
- One-line primary result: Online competition system automates submission and evaluation processes using Docker containers, git repositories, and a decoupled architecture.

## Executive Summary
This paper presents an online competition system designed to automate the submission and evaluation processes for algorithmic competitions. The system addresses the operational burden of managing large volumes of submissions and compatibility issues arising from participants developing solutions in diverse environments. By utilizing Docker containers for isolated execution, git repositories for code management, and a decoupled architecture, the system enables instant feedback, efficient resource management, and fair evaluation across different competition types.

## Method Summary
The system implements a five-component architecture consisting of a git host for code storage, MongoDB database for tracking submissions and user data, a Node.js/ReactJS web application for participant interaction, an evaluation server for monitoring and dispatching jobs, and computing units that execute submissions in Docker containers. The evaluation process involves two stages—precomputation followed by benchmarking—to ensure precise runtime measurements. Resource limits are enforced through Docker configurations, while git commit hashes provide version tracking for submissions. The system has been successfully deployed for three competitions with varying evaluation requirements, from simple correctness checks to complex multi-metric leaderboards.

## Key Results
- Successfully deployed for three competitions: AI planning teaching unit, Grid-Based Path Planning Competition (GPPC2), and League of Robot Runners
- Handles peak loads of 35 submissions per day with average of 9 submissions daily
- Provides instant feedback and fair evaluation through containerization and resource limiting
- Supports multiple scoring functions and evaluation metrics across different competition tracks

## Why This Works (Mechanism)

### Mechanism 1: Containerized Isolation for Reproducible Evaluation
- Claim: Running participant submissions in Docker containers with controlled configurations appears to reduce environment-dependent failures and enable fair comparison across diverse development setups.
- Mechanism: (1) Build base Docker image with competition-specified dependencies; (2) Install participant-declared packages via APT on job start; (3) Apply resource limits (CPU, memory, disk I/O, network); (4) Execute evaluation script; (5) Provide participants a local build script to replicate the exact container environment.
- Core assumption: Assumption: Participant code does not require system-level access, proprietary drivers, or specialized hardware beyond what Docker supports; resource limits are sufficient to prevent malicious behavior.
- Evidence anchors:
  - [abstract]: "utilising isolated environments to evaluate submissions"
  - [section]: "computing units run the evaluation jobs in a Docker container... configured to limit the resources available to the evaluation jobs, such as CPU, memory, disk space, and internet access"
  - [corpus]: Weak direct evidence—corpus papers address competition benchmarks (AlgoPerf, ML-SUPERB) but not containerization strategies.
- Break condition: If submissions require GPU/TPU access, kernel modules, or container escape mitigation beyond standard Docker.

### Mechanism 2: Git-Mediated Submission Tracking
- Claim: Using git repositories for code submission enables version-anchored retrieval and post-competition archival, though this depends on participant familiarity with git workflows.
- Mechanism: (1) Create repository per participant or per track; (2) Participant pushes code; (3) Evaluation server fetches and records commit hash to database; (4) Historical versions retrievable via commit hash for auditing.
- Core assumption: Assumption: Participants can use git effectively; repositories remain accessible throughout competition; submission sizes stay within practical limits.
- Evidence anchors:
  - [section]: "Managing submitted code using the git repositories allows efficient management of submitted codes and tracking and retrieving the history of the submissions"
  - [section]: "retrieving submissions is easy as each submission records the commit hash of the evaluated code"
  - [corpus]: No corpus papers discuss git-based submission systems.
- Break condition: If participants submit large binaries/datasets, or if git host availability becomes a bottleneck.

### Mechanism 3: Decoupled Polling-Based Evaluation Pipeline
- Claim: Separating submission intake, job dispatch, and execution into loosely coupled components appears to support scalable evaluation, as demonstrated by handling peak loads of 35 submissions/day.
- Mechanism: (1) Web app writes submission entry to MongoDB; (2) Evaluation server polls database for new submissions; (3) Server fetches code, creates job entry, dispatches to computing unit (optionally via Slurm); (4) Computing unit executes in Docker, writes results back to database; (5) Web app reads results for leaderboard.
- Core assumption: Assumption: Database handles concurrent read/write load acceptably; network between components is reliable; polling interval provides sufficient responsiveness.
- Evidence anchors:
  - [abstract]: "automates the submission and evaluation process"
  - [section]: "The evaluation server is responsible for monitoring new submissions and initiating the evaluation jobs when new submissions are found"
  - [section]: "The competition system receives on average 9 submissions per day with a peak of 35 submissions per day during the competition period"
  - [corpus]: Indirect—AlgoPerf and ML-SUPERB describe evaluation challenges but not architectural details.
- Break condition: If real-time evaluation feedback is required, or if job state must be shared across submissions.

## Foundational Learning

- Concept: **Docker containerization and resource limiting**
  - Why needed here: Core isolation mechanism; must understand images, containers, and how to apply CPU/memory limits.
  - Quick check question: How would you run a container with a 4GB memory limit and no network access?

- Concept: **Git workflow (clone, push, fetch, commit hashes)**
  - Why needed here: Submission relies on participants pushing code; evaluation server fetches by commit hash.
  - Quick check question: What command retrieves the current commit hash of a repository?

- Concept: **MongoDB document collections and basic queries**
  - Why needed here: System uses MongoDB for users, submissions, and competitions; understanding document structure is essential for debugging.
  - Quick check question: How would you query the `submissions` collection for all entries with status "pending"?

## Architecture Onboarding

- Component map:
  - Git Host (GitHub/Bitbucket) -> Web App Backend -> Database -> Evaluation Server -> Computing Units (Docker)

- Critical path:
  1. User registers → `users` collection
  2. User joins competition → `subaccounts` collection with git repo URL
  3. User pushes code → git repository updated
  4. User clicks "start evaluation" → `submissions` entry created
  5. Evaluation server polls, detects submission, fetches code, records commit hash
  6. Job dispatched to computing unit
  7. Computing unit builds Docker container, runs evaluation script
  8. Results written to database
  9. Web app displays on leaderboard

- Design tradeoffs:
  - Polling vs. event-driven: Simpler but adds latency
  - MongoDB vs. relational: Flexible for varied outputs, weaker consistency guarantees
  - Docker vs. VMs: Lightweight but weaker isolation
  - Full debug logs vs. hidden: Transparency vs. preventing gaming

- Failure signatures:
  - Submission stuck "pending" → evaluation server down or database unreachable
  - Evaluation timeout → code infinite loop or resource limits too aggressive
  - Git fetch failure → permissions or network issue
  - Leaderboard stale → database write failure from computing unit

- First 3 experiments:
  1. End-to-end smoke test: Register user, push minimal code, trigger evaluation, verify result on leaderboard
  2. Resource limit test: Submit memory-exhausting code, confirm Docker kills job within limits
  3. Concurrent submission test: Trigger multiple submissions simultaneously, verify correct queue handling and result attribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the current system architecture be modularized to support diverse competition workflows without requiring distinct codebases for each event?
- Basis in paper: [explicit] The authors state, "one target of future work is to better modularise and generalise the implementation of the system so that the common components can be maintained in one place."
- Why unresolved: Currently, the three described applications (GPPC2, League of Robot Runners, teaching unit) are maintained separately, creating high maintenance workloads and making feature addition difficult.
- What evidence would resolve it: The release of a single, open-source codebase where configuration files—rather than code rewrites—define competition-specific workflows like multi-stage evaluation.

### Open Question 2
- Question: Can the system automate the detection of malicious submissions to reduce the reliance on manual code auditing?
- Basis in paper: [inferred] The paper notes that "the cost of prohibiting cheating is high" and that the organizers currently "rely on an auditing process to detect cheating activities" by manually reviewing top participants.
- Why unresolved: While Docker isolates environments, the system lacks automated mechanisms to detect rule violations, forcing organizers to trade off security against the operational burden of manual checks.
- What evidence would resolve it: The integration of an automated static analysis or runtime monitoring tool within the evaluation pipeline that successfully flags malicious code without human intervention.

### Open Question 3
- Question: What is the optimal level of log transparency that allows participants to debug effectively without compromising the integrity of hidden evaluation instances?
- Basis in paper: [inferred] The authors discuss the challenge of "Debugging," noting the need to "trade-off transparency with the feasibility of debugging," which they handled differently across competitions (e.g., showing debug instance logs vs. hiding all evaluation output).
- Why unresolved: The paper describes ad-hoc solutions for different competitions but does not propose a generalized system feature for managing information visibility during debugging.
- What evidence would resolve it: A user study or systematic comparison of participant success rates in debugging when provided with varying levels of log access in a standardized system.

## Limitations
- Docker isolation may be insufficient for submissions requiring GPU/TPU access or kernel-level operations
- Git-based submission workflows assume participant technical proficiency and may struggle with large datasets
- Polling-based architecture introduces latency that could be problematic for real-time feedback requirements

## Confidence
- **High**: Container-based isolation mechanisms, git-based submission tracking, and decoupled polling architecture are well-established patterns with clear implementation evidence
- **Medium**: Scalability claims are based on moderate load (35 submissions/day peak) from three specific competitions, which may not generalize to large-scale machine learning or data-intensive competitions
- **Low**: Long-term maintenance, security against sophisticated attacks, and handling of diverse submission types beyond algorithmic code remain largely unaddressed

## Next Checks
1. Stress test the system with 100+ concurrent submissions to evaluate database contention and polling responsiveness under load
2. Attempt container escape or resource exhaustion attacks to validate Docker security boundaries and monitoring capabilities
3. Deploy with a competition requiring GPU resources to identify gaps in the current containerization approach