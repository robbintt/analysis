---
ver: rpa2
title: Do Large Language Models Think Like the Brain? Sentence-Level Evidences from
  Layer-Wise Embeddings and fMRI
arxiv_id: '2505.22563'
source_url: https://arxiv.org/abs/2505.22563
tags:
- language
- brain
- llms
- neural
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) align
  with human brain activity during sentence comprehension. Using fMRI data from participants
  listening to a narrative story, we constructed neural prediction models to compare
  sentence-level representations from 14 publicly available LLMs with brain region
  activations.
---

# Do Large Language Models Think Like the Brain? Sentence-Level Evidences from Layer-Wise Embeddings and fMRI

## Quick Facts
- arXiv ID: 2505.22563
- Source URL: https://arxiv.org/abs/2505.22563
- Authors: Yu Lei; Xingyang Ge; Yi Zhang; Yiming Yang; Bolei Ma
- Reference count: 24
- One-line primary result: This study investigates whether large language models (LLMs) align with human brain activity during sentence comprehension. Using fMRI data from participants listening to a narrative story, we constructed neural prediction models to compare sentence-level representations from 14 publicly available LLMs with brain region activations. Our results show that LLMs achieve stronger functional and anatomical correspondence with brain activity at higher semantic abstraction levels, particularly in intermediate layers. Instruction-tuned models consistently outperformed base versions in both comprehension and neural alignment. We also observed hemispheric asymmetry, with left-dominant regions aligning with core language processing and right-hemisphere regions reflecting higher-level cognitive functions. These findings demonstrate that improvements in model performance drive representational architectures toward brain-like hierarchies, advancing our understanding of computational parallels between LLMs and human language processing.

## Executive Summary
This paper investigates whether LLMs process language in ways that resemble human brain activity by comparing sentence-level embeddings from 14 transformer models with fMRI data from participants listening to "The Little Prince." Using neural prediction models, the authors found that intermediate layers of LLMs show the strongest alignment with brain activity, suggesting a shared hierarchical processing structure. Instruction-tuned models consistently outperformed base models in both comprehension and neural alignment. The study also revealed hemispheric asymmetry, with left-dominant regions corresponding to core language processing and right-hemisphere regions reflecting higher-level cognitive functions. These findings suggest that improvements in model performance drive representational architectures toward brain-like hierarchies.

## Method Summary
The study uses fMRI data from 34 participants listening to a Chinese audiobook of "The Little Prince" (1,577 sentences) recorded with a 3T GE scanner (TR=2000ms, ME-EPI, 2mm³ MNI space). Sentence-level neural responses were extracted using a Least-Squares Separate GLM with HRF-convolved onsets. Layer-wise embeddings were extracted from 14 LLMs (Llama-3.1, Gemma-2, Qwen2.5, GLM-4, Mistral-7B, Baichuan2-7B-Chat, DeepSeek-R1-Distill-Qwen-7B, OPT-6.7B, BERT-base). Voxel-wise ridge regression models mapped embeddings to neural responses per layer/ROI/subject using nested cross-validation for regularization selection. Alignment was quantified using Pearson correlation between predicted and actual voxel activity across 12 language ROIs (IFG, IFGorb, MFG, AntTemp, PostTemp, AngG × left/right hemispheres).

## Key Results
- Intermediate layers of LLMs show stronger functional and anatomical correspondence with brain activity than final layers
- Instruction-tuned models consistently outperformed base versions in both comprehension and neural alignment
- Left-dominant regions aligned with core language processing while right-hemisphere regions reflected higher-level cognitive functions

## Why This Works (Mechanism)

### Mechanism 1: Intermediate-Layer Hierarchical Correspondence
- **Claim:** Sentence-level representations in intermediate layers of LLMs exhibit higher correlation with fMRI recorded brain activity than final layers.
- **Mechanism:** As data propagates through transformer layers, early layers encode lexical/syntactic features while final layers abstract toward next-token prediction objectives. Intermediate layers ostensibly capture "rich linguistic information" that mirrors the hierarchical processing stages in the human cortex (specifically within the language network).
- **Core assumption:** The paper assumes that the peak correlation at intermediate layers reflects a shared computational hierarchy between transformers and biological language processing, rather than a statistical artifact of the embedding geometry.
- **Evidence anchors:**
  - [abstract]: "LLMs achieve stronger functional and anatomical correspondence with brain activity... particularly in intermediate layers."
  - [section]: Page 6, "Intermediate layers better align with brain activity... later layers, often optimized for downstream tasks, may abstract away from biologically grounded representations."
  - [corpus]: *From Language to Cognition* (arXiv:2503.01830) suggests LLMs may "outgrow" human language networks as they scale, supporting the divergence of final layers.
- **Break condition:** If the correlation peak shifts to final layers in future models with different training objectives (e.g., reasoning-focused rather than next-token prediction), the "shared hierarchy" hypothesis is weakened.

### Mechanism 2: Semantic Comprehension Drives Alignment
- **Claim:** Improvements in semantic comprehension ability—driven by instruction tuning—predict stronger alignment with human neural responses, independent of raw parameter scale.
- **Mechanism:** Instruction tuning refines the internal representational geometry to better support complex task understanding (measured by CSAA). This refinement process appears to bring the vector space closer to the brain's semantic integration space, particularly in the Left Inferior Frontal Gyrus (LIFG).
- **Core assumption:** The Cross-lingual Semantic Alignment Accuracy (CSAA) metric designed for this study serves as a valid proxy for general "comprehension ability" that translates to biological plausibility.
- **Evidence anchors:**
  - [abstract]: "Instruction-tuned models consistently outperformed base versions in both comprehension and neural alignment."
  - [section]: Page 5, Figure 6 shows Instruct > Base correlation; Page 6 notes "semantic comprehension ability as a more meaningful predictor... than model size."
  - [corpus]: *Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain* (arXiv:2506.03832) provides external validation that alignment training (brain-tuning) specifically reorganizes model hierarchies to match biology.
- **Break condition:** If larger, base models (without instruction tuning) eventually surpass tuned models in neural alignment purely due to scale, the "quality > scale" assertion fails.

### Mechanism 3: Hemispheric Functional Specialization Mapping
- **Claim:** LLM embeddings correlate differentially with left vs. right hemisphere regions, reflecting a functional division between core syntactic processing and high-level contextual integration.
- **Mechanism:** The high-dimensional embeddings contain features that linearly map to the Left IFG/PostTemp (syntactic/semantic core) and Right MFG/AntTemp (cognitive control/metaphor). This suggests LLMs implicitly learn features that respect the brain's lateralization.
- **Core assumption:** The linear mapping used in ridge regression is sufficient to uncover these complex, distributed functional asymmetries.
- **Evidence anchors:**
  - [abstract]: "Left-dominant regions aligned with core language processing and right-hemisphere regions reflecting higher-level cognitive functions."
  - [section]: Page 6, Figure 8(b) quantifies Left-Right differences (e.g., IFG: p=0.025).
  - [corpus]: Weak/missing direct external validation for this specific "LLM-to-fMRI lateralization" mechanism in the provided neighbors.
- **Break condition:** If patients with atypical lateralization (e.g., left-handers or split-brain patients) show identical correlation patterns to typical controls, the specific "lateralization" interpretation is likely an artifact of the stimulus or ROI selection rather than a generalizable mechanism.

## Foundational Learning

- **Concept:** General Linear Model (GLM) & Hemodynamic Response Function (HRF)
  - **Why needed here:** The study relies on isolating sentence-level BOLD signals from continuous fMRI noise. Without understanding how the GLM convolves sentence onsets with the HRF (to account for the 4-6 second vascular delay), the alignment results are uninterpretable.
  - **Quick check question:** Why does the paper convolve sentence *onset times* rather than using the TR (Repetition Time) as the unit of analysis for the design matrix?

- **Concept:** Ridge Regression & Regularization ($\alpha$)
  - **Why needed here:** The paper maps high-dimensional embeddings (thousands of features) to voxel activity using ridge regression to prevent overfitting. Understanding the bias-variance trade-off here is key to interpreting the "predictive performance" ($\rho$) claims.
  - **Quick check question:** In Equation 6, what happens to the weight estimates $\hat{\beta}$ if the regularization strength $\alpha$ is set too low given the multicollinearity of embedding dimensions?

- **Concept:** Representational Similarity Analysis (RSA) vs. Encoding Models
  - **Why needed here:** The paper uses an encoding approach (predicting brain activity from embeddings). Distinguishing this from RSA (comparing distance matrices) is necessary to understand why they focus on *prediction correlation* rather than *representational geometry*.
  - **Quick check question:** Why might an encoding model yield significant voxel-wise predictions even if the global representational geometry (RSA) between the LLM and brain differs?

## Architecture Onboarding

- **Component map:** The Little Prince text (1,577 sentences) -> 14 LLMs (Transformer stacks) -> Extract Layer-wise Hidden States -> Ridge Regression models (one per layer, per ROI, per subject) -> fMRI BOLD time series (processed via GLM to sentence-level estimates) -> Pearson Correlation ($\rho$) between predicted and actual voxel activity

- **Critical path:**
  1. **Preprocessing:** Run GLM (Least-Squares Separate approach) on fMRI to get $\hat{\beta}$ (neural response) per sentence.
  2. **Embedding:** Extract hidden states from LLM layers; average token embeddings to get sentence vectors.
  3. **Alignment:** Train Ridge Regression to map Sentence Vectors -> Sentence Neural Responses.
  4. **Verification:** Check for peak correlation at intermediate layers and Left-Right asymmetry.

- **Design tradeoffs:**
  - **GLM Estimation:** The study uses "Least-Squares Separate" (LS-S) to isolate single sentences. This reduces collinearity but dramatically increases computational load compared to block designs.
  - **Layer Selection:** Using the "optimal layer" for Figure 5 maximizes reported performance but obscures the fact that *different* brain regions might prefer *different* layers (though the paper argues for a general intermediate peak).
  - **Metric:** CSAA (Cross-lingual Semantic Alignment Accuracy) is a custom metric. It captures translation/semantic robustness but may not correlate perfectly with standard NLP benchmarks.

- **Failure signatures:**
  - **Flat Layer Curve:** If correlations do not peak at intermediate layers but rise monotonically, the "brain-like hierarchy" claim fails.
  - **Instruct Degradation:** If instruction-tuned models show *lower* brain alignment than base models, it implies alignment training moves representations *away* from biological processing.
  - **Symmetry:** If Left/Right hemispheres show identical correlation profiles, the "functional specialization" hypothesis is unsupported.

- **First 3 experiments:**
  1. **Sanity Check (Random Baseline):** Shuffle the sentence-order of the embeddings relative to the fMRI responses and re-run the ridge regression. Correlation should drop to near zero.
  2. **Layer Ablation:** Plot the correlation $\rho$ for *every* layer (not just the optimal one) for a specific ROI (e.g., LIFG). Verify the inverted-U shape (peak at intermediate depth).
  3. **Instruct vs. Base Delta:** Calculate the difference in CSAA score and Brain Correlation for a specific model pair (e.g., Llama-3.1-8B vs. Llama-3.1-8B-Instruct). Confirm that $\Delta \text{CSAA} > 0$ implies $\Delta \text{Correlation} > 0$.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does explicitly optimizing LLM training procedures to align with brain data result in models with greater cognitive plausibility?
- Basis in paper: [explicit] The discussion asks, "Should training procedures explicitly target alignment with brain data to produce more cognitively plausible models?"
- Why unresolved: Current research primarily correlates *post-hoc* representations with neural data rather than using brain alignment as a direct training constraint.
- What evidence would resolve it: Training models with a loss function weighted by fMRI prediction accuracy and evaluating the resulting model on cognitive benchmarks.

### Open Question 2
- Question: Can hybrid neuro-symbolic architectures better replicate the distributed and lateralized processing patterns observed in the human brain?
- Basis in paper: [explicit] The discussion asks, "Could hybrid neuro-symbolic LLMs better reflect distributed and lateralized processing patterns observed in the brain?"
- Why unresolved: Standard LLMs are monolithic and do not explicitly model the hemispheric asymmetry (e.g., left-dominant syntax vs. right-hemisphere integration) observed in the study.
- What evidence would resolve it: Developing modular architectures that separate syntactic and high-level semantic processing and testing their correspondence with fMRI lateralization data.

### Open Question 3
- Question: Do the observed brain-like patterns in LLMs emerge merely from scaling, or do they reflect a deeper convergence in computational principles with the human language pathway?
- Basis in paper: [explicit] The introduction states, "A fundamental question remains unresolved: Does this similarity merely stem from increased model scale, or does it reflect deeper convergence in computational principles...?"
- Why unresolved: While the paper links semantic comprehension (CSAA) to alignment, the specific computational mechanisms (beyond parameter count or performance metrics) that drive this convergence remain unidentified.
- What evidence would resolve it: Comparing alignment in models with similar sizes but vastly different architectures or training objectives (e.g., predictive coding vs. next-token prediction) to isolate the causal principles.

## Limitations

- The study uses a single narrative stimulus (The Little Prince) which may not generalize to other linguistic domains or languages
- The CSAA metric lacks direct validation against established comprehension benchmarks
- The linear mapping assumption (ridge regression) may oversimplify the complex, nonlinear relationship between embeddings and neural responses

## Confidence

- **High Confidence:** Layer-wise correlation patterns showing intermediate layers outperforming final layers in brain alignment; hemispheric asymmetry findings with left-dominant regions correlating with core language areas
- **Medium Confidence:** Instruction-tuned models consistently showing better brain alignment than base models; the CSAA metric as a valid proxy for comprehension ability
- **Low Confidence:** The specific claim that improvements in model performance drive representational architectures toward brain-like hierarchies; the generalizability of the lateralization findings across diverse linguistic stimuli

## Next Checks

1. **Cross-Stimulus Validation:** Replicate the alignment analysis using a different narrative stimulus (e.g., audio-only storytelling in English) to test whether the intermediate-layer correlation peak generalizes beyond The Little Prince

2. **RSA Comparison:** Complement the encoding model approach with representational similarity analysis (RSA) to verify that the representational geometry between embeddings and brain activity shows similar hierarchical patterns, not just predictive correlations

3. **Patient Population Test:** Apply the same alignment analysis to fMRI data from patients with known language network abnormalities (e.g., stroke patients with left-hemisphere damage) to determine whether the hemispheric asymmetry findings reflect true functional specialization or stimulus-specific artifacts