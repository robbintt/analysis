---
ver: rpa2
title: 'ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding'
arxiv_id: '2506.01274'
source_url: https://arxiv.org/abs/2506.01274
tags:
- frame
- arxiv
- policy
- video
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReFoCUS, a reinforcement learning framework
  that optimizes video frame selection for Large Multimodal Models (LMMs) to improve
  video understanding. Unlike existing approaches that rely on heuristic frame sampling,
  ReFoCUS learns a frame selection policy via reinforcement learning, using rewards
  derived from a reference LMM to guide the model toward selecting frames that best
  support temporally grounded responses.
---

# ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding

## Quick Facts
- **arXiv ID:** 2506.01274
- **Source URL:** https://arxiv.org/abs/2506.01274
- **Reference count:** 40
- **Primary result:** Reinforcement learning framework that learns to select video frames for Large Multimodal Models, improving video QA accuracy by 4.7% on average across benchmarks.

## Executive Summary
ReFoCUS introduces a reinforcement learning framework that optimizes video frame selection for Large Multimodal Models (LMMs) to improve video understanding. Unlike existing approaches that rely on heuristic frame sampling, ReFoCUS learns a frame selection policy via reinforcement learning, using rewards derived from a reference LMM to guide the model toward selecting frames that best support temporally grounded responses. The method employs an autoregressive, conditional frame selection architecture that ensures temporal coherence while reducing the combinatorial complexity of exploring the large frame space. ReFoCUS consistently improves reasoning performance across multiple video QA benchmarks without requiring explicit frame-level supervision.

## Method Summary
ReFoCUS learns a frame selection policy that autoregressively chooses T'=32 frames from videos (sampled at 4fps, up to 512 frames) to maximize a margin-based reward computed by a frozen reference LMM (InternVL3). The policy uses a Video-MA2mba backbone with modified attention heads for frame selection, employing conditional attention over frame embeddings. Training uses GRPO-style optimization with entropy regularization (β=0.002) on 98K QA pairs filtered from 962K by variance threshold τ=0.21. The method addresses the challenge of aligning frame selection with LMM preferences without ground-truth frame-level supervision.

## Key Results
- Improves video QA accuracy by 4.7% on average across Video-MME, LongVideoBench, MLVU, and Video-MMMU benchmarks
- Outperforms heuristic frame selection methods while reducing computational overhead
- Demonstrates consistent improvements across diverse video QA tasks without requiring frame-level annotations

## Why This Works (Mechanism)

### Mechanism 1: Margin-Based Reward as Proxy for Frame Utility
Frame subsets that maximize the prediction margin between correct and most-competitive incorrect answers provide learnable signals for frame selection. The reward model rφ evaluates each candidate frame subset f^(j) by computing r_j = [rφ(y*|f^(j),q) - max_{ŷ≠y*} rφ(ŷ|f^(j),q)] / [rφ(y*|f^(j),q) + max_{ŷ≠y*} rφ(ŷ|f^(j),q)], normalized to [-1, 1]. This margin reflects residual uncertainty between top choices. Policy gradient optimization then pushes πθ toward frame combinations that maximize this margin. Core assumption: The reference LMM's confidence margin generalizes to query-relevant visual evidence; high margin implies the selected frames contain disambiguating information rather than spurious correlations.

### Mechanism 2: Autoregressive Conditional Selection Reduces Combinatorial Complexity
Autoregressive frame selection conditioned on previously selected frames enables tractable exploration of the frame subset space. Starting from a special token, the policy model autoregressively produces latent outputs. At each step i, the previously selected frame f_{<i} serves as a query over candidate frame embeddings via scaled dot-product attention: f_i ∼ πθ(·|f_{<i}, v, q). This yields a probability distribution from which the next frame is sampled, with already-selected frames masked to prevent duplication. Selection continues for T' frames (default: 32). Core assumption: Sequential conditioning captures inter-frame dependencies relevant to the query; the non-causal strategy avoids unnecessary permutation complexity while maintaining semantic coherence.

### Mechanism 3: Reward Variance Filtering Ensures Discriminative Learning Signals
Filtering QA pairs by prediction variance across diverse frame subsets retains samples where visual evidence meaningfully affects model predictions. For each QA pair, 16 candidate frame subsets are constructed by sampling from 8 overlapping temporal windows and their complements. A pretrained LMM computes prediction margins for each subset. QA pairs with variance below threshold τ=0.21 are discarded, yielding 98K pairs from 962K. This removes samples where the model's answer is insensitive to which frames are shown. Core assumption: High variance indicates genuine temporal grounding requirements; low variance indicates either trivial questions or questions answerable without temporal visual evidence.

## Foundational Learning

- **Concept: Policy Gradient with Group-Relative Advantages (GRPO)**
  - **Why needed here:** Standard policy gradient requires an explicit value function baseline, but no ground-truth frame-level value exists. GRPO uses group-wise normalized rewards as relative baselines, enabling optimization without a learned value function.
  - **Quick check question:** Given N=16 candidate frame subsets with rewards r_j, how is the advantage Â_j computed? (Answer: Â_j = (r_j - mean(r)) / (std(r) + ε), normalized within each group)

- **Concept: Entropy Regularization for Exploration**
  - **Why needed here:** Without frame-level ground truth, the policy can collapse to selecting temporally redundant frames or overfitting to high-reward regions. Entropy regularization maintains exploration diversity.
  - **Quick check question:** Why is entropy bonus equivalent (up to a constant) to KL divergence from policy to uniform distribution over remaining frames? (Answer: D_KL(π||U) = -H(π) + log|A|; gradients are identical)

- **Concept: Margin-Based Reward Design**
  - **Why needed here:** Raw probability of correct answer is insufficient because it doesn't distinguish between confident correct predictions and uncertain correct predictions. Margin captures decisiveness against the strongest distractor.
  - **Quick check question:** Why does tanh((z_{y*} - z_ỹ)/2) provide a more stable computation than the ratio formulation? (Answer: Avoids explicit probability computation and division; operates directly on logits with bounded output range)

## Architecture Onboarding

**Component map:**
Input: video v ∈ R^{T×H×W×3}, question q → [Policy Model πθ: Video-MA2mba backbone] → Frame Subsets: N=16 candidates, T'=32 frames each → [Reward Model rφ: InternVL3 (frozen)] → Reward Computation: margin-based r_j per Eq. 1 → Policy Optimization: GRPO with entropy β=0.002

**Critical path:**
1. Frame embedding extraction: Vision encoder → per-frame token sequences → last-token aggregation for frame-level embeddings
2. Autoregressive selection loop: Query vector from current state attends over frame embeddings; sample from softmax; mask selected; repeat T' times
3. Reward estimation: Forward pass through frozen rφ with selected frames; extract logits; compute margin
4. Gradient computation: Group-normalize rewards; compute policy gradient with importance sampling ratio πθ/πθ_old

**Design tradeoffs:**
- Mamba vs. Transformer backbone: Mamba's state-space modeling handles long sequences (512 frames ≈ 10^5 tokens) more efficiently than attention, but requires re-initialization strategy for frame selection heads
- Non-causal vs. causal selection: Enforcing temporal order (causal) would increase search space as permutation task; non-causal allows flexible attention but requires masking to prevent duplicates
- Multiple-choice vs. open-ended QA: Multiple-choice provides stable reward signals; open-ended would require LLM-as-judge or embedding-based similarity, introducing noise
- Threshold τ=0.21: Empirically determined; higher values reduce data, lower values retain noisy samples

**Failure signatures:**
1. Temporal collapse: Policy selects frames from narrow time window; detect via Kozachenko-Leonenko entropy dropping significantly
2. Reward hacking: Policy learns to select frames that trigger high reward model confidence without semantic relevance; monitor reward-to-ground-truth-accuracy correlation on validation set
3. Gradient saturation: Near-zero gradients despite training; check reward variance distribution - if too many low-variance samples remain, increase τ
4. Attention degeneracy: Query attends to single frame repeatedly; verify masking is applied correctly

**First 3 experiments:**

1. Sanity check - reward variance filtering validation: Train policy on filtered (τ=0.21) vs. unfiltered data. Measure: gradient magnitude distribution, final performance on Video-MME. Expected: unfiltered data shows gradient saturation, lower final accuracy.

2. Ablation - frame selection size T': Train policies with T' ∈ {4, 8, 32} frames. Measure: Video-MME accuracy, temporal entropy of selections. Expected: Larger T' improves accuracy but entropy patterns differ.

3. Robustness - reward model swap: Train with different frozen reward models (e.g., LLaVA-OV-7B vs. InternVL3-8B). Measure: policy behavior divergence (JS/KL divergence of selection distributions), final accuracy. Expected: Stronger reward model (InternVL3) yields better policies, but patterns should be qualitatively similar.

## Open Questions the Paper Calls Out

### Open Question 1
How can the reward formulation be adapted to support open-ended video QA tasks where discrete answer margins are unavailable? The current reliance on prediction confidence margins (Eq. 1) requires a fixed set of candidate answers to calculate the normalized difference between the correct label and distractors. A modified reward mechanism (e.g., using a learned verifier or semantic similarity scores) that successfully trains the policy model on open-ended generative benchmarks would resolve this.

### Open Question 2
To what extent does the frame selection policy overfit to the specific biases or visual preferences of the reference LMM used for reward estimation? While the method aligns frame selection with the model's internal utility, it is unclear if the selected frames represent objective "ground truth" relevance or merely the idiosyncrasies of the specific InternVL3 reward model used. A cross-validation study showing whether a policy trained with one reference LMM (e.g., InternVL) improves performance when transferred to a different base model (e.g., LLaVA) without retraining would resolve this.

### Open Question 3
Can the computational overhead of the reinforcement learning pipeline be reduced while maintaining the diversity of frame exploration? The current method requires sampling N candidates (subsets) and computing rewards via a full forward pass of the reference LMM for each, which is resource-intensive compared to static heuristics. Demonstrating an efficient sampling strategy (e.g., guided search or distillation) that reduces the number of required candidates per step without degrading the temporal coherence or accuracy reported in Table 1 would resolve this.

## Limitations

- The learned policy is inherently dependent on the preferences of the reference LMM, potentially inheriting its biases or suboptimal visual preferences
- The reinforcement learning pipeline requires considerable computational cost due to repeated autoregressive sampling and reward estimation
- The method is currently constrained to multiple-choice QA tasks, limiting applicability to open-ended video understanding scenarios

## Confidence

- **High Confidence:** The autoregressive frame selection mechanism with conditional attention is well-specified and theoretically sound
- **Medium Confidence:** The variance filtering approach for identifying informative QA pairs is justified by the manuscript's analysis but would benefit from broader validation across datasets
- **Medium Confidence:** The margin-based reward formulation is principled, but its effectiveness depends on the reward model's calibration and generalization to frame-level utility

## Next Checks

1. **Reward-to-Accuracy Correlation:** Measure the correlation between InternVL3 margin scores and actual QA accuracy on a held-out validation set. Low correlation would indicate reward model miscalibration.

2. **Frame Selection Ablation:** Systematically vary T' (number of selected frames) and τ (variance threshold) to identify optimal configurations and test sensitivity to hyperparameters.

3. **Reward Model Transferability:** Train ReFoCUS policies using different frozen reward models (e.g., LLaVA-OV-7B vs. InternVL3-8B) and compare policy behavior divergence and final accuracy to assess generalizability of the LMM-as-evaluator paradigm.