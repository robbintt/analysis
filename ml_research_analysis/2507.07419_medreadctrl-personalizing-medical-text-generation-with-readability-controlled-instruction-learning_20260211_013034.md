---
ver: rpa2
title: 'MedReadCtrl: Personalizing medical text generation with readability-controlled
  instruction learning'
arxiv_id: '2507.07419'
source_url: https://arxiv.org/abs/2507.07419
tags:
- readability
- grade
- text
- medical
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating readable medical
  text tailored to individual patient literacy levels, a critical barrier to effective
  AI-human communication in healthcare. The authors propose MedReadCtrl, a readability-controlled
  instruction tuning framework that enables large language models (LLMs) to dynamically
  adjust text complexity while preserving meaning.
---

# MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning

## Quick Facts
- arXiv ID: 2507.07419
- Source URL: https://arxiv.org/abs/2507.07419
- Reference count: 40
- This paper addresses the challenge of generating readable medical text tailored to individual patient literacy levels, a critical barrier to effective AI-human communication in healthcare.

## Executive Summary
This paper addresses the challenge of generating readable medical text tailored to individual patient literacy levels, a critical barrier to effective AI-human communication in healthcare. The authors propose MedReadCtrl, a readability-controlled instruction tuning framework that enables large language models (LLMs) to dynamically adjust text complexity while preserving meaning. MedReadCtrl was evaluated across nine datasets and three tasks (text simplification, paraphrase generation, semantic entailment) in both medical and general domains. Results show that MedReadCtrl significantly outperformed GPT-4 in readability instruction-following errors (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and achieved substantial gains on unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples). Human evaluations found experts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low literacy levels. These results demonstrate MedReadCtrl's ability to restructure clinical content into accessible, readability-aligned language while preserving medical intent, offering a scalable solution for personalized patient education and equitable AI-enabled care.

## Method Summary
MedReadCtrl fine-tunes LLaMA3-8B using FLAN-style instruction tuning with a novel readability-controlled framework. The method trains on nine datasets across three tasks: text simplification, paraphrase generation, and semantic entailment. Each training example pairs input text with a readability grade target (1-12) using natural language instructions. The model learns to adjust output complexity while preserving meaning by optimizing across four readability metrics (FKGL, GFI, ARI, CLI) averaged into RGL. Training uses Adam optimizer with LR=3×10⁻⁴, batch size=128, for 5 epochs on 2× NVIDIA A100 40GB. The approach enables fine-grained complexity adjustment validated through automatic metrics (ROUGE, BLEU, SARI) and human evaluation.

## Key Results
- Achieved significantly lower readability instruction-following errors than GPT-4 (1.39 vs. 1.59 on ReadMe, p<0.001)
- Outperformed baselines by +14.7 ROUGE-L and +6.18 SARI on unseen clinical task (MTSamples)
- Human experts consistently preferred MedReadCtrl outputs (71.7% vs. 23.3%), especially at low literacy levels

## Why This Works (Mechanism)

### Mechanism 1
Explicit readability-level instructions embedded in training data calibrate output complexity more precisely than implicit prompting. The model learns a mapping between numeric grade targets (1-12) and linguistic complexity features through supervised instruction tuning on paired (text, grade-level) examples. During inference, the instruction "output with a readability score around X" triggers this learned mapping. Core assumption: Readability grade levels can be reliably approximated by traditional formulas (FKGL, GFI, ARI, CLI) and these correlate with human comprehension.

### Mechanism 2
Multi-task training across simplification, paraphrasing, and entailment creates transferable semantic preservation skills. Training on three related but distinct tasks forces the model to learn content preservation as an invariant constraint across varying output complexity levels. Paraphrase tasks anchor semantic equivalence; entailment tasks enforce logical consistency; simplification tasks practice complexity reduction. Shared representations from these tasks reinforce a "preserve meaning while adjusting form" capability. Core assumption: Semantic preservation skills learned on general-domain tasks transfer to medical-domain tasks.

### Mechanism 3
Averaging multiple readability formulas reduces metric-specific gaming and improves robustness. Four formulas (FKGL, GFI, ARI, CLI) measure different linguistic features—syllable count, long-word frequency, character-to-word ratio, sentence structure. Optimizing their average (RGL) prevents the model from exploiting loopholes in any single metric. Core assumption: The arithmetic mean of these four formulas approximates human-judged readability better than any single metric.

## Foundational Learning

- **Instruction Tuning (FLAN-style)**: Why needed here: MedReadCtrl's core approach is not architectural modification but training-data engineering—formatting readability targets as natural-language instructions. Quick check: Can you explain the difference between fine-tuning on task outputs vs. fine-tuning on (instruction, output) pairs?

- **Readability Formulas (FKGL, GFI, ARI, CLI)**: Why needed here: These metrics define the optimization target; understanding their components reveals what linguistic features the model learns to manipulate. Quick check: Which formula penalizes long words most heavily? (Answer: GFI uses "words with more than seven characters" as a key factor)

- **Semantic Entailment in Clinical NLP**: Why needed here: MedReadCtrl trains on MedNLI, which requires understanding that clinical statements can imply, contradict, or be neutral to each other—critical for avoiding simplification that changes diagnostic meaning. Quick check: Given "Patient has type 2 diabetes," would "Patient has a metabolic disorder" be entailed, contradicted, or neutral? (Entailed—diabetes is a metabolic disorder)

## Architecture Onboarding

- **Component map**: Base model (LLaMA3-8B) -> Instruction formatting (input text + target grade) -> Training data (9 datasets) -> Readability scoring module (FKGL, GFI, ARI, CLI averaged to RGL) -> Evaluation layer (ROUGE, BLEU, SARI, human rating)

- **Critical path**: 1. Prepare instruction-formatted training pairs: (input_text, "output with readability around grade X", reference_output) 2. Fine-tune LLaMA3-8B with cross-entropy loss on instruction-following objective 3. Inference: feed input + target grade instruction -> generate output 4. Validate: compute RGL of output, compare to target; compute ROUGE/BLEU/SARI vs. reference

- **Design tradeoffs**: Single-model vs. routing (MedReadCtrl uses one model for all grade levels); Metric-averaging vs. single-metric (averaging reduces gaming but may dilute signal); Medical-only vs. mixed-domain training (paper uses both; medical-only might improve domain calibration but reduce generalization)

- **Failure signatures**: Hallucination at low grades (over-simplification introduces inaccuracies); Verbosity at high grades (grade 11 outputs become overly technical with redundant phrasing); Instruction misinterpretation (model generates definitions instead of simplifying); Metric-comprehension gap (low RGL scores do not guarantee actual patient understanding)

- **First 3 experiments**: 1. Reproduce readability calibration curve: Test LLaMA3-MedReadCtrl on held-out MedNLI samples, plotting requested grade (x-axis) vs. achieved RGL (y-axis); expect near-identity line; deviation patterns reveal systematic over/under-complexity. 2. Ablate multi-task training: Train separate models on each task; compare semantic preservation and readability control on MTSamples; expect multi-task model to outperform single-task on generalization. 3. Stress-test semantic fidelity: Input clinical statements with critical diagnostic information; manually review Grade 2 outputs for meaning preservation; failure = any output that could reasonably lead to opposite clinical interpretation.

## Open Questions the Paper Calls Out

### Open Question 1
Does MedReadCtrl improve actual comprehension and adherence for patients with low health literacy compared to standard medical texts? Basis: The authors acknowledge the study "did not incorporate real users (e.g., patients or caregivers) from the target literacy groups" and state future work must include "end-user evaluation." Why unresolved: Current evaluations rely on medical experts and NLP metrics, not the target demographic. What evidence would resolve it: A randomized study measuring information retention, comprehension quiz scores, and care adherence rates among patients with varying literacy levels.

### Open Question 2
Can retrieval-augmented generation (RAG) mitigate factual hallucinations in readability-controlled outputs without degrading readability alignment? Basis: The authors note the model is "susceptible to factual hallucinations" and suggest "integration with retrieval-augmented generation (RAG) frameworks... may help reduce the risk." Why unresolved: While RAG improves factuality, it is unknown if retrieving raw medical documents disrupts the model's ability to maintain the specific simplified style or target readability grade. What evidence would resolve it: Experiments combining MedReadCtrl with RAG, measuring the trade-off between fact-verification scores and readability error margins.

### Open Question 3
How can frameworks disentangle readability control from verbosity to prevent overly lengthy outputs at high grade levels? Basis: The authors observe that "readability control sometimes becomes conflated with verbosity" and suggest future work should "incorporate multidimensional control signals" such as length and tone. Why unresolved: Current instruction tuning may implicitly teach the model that "complex" equals "longer," leading to redundant Grade 11 outputs that overwhelm the reader. What evidence would resolve it: Implementation of control tokens for length alongside readability, evaluated by semantic density metrics and human judgment of conciseness.

## Limitations

- Readability formula applicability: The paper assumes traditional readability formulas reliably predict actual patient comprehension across diverse populations, but this assumption is untested with human comprehension validation.
- Multi-task transfer generalization: While multi-task training shows strong performance, the mechanism relies on skill transfer from general-domain datasets to medical-domain tasks without validation for highly specialized clinical subdomains.
- Error type distribution: Hallucination and semantic drift are noted in error analysis, but no quantitative breakdown of error types or severity thresholds is provided, making it difficult to assess real-world risk in clinical deployment.

## Confidence

- **High confidence**: Readability control precision on seen tasks (ASSET, ReadMe, MedNLI)—the paper provides clear metrics (MAE, ROUGE, SARI) and direct comparisons to GPT-4, with statistically significant improvements (e.g., p<0.001 for ReadMe).
- **Medium confidence**: Generalizability to unseen clinical tasks (MTSamples)—performance gains are reported, but evaluation is limited to one dataset and lacks ablation studies isolating the contribution of multi-task vs. domain adaptation.
- **Low confidence**: Human comprehension outcomes—no patient-facing validation or comprehension testing is reported; all claims about "accessible, readability-aligned language" rest on metric optimization rather than actual user understanding.

## Next Checks

1. **Comprehension validation study**: Recruit 50+ participants across literacy levels (1-12 grade equivalents) and native language backgrounds. Present MedReadCtrl outputs alongside original clinical text; measure comprehension accuracy, time-to-understanding, and subjective difficulty. Compare to GPT-4 outputs and original text.

2. **Subdomain stress test**: Test MedReadCtrl on three high-specialization clinical subdomains (e.g., oncology, rare diseases, pediatric dosing). Manually inspect outputs for semantic drift, hallucination, and readability calibration accuracy. Quantify error rates per subdomain.

3. **Metric correlation audit**: Collect 200+ human-rated readability scores for MedReadCtrl outputs. Compute correlation (Pearson/Spearman) between averaged RGL and human ratings. Test whether any single formula (e.g., GFI) correlates better than the average, and whether population-specific adjustments improve prediction.