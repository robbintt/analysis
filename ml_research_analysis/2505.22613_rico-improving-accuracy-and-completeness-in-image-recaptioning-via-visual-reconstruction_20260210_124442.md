---
ver: rpa2
title: 'RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual
  Reconstruction'
arxiv_id: '2505.22613'
source_url: https://arxiv.org/abs/2505.22613
tags:
- image
- caption
- rico
- captions
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RICO, a framework for improving image recaptioning
  accuracy and completeness through visual reconstruction. The key insight is that
  typical recaptioning methods suffer from inaccuracies and incompleteness due to
  hallucinations and missing fine-grained details.
---

# RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction

## Quick Facts
- arXiv ID: 2505.22613
- Source URL: https://arxiv.org/abs/2505.22613
- Reference count: 25
- Primary result: Iterative refinement via visual reconstruction improves image recaptioning accuracy and completeness, achieving over 10-point gains on CapsBench and CompreCap benchmarks.

## Executive Summary
RICO introduces a framework for improving image recaptioning accuracy and completeness through visual reconstruction. The method addresses hallucinations and missing fine-grained details in captions by iteratively refining them based on discrepancies between original images and images reconstructed from candidate captions using a text-to-image model. Extensive experiments demonstrate that RICO significantly outperforms baseline methods on multiple benchmarks, with RICO-Flash providing an efficient alternative via Direct Preference Optimization.

## Method Summary
RICO uses a powerful multimodal model (GPT-4o) to identify and correct discrepancies between original images and images reconstructed from candidate captions using FLUX.1-dev. The framework iteratively refines captions across eight key aspects including visual details, composition, human attributes, and more. To reduce computational costs, RICO-Flash learns the refinement behavior using Direct Preference Optimization (DPO) on preference pairs derived from the iterative process.

## Key Results
- RICO achieves over 10-point gains on CapsBench and CompreCap benchmarks compared to baselines
- Performance improves consistently across iterations, with gains plateauing after approximately 2 steps
- RICO-Flash maintains comparable performance to RICO while being more computationally efficient
- Superior performance on fine-grained aspects like color, entity shape, and relative position
- Strong generalization across different initial captioning models and prompts

## Why This Works (Mechanism)

### Mechanism 1
Visual reconstruction makes semantic loss observable in a comparable modality. Rather than directly comparing text to image, the framework reconstructs the caption into an image, enabling direct visual-to-visual comparison. Missing or incorrect caption details manifest as visible discrepancies. The refinement MLLM identifies these gaps more reliably than through image-text comparison alone.

### Mechanism 2
Iterative refinement progressively recovers fine-grained details that single-pass editing misses. Each refinement iteration exposes residual discrepancies from the previous step. Early iterations fix obvious errors; later iterations recover subtle attributes. The process saturates around 2-3 iterations as remaining gaps become irrecoverable given model capabilities.

### Mechanism 3
DPO distills iterative refinement behavior into a single-pass model while preserving preference structure. The iterative RICO pipeline naturally generates preference pairs: initial captions (dispreferred) → refined captions (preferred). DPO fine-tunes a base model to maximize the likelihood gap between preferred and dispreferred outputs without requiring explicit reward modeling or RL infrastructure.

## Foundational Learning

- **Cross-modal Semantic Alignment**: Understanding that semantic consistency requires bidirectional validation (image→text→reconstructed image) is essential. Quick check: Can you explain why comparing two images is easier for an MLLM than comparing an image and a text description?

- **Direct Preference Optimization (DPO)**: DPO treats preference learning as binary classification over response pairs, eliminating the need for a separate reward model. Quick check: Given a preference pair (y⁺, y⁻), does DPO maximize the absolute likelihood of y⁺ or the relative likelihood ratio between y⁺ and y⁻?

- **Hallucination in MLLMs**: Understanding that MLLMs generate plausible but ungrounded content is critical. Quick check: Why would an MLLM generate an incorrect bus count even when the image clearly shows three buses?

## Architecture Onboarding

- **Component map**: Initial Captioner (Qwen2-VL) -> Reconstruction Model T (FLUX.1-dev) -> Refinement Model R (GPT-4o) -> Revised Caption
- **Critical path**: Generate initial caption from image → Reconstruct caption→image using FLUX.1-dev → Feed (original image, reconstructed image, current caption) to GPT-4o with structured prompt → Extract revised caption → Repeat for N=2 iterations
- **Design tradeoffs**: FLUX.1-dev vs. other T2I models (FLUX uses T5 encoder supporting >77 tokens), N=2 iterations vs. higher (diminishing returns), GPT-4o vs. open-source reviser (cost vs. quality), DPO vs. supervised fine-tuning (preference contrast matters)
- **Failure signatures**: Reconstruction quality insufficient, over-correction loops, DPO data noise, prompt sensitivity
- **First 3 experiments**: Validate reconstruction-discrepancy correlation on 50 images, ablate iteration count on CapsBench subset, test DPO preference pair quality by sampling 100 pairs

## Open Questions the Paper Calls Out

- How robust is the RICO framework when utilizing less capable text-to-image models for reconstruction, and does performance scale linearly with improvements in generative model capability?
- How can the refinement model be optimized to select the most concise yet effective revision when multiple plausible corrections exist for a visual discrepancy?
- Can the iterative pipeline be structurally decoupled to reduce computational overhead without resorting to DPO distillation?

## Limitations

- The framework relies heavily on the text-to-image model's capability to faithfully reconstruct caption details, creating high demands on model quality
- Iterative refinement requires substantial computational resources and API access costs for GPT-4o
- Determining how to refine captions in a concise yet effective manner remains challenging when multiple plausible revisions exist

## Confidence

- **High Confidence**: Iterative refinement mechanism effectiveness, RICO-Flash training procedure
- **Medium Confidence**: Claims about RICO's superiority over specialized methods
- **Low Confidence**: Generalization claims to diverse downstream applications

## Next Checks

1. **Reconstruction Fidelity Analysis**: Manually examine 50 image-reconstruction pairs to quantify how often caption errors produce visible reconstruction discrepancies versus when reconstruction failures confound the signal.

2. **Cross-Model Generalization**: Apply RICO-Flash to captions generated by models outside the Qwen2-VL family (e.g., Gemini, Claude) to assess robustness to different captioning styles and error patterns.

3. **Computational Cost-Benefit Analysis**: Measure actual API costs and inference times for GPT-4o refinement across varying iteration counts to determine the economic feasibility for production deployment.