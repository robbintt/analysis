---
ver: rpa2
title: Scaling Laws for Differentially Private Language Models
arxiv_id: '2501.18914'
source_url: https://arxiv.org/abs/2501.18914
tags:
- uni00000015
- uni00000014
- uni00000049
- uni00000058
- uni00000016
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work establishes scaling laws for differentially private (DP)
  language models, addressing the challenge of training large models while protecting
  sensitive user data. The core method involves extending traditional scaling laws
  to account for compute-privacy-utility tradeoffs under DP constraints, using a semi-parametric
  modeling approach that decouples noise calibration from privacy accounting.
---

# Scaling Laws for Differentially Private Language Models

## Quick Facts
- **arXiv ID:** 2501.18914
- **Source URL:** https://arxiv.org/abs/2501.18914
- **Reference count:** 40
- **Primary result:** Optimal DP language model sizes are typically an order of magnitude smaller than non-private scaling laws predict, with compute-optimal configurations saving 5× to 100× compute while maintaining comparable utility.

## Executive Summary
This work establishes scaling laws for differentially private (DP) language models, addressing the challenge of training large models while protecting sensitive user data. The core method involves extending traditional scaling laws to account for compute-privacy-utility tradeoffs under DP constraints, using a semi-parametric modeling approach that decouples noise calibration from privacy accounting. Experiments with BERT models ranging from 4.5M to 778M parameters reveal that optimal model sizes under DP are typically an order of magnitude smaller than non-private scaling laws predict, with compute-optimal configurations saving 5× to 100× compute while maintaining comparable utility. The study finds that increasing compute budget can provide diminishing returns unless accompanied by corresponding increases in privacy or data budgets, and identifies token-to-model ratios of 1000-100000 as effective under moderate privacy budgets.

## Method Summary
The authors extend traditional LLM scaling laws to the DP setting by analyzing the impact of noise-batch ratio (σ̄) rather than directly modeling privacy budget ε. They train BERT models of various sizes with DP-Adam, sweeping across noise-batch ratios, learning rates, and compute budgets. A semi-parametric function is fitted to predict loss based on model size (M), training tokens (T), and noise-batch ratio (σ̄), enabling optimization over the three-way trade-off between compute (C), privacy (ε), and data (N) budgets. The approach assumes that DP noise dominates minibatch sampling noise and that the noise-batch ratio captures the essential privacy-utility trade-off.

## Key Results
- Optimal model sizes under DP are typically 10× smaller than non-private scaling laws predict
- Compute-optimal configurations achieve 5× to 100× compute savings while maintaining comparable utility
- For moderate privacy budgets (ε in [1, 10]), token-to-model ratios of 1000-100000 are optimal (vs. ~20× non-privately)
- Budget interaction effects show diminishing returns when increasing compute, privacy, or data budgets in isolation

## Why This Works (Mechanism)

### Mechanism 1: Noise-Batch Ratio Decoupling
Separating noise calibration from privacy accounting via the noise-batch ratio (σ̄) enables more interpretable scaling predictions than direct privacy budget modeling. Rather than modeling loss directly as a function of ε, the approach uses σ̄ (noise std dev added to mean minibatch gradient) as an intermediate variable. Privacy accounting is applied post-hoc to map (ε, B, N, T) → σ̄. This avoids nonlinearities in DP accounting that would otherwise obscure trade-offs.

### Mechanism 2: Compute Reallocation to Data Over Parameters
Under DP constraints, compute-optimal configurations allocate more resources to processing more tokens rather than increasing model capacity. DP noise scales with model dimension through gradient size. Larger models amplify the signal-to-noise problem because each parameter's gradient receives independent noise. Smaller models with more training iterations achieve better utility per FLOP under privacy constraints.

### Mechanism 3: Budget Interaction Effects
Privacy (ε), data (N), and compute (C) budgets exhibit synergistic relationships where increasing one in isolation yields diminishing returns. The noise-batch ratio σ̄ ≈ f(ε, B, N, T). For fixed compute, increasing batch size B requires decreasing iterations T, which affects σ̄ nonlinearly. Similarly, more data N dilutes per-example privacy loss but requires more compute to process.

## Foundational Learning

- **Concept: Differential Privacy (ε, δ)-DP**
  - **Why needed here:** The entire framework builds on formal DP guarantees. Without understanding that ε controls privacy loss and δ is a failure probability, the budget trade-offs are meaningless.
  - **Quick check question:** If ε = 1 vs ε = 8, which provides stronger privacy protection, and by roughly what factor?

- **Concept: DP-SGD (Gradient Clipping + Noise Addition)**
  - **Why needed here:** The mechanism assumes DP-Adam (DP-SGD variant). You must understand that per-example clipping bounds sensitivity and Gaussian noise provides the privacy guarantee.
  - **Quick check question:** Why does DP-SGD clip per-example gradients rather than batch gradients, and what determines the noise scale?

- **Concept: Power-Law Scaling in LLMs**
  - **Why needed here:** The paper extends Kaplan/Hoffmann scaling laws. Understanding L ∝ M^(-α) · D^(-β) relationships is prerequisite to seeing how DP modifies them.
  - **Quick check question:** In non-private scaling, what happens to optimal model size if you 10× your compute budget?

## Architecture Onboarding

- **Component map:** Input Budgets (ε, N, C) → Constant-Compute Configurations (M, B, T) → Privacy Accountant (dp_accounting) → Noise-Batch Ratio (σ̄) → Fitted Scaling Function L(M, T, σ̄) → Optimal Configuration Selection

- **Critical path:**
  1. Define your constraints (ε, N, C budgets)
  2. Enumerate configurations satisfying C = 6·M·B·S·T
  3. For each config, compute σ̄ via privacy accountant
  4. Query fitted function to predict loss
  5. Select configuration minimizing loss

- **Design tradeoffs:**
  - **Larger B vs. larger T:** At fixed compute, larger batch improves σ̄ but reduces iterations. Paper suggests B in 10K-100K range is typically near-optimal.
  - **Larger M vs. more tokens:** Under moderate DP, prefer smaller models (10-50× smaller than Chinchilla) with more training tokens.
  - **Physical batch size choice:** Paper uses 1024; Appendix C.3 shows this may underestimate benefits of larger B for some σ̄ values.

- **Failure signatures:**
  - Loss plateau despite increasing compute → likely ε or N constrained
  - Non-monotonic loss curves → check isotonic regression smoothing hasn't masked divergence
  - σ̄ predictions don't match realized noise → verify accountant matches actual subsampling scheme

- **First 3 experiments:**
  1. **Reproduce baseline curve:** Train BertMedium with fixed ε=4, N=10⁷, varying compute budgets to validate fitted function predictions match realized loss.
  2. **Batch size ablation:** At fixed σ̄ and compute, vary physical batch size (128, 512, 2048, 8192) to quantify the residual effect not captured by noise-batch ratio.
  3. **Budget interaction test:** Fix compute, systematically vary (ε, N) pairs to verify the synergistic improvement pattern from Section 4.5 holds for your data distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do scaling laws for differentially private language models differ between pretraining and finetuning regimes?
- **Basis in paper:** Appendix A explicitly identifies this as an open question, noting that while the study focused on pretraining, "Finetuning a pretrained model with DP is often a preferable approach in practice."
- **Why unresolved:** The authors focused on training from scratch. Finetuning involves different privacy-utility dynamics due to the existence of a pretrained base model, potentially altering the optimal model size and compute allocation strategies.
- **Evidence to resolve:** Experiments replicating the scaling law methodology using pretrained models on downstream tasks (e.g., GLUE benchmarks) to determine if the "smaller model is better" heuristic holds when initial weights are non-random.

### Open Question 2
- **Question:** What is the impact of variable sequence length on the compute-privacy-utility trade-off?
- **Basis in paper:** Appendix A states that "sequence length is yet another important knob that can be tuned" and suggests there are "interesting trade-offs" regarding whether saved computation from shorter sequences should be reallocated to batch size or model size.
- **Why unresolved:** The experimental setup fixed the sequence length at 512 tokens to control variables.
- **Evidence to resolve:** A sweep of experiments varying sequence length alongside model size and batch size to determine if reducing context window allows for larger batch sizes (lower noise) that improve overall utility.

### Open Question 3
- **Question:** Why do smaller physical batch sizes sometimes yield lower loss than larger ones at the same noise-batch ratio, and does this invalidate the noise-batch ratio assumption?
- **Basis in paper:** Appendix A and Appendix C.3 discuss a "surprising phenomenon" where smaller physical batch sizes outperformed larger ones, which the authors explicitly state they "cannot fully explain."
- **Why unresolved:** The authors ruled out learning rate tuning and pipeline bugs. This finding challenges the core assumption that the noise-batch ratio is the sole determinant of performance, independent of physical batch size.
- **Evidence to resolve:** A theoretical analysis or broader empirical sweep to determine if this is an artifact of specific model sizes or a fundamental interaction between gradient variance and DP noise clipping.

## Limitations

- The scaling function is fitted to a finite design space and may not extrapolate well to extreme configurations (very large models, extreme privacy budgets, or unconventional batch sizes)
- The assumption that DP noise dominates minibatch sampling noise may break down in low-noise regimes, potentially invalidating the decoupling mechanism
- The fixed δ=10^-8 may not be appropriate for all applications requiring DP guarantees

## Confidence

**High Confidence:**
- The direction of DP's impact on scaling (smaller optimal models, higher token-to-parameter ratios)
- The qualitative budget interaction effects (diminishing returns from isolated budget increases)
- The methodology of decoupling noise calibration from privacy accounting for interpretability

**Medium Confidence:**
- The quantitative optimal configurations (specific model sizes, token ratios) which depend on fitted parameters
- The exact compute savings factors (5× to 100×) which are sensitive to the underlying scaling law parameters
- The assertion that DP noise dominates sampling noise in practical regimes

**Low Confidence:**
- Extrapolation beyond the trained configuration space (e.g., models larger than 778M parameters or privacy budgets outside [1, 8])
- Performance in non-English or specialized domain datasets not represented in the BERT corpus
- The specific learning rate tuning with DP noise active, as the paper only sweeps rates without noise

## Next Checks

1. **Configuration Sensitivity Test:** Systematically vary one budget parameter (ε, N, or C) while holding others constant across the full range studied. Verify that loss predictions from the fitted function match realized training loss within 10% error across all noise-batch ratios and model sizes.

2. **Physical Batch Size Validation:** Design an experiment with fixed compute and σ̄ but varying physical batch sizes (e.g., B=256, 1024, 4096, 16384). Quantify the residual variance explained by physical batch size after accounting for σ̄, and assess whether the noise-batch ratio decoupling holds across this range.

3. **Extreme Privacy Budget Probe:** Train at ε=0.5 and ε=16 (beyond the paper's [1, 8] range) to test whether the scaling laws extrapolate correctly. Specifically check whether optimal model sizes converge to non-private scaling laws at high ε and whether training remains stable at very low ε.