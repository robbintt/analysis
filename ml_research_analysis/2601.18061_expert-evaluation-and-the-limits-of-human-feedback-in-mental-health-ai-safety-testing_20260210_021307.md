---
ver: rpa2
title: Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety
  Testing
arxiv_id: '2601.18061'
source_url: https://arxiv.org/abs/2601.18061
tags:
- expert
- disagreement
- evaluation
- mental
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an empirical test of the assumption that expert
  human feedback provides a reliable ground truth signal for AI alignment, focusing
  on the safety-critical domain of mental health. Three certified psychiatrists evaluated
  360 AI-generated mental health responses using a calibrated rubric, producing 1,080
  expert annotations per rater.
---

# Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety Testing

## Quick Facts
- **arXiv ID:** 2601.18061
- **Source URL:** https://arxiv.org/abs/2601.18061
- **Reference count:** 40
- **Primary result:** Expert human feedback showed poor reliability (ICC 0.087-0.295) when evaluating AI-generated mental health responses, challenging assumptions about human feedback as stable ground truth for AI alignment

## Executive Summary
This study challenges a fundamental assumption in AI alignment research: that expert human feedback provides reliable ground truth signals for evaluating AI systems. Three certified psychiatrists evaluated 360 AI-generated mental health responses using a calibrated rubric, producing 1,080 expert annotations per rater. Despite similar training and shared instructions, inter-rater reliability was consistently poor across all evaluation criteria, with disagreement highest on safety-critical items like suicide and self-harm. The findings suggest that in domains with longitudinal consequences and consensus-based standards, human feedback may not provide the stable ground truth assumed by current AI alignment methods.

## Method Summary
Three certified psychiatrists evaluated 360 AI-generated mental health responses using a standardized rubric with 12 safety and quality criteria. Each rater produced 1,080 annotations, creating a dataset of approximately 3,240 total annotations. The study employed rigorous statistical measures including Intraclass Correlation Coefficient (ICC) and Krippendorff's alpha to assess inter-rater reliability. Qualitative interviews with raters were conducted to understand the nature of disagreements, revealing that differences reflected coherent but incompatible clinical frameworks rather than measurement error.

## Key Results
- Inter-rater reliability was consistently poor (ICC 0.087-0.295) across all evaluation criteria
- One safety-critical factor yielded negative reliability (Krippendorff's α=-0.203), indicating structured disagreement worse than chance
- Disagreement was highest on safety-critical items like suicide and self-harm, where consistent evaluation is most crucial
- Qualitative interviews revealed disagreements reflected coherent but incompatible clinical frameworks rather than measurement error

## Why This Works (Mechanism)
The mechanism underlying these findings relates to the fundamental nature of psychiatric practice and evaluation. In mental health, clinical judgments often involve complex trade-offs between competing priorities, interpretation of nuanced patient communications, and application of evolving clinical standards. When AI systems generate responses that require human evaluation, these same complexities transfer to the evaluation process itself. The study demonstrates that even among experts with similar training, the interpretation of safety and quality criteria can vary significantly based on individual clinical frameworks and professional experiences.

## Foundational Learning
1. **Inter-rater reliability metrics** - Why needed: To quantify agreement between experts beyond simple percentage agreement; Quick check: Calculate ICC and Krippendorff's alpha for any expert evaluation dataset
2. **Clinical framework variability** - Why needed: Understanding that expert disagreement may reflect legitimate differences in approach rather than error; Quick check: Conduct qualitative interviews with disagreeing experts to identify framework differences
3. **AI alignment assumptions** - Why needed: Recognizing that human feedback may not provide stable ground truth in complex domains; Quick check: Test inter-rater reliability in other safety-critical domains beyond mental health
4. **Safety-critical evaluation criteria** - Why needed: Identifying where human disagreement poses the greatest risk to system safety; Quick check: Map evaluation criteria by criticality and measure reliability separately

## Architecture Onboarding
**Component Map:** AI Response Generation -> Expert Evaluation Rubric -> Multiple Rater Annotations -> Statistical Reliability Analysis -> Qualitative Framework Analysis

**Critical Path:** The study's critical path follows: AI response generation → standardized rubric application → multiple expert ratings → statistical reliability measurement → qualitative framework analysis. This path ensures both quantitative measurement of disagreement and qualitative understanding of its sources.

**Design Tradeoffs:** The study prioritized methodological rigor over sample diversity, using three expert raters (typical for intensive studies) rather than larger but less expert groups. This tradeoff maximized annotation quality but may limit generalizability.

**Failure Signatures:** Poor inter-rater reliability indicates fundamental challenges in establishing ground truth for AI alignment. Negative reliability values suggest structured disagreement worse than random chance, signaling incompatible evaluation frameworks rather than simple noise.

**First Experiments:**
1. Replicate the study with larger sample sizes (4-6 psychiatrists) to test whether reliability improves with more raters
2. Test inter-rater reliability with non-expert human raters to establish baseline agreement levels
3. Evaluate responses from multiple AI models to determine if model differences affect expert agreement

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of three psychiatrists may not represent full diversity of clinical perspectives
- Predetermined rubric may have artificially amplified disagreement by forcing binary choices
- Single AI model (GPT-4) limits generalizability to other AI systems

## Confidence
- **Empirical finding of poor inter-rater reliability:** High confidence (supported by robust statistical measures including ICC and Krippendorff's alpha)
- **Interpretation of disagreement as coherent frameworks:** Medium confidence (supported by qualitative interviews but small sample size)
- **Broader claim about human feedback limitations:** Medium confidence (extends beyond mental health domain and requires additional validation)

## Next Checks
1. Replicate the study with larger and more diverse groups of mental health professionals, including different specialties and cultural backgrounds
2. Test multiple AI models to determine whether inter-rater disagreement varies by the type of AI-generated responses
3. Conduct longitudinal studies tracking how expert agreement evolves as AI systems and clinical standards both continue to develop