---
ver: rpa2
title: 'EarthSight: A Distributed Framework for Low-Latency Satellite Intelligence'
arxiv_id: '2511.10834'
source_url: https://arxiv.org/abs/2511.10834
tags:
- image
- satellite
- images
- ground
- filter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'EARTHSIGHT is a distributed runtime framework for low-latency
  satellite intelligence that treats onboard image analysis as a distributed decision
  problem between ground and orbit. The framework introduces three core innovations:
  multi-task inference on satellites using shared backbones to amortize computation
  across multiple vision tasks, a ground-station query scheduler that aggregates user
  requests and assigns compute budgets, and dynamic filter ordering that integrates
  model selectivity, accuracy, and execution cost to reject low-value images early.'
---

# EarthSight: A Distributed Framework for Low-Latency Satellite Intelligence

## Quick Facts
- arXiv ID: 2511.10834
- Source URL: https://arxiv.org/abs/2511.10834
- Reference count: 40
- Primary result: Reduces average compute time per image by 1.9x and lowers 90th percentile end-to-end latency from 51 to 21 minutes

## Executive Summary
EARTHSIGHT is a distributed runtime framework for low-latency satellite intelligence that treats onboard image analysis as a distributed decision problem between ground and orbit. The framework introduces three core innovations: multi-task inference on satellites using shared backbones to amortize computation across multiple vision tasks, a ground-station query scheduler that aggregates user requests and assigns compute budgets, and dynamic filter ordering that integrates model selectivity, accuracy, and execution cost to reject low-value images early. EARTHSIGHT leverages global context from ground stations with resource-aware adaptive decisions in orbit, enabling constellations to perform scalable, low-latency image analysis within strict downlink bandwidth and onboard power budgets.

## Method Summary
EARTHSIGHT implements multi-task inference using shared EfficientNet backbones with task-specific MLP heads for satellite image analysis. Ground stations aggregate user queries, compile them into disjunctive normal form (DNF) filter formulas, and simulate future downlink windows to set priority thresholds. On-orbit runtime uses a utility-driven filter ordering heuristic to select the next filter based on execution time, accuracy, and rejection probability. The system dynamically adapts compute aggression through a confidence threshold that responds to onboard power states. Evaluation uses a constellation simulator with PlanetScope Dove satellite data and multiple classification/segmentation datasets.

## Key Results
- Reduces average compute time per image by 1.9x through shared backbone amortization
- Lowers 90th percentile end-to-end latency from first contact to delivery from 51 to 21 minutes
- Enables real-time prioritization within strict downlink bandwidth and onboard power budgets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Shared neural backbones amortize feature extraction costs, reducing the compute energy required for multi-query analysis compared to independent single-task models.
- **Mechanism:** A single feature extractor (backbone) processes the raw image once. Task-specific heads (lightweight classifiers) reuse these shared features rather than reprocessing the image.
- **Core assumption:** The tasks (e.g., ship detection, fire classification) are semantically related enough that a shared representation does not degrade accuracy below acceptable operational limits.
- **Evidence anchors:**
  - [abstract] "...multi-task inference on satellites using shared backbones to amortize computation..."
  - [Section 3.1] "...multi-task inference process is divided into two stages: the shared backbone encodes... task-specific heads then compute..."
  - [corpus] Related work supports the shift to onboard AI, but specific evidence for *shared backbones* in this specific configuration is limited to the paper's internal evaluation.
- **Break condition:** High task heterogeneity (e.g., mixing disparate spectral analyses) causes "negative transfer," degrading head accuracy significantly.

### Mechanism 2
- **Claim:** Utility-driven filter ordering minimizes the expected execution time to reach a confident priority decision, enabling early rejection of low-value images.
- **Mechanism:** Instead of a static order, the system greedily selects the next filter based on a utility score. This score combines execution time, filter accuracy, and the probability of the filter rejecting the image (negative result).
- **Core assumption:** The system possesses reasonable priors (estimates) for filter pass probabilities and execution costs to calculate utility effectively.
- **Evidence anchors:**
  - [abstract] "...integrates model selectivity, accuracy, and execution cost to reject low-value images early..."
  - [Section 3.3] "...utility of a filter fi given a partial execution state E is: U(fi, E) = (1-pi) * tpri * ni / teff(fi, E)"
  - [corpus] Related literature confirms scheduling is a bottleneck, validating the need for optimization, but does not validate this specific heuristic.
- **Break condition:** Real-time filter outcomes deviate significantly from historical priors, causing the heuristic to select expensive, uninformative filters first.

### Mechanism 3
- **Claim:** Decoupling global scheduling (ground) from local execution (orbit) lowers tail latency by ensuring satellites only process data likely to meet downlink bandwidth constraints.
- **Mechanism:** Ground stations simulate future downlink windows to set a dynamic "priority threshold" (p*). Satellites receive this context and adapt their compute aggression (the alpha threshold) to local power states.
- **Core assumption:** The uplink window is sufficient to transmit the compressed schedule, and the ground station's "look-ahead" simulation accurately reflects the satellite's near-term state.
- **Evidence anchors:**
  - [abstract] "...leverages global context from ground stations with resource-aware adaptive decisions in orbit..."
  - [Section 3.2] "To address potential misclassifications... an intermediate priority level pcompute is introduced..."
  - [corpus] Related papers discuss distributed scheduling generally, but specific causal links for this specific ground-orbit feedback loop are internal to the study.
- **Break condition:** Uplink failures or stale global context cause satellites to operate on outdated priority thresholds, leading to bandwidth waste or critical data drop.

## Foundational Learning

- **Concept:** **Stochastic Boolean Function Evaluation (SBFE)**
  - **Why needed here:** The core optimization problem (deciding which filter to run next to prove a complex logical query) is NP-hard. Understanding SBFE explains why the system uses a greedy approximation rather than an optimal solver.
  - **Quick check question:** Why can't the system calculate the mathematically perfect sequence of filters for every image in real-time?

- **Concept:** **Disjunctive Normal Form (DNF)**
  - **Why needed here:** User queries (e.g., "fire AND populated" OR "flood AND infrastructure") are compiled into DNF formulas. The system evaluates these terms to determine if an image is "accepted."
  - **Quick check question:** How does representing a user query in DNF facilitate the "early rejection" of an image?

- **Concept:** **Hard Parameter Sharing**
  - **Why needed here:** This is the architectural strategy for the multi-task model. It explains how the system reduces memory footprint (critical for nanosatellites) at the potential cost of task interference.
  - **Quick check question:** In hard parameter sharing, which part of the network is duplicated per task, and which part is common?

## Architecture Onboarding

- **Component map:**
  - Ground Segment: Query Aggregator -> Look-ahead Simulator -> Schedule Generator (compresses DNFs)
  - Space Segment: Schedule Cache -> Runtime (CPU) -> Filter Engine (xPU/TPU) -> Downlink Queue
  - Data Flow: User Query -> Ground (DNF) -> Uplink -> Satellite Cache -> Image Capture -> Filter Loop -> Prioritization

- **Critical path:** The **Filter Loop** on the satellite. If the CPU-xPU pipeline (Figure 4) stalls or the filter selection heuristic mispredicts, the latency gains are lost.

- **Design tradeoffs:**
  - **Memory vs. Accuracy:** Larger backbones (e.g., B1/B2 vs B0) improve feature sharing but consume scarce satellite memory (Figure 5)
  - **Approximation vs. Optimality:** The scheduler uses a greedy heuristic (approximation) for filter ordering because the exact SBFE solution is exponential in time (Table 5)
  - **False Positives vs. False Negatives:** The system biases toward false positives (transmitting low-priority images) to strictly avoid missing critical data (Section 3.2)

- **Failure signatures:**
  - **High Tail Latency:** Likely caused by the "look-ahead" simulator failing to predict bandwidth bottlenecks, forcing fallback to FIFO
  - **Power Drain:** Dynamic threshold $\alpha$ failing to adapt to low power states, causing sustained high-compute cycles

- **First 3 experiments:**
  1.  **Profile Filter Utility:** Run the onboard runtime on a representative image stream (Jetson/Coral) to verify that the utility heuristic $U(f_i, E)$ actually reduces filter counts compared to a static ordering
  2.  **Stress Test Schedule Compression:** Measure the time to decompress and load the "byte-indexed" DNF schedule on the target hardware to ensure it fits within the contact window
  3.  **Validate Dynamic Thresholding:** Simulate a power drop event and verify that the adaptive threshold $\alpha_t$ correctly relaxes selectivity to preserve battery (Eq 1)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can mixed-precision frameworks be integrated into EarthSight to further scale analytical abilities?
- Basis in paper: [explicit] The Conclusion states future work "may explore mixed-precision frameworks for neural analysis of images or analytics with different stopping points."
- Why unresolved: The current implementation relies on standard precision models (e.g., INT8) and does not dynamically adjust numerical precision to optimize the accuracy-energy-latency trade-off during inference.
- What evidence would resolve it: Empirical results showing latency or energy reductions from a mixed-precision runtime handling the shared backbone and task heads under identical power budgets.

### Open Question 2
- Question: Does soft parameter sharing offer better accuracy-efficiency trade-offs than EarthSight's hard parameter sharing for highly heterogeneous tasks?
- Basis in paper: [inferred] The paper selects hard parameter sharing for "greater computational efficiency" but acknowledges soft sharing exists, implying the performance gap for diverse satellite tasks is not fully closed.
- Why unresolved: Hard sharing may underutilize representations for unrelated tasks (e.g., maritime vs. desert), but the memory overhead of soft sharing on constrained devices (like the Coral TPU) is unexplored.
- What evidence would resolve it: Benchmarks comparing accuracy drops and memory footprints of soft vs. hard sharing strategies on the specified edge hardware (Jetson/Coral).

### Open Question 3
- Question: How does the greedy filter ordering heuristic perform as the complexity of query formulas scales significantly beyond the evaluated limit of 15 filters?
- Basis in paper: [inferred] The comparison to the optimal exact solution is restricted to formulas with $\le 15$ filters because the exact solution is NP-hard and intractable for larger sets.
- Why unresolved: It is unclear if the observed ~7.5% sub-optimality remains stable or degrades as the filter count increases for complex, multi-domain global monitoring.
- What evidence would resolve it: Analysis of heuristic performance relative to bounded approximation algorithms on synthetic formulas with 50+ filters.

## Limitations
- Relies on idealized assumptions about sensor coverage, query arrival patterns, and uplink/downlink windows
- Performance gains depend heavily on scenario-specific hyperparameters and simulation fidelity
- Limited real-world validation on actual satellite hardware with live imagery

## Confidence
- **Mechanism Claims:** Medium-High - The architectural innovations (shared backbones, utility-based ordering) are well-grounded in literature and internal evaluation
- **Absolute Performance Claims:** Medium - Latency reduction numbers are based on controlled constellation simulations that may not fully capture real-world variability
- **Scalability Claims:** Medium - Performance with complex queries (>15 filters) and highly heterogeneous tasks remains untested

## Next Checks
1. Deploy EARTHSIGHT on a real nanosatellite with a small camera and TPU-like accelerator; measure actual power draw and filter ordering efficacy on live imagery
2. Conduct an ablation study comparing the utility heuristic against both random and accuracy-ordered filter selection across diverse query workloads
3. Test robustness by injecting uplink failures and degraded ground station availability into the constellation simulator; quantify the impact on tail latency and missed high-priority captures