---
ver: rpa2
title: Advancing General-Purpose Reasoning Models with Modular Gradient Surgery
arxiv_id: '2602.02301'
source_url: https://arxiv.org/abs/2602.02301
tags:
- gradient
- training
- arxiv
- math
- chat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Modular Gradient Surgery (MGS) resolves cross-domain interference
  in multi-task reinforcement learning by applying gradient conflict resolution at
  the module level within transformers, rather than globally. Applied to Llama and
  Qwen models across math, chat, and instruction-following domains, MGS achieves average
  improvements of 4.3 (16.6%) and 4.5 (11.1%) points respectively over standard multi-task
  RL baselines.
---

# Advancing General-Purpose Reasoning Models with Modular Gradient Surgery

## Quick Facts
- arXiv ID: 2602.02301
- Source URL: https://arxiv.org/abs/2602.02301
- Reference count: 40
- Primary result: Modular Gradient Surgery achieves 4.3-4.5 point average improvements over standard multi-task RL baselines by resolving cross-domain interference at the module level

## Executive Summary
Modular Gradient Surgery (MGS) addresses cross-domain interference in multi-task reinforcement learning by applying gradient conflict resolution at the transformer module level rather than globally. Applied to Llama and Qwen models across math, chat, and instruction-following domains, MGS achieves average improvements of 4.3 (16.6%) and 4.5 (11.1%) points respectively over standard multi-task RL baselines. The approach maintains effectiveness under prolonged training and generalizes to additional tasks, achieving a 19.4% relative average improvement on Llama-3.1-8B.

## Method Summary
MGS partitions transformer parameters into modules (attention, MLP, LayerNorm by layer) and applies PCGrad projection locally. When gradients from two tasks conflict within a module (negative dot product), MGS projects one gradient onto the orthogonal complement of the other. This preserves compatible gradient directions while removing destructive components, allowing different modules to specialize for different domain requirements without monolithic interference.

## Key Results
- MGS achieves average improvements of 4.3 (16.6%) and 4.5 (11.1%) points over standard multi-task RL baselines on Llama and Qwen models respectively
- LayerNorm modules contribute disproportionately to cross-domain gradient conflicts despite containing fewest parameters
- MGS generalizes to three-task training (adding instruction following) with 19.4% relative average improvement on Llama-3.1-8B

## Why This Works (Mechanism)

### Mechanism 1: Module-Level Gradient Conflict Resolution
MGS resolves gradient conflicts at the module level within transformer architectures rather than globally. By partitioning parameters into modules (attention, MLP, LayerNorm by layer) and applying PCGrad projection locally, MGS preserves compatible gradient directions while removing destructive components. This allows different modules to specialize for different domain requirements without monolithic interference.

### Mechanism 2: Sequential RL Mode Interference
Sequential RL suffers from asymmetric "Mode Interference" comprising Forgetting (performance degradation on previously trained domains) and Rigidity (constrained learning on subsequent domains). Math training reduces entropy, creating a "rigid" low-entropy prior that constrains exploration in subsequent Chat training. Chat training increases entropy, creating a more permissive foundation for Math but causes severe forgetting when Chat is trained first.

### Mechanism 3: LayerNorm Conflict Sensitivity
LayerNorm modules, despite containing fewest parameters, contribute disproportionately to cross-domain gradient conflicts and model behavior. LayerNorm regulates feature statistics across the embedding space rather than encoding semantic functions. Conflicts in LayerNorm propagate broadly through the network, affecting all subsequent layers' activations.

## Foundational Learning

- **PCGrad (Gradient Surgery)**: MGS builds directly on PCGrad's conflict detection and projection mechanism, extending it from global to module-level application. Quick check: Can you explain when two task gradients are considered "conflicting" and how PCGrad resolves this?

- **Transformer Modularity (Attention vs MLP Specialization)**: MGS exploits the observation that MLP layers handle knowledge storage while attention layers handle information routing—conflicts manifest differently across these modules. Quick check: What functional differences between attention and MLP layers might cause different gradient conflict patterns?

- **GRPO (Group Relative Policy Optimization)**: The paper uses token-level GRPO as its RL objective; understanding advantage normalization and clipping is essential for debugging training dynamics. Quick check: How does GRPO normalize advantages within groups, and why might this interact with multi-domain gradient conflicts?

## Architecture Onboarding

- **Component map**:
```
Input: Batch B with K task domains
  ↓
Per-task gradient computation: gk = ∇θL(τ;θ) for each task k
  ↓
Module partition: θ = ∪ϕ∈Φ ϕ (e.g., per-layer Attn/MLP/LayerNorm)
  ↓
Per-module conflict detection: For each module ϕ, check if vi · vj < 0
  ↓
Per-module projection: gPC_i[ϕ] ← projection onto orthogonal complement
  ↓
Gradient aggregation: Δθ = Σi gPC_i
  ↓
Parameter update
```

- **Critical path**:
  1. Correctly partition `model.named_parameters()` into disjoint modules
  2. Compute per-task gradients before any aggregation
  3. Apply PCGrad projection independently per-module (not globally)
  4. Sum resolved gradients before optimizer step

- **Design tradeoffs**:
  - **Module granularity**: Per-layer vs. per-component (Attn/MLP/LN) vs. global—finer granularity preserves more information but increases O(M·T²) time overhead
  - **Conflict threshold**: Negative dot product is conservative; near-zero positive similarity may also indicate interference
  - **Memory overhead**: O(T×N) gradient storage required before resolution; mitigated by FSDP sharding to O(T×N/world_size)

- **Failure signatures**:
  - **Excessive conservativism**: If all gradients are projected away, training stalls—monitor gradient norm ratios across tasks
  - **Persistent interference**: If MGS doesn't improve over Naïve Mixing, check module partition correctness
  - **Single-domain collapse**: If one domain dominates, gradient norm imbalance exists beyond conflict resolution

- **First 3 experiments**:
  1. **Baseline validation**: Reproduce Naïve Mixing gradient cosine similarity curves (Figure 5) on your task mix to confirm conflicts exist
  2. **Module ablation**: Run MGS with LayerNorm excluded vs. included to verify the paper's finding that LayerNorm conflicts are critical
  3. **Training order test**: Compare Chat→Math vs. Math→Chat with Sequential RL to observe entropy-driven rigidity in your setting before applying MGS

## Open Questions the Paper Calls Out

### Open Question 1
Can gradient norm magnitude serve as a principled heuristic for selectively applying gradient surgery to specific modules, rather than applying MGS uniformly across all modules? The paper suggests this warrants further investigation after observing that high-norm modules boost Chat but hurt Math performance.

### Open Question 2
Why do conflicts in LayerNorm parameters lead to disproportionately severe performance degradation compared to attention or MLP modules? The paper speculates this may relate to LayerNorm regulating embedding space statistics, but no mechanistic validation is provided.

### Open Question 3
Does MGS generalize effectively to domains beyond math, chat, and instruction following—such as code reasoning, scientific QA, or multimodal tasks? The paper acknowledges this remains unexplored while systematic evaluation across a broader domain spectrum is needed.

## Limitations

- **Theoretical understanding incomplete**: While MGS demonstrates empirical effectiveness, the theoretical understanding of why module-level gradient surgery works better than global approaches remains limited.
- **Architecture-specific findings**: The finding that LayerNorm conflicts are particularly critical appears architecture-dependent and may not generalize to other transformer variants.
- **Mechanism validation scope**: The entropy-rigidity mechanism explanation for sequential RL failure modes is supported only by internal analysis without external validation.

## Confidence

- **High confidence**: Empirical improvements of MGS over baselines (4.3 and 4.5 point average gains) are well-supported by the experimental results across multiple datasets and model scales.
- **Medium confidence**: The module-level gradient conflict hypothesis is supported by experimental evidence but lacks comprehensive theoretical grounding.
- **Low confidence**: The entropy-rigidity mechanism explanation for sequential RL failure modes remains speculative without external validation.

## Next Checks

1. **Cross-architecture validation**: Test MGS on transformer architectures with different normalization strategies (RMSNorm, GroupNorm) to determine whether LayerNorm-specific findings generalize or are architecture-dependent.

2. **Module granularity ablation**: Systematically vary the module partitioning strategy (per-layer, per-component, mixed granularity) to identify the optimal balance between conflict resolution effectiveness and computational overhead.

3. **Alternative conflict resolution comparison**: Compare MGS against other gradient conflict resolution methods (GradDrop, PCGrad variants) in the same multi-domain RL setting to isolate whether the improvements come from module-level application specifically or from conflict resolution in general.