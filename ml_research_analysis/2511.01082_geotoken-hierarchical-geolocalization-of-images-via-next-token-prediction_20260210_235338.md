---
ver: rpa2
title: 'GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction'
arxiv_id: '2511.01082'
source_url: https://arxiv.org/abs/2511.01082
tags:
- image
- geotoken
- sequence
- location
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GeoToken, a hierarchical sequence prediction
  approach for worldwide image geolocalization. The method treats geographic coordinates
  as sequences of S2 cells at multiple resolution levels, and autoregressively predicts
  them token-by-token using a transformer model conditioned on image and retrieved
  context.
---

# GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction

## Quick Facts
- **arXiv ID**: 2511.01082
- **Source URL**: https://arxiv.org/abs/2511.01082
- **Reference count**: 34
- **Primary result**: Hierarchical S2-cell token prediction achieves SOTA geolocalization accuracy, up to 13.9% gain over prior methods at 1 km threshold, especially in MLLM-free settings.

## Executive Summary
GeoToken introduces a hierarchical sequence prediction approach for worldwide image geolocalization. It treats geographic coordinates as sequences of S2 cells at multiple resolution levels and autoregressively predicts them token-by-token using a transformer model conditioned on image and retrieved context. To handle uncertainty, the model generates multiple candidates during decoding, which are later selected via log-probability, similarity, or an MLLM-based judge. Evaluated on IM2GPS3K and YFCC4K, GeoToken achieves state-of-the-art accuracy in both MLLM-free (up to 13.9% gain) and MLLM-augmented settings, with strong privacy benefits due to fully local inference.

## Method Summary
GeoToken converts GPS coordinates into 21-token S2 sequences (1 face token + 20 quad tokens) and predicts them autoregressively using a transformer decoder. The method uses a two-stage training pipeline: first, a Geo-Alignment phase pretrains image, GPS, and text encoders with InfoNCE contrastive loss to align embeddings; second, a GeoToken phase trains the transformer to generate S2 tokens conditioned on the query image and top-15 retrieved neighbors (their embeddings and tokens). Inference samples K=30 candidates and re-ranks them via log-prob, similarity, or an MLLM judge.

## Key Results
- Achieves 16.8% accuracy at 1 km on IM2GPS3K in MLLM-free setting (vs. 12.9% prior SOTA).
- Reaches 19.0% accuracy at 1 km when augmented with MLLM-based selection (13.9% gain over MLLM-free).
- Maintains strong performance across long-range thresholds (≥25 km), with accuracy gains of 7.8% over baselines in MLLM-free setting.

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Decomposition of Coordinate Space
If coordinates are represented as a sequence of discrete spatial tokens (S2 cells) rather than continuous regression targets or flat classes, the model can leverage the compositional structure of geography (broad region → specific cell) to manage the large search space. The model converts a GPS coordinate into a token sequence T = [t_0, ..., t_{L-1}] and predicts autoregressively, modeling the conditional probability P(t_s | t_{<s}, Image). This mirrors the "coarse-to-fine" reasoning process, where early predictions constrain the search space for later, finer-grained predictions.

### Mechanism 2: In-Context Spatial Grounding via Retrieval
Conditioning the generative model on retrieved visual neighbors and their ground-truth location tokens likely provides a non-parametric "hint" that grounds the prediction, reducing hallucination in data-sparse regions. A retrieval step fetches M neighbors. The Transformer encoder input is constructed as a sequence: X = [v_q ⊕ v_{q,1} ⊕ t_{q,1} ...]. This forces the model to attend to both the query features and the specific geographic "exemplars" before generating the target sequence.

### Mechanism 3: Inference-Time Scaling via Candidate Selection
Treating localization as a generative process allows for sampling multiple diverse hypotheses at inference time, enabling a secondary selection phase to correct for early cascading errors in the token sequence. Instead of greedy decoding, the model samples K candidate sequences (e.g., 30 candidates). A selection strategy (e.g., MLLM-as-Judge or Similarity) evaluates these candidates post-hoc to select the final output.

## Foundational Learning

- **Concept: S2 Geometry & Hierarchical Indexing**
  - **Why needed here**: You cannot understand the model's output without understanding how 2D coordinates are mapped to a 1D token hierarchy. GeoToken uses S2 cells (level 0-20) rather than semantic labels (City/Country).
  - **Quick check question**: If two locations share the first 10 tokens but differ at token 11, are they in the same country? (Answer: Likely yes, they are in the same high-level cell but different sub-cells.)

- **Concept: Contrastive Learning (InfoNCE)**
  - **Why needed here**: This is the pretraining phase ("Geo-Alignment"). It aligns image embeddings with GPS/Text embeddings to make the retrieval step possible.
  - **Quick check question**: What is the primary objective of the InfoNCE loss in this architecture? (Answer: To maximize cosine similarity between an image and its own GPS coordinates/text while minimizing similarity with negatives.)

- **Concept: Autoregressive Generation (Next Token Prediction)**
  - **Why needed here**: The core inference engine is a Transformer Decoder generating a sequence. Understanding the difference between classification (one-shot) and generation (sequential) is critical.
  - **Quick check question**: During inference, does the prediction of token t_5 depend on t_0 through t_4? (Answer: Yes, via the causal mask and autoregressive factorization.)

## Architecture Onboarding

- **Component map**: CLIP ViT-L/14 (Frozen) + Trainable Projection Heads -> Top-M Retrieval Neighbors -> 10-layer Encoder-Decoder Transformer (d_model=512) -> Linear Head to S2 Vocabulary

- **Critical path**: Geo-Alignment (Pretraining): Train encoders to align Image ↔ GPS/Text -> Sequence Construction: For a query, retrieve neighbors → Concatenate [Query Emb, Neighbor Emb, Neighbor Tokens] -> Generative Training: Encoder processes context; Decoder learns P(t_s | t_{<s}) using weighted Cross-Entropy (higher weight for coarse tokens) -> Inference: Sample K=30 sequences → Rerank/Select

- **Design tradeoffs**:
  - MLLM-Free vs. MLLM-Augmented: MLLM-free preserves privacy and reduces cost but drops accuracy (e.g., IM2GPS3K 1km accuracy drops from 19.0% to 16.8%).
  - Temperature Scaling: T < 1 sharpens distribution (risk of false confidence); T > 1 flattens it (risk of incoherent sequences). Paper finds T ≈ 0.7 optimal.
  - Weighted Loss: Coarse levels are weighted higher (w_t = 2.0 - t/20) to ensure the "country" is correct before the "street."

- **Failure signatures**:
  - Early Token Error: If the model predicts the wrong S2 face (token 0), the entire subsequent sequence is geographically invalid (e.g., predicting USA when the image is in France).
  - Retrieval Noise: If neighbors come from a visually similar but geographically distant location (e.g., two distinct Gothic cathedrals), the generation may be pulled toward the wrong region.

- **First 3 experiments**:
  1. Sanity Check (Greedy): Run the MLLM-free version (Greedy decoding) on IM2GPS3K to verify the baseline SOTA claims (Target: ~16.8% @ 1km).
  2. Ablation (Context): Remove the retrieved neighbor tokens from the encoder input and measure the drop in accuracy to quantify the value of RAG (Retrieval-Augmented Generation).
  3. Hyperparameter Sweep (Selection): Generate a pool of 30 candidates and compare "Log-Probability Selection" vs. "CLIP Similarity Selection" to see which better correlates with ground truth (Hint: Paper suggests simple similarity often underperforms log-prob).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can candidate selection strategies be improved to close the performance gap between current methods (e.g., MLLM-as-a-Judge) and the theoretical "Ideal Selector"?
- **Basis in paper**: [explicit] The authors state in Section IV.E.2 that "a significant gap remains between our best selector and the 'Ideal Selector'... suggesting great potential for future work on more powerful selection mechanisms."
- **Why unresolved**: While the model generates a high-quality pool of candidates, current selection strategies fail to reliably identify the best candidate among them.
- **What evidence would resolve it**: A new selection mechanism that significantly increases accuracy metrics (e.g., Acc@1km) by more consistently choosing the closest candidate from the pool, narrowing the gap to the "Ideal Selector" baseline.

### Open Question 2
- **Question**: To what extent does the integration of auxiliary modalities, such as timestamps or satellite imagery, enhance robustness in underrepresented regions?
- **Basis in paper**: [explicit] Section VI (Conclusion) invites "future work that... integration of additional modalities (e.g., timestamps, low-resolution satellite imagery) or more advanced retrieval schemes, further enhancing robustness in underrepresented regions."
- **Why unresolved**: The current GeoToken framework relies primarily on image embeddings and GPS context; the specific impact of temporal or overhead data on the hierarchical token prediction is unexplored.
- **What evidence would resolve it**: Experimental results showing improved accuracy on datasets specifically containing rural or sparse-image areas (like IM2GPS3K) when temporal or satellite data streams are added to the model input.

### Open Question 3
- **Question**: How robust is the hierarchical generation process when the retrieval step fails to return visually similar neighbors (e.g., in zero-shot regions)?
- **Basis in paper**: [inferred] Section III.C details the reliance on retrieved neighbors to ground the decoder, but the ablations do not explicitly measure performance degradation when the retrieval gallery has no near-duplicates of the query image.
- **Why unresolved**: The model conditions its predictions on retrieved context; it is unclear if the autoregressive transformer can maintain high accuracy solely via its parametric knowledge when the non-parametric retrieval fails.
- **What evidence would resolve it**: An ablation study evaluating GeoToken's performance on a dataset of locations strictly excluded from the retrieval gallery, comparing "retrieval-augmented" vs. "retrieval-free" inference.

## Limitations
- **Evaluation granularity bias**: Reported gains rely heavily on long-range thresholds (≥25 km), where coarse geographic estimates pass; accuracy drops at tighter thresholds (≤1 km).
- **Retrieval dependency**: Performance is contingent on retrieval gallery quality; sparse coverage in rural/remote areas may introduce geographic distractors.
- **Unvalidated token semantics**: The paper does not validate whether intermediate S2 tokens correspond to meaningful geographic units or explore alternative spatial decompositions.

## Confidence
- **High Confidence**: The core architectural design (autoregressive generation over S2 tokens with context conditioning) is technically sound and well-documented.
- **Medium Confidence**: The reported accuracy improvements are credible within the evaluation setup, but the reliance on long-range thresholds and the lack of per-level error analysis introduce uncertainty about practical gains at fine scales.
- **Low Confidence**: The paper's claims about privacy benefits (fully local inference) and the scalability of the candidate selection strategy (especially with MLLM judges) are asserted but not empirically validated across diverse deployment scenarios or cost analyses.

## Next Checks
1. **Per-Level Error Analysis**: Segment the evaluation results by S2 token level (e.g., accuracy at tokens 0–5 vs. 10–15 vs. 16–20) to reveal whether errors cascade from coarse to fine levels and identify the bottleneck in the generation process.
2. **Retrieval Coverage Audit**: Analyze the MP16-Pro gallery for geographic coverage gaps by plotting retrieval density per continent/country. Simulate the effect of missing regions by masking parts of the gallery and measuring accuracy drop, quantifying the method's sensitivity to retrieval quality.
3. **Alternative Spatial Tokenizations**: Re-run the model with a different spatial decomposition (e.g., quad-tree cells at only levels 0–10, or a semantic hierarchy like Country→City→District) to test whether the gains are specific to S2 or generalizable to other hierarchical spatial representations.