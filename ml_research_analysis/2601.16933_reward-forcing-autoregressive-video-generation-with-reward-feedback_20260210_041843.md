---
ver: rpa2
title: 'Reward-Forcing: Autoregressive Video Generation with Reward Feedback'
arxiv_id: '2601.16933'
source_url: https://arxiv.org/abs/2601.16933
tags:
- video
- diffusion
- generation
- reward
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Reward-Forcing, an autoregressive video generation
  method that uses reward signals instead of distillation from bidirectional teachers.
  The key insight is that diffusion models generate coarse global structures before
  refining texture details, so motion can be learned through ODE trajectories while
  reward models handle texture refinement.
---

# Reward-Forcing: Autoregressive Video Generation with Reward Feedback

## Quick Facts
- arXiv ID: 2601.16933
- Source URL: https://arxiv.org/abs/2601.16933
- Authors: Jingran Zhang; Ning Li; Yuanhao Ban; Andrew Bai; Justin Cui
- Reference count: 40
- One-line primary result: Achieves 84.92 VBench total score without extensive teacher distillation

## Executive Summary
This paper introduces Reward-Forcing, an autoregressive video generation method that uses reward signals instead of distillation from bidirectional teachers. The key insight is that diffusion models generate coarse global structures before refining texture details, so motion can be learned through ODE trajectories while reward models handle texture refinement. The method first uses ODE trajectories from a teacher model to learn motion dynamics, then applies reward feedback to enhance texture quality. On VBench, this approach achieves a total score of 84.92, closely matching state-of-the-art autoregressive methods (84.31) while avoiding the need for extensive heterogeneous distillation.

## Method Summary
Reward-Forcing is a two-stage training framework that converts bidirectional video diffusion models into autoregressive generators. First, it distills a bidirectional teacher (Wan2.1-1.3B) to a 4-step generator using Distribution Matching Distillation. Then, it samples ~1.4K ODE trajectories from the teacher to pretrain the autoregressive student with L2 loss, learning motion dynamics. Finally, it fine-tunes with reward loss on the last frame only, using ImageReward to enhance texture quality. The approach is data-free after ODE initialization and achieves competitive performance on VBench while avoiding the computational burden of extensive teacher distillation.

## Key Results
- Achieves 84.92 VBench total score, close to SOTA autoregressive methods (84.31)
- Surpasses diffusion distillation baselines (81.15) and combined reward+distillation (82.55)
- Strong performance on individual metrics: aesthetic quality (70.51) and dynamic degree (81.94)
- Demonstrates effective decoupling of motion learning from texture refinement

## Why This Works (Mechanism)

### Mechanism 1: Sequential Learning of Motion Before Texture
- Claim: Diffusion models generate coarse structure (motion) before texture, enabling staged training that first establishes temporal dynamics, then refines appearance.
- Mechanism: ODE trajectory sampling from a bidirectional teacher provides motion priors to the autoregressive student. The student learns to generate temporally consistent sequences through L2 loss on sampled trajectories before any reward supervision begins.
- Core assumption: Motion and texture are sufficiently separable that motion can be learned in isolation without destabilizing later texture refinement.
- Evidence anchors:
  - [abstract] "uses ODE-based trajectory sampling for motion initialization and refines texture quality through reward model supervision"
  - [Section 3.2] "we empirically observe that the model has learned to generate consistent motions without much texture info" after ODE training
  - [Section 4.6] "ODE-based initialization provides a compact motion prior, enabling smooth dynamics with only a small set of teacher trajectories"
  - [corpus] Weak direct evidence; related work on motion disentanglement exists (Motion-I2V mentioned) but no direct corpus validation of this specific decoupling

### Mechanism 2: Last-Frame Reward Supervision Preserves Motion
- Claim: Applying reward loss only to the final frame of generated video preserves motion quality better than multi-frame supervision.
- Mechanism: ImageReward evaluates static visual-textual alignment without temporal awareness. Supervising random frames incentivizes the model to prioritize texture over motion, producing static outputs. Last-frame supervision places the reward gradient at the trajectory's end, allowing preceding frames to maintain learned motion dynamics.
- Core assumption: The reward model's lack of temporal understanding makes it a poor guide for motion; positioning supervision at the trajectory end minimizes interference with motion generation.
- Evidence anchors:
  - [Section 3.2] "supervising more frames encourages static content with reduced motion, while supervising the last frame preserves motion better"
  - [Section 5.2] "supervising randomly selected frames results in a degradation in motion quality, with the motion degree dropping by more than 10 percentage points"
  - [corpus] No direct corpus evidence for this specific frame-selection strategy

### Mechanism 3: Reward-Distillation Objective Conflict Avoidance
- Claim: Separating distillation (motion learning) from reward optimization (texture refinement) avoids objective conflict that degrades performance.
- Mechanism: Distillation loss explicitly aligns student output distribution with teacher distribution. Reward loss incentivizes outputs that diverge from teacher to maximize reward scores. Joint optimization creates gradient interference that weakens both objectives.
- Core assumption: The teacher distribution and high-reward outputs are not perfectly aligned; forcing alignment to both simultaneously creates contradictory learning signals.
- Evidence anchors:
  - [Section 5.1] Combined reward + distillation achieves 82.55 total score vs. 84.92 for sequential approach
  - [Section 5.1] "This divergence undermines the distribution alignment objective, weakening the distillation process"
  - [corpus] No corpus papers explicitly validate this decoupling; ROCM applies RLHF to consistency models with combined objectives, suggesting context-dependence

## Foundational Learning

- **Flow Matching and Probability-Flow ODEs**
  - Why needed here: The method uses Wan2.1 trained with flow matching; understanding how ODE solvers generate trajectories from noise is essential for implementing motion initialization.
  - Quick check question: Can you explain why DDIM/ODE sampling requires fewer steps than DDPM while remaining deterministic?

- **Distribution Matching Distillation (DMD)**
  - Why needed here: The baseline and initialization use DMD to create few-step generators; understanding the KL divergence formulation explains why teacher constraints matter.
  - Quick check question: In DMD, what does the "fake" score network approximate versus the "real" score network?

- **Reward Models for Generative Alignment**
  - Why needed here: ImageReward provides the scalar supervision signal; understanding how reward models encode human preferences (and their limitations) informs selection of appropriate reward functions.
  - Quick check question: Why might an image-based reward model fail to capture temporal coherence in video generation?

## Architecture Onboarding

- **Component map:**
  - Teacher: Wan2.1-1.3B (bidirectional, flow-matching trained)
  - Student: Autoregressive variant with causal attention (same 1.3B params)
  - ODE Sampler: Generates ~1.4K trajectory samples from teacher (offline, one-time)
  - Reward Model: ImageReward (frozen, provides scalar feedback on final frame)
  - Training: Two-stage (ODE motion pretraining → reward fine-tuning)

- **Critical path:**
  1. Distill teacher to 4-step model via DMD
  2. Sample ODE trajectories for motion initialization (L_ode loss)
  3. Fine-tune with reward loss on last frame only
  4. Enable EMA (decay 0.99) throughout

- **Design tradeoffs:**
  - ODE trajectory quantity vs. training efficiency: Paper uses 1.4K trajectories; fewer may insufficiently cover motion space
  - Reward model selection: ImageReward optimizes for texture/aesthetics but ignores motion; video-specific reward models (VideoAlign, VisionReward) could improve temporal metrics
  - Frame supervision strategy: Last-frame-only preserves motion but may underutilize reward signal on longer sequences

- **Failure signatures:**
  - Static/low-motion outputs: Likely random-frame reward supervision or excessive distillation
  - Texture degradation: Insufficient reward training or reward model misalignment with target domain
  - Training instability: Combining distillation + reward losses (use sequential approach instead)
  - Motion incoherence: ODE trajectory sampling insufficient or teacher quality issues

- **First 3 experiments:**
  1. **Reproduce ODE-only baseline:** Train student on 1.4K ODE trajectories with L_ode only; verify motion emerges (VBench dynamic degree) but texture lags (aesthetic quality ~62 vs 70)
  2. **Last-frame vs. random-frame ablation:** Compare reward supervision on last frame vs. random frames; expect >10 point dynamic degree drop with random frames
  3. **Alternative reward models:** Swap ImageReward for VisionReward or VideoAlign; measure impact on motion-specific metrics (dynamic degree, motion smoothness) vs. texture metrics (aesthetic quality)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a purely reward-driven learning paradigm completely eliminate the need for initial ODE-based trajectory sampling in autoregressive video generation?
- Basis in paper: [explicit] Section 6 (Conclusion & Future Work) suggests investigating the "possibility of eliminating the initial ODE training phase entirely in favor of a reward-driven learning paradigm."
- Why unresolved: The current framework relies on a small set of teacher ODE trajectories (1.4K) to initialize motion dynamics before applying reward feedback.
- What evidence would resolve it: Successful training of an autoregressive model using only reward signals from random initialization, achieving comparable VBench scores without ODE pre-training.

### Open Question 2
- Question: Does using video-specific reward models improve temporal consistency compared to the image-based ImageReward used in the current study?
- Basis in paper: [explicit] Section 6 proposes exploring "alternative reward models, including both image- and video-based rewards," while Section 7 notes "some inconsistency in certain videos" due to limited teacher supervision.
- Why unresolved: The paper identifies that ImageReward lacks a notion of temporal continuity, potentially prioritizing static features over coherent motion.
- What evidence would resolve it: An ablation study replacing ImageReward with a video-aware reward model (e.g., VideoReward) showing improved temporal consistency scores without sacrificing texture quality.

### Open Question 3
- Question: Does the reward-forcing optimization framework provide performance benefits when applied to standard bidirectional video diffusion models?
- Basis in paper: [inferred] Section 7 (Limitations) states the authors "did not apply our methods to regular models," focusing solely on autoregressive variants.
- Why unresolved: It is undetermined if the decoupling of motion and texture refinement via rewards is beneficial for non-autoregressive architectures that already possess global context.
- What evidence would resolve it: Application of the reward-forcing pipeline to a baseline bidirectional model (e.g., Wan2.1) demonstrating improvements in aesthetic or quality scores.

## Limitations

- Method depends on specific bidirectional teacher architecture that enables ODE trajectory sampling
- Uses image-based reward model that cannot capture temporal coherence, potentially limiting motion quality
- Unknown sensitivity to trajectory sampling quantity and quality - 1.4K may be suboptimal
- Only tested on autoregressive models, not standard bidirectional diffusion models

## Confidence

- **High confidence**: Sequential training (ODE → reward-only) outperforms combined optimization; last-frame reward supervision preserves motion better than random-frame supervision
- **Medium confidence**: Motion-texture separation enables staged learning; 1.4K ODE trajectories provide sufficient motion coverage
- **Low confidence**: Objective conflict between distillation and reward optimization is fundamental rather than implementation-dependent

## Next Checks

1. **Trajectory Sampling Coverage Analysis**: Systematically vary the number of ODE trajectories (100, 500, 1.4K, 5K) and measure motion quality saturation point to determine if 1.4K is optimal or excessive

2. **Motion-Specific Reward Modeling**: Replace ImageReward with video-aware reward models (VideoAlign, VisionReward) and measure impact on motion-specific VBench metrics while tracking any trade-offs with texture quality

3. **Teacher Model Dependency**: Test the method with alternative bidirectional video diffusion models (e.g., Stable Video Diffusion) to verify the approach generalizes beyond Wan2.1 architecture