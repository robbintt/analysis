---
ver: rpa2
title: Towards SFW sampling for diffusion models via external conditioning
arxiv_id: '2505.08817'
source_url: https://arxiv.org/abs/2505.08817
tags:
- diffusion
- images
- content
- prompts
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of preventing diffusion models
  from generating unsafe content like violence or nudity. The proposed solution introduces
  a Safe-for-Work (SFW) sampler that uses external conditioning to guide samples away
  from undesired regions in the latent space.
---

# Towards SFW sampling for diffusion models via external conditioning

## Quick Facts
- **arXiv ID:** 2505.08817
- **Source URL:** https://arxiv.org/abs/2505.08817
- **Reference count:** 35
- **Primary result:** A Safe-for-Work (SFW) sampler reduces NSFW content generation from 15.93% to 5.26% for sexual content detection while maintaining image quality on safe prompts.

## Executive Summary
This paper introduces a Safe-for-Work (SFW) sampler for diffusion models that prevents generation of unsafe content like violence or nudity. The method uses external conditioning via CLIP embeddings to calculate a harmfulness density and perturb the denoising trajectory accordingly. During inference, the sampler calculates gradients of the harmfulness density with respect to the predicted clean image and applies gradient descent steps to push samples away from harmful regions in the latent space. Experiments with Stable Diffusion show the SFW sampler effectively reduces NSFW content generation while maintaining quality for safe prompts.

## Method Summary
The SFW sampler modifies the standard DDIM denoising process by introducing a Conditional Trajectory Correction (CTC) step. After each denoising step, the method calculates the predicted clean image $\hat{x}_0$ and uses a CLIP-based harmfulness density $p_h$ to assess potential unsafe content. If $p_h$ exceeds a threshold $\eta$, the sampler computes the gradient $\nabla_{\hat{x}_0} \log p_h$ and perturbs $\hat{x}_0$ to redirect the sampling trajectory away from harmful regions. This perturbation is designed to preserve the image manifold while reducing NSFW content generation. The method claims negligible effect on safe prompts while significantly reducing unsafe content.

## Key Results
- NSFW content generation reduced from 15.93% to 5.26% for sexual content detection (HP)
- Inappropriate content detection reduced from 30.81% to 26.63% (HP)
- Minimal degradation on safe prompts (COCO dataset CLIP score dropped from 0.1373 to 0.1353)
- Significant reduction in prompt-image concordance for harmful prompts (Template dataset CLIP score dropped from 0.1808 to 0.0918)

## Why This Works (Mechanism)

### Mechanism 1: Gradient-based Trajectory Deflection
The method reduces NSFW likelihood by iteratively pushing the predicted clean image away from regions of high "harmfulness density" in the embedding space. During denoising, the sampler calculates the gradient of the log-density of harmfulness $\nabla_{\hat{x}_0} \log p_h(\hat{x}_0)$ and performs a gradient descent step on the predicted clean image before predicting the next latent step. This effectively subtracts the "direction" of harmful content from the generation trajectory.

### Mechanism 2: Manifold Preservation via $\hat{x}_0$ Perturbation
Perturbing the predicted clean image $\hat{x}_0$ rather than the noisy latent $x_t$ maintains sample validity while applying guidance. The method leverages the manifold hypothesis, claiming that by calculating gradients w.r.t. $\hat{x}_0$, the correction vector lies on the tangent space of the data manifold, ensuring the updated sample remains a valid image encoding.

### Mechanism 3: Conditional Trajectory Correction (CTC)
Selective application of the correction step minimizes degradation for safe prompts. The correction is only applied if the predicted harmfulness probability $p_h(\hat{x}_0)$ exceeds a threshold $\eta$. For safe prompts where $p_h < \eta$, the sampler defaults to standard DDIM, preserving the original image quality and prompt alignment.

## Foundational Learning

- **Denoising Diffusion Implicit Models (DDIM)**
  - Why needed: The SFW sampler is built as a modification of the DDIM reverse process. You cannot implement the trajectory correction without understanding the transition from $x_t$ to $\hat{x}_0$.
  - Quick check: Can you derive the predicted clean image $\hat{x}_0$ from a noisy latent $x_t$ and the noise prediction $\epsilon_\theta$?

- **Contrastive Language-Image Pre-training (CLIP)**
  - Why needed: The entire external conditioning signal relies on CLIP embeddings to define the harmfulness density $p_h$ via cosine similarity.
  - Quick check: How does the cosine similarity between a text embedding and an image embedding represent semantic alignment, and how can its gradient be used for optimization?

- **Manifold Hypothesis & Tangent Space**
  - Why needed: The paper justifies its lack of image artifacts by claiming the gradient update stays on the image manifold's tangent space.
  - Quick check: Why does perturbing a latent vector arbitrarily usually result in "garbage" images, and how does the $\hat{x}_0$-based gradient theoretically avoid this?

## Architecture Onboarding

- **Component map:** Input (Text Prompt + Noise) -> Standard Denoising Step (U-Net predicts noise $\epsilon_\theta$) -> Tweedie Projection (compute $\hat{x}_0$) -> External Monitor (CLIP calculates $p_h$) -> Conditional Logic (is $p_h \geq \eta$?) -> Gradient Calculation (compute $\nabla_{\hat{x}_0} \log p_h$) -> Trajectory Update (update $\hat{x}_0$ and recalculate $x_{t-1}$) -> Output (Safe Latent $x_{t-1}$)

- **Critical path:** The backpropagation step through the CLIP image encoder to get the gradient w.r.t. $\hat{x}_0$ is the compute bottleneck and the critical logic path.

- **Design tradeoffs:**
  - Threshold $\eta$: High threshold = higher recall of unsafe content but risks more distortion (False Positives). Low threshold = safer output but may distort benign images.
  - Guidance Scale $\gamma$: High $\gamma$ = stronger censorship but risk of "burning" the image (loss of texture/quality).
  - Speed: The method adds significant latency per step compared to standard DDIM due to the gradient calculation.

- **Failure signatures:**
  - Semantic Drift: The image is "safe" but no longer matches the prompt.
  - Adversarial Bypass: Prompts that trigger low CLIP similarity during early denoising steps but converge to NSFW content later.
  - Aesthetic Degradation: Blurry or washed-out images caused by aggressive gradient updates.

- **First 3 experiments:**
  1. Visualize the perturbation vector $\nabla_{\hat{x}_0} \log p_h$. Does it actually highlight the "unsafe" regions of the image?
  2. Plot NSFW Detection Rate vs. CLIP Score to find the Pareto frontier for the threshold $\eta$.
  3. Measure the added ms/step introduced by the external CLIP forward/backward pass compared to standard inference.

## Open Questions the Paper Calls Out
- Can the corrected latent directions be distilled into the base diffusion model via fine-tuning to create a permanently safe model without inference-time overhead?
- How robust is the SFW sampler against adversarial attacks specifically designed to exploit the CLIP embedding space or obfuscate harmful concepts?
- Does the gradient-based redirection disproportionately degrade image aesthetics or semantic fidelity when the safe target concept is close to the unsafe region in latent space?

## Limitations
- The method depends on CLIP's semantic representation, which may have blind spots for certain harmful concepts
- Threshold selection is a fragile hyperparameter that may not generalize across different domains
- The Tweedie projection's sensitivity to noise prediction quality is not thoroughly analyzed

## Confidence

- **High Confidence**: The core mathematical framework (gradient-based trajectory correction) is sound and well-explained. The method is implementable and the reported metrics are verifiable.
- **Medium Confidence**: The reported performance metrics are based on a single model (Stable Diffusion 1.4) and a single harmfulness detector (HP). The generalizability to other diffusion models and the robustness against more sophisticated harmfulness classifiers are unknown.
- **Low Confidence**: The claim that the method "maintains image quality" is weakly supported. The paper reports CLIP score degradation but does not provide human perceptual studies or FID metrics to quantify the aesthetic impact.

## Next Checks

1. **Cross-Model Generalization Test**: Evaluate the SFW sampler on multiple diffusion models (e.g., SDXL, Kandinsky, DeepFloyd) to determine if the CLIP-based harmfulness density generalizes across different latent spaces and noise schedules.

2. **Adversarial Prompt Robustness**: Construct a battery of adversarial prompts designed to exploit the early-stage CLIP similarity. Measure if the threshold-based trigger fails to activate until it's too late.

3. **Human Perceptual Quality Study**: Conduct a blinded study where human raters score image quality and prompt alignment for both safe and unsafe prompts, comparing the SFW sampler against standard DDIM and other safety-filtered methods.