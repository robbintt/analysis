---
ver: rpa2
title: Representative Action Selection for Large Action Space Bandit Families
arxiv_id: '2505.18269'
source_url: https://arxiv.org/abs/2505.18269
tags:
- action
- regret
- afull
- full
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of selecting a small, representative
  subset of actions from a large action space that is shared by a family of bandits.
  The goal is to achieve performance nearly matching that of using the full action
  space, leveraging correlations between rewards of different actions.
---

# Representative Action Selection for Large Action Space Bandit Families

## Quick Facts
- arXiv ID: 2505.18269
- Source URL: https://arxiv.org/abs/2505.18269
- Reference count: 40
- Primary result: Algorithm selects small representative action subsets that achieve near-optimal regret across bandit families by sampling optimal actions from random bandit instances

## Executive Summary
This paper addresses the challenge of selecting a small, representative subset of actions from a large action space shared by a family of bandits. The authors propose a simple algorithm that samples bandit instances and selects their optimal actions to form a representative subset, achieving performance nearly matching that of using the full action space. The method leverages correlations between rewards of different actions without requiring prior knowledge of the correlation structure.

Theoretical guarantees show that when the action space exhibits clustering or low-dimensional manifold structure, the expected regret of the selected subset is bounded in terms of the covering numbers of the set. Empirical validation demonstrates the effectiveness of the approach, outperforming Thompson Sampling and Upper Confidence Bound methods in identifying near-optimal action subsets, particularly in settings with varying correlation structures.

## Method Summary
The core approach involves sampling K bandit instances θ ~ P from the bandit family distribution, computing the optimal action a*(θ) for each instance using an argmax oracle, and accumulating these optimal actions into a representative subset A. The algorithm exploits the correlation structure among action rewards, where similar actions cluster together under the correlation-induced metric. The theoretical analysis shows that with appropriate parameter choices, the expected regret approaches zero as the subset size increases, provided the action space has favorable geometric properties like clustering or low-dimensional manifold structure.

## Key Results
- Algorithm 1 achieves near-optimal regret using a small subset of actions when action space exhibits clustering or manifold structure
- Theoretical bounds show expected regret scales with covering numbers N(A_full, ε) rather than full action space size
- Empirical validation shows the method outperforms Thompson Sampling and UCB in identifying representative action subsets
- The approach works without prior knowledge of correlation structure, sampling optimal actions to automatically discover relevant geometry

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sampling optimal actions from randomly drawn bandit instances produces a representative subset that covers high-probability regions of the action space.
- Mechanism: Algorithm 1 samples bandit instances θ ~ P and computes optimal actions a*(θ). These optima are i.i.d. draws from the "importance measure" q, which assigns probability mass proportional to how often each action is optimal across the bandit family. By Lemma 3.3, after K samples the resulting set A is a measure-theoretic ε-net: it intersects every cluster r where q(r) > ε with high probability.
- Core assumption: The distribution P over bandit instances induces non-uniform importance measure q, such that some action clusters are optimal more frequently than others.
- Evidence anchors:
  - [abstract]: "proposes an algorithm that can significantly reduce the action space when such correlations are present, without the need to a-priori know the correlation structure"
  - [section 3]: "the samples a* in lines 5&6 of Algorithm 1 may be viewed as i.i.d samples from q"
  - [corpus]: Related work on action selection exists (arXiv:2511.22104 extends this to MDPs), but the ε-net sampling mechanism is specific to this paper.
- Break condition: If q is near-uniform over A_full (e.g., i.i.d. rewards, Section N), no small representative subset exists—K must scale with |A_full|.

### Mechanism 2
- Claim: Correlation structure among action rewards induces a geometry where similar actions cluster together; regret is controlled by cluster diameters rather than full action space size.
- Mechanism: The reward process {μ_a} defines an L2 metric on actions (Eq. 6). When rewards are correlated, actions lie in a low-dimensional manifold or clustered structure. Theorem 4.2 bounds regret by max over clusters of E[max_{a∈r_ℓ} (μ_a - μ_{a_ℓ})], which scales with cluster diameter ε, not |A_full|.
- Core assumption: Actions exhibit clustering or low-dimensional manifold structure under the correlation-induced metric.
- Evidence anchors:
  - [section 4]: "sets which exhibit clustering, or sets with low dimensional manifold structure, would have low covering numbers"
  - [section 1]: "actions a and a' will be considered similar if the rewards μ_a and μ'_a are similar for most bandits sampled from P"
  - [corpus]: Weak direct corpus evidence; related bandit work assumes known kernel structures rather than discovering them via sampling.
- Break condition: If covering numbers N(A_full, ε) are large at all scales (no structure), Theorem 4.9 requires K ~ N(A_full, ε) · (M/ε)², negating benefits.

### Mechanism 3
- Claim: Expected regret decomposes into a "within-cluster approximation" term plus an exponentially-decaying "missing cluster" term.
- Mechanism: Theorem 4.5 shows E[Regret] ≤ (cluster complexity term) + (sampling correction term). The sampling term E_q[(1-q(r))^{2K}] decays exponentially with K because (1-q(r))^K = exp(K·log(1-q(r))) ≤ exp(-Kq(r)).
- Core assumption: Gaussian (or sub-Gaussian) reward process; partition R with bounded cluster diameters.
- Evidence anchors:
  - [section 4.2]: "this term decays exponentially with K" (Remark 4.7)
  - [theorem 4.5]: full regret decomposition with both terms
  - [corpus]: No comparable regret decomposition in neighbors; standard bandit analyses focus on cumulative regret over time, not subset selection regret.
- Break condition: If clusters have large diameter (poor correlation within clusters), the cluster complexity term dominates regardless of K.

## Foundational Learning

- Concept: **Measure-theoretic ε-nets**
  - Why needed here: Understanding why random sampling from q produces coverage requires knowing that ε-nets intersect all "heavy" sets under measure q.
  - Quick check question: Given partition R and measure q, what probability guarantee does a K-sample net provide for clusters with q(r) > ε?

- Concept: **Gaussian width and expected supremum**
  - Why needed here: The cluster complexity term E[max_{a∈S} μ_a] is the Gaussian width of S—key to interpreting Theorem 4.2 bounds.
  - Quick check question: For a finite set S ⊂ ℝⁿ, how does Gaussian width scale with |S| and diameter?

- Concept: **Covering numbers and metric entropy**
  - Why needed here: Theorem 4.9 expresses sample complexity via N(A_full, ε); knowing covering number scaling for different geometries is essential.
  - Quick check question: What is the covering number of a d-dimensional ball of radius M in ℝⁿ?

## Architecture Onboarding

- Component map:
  Bandit sampler -> Argmax oracle -> Accumulator -> Representative subset A

- Critical path:
  1. Define action space A_full and bandit distribution P
  2. Choose K based on desired ε and covering number estimate (Eq. 10)
  3. Run Algorithm 1: sample θ → compute a*(θ) → add to A (if novel)
  4. Output A for downstream bandit learning

- Design tradeoffs:
  - **Oracle vs. TS approximation**: Exact argmax is costly; EpsilonNet+TS (Section 5.2) uses 300 TS rounds per sample—faster but approximate
  - **K selection**: Larger K reduces regret but increases precomputation; Eq. 10 provides theoretical guidance but requires covering number estimate
  - **Assumption on θ distribution**: Paper assumes θ ~ N(0, I); real deployments need to verify or approximate this

- Failure signatures:
  - High regret with increasing K → action space lacks correlation structure (covering numbers large)
  - Subset concentrates in one region → bandit distribution P is degenerate or argmax oracle biased
  - TS approximation fails → insufficient rounds or poor prior mismatch

- First 3 experiments:
  1. **Synthetic RBF validation**: Replicate Figure 2 with varying length-scales l; verify regret decreases as correlation increases
  2. **Cluster diameter sensitivity**: Replicate Figure 4—sweep spread parameter and confirm regret scales with cluster diameter, not |A_full|
  3. **Oracle-free stress test**: Run EpsilonNet+TS on 500-action space with nonstationary Gibbs kernel (Section M); compare histogram of selected actions against importance measure theory

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can data-dependent stopping rules be developed to automatically determine the subset size K without requiring knowledge of the covering number or correlation structure?
- Basis in paper: [explicit] Section 6 states: "Natural directions for future work include developing data-dependent stopping rules for choosing the subset size K."
- Why unresolved: Currently K is an input parameter, but optimal K depends on unknown geometric structure (covering numbers N(A_full, ε)) of the action space.
- What evidence would resolve it: A stopping criterion with provable regret guarantees that adapts to unknown clustering or manifold structure.

### Open Question 2
- Question: How can the representative action selection framework be extended to Markov decision processes (MDPs) to handle sequential decision-making?
- Basis in paper: [explicit] Section 6 states the authors want to extend "the setting from bandits to Markov decision processes to capture sequential decision making."
- Why unresolved: In MDPs, actions have sequential effects, and the notion of an "optimal action" depends on policy and state—unlike bandits where each action's value is independent per round.
- What evidence would resolve it: Regret bounds for subset selection in MDPs relating subset size to performance over a planning horizon.

### Open Question 3
- Question: What are the theoretical guarantees when the oracle argmax in Algorithm 1 is replaced with approximate methods, and how does approximation error propagate to the final regret?
- Basis in paper: [inferred] Algorithm 1 assumes access to an exact oracle for argmax, but Section 5.2 acknowledges practical infeasibility and uses Thompson Sampling as an approximation without theoretical analysis of this substitution.
- Why unresolved: The main theorems assume exact optimal action identification; the effect of approximation errors on the covering properties and regret bounds remains unquantified.
- What evidence would resolve it: Regret bounds that explicitly include the approximation error term from the inner optimization oracle.

## Limitations
- The approach requires knowledge or estimation of covering numbers N(A_full, ε), which may be challenging in high-dimensional or complex action spaces
- Performance depends heavily on the existence of correlation structure; no small representative subset exists for i.i.d. reward settings
- The assumption that optimal actions from sampled bandits adequately represent the full space's geometry may fail for heavy-tailed or multimodal bandit distributions

## Confidence
- **High confidence**: The mechanism of sampling optimal actions to form ε-nets (Mechanism 1) and the regret decomposition showing exponential decay in K (Mechanism 3) are mathematically rigorous and well-supported by the proofs
- **Medium confidence**: The claim that correlation structure enables small representative subsets (Mechanism 2) is plausible but depends heavily on the specific geometry of the action space and reward process
- **Low confidence**: The practical performance of the oracle-free variant (EpsilonNet+TS) in high-dimensional or nonstationary settings, as the 300-round TS approximation may not capture optimal actions accurately for complex problems

## Next Checks
1. **Covering number sensitivity**: Vary the length-scale l in the RBF kernel systematically and measure how the achieved regret scales with the estimated covering number N(A_full, ε). Verify the theoretical relationship E[Regret] ∝ √N(A_full, ε) holds empirically.

2. **Nonstationary stress test**: Replace the RBF kernel with a nonstationary Gibbs kernel (as mentioned in Section M) and evaluate whether the algorithm maintains performance or degrades significantly. Compare the histogram of selected actions against the theoretical importance measure q.

3. **High-dimensional generalization**: Test the algorithm on action spaces with d > 10 dimensions using a discretized grid or random projection method. Measure the scaling of required K with dimension and compare against the theoretical bound involving N(A_full, ε).