---
ver: rpa2
title: Program Synthesis Dialog Agents for Interactive Decision-Making
arxiv_id: '2502.19610'
source_url: https://arxiv.org/abs/2502.19610
tags:
- user
- eligibility
- dialog
- code
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ProADA, a novel agent that combines program
  synthesis and dialog planning to improve interactive decision-making for eligibility
  determination. The method converts natural language eligibility requirements into
  Python code, which the agent uses to structure dialog and request minimal user input.
---

# Program Synthesis Dialog Agents for Interactive Decision-Making

## Quick Facts
- arXiv ID: 2502.19610
- Source URL: https://arxiv.org/abs/2502.19610
- Reference count: 21
- Primary result: ProADA achieves 56.2 F1 score with 20.4 dialog turns on BeNYfits eligibility benchmark, outperforming GPT-4o (35.7 F1, 27.2 turns)

## Executive Summary
ProADA introduces a novel approach to interactive decision-making by combining program synthesis with dialog planning for eligibility determination tasks. The system converts natural language eligibility requirements into Python code, which structures dialog flow and requests minimal user input. By offloading dialog planning and memory to generated code, ProADA addresses limitations in large language models including hallucination and poor long-range reasoning. Evaluated on BeNYfits, a new benchmark for determining eligibility for public benefits, ProADA significantly outperforms baselines while maintaining dialog efficiency.

## Method Summary
ProADA uses GPT-4o to generate a Python DECIDE function from natural language eligibility requirements, then executes this function with a UserFeatures dictionary. When KeyError exceptions occur due to missing information, the dialog module (Llama 3.1 70B) generates targeted questions using constrained decoding to parse user responses into typed values. The system iterates until the DECIDE function returns a boolean eligibility result. A CLARITY module handles ambiguous responses with up to three clarification turns. The architecture leverages static code for planning while maintaining natural language flexibility for user interaction.

## Key Results
- ProADA achieves 56.2 F1 score versus 35.7 for GPT-4o with ReAct prompting
- Dialog efficiency: 20.4 turns versus 27.2 turns for GPT-4o
- Turn-Weighted F1 of 46.7 compared to 30.0 for GPT-4o
- Code generation errors account for 23% of failure cases
- CLARITY module reduces parsing errors from 10% to 2%

## Why This Works (Mechanism)

### Mechanism 1
- Offloading dialog planning to generated Python code reduces hallucination and improves long-range reasoning compared to pure LLM prompting. The code generation module creates a DECIDE function that explicitly encodes eligibility logic. When executed with incomplete user data, KeyError exceptions deterministically identify which information is needed next. This replaces implicit LLM "memory" with explicit program state.
- Core assumption: Code generation models exhibit better planning and reasoning over multi-step logic than equivalent models working in natural language alone.
- Evidence anchors: Abstract claims, Section 3 explanation, and related work on CodeARC and HyCodePolicy.
- Break condition: If eligibility logic requires subjective judgment or ambiguous interpretation not expressible in deterministic code, the mechanism may fail or oversimplify.

### Mechanism 2
- Structured data mapping from natural language responses to typed key-value pairs enables consistent state tracking and reduces repetition errors. The dialog module uses constrained generation to parse user responses into values matching pre-defined types (int, float, choice). These values populate a UserFeatures dictionary that persists across turns. The DECIDE function only reads from this structured representation.
- Core assumption: User responses can be reliably mapped to discrete typed values with minimal ambiguity in the eligibility domain.
- Evidence anchors: Section 3 implementation details and Section 6.2 error analysis showing 2% mapping errors across 102 trials.
- Break condition: If user responses are ambiguous, evasive, or require multi-sentence interpretation, the 2% error rate may not hold.

### Mechanism 3
- Exception-driven execution flow naturally prioritizes questions by execution order, creating implicit information-gathering policies without explicit planning. The Python DECIDE function executes until hitting a missing key. The first missing key determines the next question. This means the code's control flow (conditionals, early returns) implicitly defines question ordering and can implement pruning (e.g., skip child-related questions if household has no children).
- Core assumption: The generated code correctly implements eligibility logic including edge cases and conditional dependencies between features.
- Evidence anchors: Section 3 description and Figure 1 showing implicit question ordering through code execution.
- Break condition: If generated code has logical errors or fails to model dependencies correctly, the implicit question ordering will be suboptimal or incorrect.

## Foundational Learning

- **Program synthesis for task decomposition**: Understanding how code generation can serve as an intermediate representation for complex multi-step tasks (dialog planning) is essential for extending this approach.
  - Quick check: Can you explain why generating a Python function might produce better planning than generating a natural language plan?

- **Constrained decoding in language models**: The dialog module uses constrained generation to ensure user responses map to valid typed values; understanding this technique is necessary for debugging parsing failures.
  - Quick check: How does constrained decoding differ from post-hoc validation, and what are its failure modes?

- **Exception handling as control flow**: ProADA uses KeyError exceptions to drive the dialog loop; this unconventional pattern requires understanding Python execution models.
  - Quick check: What happens if DECIDE raises an exception for reasons other than missing keys?

## Architecture Onboarding

- **Component map**: Code Generation Module (GPT-4o) → DECIDE function → Dialog Module (Llama 70B) → UserFeatures dictionary → CLARITY module → READY/PREDICT prompts
- **Critical path**: Requirements text → Code generation → DECIDE function → Dialog loop (execute DECIDE → catch KeyError → generate question → parse response → update UserFeatures) → Eligibility prediction
- **Design tradeoffs**: One-time code generation using stronger model (GPT-4o) vs. per-turn inference cost savings; constrained generation accuracy vs. flexibility in handling verbose/unusual user responses; Python-specific implementation vs. language-agnostic symbolic planner
- **Failure signatures**: Code generation errors (23% of failure cases); multi-member tracking struggles; premature termination (hallucination); suggestibility (assuming implied facts)
- **First 3 experiments**: 1) Replicate single-opportunity scenario on BeNYfits subset to validate DECIDE generation quality independently of dialog module; 2) Ablate CLARITY module to measure contribution of disambiguation handling (paper reports Turn-Weighted F1 drops from 46.7 to 43.5); 3) Test code generation with alternative models (Claude 3.7, Qwen 32B) to isolate code quality impact—paper shows Claude slightly better (47.1 vs 46.7), Qwen dramatically worse (17.9)

## Open Questions the Paper Calls Out

- **Adapting to subjective criteria**: Can the program synthesis approach be effectively adapted for eligibility tasks involving subjective criteria (e.g., merit-based scholarships) rather than purely objective boolean logic?
  - Basis: Limitations section states domains requiring subjective evaluation present additional challenges as eligibility cannot necessarily be determined consistently based on self-reported user information.
  - Why unresolved: ProADA relies on converting requirements into deterministic Python code to return a boolean, a process that fails if the criteria are open to interpretation or require qualitative assessment.
  - What evidence would resolve it: An evaluation of ProADA on a dataset containing subjective eligibility criteria, or the introduction of a probabilistic reasoning module that successfully handles non-binary user inputs.

- **Domain knowledge retrieval for code generation**: To what extent can incorporating automated domain knowledge retrieval (e.g., legal definitions) or self-correction strategies reduce logical reasoning errors in the generated eligibility code?
  - Basis: Discussion notes that a significant portion of errors are due to inaccurate code generation related to domain expertise and suggests coding agents could search for domain knowledge before conducting program synthesis.
  - Why unresolved: The current implementation assumes the code generation module possesses sufficient intrinsic knowledge to write error-free logic for complex bureaucratic rules, which the failure analysis shows is not always true.
  - What evidence would resolve it: An ablation study comparing the standard ProADA against a variant equipped with a retrieval-augmented generation (RAG) step for domain-specific definitions, measuring the reduction in code logic errors.

- **Question complexity and user burden**: How does incorporating question complexity and user burden into the reward function impact the efficiency and user experience of the dialog agent?
  - Basis: Limitations section notes that complex, multi-hop queries are weighted the same as simple yes or no questions and that trade-offs of question complexity, length, and user burden may be addressed in future work.
  - Why unresolved: The current metric optimizes for the raw number of turns but ignores the cognitive load of individual questions, potentially incentivizing the model to ask difficult, compound questions to lower the turn count artificially.
  - What evidence would resolve it: A user study measuring perceived workload (e.g., NASA-TLX scores) comparing agents optimized for turn count versus those optimized for a metric weighted by question complexity.

## Limitations

- Code generation errors account for 23% of failure cases, indicating the approach is sensitive to the quality of the generated eligibility logic
- The system struggles with multi-member household scenarios where questions need to specify which member is being discussed
- The benchmark is specific to NYC public benefits, limiting generalizability to regions with different administrative structures or fewer overlapping services

## Confidence

- **High Confidence**: The core architectural contribution (code generation + exception-driven dialog loop) is clearly specified and the quantitative results are reproducible from the benchmark setup
- **Medium Confidence**: The mechanism claims about hallucination reduction and long-range reasoning improvements are supported by comparison to baselines but lack ablation evidence isolating code generation's specific contribution
- **Low Confidence**: The claim that the approach generalizes beyond eligibility determination to broader interactive decision-making problems is speculative, as the evaluation is confined to a specific domain with deterministic eligibility rules

## Next Checks

1. **Ablation of Code Generation Quality**: Test ProADA with code generated by weaker models (Qwen 32B, Llama 3.1 70B) while keeping the same dialog architecture to measure the isolated contribution of code generation quality to F1 scores.

2. **Parsing Robustness Analysis**: Systematically test the constrained generation parser with ambiguous, multi-sentence, or evasive user responses to verify the 2% error rate holds under realistic conversational stress and identify failure modes.

3. **Cross-Domain Transfer**: Apply ProADA to a non-eligibility decision-making task (e.g., technical support troubleshooting or loan application) where requirements include subjective judgment or probabilistic reasoning to test the limits of deterministic code representation.