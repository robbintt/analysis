---
ver: rpa2
title: 'ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for
  discrete-choice reasoning with EMA-guided routing'
arxiv_id: '2602.01797'
source_url: https://arxiv.org/abs/2602.01797
tags:
- orch
- accuracy
- routing
- agent
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces ORCH, a deterministic multi-agent orchestration
  framework for discrete-choice reasoning that improves accuracy over single-model
  baselines by 10+ points on MMLU-Pro and 50+ points on GSM8K. The system decomposes
  tasks into parallel analyses by multiple heterogeneous LLMs, then merges results
  via a dedicated merge agent.
---

# ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for discrete-choice reasoning with EMA-guided routing

## Quick Facts
- arXiv ID: 2602.01797
- Source URL: https://arxiv.org/abs/2602.01797
- Reference count: 6
- Primary result: ORCH improves accuracy by 10+ points on MMLU-Pro and 50+ points on GSM8K over single-model baselines using deterministic multi-agent orchestration.

## Executive Summary
This paper introduces ORCH, a deterministic multi-agent orchestration framework for discrete-choice reasoning tasks. ORCH improves accuracy by 10+ points on MMLU-Pro and 50+ points on GSM8K by decomposing questions into parallel analyses across three heterogeneous LLMs, then merging results via a dedicated merge agent. An optional EMA-guided router provides additional 0.7-2.0 point gains by dynamically selecting agents based on historical performance. The system offers reproducible, interpretable reasoning while maintaining consistency through fixed rules and temperature=0 settings.

## Method Summary
ORCH follows a "many analyses, one decision" paradigm where a dispatcher agent decomposes each question into three complementary sub-questions, which are routed to three heterogeneous LLM agents in parallel. Each agent produces an independent analysis, and a dedicated merge agent synthesizes these analyses to output the final answer choice. The system uses temperature=0 for determinism and can optionally employ an EMA-guided router that tracks historical accuracy, latency, and cost to dynamically select agents. This architecture is specifically designed for discrete-choice reasoning tasks like multiple-choice QA and mathematical reasoning.

## Key Results
- ORCH achieves 10+ point accuracy improvements over single-model baselines on MMLU-Pro
- ORCH achieves 50+ point accuracy improvements over single-model baselines on GSM8K
- EMA-guided routing provides an additional 0.7-2.0 point accuracy gain in benchmarking settings
- Multi-agent collaboration consistently outperforms simple majority voting baselines

## Why This Works (Mechanism)

### Mechanism 1
Replacing stochastic routing with fixed, parallel analysis and a dedicated merge agent improves reproducibility and accuracy. The system dispatches problems to multiple heterogeneous LLMs in parallel using shared prompt templates, then employs a dedicated "merge agent" to synthesize textual analyses and candidate answers into a single final decision. This enforces structured collaboration rather than simple voting.

### Mechanism 2
Automated task decomposition improves reasoning performance by forcing specific analytical perspectives. A dispatcher agent breaks the original question into three complementary sub-questions based on fixed rules, routing these to the agent pool. This creates structured chain-of-thought reasoning rather than single-step inference.

### Mechanism 3
EMA-guided routing provides modest performance gains by dynamically selecting agents based on historical performance metrics. The router updates scores using exponential moving averages of accuracy, latency, and cost, determining which agents to select for the pool or merger roles. This optimization requires ground truth feedback and is primarily viable for benchmarking.

## Foundational Learning

- **Protocol-level Determinism vs. Bit-level Reproducibility**: The paper redefines "determinism" as fixed rules with temperature=0, acknowledging provider API updates can cause output drift. Quick check: If the API provider silently updates their model version, does ORCH guarantee exact token-by-token reproducibility? (No, only the same process/rules are followed.)

- **Discrete-choice Reasoning Constraints**: The architecture is specialized for selecting from finite options (e.g., A-J). The merge mechanism depends on comparing candidate letters and analyzing structured text, differing from open-ended generation tasks. Quick check: How does the system handle a 1-1-1 tie in voting versus ORCH merge? (Voting uses priority tie-break; ORCH uses merge agent reasoning.)

- **Cost-Latency Trade-offs in Ensembles**: ORCH increases latency 3x-6x and cost 3x-5x compared to single models. Systems engineers must understand accuracy gains come with substantially higher resource consumption. Quick check: If latency must be <2s, is 3-agent ORCH suitable? (Likely not; average latencies are 11s-18s+.)

## Architecture Onboarding

- **Component map**: Input -> Dispatcher (Decomposition) -> Agent Pool (Parallel Analysis) -> Merger (Synthesis) -> Output
- **Critical path**: The dispatcher decomposes questions, three agents analyze in parallel, the merge agent synthesizes results, and the final answer is output
- **Design tradeoffs**: Merge vs. Vote (merging is slower/expensive but yields higher accuracy); EMA vs. Fixed (EMA offers optimization but requires ground truth); Consistency Module (K>1 sampling increases latency 4x while slightly reducing accuracy)
- **Failure signatures**: Malformed agent output (regex parsing fails, system proceeds with remaining agents); API timeout/rate limit (logs failure, falls back to default agent); Stochastic drift (results change despite seed 42 + temp=0, check provider API version logs)
- **First 3 experiments**: 1) Run fixed-routing ORCH (3 agents) against MMLU subset and compare accuracy/latency against majority vote baseline to isolate merge agent value; 2) Remove one agent and run pipeline to quantify marginal value of diversity vs. cost reduction; 3) Run same benchmark with EMA routing enabled (assuming cached ground truth) and fixed routing to verify 0.7-2.0 point gain

## Open Questions the Paper Calls Out

- **Open Question 1**: How can EMA-guided routing be adapted for real-time deployment without immediate ground-truth feedback? The current design depends on answer-based feedback absent in unlabelled, real-world interactions.

- **Open Question 2**: Does ORCH improve performance on non-discrete-choice tasks like code generation or open-ended dialogue? The framework is optimized for finite option selection, and it's unclear if the merge agent can synthesize open-ended outputs effectively.

- **Open Question 3**: Can multi-agent reasoning traces be distilled into a single student model to reduce inference costs? It's unknown if high-quality reasoning from complex orchestration can be captured by smaller models without significant accuracy loss.

- **Open Question 4**: How does ORCH's accuracy and latency scale with larger agent pools beyond three heterogeneous models? The study didn't explore how increasing agents (N>3) affects coordination overhead or reasoning diversity.

## Limitations
- External dependencies and cost: Requires 3x-6x higher latency and 3x-5x higher token costs due to parallel API calls to three providers
- Determinism constraints: True bit-level reproducibility is not guaranteed due to potential provider-side API updates
- EMA router applicability: Shows only modest gains and is primarily validated in benchmarking settings with available ground truth

## Confidence
- **High Confidence**: The core mechanism of using a dedicated merge agent to synthesize heterogeneous analyses is well-supported by ablation studies showing 10+ point accuracy gains over voting baselines
- **Medium Confidence**: EMA routing claims are supported by experiments but show statistically insignificant improvements in some comparisons, suggesting context-dependent benefits
- **Low Confidence**: Exact prompt templates and sub-question generation logic are not provided, making faithful reproduction difficult without significant engineering effort

## Next Checks
1. **Prompt Template Reconstruction**: Implement ORCH pipeline using inferred prompt templates and validate against reported 10+ point MMLU-Pro accuracy gains using the same 60-question subset
2. **Cost-Latency Trade-off Analysis**: Measure actual token usage and latency across all agent calls plus merge agent on GSM8K to verify reported 3x-6x overhead and assess resource cost justification
3. **EMA Router in Production**: Deploy ORCH with EMA routing in a setting where delayed ground truth becomes available to measure whether 0.7-2.0 point accuracy gains materialize in realistic deployment scenarios