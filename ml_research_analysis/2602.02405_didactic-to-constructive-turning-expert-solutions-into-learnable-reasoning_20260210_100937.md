---
ver: rpa2
title: 'Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning'
arxiv_id: '2602.02405'
source_url: https://arxiv.org/abs/2602.02405
tags:
- reasoning
- dail
- expert
- solutions
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Distribution Aligned Imitation Learning\
  \ (DAIL), a method that improves large language models' reasoning by converting\
  \ expert solutions into in-distribution reasoning traces. The key challenge addressed\
  \ is that expert solutions are \"didactic\"\u2014they skip steps and omit reasoning\
  \ chains that are clear to humans but not to models."
---

# Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning

## Quick Facts
- arXiv ID: 2602.02405
- Source URL: https://arxiv.org/abs/2602.02405
- Reference count: 40
- Key outcome: Distribution Aligned Imitation Learning (DAIL) improves LLM reasoning by 10-25% on AIME 2024/2025 using <1000 expert solutions, outperforming RLVR on hard problems

## Executive Summary
This paper addresses the challenge of learning from expert solutions when those solutions are "didactic"—containing implicit reasoning steps that models cannot follow. The proposed Distribution Aligned Imitation Learning (DAIL) method converts expert solutions into in-distribution reasoning traces through mixed policy rollouts and a contrastive loss that prevents shortcut learning. DAIL successfully improves reasoning performance on mathematical benchmarks while using fewer than 1000 expert solutions, and demonstrates better generalization to out-of-domain tasks compared to reinforcement learning approaches.

## Method Summary
DAIL is a two-stage imitation learning method that first generates in-distribution reasoning traces from expert solutions using mixed policy rollouts, then trains a student model with a contrastive objective. The mixed policy rollout interleaves student and privileged-student generation, accepting student tokens when they meet a probability threshold τ against the privileged student (conditioned on the expert solution). The contrastive loss penalizes tokens where a negative reference model (conditioned only on solution waypoints) has higher probability than the privileged student, suppressing shortcut behaviors while preserving valid reasoning patterns.

## Key Results
- DAIL improves pass@k by 10-25% on AIME 2024/2025 and Beyond AIME benchmarks
- Achieves 2-4x reduction in reasoning tokens while maintaining or improving accuracy
- Outperforms RLVR on hard problems where verifiable rewards are sparse
- Generalizes well to GPQA-Diamond out-of-domain tasks
- Uses fewer than 1000 expert solutions to achieve significant gains

## Why This Works (Mechanism)

### Mechanism 1
Direct imitation of expert solutions degrades reasoning because expert traces are out-of-distribution for the model. Expert solutions are "didactic"—they omit granular intermediate steps implied for humans but essential for models. Standard behavioral cloning (NLL loss) forces the model to shortcut its internal reasoning process, collapsing generalization.

### Mechanism 2
Mixed policy rollouts convert didactic expert solutions into in-distribution reasoning traces by interleaving student and privileged-student generation. At each token, the student samples first; the privileged student (frozen model conditioned on the expert solution) verifies if probability ≥ τ. If rejected, privileged student samples instead. This anchors generation to expert methodology while preserving the student's natural reasoning artifacts.

### Mechanism 3
Contrastive loss prevents learning rationalization shortcuts by penalizing tokens favored by a "negative reference" model conditioned only on solution waypoints. The negative reference M_NR receives only coarse intermediate results, pushing it to generate shortcut-laden reasoning that jumps between waypoints. The contrastive loss reduces likelihood of tokens where M_NR probability exceeds M_PS probability, selectively suppressing shortcut patterns while preserving valid reasoning.

## Foundational Learning

- Concept: Behavioral Cloning / Imitation Learning
  - Why needed here: DAIL is fundamentally an imitation learning method; understanding why naive BC fails on suboptimal/out-of-distribution data is core to the paper.
  - Quick check question: Why does minimizing NLL loss on expert demonstrations sometimes degrade performance on the target task?

- Concept: Distribution Shift / Covariate Shift
  - Why needed here: The central problem is that expert solutions are out-of-distribution relative to model reasoning; DAIL explicitly addresses this distributional gap.
  - Quick check question: What happens when a model trained on distribution P_data is evaluated on distribution P_test ≠ P_data?

- Concept: KL Divergence in Language Models
  - Why needed here: The contrastive objective uses token-level KL divergence between student, privileged student, and negative reference distributions.
  - Quick check question: What does minimizing KL(P_student || P_reference) encourage, versus KL(P_reference || P_student)?

## Architecture Onboarding

- Component map:
  - Student model M_θ -> LoRA adapter on frozen base
  - Privileged student M_PS -> Frozen copy of student conditioned on (problem x, expert solution s)
  - Negative reference M_NR -> Frozen copy conditioned on (problem x, partial waypoints s̃)
  - Expert dataset D -> Small set (<1000) of (problem, expert solution) pairs
  - Synthetic dataset D_syn -> Generated in-distribution traces via mixed policy rollouts

- Critical path:
  1. Data generation: Run mixed policy rollouts to generate D_syn from D
  2. Training loop: For each batch from D_syn, compute contrastive loss requiring three forward passes
  3. Memory efficiency: Single frozen model weights + LoRA adapter toggled for each role

- Design tradeoffs:
  - τ threshold: Higher = more expert anchoring, less natural student reasoning; lower = opposite. Paper uses τ=0.8
  - γ contrastive weight: Higher = stronger shortcut suppression, risk of underfitting; lower = weak suppression. Paper uses γ=0.1
  - Direct sampling vs. mixed policy: Direct sampling simpler but produces citation artifacts in reasoning models; mixed policy required for LRMs

- Failure signatures:
  - High training accuracy but low test generalization → model learning shortcuts
  - Generated traces explicitly reference "reference solution" → τ too high or direct sampling used
  - Performance degrades on out-of-domain tasks → overfitting to training distribution
  - Loss spikes or instability → γ too high

- First 3 experiments:
  1. Baseline replication: Train Qwen2.5-7B-Instruct with direct SFT on expert solutions; confirm performance degradation vs. untrained model on AIME 2024/2025
  2. Ablation: NLL vs. contrastive: Generate traces via direct sampling; train with (a) NLL only, (b) contrastive loss; measure pass@k gap
  3. Threshold calibration: Sweep τ ∈ {0.5, 0.7, 0.8, 0.9, 0.99} on small validation subset; plot training set correctness vs. trace quality

## Open Questions the Paper Calls Out

### Open Question 1
Can DAIL be effectively adapted to improve model safety and alignment, such as teaching models to reason about refusals or privacy policies? The Impact Statement explicitly calls for future work to explore applying DAIL to domains like safety, specifically for reasoning about refusals and privacy policies. The paper currently evaluates DAIL exclusively on mathematical reasoning where correctness is objective.

### Open Question 2
Is there a minimum reasoning capability or instruction-following threshold required for a model to benefit from DAIL? Appendix B.6 reports "mixed results" when applying DAIL to weaker models (e.g., Qwen3-4B or DeepSeek-R1-Distill-7B), attributing failures to poor instruction following and invalid trace generation. It is unclear if the failure is due to the specific hyperparameters used or a fundamental inability of weaker models to generate the necessary valid reasoning traces.

### Open Question 3
How can the construction of the "negative reference" be adapted for domains that lack discrete numerical or symbolic waypoints? Appendix B.1 details the construction of the negative reference using regular expressions to extract "intermediate waypoints" (numbers/symbols) for the contrastive loss. The reliance on regex for extracting mathematical expressions suggests the method may struggle in qualitative domains (e.g., law, humanities) where intermediate reasoning steps are not easily defined by symbolic extraction.

## Limitations

- The method requires expert solutions that contain intermediate results extractable via regex, limiting applicability to domains without symbolic waypoints
- Performance gains are demonstrated primarily on mathematical reasoning benchmarks, with uncertain generalization to other reasoning types
- The mixed policy rollout mechanism shows contradictory results across model families, working better for reasoning models than non-reasoning models
- Implementation details for waypoint extraction and validation set composition are underspecified in the main text

## Confidence

**High Confidence:** The fundamental problem identification (expert solutions are didactic and OOD for models) is well-supported by empirical demonstrations. The contrastive loss mechanism's theoretical justification and empirical superiority over NLL are clearly established across multiple experiments.

**Medium Confidence:** The mixed policy rollout mechanism's contribution is well-demonstrated for reasoning models (Qwen3) but shows contradictory results for non-reasoning models (Qwen2.5). The paper provides theoretical explanation for this discrepancy, but the optimal approach for different model types could benefit from more systematic exploration.

**Low Confidence:** The exact implementation of waypoint extraction for the negative reference model could significantly impact results. The paper describes the categories of waypoints but not the precise regex patterns or extraction logic, creating potential reproducibility gaps.

## Next Checks

1. **Waypoint Extraction Validation:** Implement the negative reference waypoint extraction using the described categories (numbers, exponentials, symbolic coefficients, linear expressions) and verify that generated traces from the negative reference model indeed show the shortcut behavior that the contrastive loss is designed to suppress.

2. **Cross-Domain Generalization Test:** Apply DAIL to a non-mathematical reasoning domain (e.g., logical reasoning or code generation) where expert solutions have different structural characteristics, measuring whether the distribution alignment benefits observed in mathematics transfer to other reasoning types.

3. **Threshold Sensitivity Analysis:** Systematically vary the τ parameter in mixed policy rollouts across a broader range (0.5-0.99) on a held-out validation set, measuring the trade-off between expert alignment and student autonomy, and identifying the optimal threshold for different reasoning model families.