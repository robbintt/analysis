---
ver: rpa2
title: 'Aggregation Queries over Unstructured Text: Benchmark and Agentic Method'
arxiv_id: '2602.01355'
source_url: https://arxiv.org/abs/2602.01355
tags:
- aggregation
- query
- evidence
- queries
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of entity-level aggregation queries
  over unstructured text, a task that requires identifying all entities satisfying
  compositional conditions. Unlike standard QA or retrieval tasks, aggregation demands
  completeness and exhaustive evidence collection.
---

# Aggregation Queries over Unstructured Text: Benchmark and Agentic Method

## Quick Facts
- arXiv ID: 2602.01355
- Source URL: https://arxiv.org/abs/2602.01355
- Reference count: 40
- Primary result: DFA achieves up to 5× higher evidence recall and substantially lower errors compared to strong RAG and agentic baselines on entity-level aggregation queries

## Executive Summary
This paper addresses the problem of entity-level aggregation queries over unstructured text, a task that requires identifying all entities satisfying compositional conditions. Unlike standard QA or retrieval tasks, aggregation demands completeness and exhaustive evidence collection. To tackle this, the authors introduce AGGBench, a benchmark with 362 queries and evidence-grounded annotations, and propose DFA, a modular agentic baseline. DFA decomposes the task into Disambiguation, Filtering, and Aggregation stages, explicitly modeling completeness. Experiments show DFA achieves up to 5× higher evidence recall and substantially lower errors compared to strong RAG and agentic baselines. The framework remains stable across model scales and outperforms baselines even when retrieval windows are enlarged.

## Method Summary
The method introduces DFA (Disambiguation, Filtering, Aggregation), a modular agentic baseline for entity-level aggregation queries. It consists of three stages: (1) Disambiguation uses a taxonomy-guided approach to classify query ambiguity and rewrite queries accordingly; (2) Filtering employs completeness-aware iterative filtering with rollback capability, maintaining corpus snapshots and preserving discarded chunks for observation; (3) Aggregation uses semantic-aware greedy batching with LLM-as-judge entity verification. The framework is evaluated on AGGBench, a benchmark with 362 queries over 45 research papers, using metrics like chunk-level recall, ACE (Absolute Count Error), and NACE (Normalized ACE).

## Key Results
- DFA achieves up to 5× higher evidence recall compared to strong RAG and agentic baselines
- DFA maintains stability across model scales and outperforms baselines even with enlarged retrieval windows
- The framework shows significant improvements in completeness for aggregation queries requiring exhaustive evidence collection

## Why This Works (Mechanism)

### Mechanism 1: Completeness-Preserving Iterative Filtering with Rollback
DFA maintains corpus snapshots where each represents candidate chunks after filtering rounds, preserving discarded chunks rather than permanently removing them. When over-filtering is detected via observation of discarded content samples, the system rolls back to an earlier snapshot and reapplies different filtering parameters. This contrasts with RAG's rank-then-discard paradigm where omitted passages cannot be recovered.

### Mechanism 2: Ambiguity Classification-Guided Query Rewriting
DFA identifies ambiguity subtypes using a predefined taxonomy and applies tailored prompt templates for each subtype before generating clarification questions. This front-loads interpretive decisions before costly retrieval, achieving accuracy gains of roughly 10% across all models compared to direct rewriting.

### Mechanism 3: Semantic-Aware Greedy Batching for Entity Alignment
The aggregation module groups chunks by semantic similarity using embedding cosine similarity and TF-IDF overlap, then applies greedy merging to balance entity coherence against context utilization. This reduces cross-batch entity comparison overhead while maintaining alignment quality, achieving a reduction of about 47% in candidate pairs.

## Foundational Learning

- **Concept: Completeness vs. Precision Trade-off in Retrieval**
  - Why needed here: Aggregation queries require "find all" rather than "find one," inverting standard RAG optimization objectives
  - Quick check question: Given a corpus where relevant evidence is 1% of chunks, what retrieval strategy maximizes recall when k=50?

- **Concept: Agentic Tool Orchestration with Memory**
  - Why needed here: DFA's filtering stage requires tracking tool invocations, observations, and rollback decisions across iterations
  - Quick check question: How would you design a memory module that stores both retained chunk summaries and discard-buffer observations for rollback decisions?

- **Concept: Entity Resolution Across Document Boundaries**
  - Why needed here: Evidence for the same entity may appear in multiple chunks; aggregation must deduplicate without external knowledge bases
  - Quick check question: Two chunks mention "Vaswani et al." and "the Transformer authors"—what features indicate these refer to the same entity?

## Architecture Onboarding

- **Component map**: Disambiguation Module -> Filtering Module -> Aggregation Module
- **Critical path**: Query ambiguity classification determines rewriting strategy → First filtering round applies initial filters → Observation generation triggers rollback if discard buffer shows relevant content → Final snapshot is clustered and batched for LLM verification → Entity alignment across batches produces final count
- **Design tradeoffs**: Iteration depth vs. latency (more rounds improve recall but increase cost); cluster granularity vs. context utilization (finer clusters reduce batch sizes but risk context overflow); discard buffer size vs. observation quality (larger buffers improve detection but consume context budget)
- **Failure signatures**: Over-filtering loop (repeated rollback without convergence, ~5% of failures); tool parameterization errors (~15% of failures from incorrect filtering tool usage); ambiguity misclassification (wrong clarification template applied)
- **First 3 experiments**: (1) Ablate rollback mechanism and measure recall drop on AGGBench-Core (expected: recall decreases from 0.63→0.55); (2) Vary ambiguity classifier accuracy by replacing taxonomy-guided classification with direct rewriting and measure NACE increase (expected: ~10% degradation); (3) Stress-test batching at corpus scale by running aggregation module on AGGBench vs. AGGBench-Core and measure LLM call count and candidate pair explosion

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can incorporating tool-awareness or tool-evolution learning significantly reduce the 15% error rate caused by incorrect tool parameterization in the DFA framework?
- **Basis in paper:** Section 6.4.2 identifies that 15% of DFA errors result from the model misusing filtering tools due to limited familiarity with interfaces, suggesting tool-awareness as a solution
- **Why unresolved:** The current DFA framework assumes static tool competence, meaning the agent does not dynamically improve its understanding of tool interfaces during execution
- **What evidence would resolve it:** A comparative evaluation of DFA variants with and without dynamic tool learning modules, specifically measuring the reduction in parameterization errors on the AGGBench benchmark

### Open Question 2
- **Question:** To what extent do adaptive and explicit memory management strategies mitigate the 5% failure rate caused by redundant, repeated tool calls in multi-round aggregation tasks?
- **Basis in paper:** Section 6.4.2 notes that 5% of failures are redundant repeated calls to the same tool, likely driven by poorly managed intermediate memory states
- **Why unresolved:** Current memory states can become overly long, causing the agent to lose track of previous actions and repeat steps, a common issue in long-horizon agentic tasks
- **What evidence would resolve it:** Experiments implementing memory compression or state-tracking mechanisms in DFA, reporting the frequency of redundant calls and the resulting impact on accuracy and latency

### Open Question 3
- **Question:** Does tightly coupling the aggregation and filtering stages to enable iterative evidence discovery improve recall while maintaining validity?
- **Basis in paper:** Section 5.3 ("Insight") proposes that aggregation evidence may signal missing contexts, which could be used to trigger further retrieval or relaxed filtering
- **Why unresolved:** In the current DFA design, aggregation is a final step; there is no feedback loop for the aggregation module to inform the filtering module of missing entities
- **What evidence would resolve it:** A modified DFA architecture with a feedback loop where low-confidence aggregation results trigger re-filtering, evaluated on improvements in recall and Normalized Absolute Count Error (NACE)

## Limitations
- The 8-subtype taxonomy for query ambiguity is author-defined without external validation, limiting robustness to novel query patterns
- Rollback detection reliability lacks quantitative analysis for false-positive/negative rates in over-filtering detection
- Scaling bottlenecks may emerge at full corpus scale (16K chunks), potentially causing context overflow or prohibitive computation

## Confidence
- **High Confidence**: Completeness-preserving filtering mechanism and its empirical improvement in evidence recall (up to 5×) are well-supported by ablation studies
- **Medium Confidence**: Ambiguity classification-guided rewriting shows consistent gains but relies on a closed taxonomy whose robustness to unseen patterns is unclear
- **Low Confidence**: Semantic-aware greedy batching's claimed 47% reduction in candidate pairs lacks stress-testing at full corpus scale

## Next Checks
1. **Rollback Mechanism Robustness**: Disable the discard-buffer observation and rerun DFA on AGGBench-Core. Measure recall drop and iteration count changes to quantify rollback's contribution under varying over-filtering rates.
2. **Taxonomy Generalization**: Replace the 8-subtype ambiguity classifier with a direct rewriting approach (no classification) on a held-out query set. Compare NACE and ambiguity classification accuracy to assess taxonomy brittleness.
3. **Full-Scale Scaling Test**: Run DFA on the full AGGBench (16K chunks) and record: (a) maximum context window utilization per batch, (b) total LLM call count, (c) candidate pair count before/after clustering. Identify thresholds where batching or clustering fail to contain computational explosion.