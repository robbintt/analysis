---
ver: rpa2
title: 'Modular Diffusion Policy Training: Decoupling and Recombining Guidance and
  Diffusion for Offline RL'
arxiv_id: '2506.03154'
source_url: https://arxiv.org/abs/2506.03154
tags:
- guidance
- diffusion
- training
- learning
- module
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Modular Diffusion Policy Training addresses the challenge of unstable
  and inefficient joint training in diffusion-based offline RL by decoupling the guidance
  module from the diffusion model. The core idea is to first train the guidance module
  as a value estimator using supervised learning on offline data, then freeze it to
  guide the diffusion model during training.
---

# Modular Diffusion Policy Training: Decoupling and Recombining Guidance and Diffusion for Offline RL

## Quick Facts
- arXiv ID: 2506.03154
- Source URL: https://arxiv.org/abs/2506.03154
- Authors: Zhaoyang Chen; Cody Fleming
- Reference count: 8
- Primary result: 9.54% peak reward gain and 4.09% AUC gain on HalfCheetah-Medium tasks via guidance-first training

## Executive Summary
Modular Diffusion Policy Training addresses the challenge of unstable and inefficient joint training in diffusion-based offline RL by decoupling the guidance module from the diffusion model. The core idea is to first train the guidance module as a value estimator using supervised learning on offline data, then freeze it to guide the diffusion model during training. This approach reduces memory usage, improves computational efficiency, and enhances sample efficiency. Experiments on bullet-D4RL benchmarks show significant improvements: 9.54% peak reward gain and 4.09% AUC gain on HalfCheetah-Medium tasks. Additionally, using different guidance modules during training and inference reduces variance by over 85%, and cross-module transferability allows guidance modules from one algorithm (e.g., IDQL) to be reused with another (e.g., DQL) without additional training, achieving baseline-level performance.

## Method Summary
The method trains guidance and diffusion modules separately rather than jointly. First, a Q-network guidance module is trained via supervised learning on offline data to estimate expected returns. This guidance module is then frozen and used to guide the diffusion model during its training phase. The diffusion model learns to generate actions conditioned on states, with the frozen Q-network providing reward gradients. At inference time, the same or a different guidance module can be used to refine sampled actions. This modular approach reduces variance, enables cross-algorithm transferability, and accelerates convergence compared to joint training approaches.

## Key Results
- 9.54% peak reward gain and 4.09% AUC gain on HalfCheetah-Medium tasks compared to joint training
- 86% variance reduction (IQR from 5.59e-5 to 7.81e-6) when using different guidance modules for training vs. inference
- 37% faster initial convergence when using IDQL guidance with DQL diffusion, achieving baseline-level final performance
- Successful cross-algorithm transferability: IDQL guidance can guide DQL diffusion without retraining

## Why This Works (Mechanism)

### Mechanism 1: Guidance-First Diffusion Training Accelerates Convergence
Pre-training and freezing the guidance module before diffusion training reduces noisy learning signals in early training, accelerating convergence. The guidance module (Q-network) is trained independently via supervised learning on offline data (s, a, r pairs), then frozen. This provides stable reward gradients to the diffusion model from step one, avoiding the phase where an unconverged guidance module misguides policy learning.

### Mechanism 2: Decoupled Guidance-Diffusion Reduces Variance via Bias Decoupling
Using independently-trained guidance modules at train vs. inference time reduces variance by breaking self-reinforcing bias loops. When a model uses its own Q-estimates to supervise itself, over/under-estimation errors amplify. Using a structurally identical but independently trained guidance model acts as implicit regularization.

### Mechanism 3: Cross-Algorithm Guidance Transfer via Shared Value Estimation
Guidance modules trained under one algorithm (e.g., IDQL) can guide diffusion models from another algorithm (e.g., DQL) without retraining. As long as the guidance module estimates E[r|s,a] with reasonable accuracy, its gradient can steer any diffusion model toward higher-reward regions.

## Foundational Learning

- **Concept: Diffusion Models for Policy Parameterization**
  - Why needed here: The policy is a denoising network that generates actions by reversing a noising process. Understanding Eq. 1-2 (forward/reverse diffusion) is prerequisite.
  - Quick check question: Can you explain how action a_0 is reconstructed from noisy a_k using the denoising network ε_θ?

- **Concept: Classifier-Free Guidance (CFG)**
  - Why needed here: CFG is the baseline method for injecting reward signals into diffusion. The paper modularizes CFG's guidance component.
  - Quick check question: How does CFG interpolate between unconditional and reward-conditioned outputs to bias sampling?

- **Concept: Offline RL Constraints**
  - Why needed here: Offline RL prohibits environment interaction; policy learning depends entirely on fixed dataset D. This justifies why guidance can be decoupled—no online exploration needed.
  - Quick check question: Why does offline RL require avoiding out-of-distribution actions, and how does this relate to batch-constrained Q-learning (BCQL)?

## Architecture Onboarding

- **Component map:**
  - Guidance Module Q_φ(s,a) → Diffusion Model ε_θ(a_k, k, s) → Action a_0

- **Critical path:**
  1. Train guidance module to convergence on offline data (verify Q-values correlate with returns)
  2. Freeze guidance weights; DO NOT continue training during diffusion phase
  3. Train diffusion model with frozen guidance; monitor for overfitting (early stopping often needed)
  4. For variance reduction: train second guidance module with different seed; use for inference only

- **Design tradeoffs:**
  - Pre-training guidance cost: Upfront compute before diffusion training begins; offsets later instability
  - Frozen vs. fine-tuned guidance: Paper finds frozen guidance outperforms continuing training (overfitting risk)
  - Double guidance: Adds inference-time compute (second Q-forward pass); reduces variance but not peak reward

- **Failure signatures:**
  - Early training divergence: Guidance module likely unconverged; verify Q-value distribution before freezing
  - High variance across seeds: Consider double guidance; check for self-reinforcing bias loops
  - Cross-module transfer fails: Check Q-value scale/normalization mismatch between algorithms
  - Medium-Replay dataset degradation: Paper notes this environment underperforms; may require adjusting loss ratio (L_reward / L_BC)

- **First 3 experiments:**
  1. **Ablate guidance timing:** Compare (a) joint training, (b) guidance-first frozen, (c) guidance-first continued training on HalfCheetah-Medium. Verify paper's claim that (b) converges fastest.
  2. **Variance diagnosis:** Train 5 seeds with baseline vs. double guidance. Measure IQR and max variance across evaluation episodes. Target >80% IQR reduction.
  3. **Cross-algorithm sanity check:** Train IDQL guidance + DQL diffusion (no joint training). Verify final performance within 5% of DQL baseline on Walker2d-Medium-Expert.

## Open Questions the Paper Calls Out

- **Can Guidance-First Diffusion Training (GFDT) be effectively adapted for trajectory-based diffusion models?**
  - Basis in paper: [explicit] The authors note that GFDT currently shows "little improvement" on trajectory-based models due to noisy returns over long sequences, limiting the method to TD-based approaches.
  - Why unresolved: The current theory relies on granular (s,a) pairs; reward attribution for long-horizon sequences remains ambiguous and unstable.
  - What evidence would resolve it: A modified GFDT algorithm that improves convergence rates on trajectory-based benchmarks like Diffuser.

- **Can this modular pipeline support plug-and-play reward shaping without retraining the diffusion backbone?**
  - Basis in paper: [explicit] Section 6 identifies "plug-and-play reward shaping" as a promising direction that enables rapid adaptation but leaves it for future exploration.
  - Why unresolved: It is unverified whether a frozen diffusion model can sufficiently generalize to new reward objectives solely through external guidance swapping.
  - What evidence would resolve it: Demonstration of a frozen diffusion model adapting to new downstream tasks or preferences via a swapped guidance module.

- **Does integrating monotonic guidance weight scheduling stabilize training in high-variance scenarios?**
  - Basis in paper: [explicit] The limitation section suggests that monotonically increasing the guidance loss weight (citing Wang et al., 2024) could improve stability in large variance cases.
  - Why unresolved: The current implementation uses static loss ratios, which may fail when initial noise or environmental stochasticity is high.
  - What evidence would resolve it: Empirical results showing reduced variance and stable convergence when applying a scheduled weight to the guidance loss.

## Limitations

- Medium-Replay environment underperforms baseline, suggesting task-specific limitations
- No analysis of guidance coefficient values or loss weight ratios, making reproduction difficult
- Limited discussion of how Q-value normalization differences between algorithms affect cross-module transfer

## Confidence

- High confidence: Variance reduction claims (86% IQR reduction with double guidance, supported by Table 3)
- Medium confidence: Peak reward gains (9.54% on HalfCheetah-Medium, but Medium-Replay underperforms baseline)
- Medium confidence: Cross-algorithm transferability (37% faster convergence with IDQL→DQL, but performance within 5% of baseline)

## Next Checks

1. Replicate guidance-first training on HalfCheetah-Medium, measuring early-stage divergence between joint vs. decoupled training
2. Implement double guidance with 5 random seeds, quantifying IQR and max variance reduction across all 8 D4RLMuJoCoTD tasks
3. Train IDQL guidance + DQL diffusion without joint training, verifying cross-algorithm performance on Walker2d-Medium-Expert within 5% of DQL baseline