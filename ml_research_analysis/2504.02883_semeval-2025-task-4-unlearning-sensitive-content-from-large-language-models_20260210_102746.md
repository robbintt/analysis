---
ver: rpa2
title: 'SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models'
arxiv_id: '2504.02883'
source_url: https://arxiv.org/abs/2504.02883
tags:
- unlearning
- retain
- forget
- task
- split
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper summarizes SemEval-2025 Task 4 on unlearning sensitive
  content from Large Language Models (LLMs). The task featured three subtasks: unlearning
  long-form synthetic creative documents, short-form synthetic biographies containing
  personally identifiable information (PII), and real documents sampled from the target
  model''s training dataset.'
---

# SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models

## Quick Facts
- arXiv ID: 2504.02883
- Source URL: https://arxiv.org/abs/2504.02883
- Reference count: 40
- Primary result: Top-performing team achieved 0.706 final score on 7B model while balancing unlearning effectiveness against utility degradation

## Executive Summary
This paper summarizes SemEval-2025 Task 4 on unlearning sensitive content from Large Language Models. The task featured three subtasks: unlearning long-form synthetic creative documents, short-form synthetic biographies containing personally identifiable information (PII), and real documents sampled from the target model's training dataset. The benchmark evaluated both regurgitation (sentence completion) and knowledge (question-answer pairs) for each subtask, totaling 12 distinct metrics.

Results showed that while top teams achieved near-perfect performance on the forget sets, substantial degradation occurred on retain sets, indicating over-unlearning. The best final score achieved was 0.706 for the 7B model, with a notable drop in MMLU utility from 0.494 to 0.443. The challenge highlighted the difficulty of balancing unlearning effectiveness with maintaining model utility, suggesting this remains an open research problem.

## Method Summary
The benchmark evaluated unlearning methods on OLMo-7B-0724-Instruct-hf and OLMo-1B-0724 models already fine-tuned on sensitive documents. Top teams used parameter-efficient gradient-based approaches, primarily Gradient Difference with LoRA adapters on transformer projection layers. The key strategy involved iterative unlearning on carefully sampled chunks of the forget set mixed with a large, resampled retain set. Teams monitored MMLU utility to ensure scores stayed above the threshold (0.371 for 7B) while optimizing unlearning effectiveness across 12 evaluation metrics including regurgitation, knowledge retention, membership inference attacks, and overall utility.

## Key Results
- AILS-NTUA achieved the highest final score of 0.706 using Gradient Difference with LoRA adapters on transformer projection layers
- Substantial over-unlearning occurred across top teams, with retain set performance degrading significantly despite near-perfect forget set performance
- MMLU utility dropped from 0.494 to 0.443 for the best 7B model submission
- The task highlighted the fundamental tradeoff between unlearning effectiveness and model utility preservation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parameter-efficient gradient-based unlearning using LoRA adapters on projection layers can remove sensitive content while partially preserving utility.
- Mechanism: LoRA adapters constrain gradient updates to low-rank subspaces of transformer projection layers (likely q_proj, v_proj, etc.). Combined with Gradient Difference loss (-LCE(forget;θ) + LCE(retain;θ)), this simultaneously reduces likelihood on forget set while reinforcing retain set. The low-rank constraint limits how much the model can deviate, providing implicit regularization against catastrophic over-unlearning.
- Core assumption: Sensitive content is sufficiently localized in representation space that low-rank perturbations suffice; retain set adequately covers knowledge that should be preserved.
- Evidence anchors:
  - [abstract]: "AILS-NTUA...used a parameter-efficient unlearning method based on Gradient Difference with LoRA adapters on transformer projection layers"
  - [table 5]: "Iterative unlearning on carefully sampled chunks of forget set, mixed with a larger volume of retain set"
  - [corpus]: AILS-NTUA paper (2503.02443) confirms "parameter-efficient, gradient-based unlearning using low-rank (LoRA) adaptation and layer-focused fine-tuning"

### Mechanism 2
- Claim: Merging models unlearned with different hyperparameters interpolates between under- and over-unlearning, achieving a Pareto-improvement on the forgetting-utility frontier.
- Mechanism: Two models trained with NPO+KL+GD at different hyperparameter settings (likely different learning rates, epochs, or forget/retain weighting) are merged via parameter averaging. One model may under-unlearn (good utility, incomplete forgetting) while the other over-unlearns (complete forgetting, degraded utility). Merging produces intermediate behavior that balances both objectives.
- Core assumption: The unlearning-utility tradeoff is approximately continuous and linear in parameter space; merged parameters yield coherent outputs rather than mode-switching or gibberish.
- Evidence anchors:
  - [abstract]: "ZJUKLAB merged two models unlearned with distinct hyperparameters to balance under/over-unlearning"
  - [table 5]: "Two distinct NPO+KL+GD trained models are merged to balance under/over-unlearning between them"
  - [corpus]: ZJUKLAB paper (2503.21088) titled "Unlearning via Model Merging" confirms this approach

### Mechanism 3
- Claim: Large-volume resampled retain data mixed with chunked forget data stabilizes gradient updates and reduces over-unlearning.
- Mechanism: Oversampling retain data relative to forget data increases gradient signal from the retain set within each batch, counteracting excessive erasure. Chunking the forget set enables iterative, fine-grained unlearning rather than exposing the model to the full forget distribution simultaneously. This creates a curriculum where the model learns to forget incrementally while retain gradients provide continuous stabilization.
- Core assumption: Retain set is representative of knowledge that should be preserved; gradient updates from retain examples can selectively reinforce shared representations while allowing forget-specific patterns to decay.
- Evidence anchors:
  - [abstract]: "carefully sampled chunks of the forget set mixed with a large, resampled retain set"
  - [table 5]: AILS-NTUA strategy explicitly lists this approach
  - [corpus]: Limited corpus detail on chunking/resampling specifics; primary evidence from main paper and AILS-NTUA system description

## Foundational Learning

- Concept: **Gradient Difference vs Gradient Ascent unlearning objectives**
  - Why needed here: Understanding why GA causes model divergence while GD provides bounded optimization is essential for selecting the right loss function.
  - Quick check question: Why does flipping the cross-entropy sign (GA) lead to unbounded loss, while adding retain set loss (GD) constrains optimization?

- Concept: **Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning**
  - Why needed here: Top-performing teams used LoRA adapters; understanding rank, alpha scaling, and target modules is necessary for implementation.
  - Quick check question: If LoRA rank is set too low, what happens to the model's ability to modify its behavior on the forget set?

- Concept: **Membership Inference Attacks (MIA) and why 0.5 AUC is the target**
  - Why needed here: Low task metrics don't guarantee unlearning; MIA measures whether information remains detectable in parameter space.
  - Quick check question: Why would a model with 0% regurgitation rate still fail MIA evaluation (AUC >> 0.5)?

## Architecture Onboarding

- Component map:
Target Models: OLMo-7B-Instruct (7B), OLMo-1B (1B) — fine-tuned on all 3 task documents
├── Forget Set (F): ~697 documents across tasks (creative, PII biographies, real Wikipedia)
├── Retain Set (R): ~690 documents (1:1 ratio with forget)
└── Evaluation: 12 metrics (3 tasks × 2 subtasks × 2 splits)
    ├── Regurgitation: ROUGE-L on sentence completion
    ├── Knowledge: Exact match on QA pairs
    ├── MIA Score: 1 - 2|AUC - 0.5|
    └── Utility: MMLU accuracy (threshold: 0.371 for 7B)

- Critical path:
  1. Load target model (OLMo-7B or OLMo-1B) already fine-tuned on forget+retain documents
  2. Initialize LoRA adapters on transformer projection layers (configurable rank, alpha)
  3. For each unlearning epoch: sample forget chunks + oversampled retain batch → compute GD loss → update adapters only
  4. Evaluate on private forget/retain splits (regurgitation + knowledge metrics)
  5. Check MMLU threshold before submission; if below 0.371 (7B), model is disqualified
  6. Tune stopping point: too early = under-unlearning (high MIA); too late = over-unlearning (low retain scores)

- Design tradeoffs:
  | Dimension | Aggressive Unlearning | Conservative Unlearning |
  |-----------|----------------------|------------------------|
  | Forget metrics | Near-perfect | Incomplete |
  | Retain metrics | Degraded | Preserved |
  | MMLU utility | Risk of threshold violation | Safe |
  | MIA score | Low (good) | May remain high |
  | Recommended for | Copyright/PII removal | Non-critical forgetting |

- Failure signatures:
  - **Gradient Ascent divergence**: Model outputs garbage tokens (unbounded loss); solution → switch to GD/NPO/KL
  - **Over-unlearning**: MMLU drops below 0.371 (7B), retain scores < 0.5; solution → reduce epochs, increase retain ratio, use LoRA constraint
  - **Under-unlearning**: Forget regurgitation/knowledge > 0.3, MIA AUC >> 0.5; solution → increase epochs, higher learning rate, ensure forget data is correctly exposed
  - **MIA failure with good task metrics**: Information compressed but not erased; solution → continue training, consider layer-specific approaches (RMU, causal mediation)

- First 3 experiments:
  1. **Baseline sweep**: Implement Gradient Difference with LoRA (rank=8, 16, 32) and measure forget/retain/MMLU tradeoffs. Establish Pareto frontier before trying complex methods.
  2. **Retain:Forget ratio ablation**: Test 1:1 (baseline), 2:1, 4:1, 8:1 retain oversampling with fixed epochs. Identify point where retain metrics stabilize without sacrificing forget effectiveness.
  3. **Early stopping curve**: Run GD+LoRA for 15 epochs, log all 12 metrics + MIA + MMLU per epoch. Plot forget vs retain scores to identify optimal stopping region before utility cliff. This informs epoch selection for final submission.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can unlearning algorithms be developed that achieve near-perfect forget-set performance without inducing degradation in general model utility (e.g., MMLU) or stability issues such as generating "garbage tokens"?
- Basis in paper: [explicit] Section 5 asks "Is the task solved?" and concludes that balancing utility and effectiveness remains "challenging and open" because the winning team still suffered a "notable drop" in utility.
- Why unresolved: The winning approach (AILS-NTUA) demonstrated that even high-performing submissions suffer from model degradation and utility loss.
- What evidence would resolve it: An unlearning method that maintains MMLU scores statistically indistinguishable from the original model while achieving >0.95 on forget metrics.

### Open Question 2
- Question: Can feasible evaluation metrics be established for LLM unlearning that correlate with statistical hypothesis testing against a gold-standard retrained model, without requiring the prohibitive compute of full pretraining?
- Basis in paper: [explicit] Section 6 identifies "Evaluation metrics" as an avenue for future exploration, noting that standard statistical testing is "not feasible for LLMs" due to computational costs.
- Why unresolved: Current metrics (ROUGE-L, MIA) may not fully capture the theoretical success of unlearning compared to a model retrained from scratch.
- What evidence would resolve it: A proxy metric that strongly correlates with the posterior distribution of a retrained model while being computationally efficient.

### Open Question 3
- Question: Do the parameter-efficient strategies (e.g., LoRA-based Gradient Difference) that succeeded on 1B and 7B models scale effectively to significantly larger parameter counts?
- Basis in paper: [explicit] Section 6 lists "Larger model checkpoints" as a future exploration area, noting the current study was limited to smaller models due to compute availability.
- Why unresolved: It is unclear if the compute savings from PEFT methods observed in the challenge translate to models with 70B+ parameters.
- What evidence would resolve it: Benchmark results showing similar Task Aggregate and MMLU retention scores when applying winning methods to 70B or 175B parameter models.

## Limitations

- The 1:1 forget:retain ratio, while standard for privacy benchmarks, may be insufficient for some domains where sensitive content is deeply entangled with general knowledge
- Current evaluation metrics cannot fully capture whether sensitive information has been truly erased versus merely compressed or obfuscated in model representations
- The MMLU threshold (0.371 for 7B) provides a hard utility constraint but may not reflect task-specific utility requirements in real-world applications

## Confidence

- **High confidence**: The fundamental tradeoff between unlearning and utility preservation (near-perfect forget performance consistently achieved at the cost of retain degradation), the effectiveness of LoRA-based gradient methods for parameter-efficient unlearning, and the difficulty of balancing these competing objectives
- **Medium confidence**: The specific hyperparameter configurations that would optimize the forget/retain tradeoff for different domains, the long-term stability of unlearning effects beyond the evaluation period, and the generalizability of results across different model architectures
- **Low confidence**: Whether the unlearning methods truly erase sensitive information versus making it inaccessible through normal model behavior, and the effectiveness of MIA scores as definitive proof of unlearning

## Next Checks

1. **MIA AUC Validation**: Verify that top-performing models indeed achieve AUC scores close to 0.5 (random guessing) rather than just low task metrics. This confirms information has been truly erased rather than compressed or obfuscated.

2. **Longitudinal Stability Test**: Evaluate unlearning persistence after additional fine-tuning on retain-only data or after model updates. This checks whether unlearning effects degrade over time or under different training conditions.

3. **Domain Transfer Analysis**: Apply winning methods to a different sensitive content domain (e.g., medical records instead of PII) to assess whether the 1:1 forget:retain ratio and LoRA gradient approaches generalize beyond the benchmark tasks.