---
ver: rpa2
title: A Comparison of DeepSeek and Other LLMs
arxiv_id: '2502.03688'
source_url: https://arxiv.org/abs/2502.03688
tags:
- classification
- llms
- error
- citation
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares DeepSeek-R1 with four other large language
  models (LLMs) on two classification tasks: authorship detection (human vs AI-generated
  text) and citation classification (academic citation types). Using both MADStat
  and a newly created CitaStat dataset, the study finds that Claude-3.5-sonnet consistently
  outperforms other models in classification accuracy, while DeepSeek-R1 shows competitive
  performance, outperforming Gemini, GPT, and Llama in most cases but falling short
  of Claude.'
---

# A Comparison of DeepSeek and Other LLMs

## Quick Facts
- arXiv ID: 2502.03688
- Source URL: https://arxiv.org/abs/2502.03688
- Authors: Tianchen Gao; Jiashun Jin; Zheng Tracy Ke; Gabriel Moryoussef
- Reference count: 7
- This paper compares DeepSeek-R1 with four other LLMs on authorship and citation classification tasks, finding Claude-3.5-sonnet consistently outperforms other models while DeepSeek-R1 shows competitive performance.

## Executive Summary
This paper evaluates five large language models (Claude-3.5-sonnet, DeepSeek-R1, Gemini-1.5-flash, GPT-4o-mini, and Llama-3.1-8b) on two text classification tasks: authorship detection and citation classification. Using both MADStat and a newly created CitaStat dataset, the study finds that Claude-3.5-sonnet consistently achieves the lowest error rates across all tasks. DeepSeek-R1 performs competitively, outperforming Gemini, GPT, and Llama in most cases but falling short of Claude. The authors demonstrate that combining traditional statistical methods (Higher Criticism) with LLMs significantly improves classification performance, with Claude-HC achieving the best results. The datasets created provide valuable benchmarks for future LLM research.

## Method Summary
The study evaluates five LLMs on authorship detection (human vs AI-generated text) and citation classification (academic citation types) using zero-shot classification via prompts. The MADStatAI dataset contains 582 abstract triplets from 15 authors, while CitaStat contains 2,980 manually labeled citation instances. The authors also develop a hybrid classifier that combines the Higher Criticism statistical method with LLM prompting, injecting statistically significant words directly into the LLM prompt to improve performance. Classification error rates, runtime, cost, and temporal stability (agreement percentage between runs) are measured.

## Key Results
- Claude-3.5-sonnet consistently achieves the lowest error rates across all classification tasks
- DeepSeek-R1 shows competitive performance, outperforming Gemini, GPT, and Llama in most cases but falling short of Claude
- The hybrid HC+LLM classifier significantly outperforms pure LLM approaches, with Claude-HC achieving the best results
- DeepSeek-R1 is substantially slower (183-235 minutes) than competitors (~7 minutes) but significantly cheaper (<$0.30 vs $12.30 for Claude)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DeepSeek-R1 achieves competitive accuracy at the cost of significant latency due to extended internal reasoning chains.
- **Mechanism:** DeepSeek-R1 utilizes multi-step verification and internal deliberation layers before generating output, resulting in higher accuracy but substantially longer runtimes compared to models like GPT-4o-mini or Gemini.
- **Core assumption:** The extended processing time correlates directly with multi-step reasoning rather than mere architectural inefficiency.
- **Evidence anchors:**
  - Remark 2 attributes runtime to "complex reasoning chains, multi-step verification processes, and additional internal deliberation layers"
  - Table 3 shows DeepSeek-R1 takes 183-235 minutes versus ~7 minutes for Claude, while achieving lower error rates in AC2
- **Break condition:** If latency requirements are strict, this mechanism makes DeepSeek-R1 impractical despite accuracy gains

### Mechanism 2
- **Claim:** Pure LLMs detect AI-generated text by recognizing statistical patterns in word usage, but they struggle with "rare and weak" signals without explicit feature engineering.
- **Mechanism:** LLMs act as zero-shot classifiers using internal representations of word frequencies, but when distinguishing signals are subtle, standard LLMs may miss weak features that require statistical methods like Higher Criticism to amplify.
- **Core assumption:** LLMs fail on weak signals not because they lack the data, but because the prompt interface does not guide attention to specific low-frequency differentiators.
- **Evidence anchors:**
  - Remark 3 conjectures that LLM performance depends on internal layers capturing word frequency information
  - Section 2.4 demonstrates that the HC classifier significantly outperforms pure LLM approaches
- **Break condition:** If the classification task relies on semantic reasoning rather than statistical anomalies in word choice, HC feature selection may offer less marginal utility

### Mechanism 3
- **Claim:** Hybridizing statistical feature selection with LLM prompting creates a superior classifier by forcing the LLM to attend to statistically significant discriminators.
- **Mechanism:** The "Hybrid Classifier" (A-HC) first runs Higher Criticism to identify discriminative words, then injects this list directly into the LLM prompt, grounding the LLM's reasoning in statistically validated features.
- **Core assumption:** Providing explicit feature importance in the prompt is more effective than relying on the LLM's implicit pre-training for specific classification tasks.
- **Evidence anchors:**
  - Figure 6 and Algorithm 1 describe the prompt injection of HC-selected words
  - Section 2.4 results show Claude-HC reduces error rates significantly compared to pure Claude
- **Break condition:** This mechanism relies on availability of labeled training data for the HC step; it cannot function in strict zero-shot settings

## Foundational Learning

- **Concept: Zero-shot Classification**
  - **Why needed here:** The paper evaluates LLMs primarily on their ability to classify text using only a prompt, without training weights
  - **Quick check question:** Does the classifier require a gradient update on training data to function? (If no, it fits the paper's primary evaluation mode)

- **Concept: Higher Criticism (HC) Thresholding**
  - **Why needed here:** HC is the central statistical tool proposed to improve LLM performance by detecting sparse, weak signals in high-dimensional data
  - **Quick check question:** How does HC handle a feature set where most features are noise and only a few are weakly predictive? (Answer: It selects a data-driven threshold to isolate the few useful features)

- **Concept: Temporal Stability**
  - **Why needed here:** LLMs are "black boxes" that can update or behave non-deterministically; the paper measures this explicitly
  - **Quick check question:** If you run the same prompt on the same model a week apart, should you expect the exact same output? (The paper suggests "mostly yes" for Claude/DeepSeek, but "no" for Llama)

## Architecture Onboarding

- **Component map:**
  MADStat -> MadStatAI (Human vs. AI abstracts); CitaStat (Citation texts) -> 5 LLMs (Claude, DeepSeek, Gemini, GPT, Llama) + 1 Statistical (HC) + Hybrid (HC+LLM) -> Prompts (Zero-shot) vs. Feature-Injected Prompts (Hybrid)

- **Critical path:**
  1. Generate AI variants using GPT-4o-mini from human abstracts
  2. For Hybrid only: Run HC on training split to identify discriminative words
  3. Construct prompt with definitions (AC1/AC2) or word lists (Hybrid)
  4. Calculate error rates and "Agreement %" (stability) across two time-separated runs

- **Design tradeoffs:**
  - Accuracy vs. Cost: Claude offers best accuracy but significantly higher financial cost ($12.30 vs <$0.30 for others)
  - Accuracy vs. Speed: DeepSeek-R1 offers high accuracy but is orders of magnitude slower (60+ hours vs minutes)
  - Zero-shot vs. Hybrid: Zero-shot requires no training data but performs worse; Hybrid requires training data for HC but significantly boosts performance

- **Failure signatures:**
  - Random Guessing Behavior: GPT and Llama showed error rates â‰ˆ 0.5 in AC1, often defaulting to predicting "human-written"
  - Instability: Llama-3.1-8b showed only ~50% agreement between runs, indicating high variance/non-determinism
  - Confounding in Generation: Performance drops if LLM generating data differs from classifying it

- **First 3 experiments:**
  1. Run a subset of MadStatAI through GPT and Claude to establish baseline for "Human vs. AI" detection
  2. Run same classification task twice with 1-week gap to measure "Agreement %" for chosen model
  3. Implement HC feature selection on training split; verify that injecting top 10-20 discriminative words into prompt lowers error rate vs zero-shot baseline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does DeepSeek compare to other LLMs in non-text domains, such as computer vision classification using the ImageNet dataset?
- **Basis in paper:** The authors state it's desirable to compare these LLMs with many other tasks (e.g., natural language processing, computer vision, etc.) and suggest using ImageNet
- **Why unresolved:** The current study restricts its scope strictly to text-based prediction tasks
- **What evidence would resolve it:** A benchmark study evaluating classification accuracy of DeepSeek against Claude, Gemini, GPT, and Llama on image data

### Open Question 2
- **Question:** Does integrating complex features (n-grams, sentence parsing) into the Higher Criticism feature selection improve the hybrid LLM classifier?
- **Basis in paper:** The Discussion notes that current features are "all bag-of-words features" and suggests using HC to select other useful features, such as n-gram features and sentence parsing features
- **Why unresolved:** The proposed hybrid method was tested exclusively using simple word-count vectors
- **What evidence would resolve it:** Experiments showing error rates of hybrid classifier when HC selects syntactic or multi-word features instead of single words

### Open Question 3
- **Question:** Can the MadStatAI dataset be effectively used to identify specific patterns in AI-generated text or to estimate the research interests of specific authors?
- **Basis in paper:** The authors propose that the datasets "can be used... to identify the patterns of AI generated documents" and for "estimating the research interest of an author"
- **Why unresolved:** The paper focuses on classification benchmarks and does not perform pattern analysis or interest estimation on the generated data
- **What evidence would resolve it:** Studies extracting stylistic markers from MadStatAI or deriving author interest profiles from CitaStat citation types

## Limitations

- The MADStatAI dataset contains only 582 samples (triplets from 15 authors), which may not represent the full diversity of statistical writing styles
- The study uses zero-shot classification without fine-tuning any models, which may not reflect real-world deployment scenarios
- The HC method requires labeled training data, limiting its applicability in true zero-shot settings

## Confidence

- **High Confidence:** Claude-3.5-sonnet's superior classification accuracy - supported by multiple metrics and consistent across datasets
- **Medium Confidence:** DeepSeek-R1's competitive performance relative to other non-Claude models - consistent results but fewer comparative baselines
- **Medium Confidence:** Hybrid classifier improvement claims - statistically significant but dependent on HC method assumptions
- **Low Confidence:** Cost-effectiveness comparisons - runtime and cost data may vary significantly with different API configurations

## Next Checks

1. **Dataset Expansion Test:** Validate whether classification performance generalizes to a larger, more diverse dataset (minimum 5,000 samples across multiple disciplines) to assess scalability of current conclusions
2. **Temporal Drift Analysis:** Implement weekly classification runs over 8+ weeks to quantify long-term model stability beyond the current 2-time-point measurement
3. **Hybrid Method Robustness:** Test the HC+LLM hybrid approach with varying HC parameters (different significance thresholds, feature counts) to identify sensitivity to hyperparameter choices