---
ver: rpa2
title: Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed
  LFP Modeling
arxiv_id: '2512.12461'
source_url: https://arxiv.org/abs/2512.12461
tags:
- sessions
- signals
- ms-spike
- neural
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a cross-modal representational knowledge distillation
  framework that transfers high-fidelity latent representations from multi-session
  spike transformer models to LFP transformer models. By aligning LFP representations
  with spike representations using a cosine similarity objective combined with an
  autoencoding loss, the Distilled LFP models significantly improve behavior decoding
  performance compared to LFP-only baselines.
---

# Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling

## Quick Facts
- **arXiv ID:** 2512.12461
- **Source URL:** https://arxiv.org/abs/2512.12461
- **Reference count:** 40
- **Key outcome:** Cross-modal knowledge distillation from spike to LFP representations achieves R²=0.71 for behavior decoding vs 0.27-0.24 for LFP-only baselines

## Executive Summary
This paper introduces a cross-modal representational knowledge distillation framework that transfers high-fidelity latent representations from multi-session spike transformer models to LFP transformer models. By aligning LFP representations with spike representations using cosine similarity combined with autoencoding losses, the Distilled LFP models significantly improve behavior decoding performance compared to LFP-only baselines. The framework demonstrates strong zero-shot generalization capabilities, maintaining superior performance when applied to new sessions without additional distillation. The approach enables alignment analysis between spike and LFP modalities at multiple spatiotemporal scales, providing insights into the relationship between these neural signals.

## Method Summary
The framework employs a dual-stage approach where spike and LFP transformers are first pre-trained on spike data, then LFP representations are distilled from spike representations using a cosine similarity objective combined with an autoencoding loss. The spike transformer uses 10 layers with 128-dimensional embeddings, while the LFP transformer uses 6 layers with 64-dimensional embeddings. The distillation process involves pre-training on 80% of sessions, followed by evaluation on held-out sessions. The model architecture is optimized through a supervised stage where both spike and LFP decoders are trained on decoded behaviors. The framework is evaluated across three datasets from six monkeys, with performance measured using R² scores for behavior decoding tasks including arm position, velocity, and gripping force.

## Key Results
- Distilled LFP models achieved average R² of 0.71 in unsupervised settings, outperforming multi-session LFP models (0.27) and single-session LFP models (0.24) with p<2.6×10⁻¹⁰
- Distilled models maintained superior performance when generalized to new sessions without additional distillation
- In supervised and multi-session distillation settings, R² reached 0.82-0.83
- The framework enabled alignment analysis between spike and LFP modalities at multiple spatiotemporal scales

## Why This Works (Mechanism)
The framework works by leveraging the higher information content in spike data to inform and enhance lower-dimensional LFP representations. The cross-modal distillation process aligns the latent spaces of spike and LFP representations through cosine similarity optimization, allowing LFP models to capture more behaviorally relevant information. The combination of representation alignment and autoencoding losses ensures that the distilled LFP representations maintain their ability to reconstruct original signals while incorporating the richer representational capacity learned from spike data. This approach effectively bridges the information gap between high-dimensional spike data and lower-dimensional LFP signals, resulting in more powerful behavior decoding capabilities.

## Foundational Learning

**Neural Signal Processing** - Understanding spike and LFP signal characteristics is essential for appropriate model design. Spike data provides high temporal resolution and direct neural activity measurements, while LFP captures population-level oscillatory activity. This knowledge guides the architectural choices and distillation objectives.

**Knowledge Distillation** - The technique of transferring knowledge from a large, high-capacity model (spike transformer) to a smaller, more efficient model (LFP transformer) through representation alignment. This allows the LFP model to benefit from the superior representational capacity of the spike model.

**Transformer Architectures** - Multi-head attention mechanisms and positional embeddings are crucial for capturing temporal dependencies in neural signals. The different layer depths and embedding dimensions reflect the distinct characteristics of spike versus LFP data.

**Representation Learning** - The framework relies on learning meaningful latent representations that capture behaviorally relevant information. The distillation process enhances these representations by aligning them with higher-quality spike representations.

## Architecture Onboarding

**Component Map:** Spike Transformer (10 layers, 128D) -> Cosine Similarity Loss -> LFP Transformer (6 layers, 64D) -> Autoencoding Loss -> Behavior Decoders

**Critical Path:** Pre-training on spike data → Cross-modal distillation → Supervised fine-tuning → Behavior decoding

**Design Tradeoffs:** Higher spike transformer capacity improves distillation quality but increases computational cost. Lower LFP dimensionality improves efficiency but may limit representational capacity. The cosine similarity objective balances alignment precision with computational tractability.

**Failure Signatures:** Poor distillation performance indicates misalignment between spike and LFP representational spaces. Performance degradation on new sessions suggests overfitting to specific recording conditions. Inconsistent behavior decoding results may indicate insufficient representational capacity in the LFP model.

**First Experiments:**
1. Compare single-session vs multi-session distillation performance to assess generalization benefits
2. Evaluate different distillation objectives (KL divergence vs cosine similarity) to optimize alignment
3. Test architectural variations (layer depth, embedding dimensions) to identify optimal configurations

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Model architecture differences between spike and LFP transformers may contribute to performance gaps beyond the distillation process itself
- Generalization claims are based on limited session splits within known datasets, not truly novel recording conditions
- Biological interpretability of distilled representations is not fully explored, raising questions about genuine neural computation capture versus statistical artifacts

## Confidence
- **High confidence:** Cross-modal knowledge distillation methodology is well-established and properly implemented with robust statistical significance
- **Medium confidence:** Performance superiority over LFP-only baselines may partially reflect architectural differences rather than distillation effectiveness
- **Medium confidence:** Zero-shot generalization claims are supported by presented experiments but would benefit from broader testing

## Next Checks
1. Conduct systematic ablation studies varying architectural similarity between spike and LFP transformers to isolate distillation effects from architectural contributions

2. Test distilled models on completely new recording sessions from the same monkeys that were excluded from all training and validation phases to rigorously evaluate true zero-shot generalization

3. Perform interpretability analyses comparing representations before and after distillation, examining whether distilled LFP representations capture known neural coding principles or exhibit spurious correlations