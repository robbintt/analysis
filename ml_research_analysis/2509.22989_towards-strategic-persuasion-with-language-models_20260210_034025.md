---
ver: rpa2
title: Towards Strategic Persuasion with Language Models
arxiv_id: '2509.22989'
source_url: https://arxiv.org/abs/2509.22989
tags:
- persuasion
- receiver
- sender
- llms
- claim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a principled framework for measuring the persuasive
  capabilities of large language models (LLMs) by grounding the evaluation in the
  Bayesian persuasion framework. The authors repurpose existing human-human persuasion
  datasets to construct environments where both Sender and Receiver are implemented
  as LLMs, validating their approach through a human study.
---

# Towards Strategic Persuasion with Language Models

## Quick Facts
- arXiv ID: 2509.22989
- Source URL: https://arxiv.org/abs/2509.22989
- Reference count: 40
- Primary result: Smaller LLMs trained with RL can achieve persuasion capabilities comparable to larger models

## Executive Summary
This paper proposes a principled framework for measuring LLM persuasive capabilities using Bayesian persuasion theory. The authors repurpose human-human persuasion datasets to create environments where both Sender and Receiver are implemented as LLMs, validated through human studies. Experiments demonstrate that stronger models achieve significantly higher persuasion gains, particularly in dynamic settings, while reinforcement learning enables smaller models like Llama-3.2-3B-Instruct to match larger models' persuasive performance.

## Method Summary
The framework uses Bayesian persuasion theory where a Sender LLM persuades a Receiver LLM to shift stance on claims. Four datasets (Anthropic, CMV, DDO, Perspectrum) are repurposed and claims extracted via LLM summarization. The environment includes prompts with claims, priors, utilities, and states. Evaluation uses persuasion gain (score delta on 1-7 Likert scale). RL training (PPO/GRPO via verl) optimizes Llama-3.2-3B-Instruct senders against fixed Llama-3.1-8B-Instruct receivers on ~2,700 instances using 4 NVIDIA A6000 GPUs.

## Key Results
- Larger models (DeepSeek-R1, Claude 3.7) achieve significantly higher persuasion gains, especially in dynamic settings
- RL-trained Llama-3.2-3B-Instruct models reach persuasive capabilities comparable to larger models
- LLMs exhibit strategic information disclosure aligned with theoretical predictions, with scaling improving both persuasive power and strategy sophistication

## Why This Works (Mechanism)

### Mechanism 1: Strategic Information Design via Bayesian Persuasion
The Sender controls information flow to shape the Receiver's posterior beliefs without deception, selecting signaling schemes that map states to messages. Instead of full disclosure, the Sender chooses partial transparency to shift Receiver beliefs to points where optimal action aligns with Sender utility. This formalizes persuasion as maximizing expected payoff over Bayes-plausible belief distributions.

### Mechanism 2: Scaling Laws for Adaptive Signaling
Larger models achieve disproportionately higher gains in dynamic settings through lower semantic similarity across turns, suggesting adaptive messaging. In evolving states, stronger models (DeepSeek-R1, Claude 3.7) calculate conditional mutual information more effectively, diversifying messages based on history rather than repeating static arguments.

### Mechanism 3: Reinforcement Learning on Persuasion Gains
Small LLMs are fine-tuned to match larger models by optimizing for persuasion gain (utility improvement over prior). Using RL with PPO/GRPO, the Sender learns information design principles without needing frontier model scale. The fixed Receiver LLM serves as a proxy for human belief updating, allowing policy generalization.

## Foundational Learning

- **Concept: Bayesian Persuasion (Information Design)**
  - Why needed: This is the theoretical engine; persuasion is mathematical optimization over Bayes-plausible beliefs, not just rhetoric.
  - Quick check: Does the Sender lie about the state, or selectively reveal information?

- **Concept: Conditional Mutual Information ($I(M_t; \Omega_t | H_{t-1})$)**
  - Why needed: Quantifies strategic information disclosure by measuring new state-relevant information revealed by messages given history.
  - Quick check: If a Sender repeats the same argument in Round 2 that they used in Round 1, would the conditional mutual information be high or low?

- **Concept: PPO (Proximal Policy Optimization) and GRPO**
  - Why needed: These RL algorithms train the Sender; understanding their stability is necessary to interpret results.
  - Quick check: Why is a "fixed Receiver" architecture crucial for RL training loop stability?

## Architecture Onboarding

- **Component map:** Datasets (Anthropic, CMV, DDO, Perspectrum) → Processor → Extracted Claims → Environment (Claim, Prior, Utility, State) → Agents (Sender Policy, Receiver Environment) → Trainer (RL Framework)

- **Critical path:** The training loop is most sensitive. Run Sender against Receiver, parse Receiver's text output into discrete action (Likert 1-7), calculate score delta relative to prior, backpropagate reward. Accuracy in parsing Receiver output is critical.

- **Design tradeoffs:**
  - LLM-as-Receiver vs. Human: Uses LLMs for scalability, acknowledging they're imperfect Bayesian updaters (trades fidelity for scale)
  - Static vs. Dynamic: Dynamic settings show higher gains but cost 3x inference compute and require managing conversation history context

- **Failure signatures:**
  - Stubborn Receiver: Extreme priors cause flatline persuasion gains
  - Reward Hacking: Sender learns to trigger specific tokens rather than genuinely persuading
  - Repetition Loops: Smaller models repeat arguments in dynamic settings, failing to persuade

- **First 3 experiments:**
  1. Baseline Evaluation: Run DeepSeek-R1 and Llama-3.1-8B-Instruct as Senders against fixed Receiver on full dataset to establish scaling gap
  2. Human Validation: Replicate 45-participant human study to verify LLM-Receiver updates beliefs reasonably
  3. RL Ablation: Train Llama-3.2-3B-Instruct using PPO against Receiver, test against different Receiver (Mistral-7B) to test strategy generalization vs. overfitting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs' strategic persuasion capabilities scale in complex environments with multiple competing senders or multiple receivers?
- Basis: Appendix A explicitly identifies need to examine strategic behaviors in broader persuasion settings with multiple receivers and competing senders.
- Why unresolved: Current framework restricts to dyadic setup (one Sender, one Receiver) without modeling competitive dynamics.
- What evidence would resolve it: Simulations where single Sender persuades group with heterogeneous priors, or multiple Senders compete to shift Receiver's belief.

### Open Question 2
- Question: To what extent can LLMs independently modify persuasive strategies without human oversight while maintaining safety?
- Basis: Appendix A argues it's essential to examine AI systems' independent message modification without human oversight.
- Why unresolved: Current study involves controlled environments with fixed prompts and utility functions, not evaluating autonomous strategy drift risks.
- What evidence would resolve it: Experiments analyzing safety constraint violations and goal alignment in RL agents given high autonomy to optimize persuasion in unconstrained domains.

### Open Question 3
- Question: Do persuasion strategies learned via LLM-LLM interactions effectively transfer to human subjects?
- Basis: While authors validate environment with human study (N=45) on "reasonableness," training and primary evaluation rely on LLMs as simulated Receivers, creating potential sim-to-real gap.
- Why unresolved: LLMs may update beliefs differently than humans (lacking emotional reactivity or specific biases), so strategies optimized against LLM Receivers might not yield optimal persuasion gains on humans.
- What evidence would resolve it: Comparative study measuring persuasion gain of RL-trained Sender models against human Receivers versus their performance against simulated LLM Receivers.

## Limitations

- Human-like belief updating remains uncertain as LLMs may not truly update beliefs as humans do despite limited human validation
- RL generalization concerns exist as transfer is tested only against one alternative receiver, risking overfitting or reward hacking
- Theoretical grounding in Bayesian persuasion assumes perfect Bayesian rationality but practical implementation introduces approximations

## Confidence

- **High confidence**: Claims about scaling effects in static settings and basic framework architecture; clear, consistent patterns where larger models outperform smaller ones
- **Medium confidence**: Claims about dynamic setting advantages and RL training effectiveness; promising but smaller sample sizes and reliance on proxy metrics
- **Low confidence**: Claims about RL models achieving "comparable" performance to larger models and assertion that models learn generalizable persuasion principles rather than architecture-specific exploitation

## Next Checks

1. **Cross-architecture receiver testing**: Evaluate RL-trained models against 3-5 diverse receiver architectures to rigorously test for overfitting versus genuine strategy learning

2. **Human receiver validation**: Conduct controlled study where human participants act as receivers for both baseline and RL-trained sender models, measuring correlation between LLM and human persuasion outcomes

3. **Temporal stability analysis**: Test whether persuasion gains persist across extended conversations (10+ turns) to validate RL-trained models maintain strategic information disclosure rather than collapsing to repetitive patterns