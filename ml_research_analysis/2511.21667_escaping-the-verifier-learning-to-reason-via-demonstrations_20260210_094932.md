---
ver: rpa2
title: 'Escaping the Verifier: Learning to Reason via Demonstrations'
arxiv_id: '2511.21667'
source_url: https://arxiv.org/abs/2511.21667
tags:
- policy
- reasoning
- critic
- raro
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RARO (Relativistic Adversarial Reasoning Optimization) addresses
  the challenge of training reasoning-capable language models when task-specific verifiers
  are unavailable. The method employs Inverse Reinforcement Learning through an adversarial
  game between a policy and a relativistic critic, using only expert demonstrations.
---

# Escaping the Verifier: Learning to Reason via Demonstrations

## Quick Facts
- arXiv ID: 2511.21667
- Source URL: https://arxiv.org/abs/2511.21667
- Reference count: 40
- Primary result: RARO trains reasoning models without verifiers by learning a discriminative critic via pairwise comparison of expert vs policy outputs

## Executive Summary
RARO (Relativistic Adversarial Reasoning Optimization) addresses the challenge of training reasoning-capable language models when task-specific verifiers are unavailable. The method employs Inverse Reinforcement Learning through an adversarial game between a policy and a relativistic critic, using only expert demonstrations. The policy learns to produce expert-like answers while the critic learns to discriminate between expert and policy answers through pairwise comparison. Both components are trained jointly and continuously via reinforcement learning with key stabilization techniques including data mixing and replay buffers. Across three diverse reasoning tasks - Countdown, DeepMath, and Poetry Writing - RARO significantly outperforms strong verifier-free baselines, nearly matching RLVR performance on verifiable tasks. The method demonstrates effective scaling with both model size and reasoning budget, successfully eliciting reasoning behaviors even in open-ended domains without explicit verification signals.

## Method Summary
RARO introduces a novel approach to learning reasoning capabilities in language models without requiring task-specific verifiers. The method establishes an adversarial game between a policy network that generates answers and a relativistic critic that learns to distinguish expert demonstrations from policy outputs through pairwise comparison. During training, the critic is presented with pairs of answers (one expert, one policy-generated) and learns to predict which is which, effectively learning a reward function that captures expert-like reasoning patterns. The policy is then trained to maximize rewards from this critic while minimizing its own ability to discriminate between expert and policy outputs. This continuous joint training setup employs reinforcement learning with stabilization techniques including data mixing to prevent overfitting, replay buffers for sample efficiency, and model-based rollouts for more stable updates. The approach enables reasoning model training even in domains where traditional reward modeling or verification is impractical or impossible.

## Key Results
- RARO significantly outperforms strong verifier-free baselines across three diverse reasoning tasks
- The method achieves performance nearly matching RLVR on verifiable tasks despite lacking explicit verifiers
- RARO demonstrates effective scaling with both model size and reasoning budget
- The approach successfully elicits reasoning behaviors in open-ended domains without explicit verification signals

## Why This Works (Mechanism)
RARO leverages inverse reinforcement learning principles through an adversarial framework where the critic learns to capture expert reasoning patterns without explicit reward engineering. By framing the problem as a relativistic comparison task, the method sidesteps the need for absolute quality judgments that are difficult to obtain in reasoning domains. The pairwise discrimination task forces the critic to learn subtle differences between expert and policy reasoning trajectories, creating a rich reward signal that captures the essence of expert-level reasoning. The continuous joint training ensures both components evolve together, with the critic providing increasingly nuanced feedback as the policy improves, while the policy progressively masters the reasoning patterns the critic has learned to identify.

## Foundational Learning
- **Inverse Reinforcement Learning**: Learning reward functions from expert demonstrations rather than hand-crafted rewards - needed because reasoning quality is hard to specify explicitly; check by verifying the critic learns meaningful reward signals from demonstration pairs
- **Adversarial Training**: Two models competing in a minimax game to improve each other - needed to create a self-improving training loop without external supervision; check by monitoring training stability and convergence
- **Pairwise Comparison**: Learning from relative judgments between two options rather than absolute scores - needed because reasoning quality is often subjective and easier to compare than score; check by validating critic accuracy on held-out pairs
- **Reinforcement Learning with Stabilizers**: Using techniques like data mixing and replay buffers in RL training - needed because pure adversarial RL is notoriously unstable; check by comparing performance with and without stabilizers
- **Model-based Rollouts**: Using learned models to simulate future trajectories during training - needed to improve sample efficiency and training stability; check by measuring sample complexity vs model-free approaches
- **Continuous Joint Training**: Simultaneously updating both policy and critic rather than alternating phases - needed for efficient learning in this adversarial setup; check by monitoring the balance between policy and critic performance

## Architecture Onboarding

### Component Map
RARO consists of three interconnected components: the Policy Network (generates answers), the Relativistic Critic (discriminates expert vs policy outputs), and the Training Loop (coordinates updates using reinforcement learning). The policy takes in a prompt and generates reasoning trajectories, which are then paired with expert demonstrations. The critic receives these pairs and learns to classify which is expert vs policy-generated. The policy is updated using rewards from the critic via policy gradient methods, while the critic is updated using standard supervised learning on the classification task. Data mixing, replay buffers, and model-based rollouts serve as supporting infrastructure throughout the training loop.

### Critical Path
1. Prompt → Policy Network → Generated Answer
2. Expert Answer paired with Generated Answer → Relativistic Critic → Classification Prediction
3. Classification Results → Policy Gradient Update (for Policy) and Supervised Update (for Critic)
4. Stabilizers (Data Mixing, Replay Buffer, Model-based Rollouts) applied throughout

### Design Tradeoffs
- **Pairwise vs Absolute Rewards**: Pairwise comparison is more sample-efficient and captures relative quality better, but may not provide absolute quality guidance
- **Continuous vs Alternating Training**: Joint training is more efficient but harder to stabilize than alternating phases
- **Model-based vs Model-free**: Model-based rollouts improve sample efficiency but add complexity and potential compounding errors
- **Critic as Reward vs Separate Reward Model**: Using the critic directly as reward avoids reward modeling overhead but may introduce instability

### Failure Signatures
- Oscillating rewards during training (particularly in subjective domains) indicating adversarial instability
- Critic collapse where it fails to discriminate between expert and policy outputs
- Policy collapse where it produces degenerate outputs that maximize critic rewards without meaningful reasoning
- Slow convergence indicating poor reward signal quality or ineffective stabilization

### First Experiments
1. Verify the critic can learn to discriminate between expert and random policy outputs on a simple task
2. Test policy learning with a frozen, pre-trained critic on a verifiable reasoning task
3. Evaluate the impact of each stabilization technique (data mixing, replay buffer, model-based rollouts) through systematic ablation

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What techniques can stabilize the adversarial training dynamics of RARO in long-form, subjective domains where policy-critic rewards oscillate?
- Basis in paper: [explicit] "Future work will focus on developing techniques to stabilize this adversarial game in subjective domains."
- Why unresolved: Figure 6 shows oscillating rewards during Poetry Writing training; the authors note this resembles GAN instability but do not resolve it.
- What evidence would resolve it: Demonstrated reduction in reward variance during training on subjective tasks, with accompanying performance gains.

### Open Question 2
- Question: Can curriculum learning or critic pretraining close the sample efficiency gap between RARO and RLVR?
- Basis in paper: [explicit] "RARO requires more training iterations to reach performance levels comparable to RLVR... future work could explore techniques to accelerate convergence, such as curriculum learning and critic pretraining."
- Why unresolved: Figure 8 shows RARO lagging RLVR under identical hyperparameters; no remedy is tested.
- What evidence would resolve it: Matching RLVR's sample complexity on Countdown or DeepMath with a proposed acceleration method.

### Open Question 3
- Question: Does RARO's relativistic critic learn a reward function that generalizes across different task distributions, or is it task-specific?
- Basis in paper: [inferred] The critic is trained jointly per-task; no experiments test transfer of the learned reward function to new tasks.
- Why unresolved: Transferability would reduce the cost of training on new demonstration-only domains.
- What evidence would resolve it: Zero-shot or few-shot application of a critic trained on one domain (e.g., math) to another (e.g., coding) with retained discriminative performance.

## Limitations
- Requires expensive expert demonstrations, which are labor-intensive to obtain
- Performance degrades on more open-ended tasks like poetry generation compared to structured mathematical reasoning
- Adversarial training framework introduces potential stability challenges, especially at larger scales
- Theoretical foundations for convergence in the continuous joint training setup remain incompletely explored

## Confidence
- High: Experimental results demonstrating RARO's superiority over strong baselines across multiple reasoning tasks are robust and well-supported by the data
- Medium: Claims about RARO's ability to elicit reasoning behavior in the absence of verifiers are supported by the results but would benefit from additional ablation studies on different types of reasoning tasks and demonstration qualities
- Low: The assertion that RARO successfully handles "open-ended domains without explicit verification signals" requires more careful qualification, as performance degradation on poetry writing suggests limitations in truly open-ended settings

## Next Checks
1. Conduct a systematic ablation study removing each stabilization component (data mixing, replay buffer, model-based rollouts) to quantify their individual contributions to RARO's performance

2. Evaluate RARO on tasks with progressively less structured reasoning requirements to map the boundaries of its effectiveness in open-ended domains

3. Test RARO's performance when trained with noisy or imperfect demonstrations to assess its robustness to demonstration quality variations