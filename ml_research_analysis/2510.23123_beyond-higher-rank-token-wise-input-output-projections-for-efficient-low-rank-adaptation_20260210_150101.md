---
ver: rpa2
title: 'Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank
  Adaptation'
arxiv_id: '2510.23123'
source_url: https://arxiv.org/abs/2510.23123
tags:
- lora
- toplora
- rank
- should
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limited expressiveness of LoRA caused
  by shared input-output projections across tokens. It proposes Token-wise Projected
  LoRA (TopLoRA), which dynamically adjusts LoRA weights for each input token by generating
  token-specific diagonal matrices from a learned projector.
---

# Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation

## Quick Facts
- arXiv ID: 2510.23123
- Source URL: https://arxiv.org/abs/2510.23123
- Reference count: 40
- Key outcome: TopLoRA achieves 2-4% accuracy gains over LoRA at same rank while using fewer parameters

## Executive Summary
This paper addresses the limited expressiveness of LoRA caused by shared input-output projections across tokens. It proposes Token-wise Projected LoRA (TopLoRA), which dynamically adjusts LoRA weights for each input token by generating token-specific diagonal matrices from a learned projector. TopLoRA maintains the same low-rank structure as LoRA but achieves finer-grained adaptation without increasing rank. Experiments across multiple models and datasets show TopLoRA consistently outperforms LoRA and its variants, achieving 2-4% accuracy gains at the same rank, while using fewer parameters.

## Method Summary
TopLoRA extends LoRA by adding a token-wise projection mechanism that generates diagonal matrices Σ_X for each input token X. The core formula ΔW_X = BΣ_XA uses standard LoRA A,B matrices plus a projector Θ that generates Σ_X = Diag(Exp(RMSNorm(ΘX))). This creates token-specific weight adjustments while maintaining low-rank structure. The method is applied to different weight groups depending on task: Q,V for GLUE tasks and Q,K,V for mathematical and commonsense reasoning. Experiments use rank 8/16, α=2r, dropout 0.05, and standard training configurations across RoBERTa and various LLM variants.

## Key Results
- Achieves 2-4% accuracy gains over LoRA at same rank across multiple tasks
- Outperforms LoRA and its variants on GLUE benchmark (8 tasks) with RoBERTa models
- Demonstrates effectiveness on mathematical reasoning (6 tasks) and commonsense reasoning (8 tasks) with various LLM sizes
- Maintains parameter efficiency while providing finer-grained token-level adaptation

## Why This Works (Mechanism)
Standard LoRA uses shared low-rank matrices across all tokens, limiting its ability to capture token-specific patterns. TopLoRA addresses this by generating token-specific diagonal matrices that modulate the LoRA weights dynamically. The projector Θ takes token embeddings as input and produces values that are normalized and exponentiated to ensure positive diagonal values. This allows the model to adapt differently for each token while preserving the computational efficiency of low-rank decomposition.

## Foundational Learning
- Low-rank adaptation (LoRA): Matrix decomposition technique that approximates weight updates with low-rank matrices A and B. Needed because full fine-tuning is computationally expensive for large models.
- RMS normalization: Normalizes values by root mean square, preventing extreme values before exponentiation. Critical for stable training and preventing numerical overflow.
- Diagonal matrix projection: Creates token-specific scaling factors applied element-wise. Enables fine-grained adaptation without increasing rank or computational cost.
- Token-wise adaptation: Adjusts model parameters based on individual token context. Addresses limitation of shared parameters across all tokens in standard PEFT methods.
- Exponentiation for positivity: Ensures diagonal matrix values remain strictly positive. Prevents zeroing out important weight components during adaptation.

## Architecture Onboarding
Component map: Input tokens -> Projector Θ -> RMSNorm -> Exp -> Diagonal matrix Σ_X -> ΔW_X = BΣ_XA -> Weight update

Critical path: Token embeddings flow through projector, normalization, and exponentiation to generate token-specific diagonal matrices that modulate LoRA weight updates.

Design tradeoffs: The projector adds parameters but enables token-specific adaptation without increasing rank. Choice of projector architecture (MLP depth/width) affects expressiveness vs efficiency. RMSNorm before Exp prevents numerical instability but may limit extreme adaptations.

Failure signatures: Σ_X values collapsing to identity (all ~1) reverts to standard LoRA. Numerical overflow from large projector outputs causing NaN/Inf. Projector failing to capture meaningful token patterns resulting in random noise.

First experiments:
1. Validate projector generates token-specific Σ_X values by comparing distributions across different tokens
2. Test RMSNorm + Exp combination prevents numerical instability while maintaining adaptation capability
3. Compare parameter counts between TopLoRA and standard LoRA to verify efficiency claims

## Open Questions the Paper Calls Out
None

## Limitations
- Projector architecture details are unspecified, creating ambiguity in reproduction
- Parameter efficiency claims require careful validation as projector adds parameters
- Different weight group choices (Q,V vs Q,K,V) lack clear justification for task-specific selection
- Performance benefits may vary significantly based on projector implementation choices

## Confidence
- High confidence: Mathematical formulation is clearly defined and reproducible. Experimental setup details are specified.
- Medium confidence: Performance claims are supported by experiments but projector architecture ambiguity creates uncertainty.
- Low confidence: Parameter efficiency claims are difficult to verify without exact projector implementation details.

## Next Checks
1. Implement minimal TopLoRA module with configurable projector and validate Σ_X values are token-specific and not collapsing to identity
2. Run controlled experiments on single GLUE task comparing TopLoRA vs LoRA at rank 8, measuring accuracy and parameter counts
3. Analyze learned Σ_X matrices across different tokens using visualization to confirm they capture meaningful patterns rather than random noise