---
ver: rpa2
title: 'Temporal vs. Spatial: Comparing DINOv3 and V-JEPA2 Feature Representations
  for Video Action Analysis'
arxiv_id: '2509.21595'
source_url: https://arxiv.org/abs/2509.21595
tags:
- video
- temporal
- action
- vision
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares DINOv3 and V-JEPA2 feature representations
  for video action analysis, addressing the fundamental architectural trade-off between
  spatial detail preservation and temporal modeling in self-supervised video understanding.
  DINOv3 processes frames independently using powerful spatial feature extraction,
  while V-JEPA2 employs joint temporal modeling across video sequences.
---

# Temporal vs. Spatial: Comparing DINOv3 and V-JEPA2 Feature Representations for Video Action Analysis

## Quick Facts
- arXiv ID: 2509.21595
- Source URL: https://arxiv.org/abs/2509.21595
- Authors: Sai Varun Kodathala; Rakesh Vunnam
- Reference count: 17
- DINOv3 achieves superior clustering (Silhouette 0.31) but shows 3x higher variance than V-JEPA2 (σ=0.288 vs 0.094)

## Executive Summary
This study directly compares DINOv3 and V-JEPA2 feature representations for video action analysis on the UCF Sports dataset. DINOv3 processes frames independently using powerful spatial feature extraction, while V-JEPA2 employs joint temporal modeling across video sequences. The evaluation reveals a fundamental architectural trade-off: DINOv3 achieves superior discrimination capability (6.16x separation ratio) and clustering performance (Silhouette score: 0.31 vs 0.21), particularly for pose-identifiable actions, but shows high performance variance. V-JEPA2 demonstrates consistent reliability across all action types with significantly lower variance (σ=0.094) but reduced peak performance. K-NN classification accuracy favors DINOv3 (89.47% vs 87.86%), though V-JEPA2 provides more predictable results across diverse action categories.

## Method Summary
The study compares DINOv3 (frame-independent spatial features) and V-JEPA2 (joint temporal modeling) using the UCF Sports dataset (140 videos, 13 action classes). Keyframes are extracted using a DWT-VGG16-LDA framework (16 frames per video via Haar wavelets, VGG-16 dual-path features, K-means clustering). DINOv3 features are extracted per-frame (768-dim) then temporally pooled, while V-JEPA2 processes 16-frame sequences jointly (1024-dim). Evaluation uses clustering metrics (Silhouette, Calinski-Harabasz), k-NN accuracy (k=1,3,5), intra-class similarity distributions, and inter-class discrimination ratios.

## Key Results
- DINOv3 achieves superior clustering performance (Silhouette score: 0.31 vs 0.21)
- DINOv3 demonstrates exceptional discrimination capability (6.16x separation ratio) for pose-identifiable actions
- V-JEPA2 exhibits consistent reliability across all action types with significantly lower variance (σ=0.094 vs 0.288)
- K-NN classification accuracy favors DINOv3 (89.47% vs 87.86%)

## Why This Works (Mechanism)

### Mechanism 1: DINOv3 Spatial Feature Extraction via Independent Frame Processing
- **Claim:** DINOv3 achieves superior clustering and discrimination for pose-identifiable actions by processing frames independently through a spatial encoder trained on large-scale image collections.
- **Mechanism:** The Vision Transformer Large backbone (ViT-L/16) extracts 768-dimensional dense visual representations from each frame without temporal context. Temporal aggregation (mean/max/attention pooling) is applied post-hoc to create sequence-level representations. The self-supervised distillation objective during pre-training creates features that capture object relationships and spatial semantics within single frames.
- **Core assumption:** Spatial configurations alone contain sufficient discriminative information for the target action categories.
- **Evidence anchors:**
  - [abstract] "DINOv3, which processes frames independently through spatial feature extraction... achieves superior clustering performance (Silhouette score: 0.31 vs 0.21) and demonstrates exceptional discrimination capability (6.16x separation ratio) particularly for pose-identifiable actions"
  - [Section 3.3.1] "The independence of frame-level processing in DINOv3 enables parallel computation and maintains computational efficiency... however, this approach relies entirely on post-hoc temporal aggregation to capture sequential dynamics"
  - [corpus] SOAP paper (FMR=0.56) confirms that spatio-temporal relation and motion information density are critical challenges in action recognition, supporting the observed performance gap on motion-dependent actions.
- **Break condition:** When actions require temporal dynamics to distinguish (Walk, Run show 0.187-0.280 intra-class similarity), spatial-only features fail to capture discriminative motion patterns.

### Mechanism 2: V-JEPA2 Joint Temporal Modeling via Masked Prediction
- **Claim:** V-JEPA2 provides consistent representation quality across diverse action types by jointly processing frame sequences through spatiotemporal tokenization with masked prediction objectives.
- **Mechanism:** The model processes input sequences of shape (16, 3, 256, 256) through spatiotemporal patch embeddings, creating tokens that encode both spatial content and temporal relationships. Masked prediction in learned representation space (not pixel reconstruction) forces the model to learn predictive representations of motion dynamics and cross-frame dependencies.
- **Core assumption:** Joint processing of temporal context improves generalization across actions with varying motion complexity.
- **Evidence anchors:**
  - [abstract] "V-JEPA2 exhibits consistent reliability across all action types with significantly lower performance variance (σ=0.094 vs 0.288)"
  - [Section 3.3.2] "The joint processing approach enables the model to capture subtle motion patterns, temporal dependencies, and dynamic interactions that unfold across multiple frames"
  - [corpus] SOAP paper (5 citations) emphasizes that capturing spatio-temporal relations and motion information is essential for few-shot action recognition, providing external validation for V-JEPA2's design philosophy.
- **Break condition:** When peak discrimination between similar actions is required and actions are primarily pose-distinguishable, the temporal smoothing may reduce inter-class separation.

### Mechanism 3: DWT-VGG16-LDA Keyframe Selection for Controlled Comparison
- **Claim:** Discrete Wavelet Transform-based keyframe extraction with VGG16 features and LDA clustering ensures both models receive representative temporal samples while controlling for sampling bias.
- **Mechanism:** Haar wavelets (L=2) compute approximation coefficients for each frame, with inter-frame differences Di capturing motion. Dual-path VGG16 extraction (appearance + motion pathways) creates fused features F=[Fa, Fm]. K-means (K=16) with LDA projection selects frames closest to cluster centroids, ensuring temporal coverage of biomechanical phases.
- **Core assumption:** 16 keyframes capture sufficient temporal information for action discrimination without information overload.
- **Evidence anchors:**
  - [Section 3.2.1] "This approach ensures that selected keyframes represent distinct phases of the athletic action while collectively providing comprehensive temporal coverage"
  - [Section 2.4] "DWT-based approach provides a principled framework that balances computational efficiency with temporal representativeness"
  - [corpus] No direct corpus evidence for this specific keyframe selection method; related work on video condensation (PRISM paper, FMR=0.47) addresses similar temporal sampling challenges but uses different approaches.
- **Break condition:** When actions have sub-second critical phases (e.g., impact moments in golf swings), uniform keyframe distribution may miss transient but discriminative frames.

## Foundational Learning

- **Self-Supervised Learning Paradigms**
  - Why needed here: DINOv3 uses teacher-student distillation; V-JEPA2 uses masked prediction. Understanding these objectives explains feature behavior.
  - Quick check question: Can you explain why predicting masked regions in representation space differs from pixel-space reconstruction?

- **Temporal Aggregation Strategies**
  - Why needed here: DINOv3 requires post-hoc temporal pooling (mean, max, attention) to create sequence representations. Choice affects performance.
  - Quick check question: When would max pooling outperform mean pooling for aggregating frame-level features in action recognition?

- **Clustering Quality Metrics**
  - Why needed here: Silhouette score and Calinski-Harabasz index measure representation quality without downstream classifiers. Essential for interpreting results.
  - Quick check question: What does a bimodal intra-class vs inter-class similarity distribution indicate about representation quality?

## Architecture Onboarding

- **Component map:**
Video Input → Preprocessing (256x256, 30fps normalization) → DWT-VGG16-LDA Keyframe Selection (16 frames) → Feature Extraction → Evaluation
  ├─ DINOv3 Path: Independent frame processing → 768-dim per frame → Temporal pooling → 768-dim sequence
  └─ V-JEPA2 Path: Joint sequence processing → 1024-dim spatiotemporal tokens → 1024-dim sequence

- **Critical path:** The keyframe selection module controls input to both models. If this fails (e.g., missing critical motion phases), downstream comparison is invalid regardless of model quality.

- **Design tradeoffs:**
  - DINOv3: Higher peak discrimination (6.16x separation) vs. 3x higher performance variance across action types
  - V-JEPA2: Consistent reliability (σ=0.094) vs. reduced inter-class separation for pose-distinguishable actions
  - Keyframe count: 16 frames balances temporal coverage vs. computational cost; fewer frames may miss motion details

- **Failure signatures:**
  - DINOv3: Intra-class similarity <0.3 on motion-heavy actions (Walk: 0.187, Run: 0.280) indicates spatial features insufficient
  - V-JEPA2: Inter-class similarity >0.7 between distinct actions indicates temporal smoothing over-dampens discrimination
  - Both models: Extraction failures on corrupted frames (DINOv3: 94.3% success vs V-JEPA2: 100%) signal preprocessing issues

- **First 3 experiments:**
  1. **Baseline validation:** Extract features from UCF Sports using default settings. Verify clustering metrics match reported values (DINOv3: Silhouette=0.31, V-JEPA2: Silhouette=0.21). If values diverge >10%, check preprocessing pipeline.
  2. **Action-specific probe:** For each action category, compute intra-class similarity distributions. Confirm DINOv3 shows bimodal performance (high on Lifting/Diving, low on Walk/Run) while V-JEPA2 shows consistent distributions. This validates the architectural trade-off.
  3. **Keyframe sensitivity:** Vary keyframe count (8, 16, 32) and measure performance variance. Assumption: V-JEPA2 performance should be more sensitive to reduced temporal sampling than DINOv3 due to its joint processing design.

## Open Questions the Paper Calls Out
- Can hybrid architectures combining DINOv3 and V-JEPA2 achieve both high discrimination and reliable temporal consistency?
- Do the observed performance trade-offs generalize to larger-scale, non-sports video datasets?
- How does the specific temporal aggregation strategy impact DINOv3's ability to capture motion dynamics?

## Limitations
- Temporal aggregation strategy for DINOv3 not specified (mean/max/attention options mentioned but configuration unknown)
- Keyframe selection framework may introduce sampling bias favoring one model
- Results may not generalize beyond UCF Sports dataset (13 action classes, 140 sequences)

## Confidence
- High confidence in performance variance patterns (DINOv3 σ=0.288 vs V-JEPA2 σ=0.094) given statistical consistency
- Medium confidence in architectural trade-off conclusions due to potential sampling bias
- Low confidence in generalizability to other video datasets beyond UCF Sports

## Next Checks
1. **Temporal pooling ablation study**: Systematically evaluate mean, max, and attention pooling strategies for DINOv3 across all action categories to identify the configuration responsible for peak performance and verify if results are sensitive to pooling choice.

2. **Dataset generalization probe**: Apply the same comparative framework to two additional video action datasets (e.g., HMDB51 and Something-Something V2) to test whether the observed architectural trade-offs hold across different action vocabularies and motion complexities.

3. **Full-sequence vs keyframe comparison**: Evaluate both models on complete video sequences without keyframe reduction to determine if the observed performance patterns persist when models receive temporal context beyond the sampled 16 frames.