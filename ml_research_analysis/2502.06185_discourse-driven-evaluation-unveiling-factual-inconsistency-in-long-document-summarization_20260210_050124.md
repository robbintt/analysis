---
ver: rpa2
title: 'Discourse-Driven Evaluation: Unveiling Factual Inconsistency in Long Document
  Summarization'
arxiv_id: '2502.06185'
source_url: https://arxiv.org/abs/2502.06185
tags:
- discourse
- long
- score
- document
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies factual inconsistency detection in long document
  summarization through discourse analysis. The paper identifies that complex sentences
  with multiple discourse units and certain discourse features are more prone to factual
  errors.
---

# Discourse-Driven Evaluation: Unveiling Factual Inconsistency in Long Document Summarization

## Quick Facts
- arXiv ID: 2502.06185
- Source URL: https://arxiv.org/abs/2502.06185
- Reference count: 40
- Key outcome: Discourse-based re-weighting improves factual inconsistency detection on long document summarization, achieving up to 7-point gains over strong baselines including LLMs.

## Executive Summary
This paper addresses factual inconsistency detection in long document summarization by leveraging discourse analysis. The authors propose a framework that re-weights NLI-based sentence scores based on discourse features like sentence depth and subtree height. By segmenting documents using RST tree structure rather than fixed-size windows, the approach better preserves context for verification. Experiments across scientific and legal domains demonstrate consistent performance improvements, with gains of up to 7 points on long document tasks. The method outperforms strong LLM-based baselines while adding minimal computational overhead through discourse parsing.

## Method Summary
The approach uses DMRST parser to create discourse trees from source documents and summaries, extracting features like normalized depth score and subtree height. It segments long documents via level-N tree traversal (falling back to 350-token windows if parsing fails), computes baseline NLI scores, applies re-weighting with exponential function based on depth, and aggregates to summary-level scores. The re-weighting formula emphasizes complex sentences (spanning multiple discourse units) and those with lower depth scores, which are statistically more error-prone.

## Key Results
- Outperforms strong LLM-based fact-checking models on 7 out of 11 benchmark datasets
- Achieves up to 7-point improvement on long document summarization tasks
- Demonstrates consistent gains across scientific (arXiv, ChemSumm) and legal (LEGAL SUMM) domains
- Adds minimal computational overhead while providing significant accuracy gains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Factual inconsistency errors in summaries are statistically correlated with specific discourse structures, allowing for targeted re-weighting of sentence scores.
- **Mechanism:** The paper posits that errors are non-randomly distributed; they concentrate in "complex sentences" (those spanning multiple Elementary Discourse Units) and units with lower normalized depth scores (often satellites). By applying an exponential re-weighting function based on `subtree_height` and `depth_score`, the framework amplifies the penalty for these high-risk sentences rather than treating all sentences equally.
- **Core assumption:** The statistical patterns observed in the `DIVER SUMM -SENT` analysis generalize to other domains.
- **Evidence anchors:** [abstract], [section 4.1], [corpus]
- **Break condition:** If a summary is very short (1-2 sentences), the discourse tree lacks sufficient structure to calculate meaningful depth differentials.

### Mechanism 2
- **Claim:** Segmentation based on discourse topology preserves the logical context required for accurate NLI verification better than fixed-size windowing.
- **Mechanism:** Standard approaches chunk text by fixed token counts, often severing the rhetorical link between a claim and its context. This method traverses the RST tree to extract Level 1 or Level 2 segments, ensuring related EDUs remain in the same context window.
- **Core assumption:** The DMRST parser can successfully construct coherent trees for long documents (3,000+ words).
- **Evidence anchors:** [abstract], [section 5.2], [corpus]
- **Break condition:** If the source document is a concatenation of unrelated texts (e.g., Multi-news), the global RST tree fails to form.

### Mechanism 3
- **Claim:** Weighted aggregation of sentence-level NLI scores outperforms LLM-based fact-checking by explicitly modeling linguistic salience rather than relying on implicit attention.
- **Mechanism:** Instead of prompting an LLM to judge the whole summary (which struggles with long contexts), this framework uses smaller NLI models for verification but controls the final verdict via discourse-weighted aggregation.
- **Core assumption:** NLI models provide a reliable raw signal for entailment if the context window issue is resolved via segmentation.
- **Evidence anchors:** [section 7], [corpus]
- **Break condition:** If the underlying NLI model is fundamentally biased or hallucinates entailment, the discourse weighting only scales the error.

## Foundational Learning

**Concept: Rhetorical Structure Theory (RST)**
- **Why needed here:** To understand how the `DMRST` parser breaks text into EDUs and trees, which creates the `depth_score` and `subtree_height` features used for weighting.
- **Quick check question:** Can you identify a "Nucleus" (core info) vs. a "Satellite" (supporting info) in a sentence relation?

**Concept: NLI-based Factuality**
- **Why needed here:** This is the base evaluation technique. You must understand Entailment vs. Contradiction to interpret the raw scores that the framework re-weights.
- **Quick check question:** If a summary says "The cat is on the mat" but the document says "The cat is near the mat," is this an entailment or a contradiction?

**Concept: Aggregation Strategies**
- **Why needed here:** The paper critiques standard "mean" or "min" pooling. You need to understand why a weighted average based on linguistic features offers a more nuanced summary-level score.
- **Quick check question:** Why would taking the *minimum* sentence score be too harsh for a 10-page document summary?

## Architecture Onboarding

**Component map:**
Input -> DMRST Parser -> Segmenter -> Feature Extractor -> Scorer -> Aggregator

**Critical path:** Parsing the Source Document is the computational bottleneck (though the authors claim it is fast). The accuracy of the Segmenter dictates the quality of the NLI Scorer's input.

**Design tradeoffs:**
- **Lv1 vs. Lv2 Segmentation:** Lv1 segments are larger (better context preservation) but may exceed model context limits; Lv2 is safer but risks fragmentation.
- **Parser dependency:** The system relies entirely on the DMRST parser's ability to handle domain-specific text.

**Failure signatures:**
- **Flat Scores:** If all summary sentences receive similar weights, the re-weighting logic is likely not triggering.
- **Context Miss:** If NLI scores are universally low, the Segmenter may be isolating sentences from their necessary context.

**First 3 experiments:**
1. **Sanity Check:** Run `AlignScore` on `AGGRE FACT` (short docs) to establish a baseline AUC.
2. **Segmentation Ablation:** Implement only the Level-1 Segmentation logic on long docs (`DIVER SUMM`) without re-weighting to isolate the impact of structure preservation.
3. **Weighting Ablation:** Apply the re-weighting logic to short summaries to verify the authors' claim that it provides "minimal" benefit on short text.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can incorporating specific discourse relation types (e.g., Elaboration, Contrast) improve inconsistency detection performance beyond structural features?
  - **Basis in paper:** Authors state discourse-relation information remains unused on the system level.
  - **Why unresolved:** Current method only utilizes tree structure (depth, height) and nuclearity, ignoring semantic labels of edges.
  - **What evidence would resolve it:** Experiments adding relation labels as features to aggregation model.

- **Open Question 2:** Do the correlations between discourse features and factual errors generalize to narrative domains like story or book-length summarization?
  - **Basis in paper:** Authors list story summarization or further book-length summarization tasks as future work.
  - **Why unresolved:** Current study focuses on scientific and legal texts, which possess different rhetorical structures than narratives.
  - **What evidence would resolve it:** Analyzing error patterns and model performance on narrative-focused datasets like BookSum.

- **Open Question 3:** Would utilizing graph neural networks (GNNs) to model the discourse tree provide superior performance over the current exponential re-weighting algorithm?
  - **Basis in paper:** Authors propose extending the modeling into a more complex version, such as applying a graph neural network to the tree structure.
  - **Why unresolved:** Current approach uses a hand-crafted exponential function for re-weighting scores based on depth.
  - **What evidence would resolve it:** Implementing and benchmarking a GNN-based aggregation layer against current StructScore.

## Limitations
- The framework's effectiveness depends on the quality and availability of the DMRST parser for different domains
- Performance gains are minimal on short summaries (1-3 sentences) where discourse structure lacks sufficient variation
- The method requires parsing entire source documents, which may be computationally expensive for extremely long texts

## Confidence
**High:** The experimental results show consistent improvements across multiple benchmarks and domains, with specific statistical significance reported for key findings.
**Medium:** The discourse feature correlation analysis is well-supported, but the generalization to narrative domains remains untested.
**Low:** The specific DMRST parser implementation details and dataset access for LEGAL SUMM are not fully specified in the paper.

## Next Checks
1. Verify DMRST parser installation and test on a sample scientific and legal document to ensure proper tree construction
2. Implement the discourse feature extraction (normalized depth score and subtree height) and validate against the paper's reported distributions
3. Run the baseline NLI model (AlignScore) on short documents to establish reference AUC before implementing the discourse-weighted aggregation