---
ver: rpa2
title: 'Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models
  for German Law'
arxiv_id: '2601.14160'
source_url: https://arxiv.org/abs/2601.14160
tags:
- legal
- data
- generation
- german
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a synthetic data generation pipeline for
  fine-tuning large language models (LLMs) on German legal question answering. The
  method generates diverse, statute-grounded question-answer pairs directly from authoritative
  German legal texts, using difficulty-graded generation and LLM-based filtering to
  ensure factual consistency and reduce hallucinations.
---

# Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law

## Quick Facts
- arXiv ID: 2601.14160
- Source URL: https://arxiv.org/abs/2601.14160
- Reference count: 27
- Key outcome: Synthetic data generation pipeline fine-tunes LLaMA 3.1 and Gemma 3 on German legal QA, achieving up to 40 pp improvement over baselines while maintaining general language understanding

## Executive Summary
This paper presents a synthetic data generation pipeline for fine-tuning large language models (LLMs) on German legal question answering. The method generates diverse, statute-grounded question-answer pairs directly from authoritative German legal texts, using difficulty-graded generation and LLM-based filtering to ensure factual consistency and reduce hallucinations. The approach produces training data without manual annotation or reliance on web-crawled sources. Experiments with LLaMA 3.1 (8B) and Gemma 3 (12B) show that models fine-tuned with this synthetic data significantly outperform baselines on held-out German legal QA benchmarks, achieving up to 40 percentage points improvement in open-ended correctness on statute-based tests, while maintaining or slightly improving general language understanding. The results demonstrate that carefully designed synthetic supervision can effectively specialize LLMs for high-stakes, knowledge-intensive domains like German law.

## Method Summary
The approach generates synthetic German legal QA pairs through a multi-stage pipeline using authoritative legal texts as source material. First, the method extracts relevant statutes from German legal corpora (BGB, StGB, etc.) and partitions them into difficulty levels. Then, it employs GPT-4 to generate question-answer pairs conditioned on these statutes, with prompts designed to ensure variety and legal grounding. A multi-stage filtering process follows, where initial generations are reviewed by GPT-4 for factual consistency, adherence to statute-only grounding, and answer quality. This reviewer model rejects hallucinated or inconsistent pairs. The surviving data is then used to fine-tune open-source LLMs (LLaMA 3.1-8B, Gemma 3-12B) via supervised fine-tuning (SFT). The pipeline avoids manual annotation and web-crawled sources, relying entirely on authoritative legal text and controlled generation.

## Key Results
- Models fine-tuned on synthetic legal data achieve up to 40 percentage points improvement in open-ended correctness on statute-based German legal QA benchmarks
- Fine-tuned models outperform both general-purpose LLMs and other fine-tuned models on German legal question answering tasks
- Fine-tuning preserves or slightly improves general language understanding as measured by standard benchmarks

## Why This Works (Mechanism)
The synthetic data generation pipeline works by creating high-quality, domain-specific training examples that are both diverse and factually grounded in authoritative legal sources. By using difficulty-graded generation, the pipeline produces examples across a spectrum of complexity, preventing overfitting to simple patterns. The LLM-based filtering stage acts as a quality gate, rejecting hallucinated or inconsistent pairs before they enter training, which directly addresses the well-known tendency of LLMs to confabulate facts. This controlled, multi-stage process allows the model to learn accurate legal reasoning patterns from synthetic supervision without the cost and bias risks of manual annotation.

## Foundational Learning
- **German Legal Corpus Structure**: Understanding how German statutes (BGB, StGB, etc.) are organized and referenced is essential for extracting valid training examples. Quick check: Verify that extracted passages are complete, properly cited sections.
- **Difficulty Grading in Legal Texts**: Legal passages vary in complexity; accurate grading ensures balanced training data. Quick check: Compare model performance on easy vs. hard generated examples.
- **Hallucination Detection in LLMs**: Recognizing when generated answers introduce non-existent legal concepts or misinterpret statutes. Quick check: Run reviewer filter on known hallucinated examples.
- **Supervised Fine-Tuning (SFT) Principles**: The process of adapting a pretrained LLM to a specific domain using labeled examples. Quick check: Monitor validation loss during fine-tuning to detect overfitting.
- **Prompt Engineering for Controlled Generation**: Designing prompts that elicit consistent, statute-grounded outputs from the generator model. Quick check: Inspect a sample of generated QA pairs for grounding compliance.
- **Statute-Only Grounding Requirement**: Ensuring all answers are directly traceable to provided legal text, excluding external knowledge or inference. Quick check: Verify each answer cites the relevant statute passage.

## Architecture Onboarding

### Component Map
Legal Corpus -> Difficulty Grading -> Prompt Generation -> GPT-4 Generator -> GPT-4 Reviewer -> Filtered QA Pairs -> SFT Fine-tuning -> Specialized LLM

### Critical Path
The critical path is: Legal Corpus extraction and difficulty grading → Prompt generation → GPT-4 generator → GPT-4 reviewer → Filtered QA pairs → SFT fine-tuning. Each step must succeed for high-quality synthetic data to be produced; failure at any stage (e.g., poor prompt design, weak reviewer filtering) propagates to degraded model performance.

### Design Tradeoffs
- **Generation vs. Filtering Cost**: Using GPT-4 for both generation and review increases quality but also cost and latency; weaker generators reduce cost but may require more aggressive filtering.
- **Statute-Only vs. Open Domain**: Restricting to statute-only grounding improves factual consistency but may limit the model's ability to handle case-based reasoning found in real legal practice.
- **Synthetic vs. Manual Data**: Synthetic data avoids manual annotation costs and biases but introduces risks of compounding LLM errors; manual data is expensive but potentially more reliable.

### Failure Signatures
- **Low Reviewer Acceptance Rate**: Indicates overly strict filtering or poor generator quality, leading to insufficient training data.
- **High Rejection Rate on Hallucination**: Suggests the generator is prone to confabulation; requires prompt redesign or stronger conditioning.
- **Performance Plateau During SFT**: May indicate synthetic data lacks sufficient diversity or difficulty, or that the model has memorized training patterns.
- **Generalization Drop on Unseen Legal Topics**: Suggests synthetic data does not adequately cover the breadth of the legal domain.

### First 3 Experiments
1. Generate and review a small batch of QA pairs to test prompt effectiveness and reviewer accuracy before full-scale data production.
2. Fine-tune a baseline model on a subset of filtered data and evaluate on a held-out legal QA benchmark to measure early learning signal.
3. Perform an ablation study comparing models fine-tuned on data generated by different teacher models (e.g., GPT-4 vs. smaller open-source model) to assess dependency on generation quality.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does incorporating real-time retrieval of legal statutes during inference further improve the accuracy of models fine-tuned on this synthetic data?
- Basis in paper: [explicit] The conclusion states: "Future research will focus on incorporating real-time retrieval of legal statutes to improve accuracy further."
- Why unresolved: The current study focuses on static supervised fine-tuning (SFT) and evaluates the models as standalone generators or passive recipients of context, without an active retrieval component.
- What evidence would resolve it: A comparison of the fine-tuned models' performance in a Retrieval-Augmented Generation (RAG) framework versus the static SFT baseline on dynamic legal queries.

### Open Question 2
- Question: Can the difficulty-graded generation pipeline be effectively adapted for domains relying on case law (precedent) rather than statutory text?
- Basis in paper: [inferred] The methodology strictly utilizes statutory texts (BGB, StGB, etc.) and enforces "statute-only grounding," excluding the interpretive nature of case law found in datasets like LegalMC4.
- Why unresolved: The "statute-only" constraint and explicit citation requirements are designed for fixed legislative provisions; it is unclear if this logic holds for unstructured, evolving court decisions.
- What evidence would resolve it: Applying the generation pipeline to a corpus of court rulings and evaluating the resulting model's ability to perform case-based reasoning.

### Open Question 3
- Question: To what extent is the success of the fine-tuning dependent on the specific proprietary model (GPT-4) used to generate the synthetic data?
- Basis in paper: [inferred] The pipeline relies on GPT-4 for generation (Appendix A), and the authors note that "naive data generation approaches can even lead to diminishing performance."
- Why unresolved: It is not tested whether a weaker or open-source generator could produce data that passes the reviewer filter while still providing a strong training signal.
- What evidence would resolve it: An ablation study generating synthetic data with different teacher models (e.g., smaller open-source models) and comparing the downstream performance of the student models.

## Limitations
- Reliance on synthetic data introduces potential compounding errors from the generation process itself, despite multi-stage filtering
- Evaluation focuses on specific German legal QA benchmarks, which may not fully capture real-world performance across all German legal subdomains
- Uses relatively small open-source models (8B and 12B parameters), so scalability and performance with larger architectures remain uncertain

## Confidence
- **High confidence** in the technical pipeline design and its ability to generate synthetic legal QA pairs with improved factual consistency compared to naive generation
- **Medium confidence** in the quantitative results, given the controlled benchmark evaluation, though external validation on real-world legal tasks would strengthen claims
- **Medium confidence** in the generalization claims, as the study demonstrates effectiveness within the specific domain of German law but does not extensively test transfer to other legal systems or domains

## Next Checks
1. Conduct human evaluation of synthetic examples across multiple German legal domains to assess real-world applicability and identify systematic generation errors
2. Test the fine-tuned models on a broader range of legal reasoning tasks beyond QA, including statutory interpretation and case law analysis
3. Evaluate model performance when scaled to larger parameter sizes (e.g., 70B+ models) to determine if the synthetic data approach remains effective at scale