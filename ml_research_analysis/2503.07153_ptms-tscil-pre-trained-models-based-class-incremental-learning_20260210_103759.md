---
ver: rpa2
title: PTMs-TSCIL Pre-Trained Models Based Class-Incremental Learning
arxiv_id: '2503.07153'
source_url: https://arxiv.org/abs/2503.07153
tags:
- learning
- feature
- drift
- time
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first exploration of pre-trained models
  (PTMs) for time series class-incremental learning (TSCIL), addressing the challenge
  of catastrophic forgetting when learning new classes sequentially. The proposed
  method leverages frozen PTM backbones with shared adapters for parameter-efficient
  tuning, combined with knowledge distillation and a two-stage Feature Drift Compensation
  Network (DCN) to correct feature drift across tasks.
---

# PTMs-TSCIL Pre-Trained Models Based Class-Incremental Learning

## Quick Facts
- arXiv ID: 2503.07153
- Source URL: https://arxiv.org/abs/2503.07153
- Reference count: 40
- This paper presents the first exploration of pre-trained models (PTMs) for time series class-incremental learning (TSCIL), achieving state-of-the-art performance with 1.4%-6.1% accuracy gains.

## Executive Summary
This paper introduces the first method leveraging pre-trained models for time series class-incremental learning, addressing catastrophic forgetting when learning new classes sequentially. The approach combines frozen PTM backbones with shared adapters for parameter-efficient tuning, knowledge distillation for stability, and a two-stage Feature Drift Compensation Network to correct feature drift across tasks. The method achieves state-of-the-art performance on five real-world datasets, demonstrating 1.4%-6.1% accuracy improvements over existing approaches.

## Method Summary
The method uses frozen pre-trained PTM backbones with shared adapters for incremental adaptation. A two-stage training strategy addresses feature drift: Stage 1 jointly trains adapters, DCN, and classifiers with knowledge distillation; Stage 2 refines the DCN to map old feature spaces to new ones. The approach employs Gaussian prototype modeling and unified classifier retraining with synthetic samples. Training uses SGD with OneCycleLR (LR=0.005, Batch Size=16) across three stages: adapter/DCN training with KD losses, DCN refinement, and unified classifier retraining.

## Key Results
- Achieves final accuracy gains of 1.4%-6.1% compared to existing PTM-based approaches
- Outperforms state-of-the-art methods across five real-world time series datasets
- Demonstrates effective mitigation of catastrophic forgetting through the proposed drift compensation framework

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constraining feature extractor updates via Parameter-Efficient Tuning preserves PTM generalization while allowing adaptation.
- **Mechanism:** Freezes transformer backbone, inserts bottleneck adapters (down-scale → non-linear → up-scale) to limit parameter updates and prevent overwriting pre-trained temporal features.
- **Core assumption:** PTM backbones possess universal temporal feature extraction capabilities requiring minimal tuning for downstream discrimination.
- **Evidence anchors:** Abstract mentions frozen PTM backbones with shared adapters; Section III-B discusses minimizing parameter updates; corpus cites PTM efficacy in CIL.

### Mechanism 2
- **Claim:** Two-stage Drift Compensation Network enables accurate projection of old class prototypes into new feature space.
- **Mechanism:** Learns transformation from old space (F^t-1) to new space (F^t) via bias-free linear layer, trained in two stages: joint training with adapter for preliminary estimation, then refining with frozen models.
- **Core assumption:** Feature space transformation between tasks can be approximated by a learnable function consistent across old and new class distributions.
- **Evidence anchors:** Abstract describes two-stage training for modeling feature space transformations; Section III-C details freezing both spaces for DCN refinement.

### Mechanism 3
- **Claim:** Unified classifier retraining with Gaussian-sampled features consolidates knowledge and resolves decision boundary bias.
- **Mechanism:** Models classes as Gaussian distributions using DCN-corrected prototypes and estimated covariance to sample synthetic features for retraining a single unified classifier.
- **Core assumption:** Class representations in PTM feature space form unimodal Gaussian distributions allowing synthetic sampling to approximate true data manifold.
- **Evidence anchors:** Section III-D describes modeling each class as Gaussian distribution for feature sampling; Section IV-F-1 shows unified classifier training enhances plasticity.

## Foundational Learning

- **Concept: Stability-Plasticity Dilemma**
  - **Why needed here:** The paper frames TSCIL as balancing catastrophic forgetting (stability) and new knowledge acquisition (plasticity).
  - **Quick check question:** Does freezing the entire model solve TSCIL? (Answer: No, it solves stability but fails plasticity/adaptation).

- **Concept: Feature Drift (Semantic Drift)**
  - **Why needed here:** The DCN specifically addresses this problem where model updates change feature vector meanings, invalidating old prototypes.
  - **Quick check question:** If I update my model weights, does the feature vector for an "old" image remain the same? (Answer: No, it shifts/drifts).

- **Concept: Parameter-Efficient Tuning (PET)**
  - **Why needed here:** The method relies on Adapters - small bottleneck modules inserted into transformers to avoid forgetting risks of full fine-tuning.
  - **Quick check question:** Why add an adapter layer instead of just fine-tuning the top layers of the transformer? (Answer: To preserve pre-trained representations and reduce trainable parameters).

## Architecture Onboarding

- **Component map:** Moment (Pre-trained Time Series Transformer) → Frozen → Adapter (Bottleneck modules) → Trainable → DCN (Bias-free Linear Layer) → Trainable (2-stage) → Classifier (Cosine Classifier) → Retrained

- **Critical path:**
  1. Task t Start: Copy backbone state from t-1 (frozen immediately for reference)
  2. Stage 1 (Joint): Train Adapter + DCN + New Classifier using Cosine Loss, KD Loss, and DC Loss
  3. Stage 2 (Refine): Freeze everything, train only DCN to map frozen t-1 features → frozen t features
  4. Stage 3 (Retrain): Update old prototypes using DCN, sample features from Gaussian distributions, retrain unified classifier

- **Design tradeoffs:**
  - Adapter Scale Factor (s): Controls plasticity - low s restricts updates (stable but low accuracy); high s increases plasticity but requires stronger KD
  - DCN Architecture: Paper assumes simple linear layer is optimal; more complex networks might overfit or fail to generalize drift
  - Exemplar-Free: Design avoids storing raw data, relying entirely on Gaussian prototype approximation accuracy

- **Failure signatures:**
  - High Forgetting (F_T): DCN failing to correct prototypes (check Prototype Distance metric); Adapter scale too high without sufficient KD
  - Low Learning Accuracy (A_cur): Adapter scale too low (model cannot learn new time series patterns)
  - Prototype Distance Divergence: If DCN distance increases in later tasks, drift assumption may be breaking down

- **First 3 experiments:**
  1. Baseline Validation: Run "Fine-tuning" vs. "Joint-train" on five datasets to establish upper/lower performance bounds
  2. Ablation on DCN: Compare "S1-only" (joint training) vs. "S1+S2" (proposed) to verify if 2nd stage refinement reduces Prototype Distance
  3. Hyperparameter Sensitivity: Vary Adapter Scale (s) and KD weight (α) to find stability-plasticity sweet spot

## Open Questions the Paper Calls Out

- **Open Question 1:** How does performance vary when utilizing alternative large-scale time series pre-trained backbones like Timer or MOIRAI instead of Moment?
  - Basis: Authors list Timer and MOIRAI as recent models but exclusively use Moment for all experiments
  - Why unresolved: Method's reliance on Moment's specific embedding structure may not transfer directly to generative or unified architectures
  - Evidence needed: Comparative experiments implementing framework on Timer and MOIRAI backbones

- **Open Question 2:** Can the framework generalize to non-sensor time series domains like financial or medical data lacking periodicity of human activity data?
  - Basis: Experimental evaluation restricted to five HAR or gesture-based datasets
  - Why unresolved: HAR data contains distinct repetitive patterns easier for DCN to model compared to stochastic or chaotic financial signals
  - Evidence needed: Benchmarking on standard time series classification datasets from non-sensor domains

- **Open Question 3:** Is the Gaussian distribution assumption valid for complex time series classes that might exhibit multi-modal feature distributions?
  - Basis: Section III-D explicitly states each class exhibits unimodal distribution modeled as Gaussian
  - Why unresolved: If true feature distribution is multi-modal, generated Gaussian samples will be unrepresentative
  - Evidence needed: Analysis of feature embeddings' distribution shape and performance comparisons using non-parametric sampling

## Limitations
- Three hyperparameters underspecified (Adapter bottleneck size, loss weighting factors α/β, and optimal adapter scale s) requiring reproduction
- Gaussian prototype assumption may break for multi-modal classes, though this limitation is not explored
- Method's effectiveness on non-sensor time series domains remains unverified

## Confidence
- **High confidence** in overall framework architecture and necessity of drift compensation for TSCIL
- **Medium confidence** in DCN's effectiveness as simple linear layer, pending validation on non-linear drift cases
- **Low confidence** in generalization of Gaussian prototype assumption to complex, real-world class distributions

## Next Checks
1. Reproduce baseline comparison: Run "Fine-tuning" vs. "Joint-train" on all five datasets to establish performance bounds
2. DCN ablation validation: Compare Stage 1-only vs. Stage 1+2 to verify prototype drift reduction (Prototype Distance metric)
3. Hyperparameter sensitivity sweep: Vary adapter scale s (0.1→1.5) and KD weight α to identify stability-plasticity sweet spot on validation task