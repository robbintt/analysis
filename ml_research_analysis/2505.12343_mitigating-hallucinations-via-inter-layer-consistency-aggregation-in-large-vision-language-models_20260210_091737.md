---
ver: rpa2
title: Mitigating Hallucinations via Inter-Layer Consistency Aggregation in Large
  Vision-Language Models
arxiv_id: '2505.12343'
source_url: https://arxiv.org/abs/2505.12343
tags:
- decoding
- arxiv
- layer
- dcla
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Mitigating Hallucinations via Inter-Layer Consistency Aggregation in Large Vision-Language Models

## Quick Facts
- arXiv ID: 2505.12343
- Source URL: https://arxiv.org/abs/2505.12343
- Reference count: 40
- Primary result: DCLA reduces hallucinations in LVLMs via layer-wise consistency aggregation, improving MME scores from 1491.56 to 1520.14 (LLaVA-1.5-7b)

## Executive Summary
This paper introduces Dynamic Cross-Layer Aggregation (DCLA), a training-free method to mitigate hallucinations in Large Vision-Language Models (LVLMs) by enforcing inter-layer consistency during autoregressive decoding. The approach constructs a dynamic semantic reference from earlier layers and selectively corrects later layers that deviate from this reference. Unlike prior methods that apply fixed corrections or contrastive decoding, DCLA uses adaptive gating based on cosine similarity thresholds to intervene only when semantically deviated representations are detected. Experimental results on MME and POPE benchmarks demonstrate significant improvements in perception tasks while maintaining robustness across model variants.

## Method Summary
DCLA operates during inference by aggregating hidden states from previous transformer layers with exponential decay weighting to create a semantic reference (H_agg). For each layer, cosine similarity between the current hidden state and H_agg is computed; if similarity falls below threshold τ, a linear fusion correction is applied. The method maintains a registry of corrected layers to ensure persistent integration of refined states into subsequent aggregations. Key hyperparameters include correction strength α≈0.82 and trigger threshold τ≈0.74, determined via grid search. The approach requires caching intermediate hidden states but adds no training overhead.

## Key Results
- MME total score improves from 1491.56 (baseline) to 1520.14 (LLaVA-1.5-7b) and 1525.73 (LLaVA-NEXT)
- Outperforms baseline Regular decoding, VCD, DoLa, and DAMO on perception tasks
- Adaptive correction (τ=0.74) significantly better than fixed-range correction (1404.00 for all layers vs. 1520.14 optimal)
- High sensitivity to hyperparameters α and τ observed across multiple model variants

## Why This Works (Mechanism)

### Mechanism 1: Cross-Layer Semantic Reference Construction
DCLA constructs a dynamic semantic reference by aggregating representations from previous layers with exponential decay favoring recent but grounded context. This anchor stabilizes decoding when later layers deviate from visual-grounded information. Earlier layers encode more faithful visual-semantic structures before overfitting, noise, or language priors dominate in deeper layers.

### Mechanism 2: Adaptive Gated Correction via Similarity Threshold
The method computes cosine similarity between current hidden state and aggregated reference at each layer. Correction triggers only when similarity falls below threshold τ, preventing disruption of natural semantic evolution. This selective intervention addresses the finding that indiscriminate correction degrades performance significantly.

### Mechanism 3: Persistent Corrected State Integration
Once corrected, refined states contribute to all subsequent aggregation steps through a registry tracking corrected layers. This ensures that beneficial corrections propagate forward and prevents error recurrence, maintaining semantic consistency throughout the decoding process.

## Foundational Learning

- **Concept: Layer-wise representation evolution in transformers**
  - Why needed: DCLA's core insight relies on understanding how semantics transform across layers, with early layers capturing different information than later ones
  - Quick check question: Why might a transformer's earlier layers contain more faithful input-grounded information than its later layers?

- **Concept: Cosine similarity on hidden states**
  - Why needed: The adaptive gating mechanism uses cosine similarity as a deviation signal to detect semantically deviated representations
  - Quick check question: What properties of cosine similarity make it suitable for comparing semantic content in hidden state space?

- **Concept: Autoregressive decoding in LVLMs**
  - Why needed: DCLA operates within the decoding loop, requiring understanding of where and how to intervene during token-by-token generation
  - Quick check question: At what point during autoregressive generation does DCLA inject its correction, and what information does it require access to?

## Architecture Onboarding

- **Component map:** Layer Aggregator -> Deviation Detector -> Correction Module -> State Registry
- **Critical path:** 1) Forward pass produces h_i at layer i, 2) Aggregator constructs H_agg from layers 0 to i-1 using corrected states where available, 3) Deviation detector computes cos_sim(h_i, H_agg), 4) If cos_sim < τ: apply correction, store ĥ_i, add i to C, 5) If cos_sim ≥ τ: pass h_i unchanged, 6) Proceed to layer i+1
- **Design tradeoffs:** Correction strength α (higher trusts original more), trigger threshold τ (lower triggers more corrections), weighting scheme (exponential decay vs. uniform), memory overhead (must cache all intermediate states)
- **Failure signatures:** Total score drops sharply when correcting all 32 layers (1404.00 vs. 1491.56 baseline), performance varies non-monotonically with τ and α
- **First 3 experiments:** 1) Replicate Table 2 comparing DCLA against baselines on MME perception subset, 2) Ablate correction range testing fixed ranges vs. adaptive τ-gated correction, 3) Hyperparameter sensitivity sweep with α ∈ [0.80, 0.90], τ ∈ [0.70, 0.80] on POPE subsets

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the optimal correction strength (α) and trigger threshold (τ) be determined dynamically per sample rather than relying on dataset-wide grid search?
- **Basis in paper:** The authors perform extensive ablation studies showing high sensitivity to α and τ, with MME scores ranging from ~1492 to 1520 based on these fixed values
- **Why unresolved:** The paper establishes a "best" fixed setting (α=0.82, τ=0.74) via grid search but doesn't propose instance-level adaptation mechanisms
- **What evidence would resolve it:** A method that adaptively adjusts hyperparameters based on input complexity or internal uncertainty estimates

### Open Question 2
- **Question:** What is the computational latency and memory overhead of DCLA during inference compared to vanilla decoding and baseline methods?
- **Basis in paper:** The method requires storing and aggregating hidden states from all preceding layers but provides no analysis of inference speed or resource consumption
- **Why unresolved:** While training-free, practical utility depends on throughput efficiency which is unstated
- **What evidence would resolve it:** Reported latency (ms/token) and peak GPU memory usage for DCLA versus standard autoregressive decoding

### Open Question 3
- **Question:** Does the cosine similarity threshold effectively distinguish between harmful hallucinations and beneficial semantic abstraction in deeper layers?
- **Basis in paper:** The paper posits that "semantically deviated layers" cause hallucinations but later layers naturally perform abstract reasoning which may structurally diverge from early layers
- **Why unresolved:** Correcting "deviated" layers might inadvertently suppress high-level reasoning required for complex tasks
- **What evidence would resolve it:** Analysis of task performance on complex reasoning benchmarks comparing cases where correction triggers frequently versus rarely

## Limitations
- Scalability concerns with O(L²) memory complexity for larger models with 70+ layers
- High sensitivity to hyperparameters α and τ requiring careful tuning
- Limited evaluation to perception tasks without testing on open-ended reasoning or abstract concepts

## Confidence

**High Confidence (Level 1):**
- DCLA successfully reduces hallucinations on perception tasks compared to baselines
- The core mechanism of cross-layer aggregation and adaptive correction is technically sound
- Performance improvements are reproducible on tested models and datasets

**Medium Confidence (Level 2):**
- Earlier layers consistently contain more faithful visual-semantic information than later layers
- Cosine similarity on flattened hidden states reliably detects semantically deviated representations
- The adaptive gating mechanism generalizes across different LVLM architectures

**Low Confidence (Level 3):**
- Performance gains will transfer to non-perception tasks and open-ended reasoning
- The method scales effectively to models with 70+ layers
- Hyperparameter settings generalize to other model families

## Next Checks

1. **Cross-Domain Generalization Test:** Evaluate DCLA on abstract reasoning benchmarks (like BBH for mathematical reasoning or LogicVLA for logical inference) to verify if hallucination mitigation transfers beyond perception tasks.

2. **Scalability Benchmark:** Implement DCLA on a 70-layer vision-language model and measure (a) memory consumption during decoding, (b) relative decoding speed compared to baseline, and (c) whether performance gains persist at scale.

3. **Ablation on Correction Range:** Systematically test different layer ranges for correction (e.g., layers 16-32, 24-32, 8-24) while keeping τ and α fixed to understand whether the adaptive mechanism is essential or if simple range restriction suffices.