---
ver: rpa2
title: 'MuRating: A High Quality Data Selecting Approach to Multilingual Large Language
  Model Pretraining'
arxiv_id: '2507.01785'
source_url: https://arxiv.org/abs/2507.01785
tags:
- multilingual
- data
- language
- quality
- murater
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of multilingual data selection
  for large language model pretraining, where existing model-based approaches focus
  almost exclusively on English. The proposed method, MuRating, aggregates multiple
  English-language quality raters via pairwise comparisons using a Bradley-Terry model,
  then transfers these quality judgments across 17 target languages through translation.
---

# MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining

## Quick Facts
- **arXiv ID**: 2507.01785
- **Source URL**: https://arxiv.org/abs/2507.01785
- **Reference count**: 40
- **Primary result**: MuRating improves multilingual LLM pretraining by transferring English quality judgments through translation, achieving consistent gains across 17 languages

## Executive Summary
This work addresses the challenge of multilingual data selection for large language model pretraining, where existing model-based approaches focus almost exclusively on English. The proposed method, MuRating, aggregates multiple English-language quality raters via pairwise comparisons using a Bradley-Terry model, then transfers these quality judgments across 17 target languages through translation. This creates a unified multilingual evaluator trained on monolingual, cross-lingual, and parallel text pairs. When applied to pretrain 1.2B and 7B parameter LLaMA models, MuRating consistently improves performance across both English and multilingual benchmarks compared to strong baselines including QuRater, AskLLM, and FineWeb2-HQ, achieving average gains of 1-3.4 points on English tasks and 1.8 points on multilingual evaluations.

## Method Summary
MuRating builds a multilingual quality evaluator by first aggregating four English raters (AskLLM, DCLM, FineWeb-Edu, QuRater) through pairwise comparisons using a Bradley-Terry model. The resulting scorer is then transferred to 17 target languages by translating scored English document pairs, creating a training set of monolingual, cross-lingual, and parallel pairs. A BGE-M3 encoder is fine-tuned on this data to learn language-agnostic quality judgments. The trained MuRater scores a large multilingual corpus, selecting the top 10% per language for LLM pretraining. This approach enables effective data selection without requiring native multilingual quality labels.

## Key Results
- MuRating consistently outperforms strong baselines (QuRater, AskLLM, FineWeb2-HQ) on both English and multilingual benchmarks
- Average gains of 1-3.4 points on English tasks and 1.8 points on multilingual evaluations
- Pairwise supervision provides more stable multilingual scoring than pointwise methods
- Incorporating cross-lingual and parallel pairs improves language-agnostic quality assessment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggregating multiple diverse English "raters" via a Bradley-Terry model produces a more robust and unified quality signal than any single rater.
- **Mechanism:** The system samples text pairs ($t_A, t_B$) and collects scalar scores from four distinct English raters. It converts these into a probabilistic preference $P_{A>B}$ using majority vote. A Bradley-Terry model is then trained to output a scalar $s_\theta(t)$ such that the difference $s_\theta(t_A) - s_\theta(t_B)$ predicts the preference probability.
- **Core assumption:** Individual model-based raters exhibit idiosyncratic biases; a majority-vote aggregation cancels out these orthogonal errors while preserving the "true" quality signal.
- **Evidence anchors:** [abstract] "aggregates multiple English 'raters' via pairwise comparisons... to learn unified document-quality scores" [section 3.1] Describes the confidence score $P_{A>B}$ and the Bradley-Terry loss function (Equation 2).

### Mechanism 2
- **Claim:** Translating English preference pairs allows for effective transfer of quality judgment to non-English languages without requiring native multilingual quality labels.
- **Mechanism:** High-quality English pairs $(t_A^{en}, t_B^{en})$ with known preferences are translated into target languages $m$. The preference label is directly transferred ($P_{A>B}^{en} \approx P_{A>B}^{m}$). A multilingual encoder (BGE-M3) is fine-tuned on these translated pairs.
- **Core assumption:** Translation preserves relative quality; if Text A is better than Text B in English, the translated versions maintain that ranking.
- **Evidence anchors:** [abstract] "projects these judgments through translation to train a multilingual evaluator" [section 4.3.2] Shows pairwise transfer exhibits strong cross-lingual consistency compared to pointwise scoring.

### Mechanism 3
- **Claim:** Inclusion of cross-lingual and parallel pairs acts as a regularizer to enforce language-agnostic scoring consistency.
- **Mechanism:**
  1. **Cross-lingual pairs:** Compare Text A (Lang X) vs Text B (Lang Y) to bridge representation gaps.
  2. **Parallel pairs:** Compare Text A (Lang X) vs Text A (Lang Y) and assign a neutral preference (0.5). This forces the model to output identical scores for semantically equivalent content regardless of language.
- **Core assumption:** "Quality" is a property of semantic content and structure, not the specific language tokenization or syntax.
- **Evidence anchors:** [section 3.3] Explicitly defines the parallel pair loss $L_{parallel}$ to minimize score divergence. [figure 4] Shows lower Mean Squared Error (MSE) and slopes closer to 1.0 for raters trained with this alignment.

## Foundational Learning

- **Concept: Bradley-Terry Models**
  - **Why needed here:** This is the mathematical engine used to convert noisy pairwise comparisons into a stable, continuous quality score for every document.
  - **Quick check question:** If Model X prefers Doc A and Model Y prefers Doc B, how does the Bradley-Terry formulation resolve this into a single score? (Answer: It predicts the probability of preference based on the latent score difference).

- **Concept: Pairwise vs. Pointwise Supervision**
  - **Why needed here:** The paper explicitly argues that transferring *pairwise* rankings is more robust than transferring absolute *pointwise* scores because translation noise affects absolute values more than relative rankings.
  - **Quick check question:** Why would a translation that changes the tone of a document hurt pointwise scoring more than pairwise scoring?

- **Concept: Multilingual Representation Learning (BGE-M3)**
  - **Why needed here:** The MuRater is built on BGE-M3. Understanding that this model already maps different languages into a shared semantic space is key to understanding why the transfer learning works.
  - **Quick check question:** Why is a shared semantic space critical for the "Parallel Pair" regularization mechanism to work?

## Architecture Onboarding

- **Component map:** Raw web text -> English Scorer (Bradley-Terry aggregation) -> Data Builder (GPT-4o translation) -> Trainer (BGE-M3 fine-tuning) -> Selector (top 10% scoring) -> Downstream (LLaMA pretraining)
- **Critical path:** The generation of the training pairs (specifically the translation step) is the primary cost center and bottleneck. If translation fidelity is low, the MuRater learns to score based on "translation artifacts" rather than content quality.
- **Design tradeoffs:** MuRater(E) vs. MuRater(M): The paper compares scoring English pairs then translating (E) vs translating then scoring (M). Onboarding should start with **MuRater(E)** as it shows better stability.
- **Failure signatures:**
  - **Language Collapse:** The rater gives uniformly high scores to one language regardless of content (check slope in Figure 4).
  - **Translation Bias:** High-scoring documents in target languages read like obvious translations rather than natural native text.
- **First 3 experiments:**
  1. **Validation of English Aggregation:** Check if the Bradley-Terry aggregation aligns with human judgment on a small held-out English set before scaling.
  2. **Translation Fidelity Check:** Replicate the scatter plot analysis (Figure 5) for your specific target languages to verify that pairwise transfer is indeed more stable than pointwise for your data.
  3. **Ablation on Regularization:** Train a MuRater *without* parallel pairs and measure the variance of scores for the same document translated into different languages.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the MuRating framework be effectively adapted to evaluate narrative and creative text domains, given that current English raters primarily optimize for factual and informational content?
- **Basis in paper:** [explicit] The authors state in the Limitations section that "since the English raters used in our framework primarily focused on factual and informational content, our auto-rater exhibits limited performance on narrative and creative domains."
- **Why unresolved:** The current methodology aggregates existing educational/factual raters (like FineWeb-Edu and QuRater), which inherently bias the selection against creative writing; the paper does not propose a mechanism to balance these distinct quality dimensions.
- **What evidence would resolve it:** A modified MuRater trained on pairs specifically annotated for narrative coherence or creative merit, demonstrating improved performance on storytelling benchmarks without losing factual knowledge capabilities.

### Open Question 2
- **Question:** Do language-specific rater designs or culturally aligned data selection strategies yield superior performance compared to the current English-anchored transfer approach?
- **Basis in paper:** [explicit] The authors suggest that "further research could explore language-specific rater designs or culturally aligned data selection strategies to better capture the unique characteristics of each language."
- **Why unresolved:** The current framework relies on translating English preferences to other languages, which assumes a universal definition of quality that may overlook culturally specific nuances or values inherent to non-English corpora.
- **What evidence would resolve it:** A comparative study showing that a rater trained on native human annotations for a specific language outperforms MuRater(E) on culturally specific benchmarks or localized knowledge tasks.

### Open Question 3
- **Question:** To what extent does the reliance on GPT-4o for translation and annotation introduce idiosyncratic biases into the data selection pipeline?
- **Basis in paper:** [explicit] The Limitations section notes that "the reliance on GPT-4o introduces potential biases and idiosyncrasies inherent to proprietary large language models."
- **Why unresolved:** The framework uses GPT-4o both to translate the training pairs and to provide the ground-truth preference labels, potentially creating a feedback loop where GPT-4o's specific stylistic preferences are amplified in the final pretraining data.
- **What evidence would resolve it:** An analysis comparing data selected via GPT-4o annotation versus selection via human-annotated pairs or alternative open-source models, measuring the stylistic and topical differences in the resulting pretraining corpora.

## Limitations
- The method assumes translation perfectly preserves pairwise quality judgments across all 17 target languages
- Aggregating English raters with potentially correlated biases may amplify systematic preferences rather than detect true quality
- Forcing language-agnostic scoring may not account for legitimate differences in what constitutes "quality" across languages

## Confidence

- **High confidence**: English-to-multilingual performance gains (3.4 points average on English tasks, 1.8 points on multilingual evaluations)
- **Medium confidence**: The Bradley-Terry aggregation mechanism for combining English raters
- **Low confidence**: The assumption that translation preserves pairwise quality judgments across all language pairs

## Next Checks

1. **Translation fidelity validation**: Score 1,000 parallel document pairs (original + translated) across 3-4 target languages. Measure correlation between original and translated scores; if correlation <0.95, investigate whether translation artifacts are driving the quality judgments rather than content.

2. **Rater bias analysis**: Train two MuRaters using different subsets of English raters (e.g., {AskLLM, DCLM} vs {FineWeb-Edu, QuRater}). Compare downstream performance and examine where the raters disagree on English pairs to identify systematic biases.

3. **Language-specific quality calibration**: Select 100 high-scoring documents from each of 3 target languages and have native speakers rate quality independently. Compare human ratings with MuRater scores to identify systematic over/under-scoring for specific languages.