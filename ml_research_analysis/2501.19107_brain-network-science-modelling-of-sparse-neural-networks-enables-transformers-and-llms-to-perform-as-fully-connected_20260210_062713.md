---
ver: rpa2
title: Brain network science modelling of sparse neural networks enables Transformers
  and LLMs to perform as fully connected
arxiv_id: '2501.19107'
source_url: https://arxiv.org/abs/2501.19107
tags:
- training
- network
- chts
- sparsity
- links
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Cannistraci-Hebb Training soft rule (CHTs),
  a brain-inspired dynamic sparse training method that addresses limitations of prior
  gradient-free approaches. CHTs improves upon the original Cannistraci-Hebb training
  (CHT) by introducing a soft sampling strategy for link removal and regrowth, balancing
  exploration and exploitation of network topology to avoid epitopological local minima.
---

# Brain network science modelling of sparse neural networks enables Transformers and LLMs to perform as fully connected

## Quick Facts
- arXiv ID: 2501.19107
- Source URL: https://arxiv.org/abs/2501.19107
- Reference count: 40
- Primary result: Dynamic sparse training method achieves up to 99% sparsity while outperforming fully connected networks in image classification and matching performance in language models

## Executive Summary
This paper introduces CHTs, a brain-inspired dynamic sparse training method that improves upon the original Cannistraci-Hebb training by incorporating soft sampling strategies and GPU-friendly link prediction. The method enables extremely sparse neural networks (up to 99% sparsity) to match or exceed fully connected network performance across MLPs, Transformers, and LLaMA models. Key innovations include a soft sampling approach for link removal and regrowth that avoids epitopological local minima, a node-based link prediction algorithm with reduced computational complexity, and brain-like initialization using the Bipartite Receptive Field model.

## Method Summary
CHTs combines brain-inspired network science with gradient-free dynamic sparse training. It uses Bipartite Receptive Field (BRF) initialization to create brain-like spatial connectivity patterns, followed by iterative remove-regrow cycles with soft sampling. Link removal uses either weight magnitude (WM) or relative importance (RI) metrics with probabilistic sampling controlled by temperature parameter δ. Regrowth employs node-based CH2-L3n link prediction that approximates path-based methods with O(N³) complexity. The method optionally includes a sigmoid gradual density decay strategy (CHTss) for better high-sparsity performance. Training proceeds with standard forward/backward passes, network percolation to remove disconnected neurons, and mask updates at specified intervals.

## Key Results
- CHTs achieves up to 99% sparsity in MLP architectures while outperforming fully connected networks on image classification tasks
- LLaMA models trained with CHTss at 90% sparsity match fully connected performance on language modeling with perplexity within 3-5 points
- The method is robust to low-precision training (bfloat16), unlike competing methods such as RigL and GraNet
- BRF initialization with r≈0.25 provides optimal performance across different model architectures

## Why This Works (Mechanism)

### Mechanism 1
Soft (probabilistic) sampling for link removal and regrowth enables escape from epitopological local minima (ELM)—states where removed and regrown links significantly overlap. Instead of deterministically selecting top-scoring links, CHTs samples from a multinomial distribution where probabilities derive from link prediction scores (regrowth) or importance metrics (removal). The temperature parameter δ adapts from 0.5→0.75 during training, starting exploratory and becoming more exploitative as weights stabilize. Early-training weights and topology scores are unreliable; rigid selection traps networks in cyclical removal-regrowth patterns.

### Mechanism 2
Node-based CH2-L3n link prediction approximates path-based CH3-L3p with O(N³) complexity (vs. O(N·d³)), enabling GPU-friendly matrix operations for large-scale models. The node-based variant scores candidate links via common neighbors on L3 paths, computing di*z/de*z (internal/external local community links) rather than enumerating all length-3 paths. This translates to sparse matrix multiplications amenable to GPU parallelization. Common-neighbor topology sufficiently captures the local-community structure that CH theory leverages for link prediction.

### Mechanism 3
Bipartite Receptive Field (BRF) initialization provides brain-like spatial connectivity that outperforms random (ER) or rewiring-based (BSW) alternatives. BRF parameter r∈[0,1] controls clustering around the adjacency matrix diagonal. Low r→spatially local connections (receptive field structure); r=1→ER random. The model also allows fixed or uniform degree distributions for output nodes. Spatial locality in weight matrices correlates with meaningful feature hierarchies in vision/language tasks.

## Foundational Learning

- **Concept: Dynamic Sparse Training (DST)**
  - Why needed here: CHTs operates within the DST paradigm—fixed sparsity budget with periodic remove-regrow cycles. Without understanding DST fundamentals (pruning frequency, sparsity scheduling), the soft rule modifications won't make sense.
  - Quick check question: Can you explain why DST differs from "prune once, then train"?

- **Concept: Link Prediction in Bipartite Networks**
  - Why needed here: The CH2-L3n regrowth mechanism is fundamentally a link prediction problem on bipartite graphs (layer-to-layer connections). Understanding common neighbors, local community structure, and why L3 paths matter in bipartite settings is essential.
  - Quick check question: Why do bipartite networks require L3 (not L2) paths for link prediction?

- **Concept: Weight Magnitude vs. Relative Importance Pruning**
  - Why needed here: CHTs offers two removal criteria with different percolation behaviors. WM preserves high-magnitude weights (rapid percolation); RI preserves connections to low-degree nodes (higher active neuron retention).
  - Quick check question: Which removal method would you expect to preserve more neurons after percolation, and why?

## Architecture Onboarding

- **Component map:**
  BRF Initialization → Forward/Backward Pass → Soft Removal → Network Percolation → Soft Regrowth via CH2-L3n

- **Critical path:**
  1. Initialization choice: BRF with r=0.1-0.25 (Transformer/LLMs) or CSTI (MLPs with direct input access)
  2. Removal softness δ: Linear schedule 0.5→0.9 for all tasks
  3. Removal fraction ζ: 0.3 for MLP/Transformer, 0.1 for LLaMA
  4. Regrowth: CH2-L3n-soft (node-based, multinomial sampling from scores)

- **Design tradeoffs:**
  - CHTss (with sigmoid decay) superior at high sparsity (90%+); CHTs may match or exceed at moderate sparsity (70%) where decay schedule offers less benefit
  - Fixed vs. Uniform degree (BRF): Uniform provides marginal gains on some tasks; fixed is simpler
  - WM vs. RI removal: WM better for MNIST/EMNIST; RI better for Fashion-MNIST (dataset-dependent)
  - Precision: CHTs/CHTss robust to bfloat16; RigL/GraNet degrade significantly under low precision

- **Failure signatures:**
  - ELM trap: ITOP rate plateaus early (check via cumulative link activation metric)—increase softness or ζ
  - Excessive percolation: ANP rate drops too low (<30%)—switch WM→RI or reduce ζ
  - Runtime explosion at high density: Node-based helps but O(N³) still bites—consider limiting to ≤30% density layers

- **First 3 experiments:**
  1. Baseline sanity check: Run CHTs on MLP-MNIST at 99% sparsity with BRF(r=0.25), WM-soft removal, ζ=0.3. Target: >98.7% accuracy, ANP≈25-35%. Compare against SET and RigL baselines.
  2. Component ablation: Disable soft sampling (use deterministic top-k) and measure ITOP convergence. Expect faster plateau and ~1-2% accuracy drop.
  3. Scale test: Apply CHTss to LLaMA-60M on OpenWebText at 90% sparsity with sigmoid decay (si=0.5→sf=0.9). Target: perplexity within 3-5 points of FC baseline. Monitor for bfloat16 stability vs. RigL.

## Open Questions the Paper Calls Out

### Open Question 1
Can the soft sampling temperature (δ) be determined automatically for each layer based on its topological features rather than a fixed schedule? The current implementation uses a heuristic, linearly increasing δ from 0.5 to 0.75 across training, which requires manual tuning and may not be optimal for every layer's unique topology. A dynamic, topology-aware scheduling algorithm for δ that converges faster or achieves higher final accuracy without manual hyperparameter search would resolve this.

### Open Question 2
Does the CHTs/CHTss framework maintain its performance advantage when scaling to larger Large Language Models (LLMs) such as LLaMA-7B? Experiments in the paper are limited to LLaMA-1B; it is unverified if the O(N³) link prediction approximation or the gradient-free regrowth strategy scales effectively to billions of parameters with denser connectivity. Benchmark results on LLaMA-7B showing that CHTs/CHTss matches or exceeds dense baseline performance while maintaining the reported sparsity levels would resolve this.

### Open Question 3
Do CHTs and CHTss provide tangible wall-clock speedups during training compared to fully connected networks on hardware optimized for unstructured sparsity? While the method reduces theoretical FLOPs, the complexity of the node-based link predictor (O(N³)) and the lack of optimized hardware libraries for the specific sparse patterns mean actual latency reductions are not proven. Direct comparisons of training throughput (tokens/second) and total convergence time on specialized sparse-hardware against dense baselines would resolve this.

## Limitations
- ITOP metric's empirical grounding for ELM detection is inferred from limited ablation rather than direct ELM measurement
- BRF initialization's brain-like property is asserted through analogy rather than demonstrated correlation with biological network statistics
- Node-based CH2-L3n approximation's fidelity to path-based CH3-L3p is computationally demonstrated but not analytically proven

## Confidence

- **High confidence**: BRF initialization outperforms ER/BSW baselines in MLPs (multiple datasets, consistent gains)
- **Medium confidence**: Soft sampling improves ITOP and perplexity vs deterministic selection (based on ablation but limited to one translation task)
- **Medium confidence**: Node-based CH2-L3n matches path-based performance with better runtime (empirical comparison only)
- **Low confidence**: BRF initialization's brain-like property provides performance benefit beyond random initialization (no biological correlation analysis)

## Next Checks

1. **Theoretical ELM analysis**: Prove or disprove that soft sampling with temperature δ∈[0.5,0.75] guarantees escape from any epitopological local minimum for bipartite networks with degree distributions matching BRF initialization

2. **BRF biological correlation**: Measure whether BRF-generated networks exhibit known brain network properties (small-worldness, modularity, rich-club coefficients) and whether these properties correlate with task performance

3. **CH2-L3n approximation bounds**: Derive analytical bounds on the error introduced by node-based approximation versus path-based CH3-L3p link prediction for bipartite networks at various densities