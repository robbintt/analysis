---
ver: rpa2
title: 'InFact: Informativeness Alignment for Improved LLM Factuality'
arxiv_id: '2505.20487'
source_url: https://arxiv.org/abs/2505.20487
tags:
- answers
- answer
- llama-3
- informativeness
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of LLM factuality beyond just correctness
  - it addresses the issue that LLMs often generate factually correct but less informative
  answers when more specific ones exist. The authors propose InFACT, a two-stage training
  framework that first teaches the model to prefer more informative answers and to
  abstain when uncertain, then uses preference-based training to align the model toward
  generating maximally informative and correct responses.
---

# InFact: Informativeness Alignment for Improved LLM Factuality

## Quick Facts
- arXiv ID: 2505.20487
- Source URL: https://arxiv.org/abs/2505.20487
- Authors: Roi Cohen; Russa Biswas; Gerard de Melo
- Reference count: 11
- Primary result: Two-stage training framework that improves both informativeness and factuality of LLM responses without tradeoff

## Executive Summary
This paper addresses a critical gap in LLM evaluation by focusing not just on factual correctness but on the informativeness of generated responses. The authors observe that LLMs often provide factually correct but less specific answers when more informative alternatives exist. To address this, they propose InFACT (Informativeness FACTuality), a two-stage training framework that first teaches models to prefer informative answers and abstain when uncertain, then aligns them toward generating maximally informative and correct responses. The approach demonstrates significant improvements across multiple model sizes and QA benchmarks.

## Method Summary
The InFACT framework employs a two-stage training approach. First, the model is trained to recognize and prefer more informative answers over less specific but factually correct alternatives, while also learning to abstain from answering when uncertainty is high. In the second stage, preference-based alignment is applied to fine-tune the model toward generating responses that maximize both informativeness and factual accuracy. The method is evaluated across multiple open-source models including Llama-3.2-1B, Llama-3.2-3B, Llama-3.1-8B, Mistral-7B, and Qwen2.5-7B on various QA benchmarks.

## Key Results
- Significant improvements in both informativeness and factual precision across all tested models
- Higher F1 scores achieved by substantially increasing precision while maintaining recall
- Demonstrated that informativeness and factuality can be improved simultaneously without tradeoff
- Consistent performance gains across different model sizes from 1B to 7B parameters

## Why This Works (Mechanism)
The two-stage training framework works by first establishing a preference for informative content during the initial training phase, which creates a foundation for the model to recognize when more specific information is available. The abstention capability prevents the model from generating low-quality responses when uncertain. The subsequent preference-based alignment stage then refines this capability, ensuring the model consistently generates responses that are both maximally informative and factually accurate.

## Foundational Learning
- **Informativeness vs Correctness**: Understanding that factually correct but vague answers are suboptimal
  - Why needed: Current LLM evaluation often overlooks response specificity
  - Quick check: Compare "Paris is in France" vs "Paris is the capital of France"

- **Preference-based Learning**: Using human preference data to align model behavior
  - Why needed: Traditional supervised learning doesn't capture nuanced quality preferences
  - Quick check: Show model pairs of answers and ask which is preferred

- **Uncertainty Estimation**: Ability to recognize when not to answer
  - Why needed: Prevents degradation of overall quality through incorrect but confident responses
  - Quick check: Test model's abstention rate on unanswerable questions

- **Multi-objective Optimization**: Balancing informativeness and factual accuracy
  - Why needed: These objectives can sometimes conflict and need careful balancing
  - Quick check: Measure precision-recall tradeoff curve

## Architecture Onboarding

Component Map:
Raw Input -> Uncertainty Detection -> Informativeness Scoring -> Preference Alignment -> Output Generation

Critical Path:
The critical path flows through uncertainty detection first, as this determines whether the model should attempt to answer. If answering, the informativeness scoring guides response generation, while preference alignment ensures the final output meets both criteria.

Design Tradeoffs:
The two-stage approach adds training complexity but provides better separation of concerns compared to joint training. The abstention mechanism trades coverage for quality, which is beneficial when accuracy is prioritized over completeness.

Failure Signatures:
- Over-abstention leading to missed opportunities for informative responses
- Incorrect prioritization of less relevant but more specific information
- Preference bias in the alignment data leading to skewed responses

Three First Experiments:
1. Compare abstention rates between baseline and InFact models on unanswerable questions
2. Measure informativeness scores of responses to factoid questions
3. Evaluate precision-recall tradeoff curves across different model sizes

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several areas warrant further investigation. The scalability of the approach to larger models beyond the tested 7B parameter range remains unexplored. The impact of different preference data curation strategies on final performance is not thoroughly examined. Additionally, the framework's effectiveness across diverse domains and specialized knowledge areas needs validation.

## Limitations
- Evaluation relies on automated metrics which may not fully capture nuanced response quality
- Two-stage training approach increases computational complexity and resource requirements
- Limited testing on larger foundation models beyond the 7B parameter range
- Potential biases in preference data are not thoroughly addressed

## Confidence

High confidence in:
- Technical framework and implementation details
- Experimental methodology and baseline comparisons
- Reproducibility of the two-stage training approach

Medium confidence in:
- Claim that informativeness alignment doesn't sacrifice factuality
- Relationship between informativeness and factuality objectives
- Scalability to larger models and different domains

Low confidence in:
- Practical applicability across diverse real-world scenarios
- Long-term stability of the aligned behaviors
- Performance in specialized domains requiring expert knowledge

## Next Checks

1. Conduct comprehensive human evaluation studies to validate automated metric improvements and assess real-world user comprehension and satisfaction with more informative responses.

2. Test the approach on larger foundation models (70B+ parameters) to evaluate scalability and determine if the two-stage training framework remains effective at larger scales.

3. Evaluate the method across diverse domains including medicine, law, and technical documentation to assess generalizability and identify domain-specific challenges.