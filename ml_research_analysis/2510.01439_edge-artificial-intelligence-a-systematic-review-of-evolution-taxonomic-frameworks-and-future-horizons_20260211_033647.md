---
ver: rpa2
title: 'Edge Artificial Intelligence: A Systematic Review of Evolution, Taxonomic
  Frameworks, and Future Horizons'
arxiv_id: '2510.01439'
source_url: https://arxiv.org/abs/2510.01439
tags:
- edge
- computing
- intelligence
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic review presents a multi-dimensional taxonomy for
  analyzing Edge AI, integrating deployment location, processing capabilities (TinyML,
  TinyDL, TinyRL, federated learning), application domains, and hardware architectures.
  Following PRISMA 2020 guidelines, the analysis of 79 primary studies reveals the
  evolution from cloud to edge intelligence, identifies current technological enablers
  and application domains, and critically assesses challenges including resource constraints,
  security, power consumption, and connectivity.
---

# Edge Artificial Intelligence: A Systematic Review of Evolution, Taxonomic Frameworks, and Future Horizons

## Quick Facts
- arXiv ID: 2510.01439
- Source URL: https://arxiv.org/abs/2510.01439
- Reference count: 40
- Primary result: Multi-dimensional taxonomy for Edge AI integrating deployment location, processing capabilities, application domains, and hardware architectures, revealing evolution from cloud to edge intelligence and identifying key challenges.

## Executive Summary
This systematic review presents a comprehensive multi-dimensional taxonomy for analyzing Edge AI systems, following PRISMA 2020 guidelines to analyze 79 primary studies. The framework integrates four dimensions: deployment location (Device, Network, Cloud Edge), processing capabilities (TinyML, TinyDL, TinyRL, federated learning), application domains, and hardware architectures. The analysis reveals the evolution from cloud-centric to edge-distributed intelligence, identifies current technological enablers and application domains, and critically assesses challenges including resource constraints, security, power consumption, and connectivity. The review highlights emerging opportunities in neuromorphic hardware, continual learning algorithms, edge-cloud collaboration, and trustworthy AI.

## Method Summary
The systematic literature review followed PRISMA 2020 guidelines across 11 academic databases spanning January 2000 to June 2025, yielding 2,220 initial records reduced to 79 primary studies through duplicate removal and multi-stage screening. The methodology employed strict inclusion/exclusion criteria and a standardized data extraction template to classify studies across four dimensions: deployment location (D1), processing capabilities (D2), application domains (D3), and hardware architectures (D4). The review provides qualitative synthesis of technological trends, challenges, and future horizons while acknowledging limitations in inter-rater reliability assessment and economic trade-off analysis.

## Key Results
- Multi-dimensional taxonomy successfully classifies Edge AI systems across deployment, processing, application, and hardware dimensions
- Evolution from cloud to edge intelligence driven by latency, bandwidth, and privacy requirements
- Model compression techniques (quantization/pruning) enable deep learning on KB-scale memory, mW-scale power hardware
- Federated learning enables privacy-preserving collaborative intelligence without exposing raw data

## Why This Works (Mechanism)

### Mechanism 1: Latency Reduction via Proximity
Co-locating computation with data sources reduces operational latency and bandwidth consumption compared to centralized cloud processing. By processing data at the Device Edge or Network Edge, the system avoids physical propagation delays and network congestion inherent in round-trips to distant data centers, enabling ultralow latency required for closed-loop control systems.

### Mechanism 2: Feasibility via Model Compression (Quantization/Pruning)
Aggressive model optimization techniques enable deep learning inference on severely resource-constrained hardware. Quantization (reducing floating-point precision to integers) and pruning (removing redundant weights) decrease memory footprint and computational energy cost, allowing TinyML models to run on microcontrollers that would otherwise lack capacity for standard deep learning.

### Mechanism 3: Privacy via Decoupled Aggregation (Federated Learning)
Decoupling local data processing from global model aggregation allows collaborative intelligence without exposing raw sensitive data. Federated Learning keeps raw data on Device Edge and transmits only model updates to a central aggregator, creating a privacy-preserving loop where the system learns from distributed datasets without data ever leaving the physical device.

## Foundational Learning

- **Concept: The Von Neumann Bottleneck**
  - Why needed here: The paper discusses In-Memory Computing and specialized accelerators as future horizons. Understanding the energy cost of moving data between memory and processor is essential to grasp why these architectures are proposed.
  - Quick check question: Why does moving data from RAM to the CPU consume more energy than the computation itself in standard architectures?

- **Concept: Quantization (Int8 vs Float32)**
  - Why needed here: Critical for understanding power consumption and TinyML sections. The paper assumes understanding that reducing 32-bit floats to 8-bit integers reduces memory bandwidth and energy.
  - Quick check question: How does reducing the bit-depth of a model's weights affect its memory footprint and potential accuracy?

- **Concept: Non-IID Data (Federated Learning)**
  - Why needed here: Section 6.3 mentions handling extreme non-IID data as a challenge. Learners need to know that edge devices generate highly specific data which makes training a single global model difficult.
  - Quick check question: Why is training a model on data that is unique to each device (non-IID) harder than training on a shuffled centralized dataset?

## Architecture Onboarding

- **Component map:** Sensors -> Microcontroller (MCU) or SoC -> Accelerator (TPU/NPU) -> Radio (5G/Wi-Fi)
- **Critical path:** 1. Profile Constraints: Define Power (mW), Memory (KB/MB), and Latency (ms) limits based on Deployment Location. 2. Select Paradigm: Choose TinyML (extreme constraint), TinyDL (moderate), or FL (collaborative). 3. Optimize Model: Apply pruning and quantization to fit Hardware Type. 4. Deploy & Monitor: Push to edge and monitor for drift or connectivity failure.
- **Design tradeoffs:** Accuracy vs. Latency (optimizing for speed trades off inference accuracy), Privacy vs. Utility (keeping data local maximizes privacy but prevents global model improvement), Cost vs. Flexibility (ASICs offer efficiency but low flexibility; FPGAs offer flexibility but higher cost/complexity).
- **Failure signatures:** Accuracy Collapse (model fails to generalize after quantization), Battery Drain ("Always-on" sensing consumes too much power), Stalled Learning (global model accuracy fluctuates wildly indicating non-IID data or poisoned updates).
- **First 3 experiments:** 1. Latency Baseline: Measure inference time of standard model vs. quantized model on reference edge device to quantify trade-off. 2. Bandwidth Simulation: Simulate "disconnected" scenario to verify autonomous operation and determine minimum data retention. 3. Power Profiling: Measure energy cost of single inference cycle vs. transmitting result to cloud to identify break-even point for local processing.

## Open Questions the Paper Calls Out

### Open Question 1
Can on-device explainability methods generate meaningful model explanations using less than the computational resources required for inference itself on severely constrained TinyML hardware? The paper states this requires methods capable of generating concise, meaningful explanations for model decisions directly on edge hardware, utilizing only a fraction of the resources required for inference itself. No standardized, lightweight explanation methods exist for microcontroller-scale devices with kilobytes of memory.

### Open Question 2
How can bias detection and mitigation be performed effectively on decentralized, non-IID edge data during federated learning without centralizing raw data? The paper identifies unique challenges of bias detection in models trained on decentralized, non-IID edge data requiring specialized techniques for detecting, quantifying, and mitigating bias directly on edge devices or during federated processes.

### Open Question 3
What defense mechanisms can provide adversarial robustness for Edge AI systems while operating within the extreme computational constraints of edge hardware accelerators? The paper calls for developing robustness against adversarial attacks through defense mechanisms specifically designed for the computational constraints of edge hardware, including efficient adversarial training and runtime detection algorithms.

### Open Question 4
Can continual learning systems adapt to concept drift on TinyML devices without catastrophic forgetting, while maintaining model accuracy within extreme memory and energy budgets? The paper projects continual and lifelong learning systems as critical for edge adaptation, while establishing that TinyML devices operate with kilobyte-scale memory and milliwatt power budgets that fundamentally constrain learning capabilities.

## Limitations
- Taxonomy classification lacks inter-rater reliability reporting for the four-dimensional framework
- Federated Learning mechanisms supported primarily by theoretical assertions rather than empirical evidence from corpus
- Hardware deployment scenarios mapped comprehensively but economic trade-offs (capex vs opex) that drive real-world adoption are not systematically addressed

## Confidence
- **High Confidence**: Cloud-to-edge evolution narrative and latency/bandwidth reduction mechanisms supported by multiple primary studies
- **Medium Confidence**: Model compression techniques enabling TinyML, though accuracy degradation thresholds vary significantly across application domains
- **Low Confidence**: Federated Learning privacy guarantees, as the review acknowledges known attack vectors but doesn't critically assess whether reviewed studies adequately address these vulnerabilities

## Next Checks
1. **Taxonomy Consistency Audit**: Randomly sample 20% of the 79 primary studies and have independent reviewers classify them using the D1-D4 framework to calculate inter-rater reliability scores (Cohen's kappa).
2. **Energy-Efficiency Benchmarking**: Identify the most-cited TinyML implementations in the corpus and reproduce their power consumption measurements on identical hardware platforms to validate claimed mW-scale efficiency.
3. **FL Vulnerability Assessment**: Extract all Federated Learning studies from the corpus and systematically catalog the security/privacy measures they implement, then cross-reference these against documented gradient inversion and poisoning attack methodologies to identify coverage gaps.