---
ver: rpa2
title: Enhancing Chemical Explainability Through Counterfactual Masking
arxiv_id: '2508.18561'
source_url: https://arxiv.org/abs/2508.18561
tags:
- molecular
- molecules
- masking
- counterfactual
- masked
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of explaining molecular property
  predictions in a chemically meaningful way. The authors propose counterfactual masking
  (CM), a method that replaces masked molecular fragments with chemically plausible
  alternatives generated by diffusion models or fragment-based methods.
---

# Enhancing Chemical Explainability Through Counterfactual Masking

## Quick Facts
- arXiv ID: 2508.18561
- Source URL: https://arxiv.org/abs/2508.18561
- Authors: Łukasz Janisiów; Marek Kochańczyk; Bartosz Zieliński; Tomasz Danel
- Reference count: 40
- Primary result: Counterfactual Masking (CM) improves molecular property prediction explanation by replacing masked fragments with chemically plausible alternatives rather than zeroing features.

## Executive Summary
This paper addresses the challenge of explaining molecular property predictions in a chemically meaningful way. The authors propose counterfactual masking (CM), a method that replaces masked molecular fragments with chemically plausible alternatives generated by diffusion models or fragment-based methods. This approach ensures that masked molecules remain within the training distribution, unlike traditional feature zeroing which produces out-of-distribution examples. CM is demonstrated to improve the evaluation of explainable AI methods by providing more robust fidelity metrics and generating chemically valid counterfactual explanations.

## Method Summary
Counterfactual Masking replaces masked molecular fragments with chemically plausible alternatives rather than zeroing features. The method identifies important atoms using explainers like Grad-CAM or GNNExplainer, extracts the context and attachment points, then uses generative models (CReM or DiffLinker) to create valid replacements. CReM uses database fragments for high validity, while DiffLinker uses diffusion models for flexibility with partial rings. The approach is validated across multiple datasets and model architectures, showing improved consistency in explaining model predictions and producing more interpretable counterfactuals compared to baseline methods.

## Key Results
- CM outperforms feature zeroing in explanation consistency, achieving 63-76% consistency vs 50% for random masking
- CReM maintains high validity (>85%) while DiffLinker struggles with fragments having >2 attachment points
- Topology leakage from feature zeroing is eliminated, with prediction differences dropping significantly when using CM
- The method provides more reliable fidelity metrics by keeping masked molecules within the training distribution

## Why This Works (Mechanism)

### Mechanism 1: Distribution-Aligned Masking
Replacing masked molecular fragments with chemically plausible alternatives maintains the input within the model's training distribution, preventing erratic out-of-distribution behaviors. Standard masking creates "voids" that distort manifold geometry, while CM samples replacements from feasible molecule distributions.

### Mechanism 2: Topology Bias Elimination
Feature zeroing leaks structural information because graph edges remain connected; CM severs these connections to test true feature importance. In GNNs, zeroing node features leaves adjacency matrix intact, allowing "ghost" signals to propagate via topology.

### Mechanism 3: Counterfactual Consistency Validation
A valid explanation should cause a predictable shift in prediction when the "important" substructure is altered. If an explainer identifies fragment as positive for property, replacing it with a neutral fragment should decrease the prediction. The consistency score measures this alignment.

## Foundational Learning

- **Graph Message Passing**: Understanding how GNNs propagate information across edges is crucial for grasping topology leakage. Quick check: If you mask a node's features to zero but keep its edges, can its neighbors still receive a message? (Answer: Yes, typically a zero-vector or specific aggregation of neighbors).

- **Chemical Validity vs. Distribution**: A molecule can be "valid" (satisfies valency rules) but "OOD" (unlike anything in training set). CM aims to solve both. Quick check: Why is a molecule with all aromatic atoms replaced by zeros likely "valid" structurally but "OOD" semantically?

- **Fidelity Metrics in XAI**: The paper critiques how we measure explanation quality (fidelity) and proposes new baseline. Quick check: In standard fidelity, if removing a feature drops accuracy, is the feature "important"? What if removing it created an impossible input?

## Architecture Onboarding

- **Component map**: Explainer (e.g., Grad-CAM) -> Context Builder -> Generative Engine (CReM/DiffLinker) -> Filter (validity check) -> Evaluator (prediction comparison)

- **Critical path**: The validity of the Generative Engine. If the generative model fails to fill the gap, the pipeline stalls. CReM struggles with >2 attachment points.

- **Design tradeoffs**:
  - CReM: Higher chemical validity and synthetic accessibility; requires existing database fragments; cannot modify partial rings
  - DiffLinker: Can handle arbitrary linkers/partial rings; produces more 3D structural variety; suffers from lower validity and potential unsynthesizability

- **Failure signatures**:
  - High Std Dev in Consistency: High variance (±30%) indicates sensitivity to specific molecular contexts or explainer noise
  - Ring Constraints: CReM forces full ring replacement; if explainer marks only 1 atom in a ring, the whole ring is destroyed

- **First 3 experiments**:
  1. **Topology Leak Check**: Mask differing parts using Feature Zeroing vs. CM. If prediction difference remains high for Zeroing but drops for CM, topology leakage is confirmed.
  2. **Distribution Visuals**: Generate t-SNE plots of training data vs. masked molecules. Verify CM points cluster with training data while Feature Zeroing points drift.
  3. **Consistency Sweep**: Run Grad-CAM + CM on regression task. Check if masking "solubility-increasing" atoms actually decreases predicted solubility.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the Counterfactual Masking framework be extended to robustly handle molecular fragments with more than two attachment points? The authors note CReM struggles with >2 attachment points while DiffLinker often fails to produce chemically feasible molecules in these complex scenarios.

- **Open Question 2**: Can the computational efficiency of Counterfactual Masking be improved without sacrificing the statistical robustness provided by multiple samples? The method demands more computational resources due to sampling from generative models.

- **Open Question 3**: Is it possible to achieve high chemical validity while masking partial rings, avoiding the current trade-off between CReM and DiffLinker? There's a dichotomy between masking partial rings (DiffLinker) and maintaining chemical validity (CReM).

## Limitations

- CReM cannot modify partial rings, requiring full ring replacement when any atom is influential
- DiffLinker often produces chemically invalid molecules, especially for complex fragments with multiple attachment points
- The method requires sampling multiple compounds from generative models, increasing computational overhead

## Confidence

- **High confidence**: Distribution alignment prevents OOD artifacts in explanations (well-supported by experimental comparison with feature zeroing)
- **Medium confidence**: Topology bias elimination through CM is valid, but practical impact varies with model architecture
- **Medium confidence**: Consistency metrics provide meaningful validation of explanations, though the 50% baseline for random masking suggests room for improvement

## Next Checks

1. **OOD Robustness Test**: Train a GNN on synthetic molecular data with controlled distribution shifts. Compare explanation fidelity metrics between feature zeroing and CM when applied to in-distribution vs. out-of-distribution test sets.

2. **Generative Model Coverage Analysis**: Systematically vary the complexity of masked fragments (single atoms, small substructures, ring systems) and measure the success rate of CReM vs. DiffLinker. Identify the breaking point where generative models fail to produce valid replacements.

3. **Alternative Architecture Sensitivity**: Evaluate CM on non-GNN architectures (e.g., transformer-based molecular models) to determine if topology leakage is architecture-dependent and whether CM provides comparable benefits across different model families.