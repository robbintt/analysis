---
ver: rpa2
title: Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL
arxiv_id: '2601.09876'
source_url: https://arxiv.org/abs/2601.09876
tags:
- clinical
- evaluation
- data
- hadm
- critical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLINSQL introduces a clinical text-to-SQL benchmark requiring multi-table
  joins, temporal reasoning, and patient-similarity cohort construction over MIMIC-IV
  v3.1. The benchmark includes 633 expert-annotated tasks across six realistic clinical
  scenarios with stratified difficulty.
---

# Patient-Similarity Cohort Reasoning in Clinical Text-to-SQL

## Quick Facts
- arXiv ID: 2601.09876
- Source URL: https://arxiv.org/abs/2601.09876
- Authors: Yifei Shen; Yilun Zhao; Justice Ou; Tinglin Huang; Arman Cohan
- Reference count: 40
- One-line primary result: GPT-5-mini achieves 74.7% execution score on clinical text-to-SQL benchmark, with cohort specification drift as dominant error type.

## Executive Summary
CLINSQL introduces a clinical text-to-SQL benchmark requiring multi-table joins, temporal reasoning, and patient-similarity cohort construction over MIMIC-IV v3.1. The benchmark includes 633 expert-annotated tasks across six realistic clinical scenarios with stratified difficulty. Evaluation uses rubric-based SQL and results scoring with critical-first aggregation and execution checks. Across 22 proprietary and open-source models tested under Chain-of-Thought self-refinement, GPT-5-mini achieves the highest execution score of 74.7% on the test set, with DeepSeek-R1 leading open-source at 69.2%. Performance drops significantly on harder queries, and errors commonly stem from cohort specification drift, schema mismatches, and clinical aggregation issues.

## Method Summary
The CLINSQL benchmark evaluates LLMs on clinical text-to-SQL tasks over MIMIC-IV v3.1 using 633 expert-annotated questions with gold SQLs and tree-structured rubrics. Models are tested under Chain-of-Thought self-refinement with up to 2 refinement rounds based on BigQuery execution feedback. Evaluation uses GPT-5 as judge to apply binary leaf decisions on rubric trees, with critical-first aggregation algorithm combining scores. The benchmark includes six clinical scenarios (Mortality, Comorbidity, Phenotype, Medication, Complications, Imaging) with stratified difficulty levels. Schema-hinted inference provides ICD code patterns and expected columns to improve performance, particularly on medium and hard queries.

## Key Results
- GPT-5-mini achieves highest execution score of 74.7% on test set, with DeepSeek-R1 leading open-source at 69.2%
- Performance drops significantly on harder queries: Gemini-2.5-Pro from 85.5% to 67.2% between Easy and Hard
- Cohort specification drift causes 54% of failures, primarily through relaxed ICD constraints or keyword substitution
- Schema-hinted inference improves results by +2.78% (Easy), +10.21% (Medium), +7.20% (Hard)
- 22 proprietary and open-source models tested under Chain-of-Thought self-refinement framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Thought reasoning with execution-driven self-refinement improves clinical text-to-SQL performance over direct output generation.
- Mechanism: CoT prompting encourages stepwise decomposition of complex clinical queries into constituent reasoning stages (table selection → join construction → filtering → aggregation), while self-refinement provides error signals from failed BigQuery executions that guide targeted corrections.
- Core assumption: Models possess sufficient latent clinical and SQL knowledge that can be elicited through structured reasoning steps; execution errors are informative rather than misleading.
- Evidence anchors:
  - [abstract] "We evaluate 22 proprietary and open-source models under Chain-of-Thought self-refinement"
  - [section 5.2] Figure 5 shows Gemini-2.5-Pro improving from 73.6% to 76.1% and Qwen3-Coder from 56.1% to 60.5% execution scores with CoT vs. direct output
  - [corpus] Weak support—neighbor papers (CogniSQL-R1-Zero, LogicCat) discuss reasoning for SQL generation but lack direct comparison data
- Break condition: When models lack foundational SQL syntax knowledge or clinical terminology understanding, CoT decomposition cannot compensate; SQLCoder-7B-2 achieves 0% execution despite self-refinement (Table 4)

### Mechanism 2
- Claim: Schema-hinted inference reduces cohort specification drift by constraining the model's search space with clinically validated filters and expected output columns.
- Mechanism: Providing gold ICD code patterns and expected column names in the prompt foregrounds correct schema elements, reducing the probability that models substitute keyword heuristics for precise clinical coding or misalign SELECT aliases.
- Core assumption: The gold artifacts (ICD codes, column names) are extractable from reference SQL and correctly formatted for prompt injection; models can integrate explicit hints without over-reliance.
- Evidence anchors:
  - [abstract] "Schema-hinted inference improves results, especially for medium and hard cases"
  - [section 5.4] Table 5 shows GPT-5-mini execution gains of +2.78% (Easy), +10.21% (Medium), +7.20% (Hard)
  - [corpus] No direct corpus evidence for schema-hinting in clinical text-to-SQL; this appears novel to CLINSQL
- Break condition: When queries require ICD codes or tables not covered in the hint block, models may over-apply provided patterns to inappropriate contexts

### Mechanism 3
- Claim: Critical-first rubric aggregation enables reliable automated evaluation by enforcing hierarchical dependencies where cohort-level failures invalidate downstream analytics checks.
- Mechanism: Rubric trees encode clinical reasoning as a DAG with critical leaf nodes (e.g., correct ICD filtering) whose failure propagates zero-scores upward before non-critical nodes (e.g., rounding) are evaluated, preventing partial credit on fundamentally wrong queries.
- Core assumption: Expert-annotated rubrics correctly capture clinical reasoning dependencies; LLM-as-judge (GPT-5) reliably applies binary rubric criteria.
- Evidence anchors:
  - [section 3.3] Algorithm 1 specifies critical-first scoring: "if score[c] ≠ 1 then return 0" for critical children
  - [appendix K] Human-GPT agreement study shows 83.4% leaf-level SQL agreement and 90.2% pass/fail agreement
  - [corpus] No corpus evidence; critical-first aggregation appears adapted from agentic evaluation frameworks (Gou et al. 2025)
- Break condition: When rubric trees incompletely specify clinical requirements or when LLM-judge misinterprets criteria, aggregation structure cannot recover from upstream annotation errors

## Foundational Learning

- Concept: MIMIC-IV schema structure (HOSP vs. ICU modules, subject_id/hadm_id/stay_id relationships)
  - Why needed here: All CLINSQL queries require multi-table joins across admissions, patients, diagnoses_icd, labevents, and ICU-specific tables; misunderstanding identifiers causes join failures
  - Quick check question: Can you explain why a query joining chartevents to prescriptions needs both stay_id and hadm_id intermediation?

- Concept: ICD-9/10 code versioning and pattern matching in clinical phenotyping
  - Why needed here: Cohort specification is the dominant error source (54% of failures); models must apply version-specific code prefixes (e.g., ICD-9 '410%' vs. ICD-10 'I21%' for AMI)
  - Quick check question: Given a cohort definition "sepsis excluding septic shock," what ICD-9 and ICD-10 code patterns would you use?

- Concept: BigQuery SQL dialect specifics (APPROX_QUANTILES syntax, DATETIME_DIFF, array offset indexing)
  - Why needed here: Queries must execute on BigQuery; common failures include TIMESTAMP/DATETIME type mismatches and incorrect quantile array indexing
  - Quick check question: How do you extract the median from APPROX_QUANTILES(column, 100) in BigQuery?

## Architecture Onboarding

- Component map: Prompt builder -> Model inference -> Execution engine -> Rubric evaluator -> Score aggregator
- Critical path: Prompt construction → SQL generation → BigQuery execution → (if failure) refinement loop → rubric evaluation → critical-first aggregation. The refinement loop is the primary latency variable; top models achieve >90% first-pass execution success on Easy queries.
- Design tradeoffs:
  - Execution score vs. SQL score: Execution measures result plausibility; SQL score measures reasoning correctness. Section M notes r=0.86 correlation, but ~5% of cases show high SQL/low execution (correct logic, implausible outputs)
  - Rubric granularity vs. annotation burden: 14-23 rubric nodes per query enable fine-grained error analysis but require expert annotation; scaling beyond 633 examples requires tooling
  - Self-refinement rounds: Paper uses up to 2 rounds; more rounds might improve execution but increase latency and cost
- Failure signatures:
  - Cohort specification drift (54%): Model substitutes keyword matching for ICD codes or relaxes join constraints
  - Schema/formatting errors (24%): Incorrect column names, TIMESTAMP/DATETIME type confusion
  - Aggregation errors (14%): Wrong denominators, missing normalization, incorrect APPROX_QUANTILES indexing
- First 3 experiments:
  1. Establish baseline on validation split with GPT-5-mini using standard CoT prompt (no hints); measure SQL score, execution score, and per-scenario breakdown
  2. Implement schema-hinted inference by extracting ICD codes and expected columns from gold SQL; evaluate gains specifically on Medium and Hard queries
  3. Conduct error taxonomy analysis on 60 random failures (10 per scenario) by mapping rubric failures to cohort/schema/aggregation categories; identify highest-yield intervention targets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance on CLINSQL transfer to other EHR ecosystems or SQL dialects?
- Basis in paper: [inferred] The Limitations section states the benchmark relies on "MIMIC-IV v3.1—data from a single health system—and targets a single SQL environment," limiting generalizability.
- Why unresolved: The paper evaluates only on MIMIC-IV/BigQuery; cross-system validation was outside the scope.
- What evidence would resolve it: Evaluation of leading models on a distinct EHR dataset (e.g., eICU) using the CLINSQL framework.

### Open Question 2
- Question: Can automated or weak supervision strategies replace heavy expert annotation without losing clinical fidelity?
- Basis in paper: [explicit] The Limitations section notes that "substantial domain-expert involvement... reduces throughput and makes it difficult to scale to substantially larger datasets."
- Why unresolved: The current construction relies entirely on expert-curated gold SQL and rubrics.
- What evidence would resolve it: A study comparing model performance using synthetic/weakly supervised rubrics versus the current expert gold standard.

### Open Question 3
- Question: What architectural interventions are required to resolve "cohort specification drift"?
- Basis in paper: [explicit] Section 5.3 identifies "Cohort specification & coding" as the most frequent error type (54%), often caused by relaxed ICD constraints.
- Why unresolved: Chain-of-Thought and schema-hinting improved scores but did not eliminate these fundamental reasoning errors.
- What evidence would resolve it: Development of a model specifically fine-tuned to enforce strict adherence to clinical coding constraints in the query plan.

## Limitations
- Benchmark focuses on MIMIC-IV v3.1 from single health system, limiting generalizability to other clinical databases or SQL dialects
- Evaluation relies on expert-annotated rubrics and GPT-5 as judge, introducing potential subjectivity and bias in scoring methodology
- Dataset of 633 examples may not capture full complexity and diversity of real-world clinical query patterns

## Confidence
- High Confidence: Benchmark construction methodology with stratified difficulty and expert annotation is robust
- Medium Confidence: Performance improvements from schema-hinted inference are statistically significant but may be sensitive to prompt engineering
- Medium Confidence: Critical-first rubric aggregation methodology is novel and theoretically sound, but reliability depends on expert annotation quality

## Next Checks
1. **Schema Generalization Test**: Evaluate model performance on a different clinical database (e.g., eICU-CRD or OMOP CDM) to assess schema transferability and identify model dependencies on MIMIC-IV-specific patterns
2. **Multilingual Clinical Queries**: Test the benchmark and models with clinical questions translated into other languages or from non-English medical records to evaluate cross-lingual generalization
3. **Temporal Reasoning Stress Test**: Construct additional queries specifically targeting temporal reasoning capabilities (e.g., time-windowed cohort definitions, lagged lab comparisons) to isolate and quantify performance on this challenging dimension