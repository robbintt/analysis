---
ver: rpa2
title: 'The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic
  Robustness in Language Models'
arxiv_id: '2512.23850'
source_url: https://arxiv.org/abs/2512.23850
tags:
- epistemic
- robustness
- turn
- ddft
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DDFT introduces a novel evaluation protocol that measures epistemic\
  \ robustness\u2014a model\u2019s ability to maintain factual accuracy under progressive\
  \ information degradation and adversarial fabrication. The protocol employs a two-system\
  \ cognitive model: a Semantic System generating fluent text and an Epistemic Verifier\
  \ validating factual accuracy."
---

# The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models

## Quick Facts
- arXiv ID: 2512.23850
- Source URL: https://arxiv.org/abs/2512.23850
- Reference count: 27
- Primary result: Epistemic robustness is orthogonal to model scale and architecture; error detection capability is the critical bottleneck

## Executive Summary
DDFT introduces a novel evaluation protocol that measures epistemic robustness—a model's ability to maintain factual accuracy under progressive information degradation and adversarial fabrication. The protocol employs a two-system cognitive model: a Semantic System generating fluent text and an Epistemic Verifier validating factual accuracy. Testing nine frontier models across eight knowledge domains at five compression levels (1,800 evaluations), DDFT reveals epistemic robustness is orthogonal to model scale and architecture. Parameter count shows no significant correlation with robustness (r=0.083, p=0.832), while error detection capability emerges as the critical bottleneck (Turn 4 performance correlates with CI at ρ=-0.817, p=0.007). The Comprehension Integrity index demonstrates that flagship models exhibit brittleness despite their scale, while smaller models achieve robust performance. DDFT provides both theoretical foundation and practical tools for assessing epistemic robustness before deployment in critical applications.

## Method Summary
DDFT evaluates epistemic robustness through a 5-turn dialogue protocol with progressive context compression and an adversarial fabrication trap. The Interviewer Agent (gpt-5.1) administers the dialogue across 5 compression levels (0.0, 0.25, 0.5, 0.75, 1.0) for 8 knowledge concepts. The Subject Model responds to Socratic prompts while a Three-Judge Jury (GPT-5.1, DeepSeek-v3.1, Claude Opus 4.1) independently scores Factual Accuracy Rate (FAR) and Semantic Accuracy Score (SAS). The protocol computes HOC, CRI, FAR', SAS', and CI metrics to assess robustness. The critical fabrication trap (Turn 4) introduces plausible falsehoods to stress-test the Epistemic Verifier's error detection capability.

## Key Results
- Epistemic robustness is orthogonal to parameter count (r=0.083, p=0.832) and architectural type (r=0.153, p=0.695)
- Error detection capability strongly predicts overall robustness (rho=-0.817, p=0.007) for Turn 4 vs. CI
- Smaller models (o4-mini, 25B params) achieved higher CI scores than flagship models (gpt-5, claude-haiku-4-5)
- Semantic-epistemic dissociation occurs when fluent generation persists despite factual collapse

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Error detection capability is the critical bottleneck in epistemic robustness.
- Mechanism: The fabrication trap (Turn 4) isolates the Epistemic Verifier's V_E component, which must reject plausible falsehoods. Strong negative correlation between Turn 4 FAR and overall CI indicates that models failing to detect fabrications collapse across all robustness dimensions.
- Core assumption: The Epistemic Verifier comprises functionally distinct sub-components (V_K for retrieval, V_E for error detection) that can be stressed independently.
- Evidence anchors:
  - [abstract] "Error detection capability strongly predicts overall robustness (rho=-0.817, p=0.007)"
  - [section 6.2] "The strong predictive power of V_E (Turn 4) for overall CI scores suggests error detection is the critical bottleneck"
  - [corpus] Related work on epistemic calibration (KalshiBench, arXiv:2512.16030) supports the broader framework but does not directly validate DDFT's Turn 4 mechanism.
- Break condition: If Turn 4 performance were weakly correlated with CI (rho > -0.3), the error-detection-bottleneck hypothesis would be falsified.

### Mechanism 2
- Claim: Semantic-epistemic dissociation is a predictable failure mode where fluent generation persists despite factual collapse.
- Mechanism: The Semantic System (S) and Epistemic Verifier (V) can fail independently. Compression stress primarily degrades V, while S maintains coherence—producing high SAS / low FAR "danger zone" responses.
- Core assumption: S and V are functionally separable, not monolithic; this is a behavioral claim, not an assertion about neural modules.
- Evidence anchors:
  - [abstract] "flagship models (gpt-5, claude-haiku-4-5) showing brittleness while smaller models (o4-mini) excelled"
  - [section 8.2.4] "Robust models exhibit selective Verifier failure... producing fluent, well-structured responses (mean SAS=0.87) that contain subtle factual errors"
  - [corpus] Literature on LLM epistemics (arXiv:2506.01512) discusses fact/fiction representations but does not test the dissociation mechanism directly.
- Break condition: If SAS and FAR always degraded together (correlation > 0.8 across compression levels), the two-system model would lack explanatory power.

### Mechanism 3
- Claim: Epistemic robustness is orthogonal to parameter count and architectural paradigm.
- Mechanism: Neither scale nor reasoning-alignment reliably produces robust Verifiers. Robustness emerges from training methodology and verification mechanisms not captured by current design categories.
- Core assumption: The 9-model sample, though small, yields stable null results (bootstrap CI confirms).
- Evidence anchors:
  - [abstract] "Neither parameter count (r=0.083, p=0.832) nor architectural type (r=0.153, p=0.695) significantly predicts robustness"
  - [section 8.2.1] "The top two models (o4-mini... 25B params and grok-4-fast-non-reasoning... 60B params) represent different architectural families yet achieve nearly identical CI scores"
  - [corpus] No corpus papers directly test scale vs. robustness orthogonality; this remains an open finding requiring external replication.
- Break condition: If future work finds significant correlations (p < 0.05) with parameter count or architecture in larger model pools, orthogonality would be refuted.

## Foundational Learning

- **System 1 vs. System 2 cognition (dual-process theory)**
  - Why needed here: The paper's two-system model directly maps to this framework; understanding fast/associative vs. slow/deliberative processing clarifies why the Semantic System and Epistemic Verifier fail differently.
  - Quick check question: Can you explain why a fast, fluent response might be confidently wrong?

- **Hallucination in LLMs**
  - Why needed here: DDFT is designed to detect and stress-test hallucination-prone behavior under cognitive load.
  - Quick check question: What is the difference between a model lacking knowledge and a model whose verification mechanism has collapsed?

- **Correlation vs. causation in benchmark interpretation**
  - Why needed here: The paper reports multiple correlations (e.g., Turn 4 vs. CI); understanding what these can and cannot prove is essential for interpreting findings.
  - Quick check question: If Turn 4 performance correlates with CI at rho=-0.817, does this prove that improving Turn 4 will improve CI?

## Architecture Onboarding

- **Component map:**
  - Interviewer Agent (gpt-5.1) -> Subject Model -> Three-Judge Jury (GPT-5.1, DeepSeek-v3.1, Claude Opus 4.1) -> Metrics Pipeline

- **Critical path:**
  1. Load concept + reference text → apply compression level c.
  2. Execute 5-turn dialogue (Core Idea → Example → Detail → Fabrication → Follow-up).
  3. Submit each response to jury for FAR/SAS scoring.
  4. Aggregate across 5 compression levels → compute HOC, CRI, FAR', SAS'.
  5. Calculate CI = (HOC × CRI) / (FAR' + (1 − SAS')).

- **Design tradeoffs:**
  - **5 compression levels vs. 10**: Ablation shows τ=0.978 gain at 2× cost; 5-level is near-optimal.
  - **LLM-as-judge vs. human evaluation**: LLM jury achieves κ=0.82 FAR agreement; human annotation at 1,800 turns is cost-prohibitive (~$18K estimated).
  - **Fabrication trap specificity**: Uses fictional experts (e.g., "Professor Eleanor Vance"); may not generalize to all deception types.

- **Failure signatures:**
  - **Coupled collapse (Brittle models)**: Both SAS and FAR drop together → output is visibly broken.
  - **Selective Verifier failure (Robust models)**: SAS stays high, FAR drops → fluent hallucinations, higher danger zone rate.
  - **Turn 4 acceptance**: Model engages with fabricated premise → V_E failure.

- **First 3 experiments:**
  1. **Run DDFT on a single model across all 8 concepts at compression 0.5 only** to validate pipeline and inspect raw jury scores.
  2. **Ablate Turn 4**: Replace fabrication trap with neutral follow-up; verify that Turn 4 correlation with CI drops (mechanism validation).
  3. **Cross-validate jury scores**: Have a human expert score 20 randomly selected responses; compare to jury consensus to estimate systematic bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which specific training methodologies explain the epistemic robustness observed in smaller models?
- Basis: [explicit] Section 9.4, Prediction 2 asks what makes o4-mini robust "despite small size" given the orthogonality to scale.
- Why unresolved: The study identifies the phenomenon but lacks internal training data to distinguish between dataset quality vs. verification mechanisms.
- Evidence: Ablation studies on training runs that isolate data curation or specific RLHF techniques.

### Open Question 2
- Question: Does adversarial training on fabrication detection improve epistemic robustness more than knowledge expansion?
- Basis: [explicit] Section 9.4, Prediction 1 hypothesizes interventions strengthening $V_E$ (error detection) should improve CI.
- Why unresolved: The paper proposes this "adversarial verification training" paradigm as a theoretical direction but does not experimentally validate it.
- Evidence: Training models with explicit fabrication detection objectives and measuring changes in Turn 4 performance.

### Open Question 3
- Question: Do Comprehension Integrity (CI) scores predict actual failure rates in open-ended, real-world deployment?
- Basis: [explicit] Section 9.4, Prediction 3 calls for "external validation" to confirm CI scores predict "real-world epistemic failures."
- Why unresolved: The study evaluates controlled 5-turn dialogues, which may not fully simulate unconstrained production environments.
- Evidence: Correlating model CI scores with human-flagged hallucination rates in production usage logs.

## Limitations

- The compression methodology lacks explicit procedural detail, making exact replication difficult without access to source artifacts
- Small sample size (9 models) limits generalizability of orthogonality claims to broader model populations
- LLM-as-judge methodology, while cost-effective, introduces potential systematic bias that may differ from human judgment

## Confidence

- **High confidence**: Core mechanism (two-system dissociation, fabrication trap effectiveness) demonstrated through statistical significance and ablation
- **Medium confidence**: Orthogonality to scale claims—supported by current data but requires broader model sampling for robust validation
- **Medium confidence**: Jury evaluation reliability—κ=0.82 is acceptable but represents a single implementation of a novel scoring system

## Next Checks

1. **Compression methodology validation**: Publish exact compression algorithms and compression level specifications to enable precise replication
2. **Cross-domain generalization**: Test DDFT with domains outside formal knowledge (e.g., procedural, experiential) to assess protocol limits
3. **Human jury comparison**: Evaluate a subset of responses with human experts to quantify LLM judge bias and calibration