---
ver: rpa2
title: How to Correctly Report LLM-as-a-Judge Evaluations
arxiv_id: '2511.21140'
source_url: https://arxiv.org/abs/2511.21140
tags:
- calibration
- estimator
- 'true'
- accuracy
- interval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses bias in LLM-as-a-judge evaluations caused by
  imperfect sensitivity and specificity, which leads to overestimation or underestimation
  of true accuracy. It proposes a plug-in framework that corrects this bias and provides
  confidence intervals accounting for uncertainty from both test and calibration datasets.
---

# How to Correctly Report LLM-as-a-Judge Evaluations

## Quick Facts
- arXiv ID: 2511.21140
- Source URL: https://arxiv.org/abs/2511.21140
- Reference count: 40
- The paper proposes a plug-in framework that corrects bias in LLM-as-a-judge evaluations caused by imperfect sensitivity and specificity, providing confidence intervals that account for uncertainty from both test and calibration datasets.

## Executive Summary
This paper addresses a fundamental problem in LLM-as-a-judge evaluation: the naive accuracy estimate is biased when the judge has imperfect sensitivity and specificity. The authors propose a simple plug-in framework that corrects this bias using calibration data with human labels. The method provides both bias-corrected estimates and confidence intervals that account for uncertainty from both test and calibration datasets. Experiments show the approach reduces bias and provides reliable uncertainty quantification compared to existing methods.

## Method Summary
The framework requires two datasets: a test set (n samples) with only LLM judgments, and a calibration set (m samples) with both human labels and LLM judgments. The method computes a naive accuracy estimate from the test set, estimates the LLM's sensitivity and specificity from the calibration set, then applies a bias correction formula. Confidence intervals are constructed using a delta-method variance estimate combined with an adjusted Wald approach. An optional adaptive allocation strategy optimizes the split of calibration samples between positive and negative examples to minimize interval length.

## Key Results
- The bias correction reduces systematic overestimation at low accuracy values and underestimation at high accuracy values
- Confidence intervals achieve near-nominal 95% coverage across different accuracy levels and judge qualities
- Adaptive calibration allocation yields shorter confidence intervals compared to symmetric allocation
- The method remains unbiased under distribution shift between test and calibration datasets, unlike existing prediction-powered inference methods

## Why This Works (Mechanism)

### Mechanism 1
The naive LLM judgment proportion p̂ is a biased estimator of true accuracy θ, and this bias can be corrected via a closed-form adjustment using estimated sensitivity and specificity. By the law of total probability, E[p̂] = (q₀ + q₁ − 1)·θ + (1 − q₀), where q₁ is the LLM's sensitivity and q₀ is its specificity. Inverting this relationship yields the bias-corrected estimator θ̂ = (p̂ + q̂₀ − 1) / (q̂₀ + q̂₁ − 1). This adjustment removes systematic overestimation at low θ and underestimation at high θ.

### Mechanism 2
Confidence intervals that account for variance from both test and calibration datasets provide valid uncertainty quantification with near-nominal coverage. The asymptotic variance of θ̂ combines binomial variance from p̂ (test set, size n) and from q̂₀, q̂₁ (calibration sets, sizes m₀, m₁): Var(θ̂) = [p̂(1-p̂)/n + (1-θ̂)²q̂₀(1-q̂₀)/m₀ + θ̂²q̂₁(1-q̂₁)/m₁] / (q̂₀ + q̂₁ − 1)². The "add two successes and two failures" adjustment improves coverage for small samples.

### Mechanism 3
Asymmetric allocation of calibration samples across label types reduces confidence interval length compared to balanced allocation. The optimal allocation satisfies m̃₀ ≈ (1/p̃ − 1)√κ · m̃₁, where κ = (1 − q̃₀)/(1 − q̃₁) is the error ratio. When the LLM is worse at identifying "correct" responses (small κ) or when p̃ is large (many test items judged correct), more calibration samples should be allocated to estimating q₁.

## Foundational Learning

- **Sensitivity and Specificity** (binary classification metrics): Why needed - The entire correction framework depends on estimating how often the LLM correctly identifies "correct" responses (sensitivity q₁) and "incorrect" responses (specificity q₀). Quick check - If an LLM has sensitivity 0.9 and specificity 0.7, what proportion of truly incorrect responses does it mistakenly label as correct?

- **Delta Method for Variance Propagation**: Why needed - The bias-corrected estimator θ̂ is a nonlinear function of three random variables (p̂, q̂₀, q̂₁); the delta method approximates its variance via first-order Taylor expansion. Quick check - If Y = f(X₁, X₂) where X₁ and X₂ are independent, write the general form of Var(Y) using partial derivatives.

- **Agresti-Coull / "Add Two Successes" Adjustment**: Why needed - Standard Wald intervals for binomial proportions have poor coverage at small samples; adding pseudo-observations stabilizes estimates before CI construction. Quick check - For 5 successes out of 20 trials, what is the adjusted proportion after adding one success and one failure?

## Architecture Onboarding

- Component map: Test pipeline (P distribution → LLM judge → p̂) -> Calibration pipeline (Q distribution → LLM judge + human labels → q̂₀, q̂₁) -> Bias-correction module (θ̂ = (p̂ + q̂₀ − 1) / (q̂₀ + q̂₁ − 1)) -> Uncertainty quantification (Delta-method variance → adjusted Wald CI) -> Adaptive allocation (optional: pilot calibration → estimate κ → allocate remaining calibration budget)

- Critical path: Calibration data collection with human labels → estimate q̂₀, q̂₁ → run LLM judge on test set → compute p̂ → apply bias correction → construct CI. The calibration step is the bottleneck requiring human annotation.

- Design tradeoffs: Calibration budget vs. CI tightness (larger m reduces variance but costs more human labels); Symmetric vs. adaptive allocation (adaptive yields shorter CIs but requires pilot estimates); LLM judge quality vs. calibration size (higher q₀, q₁ reduce calibration sample size needed).

- Failure signatures: Denominator near zero (if q̂₀ + q̂₁ ≈ 1, estimator explodes indicating LLM near-random); Coverage failure (empirical coverage drops below nominal level); Negative/>1 estimates (truncation to [0,1] handles edge cases but frequent truncation suggests poor calibration).

- First 3 experiments: Validate bias reduction on Chatbot Arena data with held-out human labels; verify 95% CI coverage across Monte Carlo simulations varying (q₀, q₁, θ, m); test distribution shift robustness by varying θ_Q relative to θ_P.

## Open Questions the Paper Calls Out

- **How can the bias-correction framework be extended to account for auxiliary factors ξ (e.g., response length or stylistic attributes) that influence LLM judgments in addition to the true label?** The current analysis assumes judgments depend only on the true label, and suggest future work could condition on nuisance factors ξ.

- **Can the proposed estimation method be generalized to multinomial settings where responses are classified into more than two categories?** The authors identify the extension to a multinomial setting as a direction for future work.

- **Can a conformal prediction framework be integrated into this methodology to provide sample-specific uncertainty quantification?** The authors list the incorporation of a conformal prediction framework as a future direction.

- **How sensitive is the bias-corrected estimator to violations of the assumption that the conditional judge behavior Pr(Ẑ|Z) is invariant between the calibration and test datasets?** The paper assumes this invariance to prove robustness but doesn't quantify bias when this assumption is violated.

## Limitations

- The framework requires human labels for calibration, which remains a significant practical bottleneck despite reducing the required amount compared to direct human evaluation
- The adaptive calibration allocation strategy relies on pilot estimates that may be unreliable for small pilot samples
- The method assumes the LLM judge's behavior is stable between calibration and test datasets, which may not hold in real-world deployment scenarios

## Confidence

- **High confidence**: The bias correction mechanism is mathematically rigorous with closed-form derivations; experimental results on Chatbot Arena strongly support reduced bias in practice
- **Medium confidence**: The confidence interval coverage guarantees depend on asymptotic approximations that may not hold for very small calibration sets; empirical coverage is good but not perfect in simulations
- **Low confidence**: The adaptive calibration allocation strategy's optimality depends on pilot estimates that are themselves noisy; the theoretical minimum interval length may not be achieved in practice

## Next Checks

1. **Distribution shift robustness**: Systematically vary θ_Q relative to θ_P in synthetic experiments and measure bias for both the proposed estimator and prediction-powered inference methods across a wider range of parameter combinations.

2. **Small calibration regime**: Evaluate performance when m < 50 to determine the minimum calibration sample size required for reliable bias correction and coverage, testing the practical limits of the framework.

3. **Real-world deployment analysis**: Apply the framework to multiple LLM-as-a-judge scenarios beyond Chatbot Arena (e.g., code generation, summarization) to assess generalizability and identify task-specific failure modes.