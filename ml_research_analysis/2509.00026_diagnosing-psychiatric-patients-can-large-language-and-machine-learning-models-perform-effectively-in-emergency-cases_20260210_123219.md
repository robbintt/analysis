---
ver: rpa2
title: 'Diagnosing Psychiatric Patients: Can Large Language and Machine Learning Models
  Perform Effectively in Emergency Cases?'
arxiv_id: '2509.00026'
source_url: https://arxiv.org/abs/2509.00026
tags:
- data
- rescue
- 'false'
- psychiatric
- patients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the application of machine learning and
  large language models (LLMs) for diagnosing psychiatric conditions in emergency
  rescue scenarios. Using rescue patient data from Germany, the research integrates
  structured health vitals and unstructured textual information through natural language
  processing (NLP) to extract relevant features.
---

# Diagnosing Psychiatric Patients: Can Large Language and Machine Learning Models Perform Effectively in Emergency Cases?

## Quick Facts
- arXiv ID: 2509.00026
- Source URL: https://arxiv.org/abs/2509.00026
- Reference count: 21
- Primary result: Random Forest achieved 89.27% accuracy and 90.01% F1-score in psychiatric vs. non-psychiatric patient classification using mixed structured and unstructured rescue data.

## Executive Summary
This study investigates AI-driven psychiatric diagnosis in emergency rescue scenarios by integrating structured vital signs with unstructured rescue notes. The researchers developed machine learning models and evaluated zero-shot LLM inference capabilities using German rescue patient data. Random Forest emerged as the top-performing model with 89.27% accuracy, while Llama 8B demonstrated reliable zero-shot diagnostic capability on 6 test cases (5/6 correct). The work demonstrates that combining NLP-extracted features with physiological data can effectively support rapid psychiatric assessment in time-critical emergency settings.

## Method Summary
The study used 10,220 psychiatric and 8,758 non-psychiatric rescue cases from Germany (2012-2021). Researchers merged 83 CSV files, applied IQR-based outlier handling, and extracted 5 binary NLP features using NLTK keyword matching with negation handling. Features underwent relative deviation filtering (threshold ≥3× occurrence ratio) followed by RFECV. Seven ML models were trained and tuned via grid/random search with StratifiedKFold cross-validation. Zero-shot LLM evaluation used Llama 8B via Ollama on 6 cases with JSON-like structured prompts. Performance was measured using accuracy, F1, sensitivity, specificity, and ROC-AUC on 80/20 train/test splits.

## Key Results
- Random Forest achieved highest performance: 89.27% accuracy, 90.01% F1-score, 89.44% sensitivity, 89.07% specificity
- Llama 8B demonstrated zero-shot capability, correctly diagnosing 5 out of 6 test cases (83.3% accuracy)
- RFECV identified 9 of 10 features as valuable, with Preillness excluded post-validation
- ROC-AUC curves showed RF and XGBoost closely touching the upper left corner, indicating strong discrimination

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid feature architectures combining structured vitals with NLP-extracted text markers can improve psychiatric detection in noisy emergency data.
- Mechanism: NLP parses unstructured rescue notes using keyword matching with negation handling; extracted binary features (e.g., "suicidal," "intoxication") join physiological vitals (GCS, blood pressure) to form a unified feature vector for classification. The relative deviation filter (Eq. 1) selects features occurring ≥3× more frequently in psychiatric vs. non-psychiatric cases.
- Core assumption: Rescue personnel text entries contain discriminative psychiatric markers that outweigh documentation inconsistencies.
- Evidence anchors:
  - [abstract] "integrates structured health vitals and unstructured textual information through natural language processing (NLP) to extract relevant features"
  - [Section V] Five NLP-derived features (Preillness, Alcoholism, Psychiatric Symptoms, Abnormality, Intoxication) created via WordCounter algorithm with NLTK; negation handling prevents false positives
  - [corpus] Related work (FMR=0.40–0.53) supports multimodal integration for mental disorder classification
- Break condition: If text fields contain <50 keyword matches or negation patterns exceed affirmative patterns, feature signal degrades; if GCS/respiratory values have >30% imputation, hybrid advantage diminishes.

### Mechanism 2
- Claim: Tree-based ensemble methods (Random Forest, XGBoost) outperform linear and distance-based classifiers on hybrid psychiatric features with class imbalance.
- Mechanism: Random Forest aggregates multiple decision trees trained on bootstrapped samples; feature importance naturally handles mixed data types (binary NLP flags + continuous vitals); RFECV removes non-contributing features (Preillness excluded post-validation).
- Core assumption: Decision boundaries for psychiatric cases are non-linear and benefit from feature interaction modeling.
- Evidence anchors:
  - [Section VIII] RF achieved 89.27% accuracy, 90.01% F1, 89.44% sensitivity; XGBoost followed at 87.97% accuracy
  - [Section VI] RFECV identified 9 of 10 features as valuable; ROC-AUC showed RF/XGB/MLPC "closely touching the upper left corner"
  - [corpus] Weak direct corpus evidence on RF vs. other algorithms for psychiatric emergency data; related papers focus on deep learning and LLM approaches
- Break condition: If feature correlation exceeds 0.8, ensemble overfits; if class imbalance ratio exceeds 10:1 without SMOTE/weighting, specificity degrades.

### Mechanism 3
- Claim: Zero-shot LLM inference can approximate trained ML classifiers when prompts encode structured feature summaries.
- Mechanism: Llama 8B receives JSON-like prompt with 9 features (vitals + NLP flags); pre-trained medical knowledge maps feature patterns to psychiatric likelihood without gradient updates.
- Core assumption: Llama's pre-training corpus contains sufficient psychiatric case patterns to generalize to rescue-specific feature schemas.
- Evidence anchors:
  - [Section VII] Zero-shot approach tested on 6 cases (3 psychiatric, 3 non-psychiatric); 5/6 correct (1 false negative)
  - [Section VII] Sample prompt structure provided; Ollama enabled local inference without cloud dependency
  - [corpus] Related papers (e.g., "Llamadrs," "MentalSeek-Dx") explore LLM psychiatric assessment but report constraints from benchmark validity and fine-grained supervision
- Break condition: If prompt features conflict (e.g., GCS=15 but multiple psychiatric symptoms=TRUE), LLM hallucination risk increases; if test cases exceed training distribution, zero-shot accuracy degrades unpredictably.

## Foundational Learning

- **Concept: Natural Language Toolkit (NLTK) for keyword extraction**
  - Why needed here: Enables systematic parsing of 10,220 psychiatric case notes to extract 5 binary features; negation handling prevents false positives from phrases like "no suicidal ideation."
  - Quick check question: Given a text field "patient denies hallucinations but reports panic," would your NLP pipeline correctly set `Hallucination=FALSE` and `Panic=TRUE`?

- **Concept: Relative deviation for feature relevance scoring**
  - Why needed here: Filters features occurring ≥3× more in psychiatric vs. non-psychiatric cases (Eq. 1); reduces overfitting from non-discriminative terms like "patient" or "found."
  - Quick check question: If Feature A appears 120 times in psychiatric cases and 35 times in non-psychiatric cases, does it pass the threshold of 3?

- **Concept: Recursive Feature Elimination with Cross-Validation (RFECV)**
  - Why needed here: Post-hoc feature pruning identified Preillness as non-contributing; ensures final model uses only features validated across k-fold splits.
  - Quick check question: After RFECV removes a feature, should you re-tune hyperparameters, or is the original tuning still valid?

## Architecture Onboarding

- **Component map:**
  Raw Rescue Data (83 CSVs, 452 columns) -> Data Integration (merge by case ID, deduplicate) -> Data Reduction (filter columns, remove redundant fields) -> Data Filtering (IQR outlier handling, text normalization) -> NLP Feature Extraction (NLTK WordCounter → 5 binary features) -> Feature Selection (relative deviation filter → RFECV) -> Model Training (RF, XGB, SVM, NB, LR, K-NN, MLPC with Grid/Random search) -> Evaluation (80/20 split, confusion matrix, ROC-AUC) -> LLM Integration (Llama 8B via Ollama, zero-shot prompts)

- **Critical path:** NLP feature extraction → relative deviation filtering → RFECV → Random Forest training. Errors in negation handling propagate through all downstream models.

- **Design tradeoffs:**
  - RF vs. XGBoost: RF offers higher interpretability (feature importance); XGBoost faster inference but more hyperparameter-sensitive
  - Zero-shot LLM vs. trained ML: LLM requires no training data but lacks calibrated probabilities; ML models provide confidence scores but need labeled data
  - Local (Ollama) vs. cloud LLM: Local ensures privacy for patient data but limits model scale to 8B parameters

- **Failure signatures:**
  - Accuracy drops >10% between train and test: Likely overfitting from high feature correlation or insufficient RFECV folds
  - LLM returns non-binary responses: Prompt format not strictly enforced; add explicit "reply only TRUE or FALSE" instruction
  - NLP features all zero: Keyword dictionary missing rescue-specific terms (e.g., German abbreviations); expand stop-word exceptions

- **First 3 experiments:**
  1. **Baseline validation:** Replicate RF model with 9 features on 80/20 split; verify F1 ≥88% before architecture changes
  2. **Ablation study:** Remove NLP features one-by-one; measure accuracy drop to quantify text vs. vitals contribution
  3. **LLM scaling test:** Compare Llama 8B vs. 70B (if compute available) on 20 held-out cases; assess if larger model reduces the 1/6 error rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diagnostic reliability of LLMs change when the test dataset is expanded to represent a broader spectrum of psychiatric conditions?
- Basis in paper: [explicit] The conclusion states that future work will focus on "expanding the test dataset and assessing the reliability of different LLM models for the diagnosis of psychiatric patients in the broad spectrum."
- Why unresolved: The current study validated the Llama model on only 6 randomly chosen test cases, which is insufficient to generalize performance across the diverse range of potential emergency psychiatric scenarios.
- What evidence would resolve it: Evaluation results from applying the same zero-shot prompting methodology to a significantly larger validation set (e.g., hundreds or thousands of cases) including various conditions like psychosis, intoxication, and depression.

### Open Question 2
- Question: Do larger parameter versions of Llama (70B or 405B) or other state-of-the-art LLMs outperform the 8B model and traditional machine learning baselines in this specific diagnostic context?
- Basis in paper: [explicit] The authors explicitly list "assessing the reliability of different LLM models" as a direction for future research in the conclusion.
- Why unresolved: The study was restricted to the Llama 8B model to facilitate local execution via Ollama, leaving the potential diagnostic accuracy of larger or proprietary models unexplored.
- What evidence would resolve it: A comparative benchmark showing the accuracy, sensitivity, and specificity of larger Llama variants or models like GPT-4 against the 89.27% accuracy achieved by the Random Forest model.

### Open Question 3
- Question: Can prompt engineering strategies effectively mitigate the risks of hallucination and training data bias when using LLMs for emergency psychiatric triage?
- Basis in paper: [inferred] The paper notes that LLMs can generate "inaccurate responses" and contain "misinformation [and] biases," which can be "fatal if used solely" for diagnosis, yet it only tested simple zero-shot prompts.
- Why unresolved: While the authors identify the safety risks associated with LLM "black box" behavior, they did not investigate whether specific prompting techniques (e.g., chain-of-thought or constrained output) could reduce these specific errors.
- What evidence would resolve it: Experiments measuring the rate of hallucinations and bias-induced errors in zero-shot versus few-shot or system-prompted configurations.

## Limitations
- Limited external validity: Model performance based on data from single German region, raising questions about generalizability across different healthcare systems
- Small LLM test set: Zero-shot evaluation conducted on only 6 cases, providing limited statistical power to assess reliability
- Keyword dependency: NLP feature extraction relies on keyword matching that may miss nuanced clinical language or cultural variations in symptom expression

## Confidence
- **High confidence**: Random Forest performance metrics (89.27% accuracy, 90.01% F1) are well-documented with standard validation procedures
- **Medium confidence**: LLM zero-shot capability shows promise but requires larger-scale validation given the small test set
- **Medium confidence**: Feature selection methodology is sound, but the specific keyword lists and their clinical validity remain unclear

## Next Checks
1. **External validation**: Test the Random Forest model on psychiatric emergency data from different regions or countries to assess generalizability
2. **LLM scaling study**: Evaluate Llama 8B against larger models (e.g., Llama 70B) on a minimum of 50 held-out cases to determine if model size improves zero-shot accuracy
3. **NLP robustness test**: Assess model performance when rescue notes are anonymized or when keyword variations are introduced to simulate real-world documentation variability