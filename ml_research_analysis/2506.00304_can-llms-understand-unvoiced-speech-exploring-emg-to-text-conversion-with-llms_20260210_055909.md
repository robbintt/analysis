---
ver: rpa2
title: Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion with
  LLMs
arxiv_id: '2506.00304'
source_url: https://arxiv.org/abs/2506.00304
tags:
- speech
- llms
- unvoiced
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores using large language models (LLMs) to convert\
  \ unvoiced electromyography (EMG) signals\u2014captured muscle activations from\
  \ speech-related movements\u2014directly into text, without requiring any voiced\
  \ audio data. The authors propose a trainable EMG adaptor module that maps EMG features\
  \ into the LLM\u2019s input space, enabling the LLM to interpret these biosignals."
---

# Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion with LLMs

## Quick Facts
- **arXiv ID:** 2506.00304
- **Source URL:** https://arxiv.org/abs/2506.00304
- **Reference count:** 30
- **Primary result:** LLM-based EMG-to-text system achieves WER of 0.49 on closed-vocabulary task, outperforming specialized models by ~20% with only six minutes of training data

## Executive Summary
This paper investigates the feasibility of using large language models (LLMs) to interpret unvoiced electromyography (EMG) signals for text generation, enabling communication for individuals unable to produce vocal speech. The authors introduce a trainable EMG adaptor module that transforms muscle activation features into a format the LLM can process. Their approach demonstrates strong performance on closed-vocabulary tasks, achieving a word error rate of 0.49 with minimal training data, and surpasses specialized EMG-to-text models by nearly 20%. While promising, the method is currently limited to controlled vocabularies and has not been tested in open-vocabulary or multilingual settings.

## Method Summary
The authors propose a novel pipeline for EMG-to-text conversion using LLMs. They capture unvoiced EMG signals from facial and neck muscles associated with speech movements, then process these signals through a trainable EMG adaptor module. This adaptor maps EMG features into the input space of a pre-trained LLM, allowing the model to interpret the biosignals without requiring any voiced audio data. The system is trained end-to-end on a closed-vocabulary dataset, with performance evaluated using word error rate (WER). The approach leverages the LLM's language understanding capabilities to improve transcription accuracy, even with limited training data.

## Key Results
- Average WER of 0.49 on closed-vocabulary unvoiced EMG-to-text task
- Outperforms specialized EMG-to-text models by nearly 20% with only six minutes of training data
- Demonstrates LLM's ability to interpret EMG signals without voiced audio

## Why This Works (Mechanism)
The success of the approach hinges on the LLM's ability to leverage contextual language understanding to correct or infer missing or ambiguous EMG signal information. By mapping EMG features into the LLM's input space via a trainable adaptor, the system benefits from the model's pre-existing knowledge of language structure and syntax. This allows the LLM to fill gaps or resolve ambiguities that might confound more specialized, signal-only models, especially when training data is limited.

## Foundational Learning
- **Electromyography (EMG):** Measures electrical activity in muscles during movement; needed to capture speech-related muscle activations without sound. Quick check: Verify sensor placement and signal quality for facial/neck EMG.
- **Closed-vocabulary task:** System only needs to recognize a fixed set of words; needed to simplify the mapping from EMG to text and reduce error rates. Quick check: Confirm vocabulary size and coverage.
- **Word Error Rate (WER):** Standard metric for evaluating speech recognition accuracy; needed to quantify transcription performance. Quick check: Ensure WER is computed consistently across comparisons.
- **EMG adaptor module:** Bridges EMG signal features and LLM input space; needed to make biosignals interpretable by the LLM. Quick check: Validate adaptor's feature mapping and adaptability.
- **Trainable adaptor:** Allows adaptation to different EMG sensor configurations; needed for generalization across devices/subjects. Quick check: Test adaptor with varied sensor setups.

## Architecture Onboarding

**Component Map:**
EMG Sensors -> EMG Preprocessor -> EMG Adaptor -> LLM -> Text Output

**Critical Path:**
EMG signals are captured, preprocessed, transformed by the adaptor into LLM-compatible features, and fed into the LLM for text generation. The adaptor is the key innovation enabling this mapping.

**Design Tradeoffs:**
- **Closed vs. open vocabulary:** Closed vocabulary simplifies the task and boosts accuracy but limits real-world applicability. Open vocabulary would be more useful but significantly harder.
- **Minimal vs. extensive training data:** Using only six minutes of training data demonstrates efficiency but may not capture the full variability of EMG signals in broader use.
- **Specialized vs. LLM-based approach:** LLMs offer language understanding advantages but may be less efficient for signal-only tasks compared to specialized models.

**Failure Signatures:**
- High WER indicates the adaptor or LLM struggles with certain EMG patterns or vocabulary items.
- Poor generalization across subjects or sensor setups suggests the adaptor is overfit to specific configurations.
- Inability to scale to open vocabulary points to limitations in the adaptor's feature mapping or the LLM's adaptability.

**First Experiments:**
1. Test adaptor with varied EMG sensor placements to assess robustness.
2. Evaluate performance on a small open-vocabulary subset to probe scalability.
3. Compare adaptor-LLM pipeline with a baseline signal-only model on the same data.

## Open Questions the Paper Calls Out
The authors acknowledge that their current approach is limited to closed-vocabulary tasks and has not been tested in open-vocabulary or multilingual scenarios. They also note that the EMG adaptor's adaptability to different sensor configurations and subjects remains an open question.

## Limitations
- Performance limited to closed-vocabulary tasks (WER of 0.49 still substantial for real-world use)
- Minimal training data (six minutes) raises questions about scalability and generalization
- Not tested in open-vocabulary or multilingual settings, limiting practical deployment

## Confidence
**Medium:** The core claim that LLMs can effectively interpret unvoiced EMG signals is supported by strong results in a controlled setting, but uncertainties remain regarding scalability, generalization, and real-world applicability due to the limited scope of the experiments.

## Next Checks
1. Evaluate the EMG adaptor and LLM pipeline on open-vocabulary datasets to assess scalability and practical usability beyond controlled vocabularies.
2. Test the system's performance across diverse EMG sensor configurations and subjects to verify robustness and generalizability.
3. Conduct multilingual experiments to determine the model's adaptability to different languages and phonetic systems.