---
ver: rpa2
title: Generating Causal Explanations of Vehicular Agent Behavioural Interactions
  with Learnt Reward Profiles
arxiv_id: '2503.14557'
source_url: https://arxiv.org/abs/2503.14557
tags:
- causal
- agent
- reward
- work
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of generating causal explanations
  for vehicular agent behavioral interactions, focusing on transparency and explainability
  in autonomous vehicles. The proposed method integrates one-shot inverse reinforcement
  learning with twin-world counterfactual inference to learn reward profiles that
  capture agent motivations at the time of decision-making.
---

# Generating Causal Explanations of Vehicular Agent Behavioural Interactions with Learnt Reward Profiles

## Quick Facts
- **arXiv ID:** 2503.14557
- **Source URL:** https://arxiv.org/abs/2503.14557
- **Reference count:** 40
- **One-line primary result:** Proposed method significantly improves precision and F1 score for causal link detection in vehicular interactions compared to baselines.

## Executive Summary
This work addresses the challenge of generating causal explanations for vehicular agent behavioral interactions, focusing on transparency and explainability in autonomous vehicles. The proposed method integrates one-shot inverse reinforcement learning with twin-world counterfactual inference to learn reward profiles that capture agent motivations at the time of decision-making. The approach uses linear regression to learn weightings of reward metrics, enabling more accurate causal inference of agent planning. Quantitative experiments on the highD dataset demonstrate significant improvements over previous reward-based methods, with competitive performance across evaluation metrics. The method achieves a precision of 0.774, recall of 0.835, and F1 score of 0.768 at a threshold of zero, while maintaining higher precision than even the best-performing baseline at higher thresholds. Qualitative results on multiple datasets illustrate the enhanced expressiveness of the approach, providing insights into agent interactions and motivations. The work represents a step towards responsible development of autonomous vehicles by adhering to principles of explainability and transparency.

## Method Summary
The method learns reward profiles for vehicular agents using one-shot inverse reinforcement learning via linear regression on simulated hypothetical outcomes versus observed outcomes. It then performs twin-world counterfactual inference by comparing optimal planned actions under observed versus counterfactual world states where causing agent actions are removed. The approach uses a modular Structural Causal Model (SCM) architecture with components including Point Mass, Rigid Body, Vehicle, Entity, Link, Controller, and Planner modules to simulate vehicle dynamics and generate counterfactual scenarios. The pipeline extracts discrete time-action pairs from continuous trajectory data, learns reward profiles, simulates both observed and counterfactual worlds, compares planned actions using an action distance function, and builds causal graphs for textual explanation generation.

## Key Results
- Achieves precision of 0.774, recall of 0.835, and F1 score of 0.768 at threshold zero on highD dataset
- Significant improvement over SimCARS-v1 reward-based variant in terms of precision
- Maintains higher precision than even best-performing baseline at higher thresholds
- Competitive performance across evaluation metrics while providing interpretable reward profiles
- Successfully demonstrates causal explanations across three real-world driving datasets (highD, inD, exiD)

## Why This Works (Mechanism)

### Mechanism 1: One-Shot Reward Profile Learning via Linear Regression
- Claim: Instantaneous agent motivations can be approximated as a linear weighting of reward metrics by comparing observed actions against simulated alternatives.
- Mechanism: For an observed action $a$, generate hypothetical action set $\{\hat{a}_0, \hat{a}_1, ...\}$ using SCM simulation. Compute outcomes $\{\hat{o}_0, \hat{o}_1, ...\}$ and compare to observed outcome $o$ via distance function (Eq. 8). Formulate as linear regression: $r(\hat{o})p = e^{-d(o,\hat{o})}$, solved via Householder QR decomposition with column pivoting.
- Core assumption: Agents are intentional systems that act to maximize reward; the predefined reward metrics $r_0$-$r_4$ (lane transitions, distance headway, speed preferences, collision avoidance) sufficiently capture agent motivations.
- Evidence anchors:
  - [Section IV]: "From here we utilise the Householder rank-revealing QR decomposition with column pivoting approach implemented by Eigen to provide a solution for p."
  - [Section VI-A Results]: "The proposed method demonstrates a massive improvement over the SimCARS-v1 reward-based variant in terms of precision."
  - [corpus]: Weak direct corpus support for one-shot IRL specifically; related work (Delphos) frames specification as sequential decision-making, not single-timestep inference.
- Break condition: If agent motivations depend on reward features not in $\{r_0, ..., r_4\}$, the learned profile $p$ will use available metrics as proxies, potentially misrepresenting true motivations and degrading counterfactual accuracy.

### Mechanism 2: Twin-World Counterfactual Inference for Causal Necessity
- Claim: Causal necessity between agent actions can be detected by comparing optimal planned actions under observed vs. counterfactual world states.
- Mechanism: For agent pair $(A, C)$ with actions $(a_A, a_C)$: (1) Learn reward profile $p_A$ at time $t_{a_A}$; (2) Plan optimal action $\tilde{a}_A$ under observed world $W$; (3) Plan optimal action $\tilde{a}^{\neg C}_A$ under counterfactual world $W^{\neg C}$ where $C$ never executed $a_C$; (4) If $d(\tilde{a}_A, \tilde{a}^{\neg C}_A) > \lambda_a$, causal link exists.
- Core assumption: The SCM dynamics model and reward profile sufficiently approximate agent cognition that counterfactual predictions reflect what the agent *would have done*.
- Evidence anchors:
  - [Abstract]: "We validate our approach quantitatively and qualitatively across three real-world driving datasets."
  - [Section V]: "Provided the output of (10) is greater than a predetermined threshold $\lambda_a$, we determine that the actions are sufficiently different, and thus $C$ executing $a_C$ caused $A$ to select $a_A$."
  - [corpus]: MACIE paper addresses multi-agent causal explanation but uses different attribution methods; no direct validation of twin-world approach in corpus.
- Break condition: If SCM simulation diverges from real physics (e.g., undefined behavior at lane branching points) or reward profile is inaccurate, counterfactual actions become unreliable.

### Mechanism 3: Modular SCM Architecture for Generative Simulation
- Claim: Hierarchical SCM modules enable tractable simulation of vehicle dynamics for both reward learning and counterfactual inference.
- Mechanism: Compose modules: Point Mass (kinematics) → Rigid Body (rotation) → Vehicle (dynamic bicycle model) → Entity/Link (environment forces, collisions) → Controller (PD control from goals) → Planner (action selection via reward maximization). Each module's structural equations compute endogenous variables from parent modules.
- Core assumption: The Markovian assumption (single time-step lag) holds; vehicle dynamics are adequately captured by the dynamic bicycle model.
- Evidence anchors:
  - [Section III-B]: Full module hierarchy specified with inheritance relationships.
  - [Fig. 2]: SCM architecture diagram showing module dependencies.
  - [corpus]: No direct corpus validation of this specific SCM architecture; related work uses CARLA simulator for similar purposes.
- Break condition: If real-world dynamics violate model assumptions (complex tire friction, multi-vehicle collisions), simulated outcomes $\hat{o}$ will diverge from actual behavior.

## Foundational Learning

- **Structural Causal Models (SCMs)**
  - Why needed here: The entire architecture builds on Pearl's SCM formalism—understanding exogenous/endogenous variables, structural equations, and interventions is prerequisite to comprehending how counterfactual worlds are constructed.
  - Quick check question: Given an SCM with intervention $do(V=v)$, can you explain why this differs from conditioning on $V=v$?

- **Inverse Reinforcement Learning (IRL) Basics**
  - Why needed here: The reward profile learning mechanism is inspired by IRL—specifically the assumption that observed actions reveal information about the reward function being optimized.
  - Quick check question: If an agent consistently chooses actions that maintain large following distance, what does IRL infer about its reward function?

- **Proportional-Derivative (PD) Control**
  - Why needed here: The Controller SCM uses PD control to convert speed/lane goals into motor torque and steering commands. Understanding gain tuning affects simulation fidelity.
  - Quick check question: What happens to control output when the error term is zero but the derivative term is non-zero?

## Architecture Onboarding

- **Component map:**
  Vehicle Tracks -> Action Extractor -> Reward Profile Learner -> SCM: Point Mass -> Rigid Body -> Vehicle -> Entity <- Link (collisions, distance) <- World State W -> Controller (PD) <- Planner (reward-based action selection) -> Causal Link Detector <- Counterfactual World W¬C

- **Critical path:**
  1. Extract discrete time-action pairs from continuous trajectory data (uses method from prior work [2])
  2. For each action $a_A$, generate hypothetical action set and simulate outcomes via SCM
  3. Solve linear regression for reward profile $p_A$
  4. For each candidate cause action $a_C$, simulate $W^{\neg C}$ by intervening to maintain $C$'s previous action
  5. Compare optimal actions $\tilde{a}_A$ vs. $\tilde{a}^{\neg C}_A$ using action distance function (Eq. 10)
  6. Build causal graph from detected links; generate textual explanations using reward profile interpretation

- **Design tradeoffs:**
  - Linear weighting for interpretability vs. neural networks for expressiveness (authors chose linear explicitly for human interpretability)
  - Predefined reward metrics $\{r_0, ..., r_4\}$ vs. learned features (predefined enables interpretability but risks incompleteness)
  - Threshold $\lambda_a = 0$ maximizes recall with minor precision cost; higher thresholds increase precision at recall expense

- **Failure signatures:**
  - **Nonsensical planned actions** (e.g., vehicle veering off-road): Occurs at lane branching points where default behavior is undefined and $r_0$-$r_4$ don't encode lane-following preference
  - **Low sensitivity (recall ceiling ~0.835)**: May indicate reward metrics insufficient to capture agent motivations, causing proxy features to dominate profile
  - **High FPR on independent vehicles**: Check if simulation parameters (vehicle mass, dimensions) are misconfigured for dataset

- **First 3 experiments:**
  1. **Reproduce highD quantitative results**: Extract the 115 scenes with causal adjacency (front/rear vehicle pairs), run full pipeline, verify precision/recall match Fig. 3a. This validates implementation correctness.
  2. **Ablate reward metrics**: Remove $r_1$ (distance headway) and measure precision drop on tailgating scenarios. Quantifies each metric's contribution.
  3. **Test threshold sensitivity**: Sweep $\lambda_a \in [0, 0.5]$ and plot ROC curve. Confirm paper's claim that $\lambda_a = 0$ is practical choice.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a data-driven model (e.g., a neural network) be integrated to validate or sanity-check the linear reward profiles without sacrificing the system's interpretability?
- Basis in paper: [explicit] The authors suggest in Section VII that one could "train some form of data-driven model... to sanity check the current weighted reward model."
- Why unresolved: The current method relies on linear regression specifically because it is inherently easy to interpret, a trait a black-box replacement might lose.
- What evidence would resolve it: A hybrid architecture that maintains human-level interpretability scores while reducing the failure cases associated with the simpler linear model.

### Open Question 2
- Question: Does integrating this causal explanation method into a reinforcement learning (RL) loop improve agent efficiency and social awareness?
- Basis in paper: [explicit] Section VII proposes "integrating behavioural interaction causal modelling into a RL loop, potentially allowing for greater efficiency and socially-awareness."
- Why unresolved: The current work focuses solely on post-hoc explanation generation rather than real-time policy improvement or online interaction.
- What evidence would resolve it: Empirical results from an online RL agent utilizing this causal model to demonstrate improved social navigation metrics compared to standard baselines.

### Open Question 3
- Question: How can the reward metric design be improved to robustly handle undefined behaviors at lane branching points?
- Basis in paper: [inferred] Section VI-B identifies a failure case where agents veer off-road, attributing it to "default behaviour at the branching point... being undefined" and metrics not accounting for "lane following."
- Why unresolved: The current set of metrics ($r_0$ through $r_4$) and the planner logic fail to capture necessary constraints for complex road topologies like intersections.
- What evidence would resolve it: Successful generation of sensible counterfactual plans in intersection scenarios (like Fig. 4d) without producing "nonsensical" off-road trajectories.

## Limitations

- The method's effectiveness depends heavily on the predefined reward metrics ($r_0$-$r_4$) sufficiently capturing all relevant agent motivations, which may not hold for complex driving scenarios
- The twin-world counterfactual inference relies on accurate SCM simulation of vehicle dynamics, which may break down in edge cases involving multi-vehicle interactions or unusual road geometries
- The generalizability of the predefined reward metrics to capture all relevant driving behaviors across different cultural driving styles and road contexts remains uncertain

## Confidence

- **High Confidence:** The quantitative evaluation methodology on the highD dataset and the demonstrated precision/recall improvements over baselines (SimCARS-v1, Granger, DYNOTEARS) are methodologically sound and reproducible.
- **Medium Confidence:** The causal inference mechanism (twin-world comparison) is theoretically valid, but its practical effectiveness depends on the accuracy of both the reward profile learning and SCM simulation, which may vary across datasets.
- **Low Confidence:** The generalizability of the predefined reward metrics to capture all relevant driving behaviors across different cultural driving styles and road contexts remains uncertain.

## Next Checks

1. **Cross-dataset validation:** Apply the pipeline to inD and exiD datasets and compare performance metrics to highD results, particularly focusing on precision degradation in urban vs. highway scenarios.
2. **Reward metric ablation study:** Systematically remove individual reward metrics ($r_0$-$r_4$) and measure the impact on causal link detection accuracy for specific interaction types (e.g., lane changes, speed adjustments).
3. **Real-world intervention test:** Use the learned reward profiles to predict agent behavior in controlled scenarios where ground truth causal relationships are known through experimental manipulation of one vehicle's actions.