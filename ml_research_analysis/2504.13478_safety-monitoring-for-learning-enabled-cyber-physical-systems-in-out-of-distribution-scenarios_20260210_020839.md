---
ver: rpa2
title: Safety Monitoring for Learning-Enabled Cyber-Physical Systems in Out-of-Distribution
  Scenarios
arxiv_id: '2504.13478'
source_url: https://arxiv.org/abs/2504.13478
tags:
- safety
- prediction
- learning
- conformal
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a direct safety monitoring approach for learning-enabled
  cyber-physical systems (LE-CPS) in out-of-distribution (OOD) scenarios, arguing
  that OOD detection alone is insufficient since OOD inputs do not necessarily lead
  to safety violations. The method predicts violations of signal temporal logic (STL)
  safety specifications by predicting future trajectories using adaptive conformal
  prediction (ACP) combined with incremental learning (IL).
---

# Safety Monitoring for Learning-Enabled Cyber-Physical Systems in Out-of-Distribution Scenarios

## Quick Facts
- arXiv ID: 2504.13478
- Source URL: https://arxiv.org/abs/2504.13478
- Reference count: 40
- Primary result: ACP+IL monitors achieve 0.94-0.98 recall, 4.75-4.93 steps ahead timeliness, and 0.59-0.76 precision while maintaining probabilistic guarantees under distribution shift

## Executive Summary
This paper addresses safety monitoring for learning-enabled cyber-physical systems (LE-CPS) in out-of-distribution (OOD) scenarios. The authors argue that traditional OOD detection is insufficient since OOD inputs do not necessarily lead to safety violations. Instead, they propose a direct safety monitoring approach that predicts future trajectories and evaluates them against signal temporal logic (STL) safety specifications using adaptive conformal prediction (ACP) combined with incremental learning (IL). The method provides probabilistic guarantees while maintaining high recall and timeliness in detecting safety violations.

## Method Summary
The method monitors safety by predicting future trajectories over a horizon H using an ensemble of predictors, then computing STL robustness values. When predicted robustness falls below an adaptive threshold determined by ACP, an alarm is raised. ACP updates its significance level δt based on recent prediction errors, maintaining probabilistic guarantees under distribution shift. IL mitigates precision loss by fine-tuning predictors on high-error samples collected at runtime, using K-means clustering to maintain multiple specialized predictors. The system operates in a predict-then-evaluate loop, updating the ACP threshold and potentially triggering IL based on prediction error.

## Key Results
- ACP+IL achieves high recall (0.94-0.98) and timeliness (4.75-4.93 steps ahead) in OOD scenarios
- The approach maintains competitive precision (0.59-0.76) while other methods fail to provide guarantees
- ACP outperforms standard conformal and robust conformal prediction in OOD settings
- IL recovers precision lost to ACP's conservative intervals while maintaining safety guarantees

## Why This Works (Mechanism)

### Mechanism 1: Direct Safety Prediction vs. OOD Detection
Monitoring safety directly via predicted trajectories is more effective for LE-CPS than flagging OOD inputs, as OOD inputs do not strictly imply safety violations. The architecture bypasses input distribution monitoring by observing system state history, predicting future trajectory, and computing STL robustness score. An alarm is raised only if predicted robustness implies future violation of safety specification, ignoring whether input was statistically "novel."

### Mechanism 2: Adaptive Conformal Prediction (ACP) for Guarantees
ACP allows the monitor to maintain probabilistic guarantees on safety predictions even when data distribution shifts arbitrarily. Standard CP fails when test data distribution shifts. ACP adaptively updates significance level δt at each time step based on recent errors. If predictor errs on novel data, ACP compensates by enlarging prediction region to ensure true robustness value remains within bounds with probability ≈ 1-δ.

### Mechanism 3: Incremental Learning (IL) for Precision Recovery
IL mitigates precision loss caused by ACP's conservative intervals by reducing trajectory predictor's error on novel distributions. ACP naturally enlarges prediction intervals to maintain safety guarantees when errors are high, hurting precision. IL counters this by fine-tuning trajectory predictor on high-error samples collected at runtime, maintaining a set of predictors and selecting the best one for current input distribution dynamically.

## Foundational Learning

- **Concept: Signal Temporal Logic (STL) Robustness**
  - Why needed: Binary "safe/unsafe" labels are insufficient for prediction. STL robustness provides continuous scalar value indicating how close trajectory is to violating safety spec.
  - Quick check: Given trajectory that passes 0.1m from obstacle vs. one that passes 5m away, how would robustness value differ?

- **Concept: Exchangeability in Conformal Prediction**
  - Why needed: To understand why standard CP fails in dynamic CPS. Time-series data and OOD scenarios violate assumption that test data is exchangeable with calibration set.
  - Quick check: If you shuffle a time-series dataset, does it retain same statistical properties relevant to trajectory prediction task?

- **Concept: Catastrophic Forgetting**
  - Why needed: Paper uses predictor set rather than updating single model. Understanding that neural networks forget previous tasks when fine-tuned on new data explains complexity of proposed IL architecture.
  - Quick check: What would happen to safety monitor's performance on "straight road" scenarios if you fine-tuned predictor exclusively on "sharp turns" without ensemble approach?

## Architecture Onboarding

- **Component map:** Observer -> Predictor Ensemble -> Robustness Calculator -> ACP Module -> Decision Logic
- **Critical path:** Prediction error (Rt) drives ACP threshold (Ct). If IL reduces Rt by picking better predictor, Ct tightens, improving precision.
- **Design tradeoffs:**
  - Conservatism vs. Utility: Increasing δ or reducing learning rate γ makes monitor slower to adapt, potentially safer but more prone to false alarms
  - Timeliness vs. Accuracy: Longer prediction horizon H allows earlier warnings but generally degrades prediction accuracy, widening ACP intervals
- **Failure signatures:**
  - Hyper-Conservatism: If ACP intervals grow infinitely large, monitor always alarms, implying predictor failing or distribution shift too extreme
  - Silent Failure: If predictor overfits to specific OOD scenario and fails on new one without triggering error threshold τ
- **First 3 experiments:**
  1. Baseline Validation: Run monitor on in-distribution data to confirm empirical coverage matches theoretical 1-δ target
  2. Stress Test (ACP vs CP): Introduce synthetic distribution shift, plot coverage drops for standard CP vs. stable coverage (but wider intervals) for ACP
  3. Ablation on IL: Disable Incremental Learning component, compare precision of "ACP-only" vs. "ACP+IL" on shifted data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative methods for selecting fine-tuning data at runtime collect larger and richer datasets to improve the precision of the safety monitor?
- Basis: Conclusion states "Alternative methods for selecting fine-tuning data at runtime may allow larger and richer datasets to be collected, improving the precision of our safety monitor."
- Why unresolved: Current threshold-based selection may fail to capture sufficient data when OOD non-conformity score distributions are thin-tailed
- What evidence would resolve: Empirical evaluations comparing current approach against alternative heuristics showing statistically significant improvements in precision

### Open Question 2
- Question: How can smaller-scale models be utilized to mitigate the computational expense of fine-tuning trajectory predictors?
- Basis: Conclusion notes "Fine-tuning the trajectory predictor can also be computationally expensive. Exploring smaller-scale models may help to address this bottleneck."
- Why unresolved: Complex models (e.g., AgentFormer) ensure prediction accuracy but create computational bottleneck during incremental learning
- What evidence would resolve: Identification and evaluation of lightweight architectures maintaining required accuracy while reducing wall-clock time for incremental learning updates

### Open Question 3
- Question: How robust is the safety monitoring framework when assumptions of error-free state estimation and environment maps are violated?
- Basis: Section 5 states assumption of "error-free knowledge of environment map, system states, and obstacles"
- Why unresolved: Real-world CPS commonly experience sensor noise and state estimation errors
- What evidence would resolve: Simulation results with injected Gaussian or domain-specific noise into state observations, measuring degradation in coverage guarantees and safety metrics

## Limitations

- The incremental learning mechanism requires maintaining multiple predictors and performing K-means clustering, potentially becoming computationally prohibitive as OOD scenarios grow
- The method assumes observable system states and fixed STL safety specification, limiting applicability to partially observable systems or those requiring adaptive specifications
- The paper does not provide runtime performance metrics or memory usage analysis for the monitoring system

## Confidence

- **High** for claims about fundamental limitation of OOD detection for safety monitoring, supported by theoretical arguments and empirical results
- **Medium** for claims about ACP's effectiveness, well-supported by theory but empirical validation limited to two specific case studies
- **Medium** for claims about IL's benefits, primarily demonstrated through ablation studies rather than direct comparison with state-of-the-art OOD handling methods

## Next Checks

1. **Runtime Overhead Analysis**: Measure computational cost and memory usage of monitor during operation, particularly IL component, to assess scalability

2. **Robustness to Specification Misspecification**: Evaluate system behavior when STL safety specification contains errors or is incomplete to understand failure modes

3. **Cross-Domain Transferability**: Test monitor on third, structurally different CPS (e.g., UAV or manipulator) to validate generalizability beyond automotive scenarios