---
ver: rpa2
title: 'Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated
  Learning via LLM-Generated Survey Responses'
arxiv_id: '2506.13384'
source_url: https://arxiv.org/abs/2506.13384
tags:
- data
- llms
- responses
- factor
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study examined whether large language models (LLMs) can simulate
  human-like responses to the Motivated Strategies for Learning Questionnaire (MSLQ),
  a widely used instrument for assessing self-regulated learning (SRL). Responses
  from 1,000 simulated students were generated using five LLMs: GPT-4o, Claude 3.7
  Sonnet, Gemini 2 Flash, LLaMA 3.1-8B, and Mistral Large.'
---

# Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses

## Quick Facts
- **arXiv ID**: 2506.13384
- **Source URL**: https://arxiv.org/abs/2506.13384
- **Reference count**: 30
- **Key outcome**: Gemini 2 Flash produced the most theoretically aligned psychological networks and factor structures for MSLQ data among five tested LLMs, though all models showed limited variability compared to human responses

## Executive Summary
This study investigates whether large language models (LLMs) can simulate human-like responses to the Motivated Strategies for Learning Questionnaire (MSLQ), a widely used instrument for assessing self-regulated learning. Using five LLMs (GPT-4o, Claude 3.7 Sonnet, Gemini 2 Flash, LLaMA 3.1-8B, and Mistral Large), the researchers generated responses from 1,000 simulated students and analyzed item distributions, psychological networks, and psychometric validity. Results showed that while Gemini 2 Flash exhibited the highest variability and most theoretically aligned structures, all models produced responses with constrained variability and high predictability, suggesting potential overfitting to training data.

## Method Summary
The researchers generated synthetic MSLQ responses from 1,000 simulated students using five LLMs with standardized prompts. They conducted three types of analyses: descriptive statistics and outlier detection, psychological network analysis using Gaussian Graphical Models, and factor analysis (both CFA and EFA). The study compared model outputs against theoretical expectations of the MSLQ's five-factor structure and evaluated the quality of psychological networks through edge weights and node predictability metrics.

## Key Results
- Gemini 2 Flash showed the highest variability (SDs >1.5) and produced psychological networks most aligned with SRL theory
- All models generated responses with limited variability compared to expected human data, with GPT showing particularly compressed distributions (SDs ~0.5-0.7)
- None of the models fully replicated the original MSLQ factor structure, though Gemini and Claude demonstrated the most coherent patterns
- Psychological networks showed high predictability (R² values), suggesting artificial coherence unlike real human data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs may encode structural relationships between psychological constructs from training data that surface when generating survey responses
- Mechanism: Models trained on educational and psychological literature implicitly capture theoretical relationships between SRL constructs (e.g., negative correlation between Test Anxiety and Self-Efficacy). When prompted to generate survey responses, these latent patterns manifest in the correlation structures of generated data
- Core assumption: Training corpora contain sufficient psychological and educational research for models to internalize theoretical construct relationships
- Evidence anchors:
  - Gemini and Claude showed negative correlations between TA and SE factors, aligning with empirical findings
  - Limited direct support—neighbor paper "Large Language Models Do Not Simulate Human Psychology" explicitly challenges this mechanism

### Mechanism 2
- Claim: Response variability in LLM-generated surveys depends on model-specific sampling characteristics and temperature settings
- Mechanism: Models differ in their default sampling behavior. Gemini showed substantial variability while GPT showed compressed distributions, affecting outlier detection and psychometric properties
- Core assumption: Sampling variability is a function of model architecture/training rather than purely prompt engineering
- Evidence anchors:
  - SDs vary considerably between LLMs: Gemini and LLaMa display SDs above 1.5
  - Gemini showed 98 outliers in IV dimension; LLaMa showed zero outliers across all dimensions

### Mechanism 3
- Claim: LLMs prioritize linguistic coherence over psychological authenticity when interpreting reverse-coded items
- Mechanism: Models process item semantic content literally; reverse-coded items may load on different factors based on surface meaning rather than intended construct directionality
- Core assumption: Models lack explicit representation of questionnaire design conventions like reverse-coding
- Evidence anchors:
  - GPT, LLaMa, and Mistral show more diffuse patterns and fail to group reverse-coded items with their intended construct
  - For Gemini, reverse-coded item Q26 loaded with TA items

## Foundational Learning

- Concept: Factor analysis (EFA/CFA)
  - Why needed here: The paper evaluates whether LLM-generated data reproduces the MSLQ's theoretical factor structure; understanding loadings, fit indices (CFI, RMSEA, SRMR), and factor interpretation is essential for interpreting results
  - Quick check question: If all items load strongly on a single factor with minimal cross-loadings, what does this suggest about construct discriminant validity?

- Concept: Gaussian Graphical Models and psychological networks
  - Why needed here: The paper uses network analysis to examine conditional dependencies between SRL constructs; edges represent partial correlations controlling for all other nodes
  - Quick check question: If predictability (R²) approaches 1.0 for all nodes in a psychological network, what problem does this create for interpreting which processes are "driving" the system?

- Concept: Self-Regulated Learning (SRL) theory
  - Why needed here: The MSLQ measures five SRL dimensions (Intrinsic Value, Self-Efficacy, Test Anxiety, Cognitive Strategy Use, Self-Regulation); evaluating whether LLM outputs align with theory requires understanding expected relationships
  - Quick check question: Why would Test Anxiety be expected to correlate negatively with Self-Efficacy but potentially positively with certain Cognitive Strategy Use items related to test preparation?

## Architecture Onboarding

- Component map: Data generation pipeline (batched API calls -> dynamic prompt parameterization -> CSV output validation -> demographic profile assignment) -> Validation framework (descriptive statistics -> network analysis -> factor analysis) -> Model comparison layer

- Critical path: 1) Generate N=1,000 synthetic student profiles per model with 44-item responses 2) Clean data: handle missing values, illegal responses, repeated names 3) Compute construct-level scores with reverse-coding 4) Build psychological networks and compute predictability metrics 5) Run CFA with 4-factor and 5-factor models; assess global fit 6) Run EFA with parallel analysis; interpret factor loadings against theoretical groupings

- Design tradeoffs:
  - 44-item vs. 81-item MSLQ: Shorter version reduces token overflow/truncation but limits construct coverage
  - Batch size (10 students): Smaller batches improve reliability but increase API calls and runtime (15 hours total across 5 models)
  - Complete-case analysis: Simplifies interpretation but loses data

- Failure signatures:
  - Token exhaustion: Responses truncate mid-questionnaire (staircase missingness pattern in Appendix C)
  - Format drift: Models output markdown/explanatory text instead of pure CSV
  - Construct collapse: All items load on single general factor (GPT, LLaMa heatmap patterns)
  - Implausible R² values: Network predictability >0.90 suggests artificial coherence unlike real human data

- First 3 experiments:
  1. Baseline replication: Generate N=500 responses from a single model (recommend Gemini) with the paper's exact prompt structure; validate that factor structure and network metrics fall within reported ranges
  2. Temperature sensitivity test: Systematically vary temperature (0.0, 0.5, 1.0, 1.5) while holding prompt constant; measure impact on SDs, outlier counts, and factor structure stability
  3. Persona-conditioned generation: Compare generic student profiles against specific personas (e.g., "low motivation, high test anxiety"); assess whether persona consistency holds across 44 items and whether psychometric properties improve or degrade

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs reliably simulate persona-specific responses to self-regulated learning questionnaires, where responses align with defined demographic or motivational profiles?
- Basis in paper: The authors state: "However, we view this as a logical next step; that is, generating data from varied personas and evaluating how well certain LLMs (at minimum, Gemini) produce responses aligned with these profiles... This remains an open question that warrants investigation, specifically in the context of SRL."
- Why unresolved: The study deliberately avoided personas because baseline LLM responses already showed inconsistencies; adding persona constraints could exacerbate issues. Maintaining consistent personas across 44 items may exceed LLM working memory.

### Open Question 2
- Question: How sensitive are LLM-generated survey responses to variations in prompt wording, structure, and parameters?
- Basis in paper: The authors note: "Another important limitation is that we did not investigate prompt sensitivity, and hence, the extent to which LLM outputs are influenced by the exact phrasing and structure of the input prompts."
- Why unresolved: Even minor prompt changes can alter response distributions, threatening reproducibility and generalizability. Prior research showed synthetic responses vary significantly over time even with identical prompts.

### Open Question 3
- Question: Do LLMs produce realistic variability and outliers in psychological survey data comparable to human respondents?
- Basis in paper: The authors note overly high predictability (R²) in psychological networks and state: "this study does not provide deep insights into the variability in LLM-generated data."
- Why unresolved: High R² values suggest overfitting; real human data contains noise, outliers, and careless responses. Only Gemini showed notable outliers across dimensions.

## Limitations
- All models showed constrained variability compared to expected human data, with high predictability suggesting artificial coherence
- None of the models fully replicated the original MSLQ factor structure, though Gemini and Claude showed closest alignment
- Absence of ground-truth human MSLQ responses for direct comparison prevents determining whether LLM outputs better match theoretical expectations or human behavior

## Confidence
- **High confidence**: Models differ systematically in their output variability and psychometric properties (Gemini > Claude > Mistral > LLaMa > GPT)
- **Medium confidence**: Gemini and Claude's factor structures show the closest alignment with theoretical expectations, though none perfectly replicate the MSLQ structure
- **Low confidence**: Claims about LLMs encoding genuine psychological relationships rather than surface-level patterns

## Next Checks
1. Collect MSLQ responses from a human sample of comparable size (N=1,000) and directly compare factor structures, network properties, and variability distributions between human and LLM-generated data
2. Systematically vary temperature from 0.0 to 1.5 across all five models while holding prompts constant; measure how psychometric properties (SDs, outlier counts, factor fit indices) change with sampling randomness
3. Generate responses for synthetic students with deliberately manipulated psychological profiles (e.g., high test anxiety/low self-efficacy vs. low test anxiety/high self-efficacy); assess whether these profiles produce predictable patterns in factor loadings and network structures across models