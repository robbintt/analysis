---
ver: rpa2
title: Reinforcement Learning Finetunes Small Subnetworks in Large Language Models
arxiv_id: '2505.11711'
source_url: https://arxiv.org/abs/2505.11711
tags:
- sparsity
- training
- wang
- updates
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper reveals that reinforcement learning in large language
  models induces parameter update sparsity, where only 5-30% of parameters are actively
  updated, leaving the rest unchanged. This phenomenon occurs across seven RL algorithms
  (PPO, GRPO, DPO, etc.) and ten different LLM families without explicit sparsity-promoting
  techniques.
---

# Reinforcement Learning Finetunes Small Subnetworks in Large Language Models

## Quick Facts
- arXiv ID: 2505.11711
- Source URL: https://arxiv.org/abs/2505.11711
- Reference count: 40
- Primary result: RL training updates only 5-30% of parameters, and fine-tuning only this subnetwork matches full-finetuned model performance.

## Executive Summary
This paper demonstrates that reinforcement learning in large language models induces sparse parameter updates, with only 5-30% of parameters actively changing during finetuning. This phenomenon occurs across seven different RL algorithms (PPO, GRPO, DPO, etc.) and ten LLM families without any explicit sparsity-promoting techniques. Remarkably, training only the identified sparse subnetwork from the same initialization reproduces both the performance and near-identical parameter values of the full-finetuned model. The sparsity appears primarily driven by training on in-distribution data, with KL regularization and gradient clipping having limited impact. This discovery opens possibilities for more efficient RL training methods that explicitly leverage this naturally emerging update sparsity.

## Method Summary
The study analyzes checkpoint pairs from seven RL algorithms and ten different LLM families to measure parameter update sparsity. Sparsity is computed as 1 - ||θ₁ - θ₀||₀ / n, identifying which parameters changed beyond a bfloat16 tolerance of 1e-5. A binary mask marks the subnetwork (parameters with non-zero updates), which is then used to constrain subsequent training to only those parameters. The researchers compare full-finetuned and subnetwork-only models on test accuracy across AGIEval, MATH500, and MMLU Pro benchmarks. Detailed training configurations are provided for DPO (LLaMA-3.1-Tulu-3-8B, 8 processes, LR 5e-7, 1 epoch) and PRIME (Qwen2.5-Math-7B, 4 rollouts/sample, grad clip 10.0, 15 epochs), while other algorithms rely on public checkpoints.

## Key Results
- RL algorithms update only 5-30% of parameters during finetuning, leaving the rest unchanged
- Fine-tuning only the identified subnetwork reproduces both performance and near-identical parameter values of full-finetuned models
- Layer normalization layers are the primary exception, receiving dense rather than sparse updates
- Training on in-distribution data is the main driver of sparsity, while KL regularization and gradient clipping have limited impact

## Why This Works (Mechanism)
None provided in the paper.

## Foundational Learning
- Parameter update sparsity measurement: Required to quantify which parameters change during training; quick check is computing 1 - ||θ₁ - θ₀||₀ / n with appropriate tolerance
- Binary subnetwork masking: Essential for constraining training to only the active parameters; quick check is verifying mask correctly identifies non-zero parameter differences
- Gradient clipping effects: Important for understanding regularization impact on sparsity; quick check is comparing sparsity with/without clipping
- In-distribution vs out-of-distribution effects: Critical for understanding data's role in sparsity emergence; quick check is measuring sparsity on different data distributions
- Checkpoint differencing: Fundamental for identifying which parameters changed; quick check is validating difference calculations with known parameter modifications

## Architecture Onboarding

### Component Map
Public checkpoint pairs (SFT, RL) -> Parameter difference computation -> Binary subnetwork mask generation -> Subnetwork-only training from SFT initialization -> Performance comparison

### Critical Path
1. Load public SFT and RL checkpoint pairs
2. Compute parameter differences to identify sparse subnetwork
3. Generate binary mask from non-zero updates
4. Train subnetwork-only model from same SFT initialization
5. Compare performance and parameter values with full RL model

### Design Tradeoffs
The approach trades computational efficiency (training fewer parameters) for potential architectural complexity (managing sparse masks and ensuring proper gradient flow). The method assumes bfloat16 precision and specific tolerance thresholds, which may not generalize to all training scenarios. Using post-hoc analysis limits practical deployment compared to methods that could identify sparse subnetworks during training.

### Failure Signatures
- Incorrect sparsity measurement due to tolerance issues (using float32 expectations on bfloat16 data)
- Subnetwork-only training diverges or underperforms due to improper mask application
- Performance degradation when testing on out-of-distribution data
- Layer normalization layers failing to update adequately, causing training instability

### First Experiments
1. Compute sparsity on a simple checkpoint pair (SFT vs DPO) to verify measurement methodology
2. Apply binary mask to a small subset of parameters and confirm gradient zeroing works correctly
3. Train a minimal model (e.g., 1-2 layers) using subnetwork-only approach and compare with full training

## Open Questions the Paper Calls Out
1. Can the sparse subnetwork be identified early in the training process to maximize efficiency gains?
2. Does the parameter update sparsity phenomenon extend to multimodal and diffusion models?
3. What is the formal theoretical explanation for why training on in-distribution data induces update sparsity?
4. Do complex interactions between training factors drive sparsity more than individual factors?

## Limitations
- Analysis relies on public checkpoints without access to training seeds, data shuffles, or full algorithm configurations
- Claims about KL regularization and gradient clipping impact are based on indirect evidence without direct isolation experiments
- Study is limited to bfloat16 precision, potentially limiting generalization to other precision settings
- The correlation between in-distribution data and sparsity is established empirically but not theoretically explained

## Confidence
- High: Core empirical observation that RL induces sparse parameter updates (5-30% of parameters) is directly measured and consistent across multiple algorithms and model families
- Medium: Claim that subnetwork-only training reproduces full-finetuned performance is demonstrated but depends on precise hyperparameter matching
- Low: Hypothesis that KL regularization and gradient clipping have limited impact on sparsity lacks direct experimental isolation

## Next Checks
1. Recompute sparsity using float32 precision and compare to bfloat16 results to assess precision dependence
2. Train subnetwork-only models using exact seeds and data splits from the original runs (if available) to isolate randomness effects
3. Test subnetwork performance on out-of-distribution tasks to verify robustness beyond in-distribution data