---
ver: rpa2
title: Towards Formalizing Reinforcement Learning Theory
arxiv_id: '2511.03618'
source_url: https://arxiv.org/abs/2511.03618
tags:
- learning
- convergence
- theorem
- linear
- almost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes the almost sure convergence of Q-learning
  and linear TD learning with Markovian samples using the Lean 4 theorem prover. The
  work addresses the challenge of verifying convergence proofs in reinforcement learning,
  which are often delicate due to their reliance on ODE-based approaches and the need
  for rigorous treatment of Markov Decision Process frameworks.
---

# Towards Formalizing Reinforcement Learning Theory

## Quick Facts
- arXiv ID: 2511.03618
- Source URL: https://arxiv.org/abs/2511.03618
- Reference count: 4
- First formalization of almost sure convergence proofs for Q-learning and linear TD learning in Lean 4 theorem prover

## Executive Summary
This paper presents the first formalization of almost sure convergence proofs for Q-learning and linear TD learning using Markovian samples in the Lean 4 theorem prover. The work addresses the challenge of verifying delicate convergence proofs in reinforcement learning by developing a unified framework based on the Robbins-Siegmund theorem, replacing traditional ODE-based approaches. The formalization uses skeleton iterates techniques to convert Markovian noise to Martingale difference noise, enabling rigorous treatment of convergence properties including almost sure convergence, high probability concentration, Lp convergence, and convergence rates.

## Method Summary
The formalization employs the Robbins-Siegmund theorem framework to prove almost sure convergence of Q-learning and linear TD learning. The approach converts Markovian noise to Martingale difference noise using skeleton iterates techniques, enabling rigorous treatment of convergence. The work uses Lyapunov functions (‖x‖²_p for p≥2) and the Ionescu-Tulcea theorem to construct sample path probability spaces. The formalization proves convergence under specific step-size conditions: α_t = 1/(t+2)^ν where ν ∈ (2/3, 1) for both algorithms, operating on finite MDPs with irreducible and aperiodic Markov chains.

## Key Results
- Proved almost sure convergence `lim_t→∞ q_t = q*` for Q-learning under step sizes α_t = 1/(t+2)^ν with ν ∈ (2/3, 1)
- Established convergence `lim_t→∞ w_t = w*` for linear TD learning with same step-size conditions
- First machine-checked verification of these convergence results in Lean 4
- Demonstrated unified framework using Robbins-Siegmund theorem instead of ODE-based approaches
- Formalized not only almost sure convergence but also high probability concentration, Lp convergence, and convergence rates

## Why This Works (Mechanism)
The formalization succeeds by converting the challenging Markovian noise structure into Martingale difference noise through skeleton iterates techniques. This transformation enables the application of martingale convergence theorems and the Robbins-Siegmund framework, which are more amenable to formal verification than ODE-based approaches. The use of Lyapunov functions provides a natural way to measure progress toward convergence, while the Ionescu-Tulcea theorem ensures proper construction of the probability space for sample paths.

## Foundational Learning
- **Robbins-Siegmund Theorem**: Provides conditions for almost sure convergence of supermartingales; needed for the main convergence proof structure
- **Skeleton Iterates**: Technique to convert Markovian processes to martingale sequences; required to handle temporal dependencies in Markov chains
- **Ionescu-Tulcea Theorem**: Constructs probability spaces for infinite sequences of random variables; essential for defining MDP sample paths
- **Lyapunov Functions**: Functions that decrease along trajectories; used to measure distance to optimal solutions
- **Martingale Difference Noise**: Stochastic process with zero conditional expectation; enables application of martingale convergence theory
- **Conditional Expectations**: Key tool for analyzing stochastic processes; took ~1,000 lines of Lean code to formalize properly

## Architecture Onboarding
**Component Map**: MDP framework -> Skeleton iterates -> Martingale conversion -> Robbins-Siegmund theorem -> Convergence proof
**Critical Path**: MDP specification → Markov chain construction → Skeleton iterates implementation → Martingale difference conversion → Robbins-Siegmund application → Final convergence proof
**Design Tradeoffs**: Chose Robbins-Siegmund over ODE-based approaches for better formalizability despite potentially more complex step-size requirements
**Failure Signatures**: Mathlib version mismatches causing compilation errors; conditional expectation proof timeouts or failures
**First Experiments**:
1. Verify basic MDP framework and Markov chain irreducibility/aperiodicity proofs
2. Test skeleton iterates construction on simple Markov chains
3. Validate Robbins-Siegmund theorem application on known martingale examples

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes finite MDPs with irreducible and aperiodic Markov chains, limiting scope compared to continuous or large-scale settings
- Requires step sizes α_t = 1/(t+2)^ν with ν ∈ (2/3, 1), more restrictive than typical practical values (ν = 0.5-0.8)
- Does not explore extension to weaker conditions or practical performance implications of the formalization

## Confidence
- High confidence in mathematical correctness due to machine-checked verification by Lean theorem prover
- Medium confidence in practical applicability due to restrictive assumptions and step-size requirements

## Next Checks
1. Verify Ionescu-Tulcea theorem formalization and measurable space constructions for MDP sample paths
2. Test the conditional expectation machinery (~1,000 lines) on simpler martingale problems
3. Validate step-size condition tightness by attempting proofs for ν = 2/3 or ν = 1 boundary cases