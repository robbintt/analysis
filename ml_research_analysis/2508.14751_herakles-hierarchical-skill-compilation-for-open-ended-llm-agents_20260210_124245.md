---
ver: rpa2
title: 'HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents'
arxiv_id: '2508.14751'
source_url: https://arxiv.org/abs/2508.14751
tags:
- policy
- goal
- goals
- low-level
- skill
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HERAKLES is a hierarchical reinforcement learning framework that
  enables open-ended agents to continuously learn and adapt to increasingly complex
  goals by dynamically compiling mastered skills into a low-level policy. It uses
  a large language model as a high-level controller to decompose goals into subgoals
  and train a lightweight low-level policy to execute them using elementary actions.
---

# HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents
## Quick Facts
- arXiv ID: 2508.14751
- Source URL: https://arxiv.org/abs/2508.14751
- Reference count: 40
- Key outcome: HERAKLES enables open-ended agents to continuously learn and adapt to increasingly complex goals by dynamically compiling mastered skills into a low-level policy

## Executive Summary
HERAKLES is a hierarchical reinforcement learning framework that enables open-ended agents to continuously learn and adapt to increasingly complex goals. The system uses a large language model as a high-level controller to decompose goals into subgoals, while a lightweight low-level policy executes these subgoals using elementary actions. As the agent masters new goals, their corresponding skill trajectories are compiled into the low-level policy, expanding its skill space. The framework was evaluated in the Crafter environment, where it demonstrated significant improvements in sample efficiency and generalization to novel goals compared to baseline approaches.

## Method Summary
HERAKLES operates through a two-level architecture where a high-level policy (LLM) decomposes goals into subgoals and a low-level policy executes these subgoals using primitive actions. The LLM uses few-shot prompting to decompose goals and select subgoals, while the low-level policy is trained via imitation learning on successful trajectories. A novel skill compilation mechanism allows the system to progressively expand the low-level policy's capabilities by incorporating mastered skills. The framework uses a sliding window approach to maintain efficient subgoal selection and supports open-ended learning where new goals can be introduced without retraining from scratch.

## Key Results
- Scales linearly with goal difficulty in Crafter environment, outperforming POAD and FUN baselines
- Achieves significant sample efficiency improvements, reaching 90% success rate in fewer than 10 episodes
- Demonstrates robust generalization to novel compositional and synonym-based goals not seen during training
- Successfully learns complex skill sequences including multi-step crafting and tool usage

## Why This Works (Mechanism)
HERAKLES leverages hierarchical decomposition to break complex goals into manageable subgoals while maintaining a unified policy structure. The skill compilation mechanism allows the agent to build upon previously learned behaviors rather than starting from scratch for each new goal. The LLM provides flexible goal understanding and decomposition capabilities, while the low-level policy ensures efficient execution of learned skills. This combination enables both generalization to new goals and efficient learning through skill transfer.

## Foundational Learning
- Hierarchical RL: Why needed - to handle complex goals through decomposition; Quick check - verify subgoal decomposition improves success rates
- Imitation Learning: Why needed - to train low-level policy from expert demonstrations; Quick check - measure performance difference between IL and RL training
- Skill Compilation: Why needed - to expand low-level policy capabilities efficiently; Quick check - track skill space growth over training
- Goal Decomposition: Why needed - to break down complex objectives into manageable tasks; Quick check - validate subgoal sequence effectiveness
- Few-shot Learning: Why needed - to enable LLM to handle new goal types with minimal examples; Quick check - test generalization to unseen goal patterns

## Architecture Onboarding
**Component Map**: LLM -> Goal Decomposition -> Subgoal Selection -> Low-level Policy -> Environment Interaction -> Skill Compilation

**Critical Path**: LLM receives goal → decomposes into subgoals → selects next subgoal → low-level policy executes → environment responds → successful trajectories compiled into low-level policy

**Design Tradeoffs**: Uses LLM for flexibility vs computational cost; prioritizes low-level policy simplicity vs expressivity; balances exploration of new goals vs exploitation of learned skills

**Failure Signatures**: LLM produces invalid subgoals → low-level policy cannot execute → skill compilation fails to improve performance → goal completion rate plateaus

**3 First Experiments**:
1. Test LLM's ability to decompose simple vs complex goals in Crafter
2. Validate skill compilation effectiveness by comparing pre/post compilation performance
3. Measure subgoal selection accuracy using sliding window approach

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluated only in 2D grid-world Crafter environment, may not scale to real-world complexity
- Relies on GPT-4 for goal decomposition, introducing computational overhead and potential brittleness
- Limited evaluation of multi-agent or temporally extended task scenarios
- Compilation process scalability untested with larger state-action spaces

## Confidence
- High confidence: Linear scaling with goal complexity, baseline performance comparisons in Crafter
- Medium confidence: Generalization to novel compositional and synonym-based goals
- Medium confidence: Sample efficiency improvements versus specific baselines
- Low confidence: Applicability to real-world robotics or multi-agent scenarios

## Next Checks
1. Test HERAKLES on procedurally generated environments with larger state and action spaces to evaluate scalability limits
2. Conduct ablation studies on the LLM component to quantify its contribution versus learned low-level policies alone
3. Evaluate the framework's performance on temporally extended, multi-agent tasks where subgoal dependencies and coordination become critical factors