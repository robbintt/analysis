---
ver: rpa2
title: Pretrained Image-Text Models are Secretly Video Captioners
arxiv_id: '2502.13363'
source_url: https://arxiv.org/abs/2502.13363
tags:
- video
- captioning
- training
- cider
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study repurposes the image-based BLIP-2 model for video captioning
  by post-training it on only 6,000 video-text pairs, using simple frame concatenation
  and reinforcement learning. This approach ranks 2nd on MSR-VTT and MSVD, and 3rd
  on VATEX, outperforming several specialized video captioning models.
---

# Pretrained Image-Text Models are Secretly Video Captioners

## Quick Facts
- arXiv ID: 2502.13363
- Source URL: https://arxiv.org/abs/2502.13363
- Reference count: 40
- Ranks 2nd on MSR-VTT and MSVD, 3rd on VATEX video captioning benchmarks

## Executive Summary
This paper demonstrates that pretrained image-text models can be effectively repurposed for video captioning with minimal video-specific training. By post-training the BLIP-2 model on just 6,000 video-text pairs using simple frame concatenation and reinforcement learning, the authors achieve state-of-the-art results on major video captioning benchmarks. The approach shows that large-scale image-text pretraining provides sufficient visual-linguistic grounding for video understanding, eliminating the need for expensive video-specific pretraining.

## Method Summary
The method adapts BLIP-2's frozen vision transformer and trainable Q-Former connector to process 8 sampled frames from each video by concatenating their visual tokens before feeding them to the language model. The approach uses cross-entropy loss for initial training followed by Self-Critical Sequence Training (SCST) with CIDEr optimization. The framework requires only 6,513 training pairs and modest computational resources, yet outperforms specialized video captioning models on standard benchmarks.

## Key Results
- Achieves 73.6 CIDEr score on MSR-VTT test set
- Ranks 2nd on MSR-VTT and MSVD, 3rd on VATEX benchmarks
- SCST improves CIDEr by 3.4-6.5% over cross-entropy training
- Freezing ViT prevents overfitting while training only Q-Former and LLM
- Frame concatenation outperforms frame averaging for temporal information

## Why This Works (Mechanism)

### Mechanism 1: Frame Concatenation Preserves Temporal Granularity
Concatenating 2048 visual tokens from 8 frames preserves frame-specific details, allowing the Q-Former to learn temporal patterns rather than collapsing information through averaging.

### Mechanism 2: Image-Text Pretraining Grounds Visual-Linguistic Mapping
129M image-text pairs establish robust visual concept grounding that transfers to video frames, enabling the model to recognize objects and actions frame-by-frame without video-native pretraining.

### Mechanism 3: SCST Aligns Generation with Human Preference Metrics
SCST optimizes CIDEr through policy gradients, directly aligning caption generation with human-preferred phrasings rather than token-level cross-entropy loss.

## Foundational Learning

- **Q-Former as modality bridge**: Why needed: Essential for understanding how variable-length visual tokens compress into fixed-size prompts. Quick check: 8 frames × 256 tokens each = 2048 input tokens, Q-Former outputs 32 tokens to LLM.

- **Transfer learning vs. training from scratch**: Why needed: Central to understanding why image pretraining can substitute for video training. Quick check: Objects and scenes transfer; temporal causality may not.

- **Reinforcement learning from language feedback**: Why needed: SCST differs from standard supervised learning. Quick check: Cross-entropy optimizes token probability; CIDEr measures n-gram overlap—different objectives.

## Architecture Onboarding

- **Component map**: Video → Frame Sampler → ViT (frozen) → 8×256 tokens → Concatenation → Q-Former (trainable) → 32 tokens → LLM (trainable) → Caption

- **Critical path**: 1) Freeze ViT from BLIP-2, 2) Initialize Q-Former from BLIP-2, 3) Load Flan-T5-XL-3B, 4) Implement frame concatenation preprocessing, 5) Train with cross-entropy then SCST

- **Design tradeoffs**: Concatenation preserves temporal detail vs. averaging speed; 224×224 resolution balances efficiency vs. 364×364 stability; mid-sized LLM balances trainability vs. Vicuna-7B overfitting.

- **Failure signatures**: Training all components causes rapid overfitting (68.4 vs. 73.6 CIDEr); 4M pretraining pairs show slower convergence (65.7 vs. 71.3 CIDEr); frame averaging causes training instability after epoch 5.

- **First 3 experiments**: 1) Baseline replication: Q-Former only training on MSR-VTT targeting CIDEr ~73-74, 2) Ablation: Compare Q-Former only vs. Q-Former+LLM vs. all trainable components, 3) SCST fine-tuning: Apply after cross-entropy convergence expecting 3-6% CIDEr improvement.

## Open Questions the Paper Calls Out
The paper proposes but does not evaluate using this approach for large-scale pseudolabeling of video-text datasets, analogous to how BLIP-1 enabled the LAION dataset for image-text research.

## Limitations
- Frame concatenation without explicit temporal modeling may not capture complex temporal dynamics
- Uniform frame sampling may miss critical events between sampled frames
- CIDEr optimization may not correlate with genuine human judgment of caption quality

## Confidence

*High Confidence*: Freezing ViT while training only Q-Former and LLM prevents overfitting and improves performance.

*Medium Confidence*: Image-text pretraining transfers effectively to video captioning, though video-native pretraining could perform better.

*Low Confidence*: SCST improves caption quality for human preferences, as CIDEr may not capture temporal accuracy or factual correctness.

## Next Checks

1. **Temporal Reasoning Test**: Evaluate on videos requiring temporal causality understanding and compare against baselines using explicit temporal modeling.

2. **Frame Sampling Sensitivity**: Systematically vary frame count and sampling strategy to determine optimal temporal resolution and Q-Former compression effectiveness.

3. **Human Preference Validation**: Conduct human evaluations comparing SCST-optimized vs. cross-entropy captions on temporal accuracy, factual correctness, and coherence.