---
ver: rpa2
title: 'ResNetVLLM-2: Addressing ResNetVLLM''s Multi-Modal Hallucinations'
arxiv_id: '2504.14429'
source_url: https://arxiv.org/abs/2504.14429
tags:
- hallucination
- video
- resnetvllm
- generated
- captions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ResNetVLLM-2, an enhanced video-language
  model that addresses hallucination issues in ResNetVLLM. The approach combines a
  faithfulness detection strategy using a modified Lynx model to assess semantic alignment
  between generated captions and ground-truth references, along with a hallucination
  mitigation strategy employing Retrieval-Augmented Generation (RAG) with a dynamically
  constructed ad-hoc knowledge base.
---

# ResNetVLLM-2: Addressing ResNetVLLM's Multi-Modal Hallucinations

## Quick Facts
- arXiv ID: 2504.14429
- Source URL: https://arxiv.org/abs/2504.14429
- Authors: Ahmad Khalil; Mahmoud Khalil; Alioune Ngom
- Reference count: 29
- Primary result: 10.5% accuracy improvement on ActivityNet-QA benchmark

## Executive Summary
This paper addresses the critical issue of multi-modal hallucinations in video-language models, specifically focusing on the ResNetVLLM architecture. The authors introduce ResNetVLLM-2, a two-stage approach that first detects semantic misalignments between generated captions and ground-truth references using a modified Lynx model, then mitigates hallucinations through Retrieval-Augmented Generation (RAG) with dynamically constructed knowledge bases. The system demonstrates substantial performance gains on the ActivityNet-QA benchmark, improving accuracy from 54.8% to 65.3%.

## Method Summary
The ResNetVLLM-2 approach combines faithfulness detection and hallucination mitigation in a two-stage pipeline. The faithfulness detection stage employs a modified Lynx model to assess semantic alignment between generated captions and ground-truth references, identifying potential hallucinations. The mitigation stage utilizes Retrieval-Augmented Generation (RAG) with ad-hoc knowledge bases constructed specifically for each video input. This knowledge base is dynamically built from relevant video content and contextual information, enabling the model to ground its responses in factual evidence rather than generating spurious details.

## Key Results
- Accuracy improved from 54.8% to 65.3% on ActivityNet-QA benchmark
- Substantial reduction in multi-modal hallucinations
- Demonstrated effectiveness of the two-stage detection and mitigation approach

## Why This Works (Mechanism)
The two-stage approach works by first identifying when generated captions deviate from ground-truth references, then providing the model with relevant factual information to ground its responses. The modified Lynx model serves as an effective faithfulness detector by comparing semantic similarity between generated and reference captions. When misalignments are detected, the RAG-based mitigation stage retrieves relevant information from the dynamically constructed knowledge base, constraining the model's generation to factual content rather than hallucinated details.

## Foundational Learning
**Semantic Alignment**: Understanding how generated captions correspond to ground-truth references is crucial for detecting hallucinations. Quick check: Measure semantic similarity scores between generated and reference captions using multiple metrics (BLEU, ROUGE, BERTScore).

**Retrieval-Augmented Generation**: RAG combines information retrieval with language model generation, providing contextual grounding. Quick check: Evaluate knowledge base quality by measuring retrieval precision and recall for relevant video content.

**Multi-modal Fusion**: Integrating visual and textual information effectively prevents modality-specific hallucinations. Quick check: Test model performance with varying fusion strategies (early, late, and hybrid fusion approaches).

## Architecture Onboarding

**Component Map**: Video Encoder -> Multi-modal Fusion -> Generation Head -> Lynx Detector -> Knowledge Base Retriever -> RAG Generator

**Critical Path**: The critical execution path follows: video input → multi-modal encoding → initial caption generation → faithfulness detection → knowledge base construction/retrieval → hallucination-mitigated generation.

**Design Tradeoffs**: The two-stage approach adds computational overhead but significantly improves faithfulness. Alternative designs could include end-to-end hallucination detection during generation or simpler single-stage mitigation strategies.

**Failure Signatures**: System failures manifest as: (1) false positive detection when semantically similar but factually different captions are flagged, (2) knowledge base retrieval failures leading to unchanged hallucinated outputs, and (3) over-reliance on retrieved information causing loss of generalization.

**First Experiments**: 
1. Evaluate faithfulness detection accuracy across different semantic similarity thresholds
2. Compare knowledge base construction methods (video content vs. external sources)
3. Measure computational overhead of the two-stage approach versus baseline ResNetVLLM

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Single benchmark validation on ActivityNet-QA limits generalizability claims
- Modified Lynx model specifics and robustness across domains remain unclear
- Ad-hoc knowledge base construction raises scalability concerns for diverse video content
- Computational overhead of the two-stage approach not discussed

## Confidence

**High confidence**: 10.5% absolute accuracy improvement on ActivityNet-QA is well-documented and statistically significant

**Medium confidence**: Faithfulness detection mechanism appears sound but lacks ablation studies showing the impact of the modified Lynx model versus alternatives

**Medium confidence**: RAG-based mitigation strategy is conceptually valid, but effectiveness may vary with knowledge base quality and domain specificity

## Next Checks
1. Test the complete ResNetVLLM-2 pipeline on additional video-language benchmarks (e.g., MSR-VTT, VATEX) to verify generalizability beyond ActivityNet-QA

2. Conduct ablation studies isolating the contributions of the faithfulness detection module versus the RAG-based mitigation to quantify their individual impacts

3. Evaluate the computational efficiency and inference time overhead introduced by the two-stage hallucination detection and mitigation pipeline compared to baseline ResNetVLLM