---
ver: rpa2
title: Data-regularized Reinforcement Learning for Diffusion Models at Scale
arxiv_id: '2512.04332'
source_url: https://arxiv.org/abs/2512.04332
tags:
- reward
- ddrl
- diffusion
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DDRL introduces data-regularized diffusion reinforcement learning
  to address reward hacking in diffusion model alignment. The method anchors policy
  training to an off-policy data distribution using forward KL divergence, enabling
  robust regularization that prevents out-of-distribution generations.
---

# Data-regularized Reinforcement Learning for Diffusion Models at Scale
## Quick Facts
- arXiv ID: 2512.04332
- Source URL: https://arxiv.org/abs/2512.04332
- Reference count: 40
- Primary result: DDRL anchors diffusion RL to off-policy data distribution, preventing reward hacking while improving human preference scores

## Executive Summary
DDRL introduces data-regularized diffusion reinforcement learning to address reward hacking in diffusion model alignment. The method anchors policy training to an off-policy data distribution using forward KL divergence, enabling robust regularization that prevents out-of-distribution generations. By integrating diffusion loss minimization with reward maximization, DDRL achieves higher human preference scores across multiple video and image generation tasks while avoiding the over-stylization and quality degradation seen in baseline methods. Large-scale experiments with over a million GPU hours and ten thousand human evaluations demonstrate DDRL's superior alignment with human preferences while maintaining reward improvements.

## Method Summary
DDRL addresses reward hacking in diffusion model alignment by anchoring policy training to an off-policy data distribution through forward KL divergence regularization. The method combines diffusion loss minimization with reward maximization, creating a framework that prevents out-of-distribution generations while maintaining alignment with human preferences. The approach enables efficient integration of supervised fine-tuning and reinforcement learning, providing a scalable solution for diffusion model post-training. The regularization mechanism ensures the policy remains close to the original data distribution, preventing the model from exploiting reward functions in ways that produce undesirable outputs.

## Key Results
- DDRL achieves higher human preference scores across multiple video and image generation tasks compared to baseline methods
- The framework prevents reward hacking by anchoring policy training to an off-policy data distribution using forward KL divergence
- Large-scale experiments with over a million GPU hours and ten thousand human evaluations demonstrate superior alignment with human preferences

## Why This Works (Mechanism)
The method works by introducing a forward KL divergence regularization term that anchors the diffusion policy to the original data distribution during reinforcement learning. This prevents the policy from drifting too far from the data manifold, which is the root cause of reward hacking in diffusion models. By minimizing both the diffusion loss and the forward KL divergence simultaneously, DDRL ensures that policy updates improve reward scores without sacrificing data fidelity. The integration of supervised fine-tuning with reinforcement learning creates a stable training loop where the model can learn from both human preferences and the underlying data distribution, resulting in more reliable and controllable generation outputs.

## Foundational Learning
- **Diffusion Models**: Stochastic generative models that reverse a noising process to generate data; needed because they form the base architecture being aligned through RL
- **Reinforcement Learning for Generative Models**: Framework where policies generate samples to maximize reward signals; required to understand how alignment is achieved through reward optimization
- **Forward KL Divergence Regularization**: Statistical measure that constrains policy distribution to stay close to data distribution; essential for preventing out-of-distribution generations
- **Reward Hacking**: Phenomenon where models exploit reward functions to produce unintended outputs; critical context for understanding the problem DDRL solves
- **Off-policy Distribution Anchoring**: Technique that keeps training close to original data distribution; fundamental to DDRL's approach for preventing reward hacking

## Architecture Onboarding
- **Component Map**: Data Distribution -> Forward KL Regularization -> Policy Network -> Reward Signal -> Diffusion Loss -> Policy Update
- **Critical Path**: Policy network training loop that alternates between diffusion loss minimization and reward maximization while maintaining KL regularization
- **Design Tradeoffs**: Balance between reward improvement and data fidelity, computational cost versus alignment quality, and scalability versus implementation complexity
- **Failure Signatures**: Reward degradation without data regularization, over-stylization from pure reward maximization, and training instability without proper KL anchoring
- **First Experiments**: 1) Baseline diffusion model training without RL, 2) DDRL with simplified reward functions, 3) Ablation study removing KL regularization component

## Open Questions the Paper Calls Out
None

## Limitations
- Potential overfitting to specific human preference datasets used in evaluation
- Massive computational requirements (over a million GPU hours) limiting accessibility
- Lack of ablation studies examining relative contributions of individual components

## Confidence
- Claim cluster "DDRL prevents reward hacking effectively": High
- Claim cluster "DDRL maintains quality while improving human preferences": Medium
- Claim cluster "DDRL enables efficient integration of SFT and RL": Low

## Next Checks
1. Conduct cross-dataset validation using independently collected human preference data to verify generalization beyond the original evaluation set
2. Perform extensive ablation studies isolating the impact of the forward KL regularization term versus other components of the DDRL framework
3. Test the method's robustness across significantly different reward functions and generation tasks outside the current scope of video and image synthesis