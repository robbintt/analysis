---
ver: rpa2
title: 'Khana: A Comprehensive Indian Cuisine Dataset'
arxiv_id: '2509.06006'
source_url: https://arxiv.org/abs/2509.06006
tags:
- food
- dataset
- image
- classification
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Khana is a new benchmark dataset for Indian food image classification,
  segmentation, and retrieval, addressing the lack of comprehensive labeled datasets
  for Indian cuisine. It includes 131K images across 80 categories, each at 500x500
  resolution, organized through a taxonomy-based hierarchy.
---

# Khana: A Comprehensive Indian Cuisine Dataset

## Quick Facts
- **arXiv ID**: 2509.06006
- **Source URL**: https://arxiv.org/abs/2509.06006
- **Reference count**: 33
- **Key outcome**: 131K images across 80 Indian food categories with 500x500 resolution, achieving 86.72% top-1 accuracy using ConvNeXT-S

## Executive Summary
Khana is a new benchmark dataset for Indian food image classification, segmentation, and retrieval, addressing the lack of comprehensive labeled datasets for Indian cuisine. It includes 131K images across 80 categories, each at 500x500 resolution, organized through a taxonomy-based hierarchy. The dataset was created by web scraping and manual curation to ensure label accuracy and diversity. State-of-the-art models including ResNet-152, EfficientNet-V2-S, ViT-B-16, and ConvNeXT-S were evaluated for classification, achieving top-1 accuracies ranging from 80.47% to 86.72% and top-5 accuracies from 95.52% to 97.58%, with ConvNeXT-S performing best. Khana aims to support research and real-world applications in Indian food recognition.

## Method Summary
The dataset was created through web scraping followed by manual curation to ensure label accuracy and diversity. Images were organized into 80 categories using a taxonomy-based hierarchy. For classification evaluation, four state-of-the-art models (ResNet-152, EfficientNet-V2-S, ViT-B-16, ConvNeXT-S) were fine-tuned using ImageNet pre-trained weights. Models were trained with Adam optimizer (lr=0.001), cross-entropy loss, batch size 64, and 50 epochs. Early layers were frozen while only final classification layers were trained. Images were resized and center-cropped according to model-specific requirements (224x224 for ResNet/ViT/ConvNeXT, 384x384 for EfficientNet) and normalized using ImageNet statistics.

## Key Results
- ConvNeXT-S achieved highest top-1 accuracy (86.72%) and top-5 accuracy (97.58%) among all evaluated models
- ResNet-152 achieved 81.00% top-1 accuracy with lowest computational efficiency (79.2M parameters)
- EfficientNet-V2-S achieved 80.47% top-1 accuracy with highest computational efficiency (20.2M parameters)
- ViT-B-16 achieved 85.34% top-1 accuracy with 85.8M parameters, showing faster convergence than EfficientNet

## Why This Works (Mechanism)

### Mechanism 1
Transfer learning from ImageNet provides effective feature initialization for Indian food classification, enabling models to achieve >80% top-1 accuracy with limited fine-tuning. Pre-trained weights capture hierarchical visual features (edges → textures → object parts) in early layers that transfer across domains. By freezing early layers and fine-tuning only final classification layers on Khana, models adapt generic features to Indian cuisine specifics while preserving learned representations.

### Mechanism 2
Taxonomy-based hierarchical organization (category → dish → variety) enables structured label relationships that can improve semantic understanding and retrieval. The taxonomy groups dishes by culinary properties (regional origin, cooking method, dietary type), creating explicit semantic relationships. This structure allows models to learn shared features within categories while distinguishing between varieties.

### Mechanism 3
ConvNeXT-S's architectural design (depthwise separable convolutions without self-attention) provides better accuracy-efficiency trade-offs for fine-grained food classification compared to pure CNNs or transformers. ConvNeXT combines convolutional locality with transformer-inspired design choices (layer normalization, GELU activation, larger kernels). This hybrid approach may capture local texture patterns (critical for food) while maintaining global context.

## Foundational Learning

- **Concept: Fine-grained image classification**
  - **Why needed here:** Indian cuisine exhibits high intra-class variation and inter-class similarity. Understanding fine-grained classification challenges explains why standard models struggle and why 80-87% accuracy is non-trivial.
  - **Quick check question:** Can you explain why "masala dosa" and "rava dosa" might be harder to distinguish than "pizza" and "burger"?

- **Concept: Class imbalance in datasets**
  - **Why needed here:** Khana has imbalanced class distribution (e.g., masala dosa has ~7,884 images, chikki has ~125 images). This affects model training and evaluation, potentially biasing predictions toward overrepresented classes.
  - **Quick check question:** If you trained a model on Khana without addressing imbalance, which dishes would it likely over-predict and why?

- **Concept: Transfer learning and domain shift**
  - **Why needed here:** All baseline models use ImageNet pre-trained weights. Understanding transfer learning helps assess whether ImageNet features are appropriate for Indian food and when fine-tuning strategies might fail.
  - **Quick check question:** What visual features from ImageNet (trained on animals, vehicles, etc.) might transfer poorly to recognizing Indian curries?

## Architecture Onboarding

- **Component map:** Khana Dataset (131K images, 80 classes, 500x500px) → Data Pipeline (70/15/15 split, resize to model-specific crop size, ImageNet normalization) → Model Backbone (ResNet-152 / EfficientNet-V2-S / ViT-B-16 / ConvNeXT-S with ImageNet weights) → Classification Head (fine-tuned final layer, 80 output classes) → Training (Adam optimizer, lr=0.001, cross-entropy loss, 50 epochs, batch size 64) → Evaluation (Top-1 and Top-5 accuracy on test set)

- **Critical path:** Load and validate dataset splits (ensure 70/15/15 distribution) → Apply model-specific preprocessing (crop sizes: ResNet 224, EfficientNet 384, ViT 224, ConvNeXT 224) → Initialize model with ImageNet pre-trained weights, freeze early layers → Train only final classification layer for 50 epochs → Evaluate on held-out test set using top-1 and top-5 accuracy

- **Design tradeoffs:** Model size vs. accuracy (ViT-B-16 85.8M params vs EfficientNet-V2-S 20.2M params — EfficientNet is 4× smaller but 1.3% lower accuracy); Training stability vs. speed (ViT converged faster, EfficientNet showed slower and less stable convergence); Data augmentation (paper used NO augmentation — trading robustness for dataset originality)

- **Failure signatures:** High training loss, low validation accuracy (indicates overfitting to training set); Confusion between visually similar dishes (Misal pav ↔ pav bhaji, vada pav ↔ dabeli); Poor performance on underrepresented classes (Chikki 125 images vs masala dosa 7,884 images)

- **First 3 experiments:** Baseline reproduction (Train ConvNeXT-S with exact paper settings to verify 86.72% top-1 accuracy); Class imbalance analysis (Compute per-class accuracy breakdown, identify which underrepresented classes have lowest accuracy); Data augmentation ablation (Add standard augmentations and measure impact on top-1 accuracy and per-class performance variance)

## Open Questions the Paper Calls Out

**Multi-modal LLM Comparison**: Given the rapid growth in usage of multi-modal LLMs, researchers can explore querying over images and conducting qualitative comparisons of embeddings for several multi-modal LLMs as well.

**Data Augmentation for Imbalance**: Adding more images for underrepresented categories is crucial for improving the dataset's real-world accuracy and utility.

**Annotation Improvements**: Improving annotations is listed as a crucial step for future work.

## Limitations

- Dataset relies on web scraping, introducing potential biases in image quality, lighting conditions, and presentation styles despite manual curation
- Severe class imbalance (7,884 vs 125 images per class) not adequately addressed in model training or evaluation
- Limited evaluation scope - only classification results provided despite dataset supporting segmentation and retrieval tasks

## Confidence

**High Confidence**: Dataset creation methodology (web scraping + manual curation), taxonomy structure, basic classification results with specified models and metrics.

**Medium Confidence**: ConvNeXT-S architecture advantages, taxonomy utility for semantic understanding, transfer learning effectiveness (based on reasonable assumptions but limited direct evidence).

**Low Confidence**: Claims about ConvNeXT-S being superior without statistical validation, assumptions about ImageNet feature transferability to Indian food without empirical verification, impact of class imbalance on final results.

## Next Checks

1. **Statistical Significance Analysis**: Run each model configuration 5 times with different random seeds and compute 95% confidence intervals for top-1 accuracy. Determine if ConvNeXT-S's 1.4% advantage over ViT-B-16 is statistically significant.

2. **Class Imbalance Impact Study**: Compute per-class accuracy distribution and create confusion matrices for the most visually similar dish pairs. Quantify model bias toward overrepresented classes and test weighted cross-entropy training.

3. **Transfer Learning Ablation**: Train ConvNeXT-S from scratch on Khana (random initialization) and compare performance to ImageNet-pretrained version. Analyze which layers benefit most from fine-tuning and whether early layer freezing is optimal.