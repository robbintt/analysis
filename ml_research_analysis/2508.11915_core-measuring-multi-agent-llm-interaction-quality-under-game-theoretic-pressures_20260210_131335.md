---
ver: rpa2
title: 'CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures'
arxiv_id: '2508.11915'
source_url: https://arxiv.org/abs/2508.11915
tags:
- agent
- core
- cooperative
- neutral
- competitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CORE, a metric for evaluating conversational
  robustness in multi-agent LLM interactions. CORE integrates measures of cluster
  entropy, lexical repetition, and semantic stagnation to assess linguistic diversity
  across cooperative, competitive, and neutral game-theoretic settings.
---

# CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures

## Quick Facts
- **arXiv ID**: 2508.11915
- **Source URL**: https://arxiv.org/abs/2508.11915
- **Reference count**: 18
- **Primary result**: CORE metric effectively captures linguistic diversity differences across cooperative, competitive, and neutral multi-agent LLM interactions, revealing how game-theoretic pressures shape vocabulary growth and repetition patterns.

## Executive Summary
This paper introduces CORE, a metric for evaluating conversational robustness in multi-agent LLM interactions. CORE integrates measures of cluster entropy, lexical repetition, and semantic stagnation to assess linguistic diversity across cooperative, competitive, and neutral game-theoretic settings. The authors apply CORE to dialogs generated by eight open-source LLM pairs, grounding the analysis with Zipf's and Heaps' laws to characterize word frequency and vocabulary growth. Results show cooperative settings exhibit higher lexical repetition alongside greater vocabulary expansion, while competitive settings show more constrained vocabularies and lower diversity. CORE effectively captures these differences and reveals how social incentives influence language adaptation in multi-agent LLM systems.

## Method Summary
The CORE metric evaluates multi-agent LLM dialog quality by combining three normalized components: mode entropy (distribution of dialog clusters), repetition penalty (based on repeated n-grams), and semantic stagnation penalty (cosine similarity of consecutive utterance embeddings). The method generates 5,760 dialogs across 8 open-source LLMs in 64 model pairs × 3 game-theoretic conditions (cooperative, competitive, neutral), with 10-turn dialogs and 128-token limits per utterance. CORE is calculated using Zipf's α and Heaps' β exponents as hyperparameters, normalized by fitting these distributions on each condition's corpus. The metric is validated through behavioral analysis including toxicity, sentiment, and agreement/disagreement rates.

## Key Results
- Cooperative settings show steeper Zipf distributions and higher Heap exponents, indicating both high lexical repetition and vocabulary expansion
- Competitive interactions display lower Zipf and Heaps exponents with more constrained vocabularies
- Instruction-tuned models achieve higher CORE values in self-play compared to base models, suggesting alignment objectives promote richer language use
- CORE effectively distinguishes quality differences across game-theoretic conditions while traditional metrics may miss subtle mode collapse behaviors

## Why This Works (Mechanism)

### Mechanism 1: Multi-Factor Diversity Scoring
- Claim: CORE effectively quantifies conversational quality by integrating mode diversity, lexical repetition, and semantic novelty into a single scalar.
- Mechanism: CORE combines normalized cluster entropy, repetition penalty (n-grams raised to power α), and semantic stagnation penalty (cosine similarity of consecutive embeddings raised to power β).
- Core assumption: Assumes "quality" correlates with high topic diversity and low lexical/semantic redundancy.
- Evidence anchors: Abstract confirms CORE integrates these three measures; section 4.1 provides the formula.
- Break condition: If α and β are not tuned to the specific linguistic domain, penalties may misjudge valid repetition as stagnation.

### Mechanism 2: Incentive-Driven Vocabulary Shift
- Claim: Game-theoretic pressures causally shift statistical properties of LLM language, specifically vocabulary size and frequency distribution.
- Mechanism: Cooperative incentives encourage shared vocabulary alignment, leading to steeper Zipf curves (high α, high repetition) but larger vocabularies (high Heaps β). Competitive incentives restrict vocabulary and reduce repetition.
- Core assumption: Assumes LLMs simulate social incentives deeply enough to alter statistical output signatures.
- Evidence anchors: Abstract and section 5.3 show distinct clusters of α and β values across conditions.
- Break condition: If prompt engineering fails to elicit true competitive/cooperative intent, statistical shift will not manifest.

### Mechanism 3: Alignment Stability in Self-Play
- Claim: Instruction-tuned models maintain higher interaction quality in self-play compared to base models or cross-model play.
- Mechanism: Instruction-tuned models share aligned priors and communication protocols, resulting in smoother semantic transitions and less mode collapse when interacting with identical partners.
- Core assumption: Assumes high CORE scores in self-play represent genuine semantic richness rather than hallucination or repetitive alignment artifacts.
- Evidence anchors: Section 5.4 shows instruction-tuned models achieve higher CORE values in self-play; table 3 confirms higher unique token counts.
- Break condition: If the model overfits to its own generation style, it may create an "echo chamber" that scores high on diversity metrics but fails to communicate effectively with other models.

## Foundational Learning

- **Concept**: Zipf's Law & Heaps' Law
  - Why needed here: These empirical laws form the theoretical backbone of the CORE metric, essential for interpreting repetition and vocabulary expansion penalties.
  - Quick check question: If a dialog has a Zipf exponent (α) of 2.5 vs. 1.5, which one has a flatter frequency distribution (more diverse vocabulary)? Answer: α=1.5 has flatter distribution.

- **Concept**: Mode Collapse
  - Why needed here: CORE is primarily a diagnostic for mode collapse—the phenomenon where generative models fall into repetitive states.
  - Quick check question: Does a high Cluster Entropy (H(C)) indicate high or low susceptibility to mode collapse? Answer: Low susceptibility.

- **Concept**: Cosine Similarity in Embedding Space
  - Why needed here: The "Semantic Stagnation" component relies entirely on measuring cosine distance between consecutive utterance embeddings.
  - Quick check question: If two consecutive utterances have a cosine similarity of 0.98, does the CORE score increase or decrease? Answer: Decrease.

## Architecture Onboarding

- **Component map**: LLM Agents + Game Condition Prompts -> Dialog Generation -> Tokenization -> N-gram Extraction -> Embedding Generation -> CORE Calculation (Entropy + Repetition + Stagnation)
- **Critical path**: 
  1. Prompt Design: Seed prompts are the single point of failure for inducing game-theoretic conditions
  2. Tokenization: Uses specific regex `\b\w+\b`; changing this breaks comparability
  3. Hyperparameters: Must fit Zipf α and Heaps β on specific dataset before computing final CORE score
- **Design tradeoffs**: 
  - Dialog Length: Limited to 10 turns enables massive scale (5,760 interactions) but may miss long-horizon mode collapse
  - Metric Complexity: CORE is computationally heavier than simple distinct-n metrics due to clustering and embedding requirements
- **Failure signatures**:
  - Looping: Agents repeat exact same phrase → Repetition Penalty drives CORE to near 0
  - Static Semantics: Agents agree constantly without new info → High Cosine Similarity lowers CORE
  - Toxicity Spiral: In competitive modes, toxicity rises which may artificially inflate "diversity"
- **First 3 experiments**:
  1. Baseline Validation: Reproduce Self-Play vs. Cross-Play comparison for single model family to verify alignment improves self-play CORE scores
  2. Temperature Sensitivity: Run sweep on temperature (0.0 to 1.2) to determine if CORE measures true behavioral shifts or sampling noise
  3. Adversarial Stress Test: Introduce "malicious" prompt in cooperative setting to see if CORE detects resulting semantic stagnation

## Open Questions the Paper Calls Out

- **Open Question 1**: Does CORE generalize to multi-agent collectives with more than two agents, and how do linguistic diversity patterns scale with group size?
  - Basis: Explicit limitation to dyadic interactions in paper
  - Why unresolved: All experiments used pairwise dialogs; coordination dynamics may differ with additional agents
  - Evidence needed: Apply CORE to 3+ agent interactions and compare diversity patterns to dyadic baselines

- **Open Question 2**: How does CORE evolve over extended interactions beyond 10 turns, and does mode collapse accelerate or stabilize in longer dialogs?
  - Basis: Inferred from methodology limiting dialogs to 10 turns
  - Why unresolved: Short-term analysis cannot capture whether repetition and semantic stagnation compound over time
  - Evidence needed: Longitudinal experiments running 50-100+ turn dialogs with CORE tracking across turn indices

- **Open Question 3**: Does higher CORE correlate with improved task outcomes in goal-oriented multi-agent systems?
  - Basis: Inferred from stated objective to evaluate "effectiveness of language use" without external task rewards
  - Why unresolved: CORE measures linguistic diversity but wasn't validated against task success metrics
  - Evidence needed: Correlate CORE scores with objective performance measures on specific cooperative/competitive tasks

## Limitations

- Limited to dyadic interactions, missing complexity of emergent communication in larger or longer-term agent collectives
- Short-term analysis (10 turns) may not capture long-horizon mode collapse or equilibrium states
- CORE measures linguistic diversity but has not been validated against external task success metrics

## Confidence

- **Metric Validity**: High - CORE formula is clearly specified and grounded in established linguistic laws
- **Experimental Design**: Medium - Well-structured but limited by short dialog length and pairwise interactions only
- **Generalizability**: Low - Findings restricted to synthetic LLM-LLM interactions without human validation

## Next Checks

1. Verify clustering method and K value for mode entropy calculation to ensure proper normalization
2. Test CORE sensitivity to n-gram size selection (unigrams vs. bigrams) for repetition penalty
3. Validate Sentence-BERT model variant selection for consistent embedding quality across conditions