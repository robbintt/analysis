---
ver: rpa2
title: Intrinsic training dynamics of deep neural networks
arxiv_id: '2508.07370'
source_url: https://arxiv.org/abs/2508.07370
tags:
- intrinsic
- property
- networks
- theorem
- diag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper studies when gradient flows on high-dimensional parameters\
  \ \u03B8 can be reduced to intrinsic gradient flows on lower-dimensional variables\
  \ z = \u03D5(\u03B8). It introduces the notion of intrinsic dynamic property, which\
  \ requires that the metric M(\u03B8) = \u2202\u03D5(\u03B8)\u2202\u03D5(\u03B8)\u22A4\
  \ can be expressed as K(z) along the trajectory."
---

# Intrinsic training dynamics of deep neural networks

## Quick Facts
- **arXiv ID:** 2508.07370
- **Source URL:** https://arxiv.org/abs/2508.07370
- **Reference count:** 40
- **Primary result:** Shows when gradient flows on high-dimensional parameters can be reduced to intrinsic flows on lower-dimensional variables, with ReLU networks satisfying this for all initializations while linear networks require relaxed balanced conditions.

## Executive Summary
This paper studies when gradient flows on high-dimensional parameters θ can be reduced to intrinsic gradient flows on lower-dimensional variables z = ϕ(θ). The key insight is that for ReLU networks of any depth, the path-lifting reparametrization satisfies the Frobenius property, ensuring intrinsic recoverability for any initialization. For linear networks, this property holds if and only if the initialization satisfies a relaxed balanced condition (Uᵢ₊₁ᵀUᵢ₊₁ - UᵢUᵢᵀ = λᵢI). The authors characterize the intrinsic dynamics in these cases, showing that relaxed balanced conditions are necessary and sufficient. For infinitely deep linear networks (neural ODEs), they provide a closed-form expression for the intrinsic dynamics under relaxed balanced initializations.

## Method Summary
The paper introduces the intrinsic metric property, requiring that the metric M(θ) = ∂ϕ(θ)∂ϕ(θ)ᵀ can be expressed as K(z) along the trajectory. For ReLU networks, they prove that the path-lifting reparametrization satisfies the Frobenius property, guaranteeing intrinsic recoverability on a dense open set. For linear networks, they identify conservation laws and prove that relaxed balanced initializations are necessary and sufficient for the intrinsic metric property. The intrinsic dynamics are formulated as Riemannian gradient flows on the lower-dimensional space. For the infinite-depth limit (linear neural ODEs), they derive closed-form expressions for the intrinsic metric under relaxed balanced initializations.

## Key Results
- Path-lifting reparametrization for ReLU networks satisfies Frobenius property for any initialization
- Relaxed balanced initialization is necessary and sufficient for intrinsic metric property in linear networks
- Conservation laws exist for linear networks: UᵀU - VVᵀ = constant
- Explicit closed-form ODE for intrinsic dynamics in 3-layer scalar ReLU networks
- Closed-form expression for infinite-depth linear networks under relaxed balanced initialization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** For general deep ReLU networks, gradient flow in high-dimensional parameter space can be reduced to an intrinsic dynamic on a lower-dimensional variable for any initialization.
- **Mechanism:** The paper proves that the "path-lifting" reparametrization (ϕ_{ReLU}) satisfies the Frobenius property. This ensures that the Lie algebra generated by the gradient vector fields is closed, allowing the construction of sufficient "conservation laws" to satisfy the intrinsic recoverability property. This guarantees the dynamics depend only on the initialization and the path-lifted variable z.
- **Core assumption:** The network uses ReLU activations and the reparametrization ϕ corresponds to the path-lifting formalism.
- **Evidence anchors:**
  - Theorem 3.8: Proves ϕ_{ReLU} satisfies the Frobenius property
  - Corollary 3.9: Concludes intrinsic recoverability holds on a dense open set for ReLU networks
  - Corpus: Paper 81370 discusses diagonal linear networks, contrasting with this mechanism which handles non-linear ReLU architectures via path-lifting
- **Break condition:** If the path-lifting ϕ does not satisfy the Frobenius property for the specific architecture, the intrinsic recoverability cannot be guaranteed for arbitrary initializations.

### Mechanism 2
- **Claim:** For deep linear networks, dimensionality reduction to an intrinsic dynamic is possible if and only if the initialization satisfies a relaxed balanced condition (specifically, weight matrices satisfy U_{i+1}ᵀU_{i+1} - UᵢUᵢᵀ = λᵢI).
- **Mechanism:** The paper identifies a necessary condition involving the inclusion of kernels of linear maps (ker∂ϕ ∩ ker∂h ⊆ ker∂M). The authors prove that this condition fails for linear networks unless the "relaxed balanced" initialization is used, thereby constraining the trajectory to a manifold where the metric M(θ) depends only on the product Z_L and initialization constants λᵢ.
- **Core assumption:** The reparametrization is the product of weight matrices ϕ_{Lin}(θ) = U_L ⋯ U_1.
- **Evidence anchors:**
  - Theorem 4.4: Proves necessity of the relaxed balanced condition for the intrinsic metric property in 2-layer linear nets
  - Theorem 4.6: Shows sufficiency for deep linear nets, deriving explicit polynomial metrics
  - Corpus: Paper 2507.06367 (Achour et al.) explores similar Riemannian geometry for linear convolutional networks, supporting the focus on linear architecture constraints
- **Break condition:** If the difference U_{i+1}ᵀU_{i+1} - UᵢUᵢᵀ is not a scalar multiple of the identity (non-balanced), the metric cannot be expressed purely as a function of the end-to-end matrix Z.

### Mechanism 3
- **Claim:** In the infinite-depth limit (Linear Neural ODE), the intrinsic dynamic can be expressed in closed form if a continuous relaxed balanced condition holds.
- **Mechanism:** The discrete conservation laws of deep linear networks converge to continuous conservation laws h_s(θ) = A'_s + A'_sᵀ + [A'_s, A_s]. If this quantity equals a scalar function λ(s)I throughout the "depth" s ∈ [0,1], the complex integral over depth simplifies into an explicit expression involving matrix exponentials and polynomials of the final matrix Z_1.
- **Core assumption:** The network is a linear ODE dZ_s/ds = A_s Z_s, and the loss is a function of the final state Z_1.
- **Evidence anchors:**
  - Proposition 4.7: Defines the continuous conservation laws h_s
  - Theorem 4.8: Derives the explicit closed-form ODE for Z_1 under relaxed balanced initialization
  - Corpus: Paper 2511.16976 discusses gradient descent in deep equilibrium models, which relates to the infinite-depth limit analysis here
- **Break condition:** If the initialization does not preserve the structure h_s(θ(0)) = λ(s)I, the explicit closed-form solution for the metric is lost.

## Foundational Learning

- **Concept:** Conservation Laws in Gradient Flows
  - **Why needed here:** The paper relies on quantities h(θ) that remain constant during training (orthogonal to the loss gradient) to define the sub-manifold M_{θ_0} on which the reduced dynamics evolve. Without these, you cannot isolate the trajectory.
  - **Quick check question:** Can you explain why a function h(θ) is conserved during gradient flow on ℓ(θ) if its gradient is orthogonal to the loss gradient?

- **Concept:** Riemannian Gradient Flow
  - **Why needed here:** The "intrinsic dynamic" is defined as a flow ż = -K(z)∇f(z), which corresponds to a gradient flow with respect to a Riemannian metric K^{-1}(z). Understanding how the metric distorts the update direction is key to interpreting the dynamics.
  - **Quick check question:** How does a Riemannian metric matrix G modify the direction of steepest descent compared to the standard Euclidean gradient?

- **Concept:** Frobenius Property (Integrability)
  - **Why needed here:** This is the technical criterion used to prove Mechanism 1. It determines if a set of vector fields (gradients of ϕ) forms an integrable distribution, guaranteeing the existence of coordinates (conservation laws) to reduce the system's dimensionality.
  - **Quick check question:** What does the Frobenius theorem imply about the existence of a sub-manifold tangent to a given set of vector fields?

## Architecture Onboarding

- **Component map:**
  1. Parametrization ϕ: Maps high-dim θ to low-dim z (e.g., path-lifting for ReLU, matrix product for Linear)
  2. Conservation Laws h: Functions of θ constant along trajectories, defining the manifold M_{θ_0}
  3. Metric M(θ): The object ∂ϕ∂ϕᵀ that needs to be rewritten as K(z)
  4. Initializations: Specific setups (Relaxed Balanced) that force the kernel inclusion condition

- **Critical path:**
  1. Select architecture (ReLU vs Linear)
  2. Identify valid conservation laws h (Prop 2.9)
  3. Check initialization constraints (e.g., Relaxed Balanced for Linear)
  4. Verify the kernel inclusion condition (Theorem 2.14) to ensure the metric M(θ) is intrinsic

- **Design tradeoffs:**
  - Path-Lifting (ϕ_{ReLU}) vs Product (ϕ_{Lin}): Path-lifting guarantees intrinsic dynamics for any initialization but operates in a higher dimension (d ≈ D). The product map reduces dimension significantly (d << D) but strictly requires balanced initializations.

- **Failure signatures:**
  - Non-Balanced Linear Init: If you initialize a linear network randomly without satisfying the relaxed balanced condition, the metric M(θ(t)) will depend on the full trajectory θ(t), not just Z(t), preventing the dimensionality reduction.
  - Relevance: Theorem 4.4 explicitly proves the intrinsic property fails for non-balanced initializations in linear networks.

- **First 3 experiments:**
  1. Verify Conservation: Train a 2-layer linear network and plot UᵀU - VᵀV over time to confirm it remains constant (Prop 2.8).
  2. Test Necessity: Train two linear networks (one balanced, one random/unbalanced). Attempt to fit a function K(z) to the observed metric M(θ). It should only fit for the balanced case (Theorem 4.4).
  3. 3-Layer ReLU Dynamics: Implement the explicit ODE for a scalar 3-layer ReLU net (Prop 3.10) and compare the trajectory of Z(t) against a standard gradient descent simulation to validate the closed-form intrinsic dynamic.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the relaxed balanced initialization necessary for the intrinsic metric property to hold in two-layer linear networks when the hidden layer dimension r exceeds the input/output dimensions (r > max(n, m))?
- Basis in paper: Section 4.1 explicitly states regarding the rank condition: "The case when r > max(n, m) is still an open question."
- Why unresolved: Theorem 4.4 proves the necessity of relaxed balanced conditions for the intrinsic metric property only for r ≤ max(n, m).
- What evidence would resolve it: A counter-example finding a non-balanced initialization that satisfies the intrinsic metric property, or a mathematical proof extending the necessity to the high-rank case.

### Open Question 2
- Question: Can a closed-form expression for the intrinsic metric K(z) be derived for general deep ReLU networks of arbitrary width and depth?
- Basis in paper: Proposition 3.10 derives a closed-form for 3-layer scalar ReLU networks, but the Conclusion notes "Extending this analysis to deeper or more general architectures could shed new light" on the geometry.
- Why unresolved: The complexity of the algebraic system for α, β (Eq. 10) makes characterizing the explicit metric difficult as depth and width increase beyond the specific 3-layer case.
- What evidence would resolve it: A generalized formula for K(z) or a constructive method to determine the metric for arbitrary DAG architectures without relying on solving high-dimensional polynomial systems.

### Open Question 3
- Question: Does the trace of the generated Lie algebra Lie(W_{ϕ_{ReLU}}) have a constant dimension of D-m (where m is the number of hidden neurons) across the parameter space for general ReLU networks?
- Basis in paper: Section 3.3 states regarding the dimension of the trace: "we do not prove this here, it is empirically supported."
- Why unresolved: Confirming the dimension D-m formally verifies that the known set of conservation laws is maximal, which is required to fully guarantee the intrinsic recoverability property theoretically.
- What evidence would resolve it: A formal mathematical proof verifying the independence of the vector fields in the Lie algebra for the general ReLU parameterization.

## Limitations
- The intrinsic dynamic property is proven to hold on a dense open set for ReLU networks but not necessarily everywhere
- The relaxed balanced initialization condition for linear networks is quite restrictive and may not be satisfied in practical scenarios
- Analysis for deep ReLU networks relies on path-lifting, which can increase the dimensionality of the intrinsic variable

## Confidence
- **High Confidence:** The theoretical framework for defining intrinsic dynamics and the characterization of when the intrinsic metric property holds (Theorem 2.14) are mathematically rigorous
- **Medium Confidence:** The explicit closed-form expressions for intrinsic dynamics in specific cases (2-layer linear nets, 3-layer scalar ReLU nets) are correct, but their practical applicability may be limited by the initialization constraints
- **Low Confidence:** The behavior of intrinsic dynamics for general ReLU networks beyond the dense open set and for architectures not covered by the path-lifting formalism

## Next Checks
1. Implement and verify conservation laws in a 2-layer linear network with relaxed balanced initialization to confirm the intrinsic dynamic property holds
2. Test the necessity of the relaxed balanced condition by attempting to fit an intrinsic metric for non-balanced initializations in linear networks
3. Numerically solve the coupled system for a 3-layer scalar ReLU network and compare the intrinsic dynamic trajectory against standard gradient flow