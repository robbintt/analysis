---
ver: rpa2
title: 'Mechanisms are Transferable: Data-Efficient Low-Resource Adaptation via Circuit-Targeted
  Supervised Fine-Tuning'
arxiv_id: '2601.08146'
source_url: https://arxiv.org/abs/2601.08146
tags:
- circuit
- tuning
- heads
- depth
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of adapting large language models
  (LLMs) to low-resource languages, where labeled data is scarce and full-model fine-tuning
  can be unstable or cause catastrophic forgetting. The authors propose Circuit-Targeted
  Supervised Fine-Tuning (CT-SFT), a method that identifies a small set of task-relevant
  attention heads in a higher-resource language model using a modified Contextual
  Decomposition for Transformers (CD-T) procedure.
---

# Mechanisms are Transferable: Data-Efficient Low-Resource Adaptation via Circuit-Targeted Supervised Fine-Tuning

## Quick Facts
- arXiv ID: 2601.08146
- Source URL: https://arxiv.org/abs/2601.08146
- Reference count: 33
- This paper proposes CT-SFT to improve cross-lingual accuracy over continued full fine-tuning while updating only a small fraction of model parameters.

## Executive Summary
This paper addresses the challenge of adapting large language models to low-resource languages where labeled data is scarce. The authors introduce Circuit-Targeted Supervised Fine-Tuning (CT-SFT), a method that identifies task-relevant attention heads in a higher-resource language model using a modified Contextual Decomposition for Transformers (CD-T) procedure. CT-SFT then adapts the model to the low-resource language by updating only these heads (plus LayerNorm), using head-level gradient masking to restrict learning to the discovered circuit. The approach significantly improves cross-lingual accuracy over continued full fine-tuning while updating only a small subset of parameters and substantially reduces catastrophic forgetting.

## Method Summary
CT-SFT is a two-phase method for cross-lingual adaptation. First, it competence-tunes a source-language model on limited samples to establish baseline performance. Second, it runs modified CD-T to discover a circuit of task-relevant attention heads using label-balanced mean baselines and task-directional relevance scoring. Finally, it performs surgical fine-tuning on the target language, updating only the discovered circuit heads and LayerNorm parameters via head-level gradient masking. The method uses iterative backward expansion to grow circuits depth by depth, selecting the top 2% of heads per layer based on relevance scores. This constrained adaptation improves target-language performance while preserving source-language competence, with the degree of circuit editing versus preserving modulated by transfer difficulty.

## Key Results
- CT-SFT improves cross-lingual accuracy over continued full fine-tuning on NusaX-Senti (e.g., 0.547 vs 0.428 on Acehnese) while updating only ~2% of parameters
- CT-SFT substantially reduces catastrophic forgetting, preserving source-language competence near baseline levels while adapting to target languages
- An editing-preserving trade-off emerges: harder transfers benefit from editing circuit heads, while easier transfers favor preserving source mechanisms by updating near-zero relevance heads

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining gradient updates to task-relevant attention heads reduces destructive drift while preserving adaptive capacity in low-resource settings.
- Mechanism: Head-level gradient masking isolates learning to heads identified via directional relevance scoring. This limits parameter movement to components causally linked to the task decision boundary, preventing interference with unrelated circuits that support source-language competence.
- Core assumption: The discovered circuit in the proxy language captures decision-relevant computation that transfers structurally to the target language.
- Evidence anchors:
  - [abstract] "CT-SFT improves cross-lingual accuracy over continued full fine-tuning while updating only a small subset of model parameters"
  - [Section 6.1, Table 1] Full fine-tuning on 25-50 additional samples degrades performance (0.757→0.706), demonstrating instability
  - [Section 6.2, Table 3] CT-SFT preserves Indonesian accuracy near 0.757 while Full FT degrades to 0.341-0.451
- Break condition: If the source-language circuit has weak transfer correlation with target-language task behavior (low A0), editing alone may be insufficient and mechanism-preserving updates become preferable.

### Mechanism 2
- Claim: Task-directional relevance scoring more reliably identifies decision-aligned components than magnitude-based scoring, particularly at deeper circuit expansion depths.
- Mechanism: Projecting intermediate contributions onto a task direction vector (derived from unembedding weights) measures alignment with the correct label logit versus alternatives. This favors heads that discriminatively support correct predictions rather than merely high-activation heads.
- Core assumption: The unembedding matrix W_U provides meaningful label-specific reference directions for intermediate residual updates.
- Evidence anchors:
  - [Section 4.1.2, Eq. 5-6] Defines v_task = W_U(y_c) - mean(W_U(y_other)) and relevance as projection onto this direction
  - [Section 6.4, Figure 3] Projection scoring shows monotonically increasing faithfulness with depth; magnitude scoring is non-monotonic
  - [Section 6.4] "Magnitude-based ranking can prioritize high-norm but weakly task-aligned components"
- Break condition: If the model's unembedding directions are poorly calibrated or the task has high label ambiguity, directional scoring may conflate correlation with causation.

### Mechanism 3
- Claim: Transfer difficulty modulates an editing-preserving trade-off in how circuit updates affect adaptation outcomes.
- Mechanism: When source-target representation gap is large (low A0 competence-tuning baseline), directly editing decision-relevant heads enables necessary representation shift. When gap is small (high A0), preserving the source mechanism and adapting through weakly-aligned heads reduces unnecessary drift.
- Core assumption: Competence-tuning baseline A0(ℓ) meaningfully captures representation gap between source and target languages.
- Evidence anchors:
  - [Section 6.2, Table 2] Acehnese (A0=0.471) and Buginese (A0=0.366) benefit from Circuit editing; Javanese (A0=0.581) and Minangkabau (A0=0.554) favor NearZero at depth 2
  - [Section 6.3] XNLI reproduces pattern: Thai (A0=0.362) benefits from editing; Vietnamese/Spanish/Chinese (A0>0.50) favor preserving
  - [corpus] Limited direct corpus support; neighboring papers on low-resource adaptation do not address circuit-level editing-preserving trade-offs
- Break condition: If the competence-tuning checkpoint has not reached minimal task competence, the editing-preserving heuristic becomes unreliable (as shown in XNLI 50-sample setting).

## Foundational Learning

- Concept: **Catastrophic Forgetting in Continual Learning**
  - Why needed here: CT-SFT's primary motivation is preserving source-language competence during cross-lingual transfer. Understanding why full-model fine-tuning degrades previous capabilities is essential.
  - Quick check question: Can you explain why updating all parameters on a new task distribution interferes with previously learned representations?

- Concept: **Circuit Discovery via Activation Decomposition**
  - Why needed here: The method builds on CD-T's decomposition into relevant (β) and irrelevant (γ) streams. Understanding this framing is necessary to interpret the modified scoring protocol.
  - Quick check question: Given a baseline activation μ, how would you decompose an observed activation a_x(s) into signal vs. background?

- Concept: **Cross-Lingual Transfer and Language Representation**
  - Why needed here: The editing-preserving trade-off depends on source-target similarity. Understanding what makes transfer "easy" vs. "hard" informs method application.
  - Quick check question: What linguistic or distributional factors might cause a competence-tuned checkpoint to transfer poorly to a related language?

## Architecture Onboarding

- Component map: Competence-tuned checkpoint -> CD-T relevance scoring with label-balanced mean -> Iterative backward expansion to build circuit -> Head-level gradient masking during fine-tuning -> Target adaptation while preserving source competence

- Critical path:
  1. Competence-tuned checkpoint quality (n_src samples) directly affects circuit reliability
  2. Balanced-mean baseline (μ) quality affects β/γ decomposition stability
  3. Directional relevance scoring accuracy determines head selection quality
  4. Circuit depth (max_depth) modulates editing vs. preserving trade-off

- Design tradeoffs:
  - **Depth vs. sparsity**: Deeper circuits include more upstream heads but may dilute task-relevant computation (Section 6.2)
  - **Discovery pool overlap**: Shared pool stabilizes iteration-0 selection but violates strict holdout; separate pool increases variance (Section D.3)
  - **Selection ratio**: 2% maintains sparsity; larger ratios (5-10%) grow circuits rapidly with diminishing faithfulness returns (Section D.2)

- Failure signatures:
  - Weak source competence (e.g., XNLI 50-sample) → unstable surgical updates, potential degradation
  - Held-out discovery pool in low-resource settings → iteration-0 collapse to shallow layers, unreliable circuits
  - High selection ratio at deep expansion → faithfulness stagnation despite larger circuits

- First 3 experiments:
  1. **Sanity check**: Replicate within-language Indonesian→Indonesian setting (Table 1) to verify that continued Full FT exhibits instability and CT-SFT provides regularization benefit
  2. **Circuit faithfulness diagnostic**: Compare magnitude vs. task-direction scoring on validation set using mean-ablation protocol (Figure 3) before attempting transfer
  3. **Pilot transfer with depth sweep**: Test single easy target (e.g., Javanese) and single hard target (e.g., Acehnese) across depths 0-2 to confirm editing-preserving trade-off before scaling to full benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the editing-preserving trade-off and catastrophic forgetting reduction persist when scaling to larger models or multi-token generative tasks?
- Basis in paper: [Explicit] Section 8 (Limitations) states it is unknown whether the observed behaviors hold for "larger models," "multi-token generation tasks," or alternative architectures.
- Why unresolved: The study exclusively uses Qwen2.5-0.5B and single-token classification tasks (NLI, Sentiment). Scaling laws may alter head importance distributions, and generation requires optimizing longer sequences where circuits might be less localized.
- What evidence would resolve it: Replicating the CT-SFT protocol on >7B parameter models or translation/summarization tasks, measuring if the editing-preserving trade-off correlates with transfer difficulty as it does in the 0.5B classification setting.

### Open Question 2
- Question: What is the minimum source-language competence required to ensure stable CT-SFT adaptation?
- Basis in paper: [Explicit] Section 6.3 notes that with low competence (50 English XNLI samples), "restricted update can become unstable," and Section 8 reinforces that CT-SFT is a "post-competence method" rather than a substitute for learning.
- Why unresolved: The paper identifies the failure mode (instability under weak competence) but does not quantify the threshold of task performance or circuit faithfulness required to prevent it.
- What evidence would resolve it: A systematic sweep of source-language data sizes (e.g., 10–200 samples) to identify the inflection point where circuit discovery becomes reliable enough to prevent degradation during target adaptation.

### Open Question 3
- Question: How does CT-SFT impact catastrophic forgetting when measured by distributional shifts or multi-source retention?
- Basis in paper: [Explicit] Section 8 acknowledges that forgetting was measured primarily via source-language accuracy, noting that "forgetting can also be measured in other ways (e.g., distributional shifts... or multilingual retention across multiple sources)."
- Why unresolved: Preserving accuracy does not guarantee that the model's internal representations or confidence distributions remain unchanged; the method might preserve the decision boundary while distorting the latent space.
- What evidence would resolve it: Evaluating adapted models on out-of-distribution source-language datasets or using representation similarity metrics (e.g., CKA) to verify if the "mechanism" is truly preserved structurally, not just behaviorally.

## Limitations

- The method relies on the assumption that circuits discovered in high-resource languages transfer structurally to low-resource languages, which may not hold for linguistically distant pairs
- The competence-tuning baseline A0, while useful as a heuristic, may not fully capture the complexity of cross-lingual transfer difficulty
- The paper does not extensively explore the impact of dataset size or quality on circuit discovery reliability, particularly in extremely low-resource settings where even the discovery pool may be small

## Confidence

- **High Confidence**: The core claim that CT-SFT improves cross-lingual accuracy over continued full fine-tuning while updating only a small fraction of parameters
- **Medium Confidence**: The claim that task-directional relevance scoring is more reliable than magnitude-based scoring for identifying decision-aligned components
- **Medium Confidence**: The editing-preserving trade-off claim, which depends on the competence-tuning baseline A0

## Next Checks

1. **Transfer Failure Analysis**: Systematically test CT-SFT on language pairs with known structural divergence (e.g., typologically distant languages) to identify conditions where the discovered circuit fails to transfer

2. **Discovery Pool Sensitivity**: Evaluate the impact of discovery pool size on circuit reliability, particularly in low-resource settings, to determine minimum viable pool sizes

3. **Ablation on Gradient Masking**: Compare CT-SFT with a variant that updates all parameters but applies stronger learning rate decay to non-circuit components to isolate the source of benefits