---
ver: rpa2
title: Global explainability of a deep abstaining classifier
arxiv_id: '2504.01202'
source_url: https://arxiv.org/abs/2504.01202
tags:
- reports
- classes
- cancer
- histology
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a global explainability method to identify
  sources of errors in a deep abstaining classifier (DAC) for histology classification
  of lung and breast cancers using 1.04 million pathology reports. The DAC achieves
  97% accuracy but abstains on 78% of samples due to label noise, hierarchical class
  complexity, insufficient information, and conflicting evidence.
---

# Global explainability of a deep abstaining classifier

## Quick Facts
- arXiv ID: 2504.01202
- Source URL: https://arxiv.org/abs/2504.01202
- Reference count: 40
- Key outcome: Deep abstaining classifier achieves 97% accuracy on 1.04M cancer pathology reports by abstaining on 78% of samples, with global explainability via GradInp + PCA identifying error patterns like hierarchical class confusion and label noise.

## Executive Summary
This study introduces a global explainability method for deep abstaining classifiers applied to multi-task histology classification of lung and breast cancers using 1.04 million pathology reports. The proposed deep abstaining classifier (DAC) achieves 97% accuracy on retained samples by abstaining on 78% of cases, effectively filtering out label noise, hierarchical complexity, and ambiguous reports. The method combines local explanations from Gradient•Input (GradInp) with PCA dimensionality reduction to aggregate insights across 13,000 reports, identifying specific error patterns such as ambiguous terms and class interdependencies. Results demonstrate that abstaining simplifies the decision boundary and enables targeted improvements in training protocols, such as excluding noisy reports and refining hierarchical class relationships. This approach enhances interpretability and provides actionable strategies for improving automated cancer report annotation systems.

## Method Summary
The study employs a deep abstaining classifier (DAC) based on a multitask CNN (MTCNN) architecture with three parallel 1D convolutional layers (filter sizes 3, 4, 5; 300 filters each), max-pooling, concatenation, and dense layers. The model is trained using a modified cross-entropy loss that includes an abstention class and a penalty parameter $\alpha$ to balance accuracy and coverage. Local explanations are generated using Gradient•Input (GradInp), where gradients are multiplied by re-centered embeddings (embedding minus centroid). Global explanations are derived by aggregating local explanations into an Aggregated Local Explanations (ALE) matrix, truncating features based on a threshold $T$, and applying PCA to identify error modes such as hierarchical confusion and label noise.

## Key Results
- DAC achieves 97% accuracy on retained samples while abstaining on 78% of cases.
- Global explainability via GradInp + PCA identifies error patterns including ambiguous terms and class interdependencies.
- Abstaining simplifies the decision boundary, enabling targeted improvements in training protocols (e.g., excluding noisy reports, refining hierarchical classes).

## Why This Works (Mechanism)
The deep abstaining classifier works by dynamically identifying and abstaining from samples that are likely to be mislabeled, ambiguous, or lack sufficient information. This reduces noise in the training data and improves the model's focus on high-confidence predictions. The global explainability method leverages local explanations (GradInp) to identify which words contribute most to predictions and then aggregates these insights across the dataset using PCA. This reveals systematic error patterns, such as hierarchical class confusion or label noise, which can be addressed to improve the model and training protocols.

## Foundational Learning
- **Deep Abstaining Classifier (DAC)**: A model that can choose to abstain from making predictions on uncertain or noisy samples, improving overall accuracy. *Why needed*: To handle label noise and ambiguity in large-scale pathology datasets. *Quick check*: Verify that abstention rate and accuracy are balanced during training.
- **Gradient•Input (GradInp)**: A local explanation method that multiplies gradients with input embeddings to identify feature importance. *Why needed*: To generate interpretable insights into model predictions at the word level. *Quick check*: Ensure embeddings are re-centered before computing gradients.
- **Aggregated Local Explanations (ALE) Matrix**: A matrix that aggregates local explanations across reports, summarizing word importance scores. *Why needed*: To enable global analysis of error patterns across the dataset. *Quick check*: Confirm that feature truncation threshold $T$ is set appropriately.
- **PCA for Global Explainability**: A dimensionality reduction technique used to identify dominant error modes in the ALE matrix. *Why needed*: To uncover systematic patterns in model errors, such as hierarchical confusion or label noise. *Quick check*: Validate that PCA clusters align with known error sources.

## Architecture Onboarding

**Component Map**: Text Preprocessing -> MTCNN Architecture -> DAC Loss + Training -> GradInp Local Explanations -> ALE Matrix Aggregation -> PCA Global Analysis

**Critical Path**: The critical path is the pipeline from training the DAC model to generating global explanations. This involves implementing the custom DAC loss, tuning the abstention penalty $\alpha$, computing GradInp for local explanations, aggregating these into the ALE matrix, and applying PCA to identify error modes.

**Design Tradeoffs**: The study balances abstention rate and accuracy by tuning $\alpha$, but this requires careful validation. The use of PCA for global explainability assumes linear relationships, which may not capture all error patterns. Additionally, the choice of embedding strategy and feature truncation threshold $T$ significantly impacts the quality of explanations.

**Failure Signatures**: If the model abstains on all or no samples, the penalty term $\alpha$ is likely misconfigured. Noisy or uninformative PCA clusters may indicate issues with embedding re-centering or feature truncation in the ALE matrix.

**3 First Experiments**:
1. Implement the DAC loss and train the model to achieve a balance between abstention rate and accuracy.
2. Compute GradInp local explanations and verify that re-centering embeddings improves interpretability.
3. Apply PCA to the ALE matrix and validate that identified error modes align with known issues (e.g., hierarchical confusion, label noise).

## Open Questions the Paper Calls Out
None

## Limitations
- The embedding strategy and feature truncation threshold $T$ are not explicitly defined, limiting reproducibility.
- The dynamic adjustment of the abstention penalty $\alpha$ during training is described but not detailed in algorithmic terms.
- The global explainability approach relies on PCA's linear assumptions, which may not capture complex, non-linear error patterns.

## Confidence

**High Confidence**: The core methodology of using GradInp for local explanations and PCA for global aggregation is technically sound and aligns with established practices in explainable AI. The results showing abstention improving decision boundary clarity are well-supported by the experimental setup.

**Medium Confidence**: The reported 97% accuracy on retained samples is plausible given the abstention strategy, but the exact impact of abstention on model calibration and fairness is not fully explored. The generalizability of findings to other pathology datasets or domains is uncertain without additional validation.

**Low Confidence**: The specific threshold values for feature selection and abstention penalty tuning are critical for reproducibility but are not provided, limiting the ability to replicate results precisely.

## Next Checks

1. **Reproduce the abstention behavior** by implementing the DAC loss and tuning $\alpha$ to achieve a balance between abstention rate and accuracy. Validate that the model abstains on noisy or ambiguous samples as intended.
2. **Validate the explainability pipeline** by testing the GradInp method with different embedding strategies (e.g., GloVe, BERT) and verifying that re-centering embeddings improves the interpretability of local explanations.
3. **Assess the robustness of PCA-based global explanations** by comparing results with non-linear dimensionality reduction techniques (e.g., t-SNE, UMAP) to ensure that identified error patterns are not artifacts of linear assumptions.