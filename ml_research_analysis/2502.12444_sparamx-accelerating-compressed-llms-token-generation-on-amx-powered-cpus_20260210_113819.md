---
ver: rpa2
title: 'SparAMX: Accelerating Compressed LLMs Token Generation on AMX-powered CPUs'
arxiv_id: '2502.12444'
source_url: https://arxiv.org/abs/2502.12444
tags:
- sparsity
- weight
- kernel
- weights
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces SparAMX, a system that leverages Advanced\
  \ Matrix Extensions (AMX) and unstructured sparsity to accelerate large language\
  \ model (LLM) inference on CPUs. By combining AMX\u2019s matrix multiplication capabilities\
  \ with a compressed sparse weight format, SparAMX reduces memory transfer during\
  \ the memory-bound decoding stage, achieving up to 1.42\xD7 faster end-to-end latency\
  \ compared to PyTorch and outperforming other CPU inference solutions like DeepSparse\
  \ and llama.cpp at high batch sizes."
---

# SparAMX: Accelerating Compressed LLMs Token Generation on AMX-powered CPUs

## Quick Facts
- arXiv ID: 2502.12444
- Source URL: https://arxiv.org/abs/2502.12444
- Reference count: 34
- Primary result: Up to 1.42× faster end-to-end latency vs. stock PyTorch on AMX CPUs using unstructured sparsity.

## Executive Summary
SparAMX accelerates large language model inference on CPUs by combining Advanced Matrix Extensions (AMX) with unstructured sparsity. The system introduces a compressed sparse weight format that reduces memory transfer during the memory-bound autoregressive decoding phase. By offloading computations to AMX and pruning both model weights and KV cache, SparAMX achieves up to 1.42× speedup over PyTorch while retaining accuracy within 1% loss.

## Method Summary
SparAMX leverages AMX's tile-based matrix multiplication capabilities alongside a novel sparse weight format that stores model weights as a bitmap and non-zero values. During inference, sparse weights are decompressed on-the-fly into AMX tiles for computation, reducing memory bandwidth usage. The system also applies unstructured sparsity to the KV cache during decoding, improving attention computation speed by 1.14×. Implemented as a PyTorch extension, SparAMX works with INT8 and BF16 precision and supports any PyTorch model through preprocessing steps.

## Key Results
- 1.42× end-to-end latency speedup vs. stock PyTorch on Llama3-8B with 50% unstructured sparsity
- 1.14× faster decoding via KV cache sparsity with <1% accuracy loss on downstream tasks
- Outperforms DeepSparse and llama.cpp at high batch sizes (>16) on AMX-enabled CPUs

## Why This Works (Mechanism)
SparAMX exploits the memory-bound nature of LLM decoding by reducing data movement through sparse representations. AMX's dedicated matrix tiles accelerate computations once weights are loaded, while unstructured sparsity removes unnecessary memory transfers of zero-valued weights. The KV cache sparsity further reduces bandwidth during autoregressive generation by skipping attention computations on zero values. Together, these optimizations maximize AMX utilization while minimizing memory bottlenecks.

## Foundational Learning

**Advanced Matrix Extensions (AMX)**
- Why needed: Provides dedicated hardware for matrix multiplication acceleration on Intel CPUs
- Quick check: Verify CPU supports AMX via `lscpu | grep amx`

**Unstructured Sparsity**
- Why needed: Removes zero weights to reduce memory bandwidth without specialized hardware support
- Quick check: Confirm 50% sparsity level maintains accuracy within acceptable bounds

**Sparse Weight Format**
- Why needed: Efficient storage representation for zero-compressed weights compatible with AMX
- Quick check: Validate bitmap + values format correctly reconstructs original sparse matrix

**KV Cache**
- Why needed: Stores key/value states during autoregressive generation to avoid recomputation
- Quick check: Ensure sparsity application doesn't corrupt cached attention states

**Thread-Safe Tile Usage**
- Why needed: Prevents race conditions when multiple threads access AMX tiles simultaneously
- Quick check: Monitor for correct results across different thread counts

## Architecture Onboarding

**Component Map**
Sparse Preprocessor -> Sparse Weight Format -> AMX Kernel -> PyTorch Extension -> Inference Engine

**Critical Path**
Preprocessing dense weights → Converting to sparse format → Loading into AMX tiles → Matrix multiplication → Attention computation with sparse KV cache

**Design Tradeoffs**
- Memory vs. computation: Sparse format reduces memory but adds decompression overhead
- Flexibility vs. performance: General-purpose design sacrifices some optimizations possible with specialized kernels
- Accuracy vs. speed: Higher sparsity increases speedup but may impact model quality

**Failure Signatures**
- Incorrect results with thread count changes (requires re-preprocessing)
- AMX not detected (fallback to slower AVX kernel)
- Compilation errors with PyTorch extensions (version compatibility issues)

**First Experiments**
1. Verify AMX detection and basic matrix multiplication functionality
2. Test sparse weight format conversion on a small dense model
3. Compare latency of sparse vs. dense inference on a single batch

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can direct AVX-to-AMX tile data transfer instructions eliminate the memory round-trip overhead during sparse weight decompression?
- Basis in paper: Current hardware lacks this capability, forcing intermediate memory stores that partially offset bandwidth savings
- Why unresolved: Requires hardware changes not currently available
- What evidence would resolve it: Simulation or hardware prototype measuring latency with and without direct transfer support

**Open Question 2**
- Question: What is the performance–accuracy tradeoff when extending SparAMX to support INT4 quantization via dequantization to INT8?
- Basis in paper: AMX units do not natively support more compressed formats like INT4. Extending support to INT4 is feasible by dequantizing INT4 values into INT8 before computation
- Why unresolved: The approach is suggested but not implemented or evaluated in the paper
- What evidence would resolve it: Benchmark results comparing INT4-with-dequantization against native INT8 on latency and accuracy

**Open Question 3**
- Question: Can the KV cache sparsity method be adapted to support dynamically updated KV values during autoregressive decoding?
- Basis in paper: The attention kernel is not suitable for dynamic KV values but remains effective for cached prompts
- Why unresolved: The current sparse format requires offline preprocessing, incompatible with tokens added at runtime
- What evidence would resolve it: A modified kernel design with incremental sparse updates, benchmarked on long-context decoding

## Limitations
- AMX dependency limits applicability to Intel Sapphire Rapids or newer CPUs only
- Thread count changes require re-preprocessing of sparse formats
- KV cache sparsity not suitable for dynamic token generation during decoding
- Accuracy guarantees vary by task and sparsity level without theoretical bounds

## Confidence

**Major claim confidence:**
- End-to-end latency speedup (1.42× vs PyTorch): **High** — backed by controlled benchmarks on identical hardware, though AMX dependency limits generalizability
- KV-cache sparsity decoding speedup (1.14×): **Medium** — empirical result, but accuracy trade-off varies by task and sparsity level; re-preprocessing overhead is under-specified
- General-purpose compatibility: **Low** — weight sparsity is model-agnostic, but KV-cache sparsity is architecturally constrained and not truly "any PyTorch model"

## Next Checks

1. Reproduce the sparse weight format conversion on a dense Llama3-8B checkpoint and verify correct AMX kernel execution via thread-safe tile usage
2. Measure accuracy retention across GSM8K, ARC, and HellaSwag with KV-cache sparsity levels 40-70% to confirm <1% loss claim holds beyond reported tasks
3. Benchmark against DeepSparse and llama.cpp at batch sizes >16 to validate sustained performance lead in the high-throughput regime