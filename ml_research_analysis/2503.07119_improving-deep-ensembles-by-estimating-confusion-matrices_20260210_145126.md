---
ver: rpa2
title: Improving Deep Ensembles by Estimating Confusion Matrices
arxiv_id: '2503.07119'
source_url: https://arxiv.org/abs/2503.07119
tags:
- severity
- ensemble
- accuracy
- page
- brier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of aggregating predictions from
  deep learning ensembles by proposing a method called Soft Dawid-Skene (SDS) that
  combines soft labels with confusion matrix estimation to improve accuracy and calibration.
  The core idea is to model ensemble member predictions using a Dirichlet distribution
  and estimate confusion matrices through an Expectation-Maximization algorithm, which
  allows for weighing ensemble members according to their inferred performance.
---

# Improving Deep Ensembles by Estimating Confusion Matrices

## Quick Facts
- **arXiv ID**: 2503.07119
- **Source URL**: https://arxiv.org/abs/2503.07119
- **Reference count**: 14
- **Primary result**: SDS improves ensemble accuracy and calibration by modeling ensemble members with Dirichlet distributions and estimating confusion matrices via EM algorithm

## Executive Summary
This paper addresses ensemble aggregation by proposing Soft Dawid-Skene (SDS), which combines soft labels with confusion matrix estimation to improve accuracy and calibration. The method models ensemble member predictions using a Dirichlet distribution and estimates confusion matrices through an Expectation-Maximization algorithm, weighing ensemble members according to their inferred class-specific performance. Extensive experiments on MNIST, CIFAR10/100, and ImageNet demonstrate that SDS outperforms traditional ensemble averaging in accuracy, calibration (measured by ECE, Brier score, and NLL), and out-of-distribution detection.

## Method Summary
The Soft Dawid-Skene (SDS) method adapts the classical Dawid-Skene crowdsourcing model to deep ensemble aggregation by replacing hard labels with soft probability distributions and using Dirichlet distributions instead of multinomials. The algorithm iteratively estimates confusion matrices for each ensemble member and the true label distribution through an EM procedure, where the E-step computes posterior probabilities of true labels using Polyak averaging for stability, and the M-step updates parameters using analytical solutions for class priors and gradient descent (AdamW) for confusion matrices. This allows the method to dynamically weigh ensemble members based on their inferred class-specific reliability rather than treating all models equally.

## Key Results
- SDS outperforms traditional ensemble averaging in accuracy across 90% of tested scenarios
- Improves calibration metrics (ECE, Brier score, NLL) consistently across corruption and OOD detection tasks
- Demonstrates superior out-of-distribution detection capabilities compared to baseline methods
- Shows robustness to various data corruptions while maintaining better uncertainty estimates

## Why This Works (Mechanism)

### Mechanism 1: Latent Expertise Weighting via Confusion Matrices
Aggregation performance improves when models are dynamically weighted by their class-specific reliability rather than treated equally. The method estimates a confusion matrix $\Pi^{(k)}$ for each ensemble member $k$ and weighs predictions based on the inferred probability of correct classification for specific classes, identifying "experts" for different data subsets. This relies on the conditional independence assumption of classifiers given the true label, which the paper acknowledges as a strong simplification.

### Mechanism 2: Uncertainty Preservation via Dirichlet Priors
Utilizing full probability vectors as inputs preserves epistemic uncertainty that is discarded by hard label voting. The model replaces the Multinomial distribution of classic Dawid-Skene with a Dirichlet distribution, allowing the aggregation mechanism to distinguish between confident (e.g., 0.99) and uncertain (e.g., 0.51) predictions. This captures meaningful signal about relative uncertainty that aids generalization, even when base neural networks are miscalibrated.

### Mechanism 3: Stabilization via Polyak Averaging
Convergence stability in the non-convex M-step is maintained by smoothing the updates of the latent true labels. The M-step uses gradient descent rather than an analytical solution, introducing instability that is mitigated by using Polyak averaging (Eq 4), mixing new posterior estimates with previous ones using weight $\alpha$. The paper confirms this mechanism's necessity, showing very poor performance when $\alpha=1$ (disabling averaging).

## Foundational Learning

- **Concept: Expectation-Maximization (EM) Algorithm**
  - *Why needed here:* The SDS model is unsupervised and must simultaneously infer latent "true" labels and confusion matrices. EM allows these two unknowns to refine each other iteratively.
  - *Quick check question:* Can you explain why maximizing the likelihood is difficult when you have missing data (the true labels), and how the E-step approximates this?

- **Concept: Dirichlet Distribution**
  - *Why needed here:* The output of a classifier is a probability vector on the simplex. The Dirichlet distribution is the conjugate prior for the multinomial, making it mathematically appropriate for modeling "distributions of probability vectors."
  - *Quick check question:* How does a Dirichlet distribution differ from a Gaussian when modeling probabilities, specifically regarding the constraint that values must sum to 1?

- **Concept: Calibration Metrics (ECE, Brier Score)**
  - *Why needed here:* The paper claims improvements in "calibration," requiring understanding of Expected Calibration Error (ECE) to verify if confidence matches actual accuracy.
  - *Quick check question:* If a model predicts "Class A" with 80% confidence on 100 samples, and only 50 of those are actually Class A, is the model calibrated?

## Architecture Onboarding

- **Component map:** Input tensor $C \in [0,1]^{N \times K \times J}$ (Data points × Models × Classes) → Initialization (1 Hard Dawid-Skene iteration) → EM Core (E-step with Polyak averaging + M-step with AdamW) → Output: Aggregated label distribution $p(t_i|C)$

- **Critical path:** The M-step gradient update is critical because unlike standard Dawid-Skene, this step is not analytically tractable. The system relies on AdamW optimizer finding a valid maximizer for confusion matrices; if this optimization fails, estimated confusion matrices will be invalid, breaking the E-step in the next iteration.

- **Design tradeoffs:**
  - *Computational Cost vs. Accuracy:* SDS is slower than Ensemble Averaging (EA). SDS takes ~114s on ImageNet vs EA's 0.3s, making it impractical for real-time applications without significant resources.
  - *Batch vs. Online:* "Online SDS" (Appendix F) fixes confusion matrices after learning phase, trading adaptability for streaming data processing capability.

- **Failure signatures:**
  - *Gamma Explosion:* M-step involves Gamma functions (Eq 1). If confusion matrix values grow too large, numerical instability occurs. The paper uses weight decay in the optimizer to mitigate this.
  - *Uniform Collapse:* If ensemble is too small or correlated, confusion matrices may collapse to uniform distributions, reducing SDS to simple averaging.

- **First 3 experiments:**
  1. *Hyperparameter Sensitivity Check:* Implement SDS on MNIST. Run ablation where $\alpha=1$ (no Polyak averaging). Verify if performance degrades significantly as claimed.
  2. *Corruption Robustness:* Train 3-member ensemble on clean CIFAR-10. Apply SDS vs. EA on "Frosted Glass Blur" corruption. Plot Accuracy vs. Severity to reproduce Figure 3 divergence.
  3. *OOD Detection:* Mix CIFAR-10 (In-distribution) and SVHN (Out-of-distribution). Compare AUROC of SDS vs. EA. Check if SDS detects OOD samples earlier (at lower ratios) as shown in Figure 6.

## Open Questions the Paper Calls Out
None

## Limitations
- The conditional independence assumption (ensemble members' predictions are independent given the true label) is a critical but potentially unrealistic simplification that may degrade performance when violated
- Computational cost is significantly higher than baseline methods (380x slower on ImageNet), making it impractical for real-time applications
- The method's performance with highly correlated ensemble members hasn't been thoroughly explored

## Confidence
- **Accuracy improvements (High)**: Well-supported by extensive experiments across multiple datasets with statistically significant results showing SDS outperforming traditional ensemble averaging in 90% of cases
- **Calibration improvements (High)**: Strong evidence from ECE, Brier score, and NLL metrics showing consistent improvements across corruption scenarios and OOD detection tasks
- **OOD detection capabilities (Medium)**: Promising results on CIFAR-10 vs SVHN detection, but limited exploration of diverse OOD scenarios and calibration under OOD conditions

## Next Checks
1. **Correlation Stress Test**: Create an ensemble where members are intentionally highly correlated (e.g., same architecture with different seeds, or models trained on overlapping data subsets). Measure SDS performance degradation compared to ensemble averaging to quantify the impact of the independence assumption violation.

2. **Cross-Domain Calibration**: Test SDS on datasets with significantly different class distributions from the training data (e.g., train on CIFAR-10, test on a medical imaging dataset). Measure whether calibration improvements persist when domain shift occurs.

3. **Computational Optimization**: Implement GPU-accelerated versions of the M-step and measure the speedup achievable. Compare the accuracy-latency tradeoff of optimized SDS against simple temperature scaling methods for calibration.