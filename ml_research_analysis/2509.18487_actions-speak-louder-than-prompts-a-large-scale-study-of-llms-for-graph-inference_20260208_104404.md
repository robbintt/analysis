---
ver: rpa2
title: 'Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference'
arxiv_id: '2509.18487'
source_url: https://arxiv.org/abs/2509.18487
tags:
- graph
- node
- llms
- graph-as-code
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically evaluates large language models (LLMs)
  for node classification on text-rich graphs. It compares three LLM-graph interaction
  strategies: prompting, tool-use, and code generation.'
---

# Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference

## Quick Facts
- **arXiv ID:** 2509.18487
- **Source URL:** https://arxiv.org/abs/2509.18487
- **Reference count:** 19
- **Primary result:** Code generation achieves the strongest performance for LLM-based node classification on text-rich graphs, particularly on long-text or high-degree graphs where prompting hits token limits.

## Executive Summary
This paper systematically evaluates three LLM-graph interaction strategies—prompting, tool-use (GraphTool), and code generation (Graph-as-Code)—for node classification on text-rich graphs. The evaluation covers diverse dataset domains, structural regimes (homophilic vs. heterophilic), feature characteristics (short vs. long text), and model configurations. Experiments demonstrate that Graph-as-Code achieves the strongest overall performance, especially on graphs with long textual features or high-degree nodes where prompting quickly exceeds token budgets. All interaction strategies remain effective on heterophilic graphs, challenging the assumption that LLMs fail in low-homophily settings. The findings provide actionable guidance for choosing LLM-graph interaction methods in practical applications.

## Method Summary
The paper compares three LLM-graph interaction modes for node classification: (1) Prompting with k-hop neighborhood serialization, (2) GraphTool with a ReAct loop executing fixed tool actions, and (3) Graph-as-Code generating and executing pandas queries over a DataFrame schema. Experiments use 14 graph datasets spanning short-text homophilic, heterophilic, and long-text homophilic regimes. The primary model is o4-mini, with additional tests on Llama, DeepSeek R1, GPT-5, Phi-4, and Qwen. Evaluation metrics include classification accuracy with 1,000 test nodes per seed over 5 seeds, mean ± std reported. Ablation studies systematically delete edges, labels, and truncate features to analyze signal dependencies.

## Key Results
- Graph-as-Code achieves the strongest overall performance, with large gains on long-text or high-degree graphs where prompting exceeds token budgets
- All interaction strategies remain effective on heterophilic graphs, challenging assumptions about LLM limitations in low-homophily settings
- Graph-as-Code demonstrates adaptive reliance on structure, features, or labels based on which is most informative, making it only vulnerable when multiple signals are heavily degraded

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Graph-as-Code outperforms prompting by avoiding context window exhaustion through selective, programmatic retrieval.
- **Mechanism:** Instead of serializing the entire k-hop neighborhood upfront (prompting), Graph-as-Code generates compact pandas queries that retrieve only necessary information per step. This collapses multi-step tool sequences into single operations, preserving token budget for reasoning.
- **Core assumption:** LLMs can reliably generate and execute correct pandas/graph queries without syntax errors that compound across iterations.
- **Evidence anchors:** [abstract] "LLMs as code generators achieve the strongest overall performance... with especially large gains on long-text or high-degree graphs where prompting quickly exceeds the token budget." [Section 4.3] "Graph-as-Code can offer substantial advantages for LLM-based node classification in dense or feature-rich graphs... approaches like Graph-as-Code that restructure graph information to highlight salient structure and reduce redundancy remain crucial."
- **Break condition:** If generated code has high error rates or requires many debug iterations, token savings evaporate and performance degrades.

### Mechanism 2
- **Claim:** Graph-as-Code adaptively shifts reliance between structure, features, and labels based on which signal is most informative for the current instance.
- **Mechanism:** The code generation paradigm allows the LLM to conditionally query different data modalities (neighbors, features, labels) rather than being locked into a fixed retrieval pattern. The LLM can write logic that prioritizes features when edges are sparse/uninformative, or structure when features are weak.
- **Core assumption:** The LLM has sufficient reasoning capability to infer which signal type is most predictive for a given node and graph context, and can express this as executable code.
- **Evidence anchors:** [abstract] "Code generation is able to flexibly adapt its reliance between structure, features, or labels to leverage the most informative input type." [Section 5.2, Finding 9] "Graph-as-Code can flexibly shift its reliance to the most informative input type, and is thus only vulnerable when multiple sources of information are heavily degraded."
- **Break condition:** If the LLM cannot reliably determine signal quality (e.g., misjudges when features vs. structure matter), it may over-rely on noisy inputs and underperform fixed strategies.

### Mechanism 3
- **Claim:** LLM-graph methods remain effective on heterophilic graphs because they can leverage feature-based and non-local reasoning rather than relying solely on neighborhood label voting.
- **Mechanism:** Unlike label propagation which assumes homophily, LLMs process textual features semantically and can identify discriminative patterns independent of neighbor labels. Graph-as-Code further enables compositional queries that don't assume label agreement among neighbors.
- **Core assumption:** Textual features contain sufficient discriminative signal for classification, and LLMs can extract this signal even when neighbor labels are misleading or absent.
- **Evidence anchors:** [abstract] "All interaction strategies remain effective on heterophilic graphs, challenging the assumption that LLM-based methods collapse under low homophily." [Section 4.2, Finding 3] "All LLM-graph interaction modes achieve strong accuracy... demonstrating that LLMs can exploit non-local or feature-based cues for classification, rather than relying solely on simple neighborhood voting heuristics."
- **Break condition:** If textual features are weak or uninformative (short/noisy text), and homophily is low, all signals degrade simultaneously and LLM methods may collapse.

## Foundational Learning

- **Concept: Homophily vs. Heterophily**
  - **Why needed here:** The paper explicitly evaluates performance across homophilic (neighbors share labels) and heterophilic (neighbors have different labels) regimes. Understanding this distinction is essential for interpreting why certain methods work in different settings.
  - **Quick check question:** On a citation network where papers cite related work (same field), would you expect high or low homophily?

- **Concept: ReAct-style Reasoning (Think-Act-Observe Loop)**
  - **Why needed here:** GraphTool and Graph-as-Code build on the ReAct paradigm where LLMs interleave reasoning steps with actions (tool calls or code execution). Understanding this pattern clarifies why iterative approaches outperform single-turn prompting.
  - **Quick check question:** In a ReAct loop, what happens after the "act" step is executed?

- **Concept: Context Window Constraints**
  - **Why needed here:** A central finding is that prompting fails when serialized neighborhoods exceed token limits. Understanding context window mechanics explains why code generation's selective retrieval provides an advantage.
  - **Quick check question:** If a node has 100 neighbors each with 500-token descriptions, what happens when you try a 2-hop prompt?

## Architecture Onboarding

- **Component map:**
  - Graph data ingestion → DataFrame schema (node_id, features, neighbors, label)
  - Prompt template selection → Interaction mode (Prompting/GraphTool/Graph-as-Code)
  - Iterative reasoning loop or single-shot inference → LLM API call
  - Response parsing → Label extraction and accuracy computation

- **Critical path:**
  1. Graph data ingestion into standardized DataFrame schema (node_id index, features/neighbors/label columns)
  2. Prompt template selection based on interaction mode
  3. Iterative reasoning loop (for GraphTool and Graph-as-Code) or single-shot inference (Prompting)
  4. Response parsing and label extraction
  5. Token budget monitoring (especially for Prompting variants)

- **Design tradeoffs:**
  - **Prompting:** Simplest implementation, but hits token limits on dense/long-text graphs; fixed retrieval pattern
  - **GraphTool:** More flexible than prompting, but limited to predefined actions; moderate implementation complexity
  - **Graph-as-Code:** Most flexible and robust, but requires safe code execution sandbox and error handling for malformed queries
  - **Assumption:** The paper uses o4-mini as primary model; smaller models may struggle with code generation reliability

- **Failure signatures:**
  - Prompting returns truncated context errors or "TokenLimit" in results
  - GraphTool enters infinite loops without issuing terminal action
  - Graph-as-Code generates invalid pandas syntax or queries non-existent columns
  - All methods degrade when features + labels + edges are simultaneously degraded (Figure 4 shows Graph-as-Code maintains performance longer)

- **First 3 experiments:**
  1. **Baseline comparison on short-text homophilic data (cora, pubmed):** Run all three interaction modes with 0/1/2-hop variants for prompting. Verify Graph-as-Code matches or exceeds 2-hop prompt without token limit issues.
  2. **Long-text stress test (photo, wiki-cs):** Compare 2-hop prompt (with and without budget capping) against Graph-as-Code. Confirm prompting hits token limits while Graph-as-Code maintains accuracy.
  3. **Ablation on signal dependencies:** Run controlled feature/edge/label deletion on cora (homophilic) and cornell (heterophilic). Generate 2D heatmaps to verify Graph-as-Code's adaptive reliance (Figures 2-4 patterns).

## Open Questions the Paper Calls Out

- **Question:** Does the superiority of Graph-as-Code persist across non-classification graph tasks such as link prediction or graph generation?
  - **Basis in paper:** [inferred] The authors explicitly constrain the scope of the study to node classification, leaving the generalizability of these interaction modes to other tasks unexplored.
  - **Why unresolved:** The reasoning and structural dependencies required for link prediction or generating subgraphs may differ significantly from the label inference logic tested here.
  - **What evidence would resolve it:** A systematic evaluation of prompting, tool-use, and code generation on standard link prediction and graph generation benchmarks.

- **Question:** Will expanding LLM context windows eliminate the performance gap between Prompting and Graph-as-Code?
  - **Basis in paper:** [explicit] Section 4.3 notes that while context windows are expanding, "approaches like Graph-as-Code... remain crucial, even as model capacities grow," implying the "lost in the middle" problem persists.
  - **Why unresolved:** It is unclear if the bottleneck is purely token capacity or if the reasoning mechanism of code execution provides a fundamental advantage over long-context attention.
  - **What evidence would resolve it:** Comparing the interaction modes on models with 1M+ token contexts where prompting includes full neighborhoods without truncation.

- **Question:** What is the specific computational cost/latency trade-off for the accuracy gains achieved by Graph-as-Code?
  - **Basis in paper:** [inferred] The paper focuses on accuracy and step efficiency but does not quantify the wall-clock time or financial cost of iterative code execution versus single-shot prompting.
  - **Why unresolved:** Graph-as-Code involves multiple inference steps and code execution, which may be prohibitively slow or expensive for real-time applications compared to prompting.
  - **What evidence would resolve it:** A benchmark analyzing latency and API cost per correct prediction for each interaction mode.

## Limitations

- **Implementation variability:** The Graph-as-Code approach's effectiveness hinges on reliable code generation and execution, but the paper provides limited details about the sandbox environment, error handling, and how the DataFrame is presented to the LLM.
- **Generalization concerns:** All experiments use the same o4-mini model and similar graph structures, creating uncertainty about performance with different LLM architectures, non-English text, or graphs with different structural properties.
- **Mechanism verification gaps:** The paper claims Graph-as-Code achieves token efficiency through selective retrieval and demonstrates adaptive signal reliance, but provides limited direct evidence for these mechanisms and lacks corpus validation.

## Confidence

**High confidence** in the overall finding that code generation outperforms prompting and tool-use across most evaluated conditions, supported by extensive experimental results across 14 datasets and multiple axes of variation.

**Medium confidence** in the specific mechanisms (token efficiency, adaptive reliance, heterophily robustness) that explain why code generation performs well, as these are supported by limited direct evidence and lack corpus validation.

**Low confidence** in how these methods would perform with different LLM models, implementation details, or graph types outside the evaluated domain, due to underspecified experimental conditions and limited generalizability evidence.

## Next Checks

1. **Token budget validation:** Instrument the Graph-as-Code implementation to measure actual token consumption per iteration and compare against prompting baselines. Verify that the selective retrieval mechanism consistently reduces token usage and that any token savings translate to sustained performance gains.

2. **Code generation reliability test:** Systematically measure error rates in generated pandas expressions across different graph densities and feature lengths. Evaluate whether error rates increase with graph complexity and whether error handling impacts overall performance.

3. **Cross-model generalization:** Replicate the Graph-as-Code experiments with at least two different LLM families (e.g., GPT and Llama models) to assess whether the performance advantages are model-dependent or represent a more fundamental interaction paradigm advantage.