---
ver: rpa2
title: 'Cost-Aware Prediction (CAP): An LLM-Enhanced Machine Learning Pipeline and
  Decision Support System for Heart Failure Mortality Prediction'
arxiv_id: '2511.15357'
source_url: https://arxiv.org/abs/2511.15357
tags:
- cost
- risk
- patient
- clinical
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces CAP, a decision-support framework combining
  ML predictions with cost-benefit analysis via LLM agents to predict 1-year mortality
  in heart failure patients (N=30,021, 22% mortality). The best-performing XGB model
  achieved AUROC 0.804, AUPRC 0.529, and Brier score 0.135.
---

# Cost-Aware Prediction (CAP): An LLM-Enhanced Machine Learning Pipeline and Decision Support System for Heart Failure Mortality Prediction

## Quick Facts
- arXiv ID: 2511.15357
- Source URL: https://arxiv.org/abs/2511.15357
- Authors: Yinan Yu; Falk Dippel; Christina E. Lundberg; Martin Lindgren; Annika Rosengren; Martin Adiels; Helen Sjöland
- Reference count: 40
- One-line primary result: XGB model achieved AUROC 0.804, AUPRC 0.529, and Brier score 0.135 for 1-year heart failure mortality prediction

## Executive Summary
CAP integrates ML predictions with cost-benefit analysis via LLM agents to predict 1-year mortality in heart failure patients (N=30,021, 22% mortality). The framework combines an XGB classifier with Cost-Interpretation-Prediction (CIP) curves and four LLM agents that provide patient-specific explanations on risk certainty, cost interpretation, uncertainty reduction, and future risk mitigation. Clinician evaluation showed high reliability for risk analysis but noted inaccuracies in speculative guidance, emphasizing the need for improved technical accuracy in complex scenarios.

## Method Summary
The CAP pipeline processes EHR data from 30,021 Swedish heart failure patients to predict 1-year all-cause mortality. It uses 75 clinical features (demographics, comorbidities, labs, vitals) with XGB classifier trained via AUPRC optimization. The system computes CIP curves visualizing treatment and error costs across quality of life and healthcare dimensions at different thresholds. Four prompt-only LLM agents (GPT-4.1) generate JSON-formatted responses for patient-specific explanations, with dependencies structured in agent chains. The framework is deployed through "CAPboard" visualization combining patient profile, F1 curve, CIP curves, and agent summaries.

## Key Results
- Best-performing XGB model achieved AUROC 0.804, AUPRC 0.529, and Brier score 0.135
- CIP curves visualized treatment and error costs across quality of life and healthcare dimensions at different thresholds
- Clinician evaluation showed high reliability for risk analysis (Agent I reliability 4.70/5) but noted inaccuracies in speculative guidance (Agent IV reliability 2.60/5)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised gradient boosting models can predict 1-year heart failure mortality with meaningful discrimination for downstream decision support.
- Mechan: XGB classifier learns non-linear relationships from 75 clinical features to output calibrated risk probabilities. The model was selected via AUPRC optimization to handle class imbalance (22% mortality prevalence), with threshold selection at maximal F1 score (0.25).
- Core assumption: Historical EHR patterns generalize to future patients; feature distributions remain stable.
- Evidence anchors:
  - [abstract] "The best-performing XGB model achieved AUROC 0.804, AUPRC 0.529, and Brier score 0.135"
  - [section] "Boosting machines outperformed simpler models and were well calibrated according to the ideal calibration line"
- Break condition: Feature drift, calibration degradation over time, or deployment population differing significantly from training cohort.

### Mechanism 2
- Claim: CIP curves decompose heterogeneous cost dimensions to visualize population-level trade-offs across decision thresholds.
- Mechan: User-defined cost matrices are aggregated across all patients at each threshold using the expected cost formula. Stacked visualization shows cost composition changes as threshold varies.
- Core assumption: Costs can be meaningfully quantified on comparable semi-quantitative scales; clinicians can accurately specify relative cost weights.
- Evidence anchors:
  - [abstract] "CIP curves visualized treatment and error costs across quality of life and healthcare dimensions at different thresholds"
  - [section] "CIP cost curves provided a population-level overview of cost composition across decision thresholds"
- Break condition: Cost definitions become outdated; stakeholder disagreement on cost weightings.

### Mechanism 3
- Claim: LLM agents can reliably synthesize structured clinical and model outputs for descriptive tasks, but speculative guidance requires caution.
- Mechan: Four prompt-only agents receive structured context and generate JSON-formatted responses. Agent chains allow downstream agents to reference upstream outputs.
- Core assumption: LLMs can accurately interpret numerical model outputs and clinical context without tool integration; structured prompts constrain hallucination adequately.
- Evidence anchors:
  - [abstract] "Clinician evaluation showed high reliability for risk analysis but noted inaccuracies in speculative guidance"
  - [section] Agent I reliability 4.70/5, Agent IV reliability 2.60/5; "speculative or forward-looking guidance was unsatisfactory"
- Break condition: Complex patient profiles with ambiguous or conflicting features; speculative questions exceeding evidence grounding.

## Foundational Learning

- Concept: Cost-benefit threshold analysis
  - Why needed here: CAP fundamentally operates on threshold-dependent classification where different thresholds yield different TP/FP/TN/FN distributions, each with distinct clinical and economic consequences.
  - Quick check question: If decision threshold increases from 0.24 to 0.35, what happens to false negative rate and how does this impact error costs?

- Concept: Prompt-only LLM agent design
  - Why needed here: CAP uses LLMs without external tool integration; understanding prompt engineering and dependency chains is essential for system reliability.
  - Quick check question: What context dependencies does Agent IV require that Agent I does not?

- Concept: Calibration in clinical prediction models
  - Why needed here: Brier score (0.135) reflects calibration quality; for cost-aware decisions, predicted probabilities must reflect true event rates at each score level.
  - Quick check question: A model with AUROC 0.90 but poor calibration would produce what kind of error in CIP cost estimates?

## Architecture Onboarding

- Component map: EHR extraction (75 features) → preprocessing → XGB classifier → risk score + classification → CIP curve computation → 4 LLM agents → "CAPboard" visualization

- Critical path: Feature extraction quality → model calibration → cost matrix definition accuracy → LLM prompt grounding. Errors propagate forward; upstream calibration issues corrupt downstream cost estimates.

- Design tradeoffs:
  - XGB over neural networks: Favors interpretability over potential accuracy gains
  - Prompt-only over tool-integrated LLMs: Simplicity and traceability vs. potential accuracy gains
  - 20-feature core set for synthetic patients vs. 75 full features: Evaluation tractability vs. clinical fidelity
  - Fixed cost matrix vs. patient-specific costs: Implementation simplicity vs. precision

- Failure signatures:
  - Agent IV generating "unrealistic, ungrounded" recommendations (reliability 2.60)
  - Overconfident imperative language in speculative guidance
  - Incorrect or overreaching medical terminology in LLM outputs
  - Calibration drift over time affecting cost curve accuracy

- First 3 experiments:
  1. Reproduce XGB training on available heart failure dataset; validate calibration curve and Brier score match reported values before implementing cost layer.
  2. Implement CIP curve computation for a simplified 2-cost-dimension scenario; verify cost composition percentages match manual calculation at sample thresholds.
  3. Test Agent I in isolation with 5 synthetic patients; systematically evaluate reliability for risk certainty assessment before chaining to downstream agents.

## Open Questions the Paper Calls Out
None

## Limitations
- Data access limitations prevent independent verification of model performance and feature engineering pipeline.
- LLM agent accuracy, particularly for speculative guidance, requires external validation beyond clinician evaluation.
- Cost matrix assumptions and weightings may not generalize across different healthcare systems or stakeholder perspectives.

## Confidence
- **High confidence**: Supervised gradient boosting performance (AUROC 0.804, AUPRC 0.529) - directly measurable and reported with standard metrics
- **Medium confidence**: CIP curve methodology - well-specified but requires careful cost matrix definition
- **Medium confidence**: LLM agent reliability for descriptive tasks - supported by structured evaluation but may not generalize
- **Low confidence**: LLM agent accuracy for speculative guidance - explicitly noted as unsatisfactory in clinician evaluation

## Next Checks
1. Validate model calibration on held-out data by plotting calibration curves and computing Brier score confidence intervals to ensure predicted probabilities match observed frequencies.
2. Conduct external validation of the full CAP pipeline on a different heart failure cohort from another healthcare system to assess generalizability of both prediction performance and cost matrix assumptions.
3. Perform systematic LLM agent evaluation using structured clinical vignettes covering edge cases (missing data, conflicting features, rare comorbidities) to quantify reliability beyond the original clinician assessment.