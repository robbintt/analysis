---
ver: rpa2
title: 'CRPO: Confidence-Reward Driven Preference Optimization for Machine Translation'
arxiv_id: '2501.13927'
source_url: https://arxiv.org/abs/2501.13927
tags:
- reward
- crpo
- sentence
- translation
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CRPO (Confidence-Reward driven Preference Optimization)
  for machine translation fine-tuning of large language models. The key idea is to
  select preference data pairs by jointly considering both reward differences and
  model confidence, rather than using reward alone as previous methods do.
---

# CRPO: Confidence-Reward Driven Preference Optimization for Machine Translation

## Quick Facts
- arXiv ID: 2501.13927
- Source URL: https://arxiv.org/abs/2501.13927
- Authors: Guofeng Cui; Pichao Wang; Yang Liu; Zemian Ke; Zhu Liu; Vimal Bhat
- Reference count: 33
- Primary result: CRPO outperforms RSO, RS-DPO, MBR score and Triplet dataset on 10 translation directions, achieving best average COMET and KIWI scores

## Executive Summary
This paper proposes CRPO (Confidence-Reward driven Preference Optimization) for machine translation fine-tuning of large language models. The key idea is to select preference data pairs by jointly considering both reward differences and model confidence, rather than using reward alone as previous methods do. CRPO+ and CRPO× are derived from loss change and loss value respectively, both incorporating reward and confidence terms to select challenging sentence pairs where the model is uncertain or underperforms. Experiments on 10 translation directions show CRPO outperforms existing methods in both translation accuracy and data efficiency, achieving the best average COMET and KIWI scores.

## Method Summary
CRPO is a data selection method for preference optimization that jointly considers reward differences and model confidence when selecting preference pairs. For each source sentence, the method samples K=64 candidate translations from a reference policy, scores them with a reward model (averaged XCOMET-XL and KIWI-XL), and computes log probabilities under the reference policy. Two variants are proposed: CRPO+ uses K·Δreward + Δlogprob with hyperparameter K=50 to balance scales, while CRPO× uses Δreward × Δlogprob to naturally balance terms through multiplication. The highest-reward sentence becomes the preferred output, paired with the sentence maximizing CR-Score among those with positive CR-Score. Fine-tuning uses standard DPO with an added SFT term (coefficient=1), optimized with β=0.1, lr=0.0001, batch_size=16, 1 epoch, max_length=512, and LoRA rank=16.

## Key Results
- CRPO outperforms RSO, RS-DPO, MBR score and Triplet dataset on 10 translation directions
- Achieves best average COMET and KIWI scores across all evaluated directions
- Demonstrates data efficiency by achieving better performance with fewer training examples
- Generalizes to encoder-decoder models like NLLB-1.3B with minimal modification

## Why This Works (Mechanism)

### Mechanism 1
Jointly considering reward differences and model confidence identifies more informative training pairs than reward alone. CRPO combines reward gaps (quality difference) with log-likelihood differences (model uncertainty). Pairs with high reward gaps but low model confidence create "hard negatives" that force the model to recalibrate its preferences. Core assumption: Model likelihood reflects genuine uncertainty that correlates with learning potential. Evidence: CRPO selects challenging sentence pairs where the model is uncertain or underperforms, leading to more effective learning. Break condition: If model's likelihood distribution is miscalibrated (e.g., systematically overconfident), confidence-based selection may amplify errors.

### Mechanism 2
Loss-value-guided selection (CR×) naturally balances reward and confidence scales through multiplication, eliminating manual hyperparameter tuning. CR× computes: [R(x, yw) - R(x, yl)] × [log πref(yl|x) - log πref(yw|x)]. The multiplication ensures both terms must be non-trivial for a high score. Core assumption: Reward and log-likelihood magnitudes are roughly comparable in scale. Evidence: CR× naturally balances the two terms through multiplication, eliminating the need for manual adjustments. Break condition: If rewards and log-likelihoods have orders-of-magnitude differences, multiplication may cause numerical instability.

### Mechanism 3
Selecting sentence pairs where the model incorrectly prefers lower-reward outputs creates higher-gradient learning signal than easy pairs. When log πref(yl|x) > log πref(yw|x) (model prefers the dispreferred output), the confidence term is positive. Combined with positive reward difference, CR-Score is maximized for pairs where the model is "wrong and confident." Core assumption: High-gradient examples transfer to better generalization rather than overfitting. Evidence: CRPO+ and CRPO× select preferred sentences with worse log πref and dispreferred sentences with higher log πref compared to RS-DPO. Break condition: If hard pairs contain systematic noise, the model may learn to reinforce incorrect patterns.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: CRPO is a data selection method specifically for DPO training; understanding the DPO loss is required to understand why CR+ and CR× are derived from it
  - Quick check question: Can you explain why DPO avoids training a separate reward model compared to RLHF?

- **Concept: Bradley-Terry Preference Model**
  - Why needed here: DPO derives its loss function from the Bradley-Terry model of pairwise preferences; CR×'s derivation modifies this assumption by introducing γ(·) weighting
  - Quick check question: What does the Bradley-Terry model assume about the relationship between preference probability and reward?

- **Concept: Rejection Sampling for Data Selection**
  - Why needed here: CRPO is positioned against RSO (statistical rejection sampling) and RS-DPO (threshold-based rejection); understanding these baselines clarifies what CRPO improves upon
  - Quick check question: In RS-DPO, what does the threshold η control, and what tradeoff does it introduce?

## Architecture Onboarding

- **Component map:** Candidate Generator -> Reward Scorer -> Likelihood Computer -> CR-Score Calculator -> Pair Selector -> DPO Trainer
- **Critical path:** Candidate generation → Reward scoring → Likelihood extraction → CR-Score computation → Pair selection → DPO training. The CR-Score computation is O(K²) per source sentence since all pairs are evaluated.
- **Design tradeoffs:**
  - CR+ vs CR×: CR+ requires tuning K to balance reward/confidence scales; CR× avoids this but may have numerical sensitivity
  - K (candidate count): Higher K increases selection quality but costs more sampling and scoring
  - Filtering negative CR-Scores: Removes contradictory pairs but may reduce data volume for difficult directions
- **Failure signatures:**
  - Empty preference dataset: CR-Score threshold too strict or reward model uncalibrated
  - No improvement over baseline: Confidence term may be uninformative if πref is already well-calibrated
  - Degraded performance: Reward model may have systematic errors that CRPO amplifies by selecting hard pairs
- **First 3 experiments:**
  1. Run CRPO on a single translation direction with K=16 candidates, verify CR-Score distribution is non-degenerate and selected pairs show expected pattern
  2. Compare CR+ with K∈{10, 25, 50, 100} against CR× on 2 translation directions; validate paper's claim that CR× reduces tuning burden
  3. Replicate MinMaxR (reward-only) vs MinMaxP (confidence-only) vs CRPO on one direction to verify both terms contribute

## Open Questions the Paper Calls Out
- The paper acknowledges that selecting only the maximum CR-Score per source may discard potentially high-quality data, suggesting "leveraging preset threshold, or CR-Score distribution analysis" as potential solutions.

## Limitations
- The effectiveness of CR× multiplication depends critically on the assumption that reward and log-likelihood magnitudes are roughly comparable in scale
- The selection of only one preferred and one dispreferred sentence per source (despite K=64 candidates) may miss richer preference structure
- The method's generalization to other preference optimization tasks beyond machine translation remains unexplored

## Confidence
- **High confidence**: Claims about CRPO outperforming RSO, RS-DPO, and MinMaxR baselines across all 10 translation directions
- **Medium confidence**: Claims about CR× eliminating hyperparameter tuning, supported by comparable performance to CR+ but lacking systematic hyperparameter sensitivity analysis
- **Medium confidence**: Claims about the mechanism (joint reward-confidence signals identify harder, more informative pairs), supported by qualitative analysis but limited ablation studies on individual term contributions

## Next Checks
1. Measure and compare the distributions of reward differences versus log-likelihood differences across multiple translation directions to verify whether multiplication in CR× is numerically appropriate or requires normalization
2. Compare the diversity of selected pairs between CRPO and baselines using metrics like n-gram overlap or embedding distance to determine if CRPO's selection creates more informative contrast
3. Replicate the main results using a different reward model (e.g., BLEURT instead of COMET/KIWI) to test whether CRPO's advantage is reward-model-dependent or more general