---
ver: rpa2
title: 'Measuring How LLMs Internalize Human Psychological Concepts: A preliminary
  analysis'
arxiv_id: '2506.23055'
source_url: https://arxiv.org/abs/2506.23055
tags:
- psychological
- similarity
- language
- questionnaires
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a framework to evaluate how well large language
  models (LLMs) internalize human psychological concepts by comparing LLM-generated
  similarity scores between questionnaire items to established psychological constructs.
  Using 43 standardized questionnaires and six language models (BERT, OpenAI embedding,
  GPT-3.5, GPT-4 variants), the approach measured classification accuracy and adjusted
  rand index (ARI) through hierarchical clustering of pairwise item similarities.
---

# Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis

## Quick Facts
- arXiv ID: 2506.23055
- Source URL: https://arxiv.org/abs/2506.23055
- Reference count: 23
- Large language models (GPT-4) can accurately classify psychological questionnaire items by their underlying constructs, demonstrating internalization of human psychological concepts.

## Executive Summary
This study introduces a framework to evaluate how well large language models internalize human psychological concepts by comparing LLM-generated similarity scores between questionnaire items to established psychological constructs. Using 43 standardized questionnaires and six language models (BERT, OpenAI embedding, GPT-3.5, GPT-4 variants), the approach measures classification accuracy and adjusted rand index (ARI) through hierarchical clustering of pairwise item similarities. GPT-4 achieved the highest accuracy (66.2%, ARI 0.341), significantly outperforming GPT-3.5 (55.9%) and BERT (48.1%), all exceeding random baselines. The framework also demonstrated that LLM-estimated semantic similarities correlate with human response patterns across multiple psychological constructs, validating its utility for measuring human-LLM concept alignment and identifying representational biases.

## Method Summary
The framework evaluates LLM internalization of psychological concepts by computing pairwise similarity scores between questionnaire items using either static embeddings (BERT, OpenAI) or prompted similarity ratings (GPT models). For each questionnaire with expert-labeled sub-categories, a pairwise similarity matrix is constructed and subjected to hierarchical clustering with a predetermined number of clusters matching the expert labels. Classification accuracy and Adjusted Rand Index (ARI) measure agreement between LLM-derived clusters and expert categories. Cross-questionnaire analysis aggregates similarities to predict Pearson correlations between human response patterns across constructs.

## Key Results
- GPT-4 achieved the highest classification accuracy at 66.2% and ARI of 0.341, significantly outperforming GPT-3.5 (55.9%) and BERT (48.1%), all exceeding random baseline of 31.9%
- Continuous and discrete prompting approaches showed no significant performance difference (p=0.45, p=0.69)
- LLM-estimated semantic similarities between questionnaires correlated significantly with human response correlations (r=0.412, p<0.05 for mean scores; r=0.731, p<1e-5 for median scores)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated pairwise semantic similarity scores can proxy psychological construct membership when clustered hierarchically.
- Mechanism: Language models compute similarity scores between questionnaire items (via embeddings or prompted ratings). Hierarchical clustering applied to the resulting similarity matrices reconstructs expert-defined category structures with measurable accuracy above random baseline.
- Core assumption: If LLMs have internalized psychological concepts, items measuring the same construct will have higher pairwise similarity than items from different constructs, producing cluster boundaries that align with expert labels.
- Evidence anchors:
  - [abstract] "Our method evaluates how accurately language models reconstruct and classify questionnaire items through pairwise similarity analysis. We compared resulting cluster structures with the original categorical labels using hierarchical clustering."
  - [section 4.1] "GPT-4(1106) achieved the highest accuracy at 66.1% and ARI, while BERT showed the lowest at 48.1% above the random baseline 31.9%."
  - [corpus] Weak direct corpus support; neighbor papers address concept-based explanations generally (ConSim, CBM leakage) but not this specific clustering-similarity mechanism.
- Break condition: If LLM similarity scores were purely surface-level lexical overlap without conceptual structure, clustering would not exceed baseline; or if prompts induced task-specific reasoning rather than revealing internalized representations.

### Mechanism 2
- Claim: Semantic similarity estimates between items from different questionnaires correlate with human response correlation patterns across constructs.
- Mechanism: For multi-questionnaire datasets with known human inter-construct correlations, LLMs compute cross-questionnaire item similarities. Aggregated (mean/median) similarity between questionnaire pairs predicts Pearson correlations observed in human respondent data.
- Core assumption: LLMs trained on natural language acquire relational structure between psychological constructs that partially mirrors covariation in human responses.
- Evidence anchors:
  - [section 4.2] "The results showed statistical significance with mean (r=0.412, p<0.05) and median scores (r=0.731, p<1e-5)."
  - [section 3.3] Describes method for computing item-by-item similarity across questionnaires and comparing to human correlation matrices.
  - [corpus] "Emergence of Hierarchical Emotion Organization in Large Language Models" (arxiv:2507.10599) provides convergent evidence that LLMs model emotional state structures hierarchically, supporting the broader claim of acquired psychological structure.
- Break condition: If LLM similarity reflected only superficial topic overlap rather than construct-level relationships, cross-questionnaire similarity would not predict human response correlations.

### Mechanism 3
- Claim: Model scale and capability predict degree of psychological concept alignment.
- Mechanism: Larger autoregressive models (GPT-4 > GPT-3.5 > BERT) show progressively higher classification accuracy and ARI, suggesting training scale and architecture affect concept internalization depth.
- Core assumption: More capable models trained on larger corpora encode finer-grained semantic relationships relevant to psychological constructs.
- Evidence anchors:
  - [section 4.1] "Our analysis showed statistically significant higher classification performance with GPT-4 (0613/1106) (p < 0.001)."
  - [section 4.1] GPT-4 models achieved 63.9% accuracy vs GPT-3.5's 56.7%; embedding model was comparable to GPT-4.
  - [corpus] "From Words to Waves" (arxiv:2506.01133) suggests foundation models trained on text "internalize abstract semantic concepts," consistent with scale-dependent acquisition.
- Break condition: If performance differences were driven by prompt-following ability rather than representation quality, smaller models with better instruction tuning might match larger models—which was not observed.

## Foundational Learning

- Concept: **Hierarchical clustering with predetermined cluster count**
  - Why needed here: The framework clusters pairwise similarity matrices into a fixed number of categories (matching questionnaire sub-scale counts) and compares to expert labels.
  - Quick check question: Given a 10-item questionnaire with 2 known sub-scales, can you explain why hierarchical clustering with k=2 would be applied to the similarity matrix?

- Concept: **Adjusted Rand Index (ARI)**
  - Why needed here: ARI measures agreement between clustering outputs and ground-truth labels, correcting for chance agreement—critical when cluster sizes vary.
  - Quick check question: Why is ARI preferred over raw classification accuracy when evaluating clustering against known categories?

- Concept: **Embedding similarity vs. prompted similarity elicitation**
  - Why needed here: The study compares cosine similarity on static embeddings (BERT, OpenAI) to prompted similarity ratings from autoregressive models (GPT-3.5, GPT-4).
  - Quick check question: What is the fundamental difference between extracting similarity from embeddings versus eliciting it via prompts?

## Architecture Onboarding

- Component map:
  1. Questionnaire database -> Similarity computation module -> Similarity matrix construction -> Clustering module -> Evaluation layer

- Critical path:
  1. Select questionnaire → extract items + expert labels
  2. Generate all pairwise item similarity scores (N×N matrix)
  3. Apply hierarchical clustering with k = number of expert sub-categories
  4. Compare cluster assignments to expert labels → accuracy, ARI
  5. (Optional) Cross-questionnaire analysis: aggregate similarities, correlate with human response correlations

- Design tradeoffs:
  - **Embedding vs. prompted similarity**: Embeddings are deterministic and fast; prompts allow contextualized judgments but introduce variance and cost.
  - **Continuous vs. discrete prompts**: Paper found no significant difference (p=0.45, p=0.69), suggesting either is viable.
  - **Item ordering**: Some questionnaires showed significant ordering effects (WDGOI, FFMQ-24); counterbalancing recommended.

- Failure signatures:
  - Classification accuracy near random baseline (31.9%) → model lacks relevant concept representations or prompt is misaligned.
  - High variance across orderings → sensitivity to sequence position; implement order randomization.
  - Cross-questionnaire similarity uncorrelated with human data (r ≈ 0) → LLM not capturing construct-level relationships.

- First 3 experiments:
  1. Replicate the baseline: Run pairwise similarity + hierarchical clustering on 5–10 questionnaires with GPT-4 and BERT; verify accuracy ordering (GPT-4 > BERT > baseline).
  2. Ablate prompt design: Compare continuous [-1,1] vs. discrete (1–9) prompts on same questionnaires; confirm no significant difference per paper findings.
  3. Test ordering sensitivity: Run same questionnaire (e.g., WDGOI or FFMQ-24) in normal vs. reversed item order; check for significant accuracy differences as reported.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs embed plural semantic structures that reflect cultural variations in psychological concept representation?
- Basis in paper: [explicit] Authors state "It is interesting to check whether LLMs embed such plural semantic structures" when discussing cultural universality and variations in human semantics.
- Why unresolved: Only English questionnaires were tested; no cross-cultural or multilingual comparison was conducted.
- What evidence would resolve it: Apply the framework to validated translations of the same psychological questionnaires across multiple cultures and compare LLM-derived similarity structures.

### Open Question 2
- Question: Will the LLM-human semantic similarity correlation replicate with unpublished psychological questionnaires not in training data?
- Basis in paper: [explicit] Authors state "We plan to validate with our proposed method with unpublished data."
- Why unresolved: Current validation may reflect memorization of known questionnaires rather than genuine concept internalization.
- What evidence would resolve it: Test the framework on newly developed or unpublished psychological scales and compare LLM similarity estimates to fresh human response data.

### Open Question 3
- Question: What mechanisms cause item ordering effects in some questionnaires (WDGOI, FFMQ-24) but not others?
- Basis in paper: [inferred] Ordering effects were statistically significant in 2 of 8 tested questionnaires, but the paper does not explain why these specific questionnaires are affected.
- Why unresolved: The source of ordering sensitivity (prompt structure, item content, construct type) remains unidentified.
- What evidence would resolve it: Systematic manipulation of item characteristics (length, semantic complexity, construct type) to identify predictors of ordering sensitivity.

## Limitations
- The study relies on a specific set of 43 questionnaires without providing the complete dataset, limiting reproducibility
- Cross-questionnaire analysis depends on external human correlation data not fully described in the paper
- Item ordering effects were significant for only 2 out of 8 tested questionnaires, suggesting potential instability in the framework

## Confidence
- **High confidence**: GPT-4 outperforms other models in classification accuracy and ARI; continuous and discrete prompting approaches show no significant performance difference
- **Medium confidence**: Framework successfully measures LLM-human concept alignment; model scale correlates with concept internalization
- **Low confidence**: Cross-questionnaire semantic similarity reliably predicts human response patterns; item ordering effects are generalizable

## Next Checks
1. Replicate the study with a publicly available psychological questionnaire dataset to verify the framework's robustness
2. Test the cross-questionnaire correlation hypothesis using multiple human response datasets to validate construct-level relationships
3. Evaluate the framework's performance on questionnaires with varying numbers of sub-categories (beyond the 2-6 range used in the study)