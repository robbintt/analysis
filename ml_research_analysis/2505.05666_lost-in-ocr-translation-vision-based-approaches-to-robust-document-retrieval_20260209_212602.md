---
ver: rpa2
title: Lost in OCR Translation? Vision-Based Approaches to Robust Document Retrieval
arxiv_id: '2505.05666'
source_url: https://arxiv.org/abs/2505.05666
tags:
- document
- retrieval
- documents
- ocr-based
- pipeline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically compares vision-based (ColPali) and OCR-based
  (Llama 3.2 90B + Nougat) RAG pipelines across varying document qualities using a
  newly curated DocDeg dataset. While vision-based RAG shows strong performance on
  documents it was fine-tuned on, OCR-based RAG generalizes better to unseen documents
  of varying quality.
---

# Lost in OCR Translation? Vision-Based Approaches to Robust Document Retrieval

## Quick Facts
- **arXiv ID:** 2505.05666
- **Source URL:** https://arxiv.org/abs/2505.05666
- **Reference count:** 23
- **Primary result:** OCR-based RAG pipelines generalize better to unseen degraded documents than vision-based approaches

## Executive Summary
This study systematically compares vision-based (ColPali) and OCR-based (Llama 3.2 90B + Nougat) RAG pipelines across varying document qualities using a newly curated DocDeg dataset. While vision-based RAG shows strong performance on documents it was fine-tuned on, OCR-based RAG generalizes better to unseen documents of varying quality. The OCR-based pipeline with Llama 3.2 OCR achieved superior retrieval metrics (MRR 0.4852 vs 0.2471) and semantic answer accuracy across all degradation levels. Despite VLM's advantages in computational efficiency and memory usage, OCR-based approaches deliver better end-to-end performance in realistic document settings, highlighting a clear trade-off between accuracy and resource utilization.

## Method Summary
The study compares two RAG architectures: a vision-based ColPali pipeline using ColQwen2 embeddings and an OCR-based pipeline using Llama 3.2 90B for OCR followed by Qwen2 embeddings. Both systems use the same query embedding model (Qwen2-7B-Instruct) and evaluation metrics. The DocDeg dataset contains 4,196 OSTI documents manually annotated into four degradation levels (0=digital native, 3=severely degraded), with 10 Q&A pairs per document generated via Llama 3.3 70B. Retrieval performance is evaluated using MRR, Recall@5, and NDCG@5, while semantic accuracy uses Exact Match, BLEU, ROUGE-1, and ROUGE-L metrics.

## Key Results
- OCR-based RAG (Llama 3.2 + Qwen2) achieved MRR of 0.4852 versus VLM-based ColPali's 0.2471 across all degradation levels
- Llama 3.2 OCR engine consistently outperformed Nougat OCR across all distortion levels
- VLM pipeline showed 1.38 GB memory usage per 1,000 documents versus 9.5 GB for OCR pipeline, but had conflicting latency characteristics
- Retrieval accuracy gap widened at higher degradation levels, with OCR maintaining superior performance even on severely degraded documents

## Why This Works (Mechanism)

### Mechanism 1: Semantic Decoupling via Large-Scale OCR
OCR-based RAG pipelines generalize better to degraded documents because a robust OCR model acts as a semantic filter, stripping away visual noise before embedding. The pipeline uses Llama 3.2 90B to transcribe visual data into text, converting variable visual noise into standardized tokens. The subsequent embedding model processes a cleaner semantic signal compared to a VLM that must encode noise and content simultaneously. This mechanism breaks if the OCR model hallucinates or fails completely on severe degradation (Level 3).

### Mechanism 2: Visual Feature Brittleness in Zero-Shot VLMs
Vision-based RAG (ColPali) fails to generalize to unseen degradation types because it relies on direct visual patch embeddings which are sensitive to domain shifts in image quality. Without fine-tuning on specific noise distributions, the VLM cannot distinguish between "relevant visual features" and "visual noise" (e.g., scan lines, coffee stains), leading to lower retrieval scores. This mechanism breaks if the VLM is fine-tuned on the specific degradation patterns.

### Mechanism 3: The Accuracy-Efficiency Trade-off
VLM-based pipelines offer reduced memory footprints and indexing times by bypassing OCR, but this efficiency introduces higher query-time latency or accuracy trade-offs. ColPali processes images directly, requiring matching query embeddings against multiple image patches per document, which increases latency compared to single-vector text matching. This mechanism breaks when scaling to massive document collections where the VLM's multi-vector storage becomes prohibitive.

## Foundational Learning

### Concept: Late Interaction (ColBERT)
Why needed: ColPali adapts this for images. Understanding that similarity is calculated by matching every query token to every image patch (and taking the max) is crucial for diagnosing why VLM query times might lag behind single-vector OCR methods.
Quick check: Does the model compare one vector per document or a matrix of vectors per document?

### Concept: Visual Degradation Taxonomy
Why needed: The study relies on manual classification of documents (Level 0: Digital to Level 3: Severely Degraded). Evaluating RAG performance requires understanding which level breaks the OCR or VLM components.
Quick check: Can the system distinguish between a "bad scan" (noise) and a "complex layout" (semantic structure)?

### Concept: Semantic vs. Lexical Evaluation
Why needed: The paper argues against pure retrieval metrics alone, introducing semantic answer evaluation (BLEU, ROUGE).
Quick check: If the OCR outputs "l0ve" instead of "love", does the retrieval metric penalize it more than the semantic metric?

## Architecture Onboarding

**Component map:**
OCR Pipeline: Doc Image -> Llama 3.2 90B (OCR) -> Text -> Qwen2 (Embedder) -> Vector DB
VLM Pipeline: Doc Image -> PaliGemma (Visual Encoder) -> Patch Embeddings -> Multi-Vector DB
Query Side: User Query -> Qwen2 (Text Embedder) -> [Match against OCR Text OR Match against VLM Patches]

**Critical path:** OCR Extraction & Cleaning. The quality of the OCR stage (Llama 3.2 vs. Nougat) is the dominant factor for accuracy. Onboarding should focus on tuning OCR parameters for worst-case degradation levels (Level 2/3).

**Design tradeoffs:**
- Accuracy vs. Compute: OCR (Llama 90B) is computationally heavy at ingestion but yields high accuracy (MRR ~0.48)
- Latency vs. Robustness: VLM is memory-efficient but struggles with unseen noise (MRR ~0.24) and has conflicting reports on query latency
- Generalization: OCR pipelines generalize better "out of the box" to new document types; VLMs require fine-tuning on specific visual domain

**Failure signatures:**
- VLM Failure: High retrieval scores on training data but sudden drop to ~0.20 MRR on wild data (DocDeg Level 2/3)
- OCR Failure: Semantic scores (ROUGE) decoupling from retrieval scores, indicating OCR transcription is correct but answer extraction is failing

**First 3 experiments:**
1. DocDeg Baseline: Run OCR (Llama 3.2) vs. VLM (ColPali) comparison specifically on Level 3 documents to confirm the generalization gap
2. VLM Fine-Tuning: Fine-tune ColPali on a subset of DocDeg (Level 1 & 2) to test if VLM can close the accuracy gap with OCR pipeline
3. Latency Stress Test: Measure query latency for both systems as corpus scales from 1k to 100k documents to validate conflicting claims regarding query speed

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance comparison shift when evaluating fine-tuned OCR-based RAG pipelines against fine-tuned VLM-based pipelines that utilize an integrated vision-language QA model? The current study primarily evaluates "out-of-the-box" performance on DocDeg to simulate realistic deployments where fine-tuning is often impractical. An experiment fine-tuning both components on the same domain-specific degradation data would resolve this.

### Open Question 2
Can architectural optimizations be developed to mitigate the query latency and storage costs of late-interaction mechanisms, thereby enabling the use of larger VLMs (e.g., 90B parameters) for visual retrieval? The current ColPali architecture relies on storing and querying multiple dense vector embeddings per document, which becomes prohibitively expensive as model size increases. A study demonstrating a late-interaction system using 90B parameter models with comparable latency would resolve this.

### Open Question 3
Would the OCR-based pipeline retain its superiority if constrained to a parameter count comparable to the VLM baseline (e.g., 7B parameters)? The study compares a 7B parameter VLM against a 90B parameter OCR model, leaving unclear if superiority stems from architecture or model scale. A controlled ablation study using OCR models of varying sizes would isolate this contribution.

## Limitations
- DocDeg dataset consists entirely of scientific documents from a single source (OSTI), limiting cross-domain generalizability
- Conflicting latency claims between abstract (VLM slower) and conclusion (VLM faster) suggest measurement condition differences
- Fine-tuning experiments on DocDeg degradation patterns would strengthen conclusions about when vision-based approaches might catch up

## Confidence

**High:** OCR-based RAG generalizes better to unseen degraded documents; Llama 3.2 OCR outperforms Nougat across all degradation levels

**Medium:** Vision-based RAG struggles without fine-tuning on target document types; efficiency-accuracy trade-off is clear but latency measurements need clarification

**Low:** Claims about computational efficiency advantages of VLM without fine-tuning requirements; resource usage comparisons at scale remain unverified

## Next Checks

1. **Fine-tuning Experiment:** Train ColPali on Level 1-2 DocDeg documents and re-evaluate on Level 3 to test if VLM performance gap closes with domain adaptation

2. **Cross-Domain Test:** Apply both pipelines to non-scientific documents (legal, financial, historical) with similar degradation levels to verify generalization claims beyond OSTI corpus

3. **Scaling Analysis:** Measure memory and latency for both systems as corpus size increases from 1k to 100k documents to validate efficiency trade-offs at realistic scale