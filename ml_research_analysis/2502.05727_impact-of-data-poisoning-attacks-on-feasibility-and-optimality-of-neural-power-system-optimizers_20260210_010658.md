---
ver: rpa2
title: Impact of Data Poisoning Attacks on Feasibility and Optimality of Neural Power
  System Optimizers
arxiv_id: '2502.05727'
source_url: https://arxiv.org/abs/2502.05727
tags:
- data
- poisoning
- power
- attacks
- feasibility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the impact of data poisoning attacks on machine
  learning-based optimization proxies used for solving the DC Optimal Power Flow (OPF)
  problem in power systems. The authors implement a white-box poisoning attack that
  manipulates training data to maximize prediction errors, aiming to increase generation
  output recommendations and disrupt supply-demand balance.
---

# Impact of Data Poisoning Attacks on Feasibility and Optimality of Neural Power System Optimizers

## Quick Facts
- arXiv ID: 2502.05727
- Source URL: https://arxiv.org/abs/2502.05727
- Authors: Nora Agah; Meiyi Li; Javad Mohammadi
- Reference count: 22
- Primary result: Data poisoning attacks significantly degrade neural optimization proxies for DC-OPF, increasing optimality errors by 3-19× and causing 30-33 constraint violations per output point

## Executive Summary
This paper investigates how data poisoning attacks affect neural network-based optimization proxies used for DC Optimal Power Flow (OPF) problems in power systems. The authors implement a white-box poisoning attack that manipulates training data to maximize prediction errors and disrupt supply-demand balance by increasing generation output recommendations. Three different optimization proxy methods are evaluated: a penalty-based approach, the DC3 post-repair method, and the LOOP-LC direct mapping approach. The attack significantly degrades both optimality and feasibility across all methods, with LOOP-LC maintaining feasibility due to its hard constraint enforcement.

## Method Summary
The authors implement a white-box data poisoning attack that manipulates training data to maximize prediction errors in neural optimization proxies. The attack specifically targets DC-OPF problems by increasing generation output recommendations to disrupt supply-demand balance. Three optimization proxy methods are evaluated: penalty-based approach, DC3 post-repair method, and LOOP-LC direct mapping approach. The attack's effectiveness is measured by increases in optimality errors and constraint violations across all methods, with LOOP-LC showing greater resilience due to its built-in feasibility guarantees and iterative error correction mechanisms.

## Key Results
- Optimality error increases by factors of 3-19 across all tested methods under poisoning attack
- Constraint violations increase to 30-33 per output point when attack is successful
- LOOP-LC maintains feasibility through hard constraint enforcement despite attack
- All three proxy methods show vulnerability to data poisoning attacks, with varying degrees of resilience

## Why This Works (Mechanism)
The attack works by manipulating training data to maximize prediction errors in neural optimization proxies. By increasing generation output recommendations in the poisoned training data, the attack forces the neural network to learn incorrect mappings that prioritize higher generation values. This disruption of the supply-demand balance in the learned model causes significant increases in both optimality errors and constraint violations when the poisoned model is deployed. The white-box nature of the attack allows the adversary to have complete knowledge of the optimization architecture and training process, enabling precise targeting of vulnerabilities in each proxy method.

## Foundational Learning
- DC Optimal Power Flow (DC-OPF): A linearized version of AC power flow used for grid optimization; needed to understand the specific optimization problem being attacked
- Neural Optimization Proxies: Machine learning models that approximate optimization solutions; critical for understanding how ML can replace traditional solvers
- Data Poisoning Attacks: Adversarial techniques that manipulate training data to compromise model performance; fundamental to the attack methodology
- Feasibility vs Optimality Trade-offs: The balance between satisfying constraints and minimizing cost; essential for evaluating attack impact
- White-box vs Black-box Attacks: Different levels of attacker knowledge; important for understanding attack scenarios
- Constraint Enforcement Mechanisms: Methods to ensure solutions satisfy physical constraints; key to understanding resilience differences between proxy methods

## Architecture Onboarding

**Component Map:**
Training Data -> Poisoning Attack -> Neural Network Architecture -> Optimization Proxy Method -> Output Evaluation

**Critical Path:**
Poisoned training data → Neural network training → Proxy method application → Constraint violation and optimality error measurement

**Design Tradeoffs:**
- Hard constraint enforcement (LOOP-LC) vs soft penalty methods
- Direct mapping vs iterative repair approaches
- Model complexity vs training data requirements
- Attack resilience vs computational efficiency

**Failure Signatures:**
- Increased constraint violations (30-33 per output point)
- Degraded optimality (3-19× error increase)
- Supply-demand imbalance in generation recommendations
- Loss of feasibility guarantees in penalty-based methods

**3 First Experiments:**
1. Test attack effectiveness on different DC-OPF test cases with varying grid sizes
2. Compare white-box attack performance against black-box and gray-box scenarios
3. Evaluate defense mechanisms including adversarial training and input sanitization

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on synthetic datasets rather than real-world power system data
- White-box attack scenarios may not represent realistic adversarial conditions with limited system knowledge
- Attack methodology specific to DC-OPF may not translate to AC-OPF formulations
- Limited evaluation to only three proxy methods, potentially missing other resilient architectures

## Confidence
- Core findings regarding attack effectiveness: High confidence
- Comparative analysis of proxy architectures: Medium confidence
- Generalizability to real-world power system operations: Low confidence

## Next Checks
1. Evaluate attack transferability from synthetic to real-world power system datasets with actual grid topology
2. Test robustness against black-box and gray-box attack scenarios with limited system knowledge
3. Investigate defense mechanisms including adversarial training, input sanitization, and ensemble methods to quantify mitigation strategies