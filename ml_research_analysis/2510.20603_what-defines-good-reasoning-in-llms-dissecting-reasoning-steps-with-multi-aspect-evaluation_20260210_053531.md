---
ver: rpa2
title: What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect
  Evaluation
arxiv_id: '2510.20603'
source_url: https://arxiv.org/abs/2510.20603
tags:
- reasoning
- case
- evaluation
- step
- relevance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a multi-aspect evaluation framework for large
  language model reasoning, decomposing reasoning quality into relevance and coherence.
  Relevance assesses if a step is grounded in and addresses the problem, while coherence
  measures if it logically follows from prior steps.
---

# What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect Evaluation

## Quick Facts
- arXiv ID: 2510.20603
- Source URL: https://arxiv.org/abs/2510.20603
- Reference count: 9
- Introduces multi-aspect evaluation of LLM reasoning via relevance and coherence, validated with Causal Stepwise Evaluation (CaSE).

## Executive Summary
This paper proposes a novel multi-aspect framework for evaluating reasoning quality in large language models, decomposing reasoning into relevance (grounding to the problem) and coherence (logical consistency with prior steps). To address hindsight bias in step-level evaluation, the authors introduce Causal Stepwise Evaluation (CaSE), which assesses each step using only its preceding context. Experiments on newly constructed benchmarks (MRa-GSM8K, MRa-MATH) show that CaSE achieves stronger agreement with human judgments than whole-trace baselines. The framework is further validated through improved downstream task performance via SFT curation and inference-time guidance, demonstrating that measuring and optimizing beyond step correctness yields more robust reasoning models.

## Method Summary
The paper introduces a multi-aspect evaluation framework for LLM reasoning quality, focusing on two dimensions: relevance (whether a step is grounded in and addresses the problem) and coherence (whether it logically follows from prior steps). To avoid hindsight bias, the authors propose Causal Stepwise Evaluation (CaSE), which evaluates each step using only the question and preceding context, rather than the full solution. They construct two new expert-annotated benchmarks (MRa-GSM8K, MRa-MATH) and show that CaSE achieves stronger agreement with human judgments than whole-trace baselines. The framework is validated through downstream applications: SFT data curation by filtering low-quality steps or samples, and inference-time guidance emphasizing these aspects, both leading to improved problem-solving accuracy.

## Key Results
- CaSE achieves stronger agreement with human judgments than whole-trace baselines on newly constructed MRa-GSM8K and MRa-MATH benchmarks.
- Curating SFT training data using CaSE-evaluated relevance and coherence directly improves downstream task performance (MATH, GPQA, AIME24/25).
- Inference-time guidance based on CaSE-scored aspects enhances reasoning accuracy, showing benefits beyond step correctness evaluation.

## Why This Works (Mechanism)
The paper addresses hindsight bias in step-level evaluation by restricting the evaluation context to only preceding steps, preventing the model from using future steps to justify current ones. This design enforces genuine causal reasoning and yields more accurate assessments of step quality. By explicitly measuring relevance and coherence as distinct aspects, the framework provides a more granular and actionable view of reasoning quality than binary correctness, enabling targeted improvements in both model training and inference.

## Foundational Learning
- **Relevance**: A step's alignment with and grounding to the problem statement; needed to ensure solutions address the actual question, not just proceed logically.
- **Coherence**: Logical consistency between a step and its immediate predecessors; needed to ensure reasoning flows naturally and avoids gaps or leaps.
- **Hindsight Bias**: The tendency to judge past steps favorably by knowing the correct outcome; CaSE mitigates this by limiting context to preceding steps.
- **Stepwise Evaluation**: Assessing each reasoning step individually rather than the entire trace; enables pinpointing weaknesses and targeted improvement.
- **Multi-Aspect Evaluation**: Evaluating reasoning along multiple dimensions (relevance, coherence) rather than a single correctness score; yields richer, more actionable feedback.
- **Causal Context**: The set of preceding steps available for evaluating a current step; ensures evaluation respects the temporal and logical order of reasoning.

## Architecture Onboarding

**Component Map**: CaSE evaluator (LLM judge) -> Relevance/Coherence scoring -> Downstream SFT filtering / Inference guidance

**Critical Path**: Step-wise evaluation (CaSE) -> Multi-aspect scores (relevance, coherence) -> Curated data or guided inference -> Improved reasoning performance

**Design Tradeoffs**: Using only preceding context (CaSE) reduces hindsight bias but may miss global context; multi-aspect evaluation is more informative but requires more complex annotation and evaluation; step-level filtering risks data scarcity if too strict.

**Failure Signatures**: Low agreement with human judgments indicates poor prompt design or model capability; no downstream gains suggest filtering is too aggressive or not aligned with task demands; inconsistent step segmentation undermines evaluation reliability.

**First Experiments**:
1. Pilot CaSE evaluator on a small held-out set to calibrate prompts and verify step segmentation.
2. Ablation comparing CaSE evaluation using different judge models (e.g., GPT-4 vs. Qwen2.5).
3. Test CaSE-filtered SFT models on a held-out, unseen reasoning task to evaluate generalization.

## Open Questions the Paper Calls Out
### Open Question 1
Can the reliability of multi-aspect evaluation be improved for high-complexity tasks where current agreement scores are lower? Current models struggle to consistently assess nuanced dimensions like coherence in difficult mathematical contexts compared to simple correctness. Evidence would come from evaluator training strategies achieving significantly higher alignment with human experts on MRa-MATH.

### Open Question 2
Does the CaSE framework generalize effectively to non-mathematical reasoning domains such as coding or logical deduction? Mathematical reasoning has distinct structural properties that may facilitate stepwise decomposition differently than open-ended tasks. Evidence would come from applying CaSE to code generation or multi-hop QA benchmarks with consistent human judgment alignment.

### Open Question 3
How effective is CaSE when used directly as a dense reward signal for reinforcement learning (RL)? While CaSE improves SFT data, its utility as a stable process reward signal during the high-variance optimization of RL remains untested. Evidence would come from training an LLM with RL using CaSE-based rewards and measuring reasoning improvements.

## Limitations
- Prompt design for the CaSE evaluator is not fully specified, affecting reproducibility.
- Step segmentation procedure is unclear, complicating consistent annotation and evaluation.
- Results are validated only on mathematical reasoning tasks, limiting generalizability to other domains.
- SFT and inference guidance experiments are confined to math and science benchmarks; scaling effects across model sizes are not deeply explored.

## Confidence
- **High**: CaSE reduces hindsight bias by design and outperforms whole-trace baselines on MR-GSM8K/MR-MATH.
- **Medium**: Relevance and coherence filtering in SFT improves task accuracy; effects are consistent but not tested on broader domains.
- **Medium**: Inference-time guidance using CaSE-scored aspects improves reasoning, but results are confined to math benchmarks.

## Next Checks
1. Conduct a small pilot annotation with held-out samples to calibrate judge model prompts and verify step segmentation logic.
2. Perform an ablation study comparing CaSE evaluation using different judge models (e.g., GPT-4 vs. Qwen2.5) to assess robustness.
3. Test filtered SFT models on a held-out, unseen reasoning task (e.g., logical puzzles or commonsense QA) to evaluate generalization beyond math benchmarks.