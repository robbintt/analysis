---
ver: rpa2
title: Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning
arxiv_id: '2601.21919'
source_url: https://arxiv.org/abs/2601.21919
tags:
- reasoning
- length
- arxiv
- scma
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of redundant reasoning in Large
  Reasoning Models (LRMs), which increases inference latency and hampers deployment
  efficiency. The core method introduces Self-Compression via Multi-Agent Reinforcement
  Learning (SCMA), a framework that decomposes the reasoning process into logical
  chunks using specialized Segmentation and Scoring agents, which collaboratively
  define an importance-weighted length penalty to guide a Reasoning agent.
---

# Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.21919
- Source URL: https://arxiv.org/abs/2601.21919
- Reference count: 25
- Reduces reasoning length by 11.1%–39.0% while improving accuracy by 4.33%–10.02%

## Executive Summary
This paper tackles the problem of redundant reasoning steps in Large Reasoning Models (LRMs), which increases inference latency and deployment costs. The authors introduce Self-Compression via Multi-Agent Reinforcement Learning (SCMA), a novel framework that decomposes reasoning into logical chunks and selectively penalizes redundancy without adding inference overhead. SCMA outperforms traditional length-penalized RL baselines by 4.33%–10.02% in accuracy while reducing response length by 11.1%–39.0% across multiple benchmarks and model scales.

## Method Summary
SCMA employs a Multi-Agent Reinforcement Learning (MARL) framework with three specialized agents: a Reasoning agent that generates thought chains, a Segmentation agent that decomposes reasoning into logical chunks, and a Scoring agent that evaluates chunk importance. The Segmentation agent uses a sliding window approach with a threshold-based criterion to split reasoning into meaningful segments. The Scoring agent applies a weighted aggregation of chunk importance scores to determine redundancy. These agents collaboratively define an importance-weighted length penalty that guides the Reasoning agent to generate concise yet complete reasoning paths. This approach differs from traditional RLHF by avoiding direct reference-based rewards and instead focusing on chunk-level importance scoring.

## Key Results
- Achieves 11.1%–39.0% reduction in reasoning length across benchmarks
- Improves accuracy by 4.33%–10.02% compared to length-penalized RL baselines
- Maintains or improves performance while reducing inference latency

## Why This Works (Mechanism)
SCMA works by addressing the core problem of redundancy in reasoning chains through a multi-agent cooperative approach. The Segmentation agent identifies logical boundaries in reasoning, while the Scoring agent evaluates the importance of each chunk relative to the final answer. This enables selective penalization of redundant segments without compromising essential logic. The importance-weighted length penalty creates a more nuanced optimization objective than simple length penalties, allowing the model to retain crucial reasoning steps while eliminating unnecessary ones.

## Foundational Learning

**Multi-Agent Reinforcement Learning (MARL)**: Multiple agents coordinate to solve a shared task through communication and policy optimization. Needed because reasoning compression requires different perspectives (segmentation vs scoring). Quick check: Verify agents have distinct reward functions and observation spaces.

**Chunk-based reasoning decomposition**: Breaking down reasoning chains into meaningful logical segments. Needed to identify what constitutes redundancy versus essential reasoning. Quick check: Ensure chunk boundaries align with logical transitions in reasoning.

**Importance-weighted length penalty**: A scoring mechanism that penalizes length based on chunk importance rather than raw token count. Needed to preserve essential reasoning while compressing. Quick check: Verify the penalty function correctly weights important chunks lower than redundant ones.

## Architecture Onboarding

**Component Map**: Segmentation Agent -> Scoring Agent -> Reasoning Agent -> Output
- Segmentation Agent: Identifies logical chunk boundaries
- Scoring Agent: Evaluates chunk importance
- Reasoning Agent: Generates compressed reasoning chains
- Output: Final compressed reasoning

**Critical Path**: The optimization loop where Segmentation and Scoring agents collaboratively define the importance-weighted penalty that guides the Reasoning agent's policy updates.

**Design Tradeoffs**: SCMA trades increased training complexity (multiple agents) for zero inference overhead, unlike methods that require additional computation during deployment. The chunk-based approach balances between over-compression and under-compression.

**Failure Signatures**: If the Segmentation agent over-segments, reasoning becomes fragmented; if under-segments, redundancy persists. If the Scoring agent misidentifies importance, essential logic may be lost. Monitor for reasoning quality degradation or incomplete logical chains.

**First Experiments**:
1. Validate chunk segmentation quality on a small reasoning dataset
2. Test scoring accuracy by comparing predicted importance to human annotations
3. Evaluate reasoning compression ratio on a held-out validation set

## Open Questions the Paper Calls Out
None

## Limitations
- Requires extensive training data for all three agents
- Performance depends on the quality of chunk segmentation
- May not generalize well to domains with highly specialized reasoning patterns

## Confidence
- Length reduction claims: High
- Accuracy improvement claims: High
- Zero inference overhead claim: High
- Multi-agent coordination effectiveness: Medium

## Next Checks
1. Validate chunk segmentation quality on a diverse set of reasoning tasks
2. Test performance degradation when agents are trained with reduced data
3. Evaluate cross-domain generalization by testing on reasoning tasks outside the training distribution