---
ver: rpa2
title: Open-Source Retrieval Augmented Generation Framework for Retrieving Accurate
  Medication Insights from Formularies for African Healthcare Workers
arxiv_id: '2502.15722'
source_url: https://arxiv.org/abs/2502.15722
tags:
- drug
- prompt
- information
- insights
- accurate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents "Drug Insights," an open-source Retrieval-Augmented
  Generation (RAG) chatbot that provides healthcare workers in Africa with accurate,
  context-specific medication information. The system uses a corpus of Nigerian pharmaceutical
  data, Pinecone vector database, and GPT models to deliver precise responses with
  minimal hallucination.
---

# Open-Source Retrieval Augmented Generation Framework for Retrieving Accurate Medication Insights from Formularies for African Healthcare Workers

## Quick Facts
- arXiv ID: 2502.15722
- Source URL: https://arxiv.org/abs/2502.15722
- Reference count: 15
- System achieves 84.09% S-BERT similarity to pharmacist responses using prompt_0a

## Executive Summary
This paper presents "Drug Insights," an open-source Retrieval-Augmented Generation (RAG) chatbot that provides healthcare workers in Africa with accurate, context-specific medication information. The system uses a corpus of Nigerian pharmaceutical data, Pinecone vector database, and GPT models to deliver precise responses with minimal hallucination. Evaluation using S-BERT similarity scores showed that prompt_0a achieved the highest average score of 84.09% compared to pharmacist responses, while pharmacist feedback on relevancy, accuracy, and response construction averaged 3.76-3.88 out of 5. The tool addresses the challenge of manual drug information retrieval in Africa by leveraging advanced AI technologies to improve access to medication insights, with future work focusing on systematic testing, UI/UX improvements, and real-time feedback integration.

## Method Summary
The framework extracts pharmaceutical data from Nigerian EMDEX formularies using PyMuPDF, structures it via AzureChatOpenAI into a Drug schema, and embeds the content with AzureOpenAIEmbeddings (1536-dim) into a Pinecone vector database. User queries are processed through a LangChain-orchestrated pipeline that retrieves the top-3 most semantically similar passages using cosine similarity threshold 0.9, then generates responses via GPT-4o conditioned on this context. The system employs nine zero-shot prompts with varying constraints and safety guardrails, evaluated against pharmacist reference responses using S-BERT similarity and human assessment. A Streamlit interface provides healthcare workers with access to medication insights.

## Key Results
- S-BERT similarity score of 84.09% achieved by prompt_0a compared to pharmacist responses
- Pharmacist evaluation averaged 3.76-3.88 out of 5 for relevancy, accuracy, and response construction
- The system successfully reduces hallucinations through retrieval-augmented generation and prompt engineering
- Out-of-corpus queries demonstrated degraded performance, highlighting corpus coverage limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation reduces hallucinations by grounding LLM responses in a verified pharmaceutical corpus.
- Mechanism: Drug information documents are converted into vector embeddings and stored in Pinecone. At query time, the system retrieves the top-k most semantically similar passages using cosine similarity (threshold 0.9, k=3), then conditions the GPT-4o generation on this retrieved context rather than parametric knowledge alone.
- Core assumption: The pharmaceutical corpus (EMDEX + related documents) contains accurate, complete information for the expected query distribution.
- Evidence anchors:
  - [abstract]: "delivers accurate, context-specific responses with minimal hallucination"
  - [section]: "Pinecone database uses cosine similarity with a threshold of 0.9 and similarity count of 3, during retrieval operations"
  - [corpus]: Related paper "Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications" applies similar RAG patterns for pharmaceutical accuracy
- Break condition: If a query falls outside the corpus coverage (as tested with out-of-corpus queries), retrieval returns low-relevance passages and hallucination risk increases.

### Mechanism 2
- Claim: Prompt engineering with explicit guardrails improves response safety and semantic alignment with expert responses.
- Mechanism: Zero-shot prompts include explicit constraints (no speculation, no unverified sources, clear disclaimers, context-limited responses) that bound the LLM's generation space. The best-performing prompt_0a also compares 4 generated results before selection.
- Core assumption: GPT-4o reliably follows complex multi-constraint instructions under zero-shot conditions.
- Evidence anchors:
  - [abstract]: "integrates prompt engineering and S-BERT evaluation to optimize retrieval and response generation"
  - [section]: "prompt_0a... has no sentence limit, compares 4 generated results before selecting one and guardrails which are explicit instructions to further reduce the chance of hallucination"
  - [corpus]: Limited direct corpus evidence on this specific guardrails mechanism; related work on medical LLMs emphasizes safety constraints but with varied approaches
- Break condition: If prompt complexity exceeds instruction-following capacity, constraints may be ignored or partially applied.

### Mechanism 3
- Claim: S-BERT semantic similarity provides a quantifiable proxy for response quality against expert benchmarks.
- Mechanism: Sentence embeddings are generated for both Drug Insights responses and pharmacist reference responses using S-BERT. Cosine similarity between embeddings yields a score (0-1) reflecting semantic closeness, enabling systematic prompt comparison.
- Core assumption: Semantic similarity to pharmacist responses correlates with factual accuracy and clinical utility.
- Evidence anchors:
  - [abstract]: "Evaluation using S-BERT similarity scores showed that prompt_0a achieved the highest average score of 84.09% compared to pharmacist responses"
  - [section]: "S-BERT score measures the cosine similarity between sentence embeddings... with a higher score indicates a closer semantic relationship"
  - [corpus]: Related corpus shows limited direct validation of S-BERT as a medical response quality metric; primarily used in general NLP evaluation
- Break condition: Semantic similarity ≠ factual correctness; responses can be semantically similar but factually divergent.

## Foundational Learning

- Concept: **Vector Embeddings and Semantic Search**
  - Why needed here: The entire retrieval mechanism depends on understanding how text becomes numerical vectors and how cosine similarity captures semantic relationships beyond keyword matching.
  - Quick check question: If two documents share no keywords but describe the same drug interaction using different terminology, will cosine similarity on embeddings still retrieve the relevant document?

- Concept: **RAG Architecture Pattern**
  - Why needed here: Understanding the separation between retrieval (finding relevant context) and generation (synthesizing response) is essential for debugging either stage independently.
  - Quick check question: What behavior would you expect if the retrieval step returns documents unrelated to the query?

- Concept: **Zero-Shot Prompting with Constraints**
  - Why needed here: The system relies on prompt engineering rather than fine-tuning; understanding zero-shot instruction following is critical for modifying prompts effectively.
  - Quick check question: What is the difference between a zero-shot prompt and a few-shot prompt, and why might the authors have chosen zero-shot for this medical application?

## Architecture Onboarding

- Component map:
  Data Pipeline: PDF documents → PyMuPDF extraction → GPT-based structuring (Drug class schema) → AzureOpenAIEmbeddings (1536-dim) → Pinecone storage
  Query Pipeline: User query (Streamlit UI) → Query embedding → Pinecone retrieval (cosine ≥0.9, top-3) → Retrieved context + prompt → GPT-4o generation → Response display
  Orchestration: LangChain API connects retrieval and generation components

- Critical path:
  1. Corpus quality → extraction accuracy → embedding quality → retrieval relevance
  2. Retrieved context relevance → generation quality
  3. Prompt design → output safety and format compliance

- Design tradeoffs:
  - **Threshold 0.9**: Higher precision but potential false negatives (missing relevant passages); Assumption: domain queries benefit from strict filtering.
  - **Similarity count of 3**: Limits context window usage vs. comprehensiveness; Assumption: top-3 provides sufficient coverage for most drug queries.
  - **GPT-4o over smaller models**: Prioritizes response quality over latency/cost; Assumption: medical accuracy justifies resource expenditure.

- Failure signatures:
  - Low S-BERT scores (<70%): Likely retrieval mismatch or prompt inadequacy—check corpus coverage for query topic.
  - Hallucinated drug details: Corpus gap or prompt guardrails not followed—verify passage retrieval logs.
  - Slow response time (>5s): Check Pinecone query latency or GPT-4o generation time as bottlenecks.

- First 3 experiments:
  1. **Retrieval audit**: Log the top-3 retrieved passages and cosine scores for 20 test queries; manually verify relevance to identify threshold calibration needs.
  2. **Prompt ablation**: Run the same 50 queries through prompt_0a and prompt_1b; compute S-BERT deltas to isolate the contribution of guardrails vs. comparison step.
  3. **Out-of-corpus detection**: Test queries intentionally outside the EMDEX scope; measure whether the system appropriately declines vs. hallucinates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the systematic recall and precision performance of the Pinecone retrieval component independent of the generation model?
- Basis in paper: [explicit] Authors state future work should include "Explicit systematic evaluation of the recall and precision of database retrievals."
- Why unresolved: Current evaluation (S-BERT similarity and pharmacist feedback) measures end-to-end response quality, not retrieval isolation. It remains unclear whether inaccuracies stem from retrieval failures or generation errors.
- What evidence would resolve it: A labeled test set of queries with known relevant corpus chunks, measuring standard retrieval metrics (recall@k, precision@k, MRR) against ground truth document relevance.

### Open Question 2
- Question: How does Drug Insights perform across multi-turn clinical conversations where follow-up questions depend on prior context?
- Basis in paper: [explicit] Authors note "Current tests on Drug Insights were done with one-shot questions. Tests that evaluate performance over a chat session would be useful."
- Why unresolved: All reported evaluations used isolated single queries. Real clinical use involves sequential clarifying questions, context carryover, and accumulated patient information—none of which have been tested.
- What evidence would resolve it: Comparative evaluation of single-turn vs. multi-turn conversation accuracy using simulated clinical dialogues, measuring context retention, coherence, and factual consistency across turns.

### Open Question 3
- Question: Does S-BERT similarity to pharmacist responses correlate with actual clinical accuracy and safety of drug information?
- Basis in paper: [inferred] The paper uses S-BERT similarity (84.09%) as the primary automated metric and pharmacist ratings (3.76–3.88/5) as human validation, but does not establish whether higher S-BERT scores predict safer or more clinically useful recommendations.
- Why unresolved: Semantic similarity does not necessarily equate to clinical correctness—minor wording differences may lower S-BERT scores without affecting accuracy, while plausible but incorrect responses could score highly.
- What evidence would resolve it: A study correlating S-BERT scores with expert-verified clinical accuracy labels (including safety-critical error detection) on a held-out test set.

### Open Question 4
- Question: Can the framework trained on Nigerian formularies generalize to other African countries with different drug availability, regulatory standards, and clinical guidelines?
- Basis in paper: [inferred] The title references "African Healthcare Workers" broadly, but the corpus is exclusively Nigerian (EMDEX), and no cross-country validation was performed despite known heterogeneity in formulary standards across Africa.
- Why unresolved: Drug availability, brand names, dosing conventions, and regulatory approvals vary by country. A system tuned to Nigerian data may produce inaccurate or misleading recommendations for clinicians elsewhere.
- What evidence would resolve it: Evaluation of the current model on formulary queries from other African countries (e.g., Ghana, Kenya, South Africa), measuring accuracy gaps and identifying necessary corpus extensions.

## Limitations
- The corpus is limited to Nigerian pharmaceutical data (EMDEX), potentially missing medications healthcare workers encounter, as evidenced by out-of-corpus test performance degradation.
- S-BERT similarity scores may not capture clinical accuracy or patient safety—semantic similarity does not guarantee factual correctness, and the metric's validation for medical applications remains limited.
- The evaluation sample size is modest (50 test queries, 5 pharmacists), with limited evidence for prompt engineering effectiveness and guardrail mechanisms beyond S-BERT scores.

## Confidence

- **High**: The RAG architecture (vector embeddings → Pinecone retrieval → GPT-4o generation) functions as described and achieves the reported similarity scores against pharmacist responses.
- **Medium**: The claim that retrieval-augmented generation reduces hallucinations is supported by the corpus, but the specific contribution of prompt guardrails versus corpus grounding remains unclear without direct comparison.
- **Low**: The assertion that S-BERT similarity is a reliable proxy for medical response quality is weakly supported; the corpus shows limited direct validation of this metric for healthcare applications.

## Next Checks

1. Conduct a blinded clinical accuracy assessment where pharmacists evaluate responses for factual correctness and safety, not just semantic similarity to reference answers.
2. Test the system with queries spanning the full spectrum of EMDEX coverage versus out-of-scope medications to quantify the retrieval threshold's impact on response reliability.
3. Perform an ablation study comparing GPT-4o with and without the retrieved context (but same prompts) to isolate the RAG mechanism's contribution to reducing hallucinations.