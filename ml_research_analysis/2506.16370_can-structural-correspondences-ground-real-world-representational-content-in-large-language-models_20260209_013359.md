---
ver: rpa2
title: Can structural correspondences ground real world representational content in
  Large Language Models?
arxiv_id: '2506.16370'
source_url: https://arxiv.org/abs/2506.16370
tags:
- structural
- llms
- correspondence
- representation
- world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses whether text-bound Large Language Models (LLMs)
  can represent real-world entities. The author applies a structural-correspondence
  account of representation, arguing that while mere structural correspondences between
  LLM internal states and worldly structures are insufficient for representation,
  these correspondences can ground representation if they are exploited in a way that
  explains successful task performance.
---

# Can structural correspondences ground real world representational content in Large Language Models?

## Quick Facts
- arXiv ID: 2506.16370
- Source URL: https://arxiv.org/abs/2506.16370
- Authors: Iwan Williams
- Reference count: 10
- Key outcome: Structural correspondences between LLM internal states and worldly structures can ground real-world content if exploited in ways that explain successful task performance.

## Executive Summary
This paper addresses whether text-bound Large Language Models can represent real-world entities through structural correspondences. The author argues that while mere structural correspondences are insufficient for representation, they can ground real-world content when exploited to explain successful task performance. The text-boundedness challenge is identified as a key obstacle, but the paper contends it can be overcome by examining training history to determine appropriate success criteria. The work calls for empirical studies to investigate causal sensitivity to internal structures and to compare the effects of modulating different structural correspondences on LLM performance.

## Method Summary
The paper proposes a framework for testing whether structural correspondences in LLMs are exploited for representation. The method involves: (1) training linear probes to identify candidate correspondences between activation space and target structures, (2) perturbing activations along probe-identified gradients to test causal sensitivity, and (3) measuring performance impact to determine if task success depends on correspondence quality. The success criterion varies by training history - pre-trained models are evaluated on token prediction probability while RLHF models are assessed on semantic appropriateness. A key methodological challenge is independently modulating real-world versus linguistic-statistical correspondences to determine which is exploited.

## Key Results
- Structural correspondences between LLM internal states and worldly structures are not sufficient for representation but can ground content when exploited
- The text-boundedness challenge can be overcome by determining success criteria from training history
- Proposed empirical work includes testing causal sensitivity to internal structures and comparing effects of modulating different structural correspondences

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structural correspondences between LLM internal states and worldly structures *can* ground representational content, but only if they are exploited.
- **Mechanism:** For a structural correspondence to ground representation, the system must be causally sensitive to the internal structure, and its success on a given task must depend on the quality of that correspondence.
- **Core assumption:** A structural-correspondence based account is a valid framework for evaluating representation in artificial systems.
- **Evidence anchors:**
  - [abstract] "if these structural correspondences play an appropriate role – they are exploited in a way that explains successful task performance – then they could ground real world contents."
  - [section] "to a first approximation, the structural correspondence needs to be used or exploited by the system (or organism) in a way that explains the system's successful performance of some task or capacity."
  - [corpus] The corpus contains related work on synthetic data and ground truth, but no direct empirical test of this mechanism.
- **Break condition:** The internal structure has no causal impact on downstream processing (is epiphenomenal), or task success is independent of the correspondence's quality.

### Mechanism 2
- **Claim:** A text-bound LLM's task, and therefore its success conditions, are determined by its training history.
- **Mechanism:** Different training processes stabilize different behavioral dispositions. Pre-training stabilizes a disposition for statistically probable text generation, whereas reinforcement learning from human feedback (RLHF) stabilizes a disposition for semantically appropriate outputs because that is what was reinforced.
- **Core assumption:** The teleosemantic principle of deriving a system's function from its reinforcement history applies to machine learning systems.
- **Evidence anchors:**
  - [section] "While purely pretrained LLMs have the function or goal of predicting the most probable next token, those fine-tuned by RLHF and similar processes can develop tasks that involve producing semantically appropriate (including truth-tracking) responses to prompts."
  - [section] "Unlike pre-training, fine tuning tends to select against semantically inappropriate outputs in virtue of their semantic inappropriateness."
  - [corpus] No direct evidence.
- **Break condition:** An alternative framework for defining an LLM's task proves more predictive than its training history.

### Mechanism 3
- **Claim:** To determine if a correspondence to a real-world structure or a linguistic-statistical one is exploited, one must differentially modulate them and measure the effect on task success.
- **Mechanism:** The proposed method involves artificially perturbing an LLM's activation space to independently strengthen or weaken its correspondence to a target real-world structure versus a competing linguistic-statistical structure. The explanatory correspondence is the one whose manipulation has a greater impact on success.
- **Core assumption:** It is possible to independently manipulate correspondences to different structures in a way that produces a measurable causal effect.
- **Evidence anchors:**
  - [section] "one could artificially adjust the geometry of an LLM's activation space at a layer of interest, so as to differentially tighten the structural correspondence to either (i) the real world structure or (ii) the linguistic-statistical structure."
  - [section] Cites Chen et al. (2023), who perturbed activations along a spatial gradient and found a negative impact on performance, though they did not compare it to a competing structure.
  - [corpus] Corpus evidence is weak or missing for this specific multi-factor modulation methodology.
- **Break condition:** It is impossible to independently modulate the different candidate correspondences, or modulation has no measurable impact.

## Foundational Learning

- **Concept:** Structural Correspondence
  - **Why needed here:** This is the core theoretical construct. The entire argument hinges on understanding that representation is not just correlation, but a relation-preserving map that must be put to use.
  - **Quick check question:** Why is a simple correlation between an internal state A and an external entity B insufficient for A to structurally represent B?

- **Concept:** Exploitation
  - **Why needed here:** This is the central condition proposed to overcome the triviality problem. It distinguishes a "mere" correspondence from a content-grounding one.
  - **Quick check question:** If a system has an internal state isomorphic to a city's layout but never uses it for any task, does it represent the city according to this paper's framework?

- **Concept:** Text-Boundedness
  - **Why needed here:** This is the primary challenge the paper addresses. Understanding this constraint is critical for grasping why defining the correct "task" and "success criteria" is so pivotal.
  - **Quick check question:** Why does the "octopus thought experiment" suggest that a system with only textual inputs cannot represent real-world entities like coconuts?

## Architecture Onboarding

- **Component map:**
  - LLM Activation Space -> Probing Classifier -> Perturbation Mechanism -> Task & Success Metric

- **Critical path:** Identify a candidate correspondence via probing -> establish causal sensitivity with perturbations -> measure impact on task success -> compare against competing hypotheses (e.g., linguistic-statistical structures) to determine the primary exploited correspondence.

- **Design tradeoffs:**
  - **Probing vs. Intervention:** Probing identifies correlations but cannot establish exploitation. Causal intervention is required but is more complex.
  - **Choice of Success Metric:** Evaluating success based on a task's semantic accuracy (e.g., correct capitals) may be inappropriate for a pre-trained model whose task is statistical prediction.

- **Failure signatures:**
  - A probe successfully decodes a property, but perturbations to that property's representation have no effect on downstream behavior.
  - Strengthening a real-world correspondence degrades performance on the model's primary task.
  - Inability to independently modulate candidate structures.

- **First 3 experiments:**
  1. **Causal Sensitivity Test:** For a known structural relation (e.g., `king` - `man` + `woman` ≈ `queen`), perform vector addition/subtraction at a critical layer and measure the causal effect on the output probability of the target token.
  2. **Training History Task Attribution:** Compare a pre-trained model vs. a fine-tuned (RLHF) model. Intervene to strengthen a real-world correspondence in both and measure the impact relative to their respective success criteria (statistical likelihood vs. semantic appropriateness).
  3. **Competing Hypothesis Modulation:** Identify a case where real-world facts diverge from linguistic statistics (e.g., a recently changed capital). Train probes for both structures and perturb the activations to strengthen each one independently, then compare which manipulation has a larger effect on next-token prediction performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Are LLMs causally sensitive to internal geometric relations beyond vector arithmetic, such as distance or mereological subsets in activation space?
- **Basis in paper:** [Explicit] Section 4.2.1 asks whether processing is sensitive to "distance in activation space more broadly" or "mereological relations," stating "more empirical work is needed."
- **Why unresolved:** Current evidence (e.g., Merullo et al.) suggests sensitivity to specific vector offsets, but general sensitivity to the geometry of the state space remains unconfirmed.
- **What evidence would resolve it:** Intervention studies that perturb activation space distances or subset relations and measure the causal impact on downstream processing and output.

### Open Question 2
- **Question:** Do exploited structural correspondences track real-world facts or merely linguistic-statistical patterns?
- **Basis in paper:** [Explicit] The author argues in Section 4.2.5 that we must "tease apart these two hypotheses empirically" to know if representations are grounded in the world or just text statistics.
- **Why unresolved:** An observed correspondence with a real-world structure (like color space) could be a side effect of the model actually tracking statistical word co-occurrences.
- **What evidence would resolve it:** Experiments that differentially modulate the geometry of activation spaces to tighten either real-world or linguistic correspondences and compare the effects on task success.

### Open Question 3
- **Question:** How do different training histories (e.g., pre-training vs. RLHF) alter the success criteria that determine which structural correspondence is exploited?
- **Basis in paper:** [Inferred] While Section 4.2.4 distinguishes between success criteria based on training history, the Conclusion calls for applying these specific criteria when evaluating interventions, implying the relationship needs empirical mapping.
- **Why unresolved:** It is theoretically possible that pre-trained models exploit statistical structures while RLHF models exploit real-world structures, but this difference has not been empirically verified.
- **What evidence would resolve it:** Comparative intervention experiments on models with different training regimes, measuring performance against distinct success metrics (probability vs. semantic appropriateness).

## Limitations
- The causal intervention methodology is conceptually clear but lacks concrete protocols for independently modulating competing structural correspondences
- The text-boundedness challenge is resolved primarily through theoretical argument rather than demonstrated empirical resolution
- Corpus evidence supporting the proposed mechanisms is notably sparse, with most claims relying on theoretical reasoning

## Confidence

- **High confidence:** The structural-correspondence framework itself is well-established in philosophy of mind literature. The distinction between mere correlation and exploited correspondence is conceptually sound.
- **Medium confidence:** The claim that training history determines appropriate success criteria is reasonable but depends on accepting the teleosemantic framework for machine learning systems, which has not been universally validated.
- **Low confidence:** The specific methodology for differentially modulating real-world vs. linguistic-statistical correspondences lacks concrete implementation details, making empirical validation uncertain.

## Next Checks

1. Implement the differential modulation protocol on a simple structural correspondence (e.g., spatial coordinates of cities) and verify that perturbations produce measurable causal effects on task performance, comparing real-world vs. linguistic-statistical structure impacts.

2. Conduct a controlled experiment comparing pre-trained vs. RLHF-tuned models on the same structural correspondence intervention to test whether training history determines which structural relations are exploited for task success.

3. Develop a systematic probe-intervention pipeline that quantifies probe accuracy per layer, intervention effect sizes, and downstream behavioral changes to establish a reproducible methodology for testing structural correspondence exploitation.