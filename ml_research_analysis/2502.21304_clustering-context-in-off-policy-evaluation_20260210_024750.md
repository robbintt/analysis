---
ver: rpa2
title: Clustering Context in Off-Policy Evaluation
arxiv_id: '2502.21304'
source_url: https://arxiv.org/abs/2502.21304
tags:
- chips
- policy
- clusters
- evaluation
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the off-policy evaluation (OPE) problem in
  contextual bandits, where the goal is to estimate the value of a new policy using
  logged data from a different policy. Standard OPE estimators like IPS suffer from
  high variance or bias when the logging policy differs significantly from the evaluation
  policy or when certain actions are underrepresented in the data.
---

# Clustering Context in Off-Policy Evaluation

## Quick Facts
- arXiv ID: 2502.21304
- Source URL: https://arxiv.org/abs/2502.21304
- Reference count: 40
- The paper proposes CHIPS, a clustering-based off-policy evaluation estimator that outperforms standard methods in high policy distributional shift scenarios.

## Executive Summary
This paper addresses the challenge of off-policy evaluation (OPE) in contextual bandits, where the goal is to estimate the value of a new policy using logged data from a different policy. Standard OPE estimators like IPS suffer from high variance or bias when the logging policy differs significantly from the evaluation policy or when certain actions are underrepresented. The proposed CHIPS estimator clusters contexts into groups and applies IPS-style reweighting within each cluster, reducing variance by sharing information across similar contexts and mitigating bias from actions with low logging probabilities.

## Method Summary
CHIPS is an off-policy evaluation estimator that clusters contexts and applies importance weighting at the cluster level rather than the individual context level. The method computes cluster-level propensity weights by averaging individual context propensities within each cluster, then applies these weights to cluster-level reward estimates. The approach uses either maximum likelihood or maximum a posteriori estimation for rewards, with the latter incorporating a Beta prior. The estimator is compared against standard baselines (IPS, DM, DR, MR, MIPS) on both synthetic and real-world datasets, with clustering typically performed using MiniBatch KMeans.

## Key Results
- CHIPS outperforms IPS, DM, DR, MR, and MIPS baselines on synthetic data with controlled distributional shift
- On the Open Bandit Dataset, CHIPS achieves lower MSE than IPS and MIPS with higher probability of beating baselines
- The estimator shows particular strength in scenarios with high policy distributional shift or deficient action support
- Optimal performance requires balancing cluster granularity - too few clusters cause bias, too many converge to IPS behavior

## Why This Works (Mechanism)

### Mechanism 1
Clustering contexts enables information sharing across similar contexts, reducing variance in importance weight estimation when individual context-action pairs are underrepresented. CHIPS replaces the standard IPS weight π(a|x)/π₀(a|x) with a cluster-level weight p(a|c,π)/p(a|c,π₀), computed by averaging π(a|x) over all contexts within cluster c. This pools observations from contexts with similar behavior, effectively increasing sample size per weight estimate. The core assumption is Assumption 3.2 (Reward Homogeneity)—the context x does not affect the reward given cluster c and action a (q(a,c,x) = q(a,c)). The mechanism breaks when contexts within a cluster have heterogeneous reward distributions, causing bias proportional to the δ-homogeneity gap.

### Mechanism 2
The Common Cluster Support assumption (Assumption 3.1) is strictly weaker than standard Common Support, enabling unbiased estimation even when individual contexts lack action support. For a context x where π₀(a|x) = 0 but π(a|x) > 0, IPS fails. However, if other contexts y in the same cluster have π₀(a|y) > 0, then p(a|c,π₀) > 0 and CHIPS can still estimate. Corollary 3.9 shows bias reduction equals the contribution from actions in U(x,π₀) \ U(c,π₀). The mechanism breaks when clusters are too granular (approaching one cluster per context), causing CHIPS to converge to IPS behavior and lose this advantage.

### Mechanism 3
CHIPS provably reduces variance compared to IPS and MIPS under standard assumptions, with variance reduction proportional to weight variability within clusters. Proposition 3.11 shows N·(Var[IPS] - Var[CHIPS]) = E_{c,a}[V_{π₀(x|a,c)}[w²(a,x)] · E[r²|a,c]]. The key is V_{π₀(x|a,c)}[w²]—variance of IPS weights conditioned on cluster and action. When contexts within a cluster have similar policy behavior, this conditional variance is small. The mechanism breaks with excessive clusters or highly heterogeneous within-cluster behavior, causing variance reduction to diminish as CHIPS approaches IPS.

## Foundational Learning

- **Inverse Propensity Score (IPS) Estimation**
  - Why needed here: CHIPS is a direct modification of IPS; understanding the baseline reweighting mechanism is essential to grasp why clustering helps.
  - Quick check question: Given logged data under policy π₀, can you derive why w(a,x) = π(a|x)/π₀(a|x) produces an unbiased estimate of V(π) when Assumption 2.1 holds?

- **Importance Weight Variance and Its Consequences**
  - Why needed here: The paper's core motivation is IPS variance explosion when logging and evaluation policies diverge. Understanding this failure mode clarifies why the CHIPS solution works.
  - Quick check question: If π₀(a|x) = 0.01 and π(a|x) = 0.9, what is the importance weight and why might this cause high MSE?

- **Clustering Validity in High-Dimensional Spaces**
  - Why needed here: CHIPS effectiveness depends entirely on whether meaningful context clusters exist and can be discovered. The curse of dimensionality affects clustering quality.
  - Quick check question: Why does increasing cluster radius (Figure 6) or context-specific noise σ (Figure 8) degrade CHIPS performance relative to IPS?

## Architecture Onboarding

- Component map: Logged Data D → Clustering Module ξ → Cluster Assignments c_i → Policy π, π₀ → Weight Calculator → p(a|c,π), p(a|c,π₀) via Eq. 7 → Rewards r_i → Reward Estimator → q̂(a,c) via ML or MAP → CHIPS Estimator → V̂_CHIPS(π; D)

- Critical path:
  1. **Cluster number selection**: Use PAS-IF technique or reference Figure 12—too few clusters oversimplifies, too many converges to IPS.
  2. **α (Beta prior) selection**: For MAP reward estimation, use the process in Appendix D.4—estimate avg points per cluster-action and select α from Figure 17 reference curve.
  3. **Clustering method**: MiniBatch KMeans recommended for scalability; DBSCAN and OPTICS viable for density-based clustering.

- Design tradeoffs:
  | Choice | Benefit | Cost |
  |--------|---------|------|
  | Fewer clusters | More samples per cluster, lower variance | Higher bias from heterogeneity violation |
  | MAP vs ML reward | Robust to reward misspecification (Figure 1, 15) | Requires prior tuning; underperforms when policies are similar |
  | KMeans vs DBSCAN | Fast O(n) with mini-batch | Assumes spherical clusters |

- Failure signatures:
  - **U-shaped MSE curve** (Figure 1 left): Optimal cluster count exists; extremes fail.
  - **CHIPS ≈ IPS performance**: Cluster structure doesn't exist or is undiscovered (high σ, large cluster radius).
  - **ML severely underperforms MAP**: Significant reward misspecification present (β ≈ -1 scenarios).

- First 3 experiments:
  1. **Reproduce synthetic baseline**: Use Table 1 defaults (cexp=10, crad=1, dx=2, anum=10, cnum=10, β=-1, α=20, σ=0.2). Verify v-shaped MSE vs. cluster count (Figure 1 left) to confirm implementation.
  2. **Ablate cluster number vs. action space**: Fix other parameters, vary clusters {5, 10, 50, 100} × actions {10, 50, 100, 200}. Confirm Figure 12a pattern—fewer clusters needed for larger action spaces.
  3. **Validate on OBD real dataset**: Download Open Bandit Dataset, extract 50K/100K/500K samples. Compare CHIPS_bayes vs. IPS/MIPS using ECDF protocol (Appendix F). Target: CHIPS probability of beating IPS > 0.9 at 50K samples.

## Open Questions the Paper Calls Out

### Open Question 1
Can a hybrid approach combining CHIPS with action-embedding methods like MIPS further improve performance? The conclusion states it opens the possibility of exploring "if combining CHIPS with pure action-embedding methods like MIPS can improve general performance." This remains unresolved because the paper analyzes CHIPS and MIPS separately but does not propose or evaluate a unified estimator that leverages both context clustering and action embeddings simultaneously. What evidence would resolve it: Theoretical derivation of a combined estimator and empirical results demonstrating reduced MSE in scenarios with large action spaces and structured context distributions.

### Open Question 2
Is it possible to theoretically derive or automatically optimize the optimal number of clusters (K) and the prior parameter (α) without relying on empirical grid search? Section 5 highlights the need to explore "if it is possible to estimate the optimal value for hyperparameters beyond empirical estimation." This remains unresolved because the current work relies on heuristic references (e.g., PAS-IF) and parameter sweeps to determine K and α. What evidence would resolve it: Development of a data-driven algorithm that selects these parameters based on logged data statistics, consistently outperforming fixed or manually tuned configurations.

### Open Question 3
How robust is CHIPS when the context space lacks clear separability or when the Reward Homogeneity assumption (Assumption 3.2) is severely violated? The conclusion notes performance is "influenced by the accuracy of the clustering method," and Appendix D shows performance decays when cluster radii increase or noise (σ) rises, suggesting a gap in robustness for non-ideal cluster structures. This remains unresolved because while Proposition 3.6 provides a bias bound, the paper does not offer a mechanism to detect severe assumption violations or adapt the clustering granularity accordingly in practice. What evidence would resolve it: Experiments on adversarial datasets with high inter-cluster variance or overlapping clusters, paired with a theoretical analysis of the resulting bias-variance trade-off.

## Limitations
- Theoretical guarantees depend on clusterability of context space and Reward Homogeneity assumption, which may not hold in practice
- Empirical validation limited to synthetic data with controlled parameters and single real-world dataset (OBD)
- Baseline implementation details unclear, particularly for complex methods like DR, MR, and MIPS
- Complexity analysis assumes specific MiniBatch KMeans parameters without exploring sensitivity

## Confidence

- **High Confidence**: The mechanism by which clustering reduces variance through information pooling is well-established and mathematically proven (Proposition 3.11). The claim that CHIPS is always variance-reducing relative to IPS holds under stated assumptions.
- **Medium Confidence**: The bias reduction claim under Common Cluster Support (Assumption 3.1) is formally correct but relies on clustering quality. The theoretical bounds are tight, but practical performance depends heavily on discovering meaningful clusters in real data.
- **Low Confidence**: The real-world applicability claim based on OBD results is limited by single-dataset validation and unclear baseline implementation details. The generalization to other recommendation domains remains uncertain.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary MiniBatch KMeans parameters (batch_size, n_init, max_iter) and evaluate impact on CHIPS performance across different clusterability regimes (controlled by σ and cluster radius).

2. **Multi-Dataset Validation**: Apply CHIPS to diverse OPE benchmarks beyond OBD (e.g., Yahoo! LTR, MovieLens) to test robustness to different context-action distributions and reward structures.

3. **Clustering Method Comparison**: Compare CHIPS performance using alternative clustering approaches (DBSCAN, OPTICS, hierarchical clustering) to assess whether the benefits stem from clustering itself or specific KMeans assumptions about spherical clusters.