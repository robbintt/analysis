---
ver: rpa2
title: Automatic Robustness Stress Testing of LLMs as Mathematical Problem Solvers
arxiv_id: '2506.05038'
source_url: https://arxiv.org/abs/2506.05038
tags:
- question
- llms
- hecker
- target
- ar-c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes AR-Checker, an automatic robustness stress testing
  framework for LLMs as mathematical problem solvers. The framework dynamically generates
  problem variants via iterative rewriting and verification using multiple parallel
  streams, maintaining semantic meaning while aiming to cause LLM failure.
---

# Automatic Robustness Stress Testing of LLMs as Mathematical Problem Solvers

## Quick Facts
- **arXiv ID**: 2506.05038
- **Source URL**: https://arxiv.org/abs/2506.05038
- **Reference count**: 26
- **One-line primary result**: AR-Checker framework achieves 48.04% accuracy drop on GSM8K and 31.35% on MATH-500 through automatic generation of semantically equivalent failure-inducing variants.

## Executive Summary
This paper introduces AR-Checker, an automatic robustness stress testing framework for evaluating LLMs as mathematical problem solvers. The framework dynamically generates problem variants through iterative rewriting and verification using multiple parallel streams, maintaining semantic meaning while aiming to cause LLM failure. Experiments on GSM8K and MATH-500 demonstrate significant accuracy drops across various LLMs, with AR-Checker outperforming static robustness benchmarks. The method generalizes well to non-mathematical benchmarks like MMLU, MMLU-Pro, and CommonsenseQA.

## Method Summary
AR-Checker operates through three LLMs: a rewriter that generates semantically equivalent problem variants, a verifier that checks meaning preservation and answer consistency, and the target LLM being tested. The framework runs N parallel streams with K iterations each, where the rewriter proposes variants, the target LLM answers them, and the verifier judges whether the variant preserves meaning while causing the target to fail. The process terminates when a successful failure variant is found. The method uses zero-shot prompting for target LLMs and reports test failure rate (TFR), vanilla accuracy (VAcc), robustness accuracy (RAcc), and accuracy drop (ΔAcc) as key metrics.

## Key Results
- AR-Checker achieves 48.04% average accuracy drop on GSM8K across various LLMs
- Framework achieves 31.35% average accuracy drop on MATH-500 benchmark
- AR-Checker generalizes effectively to non-mathematical benchmarks (MMLU, MMLU-Pro, CommonsenseQA)
- Parallel streams and iterative feedback significantly improve failure detection compared to static benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel search streams increase the probability of discovering failure-inducing variants.
- Mechanism: N independent rewrite streams explore diverse variant trajectories simultaneously, reducing correlation between search paths and increasing coverage of the failure manifold.
- Core assumption: Failure-inducing variants are not uniformly distributed; multiple independent search directions have higher aggregate probability of encountering them.
- Evidence anchors:
  - [abstract] "...multi-round parallel streams of LLM-based rewriting and verification..."
  - [section 3.2] "We design the framework to perform multiple streams of rewriting conversations in parallel... This design increases the search width..."
  - [corpus] Neighbor papers on optimization (OR-Toolformer, SAT solver heuristics) similarly use parallel or multi-strategy search to handle complex solution spaces, supporting the intuition.
- Break condition: If failure modes are highly correlated across all streams (e.g., all rewriter LLMs share identical blind spots), parallelism yields diminishing returns.

### Mechanism 2
- Claim: Iterative feedback from the verifier guides the rewriter toward semantically valid, model-breaking variants.
- Mechanism: The verifier produces structured feedback (meaning change vs. preserved; answer match vs. mismatch), which the rewriter incorporates in subsequent rounds to refine its rewriting strategy.
- Core assumption: The rewriter LLM can interpret feedback and adapt its behavior within the conversational context.
- Evidence anchors:
  - [abstract] "...multi-round parallel streams of LLM-based rewriting and verification."
  - [section 3.3] "...the rewriter first generates chain-of-thoughts about the strategies to modify the instruction... based on its internal knowledge as well as feedbacks from the verifier."
  - [corpus] No direct corpus analog; this is a novel contribution of the paper.
- Break condition: If verifier feedback is noisy or if rewriter cannot generalize from feedback, iterative refinement may degrade into random exploration.

### Mechanism 3
- Claim: The verifier's high precision ensures that identified failures reflect true model weaknesses rather than semantic drift.
- Mechanism: A capable LLM verifier explicitly checks two conditions—meaning preservation and answer mismatch—before accepting a variant as a "test failure." This two-stage gate reduces false positives.
- Core assumption: The verifier itself is sufficiently reliable; its error rate is substantially lower than the target LLM's failure rate.
- Evidence anchors:
  - [abstract] "...a verifier checks if the variants maintain the original meaning and cause the target LLM to fail..."
  - [section 4.3, Table 4] "Qwen2.5-32B-Instruct achieves a precision of 98.00%... indicating a minimal probability of incorrectly classifying passing tests as failing ones."
  - [corpus] Corpus papers focus on benchmark creation but do not explicitly report verifier precision; this mechanism is distinctive to AR-Checker.
- Break condition: If verifier precision degrades (e.g., on out-of-distribution problem types), false positives will contaminate the failure set.

## Foundational Learning

- Concept: **Stress Testing (Software Engineering)**
  - Why needed here: The paper explicitly frames robustness evaluation as stress testing—generating adversarial conditions to expose hidden failures.
  - Quick check question: Can you explain the difference between stress testing and standard functional testing in software systems?

- Concept: **Semantics-Preserving Perturbations**
  - Why needed here: The core valid variant constraint is that the rewritten problem must keep the same answer as the original.
  - Quick check question: Given a math word problem, can you propose a rewrite that changes surface form but not the underlying arithmetic?

- Concept: **Multi-Agent LLM Systems**
  - Why needed here: AR-Checker orchestrates three LLMs (rewriter, verifier, target) with distinct roles and information flows.
  - Quick check question: What are the communication and failure modes in a system where one LLM generates and another critiques?

## Architecture Onboarding

- Component map: Target LLM <- Verifier <- Rewriter (parallel streams)
- Critical path:
  1. Sample a correctly-answered problem (x, a) from the benchmark.
  2. For each parallel stream: rewriter proposes variant x'.
  3. Target LLM answers x'.
  4. Verifier judges meaning and answer match.
  5. If verdict is D (meaning preserved, answer wrong), terminate all streams and record failure.
  6. Else, append feedback to stream history and repeat up to K iterations.
- Design tradeoffs:
  - Higher N and K increase failure detection (higher TFR) but linearly increase compute and API costs.
  - Stronger rewriter models (e.g., GPT-4o) may find failures faster but at higher marginal cost.
  - Verifier precision is critical; a weak verifier inflates false positives, undermining trust in reported failure rates.
- Failure signatures:
  - **Low TFR despite high budget**: Target model may be unusually robust; consider increasing N/K or inspecting verifier for over-conservatism.
  - **High false positive rate**: Verifier may be incorrectly flagging meaning changes; validate verifier on human-annotated samples.
  - **Stuck in semantic drift**: Rewriter consistently alters core meaning; check verifier prompts and rewriter instructions.
- First 3 experiments:
  1. Baseline replication: Run AR-Checker (N=5, K=3) on GSM8K for a small target model (e.g., Gemma-2-2B-IT); measure TFR and manually inspect 10–20 failure cases for semantic validity.
  2. Ablation on N and K: Systematically vary N∈{1,3,5} and K∈{1,3,5}; plot TFR vs. compute cost to identify diminishing returns.
  3. Verifier validation: Annotate 100+ samples flagged as failures; compute verifier precision, recall, and F1; if precision < 95%, iterate on verifier prompt or model.

## Open Questions the Paper Calls Out
None

## Limitations
- Verifier reliability across diverse problem domains remains uncertain, particularly for complex mathematical domains and non-mathematical benchmarks
- Computational cost scales linearly with both parallel streams (N) and maximum iterations (K), making extensive stress testing expensive
- Iterative rewriting process may converge to semantic drift if verifier becomes less reliable with complex problem structures

## Confidence

- **High confidence**: The core framework architecture and its basic functionality are well-documented and reproducible. The significant accuracy drops observed across multiple models and benchmarks (48.04% GSM8K, 31.35% MATH-500) appear robust.
- **Medium confidence**: The claimed superiority over static robustness benchmarks is plausible given the dynamic nature of AR-Checker, but direct comparative validation would strengthen this claim. The generalization to non-mathematical benchmarks is supported but requires more extensive validation.
- **Medium confidence**: The effectiveness of parallel streams and iterative feedback is theoretically sound, but the marginal benefit beyond certain thresholds (N=5, K=3) may vary significantly across problem types and model families.

## Next Checks

1. **Verifier validation across domains**: Systematically evaluate AR-Checker's verifier precision on 200+ failure cases from each benchmark (GSM8K, MATH-500, MMLU, etc.) to confirm the claimed 98% precision holds across mathematical and non-mathematical domains.

2. **Cost-benefit analysis of parallel streams**: Vary N from 1 to 10 and K from 1 to 5 systematically across all benchmarks, measuring TFR gains against API compute costs to identify optimal configurations for different model sizes and problem complexities.

3. **Human evaluation of semantic preservation**: Have domain experts independently evaluate 100 randomly selected variant-failure pairs to verify that AR-Checker maintains semantic equivalence while successfully causing target model failures, validating the framework's core design assumption.