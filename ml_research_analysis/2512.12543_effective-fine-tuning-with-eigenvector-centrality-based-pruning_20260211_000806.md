---
ver: rpa2
title: Effective Fine-Tuning with Eigenvector Centrality Based Pruning
arxiv_id: '2512.12543'
source_url: https://arxiv.org/abs/2512.12543
tags:
- pruning
- neurons
- centrality
- fine-tuning
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a graph-theory-based pruning method to improve
  fine-tuning of pre-trained neural networks by retaining only the most influential
  neurons before adaptation. The approach models neurons as nodes in a graph, with
  edges defined by cosine similarity of weight vectors, and ranks neurons using eigenvector
  centrality to identify those most critical for the network's representational capacity.
---

# Effective Fine-Tuning with Eigenvector Centrality Based Pruning

## Quick Facts
- arXiv ID: 2512.12543
- Source URL: https://arxiv.org/abs/2512.12543
- Reference count: 0
- Key outcome: Graph-theory-based pruning improves fine-tuning accuracy while reducing model complexity, achieving 48% accuracy on Oxford-Flowers102 (vs 30% baseline).

## Executive Summary
This paper introduces a graph-theory-based pruning method to improve fine-tuning of pre-trained neural networks. The approach models neurons as nodes in a graph, with edges defined by cosine similarity of weight vectors, and ranks neurons using eigenvector centrality to identify the most critical units. By retaining only high-centrality neurons before adaptation, the method achieves improved classification accuracy while reducing model complexity across multiple architectures and datasets.

## Method Summary
The method constructs a graph where nodes represent neurons and edges are weighted by cosine similarity of their weight vectors. Eigenvector centrality is computed to rank neurons by their influence in the network's representational capacity. The lowest-centrality neurons are pruned, and the resulting sparse network is fine-tuned on the target task. The approach is evaluated on VGGNet, EfficientNet, and ResNet models using TF-Flowers, Caltech101, EuroSAT, and Oxford-Flowers102 datasets.

## Key Results
- Achieved 48% accuracy on Oxford-Flowers102 dataset (vs 30% for baseline)
- Improved performance across VGGNet, EfficientNet, and ResNet architectures
- Demonstrated effectiveness on multiple datasets including TF-Flowers, Caltech101, and EuroSAT
- Maintained stable accuracy even at 90% sparsity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retaining neurons with high eigenvector centrality preserves the core representational capacity of the network while removing peripheral, less influential units.
- Mechanism: By constructing a graph where nodes are neurons and edges represent cosine similarity of weights, eigenvector centrality identifies neurons that are not just connected, but connected to other well-connected neurons. This recursive importance scoring suggests these neurons form the "backbone" of the layer's feature space.
- Core assumption: High similarity and centrality in the weight space correlate with functional importance for the task, rather than mere redundancy.
- Evidence anchors: [Abstract], [Section 1.3], [Corpus]
- Break condition: If the network's critical features are encoded in sparse, orthogonal neurons (low centrality/similarity), this method will prune essential diversity, causing accuracy collapse.

### Mechanism 2
- Claim: Pre-pruning reduces overfitting during fine-tuning by restricting the hypothesis space, which is particularly effective for small datasets.
- Mechanism: Fine-tuning typically adapts a massive parameter space to a smaller target distribution. By pruning up to 90% of parameters, the model is forced to learn robust features rather than memorizing noise, acting as a strong structural regularizer.
- Core assumption: The pre-trained network is significantly over-parameterized for the target downstream task.
- Evidence anchors: [Abstract], [Section 1.1], [Corpus]
- Break condition: If the target dataset is large and complex, aggressive pruning will simply reduce capacity below the required level, degrading performance compared to dense baselines.

### Mechanism 3
- Claim: Cosine similarity effectively maps functional neuron relationships into a graph topology suitable for spectral analysis.
- Mechanism: The method converts the problem of neuron importance into a graph-ranking problem. Unlike magnitude-based pruning, this uses the W^T W matrix to capture inter-neuron dependencies, ensuring that a "weak" neuron supporting a cluster isn't removed arbitrarily.
- Core assumption: Weight vector alignment (cosine similarity) is a sufficient proxy for functional dependency or information flow between neurons.
- Evidence anchors: [Section 3.1], [Section 2.4]
- Break condition: If the layer uses bias terms heavily or relies on magnitude-independent phase relationships not captured by cosine similarity, the graph edges will misrepresent the true network topology.

## Foundational Learning

- Concept: **Eigenvector Centrality**
  - Why needed here: This is the scoring function used to rank neurons. You must understand that a node's score depends on the scores of its neighbors, differentiating it from simple degree counting.
  - Quick check question: In a 5-neuron graph, if Neuron A connects to 4 low-importance neurons and Neuron B connects to 2 high-importance neurons, which gets the higher eigenvector score?

- Concept: **Cosine Similarity**
  - Why needed here: This defines the "edges" of the graph. You need to know that it measures orientation alignment, ignoring magnitude (unlike dot product).
  - Quick check question: Two weight vectors w1 = [10, 10] and w2 = [1, 1] have a dot product of 20. What is their cosine similarity?

- Concept: **Transfer Learning (Fine-tuning)**
  - Why needed here: The method operates specifically on the "adaptation" phase of a pre-trained model.
  - Quick check question: Why might adding a new classification head to a large frozen backbone fail on a small dataset (e.g., Oxford-Flowers102) compared to a pruned model?

## Architecture Onboarding

- Component map: Input -> Graph Builder -> Thresholding -> Ranker -> Pruner -> Trainer
- Critical path: The Thresholding (τ) and Pruning Ratio (p) hyperparameters. If τ is too high, the graph becomes disconnected (eigen-solver fails or produces trivial zeros). If p is too aggressive without a high-performing core, accuracy drops.
- Design tradeoffs:
  - VGGNet vs. ResNet/EfficientNet: The method is most effective on VGG-style architectures with large Dense layers. Modern architectures (ResNet/EfficientNet) often use Global Average Pooling, reducing the Dense layer to minimal size, limiting the pruning surface.
  - Sparsity vs. Stability: Table 2 shows accuracy is stable even at 0.9 sparsity, but Table 3 shows accuracy varies significantly with the similarity threshold (τ).
- Failure signatures:
  - Disconnected Graph: If τ is too aggressive, the graph may split into isolated components, making global centrality scores meaningless.
  - Bimodal Collapse: If the eigenvector solver picks a non-principal eigenvector, ranking will be random.
- First 3 experiments:
  1. Dense Layer Ablation: Apply the method only to the final Dense layer of VGG16 on TF-Flowers to verify the core claim.
  2. Threshold Sweep: Fix pruning ratio at 0.5, sweep τ from 0.1 to 0.9 to observe sensitivity.
  3. Architecture Comparison: Run the pipeline on ResNet50 to verify if the method generalizes or if the lack of large FC layers diminishes returns.

## Open Questions the Paper Calls Out
No open questions were explicitly identified in the provided paper content.

## Limitations
- Method effectiveness varies significantly with hyperparameter selection (similarity threshold τ), requiring manual tuning for each dataset
- Computational complexity of O(n²d) for building similarity matrix limits scalability to very deep architectures
- Performance depends heavily on architecture structure, showing limited applicability to modern architectures with Global Average Pooling

## Confidence
- **High Confidence**: The core mechanism of eigenvector centrality-based neuron ranking is mathematically sound and the ablation showing improved accuracy over dense baselines is compelling
- **Medium Confidence**: The method's generalization across architectures and datasets is supported by results but requires careful hyperparameter tuning that is not fully specified
- **Low Confidence**: The lack of specified fine-tuning hyperparameters and the unknown impact of pruning beyond the final Dense layer create significant reproduction barriers

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Replicate the τ sweep experiment (Table 3) to identify the optimal threshold range for different architectures and datasets
2. **Architecture Ablation Study**: Apply the method to ResNet50 and EfficientNet-B0, specifically targeting any remaining Dense layers, to quantify performance degradation in architectures designed for minimal FC layers
3. **Fine-tuning Protocol Replication**: Implement fine-tuning with multiple learning rate schedules and optimizers to determine if the reported accuracy improvements are robust to training variations or contingent on unreported optimal settings