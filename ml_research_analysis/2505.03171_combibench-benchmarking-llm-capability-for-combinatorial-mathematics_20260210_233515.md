---
ver: rpa2
title: 'CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics'
arxiv_id: '2505.03171'
source_url: https://arxiv.org/abs/2505.03171
tags:
- problems
- theorem
- solution
- combinatorial
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CombiBench, a comprehensive benchmark for
  evaluating large language models (LLMs) on combinatorial mathematics problems formalized
  in Lean 4. The benchmark consists of 100 problems spanning from middle school to
  IMO and university level, covering over ten combinatorial topics.
---

# CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics

## Quick Facts
- arXiv ID: 2505.03171
- Source URL: https://arxiv.org/abs/2505.03171
- Authors: Junqi Liu; Xiaohan Lin; Jonas Bayer; Yael Dillies; Weijie Jiang; Xiaodan Liang; Roman Soletskyi; Haiming Wang; Yunzhou Xie; Beibei Xiong; Zhengfeng Yang; Jujian Zhang; Lihong Zhi; Jia Li; Zhengying Liu
- Reference count: 27
- Primary result: Introduced CombiBench benchmark with 100 combinatorial problems formalized in Lean 4, showing current LLMs struggle significantly with combinatorial mathematics (best model solves 7/100 problems)

## Executive Summary
This paper introduces CombiBench, a comprehensive benchmark for evaluating large language models (LLMs) on combinatorial mathematics problems formalized in Lean 4. The benchmark consists of 100 problems spanning from middle school to IMO and university level, covering over ten combinatorial topics. To address the lack of suitable evaluation methods for fill-in-the-blank problems, the authors propose Fine-Eval, a standardized evaluation framework that rigorously verifies both proof-based and fill-in-the-blank questions. Experimental results show that current LLMs struggle significantly with combinatorial problems, with the best-performing model (Kimina-Prover Preview) solving only 7 out of 100 problems.

## Method Summary
The CombiBench benchmark consists of 100 combinatorial mathematics problems formalized in Lean 4, including both proof-based and fill-in-the-blank questions. The Fine-Eval evaluation framework uses a two-stage pipeline: Stage 1 attempts exact matching via `rfl` and `norm_num` tactics, while Stage 2 prompts the LLM to prove equivalence with a 42-character proof limit to prevent reward hacking. The benchmark evaluates models using whole-proof generation at sample budgets (1, 8, 16), measuring pass@N rates for both modes: "with solution" (ground truth provided) and "without solution" (model generates solution and proves it).

## Key Results
- CombiBench consists of 100 problems spanning middle school to IMO level across 10+ combinatorial topics
- Current LLMs struggle significantly, with Kimina-Prover Preview solving only 7/100 problems
- Two-stage Fine-Eval evaluation framework successfully handles fill-in-the-blank questions by verifying solution equivalence
- All 7B-or-smaller theorem provers solved 0 problems, while reasoning models (o1, DeepSeek-R1) solved 2-4 problems
- Most challenging problem required 67 lines of code after formalization, highlighting the overhead of custom definitions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage verification enables rigorous evaluation of solution-equivalent answers that differ syntactically from ground truth.
- Mechanism: Fine-Eval first attempts exact matching via `rfl` and `norm_num` tactics; if these fail, it prompts the LLM to prove equivalence (`solution = ground_truth`) with a 42-character proof limit to prevent reward hacking.
- Core assumption: Equivalent solutions can be proven via short tactic proofs without requiring extensive library support.
- Evidence anchors:
  - [section 4.1]: "In the first stage, the model answers '10 / 20' and the corresponding proof... we try to use 'norm_num' and 'rfl' to automatically prove that 10 / 20 is equivalent to 1 / 2."
  - [abstract]: "Fine-Eval... for the first time, the evaluation of fill-in-the-blank questions."
  - [corpus]: Weak direct evidence; related benchmarks (NLCO, GAUSS) focus on structured extraction or optimization, not equivalence verification.
- Break condition: When equivalence requires lemmas unavailable in Mathlib or proofs exceeding the character limit.

### Mechanism 2
- Claim: Combinatorics formalization difficulty scales with required custom definitions, not just problem complexity.
- Mechanism: Unlike algebra/number theory where Mathlib provides reusable structures, combinatorics problems require per-problem definitions (e.g., game states, graph properties), inflating code length and creating sparse reuse opportunities.
- Core assumption: The formalization overhead (not reasoning difficulty) is the primary bottleneck for current provers.
- Evidence anchors:
  - [section 3, "Challenges"]: "more than half of the problems have more than 10 lines of code after formalization... most challenging problem reached 67"
  - [section 1]: "In combinatorial mathematics, the gap between informal and formal is larger than in other areas. For a specific problem, it is often necessary to add some definitions that are unique to that problem."
  - [corpus]: No direct corpus evidence comparing definition density across mathematical domains.
- Break condition: When Mathlib accumulates sufficient combinatorial primitives to amortize definition overhead.

### Mechanism 3
- Claim: Current LLM provers fail to bridge informal-to-formal reasoning without combinatorics-specific training.
- Mechanism: Models trained on algebra/number theorem libraries cannot transfer pattern-matching to combinatorics where formalization requires translating natural language constraints into novel type definitions and predicates.
- Core assumption: The 7% solve rate reflects distribution shift, not fundamental reasoning limitations.
- Evidence anchors:
  - [section 4.2]: "Kimina-Prover Preview... has not been specifically trained on datasets pertaining to combinatorics or fill-in-the-blank question types."
  - [table 3]: All 7B-or-smaller theorem provers solve 0 problems; reasoning models (o1, DeepSeek-R1) solve 2-4 via informal strength.
  - [corpus]: Related work (Vibe Reasoning, RIMO) suggests frontier models possess latent capabilities requiring elicitation strategies.
- Break condition: When training data includes formalized combinatorics with diverse definition patterns.

## Foundational Learning

- Concept: **Lean 4 type class inference and custom structures**
  - Why needed here: CombiBench problems require defining problem-specific structures (e.g., `GameState`, `Bridge`, `isValidMove`) that integrate with Mathlib's type class hierarchy.
  - Quick check question: Can you explain why `DecidableRel` instances are required for the `GameState.adj` field?

- Concept: **Combinatorial formalization patterns (counting, bijections, invariants)**
  - Why needed here: 45% of CombiBench problems are fill-in-the-blank counting problems requiring solution construction plus correctness proofs.
  - Quick check question: How would you formalize "number of permutations with no fixed points" as a fill-in-the-blank theorem?

- Concept: **Equivalence vs. definitional equality in proof assistants**
  - Why needed here: Fine-Eval's second stage requires proving non-syntactic equivalence (e.g., `10/20 = 1/2`) without circular reasoning.
  - Quick check question: Why does Fine-Eval limit second-stage proofs to 42 characters?

## Architecture Onboarding

- Component map:
  CombiBench Dataset (100 problems in Lean 4) -> Fine-Eval Framework -> Kimina Lean Server (backend verification)

- Critical path: Problem formalization -> Solution generation -> Two-stage verification -> Pass/fail classification

- Design tradeoffs:
  - Two-stage vs. one-stage evaluation: Two-stage catches more valid solutions but doubles LLM calls and complexity.
  - PutnamBench-style fill-in-the-blank vs. modified theorem statements: Preserves original difficulty but requires new evaluation infrastructure.
  - Whole-proof generation vs. tree search: Paper uses whole-proof for cost; tree search may improve pass@N but wasn't evaluated.

- Failure signatures:
  - Models comment out code to pass compilation (Appendix A.2).
  - 7B models solve 0 problems -> capacity/domain mismatch.
  - IMO problems require 3-8 hours to formalize -> human-in-the-loop bottleneck.

- First 3 experiments:
  1. **Baseline replication**: Run Fine-Eval on a 10-problem CombiBench subset with Kimina-Prover to reproduce the 7% solve rate.
  2. **Ablation on evaluation stages**: Compare one-stage vs. two-stage Fine-Eval pass rates to quantify equivalence-proving value.
  3. **Definition reuse analysis**: Measure code overlap between CombiBench problem formalizations to identify high-value primitives for Mathlib contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent would expanding Mathlib with combinatorial definitions and lemmas improve LLM performance on CombiBench?
- Basis in paper: [explicit] The authors identify "absence of combinatorial mathematics content in existing theorem libraries" as a primary factor for model failures, noting that models "struggle with formalization because they lack pre-built definitions, lemmas, and theorems relevant to combinatorial topics."
- Why unresolved: The paper benchmarks current models but does not experiment with enriched theorem libraries.
- What evidence would resolve it: Ablation experiments comparing model performance before and after adding specific combinatorial content to Mathlib.

### Open Question 2
- Question: Can specialized training on combinatorial problems substantially improve LLM performance beyond the current 7% success rate?
- Basis in paper: [explicit] The authors state they aim to "actively promote the enhancement of large language models' capabilities in the field of combinatorics" and note that Kimina-Prover "has not been specifically trained on datasets pertaining to combinatorics."
- Why unresolved: No current model has been specifically trained on combinatorial formalization data.
- What evidence would resolve it: Training a model on combinatorial-specific synthetic data and evaluating on CombiBench.

### Open Question 3
- Question: Would tree-search methods significantly outperform whole-proof generation on combinatorial problems?
- Basis in paper: [inferred] The authors evaluated only whole-proof generation due to "economic cost and reproducibility" and did not test tree-search methods, leaving this approach unevaluated on CombiBench.
- Why unresolved: Tree-search was excluded from experiments despite being a competitive approach in other domains.
- What evidence would resolve it: Comparative experiments using BFS-Prover or similar tree-search methods on CombiBench.

### Open Question 4
- Question: How can the informal-to-formal reasoning gap specific to combinatorics be effectively bridged?
- Basis in paper: [explicit] The authors state "In combinatorial mathematics, the gap between informal and formal is larger than in other areas. For a specific problem, it is often necessary to add some definitions that are unique to that problem."
- Why unresolved: This is identified as a key challenge but no solutions are proposed or tested.
- What evidence would resolve it: Development of intermediate representations or automated definition-generation tools that reduce formalization complexity.

## Limitations
- The benchmark's narrow coverage (100 problems from specific sources) may not represent full combinatorial mathematics diversity
- The 7% solve rate should be interpreted cautiously as Kimina-Prover Preview wasn't specifically trained on combinatorics
- Fine-Eval's two-stage approach introduces complexity that may mask subtle differences between equivalent solutions
- The 42-character proof limit in Stage 2 could prematurely reject valid equivalence proofs requiring longer arguments

## Confidence

- **High confidence**: The fundamental claim that combinatorial mathematics poses unique formalization challenges due to the need for problem-specific definitions is well-supported by the 67-line maximum formalization length and the observation that Mathlib lacks sufficient combinatorial primitives.
- **Medium confidence**: The assertion that current LLMs struggle significantly with combinatorial problems (7% solve rate) is technically correct but should be qualified—this reflects performance of a specific model variant not trained on combinatorics.
- **Low confidence**: The paper's claims about equivalence verification being a novel contribution for fill-in-the-blank problems lack strong validation. The 42-character proof limit appears arbitrary, and there's no ablation study showing how often this limit rejects valid proofs versus preventing reward hacking.

## Next Checks
1. **Ablation study on proof length limits**: Systematically test how the 42-character limit affects Fine-Eval's ability to verify equivalent solutions across different problem types and solution complexities.
2. **Cross-domain formalization comparison**: Apply the same formalization pipeline to algebra and number theory problems to quantify whether combinatorial problems genuinely require 3× more definitions than other domains.
3. **Alternative evaluation frameworks**: Compare Fine-Eval against simpler approaches like strict syntactic matching or tolerance for numerical approximations to assess whether the two-stage complexity is justified by improved accuracy.