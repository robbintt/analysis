---
ver: rpa2
title: Why Do Multilingual Reasoning Gaps Emerge in Reasoning Language Models?
arxiv_id: '2510.27269'
source_url: https://arxiv.org/abs/2510.27269
tags:
- language
- reasoning
- understanding
- detected
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates why multilingual reasoning
  gaps emerge in reasoning language models. It identifies that the primary cause is
  failures in language understanding, where models struggle to translate multilingual
  inputs into the dominant language (typically English) used in their reasoning traces.
---

# Why Do Multilingual Reasoning Gaps Emerge in Reasoning Language Models?

## Quick Facts
- arXiv ID: 2510.27269
- Source URL: https://arxiv.org/abs/2510.27269
- Authors: Deokhyung Kang; Seonjeong Hwang; Daehui Kim; Hyounghun Kim; Gary Geunbae Lee
- Reference count: 40
- Primary finding: Understanding failures in translating inputs to the model's dominant language cause multilingual reasoning gaps

## Executive Summary
This paper investigates the emergence of multilingual reasoning gaps in language models designed for reasoning tasks. Through systematic analysis, the authors identify that the primary cause is not reasoning failures themselves, but rather failures in language understanding - specifically, the model's inability to translate multilingual inputs into the dominant language (typically English) used in its reasoning traces. The study demonstrates that these understanding failures can be reliably detected using various methods, with supervised approaches performing best. Building on this insight, the authors propose Selective Translation, a strategy that translates inputs only when understanding failures are detected, substantially bridging the multilingual reasoning gap while maintaining efficiency.

## Method Summary
The authors conduct a comprehensive analysis of multilingual reasoning gaps by first categorizing failure modes into understanding failures and reasoning failures. They evaluate multiple detection methods including prompting, contrastive evaluation, and supervised approaches to identify understanding failures. Using these detection methods, they implement and test Selective Translation, which applies English translation only when an understanding failure is detected. The approach is evaluated across three reasoning datasets (MATH, AMPS, and Polymath) using Llama-3.1-8B and Qwen2.5-7B models, comparing performance against both no translation and full translation baselines.

## Key Results
- Selective Translation improves average accuracy from 81.1 to 88.0 on Polymath-Low, approaching full translation performance (89.4)
- Translation is applied to only 19.3% of inputs with Selective Translation, compared to 100% with full translation
- Supervised detection methods achieve the highest accuracy in identifying understanding failures
- The approach effectively bridges multilingual reasoning gaps while maintaining translation efficiency

## Why This Works (Mechanism)
Selective Translation works by addressing the root cause of multilingual reasoning gaps - understanding failures. When a model encounters multilingual input, it must first translate this into its dominant reasoning language (typically English). If this translation fails, subsequent reasoning steps are compromised regardless of the model's reasoning capabilities. By detecting these understanding failures and selectively applying translation only when needed, the approach ensures that reasoning occurs on properly understood inputs while avoiding unnecessary translation overhead for inputs that are already comprehensible to the model.

## Foundational Learning

**Language Understanding in Multilingual Models**: Understanding how language models process multilingual inputs and why translation to a dominant language is necessary for reasoning tasks. *Why needed*: Forms the basis for identifying understanding failures as the primary issue. *Quick check*: Verify that models show degraded performance on reasoning tasks when inputs are not in their dominant language.

**Failure Mode Analysis**: The ability to distinguish between understanding failures and reasoning failures in model outputs. *Why needed*: Critical for developing targeted interventions like Selective Translation. *Quick check*: Ensure that failure categorization aligns with observed performance patterns across different translation strategies.

**Detection Methods for Understanding Failures**: Familiarity with prompting-based, contrastive, and supervised approaches for identifying comprehension issues. *Why needed*: Essential for implementing the Selective Translation strategy. *Quick check*: Validate that detection methods achieve high accuracy on held-out test sets.

## Architecture Onboarding

**Component Map**: Input -> Understanding Failure Detection -> Translation Decision -> Reasoning Engine -> Output
- Understanding Failure Detection: Identifies whether the model can comprehend the input without translation
- Translation Decision: Determines if translation to English is necessary based on detection results
- Reasoning Engine: Performs the actual reasoning task using the (potentially translated) input

**Critical Path**: Input → Understanding Failure Detection → (Optional Translation) → Reasoning Engine → Output

**Design Tradeoffs**: The key tradeoff is between translation efficiency and reasoning accuracy. Full translation guarantees comprehension but incurs unnecessary overhead for inputs the model can already understand. No translation risks reasoning on poorly understood inputs. Selective Translation balances these concerns by translating only when necessary.

**Failure Signatures**: Understanding failures manifest as degraded reasoning performance despite the model having adequate reasoning capabilities. These failures are distinguishable from reasoning failures through careful analysis of intermediate outputs and detection method performance.

**First Experiments**:
1. Evaluate baseline performance with no translation across multiple multilingual datasets
2. Test full translation baseline to establish upper performance bounds
3. Implement and validate understanding failure detection methods on held-out data

## Open Questions the Paper Calls Out

None

## Limitations
- Findings based on specific models (Llama-3.1-8B, Qwen2.5-7B) and datasets (MATH, AMPS, Polymath), limiting generalizability
- Detection methods primarily validated on in-distribution test sets, raising questions about robustness to out-of-distribution inputs
- Approach assumes optimal performance when translating only understanding failures, which may not hold for all multilingual scenarios
- Analysis focuses primarily on understanding failures, potentially overlooking other contributing factors to multilingual reasoning gaps

## Confidence

**High Confidence**: Understanding failures are the primary cause of multilingual reasoning gaps, supported by consistent performance improvements when addressing these failures through translation.

**Medium Confidence**: Selective Translation effectively bridges multilingual gaps on tested models and datasets, but generalizability to other reasoning models and languages requires further validation.

**Medium Confidence**: Understanding failures can be reliably detected using proposed methods, though real-world applicability across diverse multilingual contexts remains to be fully established.

## Next Checks

1. Test Selective Translation on additional reasoning models (e.g., GPT-4, Claude) and different model families to assess generalizability beyond current Llama and Qwen models.

2. Evaluate understanding failure detection methods and Selective Translation across a broader range of languages, particularly low-resource languages and those with significantly different linguistic structures from English.

3. Assess whether Selective Translation maintains effectiveness on extended reasoning chains and multi-step problems where early understanding failures might compound over subsequent steps.