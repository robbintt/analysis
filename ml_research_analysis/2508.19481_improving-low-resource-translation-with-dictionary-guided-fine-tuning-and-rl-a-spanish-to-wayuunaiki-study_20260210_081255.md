---
ver: rpa2
title: 'Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and
  RL: A Spanish-to-Wayuunaiki Study'
arxiv_id: '2508.19481'
source_url: https://arxiv.org/abs/2508.19481
tags:
- translation
- dictionary
- tool
- training
- bleu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for improving machine translation
  for low-resource languages, focusing on the Spanish-Wayuunaiki language pair. The
  approach combines supervised fine-tuning with reinforcement learning (RL) and an
  external dictionary tool to create a tool-augmented translation system.
---

# Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study

## Quick Facts
- arXiv ID: 2508.19481
- Source URL: https://arxiv.org/abs/2508.19481
- Reference count: 40
- Achieves +3.37 BLEU improvement over previous work for Spanish→Wayuunaiki translation

## Executive Summary
This paper proposes a novel method for improving machine translation for low-resource languages, focusing on the Spanish-Wayuunaiki language pair. The approach combines supervised fine-tuning with reinforcement learning (RL) and an external dictionary tool to create a tool-augmented translation system. The model is trained to selectively use the dictionary during translation, guided by BLEU similarity scores as rewards. The method achieves up to a +3.37 BLEU improvement over previous work and an 18% relative gain compared to a supervised baseline without dictionary access.

## Method Summary
The proposed approach uses a two-stage training process. First, supervised fine-tuning (SFT) on parallel Spanish-Wayuunaiki data incorporates synthetic examples showing dictionary lookups. Second, reinforcement learning with the GRPO algorithm fine-tunes the model further, using BLEU scores as rewards to encourage effective dictionary usage. The dictionary tool provides word-for-word translations, and the model learns when to call it during generation. The best-performing model uses the dictionary extensively, averaging 3.94 calls per translation sample.

## Key Results
- Achieves +3.37 BLEU improvement over previous work for Spanish→Wayuunaiki translation
- 18% relative gain compared to supervised baseline without dictionary access
- Best model (Qwen2.5-0.5B+SFT+RL) uses dictionary in every case, averaging 3.94 calls per sample

## Why This Works (Mechanism)
The method works by combining the precision of dictionary lookups with the generalization capabilities of LLMs. The reinforcement learning component trains the model to learn when dictionary consultation is beneficial, optimizing for translation quality rather than just dictionary usage frequency. This selective approach allows the model to handle the agglutinative nature of Wayuunaiki, where word-for-word translations from the dictionary are particularly challenging.

## Foundational Learning
- **Supervised Fine-Tuning (SFT)**: Needed to establish basic translation capabilities before RL. Quick check: Model produces coherent translations on validation set.
- **Reinforcement Learning (RL)**: Needed to optimize for quality metrics and learn dictionary usage patterns. Quick check: BLEU scores improve on validation set during RL training.
- **GRPO Algorithm**: Needed for stable RL training with language models. Quick check: Training loss decreases consistently.
- **BLEU Score**: Needed as both evaluation metric and RL reward signal. Quick check: BLEU correlates with human judgment.
- **Dictionary Tool Integration**: Needed to provide word-level translations for low-resource language. Quick check: Dictionary returns valid translations for test vocabulary.
- **Tool-Augmented Generation**: Needed to allow model to call external resources during inference. Quick check: Model can format and execute dictionary lookup calls correctly.

## Architecture Onboarding
**Component Map**: Parallel Corpus → SFT → GRPO Training → Dictionary Tool → BLEU Reward → Final Model
**Critical Path**: SFT establishes baseline translation capability → GRPO optimizes dictionary usage with BLEU rewards
**Design Tradeoffs**: Small model size (0.5B) chosen for efficiency vs. potential quality gains from larger models
**Failure Signatures**: 
- Poor SFT → Model produces malformed tool calls in RL
- Insufficient RL steps (<400) → Minimal performance gains
- CharacTer reward → -10.4% performance degradation
**First Experiments**:
1. Train SFT baseline with synthetic dictionary examples
2. Run GRPO with BLEU reward for 400 steps (minimum effective duration)
3. Test model with dictionary disabled to measure baseline contribution

## Open Questions the Paper Calls Out
1. Why does the CharacTer reward signal lead to performance regressions while BLEU improves translation quality in this RL setup? (Requires comparative analysis of optimization landscapes)
2. How do native speakers qualitatively assess the fluency and cultural appropriateness of the model's translations compared to metrics like BLEU? (Requires human evaluation study)
3. Do dictionary-guided RL methods provide greater relative gains for non-agglutinative languages compared to agglutinative languages like Wayuunaiki? (Requires testing on non-agglutinative low-resource language pair)

## Limitations
- Study focuses exclusively on Spanish→Wayuunaiki, limiting generalizability to other language pairs
- Small 0.5B parameter model may not represent patterns of larger models
- Key implementation details missing, making exact reproduction challenging

## Confidence
- High confidence: BLEU score improvements are directly measured and reproducible
- Medium confidence: Relative effectiveness of SFT+RL over SFT-only is well-established within this study
- Medium confidence: Observation that best model uses dictionary in every case is supported by data

## Next Checks
1. Test the exact methodology on at least two additional low-resource language pairs (e.g., Nahuatl-Spanish and Quechua-Spanish)
2. Re-implement the GRPO pipeline from scratch using open-source RL frameworks and verify similar BLEU improvements
3. Perform systematic ablation study varying dictionary calls per sample to determine optimal usage patterns