---
ver: rpa2
title: 'Punching Bag vs. Punching Person: Motion Transferability in Videos'
arxiv_id: '2508.00085'
source_url: https://arxiv.org/abs/2508.00085
tags:
- something
- coarse
- fine
- object
- motion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper explores whether action recognition models can transfer\
  \ high-level motion concepts across novel contexts, beyond just generalizing to\
  \ new classes or distributions. The authors propose a motion transferability framework\
  \ with three benchmark datasets\u2014Syn-TA (synthetic), K400-TA, and SSv2-TA\u2014\
  where each is structured with a coarse-to-fine class hierarchy, training on one\
  \ set and testing on another with the same coarse motions but different fine contexts."
---

# Punching Bag vs. Punching Person: Motion Transferability in Videos

## Quick Facts
- arXiv ID: 2508.00085
- Source URL: https://arxiv.org/abs/2508.00085
- Authors: Raiyaan Abdullah; Jared Claypoole; Michael Cogswell; Ajay Divakaran; Yogesh Rawat
- Reference count: 40
- Key outcome: Action recognition models struggle to transfer motion concepts across novel contexts, with 20-65% performance drops; a disentanglement strategy improves recognition in temporally challenging datasets.

## Executive Summary
This paper investigates whether action recognition models can transfer high-level motion concepts across novel contexts, beyond just generalizing to new classes or distributions. The authors propose a motion transferability framework with three benchmark datasets—Syn-TA (synthetic), K400-TA, and SSv2-TA—structured with a coarse-to-fine class hierarchy. Training on one set and testing on another with the same coarse motions but different fine contexts reveals that all evaluated models struggle with this task, with performance drops ranging from ~20-65%. Multimodal models perform better on coarse motions but still face large drops on fine motions. The study establishes a new benchmark for assessing motion transferability in action recognition.

## Method Summary
The authors propose a motion transferability framework with three benchmark datasets: Syn-TA (synthetic), K400-TA, and SSv2-TA. Each dataset is structured with a coarse-to-fine class hierarchy, allowing training on one set and testing on another with the same coarse motions but different fine contexts. The paper evaluates 13 state-of-the-art unimodal and multimodal models across these datasets, measuring their ability to transfer motion understanding to unknown contexts. A proposed disentanglement strategy that separates coarse and fine features improves recognition in temporally challenging datasets.

## Key Results
- All 13 evaluated models show performance drops of 20-65% when transferring motion understanding to unknown contexts across all three datasets.
- Multimodal models perform better on coarse motions but still experience significant drops on fine motions (50-60% drop on SSv2-TA).
- Larger models improve spatial generalization but fail to enhance temporal reasoning for motion transferability.
- The proposed coarse-to-fine disentanglement strategy reliably corrects base model mistakes in Syn-TA and SSv2-TA datasets.

## Why This Works (Mechanism)
None provided.

## Foundational Learning
- **Coarse-to-fine class hierarchy**: A structured labeling system where actions are organized from general (coarse) to specific (fine) categories. Why needed: Enables controlled evaluation of motion transferability across contexts. Quick check: Verify that each dataset has a clear parent-child relationship between coarse and fine classes.
- **Motion transferability**: The ability of models to recognize similar motions across different contexts. Why needed: Tests whether models understand underlying motion patterns rather than memorizing context-specific appearances. Quick check: Confirm that test sets contain novel contexts not present in training.
- **Multimodal action recognition**: Models that process both visual and textual information for action understanding. Why needed: Allows evaluation of whether text embeddings help or hinder motion transferability. Quick check: Ensure text models have access to both coarse and fine-grained descriptions.

## Architecture Onboarding
**Component Map**: Video encoder -> Feature extractor -> Classifier; Text encoder -> Feature extractor -> Classifier; Multimodal fusion -> Classifier
**Critical Path**: Video stream (spatial + temporal features) -> Classifier for action prediction
**Design Tradeoffs**: Spatial reasoning vs. temporal reasoning; unimodal vs. multimodal approaches; model size vs. transferability performance
**Failure Signatures**: Large performance drops when fine contexts change; reliance on background/object cues; poor temporal reasoning in novel contexts
**First Experiments**: 1) Test transferability from Syn-TA to K400-TA; 2) Evaluate coarse vs. fine motion recognition separately; 3) Apply disentanglement strategy to baseline models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can video architectures be specifically designed to improve transferability for actions requiring intensive temporal reasoning, given that increased parameter counts only aid spatial generalization?
- Basis in paper: [Explicit] The authors state that "larger models improve transferability when spatial cues dominate but struggle with intensive temporal reasoning," noting that model size correlates with performance on K400-TA (spatial) but not SSv2-TA (temporal).
- Why unresolved: Current scaling laws improve spatial feature extraction but fail to capture the dynamic, sequential dependencies required for temporal reasoning in novel contexts.
- What evidence would resolve it: A new architecture or training objective that improves Harmonic Mean scores on SSv2-TA (a temporal dataset) without simply increasing the total number of model parameters.

### Open Question 2
- Question: Why do some multimodal models achieve higher accuracy on unknown fine-grained actions compared to their coarse-grained counterparts in real-world datasets?
- Basis in paper: [Explicit] The paper highlights "counterintuitive behavior" where EZ-CLIP and FROSTER perform worse on unknown coarse classes than fine ones in K400-TA. The authors hypothesize this is due to models "leveraging specific objects in fine captions."
- Why unresolved: While the authors suggest object-level bias drives this, the exact mechanism by which fine-grained text embeddings outperform coarse abstractions in zero-shot transfer remains ambiguous.
- What evidence would resolve it: An ablation study manipulating the presence of specific object nouns in fine captions to determine if they are the sole drivers of this performance boost.

### Open Question 3
- Question: Does the proposed coarse-to-fine disentanglement strategy scale to other video-text architectures, or does it rely on specific properties of the EZ-CLIP adapter modules?
- Basis in paper: [Inferred] The authors implemented the disentanglement strategy only on EZ-CLIP, noting it "reliably corrects the base model's mistakes" in Syn-TA and SSv2-TA. It is unclear if this success transfers to other architectures.
- Why unresolved: The method was validated on a single efficient adapter-based model; its utility for heavier multimodal transformers or purely unimodal models was not tested.
- What evidence would resolve it: Applying the same residual connection strategy to other top-performing models (e.g., X-CLIP, ViFi-CLIP) and observing consistent improvements in CoarseMotion-UnknownContext metrics.

## Limitations
- The study relies on synthetic and curated datasets that may not fully capture real-world motion complexity and diversity.
- The coarse-to-fine hierarchy might oversimplify the relationship between motion patterns and contextual variations.
- The paper evaluates existing models rather than developing novel architectures specifically designed for motion transferability.

## Confidence
- **High Confidence**: The observation that existing action recognition models struggle with motion transferability across contexts is well-supported by consistent performance drops across multiple datasets and model architectures.
- **Medium Confidence**: The finding that multimodal models perform better on coarse motions than fine motions is supported by results, but interpretation of why this occurs could benefit from additional analysis.
- **Medium Confidence**: The proposed disentanglement strategy shows promise for improving recognition in temporally challenging datasets, but evaluation is limited to specific datasets.

## Next Checks
1. Validate motion transferability findings on additional real-world datasets with naturally occurring context variations beyond controlled synthetic and curated datasets.
2. Conduct ablation studies to isolate the contribution of spatial versus temporal features in observed performance drops, focusing on which components are most responsible for transferability limitations.
3. Test the disentanglement strategy across a broader range of action recognition models and datasets to assess general applicability and identify scenarios where it may be less effective.