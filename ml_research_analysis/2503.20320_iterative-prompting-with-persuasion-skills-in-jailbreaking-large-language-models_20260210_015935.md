---
ver: rpa2
title: Iterative Prompting with Persuasion Skills in Jailbreaking Large Language Models
arxiv_id: '2503.20320'
source_url: https://arxiv.org/abs/2503.20320
tags:
- prompts
- llms
- prompt
- attack
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study exploits large language models (LLMs) using an iterative
  prompting technique where each prompt is systematically modified and refined across
  multiple iterations to enhance its effectiveness in jailbreaking attacks. The approach
  analyzes response patterns of LLMs (GPT-3.5, GPT-4, LLaMa2, Vicuna, and ChatGLM)
  to adjust and optimize prompts for evading ethical and security constraints.
---

# Iterative Prompting with Persuasion Skills in Jailbreaking Large Language Models

## Quick Facts
- arXiv ID: 2503.20320
- Source URL: https://arxiv.org/abs/2503.20320
- Authors: Shih-Wen Ke; Guan-Yu Lai; Guo-Lin Fang; Hsi-Yuan Kao
- Reference count: 13
- Key outcome: Attack success rates increase as attacking prompts become more refined, with the highest ASR of 90% for GPT4 and ChatGLM and the lowest ASR of 68% for LLaMa2. The technique outperforms baseline methods (PAIR and PAP) in ASR and shows comparable performance with GCG and ArtPrompt.

## Executive Summary
This study exploits large language models (LLMs) using an iterative prompting technique where each prompt is systematically modified and refined across multiple iterations to enhance its effectiveness in jailbreaking attacks. The approach analyzes response patterns of LLMs (GPT-3.5, GPT-4, LLaMa2, Vicuna, and ChatGLM) to adjust and optimize prompts for evading ethical and security constraints. Persuasion strategies are employed to enhance prompt effectiveness while maintaining consistency with malicious intent. Results show that attack success rates increase as attacking prompts become more refined, with the highest ASR of 90% for GPT4 and ChatGLM and the lowest ASR of 68% for LLaMa2. The technique outperforms baseline methods (PAIR and PAP) in ASR and shows comparable performance with GCG and ArtPrompt.

## Method Summary
The method involves fine-tuning GPT-3.5-turbo-0125 as an attacker model using knowledge distillation on 520 persuasion-rephrased harmful queries from AdvBench. The attacker employs five persuasion techniques (Logical Appeal, Authority Endorsement, Misrepresentation, Evidence-based Persuasion, and one unnamed) to generate refined prompts. These prompts are iteratively tested across five victim LLMs (GPT-3.5, GPT-4, LLaMa2, Vicuna, ChatGLM) with success measured via keyword detection (absence of "I'm sorry"/"can't provide"). A Weighted Attack Success Rate (WASR) aggregates scores across models using fixed weights, and this score feeds back to the attacker for next-round refinement across up to 4 iterations.

## Key Results
- Iterative prompting with persuasion techniques achieves ASR of 90% for GPT4 and ChatGLM, and 68% for LLaMa2
- WASR increases from 61.82 (Round 1) to 83.70 (Round 4), demonstrating progressive improvement
- Technique outperforms baseline methods (PAIR and PAP) in ASR while showing comparable performance with GCG and ArtPrompt
- Certain categories show inconsistent vulnerability (e.g., "hate crime" at 0% ASR vs. "hate speech" over 80% ASR)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Persuasion techniques systematically increase jailbreak success by reframing malicious queries into formats that bypass safety classifiers.
- Mechanism: The attacker model applies persuasion strategies (Logical Appeal, Authority Endorsement, Misrepresentation, Evidence-based Persuasion) to rephrase harmful queries. These techniques leverage LLMs' training on helpful, academic, and expert discourse to trigger compliance heuristics that override refusal patterns.
- Core assumption: LLMs process persuasion cues similarly to humans, responding to authority framing and logical structuring even when underlying intent remains malicious.
- Evidence anchors:
  - [abstract] "Persuasion strategies enhance prompt effectiveness while maintaining consistency with malicious intent."
  - [section 3.2] "Logical Appeal, Authority Endorsement, Misrepresentation, and Evidence-based Persuasion proving most effective"
  - [corpus] Weak direct evidence; neighbor papers focus on semantic dissimilarity and fuzzing rather than persuasion taxonomy validation.
- Break condition: If target LLMs are fine-tuned to detect persuasion patterns or flag authority appeals in sensitive contexts, ASR gains would diminish.

### Mechanism 2
- Claim: Iterative feedback loops with weighted scoring progressively optimize attack prompts for multi-model effectiveness.
- Mechanism: Each generated prompt is tested across five victim LLMs. Success is determined via keyword detection (absence of "I'm sorry" or "can't provide"). The Weighted Attack Success Rate (WASR) assigns higher weights to more robust models (LLaMa2: 0.22, GPT-4: 0.21), rewarding prompts that breach harder targets. This score feeds back to the attacker for next-round refinement.
- Core assumption: Keyword-based detection accurately proxies harmful output; model defense rankings generalize across prompt distributions.
- Evidence anchors:
  - [section 3.4] "Successful attacks receive a score to evaluate prompt quality... the score is fed back to the attacker to refine prompts"
  - [table 2] WASR increases from 61.82 (Round 1) to 83.70 (Round 4), demonstrating iterative improvement.
  - [corpus] AutoAdv and TurboFuzzLLM use similar iterative/mutation-based refinement, supporting the general approach.
- Break condition: If keyword detection produces false positives (harmful outputs containing apologies) or false negatives (refusals without keywords), scoring signal degrades.

### Mechanism 3
- Claim: Cross-model evaluation creates transferable attack prompts that generalize across different LLM architectures.
- Mechanism: By testing each prompt against multiple victim models simultaneously and aggregating scores, the attacker learns prompt features that succeed broadly rather than overfitting to one model's refusal patterns.
- Core assumption: Safety alignment weaknesses are partially shared across models trained on similar corpora and alignment procedures.
- Evidence anchors:
  - [table 2] Round 4 ASR ranges from 68% (LLaMa2) to 90% (GPT-4, ChatGLM), showing consistent cross-model vulnerability.
  - [section 4.1] "the attacker's success rate increases with more iterations"
  - [corpus] Transferability is a common theme in neighbor papers (Alphabet Index Mapping, AutoAdv), but not explicitly validated for persuasion-based attacks.
- Break condition: If models diverge significantly in alignment training data or refusal patterns, transferability would decrease.

## Foundational Learning

- Concept: **Prompt-level vs. token-level attacks**
  - Why needed here: This paper operates in prompt-level attacks (syntactically valid sentences), distinct from gradient-based suffix methods like GCG.
  - Quick check question: Can you explain why interpretability matters for iterative refinement compared to gibberish suffixes?

- Concept: **Knowledge distillation for adversarial fine-tuning**
  - Why needed here: The attacker model is fine-tuned using a teacher model's outputs on persuasion-rephrased queries.
  - Quick check question: What information flows from teacher to student in this setup, and what role does the one-shot template play?

- Concept: **Weighted scoring for multi-objective optimization**
  - Why needed here: WASR balances attack success across models with varying defense strengths.
  - Quick check question: Why assign higher weights to more robust models rather than equal weighting?

## Architecture Onboarding

- Component map:
  Attacker Model -> Checking Mechanism -> Victim Models (GPT-3.5, GPT-4, LLaMa2, Vicuna, ChatGLM) -> WASR Calculator -> Feedback Loop

- Critical path:
  1. Harmful query input → Attacker generates persuasion-reframed prompt
  2. Checking mechanism validates intent preservation
  3. Prompt dispatched to all five victims simultaneously
  4. Keyword detection scores each response (0 or 1)
  5. ASR calculated per model, multiplied by weights, summed to WASR
  6. WASR fed back to attacker with prompt for next-round refinement

- Design tradeoffs:
  - Keyword detection vs. LLM-based evaluation: Keyword method is fast but may miss nuanced refusals; LLM evaluation is costlier but more accurate.
  - Fixed vs. adaptive weights: Weights derived from external benchmark (Shu et al.) may not match current model versions.
  - Round limit (4): Paper notes quality degrades beyond Round 4 as prompts drift from original intent.

- Failure signatures:
  - Intent drift: Prompts become irrelevant or semantically distant from original query (Round 5+).
  - Category-specific failures: "Hate crime" queries maintain 0% ASR; attacker cannot generate effective prompts for certain topics.
  - Translation edge case: Traditional Chinese prompts (OURS-TL) show higher ASR than English, suggesting language-specific safety gaps.

- First 3 experiments:
  1. Reproduce Round 1→4 ASR trajectory on a subset of 10 AdvBench prompts to validate iterative improvement claim.
  2. Ablate persuasion techniques: Test attacker with single technique (e.g., Authority only) vs. all five to measure relative contribution.
  3. Test cross-dataset generalization: Apply Round 4 attacker to a held-out harmful prompt set (not AdvBench) to assess overfitting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can an automated method be developed to effectively assess whether iteratively refined prompts maintain alignment with the original malicious intent?
- Basis in paper: [explicit] The authors state in the Conclusion that they currently rely on manual judgment to detect intent deviation, which is inefficient, and explicitly aim to develop an automated assessment method in the future.
- Why unresolved: The iterative nature of the technique causes prompts to diverge from the original query, and current automated checking mechanisms (Section 3.3) appear insufficient to handle this without manual oversight.
- What evidence would resolve it: A proposed algorithm or evaluation metric that can quantitatively measure semantic consistency between the original and refined prompts without human intervention.

### Open Question 2
- Question: To what extent does integrating reinforcement learning (RL) and human-in-the-loop verification enhance the robustness of LLMs against iterative persuasion attacks?
- Basis in paper: [explicit] The Discussion section suggests that future efforts "should integrate reinforcement learning and human-in-the-loop verification" to shift AI from reactive to proactive defense.
- Why unresolved: The current study focuses on offensive capabilities (attack success rates) and suggests these defensive mechanisms but does not implement or test them.
- What evidence would resolve it: Comparative experiments showing a reduction in Attack Success Rate (ASR) when RL-based defense agents or human verification steps are applied to the victim models.

### Open Question 3
- Question: What specific alignment or training disparities cause LLMs to exhibit zero tolerance for "hate crime" prompts while remaining highly vulnerable to "hate speech" prompts?
- Basis in paper: [inferred] Section 4.3 notes a distinct anomaly where ASR for "hate crime" was 0%, yet "hate speech" reached over 80%, prompting the authors to highlight the need for "targeted defense" without explaining the root cause.
- Why unresolved: The paper identifies the inconsistent susceptibility (0% vs. 80%+ success) but does not investigate whether this is due to dataset bias, safety tuning granularities, or semantic nuances.
- What evidence would resolve it: An analysis of the victim models' internal activations or safety classifier thresholds when processing "hate crime" versus "hate speech" prompts.

## Limitations

- The study relies on keyword-based ASR detection, which may not accurately capture nuanced harmful outputs or sophisticated refusals that avoid trigger phrases.
- The transferability claims assume shared safety weaknesses across models, but model-specific alignment training could invalidate this assumption.
- The persuasion techniques' effectiveness depends on LLMs processing authority and logical appeals similarly to humans, which may not hold for all architectures.

## Confidence

- High confidence: Iterative improvement trajectory (WASR increases from 61.82 to 83.70 over 4 rounds) - this is directly measurable and reproducible
- Medium confidence: Cross-model transferability - supported by consistent ASR ranges but not explicitly validated for persuasion-based attacks
- Medium confidence: Persuasion technique effectiveness - shows qualitative improvements but lacks ablation studies to isolate individual technique contributions
- Low confidence: Keyword detection as proxy for harmful output - known limitation acknowledged but not quantified for false positive/negative rates

## Next Checks

1. **Ablation of persuasion techniques**: Systematically test attacker model with individual techniques (Authority only, Logical Appeal only, etc.) versus full set to quantify each technique's contribution to ASR gains.
2. **Cross-dataset generalization**: Apply the Round 4 attacker model to a completely held-out harmful prompt set (not AdvBench) to assess whether iterative refinement overfits to the training distribution.
3. **Keyword detection validation**: Manually evaluate 100 randomly selected responses to determine false positive and false negative rates of the keyword-based ASR scoring mechanism.