---
ver: rpa2
title: 'Optimism Without Regularization: Constant Regret in Zero-Sum Games'
arxiv_id: '2506.16736'
source_url: https://arxiv.org/abs/2506.16736
tags:
- then
- play
- regret
- fictitious
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that Optimistic Fictitious Play (OFP) achieves
  constant regret in 2x2 zero-sum games, matching the optimal rate of regularized
  Optimistic FTRL. The key insight is that optimism prevents energy growth in the
  dual payoff space, unlike standard Fictitious Play which sees energy strictly increase.
---

# Optimism Without Regularization: Constant Regret in Zero-Sum Games

## Quick Facts
- arXiv ID: 2506.16736
- Source URL: https://arxiv.org/abs/2506.16736
- Reference count: 40
- Primary result: Optimistic Fictitious Play achieves constant regret in 2×2 zero-sum games without regularization.

## Executive Summary
This paper proves that Optimistic Fictitious Play (OFP) achieves constant regret in 2×2 zero-sum games, matching the optimal rate of regularized Optimistic FTRL. The key insight is that optimism prevents energy growth in the dual payoff space, unlike standard Fictitious Play which sees energy strictly increase. The proof technique uses a geometric view of OFP as skew-gradient descent and shows that an energy function of dual iterates remains bounded. In contrast, the paper also proves a lower bound of Ω(√T) for Alternating Fictitious Play in 2×2 games, separating the capabilities of optimism versus alternation in the unregularized regime. Experimental results on higher-dimensional games suggest constant regret bounds may extend beyond 2×2.

## Method Summary
The method implements Optimistic Fictitious Play (OFP) as a variant of Follow-the-Leader with optimism. At each round, OFP computes a predicted dual vector using the extrapolation ỹyₜ₊₁ = 2yₜ - yₜ₋₁, selects the best-response primal strategy based on this prediction, and updates the cumulative payoff vector. The algorithm maintains cumulative payoff vectors yₜ and uses a choice map Q to select primal strategies from dual vectors. For 2×2 games, the analysis reduces to a 2D subspace where an energy function remains bounded. Experiments run T=10,000 iterations on synthetic payoff matrices (Identity, RPS, Random) for n×n games, comparing regret trajectories across standard FP, OFP, and Alternating FP.

## Key Results
- OFP achieves O(1) constant regret in 2×2 zero-sum games without regularization.
- Standard Fictitious Play exhibits Ω(√T) regret in the same setting.
- Alternating Fictitious Play achieves only Ω(√T) regret without regularization, even with optimal tiebreaking.
- Experimental results on higher-dimensional games suggest constant regret may extend beyond 2×2.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimistic Fictitious Play achieves constant regret by preventing energy growth in dual payoff space.
- Mechanism: The optimism term (predicting next feedback) creates a predicted dual vector ẽyₜ₊₁ = 2yₜ - yₜ₋₁ that, when combined with skew-symmetry of the game matrix J, ensures the one-step energy change ΔΨ(yₜ) ≤ ⟨∂Ψ(yₜ₊₁), J·∂Ψ(ẽyₜ₊₁)⟩ becomes non-positive when predicted and true dual vectors map to the same primal vertex.
- Core assumption: The payoff matrix A has a unique interior Nash equilibrium (satisfied for generic 2×2 zero-sum games).
- Evidence anchors:
  - [abstract] "Our proof technique leverages a geometric view of Optimistic Fictitious Play in the dual space of payoff vectors, where we show a certain energy function of the iterates remains bounded over time."
  - [Section 3.1] Expression (5): "For Optimistic FP: ΔΨ(yₜ) ≤ ⟨∂Ψ(yₜ₊₁), J·∂Ψ(ẽyₜ₊₁)⟩" with skew-symmetry ensuring non-positive growth when ∂Ψ(yₜ₊₁) = ∂Ψ(ẽyₜ₊₁).
  - [corpus] Weak direct corpus support; related work (Fast and Furious Symmetric Learning) studies unregularized algorithms but without the optimism-energy connection.
- Break condition: If ∂Ψ(yₜ₊₁) ≠ ∂Ψ(ẽyₜ₊₁) (predicted and true dual vectors land in different primal best-response regions), energy may increase temporarily.

### Mechanism 2
- Claim: In 2×2 games, dual iterates lie in a 2D subspace where cycling invariants guarantee bounded energy.
- Mechanism: Under Assumption 1 on the payoff matrix, all dual vectors satisfy y₁² = -ρ₁·y₁¹ and y₂² = -ρ₂·y₂¹, reducing dynamics to 2D. When energy ψ(zₜ) exceeds threshold B, Lemma 4.10 proves dual vectors must cycle through regions P₁→P₂→P₃→P₄ with non-increasing energy at each transition because Q(zₜ₊₁) = Q(ẽzₜ₊₁) holds.
- Core assumption: 2×2 game structure enabling subspace reduction; energy threshold B correctly bounds the ℓ₁-ball of radius 6amax.
- Evidence anchors:
  - [Section 4] Proposition 4.2: "for every t ≥ 1, it holds that y₁² = -ρ₁·y₁¹ and y₂² = -ρ₂·y₂¹" establishing the 2D subspace.
  - [Section 4.2] Lemma 4.10: "Suppose ψ(zₜ) > B and zₜ ∈ ∂Pᵢ. Then (1) Either (i) ẽzₜ₊₁, zₜ₊₁ ∈ ∂Pᵢ or (ii) ẽzₜ₊₁, zₜ₊₁ ∈ Pᵢ₊₁; (2) Δψ(zₜ) ≤ 0."
  - [corpus] No direct corpus evidence for this specific 2×2 subspace mechanism.
- Break condition: Extension to n×n games would require proving analogous subspace structure or invariants, which remains unproven.

### Mechanism 3
- Claim: Alternation without optimism fails to achieve o(√T) regret because predicted vectors lag behind true dynamics.
- Mechanism: In Alternating FP, the predicted vector ẽzₜ₊₁ uses stale information (depending on parity of t), causing misalignment between ∂Ψ(zₜ₊₁) and ∂Ψ(ẽzₜ₊₁) near region boundaries. This produces Ω(√T) phase structure where energy strictly increases in Ω(K/2) phases with K = Ω(√T).
- Core assumption: Initialization with irrational coordinate p ∈ (3/4, 1) ensures no pathological tiebreaking on certain boundaries.
- Evidence anchors:
  - [Section 3] Theorem 3.2: "On the 2×2 Matching Pennies game, Alternating Fictitious Play, using any tiebreaking rule and for nearly all initializations, has regret at least Ω(√T)."
  - [Section D.3.1] Proposition D.9: "for at least K/2 phases k, ψ(zₜₖ) ≥ ψ(zₜₖ₋₁) + 1" combined with T = Στₖ ≤ Θ(K²).
  - [corpus] Related work (Katona et al. 2024) shows Alternating FTRL achieves O(T^(1/5)) with regularization, confirming regularization is necessary for alternation to help.
- Break condition: With regularization (bounded stepsize), alternation can achieve improved rates; the lower bound specifically targets the unregularized η → ∞ regime.

## Foundational Learning

- Concept: **Fictitious Play (FP) and Follow-the-Leader (FTL)**
  - Why needed here: OFP is the optimistic variant (α=1) of α-OFP; understanding standard FP (α=0) as FTL with unbounded stepsize η→∞ is essential to grasp why regularization typically helps and why optimism without regularization is surprising.
  - Quick check question: Explain why FTL can have Ω(T) regret in adversarial settings but FP achieves sublinear regret in zero-sum games.

- Concept: **Regret and Duality Gap**
  - Why needed here: The paper proves Reg(T) = Ψ(yₜ₊₁) (Proposition 3.4), connecting energy bounds directly to regret bounds. Sublinear regret corresponds to time-average convergence to Nash equilibrium (Proposition 2.1).
  - Quick check question: Define total regret Reg(T) for a two-player zero-sum game and state its relationship to duality gap of time-average iterates.

- Concept: **Subgradients and Support Functions**
  - Why needed here: The energy function Ψ(y) = max_{x∈Δm×Δn} ⟨x, y⟩ is the support function of the product simplex; its subgradient ∂Ψ(y) = argmax_{x∈Δm×Δn} ⟨x, y⟩ encodes the best-response mapping central to the skew-gradient descent view.
  - Quick check question: What is ∂Ψ(y) at a point y where the maximum is achieved at a unique vertex versus at multiple vertices?

## Architecture Onboarding

- Component map:
  - Primal iterates (x₁ᵗ, x₂ᵗ) -> Vertices of strategy simplices, selected by choice map Q from dual vectors
  - Dual iterates (y₁ᵗ, y₂ᵗ) -> Cumulative payoff vectors in R^{m+n}, updated as yᵗ = yᵗ⁻¹ + J·xᵗ⁻¹
  - Predicted dual vectors (ỹyₜ₊₁) -> 2yₜ - yₜ₋₁ for OFP; this is the optimism term enabling faster convergence
  - Choice map Q -> Maps dual vectors to primal vertices via best-response; encodes tiebreaking rules
  - Energy function ψ (for 2×2 case) -> ψ(z) = ⟨z, M·Q(z)⟩ where z ∈ R² is the reduced dual vector and M encodes the payoff structure

- Critical path:
  1. Initialize x₀¹, x₀² and set y₀¹ = 0, y₀² = 0
  2. Each round: compute predicted dual ỹyₜ₊₁ = 2yₜ - yₜ₋₁, get primal xₜ₊₁ = Q(ỹyₜ₊₁), update dual yₜ₊₁ = yₜ + J·xₜ
  3. Track energy ψ(zₜ); if it exceeds threshold B, verify cycling invariants hold
  4. Total regret after T rounds is Ψ(y_{T+1}) = ψ(z_{T+1})

- Design tradeoffs:
  - Any tiebreaking allowed: Theorem 3.1 holds for all tiebreaking rules, but specific dynamics depend on tiebreaking choices
  - 2×2 vs higher dimensions: Constant regret proven only for 2×2; experiments suggest extension but no proof exists
  - Optimism vs alternation: Optimism achieves O(1); alternation alone achieves only Ω(√T) without regularization

- Failure signatures:
  - If energy ψ(zₜ) grows as √T rather than remaining bounded, verify the 2×2 subspace reduction (Proposition 4.2) is correctly applied
  - If Alternating FP shows constant regret, check that alternation logic correctly uses stale predicted vectors (Definition D.4)
  - If Q(ẽzₜ₊₁) ≠ Q(zₜ₊₁) on boundary points, energy may increase; verify tiebreaking consistency

- First 3 experiments:
  1. Replicate Figure 1: Run standard FP, OFP, and Alternating FP on Matching Pennies (2×2), 15×15 identity, and 15×15 RPS for T=10000 iterations. Plot regret over time; verify OFP shows bounded regret while FP and AFP show √T growth.
  2. Test tiebreaking robustness: Run OFP on 2×2 games with random payoffs using both lexicographical and randomized tiebreaking. Verify constant regret holds regardless of tiebreaking method (as claimed in Theorem 3.1).
  3. Probe higher dimensions: Run OFP on n×n games (n=15, 25, 50) for identity, RPS, and random [0,1] matrices. Report regret at T=10000 to test whether empirical constant regret extends beyond 2×2 (current experiments in Table 2 suggest it does).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Optimistic Fictitious Play (OFP) obtain O(1) regret in general n×n zero-sum games?
- Basis: [explicit] Section 5 states, "Formally proving whether Optimistic Fictitious Play obtains constant regret in all zero-sum games remains an important open question."
- Why unresolved: The current proof relies on a geometric analysis of energy growth in a two-dimensional subspace, which is specific to the 2×2 setting.
- What evidence would resolve it: A theoretical proof generalizing the bounded energy property to n dimensions or a counter-example exhibiting ω(1) regret in higher dimensions.

### Open Question 2
- Question: Can the constant regret guarantee for OFP be extended to games with non-unique or non-interior Nash equilibria?
- Basis: [inferred] Theorem 3.1 explicitly requires the game to have a "unique, interior Nash equilibrium," suggesting the result may not hold or requires different proof techniques for degenerate cases.
- Why unresolved: The proof utilizes Assumption 1, which requires strict inequalities derived from an interior equilibrium to define the subspace dynamics.
- What evidence would resolve it: An analysis showing the energy function remains bounded for singular payoff matrices or a construction where energy diverges on the boundary.

### Open Question 3
- Question: Does the Ω(√T) lower bound for Alternating Fictitious Play hold for rational initializations?
- Basis: [inferred] Theorem 3.2 proves the lower bound for an initialization specifically chosen to be irrational (p ∈ (3/4, 1)).
- Why unresolved: The proof leverages irrationality to control tie-breaking behaviors and phase structures, leaving the behavior under standard rational initializations undefined.
- What evidence would resolve it: A generalized lower bound proof valid for rational p, or the identification of a rational initialization that admits o(√T) regret.

## Limitations

- Dimensionality constraint: The constant regret proof relies critically on the 2D subspace structure of 2×2 games. Extension to n×n games remains conjectural despite experimental evidence.
- Initialization sensitivity: The Ω(√T) lower bound for Alternating FP requires specific irrational initializations. The paper claims "nearly all" initializations satisfy this, but quantitative bounds on measure of "good" initializations are not provided.

## Confidence

- High confidence: The geometric interpretation of OFP as skew-gradient descent with bounded energy (Mechanism 1) is well-supported by the proof structure and Proposition 3.4 linking energy to regret.
- Medium confidence: The 2×2 subspace reduction and cycling argument (Mechanism 2) is mathematically rigorous but narrow in applicability. The proof is complete for 2×2 but extension methods remain open.
- Medium confidence: The lower bound for Alternating FP (Mechanism 3) is carefully constructed for the specific 2×2 Matching Pennies game with controlled initializations, but may not generalize to other game structures.

## Next Checks

1. Dimensionality test: Run OFP on n×n games (n=10, 25, 50) for T=20,000 iterations across identity, RPS, and random matrices. Plot log-regret to empirically verify whether constant regret persists beyond 2×2.

2. Initialization robustness: Systematically vary initialization in Alternating FP (rational vs irrational coordinates) on 2×2 Matching Pennies. Quantify the fraction of initializations yielding Ω(√T) vs constant regret.

3. Tiebreaking consistency: Implement multiple tiebreaking rules (lexicographic, random, nearest-neighbor) in OFP on 2×2 games with random payoffs. Verify Theorem 3.1 holds by measuring whether regret remains bounded across all rules.