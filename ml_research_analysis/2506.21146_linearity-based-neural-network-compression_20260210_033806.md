---
ver: rpa2
title: Linearity-based neural network compression
arxiv_id: '2506.21146'
source_url: https://arxiv.org/abs/2506.21146
tags:
- compression
- neurons
- layer
- weights
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces linearity-based neural network compression,
  a novel method that exploits the partial linearity of ReLU-like activation functions
  to reduce model size. The core idea is that neurons with near-constant activation
  rates behave linearly and can be replaced with shortcut connections, redistributing
  their weights to subsequent layers.
---

# Linearity-based neural network compression

## Quick Facts
- arXiv ID: 2506.21146
- Source URL: https://arxiv.org/abs/2506.21146
- Reference count: 7
- Primary result: Compresses neural networks to 1/4 of original size by exploiting partial linearity in ReLU-like activations

## Executive Summary
This paper introduces a novel neural network compression method that exploits the partial linearity of ReLU-like activation functions. The approach identifies neurons that behave nearly linearly (with near-constant activation rates) and replaces them with shortcut connections, redistributing their weights to subsequent layers. This method can compress models to 1/4 of their original size while maintaining performance in most cases, with minimal interference when combined with existing pruning techniques.

## Method Summary
The method identifies neurons with activation rates close to constant (threshold of 0.1) as candidates for removal. These linear neurons can be eliminated without changing network outputs, with their weights redistributed to subsequent layers via shortcut connections. The approach is theoretically grounded, showing that linear neurons can be removed without affecting outputs, and practically implemented using activation rate thresholds. The method works particularly well for deeper networks with varying layer sizes, achieving significant compression ratios with minimal accuracy degradation.

## Key Results
- Compresses models to 1/4 of original size while maintaining performance
- Achieves up to 10% compression without performance loss, up to 33% with minor accuracy degradation
- Minimal interference when combined with existing importance-based pruning techniques
- Particularly effective for deeper networks with varying layer sizes

## Why This Works (Mechanism)
The method exploits the mathematical property that ReLU-like activation functions are partially linear. When neurons exhibit near-constant activation rates, they behave linearly and can be replaced with shortcut connections without changing the network's output. This allows for the removal of redundant computation while preserving model functionality.

## Foundational Learning
- **ReLU linearity property**: ReLU and similar activations are piecewise linear, enabling linear behavior under constant activation rates
  - Why needed: Forms the theoretical basis for identifying compressible neurons
  - Quick check: Verify ReLU(x) = max(0,x) is linear when x>0

- **Activation rate threshold concept**: Using a threshold (0.1) to approximate linearity in practice
  - Why needed: Enables practical identification of linear neurons in trained models
  - Quick check: Test different threshold values on sample networks

- **Shortcut connection redistribution**: Mechanism for transferring weights from removed neurons to subsequent layers
  - Why needed: Preserves network functionality when removing linear neurons
  - Quick check: Verify output equivalence after weight redistribution

## Architecture Onboarding
- **Component map**: Input -> Linear neuron identification -> Weight redistribution -> Shortcut connections -> Output
- **Critical path**: Neuron activation rate calculation → Linear neuron detection → Weight redistribution → Model compression
- **Design tradeoffs**: Threshold sensitivity vs. compression ratio vs. accuracy retention
- **Failure signatures**: Performance degradation when threshold is too aggressive; insufficient compression when threshold is too conservative
- **First experiments**:
  1. Test threshold sensitivity on CIFAR-10 with standard CNN architectures
  2. Compare combined performance with sparsity-based pruning on ImageNet
  3. Evaluate compression-accuracy trade-off across different network depths

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Effectiveness depends significantly on network depth and architecture heterogeneity
- Activation rate threshold of 0.1 is heuristic and may require tuning for different domains
- Experimental validation primarily focuses on sparsity-based pruning compatibility

## Confidence
- **High confidence**: Theoretical foundation regarding linear neurons and shortcut connections
- **Medium confidence**: Empirical compression results on benchmark datasets
- **Medium confidence**: Claims about compatibility with existing pruning methods

## Next Checks
1. Test the method across diverse architecture families (transformers, recurrent networks) to assess generalizability beyond CNNs
2. Evaluate the impact of different activation rate thresholds on compression-accuracy trade-offs across multiple domains
3. Conduct ablation studies comparing this approach against state-of-the-art pruning and quantization methods in combined scenarios