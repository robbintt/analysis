---
ver: rpa2
title: 'TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models'
arxiv_id: '2511.16423'
source_url: https://arxiv.org/abs/2511.16423
tags:
- learning
- federated
- prompt
- data
- one-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TOFA, a training-free one-shot federated
  adaptation framework for vision-language models (VLMs) in federated learning settings.
  The key innovation is adapting VLMs to downstream tasks through a single round of
  communication between clients and server, without requiring additional training
  resources on either side.
---

# TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models

## Quick Facts
- arXiv ID: 2511.16423
- Source URL: https://arxiv.org/abs/2511.16423
- Authors: Li Zhang; Zhongxuan Han; XiaoHua Feng; Jiaming Zhang; Yuyuan Li; Linbo Jiang; Jianan Lin; Chaochao Chen
- Reference count: 40
- Key outcome: Training-free one-shot federated adaptation framework for VLMs using single communication round without model training

## Executive Summary
This paper introduces TOFA, a training-free one-shot federated adaptation framework for vision-language models (VLMs) in federated learning settings. The key innovation is adapting VLMs to downstream tasks through a single round of communication between clients and server, without requiring additional training resources on either side. TOFA employs both visual and textual pipelines to extract task-relevant representations, using hierarchical Bayesian models for personalized visual adaptation and LLM-generated prompt alignment for textual robustness. Extensive experiments across nine datasets demonstrate TOFA consistently outperforms existing one-shot baselines and even surpasses several training-based federated VLM adaptation methods, achieving accuracy improvements of up to 3-4% over baseline methods while maintaining training-free operation.

## Method Summary
TOFA adapts VLMs to downstream tasks in federated settings through a single communication round. The visual pipeline uses a hierarchical Bayesian model where clients compute local class statistics (means, covariances) and transmit them to the server, which computes global posteriors using Inverse-Wishart priors. Clients then compute personalized posteriors using power priors and classify via Gaussian Discriminant Analysis. The textual pipeline generates LLM descriptions for each class, computes local importance scores, and the server aggregates globally robust prompt weights. An adaptive weight calibration mechanism combines predictions from both modalities based on relative confidence, balancing personalization and generalization to handle data heterogeneity. The method operates without any model training, making it particularly suitable for resource-constrained distributed environments.

## Key Results
- TOFA achieves accuracy improvements of up to 3-4% over baseline methods on various vision datasets
- The method consistently outperforms existing one-shot baselines and even surpasses several training-based federated VLM adaptation methods
- Ablation studies show fusion improves over both visual-only and text-only variants across all tested datasets
- TOFA maintains training-free operation while achieving competitive performance in federated VLM adaptation

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Bayesian Prompt Distribution Learning
- **Claim:** Personalized class-specific prototype distributions can be inferred from visual features without gradient-based optimization by treating global statistics as priors for local posteriors
- **Mechanism:** Clients transmit local feature statistics (class means, covariances) to server → server computes global posterior using conjugate Inverse-Wishart priors → global posterior serves as informative prior for each client's local inference via power prior with coefficient α → MAP estimation yields personalized Gaussian parameters → Gaussian Discriminant Analysis (GDA) produces classification probabilities
- **Core assumption:** CLIP visual embeddings are approximately Gaussian-distributed per class; conjugate prior structure yields meaningful posteriors even with limited local samples
- **Evidence anchors:** [abstract] "visual pipeline uses a hierarchical Bayesian model to learn personalized, class-specific prototype distributions"; [Section 3.2] "we adopt the power prior to ensure that the posterior is not overly impacted by global information"; [corpus] Related work FedBEns similarly uses Bayesian inference for one-shot FL

### Mechanism 2: Global Text Prompt Alignment for Robustness
- **Claim:** LLM-generated text prompts can be globally filtered and weighted to produce robust textual classifiers that generalize across heterogeneous clients
- **Mechanism:** Each client generates M+1 text prompts per class (manual + LLM) → computes per-prompt classification confidence → server aggregates importance scores via KL-divergence-like criterion comparing LLM prompts to manual baseline → softmax weighting yields globally robust prompt ensemble → weighted average of prompt embeddings used for classification
- **Core assumption:** Manual prompts ("A photo of a {class}") are universally robust baselines; prompts with consistently high discriminative confidence across clients are more generalizable
- **Evidence anchors:** [abstract] "textual pipeline evaluates and globally aligns the generated local text prompts for robustness"; [Section 3.3] "score function (6), which is analogous to the KL divergence, assigns a higher importance score to text prompts with stronger robustness"
- **Break condition:** LLM generates semantically incorrect descriptions; heterogeneous LLMs produce incompatible embeddings; manual prompts are not robust for the target domain

### Mechanism 3: Confidence-Based Adaptive Multimodal Fusion
- **Claim:** Sample-wise fusion of visual and textual predictions based on relative confidence reduces generalization error under data heterogeneity
- **Mechanism:** Compute max-softmax confidence for both visual and textual classifiers → derive mixing coefficient η(z) via logistic function of log-confidence ratio → fused prediction f_M(z) = η(z)f_V(z) + (1-η(z))f_T(z). Higher-confidence modality receives higher weight
- **Core assumption:** Classifier confidence correlates with accuracy when properly calibrated; Theorem 1 proves covariance between η and loss difference affects generalization bound
- **Evidence anchors:** [abstract] "adaptive weight calibration mechanism...balancing personalization and generalization to handle data heterogeneity"; [Section 3.4] "the fused classifier naturally favors the modality with higher predictive accuracy for given regions"; [Section 4.3, Figure 3] Ablation shows fusion outperforms either modality alone
- **Break condition:** Both classifiers poorly calibrated (confidence ≠ accuracy); one modality consistently dominates; confidence scores saturated (temperature τ needs tuning)

## Foundational Learning

- **Concept: Gaussian Discriminant Analysis (GDA)**
  - **Why needed here:** Classification in visual pipeline relies on GDA with learned Gaussian parameters; requires understanding how quadratic discriminant functions arise from Bayes rule with Gaussian likelihoods
  - **Quick check question:** Given class means μ₁, μ₂ and shared covariance Σ, write the GDA decision boundary equation

- **Concept: Conjugate Priors (Normal-Inverse-Wishart)**
  - **Why needed here:** Enables closed-form posterior updates in hierarchical Bayesian model; critical for understanding how global-to-local inference works without iterative optimization
  - **Quick check question:** Why does conjugacy guarantee the posterior has the same functional form as the prior?

- **Concept: Calibration and Confidence-Accuracy Alignment**
  - **Why needed here:** Adaptive fusion assumes well-calibrated classifiers; temperature scaling is used to align confidence with accuracy
  - **Quick check question:** If a classifier has 80% confidence on average but 60% accuracy, is it overconfident or underconfident? How does temperature scaling fix this?

## Architecture Onboarding

- **Component map:** Φ_V (frozen CLIP ViT-B/16) → local statistics computation → server aggregation → global posterior → personalized posterior → GDA classifier; Φ_T (frozen CLIP) → text prompt encoding → importance scoring → server aggregation → global weights → weighted text classifier; confidence computation → η(z) → fused classifier

- **Critical path:**
  1. Each client extracts visual embeddings z^k_{c,i} = Φ_V(x^k_{c,i}) for all local samples
  2. Clients compute local statistics (means, covariances) and transmit to server
  3. Server computes global posterior → broadcasts to all clients
  4. Clients compute personalized posteriors using α-weighted power prior
  5. Separately: clients generate LLM prompts, compute importance scores, send to server
  6. Server aggregates and weights prompts → broadcasts global prompt weights
  7. At inference: client computes f^k_V(z), f_T(z), η(z), and fused prediction f^k_M(z)

- **Design tradeoffs:**
  - **α (power prior coefficient):** Higher α trusts global prior more; α=1 used in experiments. Lower α increases personalization but risks overfitting with few local samples
  - **τ_t (text temperature):** Set to 0.5 to aggressively favor high-scoring prompts. Lower = more selective
  - **Number of LLM prompts (M):** More prompts increase diversity but also noise; paper uses 3-4 sentences per class
  - **Assumption:** LLM access required on each client; version heterogeneity handled via embedding similarity filtering

- **Failure signatures:**
  - **Personalization collapse:** If α ≈ 0 and local data is scarce, covariance estimates become singular → GDA fails. Symptom: NaN or uniform predictions
  - **Text-visual mismatch:** If LLM describes visual features absent in images, importance scores will be low → text pipeline contributes little
  - **Domain shift not captured:** Feature shift across domains may violate shared Gaussian assumption per class → visual pipeline underperforms

- **First 3 experiments:**
  1. **Sanity check:** Run TOFA on single-client setup (K=1). Compare to zero-shot CLIP and CLIP-GDA. Verify α=1 yields same result as pure global posterior
  2. **Ablation on α:** Sweep α ∈ {0, 0.25, 0.5, 0.75, 1.0} on OxfordPets with K=10. Plot accuracy vs. α to verify α≥0.75 is near-optimal
  3. **Modality ablation:** Run visual-only, text-only, and fused TOFA on DTD dataset. Confirm fusion improves over both (per Figure 3). If not, check calibration and η(z) computation

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several unresolved issues emerge from the methodology and assumptions made throughout the work.

## Limitations

- **Gaussian assumption limitations:** The Gaussian assumption for visual features may limit effectiveness on datasets with highly multi-modal class distributions or fine-grained categories where CLIP embeddings do not cluster normally
- **LLM resource requirements:** The reliance on LLM-generated prompts impacts feasibility for resource-constrained edge devices, given the inference overhead of running local LLMs
- **Confidence calibration dependence:** The adaptive multimodal fusion mechanism remains vulnerable when the VLM is severely miscalibrated, breaking the assumption that confidence is a surrogate for accuracy

## Confidence

- **High confidence:** Training-free operation, single communication round, basic federated setup - these are well-defined and reproducible
- **Medium confidence:** Gaussian feature distribution assumption, power prior formulation - theoretically sound but dataset-dependent
- **Low confidence:** LLM robustness across heterogeneous clients, temperature hyperparameter tuning - these introduce significant variability

## Next Checks

1. Test TOFA on non-CLIP vision backbones to verify Gaussian assumption holds beyond specific embeddings
2. Evaluate performance degradation when LLM prompts are semantically mismatched to visual domains
3. Quantify sensitivity to hyperparameters (α, τ_t, κ) across diverse federated data distributions