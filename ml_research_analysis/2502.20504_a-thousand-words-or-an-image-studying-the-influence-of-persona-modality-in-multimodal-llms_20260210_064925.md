---
ver: rpa2
title: 'A Thousand Words or An Image: Studying the Influence of Persona Modality in
  Multimodal LLMs'
arxiv_id: '2502.20504'
source_url: https://arxiv.org/abs/2502.20504
tags:
- persona
- image
- text
- your
- female
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how different modalities influence persona
  embodiment in multimodal large language models (LLMs). The authors create a novel
  dataset of 40 diverse personas represented equivalently across four modalities:
  text-only, image-only, assisted image (image with text), and descriptive image (typographically
  styled text embedded in image).'
---

# A Thousand Words or An Image: Studying the Influence of Persona Modality in Multimodal LLMs

## Quick Facts
- **arXiv ID**: 2502.20504
- **Source URL**: https://arxiv.org/abs/2502.20504
- **Reference count**: 40
- **Primary result**: Text-based personas consistently outperform other modalities in multimodal LLM persona embodiment

## Executive Summary
This paper investigates how different modalities influence persona embodiment in multimodal large language models (LLMs). The authors create a novel dataset of 40 diverse personas represented equivalently across four modalities: text-only, image-only, assisted image (image with text), and descriptive image (typographically styled text embedded in image). Using 60 carefully designed questions, they evaluate 5 multimodal LLMs on persona consistency, linguistic habits, and scenario-based actions. Results show that text-based personas consistently outperform other modalities, with text achieving up to 0.2 higher scores in linguistic habits and generating 40+ more unique word types. Image and assisted image modalities show significantly lower performance, revealing that current multimodal LLMs struggle to extract and utilize visual persona information effectively.

## Method Summary
The researchers constructed a novel dataset containing 40 diverse personas, each represented in four equivalent modalities: text-only, image-only, assisted image (combining image with descriptive text), and descriptive image (typographically styled text embedded within images). They designed 60 questions to evaluate three key aspects of persona embodiment: consistency (maintaining persona identity), linguistic habits (characteristic speech patterns), and scenario-based actions (appropriate behavior in context). Five multimodal LLMs were tested across all combinations of personas and modalities. The evaluation employed both automated metrics and human judgment to assess performance differences between modalities, with text-based representations serving as the baseline for comparison.

## Key Results
- Text-based personas outperformed other modalities by up to 0.2 higher scores in linguistic habits evaluation
- Text modalities generated 40+ more unique word types compared to image-based approaches
- Image-only and assisted image modalities showed significantly lower performance, indicating multimodal LLMs struggle to extract and utilize visual persona information effectively

## Why This Works (Mechanism)
The superior performance of text-based personas stems from the fundamental architecture of current multimodal LLMs, which are primarily designed as text-to-text models with vision as an auxiliary input. These models excel at processing and generating text but show limited capability in extracting nuanced persona information from visual inputs. The vision encoders, while functional for object recognition and basic scene understanding, lack the sophistication needed to capture complex personality traits, behavioral patterns, and contextual nuances that are often visually apparent. This architectural limitation becomes particularly evident when attempting to infer detailed persona characteristics from images alone, where current vision-language integration falls short of human-level visual understanding and interpretation capabilities.

## Foundational Learning

**Vision-Language Integration**: Understanding how visual and textual information are combined in multimodal models
*Why needed*: Critical for comprehending model limitations in processing non-textual persona representations
*Quick check*: Verify whether the model uses separate encoders or unified architecture for vision and language processing

**Persona Embodiment**: The ability of language models to consistently represent and maintain character identities
*Why needed*: Central to understanding what the study measures and evaluates
*Quick check*: Confirm that persona consistency metrics align with standard benchmarks in the field

**Multimodal Representation Learning**: How different input modalities are encoded and transformed into usable representations
*Why needed*: Essential for understanding why certain modalities perform better than others
*Quick check*: Examine the embedding dimensionality and fusion mechanisms used in the evaluated models

## Architecture Onboarding

**Component Map**: Vision Encoder -> Text Encoder -> Fusion Module -> LLM Decoder -> Output Generation

**Critical Path**: Input → Vision Encoder (for image modalities) → Text Encoder (for all modalities) → Fusion Module → LLM Decoder → Response Generation

**Design Tradeoffs**: Models prioritize text processing capabilities over vision understanding, reflecting their text-to-text primary function. Vision encoders provide auxiliary information but lack depth in extracting personality and behavioral nuances from images.

**Failure Signatures**: 
- Inability to extract detailed persona characteristics from visual inputs
- Inconsistent persona representation across different image-based modalities
- Limited transfer of visual information to text generation capabilities
- Reduced linguistic diversity in image-based persona responses

**First 3 Experiments**:
1. Test baseline text-only personas across all 5 multimodal LLMs to establish performance standards
2. Evaluate image-only personas to measure pure visual understanding capabilities
3. Assess assisted image modalities to determine if textual descriptions improve visual persona extraction

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset representativeness concerns due to unclear selection criteria and demographic coverage
- Limited generalizability across the rapidly evolving landscape of multimodal systems
- Unexplored interactions between persona characteristics and modality effectiveness

## Confidence
- Core finding (text outperforms other modalities): High
- Specific performance gap magnitudes (0.2 score difference, 40+ word types): Medium

## Next Checks
1. Replicate the study with a larger and more diverse set of personas, including systematic demographic and professional representation to assess whether modality effects vary across different persona types.

2. Conduct cross-model validation by testing the same personas across a broader range of multimodal LLMs, including both commercial and open-source systems, to determine if the text advantage persists across different architectural approaches.

3. Investigate the temporal stability of these findings by repeating the experiments with the same models after their next major updates, as multimodal capabilities are rapidly evolving and the current limitations may be addressed in near-term model improvements.