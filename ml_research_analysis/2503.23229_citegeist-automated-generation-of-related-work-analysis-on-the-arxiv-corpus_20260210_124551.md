---
ver: rpa2
title: 'Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus'
arxiv_id: '2503.23229'
source_url: https://arxiv.org/abs/2503.23229
tags:
- arxiv
- work
- related
- generation
- abstract
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents Citegeist, a dynamic retrieval-augmented generation
  pipeline for producing related work sections from the arXiv corpus. The method employs
  embedding-based similarity matching on abstracts, multi-stage filtering, and summarization
  to generate citation-backed content.
---

# Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus

## Quick Facts
- **arXiv ID**: 2503.23229
- **Source URL**: https://arxiv.org/abs/2503.23229
- **Reference count**: 15
- **Primary result**: Dynamic RAG pipeline for generating citation-backed related work sections from arXiv corpus

## Executive Summary
Citegeist is a retrieval-augmented generation system that automatically produces related work sections from the arXiv corpus. The method combines embedding-based similarity matching on abstracts, multi-stage filtering, and summarization to generate coherent, citation-backed content. Three hyperparameters—breadth, depth, and diversity—allow fine-tuning of the output quality and coverage. The system handles corpus updates efficiently through hashing and incremental embedding updates, and demonstrates performance that matches or exceeds source papers' sections when evaluated using LLM-as-a-judge.

## Method Summary
Citegeist employs a dynamic retrieval-augmented generation pipeline for producing related work sections. The system first embeds all abstracts from the arXiv corpus (~2.6M papers) using sentence transformers, then retrieves candidates via cosine similarity search with iterative diversity-weighted selection. Full papers are fetched only for promising candidates, with page-level embeddings used to select representative sections based on depth parameters. GPT-4o summarizes these selections and synthesizes them into related work with citations, verified through the arXiv API. The system supports both abstract and full-paper input, with updates handled through hash-based change detection for efficient synchronization.

## Key Results
- Citegeist outperforms direct GPT-4o prompting and matches or exceeds source papers' related work quality
- Full-paper input improves output quality (7.64 vs 7.55) but increases latency
- Diversity parameter (w=0.3) provides small quality gains (+0.17) with minimal relevance loss
- System successfully retrieves accurate citations and synthesizes coherent analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage filtering with abstract-first retrieval reduces computational cost while maintaining retrieval quality.
- Mechanism: The system embeds only abstracts (~384 tokens each) for initial similarity search across 2.6M papers, fetching full documents only for candidates that pass filtering. Page-level embeddings are then computed for depth-controlled selection.
- Core assumption: Abstract similarity correlates with full-paper relevance for related work purposes.
- Evidence anchors:
  - [abstract]: "we employ a mixture of embedding-based similarity matching, summarization, and multi-stage filtering"
  - [section 3]: "we embed all abstracts contained in the corpus... To choose the k representative pages, we reuse the functionality from Equation 1"
  - [corpus]: Weak validation—neighbor papers on scientific literature analysis (e.g., GraphMind, ReviewRL) show moderate FMR (0.40-0.58) but no direct replication of this filtering approach.
- Break condition: If abstract–full-text alignment is weak for a domain (e.g., papers with misleading abstracts), early filtering may exclude relevant works.

### Mechanism 2
- Claim: Diversity-weighted iterative selection improves coverage of related work topics.
- Mechanism: Equation 1 balances similarity to input (s_i) against minimum similarity to already-selected papers, controlled by weight w (diversity parameter). This promotes topic spread while maintaining relevance.
- Core assumption: Optimal related work sections benefit from diverse perspectives, not just highest-similarity papers.
- Evidence anchors:
  - [section 3.1]: "The parameter w∈[0,1] determines the trade-off between prioritizing similarity and ensuring diversity"
  - [section 4]: "Setting diversity to 0.3, we observe a small decrease in paper relevancy (-0.05) and a small increase in quality (+0.17)"
  - [corpus]: No external validation found—mechanism is paper-specific.
- Break condition: High diversity values may surface tangentially-related papers, diluting coherence; authors note "higher diversity will often include interesting new areas but also address papers that are not directly relevant sometimes."

### Mechanism 3
- Claim: Hash-based change detection enables efficient corpus synchronization without full re-embedding.
- Mechanism: SHA-256 hashes of serialized metadata entries are compared across sync cycles; only new or modified records trigger embedding recomputation and database updates.
- Core assumption: Metadata changes (title, abstract) are reliable proxies for when re-embedding is necessary.
- Evidence anchors:
  - [abstract]: "optimized way of incorporating new and modified papers"
  - [section 3.2.1]: "When a record's hash differs but its paper ID exists, the system re-computes the embedding and updates the database record"
  - [corpus]: No external validation; mechanism is architectural, not evaluated experimentally.
- Break condition: If paper content changes without metadata updates (rare), embeddings become stale.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core architecture pattern—understanding how retrieval context is injected into LLM prompts explains why Citegeist reduces hallucinations vs. direct GPT-4 prompting.
  - Quick check question: Can you explain why retrieving abstracts before generation reduces invalid citations compared to prompting an LLM directly?

- **Embedding similarity search (cosine similarity)**
  - Why needed here: All retrieval is driven by cosine similarity between abstract/page embeddings and input; Equation 1's diversity term operates on these distances.
  - Quick check question: Given two embedding vectors, would you expect cosine similarity or Euclidean distance to be more robust to document length variations?

- **LLM-as-a-judge evaluation**
  - Why needed here: Primary evaluation method; understanding positional bias and the need for separate quality dimensions (relevance vs. writing quality) is critical for interpreting Table 1.
  - Quick check question: Why might the same LLM (GPT-4o) show bias when evaluating outputs it also authored?

## Architecture Onboarding

- **Component map**: Input layer (abstract/PDF → embedding extraction) → Retrieval layer (Milvus vector DB → similarity search → diversity-weighted selection) → Filtering layer (longlist → full-text fetch → page-level embedding → depth-limited selection → shortlist) → Generation layer (per-paper summarization → synthesis prompt → related work output with citations) → Update layer (hash table → change detection → incremental embedding updates)

- **Critical path**: Abstract embedding → similarity search → diversity filtering → full-text retrieval → page selection → summarization → synthesis. Latency bottleneck is full-text fetch and multi-page summarization.

- **Design tradeoffs**:
  - Abstract-first vs. full-paper input: Abstract is faster but may miss context; full-paper improves quality (Table 1: 7.64 vs. 7.55) but increases latency.
  - Depth parameter: Higher depth includes more pages but reduces relevance (-0.31 at depth=6 vs. 2) without quality gains.
  - Hosted vs. local vector DB: Hosted reduces install friction and improves speed; local enables offline use.

- **Failure signatures**:
  - Shallow coverage: Related work sections "tend to go over works in a rather shallow manner"—mitigate by increasing depth, but model consolidation degrades beyond ~6 pages.
  - Citation limit: Generated sections cap at 10-12 works due to context window; solve by splitting prompts or larger-context models.
  - Domain imbalance: Inherits arXiv distribution (math > philosophy); may underperform on underrepresented fields.

- **First 3 experiments**:
  1. **Parameter sweep**: Run Citegeist on 5 test abstracts with (breadth, depth, diversity) = {(5, 2, 0), (10, 4, 0.3), (20, 6, 0.5)} to observe quality-relevance tradeoffs.
  2. **Ablation on diversity**: Disable diversity term (w=0) vs. w=0.3 on same inputs; manually inspect whether diverse papers add value or noise.
  3. **Update pipeline validation**: Introduce a synthetic metadata change to a paper in a test DB; verify that hash detection triggers re-embedding and that unchanged papers are skipped.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Citegeist's output compare to human-authored related work sections when evaluated by domain experts rather than LLMs?
- Basis in paper: [explicit] The authors acknowledge resource constraints prevented human annotation and state, "We plan to expand on this evaluation in the future, potentially by conducting an evaluation involving human annotators."
- Why unresolved: The current evaluation relies on "LLM-as-a-judge," which the authors admit suffers from positional bias and potential self-preference (using GPT-4o as both author and judge).
- What evidence would resolve it: A user study involving domain experts blindly ranking the coherence and relevance of generated sections against original source papers.

### Open Question 2
- Question: Can more sophisticated full-text processing pipelines improve retrieval accuracy compared to the current shallow aggregation of page embeddings?
- Basis in paper: [explicit] Section 3.1 notes that the current method for handling full PDFs is "only a shallow modification" and that the authors "aim at investigating more sophisticated updates to the pipeline in the future."
- Why unresolved: The current method aggregates page-level similarity scores, which may fail to capture nuanced semantic structures or arguments that span across full documents.
- What evidence would resolve it: Ablation studies comparing the current aggregation method against hierarchical attention mechanisms or section-aware embedding strategies.

### Open Question 3
- Question: Does employing models with larger context windows or hierarchical summarization effectively mitigate the performance degradation observed at higher depth settings?
- Basis in paper: [explicit] The Limitations section states that increasing depth beyond a certain point causes the model to "struggle to consolidate all given pages," and suggests "choosing a model with a larger context window" as a potential solution.
- Why unresolved: It is unclear if the drop in relevance at high depth (e.g., >6 pages) is a fundamental information retrieval issue or simply a context-window limitation of the current architecture.
- What evidence would resolve it: Experiments replicating the high-depth evaluation using models with 1M+ token context windows to see if relevance scores stabilize.

## Limitations
- Evaluation relies on LLM-as-a-judge, which may not align with human assessment standards
- Citation quality bounded by arXiv's publication range (pre-2022 cutoff), excluding recent work
- Domain imbalance inherited from arXiv distribution may limit performance on underrepresented fields

## Confidence
- **High confidence**: Abstract-first filtering reduces computational cost while maintaining retrieval quality
- **Medium confidence**: Diversity-weighted selection improves topic coverage without sacrificing coherence
- **Medium confidence**: Hash-based incremental updates enable efficient corpus synchronization

## Next Checks
1. **Domain transfer test**: Run Citegeist on papers from underrepresented arXiv categories (e.g., quantitative biology, physics) and compare relevance scores to established domains to validate cross-domain robustness.
2. **Human evaluation**: Recruit 3-5 domain experts to manually rate 10 generated related work sections against source papers, measuring inter-rater agreement and comparing to LLM-as-a-judge scores.
3. **Citation completeness audit**: For 5 papers with known complete bibliographies, measure the fraction of cited works retrieved by Citegeist to quantify recall limitations.