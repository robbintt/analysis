---
ver: rpa2
title: Goals and the Structure of Experience
arxiv_id: '2508.15013'
source_url: https://arxiv.org/abs/2508.15013
tags:
- state
- telic
- learning
- states
- experience
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces telic states as a novel computational framework
  for understanding goal-directed behavior and state representation in cognitive agents.
  The framework proposes that goals and state representations co-emerge interdependently
  from experience distributions, with telic states defined as classes of goal-equivalent
  experience distributions.
---

# Goals and the Structure of Experience

## Quick Facts
- arXiv ID: 2508.15013
- Source URL: https://arxiv.org/abs/2508.15013
- Reference count: 40
- Primary result: Introduces telic states as a novel computational framework for understanding goal-directed behavior

## Executive Summary
This paper introduces telic states as a novel computational framework for understanding goal-directed behavior and state representation in cognitive agents. The framework proposes that goals and state representations co-emerge interdependently from experience distributions, with telic states defined as classes of goal-equivalent experience distributions. The authors argue that this approach provides a more cognitively plausible account of purposeful behavior than traditional reinforcement learning models, which assume separate state representations and reward functions.

The telic states framework formalizes goal-directed learning in terms of statistical divergence between behavioral policies and desirable experience features. It provides a unified account of behavioral, phenomenological, and neural dimensions of purposeful behaviors across diverse substrates. The authors present a learning algorithm that can flexibly adapt to shifting goals and demonstrate its application through a dual-goal navigation task.

## Method Summary
The telic states framework formalizes goal-directed learning through statistical divergence between behavioral policies and experience distributions. The approach introduces a learning algorithm that identifies goal-equivalent experience distributions and adapts to shifting goals through iterative refinement. The framework is demonstrated through a dual-goal navigation task, showing how telic states can represent multiple goals and facilitate flexible behavior switching.

## Key Results
- Telic states provide a unified framework linking behavioral, phenomenological, and neural dimensions of purposeful behavior
- The learning algorithm can flexibly adapt to shifting goals through statistical divergence minimization
- Telic states potentially circumvent infinite regress issues in meta-reasoning accounts of goal selection
- The framework suggests novel predictions about neural correlates of goal-directed learning

## Why This Works (Mechanism)
The framework works by recognizing that goals and state representations are fundamentally interdependent - they co-emerge from experience distributions rather than being separately encoded. By defining goals as classes of experience distributions that produce equivalent behavioral outcomes, the framework naturally captures the holistic nature of purposeful behavior. The statistical divergence approach allows for flexible adaptation as goals shift, while the unified representation of behavioral, phenomenological, and neural dimensions provides a coherent account of goal-directed cognition across multiple levels of analysis.

## Foundational Learning
- Statistical divergence measures (KL divergence, Wasserstein distance) - why needed: to quantify differences between experience distributions; quick check: ensure proper normalization and convergence
- Policy optimization algorithms - why needed: to implement goal-directed behavior; quick check: verify gradient flow and stability
- Neural encoding of state representations - why needed: to ground the framework in biological plausibility; quick check: compare predicted patterns with empirical neural data
- Experience distribution modeling - why needed: to capture the statistical structure of goal-directed behavior; quick check: validate distribution assumptions with real-world data
- Meta-reasoning frameworks - why needed: to address goal selection and formation; quick check: test for infinite regress avoidance

## Architecture Onboarding

Component map: Experience distributions -> Statistical divergence analysis -> Telic state representation -> Behavioral policy optimization -> Goal adaptation

Critical path: The core computational path involves measuring divergence between current experience distributions and goal distributions, updating telic state representations, and refining behavioral policies accordingly.

Design tradeoffs: The framework trades computational complexity for cognitive plausibility, requiring sophisticated statistical analysis of experience distributions rather than simple reward maximization.

Failure signatures: Poor convergence in divergence minimization, inability to capture goal equivalence relationships, or failure to generalize across goal shifts.

First experiments:
1. Implement basic divergence minimization between simple experience distributions
2. Test telic state representation learning on synthetic goal-structured data
3. Evaluate behavioral policy optimization in controlled navigation tasks

## Open Questions the Paper Calls Out
The paper addresses how goals can be selected based on the properties of telic state representations they induce, potentially circumventing infinite regress issues in meta-reasoning accounts. It also raises questions about the precise neural mechanisms underlying telic state encoding and how this framework might explain the emergence of complex goal hierarchies.

## Limitations
- Theoretical arguments rather than empirical validation for cognitive plausibility claims
- Limited demonstration scope focused on dual-goal navigation task
- Abstract treatment of goal formation and selection mechanisms
- Computational tractability concerns for complex environments

## Confidence
- Cognitive plausibility vs. traditional RL: High confidence (theoretical arguments)
- Mathematical formulation of divergence: Medium confidence (requires further specification)
- Unification of behavioral/neural dimensions: Medium confidence (extrapolates from limited evidence)
- Goal formation and selection: Medium confidence (remains somewhat abstract)
- Infinite regress circumvention: Medium confidence (conceptually compelling but unproven)
- Dual-goal navigation demonstration: Medium confidence (proof-of-concept level)

## Next Checks
1. Implement the learning algorithm in more challenging multi-goal environments to test scalability and robustness
2. Design experiments to empirically test predictions about neural correlates of telic states in both animal models and human subjects
3. Develop formal comparisons between telic state learning and established reinforcement learning approaches across benchmark tasks to quantify performance differences and computational costs