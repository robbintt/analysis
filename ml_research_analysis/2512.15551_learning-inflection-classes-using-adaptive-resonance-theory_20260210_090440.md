---
ver: rpa2
title: Learning inflection classes using Adaptive Resonance Theory
arxiv_id: '2512.15551'
source_url: https://arxiv.org/abs/2512.15551
tags:
- inflection
- classes
- class
- which
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores how inflection classes in languages can be learned
  by unsupervised clustering using Adaptive Resonance Theory (ART), a neural network
  model that controls generalization through a vigilance parameter. It applies this
  method to Latin, Portuguese, and Estonian, using phonetic data and evaluating clusterings
  against linguistically attested inflection classes.
---

# Learning inflection classes using Adaptive Resonance Theory

## Quick Facts
- **arXiv ID**: 2512.15551
- **Source URL**: https://arxiv.org/abs/2512.15551
- **Reference count**: 40
- **Primary result**: ART1 neural network with vigilance parameter successfully clusters lexemes into inflection classes, achieving ARI≈0.94 on Latin and demonstrating cross-linguistic applicability to Portuguese and Estonian

## Executive Summary
This paper presents a novel approach to learning inflection classes using Adaptive Resonance Theory (ART), a neural network model that clusters data through a vigilance parameter controlling generalization. The model encodes lexemes as binary trigram vectors across paradigm cells and uses ART1 to form clusters that closely match linguistically attested inflection classes. Evaluated on Latin, Portuguese, and Estonian datasets, the approach achieves high similarity to reference classifications (ARI≈0.94 for Latin) when the vigilance parameter is properly tuned. The study demonstrates that surface-based similarity clustering can capture the systematic patterns underlying inflectional morphology, with cluster templates revealing interpretable features that correspond to linguistic descriptions of inflection classes.

## Method Summary
The method uses ART1 neural network to cluster lexemes into inflection classes based on surface similarity. Lexemes are encoded as binary trigram vectors for each paradigm cell, then concatenated into single vectors per lexeme. The model presents datapoints incrementally, comparing them to existing category templates using bottom-up weights. If the match ratio exceeds the vigilance threshold, the lexeme joins that category; otherwise, a new category is created. The vigilance parameter controls generalization: low values merge similar lexemes into fewer classes, while high values create more specific categories. The approach is evaluated using Adjusted Rand Index (ARI) against linguistically attested inflection classes across three languages with varying inflectional complexity.

## Key Results
- ART1 achieves ARI≈0.94 on Latin inflection classes at vigilance 0.06, with optimal performance in narrow vigilance ranges
- Cross-linguistic application shows varying optimal vigilance values (Portuguese: 0.02, Estonian: 0.06) reflecting language-specific generalization needs
- Cluster templates reveal interpretable features matching linguistic descriptions of inflection classes, demonstrating the model's ability to capture morphological patterns
- Generalization to unseen data is effective across languages, with cluster membership predicting inflection behavior of novel lexemes

## Why This Works (Mechanism)

### Mechanism 1: Vigilance-Controlled Generalization
The vigilance parameter ρ enables adaptive clustering granularity by controlling the similarity threshold required for category membership. When vigilance is low, inputs are more likely assigned to existing categories (generalization). When high, the system creates more categories (specificity). The match ratio M = ||z||₁/||x||₁ must exceed ρ for resonance.

### Mechanism 2: Top-Down Template Intersection for Category Representation
Category templates emerge as the logical AND (intersection) of all assigned datapoint features, yielding interpretable prototypes. Top-down weights w^td_J are updated to z_{x,J} = x ∩ w^td_J upon resonance. Only features present in ALL assigned items persist, creating a "signature" trigram set per cluster.

### Mechanism 3: Concatenated Trigram Encoding Across Paradigm Cells
Encoding each lexeme as concatenated cell-specific trigram vectors preserves paradigm-internal structure for similarity comparison. For each paradigm cell, trigram presence is registered separately (n_forms × n_ngrams features). This allows cell-by-cell comparison between lexemes rather than bag-of-trigrams across the whole paradigm.

## Foundational Learning

- **Inflection Classes vs. Microclasses**: Why needed - The paper evaluates against both macroclasses (Latin: 5) and microclasses (Portuguese: 58, Estonian: 14). Quick check - Can you explain why Portuguese ARI (~0.27) is lower than Latin (0.94) despite more data, and what this suggests about the relationship between microclasses and surface similarity?

- **Paradigm Cell Selection (Distillation)**: Why needed - The model uses curated subsets of paradigm cells (9–14 per language) rather than full paradigms (50–70 cells). These selections are linguistically motivated via distillation. Quick check - If you added all 70 paradigm cells, what computational and cognitive issues would arise?

- **Adjusted Rand Index (ARI) for Clustering Evaluation**: Why needed - Unsupervised clustering requires external validation; ARI measures similarity to reference clustering adjusted for chance, penalizing both over- and under-clustering. Quick check - Why is ARI preferred over AMI when comparing clusterings with different numbers of clusters?

## Architecture Onboarding

- **Component map**: Input layer F1 -> Bottom-up weights w^bu -> Output layer F2 (category nodes) -> Top-down weights w^td -> Vigilance parameter ρ -> Learning rate L

- **Critical path**: 1) Preprocess phonetic forms → tokenize → extract trigrams per paradigm cell 2) Concatenate into single binary vector per lexeme 3) Present datapoints incrementally to ART1 4) Bottom-up activation selects candidate category J = argmax(T_j) 5) Top-down match M_J computed; if M_J ≥ ρ, update weights; else try next candidate 6) If no category matches, create new category with current input as template

- **Design tradeoffs**: Vigilance tuning: Language-specific; narrow optimal range requires experimentation or meta-learning. Cell selection: Fewer cells = faster but less signal; more cells = computational cost and potential noise. Encoding choice: Concatenated cell-specific trigrams vs. aggregated set representation (paper chooses former for cognitive plausibility)

- **Failure signatures**: Too many clusters (vigilance too high): Excessive fragmentation, low ARI due to over-segmentation. Too few clusters (vigilance too low): Macro-level grouping only, microclass structure lost. Estonian-style distributed patterns: Clusters form around correlated surface features (stem vowels) rather than true gradation-based class structure

- **First 3 experiments**: 1) Replicate Latin baseline: Use provided code with vigilance sweep [0.0, 0.3], verify ARI peak near 0.06 with 5–7 clusters. 2) Ablate paradigm cell count: Reduce Latin to only 3–4 "principal parts" and observe performance degradation pattern. 3) Cross-linguistic vigilance transfer: Train on Latin optimal vigilance (0.06), apply directly to Portuguese/Estonian without retuning—document performance gap to assess language-specific calibration needs

## Open Questions the Paper Calls Out

### Open Question 1
Can an agent-based model integrating ART1 clustering with a production component successfully simulate the diachronic evolution and emergence of inflection classes? The current study only models the acquisition task (clustering) for individual agents and lacks the production mechanism required for agent-agent interaction and iterated learning.

### Open Question 2
Can the optimal vigilance (generalisation) parameter be learned automatically based on environmental feedback during communication? The current experiments rely on a manual search for the best vigilance value (0.02–0.06) for each language to maximize clustering similarity.

### Open Question 3
Can feature transformations, such as those learned by small transformer networks, capture the paradigm-internal relations required to learn distributed inflectional patterns like Estonian consonant gradation? The current trigram representation only compares surface similarity between whole lexemes, failing to detect the abstract "strong/weak" alternations that define Estonian classes.

## Limitations
- The vigilance parameter requires language-specific tuning, with narrow optimal ranges that may not generalize to new language families without recalibration
- Trigram encoding may miss abstract morphological relationships in languages with non-transparent inflectional systems, as evidenced by modest performance on Estonian
- The model's reliance on surface similarity may conflate correlated features (stem vowels) with true class diagnostics in languages with distributed inflectional patterns

## Confidence
- **High confidence**: ART1 clustering mechanism with vigilance parameter, cluster template interpretation methodology
- **Medium confidence**: Cross-linguistic generalization patterns, comparison with linguistic annotations
- **Low confidence**: Claims about cognitive plausibility of concatenated cell-specific encoding versus aggregated representation

## Next Checks
1. Test the model's performance on a morphologically transparent language (e.g., Spanish) to verify if the narrow vigilance ranges persist across related language families
2. Conduct ablation studies removing paradigm cell subsets to quantify the contribution of each cell to clustering accuracy
3. Implement an adaptive vigilance mechanism to evaluate whether dynamic parameter adjustment improves generalization to unseen data compared to fixed optimal values