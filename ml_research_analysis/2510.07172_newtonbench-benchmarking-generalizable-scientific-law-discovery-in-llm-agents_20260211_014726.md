---
ver: rpa2
title: 'NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents'
arxiv_id: '2510.07172'
source_url: https://arxiv.org/abs/2510.07172
tags:
- easy
- medium
- hard
- newtonbench
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NEWTONBENCH is a benchmark for scientific law discovery that overcomes
  prior limitations by using counterfactual law shifts to generate memorization-resistant,
  scientifically relevant tasks. It evaluates interactive model discovery rather than
  static function fitting, requiring agents to experiment with complex simulated systems.
---

# NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents

## Quick Facts
- **arXiv ID**: 2510.07172
- **Source URL**: https://arxiv.org/abs/2510.07172
- **Reference count**: 40
- **Primary result**: NEWTONBENCH uses counterfactual law shifts to create memorization-resistant tasks evaluating interactive scientific discovery; top models show fragile performance degrading sharply with complexity and noise.

## Executive Summary
NEWTONBENCH addresses critical limitations in evaluating scientific law discovery by LLMs, introducing a benchmark that prevents memorization through counterfactual law shifts and requires interactive experimentation rather than static function fitting. The benchmark generates 324 tasks across 12 physics domains where canonical laws are structurally mutated while preserving dimensional consistency, forcing agents to derive relationships from experimental data. Across these tasks, frontier LLMs demonstrate clear but fragile discovery abilities, with performance degrading sharply as system complexity and observational noise increase.

## Method Summary
The benchmark evaluates LLMs on discovering hidden scientific laws by interacting with simulated physics environments through a virtual agent framework. Agents query systems using `<run_experiment>` tags (max 10 rounds, 20 data points/round) and submit discovered laws via `<final_law>` tags. Laws are generated via "counterfactual law shifts" - structural mutations of canonical physics equations that preserve dimensional consistency by adjusting physical constants. Evaluation uses both Symbolic Accuracy (LLM-as-a-judge for mathematical equivalence) and Root Mean Squared Logarithmic Error (RMSLE) for numerical fidelity. The benchmark includes three difficulty levels (Easy, Medium, Hard) and three system complexities (Vanilla Equation, Simple System, Complex System), with 11 LLMs tested in both vanilla and code-assisted settings.

## Key Results
- Strong models (GPT-5, Gemini-2.5-pro) show clear but fragile discovery abilities, degrading sharply with system complexity and observational noise
- Code assistance paradoxically hinders stronger models by inducing premature exploitation over exploration, causing satisficing on suboptimal solutions
- Performance crashes from ~60% to ~20% accuracy with just 0.0001 noise, indicating severe brittleness in real-world conditions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Counterfactual law shifts generate memorization-resistant tasks by altering mathematical structure while preserving dimensional consistency.
- **Mechanism**: Laws are mutated via operations on expression trees (e.g., changing inverse-square to inverse-cubic), with physical constants adjusted to maintain unit validity, forcing reasoning over recall.
- **Core assumption**: Pre-training corpus lacks sufficient exposure to specific counterfactual variants despite containing canonical forms.
- **Evidence anchors**: Abstract and Section 3.1 describe mutation operations and memorization resistance.
- **Break condition**: If models are trained on massive synthetic datasets of all possible mutations, task reduces to pattern matching.

### Mechanism 2
- **Claim**: Interactive model discovery evaluates scientific reasoning by requiring active system querying rather than passive data fitting.
- **Mechanism**: Agents receive outputs from final models (sequences of equations), not target laws directly, requiring strategic parameter selection to disentangle target laws from assisting equations and confounding variables.
- **Core assumption**: Agents possess sufficient planning to design experiments varying one variable at a time for causality inference.
- **Evidence anchors**: Abstract and Section 2 describe the shift from static fitting to interactive discovery.
- **Break condition**: Unlimited experimental rounds or absence of confounding variables reduces challenge to simple curve fitting.

### Mechanism 3
- **Claim**: Code assistance induces paradoxical performance drop in stronger models by triggering premature exploitation.
- **Mechanism**: Strong models use code for function fitting, finding numerically "good enough" solutions early and terminating search rather than exploring symbolic space, leading to low symbolic accuracy despite decent RMSLE.
- **Core assumption**: Strong models optimize first plausible solution found via code execution, lacking meta-reasoning to reject good numerical fits for further symbolic search.
- **Evidence anchors**: Abstract and Section 4.5 trace performance degradation to premature shift from exploration to exploitation.
- **Break condition**: If evaluation rewards numerical fit over exact symbolic accuracy, failure disappears but law discovery goal is lost.

## Foundational Learning

- **Concept**: **Expression Trees (ASTs)**
  - **Why needed here**: Benchmark defines equations and counterfactual shifts as operations on Abstract Syntax Trees (nodes for operators/variables).
  - **Quick check question**: If a node represents `x^2`, how does a mutation operator change the expression to `x^3`?

- **Concept**: **Exploration vs. Exploitation Trade-off**
  - **Why needed here**: Section 4.5 identifies this as the mechanism for the "code paradox."
  - **Quick check question**: Why does fitting a curve to data (exploitation) fail to discover the true symbolic law if the initial symbolic guess is wrong?

- **Concept**: **Dimensional Analysis (Units)**
  - **Why needed here**: When laws are mutated, physical constants are adjusted to preserve dimensional consistency, ensuring shifted laws remain scientifically plausible.
  - **Quick check question**: If the exponent of `mass` in a force law increases, how must the physical constant change to keep the resulting unit as "Force"?

## Architecture Onboarding

- **Component map**: Virtual Environment -> LLM Agent -> Tools (run_experiment, python) -> Evaluator (LLM-as-Judge, RMSLE calculator)
- **Critical path**:
  1. Agent receives task description and known assisting equations
  2. Agent generates inputs via `<run_experiment>`
  3. Environment executes model, returns outputs
  4. Agent (optionally) uses `<python>` to analyze data or fit curves
  5. Agent submits `<final_law>`
  6. Evaluator compares against ground-truth shifted law

- **Design tradeoffs**:
  - Noise: Adds realism but causes performance to degrade precipitously (Section 4.2)
  - Code Assistance: Helps weak models, hurts strong models (Section 4.5); toggle based on model's base reasoning capability
  - System Complexity: "Vanilla" is easy but low-signal; "Complex System" is high-signal but often unsolvable for current models

- **Failure signatures**:
  - Satisficing: Strong model with code produces law with low RMSLE but 0% Symbolic Accuracy (wrong structure)
  - Fragility: Performance crashes from ~60% to ~20% accuracy with just 0.0001 noise (Section 4.2)
  - Memorization: Model outputs standard Newtonian/Einsteinian equations for counterfactual tasks, ignoring experimental data

- **First 3 experiments**:
  1. Run a Vanilla Equation (Easy): Test agent discovering simple shifted law (e.g., `F = C * m1 * m2 / r^1.5`) without confounding variables
  2. Test the Code Paradox: Run same agent on Medium difficulty task with/without `<python>` tool; check if symbolic accuracy drops when code is enabled
  3. Inject Noise: Run Complex System task with `noise=0.0001`; observe if symbolic accuracy degrades or agent misinterprets noise as structural feature

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can agent architectures be modified to prevent capable models from prematurely exploiting code tools for local optimization at the expense of global exploration?
- **Basis in paper**: Explicit (Section 4.5, Abstract)
- **Why unresolved**: Authors identify code assistance causes capable models to "satisfice" on suboptimal solutions due to over-exploitation but don't propose solutions to enforce better exploration-exploitation balance
- **What evidence would resolve it**: Implementing and evaluating agents using strategies like "forced exploration" phases or meta-cognitive prompting before allowing tool use

### Open Question 2
- **Question**: Can specific training or prompting interventions restore the symbolic accuracy of LLMs in the presence of minimal observational noise?
- **Basis in paper**: Explicit (Abstract, Section 4.2)
- **Why unresolved**: Study finds accuracy degrades precipitously with even tiny noise levels (0.0001), identifying this as core fragility without testing specific mitigation strategies
- **What evidence would resolve it**: Comparing standard agents against those fine-tuned on noisy data or equipped with uncertainty-aware reasoning modules on noisy NEWTONBench variant

### Open Question 3
- **Question**: How can LLM agents be evolved from fitting parameters within known grammar to autonomously constructing model structure (e.g., deriving differential equations) from first principles?
- **Basis in paper**: Explicit (Appendix E.6, "Future Directions")
- **Why unresolved**: Current benchmark evaluates discovery within fixed symbolic grammar and model framework, whereas real-world discovery often requires constructing governing equations themselves
- **What evidence would resolve it**: Developing and benchmarking agents on "Free-form" NEWTONBench where operators and model topology are not pre-defined

## Limitations

- Benchmark's reliance on LLM-as-a-judge for symbolic equivalence introduces potential brittleness, as equivalence checking for complex mathematical expressions remains an open challenge
- Counterfactual law generation may still have blind spots if certain mutated forms appear in pre-training data or if mutation space is insufficiently diverse
- Code paradox mechanism is primarily observed qualitatively rather than through controlled ablation studies isolating exploration vs exploitation phases

## Confidence

- **High confidence**: Empirical finding that stronger models degrade with code assistance, general trend of performance degrading with system complexity and noise, and basic benchmark infrastructure and task generation methodology
- **Medium confidence**: Interpretation that code paradox specifically results from premature exploitation (requires inferring internal decision-making from external behavior), claim that benchmark is "memorization-resistant" (depends on unknown distribution of mutated laws in pre-training)
- **Low confidence**: Assertion that current models are "far from robust scientific discovery" (difficult to verify without human baselines or alternative discovery frameworks)

## Next Checks

1. **Symbolic judge validation**: Run subset of discovered laws through rule-based symbolic simplifier (e.g., SymPy) in addition to LLM judge to quantify false positive/negative rates and identify failure patterns

2. **Code usage analysis**: Instrument agent to log when and how code is used during discovery process; analyze whether code usage correlates with early numerical convergence versus exploratory hypothesis testing to directly test exploitation hypothesis

3. **Human benchmark**: Have physics graduate students attempt subset of Medium and Hard tasks to establish human baselines for discovery performance, providing context for whether current LLM performance represents genuine capability gaps or reflects benchmark difficulty