---
ver: rpa2
title: Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems
arxiv_id: '2506.22186'
source_url: https://arxiv.org/abs/2506.22186
tags:
- control
- function
- learning
- system
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses active learning control (ALC) for unknown
  nonlinear dynamic systems by proposing a Thompson sampling-based approach that learns
  control laws directly from online data without requiring system identification.
  The method treats the control law as an element in a reproducing kernel Hilbert
  space, parameterized using an initial control law to form a structured function
  space.
---

# Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems

## Quick Facts
- arXiv ID: 2506.22186
- Source URL: https://arxiv.org/abs/2506.22186
- Reference count: 40
- Primary result: Thompson sampling-based ALC that learns control laws directly from online data without system identification, achieving exponential convergence of learned cost functions and bounded control regret.

## Executive Summary
This paper addresses active learning control for unknown nonlinear dynamic systems by proposing a Thompson sampling-based approach that learns control laws directly from online data without requiring system identification. The method treats the control law as an element in a reproducing kernel Hilbert space, parameterized using an initial control law to form a structured function space. A Thompson sampling strategy is then employed to explore and exploit potential optimal control laws within this space, balancing exploration and exploitation during learning. Theoretical analysis demonstrates that the learned cost function converges exponentially to a neighborhood of the true cost function, and the upper bound of control regret is derived. Numerical experiments validate the effectiveness of the approach, showing improved control performance compared to baseline methods.

## Method Summary
The method parameterizes control laws in a reproducing kernel Hilbert space (RKHS) constructed from an initial control law using basis functions generated from state index subsets. Thompson sampling maintains a posterior distribution over reward functions, sampling a reward function at each segment and selecting the controller that maximizes the sampled reward. The posterior is updated using the likelihood ratio of observed costs. The algorithm operates in segments, collecting data until a stopping criterion based on information gain is met, then updating the posterior and selecting a new controller for the next segment.

## Key Results
- The learned cost function converges exponentially to a neighborhood of the true cost function
- The upper bound of control regret is derived, showing constant term from approximation error plus exponentially decaying term from learning
- Numerical experiments validate effectiveness, showing improved control performance compared to baseline methods
- The method provides closed-loop stability guarantees with high probability after sufficient exploration

## Why This Works (Mechanism)

### Mechanism 1
Parameterizing control laws in a reproducing kernel Hilbert space (RKHS) enables structured exploration of candidate controllers without requiring explicit system identification. The method constructs basis functions from an initial control law ǧ using a decomposition scheme that captures local variation characteristics across different state subspaces. These basis functions span a convex hull Gᵢ for each control dimension, and their Cartesian product G = G₁ × ... × Gₘ forms the searchable controller space. This structure allows Thompson sampling to operate over a finite-dimensional representation while maintaining expressiveness for nonlinear policies.

### Mechanism 2
Thompson sampling balances exploration and exploitation by sampling reward functions from the posterior and selecting controllers that maximize the sampled reward, leading to exponential convergence of the learned cost function. At each segment t, the algorithm samples a reward function J̄ₜ from the posterior distribution Fₜpdf(J̄), then selects gₜ = argmax_{g∈G} J̄ₜ(g). The posterior is updated via Bayes' rule using the likelihood ratio Rₜ = Πᵢ (J̄(gᵢ) / J̄*(gᵢ)). This randomized selection ensures that regions with higher posterior uncertainty are occasionally explored, while high-reward regions are exploited.

### Mechanism 3
Control regret decomposes into a constant term from function space approximation error plus an exponentially decaying term from learning, yielding bounded cumulative regret. The regret Re(gₜ) = |E[J̄*(g*)] - E[J̄ₜ(gₜ)]| is bounded by three components: (1) J̄max v̄J(B(J̄*, δ̄J)) from neighborhood size, (2) approximation error ∝ √(mM²g - Σ∥g*ᵢ∥²ₗ²) / (√2ⁿ J²min) from Theorem 1, and (3) exponentially decaying term P₀ exp(-tεₗ) from learning convergence. The first two terms are unavoidable given the finite-dimensional parameterization.

## Foundational Learning

- **Concept: Reproducing Kernel Hilbert Space (RKHS)**
  - Why needed here: The paper parameterizes control laws as elements in an RKHS to enable both structured exploration via basis functions and distance metrics for regret analysis. Without RKHS structure, Thompson sampling over infinite-dimensional function spaces would be computationally intractable.
  - Quick check question: Can you explain why the reproducing property ⟨g, Ker(x,·)⟩ = g(x) matters for evaluating candidate controllers at specific states?

- **Concept: Martingale Convergence Theorem**
  - Why needed here: The proof of Theorem 2 constructs a martingale Mₜ = Σₜ₌₁ᵀ [Td(Lₜ(Ω)/Lₜ₋₁(Ω)) + d(J̄ₜ, J̄*)] and uses martingale convergence to establish that Mₜ/Teff → 0 almost surely, which is the key step in proving exponential decay of the likelihood mass outside the convergence neighborhood.
  - Quick check question: What condition on the variance of martingale increments (Equation 35) ensures convergence, and why does bounded segment length satisfy it?

- **Concept: Hellinger Distance and KL Divergence**
  - Why needed here: These metrics quantify the distance between probability distributions (posterior vs. true reward distribution). The paper uses both: Dₕ for its metric properties in neighborhood definitions, and Dₖₗ for its information-theoretic interpretation in posterior updates.
  - Quick check question: Why does the paper use Dₖₗ for information gain (Equation 85) but allow either metric for the convergence neighborhood definition?

## Architecture Onboarding

- **Component map:**
  Initial Controller ǧ(x) → Basis Function Generator (Eq. 11) → Function Space G
                                                                        ↓
  System Dynamics f(x,u,v) → Segment Data Collector → Cost Calculator Jₜ
                                   ↓                         ↓
                          Information Gain Iₜ(k)      Likelihood Ratio Rₜ
                                   ↓                         ↓
                          Stopping Criterion (Eq. 86)  Posterior Updater
                                   ↓                         ↓
                              Segment Manager ←—————— Thompson Sampler
                                   ↓
                          Controller Selector gₜ = argmax J̄ₜ(g)

- **Critical path:**
  1. Initialization: Choose initial controller ǧ and anchor point x̃ (typically origin or typical operating point). Set prior F₀pdf(J̄) (uniform if no prior knowledge).
  2. Basis construction: Compute g^{(i)}_{w,x̃} for all 2ⁿ subsets w ⊆ {1,...,n} using Equation 11. Each basis captures variation along dimensions in w.
  3. Online loop: For each segment t: (a) sample J̄ₜ ~ Fₜpdf, (b) optimize gₜ = argmax_{g∈G} J̄ₜ(g), (c) execute gₜ and collect data until stopping criterion, (d) compute Jₜ(gₜ), (e) update posterior via Equation 23.
  4. Safety interlock: If state exceeds Xsafe, fallback to gsafe with large penalty assigned to current gₜ.

- **Design tradeoffs:**
  - δ̄J (neighborhood radius): Larger δ̄J → faster convergence εₗ but larger residual error J̄max v̄J(B(J̄*, δ̄J)). Tune based on accuracy vs. speed requirements.
  - Γ (scaling factor): Larger Γ → larger control amplitude range, potentially better performance but risk of actuator saturation. Set based on input constraints.
  - Segment length bounds (Kmin, Kmax): Smaller Kmin → faster adaptation but noisier cost estimates. Larger Kmax → more data per update but slower learning.
  - Initial controller choice: Better structural match to system dynamics (e.g., ALC2 with sin(x₁) term) reduces ΔG, but Section VI shows the method works even with unstable initial controllers.

- **Failure signatures:**
  - Exploration collapse: If posterior concentrates too quickly on suboptimal region (Kmin too large or εI too high), set ϵI < 0.01 × initial information gain.
  - Numerical instability: If Lₜ(Ω) underflows during posterior update, use log-space computation: log Rₜ = Σ log(J̄(gᵢ)/J̄*(gᵢ)).
  - Safety violation: If state frequently exceeds Xsafe during early segments, reduce Γ or tighten prior to favor conservative controllers initially.
  - Slow convergence: If regret decays slower than theoretical bound, check that noise variance is bounded (condition 35 may be violated by heavy-tailed disturbances).

- **First 3 experiments:**
  1. Sanity check on 1D system: Implement for scalar system ẋ = ax + bu + v with known optimal LQR solution. Verify that: (a) learned gₜ converges to neighborhood of LQR gain, (b) regret follows Theorem 3 bound. This validates the core TS mechanism without basis function complexity.
  2. Basis function ablation: On the 3D system from Section VI, test with: (a) full 2³ = 8 basis functions, (b) only 1st-order basis (w of size ≤1), (c) only constant + highest-order basis. Measure ΔG empirically to verify Theorem 1 scaling with n.
  3. Nonstationary stress test: Introduce slow drift in system dynamics (e.g., ẋ₃ = sin(x₁ + 0.01t) - ...). Compare: (a) standard algorithm, (b) algorithm with discounting factor λf = 0.95 (Equation 113). Measure tracking error to validate the nonstationary extension claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Thompson sampling strategy be modified to improve control performance and convergence speed specifically during the steady-state phase of the learning process?
- Basis in paper: [explicit] The conclusion states: "Future work will focus on further improving the convergence speed and control performance in the steady-state phase of the learning process."
- Why unresolved: The current method effectively balances exploration and exploitation, but the authors note that exploration near the steady-state can lead to slight performance degradation due to continued sampling of suboptimal control laws.
- What evidence would resolve it: A modified algorithm or convergence proof demonstrating reduced variance in control law selection during steady-state, or a dynamic adjustment mechanism that limits exploration once the posterior converges.

### Open Question 2
- Question: What are the theoretical convergence guarantees and regret bounds for the proposed algorithm when applied to systems with nonstationary reward distributions (e.g., time-varying dynamics)?
- Basis in paper: [explicit] Section V.A.3 discusses extensions for nonstationary processes using discounting factors but states that "analysis of specific theoretical results [is] challenging."
- Why unresolved: The martingale-based convergence proof in Theorem 2 relies on the assumption of a stationary true reward function J̄*, which is violated in time-varying scenarios.
- What evidence would resolve it: A derivation of regret bounds that explicitly accounts for the discounting factor λf and the rate of environmental variation, or a proof of convergence to a tracking error neighborhood.

### Open Question 3
- Question: Can the function space parameterization method be redesigned to mitigate the exponential computational complexity (O(2^{d_i})) associated with high-dimensional state spaces?
- Basis in paper: [inferred] Section V.C states that the computational complexity of the control law depends on 2^{d_i} basis functions, where d_i is the number of relevant states. The authors acknowledge this complexity but do not offer a solution for high-dimensional systems.
- Why unresolved: The construction of the convex hull G_i utilizes all subsets of state variables w ⊆ [1, n], which becomes computationally intractable as the state dimension n increases.
- What evidence would resolve it: A proposed modification to the basis function construction or a sparse sampling technique that reduces the dependency on d_i to polynomial time, along with an analysis of any resulting approximation error.

## Limitations
- The paper assumes bounded noise and Lipschitz cost continuity without discussing robustness to model misspecification or heavy-tailed disturbances.
- The function space construction relies critically on the initial controller's structure matching system dynamics, though this dependency is under-analyzed.
- Posterior updates over infinite-dimensional RKHSs are stated analytically but implementation details are sparse.

## Confidence
- Medium confidence in the TS mechanism and regret bounds (Theorem 3) due to strong theoretical grounding but limited experimental validation beyond one numerical example.
- Medium confidence in the RKHS parameterization approach, as the basis construction (Eq. 11) is well-defined but its practical impact on exploration efficiency remains empirically unverified.
- Low confidence in the stability guarantees under nonstationary dynamics, as the proposed discounting extension (Section V-A.3) lacks rigorous analysis.

## Next Checks
1. Test the algorithm on a 1D system with known optimal solution to verify exponential convergence and regret scaling matches Theorem 3 predictions.
2. Implement the basis function ablation study (full vs. reduced basis sets) to empirically quantify the ΔG approximation error from Theorem 1.
3. Evaluate the discounting mechanism on a slowly drifting system to measure tracking performance versus theoretical guarantees.