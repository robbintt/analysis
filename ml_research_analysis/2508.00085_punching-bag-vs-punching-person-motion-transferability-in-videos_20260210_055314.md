---
ver: rpa2
title: 'Punching Bag vs. Punching Person: Motion Transferability in Videos'
arxiv_id: '2508.00085'
source_url: https://arxiv.org/abs/2508.00085
tags:
- something
- coarse
- fine
- object
- motion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper explores whether action recognition models can transfer\
  \ high-level motion concepts across novel contexts, beyond just generalizing to\
  \ new classes or distributions. The authors propose a motion transferability framework\
  \ with three benchmark datasets\u2014Syn-TA (synthetic), K400-TA, and SSv2-TA\u2014\
  where each is structured with a coarse-to-fine class hierarchy, training on one\
  \ set and testing on another with the same coarse motions but different fine contexts."
---

# Punching Bag vs. Punching Person: Motion Transferability in Videos

## Quick Facts
- **arXiv ID:** 2508.00085
- **Source URL:** https://arxiv.org/abs/2508.00085
- **Reference count:** 40
- **Key outcome:** Motion transferability in action recognition models is limited across novel contexts, with performance drops of 20-65% across benchmarks.

## Executive Summary
This paper investigates whether action recognition models can transfer high-level motion concepts across novel contexts beyond standard generalization tasks. The authors introduce a motion transferability framework with three benchmark datasets structured with coarse-to-fine class hierarchies, enabling evaluation of models' ability to recognize similar motions in different contexts. Through extensive experiments with 13 state-of-the-art unimodal and multimodal models, the study reveals that all models struggle significantly when tested on unseen contexts, even when the underlying motions are similar. The findings demonstrate that current action recognition systems over-rely on object and background cues rather than motion understanding, highlighting a fundamental limitation in how these models generalize.

## Method Summary
The authors create three benchmark datasets—Syn-TA (synthetic), K400-TA, and SSv2-TA—each with a hierarchical class structure that separates coarse motions from fine-grained contexts. They train models on one dataset and test on another with the same coarse motions but different fine contexts, measuring transferability. The evaluation covers 13 state-of-the-art unimodal and multimodal action recognition models, assessing their ability to recognize both coarse and fine motions across the hierarchy. The study also proposes a disentanglement strategy that separates coarse and fine features, which shows promise in improving recognition performance, particularly in temporally challenging datasets. The methodology emphasizes consistent evaluation protocols and provides a systematic framework for assessing motion transferability.

## Key Results
- All 13 evaluated models show significant performance drops (20-65%) when tested on unseen contexts with similar motions
- Multimodal models perform better on coarse motions but still face large drops on fine motions
- Larger models help with spatial cues but not temporal reasoning, indicating limitations in motion understanding
- The proposed disentanglement strategy improves recognition in temporally challenging datasets by separating coarse and fine features

## Why This Works (Mechanism)
Motion transferability is fundamentally limited because current action recognition models rely heavily on object and background cues rather than understanding the underlying motion itself. When the context changes but the motion remains similar, models struggle because they've learned to associate specific actions with particular objects or settings rather than recognizing the motion pattern. This dependency on contextual information becomes problematic when those contexts are absent or different in the target domain, revealing that models have not truly learned generalizable motion concepts.

## Foundational Learning
- **Motion transferability**: The ability of models to recognize similar actions across different contexts; needed to understand generalization beyond standard class boundaries; quick check: can the model recognize "punching" whether it's a person or punching bag?
- **Coarse-to-fine hierarchies**: Class structures that separate broad motion categories from specific contextual details; needed to create controlled transferability benchmarks; quick check: does the hierarchy maintain motion similarity while varying context?
- **Unimodal vs multimodal models**: Models that process either single input types (video only) or multiple (video + audio); needed to assess whether additional modalities help with motion understanding; quick check: do multimodal models show better transferability than unimodal ones?
- **Disentanglement strategies**: Techniques that separate different feature types (coarse motion vs fine context); needed to improve recognition when contexts vary; quick check: does feature separation improve performance on unseen contexts?
- **Temporal reasoning**: The ability to understand motion patterns over time; needed for action recognition beyond static frames; quick check: do larger models improve temporal understanding despite their size?

## Architecture Onboarding
**Component Map:** Input Video/Audio -> Feature Extraction (CNN/RNN/Transformer) -> Temporal Pooling -> Classification Head
**Critical Path:** Feature extraction → Temporal modeling → Classification
**Design Tradeoffs:** Multimodal models gain spatial robustness but struggle with temporal generalization; larger models improve spatial but not temporal reasoning
**Failure Signatures:** Large performance drops on fine-grained motion recognition when contexts change; over-reliance on object/background cues
**First Experiments:** 1) Train on K400-TA, test on SSv2-TA for motion transferability; 2) Apply disentanglement strategy and compare performance; 3) Test multimodal models vs unimodal counterparts on coarse vs fine motions

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic nature of Syn-TA may not fully capture real-world motion transfer complexity
- Focus on specific architectural families may miss alternative solutions for motion transferability
- Disentanglement strategy implementation details and generalizability to other model types remain unclear

## Confidence
- **High confidence:** Core findings about model limitations in motion transferability across diverse architectures and datasets
- **Medium confidence:** Proposed disentanglement approach shows promise but requires broader validation
- **Medium confidence:** Claims about separating coarse and fine features are supported but need more ablation studies

## Next Checks
1. Test the disentanglement strategy on a wider range of model architectures beyond those evaluated in this study
2. Evaluate model performance on real-world datasets with naturally occurring context variations to validate synthetic benchmark findings
3. Conduct extensive ablation studies to determine the optimal balance between coarse and fine feature separation for different action recognition tasks