---
ver: rpa2
title: Optimized Architectures for Kolmogorov-Arnold Networks
arxiv_id: '2512.12448'
source_url: https://arxiv.org/abs/2512.12448
tags:
- learning
- networks
- gates
- kans
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies Kolmogorov-Arnold Networks (KANs) with architectural
  enhancements to improve their expressiveness while maintaining interpretability.
  The authors propose using overprovisioned architectures with differentiable sparsification,
  guided by minimum description length (MDL) principles.
---

# Optimized Architectures for Kolmogorov-Arnold Networks
## Quick Facts
- arXiv ID: 2512.12448
- Source URL: https://arxiv.org/abs/2512.12448
- Authors: James Bagrow; Josh Bongard
- Reference count: 40
- The paper proposes overprovisioned KAN architectures with differentiable sparsification that achieve competitive accuracy while discovering substantially smaller models

## Executive Summary
This paper addresses the fundamental tension in Kolmogorov-Arnold Networks (KANs) between expressiveness and interpretability by proposing architectural enhancements. The authors introduce overprovisioned architectures combined with differentiable sparsification guided by minimum description length (MDL) principles. By using DenseNet-style forward connections alongside edge gates and node gates, the approach learns compact subnetworks during training while maintaining competitive or superior accuracy compared to baseline KANs across multiple benchmarks.

The key insight is that overprovisioning provides the capacity needed for complex function approximation, while sparsification simultaneously optimizes for both accuracy and model compactness. This transforms architecture search into an end-to-end optimization problem, enabling the discovery of interpretable subnetworks that achieve high performance without sacrificing the transparency that makes KANs valuable for scientific machine learning applications.

## Method Summary
The authors propose an enhanced KAN architecture that overprovisions the network with additional connections and gates, then learns to sparsify it during training using differentiable mechanisms. The architecture incorporates DenseNet-style forward connections between nodes in the same layer, allowing information to flow through multiple paths. Edge gates (egates) control individual connection weights, while node gates (ngates) regulate the influence of each node's contribution. Sparsification is achieved through regularization terms based on MDL principles, where gate values are pushed toward binary states. The method optimizes both the underlying function parameters and the gate values simultaneously, enabling the network to discover compact, high-performing subnetworks that balance accuracy with interpretability.

## Key Results
- On Nguyen symbolic regression benchmark, achieved test R² ≈ 1 with significantly fewer active functions than ungated conditions
- For dynamical systems forecasting, reduced network size by up to 81% for the Ikeda map while maintaining accurate predictions
- On real-world datasets (concrete strength and superconductor critical temperature), improved predictive accuracy while reducing model complexity by up to 88.7%
- Demonstrated that overprovisioning and sparsification are synergistic, with combined approach typically outperforming either technique alone

## Why This Works (Mechanism)
The approach works by leveraging overprovisioning to provide sufficient representational capacity for complex functions while using differentiable sparsification to discover compact subnetworks that balance accuracy and interpretability. The DenseNet-style forward connections enable multiple information flow paths, while edge and node gates provide fine-grained control over which connections and nodes contribute to the final prediction. MDL-guided regularization encourages gate values to converge toward binary states, effectively selecting the most informative components. This end-to-end optimization transforms architecture search into a continuous optimization problem, allowing the network to discover sparse, high-performing structures without requiring separate search procedures.

## Foundational Learning
- **Kolmogorov-Arnold Networks (KANs)**: Replace fixed-weight connections with learnable univariate functions, enabling interpretable function approximation. Why needed: Provides the foundation for interpretable scientific machine learning. Quick check: Understand the difference between KANs and traditional neural networks.
- **Overprovisioning**: Intentionally creating larger architectures than needed during training. Why needed: Provides sufficient capacity for complex function approximation while enabling sparsification. Quick check: Verify that overprovisioned models can achieve better accuracy than smaller baselines.
- **Differentiable sparsification**: Using continuous gate values that can be optimized via gradient descent, rather than discrete architecture search. Why needed: Enables end-to-end optimization of both function parameters and architecture. Quick check: Confirm gate values converge toward binary states during training.
- **Minimum Description Length (MDL)**: A principle for balancing model complexity against accuracy by minimizing the total description length of the model and data. Why needed: Provides theoretical guidance for regularization in sparsification. Quick check: Understand how MDL relates to other regularization approaches like L1/L2.
- **DenseNet connections**: Forward connections between nodes in the same layer enabling multiple information flow paths. Why needed: Increases representational capacity without requiring deeper networks. Quick check: Verify that forward connections improve accuracy compared to standard KANs.
- **Edge and node gates**: Mechanisms for controlling individual connection weights and node contributions respectively. Why needed: Provide fine-grained control over model sparsity and interpretability. Quick check: Analyze which gates remain active in final subnetworks.

## Architecture Onboarding
Component map: Input -> Function Layers (with forward connections, egates, ngates) -> Output
Critical path: Data flows through input layer to output layer, with multiple paths enabled by forward connections and controlled by gates
Design tradeoffs: Overprovisioning vs. computational cost during training, sparsity level vs. accuracy, gate complexity vs. interpretability
Failure signatures: Over-regularization leading to underfitting, insufficient overprovisioning limiting representational capacity, improper MDL parameters causing ineffective sparsification
First experiments:
1. Compare ungated vs. gated KANs on simple symbolic regression tasks to verify gate effectiveness
2. Test different levels of overprovisioning on benchmark datasets to find optimal capacity
3. Analyze gate activation patterns in discovered subnetworks to verify interpretability

## Open Questions the Paper Calls Out
None

## Limitations
- Absence of theoretical analysis explaining why overprovisioning and sparsification work together effectively
- Computational efficiency concerns during training with large overprovisioned architectures
- Uncertainty about whether near-perfect benchmark scores represent genuine capabilities or overfitting
- Limited validation on high-dimensional scientific datasets where interpretability is most crucial

## Confidence
- High confidence: The empirical methodology is sound and results are reproducible as presented
- Medium confidence: The superiority of combined overprovisioning and sparsification over individual techniques
- Low confidence: Claims about interpretability improvements and generalizability to real-world scientific problems

## Next Checks
1. Test the approach on high-dimensional scientific datasets (e.g., molecular property prediction or climate modeling) where interpretability is crucial, comparing learned subnetworks against domain knowledge
2. Perform ablation studies varying the degree of overprovisioning and sparsification rates to establish optimal trade-offs between accuracy and model size
3. Analyze the computational overhead during training versus inference, particularly for architectures where the full model contains orders of magnitude more parameters than the final sparse subnetwork