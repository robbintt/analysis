---
ver: rpa2
title: 'A Different Approach to AI Safety: Proceedings from the Columbia Convening
  on Openness in Artificial Intelligence and AI Safety'
arxiv_id: '2506.22183'
source_url: https://arxiv.org/abs/2506.22183
tags:
- safety
- risks
- systems
- open
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper reports on a collaborative research effort involving
  over 45 AI experts who convened to address gaps in open-source AI safety. The authors
  identified key limitations in current risk taxonomies and AI safety tooling, particularly
  around agentic systems, content safety filters, and participatory approaches.
---

# A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety

## Quick Facts
- arXiv ID: 2506.22183
- Source URL: https://arxiv.org/abs/2506.22183
- Reference count: 0
- Over 45 AI experts identified critical gaps in open-source AI safety, particularly around agentic systems, compositional harms, and participatory mechanisms.

## Executive Summary
This paper reports on a collaborative research effort examining how openness in AI development can enhance safety. The authors convened over 45 experts to map current technical interventions and identify gaps in AI safety tooling. They found that while significant progress has been made in content filtering and risk taxonomies, critical gaps remain in handling agentic systems, compositional harms, multilingual coverage, and participatory safety mechanisms. The work emphasizes that openness—through transparent weights, interoperable tooling, and public governance—can enable independent scrutiny and culturally plural oversight of AI systems.

## Method Summary
The research involved categorizing post-training technical interventions across the AI development workflow, mapping existing tools and datasets to specific risk domains, and identifying gaps through expert analysis. The team examined tools for content safety, bias detection, PII protection, and model integrity, organizing them by development stage (data tuning, model tuning, online filtering, evaluation, monitoring) and risk taxonomy alignment. The method focused on systematic gap identification rather than experimental validation, synthesizing input from over 45 AI safety experts.

## Key Results
- Identified critical gaps in safety tooling for multimodal/multilingual coverage and standardization
- Highlighted limitations in current taxonomies for agentic systems and compositional harms
- Mapped existing technical interventions across the AI development workflow
- Emphasized need for participatory mechanisms and context-specific safety tools

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Open model access enables safety evaluation techniques that are impossible with black-box APIs
- Mechanism: When model weights and logits are accessible, evaluators can compute probability-based metrics (AUCPR, average precision) and apply interpretability methods like sparse autoencoders. Black-box APIs typically return only token outputs without probability scores, limiting evaluation fidelity.
- Core assumption: More granular access to model internals yields higher-fidelity safety assessments that translate into better interventions
- Evidence anchors:
  - [abstract]: "openness—understood as transparent weights, interoperable tooling, and public governance—can enhance safety by enabling independent scrutiny, decentralized mitigation, and culturally plural oversight"
  - [section 1.1]: "computing average precision and traditional AUCPR metrics is not feasible for black-box APIs like Azure API and GPT-4, as the APIs do not provide the probability scores required for this metric"
  - [corpus]: Limited direct testing of this mechanism; neighbor papers discuss transparency conceptually but don't empirically validate safety improvements from weight access
- Break condition: If external auditors lack resources to actually perform deeper analysis, or if model updates invalidate open evaluations faster than they can be conducted

### Mechanism 2
- Claim: Content safety classifiers catch individual harmful outputs but fail to detect compositional harm from action sequences
- Mechanism: Standard classifiers evaluate single inputs/outputs against static taxonomies. In agentic systems, each action may appear benign in isolation (checking account balance, creating a template), but the sequence enables harmful outcomes (coordinated transfers below threshold). Safety requires trajectory-level evaluation, not just per-action filtering.
- Core assumption: Current safety architectures were designed for single-turn generation, not multi-step autonomous action
- Evidence anchors:
  - [abstract]: "limited defenses against prompt-injection and compositional attacks in agentic systems"
  - [section 3.3]: "Actions that are harmless on their own may become problematic when combined... individual steps involved in synthesizing a drug may be innocuous, but when executed in a specific sequence could lead to illegal activity"
  - [section 3.4]: "Safety for agents thus requires intent detection, environmental awareness, state tracking, and assessment of a system's real-time decision-making"
  - [corpus]: No direct corpus validation; this is a gap identification, not a tested solution
- Break condition: When the combinatorial explosion of possible action sequences makes trajectory-level monitoring computationally infeasible

### Mechanism 3
- Claim: Participatory inputs surface edge cases and context-specific harms that homogeneous developer teams miss
- Mechanism: Developer teams have bounded perspectives; affected communities possess local knowledge and lived experience that reveals failure modes invisible to outsiders. Incorporating diverse feedback through multiple pipeline entry points (datasets, red-teaming, incident reporting, system prompts) produces safety systems adapted to specific contexts.
- Core assumption: Developers systematically overlook harms experienced by communities unlike themselves
- Evidence anchors:
  - [abstract]: "insufficient participatory mechanisms for communities most affected by AI harms"
  - [section 5.1]: "Local/contextual knowledge and lived experiences of affected communities may not be captured in traditional technical documentation. Collective input helps surface edge cases, unintended consequences, and real-world failure modes that developers have been known to miss"
  - [section 1.3.4]: "Many discussions on AI safety disproportionately focus on direct users and viewers... and not on a broader view of the algorithmic subjects whose lives are impacted by a system's decisions"
  - [corpus]: Neighbor papers on participatory budgeting suggest civic engagement can work but don't validate safety improvement specifically
- Break condition: If participatory mechanisms become extractive (uncompensated labor), or if collected feedback isn't meaningfully integrated into system behavior

## Foundational Learning

- **Content Safety Classifier Types**
  - Why needed here: The paper distinguishes safeguard LLMs (LlamaGuard, ShieldGemma) from programmable guardrails (NeMo, Guardrails AI) from non-LLM ML classifiers—each with different latency/accuracy/customizability tradeoffs
  - Quick check question: Given a real-time chatbot with strict latency requirements, would you choose a 27B parameter safeguard LLM or a lightweight BERT-based classifier? What do you trade off?

- **Agentic Safety vs. Content Safety**
  - Why needed here: The paper emphasizes that agentic systems require fundamentally different safety approaches—trajectory evaluation, compositional risk assessment, intent detection—than text/image generation systems
  - Quick check question: A web agent can book hotels, call rides, and read reviews. Why might a hotel+ride booking sequence be harmful even though each action passes your content filters?

- **Openness Spectrum in Foundation Models**
  - Why needed here: The paper uses "open" broadly to include open-weight models (weights available, training data closed) through fully open-source; what safety interventions are possible depends on where a model sits on this spectrum
  - Quick check question: If you have model weights but not training data or code, what safety evaluation or intervention techniques are no longer available to you?

## Architecture Onboarding

- **Component map:**
  Input classifier → filters adversarial/OT-policy inputs before model processing
  Output classifier → filters generated responses against safety taxonomy
  Trajectory monitor (agents only) → evaluates action sequences for compositional harm
  Incident reporting → collects production failures for evaluation dataset updates
  Evaluation harness → benchmarks against safety leaderboards (HELM-Safety, DecodingTrust)

- **Critical path:**
  1. Select base model using safety-focused leaderboards
  2. Implement input/output filtering with taxonomy-aligned classifiers
  3. For agentic systems: add trajectory-level monitoring with state tracking
  4. Establish feedback loops from production incidents to evaluation datasets
  5. Integrate participatory inputs at leverage points (dataset labeling, red-teaming, system prompts)

- **Design tradeoffs:**
  - Latency vs. accuracy: ShieldGemma 27B (accurate, slow) vs. LlamaGuard 1B (faster, less reliable)
  - Specificity vs. generalization: Narrow taxonomy classifiers (precise, brittle) vs. zero-shot LLM safeguards (flexible, inconsistent)
  - Centralized vs. plural: Single global taxonomy (consistent, culturally narrow) vs. localized policies (context-appropriate, fragmented)

- **Failure signatures:**
  - Over-triggering on identity terms (Perspective API flagged "Black," "Muslim" as toxic due to biased training data)
  - Language-specific over-blocking (German content blocked more than English for same policies)
  - Compositional blind spots (each action passes filters, sequence causes harm)
  - Static taxonomy drift (novel attack patterns bypass outdated classifiers)

- **First 3 experiments:**
  1. Benchmark LlamaGuard vs. ShieldGemma on a multilingual safety test set to quantify language-coverage gaps in current classifiers
  2. Build a trajectory-level classifier for a simple web agent that flags compositional risks (e.g., booking sequences) that single-action filters miss
  3. Deploy a minimal incident-reporting pipeline on a small production system; measure whether community-reported issues predict downstream harm incidents

## Open Questions the Paper Calls Out

### Open Question 1
- Question: At what specific points in the AI development and deployment pipelines can participatory inputs and democratic engagement enhance safety tools and systems?
- Basis in paper: [explicit] Section 1.4.1 states: "At what points in the AI development and deployment pipelines can participatory inputs and democratic engagement enhance safety tools and systems—making them more pluralistic and better adapted to specific communities and contexts?"
- Why unresolved: Participatory mechanisms for communities most affected by AI harms remain insufficient; current taxonomies are biased toward developer perspectives and lack deployment-specific and community-specific nuances.
- What evidence would resolve it: Empirical studies mapping participatory intervention points across the pipeline with measurable improvements in safety outcomes for diverse communities.

### Open Question 2
- Question: How can safety systems effectively evaluate compositional risks in agentic systems, where individually harmless actions combine to produce harmful outcomes?
- Basis in paper: [explicit] Section 3.3 notes: "Actions that are harmless on their own may become problematic when combined... The composability of steps in a task or decision-making process results in a vast number of possible scenarios, significantly increasing the number of potential risk vectors."
- Why unresolved: Current safety mechanisms evaluate actions in isolation rather than assessing sequences; limited defenses exist against prompt-injection and compositional attacks in agentic systems.
- What evidence would resolve it: Development and validation of trajectory-level classifiers or multi-turn evaluation benchmarks that detect harmful action sequences with high precision and recall.

### Open Question 3
- Question: What incentive mechanisms can effectively motivate diverse communities to contribute to participatory AI safety efforts without creating extractive labor dynamics?
- Basis in paper: [explicit] Section 5.3 identifies this as a research avenue: "Research on incentive mechanisms for diverse communities to contribute to participatory AI is key."
- Why unresolved: The paper notes that monetary compensation can paradoxically reduce participation, and current mechanisms risk becoming exploitative forms of free labor.
- What evidence would resolve it: Comparative studies of incentive structures across communities measuring participation rates, contribution quality, and participant-reported agency and satisfaction.

### Open Question 4
- Question: How can content safety classifiers be made more adaptable and controllable across modalities, languages, and domain-specific policies?
- Basis in paper: [explicit] Section 4.5 states the need for "greater controllability, broader modality coverage, improved transparency, and stronger evaluation practices." Gaps include scarce multimodal and multilingual benchmarks and static filters that do not generalize across applications.
- Why unresolved: Most content safety classifiers are text-only, English-focused, and trained on fixed policy sets that do not align with diverse deployment contexts.
- What evidence would resolve it: Development of parameter-efficient tuning methods enabling customization with small datasets, validated across multiple modalities, languages, and policy configurations.

## Limitations
- Limited empirical validation of proposed mechanisms
- Deployment complexity and integration challenges not fully addressed
- Unclear how to handle conflicting safety norms across cultural contexts

## Confidence
- High confidence: Identification of specific technical gaps (multimodal coverage, agentic system vulnerabilities, static taxonomies) is well-supported by literature review and expert convening discussions
- Medium confidence: Claim that openness enhances safety through independent scrutiny is conceptually sound but lacks empirical validation
- Low confidence: Specific quantitative claims about safety improvement from participatory inputs or multilingual coverage gaps would require dedicated studies to verify

## Next Checks
1. **Experimental validation of openness benefits**: Implement the same safety evaluation on both open-weight and black-box API models to measure actual differences in detection capabilities, false positive rates, and computational overhead
2. **Trajectory-level safety prototype**: Build a simple agent that performs multi-step actions (e.g., information gathering → decision → action) and test whether trajectory-level monitoring catches harmful sequences that per-action filters miss
3. **Participatory feedback integration study**: Deploy a system that collects safety feedback from diverse user groups and measure whether this feedback predicts actual safety incidents better than developer-only testing, controlling for community size and engagement levels