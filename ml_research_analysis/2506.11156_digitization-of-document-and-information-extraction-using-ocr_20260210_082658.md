---
ver: rpa2
title: Digitization of Document and Information Extraction using OCR
arxiv_id: '2506.11156'
source_url: https://arxiv.org/abs/2506.11156
tags:
- text
- extraction
- document
- documents
- scanned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents a hybrid framework combining OCR techniques
  with Large Language Models (LLMs) to extract structured information from both scanned
  and digital documents. The system preprocesses scanned documents with techniques
  like binarization and skew correction, uses OCR tools (Tesseract, DocTR, Google
  Vision API) for text extraction, and employs LLMs for semantic interpretation and
  key-value pair extraction.
---

# Digitization of Document and Information Extraction using OCR

## Quick Facts
- arXiv ID: 2506.11156
- Source URL: https://arxiv.org/abs/2506.11156
- Reference count: 0
- Primary result: Hybrid OCR-LLM framework achieves ~94% OCR accuracy on scanned docs and near-perfect digital parsing with flexible semantic extraction

## Executive Summary
This paper presents a hybrid framework that combines Optical Character Recognition (OCR) techniques with Large Language Models (LLMs) to extract structured information from both scanned and digital documents. The system preprocesses scanned documents using techniques like binarization and skew correction, applies OCR tools (Tesseract, DocTR, Google Vision API) for text extraction, and leverages LLMs for semantic interpretation and key-value pair extraction. For digital documents, the framework uses direct parsing via tools like pdfplumber and PyMuPDF. The Google Vision API achieved the highest OCR accuracy at approximately 94%, while digital parsing reached near-perfect accuracy of approximately 100%. LLM-based extraction produced structured JSON outputs with high precision and minimal false positives, outperforming traditional rule-based methods in flexibility and semantic accuracy. The framework demonstrates scalability across document types and sets the foundation for future improvements in handwriting recognition, multilingual support, and real-time feedback.

## Method Summary
The proposed framework follows a hybrid approach to document digitization and information extraction. For scanned documents, the system first applies preprocessing techniques including binarization, skew correction, and denoising to improve OCR accuracy. It then uses multiple OCR tools (Tesseract, DocTR, Google Vision API) to extract text from these preprocessed images. For digital documents, the framework bypasses OCR entirely and uses direct parsing tools like pdfplumber and PyMuPDF to extract text. After text extraction, the framework employs LLMs for semantic interpretation and key-value pair extraction, converting unstructured text into structured JSON outputs. The system handles both document types through a unified pipeline that adapts to the source format, with the LLM component providing flexibility in extracting meaningful information patterns rather than relying on rigid rule-based extraction methods.

## Key Results
- Google Vision API achieved the highest OCR accuracy at approximately 94% on scanned documents
- Digital document parsing reached near-perfect accuracy of approximately 100%
- LLM-based extraction produced structured JSON outputs with high precision and minimal false positives
- Framework outperformed traditional rule-based methods in flexibility and semantic accuracy

## Why This Works (Mechanism)
The hybrid approach combines the strengths of traditional OCR for text recognition with the semantic understanding capabilities of LLMs. OCR tools handle the technical challenge of converting visual text representations into machine-readable text, while LLMs interpret the extracted text semantically to identify meaningful patterns, relationships, and structured data. This division of labor allows the system to achieve high accuracy in both text recognition and information extraction tasks. The framework's adaptability to different document types (scanned vs. digital) ensures optimal processing paths for each format, maximizing overall efficiency and accuracy.

## Foundational Learning
- OCR preprocessing techniques (binarization, skew correction, denoising) - Why needed: Improves OCR accuracy by enhancing image quality; Quick check: Compare OCR results with and without preprocessing on degraded scans
- Multiple OCR tool integration - Why needed: Different tools have varying strengths for different document types; Quick check: Benchmark OCR tools across diverse document samples
- LLM-based semantic extraction - Why needed: Extracts meaningful information beyond simple text matching; Quick check: Validate extracted key-value pairs against ground truth annotations
- Hybrid document processing pipeline - Why needed: Optimizes processing for different document formats; Quick check: Measure accuracy differences between scanned and digital document processing

## Architecture Onboarding

Component Map:
Preprocessing -> OCR Extraction -> LLM Semantic Interpretation -> Structured Output

Critical Path:
For scanned documents: Preprocessing -> OCR (Google Vision API) -> LLM extraction -> JSON output
For digital documents: Direct parsing (pdfplumber/PyMuPDF) -> LLM extraction -> JSON output

Design Tradeoffs:
- OCR tool selection: Multiple tools tested but Google Vision API chosen for best accuracy vs. cost/performance balance
- Preprocessing complexity: More sophisticated preprocessing improves OCR but adds computational overhead
- LLM vs. rule-based extraction: LLMs provide flexibility but require more computational resources than deterministic rules

Failure Signatures:
- OCR errors manifest as garbled text, missing characters, or incorrect character recognition
- LLM extraction failures appear as missing key-value pairs, incorrect field mappings, or contextual misinterpretations
- Preprocessing failures result in poor OCR quality despite subsequent processing steps

First Experiments:
1. Test OCR accuracy on a diverse corpus including handwritten documents and low-quality scans
2. Evaluate LLM extraction performance across multiple document domains with human-annotated ground truth
3. Measure real-time performance and scalability using large document batches with varying layouts

## Open Questions the Paper Calls Out
The paper identifies several areas for future work, including improving handwriting recognition capabilities, expanding multilingual support, and implementing real-time feedback mechanisms. The framework also suggests potential enhancements in handling more complex document layouts and exploring additional LLM models for improved semantic extraction performance.

## Limitations
- Evaluation focuses primarily on scanned documents with mixed quality and digital PDFs, with unclear sample size and diversity
- Reported OCR accuracy may not generalize to more complex layouts, degraded scans, or non-Latin scripts
- LLM robustness across document domains and potential for hallucinations or contextual misinterpretations are not thoroughly addressed
- Scalability claims lack empirical validation beyond described use cases

## Confidence

**OCR accuracy claims: Medium** - Limited to specific document types and quality ranges
**LLM extraction performance: Low** - Insufficient detail on error analysis and edge cases  
**Framework generalizability: Medium** - Promising results but narrow evaluation scope

## Next Checks
1. Test the framework on a diverse corpus including handwritten documents, low-quality scans, and non-English text to assess robustness
2. Conduct error analysis comparing LLM extractions against human-annotated ground truth across multiple document types
3. Evaluate real-time performance and scalability using large document batches with varying layouts and complexities