---
ver: rpa2
title: 'MARIA: A Framework for Marginal Risk Assessment without Ground Truth in AI
  Systems'
arxiv_id: '2510.27163'
source_url: https://arxiv.org/abs/2510.27163
tags:
- risk
- evaluation
- system
- ground
- truth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MARIA, a framework for marginal risk assessment
  without ground truth when deploying AI systems to replace existing processes. The
  framework shifts evaluation from absolute risk to relative risk differences, addressing
  four key challenges: unknowable, delayed, expensive, and withheld ground truth.'
---

# MARIA: A Framework for Marginal Risk Assessment without Ground Truth in AI Systems

## Quick Facts
- arXiv ID: 2510.27163
- Source URL: https://arxiv.org/abs/2510.27163
- Authors: Jieshan Chen; Suyu Ma; Qinghua Lu; Sung Une Lee; Liming Zhu
- Reference count: 11
- One-line primary result: Framework enables risk assessment by measuring relative differences against baseline system rather than requiring absolute ground truth

## Executive Summary
MARIA addresses the challenge of evaluating AI systems when ground truth is unavailable, delayed, expensive, or withheld. The framework shifts from absolute risk assessment to marginal risk analysis by measuring the difference between a new AI system and an existing baseline. Through three complementary dimensions—predictability, capability, and interaction dominance—MARIA provides actionable insights for responsible AI deployment in real-world workflows. The framework was demonstrated on document evaluation, revealing that AI reviewers showed high self-consistency but lower agreement with human evaluators compared to human-human pairs, indicating moderate performance shifts while identifying mild fairness biases and new security considerations balanced against efficiency gains.

## Method Summary
MARIA evaluates marginal risk (MR = R_new - R_baseline) when deploying AI to replace existing human processes without ground truth. The framework uses a 4-phase workflow: (1) setup and define assumptions, (2) generate outputs and apply metrics, (3) calibrate and validate, and (4) aggregate and report relative ordering. The evaluation is structured around three dimensions: predictability (self-consistency and input stability), capability (agreement rates, efficiency, fairness, security), and interaction dominance (game-based evaluations). The method requires a document dataset with predefined evaluation criteria, paraphrased variants for stability testing, and comparison between human-AI agreement and human-human baseline pairs.

## Key Results
- AI reviewers demonstrated high self-consistency across repeated evaluations but showed lower agreement with human evaluators compared to human-human pairs
- Document evaluation revealed moderate performance shifts with AI systems showing mild fairness biases and new security considerations (e.g., prompt injection)
- The framework successfully identified efficiency gains balanced against potential risks, enabling actionable deployment decisions
- Three-dimensional evaluation approach provided comprehensive risk assessment across predictability, capability, and interaction dominance

## Why This Works (Mechanism)

### Mechanism 1: Risk Delta via Comparative Proxy
If ground truth is unavailable, the marginal risk of a new system can be assessed by measuring the relative difference in behavior and performance against an incumbent baseline rather than an absolute oracle. The framework calculates MR = ΔR = R_new - R_baseline, treating the existing process as the reference distribution. This shifts evaluation from "Is this output correct?" to "Does this new system deviate from the accepted risk profile of the old one?" The core assumption is that the baseline system represents a "tolerable" level of risk, even if it is imperfect or inconsistent.

### Mechanism 2: Internal Consistency as a Stability Signal
High self-consistency and low variance under perturbation serve as proxies for reliability when external validation is impossible. By running the same input multiple times or applying semantic-preserving transformations, the system measures dispersion. A system that agrees with itself is statistically less likely to be chaotic or hallucinating randomly, reducing operational unpredictability. The core assumption is that consistency implies reliability—a system that is stable under noise is likely more robust than one that is not.

### Mechanism 3: Interaction Dominance for Behavioral Profiling
Structured symmetric games between the new AI and the baseline system reveal emergent risks and adaptability gaps that static benchmarks miss. Instead of evaluating isolated outputs, systems are pitted against each other in controlled interactions. If the AI dominates the baseline by "winning" arguments via hallucination or manipulation rather than logic, it flags a new security/risk vector. The core assumption is that behavior in synthetic games correlates with behavior in real-world, high-stakes interactions.

## Foundational Learning

- **Concept: Marginal Risk vs. Absolute Risk**
  - Why needed: Traditional AI evaluation obsesses over "Is it right?" (Accuracy). MARIA requires the learner to ask "Is it safer or riskier than what we have now?" (Delta).
  - Quick check: If a new AI system has 85% accuracy and the human baseline has 80% accuracy, but the AI makes unpredictable errors, is the marginal risk positive or negative?

- **Concept: Ground Truth Constraints**
  - Why needed: The framework's necessity depends on the realization that ground truth is often a luxury. Distinguishing between "Unknowable," "Delayed," and "Withheld" defines which MARIA dimension to rely on.
  - Quick check: In a grant review process, the "true" quality of a proposal is often only knowable years later after funding. Which ground truth challenge is this?

- **Concept: Proxy Metrics**
  - Why needed: Without ground truth, one must measure side-effects. Learners must understand how stability, throughput, and win-rates serve as indirect indicators of system health.
  - Quick check: Why is "Self-Consistency" a valid proxy for reliability in the absence of a correct answer key?

## Architecture Onboarding

- **Component map:** Data Prep -> Metric Layer (Predictability, Capability, Interaction) -> Aggregator -> Report
- **Critical path:** The Baseline Assumption Check. If the engineer does not strictly verify that the "incumbent" system's data is not contaminated and that the baseline is a valid reference, the comparative metrics are invalid.
- **Design tradeoffs:** Coverage vs. Cost (running all three dimensions is comprehensive but expensive); Automation vs. Depth (predictability is fully automatable; capability requires human alignment checks).
- **Failure signatures:** "Confidently Wrong" Profile (High Predictability + Low Agreement with Baseline); High Variance on Edge Cases (Low Self-Consistency on noisy inputs).
- **First 3 experiments:** 1) Self-Consistency Stress Test (run AI reviewer on 50 documents with 10 different seeds each, calculate variance); 2) Agreement Delta Analysis (compare AI scores to historical Human-Human scores, calculate "Agreement Gap"); 3) Perturbation Robustness (paraphrase 20 inputs, check if AI's output changes significantly).

## Open Questions the Paper Calls Out

### Open Question 1
How should practitioners determine the weighting schemes for the multi-dimensional risk vector when dimensions such as capability and predictability conflict? The framework outputs a vector of differences, but decision-making often requires a scalar risk score or strict dominance, which current weighting ambiguity prevents.

### Open Question 2
How can MARIA maintain validity if the assumption of "controlled provenance" is violated (e.g., if the AI was trained on the baseline system's outputs)? The framework relies on comparative metrics like cross-consensus, which risk inflation or contamination if the new system has seen the baseline's data.

### Open Question 3
Does the completion of Phases 3 (Calibration) and 4 (Aggregation) significantly alter the preliminary risk profile derived from Phases 1 and 2? The paper demonstrates the analytical setup and comparative measurement but leaves unverified whether the full workflow yields different deployment decisions than the partial implementation.

## Limitations
- Reliance on baseline system as proxy for "acceptable risk" means the framework only identifies systems that replicate baseline flaws rather than improving upon them
- Assumes consistency and stability are proxies for reliability, which may not hold in domains where systematic bias is more dangerous than random error
- Interaction dominance dimension requires careful game design to ensure synthetic evaluations correlate with real-world performance

## Confidence
- **High confidence**: The three-dimensional evaluation approach is methodologically sound and well-grounded in existing research
- **Medium confidence**: Case study results are internally consistent but limited in scope; broader domain validation is needed
- **Medium confidence**: Aggregation and normalization methods are conceptually justified but not fully specified

## Next Checks
1. **Baseline Drift Analysis**: Test MARIA's sensitivity by deliberately introducing known biases into the baseline system and verifying that the framework detects increased marginal risk
2. **Cross-Domain Applicability**: Apply the framework to a different domain (e.g., medical diagnosis or legal document review) to validate generalizability beyond document evaluation
3. **Ground Truth Validation**: In a domain where delayed ground truth becomes available, compare MARIA's marginal risk assessments against actual outcome data to measure predictive accuracy