---
ver: rpa2
title: 'Strong Linear Baselines Strike Back: Closed-Form Linear Models as Gaussian
  Process Conditional Density Estimators for TSAD'
arxiv_id: '2602.00672'
source_url: https://arxiv.org/abs/2602.00672
tags:
- anomaly
- linear
- detection
- time
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work revisits time series anomaly detection (TSAD) by proposing
  a simple linear autoregressive model trained via ordinary least squares (OLS) regression.
  The authors show that this closed-form solution consistently matches or outperforms
  state-of-the-art deep learning detectors across extensive univariate and multivariate
  benchmarks, while requiring orders of magnitude fewer computational resources.
---

# Strong Linear Baselines Strike Back: Closed-Form Linear Models as Gaussian Process Conditional Density Estimators for TSAD

## Quick Facts
- arXiv ID: 2602.00672
- Source URL: https://arxiv.org/abs/2602.00672
- Reference count: 25
- Primary result: Simple linear autoregressive model trained via OLS consistently matches or outperforms state-of-the-art deep learning detectors across extensive univariate and multivariate benchmarks

## Executive Summary
This work challenges the prevailing assumption that deep learning is necessary for time series anomaly detection by demonstrating that a simple linear autoregressive model trained via ordinary least squares (OLS) achieves state-of-the-art performance. The authors show their method consistently matches or exceeds the accuracy of complex deep learning detectors while requiring orders of magnitude fewer computational resources. Theoretically, they establish that OLS-based autoregression corresponds to Gaussian process conditional density estimation, providing a principled explanation for why linear models can reliably detect diverse anomaly types.

## Method Summary
The method uses a linear autoregressive model y_t = W^T x_t + ε_t where x_t contains lagged observations and a bias term. Parameters W are estimated via closed-form OLS solution Ŵ = (X^T X + λI)^{-1} X^T Y, or via reduced-rank regression (RRR) for multivariate data using SVD truncation. Anomaly scores are computed as squared prediction residuals s_t = ||y_t - W^T x_t||_F^2, with thresholds selected via Best-F1 sweep. The approach requires minimal hyperparameter tuning and works with both univariate and multivariate time series.

## Key Results
- Linear models achieve 3.55 average rank (top 1 on 6/10 datasets) compared to 5.20 for deep learning baselines
- On multivariate benchmarks, RRR achieves 2.30 average rank (top 1 on 4/5 datasets) while all deep models rank 4.50-6.30
- The method provides up to 10,000× speedup compared to deep learning baselines
- Performance is robust across window sizes p ∈ [16,128] and RRR ranks r ∈ [4,64]

## Why This Works (Mechanism)

### Mechanism 1
OLS on lagged features is equivalent to the finite-history posterior mean of stationary Gaussian processes, making squared prediction residuals valid conditional density-based anomaly scores. For a stationary GP, conditioning on a finite history window yields a Gaussian posterior with mean m_h(i) = α_h^T y_{i-h:i-1}, which is exactly what OLS estimates. Anomalies correspond to low conditional density events.

### Mechanism 2
Closed-form OLS solutions eliminate gradient-based optimization instability. The OLS estimator Ŵ = (X^T X)^{-1} X^T Y provides the unique maximum likelihood solution under Gaussian noise, guaranteeing convergence without learning rate sensitivity, local minima, or initialization dependence.

### Mechanism 3
Reduced-rank regression exploits shared latent temporal patterns across channels in multivariate systems. Under the linear model of coregionalization, multiple output channels share Q temporal patterns with low-rank coregionalization matrices. The rank constraint rank(W) ≤ r is enforced via SVD truncation, projecting onto the r-dimensional subspace capturing dominant latent factors.

## Foundational Learning

- **Ordinary Least Squares (OLS)**: Core parameter estimation method; understanding the closed-form solution Ŵ = (X^T X)^{-1} X^T Y and ridge regularization is essential. Quick check: Given X (T×p) and Y (T×d), write the ridge-regularized OLS solution.
- **Gaussian Process posterior conditioning**: Provides theoretical justification for why linear autoregression captures diverse anomaly types through finite-history GP equivalence. Quick check: For a GP with stationary kernel k, why does the posterior mean given a finite history become a linear function of past observations?
- **Reduced-Rank Regression via SVD**: Multivariate extension requiring rank-r approximation via Ŵ_RRR = Ŵ_OLS V_r V_r^T. Quick check: How does the Eckart-Young theorem guarantee that SVD truncation yields the optimal low-rank approximation?

## Architecture Onboarding

- **Component map**: Preprocessing -> Feature construction -> Model fitting -> Anomaly scoring -> Threshold selection
- **Critical path**: Window size p → Feature matrix X → OLS/RRR fit → Residual computation → Anomaly scores → Threshold sweep
- **Design tradeoffs**: Larger p captures longer history but increases O(p·d) complexity; RRR rank r balances efficiency vs. expressiveness; differencing order depends on stationarity
- **Failure signatures**: High variance on normal data → check preprocessing; poor pattern-shape anomaly detection → expected behavior; RRR underperforms OLS → rank too low
- **First 3 experiments**: 1) Implement OLS with p=64, λ=1e-6 on AIOPS; 2) Test p ∈ {16,32,64,96,128} on SMD; 3) Sweep rank r ∈ {4,8,16,32,64} for RRR on MSL

## Open Questions the Paper Calls Out
1. What benchmark characteristics would create genuine separation between linear and deep TSAD methods?
2. Can the Gaussian process interpretation be extended to characterize when non-linear models become theoretically necessary?
3. How does OLS performance degrade under distribution shift or concept drift scenarios?

## Limitations
- Theoretical GP equivalence assumes stationarity, which may not hold for non-stationary anomaly types
- RRR mechanism requires low-rank cross-channel structure that may not generalize to all multivariate systems
- No adaptive window/rank selection procedures for new datasets; performance depends on preprocessing choices

## Confidence
- **High**: Benchmark results showing OLS outperforms deep methods on average across all metrics
- **Medium**: Theoretical GP equivalence mechanism - well-grounded but assumes stationarity
- **Medium**: RRR mechanism - supported by experimental results but requires rank structure assumption
- **Low**: Optimization stability claims - comparative evidence against deep learning training instability is limited

## Next Checks
1. Apply the method to a dataset with known non-stationary regime changes to assess GP assumption validity
2. Compute empirical cross-channel correlation structure on multivariate datasets to verify low-rank assumptions
3. Systematically vary scaling and differencing parameters to quantify impact on detection performance across dataset types