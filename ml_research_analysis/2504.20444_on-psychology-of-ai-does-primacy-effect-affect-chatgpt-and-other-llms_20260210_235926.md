---
ver: rpa2
title: On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?
arxiv_id: '2504.20444'
source_url: https://arxiv.org/abs/2504.20444
tags:
- experiment
- candidate
- adjectives
- llms
- candidates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether commercial LLMs exhibit the primacy
  effect - a psychological bias where initial information carries more weight in decision-making.
  The researchers adapted Asch's (1946) classic experiment, presenting LLMs with candidate
  descriptions containing the same characteristics in different orders.
---

# On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?

## Quick Facts
- arXiv ID: 2504.20444
- Source URL: https://arxiv.org/abs/2504.20444
- Authors: Mika Hämäläinen
- Reference count: 5
- Primary result: Commercial LLMs exhibit inconsistent positional biases in candidate evaluation, with ChatGPT showing primacy effect in paired comparisons but recency effect in individual ratings, while Gemini and Claude show varying refusal behaviors.

## Executive Summary
This study investigates whether commercial large language models (LLMs) exhibit the primacy effect - a psychological bias where initial information carries more weight in decision-making. Using Asch's (1946) classic experiment adapted for AI, researchers presented three leading LLMs (ChatGPT, Gemini, Claude) with candidate descriptions containing identical characteristics in different orders. The results revealed inconsistent behavior across models and task framings, with some showing primacy effects, others showing recency effects, and one refusing to respond entirely. These findings raise significant concerns about LLM reliability in high-stakes domains like hiring, particularly when end-users lack AI expertise.

## Method Summary
The researchers conducted two experiments using 200 candidate description pairs, each containing 6 adjectives (3 positive, 3 negative). Experiment 1 involved simultaneous comparison with binary choices (Candidate A or B), while Experiment 2 required individual rating on a 1-5 scale for each candidate separately. Three commercial LLMs (GPT-4o, Gemini 1.5 Flash, Claude 3.5 Sonnet) were tested using standardized prompt templates. The key manipulation was the order of positive versus negative adjectives, allowing researchers to test whether models preferred candidates with positive traits listed first (primacy effect) or negative traits listed first (recency effect).

## Key Results
- ChatGPT showed primacy effect in Experiment 1 (65.5% preferred positive-first candidates) but recency effect in Experiment 2 (23% vs 9.5% preferred negative-first)
- Gemini showed no clear preference in Experiment 1 but strong recency effect in Experiment 2 (59% preferred negative-first)
- Claude refused to respond in 100% of Experiment 1 cases but exhibited recency effect in Experiment 2 when it did respond
- All models showed preference for negative-first candidates in Experiment 2 when not scoring equally

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Positional biases in LLMs emerge from next-token prediction training, where proximity to the output position influences token weighting.
- **Mechanism:** The paper observes that in Experiment 2, all three models preferred candidates with negative adjectives first (followed by positive ones), suggesting a recency effect where later information (positive traits) carries more weight. The authors hypothesize this relates to next-token prediction training: "predicting a next token... might give more emphasis to nearby tokens."
- **Core assumption:** The attention mechanism should theoretically allow equal access to all tokens, but in practice, when all adjectives are "equally important," models default to positional proximity.
- **Evidence anchors:**
  - [section 5]: "all models showed preference for candidates that had their negative characteristics listed first when they did not score the candidates similarly"
  - [section 5]: "the models are more likely to resort to their order of appearance and proximity to the end when predicting the continuation"
  - [corpus]: Related work (arXiv:2508.07479) confirms "positional biases... where models perform better when information appears at the beginning (primacy bias) or end (recency bias)"
- **Break condition:** If attention mechanisms are explicitly modified to normalize positional importance, or if chain-of-thought reasoning is forced before decision-making, this effect may diminish.

### Mechanism 2
- **Claim:** Model-specific safeguards can override baseline positional biases, creating inconsistent cross-model behavior.
- **Mechanism:** Claude refused 100% of responses in Experiment 1 with explicit reasoning: "Since both candidates have exactly the same characteristics (just listed in a different order), I cannot make a meaningful distinction." This suggests post-training alignment or safety filters detect and block certain biased comparison tasks.
- **Core assumption:** Refusal behavior indicates architectural or training-level interventions rather than emergent reasoning.
- **Evidence anchors:**
  - [section 4]: "Claude refused every time with answers such as: Since both candidates have exactly the same characteristics..."
  - [section 4]: "It seems like Claude was trained not to answer to this very task or that it does some additional prompt processing in the background"
  - [corpus]: Weak/missing—corpus neighbors do not directly address refusal mechanisms in LLMs
- **Break condition:** When candidates are evaluated separately (Experiment 2), Claude's safeguard doesn't trigger, and it exhibits the same recency bias as other models (17.5% preference for negative-first vs 5% positive-first).

### Mechanism 3
- **Claim:** Task framing (simultaneous vs. separate evaluation) fundamentally alters which positional bias dominates.
- **Mechanism:** In Experiment 1 (simultaneous comparison), ChatGPT showed primacy effect (65.5% preferred positive-first). In Experiment 2 (separate rating), it showed recency effect (23% vs 9.5% preferred negative-first, meaning positive-last was weighted more). The evaluation context shifts which tokens anchor judgment.
- **Core assumption:** Assumption: Relative comparison prompts engage different attention patterns than absolute scoring prompts.
- **Evidence anchors:**
  - [abstract]: "In the first experiment, ChatGPT preferred the candidate with positive adjectives listed first"
  - [abstract]: "In the second experiment... both showed a clear preference to a candidate that had negative adjectives listed first"
  - [corpus]: Related work (arXiv:2512.02665) finds "Input Order Shapes LLM Semantic Alignment" in multi-document tasks
- **Break condition:** If prompt engineering explicitly instructs models to weight all characteristics equally regardless of order, or forces deliberation before response.

## Foundational Learning

- **Concept: Primacy vs. Recency Effects**
  - **Why needed here:** The paper tests whether LLMs exhibit human-like primacy effects (first impressions dominate) or recency effects (most recent information dominates). Understanding this distinction is essential for interpreting the conflicting results across experiments.
  - **Quick check question:** If an LLM rates a candidate described as "unreliable, dishonest, lazy, hardworking, honest, reliable" higher than one with traits in reverse order, which bias is it exhibiting?

- **Concept: Attention Mechanism Positional Encoding**
  - **Why needed here:** The paper speculates that attention mechanisms and positional encodings may contribute to recency effects when all tokens have equal semantic weight. Understanding how transformers assign position-based importance helps explain why models might favor adjacent tokens.
  - **Quick check question:** Why might a transformer's attention mechanism still exhibit positional bias even though attention can theoretically attend to any token in the sequence?

- **Concept: Evaluation Protocol Sensitivity**
  - **Why needed here:** The dramatic shift in results between Experiment 1 (paired comparison) and Experiment 2 (individual rating) demonstrates that LLM behavior is highly sensitive to task framing. This has direct implications for how we benchmark and deploy these systems.
  - **Quick check question:** What experiment design choice caused Claude to shift from 100% refusal to providing ratings in 100% of cases?

## Architecture Onboarding

- **Component map:** Input layer: Prompt template + candidate description (6 adjectives in specific order) -> Processing: Model-specific attention + positional encoding + safety/classification layers -> Output: Binary choice (Exp 1) or 1-5 rating (Exp 2) -> Variance sources: Model architecture (GPT-4o, Gemini 1.5 Flash, Claude 3.5 Sonnet), task framing, safety alignment

- **Critical path:**
  1. Adjective order randomization -> Positional encoding assignment
  2. Attention mechanism processes token sequence with position-aware weights
  3. Safety/filtering layer (Claude-specific) may intercept comparison tasks
  4. Output generation constrained by prompt instructions ("answer only candidate A or B" / "answer only with a number")

- **Design tradeoffs:**
  - Simultaneous comparison (Exp 1) vs. separate evaluation (Exp 2): Simultaneous enables direct comparison but triggers safeguards; separate avoids safeguards but loses relative context
  - Constrained output format vs. open response: Constraining output ("answer only X") reduces chain-of-thought but increases positional bias visibility
  - Model selection: Claude has stronger safeguards but still exhibits bias when safeguards bypassed; ChatGPT/Gemini have different bias profiles

- **Failure signatures:**
  - Claude: 100% refusal on simultaneous comparison tasks it detects as potentially biased
  - ChatGPT: Primacy effect in paired comparison, recency effect in individual rating
  - Gemini: No clear preference in paired comparison, strong recency effect (59%) in individual rating
  - All models: Inconsistent behavior across task framings undermines reliability for high-stakes decisions

- **First 3 experiments:**
  1. **Reproduce Experiment 2 with shuffled positions:** Randomize which positions (1-6) contain positive vs. negative adjectives to disentangle primacy/recency from specific position indices
  2. **Add chain-of-thought before constrained output:** Insert "Briefly reason, then answer only with [X]" to test if deliberation reduces positional bias
  3. **Test with varied adjective counts:** Use 4, 8, and 10 adjectives to determine if position effects strengthen or weaken with longer sequences

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can prompt modifications or explicit bias-mitigation instructions consistently reduce or eliminate order-based biases in LLM evaluations?
- **Basis in paper:** [explicit] "LLMs are very sensitive for prompting and it is possible that with modifications in the prompt, the results might look different."
- **Why unresolved:** Only one prompt template was tested per experiment; no systematic prompt variations were explored.
- **What evidence would resolve it:** Experiments systematically varying prompt phrasing and explicit neutrality instructions across models.

### Open Question 2
- **Question:** Why do LLMs exhibit primacy effects in simultaneous comparisons but recency effects in individual evaluations?
- **Basis in paper:** [inferred] Experiment 1 showed ChatGPT preferring positive-first candidates, while Experiment 2 showed all models preferring negative-first candidates when not scoring equally.
- **Why unresolved:** Authors speculate about next-token prediction but do not test this mechanism empirically.
- **What evidence would resolve it:** Attention weight analysis or experiments varying description length and token positions.

### Open Question 3
- **Question:** What triggers Claude's refusal behavior in comparative evaluation tasks, and can safeguards be made consistent across task reformulations?
- **Basis in paper:** [explicit] "It seems like Claude was trained not to answer to this very task or that it does some additional prompt processing in the background."
- **Why unresolved:** Safeguard mechanisms are proprietary and undocumented.
- **What evidence would resolve it:** Systematic testing of refusal patterns or transparency from model developers.

### Open Question 4
- **Question:** Does chain-of-thought reasoning before decision-making reduce order-based biases?
- **Basis in paper:** [explicit] Authors deliberately avoided "triggering a chain-of-thought reasoning" to capture implicit attitudes.
- **Why unresolved:** No chain-of-thought conditions were tested.
- **What evidence would resolve it:** Replication with "think step-by-step" prompts before final judgments.

## Limitations
- Model-specific nature of observed biases creates inconsistent cross-model behavior that may not generalize
- Controlled experimental conditions may not reflect real-world hiring scenarios with multiple interactions
- Safety alignment layers may mask or override fundamental architectural biases
- Fixed adjective sets don't capture complexity of actual candidate evaluations

## Confidence

**Major Uncertainties and Limitations:**
The primary limitation is the model-specific nature of observed biases - different LLMs exhibit opposite directional biases (primacy vs recency) depending on task framing. The study's controlled experimental conditions may not generalize to real-world hiring scenarios where candidates are evaluated across multiple interactions and contexts. The refusal behavior from Claude raises questions about whether observed biases reflect fundamental model architecture or safety alignment layers that could be modified. Additionally, the use of fixed adjective sets and controlled descriptions doesn't capture the complexity and nuance of actual candidate evaluations.

**Confidence Assessment:**
- **High confidence:** LLMs exhibit positional biases in candidate evaluation tasks; task framing (simultaneous vs. separate) significantly affects bias direction
- **Medium confidence:** The recency effect observed in Experiment 2 relates to next-token prediction training and positional encoding mechanisms
- **Low confidence:** The mechanism explaining why ChatGPT showed primacy effect in Experiment 1 but recency effect in Experiment 2; whether safety alignment or fundamental architecture drives Claude's refusal behavior

## Next Checks
1. Test with additional adjective positions (4, 8, 10 adjectives) to determine if positional effects strengthen or weaken with sequence length
2. Implement chain-of-thought reasoning before constrained output to assess whether deliberation reduces positional bias
3. Randomize specific position indices (not just polarity order) to disentangle primacy/recency from position-specific attention patterns