---
ver: rpa2
title: Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning
arxiv_id: '2508.09275'
source_url: https://arxiv.org/abs/2508.09275
tags:
- attack
- agents
- attacks
- hadamard
- align
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies black-box test-time adversarial attacks on\
  \ deployed cooperative multi-agent reinforcement learning (c-MARL) systems, where\
  \ the adversary can only perturb agents' observations without accessing policy weights\
  \ or actions. The core insight is that misalignment of agents' perceptions undermines\
  \ coordination\u2014by making agents perceive the environment inconsistently, the\
  \ team's collaborative performance degrades."
---

# Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2508.09275
- Source URL: https://arxiv.org/abs/2508.09275
- Authors: Amine Andam; Jamal Bentahar; Mustapha Hedabou
- Reference count: 40
- One-line primary result: Black-box observation perturbations degrade cooperative multi-agent RL performance by 20-30% IQM return with minimal sample access

## Executive Summary
This paper introduces black-box test-time adversarial attacks on cooperative multi-agent reinforcement learning (c-MARL) systems, where the adversary can only perturb agents' observations without accessing policy weights or actions. The key insight is that misalignment of agents' perceptions undermines coordination—by making agents perceive the environment inconsistently, the team's collaborative performance degrades. Two attack strategies are proposed: Align attack, which learns perturbations to maximize a trained observation-alignment network's loss, and Hadamard attack, which uses structured perturbations from partial Hadamard matrices for scenarios with no access. Targeted variants further select vulnerable agents using the alignment network.

Evaluated across 22 tasks from three benchmarks (LBF, RW ARE, SMAC), attacks achieve significant performance drops—e.g., 20-30% IQM return reduction—with as few as 1,000 samples versus millions for prior methods. Results demonstrate effectiveness in both fully and partially observable, highly cooperative settings, even with large agent counts and high-dimensional observations.

## Method Summary
The paper proposes two black-box adversarial attack strategies for c-MARL systems that operate by perturbing agents' observations during deployment. The Align attack learns perturbations that maximize the loss of a pre-trained observation-alignment network, while the Hadamard attack uses structured perturbations from partial Hadamard matrices when no alignment network access is available. Both approaches require only black-box access to the environment and agents' observations, not policy weights or actions. The attacks are designed to work in cooperative settings where agents must coordinate based on shared observations.

## Key Results
- Achieved 20-30% IQM return reduction across 22 tasks from three benchmarks
- Required only 1,000 samples versus millions for prior methods
- Effective in both fully and partially observable environments with large agent counts
- Demonstrated success across highly cooperative settings with homogeneous rewards

## Why This Works (Mechanism)
The attacks work by disrupting the perceptual alignment between agents, which is fundamental to their ability to coordinate actions in cooperative settings. When agents receive perturbed observations, their internal representations of the shared environment become misaligned, making it difficult to synchronize actions and achieve collective goals. The observation-alignment network identifies these perceptual misalignments, and the Align attack specifically targets these vulnerabilities. Even without the alignment network, the Hadamard attack's structured perturbations introduce enough noise to degrade coordination.

## Foundational Learning
- **Observation Alignment**: Why needed: Ensures agents share consistent perceptions of the environment for coordinated behavior; Quick check: Measure cross-agent observation similarity metrics
- **Black-box Attack Constraints**: Why needed: Reflects realistic threat models where adversaries cannot access policy internals; Quick check: Verify attack works with only observation access
- **Cooperative Multi-Agent RL**: Why needed: Foundation for understanding coordination challenges and attack surfaces; Quick check: Confirm baseline performance on cooperative tasks

## Architecture Onboarding

**Component Map**
- Environment -> Agent Observations -> Attack Perturbations -> Corrupted Observations -> Agents' Policies

**Critical Path**
1. Observe clean environment state
2. Apply adversarial perturbations to observations
3. Agents process corrupted observations through their policies
4. Degraded coordination and performance due to misaligned perceptions

**Design Tradeoffs**
- Sample efficiency vs. attack effectiveness: Align attack requires training alignment network but achieves better results with fewer samples
- Black-box vs. white-box access: Current approach balances practicality with attack power by requiring only observation access
- Targeted vs. untargeted attacks: Targeted variants offer precision but require additional computation for agent selection

**Failure Signatures**
- Performance degradation correlates with observation misalignment metrics
- Attack effectiveness decreases as agents' policies become more robust to input noise
- Success varies with task observability and agent count

**3 First Experiments**
1. Compare Align attack performance vs. Hadamard attack on simple cooperative navigation tasks
2. Measure observation misalignment before and after attacks using alignment network metrics
3. Test attack effectiveness against different c-MARL algorithms (PPO, QMIX, MADDPG)

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the text.

## Limitations
- Attacks rely on pre-trained observation-alignment network, requiring substantial policy access and data
- Generalization across policies is not fully established; alignment network must be retrained per algorithm
- Results confined to fully cooperative settings; competitive or mixed-motive scenarios not addressed
- No real-world deployment validation provided

## Confidence
- **High**: Empirical attack effectiveness on evaluated benchmarks; superiority over baseline attacks in sample efficiency and performance degradation
- **Medium**: Generalization of attacks across different c-MARL algorithms and task types; sufficiency of observation-only perturbations for attack success
- **Low**: Transferability to unseen policies without retraining the alignment network; effectiveness in competitive or non-cooperative multi-agent settings

## Next Checks
1. Test attack transferability to entirely unseen c-MARL policies without retraining the observation-alignment network
2. Evaluate attack robustness under periodic policy updates or fine-tuning during deployment
3. Assess attack impact in mixed-motive or competitive multi-agent environments where agents have misaligned incentives