---
ver: rpa2
title: 'DRAGON: A Large-Scale Dataset of Realistic Images Generated by Diffusion Models'
arxiv_id: '2505.11257'
source_url: https://arxiv.org/abs/2505.11257
tags:
- images
- diffusion
- image
- dataset
- dragon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DRAGON is a large-scale dataset of 2.6M synthetic images from 25
  diffusion models, designed to support research in detecting and attributing AI-generated
  content. It includes a diverse range of recent and established models, uses an LLM-based
  prompt expansion pipeline to improve image realism, and provides five training subsets
  of varying sizes for different experimental scenarios.
---

# DRAGON: A Large-Scale Dataset of Realistic Images Generated by Diffusion Models

## Quick Facts
- arXiv ID: 2505.11257
- Source URL: https://arxiv.org/abs/2505.11257
- Authors: Giulia Bertazzini; Daniele Baracchi; Dasara Shullani; Isao Echizen; Alessandro Piva
- Reference count: 40
- Primary result: 2.6M synthetic images from 25 diffusion models with improved detection/attribution performance

## Executive Summary
DRAGON is a large-scale dataset of 2.6M synthetic images from 25 diffusion models, designed to support research in detecting and attributing AI-generated content. It includes a diverse range of recent and established models, uses an LLM-based prompt expansion pipeline to improve image realism, and provides five training subsets of varying sizes for different experimental scenarios. Quality analysis shows substantial improvements in image realism (MPS scores increase from ~4.5 to ~14.0). Detection and attribution experiments demonstrate that retraining on DRAGON significantly improves performance over models trained on older datasets, with DE-FAKE achieving 0.62 accuracy in model attribution and improved robustness under JPEG compression and resizing. The dataset includes both training and benchmark test sets, enabling reliable evaluation of forensic methods.

## Method Summary
DRAGON generates 2.6M synthetic images using 25 diffusion models through an LLM-based prompt expansion pipeline. ImageNet class labels are expanded into detailed prompts using Phi-3 with few-shot learning from 13 curated examples. Images are generated at varying resolutions (512-1024px) using HuggingFace diffusers with default parameters. The dataset includes five training subsets (XS-5K to XL-2.5M) and a 100K test set. Quality is validated using MPS, HPS, and ImageReward metrics. Forensic detection and attribution experiments retrain DE-FAKE and UnivFD on DRAGON data, then evaluate performance under JPEG compression and resizing degradations.

## Key Results
- MPS quality scores improve from ~4.5 to ~14.0 across all models with prompt expansion
- DE-FAKE achieves 0.62 average accuracy in model attribution after DRAGON retraining
- Retrained models show improved robustness: 0.986 detection accuracy vs 0.868 baseline, maintaining performance under JPEG QF 10-90 and resizing 128-256px

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based prompt expansion improves perceptual quality of generated images
- Mechanism: An LLM (Phi-3) receives short ImageNet class labels and expands them into detailed prompts containing composition, lighting, camera specifications, and texture descriptions. These richer prompts guide diffusion models to produce more coherent, detailed outputs with better semantic alignment.
- Core assumption: Diffusion models respond to prompt complexity by allocating more computational attention to relevant image regions, producing higher-fidelity results.
- Evidence anchors: [abstract] improvements in standard quality metrics from prompt expansion; [section 2.2] Few-shot in-context learning with 13 manually curated prompts; [table 3] MPS scores improved from ~4.5 to ~14.0 across all 25 models.

### Mechanism 2
- Claim: Different diffusion architectures leave distinct frequency-domain fingerprints enabling model attribution
- Mechanism: Each diffusion model's denoising process introduces consistent artifacts in the frequency spectrum. When averaged over noise residuals (following Corvi et al.), these appear as unique spectral patterns visible in Fourier amplitude plots—even between base models and their distilled/fine-tuned variants.
- Core assumption: The inversion process used to extract noise residuals preserves model-specific traces that survive averaging across diverse image content.
- Evidence anchors: [section 3.2.1] Figure 4 shows distinct spectra for each model; notable differences between base models and distilled variants; [section 3.2.3] DE-FAKE achieves 0.62 average attribution accuracy.

### Mechanism 3
- Claim: Training detection models on contemporary, diverse datasets improves robustness under real-world degradations
- Mechanism: Detectors trained on outdated model outputs learn artifacts that may not transfer to newer architectures. DRAGON's coverage of 25 models (including 2024-2025 releases) exposes detectors to current generation traces, improving generalization and robustness to JPEG compression and resizing.
- Core assumption: Forensic traces from diffusion models share some cross-architectural patterns that detectors can learn, while older datasets (GAN-era, early diffusion) lack these patterns.
- Evidence anchors: [table 5] DE-FAKE retrained on DRAGON improved from 0.868 to 0.986 baseline accuracy; [table 5] Retrained models maintained performance under JPEG compression and resizing.

## Foundational Learning

- **Diffusion model architectures (U-Net latent diffusion vs. Diffusion Transformers vs. pixel-space)**: DRAGON includes all three families; understanding their differences explains why spectral fingerprints vary and why attribution accuracy differs across models. Quick check: Can you explain why Stable Diffusion (latent U-Net), Flux.1 (DiT), and DeepFloyd IF (pixel-space) might leave different frequency artifacts?

- **Frequency-domain analysis and noise residuals**: Section 3.2.1's Fourier analysis underpins the attribution hypothesis; understanding noise residual extraction is critical for reproducing forensic traces. Quick check: How would you compute a noise residual from an image, and why would averaging Fourier transforms of 1000 residuals reveal model-specific patterns?

- **Multi-dimensional quality metrics (MPS vs. single-score)**: MPS scores validate the prompt expansion mechanism; understanding why MPS outperforms HPS/ImageReward helps assess whether quality improvements are genuine or metric artifacts. Quick check: Why might MPS's four-dimensional scoring (aesthetics, semantic alignment, detail quality, overall impression) be more reliable than single-score metrics for validating prompt expansion effectiveness?

## Architecture Onboarding

- **Component map**: ImageNet labels → Phi-3 LLM (few-shot with 13 curated examples) → expanded prompts → 25 diffusion models via HuggingFace diffusers → quality validation (MPS/HPS/ImageReward) → forensic baseline (DE-FAKE, DIRE, CLIPDet, UnivFD) → retraining on DRAGON subsets → robustness evaluation

- **Critical path**: 1) Reproduce prompt expansion: Load Phi-3, implement few-shot prompting with the 13 seed examples; 2) Generate validation set: Sample 100 ImageNet classes, generate with/without expansion, compute MPS delta; 3) Frequency analysis: Implement noise residual extraction, compute averaged FFT for each model; 4) Detection baseline: Run pre-trained detectors on DRAGON-R, verify Table 4 metrics; 5) Retraining: Fine-tune DE-FAKE on DRAGON-XL, evaluate robustness per Table 5

- **Design tradeoffs**: Subset size vs. few-shot capability: XS (250 images) enables few-shot experiments but may undertrain large models; XL (2.5M) supports full training but requires significant compute; Prompt expansion vs. semantic fidelity: LLM expansion improves quality but risks hallucination (Section 4); Model diversity vs. architectural bias: 8/25 models are SDXL variants; attribution may overfit to this family; PNG vs. JPEG: DRAGON uses PNG for fakes, ImageNet uses JPEG; Table 4 shows this introduces bias—always evaluate with matching compression

- **Failure signatures**: DIRE fails under JPEG compression (0.745 at QF 50): Indicates detector learned compression artifacts, not generation traces; UnivFD near-chance on resizing (0.511 at 128px): Fixed CLIP features may lack spatial resolution for small images; Stable Diffusion XL attribution at 0.16: Base model confused with derivatives—expect similar confusion for other popular base models with many fine-tunes; Semantic drift: If LLM generates "floral tea" prompts for "weasel" class, quality improves but forensic training may learn wrong associations

- **First 3 experiments**: 1) Compression robustness stress test: Take retrained DE-FAKE/UnivFD, evaluate across full JPEG quality range (100→10) and resize scales (512→64). Plot accuracy degradation curves; 2) Model attribution confusion analysis: For the 8 SDXL-variant models, compute pairwise confusion matrix. Test hypothesis: distillation vs. fine-tuning produce distinguishable fingerprints; 3) Few-shot learning with DRAGON-XS: Train DE-FAKE on just 10 images per model (250 total), evaluate on full 100K test set. Compare to XL-trained model to quantify marginal value of scale vs. diversity

## Open Questions the Paper Calls Out

- **Generalization to proprietary models**: The dataset only contains open-weight diffusion models and doesn't include proprietary models like DALL-E or Midjourney. Researchers should be aware of this potential bias when using DRAGON for forensic method development.

- **Improving attribution between base models and derivatives**: Stable Diffusion XL showed the lowest attribution accuracy (0.16) due to frequent misclassification as its distilled variants or fine-tuned derivatives. The paper notes potential for further improving attribution methods to better capture subtle, model-specific cues.

- **DIRE's dependency on compression artifacts**: DIRE experiences significant accuracy drops on JPEG-compressed synthetic images, suggesting its performance may partially depend on compression artifacts rather than genuine generation traces. The paper couldn't retrain DIRE due to computational constraints.

## Limitations

- **PNG vs. JPEG format bias**: Real ImageNet images are JPEG while generated images are PNG, creating compression-related detection biases that must be corrected for fair evaluation.

- **Semantic drift risk**: LLM-based prompt expansion may introduce content mismatch between generated images and their intended ImageNet classes, potentially degrading forensic training quality despite quality improvements.

- **Architectural family bias**: With 8/25 models being SDXL variants, attribution methods may overfit to this family and perform poorly on truly diverse model comparisons.

## Confidence

- **High confidence**: Detection and attribution performance improvements (DE-FAKE 0.62 accuracy, 0.986 detection) are well-supported by systematic experiments across multiple baselines and degradation scenarios.

- **Medium confidence**: Quality improvement claims (MPS from 4.5 to 14.0) are metric-validated but rely on the assumption that MPS scores correlate with forensic utility.

- **Low confidence**: Attribution accuracy between similar models may be inflated by overfitting to architectural families rather than genuine fingerprint detection.

## Next Checks

1. **Semantic fidelity verification**: Sample 100 DRAGON images and verify that LLM-expanded prompts maintain semantic alignment with their ImageNet class labels to address the Section 4 risk of content mismatch.

2. **Compression bias quantification**: Train DE-FAKE on DRAGON-R with JPEG-compressed synthetic images (QF 96) and compare to PNG-based training to measure format-induced detection accuracy differences.

3. **Attribution confusion matrix analysis**: For the 8 SDXL-variant models, compute pairwise attribution confusion rates to test whether distillation vs. fine-tuning produce distinguishable fingerprints beyond shared base model characteristics.