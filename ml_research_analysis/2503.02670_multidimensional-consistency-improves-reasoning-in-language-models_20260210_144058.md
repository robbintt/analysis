---
ver: rpa2
title: Multidimensional Consistency Improves Reasoning in Language Models
arxiv_id: '2503.02670'
source_url: https://arxiv.org/abs/2503.02670
tags:
- reasoning
- consistency
- balls
- language
- tennis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces Multidimensional Reasoning Consistency (MRC),
  a framework that systematically tests language models'' answer consistency across
  three input variation dimensions: shot order in prompts, problem phrasing, and language.
  The core idea is that answer consistency across these variations serves as stronger
  evidence for correctness, and leveraging this consistency improves mathematical
  reasoning performance.'
---

# Multidimensional Consistency Improves Reasoning in Language Models

## Quick Facts
- **arXiv ID:** 2503.02670
- **Source URL:** https://arxiv.org/abs/2503.02670
- **Reference count:** 24
- **Key outcome:** MRC improves mathematical reasoning accuracy by 2-4% absolute points via answer consistency across shot order, phrasing, and language variations

## Executive Summary
This paper introduces Multidimensional Reasoning Consistency (MRC), a framework that systematically tests language models' answer consistency across three input variation dimensions: shot order in prompts, problem phrasing, and language. The core idea is that answer consistency across these variations serves as stronger evidence for correctness, and leveraging this consistency improves mathematical reasoning performance. Experiments on GSM8K and MGSM datasets with models ranging from 7B to 72B parameters show that answer consistency varies by dimension (highest for shot order, lowest for language), but aggregating consistency across dimensions consistently enhances performance, especially for smaller models.

## Method Summary
MRC operates by generating solution paths through systematic input variations: 8 permutations of shot exemplar order (COC), 4 paraphrase variants (CPC), and 8 language translations (CLC). Each path undergoes chain-of-thought prompting to generate reasoning traces, from which final answers are extracted. Majority voting aggregates across all 19 paths to select the final answer. The framework tests whether answer consistency across semantically equivalent but surface-different inputs correlates with correctness and can be used to improve reasoning accuracy.

## Key Results
- MRC improves accuracy by 2-4% absolute points compared to vanilla chain-of-thought prompting
- Smaller models (7B-22B) show the largest gains, with Qwen2.5-Math-7B improving from 94.4% to 96.0%
- CPC (cross-phrasing) performs best among individual dimensions, while COC (cross-order) shows highest consistency
- CLC (cross-lingual) shows weak correlation with accuracy (r=0.49) but still contributes to overall performance when aggregated

## Why This Works (Mechanism)

### Mechanism 1: Solution Path Diversification Through Controlled Perturbations
Generating diverse reasoning paths via systematic input variations improves final answer accuracy when aggregated through majority voting. Each variation dimension induces different reasoning trajectories; correct answers converge while errors disperse, making the consistent answer identifiable via majority voting.

### Mechanism 2: Cross-Dimensional Consistency as Confidence Signal
Answer agreement across semantically equivalent but surface-different inputs signals stronger epistemic confidence in the answer. LLMs exhibit prompt sensitivity—their internal representations and attention patterns shift with surface form; when answers persist despite these shifts, the underlying reasoning is less dependent on spurious features.

### Mechanism 3: Error Pattern Decorrelation Across Variation Dimensions
Different input variation dimensions trigger partially non-overlapping error modes, so aggregating across dimensions cancels dimension-specific systematic errors. Shot-order sensitivity, paraphrase sensitivity, and cross-lingual failure modes arise from different model vulnerabilities—combining them averages away dimension-specific biases.

## Foundational Learning

- **Concept: Self-Consistency / Majority Voting**
  - Why needed here: MRC extends self-consistency from stochastic sampling to controlled variation dimensions
  - Quick check question: Given 8 solution paths with answers {11, 11, 11, 9, 11, 11, 9, 11}, what final answer does majority voting select?

- **Concept: Prompt Sensitivity in LLMs**
  - Why needed here: The entire framework rests on the observation that LLM outputs vary with superficial input changes
  - Quick check question: Why might rearranging the order of few-shot examples change a model's final answer to the same question?

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed here: MRC operates on CoT reasoning traces; extracting intermediate steps enables answer extraction and path analysis
  - Quick check question: What is the key difference between standard prompting and chain-of-thought prompting for math problems?

## Architecture Onboarding

- **Component map:**
  Input Question → [Variation Generators] → [LLM with CoT Prompting] → [Answer Extractor] → [Majority Voting Aggregator] → Final Answer

- **Critical path:**
  1. Generate 8 shot-order permutations (COC) → 8 solutions
  2. Generate 4 paraphrase variants (CPC) → 4 solutions
  3. Translate to 8 languages (CLC, excluding BN/SW/TE) → 8 solutions
  4. Extract final answer from each CoT trace (regex for "The answer is X" pattern)
  5. Apply majority voting: â = argmax_a Σ I(a = a')

- **Design tradeoffs:**
  - Path count vs. compute: Paper uses 19 paths; more paths improve accuracy but cost scales linearly
  - Dimension selection: All three dimensions improve over vanilla CoT, but CPC performs best individually
  - Language selection: CLC excludes BN, SW, TE due to tokenization issues and very low performance
  - Paraphrase method: Paper's "rewrite-then-solve" outperforms "rewrite-without-solve"

- **Failure signatures:**
  - Low consistency across all dimensions (RC < 0.6): Model lacks capability for this problem type
  - High consistency on wrong answer: Systematic reasoning error reinforced across variations
  - CLC underperforms vanilla English: Model has weak multilingual capability
  - RwS accuracy drops vs. original: Paraphrase loses critical information

- **First 3 experiments:**
  1. Implement COC alone on GSM8K with 4-shot prompting and 8 permutations; verify 1-2% improvement over vanilla CoT
  2. Test all 7 combinations of {COC, CPC, CLC} to find optimal dimension subset for your model
  3. Replicate Figure 5 for your model—plot accuracy vs. number of paths (8, 16, 24, 32)

## Open Questions the Paper Calls Out

### Open Question 1
Can the diversity of reasoning paths be quantified to verify the hypothesis that larger diversity correlates with stronger consistency-based performance gains? The authors identify studying path diversity in a quantifiable way as a natural future direction, as the current framework lacks a metric to measure the semantic or structural variance of intermediate reasoning steps.

### Open Question 2
How can the different variation dimensions be integrated strategically to maximize reasoning accuracy without causing a combinatorial explosion? The authors suggest exploring nested or hierarchical variations that might yield better results with fewer inference calls, as the paper currently aggregates dimensions by summing paths without exploring optimal combinations.

### Open Question 3
Does the MRC framework improve performance on complex reasoning tasks outside of mathematics? The authors conclude their results "pave the way for using a similar framework for other (reasoning) tasks, too," but the study is restricted to math datasets, leaving unverified whether consistency across phrasing or language provides the same signal for tasks with less deterministic answers.

## Limitations

- Dimension performance rankings (CPC > COC > CLC) may vary by model family and task domain
- The weak correlation for CLC (r=0.49) suggests the consistency-accuracy relationship isn't universal across dimensions
- Model-specific variation is suggested by the exclusion of certain languages (BN, SW, TE) due to poor performance

## Confidence

- **High confidence**: Claims about overall accuracy improvements from MRC (2-4% absolute gains)
- **Medium confidence**: Claims about specific dimension performance rankings and consistency as confidence signal
- **Medium confidence**: Claims about consistency-accuracy relationship, particularly for CLC dimension

## Next Checks

1. **Ablation study for optimal dimension subsets**: Systematically test all 7 combinations of {COC, CPC, CLC} on your target model to identify whether the full 19-path aggregation is necessary or if a subset provides better compute-accuracy tradeoffs.

2. **Cross-domain consistency analysis**: Apply MRC to non-mathematical reasoning tasks (e.g., commonsense reasoning, code generation) to verify whether the consistency-accuracy relationship generalizes beyond mathematical problems.

3. **Model-specific consistency profiling**: For your target model architecture, measure per-dimension consistency and accuracy separately to determine if observed patterns replicate, informing whether to include or exclude specific dimensions.