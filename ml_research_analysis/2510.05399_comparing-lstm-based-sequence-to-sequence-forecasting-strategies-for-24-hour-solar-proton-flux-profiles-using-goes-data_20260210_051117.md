---
ver: rpa2
title: Comparing LSTM-Based Sequence-to-Sequence Forecasting Strategies for 24-Hour
  Solar Proton Flux Profiles Using GOES Data
arxiv_id: '2510.05399'
source_url: https://arxiv.org/abs/2510.05399
tags:
- data
- proton
- flux
- forecasting
- solar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates LSTM-based sequence-to-sequence (seq2seq)
  models for forecasting 24-hour solar proton flux profiles following Solar Proton
  Events (SPEs). The authors systematically compare model configurations across six
  forecasting strategies: using proton-only vs.'
---

# Comparing LSTM-Based Sequence-to-Sequence Forecasting Strategies for 24-Hour Solar Proton Flux Profiles Using GOES Data

## Quick Facts
- arXiv ID: 2510.05399
- Source URL: https://arxiv.org/abs/2510.05399
- Reference count: 22
- Primary result: One-shot LSTM seq2seq models outperform autoregressive approaches for 24-hour solar proton flux forecasting, with optimal performance at 512 hidden units and 8-dimensional embeddings.

## Executive Summary
This paper evaluates LSTM-based sequence-to-sequence models for forecasting 24-hour solar proton flux profiles following Solar Proton Events (SPEs). The authors systematically compare model configurations across six forecasting strategies: using proton-only vs. combined proton+X-ray inputs, original vs. trend-smoothed data, and autoregressive vs. one-shot prediction modes. Their analysis of 40 well-connected SPE events (1997-2017) reveals that one-shot forecasting consistently outperforms autoregressive approaches, avoiding error accumulation. While proton-only models perform better on original data, trend-smoothing significantly improves multi-input models by mitigating X-ray fluctuations. Notably, the best-performing model was trained on original data, suggesting architectural choices can outweigh data preprocessing benefits. The study provides practical insights for operational space weather forecasting systems with real-time constraints.

## Method Summary
The authors use LSTM-based sequence-to-sequence models with attention to forecast 24-hour solar proton flux profiles. The model takes 288 timesteps (24 hours at 5-minute resolution) of input and generates 288 timesteps of output. Six forecasting strategies are evaluated: proton-only vs. proton+X-ray inputs, original vs. trend-smoothed data, and autoregressive vs. one-shot prediction modes. The dataset consists of 40 well-connected SPE events from 1997-2017. Model configurations vary hidden units (512-1024) and embedding dimensions (1-20). Training uses 4-fold stratified cross-validation with mean squared error loss on log-transformed flux values.

## Key Results
- One-shot forecasting consistently yields lower error than autoregressive prediction, avoiding error accumulation seen in iterative approaches
- Trend-smoothing significantly enhances the performance of proton+X-ray models by mitigating fluctuations in the X-ray channel
- The best-performing model configuration uses 512 hidden units and 8-dimensional embedding trained on original data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: One-shot forecasting produces lower prediction error than autoregressive approaches for 24-hour solar proton flux prediction.
- Mechanism: In autoregressive mode, each predicted timestep is fed back as input for the next step. Small prediction errors compound across 288 sequential steps, causing the decoder to drift from the true trajectory. One-shot forecasting generates the entire 24-hour sequence in a single forward pass, allowing the decoder to leverage the encoder's global context without iterative error propagation.
- Core assumption: The embedding vector sufficiently compresses the input history to enable accurate full-horizon prediction without stepwise refinement.
- Evidence anchors:
  - [abstract]: "one-shot forecasting consistently yields lower error than autoregressive prediction, avoiding the error accumulation seen in iterative approaches"
  - [section V]: "using a 512-8 model, the RMSE drops from approximately 0.351 in autoregressive to 0.303 in one-shot"
  - [corpus]: Limited corpus support; neighbor papers address solar irradiance forecasting rather than SPE-specific error accumulation patterns
- Break condition: May not generalize to forecast horizons beyond 24 hours where compression becomes lossy; also may fail when predicting events with unprecedented rise/decay patterns outside the training distribution.

### Mechanism 2
- Claim: Trend-smoothing enables multi-input (proton+X-ray) models to perform comparably to proton-only models.
- Mechanism: Raw X-ray flux contains high-frequency fluctuations from secondary solar phenomena unrelated to proton acceleration. A 1-hour sliding average (±30 minutes) filters this noise while preserving the flare's overall energy release trend. This improves compatibility between proton and X-ray channels during encoder fusion.
- Core assumption: The relevant X-ray signal for proton flux prediction operates at hourly timescales; sub-hourly fluctuations are noise.
- Evidence anchors:
  - [abstract]: "trend-smoothing significantly enhances the performance of proton+X-ray models by mitigating fluctuations in the X-ray channel"
  - [Table II]: P+XR orig OS achieves RMSE 0.315 (1024-1) vs P+XR trend OS achieving RMSE 0.305 (1024-1)
  - [corpus]: No direct corpus evidence on X-ray smoothing for SPE prediction; neighbor papers focus on single-input irradiance forecasting
- Break condition: May filter out genuine predictive signals when impulsive flare events contain sub-hourly proton-relevant signatures; the best overall model used original data without smoothing.

### Mechanism 3
- Claim: Moderate model capacity (512 hidden units, 8-dimensional embedding) can match or exceed larger configurations on small datasets through implicit regularization.
- Mechanism: With only 40 events (30 training per fold), larger models (1024 units, 20-dim embeddings) have excessive capacity relative to data. Moderate compression forces the encoder to learn only the most generalizable temporal patterns, reducing overfitting despite minimal explicit regularization.
- Core assumption: Essential SPE profile dynamics can be captured in an 8-dimensional latent representation.
- Evidence anchors:
  - [Table II]: Best overall performer is P orig OS 512-8 (RMSE 0.303, 11.03% error)
  - [section III]: "Given the limited number of events (40), we employ 4-fold cross-validation to maximize training data while obtaining robust performance estimates"
  - [corpus]: No corpus evidence specifically addressing embedding dimension optimization for space weather forecasting
- Break condition: May underfit when the training set expands significantly; larger models could become superior with 100+ events.

## Foundational Learning

- Concept: **Sequence-to-sequence with attention**
  - Why needed here: The encoder must compress 288 timesteps of input into a representation that the decoder expands into 288 output steps. Attention allows the decoder to dynamically weight relevant encoder states rather than relying solely on the fixed final state.
  - Quick check question: Why does attention help more during the decay phase prediction than the rise phase?

- Concept: **Log transformation for power-law distributed data**
  - Why needed here: Proton flux spans 4+ orders of magnitude (10 to 10^5 pfu). Training on raw values causes the loss to be dominated by peak flux events, degrading performance on smaller events.
  - Quick check question: If you trained on raw flux values, why would your model systematically overpredict the decay phase of S1 events?

- Concept: **Stratified k-fold cross-validation**
  - Why needed here: With severe class imbalance (20 S1, 12 S2, 6 S3, 2 S4 events), random splits could place all rare events in one fold, making performance estimates unreliable. Stratification ensures each fold contains proportional class representation.
  - Quick check question: What happens to your confidence intervals if the two S4 events fall in the same test fold across all runs?

## Architecture Onboarding

- Component map:
  - **Encoder**: 2 stacked LSTM layers (512/768/1024 units) → processes 288-step input sequence
  - **Embedding projection**: Dense layer → compresses encoder state to 1-20 dimensions
  - **Attention module**: Computes context vector from encoder states conditioned on embedding + previous output
  - **Decoder**: 2 stacked LSTM layers → generates 288-step output sequence
  - **Output layer**: Dense projection → maps decoder hidden state to single log-flux value per timestep

- Critical path:
  1. Preprocess: log-transform flux, optionally apply 1-hour sliding average
  2. Encode: 24h input → encoder LSTM → final hidden state
  3. Embed: hidden state → dense layer → embedding vector (1-20 dim)
  4. Decode: embedding initializes decoder; attention provides context at each step
  5. Output: for one-shot, generate all 288 steps directly; for AR, feed each output back as next input

- Design tradeoffs:
  - **Hidden units vs. data scarcity**: 1024 units capture richer patterns but overfit on 30 training events; 512 units may generalize better
  - **Embedding dimension vs. information bottleneck**: 1-dim forces extreme compression (potential underfitting); 20-dim may encode noise (overfitting)
  - **Multi-input vs. preprocessing burden**: Proton-only is simpler; adding X-ray requires smoothing to avoid performance degradation

- Failure signatures:
  - **Autoregressive drift**: Later timesteps systematically underestimate flux due to accumulated errors (see P orig AR vs P orig OS in Table II)
  - **High fold variance**: RMSE standard deviation >0.025 indicates sensitivity to which events fall in test set (Table III, CV3 vs CV4)
  - **Multi-input degradation**: P+XR orig models show higher error than P orig models until smoothing is applied

- First 3 experiments:
  1. **Reproduce baseline**: Implement P orig OS with 512-8 configuration; expect RMSE 0.303 ± 0.003 on 4-fold cross-validation
  2. **Validate error accumulation**: Compare P orig AR (512-1) vs P orig OS (512-8); expect ~10-15% RMSE reduction in one-shot mode
  3. **Test multi-input smoothing**: Train P+XR trend OS (1024-1) vs P+XR orig OS (1024-1); expect ~3% RMSE improvement with smoothing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative architectures such as transformers or physics-informed neural networks improve generalization and robustness for SPE flux profile forecasting compared to LSTM-based seq2seq models?
- Basis in paper: [explicit] "We also plan to explore different deep learning approaches, such as transformers and physics-informed neural networks, to improve generalization and robustness under operational conditions."
- Why unresolved: The current study only evaluated LSTM-based architectures; transformer attention mechanisms may better capture long-range temporal dependencies in 24-hour profiles, while physics-informed approaches could incorporate known particle transport physics to constrain predictions with limited training data.
- What evidence would resolve it: Comparative evaluation of transformer and physics-informed neural network models on the same 40-event dataset using identical cross-validation splits, with analysis of performance on rare high-intensity (S3/S4) events.

### Open Question 2
- Question: Would incorporating data from additional spacecraft (e.g., STEREO A/B) and generating proxy SPE profiles effectively address the fundamental constraint of limited training samples?
- Basis in paper: [explicit] "Future work will focus on expanding the data by incorporating additional real-time observations (e.g., high energy particles), generating proxy SPE profiles, and utilizing data from other spacecraft such as STEREO A and B."
- Why unresolved: With only 40 well-connected events over 20 years, the dataset is inherently limited; it remains unclear whether data augmentation strategies or multi-spacecraft observations can provide sufficiently similar event characteristics to improve model training without introducing distributional shift.
- What evidence would resolve it: Systematic analysis of model performance when trained on augmented datasets with varying proportions of proxy/multi-spacecraft data, validated against held-out GOES events.

### Open Question 3
- Question: What architectural modifications would enable more effective utilization of X-ray flux as an auxiliary input for proton flux forecasting?
- Basis in paper: [inferred] "This implies that the added X-ray information was either redundant for the forecasting task or not effectively utilized by our current model architecture."
- Why unresolved: While trend-smoothing improved multi-input models, even optimized proton+X-ray configurations underperformed compared to proton-only models, suggesting the current seq2seq architecture may not adequately learn cross-correlations between flare X-ray signatures and subsequent proton flux evolution.
- What evidence would resolve it: Ablation studies with attention visualization showing whether decoder attention weights correlate with physically meaningful X-ray features (e.g., flare peak timing, decay rates), combined with architectural variants (cross-attention mechanisms, separate encoders) evaluated on the same benchmark.

## Limitations

- Limited dataset with only 40 well-connected SPE events constrains generalization and increases sensitivity to fold selection
- Unspecified attention mechanism implementation and training schedule details create potential reproducibility gaps
- Exclusive focus on well-connected SPEs may not represent the full operational forecasting scenario with more diverse event types

## Confidence

- **High confidence**: One-shot forecasting superiority (direct comparison across 15 configurations, consistent RMSE improvement of 10-15%)
- **Medium confidence**: Trend-smoothing benefits for multi-input models (observed but with limited theoretical grounding; best overall model used original data)
- **Medium confidence**: Optimal architecture at 512-8 configuration (based on 40 events; may not scale to larger datasets)

## Next Checks

1. Test the 512-8 one-shot model on out-of-sample SPE events from 2018-2023 to verify generalization beyond the original dataset
2. Implement ablation studies varying the attention mechanism (additive vs. dot-product) and decoder input schemes to isolate their contribution to performance
3. Conduct experiments with synthetic SPE events spanning wider flux ranges to evaluate model behavior at extreme values and potential failure modes