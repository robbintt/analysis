---
ver: rpa2
title: Voice-Interactive Surgical Agent for Multimodal Patient Data Control
arxiv_id: '2511.07392'
source_url: https://arxiv.org/abs/2511.07392
tags:
- agent
- commands
- command
- function
- zoom
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a Voice-Interactive Surgical Agent (VISA) to
  address the challenge of surgeons needing to access multimodal patient data during
  robotic surgery without disrupting workflow. VISA is a hierarchical multi-agent
  framework built on Large Language Models (LLMs), consisting of a workflow orchestrator
  agent and three task-specific agents (Information Retrieval, Image Viewer, and Anatomy
  Rendering) that autonomously interpret voice commands and execute tasks.
---

# Voice-Interactive Surgical Agent for Multimodal Patient Data Control

## Quick Facts
- **arXiv ID:** 2511.07392
- **Source URL:** https://arxiv.org/abs/2511.07392
- **Reference count:** 40
- **Primary result:** VISA achieves high stage-level accuracy and workflow-level success rates in handling voice commands for robotic surgery multimodal data control.

## Executive Summary
The paper introduces VISA, a Voice-Interactive Surgical Agent designed to help surgeons access multimodal patient data during robotic surgery without breaking their workflow. VISA employs a hierarchical multi-agent framework built on LLMs, with a workflow orchestrator and three specialized agents (Information Retrieval, Image Viewer, and Anatomy Rendering) that interpret voice commands and execute tasks autonomously. The system is evaluated on a dataset of 240 commands, demonstrating high accuracy and robust performance in handling transcription errors, linguistic ambiguity, and diverse expressions. VISA shows promise for supporting robotic surgery and integrating new functions while maintaining surgical workflow efficiency.

## Method Summary
VISA is a hierarchical multi-agent framework using Large Language Models (LLMs) to interpret free-form voice commands for controlling multimodal patient data during robotic surgery. The system employs Gemma3 27B for LLM reasoning and uses Whisper-small for wake-word detection and Whisper-medium for speech-to-text transcription. The framework consists of a workflow orchestrator agent that manages global state and routes commands, and three task-specific agents: Information Retrieval (IR), Image Viewer (IV), and Anatomy Rendering (AR). The system uses a command dataset of 240 user commands and evaluates performance using a Multi-level Orchestration Evaluation Metric (MOEM). Patient data includes surgical videos, clinical information, CT DICOM images, and 3D anatomical models from Korea University Guro Hospital.

## Key Results
- VISA demonstrates high stage-level accuracy across all workflow stages (STT, CC, CR, AF, AP, AD, OF)
- The system achieves strong workflow-level success rates with robust error recovery capability
- High accuracy in selecting appropriate agents and effective handling of complex commands, including those with linguistic ambiguity and diverse free-form expressions

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Decoupling of Planning and Execution
The system separates high-level workflow orchestration from low-level task execution, improving modularity and error isolation. A central Workflow Orchestrator Agent uses an LLM to manage global state and route commands, while specialized task agents handle domain-specific logic. This differs from prior end-to-end voice-to-action mapping approaches.

**Core assumption:** LLMs function more reliably as routers and state managers when not burdened with the specific parameter constraints of multiple distinct tools simultaneously.

**Evidence anchors:** The hierarchical multi-agent framework is explicitly described in the abstract, and the difference from direct voice-to-function mapping is highlighted in the introduction. Related work supports multi-agent workflows for complex surgical intelligence.

**Break condition:** If the Orchestrator's context window overflows or it fails to update the memory state correctly, subsequent routing decisions will be based on stale or incorrect context.

### Mechanism 2: Hybrid LLM-Rule Constraint Satisfaction
The system combines probabilistic LLM reasoning with deterministic mathematical rules to maintain workflow integrity. The LLM outputs a probability distribution for the next step, which is processed by "function selection rules" (mathematical argmax) and "missing function completion rules" (setting absent options to zero).

**Core assumption:** The LLM produces calibrated probabilities that align with the logical flow required for surgical safety and sequence.

**Evidence anchors:** The hybrid approach is described in Section II.A, and Table II shows 100% accuracy for "Orchestration Flow (OF)," suggesting the rules successfully enforce order.

**Break condition:** If the LLM assigns high confidence to an incorrect branch, the mathematical rule will dutifully select that branch, executing a valid rule on an invalid semantic choice.

### Mechanism 3: Memory-Grounded Ambiguity Resolution
Contextual memory allows the system to resolve implicit or ambiguous commands by maintaining a history of agent interactions and parameters. The system uses the last three revised commands from global memory state to resolve ambiguity, such as inheriting the active agent for implicit commands.

**Core assumption:** Surgical commands are often sequential and dependent on the immediately preceding state (sequential dependency).

**Evidence anchors:** The use of global memory state for ambiguity resolution is described in Section II.B.3, and state parameters tracking active data fields is described in Section II.C.1.

**Break condition:** If the user issues a command intended for a different agent than the one active in memory without explicitly naming it, the system may force the command onto the currently active agent.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Prompting**
  - **Why needed here:** The system uses CoT in the Command Reasoning function to analyze why a specific agent should be selected, particularly for complex or failed commands.
  - **Quick check question:** Can you trace the intermediate reasoning step the LLM uses to map "How old is the patient?" to the Information Retrieval agent?

- **Concept: Probability Distributions over Functions**
  - **Why needed here:** The Orchestrator outputs a JSON with probabilities rather than a single text token. Understanding how to parse and argmax these distributions is central to the control loop.
  - **Quick check question:** If the LLM outputs probabilities [0.45, 0.45, 0.10] for three functions, how does the "function selection rule" behave?

- **Concept: State Parameter Management (Ψ)**
  - **Why needed here:** Each task agent maintains a state Ψ (e.g., zoom level, slice position). The system must mathematically update these values rather than just generating text.
  - **Quick check question:** How does the IV agent update the slice position if the user says "move front front front"?

## Architecture Onboarding

- **Component map:** Microphone → Silero VAD → Whisper-small (Wake-word) → Whisper-medium (STT) → Correct & Validate → Command Reasoning → Action Determiner → Visual Overlay

- **Critical path:** Audio Capture → STT → Correct & Validate (Critical for error recovery) → Command Reasoning (Agent Routing) → Action Determiner (Parameter Inference) → Visual Overlay

- **Design tradeoffs:**
  - **Latency vs. Robustness:** Using Gemma3 27B for every stage ensures high accuracy but introduces cumulative latency (approx. 17GB VRAM usage), noted as a challenge in the Discussion.
  - **Generalizability vs. Rules:** Relying on "correction rules" (e.g., "city" → "CT") adds robustness to specific STT errors but requires manual maintenance compared to a purely learned model.

- **Failure signatures:**
  - **Invalid Cycles:** The workflow loops back to "Real-time Audio" repeatedly (measured by 'IC' in Table II).
  - **Composite Confusion:** The Action Determiner fails on commands like "Zoom in and rotate left" if it cannot decompose them into sequential steps.
  - **STT Lexical Drift:** Medical terms like "coronal" transcribed as "corona" may fail to correct if the rule-based corrector doesn't catch them.

- **First 3 experiments:**
  1. **Audio Robustness Test:** Feed the STT module synthesized audio with varying accents/background noise to measure the gap between "Strict" and "Single-pass" success rates.
  2. **Memory Ablation:** Disable the Global Memory input to the Correct & Validate function and measure the drop in success rate for "Implicit" commands.
  3. **Composite Command Stress Test:** Input the 15 composite commands from the dataset to verify if the Action Determiner can handle multi-step logic or if it requires sequential user turns.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the VISA framework be extended to robustly handle composite commands requiring multi-step sequential operations? The authors state that most errors arise from composite commands, indicating the need to extend the VISA to support multi-step sequential operations. This remains unresolved as the current architecture struggles to determine correct agent functions when multiple actions must be executed sequentially.

- **Open Question 2:** Can the cumulative latency of sequential LLM calls be optimized to meet the strict real-time requirements of surgical environments? The discussion notes that cumulative latency from sequential LLM calls remains a challenge in time-sensitive surgical environments. This is unresolved as the current reliance on LLM reasoning for multiple stages introduces delays that may be unacceptable during critical procedural moments.

- **Open Question 3:** Does fine-tuning lightweight Speech-to-Text models on medical terminology significantly reduce transcription errors compared to general post-hoc correction? The paper observes that most errors occur with medical domain-specific terminology, suggesting the need to fine-tune lightweight STT models on medical terms. This remains unresolved as the current system relies on general-purpose Whisper models which burden the downstream correction agent.

## Limitations
- The system's performance heavily depends on carefully engineered LLM prompts and rules, which are referenced but not fully detailed in the paper
- Domain specificity limits generalizability, as the system is trained and tested on data from a single hospital
- Composite command handling is not fully validated, with the system potentially requiring sequential user turns for complex commands

## Confidence

- **High Confidence:** The hierarchical multi-agent architecture and the core workflow (Audio Capture → STT → Correct & Validate → Command Reasoning → Action Determiner) are clearly described and logically sound
- **Medium Confidence:** The system's performance on the specific test dataset (240 commands) is well-documented, though claims about robustness rely on partially detailed mechanisms
- **Low Confidence:** Claims about real-world surgical environment performance are not directly supported by experimental results, which use synthetic voice commands in controlled settings

## Next Checks

1. **Prompt Template Validation:** Obtain and test the exact LLM prompts used for the Workflow Orchestrator and task agents to verify the system's decision-making logic
2. **External Dataset Testing:** Evaluate the system on a dataset of voice commands from a different hospital or with different medical terminology to assess its generalizability
3. **Real-World Latency Measurement:** Measure the system's end-to-end latency in a simulated surgical environment with realistic background noise to validate its practical usability