---
ver: rpa2
title: 'Fitting Different Interactive Information: Joint Classification of Emotion
  and Intention'
arxiv_id: '2501.06215'
source_url: https://arxiv.org/abs/2501.06215
tags:
- recognition
- intention
- different
- emotion
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses low-resource multimodal emotion and intention
  recognition by leveraging pseudo-label generation and interactive multimodal fusion.
  The authors propose a two-stage training strategy using pseudo-labels to expand
  the labeled dataset and a multimodal fusion module with multi-head self-attention
  to facilitate interaction between emotion and intention tasks.
---

# Fitting Different Interactive Information: Joint Classification of Emotion and Intention

## Quick Facts
- **arXiv ID**: 2501.06215
- **Source URL**: https://arxiv.org/abs/2501.06215
- **Reference count**: 11
- **Primary result**: Two-stage training with pseudo-labels and multi-head self-attention fusion achieved 0.5532 on MEIJU@2025 Track I test set

## Executive Summary
This paper tackles low-resource multimodal emotion and intention recognition through a two-stage training strategy that leverages pseudo-label generation to expand the labeled dataset. The authors propose an interactive multimodal fusion module with multi-head self-attention to facilitate information exchange between emotion and intention recognition tasks. Their approach won the MEIJU@2025 Track I competition by achieving a score of 0.5532 on the test set, demonstrating the effectiveness of their pseudo-label generation and attention-based fusion techniques.

## Method Summary
The proposed method employs a two-stage training strategy for multimodal emotion and intention recognition. First, pseudo-labels are generated to expand the limited labeled dataset. Second, an interactive multimodal fusion module with multi-head self-attention facilitates information exchange between the emotion and intention tasks. The system leverages complementary information across different attention heads, with intention recognition identified as easier to represent and benefiting from cross-task information flow. The approach also incorporates text punctuation refinement as part of the processing pipeline.

## Key Results
- Achieved competition score of 0.5532 on MEIJU@2025 Track I test set
- Demonstrated that intention recognition is easier to represent and benefits from cross-task information
- Showed effectiveness of fusing results from multiple attention heads
- Text punctuation refinement contributed to final performance

## Why This Works (Mechanism)
The method works by leveraging pseudo-label generation to address data scarcity in low-resource multimodal settings, effectively creating a larger training dataset from limited labeled examples. The multi-head self-attention fusion mechanism allows different attention heads to capture distinct aspects of the emotion-intention relationship, with intention recognition serving as a more stable representation that benefits from emotion information. The two-stage training strategy ensures that the model first learns from the expanded pseudo-labeled data before refining with interactive fusion, creating a synergistic effect between the tasks.

## Foundational Learning
- **Pseudo-label generation**: Creating artificial labels for unlabeled data to expand training sets; needed because real labeled data is scarce in low-resource settings, quick check: evaluate label quality against ground truth
- **Multi-head self-attention**: Parallel attention mechanisms that capture different feature interactions; needed to model complex relationships between emotion and intention, quick check: compare performance across different numbers of heads
- **Interactive multimodal fusion**: Combining information from multiple modalities through task interaction; needed to leverage complementary information between emotion and intention tasks, quick check: measure information gain from cross-task fusion
- **Two-stage training**: Sequential training phases with different objectives; needed to first learn from expanded data then refine through interaction, quick check: compare against single-stage training
- **Low-resource learning**: Techniques for training with limited labeled data; needed because real-world emotion/intent datasets are often small, quick check: measure performance degradation with reduced labeled data
- **Text punctuation refinement**: Processing text to enhance emotional and intentional cues; needed to improve signal quality in textual modality, quick check: ablate punctuation processing to measure impact

## Architecture Onboarding

**Component map**: Text/Visual/Audio encoders -> Pseudo-label generator -> Multimodal fusion module (multi-head self-attention) -> Joint classifier -> Emotion/Intention outputs

**Critical path**: Input modalities → Feature extraction → Pseudo-label generation (stage 1) → Interactive fusion (stage 2) → Final classification

**Design tradeoffs**: Pseudo-labels introduce noise but expand training data; multi-head attention increases model complexity but captures richer interactions; two-stage training requires careful scheduling but enables synergistic learning

**Failure signatures**: Poor pseudo-label quality leading to noisy training; insufficient attention head diversity causing limited interaction; stage transition issues disrupting learning continuity

**First experiments**: 1) Baseline performance without pseudo-labels, 2) Single-head attention fusion comparison, 3) Supervised learning on original labeled data only

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- No ablation study comparing pseudo-label approach against supervised learning on original labeled data alone
- Difficulty isolating the contribution of interactive fusion module versus model architecture
- Limited analysis of text punctuation refinement's specific impact on performance

## Confidence

**High confidence**: The two-stage training strategy using pseudo-labels and the use of multi-head self-attention for multimodal fusion are well-documented and produce the reported competition score of 0.5532.

**Medium confidence**: The claim that intention recognition is easier to represent than emotion recognition, and that this property enables cross-task benefits through attention head fusion.

**Low confidence**: The relative contributions of pseudo-label generation, interactive fusion, and text punctuation refinement to the final performance, as well as whether these gains would generalize to other low-resource multimodal emotion recognition tasks.

## Next Checks
1. Conduct an ablation study isolating the contributions of pseudo-label generation, interactive fusion module, and text punctuation refinement to determine their individual impact on performance.
2. Test the pseudo-label generation approach on a held-out subset of labeled data to evaluate label quality and its effect on model performance compared to supervised learning alone.
3. Compare the multi-head self-attention fusion approach against simpler fusion baselines (concatenation, late fusion, single attention head) to quantify the benefit of the proposed interactive fusion mechanism.