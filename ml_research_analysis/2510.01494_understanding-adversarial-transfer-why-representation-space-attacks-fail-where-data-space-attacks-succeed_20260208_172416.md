---
ver: rpa2
title: 'Understanding Adversarial Transfer: Why Representation-Space Attacks Fail
  Where Data-Space Attacks Succeed'
arxiv_id: '2510.01494'
source_url: https://arxiv.org/abs/2510.01494
tags:
- transfer
- attacks
- language
- attack
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Adversarial attacks in the input data-space can successfully transfer
  between models, but attacks in model representation space generally cannot, unless
  the models'' latent geometries are sufficiently aligned. This was demonstrated through
  four settings: kernel regression (theoretical), image classifiers, language models,
  and vision-language models.'
---

# Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed

## Quick Facts
- **arXiv ID:** 2510.01494
- **Source URL:** https://arxiv.org/abs/2510.01494
- **Reference count:** 40
- **Primary result:** Adversarial attacks in input data-space transfer between models, but attacks in model representation space generally do not, unless the models' latent geometries are sufficiently aligned.

## Executive Summary
This paper investigates why adversarial attacks transfer between models in some domains but not others. The key finding is that data-space attacks (perturbations to raw inputs like pixels or text) transfer well because models learn similar input-output mappings, while representation-space attacks (perturbations to internal activations) generally fail to transfer due to misaligned latent geometries between models. The authors demonstrate this through experiments on kernel regression (theoretical), image classifiers, language models, and vision-language models, showing that data-space attacks like image adversarial examples and textual jailbreak suffixes transfer successfully, while representation-space attacks like soft prompts or latent-space image attacks do not transfer unless models are finetuned from the same base or have highly aligned representations.

## Method Summary
The paper compares transferability of data-space attacks (optimized on raw inputs) versus representation-space attacks (optimized on internal activations) across multiple model types. For image classifiers, they train multiple ResNet18 models on CIFAR10 and optimize universal adversarial perturbations in both input and representation spaces, measuring transfer success to held-out models. For language models, they fine-tune LLaMA3-3B on different datasets and test soft prompt attack transferability. For vision-language models, they examine jailbreak attack transferability using images as inputs, analyzing how the projector component affects transfer. They measure latent similarity using metrics like AvgCosine and CKA to correlate with transfer success.

## Key Results
- Data-space attacks transfer perfectly between models trained on similar data distributions.
- Representation-space attacks generally fail to transfer between independently trained models.
- Transfer of representation-space attacks succeeds when models are finetuned from the same base (AvgCosine > 0.9, CKA > 0.99).
- In adapter-style VLMs, images function as representation-space perturbations due to the projector, preventing image jailbreak transfer between different VLMs.

## Why This Works (Mechanism)

### Mechanism 1: Data-Space Perturbations Transfer via Shared Input-Output Maps
Data-space attacks transfer because different models trained on similar data learn similar functional input-output mappings. A perturbation $\delta_{data}$ optimized on a surrogate model $f_A$ causes a specific harmful output, and because $f_A(x) \approx f_B(x)$ for models trained on similar distributions, applying the same $\delta_{data}$ to a target model $f_B$ produces a similar output error.

### Mechanism 2: Representation-Space Perturbations Require Geometric Alignment
Representation-space attacks generally do not transfer because the latent geometries of different models are misaligned. A representation-space attack $\delta_{repr}$ is a vector in the source model's latent space, and for it to transfer, the target model's latent space must be geometrically aligned such that the same vector has a similar effect.

### Mechanism 3: VLM Images Function as Representation-Space Perturbations
In adapter-style VLMs, images effectively act as perturbations to the language model's representation space, not as data-space inputs. From the LM's perspective, visual inputs are transformed by a vision encoder and projector into embeddings injected into the LM, making them behave like representation-space perturbations due to projector-induced misalignment.

## Foundational Learning

- **Concept: Adversarial Transferability (Black-box Attacks)**
  - Why needed: The entire paper investigates why some attacks transfer and others do not.
  - Quick check: Can you explain why transfer attacks are a significant security threat for proprietary models?

- **Concept: Representation Spaces and Latent Geometry**
  - Why needed: The core hypothesis hinges on the idea that different models learn different internal representations even if their input-output functions are similar.
  - Quick check: How can two models compute the same function but have different representation spaces?

- **Concept: Vision-Language Model (VLM) Architecture (Adapter-style)**
  - Why needed: The paper's key application is explaining why image jailbreaks fail to transfer in VLMs.
  - Quick check: In an adapter-style VLM, what component transforms visual features into the language model's embedding space?

## Architecture Onboarding

- **Component map:**
  1. Input Space (raw data: pixels/tokens) → Data-Space Attacks
  2. Representation Layers (internal activations) → Representation-Space Attacks
  3. Output Layer (classification/generation head) → Attack target
  4. VLM Projector (adapter-style VLMs) → Transforms vision to language space

- **Critical path:** Select task → Select models → Generate attacks (data-space vs representation-space) → Evaluate transfer (measure ASR) → Measure latent similarity (AvgCosine/CKA)

- **Design tradeoffs:**
  - Earlier layers may transfer slightly better for representation attacks
  - Larger epsilon increases source ASR but doesn't improve transfer
  - Soft prompt attacks are sensitive to random seeds

- **Failure signatures:**
  - Data-space attack fails: Target model has substantially different decision boundary
  - Representation-space attack succeeds: Models are finetuned from same base or have anomalously aligned latents
  - Soft prompt instability: High variance across random seeds

- **First 3 experiments:**
  1. Replicate Image Classifier experiment: Train 10 ResNet18 on CIFAR10, test data vs representation-space attack transfer
  2. Replicate Finetuned LM experiment: Fine-tune LLaMA3-3B on different datasets, test soft prompt transferability
  3. Test new modality: Apply paradigm to audio-language models or text-to-image models

## Open Questions the Paper Calls Out

### Open Question 1
How does model scale affect latent representation alignment and consequently the transferability of representation-space attacks? The paper focuses on fixed-size model groups and does not systematically vary model scale to isolate its effect on geometric alignment or attack transfer rates.

### Open Question 2
Do representation-space attacks transfer in early-fusion VLMs, or do they remain non-transferable as in adapter-style VLMs? The paper's results are limited to adapter-based VLMs, and early-fusion models process multimodal inputs through a unified architecture differently.

### Open Question 3
Can more sophisticated similarity metrics better predict the transferability of representation-space attacks between model pairs? The paper uses average cosine similarity and CKA as proxies, but these may not fully capture the conditions under which transfer succeeds.

## Limitations
- Lack of direct empirical validation for the core geometric alignment hypothesis - no positive demonstration that carefully aligned attacks can transfer
- Results focused on specific model families (ResNet, Llama-style LMs, adapter-style VLMs), limiting generalizability
- Soft prompt attacks show high sensitivity to randomness, potentially confounding results

## Confidence
- **High Confidence:** Data-space attacks transfer between models - well-established in literature and consistent with paper's results
- **Medium Confidence:** Representation-space attacks generally fail to transfer - supported by experimental results but lacks positive proof
- **Medium Confidence:** Images in adapter-style VLMs function as representation-space attacks - compelling argument but relies on specific architectural interpretation

## Next Checks
1. Design experiment to explicitly demonstrate successful transfer of a representation-space attack when models have sufficiently aligned latent geometries by finetuning from same base or applying alignment transformation
2. Conduct ablation study on VLMs comparing transferability between models with same vs different projectors to isolate projector effect
3. Test generality by applying same experimental paradigm to new modality like audio-language models or text-to-image models