---
ver: rpa2
title: Leveraging In-Context Learning for Political Bias Testing of LLMs
arxiv_id: '2506.22232'
source_url: https://arxiv.org/abs/2506.22232
tags:
- bias
- llama
- base
- question
- instruct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the instability of LLM political bias evaluation
  when using zero-shot prompting. It proposes Questionnaire Modeling (QM), which uses
  human survey responses as in-context examples to stabilize predictions.
---

# Leveraging In-Context Learning for Political Bias Testing of LLMs

## Quick Facts
- **arXiv ID**: 2506.22232
- **Source URL**: https://arxiv.org/abs/2506.22232
- **Reference count**: 32
- **Primary result**: QM improves stability over zero-shot prompting and shows larger models exhibit lower bias and higher personalization accuracy

## Executive Summary
This paper addresses the instability of LLM political bias evaluation when using zero-shot prompting. It proposes Questionnaire Modeling (QM), which uses human survey responses as in-context examples to stabilize predictions. QM samples respondents' answers and averages model outputs to estimate bias relative to a human population. Experiments with models from 8B to 405B parameters show that QM improves stability over zero-shot prompting and that larger models exhibit lower bias and higher personalization accuracy. Instruction tuning has mixed effects but can flip bias direction in some cases. QM enables comparison of instruction-tuned and base models while maintaining interpretability and robustness.

## Method Summary
The paper introduces Questionnaire Modeling (QM) as a method to evaluate political bias in LLMs using in-context learning. QM leverages human survey responses as examples in the prompt, allowing the model to condition its predictions on human-like reasoning patterns. The approach involves sampling respondents' answers and averaging model outputs to estimate bias relative to a human population. The method was tested across multiple model sizes (8B to 405B parameters) and compared against zero-shot prompting baselines. The framework specifically evaluates how instruction tuning affects political bias detection while maintaining interpretability of results.

## Key Results
- QM improves stability over zero-shot prompting for political bias evaluation
- Larger models (up to 405B parameters) exhibit lower bias and higher personalization accuracy
- Instruction tuning has mixed effects and can flip bias direction in some cases

## Why This Works (Mechanism)
The mechanism works by providing LLMs with human-like examples in-context, which grounds their reasoning patterns in actual human survey responses rather than relying solely on their internal representations. This approach reduces the variance in predictions that typically occurs with zero-shot prompting, where models must extrapolate political reasoning from their pretraining alone. By sampling multiple respondents and averaging outputs, QM captures population-level patterns while reducing individual response noise. The method effectively creates a bridge between abstract model representations and concrete human political viewpoints, enabling more stable and interpretable bias measurements.

## Foundational Learning

### In-Context Learning
**Why needed**: Allows models to adapt to new tasks without parameter updates by conditioning on example inputs in the prompt
**Quick check**: Can the model complete tasks given only a few examples in the prompt?

### Political Survey Analysis
**Why needed**: Provides ground truth human responses for measuring model alignment with actual political viewpoints
**Quick check**: Are the survey questions representative and the responses reliable across different demographics?

### Bias Measurement Frameworks
**Why needed**: Establishes standardized methods for quantifying and comparing political bias across different models
**Quick check**: Does the framework capture both direction and magnitude of bias consistently?

## Architecture Onboarding

### Component Map
Human Survey Data -> QM Prompt Construction -> LLM Inference -> Bias Score Aggregation -> Comparison Framework

### Critical Path
The critical path flows from survey data preparation through prompt construction to model inference, where the quality of in-context examples directly determines prediction stability. The aggregation step is crucial for reducing variance across multiple sampled responses.

### Design Tradeoffs
QM trades computational efficiency (requiring multiple inference passes for averaging) for improved stability and interpretability. The method requires careful selection of in-context examples to avoid introducing new biases while maintaining the model's ability to generalize beyond the provided examples.

### Failure Signatures
Instability in zero-shot prompting returns when prompts are too short or examples are poorly selected. The method may fail to capture nuanced political views if survey data is limited or biased. Large variance in model outputs across sampled respondents indicates insufficient grounding in human patterns.

### First Experiments
1. Compare QM vs zero-shot prompting on identical political questions across different model sizes
2. Test QM sensitivity to different numbers of in-context examples (1-10 samples)
3. Evaluate how different respondent sampling strategies affect bias measurement consistency

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on a single U.S.-centric dataset (CCES) which may not generalize to other political contexts or cultures
- QM assumes human survey responses adequately represent population distribution of political views
- Comparison between base and instruction-tuned models is limited to specific model families

## Confidence

**High Confidence**: The core finding that in-context learning with human examples improves prediction stability over zero-shot prompting is well-supported by experimental results across multiple model sizes.

**Medium Confidence**: The interpretation of instruction tuning's mixed effects on political bias requires more extensive validation.

**Low Confidence**: The claim that QM enables meaningful comparison between instruction-tuned and base models while maintaining interpretability needs further validation.

## Next Checks
1. Replicate the QM approach using multiple diverse political datasets from different countries and cultural contexts
2. Conduct longitudinal studies to evaluate whether stability improvements persist over time as models are updated
3. Test the method's robustness against adversarial prompting strategies designed to expose political biases