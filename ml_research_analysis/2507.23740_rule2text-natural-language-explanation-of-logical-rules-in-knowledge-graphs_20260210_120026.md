---
ver: rpa2
title: 'Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs'
arxiv_id: '2507.23740'
source_url: https://arxiv.org/abs/2507.23740
tags:
- rules
- rule
- explanation
- explanations
- logical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of large language models (LLMs) to
  generate natural language explanations for logical rules extracted from knowledge
  graphs. The authors mined rules from three datasets using the AMIE algorithm and
  tested various prompting strategies including zero-shot, few-shot, variable entity
  type integration, and chain-of-thought reasoning.
---

# Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs

## Quick Facts
- **arXiv ID**: 2507.23740
- **Source URL**: https://arxiv.org/abs/2507.23740
- **Reference count**: 25
- **Primary result**: LLMs generate more accurate and clearer natural language explanations for logical rules when using Chain-of-Thought prompting with variable entity type integration

## Executive Summary
This paper tackles the challenge of making logical rules extracted from knowledge graphs interpretable by generating natural language explanations. The authors mined Horn clauses from three Freebase-based datasets using AMIE and tested various prompting strategies with large language models including GPT-3.5, GPT-4o Mini, and Gemini 2.0 Flash. Their key finding is that combining Chain-of-Thought reasoning with explicit variable entity type integration significantly improves explanation quality, with Gemini 2.0 Flash achieving the highest performance. The study also reveals that concatenated relations in Freebase pose particular challenges, leading to increased hallucination rates, and demonstrates that LLM-as-a-judge correlates moderately with human evaluations.

## Method Summary
The study employed a three-phase experimental approach to evaluate natural language explanation generation. In Phase 1, the team compared zero-shot and few-shot prompting strategies using GPT-3.5 on mined rules from FB15k-237, FB-CVT-REV, and FB+CVT-REV. Phase 2 introduced variable entity type integration to address the challenge of variable role assignment. Phase 3 combined Chain-of-Thought prompting with variable types across all three LLM models. Rules were mined using AMIE 3.5.1 with parameters set to max 3 atoms, min head coverage 0.1, and min confidence 0.1. Human evaluation assessed correctness and clarity on 5-point scales, while perplexity was measured using GPT-2. The best-performing configuration used CoT with variable types and Gemini 2.0 Flash.

## Key Results
- Chain-of-Thought prompting with variable entity type integration improved explanation correctness and clarity across all evaluated models
- Gemini 2.0 Flash achieved the highest overall performance in natural language explanation generation
- Hallucination rates were particularly high for concatenated relations in Freebase, requiring additional entities to explain complex predicate labels
- LLM-as-a-judge evaluation showed moderate correlation (Pearson's r â‰ˆ 0.4-0.6) with human annotator assessments

## Why This Works (Mechanism)
The approach works by providing LLMs with structured reasoning frameworks and explicit semantic context. Chain-of-Thought prompting breaks down the explanation generation into discrete reasoning steps, helping models process the logical structure of Horn clauses systematically. Variable entity type integration supplies crucial semantic context about the roles different variables play in the rule, addressing the ambiguity that arises when variables like ?a, ?b, and ?c appear in complex predicates. This combination effectively guides the model to produce more faithful and coherent explanations by reducing the cognitive load of simultaneously parsing logical structure and generating fluent natural language.

## Foundational Learning
**Horn Clause Structure** - Logical rules in the form of implications with conjunctive antecedents and single consequents; needed to understand what the models must explain, quick check: can you identify head and body atoms in a given rule
**AMIE Algorithm** - Pattern mining tool for extracting logical rules from KGs; needed to understand the rule generation process and limitations, quick check: know the parameter constraints (max atoms, min coverage)
**Variable Entity Types** - Semantic categories assigned to rule variables; needed to provide contextual meaning to abstract symbols like ?a and ?b, quick check: can map variables to their likely entity types in a rule
**Concatenated Relations** - Multi-part predicate labels in Freebase (e.g., domain1/type1/label1./domain2/type2/label2); needed to understand a key source of hallucination, quick check: identify concatenated vs. standard relations in dataset
**Chain-of-Thought Prompting** - Multi-step reasoning instruction format; needed to guide systematic explanation generation, quick check: recognize the 5-step reasoning pattern used
**Perplexity as Metric** - Language model probability measure for text fluency; needed for automatic evaluation of generated explanations, quick check: understand that lower perplexity indicates more predictable text

## Architecture Onboarding

**Component Map**
AMIE Miner -> Rule Dataset -> LLM with Prompting Strategy -> Natural Language Explanation -> Evaluation Pipeline

**Critical Path**
The critical path flows from rule mining through LLM inference to evaluation. AMIE extracts Horn clauses from KGs, which are then formatted into prompts combining CoT instructions and variable type annotations. The LLM generates explanations, which are evaluated through human judgment (correctness, clarity, hallucination counts) and automatic metrics (perplexity). The choice of Gemini 2.0 Flash with CoT + variable types represents the optimal configuration identified.

**Design Tradeoffs**
The study trades computational efficiency for explanation quality by using sophisticated prompting strategies rather than fine-tuning. While prompting is faster and requires no training data, it may not achieve the same performance as specialized fine-tuned models. The use of concatenated relations in Freebase, while enabling more expressive rules, introduces complexity that increases hallucination risk. The reliance on human evaluation provides quality assurance but limits scalability compared to fully automated approaches.

**Failure Signatures**
Hallucination manifests as the generation of entities or relations not present in the original rule, particularly for concatenated predicates where models invent additional nouns to explain complex labels. Ambiguity failures occur when variable types are not provided, leading to vague or incorrect subject-object assignments. Perplexity spikes indicate explanations that deviate significantly from natural language patterns, often correlating with logical inconsistencies in the generated text.

**First Experiments**
1. Run AMIE with default parameters on a small KG subset to verify rule mining functionality
2. Test zero-shot prompting on simple 2-atom rules to establish baseline performance
3. Implement variable type integration on a single rule type to verify semantic role assignment

## Open Questions the Paper Calls Out
**Generalization to Different KG Schemas**: Can the approach work effectively on knowledge bases like Wikidata that use different encoding schemas? The study's restriction to Freebase variants with specific label formats raises questions about applicability to other KG schemas with different predicate naming conventions.

**Scalability to Complex Rules**: How does LLM performance scale when generating explanations for rules more complex than those extractable by AMIE? The current study is limited to AMIE's 3-atom maximum, leaving open questions about longer or more nested logical structures.

**Fine-tuning vs. Prompting**: Can smaller open-source models fine-tuned on generated pseudo-ground truth explanations outperform the current few-shot prompting baselines? The authors suggest using LLM-as-a-judge to create high-quality datasets for this purpose.

## Limitations
- Human evaluation introduces subjectivity in assessing explanation quality, despite using multiple annotators
- Findings may not generalize beyond Freebase-based datasets due to specific predicate naming conventions and concatenated relation challenges
- LLM-as-a-judge evaluation shows only moderate correlation with human judgments, limiting automated quality assessment

## Confidence
**High**: Core finding that CoT + variable type integration improves explanation quality across all metrics and models
**Medium**: Generalization of prompting strategies to non-Freebase KGs and different predicate schemas
**Low**: Long-term stability of improvements as LLMs evolve and with larger, more complex rule sets

## Next Checks
1. Test Rule2Text approach on non-Freebase KGs (Wikidata, YAGO) to assess generalizability and identify whether concatenated relation challenges persist
2. Replicate experiments using newer LLM versions and different AMIE parameter settings to determine stability of performance gains
3. Develop additional automated metrics beyond perplexity (factual consistency, semantic similarity) to reduce reliance on human evaluation