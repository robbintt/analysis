---
ver: rpa2
title: Quantifying Risks in Multi-turn Conversation with Large Language Models
arxiv_id: '2510.03969'
source_url: https://arxiv.org/abs/2510.03969
tags:
- bounds
- query
- bound
- catastrophic
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a framework for quantifying catastrophic risks
  in multi-turn LLM conversations through statistical certification. It models conversations
  as Markov processes on query graphs, defining distributions over query sequences
  to evaluate risk.
---

# Quantifying Risks in Multi-turn Conversation with Large Language Models

## Quick Facts
- arXiv ID: 2510.03969
- Source URL: https://arxiv.org/abs/2510.03969
- Reference count: 40
- Primary result: Statistical framework for certifying lower bounds on catastrophic risk in multi-turn LLM conversations, ranging from 0.1% to 70%

## Executive Summary
This paper introduces a statistical framework for quantifying catastrophic risks in multi-turn conversations with large language models. The approach models conversations as Markov processes on query graphs, using three different distributions to sample query sequences and evaluate whether responses become harmful. A judge model determines if responses are catastrophic, and statistical certification provides lower bounds on the probability of catastrophic outcomes. The framework reveals that context and distractors increase vulnerability, and that statistical guarantees offer more reliable risk assessments than fixed benchmarks.

## Method Summary
The framework treats multi-turn conversations as Markov processes where each query-response pair forms a state transition. Query graphs are constructed by generating candidate queries from seed queries and organizing them into nodes. Three distributions are used to sample query sequences: random node selection, graph path traversal, and adaptive selection with rejection. A judge model evaluates whether responses are harmful, and statistical certification provides lower bounds on catastrophic risk by bounding the probability that any response in a conversation path is harmful. The approach uses product distributions over conversation paths and rejection sampling to handle dependencies between turns.

## Key Results
- Certified lower bounds on catastrophic risk range from 0.1% to 70% across different models
- Claude-Sonnet-4 and Nova Premier show lower catastrophic risks compared to Mistral-Large and DeepSeek-R1
- Context and distractors significantly increase vulnerability to catastrophic responses
- Statistical guarantees provide more reliable risk assessments than fixed benchmarks

## Why This Works (Mechanism)
The framework works by systematically exploring the space of possible conversation trajectories through query graphs, rather than relying on fixed test suites. By modeling conversations as Markov processes and using multiple sampling distributions, it captures the stochastic nature of multi-turn interactions. The statistical certification approach provides mathematically rigorous lower bounds on catastrophic risk by bounding the probability of harmful responses across all possible conversation paths, accounting for the dependencies between turns through product distributions and rejection sampling.

## Foundational Learning
- **Markov Processes**: Needed to model state transitions in conversations where each query-response pair forms a new state. Quick check: Verify that the probability of the next state depends only on the current state.
- **Query Graphs**: Needed to represent the space of possible queries and their relationships in multi-turn conversations. Quick check: Ensure all relevant candidate queries are included and properly connected.
- **Product Distributions**: Needed to model the joint probability of sequences of responses in multi-turn conversations. Quick check: Verify that the product correctly captures dependencies between turns.
- **Statistical Certification**: Needed to provide mathematically rigorous lower bounds on catastrophic risk probabilities. Quick check: Confirm that the certification method provides valid bounds for all possible conversation paths.

## Architecture Onboarding
- **Component Map**: Seed Queries -> Query Graph Generation -> Query Sequence Sampling -> Response Generation -> Judge Model Evaluation -> Risk Certification
- **Critical Path**: The sequence from query graph generation through risk certification is critical, as errors at any stage propagate to the final risk estimates
- **Design Tradeoffs**: Synthetic query generation offers scalability but may miss real-world attack creativity; multiple distributions provide robustness but increase computational cost; statistical certification offers rigor but depends on judge model accuracy
- **Failure Signatures**: Path collapse in adaptive sampling, judge model bias leading to incorrect risk bounds, insufficient query graph coverage missing critical attack scenarios
- **First Experiments**: 1) Test risk certification with varying judge model sensitivity thresholds, 2) Compare synthetic query risk estimates with human-crafted attack scenarios, 3) Evaluate how different query graph generation parameters affect certification stability

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on synthetic query generation may not capture real-world attack diversity
- Three proposed distributions are approximations that may not fully represent actual conversation dynamics
- Risk certification is fundamentally limited by judge model accuracy and potential biases
- Markov process assumption may overlook long-range dependencies crucial for understanding catastrophic responses

## Confidence
- **High Confidence**: Statistical certification framework methodology and mathematical formulation are sound
- **Medium Confidence**: Comparative risk rankings between models are reliable within the framework but may not generalize to all real-world scenarios
- **Medium Confidence**: Finding that context and distractors increase vulnerability is methodologically sound but may be specific to synthetic scenarios

## Next Checks
1. Test certified risk bounds against human-crafted multi-turn attack scenarios to validate synthetic query estimates
2. Evaluate how judge model sensitivity thresholds affect certified risk bounds using multiple independent harm detection models
3. Compare risk estimates across different query graph generation parameters to determine certification stability and identify optimal strategies