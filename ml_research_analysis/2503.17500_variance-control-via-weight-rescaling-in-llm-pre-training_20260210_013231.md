---
ver: rpa2
title: Variance Control via Weight Rescaling in LLM Pre-training
arxiv_id: '2503.17500'
source_url: https://arxiv.org/abs/2503.17500
tags:
- standard
- deviation
- init
- weight
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Variance Control via Weight Rescaling in LLM Pre-training This
  work addresses the challenge of managing weight variance during Large Language Model
  (LLM) pre-training, where weights tend to diverge from their initial scale, undermining
  training stability and downstream performance. The authors introduce two techniques:
  Layer Index Rescaling (LIR) for initialization and Target Variance Rescaling (TVR)
  for variance control during training.'
---

# Variance Control via Weight Rescaling in LLM Pre-training

## Quick Facts
- **arXiv ID:** 2503.17500
- **Source URL:** https://arxiv.org/abs/2503.17500
- **Reference count:** 40
- **Primary result:** Variance control via weight rescaling improves downstream task performance and training stability in LLM pre-training.

## Executive Summary
This work addresses the challenge of managing weight variance during Large Language Model (LLM) pre-training, where weights tend to diverge from their initial scale, undermining training stability and downstream performance. The authors introduce two techniques: Layer Index Rescaling (LIR) for initialization and Target Variance Rescaling (TVR) for variance control during training. LIR scales weights by the inverse square root of the layer index, while TVR adjusts weights to maintain target standard deviation values during training. Experiments on a 1B parameter LLaMA model show that these methods improve downstream task performance by up to 4.6% on common benchmarks (e.g., HellaSwag, PIQA, ARC-Challenge) and reduce extreme activation values, mitigating risks associated with quantization and low-precision training. The approach also enhances training stability and throughput, demonstrating the importance of variance management in LLM training.

## Method Summary
The authors propose two variance control techniques: Layer Index Rescaling (LIR) for initialization and Target Variance Rescaling (TVR) for training. LIR scales initial weights by the inverse square root of the layer index to counteract variance accumulation. TVR monitors weight statistics during training and rescales weights to maintain target standard deviations, preventing divergence. These methods are applied to a 1B parameter LLaMA model, with experiments measuring downstream task performance, activation statistics, and training stability metrics.

## Key Results
- Improved downstream task performance by up to 4.6% on benchmarks like HellaSwag, PIQA, and ARC-Challenge
- Reduced extreme activation values, mitigating quantization and low-precision training risks
- Enhanced training stability and throughput compared to baseline initialization methods

## Why This Works (Mechanism)
The proposed methods work by controlling weight variance throughout the training process. LIR addresses initialization bias by scaling weights inversely with layer depth, preventing early variance accumulation. TVR maintains stable weight distributions during training by continuously adjusting weights to target standard deviations. This prevents the exponential growth of weight magnitudes that can lead to numerical instability and degraded performance.

## Foundational Learning

**Variance Accumulation in Deep Networks**
- Why needed: Understanding how weight variance compounds across layers is crucial for designing effective control mechanisms
- Quick check: Verify that variance grows approximately exponentially with depth in standard initialization schemes

**Weight Initialization Theory**
- Why needed: Proper initialization sets the stage for stable training and prevents early divergence
- Quick check: Compare variance distributions across different initialization methods (e.g., Xavier, Kaiming) in deep networks

**Numerical Stability in Training**
- Why needed: Extreme weight values can cause overflow/underflow and degrade gradient-based optimization
- Quick check: Monitor activation statistics and gradient norms for signs of numerical instability during training

## Architecture Onboarding

**Component Map**
LLM Model -> LIR (Initialization) -> TVR (Training-time Control) -> Variance-Monitored Training

**Critical Path**
1. Initialize weights with LIR scaling
2. Begin training with standard optimizer
3. Monitor weight statistics at intervals
4. Apply TVR rescaling when variance deviates from target
5. Continue training with rescaled weights

**Design Tradeoffs**
- LIR vs. traditional initialization: Improved variance control vs. potential underutilization of layer capacity
- TVR frequency: More frequent monitoring provides tighter control but increases computational overhead
- Target variance selection: Balancing between preventing divergence and maintaining representational power

**Failure Signatures**
- Insufficient variance control: Diverging weight magnitudes, numerical instability, degraded performance
- Excessive rescaling: Loss of learned representations, training instability, convergence issues

**3 First Experiments**
1. Compare downstream task performance with and without LIR/TVR on a small LLM
2. Measure activation statistics and extreme value occurrences during training
3. Evaluate training stability metrics (gradient norms, loss curves) across different variance control settings

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on a single model scale (1B parameters), leaving scalability questions open
- Focus on pre-training only, without validation on fine-tuning or inference tasks
- No direct experiments evaluating quantization error reduction or low-precision training benefits

## Confidence

**Major Claim Confidence:**
- Variance control improves downstream task performance: **Medium**
- LIR and TVR enhance training stability and throughput: **Medium**
- Weight rescaling mitigates quantization risks: **Low** (based on activation statistics rather than direct quantization experiments)

## Next Checks

1. Test LIR/TVR at multiple model scales (e.g., 7B, 13B parameters) to verify scalability benefits hold beyond the 1B parameter case.
2. Conduct ablation studies to isolate the impact of LIR (initialization) versus TVR (training-time control) on final performance and stability metrics.
3. Perform direct experiments evaluating quantization error reduction and numerical stability under low-precision training (e.g., 8-bit, 4-bit) with and without variance control.