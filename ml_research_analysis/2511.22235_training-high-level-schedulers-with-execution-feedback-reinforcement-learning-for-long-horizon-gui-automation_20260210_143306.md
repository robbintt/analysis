---
ver: rpa2
title: Training High-Level Schedulers with Execution-Feedback Reinforcement Learning
  for Long-Horizon GUI Automation
arxiv_id: '2511.22235'
source_url: https://arxiv.org/abs/2511.22235
tags:
- meeting
- state
- answer
- think
- coordinator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a staged execution-feedback reinforcement
  learning algorithm to address long-horizon GUI automation challenges. The key insight
  is decoupling high-level strategic planning from low-level execution by training
  two specialized agents: a Coordinator for task decomposition and a State Tracker
  for dynamic context compression and state summarization.'
---

# Training High-Level Schedulers with Execution-Feedback Reinforcement Learning for Long-Horizon GUI Automation

## Quick Facts
- arXiv ID: 2511.22235
- Source URL: https://arxiv.org/abs/2511.22235
- Authors: Zehao Deng; Tianjie Ju; Zheng Wu; Zhuosheng Zhang; Gongshen Liu
- Reference count: 40
- Primary result: Staged execution-feedback RL with Coordinator-Executor-State Tracker architecture improves GUI automation success rates by up to 18.09% on long-horizon tasks

## Executive Summary
This paper addresses long-horizon GUI automation challenges by introducing a staged execution-feedback reinforcement learning algorithm that decouples high-level strategic planning from low-level execution. The approach trains three specialized agents: a Coordinator for task decomposition, a State Tracker for dynamic context compression and state summarization, and a frozen Executor for action execution. Experiments demonstrate significant performance improvements on GUI-Odyssey benchmark, with up to 18.09% improvement in success rate, while maintaining strong generalization across different Executor models.

## Method Summary
The CES framework uses staged GRPO-based RL training with a frozen Executor model. Stage 1 trains the Coordinator using ground-truth state annotations and Executor feedback rewards; Stage 2 trains the State Tracker using the frozen Coordinator's outputs. Both agents first undergo SFT warm-up to learn basic roles and output formats. The Coordinator (Qwen2.5-VL-7B) takes user instruction, current state from State Tracker, and screenshot to output atomic instructions. The State Tracker (Qwen3-4B) takes user instruction, previous state, and Executor output to produce natural language state summaries. Evaluation uses Type/GR/SR metrics on AITZ, AMEX, and GUI-Odyssey benchmarks.

## Key Results
- CES achieves up to 18.09% improvement in task success rate on GUI-Odyssey benchmark
- Planning Error reduced from 12% to 4% and State Loss reduced from 14% to 2% compared to single-agent baselines
- Strong generalization demonstrated across different Executor models (GUI-R1-7B, GUI-Owl-7B, UI-R1-3B)
- Performance bottleneck shifts to Executor's perceptual limitations rather than high-level planning errors

## Why This Works (Mechanism)

### Mechanism 1: Architectural Decoupling via Role Specialization
- Claim: Separating high-level planning from low-level execution reduces capability conflicts that degrade single-agent performance in long-horizon tasks.
- Mechanism: The CES framework assigns three distinct roles—Coordinator (planning), Executor (action), State Tracker (memory)—preventing a single model from needing to master heterogeneous capabilities simultaneously.
- Core assumption: Capability conflicts scale with task complexity; smaller models suffer more from coupling than larger models.
- Evidence anchors:
  - [abstract] "single-agent models struggle to balance high-level capabilities and low-level execution capability, facing prevalent issues of responsibility coupling and capability conflicts"
  - [section 4.2] "Through this OS-inspired design, we completely separate high-level strategic planning from low-level precise execution"
  - [corpus] Hi-Agent (arXiv:2510.14388) similarly uses hierarchical decomposition for mobile control, suggesting convergent evidence for architectural separation in long-horizon tasks.

### Mechanism 2: Staged Execution-Feedback Reinforcement Learning
- Claim: Training high-level agents using feedback from a frozen Executor produces specialized planning policies that generalize across different executors.
- Mechanism: Stage 1 trains the Coordinator using ground-truth state annotations; Stage 2 trains the State Tracker using the frozen Coordinator's outputs. Both stages use rule-based rewards derived from Executor action correctness (type + parameters).
- Core assumption: Executor feedback provides a sufficient learning signal for high-level agents; the frozen Executor's errors are not reinforced.
- Evidence anchors:
  - [abstract] "freezes a pre-trained Executor model and uses its feedback rewards to optimize the Coordinator and State Tracker"
  - [section 4.3.2] "This reward signal, originating from the final execution, is back-propagated to optimize the policies of the Coordinator and State Tracker"

### Mechanism 3: Dynamic Context Compression via State Tracker
- Claim: Maintaining high-semantic natural language state summaries prevents progress loss in long-horizon tasks where screenshots alone are insufficient.
- Mechanism: The State Tracker compresses Executor outputs and previous state into a concise natural language summary, which the Coordinator uses for planning—replacing reliance on raw screenshots or action histories.
- Core assumption: Natural language state representations carry more task-relevant information than visual screenshots or low-level action tokens.
- Evidence anchors:
  - [abstract] "State Tracker for dynamic context compression and state summarization"
  - [section 3] Preliminary experiment shows temporal judgment accuracy drops dramatically as screenshot step interval increases, empirically demonstrating screenshot insufficiency

## Foundational Learning

- **Markov Decision Processes (MDPs)**
  - Why needed here: The paper formalizes GUI tasks as MDPs (Section 4.1) with state space S, action space A, transition function F, and reward function R. Understanding this formulation is essential for grasping how RL optimization applies.
  - Quick check question: Can you explain why a GUI task trajectory can be modeled as a sequence of (state, action, reward) tuples?

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO is the core RL algorithm used to train both Coordinator and State Tracker (Equation 4). It samples N candidates, computes relative advantages, and applies clipped policy updates with KL regularization.
  - Quick check question: How does GRPO differ from standard PPO in how it computes advantages?

- **Supervised Fine-Tuning (SFT) Warm-up**
  - Why needed here: Before RL, both agents undergo SFT warm-up (Section 4.3.1) to learn basic roles and output formats. This stabilizes subsequent RL training.
  - Quick check question: Why might skipping SFT warm-up cause instability during RL optimization?

## Architecture Onboarding

- **Component map**: User instruction → Coordinator (with State Tracker output + screenshot) → Executor → State Tracker (with previous state + Executor output) → updated state summary → back to Coordinator

- **Critical path**:
  1. User provides high-level instruction
  2. Coordinator generates atomic instruction using current state + screenshot
  3. Executor executes atomic instruction, produces action
  4. State Tracker observes Executor output, updates state summary
  5. Loop back to step 2 until task complete

- **Design tradeoffs**:
  - **Frozen Executor**: Enables plug-and-play compatibility but perception errors persist (Figure 5 shows Perception Error unchanged at ~12%)
  - **Staged training**: Simplifies optimization but requires ground-truth state annotations for Stage 1 (Section 4.3.2, "sourced directly from the ground-truth annotated states")
  - **Natural language state**: Human-interpretable but may lose fine-grained spatial information

- **Failure signatures**:
  - **State Loss** (reduced from 14% → 2% with CES): Agent loses track of progress, repeats actions
  - **Planning Error** (reduced from 12% → 4%): Coordinator issues incorrect or incoherent instructions
  - **Perception Error** (~12% unchanged): Executor misidentifies UI elements—cannot be fixed by high-level training

- **First 3 experiments**:
  1. **Baseline comparison**: Run Executor alone on AITZ/AMEX/GUI-Odyssey benchmarks to establish Type/GR/SR baselines
  2. **CES-P ablation**: Use same base model for all three roles via prompting only (no specialized training) to isolate architectural vs. training contributions
  3. **Cross-executor generalization**: Train CES with GUI-R1-7B as Executor, then swap in GUI-Owl-7B or UI-R1-3B to verify plug-and-play capability (Table 2)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can synergetic evolution and joint training of the multi-agent system outperform the current staged, frozen-executor approach?
- Basis in paper: [explicit] The conclusion states: "In the future, synergetic evolution and joint training of multi-agent system for GUI tasks may be a promising direction."
- Why unresolved: The current methodology strictly decouples training into two stages and freezes the Executor to stabilize optimization of high-level agents.
- What evidence would resolve it: Experimental results comparing the current staged RL performance against an end-to-end joint training regime where all agents update simultaneously.

### Open Question 2
- Question: How can the framework mitigate the performance bottleneck caused by the perceptual limitations of the frozen Executor?
- Basis in paper: [inferred] The analysis notes that "The performance bottleneck has now effectively shifted to the inherent perceptual limitations of the Executor itself."
- Why unresolved: The framework successfully reduces high-level errors (Planning, State Loss), leaving low-level grounding errors as the primary remaining failure mode.
- What evidence would resolve it: A mechanism allowing high-level feedback to fine-tune the Executor's visual grounding, or results integrating the framework with a state-of-the-art, adaptable Executor.

### Open Question 3
- Question: Does the reliance on ground-truth state annotations for Stage 1 Coordinator training limit the framework's scalability?
- Basis in paper: [inferred] The methodology describes sourcing the task state "directly from the ground-truth annotated states in our preprocessed dataset" for Stage 1 optimization.
- Why unresolved: Using ground-truth states avoids the difficulty of bootstrapping from noisy predictions, but creates a dependency on manual annotations that may not exist for all domains.
- What evidence would resolve it: An ablation study where the Coordinator is trained using automatically generated or historical state summaries instead of ground-truth labels.

## Limitations

- Executor error propagation remains a significant bottleneck, with unchanged ~12% Perception Error rate suggesting systematic bias in high-level agent training
- State Tracker sufficiency is demonstrated only through single ablation; alternative context compression methods (action histories, visual embeddings) were not empirically compared
- Generalization beyond GUI domains is untested, leaving uncertainty about whether architectural benefits extend to non-visual long-horizon tasks

## Confidence

**High confidence**: CES framework improves long-horizon GUI task success rates over single-agent baselines; architectural separation effectively reduces responsibility coupling and state awareness problems; staged RL training with frozen Executor enables plug-and-play compatibility.

**Medium confidence**: Natural language state summaries are superior to screenshots for context compression; the 18.09% GUI-Odyssey improvement represents meaningful progress rather than dataset-specific optimization; training stability benefits from SFT warm-up are significant.

**Low confidence**: Executor feedback provides unbiased learning signals for high-level agents; the observed improvements stem primarily from architectural separation rather than model capacity differences; CES performance gains will scale proportionally with task horizon length.

## Next Checks

1. **Error propagation analysis**: Instrument CES training to track how Executor perception errors propagate through Coordinator and State Tracker updates. Compare reward distributions when Executor succeeds vs fails on atomic actions to quantify bias introduction.

2. **State compression ablation**: Replace the natural language State Tracker with alternative context methods: (a) pure action history concatenation, (b) visual screenshot embeddings via vision encoder, (c) hybrid approach combining both. Measure performance degradation to isolate State Tracker's contribution.

3. **Cross-domain generalization**: Apply CES architecture to non-GUI long-horizon tasks like web navigation (browser control) or robotic manipulation (simulation). Keep the same Coordinator-Executor-State Tracker roles but adapt action spaces. Compare whether architectural benefits persist without GUI-specific optimizations.