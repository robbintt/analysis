---
ver: rpa2
title: 'Bias in, Bias out: Annotation Bias in Multilingual Large Language Models'
arxiv_id: '2511.14662'
source_url: https://arxiv.org/abs/2511.14662
tags:
- bias
- annotation
- cultural
- annotator
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a framework for understanding and mitigating\
  \ annotation bias in multilingual LLMs. The authors identify three types of annotation\
  \ bias\u2014instruction bias, annotator bias, and contextual/cultural bias\u2014\
  and present detection methods including inter-annotator agreement, model disagreement,\
  \ and metadata analysis."
---

# Bias in, Bias out: Annotation Bias in Multilingual Large Language Models

## Quick Facts
- arXiv ID: 2511.14662
- Source URL: https://arxiv.org/abs/2511.14662
- Authors: Xia Cui; Ziyi Huang; Naeemeh Adel
- Reference count: 32
- Key outcome: Weak Ensemble Learning (WEL) achieves higher F1 scores and better calibration metrics than single-model baselines on three of four multilingual datasets when annotator pools exceed three annotators.

## Executive Summary
This paper proposes a framework for understanding and mitigating annotation bias in multilingual LLMs. The authors identify three types of annotation bias—instruction bias, annotator bias, and contextual/cultural bias—and present detection methods including inter-annotator agreement, model disagreement, and metadata analysis. They introduce Weak Ensemble Learning (WEL) as a reactive mitigation strategy, which samples one annotator label per instance to construct diverse training sets and combines weak predictors with weighted averaging. Empirical results on the LeWiDi 2023 multilingual benchmark show that WEL achieves higher F1 scores and better calibration metrics (lower cross-entropy and Manhattan distance) compared to single-model baselines and majority-vote ensembles across three of four datasets. The only exception was ArMIS, where the small annotator pool (three annotators) limited effectiveness, establishing a practical boundary for WEL's applicability.

## Method Summary
The authors propose Weak Ensemble Learning (WEL) to mitigate annotation bias by treating annotator disagreement as signal rather than noise. WEL samples one annotator label per instance to create K label-variant datasets, trains K weak predictors (each fine-tuned mBERT), and combines predictions via weighted averaging based on held-out performance. The framework was evaluated on four multilingual datasets from the LeWiDi 2023 benchmark (ArMIS, ConvAbuse, HS-Brexit, MD-Agreement) using F1, cross-entropy, and Manhattan distance metrics. Baseline comparisons include CE-only single models and Top-5-Ann majority vote ensembles.

## Key Results
- WEL achieved higher F1 scores and lower CE/MD than CE-only and Top-5-Ann baselines on 3/4 datasets (ConvAbuse, HS-Brexit, MD-Agreement)
- ArMIS showed minimal gains due to its small annotator pool (3 annotators), establishing WEL's practical lower bound
- The ensemble approach improved calibration metrics, reducing prediction instability (MD) compared to single-model CE baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Weak Ensemble Learning (WEL) mitigates annotation bias by treating annotator disagreement as signal rather than noise, preserving minority perspectives that majority voting suppresses.
- **Mechanism:** WEL samples one annotator label per instance to construct K label-variant datasets. Each dataset trains a weak predictor fθk, weighted by held-out performance. Final predictions are weighted averages: ŷi = Σ wk fθk(xi). This explicitly models the distribution of annotator interpretations rather than collapsing to a single "ground truth."
- **Core assumption:** Annotator disagreement reflects meaningful variation (cultural, contextual, or subjective) rather than annotation error.
- **Evidence anchors:**
  - [Section 7.2]: Formal definition of WEL with equation (15)
  - [Table 2]: WEL achieves higher F1 and lower CE/MD on 3/4 datasets vs. CE-only and Top-5-Ann baselines
  - [corpus]: Weak—no direct corpus support; related work focuses on cultural bias evaluation, not ensemble methods for disagreement
- **Break condition:** Small annotator pools limit effectiveness. On ArMIS (3 annotators), WEL shows minimal gains (F1: 0.6483 vs. 0.6482 CE-only), establishing a practical lower bound.

### Mechanism 2
- **Claim:** Annotation bias propagates through model training and produces measurable downstream harms via two pathways: performance disparities across demographic groups and representational harms (stereotype reinforcement).
- **Mechanism:** Biased annotations → model learns skewed associations → higher false positive rates for minoritized groups → violations of fairness metrics (Demographic Parity, Equalized Odds).
- **Core assumption:** Models internalize and potentially amplify patterns present in training labels.
- **Evidence anchors:**
  - [Section 4]: Links annotation bias to performance disparities and representational harms
  - [Section 5.1]: Cross-cultural hate speech case shows 42% F1 drop when US-trained models applied to Korean/Arabic data
  - [corpus]: CulFiT (arXiv:2505.19484) corroborates cultural bias in LLMs neglecting low-resource regions; MAKIEval addresses cross-lingual cultural awareness disparities
- **Break condition:** Culturally grounded taxonomies, diverse annotator pools, and iterative guideline refinement can interrupt propagation at source.

### Mechanism 3
- **Claim:** Metadata analysis and model disagreement metrics detect annotation bias by identifying systematic label patterns correlated with annotator demographics or cultural context.
- **Mechanism:** Demographic gap G(a) = |(1/|Da|)Σ y(x) - (1/|D|)Σ y(x)| measures deviation between group-specific and global labeling. Cultural inference Φcultural = ||φ(Da) - φ(Da')||2 quantifies embedding-space differences across annotator groups.
- **Core assumption:** Systematic labeling differences across groups indicate bias rather than legitimate perspective diversity.
- **Evidence anchors:**
  - [Section 6, Eq. 7-9]: Formal definitions of gap metrics and cultural inference
  - [Section 3]: Typology establishes that bias types compound (instruction + annotator + cultural)
  - [corpus]: SESGO (arXiv:2509.03329) provides Spanish-language stereotypical output evaluation; Grounding Multilingual Multimodal LLMs (arXiv:2508.07414) addresses cultural entity misinterpretation
- **Break condition:** In multilingual settings, disagreement may reflect valid cultural variation; treating it uniformly as "bias" risks erasing legitimate perspectival diversity.

## Foundational Learning

- **Inter-annotator agreement metrics (Cohen's κ, Fleiss' κ, Krippendorff's α)**
  - Why needed here: Section 6 uses these to quantify label consistency; understanding them is prerequisite for interpreting detection results.
  - Quick check question: If κ = 0.4, would you conclude high or low agreement? (Answer: Moderate-low agreement; κ ranges 0-1, with >0.6 typically considered substantial)

- **Ensemble learning fundamentals (bagging, weighted averaging)**
  - Why needed here: WEL extends ensemble principles to handle label uncertainty; Section 7.2 assumes familiarity with weak learner aggregation.
  - Quick check question: Why weight predictors by held-out performance rather than simple averaging? (Answer: Performance-based weighting downweights noisy or poorly-generalizing models)

- **Cross-cultural validity in measurement**
  - Why needed here: Sections 3.3 and 5 argue that constructs like "offensive" or "emotional" don't translate cleanly; calibrating expectations for cross-lingual generalization is essential.
  - Quick check question: A gesture signaling "patience" in Egypt and "confrontation" in Italy exemplifies which bias type? (Answer: Contextual and cultural bias)

## Architecture Onboarding

- **Component map:**
  1. **Data layer:** Multi-annotator datasets with per-instance label sets {yi(j)} (LeWiDi benchmark format)
  2. **Label sampling module:** Random selection of one annotator label per instance × K iterations
  3. **Weak learner pool:** K copies of mBERT, each fine-tuned on a label-variant dataset
  4. **Weighting engine:** Computes wk from held-out F1, cross-entropy, Manhattan distance
  5. **Aggregation layer:** Weighted average prediction ŷi = Σ wk fθk(xi)

- **Critical path:**
  1. Preprocess datasets (remove HTML, URLs, mentions; flatten multi-turn dialogues)
  2. Implement label sampling with configurable K (default: align with annotator count range)
  3. Train K mBERT instances with identical hyperparameters but different sampled labels
  4. Validate on held-out dev split to compute weights; apply to test predictions

- **Design tradeoffs:**
  - Higher K → better coverage of annotator variance, but increased compute cost
  - Lower K → faster training, but risk of undersampling minority perspectives
  - mBERT vs. language-specific models: mBERT enables cross-lingual consistency; language-specific models may capture nuance better
  - Weighting by F1 vs. CE/MD: F1 optimizes accuracy; CE/MD optimizes calibration and uncertainty estimation

- **Failure signatures:**
  1. **Small annotator pools (n ≤ 3):** Limited label variance → ensemble collapses to near-duplicate predictors (ArMIS pattern)
  2. **High MD values (>>1.0):** Prediction instability, particularly in CE-only baselines (ConvAbuse: 4.81)
  3. **Cross-entropy plateau:** If CE doesn't decrease across ensemble iterations, label sampling isn't generating meaningful diversity

- **First 3 experiments:**
  1. **Reproduce ArMIS failure mode:** Run WEL with K=3 on a held-out subset to confirm annotator pool threshold; then artificially augment annotator labels via perturbation to test if diversity alone improves performance.
  2. **Ablate weighting scheme:** Compare performance-based weights vs. uniform weights vs. entropy-based weights (H(A) from Eq. 10) to isolate contribution of weighting strategy.
  3. **Cross-lingual transfer test:** Train WEL on HS-Brexit (English hate speech), evaluate on ArMIS (Arabic misogyny) to assess whether ensemble diversity compensates for language/cultural mismatch.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the minimum annotator pool size required for Weak Ensemble Learning (WEL) to effectively mitigate annotation bias, and does this threshold vary across task types or languages?
- **Basis in paper:** [explicit] The authors state that WEL showed minimal gains on ArMIS due to its "very small annotator pool (three annotators)," establishing "a practical boundary condition: WEL requires sufficient annotator diversity (likely >3) for effective ensemble learning."
- **Why unresolved:** The paper only identifies that 3 annotators is insufficient but does not systematically test the lower bound across different datasets, task types, or linguistic contexts.
- **What evidence would resolve it:** Controlled experiments varying annotator pool sizes (e.g., 3, 5, 7, 10, 15) across multiple multilingual datasets and task domains, measuring F1 and calibration metrics at each configuration.

### Open Question 2
- **Question:** Can community-driven annotation in marginalized contexts produce measurably fairer multilingual models compared to standard crowdsourcing approaches?
- **Basis in paper:** [explicit] The conclusion states: "Future work should prioritise community-driven annotation in marginalised contexts, culturally grounded benchmarks, and richer annotator metadata to improve fairness diagnostics, particularly in low-resource settings."
- **Why unresolved:** The paper advocates for this approach but does not provide empirical comparison between community-driven annotation and conventional methods in terms of downstream fairness metrics or bias reduction.
- **What evidence would resolve it:** Comparative study where identical tasks are annotated via community-driven approaches (with local stakeholders) versus standard crowdwork, followed by evaluation using fairness metrics such as demographic parity and equalized odds across cultural groups.

### Open Question 3
- **Question:** How can LLMs be reliably deployed as scalable bias-detection tools while ensuring their outputs remain grounded in real-world cultural contexts rather than reproducing their own pretraining biases?
- **Basis in paper:** [explicit] The conclusion notes: "LLMs themselves can assist as scalable annotation and bias-detection tools, but must be guided by real-world social and cultural contexts."
- **Why unresolved:** The paper acknowledges the potential but does not specify mechanisms, validation protocols, or safeguards to prevent LLM-based detection from inheriting the very biases they are meant to identify.
- **What evidence would resolve it:** Framework development with human-in-the-loop validation, comparing LLM-detected biases against gold-standard human annotations across diverse cultural contexts, with analysis of false positive/negative rates by demographic group.

### Open Question 4
- **Question:** Do proactive strategies (diverse recruitment, iterative guidelines) yield better cost-benefit ratios than reactive strategies (WEL, post-hoc debiasing) for achieving equitable multilingual NLP systems?
- **Basis in paper:** [inferred] The paper presents both proactive and reactive strategies in parallel but does not compare their relative effectiveness, resource requirements, or practical feasibility in production annotation pipelines.
- **Why unresolved:** Without empirical comparison, practitioners lack guidance on whether to invest limited resources in preventing bias upfront or mitigating it post-hoc.
- **What evidence would resolve it:** Longitudinal study measuring bias reduction, annotation time, labor costs, and downstream model fairness across matched datasets using proactive-only, reactive-only, and combined approaches.

## Limitations
- WEL requires diverse annotator pools (>3 annotators) to generate meaningful label variance, limiting applicability to small-scale annotation projects
- The study uses only one base model (mBERT) and four datasets, constraining generalizability across model architectures and task types
- No analysis of annotator demographic characteristics prevents understanding whether specific group dynamics drive bias patterns
- The weighting mechanism adds complexity that may not scale to larger annotator pools or real-time deployment scenarios

## Confidence
- **High confidence**: WEL improves F1 and calibration metrics on datasets with sufficient annotator diversity (ConvAbuse, HS-Brexit, MD-Agreement)
- **Medium confidence**: The proposed typology of annotation bias (instruction, annotator, contextual/cultural) comprehensively captures observed phenomena
- **Low confidence**: The ArMIS failure mode represents a generalizable boundary condition for WEL applicability

## Next Checks
1. Test WEL with K=3 weak learners on HS-Brexit and ConvAbuse to confirm whether small annotator pools universally limit effectiveness across datasets
2. Implement an alternative weighting scheme using annotator demographic similarity (when available) to test whether demographic proximity predicts label agreement better than performance metrics
3. Compare WEL against a simple variance-based ensemble that weights predictors by label entropy rather than held-out performance to isolate the contribution of the weighting strategy