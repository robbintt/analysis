---
ver: rpa2
title: Decomposing Direct and Indirect Biases in Linear Models under Demographic Parity
  Constraint
arxiv_id: '2511.11294'
source_url: https://arxiv.org/abs/2511.11294
tags:
- bias
- linear
- fairness
- unfairness
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a post-processing framework that decomposes
  bias in linear models into direct (sensitive-attribute) and indirect (correlated-features)
  components under demographic parity constraints. The method provides closed-form
  expressions for group-conditional model coefficients and analytically characterizes
  how fairness constraints reshape them.
---

# Decomposing Direct and Indirect Biases in Linear Models under Demographic Parity Constraint

## Quick Facts
- arXiv ID: 2511.11294
- Source URL: https://arxiv.org/abs/2511.11294
- Reference count: 40
- This work introduces a post-processing framework that decomposes bias in linear models into direct (sensitive-attribute) and indirect (correlated-features) components under demographic parity constraints.

## Executive Summary
This paper presents a post-processing framework that decomposes bias in linear models into direct and indirect components under demographic parity constraints. The method provides closed-form expressions for group-conditional model coefficients and analytically characterizes how fairness constraints reshape them. It enables feature-level interpretation of fairness interventions and reveals how bias persists or shifts through correlated variables. The approach requires no retraining and offers actionable insights for model auditing.

## Method Summary
The method involves fitting any linear base model to obtain coefficients, computing group statistics for each sensitive group, and applying a closed-form transformation to enforce demographic parity. The key transformation uses ε-parameterized convex combinations of group-specific and population-averaged moments to interpolate between exact fairness and Bayes-optimal accuracy. The framework decomposes total unfairness using Wasserstein-2 distance into mean-based and structural components, enabling interpretable feature-level attribution of indirect bias.

## Key Results
- The method captures fairness dynamics missed by prior work, achieving 43-74% reductions in unfairness on benchmark datasets
- Maintains acceptable predictive performance with 15-20% R² tradeoff for substantial unfairness reduction
- Provides closed-form expressions for optimal fair predictors requiring no retraining
- Enables feature-level interpretation of fairness interventions through additive decomposition

## Why This Works (Mechanism)

### Mechanism 1: Post-Processing via Group-Wise Moment Alignment
Applying a closed-form transformation to any linear model's output can enforce demographic parity without retraining. The optimal fair predictor standardizes predictions within each group (zero mean, unit variance), then re-scales by convex combinations of group-specific and population-averaged moments. The parameter ε ∈ [0,1] interpolates between exact fairness (ε=0) and Bayes-optimal accuracy (ε=1).

### Mechanism 2: Wasserstein-Based Decomposition of Unfairness Sources
Total unfairness decomposes additively into first-moment disparity (mean bias) and second-moment disparity (structural bias). Using Wasserstein-2 distance, unfairness U(f) = Σs ps·W²(νf|s, ν) simplifies under Gaussian assumptions to Var(E[f|S]) + Var(√Var(f|S)), cleanly separating four sources: direct mean bias, indirect mean bias, their interaction, and indirect structural bias.

### Mechanism 3: Feature-Level Attribution via Linearization
Indirect unfairness can be approximately attributed to individual features through additive decomposition. A first-order Taylor expansion linearizes the structural bias term √Var(f|S) around the average conditional variance, yielding tractable per-feature contributions combining mean disparity, variance disparity, and interaction terms.

## Foundational Learning

- **Concept: Wasserstein-2 Distance and Barycenters**
  - Why needed here: The paper quantifies unfairness as distance between group-conditional prediction distributions and their Wasserstein barycenter (the "fair center"). Understanding this geometric view is essential for interpreting the decomposition.
  - Quick check question: Given two Gaussian distributions N(µ₁,σ₁²) and N(µ₂,σ₂²), can you compute their W₂ distance? (Answer: √[(µ₁−µ₂)² + (σ₁−σ₂²)])

- **Concept: Demographic Parity (Weak vs. Strong)**
  - Why needed here: The paper targets Strong DP (full distributional independence) while showing that satisfying only Weak DP (mean equality) leaves structural bias unaddressed.
  - Quick check question: If a predictor satisfies E[f|S=1] = E[f|S=2] but Var(f|S=1) ≠ Var(f|S=2), which form of DP is violated? (Answer: Strong DP only)

- **Concept: Group-Conditional Standardization**
  - Why needed here: The core transformation standardizes predictions within each sensitive group before re-scaling. This removes indirect mean and structural biases simultaneously.
  - Quick check question: After standardizing z(s)(x) = ⟨x−µ(s),β*⟩/σ(s)f*, what are E[z(s)(X)|S=s] and Var(z(s)(X)|S=s)? (Answer: 0 and 1)

## Architecture Onboarding

- **Component map:** Base Model Fitting -> Group Statistics Estimation -> Population Moment Aggregation -> Fair Predictor Assembly
- **Critical path:** Group statistics estimation → Population moments → ε-controlled interpolation. The ε parameter is your single fairness-performance knob.
- **Design tradeoffs:**
  - ε selection: Lower ε = more fairness, higher accuracy loss. Paper shows ε≈0.2–0.4 often achieves good tradeoffs
  - Including S in base model: Improves accuracy but increases direct bias that must be corrected; excluding S leaves only indirect bias (simpler correction but potential accuracy loss)
  - Diagonal vs. full covariance: Using diagonal Σ(s) simplifies feature-level attribution but ignores correlation-based structural bias
- **Failure signatures:**
  - Unfairness plateaus above zero: Check if group sizes are extremely imbalanced or if feature covariance structures differ dramatically across groups
  - GW R² << R² global: Model is capturing between-group variance that the fairness constraint suppresses—expected behavior, but large gaps indicate structural data issues
  - Feature attribution produces negative contributions: Interactional effects between correlated features are being misattributed; consider full (non-diagonal) covariance estimation
- **First 3 experiments:**
  1. Validate decomposition on synthetic data: Generate data with controlled T=(Ty, Tmean, Tstd, Tcorr), fit base model, apply decomposition—verify that increasing each T parameter increases its corresponding bias term
  2. Trace fairness-accuracy frontier: Vary ε ∈ [0,1] on held-out test data, plot R(f) vs. U(f) to verify the interpolation traces optimal tradeoff
  3. Compare to baselines on real data: Apply framework to CRIME, LAW, or GOSSIS datasets; compare unfairness reduction and accuracy retention against CS22 and FS23 baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the analytical characterization of the optimal fair regressor and the decomposition of unfairness be extended to generalized linear models or non-linear architectures?
- Basis in paper: [explicit] The conclusion states, "Future work may extend these insights to non-linear models and broader fairness notions."
- Why unresolved: The current mathematical derivation relies on the properties of linear projections of Gaussian random variables to derive closed-form Wasserstein barycenters.
- What evidence would resolve it: A theoretical derivation of bias decomposition for a kernel regressor or an empirical study validating the coefficient adjustment mechanism in non-linear models.

### Open Question 2
- Question: What are the theoretical bounds on the estimation error when the conditional feature distributions significantly deviate from the Gaussian assumption?
- Basis in paper: [inferred] The paper relies on the assumption $X|S=s \sim \mathcal{N}(\mu^{(s)}, \Sigma^{(s)})$ for all theoretical derivations (Proposition 5), though it claims "strong empirical robustness" without theoretical guarantees for non-Gaussian data.
- Why unresolved: While empirical results are provided, there is no theoretical analysis quantifying the performance degradation or bias in the decomposition under model misspecification.
- What evidence would resolve it: Theorems bounding the error of the plug-in estimator under relaxed distributional assumptions (e.g., sub-Gaussian or bounded data).

### Open Question 3
- Question: How does the magnitude of the "interactional unfairness" term, currently neglected in the approximate decomposition, impact the fidelity of feature-level bias attribution?
- Basis in paper: [inferred] Proposition 8 approximates indirect unfairness by neglecting cross-terms (interactional unfairness) to provide an "additive" feature-level breakdown, but the paper does not quantify the error introduced by this approximation.
- Why unresolved: The paper acknowledges the existence of interactional terms in the general case but does not analyze scenarios where this approximation fails, such as in datasets with extremely high feature correlation.
- What evidence would resolve it: An ablation study comparing the accuracy of the additive approximation against the exact decomposition across varying degrees of feature correlation.

## Limitations
- The framework's reliance on Gaussian assumptions for both features and prediction distributions may limit its applicability to non-Gaussian data
- The additive approximation for indirect unfairness attribution is incomplete, omitting interaction terms that can be substantial when features are correlated across groups
- No theoretical analysis quantifies performance degradation or bias in the decomposition under model misspecification

## Confidence
- **High Confidence**: The post-processing mechanism for achieving demographic parity (Proposition 5) is mathematically sound and empirically validated
- **Medium Confidence**: The Wasserstein-2 decomposition into mean and structural bias components works well under Gaussian assumptions but may not generalize to heavy-tailed or multi-modal distributions
- **Medium Confidence**: Feature-level attribution through first-order Taylor expansion provides useful insights but is an approximation that can misattribute bias when correlational structures differ substantially across groups

## Next Checks
1. **Non-Gaussian Robustness Test**: Apply the framework to datasets with known non-Gaussian distributions (e.g., count data, binary features) and compare decomposition accuracy against ground-truth bias sources
2. **Correlation Structure Sensitivity**: Generate synthetic data with varying levels of within-group feature correlation and between-group covariance differences to quantify how interaction terms affect attribution accuracy
3. **Alternative Distance Metrics**: Replicate key experiments using alternative fairness metrics (e.g., Kolmogorov-Smirnov test, maximum mean discrepancy) to assess the sensitivity of conclusions to the Wasserstein-2 distance choice