---
ver: rpa2
title: Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures
arxiv_id: '2506.01197'
source_url: https://arxiv.org/abs/2506.01197
tags:
- features
- feature
- h-sae
- sparse
- hierarchical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem that standard sparse autoencoders
  (SAEs) do not capture or represent the hierarchical structure of semantic concepts,
  leading to feature splitting and reduced interpretability as model size increases.
  The authors propose a hierarchical SAE architecture that explicitly models semantic
  hierarchy by combining a top-level SAE for high-level concepts with expert-specific
  low-level SAEs operating in lower-dimensional subspaces.
---

# Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures

## Quick Facts
- **arXiv ID**: 2506.01197
- **Source URL**: https://arxiv.org/abs/2506.01197
- **Reference count**: 24
- **Primary result**: Hierarchical SAE architecture improves reconstruction performance while reducing feature splitting in sparse autoencoders

## Executive Summary
This paper addresses a fundamental limitation in sparse autoencoders (SAEs): their inability to capture hierarchical semantic relationships, leading to feature splitting as model size increases. The authors propose a hierarchical SAE architecture that explicitly models semantic hierarchy by combining a top-level SAE for high-level concepts with expert-specific low-level SAEs operating in lower-dimensional subspaces. This mixture-of-experts design ensures low-level features are only activated when their corresponding high-level feature is active. Experiments on Gemma 2-2B internal representations demonstrate significant improvements in reconstruction performance while maintaining or improving interpretability, with negligible computational overhead compared to standard SAEs.

## Method Summary
The hierarchical SAE architecture introduces a top-level SAE operating on the full representation space to capture high-level semantic concepts. Each active top-level feature then gates a corresponding low-level SAE operating in a reduced-dimensional subspace, creating a mixture-of-experts structure. This design ensures that fine-grained features are only activated when their parent high-level concept is active, preventing feature splitting and redundancy. The architecture maintains computational efficiency by keeping low-level SAEs in lower-dimensional spaces while capturing richer semantic hierarchies. The approach is evaluated on Gemma 2-2B using reconstruction metrics and interpretability assessments.

## Key Results
- Hierarchical SAE significantly improves reconstruction performance measured by 1-CE loss and 1-explained variance compared to standard SAEs
- The architecture reduces feature splitting and redundant cross-language features while maintaining or improving interpretability
- Computational overhead is negligible relative to standard SAEs, enabling scalable representation of fine-grained concepts

## Why This Works (Mechanism)
The hierarchical design works by explicitly modeling the semantic hierarchy present in language model representations. Standard SAEs struggle because they must represent both coarse and fine-grained concepts in the same feature space, leading to either overly broad features or feature splitting where semantically related concepts are represented by separate features. By separating the modeling of high-level and low-level concepts, the hierarchical approach allows each level to specialize in its appropriate granularity. The gating mechanism ensures that low-level features are contextually relevant to their parent concept, reducing spurious activations and improving both reconstruction accuracy and interpretability.

## Foundational Learning
- **Sparse Autoencoders (SAEs)**: Neural networks trained to reconstruct input data with sparsity constraints on hidden layers. Needed to understand the baseline architecture being improved. Quick check: Verify SAE reconstruction loss converges and features are sparse (typically <5% activation rate).
- **Feature Splitting**: Phenomenon where semantically related concepts are represented by separate features in SAEs. Needed to understand the core problem being addressed. Quick check: Count feature co-activation patterns and measure semantic similarity between top-activating inputs.
- **Mixture-of-Experts**: Architecture where different sub-models (experts) are specialized for different inputs. Needed to understand the gating mechanism. Quick check: Verify gating function properly activates exactly one expert per top-level feature.
- **Reconstruction Metrics**: Quantitative measures of how well the autoencoder reconstructs input data. Needed to evaluate performance objectively. Quick check: Compare 1-CE loss and explained variance across different SAE variants on held-out data.
- **Interpretability in SAEs**: The degree to which individual features correspond to human-understandable concepts. Needed to assess practical utility. Quick check: Manually annotate top-activating inputs for sample features and measure inter-annotator agreement.

## Architecture Onboarding

**Component Map**: Input -> Top-level SAE -> Gating Function -> Low-level SAEs -> Reconstruction

**Critical Path**: The critical path flows from input through the top-level SAE, gating function, activated low-level SAE, and finally to reconstruction. The top-level SAE identifies semantic categories, the gating function selects appropriate low-level experts, and the low-level SAEs provide fine-grained reconstruction within those categories.

**Design Tradeoffs**: The hierarchical approach trades increased architectural complexity for better semantic modeling and reduced feature splitting. The main tradeoff is between the number of low-level SAEs (one per top-level feature) and the dimensionality reduction factor. Too few low-level SAEs limit granularity, while too many increase computational cost. The gating mechanism adds a dependency that could propagate errors from top-level to low-level features.

**Failure Signatures**: 
- Poor top-level SAE performance leading to incorrect gating and poor low-level reconstruction
- Insufficient dimensionality reduction causing computational inefficiency or feature collapse
- Overly sparse low-level features that cannot capture necessary detail
- Feature splitting persisting at the low level despite hierarchical structure
- Gating function becoming too permissive, activating multiple experts simultaneously

**First Experiments**:
1. Train a baseline SAE on Gemma 2-2B representations and measure reconstruction loss and feature interpretability
2. Implement the hierarchical SAE with varying numbers of top-level features and dimensionality reduction factors
3. Compare feature activation patterns and co-occurrence statistics between baseline and hierarchical SAEs

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Experimental validation is limited to a single model (Gemma 2-2B), restricting generalizability across different architectures and scales
- Evaluation focuses primarily on reconstruction metrics without comprehensive human evaluation of interpretability, despite it being a key claimed benefit
- Computational efficiency claims lack detailed analysis of training time and resource requirements compared to standard SAEs

## Confidence
- **High confidence**: The hierarchical SAE architecture design and its implementation details are well-specified and technically sound
- **Medium confidence**: The reconstruction performance improvements are demonstrated but may not fully translate to improved interpretability
- **Low confidence**: Claims about reduced feature splitting and cross-language redundancy are based on qualitative observations without systematic quantification

## Next Checks
1. Evaluate the hierarchical SAE architecture across multiple model families (LLaMA, Mistral, Claude) and scales (1B-70B parameters) to assess generalizability
2. Conduct comprehensive human evaluation studies comparing feature interpretability between hierarchical and standard SAEs using standardized annotation protocols
3. Perform ablation studies isolating the contribution of hierarchical structure versus increased capacity (more features) to performance improvements