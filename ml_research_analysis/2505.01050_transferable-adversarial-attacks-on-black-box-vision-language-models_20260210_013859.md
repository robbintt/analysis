---
ver: rpa2
title: Transferable Adversarial Attacks on Black-Box Vision-Language Models
arxiv_id: '2505.01050'
source_url: https://arxiv.org/abs/2505.01050
tags:
- image
- arxiv
- attack
- adversarial
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive analysis of adversarial attacks
  on Vision-Language Models (VLLMs), demonstrating that targeted adversarial examples
  can effectively transfer to widely-used proprietary models like GPT-4o, Claude,
  and Gemini. The authors develop a method to craft perturbations that induce specific
  attacker-chosen misinterpretations of visual information, such as misclassifying
  hazardous content as safe or overlooking sensitive material.
---

# Transferable Adversarial Attacks on Black-Box Vision-Language Models

## Quick Facts
- arXiv ID: 2505.01050
- Source URL: https://arxiv.org/abs/2505.01050
- Reference count: 24
- Primary result: Targeted adversarial examples can transfer to GPT-4o, Claude, and Gemini with >90% success rates

## Executive Summary
This paper presents a comprehensive analysis of adversarial attacks on Vision-Language Models (VLLMs), demonstrating that targeted adversarial examples can effectively transfer to widely-used proprietary models like GPT-4o, Claude, and Gemini. The authors develop a method to craft perturbations that induce specific attacker-chosen misinterpretations of visual information, such as misclassifying hazardous content as safe or overlooking sensitive material. They also discover that universal perturbations can consistently induce these misinterpretations across multiple proprietary VLLMs. Experiments on object recognition, visual question answering, and image captioning show that this vulnerability is common across current state-of-the-art models.

## Method Summary
The authors develop a method to craft targeted adversarial perturbations that induce specific misinterpretations of visual information in VLLMs. Their approach leverages a transferability-based attack strategy, where perturbations optimized against a source model can successfully attack black-box proprietary models. The method focuses on creating perturbations that can cause VLLMs to misclassify hazardous content as safe or overlook sensitive material, demonstrating high attack success rates across multiple proprietary models including GPT-4o, Claude, and Gemini.

## Key Results
- Targeted adversarial examples achieve >90% success rate on GPT-4o under medium and large perturbation bounds
- Universal perturbations consistently induce misinterpretations across multiple proprietary VLLMs
- Vulnerability demonstrated across object recognition, visual question answering, and image captioning tasks

## Why This Works (Mechanism)
The success of these attacks stems from the transferability of adversarial perturbations across different VLLM architectures. The perturbations exploit shared vulnerabilities in how these models process and interpret visual information, allowing attacker-crafted examples to generalize across proprietary black-box systems without requiring direct access to their internal parameters.

## Foundational Learning
- **Vision-Language Model (VLLM)**: Models that process both visual and textual inputs simultaneously
  - Why needed: Understanding the target of attacks
  - Quick check: Can you name examples like GPT-4o, Claude, Gemini?
- **Adversarial Perturbations**: Small, carefully crafted modifications to input data that cause model misclassification
  - Why needed: The core attack mechanism
  - Quick check: Do you understand how tiny changes can fool AI models?
- **Transferability**: The ability of adversarial examples crafted on one model to successfully attack different models
  - Why needed: Enables black-box attacks on proprietary systems
  - Quick check: Can you explain why attacks on one model might work on another?
- **Universal Perturbations**: Fixed perturbations that cause misclassification across multiple inputs
  - Why needed: For scalable, consistent attack deployment
  - Quick check: Do you understand how one perturbation can affect many images?
- **Black-box Attacks**: Attacks that don't require access to the target model's internal parameters
  - Why needed: Most real-world VLLMs are proprietary black boxes
  - Quick check: Can you differentiate black-box from white-box attacks?

## Architecture Onboarding
- **Component Map**: Image input → Vision encoder → Cross-modal fusion → Text generation
- **Critical Path**: Perturbation generation → Transfer learning → Attack deployment
- **Design Tradeoffs**: Targeted vs. universal perturbations, perturbation magnitude vs. stealth
- **Failure Signatures**: Consistent misinterpretation of visual content across multiple models
- **First Experiments**: 1) Test perturbation transferability across different VLLM families, 2) Measure attack success under varying perturbation bounds, 3) Evaluate robustness against input transformations

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several emerge from the findings regarding the generalizability of universal perturbations and the need for comprehensive defense strategies.

## Limitations
- Limited evaluation of false positive rates and unintended model behaviors
- Focus on specific attack objectives may not capture full spectrum of vulnerabilities
- Does not explore relationship between perturbation magnitude and attack success across different model families

## Confidence
- **High confidence**: The methodology for crafting targeted adversarial perturbations is well-defined and reproducible
- **Medium confidence**: The reported attack success rates are likely accurate within the tested constraints, though real-world deployment scenarios may differ
- **Low confidence**: The generalizability of universal perturbations across the broader VLLM ecosystem remains uncertain

## Next Checks
1. Evaluate attack transferability across a broader range of VLLM architectures, including open-source models with varying pretraining strategies
2. Test the persistence of adversarial effects under different input transformations (scaling, rotation, compression)
3. Assess whether ensemble-based adversarial training can reduce attack success rates without significantly impacting benign performance