---
ver: rpa2
title: 'From Confusion to Clarity: ProtoScore -- A Framework for Evaluating Prototype-Based
  XAI'
arxiv_id: '2511.08361'
source_url: https://arxiv.org/abs/2511.08361
tags:
- data
- prototype
- prototypes
- methods
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProtoScore, a framework for evaluating prototype-based
  eXplainable AI (XAI) methods. The framework addresses the lack of standardized benchmarks
  for comparing prototype-based XAI methods, particularly for time series data.
---

# From Confusion to Clarity: ProtoScore -- A Framework for Evaluating Prototype-Based XAI

## Quick Facts
- arXiv ID: 2511.08361
- Source URL: https://arxiv.org/abs/2511.08361
- Reference count: 40
- Primary result: ProtoScore framework introduces standardized metrics for evaluating prototype-based XAI methods, achieving highest total score of 0.76 with MAP on SAWSINE and Wafer datasets

## Executive Summary
ProtoScore addresses the critical gap in standardized evaluation for prototype-based explainable AI methods, particularly for time series data. The framework defines nine quality metrics based on the Co-12 properties framework, enabling quantitative comparison of prototype methods like MAP and MSP. By clustering data in latent space and measuring prototype-to-cluster alignment, ProtoScore provides a comprehensive assessment that helps practitioners select appropriate explanations while minimizing costly user studies.

## Method Summary
ProtoScore requires a trained prototype model (autoencoder + classifier) and dataset to evaluate prototype quality through nine specific metrics. The framework encodes input data into latent space, applies k-means clustering optimized via Silhouette score, and assigns each prototype to its nearest cluster. It then computes property-specific metrics (Correctness, Consistency, Continuity, etc.) that output values from 0 to 1, with the total score being the equal-weighted average of all nine metrics.

## Key Results
- ProtoScore successfully differentiates between MAP and MSP prototype methods across multiple datasets
- MAP method achieved highest total score of 0.76 on SAWSINE and Wafer datasets
- Individual property scores reveal method-specific strengths (e.g., MSP excels at Continuity while MAP shows better Input Completeness)
- Framework demonstrates sensitivity to dataset characteristics and model quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ProtoScore evaluates prototype quality by clustering data in latent space and measuring prototype-to-cluster alignment
- Mechanism: The framework encodes input data into a latent representation via the model's encoder function (f), then applies k-means clustering optimized via Silhouette score to identify natural data groupings. Each prototype is assigned to its nearest cluster centroid, and quality is assessed by comparing prototype positions against cluster statistics (centroids, intra-cluster distances).
- Core assumption: The latent space meaningfully preserves semantic relationships from the input space, such that proximity in latent space corresponds to similarity in the original domain.
- Evidence anchors:
  - [section] Section 3.1 defines the clustering process: "The clustering of data points in the latent space is optimized by minimizing the average Silhouette score over all data points z_i using k-means clustering. This clustering will be used as baseline for assessing the prototypes."
  - [section] Section 3.2.8 defines Input Completeness as the ratio of representative prototypes to clusters, where a prototype is "representative" if its distance to the cluster centroid is smaller than the average intra-cluster distance.
  - [corpus] Weak direct corpus support; related work on prototype-based XAI for geospatial tasks (arxiv 2602.00331) mentions intrinsic interpretability but does not address evaluation benchmarking.
- Break condition: If the encoder produces a latent space where class separation is poor (low Cohesion of Latent Space score), cluster assignments become unreliable and downstream metrics may mislead.

### Mechanism 2
- Claim: Individual property metrics aggregate into a composite quality score by averaging with equal weighting
- Mechanism: Nine properties (Correctness, Consistency, Continuity, Contrastivity, Covariate Complexity, Compactness, Confidence, Input Completeness, Cohesion of Latent Space) each yield a normalized [0,1] score through property-specific computations (e.g., fidelity checks, distance measures, Silhouette scores). The total score averages all nine with equal weight, treating each dimension as equally important to prototype quality.
- Core assumption: Equal weighting appropriately balances the importance of different quality dimensions across diverse use cases and domains.
- Evidence anchors:
  - [section] Section 3.2 states: "All metrics provide values from 0 to 1, where 0 indicates poor prototype quality and 1 represents the highest quality."
  - [section] Algorithm 1 Step 7: "Compute a total score by averaging the individual scores with equal weighting."
  - [abstract] "ProtoScore integrates the Co-12 properties of Nauta et al. and defines applicable metrics for each property, specifically adapted for prototypes."
  - [corpus] No corpus papers address multi-metric aggregation strategies for XAI evaluation; this appears novel to ProtoScore.
- Break condition: If domain-specific priorities differ (e.g., healthcare prioritizing Correctness over Compactness), equal weighting may obscure critical trade-offs. The paper acknowledges this limitation in Section 5.2.

### Mechanism 3
- Claim: Robustness to noise is measured by perturbing inputs and tracking prototype assignment stability
- Mechanism: For the Continuity metric, Gaussian noise (σ = 5% of average sample range) is added to create D_noised. Each original and noised sample is mapped to its nearest prototype via Equation 6. The Continuity score computes the average Euclidean distance between prototype assignments for original vs. noised inputs, then normalizes via exponential decay.
- Core assumption: Small input perturbations should not cause large changes in prototype assignments if prototypes capture stable, meaningful patterns.
- Evidence anchors:
  - [section] Section 3.2.3: "We evaluate Continuity by introducing small Gaussian noise to the input dataset D, resulting in a noisy dataset D_noised."
  - [section] Section 4.3 outlier analysis validates this mechanism: "Continuity decreases by up to 3 percentage points, reflecting challenges in maintaining smooth prediction transitions with outliers."
  - [corpus] The meta-analysis on XAI-based decision support (arxiv 2504.13858) examines explanation effects on human performance but does not address robustness metrics.
- Break condition: If the noise level (5%) is inappropriate for the data scale or domain, Continuity scores may be either artificially inflated (noise too small) or deflated (noise too large relative to signal).

## Foundational Learning

- Concept: **Autoencoder latent spaces**
  - Why needed here: ProtoScore requires models with an encoder-decoder architecture. The encoder maps inputs to a lower-dimensional latent representation where prototypes reside; understanding what latent spaces preserve (and lose) is essential for interpreting metric outputs.
  - Quick check question: Given an autoencoder trained on ECG signals, would you expect the latent space to preserve temporal alignment, amplitude patterns, or both? How would this affect prototype interpretability?

- Concept: **Silhouette score for clustering validation**
  - Why needed here: Both the baseline clustering (k-means optimization) and the Covariate Complexity and Cohesion metrics use Silhouette scores. This metric quantifies both cluster cohesion (intra-cluster similarity) and separation (inter-cluster distance).
  - Quick check question: If a latent space yields a Silhouette score of 0.2 for a dataset, what does this suggest about class separability? How might this impact Input Completeness scores?

- Concept: **Prototype-based vs. attribution-based XAI**
  - Why needed here: ProtoScore is designed specifically for prototype methods (ante-hoc explanations using representative examples), not for post-hoc attribution methods (e.g., SHAP, LIME). Understanding this distinction prevents misapplying the framework.
  - Quick check question: A colleague suggests using ProtoScore to evaluate Grad-CAM explanations on an image classifier. Is this appropriate? Why or why not?

## Architecture Onboarding

- Component map:
  - **Input layer**: Trained prototype model (encoder f, decoder g, classifier h), dataset D with labels, learned prototypes P
  - **Latent space transformation**: Encoder maps each x_i → z_i (Equation 1)
  - **Clustering module**: k-means with Silhouette-optimized k (2-15 clusters tested), produces cluster set C and centroids M (Equation 5)
  - **Prototype assignment**: Each prototype p_i assigned to nearest cluster via Equation 6
  - **Metric computation layer**: Nine property-specific calculators (Section 3.2.1-3.2.9), each outputting [0,1] score
  - **Aggregation layer**: Equal-weight averaging → total score

- Critical path:
  1. Verify model has accessible latent space (autoencoder architecture required)
  2. Encode dataset → cluster in latent space → assign prototypes to clusters
  3. Compute all nine metrics in sequence (some depend on clustering outputs)
  4. Aggregate into total score; inspect individual metrics for trade-offs

- Design tradeoffs:
  - **Equal vs. weighted aggregation**: Paper uses equal weights for simplicity; domains may need custom weighting (e.g., healthcare: Correctness > Compactness)
  - **Euclidean vs. domain-specific distance**: Framework uses Euclidean distance for generality; time-series-specific metrics (DTW) could improve relevance but reduce cross-domain applicability
  - **Fixed vs. adaptive noise level**: Continuity uses 5% noise; this may under-/over-perturb depending on data scale

- Failure signatures:
  - **Underfitted models**: Low Cohesion of Latent Space score but potentially misleading improvements in other metrics due to loose cluster thresholds (Section 5.2)
  - **Zero Input Completeness**: Prototypes lie outside cluster radii for all classes (common for MSP on ECG200 in Table 2); suggests prototypes don't represent training distribution
  - **High validation loss but high total score**: Indicates well-constructed prototypes that don't generalize; use both metrics jointly

- First 3 experiments:
  1. **Baseline comparison on single dataset**: Train MAP and MSP models on ECG200 with identical hyperparameters, run ProtoScore, and compare total + individual property scores. Identify which method excels at which properties.
  2. **Noise sensitivity test**: Evaluate Continuity across multiple noise levels (1%, 5%, 10%) to determine if the default 5% is appropriate for your data scale.
  3. **Cross-dataset generalization**: Apply the same model architecture to Wafer and SAWSINE datasets. Compare whether property rankings (e.g., MAP vs. MSP on Correctness) remain consistent or vary by dataset, revealing method-dataset interactions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How strongly do ProtoScore's automated quantitative rankings correlate with human judgments of interpretability and explanation utility in real-world applications?
- Basis in paper: [explicit] "Future work should aim for refining these metrics and be completed by user-centric evaluations..."
- Why unresolved: The framework focuses on technical, automated evaluation to minimize user study costs, but the authors acknowledge that interpretability is inherently subjective and that the link between these metrics and actual human understanding remains unverified.
- Evidence: Empirical data from user studies comparing human rankings of prototype methods against ProtoScore total scores and individual property scores.

### Open Question 2
- Question: Does incorporating time series-specific distance metrics (e.g., Dynamic Time Warping) significantly alter the evaluation of prototype quality compared to using standard Euclidean distance?
- Basis in paper: [explicit] "Moreover, we neglected time series-specific metrics and only used the Euclidean distance... A valuable extension... is to allow users to choose from a range of metrics..."
- Why unresolved: The authors utilized Euclidean distance to ensure versatility across data types like images and time series, but this choice may fail to capture the distinct similarity features required for accurate time series analysis.
- Evidence: Comparative benchmarking results on time series datasets using both Euclidean and time-series-specific metrics within the ProtoScore framework.

### Open Question 3
- Question: How should the weights of individual ProtoScore properties be dynamically adjusted to optimize prototype selection for specific application domains (e.g., prioritizing Correctness in healthcare)?
- Basis in paper: [explicit] "Another enhancement could include dynamic weighting of properties based on user-defined priorities..."
- Why unresolved: The current framework calculates a total score using equal weighting, assuming all properties are equally important, which may not hold true for specialized domains where certain attributes like stability or correctness are critical.
- Evidence: A study establishing domain-specific weighting profiles and measuring the resulting improvement in selecting optimal prototype methods for those specific domains.

## Limitations

- The framework assumes equal weighting across all nine properties, which may not reflect domain-specific priorities (e.g., healthcare favoring Correctness over Compactness).
- The 5% Gaussian noise level for Continuity assessment is fixed and may be inappropriate for datasets with different scales or noise characteristics.
- The framework requires autoencoder architectures, limiting applicability to non-latent-space-based prototype methods.

## Confidence

- **High confidence**: The clustering-based evaluation mechanism (Mechanism 1) is well-grounded with clear mathematical definitions and appropriate normalization.
- **Medium confidence**: The equal-weight aggregation approach (Mechanism 2) is simple but may not capture domain-specific importance of different properties.
- **Medium confidence**: The noise-based Continuity metric (Mechanism 3) provides useful robustness insights but depends critically on appropriate noise scaling.

## Next Checks

1. Conduct sensitivity analysis by varying the noise level (1%, 5%, 10%) in Continuity assessment to determine optimal perturbation magnitude for your specific dataset scale.
2. Perform domain-specific weighting experiments by assigning custom weights to the nine properties based on application requirements (e.g., medical diagnosis vs. industrial monitoring).
3. Validate cross-method consistency by applying ProtoScore to multiple prototype methods (MAP, MSP, and alternative approaches) across all five datasets to identify whether property rankings remain stable or vary significantly by method-dataset combinations.