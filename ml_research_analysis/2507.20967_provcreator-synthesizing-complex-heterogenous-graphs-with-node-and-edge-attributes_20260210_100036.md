---
ver: rpa2
title: 'PROVCREATOR: Synthesizing Complex Heterogenous Graphs with Node and Edge Attributes'
arxiv_id: '2507.20967'
source_url: https://arxiv.org/abs/2507.20967
tags:
- graph
- graphs
- provcreator
- generation
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProvCreator, a synthetic graph generation
  framework that addresses the challenge of generating complex heterogeneous graphs
  with high-dimensional node and edge attributes. The core idea is to cast graph synthesis
  as a sequence generation task, leveraging transformer-based large language models
  (LLMs) through a graph-to-sequence encoder-decoder architecture.
---

# PROVCREATOR: Synthesizing Complex Heterogenous Graphs with Node and Edge Attributes

## Quick Facts
- arXiv ID: 2507.20967
- Source URL: https://arxiv.org/abs/2507.20967
- Reference count: 18
- Outperforms prior works on MMD scores and downstream program classification for synthetic provenance graphs

## Executive Summary
PROVCREATOR introduces a novel approach to generating complex heterogeneous graphs with high-dimensional node and edge attributes by framing graph synthesis as a sequence generation task. The framework leverages transformer-based LLMs through a graph-to-sequence encoder-decoder architecture, enabling joint modeling of structure and semantics. Evaluated on cybersecurity provenance graphs and knowledge graphs from the IntelliGraph Benchmark, PROVCREATOR demonstrates superior structural fidelity and retains substantial utility for downstream classification tasks.

## Method Summary
PROVCREATOR fine-tunes a transformer-based LLM (Llama 3-3.2B) to generate graph sequences by first converting input graphs into compact token sequences using a custom graph tokenizer. The tokenizer employs special tokens for graph boundaries, nodes, edges, and attributes, and orders edges based on timestamps or topological sort. After fine-tuning with LoRA and mixed-precision training on 4 GPUs, a constrained decoding process with token filtering and a state-machine parser reconstructs valid graphs from the generated sequences. The method jointly models structure and attributes in a single pass, preserving semantic dependencies.

## Key Results
- Achieves consistently lower Maximum Mean Discrepancy (MMD) scores across various graph metrics compared to prior works
- For firefox.exe and powershell.exe datasets, attains MMD scores of 0.277 and 1.109 respectively, outperforming GDSS baseline
- Retains 65-70% of real-data F1 score for program classification, indicating preserved class signal and cross-domain generalization
- Achieves high attribute validity rates: 93.77-98.70% valid process names and 96.43-98.92% valid IP addresses and ports

## Why This Works (Mechanism)
PROVCREATOR works by translating the graph synthesis problem into a sequence generation task that can be handled by large language models. By encoding graphs into compact token sequences with a carefully designed tokenizer and leveraging the generative capabilities of transformers, the framework can capture both structural patterns and attribute semantics. The joint modeling approach allows the model to learn dependencies between graph topology and node/edge attributes, while the constrained decoding with token filtering ensures the generated sequences adhere to valid graph grammar and produce usable output.

## Foundational Learning
- **Graph-to-sequence encoding:** Converts graph structure into linear token sequences; needed to leverage LLM's sequence modeling capabilities
- **Custom tokenization with special tokens:** Uses <bog>, <eog>, <bon>, <eon>, <boe>, <nodei>, <ntypej>, <etypek>, <bof>, <eof> to represent graph elements; needed to preserve structural and semantic information compactly
- **Time-based/topological edge ordering:** Establishes a well-defined sequence for edge tokens; needed to maintain consistency and enable autoregressive generation
- **LoRA fine-tuning with mixed-precision:** Enables efficient adaptation of large LLM to graph generation task; needed to handle limited computational resources while maintaining performance
- **Constrained decoding with state-machine parser:** Filters invalid tokens and reconstructs graphs from sequences; needed to ensure generated output adheres to valid graph grammar

## Architecture Onboarding

- **Component map:** Graph Tokenizer -> Fine-tuned LLM -> Token-Filtering & Decoding Parser
- **Critical path:** The most critical and novel component is the Graph Tokenizer and its corresponding Decoder. Their implementation must be perfectly symmetric. A bug in the tokenizer's edge-ordering logic will produce a dataset that the LLM cannot learn, and a bug in the decoder's state machine will cause it to fail to reconstruct valid graphs from the LLM's output.
- **Design tradeoffs:**
    - **Representation Fidelity vs. Context Window:** The paper's custom tokens are designed to be extremely compact to fit more of the graph into the LLM's limited context. A simpler text-based representation (like JSON) would be easier to debug but would fail on larger graphs.
    - **Joint vs. Separate Modeling:** Generating structure and attributes jointly in one pass is more complex but yields higher-fidelity graphs. A simpler two-stage approach (generate structure, then fill in attributes) would be easier to implement but would likely fail to capture the semantic dependencies the paper highlights.
- **Failure signatures:**
    - **Unparseable Output:** The generated token sequence frequently contains invalid transitions (e.g., an edge token followed by an attribute token without a beginning-of-node token). This indicates the LLM hasn't learned the grammar or the filtering rules are too permissive.
    - **Low Attribute Validity:** The model generates structurally correct graphs but the attributes are nonsensical (e.g., an invalid file path). This suggests the LLM is struggling to learn the specific patterns of the attribute data.
    - **Loss of Global Structure:** The model generates realistic local subgraphs but they fail to connect into a coherent larger graph. This is a classic failure mode of autoregressive models and indicates the context window or model capacity is being exceeded.
- **First 3 experiments:**
    1. **Tokenizer Unit Test:** Take a sample of your graph data, pass it through the tokenizer, and then immediately pass the result through the decoder. Assert that the input and output graphs are isomorphic with identical attributes. This validates the lossless claim.
    2. **Baseline Fidelity Test:** Train the LLM on a small dataset, generate 100 synthetic graphs, and compute a simple graph-level statistic (e.g., average degree) and compare it to the training set. This is a fast sanity check that the model is learning something meaningful.
    3. **Component Ablation:** Implement a baseline that generates graphs without the custom filtering/parser. Measure the percentage of generated sequences that can be successfully decoded into a valid graph. Compare this to the full system to quantify the impact of the robust decoding mechanism.

## Open Questions the Paper Calls Out
- **Privacy preservation:** The framework claims to enable "privacy-aware synthetic datasets," but evaluation focuses on structural fidelity and utility without privacy metrics. The authors mention releasing datasets "after applying privacy-enhancing techniques," suggesting privacy risks were not measured in the study.
- **Domain gap minimization:** While synthetic data retains 65-70% of real-data performance for program classification, accuracy declines when transferring models trained on synthetic data to real traces. The paper identifies this gap but does not propose specific mechanisms to align the synthetic distribution more closely with the real distribution.
- **Cyclic graph generation:** The methodology assumes a "well-defined order" for encoding, utilizing topological sort for general graphs. However, topological sorting is only possible for Directed Acyclic Graphs (DAGs), raising questions about how the framework handles dense cyclic graphs where no unique topological ordering exists.

## Limitations
- Heavy reliance on custom graph-to-sequence tokenization and decoding pipeline, where any implementation mismatch could silently degrade fidelity
- Attribute validity rates reported only for specific domains (process names, IP addresses), with generalization to other attribute types or domains untested
- Unspecified LoRA hyperparameters (rank, alpha, learning rate schedule) limit reproducibility and downstream performance
- Evaluation focuses on synthetic graph quality and a single downstream task (program classification), leaving real-world applicability and robustness questions open

## Confidence
**High Confidence:**
- Core idea of using LLMs for graph generation via sequence modeling is sound and well-supported by results
- Architecture components (tokenizer, LLM backbone, constrained decoding) are clearly described and logically consistent
- Structural fidelity (MMD) and attribute validity metrics are directly measurable and align with reported outcomes

**Medium Confidence:**
- Claim that ProvCreator "outperforms" GDSS and GraphTGen is supported, but GDSS is noted as weak and GraphTGen is not compared in main results table
- 65-70% F1 retention for program classification is promising but based on a single downstream task

**Low Confidence:**
- Generalization to other graph types or attribute domains is not demonstrated
- Reproducibility is limited by unspecified LoRA hyperparameters and absence of public provenance datasets

## Next Checks
1. **Tokenizer-Decoder Symmetry Test:** Implement a unit test that tokenizes a sample graph and immediately decodes it back, asserting isomorphism and attribute equality to validate the "lossless" claim.
2. **Ablation of Constrained Decoding:** Compare ProvCreator's full system to a version without token filtering and state-machine parsing, measuring percentage of valid generated graphs and attribute validity rates.
3. **Cross-Domain Generalization:** Apply ProvCreator to a different graph type (e.g., social networks or biological graphs) with distinct attribute types, evaluating MMD and attribute validity to test framework versatility.