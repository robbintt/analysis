---
ver: rpa2
title: 'STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding'
arxiv_id: '2510.14588'
source_url: https://arxiv.org/abs/2510.14588
tags:
- motion
- video
- control
- depth
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of maintaining coherent motion
  and physical plausibility in video generation, particularly for object interactions
  and trajectories. It identifies two bottlenecks: sparse, low-resolution control
  inputs that lose effectiveness after encoding, and trade-offs between appearance
  quality and motion consistency when both are optimized together.'
---

# STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding

## Quick Facts
- arXiv ID: 2510.14588
- Source URL: https://arxiv.org/abs/2510.14588
- Reference count: 14
- Primary result: Achieves Physics IQ of 47.62, outperforming strong baselines like SG-I2V (15.42), Drag-Anything (24.86), and VLIPP (36.40) on a 200k-clip dataset of rigid-object interactions.

## Executive Summary
This paper addresses the challenge of maintaining coherent motion and physical plausibility in video generation, especially for object interactions and trajectories. It identifies two main bottlenecks: sparse, low-resolution control inputs that lose effectiveness after encoding, and trade-offs between appearance quality and motion consistency when both are optimized together. The authors propose a solution that introduces Instance Cues, a pixel-aligned motion control that converts sparse user hints into a dense 2.5D motion field with depth awareness, and Dense RoPE, which preserves spatial identity and control strength after tokenization. Joint training with RGB and auxiliary structural guidance further stabilizes motion coherence. Evaluated on a curated dataset, the method achieves the highest Physics IQ score among tested models and demonstrates competitive perceptual realism.

## Method Summary
The method tackles motion coherence in video generation by converting sparse user hints (arrows, masks, mass) into dense, pixel-aligned motion controls via Instance Cues. These cues are transformed into a 2.5D motion field with depth awareness, resolving spatial ambiguities under camera motion. Dense RoPE further enhances control by selecting salient motion tokens and tagging them with first-frame rotary embeddings to maintain spatial identity and control strength after tokenization. The model is trained jointly with RGB and an auxiliary structural stream (depth or segmentation), which stabilizes motion prediction. Evaluated on a 200k-clip dataset of rigid-object interactions, the approach achieves superior Physics IQ and competitive FVD scores compared to strong baselines.

## Key Results
- Achieves Physics IQ of 47.62, outperforming SG-I2V (15.42), Drag-Anything (24.86), and VLIPP (36.40).
- Demonstrates competitive FVD scores, indicating perceptual realism.
- Shows clear quantitative improvements on a curated dataset of rigid-object interactions.

## Why This Works (Mechanism)
The method works by addressing two core bottlenecks in video generation: the loss of control effectiveness due to sparse inputs after encoding, and the trade-off between appearance and motion quality. Instance Cues convert sparse hints into a dense, pixel-aligned motion field with depth awareness, enabling better spatial disambiguation under camera motion. Dense RoPE preserves spatial identity and control strength after tokenization by selecting salient motion tokens and tagging them with first-frame rotary embeddings. Joint training with RGB and auxiliary structural guidance (depth or segmentation) further stabilizes motion prediction, ensuring both visual and physical coherence.

## Foundational Learning
- **Instance Cues**: Converts sparse user hints into dense motion controls; needed to maintain spatial and temporal coherence under camera motion. Quick check: Verify conversion quality by comparing sparse vs. dense motion field accuracy.
- **Dense RoPE**: Tags salient motion tokens with first-frame rotary embeddings; needed to preserve spatial identity and control strength after tokenization. Quick check: Measure token alignment accuracy before and after RoPE tagging.
- **Auxiliary structural stream**: Jointly trained with RGB for motion stability; needed to reinforce physical plausibility. Quick check: Compare motion coherence with and without structural guidance.

## Architecture Onboarding

### Component Map
Input (arrows, masks, mass) -> Instance Cues (2.5D motion field) -> Dense RoPE (salient tokens + rotary embeddings) -> Joint RGB + structural stream training -> Output (coherent video)

### Critical Path
Sparse control inputs → Instance Cues (pixel-aligned 2.5D motion field) → Dense RoPE (salient token selection + rotary embeddings) → Joint RGB + structural stream → Final video output.

### Design Tradeoffs
- **Instance Cues vs. Dense RoPE**: Instance Cues provides dense, pixel-aligned control but may introduce computational overhead; Dense RoPE optimizes token selection but could lose fine-grained motion details. Balancing precision and efficiency is critical.
- **Auxiliary stream choice**: Using depth vs. segmentation affects motion stability; depth is better for 3D-aware motion, segmentation for object boundaries. Choice depends on dataset characteristics.

### Failure Signatures
- Motion jitter or artifacts when control inputs are too sparse or ambiguous.
- Loss of object identity or trajectory consistency if rotary embeddings are misaligned.
- Degraded motion coherence if auxiliary structural guidance is weak or mismatched to the scene.

### First Experiments
1. Validate Instance Cues by comparing motion field accuracy for sparse vs. dense inputs.
2. Test Dense RoPE’s effect on token alignment and control strength after tokenization.
3. Assess the impact of auxiliary structural streams (depth vs. segmentation) on motion stability.

## Open Questions the Paper Calls Out
- How well does the method generalize to deformable objects or more complex, open-domain scenes?
- Can the Physics IQ metric be independently validated for broader applicability?
- What is the computational overhead introduced by Instance Cues and Dense RoPE?
- How does the method perform under varying input perturbations or failure modes?

## Limitations
- Limited to rigid-object interactions; generalization to deformable objects is unknown.
- Physics IQ metric is task-specific and lacks independent validation.
- No real-world or open-domain dataset evaluation.
- Computational overhead and failure cases are not addressed.

## Confidence
- Physics IQ and FVD improvements: **High**
- Instance Cues and Dense RoPE technical contributions: **Medium**
- Generalization beyond rigid-object interactions: **Low**

## Next Checks
1. Test generalization on deformable object datasets and open-domain video data.
2. Conduct ablation studies on auxiliary structural streams and dense RoPE to quantify individual contributions.
3. Benchmark against other video generation methods using a common, independent evaluation protocol.