---
ver: rpa2
title: 'Adapting Natural Language Processing Models Across Jurisdictions: A pilot
  Study in Canadian Cancer Registries'
arxiv_id: '2601.00787'
source_url: https://arxiv.org/abs/2601.00787
tags:
- cancer
- data
- ensemble
- registry
- pathology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This pilot study evaluated cross-provincial adaptation of NLP models
  for cancer registry workflows, demonstrating that transformer models developed at
  the British Columbia Cancer Registry can be effectively localized to Newfoundland
  & Labrador with modest fine-tuning. Using complementary synoptic- and diagnosis-focused
  preprocessing pipelines, the ensemble model achieved Tier 1 recall of 0.99 (reducing
  missed cancers to 24 cases) and Tier 2 recall of 0.99 (reducing missed reportable
  cancers to 33 cases), outperforming individual models.
---

# Adapting Natural Language Processing Models Across Jurisdictions: A pilot Study in Canadian Cancer Registries

## Quick Facts
- arXiv ID: 2601.00787
- Source URL: https://arxiv.org/abs/2601.00787
- Reference count: 25
- Primary result: Cross-provincial NLP model adaptation achieved Tier 1 recall of 0.99 and Tier 2 recall of 0.99, reducing missed cancers to 24 cases and missed reportable cancers to 33 cases.

## Executive Summary
This pilot study evaluated cross-provincial adaptation of NLP models for cancer registry workflows, demonstrating that transformer models developed at the British Columbia Cancer Registry can be effectively localized to Newfoundland & Labrador with modest fine-tuning. Using complementary synoptic- and diagnosis-focused preprocessing pipelines, the ensemble model achieved Tier 1 recall of 0.99 (reducing missed cancers to 24 cases) and Tier 2 recall of 0.99 (reducing missed reportable cancers to 33 cases), outperforming individual models. The privacy-preserving approach—sharing only model weights while training within provincial boundaries—enabled interoperable NLP infrastructure without cross-jurisdictional data transfer. These results validate the feasibility of adapting high-performing NLP models across Canadian cancer registries, supporting scalable, timely cancer surveillance and aligning with national efforts to modernize health data infrastructure.

## Method Summary
The study fine-tuned pre-trained transformer models (GatorTron and BCCRTron) on pathology reports from Newfoundland & Labrador Cancer Registry (NLCR), using two complementary pipelines: Pipeline A extracts synoptic sections for BCCRTron input, while Pipeline B extracts diagnosis sections for GatorTron input. Training data was undersampled to balance class distributions (T1: N(non-cancer) = N(cancer) × 0.8; T2: N(reportable) = N(non-reportable) × 1.2), yielding n=31,694 for T1 and n=7,821 for T2 tasks. Models were evaluated on 80/20 train/test splits, with predictions combined via OR-ensemble logic. Hardware included Intel Xeon processors, 64 GB RAM, and NVIDIA RTX 4090 GPU for fine-tuning.

## Key Results
- Ensemble model achieved Tier 1 recall of 0.99, reducing missed cancer cases from 122 to 24
- Tier 2 recall reached 0.99, decreasing missed reportable cancers from 91 to 33
- Privacy-preserving adaptation succeeded without cross-jurisdictional data transfer, using only shared model weights

## Why This Works (Mechanism)
The approach succeeds by leveraging pre-trained transformer models that capture medical language patterns, then fine-tuning them on jurisdiction-specific data while maintaining privacy through weight-only sharing. The dual-pipeline architecture addresses the variability in pathology report structures by processing both synoptic and diagnosis sections separately, allowing each model to specialize in different report formats commonly found across Canadian registries.

## Foundational Learning
- **Two-tier cancer classification**: Why needed - Canadian registries classify reports as cancer/non-cancer (Tier 1) and reportable/non-reportable (Tier 2); Quick check - Verify local registry uses identical classification schema
- **Transformer model fine-tuning**: Why needed - Pre-trained models provide medical language understanding that can be adapted to new jurisdictions; Quick check - Confirm pre-trained weights are compatible with target registry's report formats
- **Privacy-preserving model adaptation**: Why needed - Enables cross-provincial collaboration without violating data sovereignty regulations; Quick check - Ensure only model weights (not data) cross jurisdictional boundaries

## Architecture Onboarding

Component map: Pre-trained models (BCCRTron, GatorTron) -> Section extraction pipelines (synoptic/diagnosis) -> Undersampled training data -> Fine-tuning process -> OR-ensemble prediction -> Evaluation metrics

Critical path: Model weights → Section extraction → Fine-tuning → Ensemble prediction → Recall evaluation

Design tradeoffs: Ensemble methods provide higher recall but increase computational complexity; undersampling improves class balance but reduces training data size; privacy-preserving approach limits validation to single target registry.

Failure signatures: Low recall indicates insufficient fine-tuning or section extraction errors; poor precision suggests overfitting to training distribution; ensemble underperformance may indicate conflicting model predictions.

First experiments: 1) Validate section extraction rules on 50 random reports from target jurisdiction; 2) Test undersampling ratios on small subset to confirm class balance; 3) Evaluate individual model performance before ensemble combination.

## Open Questions the Paper Calls Out
- How does cross-jurisdictional model performance vary when adapting to registries with significantly different reporting conventions or languages compared to the BC and NL pilot?
- Can the proposed privacy-preserving framework be effectively scaled to a federated learning architecture for a pan-Canadian foundation model?
- To what extent does the ensemble's increased computational complexity hinder deployment in resource-constrained registry environments?

## Limitations
- Section extraction rules for parsing synoptic vs. diagnosis sections are vaguely described, creating potential variability across jurisdictions
- Validation limited to single adaptation scenario (BC to NL) without broader cross-registry testing
- Hyperparameter specifications are not provided, preventing exact replication of training conditions

## Confidence
- **High confidence**: Ensemble model achieves reported Tier 1 and Tier 2 recall rates on NLCR test data; privacy-preserving adaptation methodology is technically sound
- **Medium confidence**: Approach enables scalable cancer surveillance across Canadian registries (requires multi-jurisdiction validation); ensemble outperforms individual models for tested combinations
- **Low confidence**: Privacy-preserving approach "enables interoperable NLP infrastructure" (requires standardization beyond model weights)

## Next Checks
1. Apply the same adaptation methodology to pathology reports from at least two additional Canadian cancer registries with different reporting formats to assess generalizability
2. Systematically vary learning rates, batch sizes, and maximum sequence lengths around optimal values to determine hyperparameter robustness, particularly focusing on recall stability
3. Deploy the ensemble model in a production registry environment for 12+ months and track real-world performance metrics to validate laboratory-reported performance translates to operational effectiveness