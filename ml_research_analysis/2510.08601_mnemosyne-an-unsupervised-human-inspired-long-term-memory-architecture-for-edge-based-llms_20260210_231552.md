---
ver: rpa2
title: 'Mnemosyne: An Unsupervised, Human-Inspired Long-Term Memory Architecture for
  Edge-Based LLMs'
arxiv_id: '2510.08601'
source_url: https://arxiv.org/abs/2510.08601
tags:
- memory
- mnemosyne
- node
- core
- summary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mnemosyne is an unsupervised, graph-structured memory architecture
  for edge-based LLMs that models human-like forgetting and recall. It uses modular
  filters to manage memory intake, a graph-based storage system with temporal decay
  and boosting, and a core summary module for capturing long-term user traits.
---

# Mnemosyne: An Unsupervised, Human-Inspired Long-Term Memory Architecture for Edge-Based LLMs

## Quick Facts
- arXiv ID: 2510.08601
- Source URL: https://arxiv.org/abs/2510.08601
- Reference count: 39
- Primary result: 65.8% win rate in blind human tests vs 31.1% for naive RAG baseline

## Executive Summary
Mnemosyne introduces a graph-structured memory architecture designed for edge-based LLMs that mimics human-like forgetting and recall. It operates efficiently on constrained devices by selectively managing memory intake and retrieval, avoiding the computational overhead of long-context or RAG approaches. The system uses modular filters, temporal decay, and a core summary module to maintain a compact, relevant memory store. Evaluations show strong performance in both human preference tests and benchmark reasoning tasks, particularly in temporal reasoning and single-hop retrieval.

## Method Summary
Mnemosyne uses an unsupervised, graph-based memory system that selectively ingests and stores information relevant to ongoing conversations. It employs modular filters to determine what enters memory, a graph structure to store nodes with temporal decay and boosting, and a core summary module to capture long-term user traits. Unlike traditional RAG or long-context methods, Mnemosyne injects only salient nodes into the LLM's context during retrieval, making it suitable for edge devices. The architecture is designed to model human-like forgetting, ensuring that only pertinent information is retained and accessible over time.

## Key Results
- Achieved 65.8% win rate in blind human preference tests versus 31.1% for a naive RAG baseline
- Led LoCoMo benchmark scores in temporal reasoning (60.4%) and single-hop retrieval
- Overall J-score of 54.6%, second only to Memory-R1

## Why This Works (Mechanism)
Mnemosyne's effectiveness stems from its human-inspired memory modeling: modular filters prevent irrelevant data from entering memory, a graph structure with temporal decay ensures only salient information persists, and selective retrieval minimizes context injection overhead. This design aligns with cognitive memory processes, allowing the LLM to maintain coherent, long-term conversational context without the computational burden of processing entire conversation histories or external document collections.

## Foundational Learning
- **Graph-based memory storage**: Needed to model complex, multi-hop relationships between memory nodes; quick check: verify graph traversal retrieves correct nodes for single-hop and multi-hop queries.
- **Temporal decay and boosting**: Needed to simulate human forgetting and prioritize recent or relevant information; quick check: confirm decay/boost mechanisms update node importance appropriately over time.
- **Modular input filtering**: Needed to prevent memory bloat and maintain relevance on constrained devices; quick check: test filters on diverse conversational inputs to ensure only pertinent information is stored.
- **Core summary module**: Needed to capture persistent user traits and context; quick check: validate summary accuracy by comparing generated summaries to ground truth user profiles.
- **Selective context injection**: Needed to minimize LLM context window usage; quick check: measure memory and latency overhead when injecting only top-k salient nodes.
- **Unsupervised operation**: Needed for deployment without manual labeling; quick check: assess memory quality and relevance without supervision.

## Architecture Onboarding

**Component map**: User Input -> Modular Filters -> Graph Storage (with Temporal Decay/Boosting) -> Core Summary Module -> Selective Retrieval -> LLM Context

**Critical path**: User Input → Modular Filters → Graph Storage → Selective Retrieval → LLM Context

**Design tradeoffs**: Prioritizes efficiency and relevance over exhaustive recall; sacrifices some recall depth for faster, more targeted context injection suitable for edge devices.

**Failure signatures**: 
- Memory bloat if filters are too permissive
- Loss of important context if decay is too aggressive
- Irrelevant recall if boosting heuristics are weak
- Increased latency if retrieval selects too many nodes

**First experiments**:
1. Validate modular filters by testing on a dataset of conversational inputs and measuring relevance of stored nodes.
2. Test graph traversal for single-hop and multi-hop retrieval accuracy against ground truth.
3. Measure memory usage and latency overhead on a representative edge device (e.g., Raspberry Pi).

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on single-blind human preference tests, which may not capture robustness across diverse scenarios or long-term deployment contexts.
- No empirical validation on actual resource-constrained hardware, so efficiency claims are based on design rather than measurement.
- Memory quality depends on unsupervised filtering heuristics, which are not extensively validated for edge cases or adversarial inputs.

## Confidence
- Human evaluation win rate: Medium
- Benchmark performance: Medium
- Efficiency on edge devices: Low (design-based, not empirically validated)

## Next Checks
1. Deploy Mnemosyne on representative edge hardware (e.g., Raspberry Pi or mobile devices) to measure real-world memory and latency overhead.
2. Conduct a longitudinal user study to evaluate memory retention, relevance, and user satisfaction over extended interaction periods.
3. Benchmark against additional baselines, including advanced RAG variants and long-context approaches, on diverse datasets to assess generalizability beyond LoCoMo.