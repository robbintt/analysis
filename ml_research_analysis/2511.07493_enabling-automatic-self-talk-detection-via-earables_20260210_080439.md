---
ver: rpa2
title: Enabling Automatic Self-Talk Detection via Earables
arxiv_id: '2511.07493'
source_url: https://arxiv.org/abs/2511.07493
tags:
- self-talk
- stage
- utterances
- acoustic
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MutterMeter, a mobile system that automatically
  detects self-talk from audio captured by earable microphones. Self-talk detection
  is challenging due to its acoustic variability, linguistic incompleteness, and irregular
  patterns.
---

# Enabling Automatic Self-Talk Detection via Earables

## Quick Facts
- arXiv ID: 2511.07493
- Source URL: https://arxiv.org/abs/2511.07493
- Reference count: 40
- Primary result: Hierarchical earable system achieving 0.84 macro-averaged F1 score for self-talk detection with 41% latency reduction

## Executive Summary
This paper presents MutterMeter, a mobile system that automatically detects self-talk from audio captured by earable microphones. Self-talk detection is challenging due to acoustic variability, linguistic incompleteness, and irregular patterns. MutterMeter employs a hierarchical classification architecture that progressively integrates acoustic, linguistic, and contextual information through a sequential pipeline, adaptively balancing accuracy and efficiency. The system uses locality-aware embedding adaptation to capture short-term contextual continuity and macro-contextual transcription for improved transcription quality. Evaluated on a first-of-its-kind dataset of 31.1 hours from 25 participants, MutterMeter achieves a macro-averaged F1 score of 0.84, outperforming conventional approaches including LLM-based and speech emotion recognition models.

## Method Summary
MutterMeter uses a hierarchical classification pipeline with three stages: acoustic, linguistic, and fusion. The acoustic stage classifies utterances based on audio features alone, with confidence-gated early exits for 61% of cases. For remaining utterances, the system performs contextual transcription using Whisper-large-v3 with 30-second context windows, then applies locality-aware embedding adaptation (LAEA) that blends consecutive utterance embeddings when they occur within 4 seconds. The linguistic stage uses BERT-based classification on transcribed text, and the fusion stage combines acoustic and linguistic embeddings using learned gating. The system processes audio captured by earable microphones, with preprocessing thresholds of -20 dB RMS for vocal event detection, 300ms minimum utterance duration, and 800ms gap threshold.

## Key Results
- Achieves 0.84 macro-averaged F1 score on self-talk detection task
- Reduces average utterance latency by 41% through early-stage exits
- Outperforms conventional approaches including LLM-based and speech emotion recognition models

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Classification with Confidence-Gated Early Exit
- Claim: Stage-wise processing with early exit reduces latency while maintaining accuracy.
- Mechanism: Each utterance enters at the acoustic stage; if prediction confidence (least margin ≥ threshold), processing stops. Otherwise, it proceeds through linguistic and fusion stages.
- Core assumption: Clear-cut cases can be resolved acoustically without expensive transcription.
- Evidence anchors:
  - [abstract] "reducing average utterance latency by 41% through early-stage exits"
  - [Section 4.5, Table 8] "61% of utterances are classified at the acoustic stage... average latency per utterance reduced from 6335 ms to 3713 ms"
  - [corpus] Limited direct corpus support for early-exit mechanisms in speech.

### Mechanism 2: Locality-Aware Embedding Adaptation (LAEA)
- Claim: Exploiting short-term temporal continuity reduces intra-class variability in self-talk embeddings.
- Mechanism: When consecutive utterances occur within 4 seconds, embeddings are blended via learned EMA weights.
- Core assumption: Utterances within 4 seconds likely share the same class due to "emotional inertia."
- Evidence anchors:
  - [Section 4.2.2] "Approximately 79% of utterances occurring within four seconds belong to the same class"
  - [Section 4.2.2, Figure 9] PCA visualization shows tighter clustering after adaptation.
  - [corpus] Weak direct support; corpus papers on disfluency detection note acoustic variability.

### Mechanism 3: Macro-Contextual Transcription for Improved Linguistic Features
- Claim: ASR quality improves when preceding utterance context is provided, boosting downstream classification.
- Mechanism: Instead of transcribing single utterances, the system packs temporally proximate utterances into Whisper's input.
- Core assumption: Self-talk's mumbling and grammatical incompleteness require contextual disambiguation.
- Evidence anchors:
  - [Section 4.3.1, Table 1] BLEU improves from 0.17 to 0.30; WER drops from 0.82 to 0.60.
  - [Section 4.3.1, Table 2] Concrete examples: "Love-fifteen" correctly transcribed with context.
  - [corpus] Related work on stuttering-aware ASR similarly leverages context.

## Foundational Learning

- **Concept: Least Margin as Confidence Score**
  - Why needed here: The hierarchical exit mechanism relies on quantifying prediction certainty.
  - Quick check question: Given probabilities [0.45, 0.40, 0.15] vs. [0.80, 0.12, 0.08], which has higher confidence per least margin?

- **Concept: Exponential Moving Average (EMA) for Sequence Smoothing**
  - Why needed here: LAEA uses EMA to blend embeddings.
  - Quick check question: If α = 0.2, does the adapted embedding favor e_curr or e_prev? What happens if ΔT exceeds 4 seconds?

- **Concept: Gated Multimodal Fusion**
  - Why needed here: The fusion stage learns a gate g ∈ [0,1] to weight acoustic vs. linguistic modalities.
  - Quick check question: If transcription is unreliable, should g favor acoustic or linguistic embeddings?

## Architecture Onboarding

- **Component map:**
  Mobile side: Audio capture → dB-based vocal event detection → utterance segmentation (300ms min, 800ms gap) → cache management (30s window) → Whisper-base encoder → LAEA → acoustic classifier → confidence gate
  Server side: Contextual transcription (Whisper-large-v3, 30s context) → BERT-based linguistic encoder → linguistic classifier → confidence gate → gated acoustic-linguistic fusion → final output

- **Critical path:**
  1. Preprocessing thresholds determine what counts as an "utterance"—errors here propagate.
  2. Acoustic stage fine-tuning on domain data is essential; baseline Whisper achieves only 0.15 F1 on positive self-talk.
  3. Confidence thresholds control latency/accuracy tradeoff.

- **Design tradeoffs:**
  - Privacy vs. accuracy: Full on-device processing sacrifices ~0.04 macro F1 but avoids server transmission.
  - Responsiveness vs. completeness: Early exit prioritizes speed; applications requiring high recall may need lower thresholds.
  - Context window size: 30s captures more dependencies but risks noise contamination.

- **Failure signatures:**
  - Positive self-talk misclassified as "others": Often command-like expressions lack clear subject.
  - Domain term mistranscription: Without context, tennis-specific vocabulary degrades linguistic accuracy.
  - LAEA over-smoothing: Rapid context switches within 4 seconds may blur class boundaries.

- **First 3 experiments:**
  1. Validate preprocessing thresholds: Manually inspect utterance boundaries on held-out audio.
  2. Ablate LAEA: Run acoustic stage with static α=0.5 vs. adaptive LAEA.
  3. Transcription context strategy comparison: Compare single-utterance vs. macro-contextual transcription.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does MutterMeter generalize to everyday contexts beyond tennis play?
- Basis in paper: [explicit] "future work will extend MutterMeter to diverse real-world contexts"
- Why unresolved: Current evaluation limited to single activity domain
- What evidence would resolve it: Evaluation on datasets from multiple everyday activities

### Open Question 2
- Question: Can MutterMeter be optimized for fully on-device operation without sacrificing detection accuracy?
- Basis in paper: [explicit] "optimizing MutterMeter for fully on-device operation could enhance privacy protection"
- Why unresolved: Current architecture relies on server-side computation
- What evidence would resolve it: Benchmarking compressed models running entirely on mobile hardware

### Open Question 3
- Question: How can the system provide meaningful feedback about the causes and contexts of self-talk occurrences?
- Basis in paper: [explicit] "we aim to technically advance MutterMeter so that it can analyze why and under what contexts self-talk occurs"
- Why unresolved: Current system only classifies self-talk valence but does not identify triggering situations
- What evidence would resolve it: Framework correlating detected self-talk with contextual cues

### Open Question 4
- Question: Does detection performance generalize across gender and speaking styles?
- Basis in paper: [inferred] Dataset has 22 male and only 3 female participants
- Why unresolved: Potential gender bias in the trained model
- What evidence would resolve it: Balanced gender evaluation with statistical analysis

## Limitations
- Dataset collected in controlled tennis environments may not generalize to uncontrolled real-world settings
- System relies on server-side processing for linguistic and fusion stages, raising latency and privacy concerns
- Limited gender diversity in dataset (22 male vs 3 female participants) raises potential bias concerns

## Confidence

- **High Confidence (Level 1):** Hierarchical classification mechanism and 41% latency reduction claims are well-supported by ablation studies and quantitative metrics.
- **Medium Confidence (Level 2):** Effectiveness of LAEA and macro-contextual transcription is supported by experimental results but relies on assumptions about temporal continuity.
- **Low Confidence (Level 3):** Generalizability of 0.84 F1 score to diverse real-world conditions is uncertain due to dataset's tennis-specific nature.

## Next Checks

1. **Cross-Activity Generalization Test:** Evaluate MutterMeter on self-talk data from non-tennis activities to assess whether the 0.84 F1 score holds across diverse acoustic environments.

2. **On-Device Processing Benchmark:** Implement and test a fully on-device version on the existing dataset to quantify the actual accuracy trade-off.

3. **Temporal Continuity Validation:** Conduct a user study measuring actual emotional state persistence in real-time across varied contexts to validate the 4-second LAEA window assumption.