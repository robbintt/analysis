---
ver: rpa2
title: Multi-step retrieval and reasoning improves radiology question answering with
  large language models
arxiv_id: '2508.00743'
source_url: https://arxiv.org/abs/2508.00743
tags:
- qwen
- retrieval
- question
- reasoning
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces RaR, a multi-step retrieval and reasoning
  framework designed to improve the accuracy and factual reliability of large language
  models in radiology question answering. RaR decomposes complex clinical questions,
  retrieves targeted evidence from Radiopaedia, and synthesizes evidence-based responses
  through iterative reasoning.
---

# Multi-step retrieval and reasoning improves radiology question answering with large language models

## Quick Facts
- arXiv ID: 2508.00743
- Source URL: https://arxiv.org/abs/2508.00743
- Reference count: 40
- Multi-step retrieval and reasoning framework (RaR) significantly improves LLM radiology QA accuracy from 67% to 75% and reduces hallucinations by 9.4%

## Executive Summary
This study introduces RaR, a multi-step retrieval and reasoning framework designed to improve the accuracy and factual reliability of large language models in radiology question answering. RaR decomposes complex clinical questions, retrieves targeted evidence from Radiopaedia, and synthesizes evidence-based responses through iterative reasoning. Evaluated across 25 diverse models on 104 expert-curated radiology questions, RaR significantly outperformed zero-shot prompting (75% vs. 67%; P = 1.1 × 10−7) and conventional RAG (75% vs. 69%; P = 1.9 × 10−6), with the largest gains observed in small and mid-sized models. RaR also reduced hallucinations by 9.4% and retrieved clinically relevant context in 46% of cases. Even clinically fine-tuned models showed improvements, indicating retrieval complements embedded knowledge.

## Method Summary
RaR implements a multi-step agent-based retrieval-augmented generation framework that processes radiology multiple-choice questions through sequential reasoning stages. The system uses Mistral Large for keyword extraction, GPT-4o-mini for orchestration and synthesis, and real-time retrieval from Radiopaedia via SearXNG meta-search engine. The framework decomposes questions into research plans, retrieves targeted evidence for each answer option, synthesizes findings into structured reports, and enables the target LLM to make evidence-based decisions. Evaluation was conducted across 25 models (from Llama-3-8B to GPT-4o) on 104 questions from RadioRAG dataset, comparing against zero-shot prompting and conventional RAG baselines.

## Key Results
- RaR achieved 75% accuracy versus 67% for zero-shot prompting (P = 1.1 × 10−7)
- RaR outperformed conventional RAG (75% vs 69%; P = 1.9 × 10−6)
- Largest accuracy gains seen in small/mid-sized models: +10.7% for Llama-3-8B, +11.4% for Qwen2.5-14B
- Reduced hallucinations by 9.4% and retrieved clinically relevant context in 46% of cases
- Even clinically fine-tuned models showed improvement, suggesting retrieval augments embedded knowledge

## Why This Works (Mechanism)
RaR works by decomposing complex radiology questions into targeted research tasks and retrieving specific evidence for each answer option rather than treating the question as a monolithic search. The iterative retrieval process with up to four query refinements ensures comprehensive coverage of relevant Radiopaedia content. The structured synthesis step creates a citation-rich report that grounds the final answer selection in specific evidence rather than relying on the LLM's internal knowledge. This multi-step approach addresses the limitations of both zero-shot prompting (which relies solely on model knowledge) and conventional RAG (which often retrieves overly broad or irrelevant context).

## Foundational Learning
- **Keyword Extraction**: Mistral Large identifies diagnostic concepts from questions to construct precise search queries - needed to bridge clinical terminology with search terms; quick check: verify extracted keywords capture both imaging findings and anatomical locations
- **Iterative Retrieval**: Up to four query refinement attempts to ensure comprehensive evidence coverage - needed because single queries often miss relevant content; quick check: measure retrieval coverage across different query formulations
- **Evidence Synthesis**: GPT-4o-mini creates structured reports with introduction, option analysis, and conclusion - needed to organize retrieved information for LLM consumption; quick check: verify synthesis maintains citation fidelity
- **Citation Enforcement**: Structured output format requires specific citations for each claim - needed to reduce hallucinations and ensure factual grounding; quick check: measure hallucination rates with and without citation requirements
- **Multi-Option Analysis**: Separate research modules for each answer option - needed to avoid confirmation bias and ensure comprehensive evaluation; quick check: compare accuracy when analyzing options jointly versus separately
- **Orchestration Pipeline**: LangGraph/LangChain framework coordinates sequential reasoning steps - needed to manage complex multi-agent workflows; quick check: verify pipeline completion rates and timing

## Architecture Onboarding

**Component Map**: Question -> Keyword Extraction -> Supervisor -> Research Modules -> Writer -> Final Inference -> Answer

**Critical Path**: Supervisor generates research plan → Research modules query Radiopaedia iteratively → Writer synthesizes evidence → Target LLM selects answer from structured report

**Design Tradeoffs**: Real-time retrieval provides current evidence but introduces latency and availability dependencies versus static knowledge; iterative retrieval improves coverage but increases computational cost; structured synthesis ensures consistency but requires additional orchestration overhead

**Failure Signatures**: Irrelevant retrieval context indicates poor keyword extraction or overly broad queries; pipeline timeouts suggest SearXNG rate limiting or context window overflow; hallucination persistence indicates inadequate citation enforcement or insufficient evidence

**3 First Experiments**:
1. Compare keyword extraction quality using Mistral Large versus simpler heuristics on 20 sample questions
2. Test iterative retrieval with fixed query count versus dynamic refinement on questions with known difficulty
3. Evaluate hallucination reduction by comparing structured synthesis with unstructured retrieval outputs

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to multiple-choice radiology questions, limiting generalizability to open-ended clinical scenarios
- Performance heavily dependent on real-time retrieval availability from Radiopaedia, with no assessment of system behavior when key references are unavailable
- Critical implementation details for Supervisor module and citation enforcement are not fully specified, hindering exact replication

## Confidence
- **High Confidence**: 75% vs 67% accuracy improvement over zero-shot (P < 0.001), 75% vs 69% over RAG (P < 0.001), 9.4% hallucination reduction
- **Medium Confidence**: Differential performance gains for smaller models, clinical fine-tuned models still benefit from retrieval
- **Low Confidence**: Practical clinical utility of 8-10% accuracy improvements not established

## Next Checks
1. Test RaR's performance on open-ended radiology questions and questions from other medical specialties to assess generalizability beyond curated multiple-choice format
2. Conduct ablation studies to quantify individual contributions of keyword extraction, research planning, and iterative retrieval components
3. Evaluate system robustness by measuring performance when Radiopaedia is partially unavailable or when retrieved evidence contains contradictions