---
ver: rpa2
title: From RAG to Agentic RAG for Faithful Islamic Question Answering
arxiv_id: '2601.07528'
source_url: https://arxiv.org/abs/2601.07528
tags:
- islamic
- reasoning
- answer
- question
- arabic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors introduce ISLAMICFAITHQA, a 3,810-item bilingual (Arabic/English)\
  \ generative benchmark for Islamic question answering with atomic single-gold answers,\
  \ enabling direct measurement of hallucination and abstention. They also develop\
  \ an end-to-end grounded Islamic modeling suite: 25K Arabic text-grounded SFT reasoning\
  \ pairs, 5K bilingual preference samples for reward-guided alignment, and a verse-level\
  \ Qur\u2019an retrieval corpus of ~6k atomic verses."
---

# From RAG to Agentic RAG for Faithful Islamic Question Answering

## Quick Facts
- arXiv ID: 2601.07528
- Source URL: https://arxiv.org/abs/2601.07528
- Reference count: 40
- Introduces ISLAMICFAITHQA (3,810 bilingual QA pairs) and agentic RAG framework for Islamic QA, achieving state-of-the-art Arabic-English robustness with small models.

## Executive Summary
The authors introduce ISLAMICFAITHQA, a 3,810-item bilingual (Arabic/English) generative benchmark for Islamic question answering with atomic single-gold answers, enabling direct measurement of hallucination and abstention. They also develop an end-to-end grounded Islamic modeling suite: 25K Arabic text-grounded SFT reasoning pairs, 5K bilingual preference samples for reward-guided alignment, and a verse-level Qur’an retrieval corpus of ~6k atomic verses. Building on these, they create an agentic Quran-grounding framework (agentic RAG) using structured tool calls for iterative evidence seeking and answer revision. Experiments across Arabic-centric and multilingual LLMs show that retrieval improves correctness, and agentic RAG yields the largest gains beyond standard RAG, achieving state-of-the-art performance and stronger Arabic-English robustness even with a small model (Qwen3 4B).

## Method Summary
The work develops an end-to-end pipeline for faithful Islamic QA: first, curating ISLAMICFAITHQA (3,810 bilingual QA pairs with atomic gold answers) and supporting datasets (25K Arabic SFT reasoning pairs, 5K bilingual RL preference samples, and a 6,236-verse Quran RAG corpus). Models are fine-tuned using SFT (lr=5e-5 on Arabic reasoning) and GSPO (lr=3e-6) with LLM-as-judge rewards. Retrieval is implemented with mE5-base embeddings into ChromaDB (top-5). Agentic RAG uses structured tool calls (search_quran, get_surah_info, read_ayah, search_surah) for iterative evidence seeking and answer revision, with evaluation via a 3-way LLM-as-judge (Correct/Incorrect/Not_Attempted) and human calibration (79% agreement, κ=0.51 on 200 items).

## Key Results
- Agentic RAG yields the largest gains beyond standard RAG in correctness and Arabic-English robustness.
- Retrieval consistently improves correctness over base models.
- State-of-the-art performance achieved even with a small model (Qwen3 4B).

## Why This Works (Mechanism)
Agentic RAG enables iterative evidence seeking and answer revision via structured tool calls, grounding responses in canonical Islamic sources and reducing hallucination. The framework’s explicit retrieval and tool-mediated reasoning improve faithfulness and robustness compared to standard RAG.

## Foundational Learning
- **ISLAMICFAITHQA Benchmark**: Bilingual QA dataset with atomic gold answers for hallucination measurement and abstention calibration. Needed for direct faithfulness assessment; quick check: verify atomic single-answer format.
- **Quran RAG Corpus**: Verse-level retrieval corpus (6,236 ayat) with metadata for grounded QA. Needed for evidence retrieval; quick check: validate top-5 retrieval accuracy.
- **Agentic Tool Calls**: Structured tool-mediated evidence seeking (search_quran, get_surah_info, read_ayah, search_surah). Needed for iterative grounding; quick check: confirm tool invocation and citation in outputs.

## Architecture Onboarding
- **Component Map**: ISLAMICFAITHQA -> LLM-as-Judge -> Agentic RAG (tools) -> Quran RAG Index -> Final Answer
- **Critical Path**: QA input → Agentic RAG tool calls (evidence seek → final answer) → Retrieval (mE5-base, ChromaDB) → LLM-as-judge evaluation
- **Design Tradeoffs**: Agentic RAG increases faithfulness and robustness but requires structured tool integration and more complex inference; standard RAG is simpler but less grounded.
- **Failure Signatures**: Judge returns non-adherent labels or verbose text instead of A/B/C; Agentic RAG fails to call tools or produces ungrounded answers.
- **First Experiments**:
  1. Run judge prompt on small bilingual subset; verify 3-way outputs and compute human-LLM agreement.
  2. Preprocess 6,236 ayat into ChromaDB; validate top-5 retrieval on test queries.
  3. Implement and test 2-turn agentic RAG protocol; verify tool calls and citations.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset availability: ISLAMICFAITHQA, SFT/RL corpora, and Quran RAG index are not publicly accessible, blocking exact reproduction.
- Training details beyond learning rates (batch sizes, epochs, optimizer settings, GSPO regularization) are omitted.
- Generalizability to broader Islamic or multilingual contexts is uncertain due to small, domain-specific corpus.

## Confidence
- **High**: Retrieval consistently improves correctness over base models, and agentic RAG outperforms standard RAG within the evaluated domain.
- **Medium**: Arabic-English robustness and gains from agentic RAG are well-supported by ablation, but generalizability to larger or more diverse Islamic corpora is uncertain.
- **Low**: Exact reproducibility is not possible without access to the new datasets and full training configurations.

## Next Checks
1. Reconstruct the Quran RAG index (6,236 verse-level records with metadata) and validate retrieval accuracy on a small set of test queries.
2. Run the judge prompt (Appendix A.5) on a held-out bilingual subset using GPT-4.1 at T=0; compute 3-way agreement and kappa to confirm 79% and κ=0.51 on the 200-item calibration set.
3. Implement and test the 2-turn agentic RAG protocol (evidence seeking → answer revision) with all four tools; verify that tool invocations occur and citations are present in final outputs for a subset of questions.