---
ver: rpa2
title: Tensor-to-Tensor Models with Fast Iterated Sum Features
arxiv_id: '2506.06041'
source_url: https://arxiv.org/abs/2506.06041
tags:
- layer
- tree
- data
- corner
- anomaly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel tensor-to-tensor layer called Fast
  Iterated Sums (FIS) that efficiently computes a subset of the two-parameter sums
  signature using "corner trees" from permutation counting. The FIS layer provides
  linear-time complexity in input size and can be used as a drop-in replacement for
  convolutional layers in image processing pipelines.
---

# Tensor-to-Tensor Models with Fast Iterated Sum Features

## Quick Facts
- arXiv ID: 2506.06041
- Source URL: https://arxiv.org/abs/2506.06041
- Authors: Joscha Diehl; Rasheed Ibraheem; Leonard Schmitz; Yue Wu
- Reference count: 40
- Primary result: Replacing ResNet layers with FIS achieves similar accuracy with 20-85% fewer parameters and operations

## Executive Summary
This paper introduces the Fast Iterated Sums (FIS) layer, a tensor-to-tensor module that efficiently computes a subset of the two-parameter sums signature using corner tree structures. The method provides linear-time complexity through dynamic programming, enabling it to serve as a drop-in replacement for convolutional layers. Experiments demonstrate that FIS-based ResNets maintain accuracy (within 0.1%) while significantly reducing parameters and operations, and the approach achieves state-of-the-art anomaly detection performance (97.3% AUROC) on MVTec AD texture images.

## Method Summary
The FIS layer generalizes iterated sums signatures to tensor inputs by aggregating spatial information via random corner trees with cardinal direction edges. It uses a recursive CTPS formula with cumulative sums to achieve O(n×T1×T2) complexity, where n is tree size and T1×T2 is input spatial dimension. The method supports semiring generalization (max-plus vs. real) and can be integrated into standard CNN architectures. For classification, FIS blocks replace Basic Blocks in ResNets; for anomaly detection, an encoder-decoder architecture uses pretrained features followed by FIS layers.

## Key Results
- ResNet44 with FIS Downsample achieves 94.47% accuracy (vs 94.37% for ResNet56) with ~23% fewer operations
- L23 variant of ResNet32 reduces parameters by 85% with only 2.90% accuracy drop on CIFAR-10
- FIS-based anomaly detection achieves 97.3% average AUROC on MVTec AD texture images
- Max-plus semiring outperforms real semiring by 0.5-1% on CIFAR-10 classification

## Why This Works (Mechanism)

### Mechanism 1
- Corner trees enable linear-time computation of a subset of two-parameter sums signature features
- The recursive CTPS formula uses cumulative sums to reduce complexity from O((T1×T2)^n) to O(n×T1×T2)
- Only point constellations describable by rooted trees with cardinal direction edges are computable in linear time
- Break condition: Non-tree point constellations (like four-point cycles) cannot be computed efficiently

### Mechanism 2
- Max-plus semiring provides better feature representations than real semiring for image classification
- The max operation provides implicit pooling/selectivity, selecting the most salient feature combinations
- Max-plus aggregation captures discriminative texture patterns better than additive aggregation
- Break condition: Max operation may cause gradient instability or underperform when dense aggregation is needed

### Mechanism 3
- FIS layers can replace convolutional layers while maintaining accuracy with fewer parameters
- Unlike convolutions requiring O(C_in × C_out × K × K) parameters, FIS uses O(n × C_in × NT)
- One FIS Block replaces multiple Basic Blocks, reducing architectural redundancy
- Break condition: Tasks requiring precise local pattern matching may degrade beyond observed 0.1-3% accuracy gaps

## Foundational Learning

- **Iterated Sums / Signatures**
  - Why needed here: FIS is fundamentally a two-parameter generalization of the iterated sums signature
  - Quick check: Given sequence [1, 2, 3], compute second-order iterated sum y_3 = Σ_{i<j≤3} x_i·x_j. (Answer: 11)

- **Semirings**
  - Why needed here: The paper uses semiring abstraction to generalize aggregation operations
  - Quick check: In max-plus semiring, what is "sum" of {3, 7, 2} and "product" of 4 and 5? (Answer: sum=7, product=9)

- **Cumulative Sums / Prefix Sums**
  - Why needed here: Linear-time algorithm relies on cumulative sums via `cumsum(P, x)_t`
  - Quick check: For 4×4 data, how many operations to compute all prefix sums for NE direction? (Answer: O(16) vs naive O(256) per position)

## Architecture Onboarding

- Component map:
  Input (B, C, H, W) -> FIS Layer: Random corner tree generation -> Linear projection -> Corner tree sum computation -> (B, NT, H, W) -> BatchNorm -> ReLU -> [Repeat] -> Adaptive Pooling -> Output

- Critical path:
  1. Tree generation (instantiation-time, seeded for reproducibility)
  2. Forward pass: Compute CTPS recursively using cumulative sums
  3. Gradient flow: Backpropagate through linear projections (node weights)

- Design tradeoffs:
  - Tree depth vs. expressivity: More nodes capture higher-order interactions but increase constants
  - Random vs. structured trees: Random trees performed best but linear-NE is more interpretable
  - Semiring choice: Max-plus better for classification but may not suit all tasks
  - Number of trees (NT): Controls output channel dimension; trade-off with compute

- Failure signatures:
  - Accuracy degradation >5%: Check if tree structure is too constrained
  - Memory blowout: Reduce tree size or add intermediate pooling
  - Gradient instability with max-plus: Verify gradient through max operation is handled correctly
  - Slow training: Ensure GPU-parallel implementation of cumulative sums

- First 3 experiments:
  1. Implement single FIS layer with linear-NE tree on CIFAR-10; verify output shape and gradient flow
  2. Train CA-FIS model with real vs. max-plus semiring for 50 epochs; expect max-plus ~0.5-1% improvement
  3. Train with random, linear, and linear-NE trees on CIFAR-10; expect random > linear > linear-NE

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the linear-time computational approach be extended to non-tree point constellations, or are there theoretical complexity barriers?
- Basis: The paper explicitly excludes sums like the four-point cycle example in Figure 2 which lack tree structure
- Why unresolved: The corner tree structure ensures linear complexity O(nT1T2) by construction
- Evidence needed: Algorithm demonstrating efficient computation for non-tree signature terms or proof of computational hardness

### Open Question 2
- Question: Does parameter efficiency and accuracy preservation scale effectively to high-resolution benchmarks like ImageNet?
- Basis: The authors state they focus on CIFAR datasets to demonstrate theoretical validity before scaling to ImageNet
- Why unresolved: Experiments are restricted to CIFAR-10/100 (32x32) and MVTec (224x224 patches)
- Evidence needed: Benchmarks of FIS-based architectures trained and evaluated on ImageNet

### Open Question 3
- Question: How does FIS perform empirically on spatiotemporal data (order-3 tensors) such as video?
- Basis: Section 4 defines generalization to order-3 tensors but Section 6 restricts validation to static images
- Why unresolved: While theoretical mechanism for video processing is presented, computational cost and feature quality remain speculative
- Evidence needed: Experimental results from applying FIS layers to video classification or anomaly detection tasks

### Open Question 4
- Question: What are the theoretical properties of max-plus semiring that cause it to outperform real semiring in FIS layers for vision tasks?
- Basis: Authors note max-plus produced best accuracy as a hyperparameter tuning result rather than theoretically derived necessity
- Why unresolved: While the paper cites ReLU networks as max-plus rational maps, it lacks formal analysis of why this structure is superior
- Evidence needed: Theoretical analysis linking max-plus corner trees to image features or comparative study of feature distributions

## Limitations

- Empirical validation is limited to image classification and anomaly detection tasks on relatively small datasets
- Key hyperparameters (tree size, number of trees) lack sensitivity analysis
- Anomaly detection results depend on frozen pretrained features, complicating attribution of performance gains
- Theoretical claims about expressiveness are limited to the corner tree subset of sums signatures

## Confidence

- **High Confidence**: Theoretical foundation for linear-time complexity via corner trees is sound and well-specified
- **Medium Confidence**: Empirical claims about parameter/operation reductions and accuracy preservation supported by CIFAR-10 experiments
- **Medium Confidence**: Anomaly detection results are compelling but depend on pretrained features

## Next Checks

1. **Cross-Architecture Generalization**: Implement FIS layers in MobileNet or EfficientNet and compare accuracy-efficiency tradeoffs on CIFAR-10

2. **Ablation on Hyperparameters**: Systematically vary number of nodes per tree and number of trees (NT) in FIS layers on CIFAR-10 to quantify impact on accuracy, parameters, and training time

3. **Controlled Anomaly Detection**: Train end-to-end anomaly detection model using only FIS layers (no pretrained features) on MVTec AD to isolate FIS contribution to reconstruction quality and AUROC