---
ver: rpa2
title: 'Principles2Plan: LLM-Guided System for Operationalising Ethical Principles
  into Plans'
arxiv_id: '2512.08536'
source_url: https://arxiv.org/abs/2512.08536
tags:
- ethical
- planning
- rules
- users
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Principles2Plan is a novel interactive prototype enabling ethically-aware
  automated planning by combining human guidance with large language models. It allows
  users to input planning domains, problems, and high-level ethical principles, then
  generates, reviews, and prioritises context-specific ethical rules before producing
  plans.
---

# Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans

## Quick Facts
- arXiv ID: 2512.08536
- Source URL: https://arxiv.org/abs/2512.08536
- Reference count: 1
- Primary result: Novel interactive prototype enabling ethically-aware automated planning through human-LLM collaboration

## Executive Summary
Principles2Plan is a novel interactive prototype that enables ethically-aware automated planning by combining human guidance with large language models. The system allows users to input planning domains, problems, and high-level ethical principles, then generates, reviews, and prioritises context-specific ethical rules before producing plans. It translates natural language ethical rules into PDDL-Ethical code and feeds them to a classical planner, producing ethically-informed plans compared side-by-side with baseline plans. Evaluation using DeepSeek-R1-Distill-Llama-70B showed 0.82 Sentence-BERT similarity for generated rules and 82.2% code generation success rate.

## Method Summary
The system operates through a four-stage pipeline: (1) users provide PDDL domain/problem files, initial state, assumptions, and high-level ethical principles; (2) an LLM generates context-specific ethical rules with associated ethical features (positive/negative characteristics) and explanations; (3) users review, edit, and prioritise rules by assigning significance levels (1-5) to each ethical feature; (4) the LLM generates PDDL-Ethical code from refined rules, which is transpiled to cost-augmented PDDL using Jedwabny's 2022 method, then submitted to Fast Downward planner to produce ethical and baseline plans for side-by-side comparison.

## Key Results
- LLM-generated ethical rules achieved 0.82 Sentence-BERT similarity to reference rules
- PDDL-Ethical code generation success rate of 82.2% using DeepSeek-R1-Distill-Llama-70B
- Three example domains demonstrated: autonomous vehicles, elderly care, and firefighting/rescue
- Side-by-side comparison of ethical plans versus baseline plans

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can bridge abstract ethical principles and operational planning by generating context-specific rules when given domain/problem specifications.
- Mechanism: The LLM receives structured input (planning domain, problem details, high-level principles like beneficence or privacy) and produces natural language ethical rules annotated with ethical features (positive/negative characteristics such as "dishonesty" as a negative feature).
- Core assumption: LLMs encode sufficient commonsense ethical reasoning to generate relevant, context-appropriate rules from abstract principles without formal ethical training data.
- Evidence anchors:
  - [abstract]: "The system generates operationalisable ethical rules consistent with these principles"
  - [section]: Input Page describes how "Each rule includes ethical features, representing positive or negative ethical characteristics"
  - [corpus]: Limited direct corpus support; related work (Favier et al. 2025, Zhong et al. 2026) shows LLMs can translate natural language constraints to PDDL, but ethical rule generation specifically remains underexplored.
- Break condition: If ethical rules are incoherent, irrelevant to the domain, or fail to map to actionable planning constraints, the downstream pipeline fails.

### Mechanism 2
- Claim: Human review and prioritization of LLM-generated rules improves alignment with domain-specific ethical requirements.
- Mechanism: After LLM generates rules with explanations, users can add, remove, or modify rules and assign significance levels (1–5) to each ethical feature. These weights influence action costs during planning.
- Core assumption: Domain experts can correctly identify flawed rules and assign meaningful priorities without deep ethical theory expertise.
- Evidence anchors:
  - [abstract]: "The user can review, prioritise, and supply to a planner to produce ethically-informed plans"
  - [section]: Ethical Rules Editor enables "Users can add missing rules, remove inappropriate ones, and modify existing rules"
  - [corpus]: Corpus papers on user preferences in AI ethics (e.g., "User-first Approach to AI Ethics") suggest cultural/contextual variation in ethical priorities, reinforcing need for human oversight.
- Break condition: If users lack sufficient domain or ethical expertise, they may accept inappropriate rules or misprioritize features.

### Mechanism 3
- Claim: Translating natural language ethical rules into PDDL-Ethical, then transpiling to cost-augmented PDDL, enables classical planners to produce ethically-informed plans.
- Mechanism: LLM generates PDDL-Ethical code from refined rules → transpiler (Jedwabny 2022 method) converts to standard PDDL with action costs → Fast Downward planner generates plans → side-by-side comparison with baseline.
- Core assumption: Ethical constraints can be adequately represented as action costs in classical planning without requiring richer ethical representations.
- Evidence anchors:
  - [section]: Code Editor page—"code is then transpiled... into raw PDDL with action costs and submitted to a domain-independent classical planner (Fast Downward)"
  - [evaluation]: DeepSeek-R1-Distill-Llama-70B achieved 82.2% code generation success rate
  - [corpus]: Zhong, Song, and Pagnucco (2026) demonstrated this pipeline without interface; Favier et al. (2025) showed similar LLM-to-PDDL translation for constraints.
- Break condition: If PDDL-Ethical code is syntactically incorrect or semantically misaligned with rules, transpilation or planning fails.

## Foundational Learning

- Concept: **PDDL (Planning Domain Definition Language)**
  - Why needed here: The system requires domain.pddl and problem.pddl files as input; ethical rules are converted to PDDL-Ethical then transpiled to PDDL with action costs.
  - Quick check question: Can you explain the difference between a PDDL domain file (actions, predicates) and a problem file (initial state, goal)?

- Concept: **Computational Machine Ethics (CME) paradigms: top-down vs. bottom-up vs. hybrid**
  - Why needed here: Principles2Plan is a hybrid approach using top-down principles with LLM-assisted rule generation, requiring understanding of trade-offs between transparency and adaptability.
  - Quick check question: Why might a purely top-down ethical system fail in novel contexts?

- Concept: **Action costs in classical planning**
  - Why needed here: Ethical features are operationalized as action costs; the planner minimizes total cost, implicitly prioritizing ethically preferable actions.
  - Quick check question: How does assigning higher cost to an action influence the planner's behavior?

## Architecture Onboarding

- Component map: Input Page → Ethical Rules Editor → Code Editor → Output Plan Page
- Critical path:
  1. User provides valid PDDL domain/problem + ethical principles
  2. LLM generates syntactically correct ethical rules (0.82 Sentence-BERT similarity benchmark)
  3. User reviews/prioritizes rules without introducing errors
  4. LLM generates valid PDDL-Ethical code (82.2% success rate benchmark)
  5. Transpilation succeeds → planner returns feasible plan

- Design tradeoffs:
  - **Transparency vs. automation**: More human oversight improves rule quality but increases manual burden
  - **Expressiveness vs. tractability**: Representing ethics as action costs enables classical planning but may not capture complex ethical constraints (e.g., conditional obligations)
  - **Model selection**: Different LLMs may vary in rule quality and code generation accuracy; DeepSeek-R1-Distill-Llama-70B was evaluated but others untested

- Failure signatures:
  - LLM generates rules irrelevant to domain → indicates insufficient grounding in problem specification
  - PDDL-Ethical syntax errors → LLM failed to follow formal grammar; requires Code Editor intervention
  - Planner returns no solution → ethical costs may over-constrain the problem or domain/problem files malformed
  - Side-by-side plans identical → ethical features may not affect action selection; check priority assignments

- First 3 experiments:
  1. Run a provided example domain (autonomous vehicles, elderly care, or firefighting) end-to-end to observe baseline vs. ethical plan differences
  2. Modify ethical principle priorities (e.g., increase weight of "privacy" vs. "beneficence") and observe plan changes
  3. Deliberately introduce a malformed rule, verify LLM explanation quality and user correction workflow

## Open Questions the Paper Calls Out

- Question: Does Principles2Plan reduce or increase user workload compared to manually specifying ethical rules, given the need to review and correct LLM outputs?
  - Basis in paper: [explicit] "One may question the practicality and performance of LLM-generated outputs here and whether they add more work for the user."
  - Why unresolved: The paper reports 82.2% code generation success rate, implying ~18% of outputs require correction, but no user study measured actual time/effort savings versus manual specification.
  - What evidence would resolve it: A controlled user study comparing time, cognitive load, and error rates between Principles2Plan users and users manually writing ethical PDDL rules.

- Question: How can the system reduce or eliminate the requirement for users to have PDDL and planning expertise?
  - Basis in paper: [explicit] "We recognise that intended users are unlikely to have technical knowledge of planning and PDDL; minimising the need for such expertise remains a challenge for future work."
  - Why unresolved: The Code Editor page requires users to review PDDL-Ethical syntax, which presumes familiarity with planning languages the target users are unlikely to possess.
  - What evidence would resolve it: A redesigned interface tested with domain experts lacking PDDL knowledge, measuring task completion rates and correctness without technical assistance.

- Question: How does model selection affect rule quality and code generation reliability across diverse ethical domains?
  - Basis in paper: [inferred] Evaluation used only DeepSeek-R1-Distill-Llama-70B; the 0.82 Sentence-BERT similarity and 82.2% code success rate may vary significantly with different models.
  - Why unresolved: Single-model evaluation provides no baseline for comparison, and model performance may be inconsistent across the three example domains or new domains entirely.
  - What evidence would resolve it: Comparative evaluation across multiple LLMs (e.g., GPT-4, Claude, Llama variants) using the same benchmark domains and metrics.

- Question: Can iterative dialogue between users and the LLM improve rule refinement and reduce the need for manual correction?
  - Basis in paper: [explicit] "Future work will enhance human-LLM collaboration through iterative dialogue and suggestions."
  - Why unresolved: The current system supports only one-shot generation followed by manual editing; no conversational mechanism exists for clarifying or refining rules through back-and-forth interaction.
  - What evidence would resolve it: A prototype with conversational refinement capabilities, evaluated against the current system on rule quality, user satisfaction, and correction frequency.

## Limitations
- LLM-generated ethical rules introduce inherent uncertainty in rule quality and relevance
- The 82.2% code generation success rate indicates a non-trivial failure rate that could compromise planning outcomes
- The approach assumes ethical principles can be adequately represented through action costs in classical planning, which may not capture complex conditional or temporal ethical constraints

## Confidence
- **High Confidence**: The system architecture combining LLM rule generation, human review, and classical planning is technically sound and follows established methods in computational machine ethics
- **Medium Confidence**: The evaluation metrics (Sentence-BERT similarity, code generation success rate) demonstrate technical feasibility but don't fully validate ethical appropriateness of generated plans
- **Low Confidence**: The generalizability of the approach across diverse ethical domains and the robustness of human review processes under varying expertise levels

## Next Checks
1. **Ethical Appropriateness Validation**: Test the system across diverse domains (e.g., healthcare, finance, autonomous vehicles) and evaluate whether generated plans align with domain-specific ethical standards beyond technical correctness
2. **Human Review Robustness**: Conduct user studies with varying expertise levels to assess how review quality and priority assignments affect plan outcomes, identifying thresholds where human oversight becomes critical
3. **Constraint Expressiveness**: Experiment with increasingly complex ethical requirements (conditional obligations, temporal constraints) to determine the limits of action-cost representation in classical planning for ethical decision-making