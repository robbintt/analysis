---
ver: rpa2
title: 'BnMMLU: Measuring Massive Multitask Language Understanding in Bengali'
arxiv_id: '2505.18951'
source_url: https://arxiv.org/abs/2505.18951
tags:
- shot
- bengali
- language
- question
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BnMMLU is a new benchmark for measuring massive multitask language
  understanding in Bengali, consisting of 134,375 multiple-choice questions across
  41 academic and professional domains. The benchmark is designed to evaluate the
  factual knowledge, reasoning, and application skills of language models in Bengali.
---

# BnMMLU: Measuring Massive Multitask Language Understanding in Bengali

## Quick Facts
- **arXiv ID**: 2505.18951
- **Source URL**: https://arxiv.org/abs/2505.18951
- **Reference count**: 40
- **Primary result**: New Bengali benchmark (134,375 MCQ pairs, 41 domains) shows proprietary models lead, but best open-weight models narrow gap significantly with reasoning enabled

## Executive Summary
BnMMLU is a comprehensive benchmark for evaluating massive multitask language understanding in Bengali across 41 academic and professional domains. The benchmark consists of 134,375 multiple-choice questions with four options each, covering STEM, humanities, social sciences, and general knowledge. Evaluations on 24 model variants reveal that while proprietary models currently lead, open-weight models show promising performance, especially when reasoning capabilities are enabled. The results highlight persistent gaps in reasoning and application skills, with sublinear scaling observed beyond mid-compute models, suggesting that improvements depend more on data quality and training recipes than model scale alone.

## Method Summary
The benchmark employs a rigorous evaluation protocol with two prompting styles (Direct and Chain-of-Thought) and two context regimes (0-shot and 5-shot). Five-shot exemplars were manually curated with GPT-5-MINI-generated reasoning traces. Models must output answers in strict JSON format (`{"answer":"A/B/C/D"}`) with system prompts in English. The benchmark includes both a full version (134,375 questions) and a difficult subset (BnMMLU-HARD). Evaluation covers 24 model variants including proprietary and open-weight models, with particular attention to reasoning effects across different model scales. Math content is presented in MathML format, and questions may involve mixed-script text.

## Key Results
- Proprietary models lead overall performance, but best open-weight models narrow the gap significantly when reasoning is enabled
- Sublinear scaling observed beyond mid-compute models, indicating data and training recipe quality are more critical than model size
- Chain-of-Thought prompting shows heterogeneous effects, with smaller models (1B-3B parameters) sometimes experiencing negative performance impacts
- Subject-specific failure modes and robustness issues identified, particularly for longer questions and MathML parsing

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of 41 domains, realistic question formats with multiple-choice structure, and rigorous evaluation protocol that controls for prompting variations. The use of JSON-formatted outputs ensures standardized evaluation, while the inclusion of both Direct and CoT prompting styles reveals reasoning capabilities across different model scales. The benchmark's construction methodology, including manual screening of exemplars and careful subdomain division, creates a challenging yet representative test of Bengali language understanding.

## Foundational Learning
- **Multiple-choice question format**: Needed to standardize evaluation across diverse domains; quick check: ensure 4 options per question with single correct answer
- **Chain-of-Thought prompting**: Required to elicit reasoning capabilities; quick check: verify reasoning traces are generated and follow logical structure
- **MathML representation**: Essential for mathematical content evaluation; quick check: confirm models can parse MathML tags without errors
- **JSON output specification**: Critical for automated evaluation; quick check: validate all model outputs match required format exactly
- **5-shot prompting methodology**: Important for context-aware evaluation; quick check: ensure exemplars are relevant to each subdomain
- **Domain-specific evaluation**: Necessary for granular performance analysis; quick check: verify questions are correctly categorized into 41 subdomains

## Architecture Onboarding
- **Component map**: Dataset -> Prompt templates -> Model inference -> JSON parsing -> Accuracy calculation
- **Critical path**: BnMMLU dataset loading → prompt generation (with/without exemplars) → model inference → answer extraction → accuracy computation
- **Design tradeoffs**: Balance between comprehensive domain coverage and question difficulty; tradeoff between direct evaluation and reasoning-eliciting prompting
- **Failure signatures**: Models outputting free text instead of JSON; incorrect parsing of MathML; CoT causing performance degradation in small models; selection of salient-looking options without proper reasoning
- **First experiments**: 1) Test JSON parsing robustness across different model outputs, 2) Evaluate MathML parsing accuracy, 3) Compare Direct vs CoT performance on small models

## Open Questions the Paper Calls Out
- To what extent do multimodal capabilities improve model performance on BnMMLU items involving mathematical equations (MathML) or mixed scripts compared to the reported text-only evaluation?
- Can targeted fine-tuning on Bengali reasoning traces mitigate the "negative reasoning effects" observed in smaller models (1B–3B parameters) when using Chain-of-Thought (CoT) prompting?
- What specific data curation strategies are required to enable Bengali-centric models to surpass the "sublinear returns to scale" observed in the current benchmark?

## Limitations
- Text-only evaluation doesn't cover multimodal settings, potentially underestimating real-world performance
- Missing 5-shot exemplars per subdomain limit faithful reproduction of the 5-shot prompting regime
- Unknown inference hyperparameters for proprietary models affect reproducibility
- MathML parsing challenges may disadvantage models in mathematical domains

## Confidence
- High confidence in benchmark construction methodology and domain coverage
- High confidence in evaluation protocol design and implementation details
- Medium confidence in reproducibility due to missing 5-shot exemplars and inference hyperparameters
- Medium confidence in proprietary model results due to API access variability

## Next Checks
1. Reconstruct the 5-shot exemplars by sampling from the full BnMMLU dataset using the described GPT-5-MINI generation pipeline with reasoning_effort=minimal and verbosity=low parameters
2. Implement a standardized evaluation harness with configurable temperature (0.0-0.7) and max_tokens (64-128) to test sensitivity to inference hyperparameters across model families
3. Create a replication subset by randomly sampling 1,000 questions from BnMMLU-FULL and conducting a complete evaluation with 3 different prompting strategies to validate the claimed accuracy ranges and CoT vs Direct performance patterns observed in Table 9