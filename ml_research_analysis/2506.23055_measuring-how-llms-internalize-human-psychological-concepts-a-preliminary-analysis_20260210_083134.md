---
ver: rpa2
title: 'Measuring How LLMs Internalize Human Psychological Concepts: A preliminary
  analysis'
arxiv_id: '2506.23055'
source_url: https://arxiv.org/abs/2506.23055
tags:
- psychological
- similarity
- language
- questionnaires
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a framework to evaluate how well large language
  models (LLMs) internalize human psychological concepts by comparing LLM-generated
  similarity scores between questionnaire items to established psychological constructs.
  Using 43 standardized questionnaires and six language models (BERT, OpenAI embedding,
  GPT-3.5, GPT-4 variants), the approach measured classification accuracy and adjusted
  rand index (ARI) through hierarchical clustering of pairwise item similarities.
---

# Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis

## Quick Facts
- arXiv ID: 2506.23055
- Source URL: https://arxiv.org/abs/2506.23055
- Reference count: 23
- Primary result: GPT-4 achieves 66.2% classification accuracy in reconstructing psychological construct structure from questionnaire items, significantly outperforming smaller models.

## Executive Summary
This study develops a framework to evaluate how well large language models (LLMs) internalize human psychological concepts by comparing LLM-generated similarity scores between questionnaire items to established psychological constructs. Using 43 standardized questionnaires and six language models (BERT, OpenAI embedding, GPT-3.5, GPT-4 variants), the approach measures classification accuracy and adjusted rand index (ARI) through hierarchical clustering of pairwise item similarities. GPT-4 achieved the highest accuracy (66.2%, ARI 0.341), significantly outperforming GPT-3.5 (55.9%) and BERT (48.1%), all exceeding random baselines. Continuous and discrete prompting approaches showed no significant performance difference, while item ordering effects were observed in some questionnaires.

## Method Summary
The method computes pairwise semantic similarity scores between items from psychological questionnaires using either LLM prompts (for GPT models) or cosine similarity of text embeddings (for BERT/OpenAI models). These similarity matrices are then clustered using hierarchical clustering with predetermined numbers of categories (based on the original questionnaire structure). Classification accuracy and ARI are computed by comparing the clustering results to expert-labeled ground truth categories. The framework also aggregates item-level similarities across questionnaires to create cross-questionnaire similarity matrices, which are correlated with human response correlation matrices to assess whether LLM semantic relationships match human psychological relationships.

## Key Results
- GPT-4 achieved superior classification accuracy (66.2%, ARI 0.341) significantly outperforming GPT-3.5 (55.9%) and BERT (48.1%), all exceeding random baseline (31.9%)
- Continuous and discrete prompting approaches showed no significant performance difference (p=0.45, p=0.69)
- LLM-estimated semantic similarities correlate with human response patterns (mean r=0.412, p<0.05; median r=0.731, p<1e-5)
- Item ordering effects observed in some questionnaires (WDGOI, FFMQ-24) but not others (CBQ-Very Short)

## Why This Works (Mechanism)

### Mechanism 1: Semantic Similarity as Construct Proxy
- Claim: Pairwise semantic similarity judgments from LLMs can serve as proxies for latent psychological construct structure.
- Mechanism: LLMs compute similarity scores between questionnaire items (range [-1, 1] via prompt, or cosine similarity via embeddings); hierarchical clustering applied to resulting similarity matrices recovers construct categories with accuracy above random baseline.
- Core assumption: Psychological constructs manifest in linguistic patterns that are recoverable through text training data absorbed by LLMs.
- Evidence anchors:
  - [abstract] "Our method evaluates how accurately language models reconstruct and classify questionnaire items through pairwise similarity analysis."
  - [section 3.2] "For GPT-3.5 and GPT-4, we provide prompts instructing them to respond within the [-1, 1] range based on semantic similarity between questionnaire items."
  - [corpus] Related work on emotion organization (arxiv 2507.10599) supports hierarchical semantic structures in LLMs, though not directly validating this method.
- Break condition: If questionnaire items lack consistent linguistic markers for their constructs (e.g., abstract constructs with heterogeneous item wording), clustering accuracy should approach random baseline.

### Mechanism 2: Cross-Construct Semantic Distance Preservation
- Claim: LLM-estimated semantic distances across questionnaires correlate with human response correlations between those constructs.
- Mechanism: Mean/median pairwise similarity scores computed across items from different questionnaires produce a cross-questionnaire similarity matrix; this matrix correlates with Pearson correlation matrices from human respondent data.
- Core assumption: The statistical co-variation in human questionnaire responses reflects semantic relationships that LLMs encode from training text.
- Evidence anchors:
  - [abstract] "estimated semantic similarity from GPT-4 is associated with Pearson's correlation coefficients of human responses in multiple psychological questionnaires"
  - [section 4.2] "statistical significance with mean (r=0.412, p<0.05) and median scores (r=0.731, p<1e-5)"
  - [corpus] Weak direct validation; corpus lacks papers testing this specific cross-questionnaire correlation approach.
- Break condition: If human correlations are driven by response biases (e.g., acquiescence) rather than semantic content, LLM similarity should not correlate with human correlations.

### Mechanism 3: Model Scale Improves Construct Alignment
- Claim: Larger LLMs achieve higher construct classification accuracy, suggesting improved internalization of psychological concept structure.
- Mechanism: Greater parameter count and training data diversity enable finer-grained semantic distinctions relevant to psychological constructs.
- Core assumption: Performance gains reflect improved concept representation, not merely better instruction-following for similarity prompts.
- Evidence anchors:
  - [abstract] "GPT-4 model achieved superior classification accuracy (66.2%), significantly outperforming GPT-3.5 (55.9%) and BERT (48.1%)"
  - [section 4.1] "statistically significant higher classification performance with GPT-4 (0613/1106) (p < 0.001)"
  - [corpus] Corpus papers on concept-based interpretability (arxiv 2501.05855, 2504.09459) discuss concept representation but do not address scale effects directly.
- Break condition: If embedding-only models (no instruction-following) matched GPT-4 performance, mechanism would shift from "instruction-following + reasoning" to "pure semantic embedding quality."

## Foundational Learning

- Concept: Hierarchical Clustering with Predetermined K
  - Why needed here: The method uses hierarchical clustering with the known number of questionnaire sub-categories to generate predicted labels for comparison against ground truth.
  - Quick check question: Given a similarity matrix and k=3 clusters, can you trace how agglomerative clustering would merge items step-by-step?

- Concept: Adjusted Rand Index (ARI)
  - Why needed here: ARI provides a chance-cance-corrected measure of clustering agreement, complementing raw accuracy which can be inflated by class imbalance.
  - Quick check question: Why is ARI preferred over raw accuracy when cluster sizes are uneven?

- Concept: Prompt Engineering for Continuous Output
  - Why needed here: GPT models required prompt design to elicit numerical similarity scores rather than natural language responses.
  - Quick check question: How might prompt framing (expert persona, constrained output format) affect the distribution of similarity scores?

## Architecture Onboarding

- Component map:
  Input layer: 43 psychological questionnaires with expert-labeled sub-categories
  Similarity computation: BERT/embedding models (cosine similarity) OR GPT models (prompted similarity [-1,1])
  Clustering: Hierarchical clustering with predetermined k per questionnaire
  Evaluation: Classification accuracy and ARI against expert labels
  Cross-questionnaire extension: Mean/median similarity aggregation → correlation with human response matrices

- Critical path:
  1. Parse questionnaires into item lists with true labels
  2. Compute all pairwise similarities (O(n²) API calls for GPT models)
  3. Apply hierarchical clustering per questionnaire
  4. Compare cluster assignments to true labels
  5. For cross-questionnaire analysis: aggregate item-level similarities to construct-level distances

- Design tradeoffs:
  - Embedding models: Faster, deterministic, no API cost per comparison; but cannot follow complex similarity instructions
  - GPT models: Can follow nuanced similarity instructions; but slower, costlier, subject to ordering effects (see WDGOI, FFMQ-24 results)
  - Continuous vs. discrete prompts: No significant difference found (p=0.45, p=0.69), suggesting discrete Likert-style prompts may be preferable for interpretability without performance loss

- Failure signatures:
  - Accuracy near 31.9% random baseline → model not capturing construct-relevant semantics
  - Large variance across questionnaires → construct-specific linguistic markers vary in salience
  - Order effects (normal vs. reverse item ordering) → sensitivity to prompt structure; mitigate with averaging across orderings

- First 3 experiments:
  1. Replicate pairwise similarity + clustering pipeline on a held-out questionnaire set not used in original 43 to test generalization.
  2. Ablate prompt persona ("expert of natural language") to test whether expert framing contributes to accuracy gains.
  3. Compare embedding-only similarity (text-embedding-ada-002) against GPT-4 prompted similarity on identical questionnaires to isolate instruction-following vs. pure semantic encoding contributions.

## Open Questions the Paper Calls Out
None

## Limitations
- Results derived from 43 specific questionnaires may not extend to other psychological constructs or languages
- Expert-labeled questionnaire categories serve as the gold standard, but these may not fully capture the true psychological structure of constructs
- Claims about LLMs "internalizing" psychological concepts require stronger evidence beyond correlation with human responses, as alternative explanations (response pattern mimicry) exist

## Confidence
- **High Confidence**: Classification accuracy differences between models (GPT-4 > GPT-3.5 > BERT) are robust and well-supported by statistical tests (p < 0.001)
- **Medium Confidence**: The framework's ability to recover psychological construct structure via similarity clustering is validated, but depends on the quality and representativeness of the 43 questionnaires
- **Low Confidence**: Claims about LLMs "internalizing" psychological concepts require stronger evidence beyond correlation with human responses, as alternative explanations (response pattern mimicry) exist

## Next Checks
1. Apply the framework to psychological questionnaires from different domains (e.g., clinical vs. personality) to assess performance consistency across construct types
2. Compare LLM clustering results against alternative construct validity measures (e.g., factor analysis from human data) to validate against potential expert labeling biases
3. Control for acquiescence and social desirability effects in human response data to strengthen the case that LLM-human correlations reflect semantic understanding rather than shared response biases