---
ver: rpa2
title: Dataset Distillation via Committee Voting
arxiv_id: '2501.07575'
source_url: https://arxiv.org/abs/2501.07575
tags:
- dataset
- data
- cv-dd
- performance
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of dataset distillation by introducing
  a novel approach that leverages multiple models' perspectives through committee
  voting. The core method, Committee Voting for Dataset Distillation (CV-DD), combines
  the strengths of various models to generate high-quality synthetic datasets that
  better capture essential features and reduce overfitting.
---

# Dataset Distillation via Committee Voting

## Quick Facts
- arXiv ID: 2501.07575
- Source URL: https://arxiv.org/abs/2501.07575
- Reference count: 40
- One-line primary result: CV-DD achieves 59.5% top-1 accuracy on ImageNet-1K with ResNet18 at 50 IPC, surpassing previous methods by up to 3%

## Executive Summary
This paper introduces Committee Voting for Dataset Distillation (CV-DD), a novel approach that leverages multiple models' perspectives to generate high-quality synthetic datasets. By integrating distributions and predictions from a committee of models, CV-DD produces superior soft labels and enhances the robustness and adaptability of distilled data. The method addresses the challenge of dataset distillation by combining the strengths of various models to better capture essential features and reduce overfitting.

## Method Summary
CV-DD is built upon existing dataset distillation frameworks but introduces a key innovation: committee voting. The approach maintains a set of pre-trained models (the committee) and uses their combined predictions to generate synthetic data. During training, synthetic images are updated through gradient descent to minimize a voting-based loss function that aggregates the committee's outputs. The method also introduces Batch-Specific Soft Labeling (BSSL) to ensure soft labels are properly scaled to match the statistics of the current training batch, preventing vanishing gradient issues during optimization.

## Key Results
- On CIFAR-10 with ConvNet at 50 IPC, CV-DD achieves 92.4% accuracy, outperforming existing methods
- On CIFAR-100 with ResNet18 at 50 IPC, CV-DD achieves 65.6% accuracy, demonstrating strong performance on more challenging datasets
- On ImageNet-1K with ResNet18 at 50 IPC, CV-DD achieves 59.5% top-1 accuracy, surpassing previous state-of-the-art by up to 3%

## Why This Works (Mechanism)
CV-DD works by leveraging the diversity of multiple pre-trained models to create more robust and generalizable synthetic data. The committee voting mechanism ensures that the synthetic images capture the consensus of multiple perspectives, reducing the risk of overfitting to a single model's biases. The BSSL technique maintains proper scaling of soft labels throughout training, preventing optimization issues that plague other methods.

## Foundational Learning

**Dataset Distillation** - The process of creating a small synthetic dataset that retains the essential information of a much larger dataset. This is needed to reduce storage and computational requirements while maintaining model performance. Quick check: Verify that the synthetic dataset can train models to reasonable accuracy on the original task.

**Committee Voting** - An ensemble method where multiple models vote on predictions, with the consensus used as the final decision. This is needed to reduce individual model biases and improve robustness. Quick check: Ensure the voting mechanism produces consistent results across different committee compositions.

**Batch-Specific Soft Labeling** - A technique to dynamically adjust soft label statistics based on current batch characteristics during training. This is needed to prevent gradient vanishing and maintain stable optimization. Quick check: Monitor training stability and convergence speed with and without BSSL.

## Architecture Onboarding

**Component Map:** Synthetic Data Generator -> Committee Models -> Voting Mechanism -> BSSL Adjustment -> Updated Synthetic Data

**Critical Path:** The synthetic data is generated and passed through all committee models simultaneously. Their outputs are combined through voting, then BSSL is applied to create the final soft labels. Gradients flow back through this entire pipeline to update the synthetic data.

**Design Tradeoffs:** The main tradeoff is between committee size (diversity) and computational cost. Larger committees provide more diverse perspectives but require more computation and may introduce optimization challenges. The paper finds N=2 experts optimal.

**Failure Signatures:** Performance degradation when committee members are too similar (redundant information), when committee is too large (gradient conflicts), or when BSSL is improperly calibrated (vanishing gradients).

**First Experiments:**
1. Test CV-DD with a single committee member to establish baseline performance
2. Evaluate performance with different committee sizes (N=1,2,3) to find optimal configuration
3. Compare training stability with and without BSSL enabled

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the Committee Voting framework be effectively extended to non-visual data modalities, such as natural language processing or time-series analysis?
- Basis in paper: [explicit] The conclusion states, "Our future work will focus on applying the idea of Committee Voting to more modalities and applications of dataset distillation tasks."
- Why unresolved: The paper exclusively validates CV-DD on image datasets (CIFAR, ImageNet) using CNN architectures, leaving the interaction between voting mechanisms and sequential/textual data structures unexplored.
- What evidence would resolve it: Successful application of CV-DD to text classification benchmarks (e.g., GLUE) demonstrating improved generalization over single-model distillation baselines.

### Open Question 2
- Question: Does the Batch-Specific Soft Labeling (BSSL) technique remain effective for architectures that utilize Layer Normalization instead of Batch Normalization, such as Vision Transformers?
- Basis in paper: [inferred] The BSSL method (Section 3.7) is explicitly defined by recomputing batch-level mean and variance statistics (Equations 8-9), which presumes the use of Batch Normalization layers present in the committee models (ResNets, DenseNets).
- Why unresolved: The paper does not evaluate BSSL on normalization-agnostic architectures; Layer Normalization computes statistics per single sample, potentially nullifying the benefits of batch-specific statistical alignment.
- What evidence would resolve it: A comparative ablation study applying CV-DD to a ViT or MLP-Mixer committee, analyzing performance with and without the BSSL adjustment.

### Open Question 3
- Question: Why does increasing the number of participating experts ($N$) lead to performance degradation, and how can the voting strategy be stabilized for larger committees?
- Basis in paper: [inferred] Table 5 shows that increasing the number of experts from $N=2$ to $N=3$ consistently lowers accuracy across all IPC settings on CIFAR-100, which the authors note leads to "suboptimal optimization" without providing a theoretical remedy.
- Why unresolved: While the authors identify $N=2$ as optimal, they do not explain if the degradation is due to gradient conflicts, noise amplification, or the specific form of the voting loss, limiting the scalability of the committee.
- What evidence would resolve it: A theoretical analysis of gradient variance or conflict rates as $N$ increases, potentially leading to a modified voting loss function that tolerates more experts.

## Limitations
- Performance may degrade when committee members are too similar, reducing the diversity benefit of voting
- The approach introduces significant computational overhead from maintaining multiple models
- Limited evaluation on non-image modalities and architectures with different normalization schemes

## Confidence
- **High Confidence:** The core mechanism of combining multiple model predictions through committee voting is technically sound and aligns with established ensemble methods in machine learning.
- **Medium Confidence:** The reported performance improvements (e.g., 59.5% top-1 accuracy on ImageNet-1K) are likely valid within the tested configurations, but generalization to other settings requires further validation.
- **Medium Confidence:** The claim that CV-DD produces "superior soft labels" is supported by the experimental results, but the qualitative aspects of label quality and their impact on downstream tasks need more thorough investigation.

## Next Checks
1. **Ablation study on committee size and composition:** Systematically evaluate how varying the number of models in the committee and their architectural diversity affects distillation quality and computational efficiency.

2. **Cross-domain generalization test:** Apply CV-DD to non-standard datasets (e.g., medical imaging, scientific data) and evaluate whether the committee voting mechanism maintains its effectiveness when model predictions are less aligned.

3. **Resource efficiency analysis:** Conduct a detailed comparison of the computational overhead introduced by CV-DD relative to single-model distillation approaches, including training time, memory usage, and inference latency.