---
ver: rpa2
title: Enhancing LLM Planning Capabilities through Intrinsic Self-Critique
arxiv_id: '2512.24103'
source_url: https://arxiv.org/abs/2512.24103
tags:
- clear
- self-critique
- step
- action
- preconditions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an intrinsic self-critique method for improving
  the planning capabilities of large language models (LLMs) without external verification.
  The approach iteratively generates plans and critiques them using the same LLM,
  refining outputs until correctness is achieved.
---

# Enhancing LLM Planning Capabilities through Intrinsic Self-Critique

## Quick Facts
- arXiv ID: 2512.24103
- Source URL: https://arxiv.org/abs/2512.24103
- Reference count: 40
- Primary result: Iterative self-critique method improves LLM planning accuracy from 49.8% to 89.3% on Blocksworld benchmarks

## Executive Summary
This paper introduces an intrinsic self-critique method that improves large language models' planning capabilities without external verification. The approach iteratively generates plans and critiques them using the same LLM, refining outputs until correctness is achieved. Applied to planning benchmarks including Blocksworld, Logistics, and Mini-Grid, the method significantly improves performance—for example, increasing Blocksworld accuracy from 49.8% to 89.3% and achieving state-of-the-art results for LLM model checkpoints from October 2024.

## Method Summary
The method uses an iterative self-critique loop where an LLM generates plans, then critiques them against explicit preconditions, refining outputs until correctness is achieved. Key components include domain-specific prompts with PDDL definitions, self-consistency voting across multiple critique samples, and iterative refinement. The approach scales across multiple foundational models and demonstrates effectiveness even on obfuscated "Mystery Blocksworld" problems. The self-critique prompt includes explicit instructions to verify each action step-by-step using the domain definition, with failures accumulating in context for subsequent iterations.

## Key Results
- Blocksworld 3-5 accuracy improves from 49.8% to 89.3% using self-critique
- Self-consistency voting (5 votes) improves accuracy from 79.7% to 84.6% in 8-shot setting
- State-of-the-art results for LLM model checkpoints from October 2024 across multiple benchmarks
- Effective on obfuscated "Mystery Blocksworld" problems, demonstrating robustness to domain variations

## Why This Works (Mechanism)

### Mechanism 1
Iterative plan-critique-refinement loops improve planning accuracy without external verifiers. The LLM generates a plan → critiques it against explicit preconditions → accumulates failed attempts in context → regenerates with failure awareness. This creates a feedback signal from the model's own evaluation capabilities. Core assumption: The LLM can reliably detect precondition violations when explicitly instructed to verify each action step-by-step. Evidence: Algorithm 1 shows the loop; ablation studies confirm effectiveness. Break condition: High false-positive rate in self-critique causes premature termination.

### Mechanism 2
Providing explicit domain definitions with preconditions enables more accurate self-verification. The self-critique prompt includes the PDDL domain definition and instructs the LLM to verify each action sequentially, tracking state changes. Core assumption: LLMs can perform reliable symbolic state-tracking when given explicit rules. Evidence: Ablation shows removing domain definition drops accuracy from 79.5% to 74.4%; removing "verify each action" instruction drops it to 57.5%. Break condition: Domain definitions exceeding context length or implicit constraints not captured in PDDL.

### Mechanism 3
Self-consistency voting reduces false positives in self-critique evaluation. Run N independent self-critique calls, aggregate via majority voting to classify plans as correct/wrong. Ties default to "wrong," triggering another refinement iteration. Core assumption: Evaluation errors are somewhat independent across samples; averaging reduces noise. Evidence: Adding 5-vote self-consistency improves accuracy from 79.7% to 84.6%; nearly matches Oracle performance. Break condition: Systematic evaluation errors provide no benefit from voting.

## Foundational Learning

- **PDDL (Planning Domain Definition Language)**: The paper uses PDDL to define planning domains with explicit actions, preconditions, and effects. Understanding PDDL syntax is required to construct domain-specific prompts. Quick check: Given a PDDL action with preconditions `(and (clear ?x) (ontable ?x) (handempty))`, can you trace whether the action is executable from a given state?

- **Self-Consistency in LLMs**: The method uses majority voting over multiple LLM calls to improve evaluation reliability. Understanding when and why this works is critical for implementation. Quick check: If you run 5 self-critique samples and get [correct, correct, wrong, correct, wrong], what is the final classification?

- **Few-shot vs. Zero-shot Prompting**: The paper compares few-shot (16 exemplars) planning prompts with zero-shot self-critique prompts. Understanding this tradeoff is essential for balancing performance vs. setup cost. Quick check: Why might zero-shot self-critique be preferred over few-shot, even if few-shot achieves slightly higher accuracy?

## Architecture Onboarding

- **Component map**: PlanGenerator → SelfCritic → SelfConsistencyAggregator → PromptRevise → TerminationChecker
- **Critical path**: Construct planning prompt → Generate initial plan → Run self-critique → If "correct," return plan; else append to context and loop → Validate final plan with external PDDL validator
- **Design tradeoffs**: More few-shot exemplars → higher accuracy but longer context; self-consistency votes → higher accuracy but more compute; more iterations → diminishing returns
- **Failure signatures**: High false-positive rate, context overflow on longer problems, stuck loops with repeated errors
- **First 3 experiments**: 1) Replicate Blocksworld 3-5 baseline with 16-shot prompt and zero-shot critique, 2) Ablate self-consistency with N=1, 2, 5 votes, 3) Test generalization to new domain without domain-specific tuning

## Open Questions the Paper Calls Out

- **Integrating with search algorithms**: Can combining intrinsic self-critique with advanced search-based algorithms like Monte-Carlo Tree Search further improve planning accuracy on complex tasks beyond what either method achieves alone? The paper notes potential to swap in-context learning for more sophisticated planning methods or integrate with search algorithms.

- **Model scale requirements**: What minimum model scale or capability threshold is required for effective intrinsic self-critique in planning tasks? The paper notes Gemma-2 27B showed only modest improvement for Logistics and none for Blocksworld, suggesting larger models are better able to self-improve.

- **Reducing false positive rate**: How can the false positive rate in self-critique evaluations be reduced while maintaining high recall? The paper acknowledges the Oracle still substantially outperforms self-critique, indicating room for improvement in evaluation accuracy.

## Limitations

- Method depends on PDDL domain definitions; without explicit preconditions and effects, accuracy drops significantly (~15% degradation)
- Performance degrades on larger, more complex problems (e.g., Mini-Grid compared to Blocksworld)
- Scalability to domains with implicit constraints or non-symbolic reasoning remains untested

## Confidence

- **High Confidence**: The iterative self-critique loop mechanism and effectiveness of self-consistency voting are well-supported by ablation studies and comparative results
- **Medium Confidence**: Specific prompt engineering details are partially supported by ablation studies, but exact prompt templates and exemplars are not fully disclosed
- **Medium Confidence**: Claims about achieving "state-of-the-art results" are based on comparisons to prior LLM planning work, but exact benchmark comparisons are not detailed

## Next Checks

1. Implement the 3-step self-critique instruction and domain definition from Appendix A.2, then ablate each component (remove domain, remove "verify each action") to reproduce the ~15% accuracy drop shown in Table 4

2. Run the self-critique with N=1, 3, and 5 votes on a validation set to quantify the tradeoff between accuracy improvement and compute cost, confirming the ~4.9% absolute gain from 5 votes

3. Apply the same prompt templates to a new domain (e.g., Logistics) without domain-specific tuning to assess performance degradation and identify which components are truly generalizable versus domain-specific