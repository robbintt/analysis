---
ver: rpa2
title: 'REAMS: Reasoning Enhanced Algorithm for Maths Solving'
arxiv_id: '2509.16241'
source_url: https://arxiv.org/abs/2509.16241
tags:
- reasoning
- code
- mathematical
- problems
- solving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents REAMS, a reasoning-enhanced algorithm for solving
  complex university-level mathematics problems. The method combines zero-shot code
  generation with mathematical reasoning to improve accuracy and interpretability.
---

# REAMS: Reasoning Enhanced Algorithm for Maths Solving

## Quick Facts
- arXiv ID: 2509.16241
- Source URL: https://arxiv.org/abs/2509.16241
- Reference count: 25
- REAMS achieves 90.15% accuracy on university-level math problems, surpassing previous 81% benchmark

## Executive Summary
This paper presents REAMS, a reasoning-enhanced algorithm for solving complex university-level mathematics problems. The method combines zero-shot code generation with mathematical reasoning to improve accuracy and interpretability. CodeLlama 13B is first used to generate Python code for solving problems from MIT, Columbia University courses, and the MATH dataset. For problems where initial code fails, LLaMA 3.1 8B generates mathematical reasoning to guide a revised code generation, resulting in iterative refinement. REAMS achieves an accuracy of 90.15%, surpassing the previous benchmark of 81% set by Drori et al.

## Method Summary
The REAMS system employs a two-stage approach for solving university-level mathematics problems. First, CodeLlama 13B attempts zero-shot code generation to solve problems from various university courses and the MATH dataset. When code execution fails or produces incorrect results, LLaMA 3.1 8B generates step-by-step mathematical reasoning to identify the underlying principles and logic. This reasoning is then fed back to CodeLlama 13B as context for a revised code generation attempt. The iterative refinement process leverages external Python libraries (SymPy, NumPy, SciPy) for computation while the LLMs handle the translation from natural language to executable code.

## Key Results
- Achieves 90.15% accuracy on university-level math problems, surpassing the previous 81% benchmark
- Demonstrates significant performance gains through reasoning-guided iterative refinement
- Successfully handles problems from MIT courses (18.01, 18.02, 18.03, 18.05, 18.06, 6.042), Columbia COMS3251, and the MATH dataset

## Why This Works (Mechanism)

### Mechanism 1: Reasoning-Guided Iterative Refinement
- Claim: Integrating an intermediate reasoning step significantly improves the success rate of code generation for mathematical problems that fail initial zero-shot attempts.
- Mechanism: The system uses a feedback loop where LLaMA 3.1 8B generates mathematical reasoning when CodeLlama 13B fails, guiding a revised code generation.
- Core assumption: The reasoning model correctly identifies the mathematical principles the code model initially missed.
- Evidence anchors: Table 1 shows performance jumps when adding the reasoning step; related work suggests reasoning improves math benchmarks.
- Break condition: The reasoning model hallucinates invalid mathematical theorems or logic steps.

### Mechanism 2: Program Synthesis as a Reasoning Proxy
- Claim: Mapping natural language math problems to executable Python code yields higher accuracy than predicting answers directly.
- Mechanism: LLMs act as translators converting math problems into deterministic programs, offloading computation to Python interpreters and libraries.
- Core assumption: Mathematical problems can be solved using standard library functions available in Python.
- Evidence anchors: Section 4.1.1 details reliance on Python packages; combining "zero-shot code generation" improves accuracy.
- Break condition: Problems require algorithms not supported by installed libraries or are computationally intractable.

### Mechanism 3: Asymmetric Model Specialization
- Claim: Decoupling reasoning from code synthesis allows using smaller, specialized models optimized for distinct tasks.
- Mechanism: CodeLlama 13B handles code syntax and structure while LLaMA 3.1 8B handles semantic understanding and logic planning.
- Core assumption: The 8B parameter model possesses sufficient logical reasoning capability to guide the larger code model.
- Evidence anchors: Section 5.2 compares against CodeT5 with Reinforcement Learning, noting RL was computationally expensive; LLaMA 3.1 8B chosen for "efficient reasoning tasks."
- Break condition: Context window limits are exceeded if generated reasoning chains are excessively verbose.

## Foundational Learning

- **Concept: Zero-Shot vs. Few-Shot Prompting**
  - Why needed here: The paper claims improvement over a "few-shot" baseline using a "zero-shot" approach initially.
  - Quick check question: Does providing the "mathematical reasoning" in the second step effectively turn the second attempt into a few-shot prompt?

- **Concept: Symbolic Computation (SymPy/NumPy)**
  - Why needed here: The system relies on external tools; cannot debug failures without knowing if error comes from LLM or library.
  - Quick check question: Why would `sympy.solve` fail on a problem that `numpy.linalg.solve` might handle?

- **Concept: Automated Evaluation Pipelines**
  - Why needed here: The mechanism depends on executing untrusted code and comparing outputs.
  - Quick check question: How does the system compare generated code output against "expected" answer if formats differ?

## Architecture Onboarding

- **Component map:** Input -> CodeLlama 13B -> Python Runtime -> LLaMA 3.1 8B -> Router -> CodeLlama 13B -> Execution Environment
- **Critical path:** The transition from Execution Failure to Reasoning Generation
- **Design tradeoffs:** Complexity vs. Accuracy (trading RL complexity for second LLM inference overhead); Generalization vs. Library Constraints (opting for general Python libraries)
- **Failure signatures:** Silent Logic Errors (code executes but returns wrong answer); Import Errors (code generates calls to libraries not present); Timeouts (intractable problems)
- **First 3 experiments:**
  1. Baseline Reproduction: Run MIT dataset subset through CodeLlama 13B zero-shot to verify ~83% success rate
  2. Reasoning Ablation: Isolate LLaMA 3.1 8B step for failed cases, manually inspect generated reasoning validity
  3. Library Mapping: Test sample problems against imported libraries list to ensure environment matches model's training distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the REAMS framework be extended to solve problems requiring formal mathematical proofs?
- Basis in paper: The authors state in the Limitations section that the "model cannot handle questions that require formal proofs, as it lacks the capability to simulate or replace the logical processes necessary for proof-based solutions."
- Why unresolved: Current architecture relies on generating executable Python code, fundamentally different from constructing logical step-by-step derivations required in formal proofs.
- What evidence would resolve it: Successful application on formal mathematics benchmarks using proof assistants like Lean or Isabelle.

### Open Question 2
- Question: How can the methodology be adapted to handle computationally intractable problems or those requiring advanced algorithms not present in standard libraries?
- Basis in paper: The paper notes the approach "struggles with problems that require the application of advanced algorithms not supported by the available libraries" and fails on "computationally intractable problems, such as factoring very large primes."
- Why unresolved: System relies on synthesizing code from predefined Python packages, limiting scope to existing library functions.
- What evidence would resolve it: Demonstrating ability to decompose or solve problems exceeding computational complexity or library coverage.

### Open Question 3
- Question: Can the reasoning phase be refined to mitigate sensitivity to ambiguous or non-standard problem formulations?
- Basis in paper: Authors report "performance is sensitive to the clarity and precision of the problem statements, with ambiguities or non-standard formulations often leading to incorrect or incomplete code generation."
- Why unresolved: Pipeline lacks robust mechanism for clarifying ill-defined inputs before code generation.
- What evidence would resolve it: Ablation studies showing sustained high accuracy on adversarially paraphrased or ambiguously phrased versions.

## Limitations

- Cannot handle problems requiring formal mathematical proofs due to lack of proof-based reasoning capabilities
- Struggles with problems requiring advanced algorithms not supported by available Python libraries
- Performance is sensitive to clarity and precision of problem statements, with ambiguities leading to incorrect code generation

## Confidence

- **High Confidence:** The core mechanism of using reasoning-guided iterative refinement to improve code generation accuracy is well-supported
- **Medium Confidence:** The reported accuracy of 90.15% is plausible but exact reproducibility is limited by unknown prompt engineering and evaluation details
- **Low Confidence:** The specific claim of surpassing Drori et al.'s 81% benchmark cannot be independently verified without access to same evaluation set

## Next Checks

1. **Prompt Template Validation:** Contact authors to obtain exact system prompts used for CodeLlama and LLaMA 3.1 8B, test on small MATH dataset subset to verify pipeline functionality
2. **Output Normalization Protocol:** Implement strict answer comparison function handling format variations (fractions vs. decimals, lists vs. tuples), re-run MIT 18.01 subset to assess accuracy changes
3. **Reasoning Hallucination Audit:** For stratified sample of failed cases, manually inspect LLaMA 3.1 8B-generated reasoning, flag instances containing mathematical errors or non-sequiturs