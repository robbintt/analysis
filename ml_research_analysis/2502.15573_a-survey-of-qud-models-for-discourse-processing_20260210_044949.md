---
ver: rpa2
title: A Survey of QUD Models for Discourse Processing
arxiv_id: '2502.15573'
source_url: https://arxiv.org/abs/2502.15573
tags:
- discourse
- questions
- question
- which
- quds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey reviews three models for implementing the Question
  Under Discussion (QUD) framework in discourse processing: the QUD-tree approach,
  expectation-driven approach, and dependency-based approach. The QUD-tree approach
  uses hierarchical structures based on linguistic constraints to organize discourse
  segments, while the expectation-driven approach annotates questions evoked at discourse
  units and their answeredness in subsequent context.'
---

# A Survey of QUD Models for Discourse Processing

## Quick Facts
- arXiv ID: 2502.15573
- Source URL: https://arxiv.org/abs/2502.15573
- Authors: Yingxue Fu
- Reference count: 12
- Primary result: Reviews three QUD models (tree, expectation-driven, dependency-based) for discourse processing with varying structural assumptions and evaluation methods.

## Executive Summary
This survey examines three approaches to implementing the Question Under Discussion (QUD) framework in discourse processing: the QUD-tree approach that uses hierarchical structures based on linguistic constraints, the expectation-driven approach that tracks questions evoked at discourse units and their answeredness, and the dependency-based approach that anchors questions in previous sentences to create discourse dependency structures. The survey compares these models' properties, including their theoretical foundations, structural assumptions, and how they handle non-at-issue materials. It also examines studies exploring relationships between QUD and mainstream discourse frameworks (RST, PDTB, SDRT).

## Method Summary
The survey synthesizes existing research on QUD models by categorizing them into three main approaches based on their structural assumptions and annotation methods. For each approach, it examines the underlying theory, implementation details, and available corpora. The QUD-tree approach uses constraints like Q-A-Congruence and Q-Givenness to build hierarchical trees. The expectation-driven approach measures question answeredness through Likert-scale ratings. The dependency-based approach creates discourse dependencies by anchoring questions to prior sentences. The survey also analyzes studies comparing QUD to RST, PDTB, and SDRT frameworks.

## Key Results
- QUD-tree approach uses hierarchical trees with constraints (Q-A-Congruence, Q-Givenness, Maximize-Q-Anaphoricity, Back-to-the-Roots) to model discourse structure
- Expectation-driven approach tracks questions evoked at discourse units and measures their answeredness in subsequent context with moderate inter-annotator agreement
- Dependency-based approach anchors questions in previous sentences, achieving 41.8% high semantic similarity between annotators
- Current benchmarks include TED-Q for expectation-driven approach, DCQA for dependency-based approach, and German-English parallel corpora for QUD-tree approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical QUD trees capture discourse structure by modeling sentences as answers to nested questions.
- Mechanism: Discourse units are segmented and mapped to implicit questions organized hierarchically. Sentences answer the most immediate QUD, with constraints restricting valid question reconstructions.
- Core assumption: Discourse participants follow Gricean maxims—relevance drives questions to be answered promptly; quantity favors complete answers.
- Evidence anchors:
  - [abstract] "The QUD-tree approach, based on Roberts (2012), uses hierarchical trees to model discourse structure"
  - [section 2.1] "The QUD stack...the bottom of which is the overarching question of the discourse, called the superquestion"
  - [corpus] No direct corpus evidence for QUD-tree mechanisms; related work (QUDsim) addresses LLM discourse similarity, not hierarchical structuring.
- Break condition: When secondary structures or non-at-issue content interrupts the main QUD stack without returning, hierarchical coherence degrades.

### Mechanism 2
- Claim: Expectation-driven QUD models discourse by tracking questions evoked at each unit and measuring their subsequent answeredness.
- Mechanism: Annotators read texts incrementally at probe points, generating questions evoked by the context so far. Question answeredness is rated on a Likert scale when subsequent text addresses evoked questions.
- Core assumption: Discourse participants anticipate developments and generate implicit questions that coherent text should address.
- Evidence anchors:
  - [abstract] "the expectation-driven approach, following Onea (2016), annotates questions evoked at each discourse unit"
  - [section 2.2] "a weak but statistically significant correlation between question reliability and question answeredness in the subsequent context"
  - [corpus] No corpus validation of expectation-driven mechanisms in neighbors.
- Break condition: When evoked questions remain unanswered or answeredness ratings diverge significantly across annotators, the model loses predictive power.

### Mechanism 3
- Claim: Dependency-based QUD links sentences via implicit questions anchored in prior context.
- Mechanism: For each sentence S, identify an anchor sentence A in prior context and generate a free-form question Q such that S answers Q and Q is triggered by A.
- Core assumption: Every sentence (except the first) can be construed as answering an inquisitive question grounded in prior discourse.
- Evidence anchors:
  - [abstract] "the dependency-based approach...treats each sentence as an answer to an implicit question triggered by a previous sentence"
  - [section 2.3] "41.8% of the questions provided by different annotators are considered highly similar, while...40.7% are deemed semantically different"
  - [corpus] Weak corpus support; related work on discourse-aware retrieval (Beyond Chunking) uses RST, not QUD.
- Break condition: When sentences have no clear anchor or multiple equally plausible anchors, dependency structures become ambiguous.

## Foundational Learning

- Concept: **Rhetorical Structure Theory (RST) and coherence relations**
  - Why needed here: QUD models are compared against RST/PDTB/SDRT; understanding relation-based frameworks clarifies what QUD captures differently (hierarchical question structure vs. labeled relations).
  - Quick check question: Can you explain why RST distinguishes nucleus/satellite while QUD does not?

- Concept: **Information structure (focus/background)**
  - Why needed here: QUD-tree annotation requires identifying focus domains; Q-A-Congruence depends on matching focal alternatives to question alternatives.
  - Quick check question: In "Mary invited Alice," what is the focus if the QUD is "Who did Mary invite?"

- Concept: **Question semantics (q-alternative sets)**
  - Why needed here: Roberts' theory defines questions as partitioning possible worlds; evaluating answers requires understanding complete vs. partial answers.
  - Quick check question: What is the q-alternative set for "Did John leave?"

## Architecture Onboarding

- Component map:
  - QUD-tree: Segmenter → QUD reconstructor → Tree builder (uses Q-A-Congruence, Q-Givenness, Maximize-Q-Anaphoricity, Back-to-the-Roots constraints)
  - Expectation-driven: Incremental text reader → Question evocation module → Answeredness tracker → Similarity evaluator
  - Dependency-based: Anchor predictor → Question generator → (Optional) Reranker

- Critical path:
  1. Choose approach based on task: QUD-tree for hierarchical structure, expectation-driven for local coherence, dependency-based for sentence-level QA.
  2. For QUD-tree: Start with Riester (2019) constraints; train annotators on focus/background annotation first.
  3. For dependency parsing: Use Ko et al. (2023) pipeline—predict anchors before generating questions.

- Design tradeoffs:
  - QUD-tree: Rich structure but moderate inter-annotator agreement (κ=0.52); requires linguistic expertise.
  - Expectation-driven: Captures reader expectations but no global hierarchy; annotation coverage requires alternating probe points.
  - Dependency-based: Shallower structure but more scalable; free-form questions complicate evaluation.

- Failure signatures:
  - QUD-tree: Excessive non-at-issue annotations; low agreement on contrastive topics.
  - Expectation-driven: High variance in evoked questions; weak correlation between reliability and answeredness.
  - Dependency-based: Multiple anchors with similar plausibility; generated questions fail Q-Givenness (contain new information).

- First 3 experiments:
  1. Reproduce De Kuthy et al. (2020) rule-based QUD generation on a small corpus; evaluate via human judgment of Q-A-Congruence.
  2. Compare dependency-based anchor prediction against RST nucleus/satellite classification on DCQA dataset.
  3. Fine-tune an LLM to generate inquisitive questions; evaluate using QUDEVal criteria (well-formedness, answerability, givenness, anchor relatedness).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can hierarchical relationships between QUDs be effectively captured in dependency-based models?
- Basis in paper: [explicit] The paper states on page 7 that "A question that remains to be studied with this approach is how to capture the hierarchical relationship between QUDs," noting that current dependency links are shallower than QUD-trees.
- Why unresolved: The dependency-based approach (Ko et al., 2022) links sentences to previous anchors via questions but currently lacks a mechanism to organize these links into the deeper tree structures seen in Roberts (2012).
- What evidence would resolve it: A modified parsing algorithm or annotation schema that successfully derives hierarchical trees from dependency links, validated against QUD-tree gold standards.

### Open Question 2
- Question: Do specific question types (e.g., *why*-questions) map consistently to specific discourse relations across different frameworks?
- Basis in paper: [explicit] The conclusion notes it is an "under-studied question whether QUDs represent the same type of discourse information as that encoded by discourse relations," specifically asking "if why questions encode causal relations consistently."
- Why unresolved: While correlations exist (e.g., *why*-questions often align with causal PDTB relations), it is unclear if this mapping is consistent across RST and SDRT or if QUDs capture distinct discourse information.
- What evidence would resolve it: A comprehensive parallel annotation study showing statistically significant, bidirectional mapping rules between question types and discourse relation labels.

### Open Question 3
- Question: Can a two-step annotation process utilizing question templates improve inter-annotator agreement for QUD-trees?
- Basis in paper: [explicit] The conclusion suggests future work could adopt a "two-step approach... where QUDs are elicited first and then categorized based on a predefined set of question templates."
- Why unresolved: The paper highlights that achieving high inter-annotator agreement is challenging (citing a Kappa of 0.52 for QUD-trees) due to the unconstrained nature of free-form question generation.
- What evidence would resolve it: An experiment comparing standard free-form annotation against the two-step template method, demonstrating a statistically significant increase in Cohen’s Kappa.

## Limitations

- Limited empirical validation: The survey lacks direct experimental evidence demonstrating QUD models' superiority over traditional discourse frameworks for downstream tasks
- Dataset accessibility: Key corpora like TED-Q and QSALIENCE-data availability remains unclear, potentially blocking reproduction
- Variable inter-annotator agreement: QUD-tree approach shows moderate agreement (κ=0.52), with free-form questions particularly problematic (41.8% similar vs 40.7% different)

## Confidence

- High confidence: QUD framework definitions and the three-model taxonomy (QUD-tree, expectation-driven, dependency-based)
- Medium confidence: Comparative analysis of model properties and constraints
- Low confidence: Claims about QUD's superiority for discourse processing without task-specific benchmarks

## Next Checks

1. Replicate inter-annotator agreement studies on QUD-tree annotations using Riester et al. (2018) guidelines on a sample corpus
2. Compare dependency-based QUD parsing performance against RST parsing on the DCQA dataset using standard metrics
3. Conduct a controlled experiment testing whether QUD-enhanced discourse representations improve coherence prediction accuracy compared to PDTB-style annotations