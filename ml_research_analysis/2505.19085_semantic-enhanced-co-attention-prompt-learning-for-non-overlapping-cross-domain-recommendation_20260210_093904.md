---
ver: rpa2
title: Semantic-enhanced Co-attention Prompt Learning for Non-overlapping Cross-Domain
  Recommendation
arxiv_id: '2505.19085'
source_url: https://arxiv.org/abs/2505.19085
tags:
- domain
- u1d456
- recommendation
- domains
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Semantic-enhanced Co-attention Prompt Learning for Non-overlapping Cross-Domain Recommendation

## Quick Facts
- **arXiv ID:** 2505.19085
- **Source URL:** https://arxiv.org/abs/2505.19085
- **Reference count:** 40
- **Primary result:** Novel two-stage prompt learning framework for cross-domain recommendation with non-overlapping users/items

## Executive Summary
This paper addresses the challenging problem of many-to-one non-overlapping cross-domain sequential recommendation (MNCSR), where knowledge must transfer from multiple source domains to a target domain with no shared users or items. The proposed TCPLP framework bridges disjoint domains by encoding item titles into a shared semantic space using Pre-trained Language Models (PLMs) like BERT or Longformer, eliminating the need for overlapping entities. The model employs a two-stage learning strategy: pre-training on all domains to learn semantic representations, followed by prompt-tuning on the target domain while freezing the encoder and shared prompts. Experimental results on Amazon and cross-platform datasets demonstrate significant improvements over state-of-the-art methods, with up to 40.76% gains in NDCG@10 for specific domain pairs.

## Method Summary
The TCPLP framework addresses non-overlapping cross-domain recommendation by representing items through their textual descriptions rather than IDs, enabling semantic bridging across disjoint domains. The architecture combines a text sequence encoder (BERT/Longformer) with a co-attention prompt encoder that learns domain-shared and domain-specific prompts. The method uses a two-stage training approach: first pre-training the text encoder and prompts on all domains using contrastive loss, then fine-tuning only the domain-specific prompts on the target domain while freezing the encoder and shared prompts. This parameter-efficient approach captures invariant features across domains while preserving unique domain characteristics, achieving state-of-the-art performance on multiple benchmark datasets.

## Key Results
- TCPLP outperforms state-of-the-art baselines by 5.25% to 40.76% on NDCG@10 across different domain pairs
- Longformer-based TCPLP achieves best performance, demonstrating effectiveness of handling longer item titles
- Two-stage learning strategy shows significant advantage over one-stage approaches, validating the freezing strategy
- Ablation studies confirm the importance of both domain-shared and domain-specific prompts for optimal performance

## Why This Works (Mechanism)

### Mechanism 1: Semantic Alignment via Textual Universality
- **Claim:** The system bridges disjoint domains by mapping non-overlapping items into a shared high-dimensional semantic space, bypassing the need for ID-based overlapping entities.
- **Mechanism:** Item titles are processed by a Pre-trained Language Model (PLM) like BERT or Longformer. This converts sparse, disjoint ID sequences into dense text embeddings that share a common vocabulary and semantic geometry (e.g., "Marker Pen" and "Sketchbook" share semantic relationships regardless of domain ID).
- **Core assumption:** Item titles contain sufficient descriptive power to capture the necessary collaborative signals usually encoded in interaction IDs.
- **Evidence anchors:**
  - [abstract] "We capture semantic meanings by representing items through text rather than IDs, leveraging natural language universality..."
  - [section 3.3] "To connect the disjoint domains, we resort to the commonality of natural language... which can represent items in the general semantic space."
  - [corpus] Related work (e.g., KGBridge) validates the trend of using external knowledge/semantics to bridge non-overlapping gaps, though corpus citation counts are low (emerging field).
- **Break condition:** If items have uninformative titles (e.g., "Product 123") or if the PLM lacks domain-specific vocabulary, the semantic bridge collapses.

### Mechanism 2: Disentangled Domain Feature Extraction
- **Claim:** Explicitly separating domain knowledge into "shared" and "specific" learnable prompts reduces negative transfer and preserves unique domain characteristics.
- **Mechanism:** The model utilizes two prompt sets: Domain-shared prompts ($P_{share}$) capture invariant features (e.g., general user interest patterns), while Domain-specific prompts ($P_{specific}$) capture unique domain distributions. A Co-attention network dynamically weights these prompts based on the current sequence context.
- **Core assumption:** Cross-domain knowledge is strictly decomposable into invariant and domain-dependent components.
- **Evidence anchors:**
  - [abstract] "...two types of prompts, i.e., domain-shared and domain-specific prompts, are devised..."
  - [section 3.4] "The domain-shared prompt is responsible for learning the invariant domain features... while the domain-specific prompt focuses on learning the unique features."
  - [corpus] Paper "Disentangled Preference-Guided Diffusion" supports the efficacy of disentanglement in CDSR, reinforcing the validity of this separation logic.
- **Break condition:** If source and target domains have zero shared behavioral patterns (orthogonal distributions), the "shared" prompt may overfit to noise or fail to converge.

### Mechanism 3: Two-Stage Knowledge Adaptation
- **Claim:** Freezing the PLM and shared prompts during the tuning stage stabilizes the transfer of prior knowledge while preventing overfitting to sparse target domain data.
- **Mechanism:** The "Pre-train & Prompt-tuning" strategy first learns a robust semantic encoder using multi-domain data. In the second stage, it freezes the heavy encoder and shared prompts, updating only the lightweight specific prompts and the MLP projection head. This acts as a parameter-efficient fine-tuning (PEFT) method.
- **Core assumption:** The pre-training phase captures sufficient "prior knowledge" that remains valid for the target domain without further weight updates to the encoder.
- **Evidence anchors:**
  - [abstract] "...develop a two-stage learning strategy, i.e., pre-train & prompt-tuning paradigm..."
  - [section 3.6] "...we freeze the parameters in the text sequence encoder, domain-shared prompts... We only fine-tune the parameters in the domain-specific prompts..."
  - [corpus] "X-Cross" and other neighbor papers similarly utilize dynamic integration or tuning strategies, suggesting two-stage adaptation is a robust trend for this problem class.
- **Break condition:** If the target domain possesses a unique semantic dialect or item type unseen in pre-training, the frozen encoder will fail to represent it accurately.

## Foundational Learning

### Concept: Soft Prompt Learning (Continuous Prompts)
- **Why needed here:** Essential for understanding how the model injects domain knowledge without modifying the massive PLM weights. Unlike discrete text prompts, these are learnable tensors optimized via backpropagation.
- **Quick check question:** Can you explain how a "learnable prompt" differs from standard input embeddings in a Transformer?

### Concept: Co-attention Networks
- **Why needed here:** The paper uses co-attention to fuse sequence context with prompt context. You must understand how Q/K/V matrices are split or shared to allow one modality (sequence) to guide the attention of another (prompts).
- **Quick check question:** In this architecture, does the sequence representation guide the prompt encoding, or do the prompts guide the sequence encoding? (Hint: See Eq 4).

### Concept: Contrastive Learning (InfoNCE)
- **Why needed here:** The optimization objective uses in-batch negatives rather than cross-entropy. Understanding how positive pairs (sequence, target item) are pulled together while negatives are pushed apart is critical for debugging convergence.
- **Quick check question:** Why might in-batch negatives from *other domains* be beneficial during the pre-training stage compared to random negatives?

## Architecture Onboarding

- **Component map:** Input Layer (Item Title Tokenizer) -> PLM (BERT/Longformer) -> [CLS] Embedding -> Prompt Encoder (Learnable Prompts + Sequence Representation) -> Multi-Head Co-Attention -> Weighted Prompts -> Fusion Layer (Concatenate + MLP) -> Final Sequence Representation -> Objective (Contrastive Loss)

- **Critical path:** The co-attention mechanism (Eq. 4-7) is the most complex interaction point. Start by tracing how the sequence query ($Q$) attends to the prompt keys ($K$) to generate the prompt values ($V$). Ensure the gradient flows correctly from the contrastive loss back through the prompts.

- **Design tradeoffs:**
  - **Longformer vs. BERT:** Longformer handles longer sequences (up to 1024 tokens) but adds computational overhead. The paper notes Longformer performs better on dense datasets with longer titles.
  - **Freezing vs. Full Fine-tuning:** Freezing the PLM saves memory but limits adaptation to new linguistic patterns in the target domain.
  - **Prompt Count ($n_K$):** Higher counts allow richer knowledge but risk overfitting. Paper suggests $n_K=2$ is optimal for the tested datasets.

- **Failure signatures:**
  - **Mode Collapse:** If the contrastive loss goes to zero too fast, the model may have stopped distinguishing items. Check the temperature coefficient ($\tau$).
  - **Semantic Gap:** If the target domain is "Automotive" and source is "Arts," and performance is random, the "Shared Prompt" may be failing to find invariant features (Mechanism 2 break).

- **First 3 experiments:**
  1. **Sanity Check (ID vs Text):** Run a baseline ID-only model vs. the Text-only input (no prompts) to isolate the value added by the semantic text encoder.
  2. **Ablation (Shared vs Specific):** Disable the "Domain-shared" prompts during tuning to verify if the model relies solely on domain-specific signals or if the frozen prior knowledge is actually contributing.
  3. **Hyperparameter Sensitivity:** Run a sweep on the prompt context number ($n_K \in \{1, 2, 4, 8\}$) to verify the paper's claim that a small number (2) is sufficient to encapsulate domain knowledge.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the integration of atomic Item IDs with text-based representations improve recommendation performance compared to text-only methods?
- **Basis in paper:** [explicit] The Conclusion states, "One limitation of this work is that we do not simultaneously consider the atomic Item IDs for representation learning... We will take solving the above limitation as one of our future works."
- **Why unresolved:** The current TCPLP architecture exclusively uses text to bridge non-overlapping domains, potentially discarding unique collaborative signals that atomic IDs capture in single-domain contexts.
- **What evidence would resolve it:** A hybrid model evaluation showing performance differences between text-only, ID-only, and combined representations on the same MNCSR tasks.

### Open Question 2
- **Question:** How does TCPLP perform when item textual descriptions are sparse, noisy, or ambiguous?
- **Basis in paper:** [inferred] The method relies heavily on the "universality of natural language" (Section 3.3) and uses item titles as the sole input.
- **Why unresolved:** The paper utilizes rich Amazon metadata, but it does not evaluate scenarios where text is low-quality or missing, which is common in real-world "cold-start" or user-generated content scenarios.
- **What evidence would resolve it:** Experiments on datasets with varying text quality levels or with items lacking descriptive titles.

### Open Question 3
- **Question:** Is the computational efficiency of using Pre-trained Language Models (PLMs) like Longformer viable for large-scale, real-time inference?
- **Basis in paper:** [inferred] Section 4.4 notes the use of Longformer and BERT, which are significantly heavier than the ID-based embeddings used in baselines like GRU4Rec.
- **Why unresolved:** While the paper demonstrates effectiveness (accuracy), it does not analyze the latency or throughput trade-offs required for industrial deployment.
- **What evidence would resolve it:** A comparison of training and inference latency/throughput against ID-based baselines on industrial-scale datasets.

## Limitations

- The method relies exclusively on item titles for semantic bridging, potentially missing collaborative signals that atomic IDs provide in single-domain contexts
- Performance may degrade significantly when item descriptions are sparse, uninformative, or contain domain-specific terminology not captured by the PLM vocabulary
- The computational overhead of using heavy PLMs like Longformer raises concerns about scalability and real-time inference capabilities for industrial applications

## Confidence

- **High Confidence:** The architectural framework (BERT/Longformer + prompt learning + co-attention) is technically sound and follows established PLM fine-tuning practices. The contrastive learning objective is well-defined and standard for representation learning.
- **Medium Confidence:** The specific design choices (freezing PLM during prompt-tuning, using only 2 prompt contexts) are justified by ablation studies in the paper, but may not generalize to all domain pairs or dataset sizes.
- **Low Confidence:** The assumption that text-based semantic bridging can fully replace ID-based collaborative signals in non-overlapping scenarios remains empirically unproven, particularly for domains where item semantics don't correlate with user preferences.

## Next Checks

1. **Semantic Bridge Validation:** Create a controlled experiment where item titles are replaced with random or uninformative text while preserving ID sequences. Compare performance to isolate whether the semantic encoder is actually learning meaningful collaborative patterns versus memorizing text patterns.

2. **Disentanglement Robustness:** Test the model on domain pairs with known shared behavioral patterns (e.g., "Electronics" → "Office Supplies") versus orthogonal domains (e.g., "Automotive" → "Baby Products"). Measure whether shared prompts consistently capture invariant features or simply overfit to noise in unrelated domains.

3. **Transfer Gap Analysis:** For each target domain, measure the performance gap between pre-training on all source domains versus pre-training on only semantically similar sources. This would reveal whether the "universal semantic space" assumption holds or if domain-specific pre-training is necessary.