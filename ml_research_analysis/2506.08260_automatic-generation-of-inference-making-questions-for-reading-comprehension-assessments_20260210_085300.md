---
ver: rpa2
title: Automatic Generation of Inference Making Questions for Reading Comprehension
  Assessments
arxiv_id: '2506.08260'
source_url: https://arxiv.org/abs/2506.08260
tags:
- questions
- inference
- item
- question
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study explored using GPT-4o to automatically generate inference-making\
  \ reading comprehension questions for grades 3-12. A taxonomy of three bridging\
  \ inference types\u2014pronominal bridging, text-connecting, and gap-filling\u2014\
  was developed and validated against an operational item bank, where bridging inferences\
  \ comprised 51% of items."
---

# Automatic Generation of Inference Making Questions for Reading Comprehension Assessments

## Quick Facts
- arXiv ID: 2506.08260
- Source URL: https://arxiv.org/abs/2506.08260
- Reference count: 20
- Primary result: GPT-4o generated 93.8% high-quality reading comprehension questions, but only 42.6% matched targeted inference types, with gap-filling easiest and text-connecting most difficult to generate

## Executive Summary
This study demonstrates that GPT-4o can automatically generate high-quality reading comprehension questions for grades 3-12 using few-shot prompting, though targeting specific inference types remains challenging. The authors developed a taxonomy of three bridging inference types—pronominal bridging, text-connecting, and gap-filling—validated against an operational item bank where bridging inferences comprised 51% of items. While 93.8% of generated questions were operationally sound, only 42.6% matched the targeted inference type, with text-connecting inferences being particularly difficult (24% match). The findings suggest that combining automatic generation with human review offers a scalable approach to diagnostic reading assessments, despite limitations in precise inference-type targeting.

## Method Summary
The study employed GPT-4o with few-shot prompting using 4 or 6 training passages from Simple English Wikipedia (342-508 words each) to generate 357 reading comprehension questions across three inference types. Training examples included 58 manually-authored questions with text hints and reasoning. Four prompting conditions were tested: standard_4, standard_6, CoT_4, and CoT_6. Human experts evaluated generated questions on general item quality, inference-type accuracy, and reasoning quality using a three-rater majority vote system. The taxonomy was validated against an operational item bank to ensure coverage of the full inference spectrum required for diagnostic assessments.

## Key Results
- GPT-4o produced 93.8% of questions meeting general item quality standards (correct answer, non-confusing distractors, grade-appropriate)
- Only 42.6% of generated questions accurately matched the targeted inference type, with gap-filling easiest (60% match) and text-connecting most difficult (24% match)
- The overall distribution of generated inference types (34% factual, 22% gap-filling, 32% pronominal, 12% text-connecting) closely matched the operational item bank distribution (44%, 12%, 32%, 13%)
- CoT prompting provided no statistically significant improvement over standard prompting for inference-type accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Few-shot prompting with structured, taxonomy-aligned examples enables GPT-4o to generate high-quality RC questions, but not with reliable inference-type specificity.
- **Mechanism:** The system prompt decomposes expert reasoning into explicit steps (e.g., "find a pronoun connecting 2–3 sentences" for pronominal bridging). Training examples demonstrate the expected output format and reasoning pattern. The model leverages its pretrained text comprehension abilities to imitate this structure on new passages.
- **Core assumption:** LLMs can surface latent reasoning capabilities through in-context demonstration, even without fine-tuning.
- **Evidence anchors:**
  - [abstract] "GPT-4o produced 93.8% good-quality questions... however, only 42.6% of the generated questions accurately matched the targeted inference type."
  - [section 4.2] "We set the temperature parameter to 0... frequency penalty 0.2 proved optimal as it could consistently generate three diverse RC items without compromising quality."
  - [corpus] Limited direct corpus evidence on few-shot efficacy for inference-typed generation; related work (Säuberli & Clematide, 2024) shows structured rationales improve multi-hop question generation, but this task appears more demanding.
- **Break condition:** If training examples lack diversity or the taxonomy categories are cognitively ambiguous to the model, few-shot generalization degrades. Text-connecting inferences (24.1% accuracy) suggest this category may be under-specified or harder to operationalize.

### Mechanism 2
- **Claim:** Human expert review remains necessary for diagnostic validity, particularly for verifying inference type.
- **Mechanism:** Experts evaluate on three dimensions: general item quality, inference-type accuracy, and reasoning quality. High inter-rater agreement (κ > 0.77 for inference type) indicates reliable human judgment for the taxonomy, but model-generated reasoning was adequate only 37.2% of the time.
- **Core assumption:** Expert annotation reflects ground truth for inference categorization, and the taxonomy is valid for operational use.
- **Evidence anchors:**
  - [abstract] "Human experts evaluated them on item quality, inference-type accuracy, and reasoning quality, achieving inter-rater agreements above 0.90."
  - [section 5] "GPT-4o provided adequate reasoning for only 38.9% of the items. This finding may explain the lack of performance gains when moving from standard prompting to CoT prompting."
  - [corpus] Related work (Stasaski et al., 2021) confirms reasoning-related judgments are inherently difficult to rate, consistent with lower initial agreement on reasoning quality (65–71%).
- **Break condition:** If expert disagreement increases (e.g., with more ambiguous passages or edge-case inferences), human review throughput becomes a bottleneck.

### Mechanism 3
- **Claim:** Distribution-level coverage emerges even when instance-level targeting fails.
- **Mechanism:** When prompted across multiple inference types, the aggregate output distribution (34% factual/literal, 22% gap-filling, 32% pronominal bridging, 12% text-connecting) approximates the operational item bank (44%, 12%, 32%, 13%). This suggests the model's default generation tendencies align with natural inference frequencies in expository text.
- **Core assumption:** Passage characteristics and language statistics bias LLMs toward certain inference types regardless of prompting.
- **Evidence anchors:**
  - [section 5, Figure 6] "The overall distribution of inference types in the LLM-generated items closely matches that of our operational RC item bank."
  - [corpus] No direct corpus evidence on distributional alignment; this appears novel to the study.
- **Break condition:** If prompts over-specify rare inference types or passages lack suitable locations, forced generation produces low-quality items.

## Foundational Learning

- **Bridging inference taxonomy (pronominal bridging, text-connecting, gap-filling):**
  - Why needed: Understanding the target construct is essential for evaluating whether generated items are diagnostically valid.
  - Quick check: Given "The campfire burned uncontrollably. Tom grabbed water," what inference type is required? (Answer: gap-filling, requires world knowledge that water extinguishes fire.)

- **Few-shot prompting mechanics:**
  - Why needed: The study relies entirely on in-context learning; understanding temperature, frequency penalty, and example formatting is prerequisite to reproduction.
  - Quick check: Why did the authors set temperature to 0? (Answer: To prioritize accuracy and reproducibility over diversity.)

- **Reading comprehension assessment design (distractor quality, developmental appropriateness):**
  - Why needed: General item quality evaluation conflates correctness, distractor plausibility, and safety—knowing these criteria ensures proper human review.
  - Quick check: What disqualifies an item from being "high quality"? (Answer: Multiple correct keys, confusing wording, developmentally inappropriate content.)

## Architecture Onboarding

- **Component map:**
  Taxonomy definition -> Training example bank -> Prompting module -> Generation output -> Human evaluation -> Quality gates

- **Critical path:**
  Taxonomy → Training examples → Prompt construction → GPT-4o generation → Human review → Accept/reject → Operational item bank

- **Design tradeoffs:**
  - **Standard vs. CoT prompting:** CoT adds reasoning output but showed no statistically significant improvement (37.2% reasoning quality vs. 42.6% inference accuracy).
  - **4 vs. 6 training examples:** More examples improved quality (96.7% in CoT_6 vs. 90.0% in CoT_4) but inference-type accuracy remained similar.
  - **Temperature 0 vs. higher:** Prioritizes reproducibility over diversity; frequency penalty 0.2 balanced item variety without degradation.
  - **Human review effort:** High inter-rater agreement requires calibration rounds; inference-type and reasoning judgments needed a second review pass.

- **Failure signatures:**
  - Multiple correct keys (e.g., "Where can people buy doughnuts?" with both "doughnut shop" and "bakery" as plausible)
  - Introduction of vocabulary not in passage
  - Confusing question stems or overly long correct answers
  - Factual/literal drift (34.8% of generated items required no inference)
  - Text-connecting generation failure (only 24.1% accuracy)

- **First 3 experiments:**
  1. **Replicate with 10 new passages:** Use the released training item bank, apply standard_6 and CoT_6 prompts, measure whether 93%+ quality and ~43% inference accuracy hold.
  2. **Ablate inference-type specificity:** Generate without taxonomy constraints and compare distribution to prompt-conditioned output—test whether distributional alignment is prompt-driven or passage-driven.
  3. **Test alternative models:** Run the same pipeline with GPT-4-turbo, Claude, or open-source models to assess whether reasoning quality correlates with inference-type accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- The operational item bank used for validation is not publicly available, limiting independent verification of distributional claims
- The 10 evaluation passages are adapted from Simple Wikipedia but not released, preventing exact reproduction
- Text-connecting inference generation accuracy remains problematic (24% match), suggesting taxonomy specification may be insufficient
- Human expert availability and training requirements (2-round annotation) may limit scalability despite automated generation

## Confidence

**High Confidence:** Item quality generation (93.8% good quality), distribution-level alignment with operational bank, human evaluation methodology and inter-rater reliability

**Medium Confidence:** Inference-type targeting accuracy (42.6% overall), CoT prompting benefits (no significant improvement observed), taxonomy validity for bridging inferences

**Low Confidence:** Generalizability beyond Simple Wikipedia passages, scalability of human review process, long-term stability of few-shot prompting with evolving LLMs

## Next Checks

1. **Distribution Verification:** Obtain access to operational item bank and confirm that LLM-generated distribution (34% factual, 22% gap-filling, 32% pronominal, 12% text-connecting) matches reported validation sample (44%, 12%, 32%, 13%)

2. **Prompt Sensitivity Analysis:** Systematically vary training example count (2, 4, 6, 8) and example diversity to identify minimum effective few-shot requirements and optimize inference-type targeting accuracy

3. **Cross-Model Comparison:** Apply identical pipeline to GPT-4-turbo, Claude-3, and an open-source model (e.g., Llama-3-70B) to assess whether high-quality generation is model-dependent or architecture-general