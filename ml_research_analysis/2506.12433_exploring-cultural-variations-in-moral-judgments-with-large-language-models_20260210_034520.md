---
ver: rpa2
title: Exploring Cultural Variations in Moral Judgments with Large Language Models
arxiv_id: '2506.12433'
source_url: https://arxiv.org/abs/2506.12433
tags:
- moral
- llms
- cultural
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates whether Large Language Models (LLMs) reflect
  cross-cultural moral norms by comparing their predictions to the World Values Survey
  (WVS) and Pew Research Global Attitudes Survey (PEW). Using log-probability-based
  moral justifiability scores, we assessed 20 models ranging from GPT-2 to GPT-4o.
---

# Exploring Cultural Variations in Moral Judgments with Large Language Models

## Quick Facts
- arXiv ID: 2506.12433
- Source URL: https://arxiv.org/abs/2506.12433
- Reference count: 5
- Large Language Models show significant cultural bias, with best performance aligning with Western European and North American moral norms

## Executive Summary
This study investigates whether Large Language Models (LLMs) reflect cross-cultural moral norms by comparing their predictions to survey data from the World Values Survey and Pew Research Global Attitudes Survey. Using log-probability-based moral justifiability scores, the researchers assessed 20 models ranging from GPT-2 to GPT-4o. The results reveal substantial cultural alignment gaps, with instruction-tuned models like GPT-4o achieving higher correlations with survey data compared to smaller or base models. A significant finding is the pronounced W.E.I.R.D. (Western, Educated, Industrialized, Rich, and Democratic) bias, where models align best with Western Europe and North America but perform poorly for Sub-Saharan Africa and MENA regions. The study highlights the need for more diverse training data and bias mitigation strategies to improve cross-cultural moral reasoning in LLMs.

## Method Summary
The researchers employed a systematic evaluation approach using log-probability-based moral justifiability scores to assess 20 different Large Language Models. They compared LLM predictions against two major cross-cultural survey datasets: the World Values Survey (WVS) and Pew Research Global Attitudes Survey (PEW). The methodology involved analyzing model responses to moral scenarios across different cultural regions and topics, with particular attention to measuring correlations between model outputs and human survey responses. Instruction-tuned models were specifically compared against base models to evaluate the impact of fine-tuning on cultural alignment performance.

## Key Results
- Instruction-tuned models like GPT-4o and Gemma-2-9B-IT achieve substantially higher correlations with survey data (r â‰¥ 0.4) compared to smaller or base models
- Models show strong W.E.I.R.D. bias, aligning best with Western Europe and North America but performing poorly for Sub-Saharan Africa and MENA regions
- Topic difficulty varies significantly, with issues like political violence showing high error rates across all models

## Why This Works (Mechanism)
The study's methodology leverages the relationship between log-probability scores and moral justifiability assessments. By comparing LLM outputs against established cross-cultural survey data, the researchers can quantify how well models reflect actual human moral reasoning across different cultural contexts. The use of instruction-tuned models versus base models allows isolation of fine-tuning effects on cultural alignment, while the diverse model range (from GPT-2 to GPT-4o) provides insights into how model scale affects cross-cultural performance.

## Foundational Learning
1. **Cross-cultural survey methodology** - Understanding how large-scale surveys capture moral attitudes across different societies
   - Why needed: Provides the ground truth data for evaluating LLM cultural alignment
   - Quick check: Survey sample sizes and regional representation are sufficient for robust analysis

2. **Log-probability scoring in NLP** - The mathematical framework for converting token probabilities into interpretable moral justifiability scores
   - Why needed: Enables quantitative comparison between LLM outputs and human survey responses
   - Quick check: Log-probabilities correlate with human judgments in controlled settings

3. **W.E.I.R.D. bias in AI systems** - The systematic overrepresentation of Western cultural perspectives in AI training data and outputs
   - Why needed: Critical context for interpreting why models perform better on Western moral norms
   - Quick check: Training data composition analysis reveals cultural distribution patterns

4. **Instruction tuning effects** - How fine-tuning on specific instruction datasets impacts model behavior and capabilities
   - Why needed: Explains performance differences between instruction-tuned and base models
   - Quick check: Controlled experiments comparing tuned vs. base versions of same architecture

5. **Correlation analysis in AI evaluation** - Statistical methods for measuring agreement between model predictions and human judgments
   - Why needed: Provides quantitative metrics for cultural alignment assessment
- Quick check: Correlation coefficients are statistically significant and not due to chance

## Architecture Onboarding
Component Map: Survey Data -> LLM Input Processing -> Log-Probability Scoring -> Correlation Analysis -> Cultural Alignment Assessment

Critical Path: The evaluation pipeline follows a linear flow where survey data defines moral scenarios, LLMs generate responses, log-probabilities are computed, and correlations with human judgments are calculated to assess cultural alignment.

Design Tradeoffs: The study balances comprehensive model coverage against computational constraints, using English-only prompts for consistency but potentially missing multilingual cultural nuances. The choice of log-probability scoring provides quantitative rigor but may not capture all aspects of moral reasoning complexity.

Failure Signatures: Low or negative correlations indicate cultural misalignment, particularly pronounced for non-Western regions. High error rates on specific topics like political violence suggest topic-specific limitations rather than general cultural bias.

First Experiments:
1. Test model performance on moral scenarios specifically designed to probe cultural differences
2. Compare correlation results using different scoring methodologies (e.g., human evaluation vs. log-probability)
3. Analyze training data composition to identify potential sources of cultural bias

## Open Questions the Paper Calls Out
None

## Limitations
- Potential measurement artifacts in survey-based ground truth data that may confound results
- Relatively narrow set of 20 models tested, limiting generalizability across the broader LLM landscape
- Focus on English-language prompts may not capture full cultural nuance and could introduce additional bias

## Confidence
- High confidence in general trend that larger, instruction-tuned models perform better
- Medium confidence in specific regional performance differences
- Low confidence in absolute correlation values due to potential confounding factors

## Next Checks
1. Replicate analysis using multilingual prompts and cross-lingual evaluation to test whether observed cultural biases persist across languages
2. Conduct ablation studies systematically removing different training data sources to isolate sources of cultural bias
3. Validate log-probability-based scoring against human annotations for a subset of moral scenarios to confirm measurement validity