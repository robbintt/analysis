---
ver: rpa2
title: Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence
arxiv_id: '2511.07384'
source_url: https://arxiv.org/abs/2511.07384
tags:
- recurrence
- train
- training
- accuracy
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that pretrained non-recurrent language models
  can be efficiently converted into depth-recurrent models that outperform their static
  counterparts on reasoning tasks while using fewer unique parameters. The method
  involves surgically removing layers from a pretrained model, keeping early layers
  as a prelude, later layers as a recurrent block, and a coda, and then post-training
  with a curriculum that gradually increases recurrence depth.
---

# Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence

## Quick Facts
- **arXiv ID**: 2511.07384
- **Source URL**: https://arxiv.org/abs/2511.07384
- **Reference count**: 40
- **Primary result**: Depth-recurrent models outperform static LLMs on math reasoning while using fewer unique parameters and enabling test-time scaling.

## Executive Summary
This paper introduces a method to convert pretrained non-recurrent language models into depth-recurrent models that can perform repeated internal computations at test time. By surgically removing layers and retraining with a recurrence curriculum, the converted models achieve higher accuracy on complex reasoning tasks like GSM8K and MATH while maintaining strong general language modeling performance. The approach enables test-time scaling, where more compute can be allocated per example to solve harder problems.

## Method Summary
The method involves converting a pretrained LLM into a depth-recurrent architecture by selecting early layers as a prelude, later layers as a recurrent block, and dropping middle layers. A linear adapter (2h→h) concatenates the prelude output with the recurrent state. The model is then post-trained with a curriculum that gradually increases recurrence depth, using truncated backpropagation and a Poisson-Lognormal distribution to sample recurrence counts per step. Training uses the Muon optimizer with weight decay and gradient clipping, and can be done in two phases: first "healing" on general data, then fine-tuning on high-quality math data.

## Key Results
- Retrofitted models outperform their static counterparts on GSM8K and MATH while using fewer unique parameters.
- Models trained with recurrence curriculum scale effectively with test-time compute, achieving higher accuracy at higher recurrence depths.
- Initializing from pretrained weights yields large efficiency gains over random initialization during post-training.
- Two-phase training (general data → high-quality data) recovers general benchmark performance lost during math-only training.

## Why This Works (Mechanism)
The paper's core insight is that depth-recurrent models can perform iterative refinement on difficult problems by re-using a subset of layers multiple times. This allows the model to allocate more compute to harder examples at test time, similar to how humans might re-read a complex problem. The recurrence curriculum during training teaches the model to effectively use multiple passes, with early passes handling simpler aspects and later passes refining the solution.

## Foundational Learning
- **Depth recurrence**: A model architecture where layers are reused multiple times during inference. *Why needed*: Enables test-time compute scaling for harder problems. *Quick check*: Verify accuracy improves when increasing test-time recurrence.
- **Model surgery**: Selectively removing and reorganizing layers from a pretrained model. *Why needed*: Creates the recurrent architecture from existing models. *Quick check*: Confirm layer dimensions match after reorganization.
- **Recurrence curriculum**: Gradually increasing the maximum recurrence depth during training. *Why needed*: Teaches models to effectively use multiple computation passes. *Quick check*: Track training accuracy as a function of recurrence depth.
- **Muon optimizer**: A variant of AdamW designed for recurrent models. *Why needed*: Provides better stability than standard optimizers for recurrent training. *Quick check*: Compare loss curves between Muon and AdamW.
- **Truncated backpropagation**: Limiting backpropagation through time to a fixed number of steps. *Why needed*: Makes training computationally tractable. *Quick check*: Verify gradients flow through the correct number of recurrence steps.
- **Poisson-Lognormal distribution**: Used to sample recurrence counts per training step. *Why needed*: Provides a natural distribution of recurrence depths during training. *Quick check*: Analyze the distribution of sampled recurrence values.

## Architecture Onboarding

**Component map**: Input → Prelude layers → Linear adapter → Recurrent block → Coda layers → Output

**Critical path**: The recurrent block is the critical path, as it's executed multiple times and its state is carried forward. The linear adapter that combines prelude and recurrent state is also crucial for information flow.

**Design tradeoffs**: The method trades parameter efficiency for test-time compute, requiring fewer unique parameters but potentially more FLOPs during inference. Layer selection (which layers to keep as prelude vs. recurrent block) involves balancing the preservation of early feature extraction with the power of later layers.

**Failure signatures**: 
- Loss spikes or NaN values indicate optimizer instability (use Muon instead of AdamW).
- Degraded performance on general benchmarks suggests insufficient healing data before high-quality fine-tuning.
- Lack of accuracy improvement with increased test-time recurrence indicates ineffective recurrence curriculum during training.

**First experiments**:
1. Convert TinyLlama-1.1B to (4,8,4) configuration and verify layer dimensions after surgery.
2. Train the converted model on math data with recurrence curriculum, checking that accuracy scales with test-time recurrence.
3. Compare Muon vs AdamW optimizer on the same model/data to confirm stability claims.

## Open Questions the Paper Calls Out
- Can depth-recurrent models be trained to recur deeper at test time than during training to solve harder problems?
- How can recurrent models be equipped with native adaptive stopping mechanisms that dynamically allocate compute based on problem difficulty?
- Does retrofitting recurrence scale effectively to models with significantly more parameters (e.g., 7B+) and training on trillions of tokens?
- Does the layer selection heuristic (taking early layers for prelude, later layers for recurrent block) generalize optimally across different model architectures?

## Limitations
- All experiments were conducted on ~1B parameter models with ~50B training tokens; scaling behavior to larger models and data is unknown.
- Current models use fixed or manually specified recurrence counts; no learned early-exit mechanism was implemented.
- The layer selection heuristic was based on empirical search rather than theoretical justification.

## Confidence
- **High confidence** in core methodology: Surgically removing layers and adding linear adapters to create depth-recurrent models is well-specified and reproducible.
- **Medium confidence** in training dynamics: While optimizer choice (Muon) and curriculum are specified, missing details on recurrence sampling variance and LR scheduling could impact final performance.
- **Medium confidence** in evaluation: Task definitions are clear but some metric nuances (e.g., "flexible extract" for GSM8K) require careful implementation.

## Next Checks
1. Implement and train a (4,8,4) TinyLlama model on math-only data, verify recurrence curriculum works by checking accuracy scales with test-time recurrence depth.
2. Compare Muon vs AdamW optimizer on the same model/data to confirm stability claims.
3. Run two-phase training (FineWeb-Edu healing → high-quality mix) and measure Arc-C benchmark recovery compared to math-only baseline.