---
ver: rpa2
title: Enhancing Reliability across Short and Long-Form QA via Reinforcement Learning
arxiv_id: '2512.08944'
source_url: https://arxiv.org/abs/2512.08944
tags:
- arxiv
- long-form
- training
- reward
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the trade-off between reasoning capability
  and hallucination in large language models by introducing a targeted reinforcement
  learning framework. The method mitigates both intrinsic (unfaithfulness to provided
  context) and extrinsic (flawed internal knowledge) hallucinations across short and
  long-form question answering.
---

# Enhancing Reliability across Short and Long-Form QA via Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.08944
- Source URL: https://arxiv.org/abs/2512.08944
- Reference count: 21
- This work addresses the trade-off between reasoning capability and hallucination in large language models by introducing a targeted reinforcement learning framework.

## Executive Summary
This work addresses the trade-off between reasoning capability and hallucination in large language models by introducing a targeted reinforcement learning framework. The method mitigates both intrinsic (unfaithfulness to provided context) and extrinsic (flawed internal knowledge) hallucinations across short and long-form question answering. It uses novel training datasets constructed from TriviaQA and FineWeb, with specialized reward functions for short-form and long-form QA. The framework explicitly rewards cautious behavior, including refusal to answer unanswerable questions. Extensive experiments show significant performance gains: the model achieves over 90% accuracy on unanswerable QA benchmarks and substantially reduces hallucination rates on short-form benchmarks like TriviaQA and SimpleQA.

## Method Summary
The framework employs a targeted reinforcement learning approach using a modified GRPO algorithm with three key innovations: separate reward functions for short-form (rule-based) and long-form (LLM-as-judge) QA, dynamic sampling, and Clip-Higher optimization. Training data combines TriviaQA converted to open-ended questions, FineWeb references for long-form QA, and synthetic unanswerable math problems. The reward system explicitly incentivizes refusal for unanswerable questions while maintaining factual accuracy through claim-level verification. The method uses summarized chain-of-thought supervision to balance performance and cost, avoiding the instability of full CoT supervision while preserving reasoning signals.

## Key Results
- Achieved over 90% accuracy on unanswerable QA benchmarks by explicitly rewarding refusal behavior
- Significantly reduced hallucination rates on short-form benchmarks (TriviaQA, SimpleQA) while maintaining or improving accuracy
- Improved response-level and claim-level accuracy on long-form QA benchmarks, though with a trade-off in information density

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separate reward functions for short-form vs. long-form QA enable targeted hallucination mitigation.
- Mechanism: Short-form QA uses rule-based rewards (exact match = 1, refusal = 0.1, format error = -0.2). Long-form QA uses an LLM-as-judge composite: `f(y) = f_claim - α·p_format - β·p_information_density` (α=β=0.2), where claims are atomically verified against references.
- Core assumption: Claim-level decomposition by an LLM judge accurately reflects factual groundedness.
- Evidence anchors: [abstract] "specialized reward functions for short-form and long-form QA"; [section 3.3] Equations (1) and (2) define the distinct reward formulations.
- Break condition: If the LLM judge systematically misclassifies claims as "supported" when unsupported, reward signal becomes noisy and training degrades.

### Mechanism 2
- Claim: Cautious refusal behavior is learned asymmetrically faster than factual correctness.
- Mechanism: The model rapidly acquires a refusal policy ("I don't know") early in training, as refusal yields reliable positive reward. Factual knowledge refinement requires more samples and converges later.
- Core assumption: Refusal rewards are easier to optimize than correctness rewards due to lower variance in the reward signal.
- Evidence anchors: [abstract] "explicitly rewards the model for refusing to answer unanswerable questions"; [section 5.1] "hallucination rate drops precipitously... significant rise in accuracy observed only after hallucination rate stabilized".
- Break condition: If refusal is over-rewarded, the model may refuse answerable questions (excessive caution).

### Mechanism 3
- Claim: Summarized chain-of-thought (CoT) supervision balances cost and performance better than full CoT or no CoT.
- Mechanism: Full CoT supervision penalizes intermediate reasoning steps (evaluator may misclassify self-corrections as errors). No CoT ignores reasoning entirely. Summarized CoT condenses reasoning before verification, reducing cost while preserving signal.
- Core assumption: Summarized CoT retains sufficient reasoning structure for accurate verification without penalizing exploratory steps.
- Evidence anchors: [section 4.2] "full CoT supervision... degrades performance on open-domain tasks such as LongFact"; [section 4.2] "summarized CoT... achieves competitive performance across multiple benchmarks".
- Break condition: If summarization collapses critical distinctions in reasoning, verification becomes unreliable.

## Foundational Learning

- **Intrinsic vs. Extrinsic Hallucinations**
  - Why needed here: The framework targets both—extrinsic (flawed internal knowledge) via TriviaQA-derived data, and intrinsic (unfaithfulness to context) via FineWeb grounding.
  - Quick check question: Can you classify "stating a wrong date from memory" vs. "ignoring a provided document's date"?

- **RL with LLM-as-Judge**
  - Why needed here: Long-form rewards require claim decomposition and verification, which the paper delegates to GPT-OSS-120B.
  - Quick check question: What failure modes arise if the judge LLM itself hallucinates during verification?

- **GRPO Variants (Clip-Higher, Dynamic Sampling)**
  - Why needed here: The paper uses a modified GRPO algorithm; understanding these variants helps debug training instability.
  - Quick check question: Why might removing the KL divergence penalty increase both exploration and instability?

## Architecture Onboarding

- **Component map:**
  - Data synthesis pipeline: TriviaQA (open-ended conversion) + FineWeb (reference-grounded) + synthetic unanswerable math
  - Reward models: Rule-based (short-form) / LLM-as-judge (long-form, claim-level)
  - Training: GRPO variant with Clip-Higher, Dynamic Sampling, no KL penalty
  - Evaluation: Self-Aware, SUM, TriviaQA, SimpleQA, Facts Grounding, FactScore, LongFact

- **Critical path:**
  1. Construct training data with correct answerability labels
  2. Configure reward functions per task type (short vs. long)
  3. Run GRPO training with summarized CoT supervision
  4. Monitor hallucination rate early; accuracy gains emerge later

- **Design tradeoffs:**
  - Accuracy vs. verbosity: Higher claim counts increase information density but reduce factual accuracy (section 5.3)
  - Refusal calibration: Explicit refusal instructions required in prompts; model does not generalize refusal without them (section 5.2)
  - Judge quality: GPT-OSS-120B outperformed Gemini-Flash for reference-grounded tasks; judge selection materially affects outcomes

- **Failure signatures:**
  - Model refuses answerable questions (over-cautious refusal)
  - Output length collapses with high accuracy but near-zero claims (reward hacking)
  - CoT supervision causes accuracy regression on open-domain benchmarks

- **First 3 experiments:**
  1. **Baseline reward ablation:** Compare rule-based short-form rewards vs. unified LLM-judge rewards to validate separation.
  2. **Refusal generalization test:** Train with 50% explicit refusal instructions, evaluate without them to confirm section 5.2 finding.
  3. **Verbosity-accuracy tradeoff sweep:** Vary information density penalty (β = 0, 0.1, 0.2, 0.5) and plot claim count vs. accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance and stability of hallucination mitigation vary when using smaller or alternative reward models compared to large proprietary models like GPT-OSS-120B?
- Basis in paper: [explicit] The authors explicitly list the dependency on a single LLM evaluator as a limitation, noting that the sufficiency of judgment accuracy for smaller reward models warrants further exploration.
- Why unresolved: The study relied entirely on GPT-OSS-120B for reward signals, with only a passing mention that Gemini-Flash yielded worse results.
- What evidence would resolve it: A systematic ablation study comparing training runs guided by smaller open-source reward models versus large proprietary judges on the same hallucination metrics.

### Open Question 2
- Question: Can a reinforcement learning framework be designed to train models to modulate their tone based on confidence levels rather than relying on binary refusal?
- Basis in paper: [explicit] The Limitations section identifies the binary treatment of unanswerable queries as a constraint and proposes calibrated confidence as a necessary future extension.
- Why unresolved: The current reward function provides no signal for "tentative" responses; the model effectively learns only to answer or refuse completely.
- What evidence would resolve it: A training framework utilizing a continuous reward signal based on confidence quantification, evaluated on a dataset containing ambiguous or partially answerable questions.

### Open Question 3
- Question: What reward mechanisms can effectively decouple information density from factual accuracy to prevent models from "reward hacking" by generating overly concise responses?
- Basis in paper: [inferred] Section 5.3 discusses a "distinct trade-off" where attempts to increase verbosity (e.g., via Number of Claims penalty) caused accuracy to drop, leaving the tension unresolved.
- Why unresolved: The paper evaluated two countermeasures (Informative Win-Rate and Number of Claims) which failed to balance the trade-off, resulting in either low density or low accuracy.
- What evidence would resolve it: A novel reward function that incentivizes comprehensive coverage of a topic without penalizing the model for uncertainty, resulting in stable or increased claim counts without higher hallucination rates.

## Limitations

- The specialized datasets (particularly FineWeb-derived long-form data) are not publicly available, limiting reproducibility and external validation.
- The LLM-as-judge verification system introduces potential circular reasoning, as the judge model may share biases or failure modes with the trained model.
- The asymmetric learning dynamics (rapid refusal vs. slower factual correctness) are observed but not fully explained mechanistically.

## Confidence

- **High Confidence:** The effectiveness of separate reward functions for short vs. long-form QA, the superiority of summarized CoT supervision over full CoT, and the improved performance on unanswerable QA benchmarks (≥90% accuracy)
- **Medium Confidence:** The asymmetric learning dynamics and the relative ineffectiveness of direct CoT supervision, as these are based on observed correlations rather than mechanistic explanations
- **Low Confidence:** The generalizability of results to other model families, as all experiments use specific model sizes (7B/4B parameters) from particular architectures

## Next Checks

1. **Judge Model Ablation Study:** Systematically compare different LLM judges (varying size, training data, and architecture) to quantify the impact of judge selection on training stability and final performance.
2. **Generalization to Other Model Families:** Apply the framework to non-transformer architectures (e.g., Mamba, RWKV) and larger models (70B+) to test scalability and architecture dependence.
3. **Failure Mode Analysis:** Conduct adversarial testing where models are presented with subtly unanswerable questions to measure the precision-recall tradeoff in refusal behavior.