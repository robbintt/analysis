---
ver: rpa2
title: 'Gold Panning: Turning Positional Bias into Signal for Multi-Document LLM Reasoning'
arxiv_id: '2510.09770'
source_url: https://arxiv.org/abs/2510.09770
tags:
- position
- documents
- positions
- document
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of position bias in large language
  models, where models prioritize information based on location rather than relevance
  in multi-document contexts. The authors introduce Gold Panning, a black-box framework
  that leverages this bias as a diagnostic signal.
---

# Gold Panning: Turning Positional Bias into Signal for Multi-Document LLM Reasoning

## Quick Facts
- **arXiv ID:** 2510.09770
- **Source URL:** https://arxiv.org/abs/2510.09770
- **Reference count:** 40
- **Primary result:** Reduces LLM queries by up to 65% while maintaining or improving retrieval performance on multi-document reasoning tasks

## Executive Summary
This paper addresses the problem of position bias in large language models, where models prioritize information based on document location rather than relevance. The authors introduce Gold Panning, a black-box framework that transforms this bias from a liability into an asset by using it as a diagnostic signal. By reordering documents and observing shifts in model responses, Gold Panning identifies the most relevant content while substantially reducing computational costs. The approach uses Bayesian inference to maintain beliefs about document relevance and iteratively reorders documents to align high-belief items with positions of high discriminative power. Results show Gold Panning matches or exceeds state-of-the-art performance on knowledge-intensive NLP tasks while using significantly fewer language model queries than random permutation baselines.

## Method Summary
Gold Panning is a black-box inference-time framework that leverages position bias as a diagnostic signal for multi-document retrieval. The method involves calibrating position diagnosticity profiles through synthetic trials, then iteratively reordering documents based on current belief states to maximize information extraction. It maintains Bayesian beliefs about document relevance, updates these beliefs using position-dependent likelihood ratios from LLM responses, and uses greedy or optimal assignment algorithms to match high-belief documents to high-diagnosticity positions. The framework requires structured JSON citation outputs from LLMs and works across different model families and reasoning tasks, achieving comparable performance to baselines while reducing query counts by 30-65%.

## Key Results
- Gold Panning reduces LLM queries by 30-65% compared to random permutation baselines while maintaining or improving F1 scores
- GP-BELIEF variant outperforms GP-ENTROPY in sparse retrieval scenarios by creating virtuous belief cycles
- Cross-task calibration transfer works with modest performance degradation, suggesting position diagnosticity is a stable model property
- Framework shows graceful degradation up to 40% calibration error but fails when diagnosticity profiles are flat

## Why This Works (Mechanism)

### Mechanism 1: Position Diagnosticity as Structured Detector Model
The framework treats position bias as a structured detector array where each position has measurable true positive and false positive rates. Youden's J-statistic quantifies discriminative power, enabling strategic placement of documents to maximize information extraction.

### Mechanism 2: Signal Anchoring Creates Virtuous Belief Cycles
GP-BELIEF variant prioritizes high-belief documents for diagnostic positions, creating self-reinforcing cycles where relevant documents receive more confirming evidence. This outperforms uncertainty reduction strategies in sparse retrieval.

### Mechanism 3: Stateful Bayesian Inference vs. Stateless Ensembling
Maintaining posterior beliefs across iterations enables adaptive placement and outperforms permutation self-consistency approaches that treat each query independently, wasting queries on resolved negatives.

## Foundational Learning

- **Bayesian Belief Updating with Binary Observations**: Core inference engine using log-odds updates and likelihood ratios. *Why needed*: Essential for tracking document relevance probabilities. *Quick check*: Given prior b = 0.5, TPR = 0.8, FPR = 0.2, and observed citation, what's the posterior?

- **Active Search vs. Active Learning Distinction**: Explains why GP-BELIEF beats GP-ENTROPY. *Why needed*: Search maximizes recall while learning maximizes information gain. *Quick check*: In a 1-of-100 retrieval task, should you probe the document you're 80% sure about or the one you're 50% sure about?

- **Youden's J-Statistic for Detector Quality**: Metric for ranking positions. *Why needed*: Connects ROC analysis to assignment policy. *Quick check*: Position A has TPR=0.9, FPR=0.3. Position B has TPR=0.6, FPR=0.1. Which is more diagnostic?

## Architecture Onboarding

- **Component map**: Calibration module → Belief tracker → Scoring function → Assignment optimizer → LLM interface → Belief update
- **Critical path**: Calibration → initial uniform beliefs → [score documents → greedy assignment → LLM query → Bayesian update] × T rounds → top-k selection
- **Design tradeoffs**: GP-BELIEF vs. GP-ENTROPY (belief-anchoring faster for sparse targets); Greedy O(N log N) vs. Hungarian O(N³) (comparable performance); Sparse calibration (K=11) vs. full (mean residual < 0.071)
- **Failure signatures**: Flat diagnosticity profile (no exploitable bias); Severe miscalibration (inverted J-signs); Non-independent citations (absorbed into marginal rates)
- **First 3 experiments**: 1) Calibration validation: K=11 vs. full N-position calibration; verify residual < 0.1. 2) Ablation GP-BELIEF vs. GP-ENTROPY: N=100, k=1, T=8 on FLenQA; expect belief dominance when profile variance high. 3) Cross-task transfer: Calibrate on MonoRel, evaluate on PIR; expect modest degradation if profile is structural property.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does incorporating upstream retrieval scores as non-uniform priors improve query efficiency? The authors use uniform priors "for generality" but acknowledge potential gains from informative priors remain unquantified.

- **Open Question 2**: Does GP-BELIEF's "virtuous cycle" degrade in multi-needle scenarios? The theoretical analysis and experiments are restricted to k=1; competition for diagnostic positions may harm recall for secondary relevant documents.

- **Open Question 3**: How robust is the binary citation observation model to semantic ambiguity in open-domain QA? The framework assumes binary outcomes which may be ill-defined when relevance is subjective or multi-faceted.

## Limitations

- **Calibration fragility across models and domains**: Position diagnosticity profiles may not fully transfer across tasks despite theoretical assumptions of stability
- **Sparse retrieval assumption**: Theoretical advantages rely on k ≪ N; performance in dense retrieval scenarios remains unexplored
- **Structured output dependency**: Framework requires consistent JSON citation outputs, implementation details are sparse, and success depends on model compliance

## Confidence

**High confidence**: Core mechanism of treating position bias as structured detector noise is well-supported; GP-BELIEF outperforms GP-ENTROPY under sparse retrieval is empirically demonstrated.

**Medium confidence**: Query efficiency claims (30-65% reduction) and cross-task transferability are supported but tested in limited scenarios; position diagnosticity as stable model property needs more validation.

**Low confidence**: Behavior in dense retrieval settings, with non-independent citations, or under severe miscalibration remains largely theoretical; practical impact of implementation details is not fully characterized.

## Next Checks

1. **Dense retrieval validation**: Test Gold Panning with k=5, 10, 20 relevant documents in 100-document settings to verify if theoretical advantages persist when relevant documents are no longer sparse.

2. **Cross-domain calibration transfer**: Calibrate on one domain (e.g., biomedical literature) and evaluate on structurally different domains (e.g., legal documents, financial reports) to quantify how much diagnosticity profiles transfer across document types.

3. **Miscalibration robustness test**: Systematically corrupt calibration estimates (TPR/FPR) by ±σ noise levels (0.1, 0.2, 0.3, 0.4) and measure performance degradation to validate the framework's claimed robustness to estimation errors.