---
ver: rpa2
title: 'The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language
  Varieties'
arxiv_id: '2509.07139'
source_url: https://arxiv.org/abs/2509.07139
tags:
- speech
- challenge
- data
- languages
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The ML-SUPERB 2.0 Challenge was launched to address the problem
  of unequal performance of multilingual ASR systems across languages, accents, and
  dialects. The challenge introduced a new test suite covering 200+ languages and
  language varieties, evaluated through an online platform using DynaBench to prevent
  benchmark overfitting.
---

# The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties

## Quick Facts
- arXiv ID: 2509.07139
- Source URL: https://arxiv.org/abs/2509.07139
- Authors: William Chen; Chutong Meng; Jiatong Shi; Martijn Bartelds; Shih-Heng Wang; Hsiu-Hsuan Wang; Rafael Mosquera; Sara Hincapie; Dan Jurafsky; Antonis Anastasopoulos; Hung-yi Lee; Karen Livescu; Shinji Watanabe
- Reference count: 0
- Primary result: Challenge received 5 submissions from 3 teams, all outperforming baselines, with best submission achieving 23% improvement in LID accuracy and 18% reduction in CER on general multilingual test set, and 30.2% lower CER on accented/dialectal data

## Executive Summary
The ML-SUPERB 2.0 Challenge addresses the critical problem of unequal multilingual ASR performance across languages, accents, and dialects by introducing a new test suite covering 200+ languages and varieties. The challenge uses an online evaluation platform with hidden test sets to prevent benchmark overfitting, allowing participants to use any data or pre-trained models while submitting through a standardized API. Results show that all submissions outperformed baseline systems, with the best achieving substantial improvements in both language identification accuracy and character error rate, particularly on accented and dialectal speech.

## Method Summary
The challenge introduced a comprehensive test suite covering 200+ languages with a hidden evaluation platform using DynaBench to prevent overfitting. Participants could use any external data or pre-trained models, submitting their systems through a standardized Python API that takes raw waveforms and returns language ID and ASR transcripts. The evaluation measured standard multilingual performance alongside dialectal robustness metrics, with separate scoring for accented and dialectal speech. The platform enforced compute constraints (~46GB VRAM) and used ISO 639-3 language codes for consistency, though this required extensive manual cleaning for cases like Serbian (Latin vs. Cyrillic scripts) and Norwegian orthography variants.

## Key Results
- All 5 submissions from 3 teams outperformed baseline systems on the general multilingual test set
- Best submission achieved 23% improvement in language identification accuracy and 18% reduction in character error rate
- Dialectal robustness showed even greater improvements: 30.2% lower CER and 15.7% higher LID accuracy on accented/dialectal data
- Performance gap between standard and dialectal speech remains significant, indicating current scaling methods are insufficient

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Evaluating "dialectal robustness" as a distinct metric forces models to generalize beyond standard linguistic variants.
- **Mechanism:** By creating a hidden test set with 93 accents/dialects and scoring separately from "Standard" multilingual performance, the benchmark penalizes systems that overfit to "standard" speech profiles (e.g., news broadcast data). This explicitly ties model ranking to inclusivity.
- **Core assumption:** Accented/dialectal data distribution differs meaningfully from standard corpora, and optimizing for standard CER does not automatically optimize for dialectal CER.
- **Evidence anchors:**
  - [Section 3.6]: Defines "Dialectal Robustness" as a separate category with specific metrics (LID accuracy and ASR CER on accented data).
  - [Section 4.1]: Results show systems often have distinct performance profiles; for example, supervised models like Whisper show significantly worse CER on unseen dialects compared to fine-tuned SSL models.

### Mechanism 2
- **Claim:** An API-based evaluation server (DynaBench) prevents "benchmark hacking" and ensures generalization.
- **Mechanism:** Instead of submitting prediction files (which allow error analysis on the test set), participants submit model weights and inference code. The server runs the model on a hidden test set. This blind evaluation prevents iterative overfitting to specific test samples.
- **Core assumption:** The server has sufficient resources to host diverse architectures and that the inference API is flexible enough not to stifle novel model designs.
- **Evidence anchors:**
  - [Abstract]: Mentions the online evaluation server allows flexibility in model design while keeping the test set hidden.
  - [Section 3.3]: Describes the submission of weights/code rather than decoded results to alleviate overfitting.

### Mechanism 3
- **Claim:** Unconstrained training resources (data/models) allow immediate integration of SOTA scaling laws.
- **Mechanism:** Unlike prior "probe-based" benchmarks with fixed training sets, this challenge permits any external data or pre-trained models. This mechanism leverages the community's existing scaling efforts (e.g., larger foundation models) rather than testing a specific learning algorithm in isolation.
- **Core assumption:** The baseline provided by the organizers is sufficiently difficult that standard approaches fail, requiring scale or novel data curation to win.
- **Evidence anchors:**
  - [Section 3.4]: States "participants are allowed to use almost any resource" including LLMs or API-based distillation.
  - [Section 4.1]: Submissions outperformed baselines, implying that the flexibility allowed teams to utilize superior external resources or techniques.

## Foundational Learning

- **Concept:** Self-Supervised Learning (SSL) vs. Supervised Foundation Models
  - **Why needed here:** The paper contrasts baselines using SSL encoders (XEUS, w2v-BERT) with supervised models (Whisper, OWSM). Understanding that SSL models are pre-trained on unlabeled audio and fine-tuned, while supervised models learn from aligned text/audio, explains their different failure modes (e.g., SSL generalizes better to unseen languages in this specific benchmark setup).
  - **Quick check question:** Does the model require labeled transcripts during its initial pre-training phase, or does it learn acoustic representations from raw audio first?

- **Concept:** Orthography and Language Identification (LID)
  - **Why needed here:** The paper highlights data cleaning issues where languages share ISO codes or differ only in script (e.g., Serbian Latin vs. Cyrillic). A system must distinguish between recognizing the *speech* vs. identifying the *language variety*.
  - **Quick check question:** If a model transcribes Serbian speech perfectly but outputs Latin characters when the ground truth is Cyrillic, is this an ASR failure or a normalization failure?

- **Concept:** Robustness vs. Average Performance
  - **Why needed here:** The challenge ranks systems not just on average Character Error Rate (CER), but on "Worst 15 Languages" and "Standard Deviation." Systems must be architected to handle tail-resource languages, not just high-resource ones.
  - **Quick check question:** Does optimizing for the mean CER necessarily improve the performance of the worst-performing language cluster?

## Architecture Onboarding

- **Component map:** Raw Waveform -> Pre-trained Speech Encoder (XEUS/Whisper/custom) -> (Optional) Adapter Layer -> CTC/Transducer Decoder -> Text Transcript + Language ID
- **Critical path:**
  1. Select a pre-trained encoder with multilingual coverage
  2. Implement the standardized `API(waveform)` interface ensuring it accepts a numpy array and returns `(pred_lid, pred_asr)`
  3. Verify the model + weights fit within ~46GB VRAM constraint
  4. Optimize for the "Worst 15" languages during validation, not just the aggregate average
- **Design tradeoffs:**
  - **Supervised (Whisper/OWSM) vs. SSL (XEUS/w2v-BERT):** Supervised models offer strong zero-shot performance on *seen* languages but may exhibit higher variance on *unseen* or dialectal data compared to fine-tuned SSL models (Table 3 shows Whisper CER jumps from 40.7 seen to 76.2 unseen)
  - **Fixed Encoder vs. Full Fine-tuning:** Freezing the encoder (as done in baselines) reduces computational cost but may limit adaptability to the specific 93 dialects
- **Failure signatures:**
  - **High Standard Deviation (StD):** Indicates the model is "overfitting" to high-resource languages within the training mix
  - **LID/ASR Mismatch:** High LID accuracy but poor CER suggests the acoustic model recognizes the language pattern but fails to map it to graphemes (likely for low-resource languages)
  - **Script Confusion:** Generating valid text but in the wrong script (e.g., Cyrillic vs. Latin) results in high CER despite good phonetic recognition
- **First 3 experiments:**
  1. **Zero-Shot Baseline:** Run Whisper-Large-v3 or XEUS on the development set to establish the gap between "Standard" and "Dialectal" performance
  2. **Data Ablation:** Fine-tune an SSL encoder (like XEUS) using only the provided 1-hour/language baseline data vs. adding external accented data (e.g., VoxPopuli) to measure dialectal robustness gain
  3. **Worst-Case Analysis:** Identify the 15 worst-performing languages in the baseline and perform targeted data augmentation or language-specific adapter tuning to reduce the "Worst 15 CER" metric

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the persistent performance gap between standard multilingual ASR and dialectal/accented speech recognition be effectively closed?
- **Basis in paper:** [explicit] The conclusion states that "SOTA ASR systems continue to underperform on accented and dialectal speech," despite the improvements demonstrated by challenge submissions.
- **Why unresolved:** While the best submission reduced CER by 30.2% on dialectal data, the absolute error rates for varieties remain significantly higher than general multilingual performance, indicating current scaling methods are insufficient.
- **What evidence would resolve it:** Development of a model that achieves comparable CER on the "Dialectal" test set as on the "Standard" multilingual test set.

### Open Question 2
- **Question:** What standardized protocols for linguistic meta-data and orthography are necessary to prevent ambiguity in future multilingual benchmarks?
- **Basis in paper:** [inferred] The authors detail extensive manual cleaning required for Norwegian (orthography), Serbian (scripts), and Malay (ISO codes), concluding that "proper linguistic meta-data is necessary to guarantee that models are configured properly."
- **Why unresolved:** Current crowd-sourced datasets lack consistent linguistic distinction between macro-languages and dialects, forcing ad-hoc corrections that may not generalize to other corpora.
- **What evidence would resolve it:** A universally adopted data collection standard where language codes unambiguously map to specific orthographies and dialectal variants without manual intervention.

### Open Question 3
- **Question:** Can large-scale supervised foundation models be improved to generalize to unseen languages without relying on specific fine-tuning data?
- **Basis in paper:** [inferred] Table 3 shows that supervised models like Whisper suffer a near-doubling of CER (from 40.7 to 76.2) when moving from seen to unseen languages, whereas fine-tuned SSL models perform more consistently.
- **Why unresolved:** The paper highlights that while SSL models adapt well to the provided training set, supervised "foundation" models struggle when the benchmark covers languages outside their pre-training distribution.
- **What evidence would resolve it:** A zero-shot model achieving comparable CER scores on both "Seen" and "Unseen" language subsets in the ML-SUPERB test suite.

## Limitations
- The relatively small number of submissions (5 from 3 teams) limits generalizability of conclusions about the challenge's ability to drive innovation
- Compute/memory constraints (~46GB VRAM) may have excluded larger foundation models that could have performed differently on dialectal robustness
- The hidden test set methodology prevents detailed error analysis that could reveal specific failure modes
- The reliance on ISO 639-3 codes creates complications when languages share codes or differ only in orthography

## Confidence
**High Confidence (4/5):**
- The challenge successfully attracted participants who developed systems outperforming baselines
- The API-based evaluation prevented direct access to test data, ensuring blind evaluation
- Allowing external resources enabled teams to leverage state-of-the-art models and data

**Medium Confidence (3/5):**
- The improvement in dialectal robustness (30.2% CER reduction) is meaningful and attributable to the challenge framework
- The worst-case performance metrics (Worst 15 Languages) effectively identify models that fail on low-resource languages
- The compute constraints (~46GB VRAM) did not significantly limit model diversity

**Low Confidence (2/5):**
- The specific 23% improvement in LID accuracy and 18% CER reduction on the general test set are directly attributable to the challenge design rather than external factors
- The benchmark's dialectal test set is sufficiently representative and large to provide statistically significant differentiation between models
- The server's inference API did not constrain participants' architectural choices in meaningful ways

## Next Checks
1. **Statistical Significance Analysis**: Conduct a power analysis on the dialectal test set to determine if the observed performance differences between models are statistically significant, given the test set size and variance in performance across languages.

2. **Error Analysis Correlation**: Compare error patterns on the dialectal subset versus the standard test set for top-performing models to verify that improvements are genuinely addressing dialectal robustness rather than being artifacts of specific language distributions.

3. **Scaling Law Validation**: Test whether the compute/memory constraints (~46GB VRAM) created a meaningful ceiling by evaluating whether larger models (>46GB) would have significantly outperformed submissions on the dialectal robustness metrics.