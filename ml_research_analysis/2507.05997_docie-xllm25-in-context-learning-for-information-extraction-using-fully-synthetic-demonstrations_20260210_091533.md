---
ver: rpa2
title: 'DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic
  Demonstrations'
arxiv_id: '2507.05997'
source_url: https://arxiv.org/abs/2507.05997
tags:
- entity
- relation
- text
- extraction
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles document-level entity and relation extraction
  in zero-shot settings where large, high-quality annotated corpora are scarce. The
  authors propose a fully automatic LLM-based pipeline that generates synthetic demonstrations
  for in-context learning, eliminating the need for manual annotation.
---

# DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic Demonstrations

## Quick Facts
- arXiv ID: 2507.05997
- Source URL: https://arxiv.org/abs/2507.05997
- Reference count: 24
- Primary result: Entity Identification F1 32.86%, Relation Extraction F1 3.01-3.29%

## Executive Summary
This paper tackles document-level entity and relation extraction in zero-shot settings where large, high-quality annotated corpora are scarce. The authors propose a fully automatic LLM-based pipeline that generates synthetic demonstrations for in-context learning, eliminating the need for manual annotation. The method combines synthetic data generation with retrieval-based in-context learning using a reasoning-optimized language model to dynamically retrieve relevant examples at inference time. They construct a synthetic dataset of over 5,000 Wikipedia abstracts containing approximately 59,000 entities and 30,000 relation triples. Evaluation on the DocIE shared task shows modest performance: entity identification achieves 32.86 F1, entity classification 16.19 F1, and relation extraction scores as low as 3.29 F1 (general) and 3.01 F1 (strict). Results on Re-DocRED are broadly comparable. The findings indicate that joint entity and relation extraction at the document level remains challenging even for state-of-the-art LLMs, with substantial portions of outputs being unparseable.

## Method Summary
The method generates synthetic demonstrations using a zero-shot LLM to annotate Wikipedia abstracts, creating a database of over 5,000 documents with ~59k entities and ~30k relation triples. At inference, a reasoning-optimized model retrieves the most similar synthetic example based on semantic similarity and uses it as a single demonstration for in-context learning. The extraction process uses a two-stage pipeline: first annotating the initial paragraph, then completing extraction for the full document with partial annotations provided. A verification mechanism filters inconsistent relation triples by requiring natural language descriptions before structured outputs, enabling post-processing validation.

## Key Results
- Entity Identification F1: 32.86% (DocIE), 22.79% (Re-DocRED)
- Entity Classification F1: 16.19% (DocIE), 20.52% (Re-DocRED)
- Relation Extraction F1: 3.29% (general, DocIE), 3.01% (strict, DocIE)
- Parseability rates: 64.04% (DocIE), 38.66% (Re-DocRED)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic demonstrations with embedded verification hooks can reduce relation extraction errors, particularly directionality errors.
- Mechanism: The annotation prompt requires the model to produce natural language descriptions of each triple before emitting structured output. These descriptions serve two functions: (1) the act of description generation appears to improve triple accuracy during initial extraction, and (2) descriptions enable a secondary LLM call to verify whether the structured triple faithfully matches its description, filtering inconsistent outputs.
- Core assumption: The verification model can reliably detect mismatches between natural language descriptions and structured triples, and discarding entire relation types when inconsistencies occur improves overall quality.
- Evidence anchors:
  - [abstract] "our method combines synthetic data generation with retrieval-based in-context learning, using a reasoning-optimized language model"
  - [section 4.2] "We prompt the model to describe each extracted triple in natural language before emitting the structured triple, enabling verification in the post-processing stage"
  - [section 4.3] "producing a description already improves the accuracy of the extracted triples"
  - [corpus] Weak direct corpus support; related work (Josifoski et al. 2023, Rogulsky et al. 2024) explores synthetic IE data but not this specific verification hook pattern.
- Break condition: If verification prompts produce inconsistent judgments across runs, or if filtering entire relation types discards too many valid triples, quality may degrade rather than improve.

### Mechanism 2
- Claim: Retrieval-based in-context learning using similarity-matched synthetic demonstrations can provide relevant schema guidance without manual annotation.
- Mechanism: Given a query document, the system retrieves the most similar document from the synthetic demonstration database using sentence embeddings (all-MiniLM-L6-v2). This retrieved example—containing entity types, relation types, and annotated triples—is provided as a single-shot demonstration to the inference model, allowing it to adapt to the target schema at test time.
- Core assumption: Semantic similarity between documents correlates with overlap in relevant entity and relation types, making retrieved demonstrations useful for schema-constrained extraction.
- Evidence anchors:
  - [abstract] "dynamically retrieve relevant examples at inference time"
  - [section 3.1] "we use a single example document from our synthetic demonstration database, retrieved based on similarity to the query document"
  - [corpus] Related work on document-level RE (GLiDRE, CDER) uses evidence retrieval but not for in-context demonstration selection specifically.
- Break condition: If query documents come from domains poorly represented in the synthetic database (Wikipedia abstracts), retrieved examples may provide misleading schema guidance.

### Mechanism 3
- Claim: Two-stage inference (first paragraph annotation, then full document) reduces format adherence failures for long documents.
- Mechanism: The inference pipeline splits extraction into two LLM calls. The first call processes only the first paragraph and returns annotations. The second call receives the full document along with the first paragraph's annotations, allowing the model to continue rather than start fresh. This appears to reduce syntax errors and format violations.
- Core assumption: Providing partial annotations as a starting point anchors the model to the correct output format, reducing generation drift on longer inputs.
- Evidence anchors:
  - [section 3.1] "In the first call, we query the model with just the first paragraph of the query document, while the second LLM call supplies the entire document with the annotations provided for the beginning paragraph"
  - [section 3.1] "We find that this strategy drastically decreases failures of the model to adhere to the annotation format for long documents"
  - [section 5.1] "Text length and total verification failures are highly correlated"
  - [corpus] No direct corpus validation; this appears to be an empirical finding specific to this work.
- Break condition: If errors in the first-stage annotation propagate to the second stage, or if key entities/relations appear only in later paragraphs, the staged approach may miss or misannotate content.

## Foundational Learning

- Concept: **In-Context Learning (ICL)**
  - Why needed here: The entire approach relies on providing demonstrations in the prompt context rather than fine-tuning model weights. Understanding ICL's sensitivity to demonstration selection, ordering, and format is critical.
  - Quick check question: Can you explain why retrieval-based demonstration selection might outperform random selection for schema-constrained extraction?

- Concept: **Document-Level Relation Extraction**
  - Why needed here: Unlike sentence-level RE, document-level RE requires reasoning across sentences to identify relations between entities that may not co-occur locally. This introduces challenges the paper directly addresses.
  - Quick check question: What makes document-level RE harder than sentence-level RE, and how might this affect the choice of context window usage?

- Concept: **Schema-Constrained Extraction**
  - Why needed here: The DocIE task provides entity and relation type schemas at test time that differ from training/development data. The system must extract according to these schemas without prior exposure.
  - Quick check question: How does zero-shot schema-constrained extraction differ from open information extraction?

## Architecture Onboarding

- Component map:
  Text Collection -> Synthetic Annotation Pipeline -> Demonstration Database -> Retrieval Module -> Inference Pipeline

- Critical path:
  1. Synthetic data generation is the bottleneck—requires careful prompt engineering and multiple verification passes
  2. Retrieval quality directly impacts inference performance
  3. Format adherence remains a failure point despite two-stage inference

- Design tradeoffs:
  - Text truncation (100+ words) balances inference efficiency vs. annotation completeness; longer texts increase verification failures
  - Conservative triple filtering (discard entire relation types on inconsistency) prioritizes precision over recall
  - Single demonstration (vs. multiple) reduces context usage but may limit schema coverage

- Failure signatures:
  - High unparseable output rate (36-61% depending on dataset) indicates format adherence is a persistent issue
  - Relation extraction F1 of 3.01-3.29% suggests the model struggles with joint extraction even with demonstrations
  - Syntax errors and entity ID mismatches are the most common verification failures

- First 3 experiments:
  1. **Ablate verification hooks**: Run synthetic annotation without natural language descriptions and compare relation quality to establish baseline contribution of verification mechanism.
  2. **Vary demonstration count**: Test retrieval with 0, 1, 3, and 5 demonstrations to quantify ICL benefit and context tradeoffs.
  3. **Cross-domain retrieval**: Evaluate retrieval effectiveness when query documents come from domains outside Wikipedia (e.g., biomedical, legal) to identify generalization boundaries.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing the number of retrieved in-context demonstrations improve extraction performance, or does the single-example strategy represent a ceiling for retrieval-based approaches in document-level IE?
- Basis in paper: [explicit] The authors state they "use a single example document from our synthetic demonstration database, retrieved based on similarity to the query document," but do not ablate or vary this number.
- Why unresolved: The paper does not test whether additional demonstrations would help or hurt performance given context length constraints and potential noise in synthetic examples.
- What evidence would resolve it: An ablation study varying k=1, 2, 3, 5 retrieved examples and measuring F1 scores and parseability rates.

### Open Question 2
- Question: What causes the high rate of unparseable outputs (~36% on DocIE, ~61% on Re-DocRED) when the initial hypothesis about document length was not supported by shorter-document experiments?
- Basis in paper: [explicit] The authors "initially hypothesize that this is due, in large parts, to the long documents increasing the frequency of syntax errors... our experiments with shorter documents, outlined in Section 5.4, do not support this."
- Why unresolved: The correlation between text length and verification failures in annotation (Section 5.1) did not translate to an explanation for inference-time parse failures on shorter Re-DocRED documents.
- What evidence would resolve it: Error analysis categorizing failure modes (syntax errors, format violations, incomplete JSON) across document lengths and schema complexity levels.

### Open Question 3
- Question: Can the relation extraction performance gap (3.01–3.29 F1 vs. 16.19–32.86 F1 for entities) be closed through improved synthetic data quality, or is it fundamentally limited by the reasoning model's capabilities?
- Basis in paper: [inferred] The drastic performance disparity between entity and relation extraction, combined with the conservative triple-filtering strategy that "discard[s]... the entire set of relations of that type within the document" upon single inconsistencies, suggests potential data loss.
- Why unresolved: The paper does not analyze whether low RE scores stem from synthetic data noise, schema complexity, model limitations, or the verification strategy's over-pruning.
- What evidence would resolve it: Human evaluation of a sample of discarded triples and comparison against less aggressive filtering thresholds.

## Limitations

- **Synthetic Data Quality**: The annotation process relies on zero-shot prompts that may produce inconsistent outputs, requiring multiple verification passes that introduce complexity and potential quality degradation.
- **Format Adherence**: A significant portion of outputs are unparseable (36-61% across datasets), representing a fundamental bottleneck that affects all downstream metrics regardless of model capabilities.
- **Schema Coverage**: The retrieval-based approach assumes semantic similarity correlates with schema relevance, which may not hold when query documents come from domains poorly represented in the Wikipedia-based synthetic database.

## Confidence

**High Confidence**: The methodology of using synthetic demonstrations for zero-shot document-level extraction is clearly articulated and implemented. The two-stage inference pipeline demonstrably reduces format adherence failures. The verification mechanism for relation triples is explicitly described and operationalized.

**Medium Confidence**: Performance metrics on DocIE and Re-DocRED are reported with statistical significance, but the absolute values (entity F1 32.86%, relation F1 3.01-3.29%) suggest substantial room for improvement. The effectiveness of retrieval-based ICL versus random demonstrations is implied but not directly tested.

**Low Confidence**: The claim that synthetic data generation is "fully automatic" is qualified by the need for multiple verification passes and parameter tuning (temperature adjustments, retry mechanisms). The assumption that description generation before structured triples consistently improves relation quality requires more rigorous ablation studies.

## Next Checks

1. **Ablate verification hooks**: Run synthetic annotation without natural language descriptions and compare relation quality to establish baseline contribution of verification mechanism. Measure changes in triple accuracy and verification failure rates.

2. **Vary demonstration count**: Test retrieval with 0, 1, 3, and 5 demonstrations to quantify ICL benefit and context tradeoffs. Measure performance impact on entity and relation extraction metrics while monitoring context usage and format adherence.

3. **Cross-domain retrieval**: Evaluate retrieval effectiveness when query documents come from domains outside Wikipedia (e.g., biomedical, legal) to identify generalization boundaries. Compare similarity scores and extraction performance against in-domain retrieval to quantify domain shift effects.