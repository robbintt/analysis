---
ver: rpa2
title: 'LD-MoLE: Learnable Dynamic Routing for Mixture of LoRA Experts'
arxiv_id: '2509.25684'
source_url: https://arxiv.org/abs/2509.25684
tags:
- routing
- experts
- expert
- sparsity
- dynamic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LD-MoLE, a learnable dynamic routing mechanism
  for Mixture of LoRA Experts (MoLE) that replaces the conventional TopK routing with
  a differentiable, token-dependent, and layer-wise expert allocation strategy. The
  core idea is to use a closed-form Sparsegen projection, controlled by a token-specific
  sparsity parameter predicted via a shared lightweight MLP, enabling adaptive expert
  selection without non-differentiable operations.
---

# LD-MoLE: Learnable Dynamic Routing for Mixture of LoRA Experts

## Quick Facts
- arXiv ID: 2509.25684
- Source URL: https://arxiv.org/abs/2509.25684
- Authors: Yuan Zhuang; Yi Shen; Yuexin Bian; Qing Su; Shihao Ji; Yuanyuan Shi; Fei Miao
- Reference count: 27
- Key outcome: LD-MoLE achieves highest average scores across multiple instruction-tuning and sequence classification benchmarks using token-adaptive, layer-wise expert allocation via learnable Sparsegen routing

## Executive Summary
This paper introduces LD-MoLE, a learnable dynamic routing mechanism for Mixture of LoRA Experts that replaces conventional TopK routing with a differentiable, token-dependent, and layer-wise expert allocation strategy. The core innovation is using a closed-form Sparsegen projection, controlled by a token-specific sparsity parameter predicted via a shared lightweight MLP, enabling adaptive expert selection without non-differentiable operations. Extensive experiments on Qwen3-1.7B and Llama-3.2-3B demonstrate superior performance compared to both fixed TopK routing (MoLA) and ReLU-based dynamic routing (ReMoLE) while maintaining effective sparsity control.

## Method Summary
LD-MoLE implements learnable dynamic routing for Mixture of LoRA Experts using a closed-form Sparsegen projection instead of TopK routing. The method computes routing scores via a gate matrix, predicts token-specific sparsity parameters λ using a shared MLP, and applies the Sparsegen transformation to produce differentiable routing weights. This enables token-adaptive expert allocation while guaranteeing at least one expert is activated per token. The approach uses 8 LoRA experts with rank=8, scaling=16, and dropout=0.1, applied to multiple projection modules. Training employs load-balancing loss and optional sparsity regularization, with performance evaluated across instruction-tuning and sequence classification benchmarks.

## Key Results
- Achieves highest average scores across multiple benchmarks including ARC-Challenge, ARC-Easy, OpenBookQA, CommonsenseQA, SWAG, HellaSWAG, CoLA, and RTE
- Outperforms both fixed TopK routing (MoLA) and ReLU-based dynamic routing (ReMoLE) baselines
- Demonstrates effective sparsity control with token-adaptive expert allocation - rarer/informative tokens activate more diverse expert sets
- Shows sensitivity to shared MLP hidden dimension: 256 optimal for Qwen3, 512 for Llama

## Why This Works (Mechanism)

### Mechanism 1: Differentiable Routing via Sparsegen Projection
- Claim: Replacing TopK with closed-form Sparsegen projection enables end-to-end gradient flow through routing decisions
- Mechanism: Routing distribution p computed as p_i = [(u_i - τ)/(1-λ)]_+ where τ is analytically determined from sorted scores, enabling stable optimization with bounded derivatives
- Core assumption: Gradient information through routing decisions improves expert specialization compared to discrete selection
- Evidence anchors: Abstract states "replaces non-differentiable TopK selection with differentiable routing function"; Section 3.1 proves derivative is upper-bounded; related work addresses similar gradient sparsity issues

### Mechanism 2: Token-Adaptive Sparsity via Learned λ
- Claim: Shared MLP predicting per-token λ enables dynamic expert allocation based on token complexity
- Mechanism: Lightweight MLP predicts λ_t ∈ R from token embedding; λ controls sparsity (λ→1 increases sparsity, λ→−∞ increases uniformity)
- Core assumption: Token-level features contain sufficient signal to predict optimal expert allocation; rarer/informative tokens benefit from more experts
- Evidence anchors: Section 3.1 shows λ controls sparsity tendency; Section 4.5 shows rarer tokens activate more experts; related work suggests token-level may be insufficient for some tasks
- Break condition: MLP hidden dimension too small (128 underperforms for Qwen3) limits capacity to predict meaningful λ

### Mechanism 3: Guaranteed Non-Zero Activation via Simplex Constraint
- Claim: Sparsegen formulation mathematically guarantees at least one expert is activated per token, avoiding degenerate representations
- Mechanism: Lemma 1 proves for λ<1, Sparsegen solution always has nonempty support (||p||_0 ≥ 1) due to simplex constraint
- Core assumption: Zero-expert routing degrades representations; avoiding it improves stability
- Evidence anchors: Section 3.1 states Sparsegen always has nonempty support; Appendix C.1 shows ReMoLE can assign zero experts; no direct corpus comparison on this guarantee
- Break condition: None identified; guarantee is mathematical under stated conditions

## Foundational Learning

- Concept: **Mixture of Experts (MoE) Routing**
  - Why needed here: LD-MoLE builds on MoE principles where sparse expert activation enables scalable model capacity. Understanding TopK routing limitations motivates differentiable alternative.
  - Quick check question: Can you explain why TopK routing is non-differentiable and what "routing collapse" means in MoE training?

- Concept: **Projection onto Probability Simplex**
  - Why needed here: Sparsegen is specific simplex projection. Understanding constraint Σp_i = 1, p_i ≥ 0 defines simplex is prerequisite to following closed-form derivation.
  - Quick check question: Given scores u = [3, 1, 0], what does softmax projection produce versus sparse projection?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Each "expert" in LD-MoLE is LoRA module (ΔW = AB). Understanding LoRA adds trainable low-rank matrices to frozen weights explains parameter efficiency.
  - Quick check question: If weight matrix is 4096×4096 and LoRA rank r=8, how many trainable parameters does one LoRA expert add?

## Architecture Onboarding

- Component map: Token x → Gate scores u = W_gate · x → Shared MLP predicts λ = f(x) → Sparsegen computes τ from sorted scores and λ → Routing weights p_i = [(u_i - τ)/(1-λ)]_+ → Expert outputs aggregated: h = W_base·x + Σ_i p_i · (A_i·B_i·x)

- Critical path:
  1. Token x enters layer → gate scores computed: u = W_gate · x
  2. Shared MLP predicts λ = f(x)
  3. Sparsegen computes threshold τ from sorted scores and λ (Eq. 5)
  4. Routing weights p_i = [(u_i - τ)/(1-λ)]_+ computed
  5. Expert outputs aggregated: h_t = W_base·x_t + Σ_i p_{t,i}·(A_i·B_i·x_t)

- Design tradeoffs:
  - Shared vs Local MLP: Shared (one per input dimension) vs Local (one per module). Table 4 shows shared slightly outperforms local while using fewer parameters tied to layer count.
  - Sparsity vs Performance: Table 3 shows tradeoff—disabling sparsity loss (β=0) achieves best scores, but sparsity loss (β>0) reduces activated experts. Task-dependent optimal.
  - MLP Hidden Dimension: Table 5 shows 256 optimal for Qwen3, 512 for Llama. Larger isn't always better.

- Failure signatures:
  - Zero activation: If using ReLU-based routing instead of Sparsegen, upper layers may activate <1 expert on average (Fig 6).
  - Training instability: If λ approaches 1, denominator (1-λ) causes numerical issues.
  - Over-regularized sparsity: "Excessive sparsity can degrade performance, as we observe performance drops in later stages of training."

- First 3 experiments:
  1. Reproduce λ prediction vs fixed λ: Compare Table 2 settings on single dataset to validate learned λ outperforms any fixed value.
  2. Ablate sparsity loss coefficient: Sweep β ∈ {0, 0.01, 0.1, 1.0} to observe tradeoff curve on target task (following Table 3).
  3. Verify activation guarantee: Log per-layer average activated experts during training to confirm ≥1 expert always active and compare against ReMoLE baseline.

## Open Questions the Paper Calls Out
None

## Limitations
- Token-level routing assumptions may not hold for all tasks; sequence-level routing approaches suggest this limitation
- Numerical stability constraints require λ<1; paper doesn't specify initialization strategies to prevent λ approaching 1 during training
- Performance gains demonstrated primarily on instruction-tuning and sequence classification; effectiveness for other task types remains untested
- Parameter sensitivity to MLP hidden dimension, sparsity loss coefficient, and LoRA scaling factor not fully characterized

## Confidence
- High Confidence: Mathematical guarantee that Sparsegen routing produces at least one activated expert per token (Lemma 1) is rigorously proven and experimentally validated
- Medium Confidence: Claim that token-adaptive sparsity via learned λ improves performance is supported by experimental results but relies on assumption that token embeddings contain sufficient signal for routing decisions
- Low Confidence: Assertion that LD-MoLE "achieves highest average scores" across all benchmarks assumes sound evaluation protocol and comparable implementations across methods

## Next Checks
1. Implement sequence-level routing variant and compare performance on same benchmarks to test whether token-level features truly contain sufficient signal for optimal expert allocation
2. Instrument training to log λ values and routing distributions across layers and tokens, verifying λ remains bounded away from 1 and routing weights remain well-behaved
3. Evaluate LD-MoLE on at least two task types not included in original experiments (e.g., long-context generation or structured prediction tasks) to validate method's advantages extend beyond instruction-tuning and classification domains