---
ver: rpa2
title: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large
  Language Models and Beyond
arxiv_id: '2508.00522'
source_url: https://arxiv.org/abs/2508.00522
tags:
- lora
- efmlora
- fmlora
- learning
- perturbation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving generalization
  in fine-tuning large language models by efficiently seeking flat minima in the loss
  landscape. The authors propose Flat Minima LoRA (FMLoRA), which theoretically demonstrates
  that perturbations in the full parameter space can be equivalently transferred to
  the low-rank subspace, avoiding interference between multiple matrices.
---

# Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond

## Quick Facts
- arXiv ID: 2508.00522
- Source URL: https://arxiv.org/abs/2508.00522
- Authors: Jiaxin Deng; Qingcheng Zhu; Junbiao Pang; Linlin Yang; Zhongqian Fu; Baochang Zhang
- Reference count: 17
- Primary result: EFMLoRA achieves near-LoRA computational efficiency while attaining comparable or better performance on multiple model types and tasks.

## Executive Summary
This paper addresses the challenge of improving generalization in fine-tuning large language models by efficiently seeking flat minima in the loss landscape. The authors propose Flat Minima LoRA (FMLoRA), which theoretically demonstrates that perturbations in the full parameter space can be equivalently transferred to the low-rank subspace, avoiding interference between multiple matrices. They further introduce an efficient version, EFMLoRA, which uses exponential moving average to accelerate optimization. Extensive experiments on RoBERTa, GPT-2, CLIP, and Qwen-VL-Chat show that EFMLoRA achieves near-LoRA computational efficiency while attaining comparable or better performance.

## Method Summary
The method introduces Flat Minima LoRA (FMLoRA) as an adaptation of Sharpness-Aware Minimization (SAM) for LoRA-based fine-tuning. FMLoRA theoretically demonstrates that perturbations in the full parameter space can be equivalently transferred to the low-rank subspace, avoiding interference between multiple matrices. To address SAM's computational overhead, the authors propose Efficient FMLoRA (EFMLoRA), which uses exponential moving average (EMA) of past perturbations to approximate the adversarial perturbation with bounded error. This approach maintains LoRA's parameter efficiency while seeking flat minima that correlate with improved generalization.

## Key Results
- On GLUE with RoBERTa-large, EFMLoRA outperforms LoRA and full fine-tuning by 1.0% and 0.5% on average, respectively
- On vision-language models like Qwen-VL-Chat, EFMLoRA achieves improvements of 1.5% and 1.0% on SQA and VizWiz datasets, respectively
- EFMLoRA achieves near-LoRA computational efficiency (approximately 1.1× overhead) while attaining comparable or better performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perturbations in the full parameter space can be equivalently represented within a single low-rank matrix (B), avoiding interference between dual low-rank perturbations.
- Mechanism: The paper shows that the full-space perturbation gradient ∇L_W(W) can be approximated using LoRA's standard gradients via pseudo-inverse operations (Eq. 10-12). This allows computing the perturbation in the full space using only information available during LoRA training, then projected onto matrix B alone (Eq. 15: E_B ≈ (1/s)Ē_W A^+).
- Core assumption: The pseudo-inverse of matrix A remains well-conditioned throughout training (the paper reports condition numbers typically around 3).
- Evidence anchors:
  - [abstract]: "we theoretically demonstrate that perturbations in the full parameter space can be transferred to the low-rank subspace"
  - [section: Method - FMLoRA]: Eq. 14-15 show the mathematical transfer; the approach "eliminates the potential interference introduced by perturbations across multiple matrices in the low-rank subspace"
  - [corpus]: Related work Bi-LoRA addresses similar SAM efficiency concerns but with different architectural choices; corpus shows active but not settled research direction.

### Mechanism 2
- Claim: EMA of past perturbations approximates SAM's adversarial perturbation with bounded error that decreases during training.
- Mechanism: Instead of computing two forward-backward passes per step (SAM's standard cost), EFMLoRA maintains Ẽ_B^t = (1-β)Ẽ_B^{t-1} + βE_B^t, reusing the EMA perturbation for the next iteration's gradient computation. Theorem 2 proves this error is bounded and shrinks as t increases under convex local loss assumptions.
- Core assumption: The local loss landscape near convergence is approximately convex (Assumption 4), and gradients have bounded variance (Assumption 3).
- Evidence anchors:
  - [section: Efficient FMLoRA]: Theorem 2 provides the error bound between EMA perturbation sharpness (S_EMA) and SAM sharpness (S_SAM)
  - [section: Appendix D.1]: Figure 3 empirically validates that |S_EMA - S_SAM| decreases across all six tested datasets during training
  - [corpus]: No direct corpus validation of this specific EMA-as-perturbation-surrogate technique; approach appears novel.

### Mechanism 3
- Claim: Reduced sharpness in the parameter space correlates with improved generalization in LoRA fine-tuning.
- Mechanism: By seeking flat minima through SAM-style optimization adapted for LoRA, the model converges to regions where small parameter perturbations cause minimal loss increase. The empirical results show consistent improvements across model types and tasks when sharpness is reduced.
- Core assumption: The flat minima hypothesis (Hochreiter & Schmidhuber, 1997) extends from general deep learning to the specific case of low-rank adaptation.
- Evidence anchors:
  - [abstract]: "These empirical results also verify that the generalization of LoRA is closely related to sharpness, which is omitted by previous methods"
  - [section: Experiments]: EFMLoRA outperforms LoRA by 1.0% average on GLUE/RoBERTa-large and by 1.5%/1.0% on Qwen-VL-Chat tasks
  - [corpus]: "Flat Minima and Generalization: Insights from Stochastic Convex Optimization" (h-index 20 author) provides theoretical support for flat minima-generalization connection in broader contexts.

## Foundational Learning

- Concept: LoRA (Low-Rank Adaptation) decomposition
  - Why needed here: EFMLoRA operates entirely within LoRA's low-rank constraint; understanding W = W₀ + sBA is essential for following the perturbation transfer derivations.
  - Quick check question: Given weight dimension n×m and rank r, what are the dimensions of B and A? (Answer: B is n×r, A is r×m)

- Concept: Sharpness-Aware Minimization (SAM) intuition
  - Why needed here: EFMLoRA is fundamentally SAM adapted for LoRA; the motivation and first-order Taylor approximation (ε̂ = ρ∇L/||∇L||) are prerequisites.
  - Quick check question: Why does SAM require approximately 2× computation compared to SGD? (Answer: Two gradient computations per step—one to find the perturbation direction, one to compute the gradient at the perturbed point)

- Concept: Pseudo-inverse and matrix conditioning
  - Why needed here: The perturbation transfer relies on computing A^+ and (A^⊤)^+; numerical stability depends on condition number.
  - Quick check question: If matrix A has a high condition number, what happens to pseudo-inverse accuracy? (Answer: Accuracy degrades; small perturbations in A cause large changes in A^+)

## Architecture Onboarding

- Component map:
  - Standard LoRA: Frozen W₀ → trainable B, A matrices → output
  - FMLoRA: Adds perturbation computation to B via full-space gradient approximation, requiring 2 forward-backward passes
  - EFMLoRA: Replaces second pass with EMA-stored perturbation, maintaining Ẽ_B as additional state

- Critical path:
  1. Compute gradient at perturbed point (W₀, B_{t-1} + Ẽ_B^{t-1}, A_{t-1})
  2. Approximate full-space gradient via Eq. 12: ∇L_W ≈ 0.5×[(1/s)∇L_B·(A^⊤)^+ + (1/s)(B^⊤)^+·∇L_A]
  3. Compute target perturbation Ē_W via Eq. 13
  4. Project to B-space: E_B = (1/s)Ē_W·A^+
  5. Update EMA: Ẽ_B^t = (1-β)Ẽ_B^{t-1} + βE_B^t
  6. Update parameters, advance to next perturbed point

- Design tradeoffs:
  - ρ (perturbation radius): Task-dependent; larger ρ (0.2) better for ScienceQA, smaller ρ (0.05) better for VizWiz. Requires tuning.
  - β (EMA momentum): Paper uses 0.99 for most experiments; higher β = smoother but slower adaptation.
  - Matrix choice: Paper transfers perturbation to B (not A) based on HydraLoRA's observation that B is domain-specific while A captures common features.

- Failure signatures:
  - Exploding gradients or NaN losses: Likely ill-conditioned A matrix causing pseudo-inverse instability
  - No improvement over baseline LoRA: ρ may be too small (insufficient exploration) or too large (destabilizing)
  - Significantly slower than LoRA (>1.3×): Verify EMA is being reused correctly; should only be 1.1-1.2× per Table 6

- First 3 experiments:
  1. Reproduce RoBERTa-large on single GLUE task (e.g., MNLI) with paper's hyperparameters (ρ=0.6, β=0.99, r=8, α=16) to validate implementation.
  2. Ablation study on ρ: Sweep [0.05, 0.1, 0.2, 0.3, 0.6] on a held-out validation set to confirm task-dependent optimal values.
  3. Compare memory and runtime against vanilla LoRA on your target model scale; verify overhead matches paper's reported ~1.1×.

## Open Questions the Paper Calls Out
None

## Limitations
- The perturbation transfer mechanism relies critically on matrix A maintaining a well-conditioned pseudo-inverse throughout training, which hasn't been validated across diverse model architectures and fine-tuning scenarios.
- The empirical validation focuses primarily on benchmark datasets and specific model types (RoBERTa, GPT-2, CLIP variants), leaving generalization claims to "beyond" these cases untested.
- The claim that this approach extends seamlessly to multimodal models and domains with limited data remains unverified.

## Confidence
**High Confidence**: The computational efficiency improvements of EFMLoRA over FMLoRA are well-supported by the theoretical error bounds in Theorem 2 and validated through runtime measurements showing ~1.1× overhead versus LoRA.

**Medium Confidence**: The generalization improvements (1.0-1.5% gains across tasks) are supported by extensive experiments but depend on proper hyperparameter tuning (ρ selection is task-dependent).

**Low Confidence**: The claim that this approach extends seamlessly to "beyond" the tested model types and tasks. The matrix conditioning assumptions and perturbation transfer validity across diverse architectures remain unverified.

## Next Checks
1. Systematically measure A matrix condition numbers across different fine-tuning runs, tasks, and model scales to identify thresholds where pseudo-inverse approximation degrades.
2. Evaluate EFMLoRA on tasks known for highly non-convex optimization landscapes (e.g., fine-tuning for domain adaptation with significant distribution shift) to quantify EMA approximation error in challenging scenarios.
3. Test EFMLoRA on under-resourced domains and multimodal tasks not represented in the current validation (e.g., medical text, scientific documents, cross-lingual settings) to measure whether sharpness-generalization relationship holds when pretraining and fine-tuning distributions differ substantially.