---
ver: rpa2
title: 'Tower+: Bridging Generality and Translation Specialization in Multilingual
  LLMs'
arxiv_id: '2506.17080'
source_url: https://arxiv.org/abs/2506.17080
tags:
- translation
- data
- tower
- text
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "TOWER+ introduces a suite of multilingual language models that\
  \ achieve state-of-the-art translation performance while maintaining strong general-purpose\
  \ capabilities, addressing the common trade-off where specialized fine-tuning degrades\
  \ instruction-following and reasoning abilities. The authors develop a multi-stage\
  \ post-training pipeline\u2014comprising continued pretraining with multilingual\
  \ and instruction-following data, supervised fine-tuning with curated responses\
  \ from multiple top models, weighted preference optimization, and reinforcement\
  \ learning with verifiable rewards\u2014to jointly improve translation accuracy\
  \ and general chat abilities."
---

# Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs

## Quick Facts
- arXiv ID: 2506.17080
- Source URL: https://arxiv.org/abs/2506.17080
- Reference count: 40
- Introduces TOWER+ models that achieve state-of-the-art translation performance while maintaining strong general-purpose capabilities

## Executive Summary
TOWER+ presents a suite of multilingual language models that successfully address the common trade-off between translation specialization and general-purpose capabilities. The authors develop a multi-stage post-training pipeline that enables models to excel at both translation tasks and general chat abilities, outperforming or matching larger general-purpose models including Llama 3.3 70B and GPT-4o. The 72B variant achieves best-in-class translation performance for high-resource languages while maintaining top scores on combined instruction-following and translation benchmarks.

## Method Summary
The TOWER+ methodology employs a multi-stage post-training pipeline that includes continued pretraining with multilingual and instruction-following data, supervised fine-tuning using curated responses from multiple top models, weighted preference optimization, and reinforcement learning with verifiable rewards. This approach enables the models to jointly improve translation accuracy and general chat abilities without sacrificing performance in either domain. The training process leverages synthetic data generation and preference optimization to balance the competing objectives of translation specialization and general-purpose capability maintenance.

## Key Results
- TOWER+ models (2B, 9B, 72B) outperform or match larger general-purpose open-weight and proprietary models in translation tasks
- The 72B variant achieves best-in-class translation performance for high-resource languages on WMT24++ and XCOMET-XXL benchmarks
- TOWER+ models demonstrate top scores on the novel IF-MT benchmark combining instruction-following and translation capabilities
- Models maintain strong general chat performance on M-ArenaHard while excelling at translation tasks

## Why This Works (Mechanism)
The success of TOWER+ stems from its carefully designed multi-stage post-training pipeline that addresses the fundamental tension between specialization and generalization. By incorporating multilingual data during continued pretraining, the models maintain broad linguistic coverage while developing translation expertise. The supervised fine-tuning stage uses curated responses from multiple top models, providing diverse high-quality examples across both translation and general instruction domains. Weighted preference optimization allows the models to learn nuanced preferences for translation quality while preserving instruction-following capabilities. Finally, reinforcement learning with verifiable rewards provides direct optimization for translation accuracy while maintaining general reasoning abilities through carefully designed reward functions.

## Foundational Learning
- Multilingual pretraining: Essential for building broad linguistic understanding across diverse language families; quick check: vocabulary coverage and cross-lingual transfer performance
- Preference optimization: Enables fine-grained control over model behavior and quality trade-offs; quick check: alignment with human preferences across multiple dimensions
- Reinforcement learning with verifiable rewards: Provides direct optimization for translation accuracy while maintaining general capabilities; quick check: reward consistency and generalization to unseen examples
- Multi-task fine-tuning: Allows simultaneous improvement across translation and general capabilities; quick check: balanced performance across both task types
- Synthetic data generation: Scales training data while maintaining quality through careful curation; quick check: data diversity and representation across language pairs

## Architecture Onboarding

Component map: Multilingual pretraining -> Supervised fine-tuning -> Preference optimization -> Reinforcement learning -> IF-MT evaluation

Critical path: The model architecture follows a standard transformer design with multilingual tokenizers, but the critical innovation lies in the post-training pipeline rather than architectural changes. The progression from general pretraining through increasingly specialized fine-tuning stages creates the foundation for achieving both translation excellence and general capability maintenance.

Design tradeoffs: The primary tradeoff involves balancing the computational cost of multi-stage post-training against the performance gains achieved. The authors opt for comprehensive fine-tuning stages that maximize performance but require significant computational resources. Another tradeoff exists between translation specialization and general capability preservation, which the preference optimization and reinforcement learning stages explicitly address.

Failure signatures: Models may overfit to high-resource languages at the expense of low-resource language performance, exhibit bias toward translation tasks over general reasoning, or show degradation in instruction-following capabilities when translation performance is prioritized. The multi-stage approach helps mitigate these risks through diverse training objectives.

First experiments:
1. Evaluate baseline multilingual model performance on high vs low resource language pairs
2. Test instruction-following capability retention after initial translation fine-tuning
3. Validate preference optimization effectiveness across translation and general chat tasks

## Open Questions the Paper Calls Out
The paper acknowledges limitations in its evaluation scope, particularly regarding low-resource language performance and zero-shot generalization across diverse domains. It also notes potential biases introduced through synthetic data generation and preference optimization from base models. The authors recognize the need for more extensive human evaluation across diverse linguistic communities and cultural contexts, especially for translation and localization tasks. Real-world deployment considerations including latency, cost, and scalability are not extensively addressed.

## Limitations
- Evaluation focuses primarily on high-resource languages with limited validation for truly low-resource scenarios
- Performance claims are qualified by the focus on translation-centric benchmarks rather than broad task generalization
- The training methodology may introduce biases from base models used for synthetic response generation
- Lacks extensive human evaluation across diverse linguistic communities and cultural contexts
- Does not address real-world deployment challenges such as latency, cost, and scalability

## Confidence

- Translation performance claims: High - Well-supported by comprehensive benchmark results across multiple datasets
- General capability maintenance claims: Medium - Demonstrated on specific benchmarks but may not generalize to all task types
- Low-resource language performance: Low - Limited evaluation scope focused on high-resource languages

## Next Checks

1. Conduct extensive evaluation on low-resource language pairs and document performance degradation patterns relative to high-resource languages
2. Perform robustness testing with adversarial prompts, domain shift scenarios, and cross-cultural context variations to assess real-world deployment readiness
3. Execute comprehensive human evaluation studies across diverse linguistic communities to validate benchmark performance and identify potential cultural or contextual translation issues