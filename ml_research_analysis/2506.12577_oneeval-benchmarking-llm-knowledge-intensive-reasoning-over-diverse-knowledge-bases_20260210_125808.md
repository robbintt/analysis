---
ver: rpa2
title: 'OneEval: Benchmarking LLM Knowledge-intensive Reasoning over Diverse Knowledge
  Bases'
arxiv_id: '2506.12577'
source_url: https://arxiv.org/abs/2506.12577
tags:
- reasoning
- knowledge
- code
- answer
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# OneEval: Benchmarking LLM Knowledge-intensive Reasoning over Diverse Knowledge Bases

## Quick Facts
- arXiv ID: 2506.12577
- Source URL: https://arxiv.org/abs/2506.12577
- Reference count: 40
- Key outcome: Performance drops sharply from 53% on textual reasoning to 25% on formal logic tasks

## Executive Summary
OneEval is a comprehensive benchmark designed to evaluate large language models' knowledge-intensive reasoning capabilities across four structured knowledge modalities: unstructured text, knowledge graphs, code, and formal logic. The benchmark consists of 4,019 instances across 11 public datasets, testing models' ability to retrieve and reason over external knowledge sources. Results reveal a pronounced performance hierarchy where accuracy declines as knowledge structure increases in complexity, with reasoning-optimized models (R-LLMs) showing advantages on structured tasks but performance collapse on extended reasoning chains.

## Method Summary
The evaluation framework tests LLMs on inference-only tasks where models receive a query Q and textualized external knowledge set S, then must generate answers. Retrieval is standardized using dense vector cosine similarity, while evaluation metrics vary by task type (Accuracy, F1, ISM@1, Exact Match). The benchmark covers 18 models including GPT-4.1, DeepSeek-R1, and Llama4 across five domains. Outputs are parsed using strict formatting requirements, though this creates fragility in practical implementation.

## Key Results
- Performance hierarchy: Text (53%) → Code (35%) → KG (30%) → Logic (25%) accuracy
- R-LLMs outperform NR-LLMs by 4.3-5.6 points on structured knowledge tasks
- Cross-modality transfer fails on hard tasks: Text and Logic decouple (ρ=0.09) while KG and Code remain supportive (ρ=0.64)
- Extended reasoning chains show inverted-U performance, peaking at 800-1000 tokens before sharp decline

## Why This Works (Mechanism)

### Mechanism 1
Performance degrades predictably as knowledge modality structure increases, forming a hierarchy where each step requires more explicit symbolic manipulation rather than pattern matching. Models trained primarily on unstructured text have "latent code semantics" but lack the ability to "expose and manipulate discrete graph structures."

### Mechanism 2
Extended reasoning chains show inverted-U performance—benefits peak at ~800-1000 tokens, then decline sharply. Additional reasoning steps introduce noise and compounding error risk, with R-LLMs exhibiting non-monotonic curves that initially improve but then significantly decline.

### Mechanism 3
Cross-modality transfer is asymmetric—KG↔Code transfers well; Text↔Logic decouples on hard tasks. Easy tasks allow "surface-level pattern matching" that creates spurious cross-modality correlation, while hard tasks expose "two distinct reasoning substrates: (i) symbolic (KG/Code)... (ii) shallow lexical (Text)."

## Foundational Learning

- **Concept: Answer Set Programming (ASP) semantics**
  - Why needed: Logic tasks use ASP programs with facts, rules, and stable model computation. Understanding default negation ("not") vs. strong negation ("-") is critical.
  - Quick check: Given `W={Bird(Tweety)}` and rule `CanFly(X) :- Bird(X), not Abnormal(X)`, what is entailed?

- **Concept: Knowledge graph triple patterns and multi-hop queries**
  - Why needed: KG tasks require navigating entity-relation-entity paths; PeopleRelQA uses "multi-hop query, path checking, compositional query" patterns.
  - Quick check: How do you extract a 3-hop path from triples matching (A→B→C→D)?

- **Concept: Chain-of-thought vs. explicit reasoning scaffolds**
  - Why needed: R-LLMs (models with explicit inference scaffolds) outperform NR-LLMs by 4.3-5.6 points on structured tasks.
  - Quick check: What is the performance difference between R-LLM and NR-LLM averages on the hard set?

## Architecture Onboarding

- **Component map**: Input Query (Q) + Knowledge Base (S) → Dense Retrieval (cos similarity) → Text-formatted Context → LLM fθ → Answer (A)
- **Critical path**: Retrieval is standardized (not evaluated); focus is on reasoning over potentially noisy retrieved context
- **Design tradeoffs**: Static datasets vs. dynamic real-world tasks; unified evaluation vs. task-specific metrics; token-length proxy for reasoning depth vs. explicit step counting
- **Failure signatures**: Near-random performance on KG/Logic for small models; R-LLM performance collapse when output exceeds ~1200 tokens; cross-modality transfer failure on hard tasks
- **First 3 experiments**:
  1. Replicate the structuredness gradient (Text→Code→KG→Logic) on your target model to establish baseline position
  2. Test reasoning chain length: truncate outputs at 400, 800, 1000, 1200 tokens and measure accuracy curve
  3. Probe cross-modality transfer: fine-tune on KG tasks, evaluate on Code hard set (expect ~0.64 correlation benefit)

## Open Questions the Paper Calls Out

### Open Question 1
How can evaluation frameworks be adapted to assess LLM reasoning capabilities on dynamic, evolving knowledge bases rather than static datasets? The benchmark's "focus on static datasets may not fully capture the dynamic nature of real-world reasoning tasks."

### Open Question 2
What specific control mechanisms can prevent the performance collapse in Reasoning-LLMs when chain lengths exceed optimal thresholds? Section 4.6 identifies "diminishing returns from extended reasoning chains" with performance decline beyond ~800-1000 tokens.

### Open Question 3
What architectural innovations are necessary to bridge the performance gap between unstructured textual reasoning and highly structured formal logic? The Conclusion calls for "novel architectures, training paradigms, and reasoning strategies tailored to structured knowledge."

## Limitations
- Evaluation relies on static, pre-constructed datasets rather than dynamic retrieval from live knowledge sources
- Exact retrieval model and generation hyperparameters are not disclosed, critical for faithful reproduction
- Output parsing is fragile due to strict formatting requirements that models often violate
- Context length constraints may cause truncation of long knowledge sources, potentially biasing results

## Confidence

- **High confidence**: The structuredness performance gradient (Text→Code→KG→Logic) is empirically supported by consistent accuracy drops across all evaluated models
- **Medium confidence**: The cross-modality transfer asymmetry is supported by Spearman correlation data, but the underlying causal mechanism remains inferential
- **Low confidence**: The inverted-U curve for reasoning chain length relies on token-length as a proxy for complexity, which may conflate verbosity with actual reasoning depth

## Next Checks
1. Replicate the structuredness gradient: Evaluate your target model across Text, Code, KG, and Logic tasks to establish baseline performance position
2. Validate reasoning chain length curve: Systematically truncate model outputs at 400, 800, 1000, and 1200 tokens, measuring accuracy to confirm the predicted peak-then-decline pattern
3. Probe cross-modality transfer: Fine-tune on KG tasks, then evaluate on Code hard set to test whether the reported ρ=0.64 correlation translates to practical performance gains