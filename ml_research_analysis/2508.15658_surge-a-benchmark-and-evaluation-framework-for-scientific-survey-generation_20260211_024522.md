---
ver: rpa2
title: 'SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation'
arxiv_id: '2508.15658'
source_url: https://arxiv.org/abs/2508.15658
tags:
- survey
- learning
- generation
- evaluation
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SurGE, a benchmark and evaluation framework
  for automated scientific survey generation. SurGE provides a large-scale academic
  corpus of over one million papers and 205 expert-validated ground-truth surveys,
  along with a comprehensive multi-dimensional evaluation protocol assessing comprehensiveness,
  citation accuracy, structure, and content quality.
---

# SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation

## Quick Facts
- **arXiv ID**: 2508.15658
- **Source URL**: https://arxiv.org/abs/2508.15658
- **Reference count**: 35
- **Key outcome**: SurGE reveals that while specialized agentic pipelines outperform standard RAG baselines in fluency and structural quality, they still struggle with citation accuracy, with document-level accuracy around 47% and survey reference recall below 10%.

## Executive Summary
SurGE introduces a comprehensive benchmark and evaluation framework for automated scientific survey generation. It provides a large-scale academic corpus of over one million papers and 205 expert-validated ground-truth surveys, along with a multi-dimensional evaluation protocol assessing comprehensiveness, citation accuracy, structure, and content quality. The evaluation reveals that current state-of-the-art LLM-based systems face significant challenges in effectively utilizing retrieved information for survey generation, with retrieval recall (~36% R@100) far exceeding end-to-end citation recall (<10%), indicating a synthesis bottleneck rather than a data coverage issue.

## Method Summary
SurGE formalizes scientific survey generation as a two-stage pipeline: retrieving relevant papers from a corpus and synthesizing them into a structured survey with citations. The framework uses a dual-encoder RoBERTa retriever trained with contrastive learning to fetch papers, then evaluates generated surveys across four dimensions: reference recall for comprehensiveness, 3-level NLI-based citation accuracy, LLM-as-a-judge structural quality, and content quality scoring. The evaluation uses 205 expert-validated surveys from computer science as ground truth, with a 1M+ arXiv paper corpus providing metadata for retrieval.

## Key Results
- Retriever achieves 68.05% R@1000, yet end-to-end baselines achieve <10% recall, indicating synthesis bottlenecks
- Document-level citation accuracy around 47%, with survey reference recall below 10%
- Specialized agentic pipelines outperform standard RAG baselines in fluency and structural quality
- LLM-as-a-judge correlations with human rankings show Kendall's τ ~0.6-0.8, comparable to prior NLG benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decoupling retrieval from generation exposes whether performance bottlenecks stem from missing evidence or synthesis failures.
- **Mechanism**: The framework isolates the retriever from the generator. By evaluating retrieval recall independently (R@k metrics) against ground-truth citations, then comparing to end-to-end citation recall in generated surveys, it quantifies the "retrieval-to-usage gap."
- **Core assumption**: Ground-truth survey citations represent a reasonable approximation of the core literature for a topic.
- **Evidence anchors**: Retriever achieves 68.05% R@1000, yet end-to-end baselines achieve <10% recall, indicating the bottleneck is in agents' capacity to select/synthesize, not data coverage.

### Mechanism 2
- **Claim**: Three-level NLI-based citation accuracy assessment scales factual grounding evaluation beyond human annotation.
- **Mechanism**: For each citation, the framework constructs premise-hypothesis pairs at document, section, and sentence levels, using `nli-deberta-v3-base` to predict ENTAILMENT/NEUTRAL/CONTRADICTION. Citations not in the corpus score 0 (hallucination); those in ground-truth bibliography score 1 at document level automatically.
- **Core assumption**: NLI probability distributions meaningfully capture semantic relevance between cited paper content and survey context.
- **Evidence anchors**: Explicit scoring: 1 if p_ent > max(p_neu, p_con); 0.5 if p_neu dominates; 0 otherwise.

### Mechanism 3
- **Claim**: LLM-as-a-Judge for structure and content quality correlates sufficiently with expert rankings to serve as a scalable proxy.
- **Mechanism**: GPT-4o evaluates Structure Quality Score (SQS) by comparing generated vs. ground-truth hierarchical outlines, and Content Quality Score (CQS) section-by-section on fluency, logic, redundancy, clarity, and error-free criteria (0-5 scale). Human meta-evaluation (Kendall's τ, Spearman's ρ) confirms alignment.
- **Core assumption**: LLM judge preferences generalize beyond the specific models tested.
- **Evidence anchors**: "w/ GT" Kendall's τ = 0.805 (SQS), 0.797 (CQS); "w/o GT" τ = 0.610 (SQS), 0.594 (CQS)—comparable to prior NLG meta-evaluation benchmarks.

## Foundational Learning

- **Concept**: Natural Language Inference (NLI)
  - **Why needed here**: Core to the citation accuracy mechanism; understanding how entailment/contradiction probabilities map to relevance scores is essential for interpreting results and debugging failures.
  - **Quick check question**: Given a premise "Paper A proposes method X for task Y" and hypothesis "Paper A is relevant to a survey on task Y," what NLI label would you expect, and what score would SurGE assign?

- **Concept**: Information Retrieval Recall@k
  - **Why needed here**: Distinguishing retrieval-stage performance from generation-stage utilization is central to SurGE's diagnostic approach.
  - **Quick check question**: If a system retrieves 100 papers and 30 are ground-truth citations out of 60 total ground-truth papers, what is R@100? What does this tell you about corpus coverage vs. retrieval quality?

- **Concept**: Kendall's τ and Spearman's ρ for Ranking Correlation
  - **Why needed here**: Meta-evaluation metrics that validate whether automated scores align with human expert rankings.
  - **Quick check question**: Why does the paper report both "w/ GT" and "w/o GT" settings? What different conclusions can you draw from each?

## Architecture Onboarding

- **Component map**: Corpus (1,086,992 arXiv papers) -> Paper Retriever (RoBERTa-base dual-encoder) -> Test Instances (205 expert-validated surveys) -> Evaluation Framework (Comprehensiveness, Citation Accuracy, Structural Quality, Content Quality) -> Meta-Evaluation (Human rankings validation)

- **Critical path**: Load corpus and test instances -> For each test topic, retrieve top-k papers (default k=100) -> Run baseline generator (RAG, AutoSurvey, StepSurvey, SurveyForge) -> Extract citations from generated survey -> Compute all four dimension metrics -> (Optional) Run meta-evaluation with human rankings for validation

- **Design tradeoffs**:
  - Metadata-only corpus (no full-text): Enables legal/ethical compliance but limits granularity of relevance assessment to abstracts
  - NLI-based citation accuracy vs. human annotation: Scalable but may miss nuanced domain-specific relevance
  - Single-domain (CS) focus: Ensures expert validation quality but limits generalizability claims
  - Assumption: Expert-written surveys represent ground truth; the paper acknowledges they may not be exhaustive

- **Failure signatures**:
  - Low retrieval recall but high generation recall: Hallucination—generator citing papers not retrieved (check corpus coverage)
  - High structural scores but low citation accuracy: "Fluency trap"—coherent but ungrounded text
  - Low SHR but high SQS: Correct high-level structure but missing specific subsections
  - Sentence-level accuracy << document-level: Citations are topically relevant but misplaced

- **First 3 experiments**:
  1. Reproduce baseline comparison: Run RAG-Qwen, AutoSurvey, StepSurvey, SurveyForge on the 205 test instances; verify Table 4 metrics (Recall, Doc-Acc, SQS, CQS). Check if your retrieval stage achieves ~36% R@100 as reported.
  2. Ablate retrieval upper bound: Manually inject ground-truth citations into the retrieved set; measure citation recall lift. This quantifies the synthesis bottleneck independently of retrieval.
  3. Validate NLI scoring sanity: Sample 20 generated citations; manually label document/section/sentence relevance and compare to NLI-assigned scores. Flag systematic over-/under-scoring patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can evaluation frameworks effectively quantify "insightfulness" or the ability to identify novel future research directions in automatically generated surveys?
- Basis in paper: The authors explicitly state in the Limitations that SurGE does not evaluate "forward-looking analysis" or "insightfulness" due to the difficulty of establishing a standardized ground truth for these cognitive tasks.
- Why unresolved: Defining a ground truth for "novelty" or "synthesis" is inherently subjective, making it difficult to create an automated, scalable metric.
- What evidence would resolve it: The development of a proxy metric for insight that shows high correlation with expert assessments of a survey's predictive value.

### Open Question 2
- Question: Can sophisticated retrieval paradigms (e.g., task-oriented search agents) bridge the performance gap between high document availability and low reference recall?
- Basis in paper: The paper highlights a disconnect where the retriever successfully recalls 68.05% of ground-truth papers (R@1000), yet end-to-end baselines achieve less than 10% recall.
- Why unresolved: The bottleneck appears to be the generator's capacity to select and utilize information, rather than the availability of data in the corpus.
- What evidence would resolve it: A new generation pipeline that significantly improves reference recall (e.g., approaching 40-50%) using the same retrieval corpus.

### Open Question 3
- Question: To what extent do the current evaluation metrics and model performances generalize to scientific domains outside of Computer Science?
- Basis in paper: The authors acknowledge that SurGE is limited to Computer Science due to the team's domain expertise and the availability of open-access data on arXiv.
- Why unresolved: Different academic domains possess distinct citation densities, structural norms, and linguistic patterns that may affect the validity of current NLI-based or LLM-based evaluators.
- What evidence would resolve it: A cross-domain evaluation study demonstrating that SurGE's metrics align with human judgment in fields like Biology or Physics.

## Limitations
- Ground-truth survey citations serve as a proxy rather than exhaustive truth, potentially underestimating comprehensiveness for novel topics
- NLI-based citation accuracy mechanism lacks direct empirical validation against human annotations for this specific task
- Single-domain (CS) focus and metadata-only corpus limit generalizability claims to other fields or full-text scenarios

## Confidence
- **High Confidence**: The framework's multi-dimensional evaluation protocol is technically sound and the retriever training procedure is clearly specified
- **Medium Confidence**: The LLM-as-a-judge correlation with human rankings (Kendall's τ ~0.6-0.8) is comparable to prior NLG benchmarks
- **Low Confidence**: The NLI-based citation accuracy scoring mechanism lacks direct validation—while the mathematical formulation is clear, its semantic fidelity for scientific survey evaluation is assumed rather than empirically verified

## Next Checks
1. **NLI Scoring Validation**: Manually annotate 50-100 generated citations with document/section/sentence relevance judgments and compare against NLI-assigned scores to quantify systematic bias or calibration issues

2. **Ground-Truth Exhaustiveness Test**: For 20 test topics, identify additional relevant papers through independent literature review and measure the percentage already captured in ground-truth bibliographies to quantify coverage completeness

3. **Cross-Domain Generalization**: Apply the evaluation framework to a small set of surveys from a different domain (e.g., biomedical) to assess whether the same retrieval-to-synthesis bottleneck pattern holds outside computer science