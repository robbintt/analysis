---
ver: rpa2
title: 'Chandomitra: Towards Generating Structured Sanskrit Poetry from Natural Language
  Inputs'
arxiv_id: '2506.00815'
source_url: https://arxiv.org/abs/2506.00815
tags:
- sanskrit
- poetry
- decoding
- constrained
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Chandomitra, a dataset for generating structured
  Sanskrit poetry from English inputs in the Anushtubh meter. It benchmarks various
  models using constrained decoding and instruction fine-tuning to ensure metrical
  correctness.
---

# Chandomitra: Towards Generating Structured Sanskrit Poetry from Natural Language Inputs

## Quick Facts
- **arXiv ID:** 2506.00815
- **Source URL:** https://arxiv.org/abs/2506.00815
- **Reference count:** 40
- **One-line primary result:** 99.86% syntactic accuracy via constrained decoding; 52.5% fully valid verses via instruction fine-tuning

## Executive Summary
This paper introduces Chandomitra, a dataset for generating structured Sanskrit poetry from English inputs in the Anushtubh meter. It benchmarks various models using constrained decoding and instruction fine-tuning to ensure metrical correctness. The constrained decoding approach achieves 99.86% syntactic accuracy, while the best instruction-tuned model (Mistral-Nemo-2407-12B) achieves 52.5% fully valid verses with better semantic coherence (68.05). Human evaluation confirms that instruction-tuned models produce more poetically expressive and semantically aligned verses. The methods generalize to other Sanskrit meters and languages, demonstrating robustness beyond the initial task.

## Method Summary
The paper presents two complementary approaches for generating Sanskrit poetry: constrained decoding and instruction fine-tuning. Constrained decoding uses regex-based syllable filtering with top-k selection to enforce Anushtubh meter rules at generation time, achieving near-perfect syntactic compliance. Instruction fine-tuning trains decoder-only models via LoRA on parallel Sanskrit-English pairs with prompts specifying metrical constraints, enabling soft adherence to structure while maintaining semantic meaning. Both methods are evaluated using skrutable for metrical validation and fine-tuned embeddings for semantic similarity measurement.

## Key Results
- Constrained decoding achieves 99.86% syntactic accuracy for Anushtubh meter
- Instruction-tuned Mistral-Nemo-2407-12B achieves 52.5% fully valid verses with semantic coherence of 68.05
- Human evaluation scores instruction-tuned models higher on poeticness (3.85 vs 2.225) and semantic alignment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constrained decoding with regex-based syllable filtering can achieve near-perfect metrical compliance without modifying the base model.
- **Mechanism:** At each generation step, the algorithm samples top-k tokens, computes syllable weights (laghu/guru), and masks invalid candidates via pre-compiled regex filters (e.g., "^.{4}lgg.$" for Anushtubh pada rules). Only tokens satisfying the metrical pattern retain non-zero probability.
- **Core assumption:** The model's vocabulary contains sufficient Sanskrit tokens to express semantic content while meeting syllable constraints; greedy selection from constrained candidates preserves meaning.
- **Evidence anchors:**
  - [abstract]: "constrained decoding methodology achieves 99.86% syntactic accuracy"
  - [section] Algorithm 1 and Section 4.1 detail the regex filtering and top-k selection process
  - [corpus] MetricalARGS paper corroborates LLMs struggle with metrical constraints without specialized intervention
- **Break condition:** When semantic content requires vocabulary outside constrained token space, or when BPE tokenization fragments valid Sanskrit tokens into sub-token sequences that cannot be verified atomically (Section 10 limitations).

### Mechanism 2
- **Claim:** Instruction fine-tuning internalizes metrical patterns through exposure to constraint-aligned examples, enabling soft adherence without decoding-time intervention.
- **Mechanism:** Decoder-only models (LLaMA, Mistral, Phi-4) receive prompts specifying Anushtubh rules and are fine-tuned via LoRA on English-Sanskrit pairs. The model learns a distribution over metrically valid outputs through gradient updates.
- **Core assumption:** Instruction-tuned models can transfer learned constraint-following to novel inputs; natural language specification of prosodic rules is learnable from examples.
- **Evidence anchors:**
  - [abstract]: "instruction fine-tuned model is better able to capture the poetic aspects"
  - [section] Table 5 shows Phi-4-14B achieves 57.42% Full Anushtubh with 67.29 semantic similarity; Table 8 human evaluation confirms superior poeticness (3.85 vs 2.225)
  - [corpus] Related Sanskrit poetry-to-prose work (Krishna et al. 2019, cited in paper) demonstrates seq2seq models can learn reordering under constraints
- **Break condition:** When prompt engineering fails to convey precise syllable-count constraints, or when model capacity is insufficient for both semantic translation and structural adherence (Airavata-7B, Navarasa-9B scored 0.0% Full Anushtubh).

### Mechanism 3
- **Claim:** Cross-lingual sentence embeddings fine-tuned on parallel Sanskrit-English data provide a viable proxy for semantic fidelity in poetry generation.
- **Mechanism:** BAAI/bge-m3 is fine-tuned on mitrasam.graha dataset using multiple negatives ranking loss. Linearized cosine similarity between English input and Sanskrit output embeddings correlates with human judgments (Pearson 0.66).
- **Core assumption:** Embedding space alignment captures semantic equivalence across prose-to-poetry transformation; meaning preservation is measurable without word-order correspondence.
- **Evidence anchors:**
  - [abstract]: "semantic coherence with the English input" measured via embedding similarity
  - [section] Table 4 shows metric A (linearized cosine) has highest correlation; Table 3 shows bge-m3 achieves 92.88 translation accuracy
  - [corpus] Weak direct evidence for Sanskrit-specific embeddings; mitrasam.graha dataset cited but not in corpus neighbors
- **Break condition:** When poetic transformations involve semantic shifts (metaphor, rasa) not captured in literal embeddings, or when out-of-domain vocabulary causes embedding drift.

## Foundational Learning

- **Concept: Anushtubh meter constraints**
  - **Why needed here:** All mechanisms depend on understanding the 32-syllable structure: 4 padas × 8 syllables, with position-5 laghu, position-6 guru, position-7 alternating guru (odd padas) / hrasva (even padas).
  - **Quick check question:** Given a Sanskrit string, can you identify if syllable position 6 is guru (heavy)?

- **Concept: Laghu vs. Guru syllable classification**
  - **Why needed here:** Constrained decoding requires real-time syllable-weight computation. Laghu = short vowel + no following consonant cluster; Guru = long vowel, cluster, or anusvara/visarga.
  - **Quick check question:** Is "ma" laghu or guru? What about "mā"?

- **Concept: Top-k constrained decoding with LogitsProcessor**
  - **Why needed here:** Implementation requires hooking into model's logits output, masking invalid tokens, and managing cache for efficiency.
  - **Quick check question:** If top-k=25 but only 3 tokens satisfy regex, which token is selected under greedy sampling?

## Architecture Onboarding

- **Component map:** English prose -> tokenization -> prompt template (IFT) or direct encoding (NLLB) -> Base LLM -> LogitsProcessor hook (CD only) -> syllable weight computation -> regex filter matching -> output generation

- **Critical path:**
  1. Fine-tune NLLB on Chandomitra training split (8,306 pairs)
  2. Implement constrained decoding with regex filters for Anushtubh (4 separate filters per pada)
  3. Integrate skrutable syllabification with LRU cache (Table 9: reduces latency from 0.280s to 0.219s per sample)
  4. Validate against Full Anushtubh metric (>99% target)

- **Design tradeoffs:**
  - **CD vs. IFT:** CD guarantees syntax (99.86%) but sacrifices semantic score (64.91) and poeticness (2.225). IFT balances both (57.42% syntax, 67.29 semantic, 3.85 poeticness). Choice depends on application: liturgical accuracy vs. literary quality.
  - **k value:** k=25 optimal (Table 13); lower k risks no valid candidates, higher k increases latency without semantic gain
  - **Sampling:** Greedy outperforms nucleus/multinomial (Table 14) under constraints

- **Failure signatures:**
  - **Grammar discontinuities:** CD produces metrical but grammatically incoherent verses (human syntactic coherence 0.65 vs IFT 0.725)
  - **BPE fragmentation:** SentencePiece tokenization splits Sanskrit tokens, preventing single-step verification
  - **English leakage:** IFT models may generate English text before Sanskrit, incompatible with CD constraints

- **First 3 experiments:**
  1. **Reproduce CD baseline:** Fine-tune NLLB-dist-1.3B on Chandomitra, implement Algorithm 2, target 99%+ Full Anushtubh on test split (1,421 samples)
  2. **Ablate k and sampling:** Test k ∈ {10, 25, 50, 100} with greedy vs. nucleus sampling, measure syntax/semantic tradeoff and latency
  3. **Cross-meter generalization:** Apply CD framework to Trishtubh (11-syllable padas) using Bhagavadgita samples, verify if 96.67% compliance (Table 10) replicates

## Open Questions the Paper Calls Out
None

## Limitations
- Constrained decoding introduces significant latency overhead (5x slower) and may fail when BPE tokenization fragments Sanskrit tokens across sub-tokens
- Cross-lingual semantic evaluation using fine-tuned embeddings lacks direct validation on Sanskrit-specific poetic transformations
- Instruction fine-tuning achieves only 52.5% fully valid verses, indicating fundamental challenges in simultaneous semantic and metrical compliance

## Confidence
**High Confidence (80-100%):**
- Constrained decoding achieves 99.86% syntactic accuracy for Anushtubh meter
- Instruction fine tuned models produce more poetically expressive and semantically aligned verses than constrained decoding
- The methods generalize to other Sanskrit meters and languages

**Medium Confidence (50-80%):**
- Cross-lingual sentence embeddings provide a viable proxy for semantic fidelity in poetry generation
- k=25 is optimal for balancing syntax, semantics, and latency

**Low Confidence (0-50%):**
- The constrained decoding approach is "guaranteed to output syntactically valid verses"
- Semantic similarity measured by fine-tuned embeddings correlates well with human judgments

## Next Checks
1. **BPE Tokenization Impact Analysis:** Conduct controlled experiments to quantify how often Sanskrit tokens are split across BPE sub-tokens and measure the resulting degradation in constrained decoding performance. Implement token-level syllabification that can handle multi-token sequences and compare results against the current single-token approach.

2. **Cross-Lingual Embedding Validation:** Create a gold-standard human-annotated dataset of English-Sanskrit poetry pairs with explicit semantic alignment labels. Use this to directly evaluate the bge-m3 embedding's ability to capture meaning preservation across prose-to-poetry transformations, particularly for metaphorical and rasa-driven content.

3. **Scaling Study Across Meters and Languages:** Systematically test the constrained decoding and instruction fine-tuning approaches across a diverse set of Sanskrit meters (Vasantatilaka, Sragdhara) and other poetic traditions (Tamil Venpa, Japanese Haiku). Measure generalization capability and identify architectural limitations that emerge with increased metrical complexity.