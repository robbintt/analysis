---
ver: rpa2
title: 'Uncovering Intrinsic Capabilities: A Paradigm for Data Curation in Vision-Language
  Models'
arxiv_id: '2510.00040'
source_url: https://arxiv.org/abs/2510.00040
tags:
- data
- capabilities
- training
- recognition
- cadc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Capability-Attributed Data Curation (CADC),
  a novel paradigm for instruction data curation in vision-language models. Instead
  of relying on heuristic task-based selection, CADC discovers intrinsic capabilities
  directly from gradient-based learning dynamics, attributes training data to these
  capabilities via influence estimation, and curates balanced subsets through capability-aware
  allocation and curriculum sequencing.
---

# Uncovering Intrinsic Capabilities: A Paradigm for Data Curation in Vision-Language Models

## Quick Facts
- **arXiv ID:** 2510.00040
- **Source URL:** https://arxiv.org/abs/2510.00040
- **Reference count:** 40
- **Primary result:** With as little as 5% of the original data, CADC surpasses full-data training across diverse multimodal benchmarks.

## Executive Summary
This paper introduces Capability-Attributed Data Curation (CADC), a novel paradigm for instruction data curation in vision-language models. Instead of relying on heuristic task-based selection, CADC discovers intrinsic capabilities directly from gradient-based learning dynamics, attributes training data to these capabilities via influence estimation, and curates balanced subsets through capability-aware allocation and curriculum sequencing. With as little as 5% of the original data, CADC surpasses full-data training across diverse multimodal benchmarks, including LLaVA-Wild, POPE, and MMT-Bench. The results validate intrinsic capabilities as the fundamental building blocks of model learning and establish CADC as a highly efficient and interpretable framework for instruction tuning.

## Method Summary
CADC operates in three phases: (1) Capability discovery, where AdamW update trajectories are extracted over M snapshots from a proxy training run, aggregated by subtask, and clustered via Leiden detection to identify K intrinsic capabilities. (2) Capability attribution, where each training sample is assigned to the capability it most influences using trajectory influence with tolerance δ. (3) Curriculum curation, where self-influence per capability guides budget allocation, followed by staged sequencing that trains capabilities in order of rising/falling influence trends. The method uses LoRA random projections to reduce trajectory dimensionality and MMT-Bench validation data to proxy capability discovery.

## Key Results
- Achieves superior performance to full-data training using only 5% of original instruction data
- Outperforms baseline methods on LLaVA-Wild, POPE, and MMT-Bench benchmarks
- Demonstrates that intrinsic capabilities serve as fundamental building blocks for VLM learning

## Why This Works (Mechanism)
The core insight is that intrinsic capabilities emerge from the model's learning dynamics and can be discovered by analyzing how different subtasks influence parameter updates. By attributing training data to these discovered capabilities and curating balanced subsets through capability-aware allocation and curriculum sequencing, CADC ensures each capability receives adequate representation while eliminating redundant or less influential samples.

## Foundational Learning
- **AdamW trajectory analysis:** Understanding how parameter updates reflect learning patterns - needed to extract capability signals from training dynamics; quick check: verify trajectory extraction code produces expected update vectors.
- **Leiden graph clustering:** Community detection in similarity graphs to identify capability clusters - needed to group subtasks by learning behavior; quick check: confirm K=3 clusters are stable across runs.
- **Trajectory influence estimation:** Measuring how training samples affect capability development - needed to attribute data to capabilities; quick check: validate influence scores correlate with learning impact.

## Architecture Onboarding
- **Component map:** MMT-Bench validation data -> Capability discovery (AdamW trajectories + Leiden clustering) -> Training data attribution (influence estimation) -> Curriculum curation (budget allocation + sequencing) -> VLM training
- **Critical path:** Capability discovery → Attribution → Curation → Training
- **Design tradeoffs:** Balanced representation vs. efficiency (5% data savings), proxy validation vs. training data representativeness, staged sequencing vs. simplicity
- **Failure signatures:** Poor capability clustering (too many/few clusters), ineffective curriculum sequencing (no performance gain over random), unstable influence attribution
- **First experiments:** 1) Verify capability discovery produces K=3 clusters with clear subtask groupings, 2) Confirm influence attribution assigns training samples to expected capabilities, 3) Test staged curriculum sequencing improves convergence vs. random order

## Open Questions the Paper Calls Out
None

## Limitations
- Critical hyperparameters (LoRA rank m, snapshot timing, replay fraction) are not specified, limiting reproducibility
- No comparative analysis with alternative curation methods (e.g., diversity-based approaches)
- Sensitivity to Leiden clustering resolution parameter is not explored

## Confidence
- **High confidence:** Capability discovery and attribution methodology (Leiden clustering, trajectory influence) is clearly articulated and reproducible given model snapshots.
- **Medium confidence:** The 5% data claim is supported, but exact replay fraction and sequencing rules are unclear.
- **Low confidence:** Comparative ablations with other curation methods are not provided, limiting interpretation of CADC's relative advantage.

## Next Checks
1. **Sensitivity analysis of LoRA projection rank m and Leiden resolution:** Vary m ∈ {16, 32, 64} and resolution ∈ {0.6, 0.8, 1.0} to confirm capability stability and performance consistency.
2. **Replay fraction impact study:** Train with staged curriculum using replay fractions ∈ {0.2, 0.5, 0.8} to verify whether the 5% savings are robust or inflated by implicit full-data replay.
3. **Baseline comparison:** Implement and compare against a task-balancing or entropy-diversity data curation method to quantify CADC's unique contribution beyond balanced sampling.