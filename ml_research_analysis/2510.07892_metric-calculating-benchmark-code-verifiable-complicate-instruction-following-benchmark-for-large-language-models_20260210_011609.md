---
ver: rpa2
title: 'Metric Calculating Benchmark: Code-Verifiable Complicate Instruction Following
  Benchmark for Large Language Models'
arxiv_id: '2510.07892'
source_url: https://arxiv.org/abs/2510.07892
tags:
- llms
- code
- benchmark
- evaluation
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MCBench, a code-verifiable benchmark for\
  \ evaluating large language models\u2019 (LLMs) ability to follow complex, multi-step\
  \ instructions and perform numerical reasoning. Unlike prior benchmarks that rely\
  \ on subjective judgments, MCBench provides deterministic, objective evaluation\
  \ by requiring models to compute string-matching NLP metrics through detailed step-by-step\
  \ rubrics paired with parallel reference code."
---

# Metric Calculating Benchmark: Code-Verifiable Complicate Instruction Following Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2510.07892
- Source URL: https://arxiv.org/abs/2510.07892
- Reference count: 16
- Major result: MCBench reveals LLMs struggle with complex multi-step instructions despite high overall capabilities, with GPT-4o achieving only 40.98% accuracy

## Executive Summary
This paper introduces MCBench, a novel benchmark designed to evaluate large language models' ability to follow complex, multi-step instructions and perform numerical reasoning. Unlike traditional benchmarks that rely on subjective human judgment, MCBench provides deterministic, objective evaluation by requiring models to compute string-matching NLP metrics through detailed step-by-step rubrics paired with parallel reference code. The benchmark evaluates three key capabilities: complex instruction following, mathematical reasoning, and long-range consistency.

The experimental results demonstrate that even advanced models like GPT-4o achieve only 40.98% final accuracy on MCBench tasks, revealing significant room for improvement in instruction-following capabilities. Interestingly, specialized code and math models do not consistently outperform general models, suggesting that comprehensive instruction-following requires more than just domain expertise. MCBench thus serves as a challenging, objective tool for assessing and advancing LLM instruction-following skills.

## Method Summary
MCBench is built around metric calculation tasks where models must compute NLP evaluation metrics like BLEU, ROUGE, and BERTScore through detailed step-by-step instructions. The benchmark provides parallel reference code alongside the rubrics, enabling deterministic evaluation by comparing model outputs against code execution results. Each task requires careful adherence to complex instructions, precise numerical reasoning, and maintaining consistency across multiple calculation steps. The evaluation framework tracks intermediate step accuracy as well as final metric computation accuracy, providing granular insights into model capabilities.

## Key Results
- GPT-4o achieves only 40.98% final accuracy on MCBench, despite being among the most capable LLMs
- Specialized code and math models do not consistently outperform general-purpose models on the benchmark
- Step-by-step accuracy tracking reveals that models often fail at specific calculation stages rather than entire tasks
- The benchmark successfully discriminates between different model capabilities while maintaining objective verifiability

## Why This Works (Mechanism)
MCBench works by leveraging the inherent verifiability of mathematical and computational tasks. By requiring models to compute exact numerical metrics through specified procedures, the benchmark eliminates the ambiguity that plagues many traditional LLM evaluations. The parallel code implementation serves as ground truth, making correctness binary rather than subjective. This approach forces models to demonstrate genuine understanding of complex instructions rather than relying on pattern matching or probabilistic reasoning alone.

## Foundational Learning
- Metric calculation fundamentals (why needed: models must understand mathematical operations for NLP metrics; quick check: can compute BLEU score from scratch)
- String processing and manipulation (why needed: metric calculations involve extensive text handling; quick check: can tokenize and process strings according to specifications)
- Step-by-step instruction following (why needed: tasks require precise adherence to multi-stage procedures; quick check: can execute a 10-step mathematical process without deviation)
- Numerical reasoning (why needed: metric calculations involve mathematical operations and precision; quick check: can perform accurate floating-point calculations)

## Architecture Onboarding
Component map: Rubric -> Model Response -> Code Verification -> Accuracy Scoring
Critical path: Instruction parsing → Intermediate step computation → Final metric calculation → Code-based validation
Design tradeoffs: Deterministic evaluation vs. task specificity; comprehensive instruction coverage vs. benchmark complexity; objective scoring vs. real-world applicability
Failure signatures: Mathematical errors in intermediate steps; instruction misinterpretation at specific stages; precision loss in numerical calculations; failure to maintain consistency across calculation steps
First experiments:
1. Evaluate a simple metric calculation task to verify basic instruction following
2. Test intermediate step accuracy tracking on a complex multi-stage calculation
3. Compare model performance on metric tasks vs. general instruction-following benchmarks

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on metric calculation tasks may limit generalizability to broader instruction-following capabilities
- Deterministic evaluation framework may not capture important aspects like adaptability and handling ambiguous instructions
- Unclear whether performance translates to real-world instruction-following scenarios requiring diverse reasoning types

## Confidence
**High Confidence**: MCBench provides deterministic, objective evaluation for metric calculation tasks, supported by experimental results showing GPT-4o achieving 40.98% accuracy.

**Medium Confidence**: MCBench effectively evaluates complex instruction following, mathematical reasoning, and long-range consistency, though generalizability beyond metric tasks remains uncertain.

**Low Confidence**: Specialized models not outperforming general models indicates need for comprehensive capabilities is speculative without deeper analysis of underlying reasons.

## Next Checks
1. Test correlation between MCBench performance and real-world instruction-following tasks across diverse practical benchmarks
2. Analyze model failure patterns to determine whether errors stem from instruction-following, mathematical reasoning, or consistency issues
3. Evaluate whether fine-tuning on MCBench tasks improves performance on other instruction-following benchmarks to assess training signal effectiveness