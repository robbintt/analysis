---
ver: rpa2
title: Do Sparse Autoencoders Identify Reasoning Features in Language Models?
arxiv_id: '2601.05679'
source_url: https://arxiv.org/abs/2601.05679
tags:
- reasoning
- features
- feature
- need
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether sparse autoencoders (SAEs) can
  identify genuine reasoning features in large language models (LLMs). The authors
  first present a theoretical analysis showing that sparsity-regularized decoding
  favors stable low-dimensional correlates over high-dimensional within-reasoning
  variation, biasing learned features toward token-level cues.
---

# Do Sparse Autoencoders Identify Reasoning Features in Language Models?
## Quick Facts
- arXiv ID: 2601.05679
- Source URL: https://arxiv.org/abs/2601.05679
- Reference count: 40
- This paper shows that sparse autoencoders systematically learn token-level linguistic correlates rather than genuine reasoning features in LLMs, with 45%-90% of contrastively selected features activated by simple token injection.

## Executive Summary
This paper investigates whether sparse autoencoders (SAEs) can identify genuine reasoning features in large language models (LLMs). The authors present a theoretical analysis showing that sparsity-regularized decoding biases SAEs toward stable low-dimensional correlates (e.g., token-level patterns) over high-dimensional reasoning processes. Based on this, they develop a falsification framework combining causal token injection with LLM-guided counterexample generation to test whether contrastively selected features truly encode reasoning. Across 22 configurations spanning multiple models and datasets, they find that SAEs predominantly learn superficial linguistic patterns that co-occur with reasoning rather than the reasoning itself, with steering experiments showing no benchmark improvements.

## Method Summary
The authors first contrastively select reasoning features by computing per-feature maximum activations on reasoning (s1K-1.1, General Inquiry Thinking CoT) and non-reasoning (Pile Uncopyrighted) corpora, then ranking by Cohen's d. They then apply a falsification framework: (1) causal token injection tests where top-activating tokens are prepended, interspersed, or substituted into non-reasoning text to measure activation changes; (2) LLM-guided falsification using Gemini 3 Pro to generate hypotheses about token-level patterns and construct non-reasoning inputs that activate the feature (false positives) and paraphrased reasoning inputs that suppress it (false negatives). Features are classified by injection effect size and falsifiability. Finally, they test feature steering by activating top features during inference on benchmarks.

## Key Results
- 45%-90% of top-ranked reasoning features are highly sensitive to token injection, activating when only a few associated tokens are added to non-reasoning text
- LLM-guided falsification successfully constructs non-reasoning inputs that trigger feature activation for context-dependent features, and paraphrases that suppress it
- Steering the highest-ranked features yields no improvements on benchmarks
- Across 22 configurations, sparsity bias systematically favors low-dimensional token-level patterns that consistently co-occur with reasoning

## Why This Works (Mechanism)
The sparsity penalty in SAE decoding creates a bias toward features that are both sparse (few features active) and stable (consistent across samples). When low-dimensional token-level patterns co-occur with high-dimensional reasoning processes, the sparsity constraint favors learning the token patterns because they are more stable and require fewer active features. This creates a systematic bias where SAEs learn superficial correlates rather than the underlying reasoning mechanisms.

## Foundational Learning
- **Cohen's d effect size**: Measures separation between reasoning and non-reasoning feature activations; needed to rank features by reasoning specificity; quick check: compute d for a known reasoning vs non-reasoning pair
- **Sparse autoencoder architecture**: Encoder compresses activations, decoder reconstructs with L1 regularization; needed to understand sparsity bias; quick check: visualize decoder weights for top features
- **Contrastive feature selection**: Selects features with highest reasoning vs non-reasoning activation separation; needed to identify candidate reasoning features; quick check: plot activation distributions for top/bottom features
- **Causal token injection**: Tests whether feature activation is driven by specific tokens vs context; needed to falsify token-level correlates; quick check: inject tokens into multiple contexts and measure consistency
- **LLM-guided falsification**: Uses AI to generate counterexamples and validate feature specificity; needed to systematically test feature robustness; quick check: manually verify generated counterexamples activate the feature

## Architecture Onboarding
**Component map**: Input text -> SAE encoder -> Feature activations -> Contrastive selection -> Token injection tests -> LLM-guided falsification -> Feature classification
**Critical path**: Contrastive selection → Token injection → LLM falsification → Steering evaluation
**Design tradeoffs**: Sparsity bias favors stable low-dimensional patterns vs high-dimensional reasoning; token injection is interpretable but may miss complex patterns; LLM falsification is powerful but depends on frontier model capabilities
**Failure signatures**: High Cohen's d but low injection effect suggests reasoning vs discourse marker differences; LLM falsification failure may indicate genuine features or prompt inadequacy
**First experiments**: 1) Run contrastive selection on a simple synthetic dataset with known token vs reasoning features; 2) Apply token injection to features with varying d values; 3) Test LLM falsification on features classified as token-driven

## Open Questions the Paper Calls Out
None

## Limitations
- Results limited to two model families and narrowly defined reasoning corpora (math CoT, general inquiry)
- LLM-guided falsification depends on capabilities and prompt templates of a single frontier model (Gemini 3 Pro)
- Classification thresholds for token-driven features and falsification validity are somewhat arbitrary
- Does not address adversarial robustness of features or impact of SAE training hyperparameters

## Confidence
- Theoretical sparsity bias analysis: Medium
- Empirical falsification results across 22 configs: Medium
- Steering experiments (no benchmark gains): Medium
- Generalizability beyond tested models/corpora: Low

## Next Checks
1. Replicate the SAE training and feature selection pipeline on a held-out model family (e.g., GPT-4o-mini or Claude Haiku) using the same contrastive and falsification framework
2. Conduct ablation studies varying L1 penalty and feature dimensionality to test sensitivity of sparsity bias to architectural choices
3. Apply the falsification framework to a non-mathematical reasoning corpus (e.g., legal reasoning or scientific inference) to assess domain robustness