---
ver: rpa2
title: Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician
  Oversight
arxiv_id: '2512.19691'
source_url: https://arxiv.org/abs/2512.19691
tags:
- labels
- patient
- label
- original
- medcalc-bench
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors systematically audited a widely-used clinical benchmark
  whose labels were partially generated by LLMs. Using a phased, physician-in-the-loop
  protocol, they found that 26.6% of test labels were clinically suspect due to extraction
  errors, incorrect aggregation logic, or unanswerable note-question pairs.
---

# Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight

## Quick Facts
- arXiv ID: 2512.19691
- Source URL: https://arxiv.org/abs/2512.19691
- Authors: Junze Ye; Daniel Tawfik; Alex J. Goodell; Nikhil V. Kotha; Mark K. Buyyounouski; Mohsen Bayati
- Reference count: 40
- Primary result: Controlled RL training on corrected labels yielded an 8.7% absolute improvement in accuracy over the original baseline

## Executive Summary
This paper systematically audited a widely-used clinical benchmark (MedCalc-Bench v1.0) whose labels were partially generated by LLMs. Using a phased, physician-in-the-loop protocol, they found that 26.6% of test labels were clinically suspect due to extraction errors, incorrect aggregation logic, or unanswerable note-question pairs. Independent recomputation flagged 27.3% as likely mislabeled, and physician review agreed in 7/7 sampled cases. Controlled RL training on corrected labels yielded an 8.7% absolute improvement in accuracy over the original baseline, demonstrating that label noise materially affects both evaluation and model alignment. The work underscores the need for ongoing, hybrid oversight to keep clinical benchmarks clinically valid and safe for downstream use.

## Method Summary
The authors employed a three-phase pipeline to audit and correct MedCalc-Bench v1.0: (1) a Gemini 2.5 Pro audit agent with web search checks derivation metadata and flags suspicious instances via 4/5 supermajority voting; (2) a blind Gemini 2.5 Pro relabeling agent with web search and Python sandbox recomputes labels, routing high-disagreement cases (rel.err > 0.05) to (3) physician review. The corrected labels were then used in a controlled RL experiment where Qwen3-8B trained via GRPO achieved 71.4% test accuracy on the corrected labels versus 62.6% on the original, demonstrating the impact of label quality on model performance.

## Key Results
- 26.6% of test labels were clinically suspect (extraction errors, wrong aggregation, or unanswerable)
- Independent recomputation flagged 27.3% as likely mislabeled; physician review agreed in 7/7 sampled cases
- Controlled RL training on corrected labels yielded an 8.7% absolute improvement in accuracy over original baseline
- Automated triage via rel.err calculation maximizes information gain of scarce physician oversight

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Label noise in benchmarks acts as a biased teacher during Reinforcement Learning (RL), and correcting this noise materially improves policy alignment.
- **Mechanism:** When an LLM is fine-tuned via Group Relative Policy Optimization (GRPO), the benchmark labels serve as the reward signal's ground truth. If the label is incorrect (e.g., hallucinated features or wrong aggregation logic), the model is penalized for generating a clinically correct answer or rewarded for a wrong one. Correcting labels aligns the reward function $R(\tau)$ with the latent physician-aligned ground truth $y^*$, allowing the policy gradient to optimize for valid clinical reasoning rather than artifact fitting.
- **Core assumption:** The controlled RL experiment assumes that the difference in test accuracy is attributable solely to the change in reward labels (training signal), holding model architecture and hyperparameters constant.
- **Evidence anchors:**
  - [abstract] "Controlled RL training on corrected labels yielded an 8.7% absolute improvement in accuracy over the original baseline."
  - [section 3.4] "Averaging the final ten evaluation steps, the copy of Qwen3-8B trained on our recomputed labels ($\hat{y}^{new}$) achieves a final test accuracy of 71.4%, whereas the policy trained on the original benchmark labels ($\hat{y}^{original}$) plateaus at 62.6%."
  - [corpus] Corpus signals generally support the need for validity but do not contain experimental evidence of this specific reward-signal mechanism.
- **Break condition:** If the Phase 2 recomputed labels ($\hat{y}^{new}$) introduce a systematic bias of their own (e.g., consistent over-correction), the "improved" accuracy might reflect overfitting to a new set of artifacts rather than genuine alignment, though Phase 3 physician adjudication mitigates this.

### Mechanism 2
- **Claim:** Automated triage via independent agent recomputation maximizes the information gain of scarce physician oversight.
- **Mechanism:** An independent "relabeling agent" (blind to the original label) produces a candidate label $\hat{y}^{new}$. By measuring the relative error ($rel.err$) between the original $\hat{y}^{original}$ and $\hat{y}^{new}$, the system identifies high-contention instances. Physicians are then routed only to these instances. This acts as an uncertainty sampler, filtering out the ~73% of cases where labels agree (or disagree negligibly) to focus expert cognitive load where it changes the outcome most.
- **Core assumption:** High disagreement between two independent estimators (original pipeline vs. new agent) correlates with a higher probability of true error relative to the latent ground truth.
- **Evidence anchors:**
  - [abstract] "Independent recomputation flagged 27.3% as likely mislabeled... automated triage to reserve scarce clinician attention for the most contentious instances."
  - [section 3.3] "Across 50 sampled instances with high rel.err... $\hat{y}^{new}$ agrees with $y^*$ 37 times, compared to $\hat{y}^{original}$'s 10 times."
  - [corpus] "Tiered Agentic Oversight" (104340) describes a hierarchical oversight mechanism conceptually similar to this triage, though not the specific rel.err calculation.
- **Break condition:** If the original label and the recomputed label are both wrong in the same way (correlated error), the triage mechanism fails to flag the instance, and physician review is erroneously skipped.

### Mechanism 3
- **Claim:** Explicitly modeling "unanswerable" cases via abstention (N/A) prevents reward hacking and improves safety.
- **Mechanism:** The original benchmark forced a numeric calculation even when clinical context $C$ was missing necessary inputs or mismatched to the question $q$. By introducing "N/A" as a valid target label, the system creates a reward path for refusal. This prevents the model from learning to hallucinate missing data to satisfy the numeric reward requirement.
- **Core assumption:** There exists a non-trivial subset of clinical notes where the correct medical action is "do not calculate" rather than "calculate with inference."
- **Evidence anchors:**
  - [abstract] "unanswerable note-question pairs... controlled RL training... yielded an 8.7% absolute improvement."
  - [section 3.1] "In the case of (c), it would be more reasonable to reward the LLM for abstaining... instead of hallucinating an answer."
  - [corpus] "Medical Large Language Model Benchmarks Should Prioritize Construct Validity" (21783) argues for better task definition, implicitly supporting abstention handling.
- **Break condition:** If the definition of "unanswerable" is too broad, the model may learn a lazy policy of defaulting to N/A to avoid complex reasoning.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** The paper uses GRPO to demonstrate the causal impact of label quality. Understanding that GRPO updates model weights based on the relative advantage of generated samples against a group baseline is key to seeing why the *reward definition* (the label) is the critical variable.
  - **Quick check question:** In GRPO, if the group average reward is 0.6 and a specific trajectory gets a reward of 0.8, does the policy increase or decrease the probability of that trajectory?

- **Concept: Reference Labels as Estimators ($\hat{y}$)**
  - **Why needed here:** The paper treats benchmark labels not as perfect oracles ($y^*$) but as noisy estimates ($\hat{y}$). Shifting this mental model from "Ground Truth" to "Estimator of Ground Truth" is essential for understanding the stewardship protocol.
  - **Quick check question:** If $\hat{y}^{original}$ is derived from GPT-4 and $\hat{y}^{new}$ from Gemini 2.5 Pro, are we measuring the absolute truth or the consensus of two model-based estimators?

- **Concept: Symmetric Mean Absolute Percentage Error (sMAPE)**
  - **Why needed here:** The paper evaluates label quality using sMAPE rather than simple accuracy to capture the magnitude of numerical deviation.
  - **Quick check question:** Why is sMAPE preferred over MAPE when comparing labels that might include near-zero values (though clinical scores often start at 0, sMAPE handles symmetry better for over/underestimation)?

## Architecture Onboarding

- **Component map:** MedCalc-Bench v1.0 (Context $C$, Question $q$, Original Label $\hat{y}^{original}$) -> Phase 1 (Audit Agent) -> Phase 2 (Relabel Agent) -> Triage Engine -> Phase 3 (Human-in-the-Loop) -> RL Trainer (Qwen3-8B + GRPO)
- **Critical path:** The integrity of the system relies on the **independence** of Phase 2. If the relabeling agent sees the original label (anchoring bias), the triage signal is corrupted. Ensure Phase 2 prompts (Fig 6) strictly omit $\hat{y}^{original}$.
- **Design tradeoffs:**
  - **Precision vs. Recall in Triage:** The supermajority (4/5) voting in Phase 1 prioritizes precision (flagging only definite errors) to save physician time, potentially missing subtler errors.
  - **Cost vs. Coverage:** Full physician review is impossible at scale. The system trades absolute certainty for statistical estimation, projecting Phase 3 error rates onto the full dataset.
- **Failure signatures:**
  - **High Deferral Rate:** If Phase 2 returns "Deferred" (no supermajority) frequently, the agent may be under-calibrated or the questions fundamentally ambiguous.
  - **sMAPE Spikes:** If sMAPE worsens after relabeling in specific calculator categories, check for logic mismatches in the Python sandbox or outdated web-search guidelines.
- **First 3 experiments:**
  1. **Baseline Reproduction:** Run the Audit Agent (Phase 1) on the test set to verify the 26.6% "Suspicious" rate locally.
  2. **Triage Sensitivity:** Vary the $rel.err$ threshold (currently >0.05) to plot the curve of "Instances Flagged for Review" vs. "Estimated Error Rate" to find the optimal operating point for your physician budget.
  3. **RL Ablation:** Train a small model (e.g., Qwen-1.8B) on the original vs. corrected labels. If the performance gap holds, it confirms the label-noise mechanism without the cost of training a 8B parameter model.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does inter-rater reliability vary across multi-institutional settings for ambiguous clinical labels?
- **Basis in paper:** [explicit] The authors state that "Larger, multi-institution replications would better characterize variability across clinicians and settings, and would enable estimation of inter-rater reliability" (p. 17).
- **Why unresolved:** The current study relied on a targeted subset of 50 instances reviewed by physicians from a single institution.
- **What evidence would resolve it:** A cross-institutional annotation study to calculate inter-rater reliability scores (e.g., Kappa) for the triaged instances.

### Open Question 2
- **Question:** How should the pipeline handle "deferred" instances where agentic auditors fail to reach consensus?
- **Basis in paper:** [explicit] The authors note that "the hardest instances are plausibly those most likely to be deferred" and were excluded from the high-confidence set (p. 17).
- **Why unresolved:** These high-ambiguity cases were excluded, potentially leaving a tail of difficult clinical scenarios unaddressed.
- **What evidence would resolve it:** Qualitative analysis of deferred instances to identify systematic failure modes or data quality issues preventing consensus.

### Open Question 3
- **Question:** What mechanisms allow RL (GRPO) to maintain performance despite significant label noise?
- **Basis in paper:** [inferred] The authors observe that "the RL process displays some resilience to noisy rewards" given the 8.7% performance drop vs. 27% error rate (p. 15), but do not explain why.
- **Why unresolved:** It is unclear if the algorithm ignores noisy gradients or relies on majority correct labels.
- **What evidence would resolve it:** Controlled ablation studies varying noise type and intensity during training to isolate the robustness mechanism.

## Limitations
- External validity of triage threshold: The rel.err > 0.05 cutoff was set empirically for this benchmark and may not generalize to other clinical tasks.
- Model-dependence of label quality: The 8.7% improvement may partly reflect systematic differences between GPT-4 and Gemini 2.5 Pro rather than pure noise correction.
- Generalizability of RL gains: The controlled GRPO experiment demonstrates improved accuracy on the corrected MedCalc-Bench test set, but clinical significance and transfer to other tasks remain unproven.

## Confidence
- **High Confidence:** The finding that 26.6% of test labels are clinically suspect is well-supported by the three-phase audit protocol and physician adjudication on 7 sampled cases.
- **Medium Confidence:** The 27.3% of labels flagged as likely mislabeled by independent recomputation is statistically robust within the MedCalc-Bench context, but the exact percentage may vary with different relabeling agents or thresholds.
- **Medium Confidence:** The 8.7% absolute improvement from RL on corrected labels is demonstrated within this controlled experiment, but the mechanism's robustness to different model families and tasks requires further validation.

## Next Checks
1. **Triage Threshold Sensitivity Analysis:** Vary the rel.err threshold (e.g., 0.02, 0.05, 0.10) and plot the curve of "Instances Flagged for Review" vs. "Estimated Error Rate" to find the optimal operating point for a given physician review budget.
2. **Cross-Model Ablation Study:** Re-run the Phase 2 relabeling using the same model (e.g., GPT-4) to isolate the effect of independent computation from model-specific biases.
3. **Generalization Benchmark:** Apply the same audit protocol to a different clinical reasoning benchmark (e.g., MedQA or PubMedQA) to test if the 25-30% error rate and 8-10% RL improvement are reproducible across domains.