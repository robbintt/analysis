---
ver: rpa2
title: 'Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection'
arxiv_id: '2509.05360'
source_url: https://arxiv.org/abs/2509.05360
tags:
- arxiv
- n-gram
- https
- tensor
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel hallucination detection method using
  N-Gram tensor decomposition. The approach constructs N-Gram frequency tensors from
  LLM-generated text, capturing semantic co-occurrence patterns, then applies tensor
  decomposition to extract singular values as features for an MLP binary classifier.
---

# Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection

## Quick Facts
- arXiv ID: 2509.05360
- Source URL: https://arxiv.org/abs/2509.05360
- Authors: Jerry Li; Evangelos Papalexakis
- Reference count: 35
- Primary result: Novel N-Gram tensor decomposition method significantly outperforms traditional baselines and matches state-of-the-art LLM judges on hallucination detection

## Executive Summary
This paper introduces a novel approach for detecting LLM hallucinations using N-Gram tensor decomposition. The method constructs frequency tensors from grouped LLM-generated text, applies tensor decomposition to extract singular values as features, and trains an MLP classifier to distinguish hallucinated from factual content. Evaluated on the HaluEval dataset across multiple text categories, the approach significantly outperforms traditional baselines like ROUGE and BERTScore, and achieves competitive performance with state-of-the-art LLM judges. The method shows consistent improvement with larger text group sizes, demonstrating its effectiveness and potential for reliable hallucination detection.

## Method Summary
The approach groups M texts by label (hallucinated/factual), builds N-gram frequency tensors per group, applies SVD/Tucker/CP decomposition to extract singular values, and trains an MLP classifier on these features. For each group, a vocabulary is built from unique tokens, an N-gram frequency tensor is constructed, and decomposition extracts the principal components as features. These features are flattened, cropped/padded to fixed length k, and fed into a 4-layer MLP (48→64→32→1) with BCE loss. The method is evaluated on HaluEval dataset subsets using metrics like AUROC, AUPR, F1, and Accuracy.

## Key Results
- Outperforms traditional baselines (ROUGE, BERTScore, Perplexity) by significant margins on HaluEval dataset
- Performance improves consistently with larger text group sizes (G1 → G40), demonstrating density benefits
- Binary and log-frequency matrix variants show comparable performance, indicating robustness to textual bias
- Matches or exceeds state-of-the-art LLM judges while being computationally efficient

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Singular values from N-gram frequency tensors encode latent structural differences between hallucinated and factual text that overlap-based metrics miss.
- Mechanism: Tensor decomposition extracts principal components of N-gram co-occurrence structure, compressing distributional fingerprints that differ systematically between text types.
- Core assumption: Hallucinated text exhibits measurable structural anomalies in N-gram co-occurrence patterns.
- Evidence anchors: Abstract states method "captures richer semantic structure by encoding co-occurrence patterns"; tensor C definition shows frequency counting; limited corpus support exists.
- Break condition: If hallucinated and factual text have near-identical N-gram co-occurrence distributions, singular values provide no discriminative signal.

### Mechanism 2
- Claim: Aggregating multiple texts into a single tensor densifies the N-gram structure, improving signal-to-noise ratio of extracted singular values.
- Mechanism: Larger group sizes populate more tensor entries, reducing sparsity and yielding singular values that better represent global structural patterns.
- Core assumption: Discriminative signal is a population-level property emerging across multiple samples.
- Evidence anchors: Section 3.3 shows "results consistently improve as group size increases"; Figure 2 demonstrates monotonic AUROC improvement from G1 to G40.
- Break condition: If hallucinations are highly inconsistent in structure across samples, aggregation could dilute rather than strengthen the signal.

### Mechanism 3
- Claim: Frequency-based N-gram representations are robust to dataset-specific textual biases (e.g., length differences).
- Mechanism: Classifier learns from singular values capturing relational structure rather than absolute counts; binary and log-frequency variants perform comparably.
- Core assumption: Learned patterns generalize across domains and are not artifacts of dataset-specific N-gram distributions.
- Evidence anchors: Section 3.3.3 shows binary and log-frequency matrices have "comparable or even improved performance"; Table 1 shows length differences yet method performs well.
- Break condition: If new domain has radically different N-gram distributions, classifier may require retraining or fail to generalize.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: Core feature extraction method; transforms N-gram matrices into singular values serving as classifier inputs.
  - Quick check question: Can you explain why top-k singular values capture most important structural information in a matrix?

- Concept: N-gram Language Models
  - Why needed here: Understanding how N-gram frequencies encode local token dependencies is essential for interpreting what tensor represents.
  - Quick check question: What is the difference between unigram and bigram, and why might bigrams capture more semantic information?

- Concept: Tensor Decomposition (Tucker/CP)
  - Why needed here: Extends SVD to higher-order tensors; understanding trade-offs between Tucker and CP informs architecture choices.
  - Quick check question: How does Tucker decomposition differ from CP decomposition in terms of output structure?

## Architecture Onboarding

- Component map: Text Grouping Module → Vocabulary Builder → N-gram Tensor Constructor → Decomposition Engine → Feature Vectorizer → MLP Classifier

- Critical path: Group size selection → Tensor density → Decomposition quality → Classifier performance. Larger groups improve performance but increase memory (Table 3 shows empty entries for G40 due to memory constraints).

- Design tradeoffs:
  - Group size: Larger = better accuracy but higher memory overhead
  - N-gram order: Higher = more context but exponentially larger tensors
  - Feature size k: Larger = more information but risk of overfitting
  - Decomposition method: SVD (simpler, 2-gram only) vs. Tucker/CP (higher-order, more complex)

- Failure signatures:
  - Near-random AUROC (~0.5) at G1 indicates tensor too sparse
  - Large gap between train and eval performance suggests overfitting to dataset-specific N-gram patterns
  - Memory errors at large group sizes indicate need for sparse tensor representations

- First 3 experiments:
  1. Baseline sanity check: Run SVD-G1 on HaluEval subsets to confirm sparsity problem; expect AUROC near Perplexity baseline (~0.50-0.54).
  2. Scaling test: Compare SVD-G5 vs SVD-G20 on Summary subset; expect ~0.25 AUROC improvement if density hypothesis holds.
  3. Robustness probe: Train on General subset, evaluate on Dialogue; compare frequency vs. binary matrix performance to test bias sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific latent semantic structures do N-Gram tensor singular values represent, and how do they distinguish hallucinated content from factual text?
- Basis in paper: Conclusion states "Future work should focus on improving the interpretability of our N-Gram Matrices and Tensors to better understand how they capture differences."
- Why unresolved: Current work treats singular values as abstract features without mapping them back to specific linguistic patterns or N-gram co-occurrences.
- What evidence would resolve it: Qualitative analysis reconstructing principal components to identify which N-grams or semantic concepts contribute most to "hallucinated" classification.

### Open Question 2
- Question: How does structural nature of hallucinations vary across different text categories (e.g., Dialogue vs. Summary), and does this affect transferability of classifier?
- Basis in paper: Authors suggest "investigating how hallucinations differ across text categories could offer valuable insights into the nature of LLM hallucinations."
- Why unresolved: Paper reports performance metrics for different subsets but does not analyze structural differences in N-Gram tensors causing varying performance levels.
- What evidence would resolve it: Comparative study of tensor density and singular value distributions across HaluEval subsets to identify category-specific structural signatures.

### Open Question 3
- Question: Can method maintain high detection accuracy when applied to single-instance inference rather than relying on batched text groups?
- Basis in paper: Paper mandates grouping texts to construct frequency tensor; results show significantly lower performance for group size 1 (e.g., 0.539 Accuracy on General) compared to group size 40 (0.850).
- Why unresolved: Reliance on aggregating multiple documents to form dense tensor suggests method may struggle in real-time scenarios requiring immediate single-response detection.
- What evidence would resolve it: Evaluation of modified approach comparing single test instance against pre-computed "factual" reference tensor or background distribution.

## Limitations
- Performance at single-instance inference (G1) is near-random, limiting real-time application potential
- Cross-domain generalization not thoroughly tested; current evaluation relies on internal dataset comparisons
- Memory constraints prevent application to very large groups or high-order N-grams

## Confidence
- High confidence: Core methodological contribution and empirical finding that performance scales with group size; clear implementation details
- Medium confidence: Claim that singular values capture "latent structural differences" specific to hallucinations; lacks interpretability analysis
- Low confidence: Robustness claim against dataset-specific textual biases; limited cross-subset evaluation

## Next Checks
1. Interpretability probe: Analyze top-k singular values across hallucinated vs. factual groups to identify which N-gram patterns drive discrimination; visualize singular vectors to determine if they capture meaningful semantic differences or superficial statistical artifacts.

2. Cross-domain robustness test: Train classifier on HaluEval General subset, then evaluate on external dataset (e.g., TruthfulQA or Arcee's hallucination detection dataset) to validate generalization beyond training domain.

3. Sparsity sensitivity analysis: Systematically vary vocabulary size thresholds (e.g., min frequency cutoffs) and measure how tensor density affects singular value distributions and classifier performance to clarify whether effectiveness stems from genuine structural patterns or denser numerical representations.