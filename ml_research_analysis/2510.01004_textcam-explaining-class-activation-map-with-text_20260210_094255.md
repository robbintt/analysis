---
ver: rpa2
title: 'TextCAM: Explaining Class Activation Map with Text'
arxiv_id: '2510.01004'
source_url: https://arxiv.org/abs/2510.01004
tags:
- textcam
- visual
- saliency
- text
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TextCAM, a method that extends Class Activation
  Mapping (CAM) with natural language explanations. While CAM highlights spatial regions
  influencing model predictions, it lacks semantic insight into the visual attributes
  driving those decisions.
---

# TextCAM: Explaining Class Activation Map with Text

## Quick Facts
- arXiv ID: 2510.01004
- Source URL: https://arxiv.org/abs/2510.01004
- Authors: Qiming Zhao; Xingjian Li; Xiaoyu Cao; Xiaolong Wu; Min Xu
- Reference count: 22
- Primary result: Achieves 100% concept retrieval accuracy on CLEVR and produces interpretable textual explanations that improve human understanding and detect spurious correlations

## Executive Summary
This paper introduces TextCAM, a method that extends Class Activation Mapping (CAM) with natural language explanations. While CAM highlights spatial regions influencing model predictions, it lacks semantic insight into the visual attributes driving those decisions. TextCAM bridges this gap by combining CAM's spatial localization with vision-language model (VLM) semantic alignment. Specifically, it computes channel-level semantic representations using CLIP embeddings and linear discriminant analysis, then aggregates them with CAM weights to produce textual descriptions of salient visual evidence. The approach is extended to generate feature channel groups, enabling fine-grained visual-textual explanations.

## Method Summary
TextCAM works by first computing per-channel semantic representations through linear discriminant analysis (LDA) in CLIP embedding space, contrasting high vs. low activation samples for each channel. These channel semantics are then aggregated with CAM-derived channel weights to produce image-level textual descriptions. The method uses sparse optimization with diversity penalty to select concise, interpretable text explanations from a large vocabulary. For fine-grained analysis, TextCAM groups channels into saliency maps using a greedy relocation algorithm. The framework is validated on ImageNet, CLEVR, and CUB-200, demonstrating faithful explanations that improve human understanding and detect spurious correlations.

## Key Results
- Achieves 100% concept retrieval accuracy on CLEVR shape/color tasks
- Produces semantically plausible textual explanations on ImageNet and CUB-200
- Successfully detects and mitigates color-based spurious correlations, improving OOD accuracy by 8.66-11.44%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-channel semantic representations can be derived by contrasting high vs. low activation samples in CLIP embedding space.
- Mechanism: For each channel j, select M highest-response and M lowest-response images → encode with CLIP image encoder → apply Linear Discriminant Analysis (LDA) to find the projection vector p_j that maximally separates positive from negative classes. The channel's semantic representation is s_j = a_j · p_j, where a_j is the GAP activation.
- Core assumption: High-response images share visual attributes that differ systematically from low-response images, and CLIP's embedding space preserves these attribute differences linearly.
- Evidence anchors:
  - [abstract]: "we derive channel-level semantic representations using CLIP embeddings and linear discriminant analysis"
  - [Section 3.2]: "we apply Linear Discriminant Analysis (LDA) in the CLIP embedding space, since LDA solves the projection direction that maximizes the inter-class variance and minimizes the intra-class variance"
  - [corpus]: Weak—no direct corpus validation of LDA-for-channel-semantics; related CAM papers focus on spatial weighting, not semantic channel analysis.

### Mechanism 2
- Claim: Aggregating channel semantic representations with CAM weights produces image-level semantics aligned with model attention.
- Mechanism: Given CAM-derived channel weights w_c (from any CAM variant), compute T_c(x) = Σ_j w_c^j · s_j(x). This weighted sum fuses "where" (CAM spatial importance) with "what" (channel semantics).
- Core assumption: The CAM weights accurately reflect channel importance for the class prediction, and the per-channel semantic vectors meaningfully describe visual attributes.
- Evidence anchors:
  - [abstract]: "aggregat[e] them with CAM weights to produce textual descriptions of salient visual evidence"
  - [Section 3.2, Eq. 2]: Formal definition of T_c(x) as weighted aggregation
  - [corpus]: Finer-CAM and DiffGradCAM validate CAM weighting schemes for spatial localization, but not semantic aggregation.

### Mechanism 3
- Claim: Sparse optimization with diversity penalty selects concise, interpretable text explanations from a large vocabulary.
- Mechanism: Solve ω* = argmin_{ω≥0} [½||T_c - Eω||² + α||ω||₁ + βω^T G_off ω], where E is the text embedding matrix and G_off penalizes correlations among selected phrases. Top-K non-zero entries become the explanation.
- Core assumption: The vocabulary contains descriptors that sufficiently cover the visual concepts present in the data, and CLIP text-image alignment preserves semantic correspondence.
- Evidence anchors:
  - [Section 3.2, Eq. 3]: Formal sparse approximation objective with L1 and correlation-aware regularization
  - [Section 4.2]: "TextCAM yields qualitatively consistent and semantically plausible phrases across all w sources"
  - [corpus]: No corpus papers use sparse text selection for CAM explanation; this appears novel to TextCAM.

## Foundational Learning

- **Class Activation Mapping (CAM)**
  - Why needed here: TextCAM builds directly on CAM's spatial weighting; understanding how w_c is computed (gradient-based, forward-based, etc.) is essential.
  - Quick check question: Can you explain how Grad-CAM computes channel weights from gradients?

- **CLIP Vision-Language Alignment**
  - Why needed here: TextCAM relies on CLIP's joint embedding space to connect visual activations with textual descriptors.
  - Quick check question: What does it mean for CLIP to have a "shared embedding space" for images and text?

- **Linear Discriminant Analysis (LDA)**
  - Why needed here: LDA finds the discriminative direction separating high/low activation samples per channel.
  - Quick check question: How does LDA differ from PCA for finding discriminative projections?

## Architecture Onboarding

- **Component map:** Backbone CNN/Transformer → feature maps A_j(x) at target layer → CAM module → channel weights w_c → Channel analysis pipeline (offline): positive/negative sample selection → CLIP image encoding → LDA → per-channel vectors p_j → Aggregation: T_c = Σ w_c^j · a_j · p_j → Sparse text retrieval: vocabulary E → solve Eq. 3 → top-K phrases → Optional grouping: greedy channel partition → K saliency maps

- **Critical path:** The offline channel analysis (LDA on CLIP embeddings) is the most computationally intensive step and must be computed once per backbone/layer combination.

- **Design tradeoffs:**
  - Vocabulary size vs. coverage: larger vocabularies improve recall but increase retrieval computation
  - M (positive/negative samples per channel): paper uses M=100 as default; higher M improves robustness but requires more CLIP encodings
  - K (number of text explanations): tradeoff between completeness and interpretability

- **Failure signatures:**
  - Generic/repetitive explanations: vocabulary may be insufficient or β (diversity penalty) too low
  - Inconsistent explanations across similar images: CAM weights may be unstable (common in transformers)
  - Wrong attribute type (e.g., color when shape matters): channel analysis may have picked up spurious correlations

- **First 3 experiments:**
  1. Validate on CLEVR: Train separate shape and color heads; verify TextCAM retrieves correct attribute per head (paper reports 100% accuracy)
  2. Ablation study on M: Test M ∈ {50, 100, 150, 200} to find stable configuration for your backbone
  3. Cross-architecture test: Apply TextCAM to both CNN (ResNet-50) and transformer (Swin) to assess weight stability and explanation consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can TextCAM be made more robust to unstable CAM-style weights in transformer backbones?
- Basis in paper: [explicit] The conclusion explicitly states: "Current limitations include sensitivity of CAM-style weights in some transformer backbones."
- Why unresolved: The paper shows preliminary Swin Transformer results but notes that "CAM-derived channel weights w can be less stable due to token mixing and attention reweighting" without proposing solutions.
- What evidence would resolve it: A systematic comparison of weight stabilization techniques (attention head aggregation, layer-wise normalization) across multiple transformer architectures with quantitative faithfulness metrics.

### Open Question 2
- Question: Can TextCAM-guided debiasing scale beyond simple color-shape shortcuts to more complex, multi-attribute spurious correlations?
- Basis in paper: [inferred] The debiasing experiment (Section 4.3.2) uses a controlled 3-way shape task with only color as the spurious signal; real-world biases involve multiple entangled attributes.
- Why unresolved: The ablation subspace is constructed from a simple linear color probe; multi-attribute biases would require more sophisticated subspace identification that may interact nonlinearly.
- What evidence would resolve it: Demonstration of OOD improvement on datasets with multiple overlapping spurious correlations (e.g., Waterbirds, CelebA with multiple attributes).

### Open Question 3
- Question: What is the impact of vocabulary coverage and curation on TextCAM explanation quality?
- Basis in paper: [inferred] The vocabulary is manually curated using ChatGPT for general coverage; the paper does not evaluate sensitivity to vocabulary completeness or evaluate against automated/open-ended vocabulary generation.
- Why unresolved: Sparse text selection (Eq. 3) can only select from provided candidates; missing concepts yield no explanation even if semantically relevant.
- What evidence would resolve it: Ablation studies varying vocabulary size and source (manual vs. LLM-generated vs. open vocabulary) measured by concept alignment accuracy on controlled benchmarks like CLEVR.

## Limitations

- Sensitivity to unstable CAM-style weights in transformer backbones due to attention mixing and token reweighting
- Limited quantitative validation beyond CLEVR concept retrieval, relying heavily on qualitative assessment
- Dependency on pre-computed vocabulary quality and coverage for meaningful textual explanations

## Confidence

- **High Confidence**: CLEVR concept retrieval accuracy (100% verified on controlled dataset), basic CAM-weight aggregation mechanism
- **Medium Confidence**: ImageNet/CUB qualitative explanations (limited quantitative validation), sparse text selection framework (implementation details unspecified)
- **Low Confidence**: Cross-dataset generalization robustness, sensitivity to hyperparameter choices (M, α, β), computational efficiency at scale

## Next Checks

1. Implement ablation study varying M (50-200) to establish minimum samples needed for stable per-channel semantics across different backbones
2. Test cross-architecture transferability by applying TextCAM from ResNet-50 to Swin Transformer and measuring explanation consistency degradation
3. Validate on OOD dataset (DomainNet) with quantitative metrics beyond qualitative assessment to confirm spurious correlation detection claims