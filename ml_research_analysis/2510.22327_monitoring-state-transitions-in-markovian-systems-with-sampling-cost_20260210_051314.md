---
ver: rpa2
title: Monitoring State Transitions in Markovian Systems with Sampling Cost
arxiv_id: '2510.22327'
source_url: https://arxiv.org/abs/2510.22327
tags:
- state
- policy
- query
- time
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies a node-monitor system where the monitor must
  track a node's state, choosing between costly queries and cheaper predictions. In
  a Markovian setting, the optimal strategy is a state-dependent threshold policy
  that queries based on minimizing the time-averaged sum of query costs and prediction
  losses.
---

# Monitoring State Transitions in Markovian Systems with Sampling Cost

## Quick Facts
- arXiv ID: 2510.22327
- Source URL: https://arxiv.org/abs/2510.22327
- Reference count: 12
- Key outcome: Greedy prediction policies can be arbitrarily worse than optimal in Markovian monitoring systems with sampling costs, especially when states have low future uncertainty.

## Executive Summary
This paper studies the problem of monitoring a node's state in a Markovian system where a monitor can either query the state at a cost or predict it using a learned model. The authors characterize the optimal monitoring strategy as a state-dependent threshold policy that balances query costs against prediction losses. They demonstrate that a simple greedy policy, which predicts when expected loss is below the query cost and queries otherwise, can perform arbitrarily poorly compared to optimal, particularly in systems with absorbing states or low transition uncertainty. The paper proposes a projected stochastic gradient descent approach for learning transition probabilities when they are unknown, combined with the greedy policy.

## Method Summary
The authors formulate the monitoring problem as an infinite-horizon average-cost Markov decision process. They derive the structure of the optimal policy, showing it is a state-dependent threshold policy where the threshold depends on the expected future cost of being wrong. For unknown transition probabilities, they propose a projected stochastic gradient descent (PSGD) algorithm that updates transition estimates after each query. The method combines PSGD with the greedy policy, allowing the system to learn transition dynamics while making monitoring decisions. Numerical experiments validate the approach on randomly generated Markov chains, comparing greedy policy performance against optimal policies in both known and unknown transition probability settings.

## Key Results
- The optimal monitoring strategy is a state-dependent threshold policy that queries based on minimizing time-averaged query costs plus prediction losses
- Greedy policies that predict when expected loss is below query cost can be arbitrarily worse than optimal, especially with absorbing states or low future uncertainty
- When transition probabilities are identically distributed across states, greedy policy performance approaches optimal
- PSGD with greedy policy converges to the greedy policy over time for unknown transition probabilities
- Numerical results show greedy policy performs near-optimally in randomly generated Markov chains but degrades with highly state-dependent transition dynamics

## Why This Works (Mechanism)
The monitoring problem is fundamentally a sequential decision-making task under uncertainty where the monitor must balance the cost of obtaining perfect information (querying) against the cost of imperfect predictions. In Markovian systems, the optimal strategy depends on both the current state's uncertainty and the expected future uncertainty given the current state. The state-dependent threshold policy works by recognizing that some states have inherently lower future uncertainty (like absorbing states), making predictions more reliable and reducing the need for costly queries. The PSGD approach works by iteratively refining transition probability estimates based on observed transitions, allowing the greedy policy to improve its predictions over time.

## Foundational Learning
- Markov decision processes: Why needed - to formally model the sequential monitoring problem with costs and transitions; Quick check - verify understanding of state transitions and cost structures
- Stochastic gradient descent: Why needed - to learn transition probabilities from observed data; Quick check - confirm convergence properties for non-convex objectives
- Threshold policies: Why needed - to characterize optimal decision rules in monitoring systems; Quick check - understand how thresholds depend on state and cost parameters
- Absorbing states: Why needed - to understand worst-case scenarios for greedy policies; Quick check - identify absorbing states in sample Markov chains
- Projected optimization: Why needed - to ensure learned transition probabilities remain valid probability distributions; Quick check - verify projection maintains probability constraints

## Architecture Onboarding

Component map: Monitoring system -> State prediction/query decision -> Cost evaluation -> Transition observation -> PSGD update -> New state

Critical path: Observation of current state → Prediction/query decision (greedy policy) → Cost incurred → State transition observed → PSGD update (if queried) → Next decision cycle

Design tradeoffs: The paper trades off between the simplicity and computational efficiency of greedy policies against the potential for significant performance degradation in systems with heterogeneous transition dynamics. The PSGD approach adds computational overhead but enables learning in unknown environments.

Failure signatures: Greedy policy performance degrades arbitrarily when states have low future uncertainty (absorbing states) or highly state-dependent transition probabilities. PSGD convergence may be slow when transition probabilities change rapidly or when query frequency is low.

First experiments:
1. Implement the greedy policy on a simple two-state Markov chain and compare against optimal policy
2. Test PSGD convergence on a known Markov chain with synthetic observations
3. Vary the degree of heterogeneity in transition probabilities to quantify greedy policy degradation

## Open Questions the Paper Calls Out
The paper acknowledges that major uncertainties remain regarding the practical applicability of the greedy policy in systems with highly heterogeneous transition dynamics, where performance can degrade arbitrarily. The theoretical guarantees for the PSGD approach when combined with the greedy policy are not fully explored, particularly regarding convergence rates and stability under different Markov chain structures.

## Limitations
- Greedy policy performance can degrade arbitrarily in systems with absorbing states or low future uncertainty
- Theoretical convergence guarantees for PSGD with greedy policy are not fully established
- The assumption of identically distributed transition probabilities across states is critical for greedy policy performance but not empirically validated

## Confidence

High confidence:
- Characterization of optimal threshold policy as state-dependent
- Fundamental limitation of greedy policies in absorbing/low-uncertainty states

Medium confidence:
- PSGD convergence claims and numerical performance in randomly generated Markov chains

Low confidence:
- Performance bounds and practical recommendations for real-world systems with unknown, potentially highly heterogeneous transition dynamics

## Next Checks

1. Conduct systematic experiments varying the degree of heterogeneity in transition probabilities across states to quantify the exact threshold where greedy policy performance degrades

2. Implement the PSGD approach on benchmark Markov decision process problems with known optimal policies to measure convergence rates and compare against theoretical bounds

3. Test the methodology on real-world sequential monitoring datasets (e.g., health monitoring or network traffic) where state transition dynamics are empirically validated