---
ver: rpa2
title: '`Generalization is hallucination'' through the lens of tensor completions'
arxiv_id: '2502.17305'
source_url: https://arxiv.org/abs/2502.17305
tags:
- artifacts
- tensor
- language
- generalization
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces tensor completions and artifacts as a theoretical
  framework for understanding hallucinations and generalizations in language models.
  It shows that both phenomena arise as artifacts in tensor completions, which are
  novel sentences predicted with high probability by the model.
---

# `Generalization is hallucination' through the lens of tensor completions

## Quick Facts
- **arXiv ID**: 2502.17305
- **Source URL**: https://arxiv.org/abs/2502.17305
- **Reference count**: 20
- **Primary result**: Both hallucinations and generalizations arise as tensor completion artifacts in language models, with smaller models producing more artifacts

## Executive Summary
This paper introduces tensor completions and artifacts as a theoretical framework for understanding hallucinations and generalizations in language models. It shows that both phenomena arise as artifacts in tensor completions, which are novel sentences predicted with high probability by the model. Experiments on toy models demonstrate that artifacts are prevalent and increase when models are smaller. The framework suggests that hallucinations and generalizations are two sides of the same coin, and that efforts to mitigate one may impact the other.

## Method Summary
The paper trains simplified transformers on random triple datasets to study artifact generation. It uses toy models without layer norm, biases, or positional encodings with varying parameter counts. The objective is to count artifacts - triples not in the training data where the model assigns probability ≥0.95. Figure 2 varies training set size while Figure 3 varies model size across 165 different configurations.

## Key Results
- Artifacts increase monotonically as model parameter count decreases
- Both hallucinations and generalizations manifest as tensor completion artifacts
- Lower-rank model completions produce more artifacts
- The framework reveals an inherent trade-off between hallucination mitigation and generalization capability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Language models performing next-token prediction implicitly execute tensor completion, generating "artifacts" (novel high-probability n-grams) in unobserved regions of the probability tensor.
- **Mechanism**: Training enforces consistency only on "fibers" (conditional distributions) at contexts actually present in the corpus. All other entries in the full n-gram tensor are unconstrained, allowing spurious high-probability predictions to emerge as a mathematical consequence of low-rank completion.
- **Core assumption**: The loss function in Eq. (2) sufficiently captures LM training behavior for the tensor analogy to hold.
- **Evidence anchors**: [abstract] "both arise as tensor completion artifacts"; [Section 2] Defines D_n tensors and consistency; Section 2.1: "the only zeros that are enforced are those that belong to these training fibers"
- **Break condition**: If LM architectures impose constraints beyond low-rank structure that systematically suppress artifacts, or if training objectives fundamentally alter completion behavior.

### Mechanism 2
- **Claim**: Lower-rank model completions produce more artifacts; reducing parameter count increases hallucinations/generalizations.
- **Mechanism**: Rank-constrained tensor completion must "fill in" more entries to remain consistent with observed fibers. Fewer degrees of freedom force the completion to spread probability mass across a larger set of novel n-grams.
- **Core assumption**: Non-embedding parameter count serves as a reasonable proxy for the effective rank of the model's implicit tensor completion.
- **Evidence anchors**: [Section 2.1] "artifacts are prevalent, and increase in number when models are smaller"; [Figure 3] Shows monotonic increase in artifacts as parameter count decreases from ~500 to ~100
- **Break condition**: If scaling laws or emergent capabilities at larger model sizes introduce qualitative differences not captured by the rank-artifact relationship.

### Mechanism 3
- **Claim**: Generalization and hallucination are mathematically indistinguishable within the tensor completion framework; differentiation requires external grounding.
- **Mechanism**: Both phenomena manifest as high-probability artifacts in the completed tensor. Whether an artifact constitutes valid generalization or false hallucination depends solely on whether it aligns with an external truth standard inaccessible to the model.
- **Core assumption**: There is no intrinsic signal in the training data or model weights that distinguishes "desirable" from "undesirable" artifacts.
- **Evidence anchors**: [Section 3] Examples (3) and (4) show identical structure producing desirable vs. undesirable artifacts; [Section 3.5] "Neither should be studied in isolation without considering the trade-off"
- **Break condition**: If methods exist to encode external truth constraints into training that systematically filter artifacts without reducing valid generalizations.

## Foundational Learning

- **Concept: Tensor rank and low-rank decomposition**
  - Why needed here: The core argument hinges on rank constraints producing artifacts; understanding why lower-rank approximations necessarily differ from full-rank tensors is essential.
  - Quick check question: Given a 3×3×3 tensor with rank-3 structure, can a rank-2 approximation ever exactly reproduce all entries?

- **Concept: Conditional probability fibers in multi-dimensional arrays**
  - Why needed here: The paper models corpora as tensors where each "fiber" represents P(t_n | t_1, ..., t_{n-1}); grasping this geometrically clarifies why unobserved contexts remain unconstrained.
  - Quick check question: In a 4-dimensional tensor T_{i,j,k,l}, what does the fiber T_{i=3,j=2,:} represent?

- **Concept: Closed-world vs. open-world assumptions in databases**
  - Why needed here: The proposed mitigation (Section 3.4) references "local closed world assumptions"—explicitly marking absent entries as false rather than unknown.
  - Quick check question: How does adding (t, t_unsupported) tuples to training data differ from simply not including t at all?

## Architecture Onboarding

- **Component map**: Corpus D → n-gram tensors D_n (sparse, mostly zeros) → Training on fibers D_n[t,:] for observed contexts t ∈ D^(n-1) → Model P_θ → implicit tensor completion P^n_θ → Artifacts = {high-probability entries in P^n_θ \ D_n}

- **Critical path**: To validate this framework on a real system, you must: (1) extract n-gram statistics from training data, (2) identify model predictions on held-out contexts, (3) classify high-confidence predictions as artifacts, (4) correlate artifact counts with model capacity measures.

- **Design tradeoffs**:
  - Mitigating hallucinations via negative sampling reduces artifacts but may harm generalization
  - Overfitting reduces artifacts but increases standard generalization error
  - Rank regularization could control artifacts but may limit model expressivity for valid patterns

- **Failure signatures**:
  - Model produces confident predictions on out-of-distribution contexts with no training signal
  - Scaling up parameters reduces but does not eliminate artifacts (Figure 3 shows plateau, not zero)
  - Fine-tuning on negation data may cause over-rejection of valid outputs

- **First 3 experiments**:
  1. Replicate Figure 3 with larger vocabulary (|T| > 1000) to test scaling behavior of artifact counts
  2. Implement "unsupported token" augmentation per Section 3.4; measure both hallucination reduction and generalization degradation on a held-out test set
  3. Estimate effective rank of real datasets using randomized decomposition methods; test whether D_train ∪ D_test rank proximity predicts generalization error as claimed in Section 3.3

## Open Questions the Paper Calls Out

- **Open Question 1**: Do the trends observed in toy models—specifically that fewer parameters increase artifacts—persist when scaled up to large language models and large textual datasets?
  - Basis in paper: [explicit] The authors list as a limitation that "it is uncertain if the same trends will persist when scaled up" and call for "quantifying the number of artifacts present in larger models."
  - Why unresolved: The paper only provides empirical validation on toy models with small vocabularies.
  - What evidence would resolve it: Empirical measurements of artifact prevalence in standard LLMs across varying parameter counts.

- **Open Question 2**: How can we tractably approximate or estimate tensor completion artifacts and effective ranks in models where the context and vocabulary make exact computation impossible?
  - Basis in paper: [explicit] Section 4 states that theoretical objects "very quickly grow impractical to compute" and calls for work to "find ways to approximate or estimate these objects and quantities tractably."
  - Why unresolved: Exhaustively evaluating a model on every possible n-gram to count artifacts is computationally infeasible for large models.
  - What evidence would resolve it: The development of estimation algorithms that correlate with exact counts in smaller settings.

- **Open Question 3**: Can augmenting training data with "unsupported" tokens for inputs outside the training distribution mitigate hallucinations without significantly degrading desirable generalization?
  - Basis in paper: [explicit] The paper proposes testing if "augmenting the dataset with sentences... for a few [out-of-distribution inputs]" mitigates hallucinations, while Section 3.5 warns of the inherent trade-off.
  - Why unresolved: While proposed as a mitigation strategy, the authors note that excluding artifacts might impact generalization, requiring empirical verification.
  - What evidence would resolve it: Experiments comparing hallucination rates and generalization accuracy in models trained with negative constraints versus standard training.

## Limitations

- All experimental evidence comes from synthetic datasets with 44 tokens and triple completions, limiting applicability to real-world language models
- The assumption that non-embedding parameter count serves as a proxy for tensor rank is heuristic rather than rigorously established
- The framework doesn't provide practical guidance for distinguishing hallucinations from valid generalizations in deployed systems

## Confidence

**High confidence**: The core mathematical observation that unobserved tensor entries remain unconstrained during training is sound. The claim that smaller models produce more artifacts is well-supported by Figure 3 within the toy setting.

**Medium confidence**: The relationship between tensor rank and artifact prevalence may extend to larger models, but requires empirical validation. The proposed mitigation strategies are theoretically plausible but unproven.

**Low confidence**: Claims about the framework's applicability to real language models, the effectiveness of proposed hallucination mitigation techniques, and the practical utility of the tensor completion analogy for system design.

## Next Checks

1. **Scaling validation**: Replicate Figure 3 experiments with larger vocabularies (10^3-10^4 tokens) and longer contexts (4-5 grams) to test whether the artifact-parameter count relationship holds under realistic conditions. Measure whether artifacts plateau or continue increasing as model capacity grows.

2. **Grounding experiment**: Implement the "unsupported token" augmentation from Section 3.4 on a real dataset with ground truth labels. Measure both hallucination reduction and generalization degradation on a held-out test set to quantify the trade-off the framework predicts.

3. **Rank estimation validation**: Estimate effective rank of real language datasets using randomized decomposition methods. Test whether D_train ∪ D_test rank proximity predicts generalization error as claimed in Section 3.3, and whether this relationship holds across different model architectures and training regimes.