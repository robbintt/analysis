---
ver: rpa2
title: Training Language Model to Critique for Better Refinement
arxiv_id: '2506.22157'
source_url: https://arxiv.org/abs/2506.22157
tags:
- critique
- answer
- your
- code
- assistant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Refinement-oriented Critique Optimization
  (RCO), a method for training critic models that generate critiques specifically
  aimed at improving actor model responses. The core idea is to use critique utility
  (CU) - calculated as the preference ratio of refined responses to initial responses
  - as a reward signal to train the critic model, eliminating the need for direct
  critique preference assessment.
---

# Training Language Model to Critique for Better Refinement

## Quick Facts
- arXiv ID: 2506.22157
- Source URL: https://arxiv.org/abs/2506.22157
- Reference count: 40
- Primary result: RCO achieves 81.1 average CU and 6.49 RQS across 5 tasks, outperforming baselines in generating actionable critiques.

## Executive Summary
This paper introduces Refinement-oriented Critique Optimization (RCO), a method for training critic models that generate critiques specifically aimed at improving actor model responses. The core innovation is using critique utility - calculated as the preference ratio of refined responses to initial responses - as a reward signal to train the critic model, eliminating the need for direct critique preference assessment. RCO significantly outperforms traditional methods and open-source models across five tasks: dialog generation, summarization, question answering, mathematical reasoning, and code generation, achieving 81.1 CU and 6.49 RQS on average across tasks with strong weak-to-strong generalization capabilities.

## Method Summary
RCO trains critic models by generating critiques for initial responses, having actor models produce multiple refinements based on each critique, and using a judge model to score preferences between refined and initial responses. The critique utility (CU) - the proportion of refinements preferred over initial responses - serves as a scalar reward signal. The critic is trained with a KL-regularized objective that maximizes the likelihood of critiques proportional to their CU, approximated using Monte Carlo sampling. The method uses N=4 critiques per sample, M=5 refinements per critique, and β=0.1 regularization parameter, trained with MSE loss over 5 epochs.

## Key Results
- RCO achieves 81.1 average CU and 6.49 RQS across five tasks, significantly outperforming baselines.
- RCO shows strong weak-to-strong generalization, maintaining performance gains when applied to larger actor models (70B).
- Human evaluation confirms RCO critiques excel in accuracy, thoroughness, clarity, and constructiveness compared to baselines.

## Why This Works (Mechanism)

### Mechanism 1
Rewarding critiques by refinement outcome, not critique quality, produces more actionable feedback. RCO computes Critique Utility (CU) as the proportion of refinements preferred over initial responses. This scalar reward trains the critic via a squared-error objective that directly optimizes for "does this critique help the actor improve?" rather than "is this critique well-written?" The assumption is that actor models can reliably improve when given constructive, specific guidance; judge model preferences correlate with genuine quality improvements. Evidence shows RCO critiques excel in "constructiveness" (offering actionable suggestions), while baseline critiques are more thorough but vague. Break condition: If the judge model has systematic biases or the actor model cannot interpret critiques reliably, CU becomes a noisy proxy.

### Mechanism 2
Outcome-based rewards bypass the difficulty of directly assessing critique quality. Traditional methods require annotators or LLMs to judge "which critique is better"—a subjective task. RCO instead asks "which refined response is better than the original?"—a more grounded comparison that indirectly rewards useful critiques. The assumption is that preference judgments on responses are more reliable and consistent than preference judgments on critiques. Evidence shows human preferences for critiques do not always align with preferences for the refinements generated from those critiques—preferred critiques don't guarantee better refinements. Break condition: If refined responses are difficult to evaluate (e.g., highly subjective tasks without clear quality criteria), the advantage diminishes.

### Mechanism 3
RCO enables weak-to-strong generalization by aligning critic training with actor capability. Since CU is computed using the actual actor model's refinements, the critic learns to produce critiques that this specific actor can interpret and act upon. When applied to stronger actors, the critic's guidance remains interpretable. The assumption is that critiques that help weaker actors improve also transfer to helping stronger actors; the learned critique style is actor-agnostic enough. Evidence shows RCO-trained critics show superior performance on larger actor models (70B) compared to baselines, with CU improving from 80.8 to 92.1 on LLaMA-3-70B. Break condition: If critics overfit to specific actor idiosyncrasies, transfer to substantially different architectures may degrade.

## Foundational Learning

- **Preference-based RL / RLHF fundamentals**
  - Why needed here: RCO's training objective derives from KL-regularized reward maximization. Understanding why DPO-style objectives avoid explicit reward models helps grasp RCO's design.
  - Quick check question: Can you explain why the partition function Z_β(y₀, x) appears in the training objective and how it's approximated?

- **Monte Carlo estimation of expected values**
  - Why needed here: CU is estimated by sampling M refinements and averaging preference scores. Understanding variance-reduction tradeoffs in sampling is critical for efficient data collection.
  - Quick check question: If you double M from 5 to 10, what happens to CU estimate variance, and is the 2× computational cost justified?

- **LLM-as-judge bias and mitigation**
  - Why needed here: RCO depends on a judge model for preference scoring. Positional bias and self-preference are known issues.
  - Quick check question: Why does RCO alternate positions of refined vs. initial responses when computing preferences, and what bias does this address?

## Architecture Onboarding

- **Component map**: Dataset D: (prompt x, initial response y₀) → Critic Model → N critiques per sample → Actor Model → M refinements per critique → Judge Model → Preference scores (refined vs. initial) → CU Calculator → Scalar reward per critique → RCO Loss → Update Critic Model

- **Critical path**: Judge model quality → CU accuracy → critic learning signal. Section 6.2 shows judge choice matters; PairRM (54.2% RewardBench accuracy) yields worst RCO performance, but surprisingly self-rewarding (base critic judging its own refinements) achieves competitive results.

- **Design tradeoffs**:
  - **N (critiques per sample)**: Higher N → better partition function Z_β estimation, but 4× compute. Paper tests N=2–4 with modest degradation at N=2.
  - **M (refinements per critique)**: Higher M → better CU estimation. Paper uses M=5; M=1 is theoretically minimal but noisier.
  - **Judge model selection**: Stronger judges (GPT-4o, Qwen) yield slightly better results than weaker ones, but self-rewarding is viable for cost-sensitive scenarios.

- **Failure signatures**:
  - No improvement over baseline: Check if judge model has positional bias not mitigated by swapping; verify actor model can follow critique format.
  - CU stuck near 0.5: Critiques may be too vague or actor is not effectively using them. Inspect refinement prompts.
  - Training instability: β=0.1 used in paper; if loss diverges, consider increasing β (more KL regularization) or reducing learning rate.

- **First 3 experiments**:
  1. Reproduce with minimal sampling: Use N=2, M=2 on a subset (1,000 samples) with open-source judge (e.g., PairRM or smaller critic model) to validate pipeline before scaling.
  2. Ablate judge model: Compare Qwen-2.5-72B vs. self-rewarding vs. a dedicated reward model (InternLM2-7B-Reward) on held-out validation set to quantify judge impact for your compute budget.
  3. Test actor generalization: Train critic on one actor (e.g., LLaMA-2-7B), evaluate CU when applied to different actor (e.g., LLaMA-3-8B) to assess critic portability before committing to full training runs.

## Open Questions the Paper Calls Out

### Open Question 1
Can the RCO framework be extended to jointly train actor models to better interpret and utilize critiques, rather than training the critic model in isolation? The current method assumes a fixed actor model capable of following refinement instructions, but the actor's ability to "listen" is not optimized during training. Evidence would be experimental results from a joint training paradigm where the actor model is fine-tuned alongside the critic using the same refinement success signals, showing improved RQS compared to a fixed actor.

### Open Question 2
How can the estimation of Critique Utility (CU) and the regularization term be stabilized to require fewer sampled refinements (M) and critiques (N)? The current sampling-based approach introduces variance; reducing this variance without increasing cost remains an unsolved optimization problem. Evidence would be the development of a variance-reduction technique (e.g., control variates or learned value functions) that achieves comparable CU accuracy and model performance using N=1 or M=1.

### Open Question 3
What linguistic or structural features explain the divergence where human-preferred critiques (often thorough) fail to lead to better refinements than utility-optimized critiques? The paper identifies the phenomenon (humans prefer thoroughness, actors prefer constructiveness) but does not isolate the specific features that make a critique "actionable" for a model versus "appealing" to a human. Evidence would be an ablation study correlating specific critique attributes (e.g., presence of code snippets vs. high-level summaries) with Refinement Quality Scores, explicitly decoupling human preference from model utility.

## Limitations

- The study's dependence on a single judge model (Qwen-2.5-72B-Instruct) for preference judgments introduces potential bias and limits generalizability across domains.
- The method assumes actor models can reliably use critique feedback for refinement, but this may not hold for tasks requiring creative or highly subjective outputs.
- The training data distribution (14 datasets across 5 tasks) may not capture all possible critique-refinement scenarios, potentially limiting real-world applicability.

## Confidence

- **High confidence**: The core RCO methodology and mathematical formulation (Sections 4.1-4.2) are well-specified and theoretically sound.
- **Medium confidence**: The reported performance improvements across tasks are internally consistent, but external validation on truly unseen domains would strengthen claims.
- **Medium confidence**: Weak-to-strong generalization results are promising but based on limited model size variations; scaling to much larger models remains untested.

## Next Checks

1. **Judge model ablation**: Compare RCO performance using different judge models (GPT-4, Claude, or open-source alternatives) to quantify sensitivity to preference judgment bias.

2. **Domain generalization test**: Apply RCO-trained critics to completely new task domains (e.g., legal reasoning, creative writing) to assess true weak-to-strong generalization beyond tested tasks.

3. **Human evaluation verification**: Conduct blind human evaluation comparing RCO-generated critiques against baseline critiques across multiple dimensions (actionability, specificity, constructiveness) to validate automated preference judgments.