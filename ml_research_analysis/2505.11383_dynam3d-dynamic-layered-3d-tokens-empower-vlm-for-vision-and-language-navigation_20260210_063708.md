---
ver: rpa2
title: 'Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation'
arxiv_id: '2505.11383'
source_url: https://arxiv.org/abs/2505.11383
tags:
- navigation
- dynam3d
- instance
- feature
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dynam3D, a dynamic layered 3D representation
  model for vision-and-language navigation (VLN). The approach addresses limitations
  of video-based models in capturing 3D geometry, spatial semantics, and adapting
  to dynamic environments by constructing hierarchical patch-instance-zone representations
  from RGB-D images.
---

# Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation

## Quick Facts
- arXiv ID: 2505.11383
- Source URL: https://arxiv.org/abs/2505.11383
- Authors: Zihan Wang; Seungjun Lee; Gim Hee Lee
- Reference count: 40
- Achieves state-of-the-art results on R2R-CE (SR: 52.9%), REVERIE-CE (SR: 40.1%), and NavRAG-CE (SR: 24.7%)

## Executive Summary
This paper introduces Dynam3D, a dynamic layered 3D representation model for vision-and-language navigation (VLN) that addresses limitations of video-based models in capturing 3D geometry, spatial semantics, and adapting to dynamic environments. The approach constructs hierarchical patch-instance-zone representations from RGB-D images using CLIP features projected into 3D space, with a merging discriminator to maintain consistent 3D instance representations across views. A generalizable feature field renders panoramic 3D patch tokens, which are combined with instance and zone representations as input to a 3D-VLM for navigation action prediction. The method achieves state-of-the-art performance on multiple VLN benchmarks while demonstrating strong results in pre-exploration, lifelong memory, and real-world robot navigation.

## Method Summary
Dynam3D addresses the limitations of video-based models in VLN by constructing dynamic layered 3D representations that capture geometry, semantics, and spatial relationships. The system builds hierarchical representations from RGB-D images: patch tokens from panoramic projections of CLIP features, instance tokens using FastSAM masks and a merging discriminator for multi-view consistency, and zone tokens partitioning space into cubic regions. These representations are rendered into panoramic views and combined as input to a 3D-VLM that predicts navigation actions. The approach enables better understanding of 3D environments and more accurate navigation decisions compared to video-based baselines.

## Key Results
- Achieves 52.9% success rate on R2R-CE benchmark, surpassing previous state-of-the-art
- Demonstrates 40.1% success rate on REVERIE-CE benchmark for remote object grounding
- Shows 24.7% success rate on NavRAG-CE benchmark for navigation with retrieval-augmented generation
- Strong performance in pre-exploration scenarios and lifelong memory tasks
- Successful real-world robot navigation experiments validating practical applicability

## Why This Works (Mechanism)
Dynam3D's effectiveness stems from its layered 3D representation that explicitly models spatial relationships and geometric consistency. By projecting features into 3D space and maintaining instance-level consistency across views through the merging discriminator, the system captures true spatial semantics rather than treating navigation as a video sequence. The hierarchical structure (patch-instance-zone) allows for both fine-grained object-level understanding and broader spatial context, enabling more informed navigation decisions. The generalizable feature field provides panoramic views that preserve 3D relationships while being compatible with existing VLM architectures.

## Foundational Learning
- **3D spatial reasoning**: Understanding how objects and spaces relate in three dimensions is crucial for navigation; quick check: verify the system correctly maintains object positions across viewpoint changes
- **Multi-view geometry**: The ability to match and merge instances across different camera views; quick check: test merging accuracy with varying viewpoints and occlusions
- **Feature field rendering**: Converting 3D representations into panoramic views for VLM input; quick check: compare rendered views against ground truth for geometric fidelity
- **Hierarchical representation learning**: Building representations at multiple scales (patch, instance, zone); quick check: ablation studies showing contribution of each level
- **Vision-language grounding**: Connecting visual features with language instructions; quick check: language-conditioned retrieval accuracy in 3D space
- **Instance segmentation and tracking**: Maintaining consistent object identities across views; quick check: instance merging accuracy across viewpoint changes

## Architecture Onboarding
- **Component map**: RGB-D Images -> CLIP Feature Extraction -> 3D Projection -> Patch Tokens -> FastSAM Instance Masks -> Merging Discriminator -> Instance Tokens -> Zone Partitioning -> Zone Tokens -> Feature Field Rendering -> Panoramic Views -> 3D-VLM -> Navigation Actions
- **Critical path**: RGB-D input → 3D feature projection → hierarchical token construction → panoramic rendering → VLM action prediction
- **Design tradeoffs**: Uses instance masks for explicit 3D understanding vs. pure end-to-end approaches; trades computational overhead for geometric accuracy; requires depth information vs. working with RGB only
- **Failure signatures**: Poor depth quality affects 3D projection accuracy; occlusions break instance merging; incorrect zone partitioning leads to spatial confusion; rendering errors propagate to VLM
- **First experiments**: 1) Ablation study removing zone representations to measure spatial context contribution, 2) Test with degraded depth data to assess robustness, 3) Compare merging discriminator performance with and without semantic consistency

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can Dynam3D be extended to output explicit target instance coordinates for mobile manipulation tasks?
- Basis in paper: [explicit] The authors state: "Our Dynam3D predicts navigation actions without explicitly outputting the coordinate of target instance, limiting its applicability to some tasks such as mobile manipulation."
- Why unresolved: The current architecture outputs only atomic navigation actions (turn, move forward, stop), not 3D coordinates. Extending to coordinate prediction would require architectural modifications and new training objectives.
- What evidence would resolve it: A modified Dynam3D variant that outputs 3D target coordinates, evaluated on mobile manipulation benchmarks showing comparable or improved performance.

### Open Question 2
- Question: How does zone size affect navigation performance across environments with varying spatial scales?
- Basis in paper: [inferred] The paper mentions partitioning space into "uniform cubic zones (each spanning several cubic meters)" but does not analyze sensitivity to zone size or compare against adaptive partitioning strategies.
- Why unresolved: Ablation studies focus on presence/absence of zones, not granularity. Optimal zone size likely varies between small apartments and large commercial spaces.
- What evidence would resolve it: Systematic experiments varying zone dimensions on diverse scene types, or an adaptive zone partitioning mechanism with performance comparisons.

### Open Question 3
- Question: Can the 3D instance merging accuracy be maintained when objects undergo significant appearance changes due to viewpoint, lighting, or deformation?
- Basis in paper: [inferred] The Merging Discriminator relies on semantic and geometric consistency for multi-view instance matching, but challenging real-world scenarios with dramatic appearance variation are not extensively analyzed.
- Why unresolved: Training data covers diverse scenes, but extreme appearance changes (partial occlusion, unusual viewing angles, lighting shifts) may still cause merging failures.
- What evidence would resolve it: Evaluation on datasets with annotated viewpoint-dependent appearance changes, or adversarial testing with systematic occlusion and lighting variation.

## Limitations
- Relies on RGB-D images and instance segmentation, which may fail in scenes with poor depth quality or heavy occlusion
- Generalizability to highly complex, real-world environments beyond evaluated benchmarks remains uncertain
- Computational overhead of constructing and maintaining 3D patch-instance-zone representations is not thoroughly analyzed, potentially impacting real-time deployment

## Confidence
- Technical methodology: **High** - Detailed architectural descriptions and systematic experimental validation provided
- Generalizability claims: **Medium** - Strong performance across different tasks, but benchmarks may not capture full diversity of real-world scenarios
- Practical deployment feasibility: **Medium** - Demonstrated real-world robot navigation but computational overhead not fully characterized

## Next Checks
1. Evaluate performance on environments with significant occlusions or poor depth sensing to assess robustness of the 3D token construction pipeline
2. Conduct ablation studies isolating the contribution of each component (patch, instance, zone representations) to quantify their individual impact on navigation success
3. Measure and report computational overhead (memory and inference time) for the dynamic 3D representation construction and rendering processes to assess practical deployment feasibility