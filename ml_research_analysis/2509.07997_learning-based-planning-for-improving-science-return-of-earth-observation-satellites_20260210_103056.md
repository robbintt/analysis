---
ver: rpa2
title: Learning-Based Planning for Improving Science Return of Earth Observation Satellites
arxiv_id: '2509.07997'
source_url: https://arxiv.org/abs/2509.07997
tags:
- cloud
- state
- learning
- reward
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops learning-based planning methods to improve
  science return of Earth observation satellites through intelligent targeting. The
  authors address the challenge of maximizing informative measurements while managing
  satellite power constraints and limited sensor capabilities.
---

# Learning-Based Planning for Improving Science Return of Earth Observation Satellites

## Quick Facts
- arXiv ID: 2509.07997
- Source URL: https://arxiv.org/abs/2509.07997
- Reference count: 34
- Primary result: Learning-based methods (Q-learning and behavioral cloning) significantly outperform existing heuristics in Earth observation satellite targeting, achieving up to 98.67% and 95.84% of optimal reward respectively.

## Executive Summary
This paper develops learning-based planning methods to improve science return of Earth observation satellites through intelligent targeting. The authors address the challenge of maximizing informative measurements while managing satellite power constraints and limited sensor capabilities. They propose two learning approaches - reinforcement learning (Q-learning) and imitation learning (behavioral cloning) - both leveraging dynamic programming concepts. These methods decide when to sample based on cloud types and state of charge. Tested in cloud avoidance and storm hunting scenarios using real satellite data, the learning methods outperform existing heuristics.

## Method Summary
The authors formulate dynamic targeting as a Markov Decision Process where the satellite must decide when to sample based on sensor observations and power state. They implement two approaches: Q-learning with DP-inspired backward iteration that systematically visits all state-action pairs, and behavioral cloning that learns from expert demonstrations generated by a dynamic programming oracle. The state representation encodes sensor data and power state, while the reward structure incentivizes high-value measurements (clear skies or storm cores) while penalizing power consumption. Training uses MODIS and GPM satellite imagery classified into reward categories.

## Key Results
- Q-learning achieved 98.67% of optimal reward in cloud avoidance scenario and 94.66% in storm hunting scenario
- Behavioral cloning reached 95.84% and 91.27% of optimal reward respectively
- Both learning approaches significantly outperformed the best prior method (87.50% and 82.57%)
- Methods require relatively small training data (3,000-20,000 images) to achieve strong performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DP-inspired backward iteration improves Q-learning state coverage without requiring more data.
- Mechanism: Standard ε-greedy Q-learning fails to visit enough unique states during forward exploration. By iterating backwards through saved simulation data while varying state-of-charge (SOC) from 0–100%, the algorithm forcibly visits every state-action pair for each timestep. This transforms sparse natural exploration into dense, systematic coverage.
- Core assumption: The environment can be simulated with stored data, allowing backward time-stepping—which is not possible in real-time onboard learning.
- Evidence anchors:
  - [Methods, p.5]: "We begin iterating from the last timestep in the training data, and step backwards in time while varying the state of charge from 0 to 100... This greatly increases the number of states that the Q-learning algorithm encounters while still using the same amount of satellite data for training."
  - [Methods, p.5]: "In practice, we found that training the Q-table using the ε-greedy method did not produce sufficient results... because it did not visit enough unique states to generalize well."

### Mechanism 2
- Claim: Behavioral cloning can approximate optimal policies using compact state vectors and small expert datasets.
- Mechanism: The DP oracle generates state-action pairs across all SOC values and timesteps. A 4-layer MLP (13→32→16→8→4→1) learns a stochastic policy that outputs sampling probability. The network generalizes from <2.5% of possible states to near-optimal performance by learning decision boundaries rather than memorizing trajectories.
- Core assumption: The expert policy (DP) is sufficiently optimal and the state representation preserves decision-relevant information.
- Evidence anchors:
  - [Methods, p.6]: "The training sets that we use in this experiment represent a small fraction of this total, ranging from 0.34% to 2.41%."
  - [Results, p.7]: Figure 6 shows performance converging with ~150,000–180,000 sampled states.

### Mechanism 3
- Claim: Hierarchical reward structure drives selective high-value sampling under power constraints.
- Mechanism: Rewards are stratified by orders of magnitude (clear=100 vs. mid-cloud=10 vs. cloud=1), combined with 5% SOC cost per sample and 1% recharge per timestep. This creates a natural budget constraint where the agent learns to conserve power for high-value targets.
- Core assumption: Reward values correctly encode scientific priority and are stable across scenarios.
- Evidence anchors:
  - [Experiments, p.7]: "Cloud Avoidance: off = 0, cloud = 1, mid-cloud = 10, clear = 100"
  - [Results, p.8, Table 2]: Q-learning samples clear skies 9.06% of the time vs. 7.78% for greedy window, and never samples low-reward clouds (0.00%).

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs)**
  - Why needed here: The dynamic targeting problem is formalized as an MDP with states (sensor observations + SOC), actions (sample location), deterministic transitions, and rewards. Understanding MDPs is prerequisite to both Q-learning and DP formulations.
  - Quick check question: Can you explain why the next state s′ cannot be fully known a priori despite deterministic transitions P(s,s′)=1?

- **Concept: Dynamic Programming / Memoization**
  - Why needed here: The DP oracle (Algorithm 1) provides both the upper bound for evaluation and the expert demonstrations for behavioral cloning. Memoization stores future reward values for each (t, SOC, action) tuple.
  - Quick check question: Why does memoization require iterating backwards from the final timestep?

- **Concept: Exploration vs. Exploitation in Tabular RL**
  - Why needed here: The paper explicitly shows that ε-greedy exploration fails to visit enough states, motivating the DP-inspired training approach. Understanding this tradeoff is critical for diagnosing Q-learning failures.
  - Quick check question: In this problem, why is random exploration particularly inefficient given the state space structure?

## Architecture Onboarding

- **Component map:**
```
[Simulation Framework]
├── Primary sensor (15° from nadir, 7 km/pixel)
├── Lookahead sensor (45° from nadir)
├── Power model (−5% per sample, +1% per timestep)
└── Environment data (MODIS/GPM classified images)

[Training Pipeline]
├── DP Oracle → generates expert state-action pairs
├── Q-learning Module → DP-inspired backward iteration
└── Behavioral Cloning → MLP training on expert demonstrations

[Inference/Runtime]
├── State encoder → 7-dim (Q-learning) or 13-dim (BC) feature vectors
├── Policy lookup (Q-table) or NN forward pass (BC)
└── Action selector → sample/no-sample + pointing decision
```

- **Critical path:**
  1. State representation design (determines tractability)
  2. Training data selection (3,000–20,000 images depending on scenario)
  3. Oracle quality (DP must have accurate future state knowledge)
  4. Policy deployment (microsecond-level inference for 1 Hz sampling)

- **Design tradeoffs:**
  - Q-learning vs. BC: Q-learning achieves higher reward (98.67% vs. 95.84%) but requires more training data and careful state discretization. BC is more flexible with continuous state features but depends on oracle quality.
  - State vector size: 7 features (binary presence) vs. 13 features (fraction coverage)—more features preserve information but increase complexity.
  - Training data: More data helps until overfitting (observed in storm hunting with full 86,400 images).

- **Failure signatures:**
  - Q-learning trained with ε-greedy: underperforms greedy window (failed to visit enough states)
  - BC with insufficient expert diversity: may overfit to specific cloud patterns
  - Reward mis-specification: agent avoids sampling entirely (as Q-learning does with low-reward clouds)

- **First 3 experiments:**
  1. **Reproduce greedy baselines:** Run random, greedy nadir, greedy lateral, greedy radar, and greedy window on provided MODIS/GPM test sets to establish reference rewards.
  2. **Ablate state features:** Train Q-learning with only SOC, only primary sensor, or only lookahead sensor to quantify information contribution of each component.
  3. **Test data efficiency:** Train both methods on 1,000, 3,000, 5,000, 10,000, 20,000 states and plot reward convergence curves (replicate Figure 5) to validate minimum data requirements for a new scenario.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Deep Q-Networks (DQN) or Proximal Policy Optimization (PPO) utilizing raw images outperform the current Q-learning approach that relies on manually-engineered state vectors?
- Basis in paper: [explicit] The authors state a desire to "use full images as inputs rather than manually-engineered state vectors" and to employ "reinforcement learning methods such as deep q-networks (DQN) and proximal policy optimization (PPO)" for continuous state representations.
- Why unresolved: The current Q-learning implementation requires a discrete state vector (e.g., binary cloud presence), which discards image details that might improve decision-making.
- What evidence would resolve it: Comparative simulations showing reward accumulation using convolutional neural networks on raw imagery versus the current feature-vector approach.

### Open Question 2
- Question: How does incorporating the time and energy costs of physically moving the sensor affect the performance and feasibility of the planning algorithms?
- Basis in paper: [explicit] The authors note that the current formulation "does not take into account the power draw or time taken to move the primary sensor" and identify addressing these real-world factors as future work.
- Why unresolved: The current model simplifies the action space by assuming the sensor can point to the best target instantly and without energy cost, an assumption that may not hold on actual spacecraft.
- What evidence would resolve it: Simulation results where the transition model includes a cost function for actuator movement and a time delay for pointing reconfiguration.

### Open Question 3
- Question: Can these learning-based planning methods be successfully deployed and executed on actual satellite hardware with limited flight processors?
- Basis in paper: [explicit] The conclusion states the intent to "deploy and test these algorithms on different satellite platforms, especially those that have flight processors that support deep learning."
- Why unresolved: While the methods run in microseconds in simulation, real-time performance depends on the specific constraints of the onboard flight computer and radiation-hardened hardware.
- What evidence would resolve it: Flight demonstration data or hardware-in-the-loop benchmarks showing inference latency and power consumption on target satellite processors.

## Limitations

- The current formulation does not account for power draw or time taken to move the primary sensor, assuming instantaneous pointing without energy cost
- The methods require access to training data and simulation environments that may not be available in real-time deployment scenarios
- The reward structure and state representation require manual engineering, which may limit generalizability to different mission objectives or sensor configurations

## Confidence

- **High confidence:** Q-learning's superior performance (98.67% vs 95.84% for BC) and the effectiveness of DP-inspired backward iteration for state coverage are well-supported by direct comparisons and ablation studies
- **Medium confidence:** The data efficiency claims (3,000-20,000 images) are supported by empirical curves but depend on the assumed standard training setup for the MLP
- **Low confidence:** The exact mechanism by which the 13-dimensional state vector preserves all decision-relevant information for BC is not fully validated through ablation

## Next Checks

1. **State representation ablation study:** Systematically train Q-learning with progressively reduced state vectors (SOC only, sensor data only, various combinations) to quantify information contribution of each component

2. **Reward sensitivity analysis:** Vary the reward ratios (clear/mid-cloud/cloud or storm types) by factors of 2-10 and measure policy stability to test robustness of the learned policies to reward specification

3. **Online learning test:** Implement a streaming version of the Q-learning algorithm without replay data to verify whether the DP-inspired backward iteration approach can be adapted for true onboard learning scenarios