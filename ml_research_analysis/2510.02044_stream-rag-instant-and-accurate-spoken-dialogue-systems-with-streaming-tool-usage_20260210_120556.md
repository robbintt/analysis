---
ver: rpa2
title: 'Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool
  Usage'
arxiv_id: '2510.02044'
source_url: https://arxiv.org/abs/2510.02044
tags:
- query
- tool
- prediction
- streaming
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first approach to integrate tool usage
  into end-to-end speech-in speech-out dialogue systems. The key innovation is Streaming
  RAG, which enables the system to issue tool queries in parallel with user speech,
  even before the user finishes speaking.
---

# Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage

## Quick Facts
- arXiv ID: 2510.02044
- Source URL: https://arxiv.org/abs/2510.02044
- Reference count: 40
- Primary result: 200% relative accuracy improvement and 20% latency reduction via streaming tool queries in E2E speech-in speech-out dialogue systems

## Executive Summary
This paper introduces Streaming RAG, the first approach to integrate tool usage into end-to-end speech-in speech-out dialogue systems. The key innovation enables the system to issue tool queries in parallel with user speech, even before the user finishes speaking. This is achieved through a post-training pipeline that teaches the model when to trigger tool calls and how to generate spoken summaries fusing audio queries with retrieved text results. Experiments show up to 200% relative accuracy improvement and 20% latency reduction compared to sequential tool usage baselines.

## Method Summary
Streaming RAG enables E2E SISO models to generate tool queries in parallel with user speech through a two-stage post-training pipeline. The method trains models to predict tool queries after processing each audio block (Fixed-Interval) or when new information is detected (Model-Triggered). A similarity-based labeling strategy determines when to issue NO_QUERY tokens versus new queries. The approach includes negative sampling to improve robustness to early prediction errors. A new benchmark, AudioCRAG, is constructed by converting spoken queries from the CRAG dataset, enabling evaluation of streaming tool usage in realistic spoken dialogue scenarios.

## Key Results
- Up to 200% relative accuracy improvement (from 11.1% to 34.2% absolute) on AudioCRAG benchmark
- 20% latency reduction through parallel tool query generation
- Model-Triggered approach reduces computational overhead by maintaining single tool query thread
- Negative sampling during post-training improves error recovery and robustness

## Why This Works (Mechanism)

### Mechanism 1
Streaming tool queries in parallel with user speech reduces user-perceived latency by overlapping tool latency with ongoing user speech. The system divides user speech into blocks and issues tool queries after processing each block or when the model decides sufficient new information is present. This mechanism assumes tool calls are the primary latency bottleneck and the system can predict full query intent from partial audio.

### Mechanism 2
Model-Triggered Streaming RAG intelligently decides when to issue new tool calls by comparing current query predictions with previous ones. The model generates NO_QUERY tokens when predicted queries are semantically similar to the last issued query, preventing redundant calls. This mechanism assumes the model can learn stable semantic similarity for determining when new information has arrived.

### Mechanism 3
Post-training with negative sampling improves robustness to intermediate query prediction errors. During training, the model is sometimes given incorrect previous queries and trained to recover by generating correct new queries instead of NO_QUERY. This mechanism assumes the distribution of errors seen during training is representative of inference-time errors.

## Foundational Learning

- **End-to-End Speech-in Speech-Out (E2E SISO) Models**: Why needed - The entire method is built for these models that process audio directly without intermediate ASR text stage. Quick check - Can you name a key architectural difference between traditional cascaded SDS and E2E SISO models?

- **Retrieval-Augmented Generation (RAG)**: Why needed - The core problem integrates external knowledge via RAG into E2E SISO models. Quick check - In a standard RAG pipeline for a text LLM, what are the two main steps after a user asks a question?

- **Voice Activity Detection (VAD) and Streaming Inference**: Why needed - The paper exploits continuous streaming nature of audio, processing chunks before utterance completion. Quick check - Why would a system process audio in "blocks" instead of waiting for user to stop speaking?

## Architecture Onboarding

- **Component map**: Input Speech Stream -> E2E SISO Model -> Streaming Tool Query Generator -> Tool Interface -> Query Cache -> Response Generator -> Spoken Answer

- **Critical path**: 1) Audio stream chunked into blocks, 2) Each block processed by Streaming Tool Query Generator, 3) If new query, send to Tool Interface, 4) On user finish, use last successful tool results, 5) Feed full audio + results to Response Generator, 6) Stream spoken answer

- **Design tradeoffs**: Fixed-Interval vs Model-Triggered (plug-and-play vs resource-efficient), Query Similarity Heuristics (conservative vs aggressive), Negative Sampling Ratio (10% hyperparameter)

- **Failure signatures**: High latency with no accuracy gain (redundant queries), Low accuracy with "I don't know" (too many NO_QUERYs), System hangs after partial utterance (single-thread logic error)

- **First 3 experiments**: 1) Latency Ablation - Implement Fixed-Interval on baseline E2E SISO to measure raw latency savings, 2) Model-Triggered Post-training - Fine-tune on TriviaQA converted to audio, 3) Negative Sampling Ablation - Compare models with/without negative sampling on held-out set

## Open Questions the Paper Calls Out

### Open Question 1
What architectural or training modifications are necessary to close the accuracy gap between text and speech output modalities in retrieval-augmented dialogue systems? The paper identifies this persistent challenge but doesn't offer specific mechanisms to align speech generation performance with text generation capabilities.

### Open Question 2
How can end-to-end spoken dialogue systems be optimized to handle full volume of tool result references (e.g., 23K+ tokens) common in RAG without architectural failures? Current speech-in speech-out architectures appear to lack context window or stability required for dense text contexts.

### Open Question 3
Is Model-Triggered Streaming RAG sufficiently efficient for deployment on resource-constrained wearable devices? While theoretically more efficient, the paper doesn't quantify power consumption or compute load relative to wearable hardware constraints.

## Limitations
- Post-training pipeline effectiveness depends heavily on quality of pseudo ground truth query generation and similarity-based labeling strategy
- Narrow scope of tool integration focused only on web search and knowledge graph APIs
- Single-thread management could create bottlenecks with frequent query generation or variable tool response times

## Confidence

**High Confidence (4/5)**: The fundamental claim that parallel tool querying can reduce perceived latency in E2E SISO systems is well-supported with 20% latency reduction.

**Medium Confidence (3/5)**: Accuracy improvements and Model-Triggered effectiveness are supported by experiments but rely on post-training pipeline quality and AudioCRAG benchmark construction.

**Low Confidence (2/5)**: Claim of applicability to typed input is demonstrated only through brief mention without extensive validation.

## Next Checks

1. **Real-World Speech Robustness Test**: Evaluate on natural human speech with disfluencies and background noise to validate similarity-based labeling strategy beyond clean TTS speech.

2. **Tool Failure Simulation**: Systematically inject tool failures to assess single-thread management and error recovery mechanisms performance.

3. **Multi-Tool Ecosystem Stress Test**: Extend evaluation to diverse tool types to assess scalability and measure overhead of managing multiple concurrent tool queries.