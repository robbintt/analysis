---
ver: rpa2
title: 'FedAPM: Federated Learning via ADMM with Partial Model Personalization'
arxiv_id: '2506.04672'
source_url: https://arxiv.org/abs/2506.04672
tags:
- fedapm
- learning
- client
- personalization
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of client drift in federated
  learning when using partial model personalization for heterogeneous and multimodal
  data. The authors propose FedAPM, an ADMM-based framework that incorporates first-order
  and second-order proximal terms to mitigate client drift and stabilize training.
---

# FedAPM: Federated Learning via ADMM with Partial Model Personalization

## Quick Facts
- **arXiv ID:** 2506.04672
- **Source URL:** https://arxiv.org/abs/2506.04672
- **Authors:** Shengkun Zhu; Feiteng Nie; Jinshan Zeng; Sheng Wang; Yuan Sun; Yuan Yao; Shangfeng Chen; Quanqing Xu; Chuanhui Yang
- **Reference count:** 40
- **Primary result:** Achieves 12.3% accuracy, 16.4% F1, and 18.0% AUC improvements over state-of-the-art federated learning methods on heterogeneous and multimodal datasets

## Executive Summary
This paper addresses the challenge of client drift in federated learning when using partial model personalization for heterogeneous and multimodal data. The authors propose FedAPM, an ADMM-based framework that incorporates first-order and second-order proximal terms to mitigate client drift and stabilize training. FedAPM explicitly estimates Lagrange multipliers to avoid ill-conditioning issues associated with large penalty parameters in traditional penalty methods. Theoretical analysis establishes global convergence to stationary points with constant, linear, and sublinear rates under mild assumptions.

## Method Summary
FedAPM is an ADMM-based federated learning framework that addresses client drift through partial model personalization. The method introduces proximal terms in the augmented Lagrangian to stabilize training and avoid ill-conditioning issues from large penalty parameters. It explicitly estimates Lagrange multipliers and employs a multi-stage local update procedure where each client iteratively updates personalized models, local shared models, and dual variables. The framework supports input, output, and split input personalization modes for handling heterogeneous and multimodal data. Convergence is guaranteed to stationary points under mild assumptions, with theoretical rates spanning constant, linear, and sublinear depending on problem structure.

## Key Results
- Achieves average improvements of 12.3% in test accuracy, 16.4% in F1 score, and 18.0% in AUC compared to state-of-the-art methods
- Demonstrates faster convergence requiring fewer communication rounds than baseline approaches
- Shows robust performance across four heterogeneous and multimodal datasets (CIFAR10, CrisisMMD, KU-HAR, Crema-D)

## Why This Works (Mechanism)
The proximal terms in the augmented Lagrangian stabilize the optimization landscape by penalizing deviations from personalized models, effectively controlling client drift. By explicitly estimating Lagrange multipliers rather than embedding them in large penalty parameters, the method avoids ill-conditioning that typically plagues penalty-based approaches. The multi-stage local update procedure allows clients to first refine their personalized models before updating the shared component, creating a more stable optimization path that converges to stationary points.

## Foundational Learning
- **ADMM (Alternating Direction Method of Multipliers):** Why needed - provides a framework for decomposing complex optimization problems; Quick check - verify convergence properties for non-convex problems
- **Client Drift in FL:** Why needed - understanding how local training without coordination degrades global model quality; Quick check - measure divergence between local and global models over training
- **Partial Model Personalization:** Why needed - enables customization while maintaining shared knowledge across heterogeneous clients; Quick check - evaluate performance with different personalization modes
- **Proximal Methods:** Why needed - stabilize optimization by incorporating distance penalties to reference points; Quick check - confirm that proximal parameters satisfy theoretical bounds
- **Augmented Lagrangian Methods:** Why needed - combine penalty methods with multiplier updates for improved convergence; Quick check - monitor dual variable evolution for stability

## Architecture Onboarding
- **Component Map:** Client personalized models (v_i) -> Local shared models (u_i) -> Global shared model (u) -> Lagrange multipliers (π_i)
- **Critical Path:** Local update (v_i, u_i, π_i) -> Server aggregation (u) -> Dual update (π_i) -> Repeat
- **Design Tradeoffs:** Proximal terms add computational overhead but stabilize convergence; explicit Lagrange multiplier estimation avoids ill-conditioning but requires careful tuning; multi-stage updates improve stability at cost of additional iterations
- **Failure Signatures:** Non-convergence occurs when penalty parameter ρ is too small for high heterogeneity; ill-conditioning manifests as violent loss fluctuations when ρ is too large relative to proximal parameters
- **First Experiments:** 1) Test convergence with varying ρ values to identify stability threshold; 2) Compare performance across personalization modes (input, output, split) on CIFAR10; 3) Monitor Lagrange multiplier norms to verify boundedness

## Open Questions the Paper Calls Out
None

## Limitations
- Requires careful tuning of proximal parameter σ_i and accuracy level ξ_i, which are not fully specified in experimental ranges
- Theoretical convergence guarantees depend on assumptions about Lipschitz continuity and convexity that may not hold perfectly for multimodal datasets
- Performance sensitivity to Dirichlet concentration parameter α for non-IID data generation is not fully explored

## Confidence
- **High Confidence:** Theoretical convergence analysis establishing stationary point guarantees under mild assumptions, and the general algorithmic framework using ADMM with proximal terms
- **Medium Confidence:** Reported performance improvements (12.3% accuracy, 16.4% F1, 18.0% AUC) over baselines, given that hyperparameter ranges for learning rate and penalty parameter are specified
- **Low Confidence:** Exact reproduction of convergence behavior and final performance metrics due to unspecified hyperparameters σ_i, ξ_i, μ_i, and α

## Next Checks
1. **Hyperparameter Sensitivity Analysis:** Systematically vary σ_i across multiple orders of magnitude (e.g., 0.01, 0.1, 1.0) while monitoring convergence stability and final performance to identify optimal ranges
2. **Data Heterogeneity Impact:** Experiment with different Dirichlet concentration parameters α (e.g., 0.1, 1.0, 10.0) to assess how data heterogeneity affects FedAPM's performance advantage over baselines
3. **Dual Variable Behavior:** Track the norm of Lagrange multipliers π_i across training rounds to verify that they remain bounded and do not grow unboundedly, which would indicate ill-conditioning issues not fully addressed by the theoretical analysis