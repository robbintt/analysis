---
ver: rpa2
title: 'MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal
  Critique'
arxiv_id: '2511.09067'
source_url: https://arxiv.org/abs/2511.09067
tags:
- critique
- response
- score
- quality
- lmms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MM-CRITIC, a comprehensive benchmark for
  evaluating the multimodal critique capabilities of large language models across
  three dimensions: basic correctness critique, correction critique, and comparative
  critique. The benchmark covers 8 task types with over 500 tasks and 4,471 samples,
  using expert-informed scoring rubrics and reference critiques to guide evaluation
  by GPT-4o.'
---

# MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique

## Quick Facts
- arXiv ID: 2511.09067
- Source URL: https://arxiv.org/abs/2511.09067
- Reference count: 10
- Key outcome: MM-CRITIC benchmark evaluates LMM critique capabilities across basic, correction, and comparative dimensions, revealing scaling trends and length bias in judge models

## Executive Summary
This paper introduces MM-CRITIC, a comprehensive benchmark designed to evaluate large multimodal models' (LMMs) capabilities as multimodal critics. The benchmark assesses three dimensions of critique: basic correctness (identifying errors), correction (suggesting fixes), and comparative (comparing multiple responses). Using over 4,471 samples across 8 task types sourced from MEGA-BENCH, the evaluation employs GPT-4.1 as a judge model to score LMM-generated critiques against reference critiques. The study reveals that larger models generally achieve higher critique performance, with scaling laws observed within model families. Notably, correction critiques are more challenging than basic critiques, and medium-quality responses are the most difficult to critique. The work also identifies a length bias where judge models tend to favor longer textual critiques regardless of accuracy.

## Method Summary
MM-CRITIC evaluates LMMs' multimodal critique capabilities through a three-stage pipeline. First, responses are generated using diverse LMMs across 8 task types (coding, math, reasoning, etc.) from the MEGA-BENCH dataset. Second, reference critiques are created using GPT-4o with expert-informed rubrics, anchored at a quality score of 8. Third, target LMMs act as critics using specific prompt templates, and their outputs are evaluated by GPT-4.1 (the judge model) against the reference critiques. The evaluation produces three metrics: Critique Accuracy (binary), Preference Accuracy (pairwise), and Critique Score (0-10 scale). The benchmark covers three critique dimensions: basic correctness critique, correction critique, and comparative critique, with tasks including image analysis, response evaluation, and multi-response comparison.

## Key Results
- Larger models generally achieve higher critique performance, with scaling laws observed within model families
- Correction critique scores are lower than basic critique scores, indicating higher difficulty
- Medium-quality responses are the most challenging to critique, producing the lowest scores among quality groups
- Judge models exhibit length bias, favoring longer textual critiques regardless of semantic accuracy

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its multi-dimensional evaluation framework that captures different aspects of critique ability. By using a judge model (GPT-4.1) to compare model-generated critiques against reference critiques, the evaluation creates a standardized scoring mechanism. The three critique dimensions (basic, correction, comparative) provide comprehensive coverage of critique capabilities, while the expert-informed rubrics ensure evaluation quality. The reference critiques anchored at a quality score of 8 serve as consistent ground truth for comparison, enabling quantitative assessment of model performance across different task types and quality levels.

## Foundational Learning
- **Multimodal critique evaluation**: The ability to assess whether an LMM can identify, explain, and correct errors in multimodal responses is crucial for understanding model reliability and reasoning capabilities
  - Why needed: As LMMs become more autonomous, their ability to self-evaluate and critique is essential for safe deployment
  - Quick check: Verify the benchmark covers all three critique dimensions across multiple task types

- **Reference critique generation**: Using AI models to create evaluation standards introduces both efficiency and potential bias
  - Why needed: Manual critique annotation would be prohibitively expensive for large-scale evaluation
  - Quick check: Examine the consistency of reference critiques across similar samples

- **Judge model scoring**: The use of GPT-4.1 to evaluate critiques creates a scalable but potentially biased evaluation mechanism
  - Why needed: Human evaluation is too slow for comprehensive benchmarking of multiple models
  - Quick check: Analyze correlation between critique length and scores to identify length bias

## Architecture Onboarding

**Component Map**: MEGA-BENCH dataset -> Response generation -> Reference critique generation -> Critique prompt generation -> LMM critique generation -> Judge model evaluation -> Score calculation

**Critical Path**: Dataset preparation → Response generation → Reference critique creation → Critique generation → Judge evaluation → Result aggregation

**Design Tradeoffs**: The use of AI-generated reference critiques enables scalability but may introduce bias; judge model evaluation provides consistency but shows length bias; the three-dimensional framework captures comprehensive critique abilities but increases evaluation complexity

**Failure Signatures**: Length bias where longer critiques receive higher scores regardless of accuracy; performance drop on medium-quality responses suggesting difficulty with partial correctness; scaling trends that may not generalize beyond model families

**First 3 Experiments**:
1. Run inference on a small subset (100 samples) to verify prompt templates work correctly
2. Test judge model scoring consistency by evaluating the same critique multiple times
3. Analyze correlation between critique length and scores on a sample subset

## Open Questions the Paper Calls Out
### Open Question 1
How does the evaluation of critique capabilities generalize to temporal modalities like video or audio, where error identification requires tracking continuity and change over time?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that MM-CRITIC "currently focuses only on text and image modalities, lacking broader evaluation across other multimodal domains such as video, audio, and 3D data."
- Why unresolved: The current benchmark is restricted to static images; thus, the ability of LMMs to critique dynamic or temporal reasoning (e.g., physical plausibility in video) remains unmeasured
- What evidence would resolve it: Results from an extension of the benchmark to video/audio tasks, specifically analyzing if the scaling laws and "correction critique" difficulty hold true in temporal domains

### Open Question 2
Can the evaluation bias where judge models favor longer, more elaborate critiques be mitigated to ensure scoring reflects semantic accuracy rather than verbosity?
- Basis in paper: [explicit] The analysis reveals that "the critique score appears to be positively correlated with the length of the textual critique," and "the judge model tends to assign higher scores to such responses" (Figure 4)
- Why unresolved: While the paper identifies this length bias as a factor contributing to evaluation discrepancies (e.g., Qwen2.5-vl-32b outperforming the 72b model), it does not propose or test methods to normalize for text richness
- What evidence would resolve it: Ablation studies using length-controlled constraints or normalized scoring prompts that show stable evaluation metrics independent of the word count of the generated critique

### Open Question 3
What specific features of "medium-quality" responses cause them to be the most difficult for LMMs to critique, resulting in lower scores than for low or high-quality responses?
- Basis in paper: [explicit] The paper notes that "medium-quality responses are the most challenging, producing the lowest critique scores among the three groups," suggesting unique difficulties in handling partial correctness
- Why unresolved: The paper observes the "intriguing insight" and the correlation, but does not deeply investigate the mechanistic reasons (e.g., ambiguity in visual grounding vs. reasoning errors) behind this U-shaped performance curve
- What evidence would resolve it: Fine-grained error analysis on medium-quality samples to determine if failures are driven by ambiguous visual features or the model's inability to discern partial logic

## Limitations
- Benchmark focuses only on text and image modalities, lacking evaluation of video, audio, and 3D data
- Reliance on GPT-4.1 as both reference generator and judge may introduce systematic bias
- Length bias in judge models may reward verbosity over semantic accuracy in critiques
- Benchmark covers primarily English-language tasks and common benchmark domains

## Confidence
- **High confidence**: Core methodology of creating critique evaluation benchmark with three dimensions; observed scaling trends within model families; identification of medium-quality responses as most challenging
- **Medium confidence**: Specific numerical results and absolute score values; generalizability of findings to non-benchmark scenarios
- **Low confidence**: Claims about model critique capabilities in real-world applications without additional validation

## Next Checks
1. Conduct human evaluation on a subset of critiques to validate judge model scoring and identify systematic biases
2. Test benchmark with domain-specific multimodal models (medical, legal, technical) to assess generalizability
3. Perform ablation studies on reference critique generation to quantify impact of AI-generated vs human-annotated standards on final scores