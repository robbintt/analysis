---
ver: rpa2
title: Multi-Domain Explainability of Preferences
arxiv_id: '2505.20088'
source_url: https://arxiv.org/abs/2505.20088
tags:
- concepts
- response
- user
- concept
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work proposes a fully automated method for generating interpretable,
  multi-domain concept-based explanations of preference mechanisms, such as human
  preferences, LLM-as-a-Judge (LaaJ), and reward models. The method involves four
  stages: (1) using an LLM to discover concepts that distinguish chosen from rejected
  responses per domain; (2) representing each query-response triplet as a concept
  vector; (3) training a white-box Hierarchical Multi-Domain Regression (HMDR) model
  to predict preferences; and (4) analyzing model weights to derive local and global
  explanations.'
---

# Multi-Domain Explainability of Preferences

## Quick Facts
- arXiv ID: 2505.20088
- Source URL: https://arxiv.org/abs/2505.20088
- Reference count: 40
- This work proposes a fully automated method for generating interpretable, multi-domain concept-based explanations of preference mechanisms

## Executive Summary
This work introduces a novel approach to explain preference mechanisms across multiple domains by generating interpretable concept-based explanations. The method automatically discovers domain-specific concepts that distinguish chosen from rejected responses, represents data as concept vectors, and trains a white-box Hierarchical Multi-Domain Regression model to predict preferences while enabling interpretability. The approach achieves competitive prediction accuracy with black-box alternatives while providing both local and global explanations that can be applied to improve response selection and tie-breaking.

## Method Summary
The method consists of four stages: (1) LLM-based concept discovery to identify concepts that distinguish chosen from rejected responses in each domain, (2) conversion of query-response triplets into concept vectors, (3) training of a white-box HMDR model to predict preferences with explicit out-of-domain generalization optimization, and (4) analysis of model weights to derive interpretable explanations. The HMDR architecture captures both shared and domain-specific effects through hierarchical modeling, enabling the method to handle diverse preference mechanisms including human preferences, LLM-as-a-Judge systems, and reward models.

## Key Results
- Achieves ~66% accuracy for human preferences and ~80% average for LaaJs and RMs, comparable to black-box alternatives
- Explanation-guided responses are preferred by judges in the Judge Hack task
- Tie-breaking using concept-based prompts improves alignment with human preferences in the Tie Break task

## Why This Works (Mechanism)
The method works by leveraging LLM-generated concepts as interpretable features that capture the essence of preference mechanisms across domains. The hierarchical modeling structure allows the system to learn both domain-specific and shared preference patterns, while the white-box nature of the HMDR model enables transparent analysis of decision-making processes through weight examination.

## Foundational Learning
- LLM concept discovery - Needed to automatically identify interpretable features distinguishing preferences; Quick check: Validate concept relevance through human evaluation
- Concept vector representation - Needed to transform raw data into structured features for modeling; Quick check: Test vector quality through ablation studies
- Hierarchical Multi-Domain Regression - Needed to capture both shared and domain-specific effects; Quick check: Validate hierarchical structure through cross-domain performance
- Out-of-domain optimization - Needed to ensure generalization beyond training domains; Quick check: Test on completely novel domains
- Weight-based explanation analysis - Needed to extract interpretable insights from model decisions; Quick check: Verify explanation faithfulness through application tasks

## Architecture Onboarding

**Component Map:** LLM Concept Discovery -> Concept Vector Creation -> HMDR Training -> Explanation Extraction

**Critical Path:** Concept discovery (LLM) → vector representation → HMDR prediction → weight analysis for explanations

**Design Tradeoffs:** White-box interpretability vs potential accuracy loss compared to black-box models; automated concept discovery vs manual curation control

**Failure Signatures:** Poor concept quality leading to inaccurate predictions; overfitting to training domains reducing out-of-domain performance; insufficient hierarchical structure failing to capture domain differences

**First 3 Experiments to Run:**
1. Compare preference prediction accuracy across all twelve preference mechanisms to establish baseline performance
2. Evaluate Judge Hack task with explanation-guided responses versus baseline responses
3. Test Tie Break task performance with concept-based prompts on domain-specific tie scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Concept discovery relies heavily on LLM-generated explanations, potentially introducing bias
- Limited validation of interpretability claims beyond application-specific tasks
- Performance gap between in-domain and out-of-domain generalization suggests unresolved domain-specific challenges

## Confidence
- Preference prediction accuracy: High
- Concept-based explanations: Medium
- Application-driven evaluations: Medium
- Generalizability across domains: Low

## Next Checks
1. Conduct human evaluation studies with domain experts to validate that discovered concepts accurately represent preference mechanisms
2. Perform ablation studies systematically removing or modifying concept discovery parameters
3. Test the method on completely novel domains not seen during training or concept discovery