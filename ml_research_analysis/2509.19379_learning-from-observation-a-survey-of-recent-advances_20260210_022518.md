---
ver: rpa2
title: 'Learning from Observation: A Survey of Recent Advances'
arxiv_id: '2509.19379'
source_url: https://arxiv.org/abs/2509.19379
tags:
- expert
- learning
- imitator
- state
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys recent advances in learning from observation
  (LfO), where agents learn to imitate expert behavior using only state information,
  without requiring action data. The authors present a novel taxonomy categorizing
  LfO methods based on expert identity, trajectory collection methods (first-person
  vs.
---

# Learning from Observation: A Survey of Recent Advances

## Quick Facts
- **arXiv ID**: 2509.19379
- **Source URL**: https://arxiv.org/abs/2509.19379
- **Reference count**: 40
- **Primary result**: Presents a novel taxonomy categorizing LfO methods based on expert identity, viewpoint, and algorithmic design choices into four main categories.

## Executive Summary
This survey comprehensively categorizes recent advances in Learning from Observation (LfO), where agents learn to imitate expert behavior using only state information without requiring action data. The authors present a novel taxonomy organizing LfO approaches into four main categories: supervised methods, goal-based methods, reward engineering, and distribution matching. They connect LfO to related fields like offline RL and hierarchical RL while identifying key open problems including learning from diverse expert skills, third-person imitation, and developing better evaluation metrics for imitation performance.

## Method Summary
The paper surveys and taxonomizes the field of Learning from Observation (LfO), where agents imitate experts using state information only. The core methodology involves formalizing LfO objectives mathematically, such as minimizing the divergence between state-transition occupancy measures μ^π(s, s') || μ^E(s, s') or using inverse dynamics models to reconstruct actions. The survey analyzes 40 referenced papers and standard RL definitions to create a classification framework based on expert identity, trajectory collection methods, dataset types, and algorithmic design choices.

## Key Results
- LfO methods can be organized into four main categories: supervised methods, goal-based methods, reward engineering, and distribution matching
- The choice of method depends critically on whether the expert and imitator share the same dynamics and viewpoint
- Current evaluation metrics focus on task return rather than behavioral fidelity, representing a significant gap in the field
- Open problems include learning from diverse expert skills, third-person imitation, and developing better evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1: Inverse Dynamics Action Inference
If an agent can predict the action required to transition between two observed states, it can convert state-only demonstrations into state-action pairs for standard imitation. The system learns an inverse dynamics model IDM(s_t, s_{t+1}) → a_t by interacting with the environment. When presented with an expert state trajectory (s_0, s_1, ...), the model infers the missing action labels. A policy is then trained via supervised learning (Behavioral Cloning) on these reconstructed pairs. This works when the imitator shares the same dynamics as the expert, or when state transitions observed in the expert trajectory are physically realizable by the imitator.

### Mechanism 2: State-Transition Occupancy Matching
Matching the distribution of state transitions between expert and imitator induces behavioral similarity without requiring action labels. Formulated as an adversarial game (GAIfO), a discriminator is trained to distinguish between expert state-transitions (s, s')_E and imitator transitions (s, s')_I. The imitator policy is rewarded for generating transitions that fool the discriminator, effectively minimizing the divergence between the stationary distributions μ^π(s, s') and μ^E(s, s'). This works when the optimal imitation policy is uniquely determined by the sequence of states visited, ignoring the specific actions taken to achieve them.

### Mechanism 3: Hierarchical Goal Decomposition
Decoupling the "intent" (goal selection) from the "execution" (reaching the goal) allows an imitator to mimic experts with different dynamics. The policy is split into a high-level meta-policy (goal predictor) and a low-level controller. The meta-policy selects a target state s_goal from the expert demonstration. The low-level policy, which is task-agnostic and trained to reach states, executes the actions to achieve s_goal. This works when the low-level controller is sufficiently robust to reach feasible states, and the high-level policy can identify goals that are achievable by the imitator despite dynamics differences.

## Foundational Learning

- **Concept: Occupancy Measure (μ^π)**
  - Why needed: LfO algorithms like GAIfO optimize the probability distribution of visiting states or transitions rather than step-by-step rewards
  - Quick check: Can you explain why matching the occupancy measure is different from matching the trajectory step-by-step?

- **Concept: Inverse Dynamics Models**
  - Why needed: This is the bridge from Observation (States) to Demonstration (Actions) without which an LfO agent cannot ground "what happened" to "what to do"
  - Quick check: If you observe a robot arm moving from A to B, what information is required to predict the torque command that caused that movement?

- **Concept: GANs (Generative Adversarial Networks)**
  - Why needed: The most successful LfO methods frame imitation as a density estimation problem solved via a minimax game between a generator (policy) and discriminator (reward function)
  - Quick check: In the context of LfO, what corresponds to the "generator" and what corresponds to the "real data"?

## Architecture Onboarding

- **Component map**: Expert Dataset → Preprocessing → Choice of Objective → Training Loop → Optimized Policy
- **Critical path**:
  1. Data Preprocessing: Ensure expert states s_E and imitator states s_I are aligned (handle third-person view vs. first-person view alignment here)
  2. Choice of Objective: Select mechanism based on expert identity. If Expert = Imitator, use Inverse Dynamics. If Expert ≠ Imitator, use Distribution Matching
  3. Training Loop: Alternate between collecting imitator samples (s, s')_I and updating the discriminator/reward signal
- **Design tradeoffs**:
  - Supervised (BCO) vs. Adversarial (GAIfO): BCO is sample-efficient but brittle to dynamics mismatch. GAIfO is robust to dynamics but computationally expensive and unstable to train
  - Online vs. Offline: Online LfO allows learning dynamics during training; Offline LfO requires separate dataset of imitator transitions
- **Failure signatures**:
  - State Collapse (Supervised): Imitator visits states not in expert dataset and hallucinates sub-optimal actions
  - Mode Collapse (Adversarial): Imitator repeats specific successful state transition endlessly to fool discriminator
  - Dynamics Mismatch Error: Imitator tries to reach expert state physically impossible given its own morphology
- **First 3 experiments**:
  1. Sanity Check (BCO): Implement Behavioral Cloning from Observation using Inverse Dynamics Model on HalfCheetah where expert = imitator. Verify if state trajectories match
  2. Dynamics Shift (GAIfO): Train GAIfO where expert has different physical parameters than imitator. Compare performance against BCO baseline
  3. Third-Person View: Implement domain adaptation module to learn policy from video of human performing task, mapped to robot simulation

## Open Questions the Paper Calls Out

### Open Question 1
How can the field develop robust evaluation metrics that specifically measure the fidelity of imitation (behavioral similarity) rather than just task return, given that an imitator might solve a task using completely different behaviors than the expert? The paper notes most studies measure performance by return, which doesn't reflect true imitation if agent achieves similar returns through divergent behaviors.

### Open Question 2
How can LfO algorithms be adapted to learn effectively from uncurated, third-person internet-scale videos (e.g., YouTube) where camera angles vary significantly and demonstrations are not temporally aligned? Current domain adaptation techniques often rely on paired proxy data or mutual information constraints that may fail under extreme viewpoint diversity found in passive internet video data.

### Open Question 3
How can LfO frameworks be modified to prioritize adherence to safety constraints over replication of expert behavior, particularly when expert's actions are risky or infeasible for the imitator? Standard LfO objectives maximize alignment with expert state distributions; integrating safety constraints without access to expert actions remains an open algorithmic challenge.

### Open Question 4
Can model-based LfO methods improve sample efficiency by utilizing learned dynamics models as planners rather than just using them to generate synthetic samples? While model-based RL has succeeded via planning, LfO methods typically use models only as proxies or regularizers. Planning in LfO requires predicting future states and reasoning about trajectories without explicit action labels.

## Limitations
- The taxonomy provides useful organizing framework but doesn't validate whether categories are mutually exclusive or exhaustive
- Significant overlap exists between categories as many algorithms combine two or more objectives
- Empirical evaluation focuses on algorithmic performance comparisons rather than validating taxonomy's predictive power for understanding new LfO methods

## Confidence

- **High confidence**: Mathematical formulations of core mechanisms (inverse dynamics, occupancy measure matching, hierarchical decomposition) are clearly defined and grounded in established RL theory
- **Medium confidence**: Taxonomy's practical utility for researchers navigating LfO landscape is reasonable but untested for new algorithm classification
- **Low confidence**: Claims about which category combinations are most effective lack empirical support, as survey focuses on classification rather than comparative performance analysis

## Next Checks

1. **Taxonomy test case**: Apply four-category framework to recently published LfO algorithm (2024) and document whether it fits cleanly into one category or requires hybrid classification
2. **Empirical replication**: Reproduce BCO vs. GAIfO comparison using inverse dynamics + behavioral cloning pipeline against adversarial occupancy matching approach, measuring both performance and training stability
3. **Cross-dataset generalization**: Test whether LfO methods trained on one expert morphology (e.g., humanoid) can effectively imitate demonstrations from different morphology (e.g., quadruped), validating claims about dynamics mismatch challenges