---
ver: rpa2
title: Holistic Audit Dataset Generation for LLM Unlearning via Knowledge Graph Traversal
  and Redundancy Removal
arxiv_id: '2502.18810'
source_url: https://arxiv.org/abs/2502.18810
tags:
- knowledge
- unlearning
- audit
- arxiv
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating Large Language
  Model (LLM) unlearning effectiveness, which is crucial for removing sensitive information
  while maintaining model utility. Existing benchmarks are limited in scale and comprehensiveness,
  typically containing only a few hundred test cases.
---

# Holistic Audit Dataset Generation for LLM Unlearning via Knowledge Graph Traversal and Redundancy Removal

## Quick Facts
- arXiv ID: 2502.18810
- Source URL: https://arxiv.org/abs/2502.18810
- Reference count: 38
- Generates over 69,000-111,000 audit cases, identifying thousands of previously undetected knowledge memorization instances

## Executive Summary
This paper addresses the critical challenge of evaluating Large Language Model (LLM) unlearning effectiveness by proposing HANKER, an automated framework that generates comprehensive audit datasets. Current benchmarks are limited in scale, typically containing only a few hundred test cases, which constrains the evaluation of unlearning methods. HANKER leverages knowledge graphs (KGs) to transform unstructured text into structured representations, removes redundant knowledge between forget and retain datasets, and generates targeted test questions guided by specific facts, ensuring fine-grained coverage and eliminating knowledge redundancy.

When applied to the MUSE benchmark, HANKER generated over 69,000 and 111,000 audit cases for News and Books datasets respectively, identifying thousands of knowledge memorization instances previously undetected. The empirical analysis revealed that knowledge redundancy significantly skews unlearning effectiveness metrics, artificially inflating ROUGE scores from 19.7% to 26.1% and Entailment Scores from 32.4% to 35.2%, highlighting the necessity of systematic deduplication for accurate assessment.

## Method Summary
HANKER addresses the limitations of existing LLM unlearning benchmarks by providing an automated framework that generates large-scale, comprehensive audit datasets. The approach transforms unstructured text into structured knowledge graphs using REBEL-large for knowledge extraction, then removes redundant knowledge between forget and retain datasets through exact triple matching. Finally, it generates targeted test questions guided by specific facts from the knowledge graphs. This methodology ensures fine-grained coverage of unlearning scenarios while eliminating the bias introduced by overlapping knowledge between datasets, providing a more accurate assessment of unlearning effectiveness.

## Key Results
- Generated over 69,000 audit cases for News dataset and 111,000 for Books dataset from MUSE benchmark
- Identified thousands of knowledge memorization instances previously undetected by existing methods
- Demonstrated that knowledge redundancy artificially inflates unlearning metrics: ROUGE scores increase from 19.7% to 26.1% and Entailment Scores from 32.4% to 35.2%
- Showed that systematic deduplication is essential for accurate unlearning assessment

## Why This Works (Mechanism)
The effectiveness of HANKER stems from its ability to systematically capture and evaluate all knowledge relationships within the training data. By transforming unstructured text into structured knowledge graphs, the framework can precisely identify which facts should be forgotten versus retained, eliminating the ambiguity that plagues traditional evaluation methods. The redundancy removal stage ensures that evaluation metrics reflect genuine unlearning rather than the model's ability to handle overlapping information. This comprehensive approach provides fine-grained coverage of unlearning scenarios that existing benchmarks cannot achieve due to their limited scale.

## Foundational Learning
**Knowledge Graph Construction** - Converting unstructured text into structured representations using REBEL-large. Why needed: Enables precise identification of facts that should be forgotten versus retained. Quick check: Verify that extracted triples accurately represent the source text's factual content.

**Redundancy Detection** - Identifying and removing overlapping knowledge between forget and retain datasets through exact triple matching. Why needed: Prevents artificial inflation of unlearning metrics by eliminating shared information. Quick check: Confirm that no semantically identical triples remain across both datasets.

**Question Generation from KG Facts** - Creating targeted test questions based on specific knowledge graph facts. Why needed: Ensures evaluation covers all relevant unlearning scenarios with fine-grained precision. Quick check: Validate that generated questions accurately test the corresponding KG facts.

## Architecture Onboarding

**Component Map**: Unstructured Text -> Knowledge Graph Construction -> Redundancy Removal -> Question Generation -> Audit Dataset

**Critical Path**: The most critical path is from Knowledge Graph Construction through Redundancy Removal to Question Generation, as errors in any of these stages propagate through the entire pipeline and compromise the quality of the audit dataset.

**Design Tradeoffs**: HANKER prioritizes precision over recall in knowledge extraction to minimize false positives in unlearning evaluation, accepting that some relevant facts might be missed rather than risk including incorrect ones. The framework also trades computational efficiency for comprehensiveness by generating exhaustive audit cases rather than sampling.

**Failure Signatures**: 
- Low extraction accuracy leads to sparse knowledge graphs and incomplete audit coverage
- Incomplete redundancy removal results in artificially inflated unlearning metrics
- Poor question generation produces ambiguous or irrelevant test cases
- Knowledge graph traversal missing key relationships results in blind spots in unlearning evaluation

**First Experiments**:
1. Apply HANKER to a small, well-defined dataset with known facts to verify the complete pipeline works correctly
2. Compare ROUGE scores with and without redundancy removal on a sample dataset to quantify the impact
3. Validate generated questions against human-annotated ground truth to assess question quality

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Currently validated only on the MUSE benchmark's News and Books subsets, limiting generalizability to other unlearning benchmarks like TOFU or Unlearn-Bench
- Exact triple matching for redundancy removal may miss semantic nuances where identical facts have different contextual meanings
- Framework's effectiveness depends on knowledge extraction accuracy, which varies significantly across domains (0.76 for News vs 0.61 for Books)

## Confidence
- **High Confidence**: The methodology for generating audit datasets through KG traversal and redundancy removal is technically sound and well-implemented
- **Medium Confidence**: The claim that existing benchmarks are fundamentally limited due to their small scale is supported by the data
- **Medium Confidence**: The assertion that HANKER provides "holistic" coverage of unlearning scenarios requires additional verification across diverse domains

## Next Checks
1. Conduct cross-domain validation by applying HANKER to scientific literature and technical documentation to verify the framework's generalizability beyond News and Books datasets
2. Perform ablation studies to quantify the individual contributions of KG traversal versus redundancy removal to the overall effectiveness of generated audit datasets
3. Implement human evaluation studies to assess whether the generated audit cases truly capture meaningful unlearning scenarios that reflect real-world concerns about information removal