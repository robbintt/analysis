---
ver: rpa2
title: 'SlotFM: A Motion Foundation Model with Slot Attention for Diverse Downstream
  Tasks'
arxiv_id: '2509.21673'
source_url: https://arxiv.org/abs/2509.21673
tags:
- tasks
- each
- slot
- signal
- foundation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SlotFM, a motion foundation model for wearable
  accelerometer data that generalizes across diverse downstream tasks. The core method,
  Time-Frequency Slot Attention, extends Slot Attention to process both time and frequency
  representations of raw signals, generating multiple slot-based embeddings that capture
  distinct signal components.
---

# SlotFM: A Motion Foundation Model with Slot Attention for Diverse Downstream Tasks

## Quick Facts
- arXiv ID: 2509.21673
- Source URL: https://arxiv.org/abs/2509.21673
- Reference count: 18
- Key outcome: SlotFM outperforms existing self-supervised approaches on 13 out of 16 tasks, achieving an average 4.5% performance gain

## Executive Summary
SlotFM introduces a motion foundation model for wearable accelerometer data that leverages Time-Frequency Slot Attention to generalize across diverse downstream tasks. The model generates multiple slot-based embeddings capturing distinct signal components through processing both time and frequency representations. Evaluated on 16 tasks spanning classification and regression, SlotFM demonstrates significant performance improvements over existing self-supervised approaches and even surpasses fully supervised models in some cases. The approach maintains consistent performance across tasks and shows promise for multi-task inference in resource-constrained wearable devices.

## Method Summary
SlotFM employs Time-Frequency Slot Attention to extend Slot Attention for processing raw accelerometer signals by incorporating both time and frequency representations. The model uses two loss regularizers - SSIM for structural patterns and MS-STFT for high-frequency details - to improve reconstruction quality. During pre-training, the model learns to reconstruct input signals while generating slot-based embeddings that capture different aspects of the motion data. For downstream tasks, task-specific heads attend to different slots, enabling the model to adapt its embeddings to various applications ranging from gesture recognition to sports activity classification.

## Key Results
- Outperformed existing self-supervised approaches on 13 out of 16 evaluated tasks
- Achieved an average 4.5% performance gain across all tasks
- Surpassed fully supervised models in some cases while maintaining consistent performance across diverse applications

## Why This Works (Mechanism)
The effectiveness of SlotFM stems from its ability to capture both temporal and frequency domain information simultaneously through Time-Frequency Slot Attention. By generating multiple slot-based embeddings, the model can represent different signal components that may be relevant for different downstream tasks. The SSIM and MS-STFT loss regularizers ensure that the model preserves both structural patterns and high-frequency details during reconstruction, leading to more informative embeddings. The slot attention mechanism allows the model to focus on different aspects of the signal depending on the task, making it adaptable to various applications without requiring task-specific modifications to the core architecture.

## Foundational Learning
1. **Time-Frequency Analysis**: Understanding how signals can be represented in both time and frequency domains simultaneously - needed for processing wearable accelerometer data; quick check: verify understanding of STFT and its applications
2. **Self-Supervised Learning**: Learning representations without explicit labels - needed for pre-training on large unlabeled datasets; quick check: confirm knowledge of contrastive learning vs reconstruction-based approaches
3. **Attention Mechanisms**: Dynamic weighting of input features - needed for focusing on relevant signal components; quick check: review standard attention vs slot attention differences
4. **Multi-Task Learning**: Training models to perform well across diverse tasks - needed for foundation model generalization; quick check: understand task-specific head architectures
5. **Signal Reconstruction**: Learning to recreate input signals - needed for pre-training objectives; quick check: verify understanding of reconstruction loss functions
6. **Embedding Transfer**: Applying pre-trained representations to new tasks - needed for downstream fine-tuning; quick check: confirm knowledge of few-shot vs full fine-tuning approaches

## Architecture Onboarding

Component Map: Raw Signal -> Time-Frequency Encoding -> Slot Attention -> Multiple Slots -> Reconstruction + Downstream Heads

Critical Path: Input signal undergoes STFT conversion, passes through Time-Frequency Slot Attention to generate slots, which are then used for both reconstruction (pre-training) and task-specific predictions (fine-tuning).

Design Tradeoffs: The model balances between capturing fine-grained temporal details and broader frequency patterns, while maintaining computational efficiency through the slot attention mechanism. The choice of two loss functions addresses different aspects of signal reconstruction but adds complexity to the training process.

Failure Signatures: Poor performance may arise from insufficient slot diversity (slots capture similar information), inadequate frequency resolution in the STFT, or mismatch between pre-training and downstream task distributions. The model may also struggle with signals that have non-stationary characteristics or require long-term temporal dependencies.

First Experiments:
1. Validate the Time-Frequency Slot Attention mechanism on a simple synthetic dataset with known frequency components
2. Test reconstruction quality with different combinations of the SSIM and MS-STFT loss functions
3. Evaluate slot diversity by clustering slot embeddings and measuring inter-slot similarity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Time-Frequency Slot Attention be effectively adapted for other wearable time-series modalities such as gyroscopes or PPG?
- Basis in paper: [explicit] The authors state, "future work could investigate extending our approach to these modalities," noting that sensors like gyroscopes also contain temporal and frequency information.
- Why unresolved: SlotFM is currently validated exclusively on accelerometer data.
- Evidence: Comparative benchmarks of SlotFM trained on multimodal datasets against unimodal baselines on relevant tasks (e.g., pose estimation or stress detection).

### Open Question 2
- Question: Does using slot vectors as tokens for a larger Transformer model outperform the current MLP concatenation approach?
- Basis in paper: [explicit] The paper suggests, "slots could be used as tokens for a larger model, allowing more dynamic interactions between them."
- Why unresolved: The current architecture processes slots via a single self-attention layer followed by flattening and concatenation.
- Evidence: Ablation studies replacing the MLP head with a deeper Transformer decoder to capture complex inter-slot dependencies.

### Open Question 3
- Question: Can learning task-specific initial slot vectors improve performance compared to the fixed global initialization used during pre-training?
- Basis in paper: [explicit] The authors propose "learning an initial slot vector specific to each task that acts as a query for that task alone" as a direction for future work.
- Why unresolved: SlotFM currently relies on fixed learnable parameters for slot initialization to ensure consistency across forward passes.
- Evidence: Experiments fine-tuning the initial slot vectors for specific downstream tasks (e.g., high-frequency gesture recognition) versus freezing them.

## Limitations
- Limited validation to accelerometer data only, with no testing on other wearable modalities like gyroscopes or PPG
- No empirical assessment of computational overhead for edge deployment or inference efficiency
- Potential performance degradation with distribution shifts in real-world deployment scenarios (sensor placement, user demographics)

## Confidence
- **High**: Performance gains over baseline self-supervised methods on 16 tasks (supported by quantitative results and statistical comparisons)
- **Medium**: Claims of surpassing fully supervised models in some cases (depends on specific task and dataset configurations not fully detailed)
- **Low**: Broader claim of potential for resource-constrained multi-task inference (lacks empirical validation of deployment efficiency or real-world robustness)

## Next Checks
1. Test SlotFM's performance on accelerometer datasets from domains not included in the training corpus (e.g., healthcare monitoring, industrial motion tracking) to assess true generalization.
2. Measure the inference latency and memory footprint of SlotFM with multiple slot-based embeddings on representative edge hardware to evaluate deployment feasibility.
3. Conduct an ablation study varying the number of slots and alternative architectural designs (e.g., different attention mechanisms) to confirm the optimality of the proposed Time-Frequency Slot Attention.