---
ver: rpa2
title: Probing Knowledge Holes in Unlearned LLMs
arxiv_id: '2511.00030'
source_url: https://arxiv.org/abs/2511.00030
tags:
- knowledge
- unlearning
- test
- table
- cases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for probing unintended knowledge
  loss in unlearned large language models (LLMs). While current unlearning methods
  effectively remove targeted harmful content, the authors find that they can inadvertently
  create "knowledge holes" - significant losses of benign knowledge that standard
  benchmarks fail to detect.
---

# Probing Knowledge Holes in Unlearned LLMs

## Quick Facts
- arXiv ID: 2511.00030
- Source URL: https://arxiv.org/abs/2511.00030
- Authors: Myeongseob Ko; Hoang Anh Just; Charles Fleming; Ming Jin; Ruoxi Jia
- Reference count: 40
- Primary result: Unlearning methods can inadvertently remove significant benign knowledge, with up to 98.7% of generated test cases producing irrelevant responses despite being answerable by the original model.

## Executive Summary
This paper introduces a framework for detecting unintended knowledge loss ("knowledge holes") in unlearned large language models that standard benchmarks fail to capture. While existing unlearning methods effectively remove targeted harmful content, they can inadvertently degrade performance on benign but semantically related knowledge. The authors propose using reinforcement learning to generate test cases that expose these hidden knowledge gaps, exploring both adjacent knowledge (related to unlearned content) and latent knowledge (unrelated but affected areas). Evaluation across multiple unlearning techniques shows that standard benchmarks like MMLU and MT-bench underestimate the extent of knowledge degradation, while the RL-based probing framework reveals significant performance drops in both adjacent and latent knowledge domains.

## Method Summary
The framework consists of three stages: (1) generating adjacent knowledge test cases from keywords in forgetting and retained datasets, (2) training a policy network via PPO to generate latent knowledge test prompts that maximize low-quality responses from unlearned models, and (3) post-hoc filtering to ensure test quality and remove overlap with forgetting data. The policy network uses a custom reward function combining judge scores, diversity penalties, KL divergence from a reference policy, and entropy bonuses. Unlearning is performed using four methods (LLMU, GAKL, NPOGD, RMU) with LoRA adapters on Zephyr-7B or Mistral-7B-Instruct models. Knowledge degradation is measured using LLM-as-judge scores on both adjacent and latent test sets, with standard benchmarks used for comparison.

## Key Results
- Up to 98.7% of generated latent knowledge test cases produce irrelevant or nonsensical responses from unlearned models, despite being answerable by the original pretrained model.
- Standard benchmarks (MMLU, MT-bench, ARC-easy) show only modest differences between pretrained and unlearned models, while TruthfulQA scores can even improve post-unlearning.
- GAKL exhibits the most pronounced knowledge gap with a 5.006 decline on adjacent knowledge DAP test sets.
- Mitigation strategies face inherent trade-offs: restoring lost knowledge often revives harmful content or creates new knowledge holes.

## Why This Works (Mechanism)

### Mechanism 1
Reinforcement learning systematically discovers model-specific knowledge holes that static benchmarks miss by training a policy network via PPO to generate test prompts maximizing low-quality responses from unlearned models. The reward function combines judge scores, diversity penalties, KL divergence, and entropy bonuses to encourage diverse, high-reward prompts. High-reward prompts (score 10, indicating gibberish responses) are collected during training.

### Mechanism 2
Unlearning operations cause collateral damage to semantically adjacent and lexically overlapping benign knowledge through shared neural representations. Keywords appearing in harmful contexts trigger degraded responses even in benign contexts, operationalized via adjacent datasets generated by extracting keywords from forgetting samples and creating benign questions.

### Mechanism 3
Static benchmarks systematically underestimate knowledge loss because they test fixed, pre-defined domains rather than model-specific failure modes. Each unlearning method-dataset-hyperparameter combination produces unique degradation footprints that stable benchmark distributions may not capture.

## Foundational Learning

- **Proximal Policy Optimization (PPO)**: The paper uses PPO to train the policy network for latent knowledge probing. Understanding the balance between policy updates, reward shaping, and KL penalties is essential for reproducing the framework. *Quick check*: Can you explain why PPO uses a clipped surrogate objective rather than vanilla policy gradients?

- **Machine Unlearning Objectives**: The paper evaluates four unlearning methods with different mechanisms. Understanding how each defines "forgetting" (loss maximization, negative preference, activation noise) clarifies why each produces distinct knowledge hole patterns. *Quick check*: What is the difference between gradient ascent on forgetting data vs. treating forgetting data as negative preference samples in DPO-style training?

- **LLM-as-Judge Evaluation**: The framework depends on judge scores (1-10 scale) to identify low-quality responses and compute rewards. Understanding judge biases, prompt design, and score calibration is critical for interpreting results. *Quick check*: What are potential failure modes of using an LLM to judge another LLM's response quality, and how might they affect reward signals?

## Architecture Onboarding

- **Component map**: Seed prompt generation (D_adj(D_f) + D_adj(D_r)) -> Policy network (LLaMA-2-7B fine-tuned via PPO) -> Reward computation (GPT-4o-mini judge + diversity terms + KL penalty + entropy bonus) -> Response generation (Unlearned target model) -> Post-hoc filtering (GPT-4o-mini validation + overlap exclusion) -> Diversity filtering (cosine similarity > 0.8 + VENDI score matching)

- **Critical path**: Prepare forgetting dataset D_f and retained dataset D_r -> Generate adjacent datasets D_adj(D_f) and D_adj(D_r) using LLM-based question generation -> Train unlearned model using chosen method -> Train policy network via PPO with custom reward function -> Apply post-hoc filtering to D_adj and D_RL -> Evaluate unlearned model on D_AP and D_LP -> Compute judgescores, harmscores, and diversity metrics

- **Design tradeoffs**: Judge model choice affects scoring consistency and cost; diversity vs. coverage trade-off impacts D_RL size; seed prompt composition balances exploitation and exploration; filtering threshold affects candidate preservation

- **Failure signatures**: Low D_LP diversity indicates policy collapse; high post-filtering rejection rate suggests domain overlap issues; stable benchmark scores with low D_LP scores confirm benchmark limitations; mitigation reviving harmful content indicates trade-off challenges

- **First 3 experiments**: 1) Reproduce D_AP construction for NPOGD: Use PKU-SafeRLHF, extract 4 keywords per sample, generate 200 questions, filter with GPT-4o-mini, verify judgescore drop matches Table 1 (7.26 → 2.26). 2) Ablate reward components: Train policy with/without diversity terms, compare D_LP diversity and judgescore distribution to Table 10 baselines. 3) Test mitigation tradeoff on LLMU: Apply one-shot gradient descent on D_AP + D_LP-used, measure harmscore revival and D_LP-new judgescores, confirm tradeoff pattern (harmscore 0 → 0.1, D_LP-new at 1.46).

## Open Questions the Paper Calls Out

- **Can we predict which benign knowledge is at risk before unlearning?** Future work should aim to predict which benign knowledge is at risk when specific data are forgotten and proactively incorporate preventive measures into training.

- **Can pre-emptive strategies break the "onion effect"?** To break this "onion effect," we need pre-emptive strategies that prevent new knowledge holes from forming when addressing existing ones.

- **What is the fundamental relationship between knowledge structure and forgetting propagation?** The paper shows knowledge holes appear in both adjacent and unrelated latent areas but doesn't explain why certain unrelated domains become affected while others remain intact.

- **Can standardized dynamic evaluation protocols be developed?** The efficacy of the RL search procedure depends on numerous hyperparameters requiring extensive tuning, limiting practical adoption.

## Limitations

- **Evaluation artifacts**: Reliance on LLM-as-judge scoring introduces potential judge-specific artifacts and calibration challenges.
- **Mitigation trade-offs**: The one-shot gradient descent approach may not represent optimal mitigation strategies, and both one-shot and iterative approaches face fundamental trade-off challenges.
- **Generalizability concerns**: Results may not directly translate to other model families, scales, or unlearning scenarios beyond the tested Zephyr-7B and Mistral-7B-Instruct models.

## Confidence

**High Confidence**: Unlearning methods create adjacent knowledge holes detectable by the framework; standard benchmarks systematically underestimate knowledge loss; knowledge holes appear in both adjacent and latent domains; mitigation attempts show inherent trade-offs between knowledge restoration and harm revival.

**Medium Confidence**: Reinforcement learning effectively discovers model-specific failure modes; the 98.7% knowledge hole statistic represents a general phenomenon; adjacent knowledge degradation is primarily caused by shared representation effects.

**Low Confidence**: The framework will scale to larger models or different unlearning objectives; alternative mitigation strategies cannot better balance the trade-off; the specific reward function components are optimal for knowledge hole discovery.

## Next Checks

1. **Judge robustness validation**: Replicate core experiments using different LLM judges (GPT-4, Claude, different prompts) and quantify variance in judgescore distributions. Test whether the 10-point threshold for identifying knowledge holes remains meaningful across judges and whether relative performance between unlearning methods remains consistent.

2. **Cross-model generalization test**: Apply the framework to a different model family (e.g., Llama-2, Gemma) with the same unlearning datasets and methods. Compare knowledge hole rates and patterns to determine if the 98.7% figure and adjacent knowledge degradation patterns are model-specific or generalizable.

3. **Alternative mitigation strategy exploration**: Implement and evaluate a multi-step mitigation approach that alternates between knowledge restoration and harm suppression (e.g., alternating gradient steps on D_adj(D_f) and D_f). Compare the trade-off curve against the one-shot approach to determine if better balance is achievable.