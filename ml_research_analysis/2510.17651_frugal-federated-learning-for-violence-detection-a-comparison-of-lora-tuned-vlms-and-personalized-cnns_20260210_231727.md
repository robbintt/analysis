---
ver: rpa2
title: 'Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned
  VLMs and Personalized CNNs'
arxiv_id: '2510.17651'
source_url: https://arxiv.org/abs/2510.17651
tags:
- federated
- energy
- learning
- lora
- llav
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares two federated learning strategies for violence
  detection: LoRA-based fine-tuning of vision-language models (VLMs) and personalized
  training of lightweight 3D CNNs. Both approaches achieved over 90% accuracy under
  non-IID conditions, but with different trade-offs.'
---

# Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs

## Quick Facts
- arXiv ID: 2510.17651
- Source URL: https://arxiv.org/abs/2510.17651
- Reference count: 23
- Primary result: Both LoRA-tuned VLMs and personalized CNNs exceed 90% accuracy under non-IID conditions, with CNNs consuming 57% less energy

## Executive Summary
This paper evaluates two federated learning approaches for privacy-preserving violence detection in surveillance videos: LoRA-based fine-tuning of vision-language models (VLMs) and personalized training of lightweight 3D CNNs. Under non-IID conditions with domain shift between shopping mall and subway datasets, both approaches achieve over 90% accuracy. The CNN3D model consumes less than half the energy (240 Wh vs 570 Wh) and emits less CO₂ while slightly outperforming the VLM in ROC AUC and calibration. The study supports a hybrid deployment model using efficient CNNs for routine classification and VLMs for high-context queries requiring multimodal reasoning.

## Method Summary
The study compares LoRA-tuned VLMs (LLaVA-7B with rank-8 adapters) against personalized 3D CNNs under federated learning with non-IID data partitions. Ten simulated clients receive either RWF-2000 or RLVS datasets with Dirichlet-distributed label imbalance. Models are trained via FedAvg for 20 rounds with 50% client participation. CNN3D uses parameter decoupling with shared feature extractors and local classification heads. Energy consumption is measured using CodeCarbon on NVIDIA A10 GPUs. Performance is evaluated on held-out validation sets spanning both domains.

## Key Results
- Both approaches exceed 90% accuracy under non-IID conditions
- CNN3D consumes 57% less energy (240 Wh vs 570 Wh) and emits 58% less CO₂ (10.1 g vs 24 g)
- CNN3D achieves highest ROC AUC (92.59) and lowest log loss (0.546), indicating superior calibration
- VLMs maintain advantages in multimodal reasoning and interpretability for complex cases

## Why This Works (Mechanism)

### Mechanism 1: LoRA Adapter-Based Federated Fine-Tuning
LoRA enables communication-efficient federated adaptation by transmitting only rank-8 adapter weights rather than full 7B-parameter VLM updates. Base representations remain frozen while adapters capture task-relevant information. This reduces communication payload by 99.9% while preserving representational capacity under non-IID conditions.

### Mechanism 2: Personalized Federated Learning via Parameter Decoupling
Decoupling shared feature extraction layers from client-specific classification heads enables local specialization while preserving collaborative learning benefits. CNN3D's spatiotemporal feature extractors are aggregated globally via FedAvg, while classification layers are trained locally per client. This accommodates domain heterogeneity (RWF vs. RLVS) without forcing homogeneous predictions.

### Mechanism 3: Energy-Accuracy Trade-off via Hybrid Model Selection
Deploying lightweight CNNs for routine classification and selectively activating VLMs for high-context queries optimizes aggregate energy consumption while preserving interpretability where needed. CNN3D processes standard frames at 240 Wh training cost; VLMs are reserved for ambiguous cases requiring multimodal reasoning, reducing overall system footprint by 57%.

## Foundational Learning

- **Federated Learning (FedAvg)**: Privacy-preserving collaborative training across distributed clients without sharing raw data. Quick check: Can you explain why transmitting model updates rather than raw video protects privacy, and what happens to convergence when client participation drops to 30%?
- **Low-Rank Adaptation (LoRA)**: Parameter-efficient fine-tuning using low-rank matrices inserted into attention blocks. Quick check: Given rank r=8 and scaling α=32, how many trainable parameters are added per attention layer, and why does this reduce communication costs in federated settings?
- **Non-IID Data Heterogeneity**: Real-world surveillance exhibits domain shift and label imbalance. Quick check: If clients 1-5 receive only RWF data and clients 6-10 only RLVS, what type of non-IID pattern does this represent, and how does parameter decoupling address it?

## Architecture Onboarding

- **Component map**: Edge Clients (×10) -> Local Data Partition -> CNN3D Branch or VLM Branch -> Aggregation Server (FedAvg)
- **Critical path**: 1) Partition merged dataset across 10 clients with domain-based split 2) Initialize models (CNN3D random or LLaVA-7B pretrained with LoRA) 3) Run FedAvg for 20 rounds with 50% participation 4) Track CodeCarbon metrics alongside accuracy, ROC AUC, log loss 5) Evaluate on held-out 800-video validation set
- **Design tradeoffs**: Model architecture (CNN3D 65.8M vs VLM 7B) favors CNN3D for routine cases; Fine-tuning method (LoRA vs full backprop) favors LoRA for communication efficiency; Personalization (FedAvg only vs parameter decoupling) favors decoupling for non-IID; Client participation (100% vs 50%) balances convergence vs. energy
- **Failure signatures**: Calibration collapse (log loss >0.7 despite accuracy gains), Energy spikes (>2× expected), Domain dominance (one client >> global), Convergence stall (ROC AUC fails after round 10)
- **First 3 experiments**: 1) Baseline zero-shot evaluation of LLaVA-7B without fine-tuning, 2) Non-IID partition validation with Dirichlet distribution, 3) Single-client LoRA sanity check vs federated result

## Open Questions the Paper Calls Out
None

## Limitations
- Energy measurements depend on specific hardware configurations and grid intensity that may not generalize to edge devices
- The hybrid deployment strategy requires reliable confidence estimation not explicitly validated
- Optimal balance between shared and local parameters remains dataset-dependent

## Confidence
- High confidence: Energy consumption comparisons, ROC AUC and calibration metrics, federated averaging implementation
- Medium confidence: Non-IID simulation validity, personalization effectiveness
- Low confidence: Hybrid deployment threshold selection, multimodal reasoning advantages

## Next Checks
1. Conduct ablation study varying LoRA rank and scaling parameters (r=4,8,16; α=16,32,64) to identify optimal trade-off
2. Test personalized CNN3D under extreme non-IID conditions (one client with 90% positive samples) to verify parameter decoupling prevents catastrophic forgetting
3. Implement threshold-based VLM activation on validation set to measure actual energy savings and identify false-negative rates in high-context cases