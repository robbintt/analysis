---
ver: rpa2
title: Multi-Condition Conformal Selection
arxiv_id: '2510.08075'
source_url: https://arxiv.org/abs/2510.08075
tags:
- selection
- conditions
- conformal
- control
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MCCS, a conformal selection method for multi-condition
  settings. Existing methods only handle single-threshold selection (y c), but practical
  applications often require more complex criteria like conjunctions (c1 < y < c2)
  or disjunctions (y < c1 or y c2).
---

# Multi-Condition Conformal Selection

## Quick Facts
- arXiv ID: 2510.08075
- Source URL: https://arxiv.org/abs/2510.08075
- Reference count: 40
- Key outcome: MCCS achieves finite-sample FDR control for complex multi-condition selection tasks, outperforming baselines like intersection/union of single-condition selections.

## Executive Summary
This paper addresses the challenge of conformal selection in multi-condition settings, where practical applications often require complex criteria like conjunctions (c1 < y < c2) or disjunctions (y < c1 or y > c2). Existing methods only handle single-threshold selection (y > c), but MCCS extends this to handle conjunctions through regionally monotone nonconformity scores and disjunctions through a global Benjamini-Hochberg procedure. The method provides rigorous finite-sample FDR control while maintaining high statistical power across various scenarios including univariate/multivariate responses, real-world tasks (NLP, CV, VQA), and multi-class scenarios.

## Method Summary
MCCS introduces a novel framework for multi-condition conformal selection that maintains finite-sample FDR control. For conjunctive conditions, it uses a specially designed nonconformity score with regional monotonicity that assigns higher scores to samples outside the target interval. For disjunctive conditions, it employs a global Benjamini-Hochberg procedure applied to all m×K conformal p-values. The method requires splitting data into training, calibration, and test sets, training a predictor on the training set, computing region-specific nonconformity scores, generating conformal p-values, and applying global BH selection. The approach is theoretically grounded with formal proofs of FDR control and validated across synthetic and real-world datasets.

## Key Results
- MCCS maintains FDR control at target level q=0.3 for both conjunctive and disjunctive conditions
- In univariate conjunctive conditions, MCCS achieved FDR=0.287 vs 0.377 for baseline intersection/union methods
- Outperforms baselines across multiple real-world tasks including NYU Depth V2 (CV), Real-Toxicity-Prompts (NLP), and CIFAR-10/100 (multi-class)
- Demonstrates robust performance across univariate and multivariate response settings

## Why This Works (Mechanism)

### Mechanism 1: Regionally Monotone Nonconformity Scores for Conjunctive Conditions
MCCS uses a specially designed nonconformity score function that assigns higher scores to samples outside the target interval I_k. By using a large constant M, samples inside I_k receive scores M - min(pred - c_k^L, c_k^R - pred) that are guaranteed to be greater than or equal to scores of samples outside I_k. This "regional monotonicity" ensures that samples closer to the center of the target interval (more likely to be true positives) are preferentially selected.

### Mechanism 2: Global Benjamini-Hochberg Procedure for Disjunctive Conditions
For disjunctive conditions, MCCS avoids error accumulation by pooling all p_j^k values from all test samples j and all conditions k into a single set and applying the BH procedure once. This integrated testing framework inherently accounts for dependencies between conditions, unlike naive union approaches that cause FDR inflation due to additive false discoveries.

### Mechanism 3: Finite-Sample FDR Guarantee via Proof
The MCCS algorithm provides mathematically proven FDR <= q for any finite sample size. The proof relies on exchangeability of nonconformity scores and decomposes FDR into a sum over all null hypotheses. It shows that for each null hypothesis H_j^k, the conditional expectation of selection probability is controlled by the BH threshold, establishing the total FDR bound.

## Foundational Learning

- **Concept: Conformal Prediction & Nonconformity Scores**
  - Why needed here: MCCS is built on the conformal prediction framework. Understanding how nonconformity scores translate to p-values is non-negotiable.
  - Quick check question: Can you explain how a nonconformity score for a calibration point with a known label differs from that of a test point with an unknown label, and how this difference is used to construct a conformal p-value?

- **Concept: False Discovery Rate (FDR) & Benjamini-Hochberg (BH) Procedure**
  - Why needed here: The goal of MCCS is to select a subset of samples while rigorously controlling the FDR. The BH procedure is the core multiple hypothesis testing method used to achieve this.
  - Quick check question: In the BH procedure, what does it mean for the threshold to be q * l / NUM, and why is the final selection set defined by all p-values less than this final critical value, not just the ones below their individual thresholds?

- **Concept: Multiple Hypothesis Testing for Conjunctions and Disjunctions**
  - Why needed here: This paper's primary contribution is generalizing conformal selection to complex, multi-condition scenarios. You must understand the difference between testing a conjunction and a disjunction to grasp the problem setup.
  - Quick check question: For a conjunctive condition c1 < y < c2, how is the null hypothesis framed, and how does it differ from a disjunctive condition y < c1 or y > c2?

## Architecture Onboarding

- **Component map:** Data Splitter -> Predictor Model -> Nonconformity Score Calculator -> Conformal P-value Generator -> Global BH Selector
- **Critical path:**
  1. Train predictor μ̂ on training set
  2. Compute calibration scores V_k(x_i, y_i) for all calibration points
  3. Compute test scores and p-values for each test point and interval
  4. Apply global BH procedure to aggregate p-values and produce final selection
- **Design tradeoffs:**
  - Choice of M: Large M enforces monotonicity but reduces score sensitivity; small M increases nuance but risks FDR violation
  - Model Accuracy vs. Power: Poor predictors maintain FDR control but result in low power
  - Global BH vs. Naive Union/Intersection: Global BH is theoretically sound but more complex
- **Failure signatures:**
  - FDR Inflation: Check regional monotonicity implementation and data exchangeability
  - Zero/Very Low Power: Examine score distributions and predictor quality
  - P-value Calculation Errors: Verify tie-breaking logic and ranking procedure
- **First 3 experiments:**
  1. Replicate univariate conjunctive experiment from Table 1a using synthetic data and SVR with RBF kernel
  2. Implement disjunctive condition test from Table 1b to validate Global BH against naive union
  3. Perform sensitivity analysis on constant M by varying it and plotting FDP vs. Power

## Open Questions the Paper Calls Out
- **Open Question 1:** Can MCCS be extended to handle covariate shift between calibration and test sets while maintaining FDR control? The current theoretical guarantees rely on exchangeability, which is violated under covariate shift.
- **Open Question 2:** How sensitive is FDR control to the choice and estimation of the constant M in the nonconformity score? The paper assumes M satisfies theoretical constraints but doesn't analyze practical estimation challenges.
- **Open Question 3:** Can the global BH procedure be replaced with adaptive multiple testing procedures (e.g., Storey's procedure) to increase statistical power? The validity of MCCS p-values and selection mechanism is tied to specific BH thresholding behavior.

## Limitations
- Dependence on predictor quality: Method's power heavily relies on accuracy of pre-trained predictor μ̂
- Exchangeability assumption: Theoretical guarantees assume i.i.d. data, which may not hold in all real-world applications
- Hyperparameter sensitivity: Constant M requires careful tuning; overly large values reduce sensitivity while small values risk breaking FDR control

## Confidence
- **High**: FDR control guarantees for conjunctive and disjunctive conditions
- **Medium**: Experimental validation across diverse datasets and scenarios
- **Low**: Practical performance in non-i.i.d. or adversarial settings

## Next Checks
1. Perform ablation study on constant M by testing different values and evaluating FDR and power sensitivity
2. Evaluate MCCS on datasets with potential exchangeability violations (time-series or clustered data) to assess robustness
3. Benchmark MCCS against adaptive conformal selection methods in scenarios with complex dependencies