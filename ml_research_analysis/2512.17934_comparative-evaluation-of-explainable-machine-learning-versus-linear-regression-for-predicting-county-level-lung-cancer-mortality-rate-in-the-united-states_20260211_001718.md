---
ver: rpa2
title: Comparative Evaluation of Explainable Machine Learning Versus Linear Regression
  for Predicting County-Level Lung Cancer Mortality Rate in the United States
arxiv_id: '2512.17934'
source_url: https://arxiv.org/abs/2512.17934
tags:
- mortality
- cancer
- rates
- health
- factors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared explainable machine learning models with linear
  regression for predicting county-level lung cancer mortality in the United States.
  Random forest and gradient boosting regression models were evaluated against linear
  regression using county-level data across 2,820 counties, with predictors including
  smoking rates, socioeconomic factors, and environmental variables.
---

# Comparative Evaluation of Explainable Machine Learning Versus Linear Regression for Predicting County-Level Lung Cancer Mortality Rate in the United States

## Quick Facts
- arXiv ID: 2512.17934
- Source URL: https://arxiv.org/abs/2512.17934
- Reference count: 38
- Primary result: Random Forest achieved R² of 41.9% and RMSE of 12.8, outperforming Linear Regression (R² 31.2%, RMSE 14.0)

## Executive Summary
This study evaluated the performance of explainable machine learning models against linear regression for predicting county-level lung cancer mortality in the United States. Using 15 county-level predictors including smoking rates, socioeconomic factors, and environmental variables across 2,820 counties, Random Forest achieved the highest predictive accuracy with an R² of 41.9% and RMSE of 12.8. SHAP analysis revealed smoking prevalence as the strongest predictor, followed by median home value and Hispanic population percentage. Geographic analysis identified elevated mortality clusters in mid-eastern counties. The findings demonstrate the superior performance of ensemble machine learning approaches and highlight key modifiable risk factors for targeted interventions.

## Method Summary
The study used 15 county-level predictors from ACS, CDC, EPA, and NCI datasets (2013-2021) to predict lung cancer mortality rates (2015-2019) across 2,820 counties. Missing values were handled through K-nearest neighbors imputation (k=20), followed by min-max scaling. Models included Random Forest, Gradient Boosting Regression, and Linear Regression, trained on 75% of data with 5-fold cross-validated grid search for hyperparameters. Performance was evaluated on held-out test data using R², RMSE, and MAE. SHAP values provided feature importance interpretation, and Getis-Ord Gi* hotspot analysis identified geographic mortality clusters.

## Key Results
- Random Forest achieved highest predictive accuracy with R² of 41.9% and RMSE of 12.8
- SHAP analysis identified smoking prevalence as the strongest predictor, followed by median home value and Hispanic population percentage
- Geographic analysis revealed elevated mortality clusters in mid-eastern counties

## Why This Works (Mechanism)

### Mechanism 1: Ensemble Variance Reduction Through Bagging
Random Forest outperforms single models by aggregating predictions across multiple decorrelated decision trees, reducing variance without increasing bias. Each tree is trained on a bootstrap sample with random feature subsets at each split; final prediction averages all trees, canceling individual tree errors while preserving signal. Core assumption: Individual tree errors are at least partially independent; signal is consistent across trees. Evidence: Random Forest achieved R² of 41.9% and RMSE of 12.8, outperforming Linear Regression (R² 31.2%, RMSE 14.0).

### Mechanism 2: Non-Linear Relationship Capture in Health Outcome Data
County-level health outcomes exhibit complex non-linear interactions among socioeconomic, environmental, and behavioral predictors that linear models cannot adequately represent. Decision trees partition feature space into axis-aligned rectangular regions; ensembles approximate arbitrary non-linear surfaces by combining many such regions. Core assumption: The true relationship between predictors and lung cancer mortality is non-linear with interaction effects. Evidence: SHAP summary plots show nonlinear relationships between factors and outcomes, explaining why Linear Regression underperformed compared with ML models such as Random Forest and Gradient Boosting Regression.

### Mechanism 3: Post-Hoc Interpretability via SHAP Value Decomposition
SHAP values enable transparent interpretation of ensemble models by quantifying each feature's contribution to individual predictions and global importance. Shapley values from cooperative game theory allocate prediction credit among features based on marginal contributions across all possible feature orderings; SHAP approximates this efficiently for tree ensembles. Core assumption: Feature contributions are approximately additive; interaction effects can be attributed to individual features for practical interpretation. Evidence: SHAP analysis identified smoking prevalence as the strongest predictor, followed by median home value and Hispanic population percentage.

## Foundational Learning

- Concept: **Decision Tree Ensembles (RF vs GBR)**
  - Why needed here: The paper compares RF and GBR; understanding their different training paradigms (parallel bagging vs sequential boosting) explains performance differences.
  - Quick check question: Why does RF train trees independently in parallel while GBR trains trees sequentially on residuals?

- Concept: **SHAP Values and Feature Attribution**
  - Why needed here: Core to the paper's claim of "explainable ML"; SHAP transforms black-box predictions into interpretable contributions.
  - Quick check question: In a regression context, does a positive SHAP value for smoking rate mean higher smoking increases or decreases predicted mortality?

- Concept: **Spatial Autocorrelation and Hotspot Analysis (Getis-Ord Gi*)**
  - Why needed here: The paper uses spatial clustering to identify mortality hotspots; understanding Gi* is necessary to interpret geographic findings.
  - Quick check question: What does a statistically significant Gi* hotspot indicate about a county and its neighbors?

## Architecture Onboarding

- Component map: Data preprocessing -> Feature matrix creation -> Train-test split -> Model training -> Evaluation -> Interpretability -> Spatial analysis
- Critical path: Data quality (missing value handling) -> Feature standardization -> Hyperparameter tuning via CV -> Model selection on test R² -> SHAP interpretation -> Geographic validation
- Design tradeoffs:
  - Interpretability vs accuracy: RF/GBR achieve higher R² but require SHAP for explanation; LR is directly interpretable but underperforms
  - Spatial granularity vs data availability: County-level aggregation masks individual-level heterogeneity and introduces ecological fallacy risk
  - Temporal alignment: Predictors (2013-2021) imperfectly align with mortality outcomes (2015-2019), potentially missing latency effects
  - Feature selection: Key variables (occupational exposures, radon, indoor pollution) excluded due to unavailable county-level data
- Failure signatures:
  - R² < 35%: Model is missing critical predictors; reassess feature set
  - SHAP direction contradicts domain knowledge: Check for data leakage, coding errors, or confounding
  - Hotspots do not align with predictor hotspots: Model may not be capturing spatial structure; consider spatial explicit models
  - Large RMSE variance across regions: Model may not generalize geographically; consider region-specific models
- First 3 experiments:
  1. Baseline RF reproduction: Train RF with 5-fold CV grid search on 75/25 split; verify R² ≈ 42% and RMSE ≈ 12.8 on test set using the same 15 features
  2. SHAP validation on top 6 features: Compute SHAP values for test set; confirm smoking rate, median home value, and Hispanic population have highest mean |SHAP|; verify directional consistency
  3. Geographic alignment check: Reproduce Getis-Ord Gi* hotspot map for mortality; overlay with smoking prevalence hotspot map; confirm spatial overlap in mid-eastern counties

## Open Questions the Paper Calls Out

### Open Question 1
Would the inclusion of standardized county-level data on occupational hazards (e.g., asbestos) and residential radon exposure significantly improve the predictive accuracy of the Random Forest model? Basis: The limitations section states that key variables linked to lung cancer, such as occupational hazards and radon, were excluded due to a lack of standardized, county-level data. Why unresolved: Current public datasets do not consistently provide these specific environmental and occupational metrics at the county level. What evidence would resolve it: Re-training the models by integrating proxy datasets (e.g., EPA radon zones or mining employment data) and measuring the change in R² and RMSE.

### Open Question 2
How does accounting for the biological latency period of lung cancer (e.g., lagging predictors by 10-20 years) alter the variable importance rankings identified by SHAP analysis? Basis: The authors note a reliance on aggregated data with temporal misalignment that may not fully account for the latency periods between exposures and cancer outcomes. Why unresolved: The study design primarily used near-contemporaneous data, which cannot capture the long-term historical exposure that drives current mortality. What evidence would resolve it: A sensitivity analysis using historical predictor data from 1990-2000 to model current mortality rates and comparing the resulting SHAP values.

### Open Question 3
Can the predictive performance of the ensemble model be maintained when applied to individual-level electronic health record (EHR) data rather than county-level aggregates? Basis: The conclusion suggests future studies should address current gaps by integrating individual-level electronic health records to validate the findings found in aggregated data. Why unresolved: The current study is ecological (county-level) and subject to ecological fallacy; it remains unclear if the identified nonlinear relationships hold true at the individual patient level. What evidence would resolve it: Validating the RF model framework using patient-level EHR datasets to see if smoking and home value remain the dominant predictors.

## Limitations
- Potential ecological fallacy from county-level aggregation masks individual-level heterogeneity
- Imperfect temporal alignment between predictors (2013-2021) and mortality outcomes (2015-2019)
- Missing critical risk factors like occupational exposures and radon due to data unavailability

## Confidence

- **High Confidence**: RF's superior predictive accuracy (R² 41.9%, RMSE 12.8) over LR and GBR; SHAP-identified smoking prevalence as top predictor; geographic clustering in mid-eastern counties
- **Medium Confidence**: The mechanism explaining RF's advantage through ensemble variance reduction; the claim that non-linear relationships explain LR's underperformance
- **Low Confidence**: The completeness of feature set for capturing all relevant lung cancer risk factors; the stability of SHAP attributions across different model configurations

## Next Checks
1. **Sensitivity Analysis**: Re-run models with alternative temporal alignments (e.g., using 2010-2014 predictor data) to assess robustness to latency assumptions
2. **Feature Addition**: Incorporate available county-level occupational exposure and radon data where possible to test if R² increases beyond 41.9%
3. **Spatial Model Comparison**: Implement geographically weighted regression (GWR) or spatial autoregressive models to compare against RF's spatial clustering results