---
ver: rpa2
title: Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?
arxiv_id: '2510.20154'
source_url: https://arxiv.org/abs/2510.20154
tags:
- stance
- text
- dataset
- detection
- complexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates bias in large language models (LLMs) for
  zero-shot stance detection, focusing on how stereotypes embedded in their training
  data influence predictions based on dialect (African-American English vs Standard
  American English) and text complexity (measured by Flesch-Kincaid readability).
  The authors automatically annotated existing stance detection datasets with these
  attributes and evaluated five popular LLMs (GPT-3.5, Llama, Mistral, Falcon, and
  Flan) using fairness metrics including Equal Opportunity, Disparate Impact, and
  Predictive Parity.
---

# Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?

## Quick Facts
- arXiv ID: 2510.20154
- Source URL: https://arxiv.org/abs/2510.20154
- Authors: Anthony Dubreuil; Antoine Gourru; Christine Largeron; Amine Trabelsi
- Reference count: 11
- Primary result: LLMs exhibit significant bias in stance detection based on dialect and text complexity, with Falcon most biased and GPT-3.5 least biased.

## Executive Summary
This paper investigates bias in large language models for zero-shot stance detection, focusing on how stereotypes embedded in their training data influence predictions based on dialect (African-American English vs Standard American English) and text complexity (measured by Flesch-Kincaid readability). The authors automatically annotated existing stance detection datasets with these attributes and evaluated five popular LLMs (GPT-3.5, Llama, Mistral, Falcon, and Flan) using fairness metrics including Equal Opportunity, Disparate Impact, and Predictive Parity. Results show significant biases: models incorrectly associate pro-marijuana stances with low text complexity and African-American dialect with opposition to Donald Trump. Falcon exhibited the highest overall bias, while GPT-3.5 showed the lowest. The study demonstrates that LLMs rely on demographic linguistic cues and readability when making stance predictions, highlighting the need for more equitable models and debiasing techniques in sensitive NLP tasks like stance detection.

## Method Summary
The study evaluates five LLMs (GPT-3.5-turbo-0125, Llama3-8B-Instruct, Mistral-7B-Instruct-v0.2, Falcon-7b-instruct, FLAN-T5-large) using zero-shot stance detection on three datasets: PStance (for dialect bias), SCD and KE-MLM (for text complexity bias). The Context Analyze prompting method was employed with a specified template. Datasets were automatically annotated with African-American English (AAE) labels using the Blodgett et al. (2016) model and Flesch-Kincaid readability scores, then balanced by downsampling to equal class sizes. Performance was measured using weighted F1, while fairness was assessed via Equal Opportunity (primary), Disparate Impact, and Predictive Parity metrics. The evaluation used 1000 balanced samples per configuration.

## Key Results
- LLMs show significant bias in stance detection based on dialect and text complexity
- Models associate pro-marijuana stances with low text complexity and AAE with opposition to Trump
- Falcon exhibited the highest overall bias, while GPT-3.5 showed the lowest bias among evaluated models

## Why This Works (Mechanism)
None

## Foundational Learning
- **Zero-shot stance detection**: Classifying text as favoring or opposing a target without task-specific training, using only the model's pre-existing knowledge. Why needed: Evaluates model biases that stem from training data rather than fine-tuning.
- **Dialect bias**: Systematic prediction differences based on linguistic features associated with specific demographic groups. Why needed: Identifies how demographic stereotypes affect model decisions.
- **Text complexity metrics**: Measures like Flesch-Kincaid score that quantify readability based on sentence and word length. Why needed: Determines if models use text sophistication as a proxy for stance.
- **Fairness metrics (EO, DI, PP)**: Statistical measures comparing model performance across demographic groups. Why needed: Quantifies bias in a principled, comparable way.
- **Automatic attribute annotation**: Using models or formulas to label data with sensitive attributes. Why needed: Enables large-scale bias evaluation without manual annotation.
- **Context Analyze prompting**: A structured prompt format that guides models to analyze text and output a specific stance label. Why needed: Standardizes zero-shot inference across different models.

## Architecture Onboarding
**Component map:** Datasets (PStance, SCD, KE-MLM) -> Annotation pipeline (AAE classifier, Flesch-Kincaid) -> Balanced samples -> LLMs (GPT-3.5, Llama, Mistral, Falcon, Flan) -> Fairness metrics (EO, DI, PP) -> Results

**Critical path:** Annotate -> Balance -> Zero-shot inference -> Compute metrics

**Design tradeoffs:** Using smaller, instruction-tuned models for efficiency vs. potentially missing biases present in larger models; automatic annotation vs. potential annotation errors; downsampling for balance vs. losing original data distribution.

**Failure signatures:** High neutral prediction rates (especially Llama at 60%+) skewing results; AAE classifier returns low-confidence predictions on short tweets; class imbalance after annotation.

**First experiments:**
1. Run AAE classifier on PStance to verify ~50% AAE/50% SAE after balancing
2. Compute Flesch-Kincaid scores on SCD to confirm expected complexity distribution
3. Test zero-shot inference on 100 samples with GPT-3.5 to verify prompt format works

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does combining static linguistic metrics with LLM-based classifications yield more robust sensitive attribute annotations for bias evaluation than static methods alone?
- Basis in paper: The authors state in the conclusion that "a promising line of research would be to combine static and LLM-based metrics for automatic group categorization."
- Why unresolved: The current study relies solely on a static model for dialect (Blodgett et al., 2016) and a formula for complexity, which may lack the nuance of newer LLM-based approaches.
- What evidence would resolve it: A comparative study measuring the accuracy and consistency of hybrid annotation pipelines against human-annotated ground truth for dialect and complexity.

### Open Question 2
- Question: Do fairness-aware prompting or causal modeling techniques effectively mitigate the dialect and complexity-based biases observed in zero-shot stance detection?
- Basis in paper: The Discussion suggests these methods as solutions, stating, "This could be achieved using fairness-aware prompting or calibration... or by causal modeling," but does not test them.
- Why unresolved: The paper identifies the existence of bias but stops short of validating specific debiasing interventions in this context.
- What evidence would resolve it: A follow-up experiment applying counterfactual inference or fairness prompts to the same models, followed by a reassessment of Equal Opportunity scores.

### Open Question 3
- Question: Do larger parameter variants of the evaluated models exhibit reduced bias or distinct stereotypical associations compared to the smaller 7B/8B versions?
- Basis in paper: The Limitations section notes that "larger model variants may perform better... and reveal additional biases" since the study was restricted to smaller instruction-tuned models.
- Why unresolved: It is unclear if the observed stereotypes are an artifact of model capacity or inherent to the training data across all model scales.
- What evidence would resolve it: Replicating the evaluation protocol using larger checkpoints (e.g., Llama-70B) on the same balanced datasets to compare bias metrics.

## Limitations
- The study used only smaller instruction-tuned models, potentially missing biases present in larger variants
- Automatic annotation with static models may introduce errors or miss nuanced dialect features
- High neutral prediction rates in some models suggest the prompting method may need refinement

## Confidence
- **Relative ranking of model biases**: High - The methodology is sound and the pattern across multiple datasets and metrics is consistent
- **Exact magnitude of disparities**: Medium - Missing sampling methodology details and API parameters affect precision
- **Generalizability to other tasks**: Medium - Stance detection is a specific task; results may not transfer to other NLP applications

## Next Checks
1. Verify AAE annotation proportions match expected values (e.g., PStance should show ~50% AAE/50% SAE after balancing per Table 4)
2. Replicate the analysis with multiple random seeds for the 1000-sample selection to assess variance in fairness metrics
3. Test whether filtering neutral predictions significantly alters bias measurements, particularly for models with high neutral rates