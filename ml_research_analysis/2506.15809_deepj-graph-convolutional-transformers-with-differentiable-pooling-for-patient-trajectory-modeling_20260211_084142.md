---
ver: rpa2
title: 'DeepJ: Graph Convolutional Transformers with Differentiable Pooling for Patient
  Trajectory Modeling'
arxiv_id: '2506.15809'
source_url: https://arxiv.org/abs/2506.15809
tags:
- graph
- deepj
- medical
- patient
- codes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the limitation of existing graph-based EHR
  models that fail to capture longitudinal interactions across multiple encounters.
  The proposed DeepJ model integrates a Graph Structure Learning (GSL) component using
  extended graph convolutional transformers and a Clinical Module Discovery (CMD)
  component using differentiable graph pooling to identify temporally and functionally
  related medical event clusters.
---

# DeepJ: Graph Convolutional Transformers with Differentiable Pooling for Patient Trajectory Modeling

## Quick Facts
- arXiv ID: 2506.15809
- Source URL: https://arxiv.org/abs/2506.15809
- Reference count: 0
- DeepJ achieved AUROC of 0.893 and 0.727, and AUPRC of 0.591 and 0.332 for ICU mortality and AKI onset prediction on eICU and KUMC datasets

## Executive Summary
This study addresses the limitation of existing graph-based EHR models that fail to capture longitudinal interactions across multiple encounters. The proposed DeepJ model integrates a Graph Structure Learning (GSL) component using extended graph convolutional transformers and a Clinical Module Discovery (CMD) component using differentiable graph pooling to identify temporally and functionally related medical event clusters. On two datasets (eICU and KUMC), DeepJ achieved AUROC of 0.893 and 0.727, and AUPRC of 0.591 and 0.332 for ICU mortality and AKI onset prediction tasks, respectively, significantly outperforming five state-of-the-art baselines. The model demonstrated strong interpretability by uncovering meaningful clinical modules and inter-encounter relationships, showing its potential for improved patient risk stratification.

## Method Summary
DeepJ is a graph-based neural network that models patient trajectories by learning relationships between medical events across multiple clinical encounters. The model consists of two main components: Graph Structure Learning (GSL) that uses extended graph convolutional transformers with masked self-attention and temporal encoding to learn both intra-encounter and inter-encounter relationships, and Clinical Module Discovery (CMD) that employs differentiable graph pooling to hierarchically identify clinically meaningful subgroups of medical codes. The model takes code embeddings and temporal information as input, processes them through stacked transformer and pooling layers, then aggregates the final pooled representations with attention weighting for outcome prediction. Training includes auxiliary losses for KL divergence, link prediction, and entropy regularization to encourage meaningful graph structures and cluster assignments.

## Key Results
- Achieved AUROC of 0.893 and 0.727 on ICU mortality prediction (eICU) and AKI onset prediction (KUMC) tasks
- Outperformed five state-of-the-art baselines including Transformer-based and graph-based models
- Demonstrated strong interpretability by discovering clinically meaningful modules like "Acute Renal Failure" clusters
- Ablation studies showed both GSL and CMD components significantly contribute to performance gains

## Why This Works (Mechanism)

### Mechanism 1: Cross-Encounter Graph Structure Learning via Causal Masking
The Extended Graph Convolutional Transformer (EGCT) with temporal encoding captures both intra-encounter and inter-encounter medical code relationships. Masked self-attention learns edge weights between codes across all encounters, while a causal mask prevents future codes from influencing past codes. Temporal encoding injects elapsed time information into embeddings. Medical codes from different encounters have meaningful temporal dependencies that influence outcomes (e.g., prior diabetes diagnosis affecting current insulin prescription).

### Mechanism 2: Co-occurrence Initialization with Learned Refinement
Initializing the first attention block with real-world co-occurrence statistics, then allowing learned deviations, produces higher-quality graphs than pure learning. Replace first block's attention matrix with co-occurrence matrix; subsequent blocks learn deviations with KL-divergence regularization encouraging continuity. Statistical co-occurrence from population-level data provides useful prior knowledge that can be refined for individual patients.

### Mechanism 3: Hierarchical Pooling for Clinical Module Discovery
Differentiable pooling identifies clinically meaningful subgroups of codes that span multiple encounters. DiffPool blocks use parallel GCNs—one for node features, one for soft cluster assignments—to hierarchically pool nodes into coarser representations. Link prediction and entropy losses encourage meaningful, unambiguous clusters. Patients have multiple coexisting conditions (e.g., heart failure + lung cancer) with distinct code subgroups that contribute differently to outcomes.

## Foundational Learning

- **Concept: Graph Convolutional Networks (GCN)**
  - Why needed here: GCN layers aggregate neighborhood information in both GSL (implicitly via attention) and explicitly in DiffPool for node representation updates and cluster assignment.
  - Quick check question: Can you explain how a GCN layer propagates information through adjacency matrix multiplication?

- **Concept: Transformer Self-Attention with Masking**
  - Why needed here: EGCT uses masked self-attention to learn edge weights; understanding attention-as-edge-weight is critical.
  - Quick check question: Why does the causal mask (M_{ij} = -∞ if code i happened before code j) prevent future-to-past edges?

- **Concept: Differentiable Graph Pooling (DiffPool)**
  - Why needed here: CMD hierarchically clusters nodes; understanding how soft cluster assignments enable gradient flow is essential.
  - Quick check question: How does the cluster assignment matrix S enable backpropagation through the pooling operation?

## Architecture Onboarding

- **Component map:** Input embedding → EGCT block 1 (CO-initialized, KL loss) → EGCT blocks 2-N → adjacency matrix extraction → DiffPool block 1 (link + entropy losses) → DiffPool blocks 2-M → attention weighting → classifier

- **Critical path:** Input embedding → EGCT block 1 (CO-initialized, KL loss) → EGCT blocks 2-N → adjacency matrix extraction → DiffPool block 1 (link + entropy losses) → DiffPool blocks 2-M → attention weighting → classifier

- **Design tradeoffs:**
  - More EGCT blocks: richer graph structure vs. computational cost and over-smoothing risk
  - More DiffPool blocks: finer hierarchy vs. information loss at each pooling step
  - Cluster count (g_max): higher granularity vs. harder interpretation

- **Failure signatures:**
  - Uniform attention weights across all edges: co-occurrence initialization ineffective or learning rate too low
  - Single dominant cluster in final DiffPool: g_max too low or entropy regularization (λ_ent) too weak
  - Ablation shows no GSL benefit: cross-encounter dependencies may not exist in your specific dataset

- **First 3 experiments:**
  1. Run DeepJ w/o GSL and DeepJ w/o CMD ablations on validation data to confirm each component's contribution matches paper (Table 2).
  2. Visualize learned graphs for 5-10 sample patients; manually verify inter-encounter edges are clinically plausible.
  3. Sweep cluster count (g_max) and number of DiffPool blocks (M) to find the point where performance plateaus but interpretability remains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can causal inference methods be integrated into the DeepJ architecture to distinguish between associations and causal relationships in the learned graph structures?
- Basis in paper: The authors explicitly state in the Discussion that "uncovered graphs represent associations rather than causal relationships" and suggest future research focus on "enhancing causal inference methods."
- Why unresolved: The current architecture optimizes for predictive performance (AUROC/AUPRC) and structural similarity (KL divergence), which captures correlation but does not enforce causal constraints (e.g., acyclicity).
- What evidence would resolve it: A modified DeepJ model that integrates causal discovery constraints (e.g., NOTEARS-style penalties) and validates identified edges against known causal benchmarks or clinical trial data.

### Open Question 2
- Question: Does the reliance on the co-occurrence matrix (CO) for initialization limit the model's ability to discover novel relationships in sparse data regimes or rare diseases?
- Basis in paper: The paper notes that "the number of medical codes within a single encounter is often small," and the GSL component initializes using CO. If CO is sparse, the model might struggle to learn new edges.
- Why unresolved: While the GSL learns new edges, it is anchored by CO. The performance sensitivity to the density and quality of the initial CO matrix remains unquantified in sparse scenarios.
- What evidence would resolve it: Ablation studies on datasets with artificially induced sparsity or subsets of rare diseases, comparing performance against random or cold-start initialization.

### Open Question 3
- Question: Can the Clinical Module Discovery (CMD) component identify stable, phenotype-consistent clusters across institutions with varying coding practices?
- Basis in paper: The model was tested on eICU (public) and KUMC (private). While performance was consistent, the semantic stability of the "clinical modules" (clusters) across these distinct data distributions was not explicitly analyzed.
- Why unresolved: Different institutions often use different coding frequencies or billing habits. Clusters might be driven by site-specific documentation practices rather than underlying physiological states.
- What evidence would resolve it: Cross-domain validation showing that specific clusters (e.g., the "Acute Renal Failure" module) share high overlap in code composition and predictive importance when trained on one dataset and tested on another.

## Limitations
- Missing critical implementation details including exact hyperparameter values, code preprocessing specifications, and loss weight optimization
- Clinical module discovery claims rely on qualitative interpretation without external clinical validation against established diagnostic groupings
- Performance claims depend heavily on unspecified hyperparameters and initialization strategies that may not generalize

## Confidence
- **Graph Structure Learning Mechanism**: Medium confidence - The architectural approach is well-described, but performance claims depend heavily on unspecified hyperparameters and initialization strategies that may not generalize.
- **Clinical Module Discovery Interpretability**: Low confidence - While the method can produce clusters, the clinical validity of discovered modules requires domain expert validation beyond the qualitative examples provided.
- **Outperformance Claims**: Medium confidence - The reported AUROC/AUPRC improvements are statistically significant, but exact hyperparameter matching is impossible without the missing details, and baseline implementations may differ from the original papers.

## Next Checks
1. Run DeepJ w/o GSL and DeepJ w/o CMD ablations on validation data to confirm the paper's reported performance gaps (0.8933→0.8747 AUROC for CMD ablation) match your implementation within acceptable tolerance.
2. Visualize the learned adjacency matrices from the first and last EGCT blocks for 10 sample patients; manually verify that inter-encounter edges capture clinically plausible temporal dependencies rather than random connections.
3. For the final DiffPool output, plot the distribution of cluster assignment entropies and the average number of non-zero entries per node; verify that clusters are sufficiently granular (entropy > 0.5) but not overly fragmented (average degree < 5).