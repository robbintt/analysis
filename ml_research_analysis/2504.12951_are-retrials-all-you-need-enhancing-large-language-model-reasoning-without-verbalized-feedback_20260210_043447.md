---
ver: rpa2
title: Are Retrials All You Need? Enhancing Large Language Model Reasoning Without
  Verbalized Feedback
arxiv_id: '2504.12951'
source_url: https://arxiv.org/abs/2504.12951
tags:
- reasoning
- methods
- language
- arxiv
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether retrial mechanisms can enhance reasoning
  in large language models (LLMs) without requiring verbalized feedback or explicit
  self-reflection. The authors introduce a retrial-based approach where models retry
  problem-solving attempts until reaching a correct answer or exhausting a predefined
  computational budget.
---

# Are Retrials All You Need? Enhancing Large Language Model Reasoning Without Verbalized Feedback

## Quick Facts
- arXiv ID: 2504.12951
- Source URL: https://arxiv.org/abs/2504.12951
- Reference count: 5
- Primary result: Simpler Chain-of-Thought prompting outperforms complex reasoning frameworks like Tree-of-Thoughts and Reflexion in cost-efficiency for retrial-based reasoning

## Executive Summary
This paper investigates whether retrial mechanisms can enhance reasoning in large language models without requiring verbalized feedback or explicit self-reflection. The authors introduce a straightforward approach where models retry problem-solving attempts until reaching a correct answer or exhausting a predefined computational budget. Across multiple benchmarks including Game of 24, HumanEval, and HotpotQA, simpler methods like Chain-of-Thought prompting demonstrate superior cost-efficiency compared to more complex frameworks like Tree-of-Thoughts and Reflexion.

The results challenge the assumption that sophisticated reasoning approaches necessarily lead to better outcomes, showing that iterative retrial strategies can achieve strong performance with lower computational overhead. Temperature tuning further improves retrial performance, suggesting that simple parameter adjustments can yield meaningful gains without the need for complex reasoning frameworks.

## Method Summary
The authors propose a retrial-based approach where LLMs are allowed to attempt problem-solving multiple times until either finding a correct answer or reaching a computational budget limit. This approach was tested against established methods including Chain-of-Thought, Tree-of-Thoughts, and Reflexion across three benchmark datasets: Game of 24 (mathematical puzzles), HumanEval (code generation), and HotpotQA (factoid reasoning). The evaluation measured both accuracy and computational cost, comparing different prompting strategies and temperature settings to identify optimal configurations for reasoning performance.

## Key Results
- Chain-of-Thought prompting outperforms more complex frameworks like Tree-of-Thoughts and Reflexion in terms of cost-efficiency
- Temperature tuning significantly improves retrial performance across benchmarks
- Retrial mechanisms achieve strong reasoning performance without requiring verbalized feedback or explicit self-reflection

## Why This Works (Mechanism)
The retrial mechanism works by allowing the model multiple attempts at solving a problem, effectively giving it opportunities to explore different solution paths or correct initial errors. Without requiring verbalized feedback, the model implicitly learns from failed attempts through the natural progression of reasoning attempts. This approach leverages the model's existing capabilities while reducing the computational overhead associated with more complex reasoning frameworks that explicitly track and evaluate multiple solution paths.

## Foundational Learning

**Chain-of-Thought prompting**: Breaking down complex reasoning into intermediate steps
*Why needed*: Enables systematic problem decomposition without requiring complex architectures
*Quick check*: Verify the model can follow step-by-step instructions consistently

**Retrial mechanisms**: Multiple attempts at problem-solving within computational budget
*Why needed*: Provides opportunities for error correction without increasing model complexity
*Quick check*: Track success rate improvements across retry attempts

**Temperature tuning**: Adjusting sampling randomness in model outputs
*Why needed*: Balances exploration of solution space with exploitation of known patterns
*Quick check*: Test performance across temperature values 0.0 to 1.0

## Architecture Onboarding

**Component map**: LLMs -> Prompting Strategy -> Retrial Mechanism -> Answer Verification -> (Loop or Terminate)

**Critical path**: Problem Input → Prompting Strategy → Model Generation → Verification → (Success or Retry)

**Design tradeoffs**: Simplicity and cost-efficiency vs. potential performance gains from complex frameworks

**Failure signatures**: Persistent errors across retries, early termination without solution found, degraded performance with increased retries

**First experiments**:
1. Baseline comparison: Single attempt vs. multiple retrials with CoT prompting
2. Temperature sweep: Performance across temperature values 0.0, 0.5, 1.0
3. Budget variation: Performance with 1, 3, 5, and 10 allowed retries

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Results may not generalize beyond mathematical puzzles, code generation, and factoid reasoning
- Computational budget parameter not systematically varied to determine optimal trade-offs
- Lack of controlled experiments comparing retrial mechanisms with and without verbalized feedback

## Confidence

**High**: Simpler Chain-of-Thought prompting outperforms complex frameworks in cost-efficiency across multiple benchmarks

**Medium**: Temperature tuning results are promising but not fully explored across parameter space

**Medium**: Claims about retrial mechanisms without verbalized feedback need controlled validation

## Next Checks

1. Test the retrial approach on benchmarks requiring more complex reasoning patterns, such as MultiArith, StrategyQA, or DROP, to assess whether the performance gains observed in simpler tasks transfer to more challenging reasoning scenarios.

2. Conduct ablation studies varying the number of allowed retries systematically (e.g., 1, 3, 5, 10, 20) to quantify the relationship between computational budget and performance gains, and identify diminishing returns thresholds.

3. Implement controlled experiments comparing retrial mechanisms with and without verbalized feedback to isolate the specific contribution of the retrial strategy versus self-reflection capabilities in improving reasoning performance.