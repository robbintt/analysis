---
ver: rpa2
title: 'Trajectory2Task: Training Robust Tool-Calling Agents with Synthesized Yet
  Verifiable Data for Complex User Intents'
arxiv_id: '2601.20144'
source_url: https://arxiv.org/abs/2601.20144
tags:
- user
- task
- agent
- tool
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Trajectory2Task is a verifiable data generation pipeline that synthesizes
  complex multi-turn tool-use tasks and trajectories under ambiguous, changing, and
  infeasible user intents. It generates tasks by converting executed trajectories
  into user-facing scenarios with controlled intent adaptations, ensuring verifiable
  ground truth labels.
---

# Trajectory2Task: Training Robust Tool-Calling Agents with Synthesized Yet Verifiable Data for Complex User Intents

## Quick Facts
- **arXiv ID**: 2601.20144
- **Source URL**: https://arxiv.org/abs/2601.20144
- **Reference count**: 40
- **Primary result**: Fine-tuning on fewer than 3,000 verified multi-turn trajectories improves robustness and generalizes to unseen tool domains, outperforming strong baselines on tasks with ambiguous, changing, or infeasible user intents.

## Executive Summary
Trajectory2Task introduces a verifiable data generation pipeline that synthesizes complex multi-turn tool-use tasks and trajectories under ambiguous, changing, and infeasible user intents. By starting from executable trajectories and then summarizing them into user-facing scenarios, the method guarantees verifiable ground truth labels for every task. The pipeline explicitly injects realistic perturbations—ambiguity, intent drift, and infeasibility—to stress-test model robustness. Benchmarking seven LLMs shows significant performance drops under these conditions, especially for smaller models. Fine-tuning lightweight LLMs on fewer than 3,000 successful trajectories improves robustness and generalizes to unseen tool domains, suggesting that trajectory-based supervision teaches general tool-calling behaviors beyond narrow memorization.

## Method Summary
Trajectory2Task is a two-stage pipeline: (1) exploration and filtering, where a strong LLM (Claude-4.5-Sonnet) generates multi-turn trajectories in a simulated tool environment, and low-quality trajectories are filtered out; (2) task summarization and validity checking, where filtered trajectories are converted into user-facing tasks with controlled intent perturbations (ambiguous, changing, or infeasible), and a validity-check LLM scores and filters tasks for realism and correctness. Successful trajectories are collected and formatted for supervised fine-tuning. The method uses Tau2-Bench retail and airline domains, and trains lightweight models (Qwen3-4B/8B) on 2,872 trajectories for 3 epochs with Adam (lr=1e-5).

## Key Results
- Significant performance drops for all models when moving from general to ambiguous, changing, or infeasible intent scenarios, especially for smaller models.
- Fine-tuning on fewer than 3,000 successful trajectories improves robustness on complex intent scenarios and generalizes to unseen airline domain.
- Qwen3-4B/8B models trained on trajectory data outperform baselines and show better handling of intent drift and infeasibility.

## Why This Works (Mechanism)

### Mechanism 1: Trajectory-First Synthesis for Verifiable Ground Truth
- **Claim**: Starting from executable trajectories rather than natural-language task descriptions guarantees every generated task has a verified solution path.
- **Mechanism**: The pipeline explores trajectories first (via multi-turn tool interaction), then summarizes them into user-facing tasks. This reverse-engineering approach ensures task–trajectory consistency by construction, enabling closed-loop evaluation with database state as ground truth.
- **Core assumption**: The summarizer LLM can faithfully infer plausible user intent from observed agent actions without introducing logical inconsistencies.
- **Evidence anchors**: [abstract] "This process yields verifiable task that support closed-loop evaluation and training." [section 3.2.1] "Compared to prior data synthesis pipelines that start from a natural-language task and then generate trajectories, Trajectory2Task starts from executable trajectories and only then derives tasks. This guarantees that every valid task has an associated executable gold trajectory."
- **Break condition**: If the summarizer introduces intent–action misalignment (e.g., task description requires actions not present in the trajectory), validity-check filtering should reject the instance.

### Mechanism 2: Controlled Intent Perturbation for Robustness Stress-Testing
- **Claim**: Explicitly injecting ambiguity, intent drift, and infeasibility into synthesized tasks reveals failure modes that static benchmarks miss.
- **Mechanism**: The pipeline modifies task-summarization prompts to withhold information (ambiguous), introduce mid-dialogue goal changes (changing), or specify policy-violating requests (infeasible). Each scenario has dedicated validity-check criteria to ensure perturbations are realistic and well-defined.
- **Core assumption**: The user simulator (LLM-driven) sufficiently approximates real user behavior distributions for these specific stress factors.
- **Evidence anchors**: [abstract] "Benchmarking seven LLMs on the generated tasks shows significant performance drops under these realistic conditions, especially for smaller models." [section 4.3] "Moving from GENERAL to AMBIGUOUS, performance drops for every model, but the degradation is much larger for smaller models."
- **Break condition**: If perturbation prompts produce tasks where infeasibility is ambiguous (e.g., multiple valid interpretations of "forbidden actions"), validity-check scores should flag low INFEASIBILITY_REASONABILITY.

### Mechanism 3: Trajectory-Based SFT for Generalizable Decision Behavior
- **Claim**: Fine-tuning on successful multi-turn trajectories teaches *when* to ask, adapt, or act—skills that transfer to unseen tool domains.
- **Mechanism**: The SFT objective maximizes likelihood of agent outputs (reasoning tokens + actions) conditioned on task and observation history. Training on <3k diverse trajectories with explicit reasoning traces enables learning of procedural patterns rather than domain-specific tool memorization.
- **Core assumption**: Reasoning tokens in trajectories capture transferable decision logic (e.g., "verify order status before cancellation").
- **Evidence anchors**: [abstract] "Fine-tuning lightweight LLMs on fewer than 3,000 successful trajectories improves robustness and generalizes to unseen tool domains." [section 4.3, Table 3] "Although the SFT data is collected from the retail tool environment, the trained models also improve on the AIRLINE domain... this cross-domain generalization suggests that trajectory-based SFT teaches general tool-calling behaviors and reasoning patterns."
- **Break condition**: If trajectory diversity is insufficient (e.g., all trajectories use the same tool-calling pattern), SFT may overfit to surface form rather than decision logic, failing to transfer.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Process (POMDP) with Non-Stationary Intent**
  - **Why needed here**: The paper models tool-calling as decision-making under partial observability, where user intent can drift mid-dialogue. Understanding POMDPs clarifies why the agent must maintain belief states and update plans dynamically.
  - **Quick check question**: Given an agent that observes user message "cancel my order" followed by "actually, exchange it instead," what belief-state update should occur, and how does this differ from a fixed-intent MDP?

- **Concept: Supervised Fine-Tuning with Chain-of-Thought Distillation**
  - **Why needed here**: The SFT objective includes reasoning tokens, not just final actions. This requires understanding how to format training data to include intermediate reasoning and why this aids generalization.
  - **Quick check question**: If you strip reasoning tokens from training trajectories and keep only final tool calls, what performance changes would you expect on infeasible-intent tasks?

- **Concept: Synthetic Data Verification via Closed-Loop Execution**
  - **Why needed here**: The pipeline's verifiability depends on executing trajectories in a simulated environment and checking final database state. Understanding closed-loop evaluation is critical for reproducing or extending the benchmark.
  - **Quick check question**: A trajectory includes `cancel_pending_order` followed by `get_order_detail`. If the database shows the order is still pending after execution, what does this indicate about trajectory validity?

## Architecture Onboarding

- **Component map**: User Sampler -> Tool Sampling (DAG walk) -> Explorer LLM -> Trajectory Filter -> Summarizer LLM -> Validity-Check LLM -> Training Data Generator
- **Critical path**: 1. User sampling → Tool sampling (DAG walk) → Trajectory exploration → Filtering 2. Filtered trajectory → Task summarization (with scenario-specific prompt) → Validity check 3. Valid task → Simulation rollout → Successful trajectory → SFT data formatting 4. SFT on 2,872 trajectories → Evaluation on Retail-3I + Tau2-Bench domains
- **Design tradeoffs**:
  - **Explorer model strength vs. cost**: Paper uses Claude-4.5-Sonnet for exploration; weaker models may produce noisier trajectories requiring more aggressive filtering.
  - **Validity-check strictness vs. data yield**: Higher score thresholds reduce false positives but may discard diverse tasks. Paper does not report explicit threshold values.
  - **Scenario complexity vs. task clarity**: Changing-intent tasks must explicitly document intent drift; overly complex drifts may fail validity-check relevance scoring.
  - **Assumption**: The current TAU2-BENCH framework limits user simulator expressiveness; real user distributions (adversarial, long-term preferences) are not modeled.
- **Failure signatures**:
  - **Trajectory filter over-aggressive**: If filter criteria are too strict, generated tasks may lack diversity (e.g., all tasks involve simple cancellations).
  - **Validity-check LLM hallucinates criteria**: If validity-check prompt is underspecified, LLM may apply inconsistent standards, introducing label noise.
  - **SFT overfits to retail-specific tool names**: If reasoning tokens are insufficient or repetitive, models may memorize tool-calling patterns that don't generalize to Airline domain.
  - **Infeasible tasks mislabeled**: If "forbidden actions" are under-specified, models may learn incorrect policy constraints (e.g., attempting cancellation without authentication).
- **First 3 experiments**:
  1. **Reproduce Retail-3I benchmark**: Generate 100 exploratory trajectories, apply filtering and summarization, run validity check, evaluate Claude-3.5-Sonnet and Qwen3-8B on resulting tasks. Compare Pass@k rates to paper's Table 1.
  2. **Ablate validity-check strictness**: Vary validity-check score thresholds (e.g., require all scores ≥7 vs. ≥5) and measure impact on task yield and downstream SFT performance on infeasible-intent scenarios.
  3. **Cross-domain transfer probe**: Fine-tune Qwen3-4B on Retail-only trajectories, evaluate on Airline domain. Analyze failure cases: are errors due to tool-selection, argument-filling, or policy-compliance? Compare to paper's Table 3 results.

## Open Questions the Paper Calls Out
- **Can the Trajectory2Task pipeline maintain robustness and verifiability when applied to open-world tool ecosystems with dynamic or significantly larger API schemas (e.g., MCP servers) beyond the static TAU2-BENCH environment?**
  - **Basis**: [explicit] The authors note in the Limitations that they "instantiate Trajectory2Task only on TAU2-BENCH" due to cost and suggest future work should "apply the pipeline to other realistic tool environments."
  - **Why unresolved**: The current validation relies on a controlled retail environment; it is unclear if the validity-check LLM scales to more complex or disordered tool sets without human intervention.
  - **What evidence would resolve it**: Successful generation of high-validity tasks and trajectories in a live, open-world tool environment like MCP-Universe.

- **To what extent does the LLM-driven user simulator accurately reflect real-world user behavior distributions, such as adversarial attempts or nuanced social dynamics, compared to human-grounded logs?**
  - **Basis**: [explicit] The Limitations section states the simulator "may not fully reflect real user behavior distributions" and suggests future work should "validate simulators with human-grounded logs."
  - **Why unresolved**: The current approach models user behavior via prompting, which may lack the unpredictability or specific drift patterns found in actual human-agent interactions.
  - **What evidence would resolve it**: A comparative study measuring the behavioral gap between the synthetic user simulator and human crowd-workers acting out the same complex intent scenarios.

- **Does extending the current POMDP formulation to include richer user states and longer-horizon memory improve the agent's ability to handle complex, multi-session intent drift?**
  - **Basis**: [explicit] The authors list "extend the underlying framework to support richer user state [and] longer-horizon memory" as a specific limitation and area for future work.
  - **Why unresolved**: The current model updates beliefs based on immediate dialogue history but may fail to capture long-term user preferences or history spanning multiple distinct sessions.
  - **What evidence would resolve it**: Performance improvements on a modified benchmark where tasks require recalling specific user constraints or preferences established in previous, separate dialogue sessions.

## Limitations
- The pipeline is currently instantiated only on the TAU2-BENCH environment due to cost, limiting generalizability to open-world tool ecosystems.
- The LLM-driven user simulator may not fully reflect real user behavior distributions, such as adversarial attempts or nuanced social dynamics.
- The current POMDP formulation does not support richer user states or longer-horizon memory, limiting the ability to handle complex, multi-session intent drift.

## Confidence
- **High confidence**: The trajectory-first synthesis mechanism guarantees verifiable ground truth by construction, and the basic SFT training procedure is standard and well-documented.
- **Medium confidence**: The controlled intent perturbation approach effectively reveals failure modes, but the realism of these scenarios depends heavily on user simulator quality which isn't fully characterized.
- **Low confidence**: The claim that trajectory-based SFT teaches general tool-calling behaviors beyond memorization requires more evidence - the cross-domain transfer results are promising but not systematically explained.

## Next Checks
1. **Cross-domain transfer analysis**: Fine-tune Qwen3-4B on Retail trajectories, then systematically categorize Airline domain errors (tool-selection vs. argument-filling vs. policy-compliance) to determine if improvements reflect genuine reasoning transfer or surface pattern matching.
2. **Validity-check sensitivity analysis**: Vary the validity-check score thresholds (e.g., require all scores ≥7 vs. ≥5) and measure the impact on both task yield and downstream SFT performance, particularly on infeasible-intent scenarios.
3. **Reasoning token ablation study**: Compare SFT performance with full trajectories versus trajectories stripped of reasoning tokens to isolate whether improvements stem from procedural pattern learning or mere tool-name memorization.