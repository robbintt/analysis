---
ver: rpa2
title: 'FLAMES: A Hybrid Spiking-State Space Model for Adaptive Memory Retention in
  Event-Based Learning'
arxiv_id: '2504.01257'
source_url: https://arxiv.org/abs/2504.01257
tags:
- flames
- temporal
- memory
- state
- spike
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FLAMES introduces a hybrid spiking-state space model for event-based
  learning, addressing the challenge of retaining long-range temporal dependencies
  in spiking neural networks. The key innovation is the Spike-Aware HiPPO (SA-HiPPO)
  mechanism, which dynamically adjusts memory retention based on inter-spike intervals,
  preserving both short- and long-range dependencies.
---

# FLAMES: A Hybrid Spiking-State Space Model for Adaptive Memory Retention in Event-Based Learning

## Quick Facts
- arXiv ID: 2504.01257
- Source URL: https://arxiv.org/abs/2504.01257
- Reference count: 40
- Primary result: FLAMES achieves state-of-the-art results on Long Range Arena benchmark and event datasets, demonstrating scalable long-range reasoning in event-driven systems

## Executive Summary
FLAMES addresses the challenge of retaining long-range temporal dependencies in spiking neural networks by introducing a hybrid spiking-state space model. The key innovation is the Spike-Aware HiPPO (SA-HiPPO) mechanism, which dynamically adjusts memory retention based on inter-spike intervals. This allows the model to preserve both short- and long-range dependencies while maintaining the sparsity and efficiency of spiking neural networks. To achieve computational efficiency, FLAMES employs a normal-plus-low-rank (NPLR) decomposition that reduces complexity from O(N²) to O(Nr).

## Method Summary
FLAMES is a five-component architecture that processes event-based inputs as (x, y, t, p) spike tuples. The model consists of a Dendrite Attention Layer with DH-LIF neurons featuring multiple dendritic branches with heterogeneous time constants, spatial pooling, SA-HiPPO convolution with NPLR decomposition, layer normalization, and event-pooling readout. The SA-HiPPO mechanism adapts the state-space updates to the event-driven nature of spiking neural networks by incorporating inter-spike intervals into the memory retention process. The NPLR decomposition enables efficient state updates while maintaining expressivity through structured matrix decomposition.

## Key Results
- Achieves state-of-the-art accuracy on Long Range Arena benchmark tasks
- Demonstrates superior performance on event datasets (HAR-DVS, Celex-HAR, DVS Gesture)
- Maintains computational efficiency with O(Nr) complexity versus traditional O(N²) approaches
- Successfully captures multi-timescale temporal dynamics through dendritic attention mechanism

## Why This Works (Mechanism)

### Mechanism 1: Spike-Aware HiPPO (SA-HiPPO) for Adaptive Memory Retention
The SA-HiPPO mechanism dynamically adjusts memory retention based on inter-spike intervals, allowing short-term reinforcement during frequent spiking and long-term preservation during sparse intervals. The adaptive state transition matrix A_S = A ∘ F(Δt) combines a base HiPPO matrix with an element-wise decay matrix F_ij(Δt) = e^(-α_ij·Δt). This enables the model to selectively retain memory based on temporal distance from spike events.

### Mechanism 2: NPLR Decomposition for Efficient State Updates
The normal-plus-low-rank decomposition reduces computational complexity from O(N²) to O(Nr) while maintaining expressivity. The state matrix is decomposed as A_S = VΛV* - PQ* where V is unitary, Λ is diagonal, and P, Q ∈ C^(N×r) are low-rank matrices. This enables efficient matrix-vector products and FFT-based convolution.

### Mechanism 3: Dendritic Attention for Multi-Timescale Feature Extraction
Multiple dendritic branches with heterogeneous time constants τ_d capture temporal dynamics across diverse scales from asynchronous spike inputs. Each dendritic branch computes i_d(t+1) = α_d·i_d(t) + Σ w_j·p_j with branch-specific decay α_d = e^(-1/τ_d), enabling the model to extract features at multiple temporal resolutions.

## Foundational Learning

- **State Space Models (SSMs) and HiPPO Framework**: Understanding how HiPPO compresses input history via polynomial projections is essential for grasping SA-HiPPO's memory compression strategy. Quick check: Can you explain how the HiPPO matrix A encodes optimal polynomial projection for memory compression?

- **Leaky Integrate-and-Fire (LIF) Neuron Dynamics**: The Dendritic Attention Layer builds on LIF with multi-compartment extensions; membrane potential decay and spike thresholding are foundational. Quick check: How does the decay factor β = e^(-1/τ_s) affect the temporal integration window of an LIF neuron?

- **Event-Based / Neuromorphic Data Representation**: FLAMES processes inputs as (x, y, t, p) spike tuples; understanding sparse, asynchronous event streams is critical. Quick check: What is the key difference between frame-based and event-based vision in terms of temporal resolution and computational sparsity?

## Architecture Onboarding

- **Component map:** Input spikes → dendritic integration (multi-τ filtering) → spatial pooling → SA-HiPPO state evolution → thresholded output
- **Critical path:** The SA-HiPPO layer is the memory bottleneck; errors here propagate to all downstream tasks. State updates occur asynchronously at spike times using the adaptive transition matrix.
- **Design tradeoffs:** Larger state dimension N increases memory capacity but computational cost; more dendritic branches capture more timescales but increase parameters; FFT convolution efficient for long sequences but time-domain better for short sequences.
- **Failure signatures:** Accuracy drops on long-range tasks → check if α decay is too aggressive; high latency on HD streams → verify NPLR decomposition; poor performance on irregular spike patterns → inspect {τ_d} coverage
- **First 3 experiments:**
  1. Train FLAMES-Tiny on Sequential CIFAR-10; verify accuracy ≈ 83%. If significantly lower, check SA-HiPPO state initialization.
  2. Remove dendritic attention layer on DVS Gesture; expect ~1-2% accuracy drop. Larger drops indicate implementation error.
  3. Run FLAMES-Normal on Celex-HAR (1280×800); monitor latency. Should be <2 ms on A100. If higher, verify FFT convolution path.

## Open Questions the Paper Calls Out

### Open Question 1
How does the energy efficiency and latency of FLAMES compare when deployed on neuromorphic hardware versus the GPU implementation presented in the paper? While the Introduction emphasizes "energy-efficient neuromorphic computing," all experiments are conducted on NVIDIA A100 GPU. The actual energy savings and real-time latency can only be verified on hardware that exploits sparsity (e.g., Loihi, TrueNorth, or FPGA event processors).

### Open Question 2
How does the second-order Taylor approximation of the matrix exponential impact performance stability when inter-spike intervals (Δt) become significantly large? Equation (5) approximates the matrix exponential using a second-order Taylor expansion, and the error bound grows with ||A·Δt||^(n+1). For extremely sparse data or large state norms, the approximation error could theoretically destabilize the state updates.

### Open Question 3
Does the NPLR decomposition function as a critical regularizer for the SA-HiPPO matrix, or merely a computational speedup? In Table 8, removing NPLR results in lower accuracy (88.05%) than the full model (90.25%) despite much higher computational cost. This suggests the low-rank constraint may be enforcing a beneficial structure or stability on the spike-aware state dynamics.

## Limitations

- Training hyperparameters (learning rate, batch size, epochs, optimizer, weight decay) are not specified, making exact reproduction challenging
- The number of repeated SA-HiPPO convolution blocks is not explicitly stated
- Implementation details of the surrogate gradient function for backpropagation through spikes are missing
- Pooling window sizes for spatial and event pooling are not specified

## Confidence

- **High Confidence:** The NPLR decomposition mechanism and its computational complexity reduction from O(N²) to O(Nr) - supported by formal proof in Lemma 1
- **Medium Confidence:** The SA-HiPPO mechanism's ability to preserve both short- and long-range dependencies - based on theoretical formulation but limited direct empirical validation in corpus
- **Medium Confidence:** The Dendritic Attention Layer's effectiveness for multi-timescale feature extraction - supported by ablation results (1-2% accuracy drop when removed) but lacks extensive ablation studies

## Next Checks

1. **Sanity Check:** Train FLAMES-Tiny on Sequential CIFAR-10; verify accuracy ≈ 83% per Table 9. If significantly lower, check SA-HiPPO state initialization and dendritic branch implementation.

2. **Memory Retention Test:** Implement a controlled experiment varying inter-spike intervals (Δt) on a synthetic sequence classification task to empirically validate SA-HiPPO's adaptive memory retention claims.

3. **Scaling Efficiency Validation:** Benchmark FLAMES-Normal on Celex-HAR (1280×800) and verify inference latency <2 ms on A100 GPU, confirming the practical benefits of NPLR decomposition for large-scale event processing.