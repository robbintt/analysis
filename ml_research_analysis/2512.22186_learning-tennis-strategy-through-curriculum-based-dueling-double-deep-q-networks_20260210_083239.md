---
ver: rpa2
title: Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks
arxiv_id: '2512.22186'
source_url: https://arxiv.org/abs/2512.22186
tags:
- learning
- opponent
- tennis
- agent
- dueling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a reinforcement learning framework for optimizing
  tennis strategy using a custom simulation environment combined with a Dueling Double
  Deep Q-Network trained via curriculum learning. The environment models complete
  tennis scoring, rally-level tactical decisions, symmetric fatigue dynamics, and
  parameterized opponent skill.
---

# Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks

## Quick Facts
- arXiv ID: 2512.22186
- Source URL: https://arxiv.org/abs/2512.22186
- Authors: Vishnu Mohan
- Reference count: 17
- Primary result: Dueling Double DQN with curriculum learning achieves 98%+ win rates in tennis strategy simulation

## Executive Summary
This paper introduces a reinforcement learning framework for optimizing tennis strategy using a custom simulation environment combined with a Dueling Double Deep Q-Network trained via curriculum learning. The environment models complete tennis scoring, rally-level tactical decisions, symmetric fatigue dynamics, and parameterized opponent skill. The dueling architecture with double Q-learning addresses sparse rewards and high-variance value estimates in this long-horizon stochastic domain. Curriculum learning progressively increases opponent difficulty, enabling robust skill acquisition where fixed-difficulty training fails completely.

## Method Summary
The framework consists of a tennis simulation environment with 18-dimensional state space and 10 discrete actions, a dueling deep Q-network that separates state-value from action-advantage estimation, and curriculum learning that progressively increases opponent difficulty through four phases. The agent uses experience replay with a buffer of 20,000 transitions and periodic target network updates. Training runs for 1500 episodes with opponent skill increasing from 0.40 to 0.50, achieving stable convergence where standard DQN fails to learn effective policies.

## Key Results
- Trained agent achieves 98.2-100% win rates against balanced opponents (skill 0.50)
- Performance remains strong at 98% win rate against challenging opponents (skill 0.55)
- Serve efficiency reaches 63.0-67.5% and return efficiency reaches 52.8-57.1%
- Ablation studies confirm both dueling architecture and curriculum learning are essential for stable convergence

## Why This Works (Mechanism)

### Mechanism 1: Dueling Architecture Decomposition
The dueling network architecture stabilizes learning in tennis by separating state-value estimation from action-advantage estimation. The Q-network splits into two streams after shared feature layers: V(s) estimates the scalar value of being in a state, while A(s,a) captures relative action advantages. These combine as Q(s,a) = V(s) + A(s,a) - mean(A). In tennis rallies, many states have similar baseline value regardless of action—the dueling structure allows V(s) to absorb this shared signal, leaving A(s,a) to learn only action-specific differences with lower variance.

### Mechanism 2: Double Q-Learning Bias Reduction
Double Q-learning prevents overestimation bias that would otherwise destabilize training in this long-horizon stochastic domain. Standard DQN uses y_t = r_t + γ·max_a' Q(s', a'; θ⁻), where the same network both selects and evaluates actions, systematically overestimating values. Double DQN decouples this: y_t = r_t + γ·Q(s', argmax_a' Q(s', a'; θ); θ⁻). The online network (θ) selects actions; the target network (θ⁻) evaluates them independently.

### Mechanism 3: Curriculum Learning for Stable Skill Acquisition
Progressive opponent difficulty enables convergence where fixed-difficulty training fails completely. The curriculum increases opponent skill over 4 phases: 0.40→0.44→0.47→0.50. Early phases (skill 0.40) allow the agent to win ~60% of matches, generating positive reward experiences for value bootstrapping. As skill increases, the agent has already learned basic tactics and can refine policies under moderate challenge.

## Foundational Learning

- **Q-Learning and Temporal-Difference Updates**
  - Why needed here: The agent must learn Q(s,a) values through bootstrapped updates. Tennis matches span 400-900 steps with delayed rewards, requiring understanding of γ (discount factor, set to 0.99) and how TD errors propagate credit across long horizons.
  - Quick check question: Can you explain why γ=0.99 is appropriate for a task where rewards arrive primarily at point/match conclusions?

- **Experience Replay and Target Networks**
  - Why needed here: Training stability in deep Q-learning requires decorrelated samples (via replay buffer of 20,000 transitions) and stable bootstrap targets (via target network updated every 5 episodes). Without these, Q-values diverge.
  - Quick check question: Why does using the same network to select and evaluate actions cause overestimation, and how does a target network help?

- **Phase-Dependent Action Masking**
  - Why needed here: Tennis has context-specific valid actions—serve actions only during service games, return actions when receiving, rally actions otherwise. The agent must learn when each action group is available rather than treating all 10 actions as always valid.
  - Quick check question: What would happen if the agent could select "serve flat wide" during an opponent's service game?

## Architecture Onboarding

- **Component map:**
  - TennisEnvironment: 18-dim state space (scores, fatigue, positions, rally length), 10 discrete actions, probabilistic transitions with skill/fatigue modifiers
  - DuelingQNetwork: Input(18) → Shared(128→128) → V-stream(64→1) + A-stream(64→10) → Q-combination
  - ReplayBuffer: Capacity 20,000, stores (s, a, r, s', done) tuples
  - CurriculumScheduler: Maps episode number to opponent skill [0.40, 0.44, 0.47, 0.50]
  - get_valid_actions(): Returns context-appropriate action subset for masking

- **Critical path:**
  1. Initialize environment and verify scoring logic (play random matches, check deuce/tiebreak handling)
  2. Validate action masking—ensure invalid actions never execute
  3. Train vanilla DQN baseline to confirm failure mode (should plateau ~15% win rate)
  4. Deploy Dueling DDQN with curriculum; expect convergence by episode 1200
  5. Evaluate against multiple opponent skills (0.35–0.55) to test generalization

- **Design tradeoffs:**
  - Hard target updates (C=5) vs soft Polyak updates: Hard chosen for simplicity; soft tested but provided no benefit
  - Episode truncation at 750 steps: Prevents memory pressure but introduces incomplete-match resolution heuristic (compare sets→games→points)
  - CPU-only training (72 min): Accessible but slower; GPU would accelerate but isn't required
  - Discrete 10-action space: Limits tactical expressiveness but enables DQN; continuous control would require policy gradient methods

- **Failure signatures:**
  - Q-values exceeding 100 despite max rewards ~80 (overestimation, likely missing double Q-learning)
  - Win rate stuck <20% after 500 episodes (curriculum may be too aggressive or architecture inadequate)
  - Action distribution collapsing to single action (e.g., 75%+ defensive lob indicates reward shaping failure)
  - Training loss spiking without recovery at curriculum transitions (phase durations may be insufficient)

- **First 3 experiments:**
  1. **Environment validation**: Run 100 random-policy matches against skill 0.50; expect ~50% win rate. Verify scoring logic produces realistic game/set structures.
  2. **Vanilla DQN baseline**: Train standard DQN (no dueling, no double Q) for 1500 episodes against fixed skill 0.50. Confirm failure mode: win rate should plateau around 1–15% with unstable Q-values.
  3. **Full system with ablation check**: Train Dueling DDQN with curriculum. Monitor win rate progression through phases (target: 99%+ by phase 4). Remove curriculum to verify collapse; remove dueling to verify instability.

## Open Questions the Paper Calls Out
None

## Limitations
- Simulation environment abstracts many tactical nuances like player positioning dynamics and shot trajectories
- 10 discrete actions constrain tactical expressiveness compared to continuous control spaces
- Curriculum design is empirically derived rather than theoretically optimized

## Confidence
- **High confidence**: Dueling DDQN architecture improves stability over standard DQN (supported by ablation showing 1-15% vs 98%+ win rates)
- **Medium confidence**: Curriculum learning is essential for convergence (demonstrated by training collapse without it, though alternative scheduling methods weren't tested)
- **Medium confidence**: The specific skill progression and phase durations are near-optimal (empirically validated but not systematically tuned)

## Next Checks
1. **Environment Transfer Test**: Deploy the trained agent against a high-skill opponent (skill 0.55) not seen during training to assess generalization beyond the curriculum's upper bound.

2. **Architecture Ablation with Curriculum**: Retrain with Dueling architecture but without Double Q-learning (and vice versa) while maintaining curriculum to isolate each component's contribution to stability.

3. **Continuous Action Space Validation**: Implement a continuous-action variant (e.g., policy gradient method) to verify whether the tactical patterns learned transfer when moving beyond the discrete 10-action constraint.