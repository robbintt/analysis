---
ver: rpa2
title: 'KGBridge: Knowledge-Guided Prompt Learning for Non-overlapping Cross-Domain
  Recommendation'
arxiv_id: '2511.02181'
source_url: https://arxiv.org/abs/2511.02181
tags:
- knowledge
- cross-domain
- user
- recommendation
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KGBridge, a knowledge-guided prompt learning
  framework for cross-domain sequential recommendation under non-overlapping user
  scenarios. The method addresses key challenges in knowledge graph (KG)-enhanced
  cross-domain recommendation, including KG sparsity and popularity bias, dependence
  on overlapping users, and lack of semantic disentanglement between transferable
  and domain-specific knowledge.
---

# KGBridge: Knowledge-Guided Prompt Learning for Non-overlapping Cross-Domain Recommendation

## Quick Facts
- arXiv ID: 2511.02181
- Source URL: https://arxiv.org/abs/2511.02181
- Authors: Yuhan Wang; Qing Xie; Zhifeng Bao; Mengzi Tang; Lin Li; Yongjian Liu
- Reference count: 40
- One-line primary result: KGBridge achieves state-of-the-art performance in cross-domain sequential recommendation under non-overlapping user scenarios.

## Executive Summary
This paper introduces KGBridge, a knowledge-guided prompt learning framework for cross-domain sequential recommendation under non-overlapping user scenarios. The method addresses key challenges in knowledge graph (KG)-enhanced cross-domain recommendation, including KG sparsity and popularity bias, dependence on overlapping users, and lack of semantic disentanglement between transferable and domain-specific knowledge. KGBridge consists of two core components: a KG-enhanced Prompt Encoder that models relation-level semantics as soft prompts to provide structured and dynamic priors for user sequence modeling, and a Two-stage Training Paradigm that combines cross-domain pretraining and privacy-preserving fine-tuning to enable knowledge transfer without user overlap. By combining relation-aware semantic control with correspondence-driven disentanglement, KGBridge explicitly separates and balances domain-shared and domain-specific semantics. Extensive experiments on benchmark datasets demonstrate that KGBridge consistently outperforms state-of-the-art baselines, achieving significant improvements in recommendation accuracy across multiple cross-domain scenarios while remaining robust under varying KG sparsity levels.

## Method Summary
KGBridge addresses non-overlapping cross-domain sequential recommendation by learning relation-level semantics as soft prompts from knowledge graphs, then disentangling and transferring these prompts across domains without requiring shared users. The method uses a two-stage training paradigm: first jointly pretraining on all available domain data to learn transferable relational priors, then fine-tuning on the target domain while freezing domain-shared prompts and updating domain-specific components. Relation embeddings from TransE initialize prompt banks, which are then used to enrich item representations through attention-based fusion before being processed by a Transformer encoder. A correspondence-driven contrastive loss enforces semantic separation between shared and specific prompts during fine-tuning.

## Key Results
- KGBridge achieves state-of-the-art performance in cross-domain sequential recommendation, significantly outperforming baseline methods.
- The framework demonstrates robustness to KG sparsity, maintaining strong performance even with reduced knowledge graph information.
- KGBridge successfully enables knowledge transfer without requiring overlapping users between source and target domains.

## Why This Works (Mechanism)

### Mechanism 1
Shifting from entity-level to relation-level embeddings mitigates popularity bias and structural imbalance in sparse Knowledge Graphs (KGs). Relations are lower in cardinality and exhibit greater semantic consistency across domains compared to entities. By encoding relations into soft prompts, the model relies on a denser, more stable semantic space, reducing sensitivity to long-tail entity distributions. Core assumption: Relation distributions are more stable and less prone to sparsity than entity distributions. Break condition: If a KG has a highly skewed relation distribution (e.g., very few relations dominate) or relations lack semantic continuity across domains, this mechanism's effectiveness would be diminished.

### Mechanism 2
Explicit disentanglement of domain-shared and domain-specific knowledge via prompts with contrastive regularization stabilizes cross-domain transfer. The model uses separate prompt banks for shared and specific knowledge. During fine-tuning, shared prompts are frozen (semantic anchors), and specific prompts are updated. A correspondence-driven contrastive loss (`L_disen`) ensures the two prompt sets remain complementary rather than redundant or contradictory. Core assumption: User preferences can be decomposed into transferable (domain-shared) and context-dependent (domain-specific) components. Break condition: If user preferences are not easily separable or the contrastive loss weight is misconfigured, this could lead to negative transfer or suboptimal adaptation.

### Mechanism 3
A two-stage training paradigm enables knowledge transfer in non-overlapping user scenarios under privacy constraints. Stage 1 (Pre-training) jointly learns transferable relational priors from all available domain data. Stage 2 (Fine-tuning) adapts to the target domain by updating only domain-specific components and the sequence encoder, without requiring user overlap. This isolates adaptation while leveraging the pre-trained semantic bridge. Core assumption: The relational knowledge learned during pre-training is sufficiently general to serve as a bridge for the target domain. Break condition: If the source and target domains have no shared relations or fundamentally different relational semantics, the pre-trained priors will not transfer effectively.

## Foundational Learning

### Concept: Soft Prompts
Why needed here: KGBridge uses soft prompts (continuous, learnable embeddings) instead of textual prompts to condition the model. Understanding this is key to grasping how relation embeddings are transformed into semantic controllers. Quick check question: How do soft prompts differ from the text prompts used in LLMs like GPT?

### Concept: Knowledge Graph Embeddings (KGE) - TransE
Why needed here: The framework initializes prompts from KG relations using TransE. A basic understanding of translational distance models (h+r≈o) is required to comprehend the input to the Prompt Encoder. Quick check question: What geometric principle does TransE use to learn embeddings for entities and relations?

### Concept: Contrastive Learning (InfoNCE)
Why needed here: The disentanglement mechanism relies on an InfoNCE-based loss. Familiarity with positive/negative pairs and the role of temperature is necessary for understanding how the model enforces semantic separation. Quick check question: In the InfoNCE loss for disentanglement, what constitutes a "positive pair" and a "negative pair"?

## Architecture Onboarding

### Component map:
KG -> TransE Embeddings -> Prompt Generators -> P_shared, P_spec -> Knowledge-guided Item Enrichment -> Transformer Encoder -> Softmax Classifier

### Critical path:
Stage 1 (Pretraining): KG, all domain user sequences -> PromptGenerator(R_*) -> Knowledge-guided Item Enrichment -> TransformerEncoder -> Softmax Classifier -> L_pretrain (Cross-entropy) -> Trained shared/specific prompts and sequence encoder

Stage 2 (Fine-tuning): Target domain user sequences, frozen P_shared, pre-trained other components -> Forward pass with frozen shared prompts -> L_finetune = L_rec + λL_disen -> Final recommendation model for the target domain

### Design tradeoffs:
Aggregation Strategy: The paper chooses "mean pooling with stochastic noise" over attention/transformer pooling to avoid overfitting to high-frequency relations, trading some expressive power for robustness. Prompt Initialization: Initializing solely from relation embeddings (not entities) avoids popularity bias but may miss fine-grained entity-level nuances. Two-Stage Paradigm: Separates pre-training and fine-tuning, which is effective for non-overlapping users but requires a more complex training pipeline than single-stage models.

### Failure signatures:
Negative Transfer: Performance drops if shared prompts capture domain-specific noise. Symptom: Pre-training loss decreases, but fine-tuning fails to converge or performance is poor. Semantic Drift: Specific prompts diverge too far from shared prompts during fine-tuning. Symptom: L_disen increases or oscillates; low robustness to KG sparsity. Overfitting to Source Domain: The model performs well on source domains but fails on the target. Symptom: Large gap between source and target metrics.

### First 3 experiments:
Baseline Comparison: Replicate the non-overlapping user setup from the paper (e.g., FB-Movie → FB-Book). Train KGBridge and compare against SASRec and MCRPL. This validates the overall performance claim. Ablation on Disentanglement (-Disen): Run fine-tuning with λ=0. Compare performance to the full model. This isolates the contribution of the correspondence-driven disentanglement loss. Robustness Test: Randomly remove 40% of triples from the KG and run the full training pipeline. Compare performance degradation against a baseline like GRU4RecKG to verify robustness to KG sparsity.

## Open Questions the Paper Calls Out
In future work, we plan to integrate large language models with incomplete knowledge graphs to enhance semantic reasoning and further improve cross-domain knowledge transfer in dynamic environments.

## Limitations
Architectural completeness: The paper does not specify Transformer layer count, attention heads, or feed-forward dimensions, which are critical for exact reproduction and performance benchmarking. KG preprocessing: Exact details on entity linking to DBpedia and KG filtering thresholds are not provided, potentially affecting relation coverage and downstream prompt initialization. Noise specification: The "stochastic noise" added during prompt aggregation lacks distribution details (e.g., Gaussian variance), introducing variability in prompt bank initialization.

## Confidence
High: The core claim that KGBridge achieves state-of-the-art performance under non-overlapping user scenarios is well-supported by experimental results and consistent with prior work on relation-level modeling and prompt disentanglement. Medium: The mechanism claims about robustness to KG sparsity and disentanglement benefits are plausible but rely on assumed generalization from ablation studies; exact ablation results are not shown in the provided text. Low: The claim that KGBridge generalizes to arbitrary domain pairs is not fully validated, as only three datasets are tested; the method's behavior with highly dissimilar domains remains unclear.

## Next Checks
Prompt bank sensitivity: Systematically vary the stochastic noise scale and measure impact on downstream recommendation accuracy; determine if performance is robust to initialization noise. Cross-domain transfer limits: Test KGBridge on a pair of domains with minimal relation overlap (e.g., FB-Movie → Amazon Electronics) to assess transfer boundaries and identify failure conditions. Disentanglement ablation: Run full experiments with λ=0 (no disentanglement loss) and compare performance to the full model; quantify the marginal benefit of correspondence-driven contrastive regularization.