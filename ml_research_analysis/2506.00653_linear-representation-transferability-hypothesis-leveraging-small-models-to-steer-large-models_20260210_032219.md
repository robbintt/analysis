---
ver: rpa2
title: 'Linear Representation Transferability Hypothesis: Leveraging Small Models
  to Steer Large Models'
arxiv_id: '2506.00653'
source_url: https://arxiv.org/abs/2506.00653
tags:
- steering
- linear
- representation
- hypothesis
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the Linear Representation Transferability (LRT)
  Hypothesis, which posits that neural networks trained on similar data can have their
  representations linearly mapped to one another. The authors introduce a conceptual
  framework where representations are expressed as linear combinations of universal
  basis features, with each model projecting these onto its own subspace.
---

# Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models

## Quick Facts
- arXiv ID: 2506.00653
- Source URL: https://arxiv.org/abs/2506.00653
- Reference count: 19
- Primary result: Small models' steering vectors can be linearly mapped to guide large models' behavior with 0.8-0.98 correlation

## Executive Summary
This paper proposes the Linear Representation Transferability (LRT) Hypothesis, which posits that neural networks trained on similar data can have their representations linearly mapped to one another. The authors introduce a conceptual framework where representations are expressed as linear combinations of universal basis features, with each model projecting these onto its own subspace. They empirically test this by learning affine mappings between hidden states of Gemma-2B and Gemma-9B models, and evaluating whether steering vectors can be transferred between them. The results show that affine mappings successfully preserve steering behaviors, with high correlation (0.8-0.98) and low mean squared error (0.013-0.087) when transferring steering vectors from smaller to larger models. The findings suggest that representations learned by small models can guide the behavior of larger models, offering practical implications for efficient inference, model distillation, and performance prediction.

## Method Summary
The authors learn affine mappings (A, p) between hidden states of Gemma-2B and Gemma-9B models by minimizing reconstruction loss over 10M tokens from distribution-matched datasets. Steering vectors are computed on the source model using Contrastive Activation Addition (CAA) with positive (Alpaca) and negative (AdvBench) prompt datasets. The steering vectors are then transferred to the target model via the learned affine transformation, and their behavioral effects are evaluated. The method supports both hidden-to-hidden (h2h) and sparse autoencoder-to-hidden (s2l) mapping pathways, with the latter requiring pretrained SAEs. Layer matching is done via relative depth rather than absolute indices.

## Key Results
- Affine mappings preserve steering behaviors with correlation 0.8-0.98 between direct and transferred steering vectors
- Transferred steering vectors achieve comparable refusal score curves to natively-learned vectors across steering strengths
- Decoder matrix reconstruction error between models is ~114 (Frobenius norm), substantially lower than random matrix baseline
- Validation loss for affine mappings remains below 6.0 across tested layer pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models trained on similar data distributions with shared tokenizers learn representations that occupy different subspaces of a common universal feature space, enabling linear transfer.
- Mechanism: Each model projects a universal feature matrix W_U through model-specific projection matrices (P_S, P_T), creating feature subspaces that remain linearly related. When both models activate the same features with similar strengths, hidden states relate via h_T ≈ A·h_S + p.
- Core assumption: The coefficients c_S(x) and c_T(x) activating features are approximately equal across models (after accounting for feature reordering).
- Evidence anchors:
  - [abstract]: "representations learned across models trained on the same data can be expressed as linear combinations of a universal set of basis features"
  - [Section 2.2]: Formal derivation showing if R_S^(-1)c_S = R_T^(-1)c_T = c(x), then h_T = Ah_S + p where A = P_T^T(P_S^T)^†
  - [corpus]: Cross-model semantics work (arxiv:2508.03649) explores structural constraints for alignment, but direct evidence for universal feature spaces remains limited
- Break condition: Different tokenizers, dissimilar training distributions, or extreme scale differences (e.g., 100M → 70B may fail)

### Mechanism 2
- Claim: Steering vectors computed on small models retain semantic effects when transferred to large models via learned affine mappings.
- Mechanism: Steering vectors represent directions in activation space (differences in mean activations between positive/negative prompts). The affine transformation preserves directional relationships, so functional interventions transfer.
- Core assumption: Layers with similar relative depth perform similar computations across model scales.
- Evidence anchors:
  - [abstract]: "steering vectors—directions in hidden state space associated with specific model behaviors—retain their semantic effect when transferred"
  - [Section 3.3, Figure 4]: Transferred steering vectors achieve comparable refusal score curves to natively-learned vectors across steering strengths
  - [corpus]: arxiv:2506.06609 independently demonstrates affine mappings can transfer SAE features between models
- Break condition: Mismatched layer depths (e.g., early layer in source → late layer in target)

### Mechanism 3
- Claim: Sparse autoencoder decoder matrices from different models are approximately linearly reconstructible from each other.
- Mechanism: If both models' features lie in subspaces of W_U, their decoder matrices W_S and W_T satisfy W_T ≈ W_S·M for some linear map M.
- Core assumption: SAEs successfully isolate monosemantic features corresponding to universal basis features.
- Evidence anchors:
  - [Section 2.2]: Reconstruction error of ~114 (Frobenius norm) between Gemma-2B and Gemma-9B decoder matrices, substantially lower than random matrix baseline
  - [Figure 2]: Visual comparison showing low reconstruction error vs. random projection baseline
  - [corpus]: Weak direct evidence; concurrent work takes different approaches to feature transfer
- Break condition: SAEs trained on insufficiently representative data

## Foundational Learning

- **Concept: Contrastive Activation Addition (CAA)**
  - Why needed here: Primary method for computing steering vectors from positive/negative prompt datasets
  - Quick check question: Given datasets D_p and D_n, what is the formula for steering vector v_l at layer l?

- **Concept: Affine Transformations (y = Ax + p)**
  - Why needed here: Core mathematical object being learned to map between representation spaces
  - Quick check question: What constraints on A determine if the mapping is invertible when d_S ≠ d_T?

- **Concept: Sparse Autoencoders for Feature Decomposition**
  - Why needed here: Used to validate universal feature space hypothesis and enable feature-to-hidden (s2l) mapping
  - Quick check question: In h(x) = W_U^T c(x) + b, what do W_U and c(x) each represent?

## Architecture Onboarding

- **Component map:**
  - Source model (small) → hidden states h_S^{l_S} ∈ R^{d_S}
  - Target model (large) → hidden states h_T^{l_T} ∈ R^{d_T}
  - Affine mapping: A ∈ R^{d_T × d_S}, p ∈ R^{d_T}
  - Optional: SAE encoder E_S for s2l pathway

- **Critical path:**
  1. Collect paired activations (h_S, h_T) on ~10M tokens from distribution matching training data
  2. Train affine mapping minimizing E[||A·h_S + p - h_T||²]
  3. Compute steering vector v_S on source model via CAA
  4. Transfer: ṽ_T = A·v_S + p
  5. Apply steering: h'_T ← h_T + α·ṽ_T/||ṽ_T||

- **Design tradeoffs:**
  - s2l vs l2l: Feature-to-hidden preserves more semantics but requires pretrained SAE; hidden-to-hidden is simpler but potentially lossier
  - Dataset choice: The Pile for base models; add chat data (33%) for instruction-tuned models
  - Layer matching: Use relative depth (e.g., layer 20/26 for both), not absolute indices

- **Failure signatures:**
  - Validation loss > 6.0 suggests poor mapping fit
  - Transferred steering produces opposite or inconsistent behavioral changes
  - High variance in steering effectiveness across α values

- **First 3 experiments:**
  1. **Baseline reconstruction**: Train l2l mapping on The Pile, report final validation loss; compare reconstruction error against random A initialization
  2. **Layer sweep**: Test 5+ layer pairs across relative depths; identify which pairs yield lowest mapping loss and best steering transfer
  3. **Single-behavior transfer**: Choose "capitalization" or "French speaking" behavior; compare propensity scores (mLD) between direct steering on target vs. transferred steering from source

## Open Questions the Paper Calls Out

None

## Limitations

- The universal feature space hypothesis remains largely unproven, with evidence primarily from SAE decoder reconstruction rather than direct experimental validation
- Method effectiveness across vastly different model scales (e.g., 100M → 70B) remains unexplored and speculative
- Layer matching via relative depth heuristics may not capture functional equivalence across all model pairs

## Confidence

**High Confidence (9/10):**
- Affine mappings between Gemma-2B and Gemma-9B successfully preserve steering behaviors (Section 3.3)
- Linear projections between decoder matrices are feasible with measurable reconstruction error (Section 2.2)
- Steering vector transfer produces comparable behavioral changes to direct steering (Figure 4)

**Medium Confidence (6/10):**
- The universal feature space hypothesis (Section 2.1) is plausible but not conclusively proven
- Layer matching via relative depth works well for Gemma-2B↔Gemma-9B but generalizability is untested
- Sparse autoencoder-based s2l mapping adds fidelity but requires pretrained SAEs

**Low Confidence (3/10):**
- Performance claims across vastly different scales (e.g., 100M → 70B) are speculative
- Universal feature space assumptions lack direct empirical validation beyond SAE analysis
- Cross-tokenizer transfer remains theoretical without experimental validation

## Next Checks

1. **Cross-Scale Validation**: Test the affine mapping approach between models with extreme scale differences (e.g., 100M → 1B or 2B → 70B) to identify the limits of linear transferability. Measure mapping loss and steering effectiveness across this expanded scale range.

2. **Universal Feature Space Verification**: Conduct controlled experiments where both models' SAEs are trained on the same data distribution, then measure whether their decoder matrices can be linearly reconstructed with lower error than random baselines. Compare against SAEs trained on different distributions.

3. **Cross-Tokenizer Transfer**: Evaluate whether affine mappings and steering transfer work when source and target models use different tokenizers (e.g., Gemma vs Llama). Measure the degradation in mapping quality and steering effectiveness when tokenizer vocabularies differ substantially.