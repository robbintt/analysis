---
ver: rpa2
title: Harnessing Optimization Dynamics for Curvature-Informed Model Merging
arxiv_id: '2509.11167'
source_url: https://arxiv.org/abs/2509.11167
tags:
- merging
- curvature
- across
- layers
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OTA-Merging, a curvature-aware framework
  for merging specialized LLMs without joint retraining. The method leverages optimizer
  second-moment statistics as a diagonal curvature proxy, using Fast Fisher Grafting
  (FFG) to denoise parameter updates by reverting low-saliency fine-tuning deltas
  and aggregating the remaining parameters with curvature-preconditioned averaging.
---

# Harnessing Optimization Dynamics for Curvature-Informed Model Merging

## Quick Facts
- arXiv ID: 2509.11167
- Source URL: https://arxiv.org/abs/2509.11167
- Reference count: 40
- One-line primary result: OTA-Merging with FFG achieves 0.582 average score vs 0.565 TIES, outperforming strong baselines

## Executive Summary
This paper introduces OTA-Merging, a curvature-aware framework for merging specialized LLMs without joint retraining. The method leverages optimizer second-moment statistics as a diagonal curvature proxy, using Fast Fisher Grafting (FFG) to denoise parameter updates by reverting low-saliency fine-tuning deltas and aggregating the remaining parameters with curvature-preconditioned averaging. Experiments with five capability-based Llama 3.1 8B specialists (math, code, general, precise instruction following, knowledge recall) show that OTA-FFG achieves an average score of 0.582 across diverse benchmarks, outperforming strong baselines including TIES (0.565) and Fisher merging (0.541).

## Method Summary
OTA-Merging merges fine-tuned LLMs by treating optimizer second moments (exp_avg_sq) as a diagonal approximation of the Fisher Information Matrix. Fast Fisher Grafting (FFG) identifies and reverts low-saliency parameter changes based on curvature-weighted importance scores, effectively denoising the expert updates. The remaining task vectors are then aggregated using curvature-preconditioned averaging. The method can optionally compress second moments via rank-1 approximation for memory efficiency while maintaining performance.

## Key Results
- OTA-FFG achieves 0.582 average score across 8 benchmarks vs 0.565 TIES and 0.541 Fisher merging
- FFG consistently outperforms magnitude pruning, especially at high sparsity rates
- Visual analysis reveals shared curvature geometry across diverse SFT checkpoints, enabling effective linear merging
- Rank-1 compression reduces storage by 99% while maintaining performance (0.582 → 0.571)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Adam's second-moment accumulator (`exp_avg_sq`) serves as a low-cost proxy for the diagonal Fisher Information Matrix (FIM).
- **Mechanism**: Under specific theoretical conditions, the Hessian of the loss approximates the observed empirical FIM. Since Adam's second moment tracks the exponential moving average of squared gradients, it implicitly captures curvature information required for second-order optimization without re-computing Hessians.
- **Core assumption**: The model is in the "Late NTK Locality" phase (network is locally linear) and is "Perfectly Calibrated" (predictive distribution matches data distribution).
- **Evidence anchors**:
  - [abstract]: "leverages optimizer second-moment statistics as a diagonal curvature proxy"
  - [section 4.1]: "The second moment, vk... accumulates information about the diagonal of the empirical FIM over the optimization trajectory."
  - [corpus]: Neighbor paper "Bridging Training and Merging Through Momentum-Aware Optimization" supports the link between training momentum and merging, though specific proofs for Adam=Fisher remain theoretical.
- **Break condition**: If the model is under-fitted, poorly calibrated, or if batch statistics deviate significantly from the true gradient distribution, the proxy may fail.

### Mechanism 2
- **Claim**: Interference in model merging is driven by low-saliency parameter noise rather than geometric misalignment.
- **Mechanism**: Fast Fisher Grafting (FFG) identifies low-importance updates by calculating a saliency score $s_i = \Delta w_i^2 \cdot v_i$ (change squared weighted by curvature). It reverts these parameters to the base model values, effectively "denoising" the expert before merging.
- **Core assumption**: Large magnitude updates in high-curvature directions are essential for the specific task, whereas others are noise or conflicting signals.
- **Evidence anchors**:
  - [abstract]: "reverting low-saliency fine-tuning deltas... FFG consistently outperforms magnitude pruning."
  - [section 4.2]: "This score quantifies the importance of the change acquired during fine-tuning... deciding on a parameter-wise basis whether to retain a specialized update."
  - [corpus]: "Optimal Brain Iterative Merging" also utilizes saliency-based metrics for merging, validating the general approach of pruning before combining.
- **Break condition**: If the base model lacks the foundational knowledge for a task, reverting parameters to base values (even if low saliency for the fine-tune) might degrade capability transfer.

### Mechanism 3
- **Claim**: SFT checkpoints from distinct domains share similar curvature geometry, enabling effective linear aggregation.
- **Mechanism**: Visual analysis of second-moment matrices shows high overlap in curvature patterns across different experts (e.g., Math vs. Code). This implies that while weights differ, the "shape" of the loss basin is consistent, meaning a simple weighted average is geometrically sound.
- **Core assumption**: Fine-tuning converges to basins that are mode-connected or geometrically similar.
- **Evidence anchors**:
  - [abstract]: "Analysis shows SFT checkpoints share remarkably similar curvature geometry, explaining the effectiveness of linear merging."
  - [section 5.5]: "We observed a very high overlap between the subsets of input features that have the largest input curvatures for each model."
  - [corpus]: "Merging Beyond" explores similar geometric alignment, though the specific "shared curvature" finding is unique to this paper's analysis.
- **Break condition**: If experts are trained from radically different initializations or on fundamentally incompatible domains, the geometric alignment may degrade.

## Foundational Learning

- **Concept**: **Fisher Information Matrix (FIM)**
  - **Why needed here**: Understanding FIM as a measure of parameter importance/curvature is required to grasp why Adam's gradient statistics can replace it for weighting and pruning.
  - **Quick check question**: How does the diagonal approximation of FIM simplify the computational cost of weighting parameter updates?

- **Concept**: **Task Arithmetic**
  - **Why needed here**: The paper frames merging as operating on "task vectors" ($\Delta w = w_{expert} - w_{base}$). You must understand vector addition/subtraction in weight space to follow the grafting process.
  - **Quick check question**: If $\Delta w$ represents a skill, what does setting a subset of $\Delta w$ to zero (reverting to base) imply for the model's behavior?

- **Concept**: **AdaFactor / Rank-1 Compression**
  - **Why needed here**: The method relies on compressing the massive second-moment matrices to be practical. Understanding row/column factoring explains how they reduce storage from $O(n^2)$ to $O(n)$.
  - **Quick check question**: Why is maintaining non-negativity critical when factoring the second-moment matrix for optimization?

## Architecture Onboarding

- **Component map**: Base Model ($w_0$) -> Fine-tuned Experts ($w_\tau$) -> Optimizer States ($v_\tau$) -> FFG Module -> Compression Layer (Optional) -> Aggregation Engine -> Merged Model ($w_{merged}$)

- **Critical path**: The Optimizer State Integrity is the critical path. If the `exp_avg_sq` state from training is lost or corrupted, FFG cannot compute saliency, and OTA cannot compute curvature-aware weights. You cannot run this method on just the model checkpoints; you must have the training dynamics data.

- **Design tradeoffs**:
  - Storage vs. Accuracy: Storing full second moments doubles checkpoint size. The paper's Rank-1 compression reduces this drastically but incurs a small performance drop (0.582 → 0.571 avg score).
  - Sparsity vs. Transfer: Aggressive FFG sparsity (e.g., 10% density) improves interference mitigation but risks dropping rare but critical "knowledge" parameters (e.g., specific code syntax).

- **Failure signatures**:
  - Catastrophic Forgetting: If FFG reverts too many parameters in shared layers, the merged model may lose general capabilities found in the base model.
  - Rank Collapse in Compression: If the stable rank of second moments is higher than expected (contradicting Section 5.5), Rank-1 compression will destroy curvature information, degrading OTA performance.

- **First 3 experiments**:
  1. Saliency Baseline: Compare FFG (curvature-based) vs. Magnitude Pruning on a single expert to verify that curvature-weighted sparsity preserves performance better at high sparsity rates.
  2. Ablation Study: Run OTA aggregation without FFG on 2-3 experts. Verify that performance drops, confirming that denoising (not just weighting) is the primary driver of success.
  3. Rank-1 Stress Test: Implement the second-moment compression and compare merged model quality against the uncompressed version to quantify the memory/performance tradeoff.

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions in the text, but the limitations section raises several areas for future work including the theoretical justification for the curvature proxy assumption and the reliance on per-expert sparsity tuning.

## Limitations
- The theoretical grounding for using Adam's second-moment statistics as a Fisher Information Matrix proxy remains asymptotic and assumes specific conditions (late NTK locality, perfect calibration) that may not hold across all fine-tuning scenarios
- While the method demonstrates strong performance across diverse benchmarks, the analysis of when FFG reverts too many parameters (causing catastrophic forgetting) versus when it successfully preserves essential knowledge remains largely empirical rather than principled
- The reliance on per-expert hyperparameter search for sparsity ratios introduces computational overhead and limits scalability

## Confidence
**High Confidence**: The empirical superiority of OTA-FFG over baselines (0.582 vs 0.565 TIES, 0.541 Fisher merging) is well-supported by the experimental results. The consistent outperformance of FFG over magnitude pruning across sparsity levels is also robustly demonstrated.

**Medium Confidence**: The claim that SFT checkpoints share similar curvature geometry enabling linear aggregation is supported by visual analysis but lacks rigorous mathematical proof. The stability of rank-1 compression performance (0.582 → 0.571) is demonstrated but tested on a limited set of sparsity configurations.

**Low Confidence**: The theoretical justification for the curvature proxy assumption (Adam second moments ≈ FIM diagonal) relies on asymptotic analysis that may not generalize to practical fine-tuning scenarios with limited data or non-convex landscapes.

## Next Checks
1. **Calibration Stress Test**: Systematically vary model calibration (temperature scaling, label smoothing) during fine-tuning and measure how this affects OTA-FFG performance to identify the boundary conditions for the curvature proxy assumption.

2. **Interference Boundary Analysis**: Design experiments where base models are intentionally weakened on certain capabilities, then measure whether FFG's parameter reversion strategy causes disproportionate forgetting versus when the base is strong, to better understand the risk-reward tradeoff of denoising.

3. **Geometric Alignment Under Distribution Shift**: Train experts on datasets with increasing domain divergence (e.g., math→legal documents) and quantify the breakdown point where shared curvature geometry no longer supports effective linear merging, validating the geometric alignment hypothesis.