---
ver: rpa2
title: 'MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models'
arxiv_id: '2510.17519'
source_url: https://arxiv.org/abs/2510.17519
tags:
- video
- arxiv
- generation
- training
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the challenge of training large-scale video
  generation models, which are particularly resource-intensive due to cross-modal
  text-video alignment, long sequences, and complex spatiotemporal dependencies. To
  overcome these issues, the authors present a training framework that optimizes four
  pillars: data processing, model architecture, training strategy, and infrastructure.'
---

# MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models

## Quick Facts
- arXiv ID: 2510.17519
- Source URL: https://arxiv.org/abs/2510.17519
- Reference count: 40
- Primary result: MUG-V 10B matches SoTA video generators overall and surpasses leading open-source baselines on e-commerce-oriented video generation tasks in human evaluations.

## Executive Summary
This work addresses the challenge of training large-scale video generation models by optimizing four pillars: data processing, model architecture, training strategy, and infrastructure. The authors present MUG-V 10B, a 10B-parameter Diffusion Transformer trained on 70M video-text pairs, achieving state-of-the-art performance while maintaining high training efficiency through innovative compression and curriculum-based approaches.

## Method Summary
The training pipeline centers on a high-compression VideoVAE (8×8×8 spatiotemporal downsampling) that enables efficient latent-space diffusion training, a 10B DiT trained through a three-stage curriculum from images to 720p videos, and post-training preference optimization using human feedback. The system leverages Megatron-Core for distributed training across 500 H100 GPUs, achieving near-linear scaling through hybrid parallelism strategies. The complete stack, including model weights and training code, is open-sourced.

## Key Results
- MUG-V 10B achieves VBench score of 88.46 (3rd on I2V leaderboard)
- Outperforms leading open-source baselines on e-commerce video generation tasks in human evaluations
- First public release of large-scale video generation training code leveraging Megatron-Core for high efficiency
- Demonstrates near-linear multi-node scaling on 500 H100 GPUs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** High-ratio spatiotemporal compression (8×8×8) in the VideoVAE enables training long video sequences that would otherwise exceed memory, provided the latent space preserves sufficient detail.
- **Mechanism:** The VideoVAE downsamples each input clip by a factor of 8 along temporal, height, and width axes (512× volumetric compression). Combined with non-overlapping 2×2 patchification, this yields approximately 2048× compression relative to pixel space. The minimal encoding principle ensures each latent token is derived solely from its corresponding 8-frame chunk, eliminating information imbalance that causal convolutions introduce. A widened bottleneck channel dimension (C=24) compensates for aggressive compression.
- **Core assumption:** The latent space capacity is sufficient to encode both appearance and motion cues without critical information loss that the DiT cannot recover.
- **Evidence anchors:**
  - [abstract] Mentions "high-compression VideoVAE" as a key component.
  - [Section 3.1] Describes 8×8×8 compression, minimal encoding principle, and C=24 trade-off analysis.
  - [corpus] Vchitect-2.0 and related work similarly rely on latent diffusion for efficiency, but corpus does not directly validate this specific compression ratio.
- **Break condition:** If reconstruction PSNR/SSIM degrades significantly on high-motion clips, or if DiT training diverges due to latent space irregularities, compression ratio or channel capacity must be revisited.

### Mechanism 2
- **Claim:** Curriculum-based pre-training progressively builds video generation capabilities while maximizing training throughput.
- **Mechanism:** The three-stage curriculum exploits the observation that semantics dominate at low resolution while textures emerge at high resolution. Stage 1 mixes images with 360p videos (2s clips), annealing image-to-video ratio. Stage 2 extends clip length to 5s at 360p. Stage 3 uses curated 720p clips. Stages 1 and 2 process >10× more samples than Stage 3, building robust general abilities before expensive high-resolution refinement.
- **Core assumption:** Skills learned at low resolution and short duration transfer to high-resolution, longer clips without catastrophic forgetting.
- **Evidence anchors:**
  - [abstract] References "curriculum-based pretraining" as part of the four-pillar framework.
  - [Section 4.2.2] Details the three-stage curriculum with resolution/duration progression.
  - [corpus] No direct corpus evidence for this specific curriculum; related work uses progressive training but with different schedules.
- **Break condition:** If validation loss spikes when transitioning between stages, or if artifacts from early stages persist through fine-tuning, the curriculum pacing or data filtering must be adjusted.

### Mechanism 3
- **Claim:** Preference optimization with human annotations reduces persistent physical errors and improves motion quality where supervised training plateaus.
- **Mechanism:** After SFT plateaus, two complementary preference signals are collected: (1) pairwise "better/worse" comparisons for motion quality, optimized via DPO; (2) absolute pass/fail labels for physical plausibility (e.g., interpenetration, deformation), optimized via KTO. Retaining the SFT objective as a regularizer prevents reward hacking. Multi-stage iterative training sequentially exposes distinct error classes.
- **Core assumption:** Human annotators can reliably identify physical plausibility and motion quality, and these preferences are learnable without degrading other capabilities.
- **Evidence anchors:**
  - [Section 4.2.3] Describes KTO for error-free generation and DPO for motion quality, plus regularization strategy.
  - [Section 2.2.2] Details preference data collection methodology.
  - [corpus] DiffusionRL and related work apply RL to diffusion, but corpus does not directly validate this specific preference optimization approach for video.
- **Break condition:** If preference optimization causes over-smoothing, exaggerated motion, or regression in text-video alignment, regularization weight or annotation quality must be re-examined.

## Foundational Learning

- **Concept:** Latent diffusion models and flow matching objectives.
  - **Why needed here:** MUG-V operates in a learned latent space rather than pixel space, using flow matching to train the denoising trajectory. Understanding how VAE compression interacts with diffusion is essential for debugging reconstruction and generation quality.
  - **Quick check question:** Can you explain why training diffusion in latent space is more efficient than pixel space, and what trade-offs it introduces?

- **Concept:** Transformer parallelism strategies (tensor, pipeline, sequence, data parallelism).
  - **Why needed here:** The 10B DiT requires hybrid parallelism to fit across 500 H100 GPUs. Knowing when to shard activations (sequence parallelism) vs. partition layers (pipeline parallelism) is critical for achieving near-linear scaling.
  - **Quick check question:** For a long-sequence video model, why might sequence parallelism be more important than for a language model of the same size?

- **Concept:** Preference optimization algorithms (DPO, KTO).
  - **Why needed here:** Post-training alignment uses these algorithms to incorporate human feedback. Understanding their loss formulations helps diagnose training instability and reward hacking.
  - **Quick check question:** What is the key difference between DPO and KTO in terms of the type of preference signal they require?

## Architecture Onboarding

- **Component map:**
  Data pipeline (filtering, captioning, balancing) -> VideoVAE encoding (minimal encoding, reshape for decoder window) -> DiT training (curriculum stages 1→2→3) -> Post-training (SFT → preference optimization) -> Inference (VideoVAE decoding)

- **Critical path:**
  1. Data pipeline (filtering, captioning, balancing) → VideoVAE encoding (minimal encoding, reshape for decoder window) → DiT training (curriculum stages 1→2→3) → Post-training (SFT → preference optimization) → Inference (VideoVAE decoding)

- **Design tradeoffs:**
  - **Full attention vs. spatio-temporal separated attention:** Chose full attention for global coherence (e.g., same subject at start and end of clip). Acceptable because high compression reduces token count.
  - **Compression ratio vs. reconstruction fidelity:** 8×8×8 with C=24 balances storage and quality. Aggressive compression risks detail loss, mitigated by wider bottleneck.
  - **Parameter expansion vs. training from scratch:** HyperCloning from 2B to 10B reduces experimentation cost but assumes hyperparameter transfer holds.

- **Failure signatures:**
  - **Reconstruction artifacts:** Blurred textures or temporal discontinuities in VideoVAE output → check adaptive reconstruction loss weighting, increase decoder window.
  - **Training instability:** Loss spikes or divergence at scale → verify QK/cross-attention normalization, check pipeline balance across ranks.
  - **Physical implausibility after SFT:** Interpenetration, deformation, impossible trajectories → inspect preference annotation quality, increase KTO weight or iteration count.
  - **Motion degradation after preference optimization:** Exaggerated or recurring patterns → reduce preference loss weight, increase SFT regularization.

- **First 3 experiments:**
  1. **VideoVAE reconstruction benchmark:** Encode/decode a held-out set of clips across resolutions (256p, 480p, 720p). Report PSNR, SSIM, LPIPS, FloLPIPS against baselines (OpenSora VAE, CogVideoX VAE). Flag any systematic degradation on high-motion clips.
  2. **Small-model curriculum validation:** Train the 2B DiT through curriculum stages 1-2. Validate text-video alignment and motion plausibility on a curated set. Confirm hyperparameters transfer before scaling to 10B.
  3. **Distributed training scaling profile:** Run weak-scaling benchmarks from 64 to 500 GPUs. Measure MFU, memory high-water mark, and inter-node communication overhead. Identify bottlenecks in pipeline balancing or activation recomputation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the faithfulness and controllability of mappings from conditioning signals (text, image, mixed) to generated videos be strengthened for reliable real-world deployment?
- Basis: [Explicit] Section C states that strengthening this mapping remains a "prerequisite for reliable real-world deployment."
- Why unresolved: The paper notes that while performance is strong, precise adherence to complex conditioning inputs is not yet fully solved.
- Evidence: Significant improvements in automated alignment scores or higher human evaluation pass rates for specific conditioning tasks.

### Open Question 2
- Question: Can fine-grained appearance fidelity (material and texture preservation) be improved despite sensitivity to VAE compression and DiT noise initialization?
- Basis: [Explicit] Section C highlights that appearance fidelity "still lags" and suffers from "subtle but consequential degradations" due to these sensitivities.
- Why unresolved: The high 8×8×8 compression ratio creates an information bottleneck that current DiT initialization strategies struggle to overcome for fine details.
- Evidence: Quantitative improvements in texture-specific metrics or a reduction in human-detected material errors in generated outputs.

### Open Question 3
- Question: What algorithmic and system optimizations are required to scale video generation to longer durations and higher resolutions without sacrificing temporal consistency?
- Basis: [Explicit] Section C identifies scaling to longer durations and higher resolutions as a challenge requiring new coping strategies for "long-sequence training" and "inference efficiency."
- Why unresolved: The current full-attention mechanism becomes prohibitively expensive for sequences much longer than the current 5-second standard.
- Evidence: Successful training runs and coherent inference results on video clips substantially longer than 5 seconds or resolutions higher than 720p.

## Limitations
- VideoVAE compression ratio's impact on detail preservation across diverse content types lacks systematic ablation studies
- Curriculum effectiveness relies on stage progression claims without quantitative validation of skill transfer
- Preference optimization introduces subjective human judgment without disclosed annotation guidelines or inter-annotator agreement statistics

## Confidence
- **High Confidence:** The four-pillar framework structure (data, model, training, infrastructure) and the use of Megatron-Core for distributed training are well-established approaches with clear implementation details.
- **Medium Confidence:** The VideoVAE compression ratio and its impact on generation quality are plausible based on latent diffusion literature, but the specific 2048× compression claim requires independent verification on diverse video datasets.
- **Low Confidence:** The curriculum's claimed benefits and the preference optimization's ability to eliminate physical errors without degrading other capabilities are asserted but not empirically validated through controlled experiments or ablations.

## Next Checks
1. **Compression Robustness Test:** Systematically evaluate VideoVAE reconstruction quality across compression ratios (4×4×4, 8×8×8, 16×16×16) on a held-out set of high-motion clips, measuring PSNR/SSIM degradation and visual artifact prevalence.
2. **Curriculum Transfer Validation:** Train the 2B DiT through curriculum stages 1-2, then freeze and fine-tune only on Stage 3 data. Compare this two-stage approach against the full three-stage curriculum to isolate the marginal benefit of each stage.
3. **Preference Optimization Ablation:** Train a baseline model with SFT only and compare its physical error rate and motion quality against the full SFT + DPO + KTO pipeline on a curated test set with ground-truth physical plausibility annotations.