---
ver: rpa2
title: Heterogeneous Multisource Transfer Learning via Model Averaging for Positive-Unlabeled
  Data
arxiv_id: '2511.10919'
source_url: https://arxiv.org/abs/2511.10919
tags:
- data
- learning
- target
- condition
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a transfer learning framework for Positive-Unlabeled
  (PU) data that integrates heterogeneous data sources without direct data sharing.
  The method employs model averaging to combine logistic regression models from fully
  labeled, semi-supervised, and PU data sources, with optimal weights determined via
  cross-validation minimizing Kullback-Leibler divergence.
---

# Heterogeneous Multisource Transfer Learning via Model Averaging for Positive-Unlabeled Data

## Quick Facts
- **arXiv ID**: 2511.10919
- **Source URL**: https://arxiv.org/abs/2511.10919
- **Reference count**: 13
- **Primary result**: Integrates heterogeneous source domains (fully labeled, PU, semi-supervised) for PU target learning without data sharing, using KL-divergence optimized model averaging.

## Executive Summary
This paper addresses the challenge of learning from Positive-Unlabeled (PU) data by leveraging heterogeneous source domains without sharing raw data. The proposed TLMA-PU framework employs model averaging to combine logistic regression models from different source types, with optimal weights determined via cross-validation minimizing Kullback-Leibler divergence. The method theoretically guarantees optimal performance even under model misspecification and demonstrates superior predictive accuracy compared to existing methods in both simulations and real-world credit risk applications.

## Method Summary
The framework integrates information from heterogeneous source domains (Binary, PU, Semi-supervised) through model averaging without direct data sharing. Each source domain type has a tailored logistic regression model accounting for its specific label structure and bias. The target domain (PU) uses a case-control corrected likelihood to address the inherent negative bias in standard PU approaches. Optimal weights for combining source models are determined through K-fold cross-validation on the target domain, minimizing an out-of-sample negative log-likelihood criterion that approximates KL divergence. The method extends to high-dimensional settings using â„“1 penalties and establishes theoretical guarantees for weight optimality under both misspecified and correctly specified target models.

## Key Results
- Model averaging with KL-divergence optimized weights achieves superior AUC performance compared to naive averaging and existing transfer baselines.
- The framework demonstrates robustness to heterogeneous source domains and limited labeled data in target domains.
- Theoretical guarantees ensure optimal weight convergence and parameter estimation consistency under various conditions.

## Why This Works (Mechanism)

### Mechanism 1: Bias Correction via Non-Canonical Likelihood
The framework replaces standard logistic likelihood with a case-control corrected formulation for PU data. Under the SCAR assumption, it models observation probability using a non-canonical link function dependent on labeling mechanism $b^{(m)}$, correcting the systematic negative bias that occurs when treating all unlabeled data as negatives.

### Mechanism 2: Privacy-Preserving Parameter Aggregation
Knowledge transfer is achieved without raw data exchange by aggregating model coefficients ($\beta$) rather than gradients or data vectors. The weighted average estimator $\hat{\beta}(\hat{w}) = \sum \hat{w}_m \hat{\beta}^{(m)}$ operates as a parameter server architecture where only coefficient vectors from heterogeneous sources are shared and combined.

### Mechanism 3: Negative Transfer Suppression via KL Divergence Weighting
The method assigns optimal weights $\hat{w}$ by minimizing cross-validation criterion approximating out-of-sample KL divergence. This creates an adaptive filter where informative sources maintain high weights while uninformative sources are automatically down-weighted to zero, preventing negative transfer from misspecified or irrelevant domains.

## Foundational Learning

- **Concept: Positive-Unlabeled (PU) Learning**
  - Why needed here: This is the target domain constraint where only positive labels are observed, making standard classification impossible without specific bias adjustments.
  - Quick check question: Can you identify a single instance that is definitively negative (Class 0) in your training set? (If yes, this is not PU data).

- **Concept: Kullback-Leibler (KL) Divergence**
  - Why needed here: This acts as the "distance metric" for the averaging weights, measuring difference between probability distributions suitable for the logistic framework.
  - Quick check question: Is the divergence symmetric? (No, $KL(P||Q) \neq KL(Q||P)$ usually).

- **Concept: Sparsity-inducing Penalization ($\ell_1$)**
  - Why needed here: Required for high-dimensional settings ($p \gg n$) to reduce parameter space to manageable sparse set before averaging.
  - Quick check question: Does the penalty parameter $\lambda$ control model complexity or model fit? (Complexity/Sparsity).

## Architecture Onboarding

- **Component map:** Local Trainers -> Aggregation Layer -> Predictor
- **Critical path:** Calculating constants $b^{(m)}$ and class priors $\pi^{(m)}_1$ correctly for likelihood formulations. Errors here propagate silently into $\beta$ estimates.
- **Design tradeoffs:**
  - *Misspecification vs. Optimality:* Theoretical guarantees hold even under misspecification but rely on "pseudo-true" values being distinct enough.
  - *Computation vs. Privacy:* KL divergence via CV is computationally heavier than simple averaging but necessary to avoid squared loss bias.
- **Failure signatures:**
  - **Weight collapse:** All source weights drop to near zero while target weight dominates, indicating sources are "uninformative" or target sample size too small.
  - **Negative Bias in Prediction:** Systematically lower predicted probabilities suggest fallback to treating unlabeled as negatives.
- **First 3 experiments:**
  1. **Likelihood Validation:** Run `Single-PU` vs. `Standard Logistic` on synthetic dataset with known class priors to verify bias correction.
  2. **Weight Convergence Check:** Replicate "Case 2 (Correct specification)" to verify sum of weights assigned to uninformative sources decreases as target sample size increases.
  3. **High-Dimensional Stress Test:** In setting where $p > n$, apply $\ell_1$ penalty and compare AUC against `Translasso` in sparse regimes.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the TLMA-PU framework be extended to adapt dynamically to temporal distribution shifts in data?
- **Basis in paper:** [explicit] The conclusion states that in practical applications, "data distributions may exhibit temporal dynamics," and suggests that "developing adaptive transfer learning algorithms capable of adjusting dynamically" is a key research direction.
- **Why unresolved:** Current theoretical guarantees and empirical validations assume static distributions without modeling time-varying changes.
- **What evidence would resolve it:** A modified algorithm that updates weights or parameters over time, accompanied by theoretical convergence proofs or simulations on time-series data.

### Open Question 2
- **Question:** How can differential privacy techniques be integrated into the model averaging process without compromising predictive accuracy?
- **Basis in paper:** [explicit] The conclusion proposes integrating "differential privacy techniques" and "strategic injection of calibrated noise" within federated learning frameworks to rigorously maintain data privacy guarantees.
- **Why unresolved:** While the current method preserves privacy by avoiding raw data sharing, it does not provide formal differential privacy guarantees against potential inference attacks on transferred model parameters.
- **What evidence would resolve it:** A variant of the algorithm that satisfies $\epsilon$-differential privacy with theoretical analysis of utility-privacy trade-off.

### Open Question 3
- **Question:** How does the violation of the covariate-independent labeling assumption (SCAR) impact the bias and optimality of the weight estimators?
- **Basis in paper:** [inferred] Remark 2.2 states that the framework relies on SCAR assumption, noting that handling covariate-dependent labeling is "computationally complex" and was not addressed.
- **Why unresolved:** Theoretical guarantees rely on specific likelihood forms valid only under SCAR; behavior when labeling probability depends on $x$ remains unexplored.
- **What evidence would resolve it:** Sensitivity analysis or theoretical bounds demonstrating method's robustness when labeling mechanism is covariate-dependent.

## Limitations
- Theoretical guarantees assume specific asymptotic regimes with finite or slowly growing $p$, leaving performance gaps in finite samples unaddressed.
- Framework critically depends on SCAR assumption; if labeling depends on covariates, bias correction mechanism fails silently.
- Weight optimization via K-fold CV introduces sampling variance, especially with limited target data, with robustness not empirically characterized.

## Confidence
- **High**: The general transfer learning framework (model averaging without data sharing) and its privacy-preserving nature are well-supported.
- **Medium**: The bias correction mechanism for PU data is theoretically sound under SCAR, but practical implementation details significantly affect outcomes.
- **Low**: Theoretical optimality guarantees are asymptotic and rely on precise conditions that may not hold in practice.

## Next Checks
1. **Solver Robustness Test**: Implement PU MLE using multiple optimization algorithms (BFGS, EM-style) and compare recovered parameters against ground truth in controlled simulations to assess sensitivity to initialization and algorithm choice.
2. **SCAR Violation Sensitivity**: Design simulation where probability of observing positive label depends on covariates (violating SCAR). Measure degradation in AUC_adj to quantify framework's vulnerability to assumption breaches.
3. **Weight Stability Analysis**: For fixed set of source domains, run weight optimization across multiple random train/test splits of target data. Calculate variance of top-weighted source coefficients to assess stability of transfer learning process.