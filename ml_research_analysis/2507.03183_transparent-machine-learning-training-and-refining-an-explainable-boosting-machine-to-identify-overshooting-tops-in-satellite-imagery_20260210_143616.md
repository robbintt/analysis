---
ver: rpa2
title: 'Transparent Machine Learning: Training and Refining an Explainable Boosting
  Machine to Identify Overshooting Tops in Satellite Imagery'
arxiv_id: '2507.03183'
source_url: https://arxiv.org/abs/2507.03183
tags:
- feature
- imagery
- features
- brightness
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Transparent Machine Learning: Training and Refining an Explainable Boosting Machine to Identify Overshooting Tops in Satellite Imagery

## Quick Facts
- arXiv ID: 2507.03183
- Source URL: https://arxiv.org/abs/2507.03183
- Reference count: 24
- None

## Executive Summary
This paper demonstrates the use of Explainable Boosting Machines (EBMs) for detecting Overshooting Tops (OTs) in satellite imagery, emphasizing transparency and interpretability over raw performance. By engineering three interpretable features from GOES-16 data and allowing post-training editing of feature functions, the approach bridges machine learning with domain expertise in meteorology. The work highlights both the promise and current limitations of interpretable ML for operational severe weather detection.

## Method Summary
The method uses GOES-16 ABI visible and infrared channels to extract three engineered features: a 9×9 box-blurred brightness map, a GLCM-based cool contrast texture statistic masked at 250K, and infrared temperature. These features feed into an EBM trained to classify convection pixels, with post-training manual editing of feature functions to better align with physical expectations. The approach prioritizes interpretability, allowing domain experts to modify learned decision strategies without retraining.

## Key Results
- EBMs enable transparent OT detection by decomposing predictions into inspectable feature functions.
- Manual editing of feature functions improves alignment with domain knowledge.
- Current feature set struggles to distinguish OTs from cold, textured non-OT regions (Cases IV/V).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EBMs decompose predictions into inspectable, additive feature functions enabling direct strategy visualization and modification
- Mechanism: Each feature gets a learned function f_i(x_i) mapping input values to log-odds scores; pairwise interactions f_ij(x_i, x_j) capture feature combinations; final prediction sums all contributions plus intercept
- Core assumption: The target relationship can be adequately captured by main effects and pairwise interactions (no higher-order interactions needed)
- Evidence anchors:
  - [abstract]: "Once trained, the EBM was examined and minimally altered to more closely match strategies used by domain scientists"
  - [section 3a]: "EBMs take the form: g(E[y]) = β₀ + Σf_i(x_i) + Σf_ij(x_i, x_j)"
  - [corpus]: "Pushing the Boundaries of Interpretability" confirms EBM as state-of-the-art glass-box model
- Break condition: When prediction requires complex multi-feature interactions beyond pairwise terms, interpretability-performance tradeoff emerges

### Mechanism 2
- Claim: Physics-informed feature engineering transforms image-based OT detection into scalar classification
- Mechanism: GLCM contrast statistic quantifies texture by summing adjacency probabilities weighted by squared gray-level differences; temperature masking at 250K limits inference to high-cloud regions
- Core assumption: Brightness, texture, and temperature scalars encode sufficient discriminative information for OT vs. anvil distinction
- Evidence anchors:
  - [abstract]: "simplify the process of OT detection by first using mathematical methods to extract key features, such as cloud texture using Gray-Level Co-occurrence Matrices"
  - [section 2c]: "f_contrast = Σp(i,j)(i-j)²...higher contrast values indicate greater gray-level dissimilarity between adjacent pixels"
  - [corpus]: Weak corpus signal—neighbor papers focus on EBM applications, not GLCM feature engineering specifically
- Break condition: When OT detection requires spatial context (shape, scale) or temporal dynamics beyond pixel-wise scalar features

### Mechanism 3
- Claim: Post-training feature function editing allows domain knowledge integration without retraining
- Mechanism: Feature functions stored as lookup tables; domain experts identify faulty regions via importance maps and directly modify function outputs (e.g., flattening spurious spikes)
- Core assumption: Initial training captures useful patterns needing refinement rather than wholesale replacement
- Evidence anchors:
  - [abstract]: "EBM was examined and minimally altered to more closely match strategies used by domain scientists"
  - [section 3c]: "decision was made to flatten out the left-hand side of the feature function to match the lowest score value seen"
  - [corpus]: "Pushing the Boundaries" discusses incremental EBM enhancements, supporting editability concept
- Break condition: When learned strategies are fundamentally incorrect across broad feature ranges, requiring full retraining

## Foundational Learning

- Concept: Generalized Additive Models (GAMs)
  - Why needed here: EBMs extend GAMs with learned nonlinear functions; understanding additive decomposition is essential for interpreting feature contributions
  - Quick check question: With 3 input features, how many main effect functions and pairwise interaction functions will the EBM learn?

- Concept: Gray-Level Co-occurrence Matrix (GLCM) texture features
  - Why needed here: The cool contrast tiles feature relies on GLCM contrast statistic to quantify cloud texture
  - Quick check question: Why does weighting matrix entries by (i-j)² give higher contrast values for textured vs. smooth regions?

- Concept: Transfer learning via label proxy
  - Why needed here: Model trains on MRMS convection labels but targets OT detection; feature engineering and model editing bridge this gap
  - Quick check question: What are two failure modes introduced by using convection labels instead of true OT labels?

## Architecture Onboarding

- Component map:
  - GOES-16 ABI Channels 2+13 -> Brightness (box blur + downsample) -> Infrared temperature -> GLCM cool contrast tiles (4×4 tiles + 250K mask) -> EBM (3 main effects + 3 pairwise interactions) -> Feature function visualization -> Domain expert review -> Manual function editing

- Critical path:
  1. Preprocess: Radiance → reflectance with solar zenith normalization; filter scenes with solar zenith >65°
  2. Feature compute: GLCMs on 4×4 tiles, apply contrast formula, mask at 250K
  3. Train EBM: Bagging + gradient boosted trees, low learning rate ensures order-independence
  4. Inspect: Plot f_i functions, generate importance maps for validation scenes
  5. Edit: Modify function regions contradicting domain knowledge (e.g., flatten spurious low-brightness spike)

- Design tradeoffs:
  - 250K threshold: Warmer than optimal for OTs, but preserves flexibility for post-hoc editing
  - 4×4 GLCM tiles: Reduces resolution 0.5km→2km, but enables tractable texture computation
  - Pairwise interactions only: Limits complexity for interpretability, but may miss 3+ feature patterns
  - Daylight-only: Visible channel unusable at night; model scope inherently limited

- Failure signatures:
  - Cold U/V shapes (Case IV): High texture + cold temperature triggers false positives on non-OT severe weather signatures
  - Crease artifacts (Case V): Linear high-contrast features incorrectly flagged; GLCM cannot distinguish bubbly vs. linear texture
  - Shadow confusion: Interaction term assigns positive scores to cold+dark regions, catching shadows but potentially over-flagging
  - Diffuse predictions (Case III): Texture signal too broad for localized OT detection

- First 3 experiments:
  1. Replicate baseline: Train EBM on engineered features without editing; compare raw feature functions to Figure 11 to verify implementation
  2. Feature ablation: Remove cool contrast tiles, measure prediction degradation on validation set to quantify texture contribution
  3. Threshold sweep: Test 240K, 250K, 260K temperature masks; evaluate false positive rates on Cases IV/V failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EBM-based OT detection performance compare when validated against a large hand-labeled OT dataset rather than MRMS convection labels?
- Basis in paper: [explicit] The authors state: "the next step in this research should be the creation of a large hand-labeled data set that identifies OTs in GOES visible imagery... it is needed to fully evaluate how well the EBM model matches human labeling."
- Why unresolved: Current validation uses MRMS convection flags, which detect convection rather than OTs specifically; this mismatch prevents accurate OT-specific performance assessment.
- What evidence would resolve it: Performance metrics (precision, recall, F1) computed against expert-labeled OT locations across diverse storm events.

### Open Question 2
- Question: Can additional engineered features be developed to distinguish "bubbly" OT texture from non-OT high-contrast regions such as cloud creases and cold U/V shapes?
- Basis in paper: [explicit] The authors note: "regions of high contrast caused by the crease lead to large texture values despite the texture not being 'bubbly.' This is due to how the texture values are calculated, meaning our current methodology does not offer a way to correct for this."
- Why unresolved: GLCM contrast statistic captures local intensity variation but not the specific spatial pattern characterizing OTs versus other textured features.
- What evidence would resolve it: New texture features (e.g., shape descriptors, multi-scale texture metrics) that reduce false positives in Case IV/V scenarios while maintaining OT detection.

### Open Question 3
- Question: How does model performance generalize to seasons, regions, and satellite zenith angles outside the summer CONUS training domain?
- Basis in paper: [explicit] "Given these data were collected only during the northern hemisphere's summer months, a limitation of the proposed algorithm is that it has not seen data from other time periods or locations. Performance may suffer if it encounters such data."
- Why unresolved: Training data limited to May–August over central/eastern CONUS; solar zenith angle constrained to <65°; no evaluation on winter storms, tropical regions, or higher latitudes.
- What evidence would resolve it: Systematic evaluation across seasons, geographic regions, and viewing geometries with quantitative performance metrics.

### Open Question 4
- Question: What additional spectral channels or derived products could enhance the current three-feature EBM for improved OT detection accuracy?
- Basis in paper: [inferred] The paper demonstrates that only three features (brightness, cool contrast tiles, infrared) limit the model's ability to discriminate OTs from cold/textured non-OT regions. The authors explicitly call for "developing a larger range of engineered features."
- Why unresolved: Current feature set cannot encode distinctions between OTs and meteorologically similar features (cold U/V shapes, enhanced-V signatures).
- What evidence would resolve it: Ablation study showing incremental performance gains from additional ABI channels (e.g., water vapor, near-infrared) or derived products (e.g., brightness temperature differences).

## Limitations
- Label proxy mismatch: Using MRMS convection flags instead of true OT labels introduces systematic noise and limits validation reliability.
- Feature engineering constraints: Current GLCM-based texture features cannot distinguish OT-specific patterns from other high-contrast cloud regions.
- Limited generalizability: Model trained only on summer CONUS data; performance on other seasons, regions, and viewing geometries unknown.

## Confidence
- EBM mechanism claims (High): The additive feature decomposition and post-training editing capabilities are well-established in the EBM literature.
- GLCM feature engineering claims (Medium): The texture extraction methodology is sound, but effectiveness depends heavily on the 4×4 tile size and 250K threshold choices.
- Post-training editing claims (Medium): While theoretically valid, the paper provides limited detail on how edits were determined and validated against domain knowledge.

## Next Checks
1. Implement feature ablation study to quantify texture contribution by comparing performance with and without cool contrast tiles.
2. Conduct threshold sensitivity analysis across 240K-260K temperature masks to assess false positive rate variations.
3. Apply the edited EBM to independent validation scenes containing known failure modes (Cases IV-V) to verify problematic regions were adequately addressed.