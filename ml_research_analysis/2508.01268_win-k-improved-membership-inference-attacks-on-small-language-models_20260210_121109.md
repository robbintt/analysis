---
ver: rpa2
title: 'Win-k: Improved Membership Inference Attacks on Small Language Models'
arxiv_id: '2508.01268'
source_url: https://arxiv.org/abs/2508.01268
tags:
- win-k
- mias
- language
- min-k
- slms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses membership inference attacks (MIAs) on small
  language models (SLMs). The core method idea is to extend the state-of-the-art min-k
  attack by computing log probability scores over sliding windows of consecutive tokens,
  mitigating noise and outlier sensitivity observed in token-level analyses.
---

# Win-k: Improved Membership Inference Attacks on Small Language Models

## Quick Facts
- arXiv ID: 2508.01268
- Source URL: https://arxiv.org/abs/2508.01268
- Reference count: 31
- Primary result: Proposes win-k attack that outperforms existing MIAs on small language models, especially on models under 1B parameters

## Executive Summary
This paper introduces win-k, a membership inference attack that improves upon the state-of-the-art min-k attack by aggregating log probabilities over sliding windows of consecutive tokens. The method addresses two key limitations of token-level attacks on small language models: high variance in probability estimates and sensitivity to outlier tokens. By averaging log probabilities within windows before selecting the lowest-scoring windows, win-k provides more stable membership scores that are less affected by individual token noise. The approach is particularly effective for smaller models (under 1B parameters) where probability estimates are noisier.

## Method Summary
Win-k extends the min-k attack by computing log probability scores over sliding windows of consecutive tokens rather than individual tokens. For each window of w consecutive tokens, the method calculates the average log probability and then selects the k% lowest-scoring windows to compute the final membership score. This window-based aggregation reduces variance and mitigates the impact of outlier tokens that can disproportionately influence token-level scores. The approach is specifically designed to address the noisier probability estimates characteristic of small language models, where individual token scores exhibit higher variance and are more susceptible to outliers.

## Key Results
- Win-k achieves higher AUROC than min-k in 17 out of 24 tested configurations across three datasets
- The attack shows particularly strong performance on smaller models (GPT-Neo 125M, Pythia 70M/160M) with AUROC improvements of 0.02-0.05
- Sample length has a significant impact, with AUROC increasing substantially as sample length grows beyond 64 tokens, especially for smaller models
- Variance reduction is empirically demonstrated, with member sample variance dropping from 4.72 (min-k) to 1.21 (win-k) and non-member from 10.25 to 2.24

## Why This Works (Mechanism)

### Mechanism 1: Variance Reduction via Window-Level Aggregation
Aggregating log probabilities over sliding windows reduces variance in membership scores compared to token-level analysis in SLMs. Individual token log probabilities exhibit high variance due to limited model capacity and noisy approximations of P(x_t|x_{<t}). By computing average log probabilities over windows of w consecutive tokens, random fluctuations tend to cancel out while systematic differences between members and non-members are preserved. The normalized window score provides a more stable signal.

### Mechanism 2: Robustness to Outlier Token Dominance
Window-based scoring prevents single low-probability tokens from disproportionately influencing the final membership score. Min-k aggregates the bottom k% of individual token log probabilities, making the score sensitive to a few outlier tokens with very low probability. Win-k first averages log probabilities within each window, so a single low-probability token's impact is moderated by its neighboring tokens. The bottom k% of window-level scores are then aggregated, yielding aggregate membership scores that better represent overall sample behavior rather than being dominated by outliers.

### Mechanism 3: Scale-Adaptive Effectiveness with Stronger Gains on Smaller Models
Win-k's advantage over token-level attacks is most pronounced on smaller models where probability estimates are noisier. Larger models have sufficient capacity to produce relatively smooth, well-calibrated probability distributions; token-level approaches perform adequately. Smaller models have reduced memorization capacity and produce noisier token-level probabilities, making the smoothing from window aggregation more valuable. The paper shows a clear degradation of existing MIAs as model size decreases, which win-k partially counteracts.

## Foundational Learning

- **Concept: Membership Inference Attacks (MIAs) on Language Models**
  - Why needed here: Core problem being solved—determining whether a text sample was used in a model's training data
  - Quick check question: Can you explain why a training sample typically yields lower loss (higher log probability) than a non-training sample on a fine-tuned model?

- **Concept: Log-Likelihood and Loss in Autoregressive LMs**
  - Why needed here: Win-k operates on log probabilities log(P(x_i|x_{<i})); understanding how these aggregate into sample-level scores is essential
  - Quick check question: Given a sequence of token log probabilities, how would you compute the average per-token log-likelihood for the sequence?

- **Concept: ROC Curves, AUROC, and Threshold-Dependent Metrics (TPR@1%FPR, FPR@99%TPR)**
  - Why needed here: The paper evaluates attacks using AUROC for overall effectiveness and threshold-constrained metrics for strict operational regimes
  - Quick check question: Why might AUROC alone be insufficient for assessing an MIA in a real deployment with a low false-positive budget?

## Architecture Onboarding

- **Component map:** Tokenizer & Model Inference -> Window Constructor -> Window Scorer -> Aggregator -> Threshold Comparator
- **Critical path:** Accurate extraction of per-token log probabilities → correct sliding-window construction without off-by-one errors → proper normalization by w → stable sorting of window scores → correct selection of bottom k% (handle edge cases where k×T is non-integer)
- **Design tradeoffs:**
  - Window size (w): Small w (2–4) better for very small models; larger w (8–10) better for larger models. Smaller w retains more local detail but less noise reduction; larger w smooths more but may blur short discriminative patterns
  - Fraction (k): Values between 0.2–0.5 generally best; very low k (0.1) loses signal; very high k (≥0.7) includes too many high-score windows, diluting discrimination
  - Tokenizer choice: Paper notes MobileLLM's different tokenizer (32k vocab) may reduce win-k's advantage compared to GPT-Neo/Pythia's 50k BPE tokenizers; tokenizer granularity affects what constitutes a "window" of semantic content
- **Failure signatures:**
  - AUROC near 0.5: Model not fine-tuned (0 epochs) or no membership signal present
  - Win-k matches or underperforms min-k on larger models: Expected; revert to min-k for efficiency if gains are marginal
  - High variance in scores despite windowing: Window size too small relative to noise level; increase w
  - Consistent misclassification of short samples: Sample length T too short for meaningful window statistics; ensure T ≥ 32 and ideally ≥ 64
- **First 3 experiments:**
  1. Reproduce baseline comparison: Run min-k and win-k on GPT-Neo 125M with WikiText/AGNews/XSum; verify AUROC improvements reported in Table 1 and variance reduction in Figure 3
  2. Hyperparameter sweep: Test w ∈ {2,4,6,8,10} and k ∈ {0.2,0.3,0.4,0.5} across multiple SLM sizes (70M–1B); confirm smaller models benefit from smaller w and k ≈ 0.3, larger models from larger w
  3. Ablate sample length: Truncate samples to T ∈ {32,64,128} and measure AUROC impact; verify Table 5 trend that longer samples substantially improve attack effectiveness, especially for smaller models

## Open Questions the Paper Calls Out
- How can the False Positive Rate (FPR) of the win-k attack be minimized under strict True Positive Rate (TPR) constraints?
- Does the win-k attack effectively generalize to multilingual models and non-text modalities?
- To what extent does the choice of tokenizer (e.g., BPE vs. SentencePiece) influence the effectiveness of window-level MIA scores?
- Is win-k effective at detecting pre-training data membership, or is it limited to the supervised fine-tuning (SFT) scenarios tested?

## Limitations
- The variance reduction mechanism relies on the assumption that token-level probability noise is independent and random rather than systematic
- The outlier mitigation mechanism assumes membership signals are distributed across sequences rather than concentrated in isolated rare tokens
- The scale-adaptive effectiveness claim depends on model parameter count correlating with noise quality in conditional probability estimates
- The empirical evaluation focuses primarily on autoregressive transformer architectures using specific tokenization schemes

## Confidence
- **High Confidence:** The variance reduction mechanism is well-supported by empirical evidence showing substantial decreases in sample variance for both members and non-members, and the mathematical framework for window-level aggregation is sound
- **Medium Confidence:** The outlier mitigation mechanism has strong qualitative support through the "fridge" example demonstrating how single low-probability tokens can mislead min-k while win-k provides more balanced scoring, but generalizability requires further validation
- **Low Confidence:** The fundamental assumption that token-level probability noise is random and independent across positions lacks direct corpus validation

## Next Checks
1. **Correlation Structure Analysis:** Conduct experiments to empirically measure the correlation structure of token-level log probabilities across consecutive positions in small versus large models to determine whether the independence assumption underlying the variance reduction mechanism holds
2. **Cross-Architecture Generalization:** Test win-k on encoder-decoder models (e.g., T5, BART) and other architectures using different tokenization schemes beyond the GPT-Neo/Pythia BPE setup to determine whether effectiveness is tied to specific architectural features
3. **Signal Localization Experiment:** Design controlled experiments where membership signals are artificially concentrated in either isolated tokens or specific window patterns to validate whether the outlier mitigation mechanism preserves or attenuates membership signals depending on their distribution pattern