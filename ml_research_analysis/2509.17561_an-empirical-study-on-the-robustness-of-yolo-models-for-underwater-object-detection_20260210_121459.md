---
ver: rpa2
title: An Empirical Study on the Robustness of YOLO Models for Underwater Object Detection
arxiv_id: '2509.17561'
source_url: https://arxiv.org/abs/2509.17561
tags:
- underwater
- detection
- object
- images
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive evaluation of YOLO models for
  underwater object detection, examining their robustness across six simulated underwater
  conditions. The study benchmarks five recent YOLO variants (YOLOv8-YOLOv12) using
  a unified dataset of 10,000 annotated underwater images, revealing that YOLOv12
  delivers the strongest overall performance but is highly vulnerable to noise distortion.
---

# An Empirical Study on the Robustness of YOLO Models for Underwater Object Detection

## Quick Facts
- arXiv ID: 2509.17561
- Source URL: https://arxiv.org/abs/2509.17561
- Authors: Edwine Nabahirwa; Wei Song; Minghua Zhang; Shufan Chen
- Reference count: 40
- Primary result: YOLOv12 delivers strongest performance but is highly vulnerable to noise distortion

## Executive Summary
This paper presents a comprehensive evaluation of YOLO models for underwater object detection, examining their robustness across six simulated underwater conditions. The study benchmarks five recent YOLO variants (YOLOv8-YOLOv12) using a unified dataset of 10,000 annotated underwater images, revealing that YOLOv12 delivers the strongest overall performance but is highly vulnerable to noise distortion. The research analyzes how underwater distortions affect low-level features such as texture, edges, and color, finding that noise particularly disrupts edge and texture features, explaining poor detection performance in noisy conditions. The study also investigates class imbalance effects, demonstrating that image counts and instance frequency primarily drive detection performance rather than object appearance. To improve robustness, the authors propose lightweight training-aware strategies including noise-aware sample injection, which enhances performance in both noisy and real-world conditions, and fine-tuning with advanced enhancement methods, which boosts accuracy in enhanced domains but slightly reduces performance in original data.

## Method Summary
The study employs a unified dataset of 10,000 annotated underwater images to evaluate five YOLO variants (v8 through v12) across six simulated underwater conditions: Low-Contrast, Blur, Noise, Bluish, Greenish, and Clean-water. The authors use GLCM (Gray Level Co-occurrence Matrix) features to analyze how distortions affect texture, edges, and color, establishing that noise particularly disrupts edge and texture features. They propose two lightweight training strategies: noise-aware sample injection (10% noisy samples during training) and domain adaptation via enhancement fine-tuning. The noise injection strategy enhances robustness to both noisy and real-world conditions, while fine-tuning on enhanced data (HybSense) improves performance in enhanced domains but reduces accuracy in original data.

## Key Results
- YOLOv12 delivers the strongest overall performance (0.770 mAP@50:95) but is highly vulnerable to noise distortion
- Noise disrupts high-frequency features like edges and texture, causing models to misclassify background noise as objects or miss objects entirely
- Image counts and instance frequency primarily drive detection performance, while object appearance exerts only a secondary influence
- Noise-aware sample injection (10% noisy samples) improves robustness in both noisy and real-world conditions
- Fine-tuning with advanced enhancement methods (HybSense) boosts accuracy in enhanced domains but slightly lowers performance in original data

## Why This Works (Mechanism)

### Mechanism 1: Noise-Aware Sample Injection for Robustness
- **Claim:** Introducing a low ratio (10%) of noisy samples during training enhances detection robustness in noisy conditions without significantly degrading performance on clean data.
- **Mechanism:** Underwater noise disrupts high-frequency features like edges and texture, causing models to misclassify background noise as objects or miss objects entirely. By injecting a controlled "low noise" subset (5% diffusion + 5% salt-and-pepper), the model learns invariance to these high-frequency perturbances, shifting reliance toward more robust structural features rather than spurious texture artifacts.
- **Core assumption:** The model can generalize a noise-resistant feature representation from a small subset (10%) without catastrophic forgetting of the clean data distribution.
- **Evidence anchors:** [abstract] Mentions "noise-aware sample injection... improves robustness in both noisy and real-world conditions." [section] "Small Sample Injection Strategy" shows 10% noise training achieves 0.298 mAP@50:95 on noisy data (vs 0.151 baseline) while maintaining 0.505 on original data.
- **Break condition:** If noise injection exceeds ~33% (High Noise setup), the model overfits to distortion patterns, causing performance on clean/real-world images to drop significantly (e.g., mAP drops from 0.504 to 0.415).

### Mechanism 2: Domain Adaptation via Enhancement Fine-Tuning
- **Claim:** Fine-tuning a pre-trained model on a small fraction (10%) of advanced enhanced data (e.g., HybSense) effectively adapts the detector to new underwater domains but introduces a trade-off with original domain accuracy.
- **Mechanism:** Advanced deep-learning enhancement methods (like HybSense) restore color fidelity and texture consistency, creating a distinct feature distribution compared to raw underwater images. Fine-tuning re-aligns the model's weights to this "restored" distribution, improving feature extraction capability in the target domain.
- **Core assumption:** The "enhanced" visual domain is sufficiently representative of the target deployment environment to justify the loss in accuracy on unenhanced source data.
- **Evidence anchors:** [abstract] "fine-tuning with advanced enhancement... boosts accuracy in enhanced domains but slightly lowers performance in original data." [section] "Fine-tuning for Domain Shift Adaptation" reports an increase to 0.899 mAP@50 on enhanced data but a drop to 0.672 on original data.
- **Break condition:** If the enhancement method introduces structural artifacts (hallucinated textures) rather than restoring ground truth, the fine-tuned model will learn false features, reducing real-world reliability.

### Mechanism 3: Class Imbalance Driven by Frequency over Appearance
- **Claim:** Detection performance is primarily driven by image count and instance frequency during training, while visual distinctiveness (appearance) acts only as a secondary modifier.
- **Mechanism:** The model learns class-specific features proportional to exposure (gradient updates). Dominant classes (e.g., Echinus) saturate performance due to high instance frequency. Classes with moderate frequency but high distinctiveness (e.g., Starfish) maintain competitive performance, whereas rare classes with low visual salience (e.g., Scallop) suffer double jeopardyâ€”insufficient training data and ambiguous features.
- **Core assumption:** The evaluation metric (mAP) captures the functional relationship between frequency and feature learning capacity linearly.
- **Evidence anchors:** [abstract] "image counts and instance frequency primarily drive detection performance, while object appearance exerts only a secondary influence." [section] "Impact of Class Frequency" details how Scallop (rare/ambiguous) fails to improve despite dataset increases, while Echinus (frequent) succeeds.
- **Break condition:** If a rare class has extremely high visual contrast or a unique shape compared to the background, it may defy the frequency-driven trend.

## Foundational Learning

- **Concept:** **GLCM (Gray Level Co-occurrence Matrix)**
  - **Why needed here:** The paper uses GLCM features (contrast, correlation, energy, homogeneity) to quantify *why* detection fails under noise. Understanding that "High Contrast" in GLCM often correlates with *worse* detection in noisy images is critical for diagnosing failure modes.
  - **Quick check question:** Does a high GLCM Contrast value always indicate a "better" image for detection? (Answer: No; in this context, high contrast in noisy images correlates with low mAP).

- **Concept:** **Domain Shift (Covariate Shift)**
  - **Why needed here:** The core contribution involves transferring a model from "Original/Real-world" underwater data to "Enhanced" or "Noisy" domains. Recognizing that fine-tuning on Domain B degrades performance on Domain A is essential for the proposed adaptation strategy.
  - **Quick check question:** Why does fine-tuning on enhanced images lower the accuracy on original images? (Answer: The model overfits to the statistical distribution of the enhanced domain).

- **Concept:** **Feature Hierarchy in CNNs (Texture vs. Color)**
  - **Why needed here:** The paper establishes that YOLO models prioritize texture over color for UOD.
  - **Quick check question:** Which feature type did the paper identify as the "most consistent predictor" of detection accuracy? (Answer: Texture-related features).

## Architecture Onboarding

- **Component map:** Input: Underwater Image -> Preprocessing (Optional: Noise Injection / HybSense Enhancement) -> Backbone (YOLOv8-v12 variants: CSPDarknet, ELAN, Area Attention) -> Neck (PANet/C3K2) -> Head (Anchor-free detection) -> Output (Bounding Box + Class)

- **Critical path:**
  1. Data Preparation: Select base YOLO (recommend YOLOv12m for best baseline)
  2. Distortion Simulation: Apply specific degradations (Blur, Noise, Color Cast) to test set to establish baseline vulnerability
  3. Training Strategy: Implement "Low Noise" injection (10% noisy samples) into training loop if robustness is the goal

- **Design tradeoffs:**
  - v12 vs. Older Models: YOLOv12m offers superior overall mAP (0.770) but is computationally heavier and slightly more sensitive to noise drops than the relative stability of simpler models
  - Enhancement Choice: Advanced enhancement (HybSense) creates a domain shift requiring fine-tuning (high gain, high maintenance) vs. Basic Clean-Water simulation (low gain, low maintenance)

- **Failure signatures:**
  - Noise Failure: High edge density (Sobel) but low mAP; model detects noise artifacts as objects
  - Class Failure: "Scallop" class mAP remains flat (~0.5) despite increasing training epochs (indicates data insufficiency or lack of visual distinctiveness, not optimization failure)
  - Domain Failure: Sudden drop in "Original" mAP after fine-tuning on "Enhanced" data

- **First 3 experiments:**
  1. Baseline Robustness Test: Train YOLOv12m on Original data. Test on the 6 simulated conditions (Low-Contrast, Blur, Noise, Bluish, Greenish, Clean-water) to confirm the noise vulnerability
  2. Noise Injection Ablation: Train two variants: one with 10% noise injection and one with 66% noise injection. Compare mAP on Original vs. Noisy test sets to find the efficiency trade-off
  3. Enhancement Fine-Tuning: Take the baseline model and fine-tune using only 10% HybSense-enhanced images. Verify the specific trade-off (Enhanced mAP rises >0.85, Original mAP falls <0.70)

## Open Questions the Paper Calls Out
None

## Limitations
- The six simulated distortion types may not fully capture the stochastic complexity of real underwater environments where multiple degradation types co-occur
- The noise-aware sample injection strategy relies on precise parameter tuning (10% ratio) that may not transfer optimally across different datasets or YOLO architectures
- The domain adaptation results reveal a significant trade-off that limits practical deployment when both enhanced and original domains are required simultaneously

## Confidence
- **High Confidence:** The identification of YOLOv12 as the best-performing baseline architecture and the characterization of noise as the most detrimental distortion type are well-supported by systematic benchmarking across five model variants
- **Medium Confidence:** The class imbalance analysis and the proposed lightweight training strategies show consistent patterns, though the specific effectiveness of noise injection ratios and enhancement methods may vary with different underwater datasets
- **Medium Confidence:** The GLCM-based feature analysis explaining detection failures under noise conditions is mechanistically sound, though the correlation between feature metrics and detection performance could benefit from additional validation

## Next Checks
1. Test the noise-aware sample injection strategy on a real-world underwater dataset with naturally occurring noise to verify performance improvements beyond simulated conditions
2. Evaluate the domain adaptation approach using multiple enhancement methods to determine if the observed trade-off between enhanced and original domain performance is method-specific or represents a general limitation
3. Conduct a longitudinal study to assess whether the proposed lightweight training strategies maintain their effectiveness when deployed across diverse underwater environments with varying lighting, turbidity, and ecological conditions