---
ver: rpa2
title: 'Examining the Threat Landscape: Foundation Models and Model Stealing'
arxiv_id: '2502.18077'
source_url: https://arxiv.org/abs/2502.18077
tags:
- victim
- thief
- stealing
- foundation
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the susceptibility of foundation models
  to model stealing attacks in computer vision. The study finds that models fine-tuned
  from vision transformers (ViTs) are more vulnerable to model stealing compared to
  conventional architectures like ResNets, even when the thief model has access to
  similar foundation models.
---

# Examining the Threat Landscape: Foundation Models and Model Stealing

## Quick Facts
- arXiv ID: 2502.18077
- Source URL: https://arxiv.org/abs/2502.18077
- Reference count: 40
- Primary result: ViT models show significantly higher susceptibility to model stealing attacks compared to ResNets, with agreement scores up to 94.28% for ViT-L/16 victims on CIFAR-10

## Executive Summary
This paper investigates model stealing vulnerabilities in foundation models, specifically comparing vision transformers (ViTs) against conventional architectures like ResNets. The study demonstrates that models fine-tuned from ViTs are substantially more susceptible to model extraction attacks, even when attackers have access to similar foundation models. Using a ViT-L/16 thief model, the research shows dramatically higher agreement scores with ViT victims (94.28%, 60.52%, and 62.94% across three datasets) compared to ResNet victims (73.20%, 40.22%, and 46.23%). These findings reveal a critical trade-off between accuracy and privacy when deploying ViT-based models in commercial APIs, suggesting the need for enhanced security measures in production environments.

## Method Summary
The paper employs a model stealing attack methodology where a thief model queries a victim model through a black-box interface, collecting input-output pairs to train a substitute model. The study uses three datasets (CIFAR-10, Indoor-67, and Caltech-256) and compares two architectures: ViT-L/16 and ResNet-18. The thief model is always a ViT-L/16, allowing the researchers to isolate the effect of the victim model's architecture on vulnerability. Agreement scores between thief and victim models serve as the primary metric for measuring attack success. The experiments systematically vary victim model architecture while keeping the thief model constant, providing clear evidence of architectural susceptibility differences.

## Key Results
- ViT-L/16 victims achieved agreement scores of 94.28%, 60.52%, and 62.94% on CIFAR-10, Indoor-67, and Caltech-256 respectively
- ResNet-18 victims showed significantly lower agreement scores of 73.20%, 40.22%, and 46.23% on the same datasets
- The vulnerability gap persists even when the thief model has access to similar foundation models, indicating architectural susceptibility rather than resource advantage

## Why This Works (Mechanism)
The increased vulnerability of ViTs stems from their attention-based architecture, which may create more predictable patterns in the decision boundaries that can be exploited during model extraction. The self-attention mechanism in ViTs could be making the model's reasoning more transparent to external observers through query responses, allowing thief models to more effectively reverse-engineer the decision logic. Additionally, the patch-based tokenization in ViTs might preserve spatial relationships in ways that make the model's learned representations more easily transferable.

## Foundational Learning

**Model Stealing Attacks**: Techniques where an adversary queries a target model to create a functionally equivalent substitute model. Needed to understand the threat landscape for deployed AI systems; quick check: can be implemented through black-box access to model predictions.

**Agreement Score**: Metric measuring similarity between thief and victim model predictions. Essential for quantifying attack success; quick check: calculated as the percentage of matching predictions on a test set.

**Vision Transformer Architecture**: Transformer-based models adapted for computer vision using patch tokenization and self-attention. Critical context for understanding architectural differences; quick check: replaces convolutional layers with multi-head attention mechanisms.

**Fine-tuning from Foundation Models**: Process of adapting pre-trained models to specific tasks using smaller, domain-specific datasets. Relevant because vulnerability may stem from the fine-tuning process; quick check: involves updating weights while retaining pre-trained knowledge.

**Black-box Access**: Scenario where an attacker can only query a model and observe outputs, without access to internal parameters or gradients. Defines the realistic threat model; quick check: attacker has API-like access to model predictions.

## Architecture Onboarding

Component map: Foundation Model -> Fine-tuned Model -> API Endpoint -> Thief Model Queries -> Substitute Model Training

Critical path: Thief Model (ViT-L/16) queries Victim Model -> Collect input-output pairs -> Train substitute model -> Measure agreement with victim

Design tradeoffs: The paper prioritizes architectural isolation by keeping thief model constant while varying victim architecture, sacrificing broader exploration of thief model variations for clearer attribution of vulnerability differences.

Failure signatures: When victim models are less vulnerable, agreement scores drop significantly (40-46% range), indicating successful resistance to model extraction under black-box conditions.

First experiments:
1. Replicate agreement score measurements using different query budgets to test scalability of attacks
2. Test cross-architecture stealing (ViT thief targeting ResNet victim) to validate architectural specificity
3. Measure agreement decay over time as victim model is subjected to continuous extraction attempts

## Open Questions the Paper Calls Out
None

## Limitations
- Results are limited to classification tasks and may not generalize to other computer vision applications like object detection or segmentation
- The study does not explore potential defense mechanisms or mitigation strategies that could reduce model stealing vulnerability
- Experimental conditions represent controlled environments without accounting for real-world API protection measures like rate limiting or input filtering

## Confidence

High: Core finding that ViTs are more susceptible to model stealing than ResNets under controlled experimental conditions

Medium: Broader implications for commercial deployment and real-world API scenarios, as the study uses idealized attack conditions

Low: Applicability across different foundation model families and fine-tuning approaches without additional validation

## Next Checks

1. Test the same attack methodology on additional vision tasks beyond classification, including object detection and semantic segmentation, to establish task-level vulnerability patterns

2. Evaluate the impact of common API defense mechanisms like rate limiting, input filtering, or gradient obfuscation on the success rates of model stealing attacks

3. Investigate whether hybrid architectures combining transformer and convolutional components maintain similar vulnerability patterns or offer improved security-accuracy trade-offs