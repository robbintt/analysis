---
ver: rpa2
title: 'Subjective Depth and Timescale Transformers: Learning Where and When to Compute'
arxiv_id: '2511.21408'
source_url: https://arxiv.org/abs/2511.21408
tags:
- routing
- computation
- surprise
- layer
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces two novel conditional computation architectures
  for decoder-only Transformers: Subjective Depth Transformers (SDT) and Subjective
  Timescale Transformers (STT). Both leverage Bayesian surprise signals to dynamically
  route computation, learning where and when to compute within the model.'
---

# Subjective Depth and Timescale Transformers: Learning Where and When to Compute

## Quick Facts
- arXiv ID: 2511.21408
- Source URL: https://arxiv.org/abs/2511.21408
- Reference count: 12
- Key outcome: Introduces Subjective Depth Transformers (SDT) and Subjective Timescale Transformers (STT) that achieve 37.5% self-attention savings and 25% KV-cache reduction at 50% capacity versus dense baselines.

## Executive Summary
This paper introduces two novel conditional computation architectures for decoder-only Transformers: Subjective Depth Transformers (SDT) and Subjective Timescale Transformers (STT). Both leverage Bayesian surprise signals to dynamically route computation, learning where and when to compute within the model. SDT uses alternating Decision and Dynamic layers with a Prior Feed-Forward Network to predict block outputs, while STT uses a Transition Network to predict temporal changes. The architectures demonstrate stable training and learn routing dynamics consistent with predictive coding principles. At reduced capacity (50% token processing), they achieve 37.5% self-attention savings and 25% KV-cache reductions compared to dense baselines, though downstream benchmark performance is lower due to the compute-accuracy trade-off. The STT variant shows particular promise, learning to reduce computational capacity in deeper layers, aligning with hierarchical processing theories. These architectures establish a framework for more efficient Transformer models while providing insights into the relationship between surprise-based routing and conditional computation effectiveness.

## Method Summary
The method introduces conditional computation architectures for decoder-only Transformers that route computation based on Bayesian surprise signals. SDT uses alternating Decision and Dynamic layers where the Decision layer computes both full posterior transformations and lightweight prior predictions via PriorFFN, while the Dynamic layer computes surprise metrics and applies Top-K routing. STT uses a unified layer with a Transition Network (TPN) that predicts residual updates based on the previous token's state, forming a temporal "change hypothesis." Both architectures use Top-K routing with fixed capacity (γ=0.5) to maintain hardware efficiency. Training involves multiple auxiliary losses: MSE prediction loss for the PriorFFN/TPN (λ=0.05), binary cross-entropy for causal router training (λ=0.01), and sparsity regularization (λ=0.001). The routing decision is based on Expected Change (CE) and Unexpected Change (CU) criteria derived from Variational Predictive Routing literature, computed as MSE between actual and predicted residuals.

## Key Results
- Achieved 37.5% self-attention savings and 25% KV-cache reduction at 50% capacity versus dense baselines
- STT learns to reduce computational capacity in deeper layers, aligning with hierarchical processing theories
- Demonstrated stable training with Top-K routing while achieving compute efficiency gains
- Shows training dynamics consistent with predictive coding principles (novelty-to-prediction shift)

## Why This Works (Mechanism)

### Mechanism 1: Bayesian Surprise as a Routing Signal
Tokens that induce larger belief updates in the model's internal representations may benefit more from additional computation. The architecture approximates Bayesian surprise (KL divergence between posterior and prior beliefs) using MSE between hidden state vectors, justified by treating token representations as means of isotropic Gaussian distributions with shared covariance. Two surprise metrics are computed: static surprise (magnitude of block residual) and change surprise (divergence between actual and predicted residual). These feed into differentiable gating criteria (Expected Change and Unexpected Change) that determine which tokens receive full processing.

### Mechanism 2: Alternating Decision-Dynamic Layer Architecture (SDT)
Separating prediction generation from conditional execution stabilizes training while enabling surprise-based routing. Decision Layers compute both a full "posterior" transformation via standard TF blocks and a lightweight "prior" prediction via PriorFFN. The subsequent Dynamic Layer receives both outputs, computes surprise metrics, and applies Top-K routing to select tokens for processing. Only selected tokens pass through the Dynamic Layer's TF block; others bypass via residual connection.

### Mechanism 3: Temporal Prior via Transition Network (STT)
Using the previous token's processed state to predict the current token's residual provides a more effective surprise signal than spatial self-prediction. STT replaces alternating layers with a unified architecture where a Transition Network (TPN) predicts the residual update for token t based on token t-1's output. This temporal prediction forms the "change hypothesis" compared against the actual block residual.

## Foundational Learning

- **Bayesian Surprise / Information-Theoretic Surprise**: The core routing mechanism depends on understanding how belief updates are quantified as KL divergence between posterior and prior distributions. Without this, the MSE approximation appears arbitrary. Quick check: Given two Gaussian distributions with means μ₁ and μ₂ and shared variance σ², does larger distance between means indicate higher or lower surprise?

- **Top-K Routing with Fixed Capacity**: The architectures maintain hardware efficiency through fixed-capacity routing. Understanding why Top-K (versus threshold-based) routing preserves static computation graphs is essential for implementation. Quick check: Why does selecting exactly K tokens per layer guarantee a static computation graph while threshold-based selection does not?

- **Predictive Coding Framework**: The paper's theoretical motivation and the two gating criteria (CE/CU) derive from predictive coding principles in neuroscience. The observed training dynamics (novelty-to-prediction shift) are interpreted through this lens. Quick check: In predictive coding, what does it mean when the "change hypothesis" better explains data than the "static hypothesis"?

## Architecture Onboarding

- Component map: Input → Decision Layer (TF-Block || PriorFFN) → Dynamic Layer (Router → Top-K → TF-Block) for SDT; Input → STT Layer (TPN predicts residual || TF-Block computes actual) → Router → Top-K → conditional execution for STT.

- Critical path: The surprise computation (MSE between actual and predicted residuals) → gating score generation (probabilistic OR of CE/CU) → Top-K selection → conditional block execution. Errors in the predictive network or router training cascade directly to routing quality.

- Design tradeoffs: SDT offers more modular design but requires alternating layers and separate PriorFFN training; STT uses simpler unified layer with better temporal priors but requires dual-token context for causal routing; fixed-capacity (Top-K) provides hardware efficiency but rigid allocation versus adaptive threshold-based selection.

- Failure signatures: Causal router accuracy collapse (SDT's single-token router struggled to learn mapping); PriorFFN/TPN prediction loss not decreasing (surprise signal becomes noise); all tokens selected or none selected (routing collapse); training dynamics not showing novelty-to-prediction shift (misalignment with surprise principles).

- First 3 experiments: 1) Train PriorFFN/TPN in isolation on frozen backbone; verify prediction loss decreases and correlates with manual surprise annotations. 2) Compare SDT, STT, and MoD at identical capacity (γ=0.5) on held-out validation perplexity. 3) For STT with dynamic capacity, log average tokens processed per layer across validation sequences; verify deeper layers learn reduced capacity.

## Open Questions the Paper Calls Out

- How do SDT and STT performance and routing dynamics scale with model size and extended pre-training compared to dense baselines? The authors identify the "single model scale" and limited adaptation regime as constraints, suggesting future work should "scale the experiments to larger models and longer training schedules."

- Can the Causal Router for the Subjective Depth Transformer (SDT) be redesigned to effectively approximate non-causal routing decisions using only the current token state? The paper notes that SDT's causal router "struggled to learn this mapping effectively" compared to the STT, identifying "improving and rigorously evaluating the causal router" as a key future direction.

- Do alternative metrics for Bayesian surprise, such as layer-normalized cosine similarity, provide a more effective signal for conditional computation than the MSE-based approximation? The authors propose exploring "more calibrated surprise metrics beyond MSE, such as layer-normalised cosine similarity."

## Limitations

- Theoretical grounding relies on Gaussian assumptions about token representations that are not empirically validated.
- Empirical validation limited to single model scale (0.5B parameters) and capacity setting (γ=0.5).
- Ablation gaps in PriorFFN/TPN architectural variants and comparison against alternative predictors.
- Routing stability characterization lacks analysis of dynamics over time and across different data distributions.

## Confidence

**High confidence**: The mathematical framework connecting Bayesian surprise to MSE routing signals under Gaussian assumptions is sound and internally consistent. The training methodology (multiple auxiliary losses, annealing schedules) is well-specified and reproducible.

**Medium confidence**: The empirical results demonstrating 37.5% attention savings and 25% KV-cache reduction at γ=0.5 are verifiable from reported metrics. The observation that STT learns to reduce capacity in deeper layers aligns with hierarchical processing theories and is supported by Figure 2C.

**Low confidence**: Claims about predictive coding principles guiding learned routing behavior (novelty-to-prediction shift) are inferred from training dynamics rather than explicitly measured. The assertion that temporal priors are inherently superior to spatial priors lacks comprehensive ablation across different data types and model scales.

## Next Checks

1. **Surprise signal validation**: Train PriorFFN and TPN in isolation on a frozen Qwen2.5-0.5B backbone. Measure prediction loss convergence and correlate with manually annotated surprise tokens (rare words, punctuation boundaries, syntactic shifts). Verify that MSE reduction correlates with downstream task importance.

2. **Router stability analysis**: During training of SDT and STT, log the percentage of tokens selected per layer, router accuracy metrics, and β parameter trajectories. Identify conditions under which routing collapses (all/none selected) and test whether modified annealing schedules prevent saturation.

3. **Capacity scaling study**: Train STT across γ ∈ {0.25, 0.5, 0.75} and measure the Pareto frontier of compute savings versus downstream performance. Verify that deeper layers consistently receive reduced capacity across different capacity settings, confirming hierarchical processing patterns.