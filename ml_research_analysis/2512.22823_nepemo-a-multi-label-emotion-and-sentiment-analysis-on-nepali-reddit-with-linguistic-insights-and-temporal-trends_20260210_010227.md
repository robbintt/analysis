---
ver: rpa2
title: 'NepEMO: A Multi-Label Emotion and Sentiment Analysis on Nepali Reddit with
  Linguistic Insights and Temporal Trends'
arxiv_id: '2512.22823'
source_url: https://arxiv.org/abs/2512.22823
tags:
- emotion
- sentiment
- nepali
- classification
- sadness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NepEMO, a novel dataset for multi-label emotion
  and sentiment analysis in Nepali social media text, addressing the lack of resources
  for emotion detection in low-resource languages. The dataset comprises 4,462 Reddit
  posts annotated for five emotions (fear, anger, sadness, joy, depression) and three
  sentiments (positive, negative, neutral).
---

# NepEMO: A Multi-Label Emotion and Sentiment Analysis on Nepali Reddit with Linguistic Insights and Temporal Trends

## Quick Facts
- arXiv ID: 2512.22823
- Source URL: https://arxiv.org/abs/2512.22823
- Reference count: 40
- Primary result: NepEMO dataset enables emotion/sentiment analysis in low-resource Nepali, with transformer models achieving high accuracy and F1-scores

## Executive Summary
This paper introduces NepEMO, a novel dataset for multi-label emotion and sentiment analysis in Nepali social media text, addressing the lack of resources for emotion detection in low-resource languages. The dataset comprises 4,462 Reddit posts annotated for five emotions (fear, anger, sadness, joy, depression) and three sentiments (positive, negative, neutral). The authors employ traditional ML models (SVM, KNN, RF, etc.), deep learning architectures (CNN, LSTM, Bi-LSTM), and transformer-based models (XLM-RoBERTa, DistilBERT) for classification tasks. Results show that transformer models, particularly XLM-RoBERTa, consistently outperform other approaches, achieving high accuracy and F1-scores across emotion and sentiment classification. The study also provides insights into linguistic patterns, temporal trends, and co-occurrence of emotions, contributing valuable resources for NLP research in Nepali and similar low-resource languages.

## Method Summary
The NepEMO dataset was created by collecting 4,462 Reddit posts in Nepali, which were manually annotated for five emotions (fear, anger, sadness, joy, depression) and three sentiments (positive, negative, neutral). The authors employed a multi-modal approach to classification, using traditional ML models (SVM, KNN, RF), deep learning architectures (CNN, LSTM, Bi-LSTM), and transformer-based models (XLM-RoBERTa, DistilBERT). Performance was evaluated using accuracy and F1-scores, with transformer models showing superior results. The study also conducted linguistic analysis and temporal trend examination to provide additional insights into the data.

## Key Results
- Transformer models (XLM-RoBERTa, DistilBERT) consistently outperformed traditional ML and deep learning approaches across all classification tasks
- XLM-RoBERTa achieved the highest accuracy and F1-scores for both emotion and sentiment classification
- The dataset revealed interesting temporal trends and co-occurrence patterns among emotions in Nepali Reddit discourse

## Why This Works (Mechanism)
The success of transformer models in this study can be attributed to their ability to capture contextual relationships in language through self-attention mechanisms. For low-resource languages like Nepali, pre-trained multilingual transformers (XLM-RoBERTa) leverage knowledge from related languages to compensate for limited training data. The multi-label classification framework effectively handles the complexity of emotions that often co-occur in social media text, while the Reddit corpus provides authentic, user-generated content that reflects natural language patterns and emotional expression in Nepali online discourse.

## Foundational Learning
- **Multi-label classification**: Needed to handle posts expressing multiple emotions simultaneously; quick check: verify label co-occurrence statistics
- **Transformer pre-training**: Provides language understanding for low-resource languages through transfer learning; quick check: compare with models trained from scratch
- **Cross-lingual embeddings**: Enable knowledge transfer from high-resource to low-resource languages; quick check: evaluate performance drop when using monolingual models
- **Reddit corpus characteristics**: Offers authentic, informal language data; quick check: analyze vocabulary diversity and informality markers
- **Emotion detection challenges**: Low-resource languages lack annotated data and linguistic resources; quick check: assess annotation consistency and coverage
- **Temporal trend analysis**: Reveals how emotional expression patterns change over time; quick check: validate against external events or seasonal patterns

## Architecture Onboarding
**Component Map**: Data Collection -> Annotation -> Preprocessing -> Traditional ML Models -> Deep Learning Models -> Transformer Models -> Evaluation -> Analysis

**Critical Path**: The evaluation pipeline follows: preprocessing → model training → performance metrics calculation → statistical significance testing → linguistic analysis

**Design Tradeoffs**: Traditional ML models offer interpretability but lower performance; deep learning models balance complexity and performance; transformer models provide highest accuracy but require more computational resources and lack interpretability

**Failure Signatures**: Poor performance on underrepresented emotion classes indicates dataset imbalance; low inter-annotator agreement suggests annotation ambiguity; inconsistent results across models may indicate preprocessing issues

**First Experiments**:
1. Baseline comparison: Evaluate all model types on a held-out test set with standard metrics
2. Ablation study: Test performance with and without preprocessing steps to identify critical features
3. Cross-lingual transfer: Compare multilingual transformer performance against monolingual Nepali-specific models

## Open Questions the Paper Calls Out
None

## Limitations
- Small dataset size (4,462 posts) may not capture full diversity of Nepali social media discourse
- Reddit-specific data may not generalize to other Nepali social media platforms or communication channels
- Limited representation of certain emotion categories affects statistical power for temporal trend analyses

## Confidence
- High confidence: Transformer models outperforming traditional ML approaches aligns with established NLP literature
- Medium confidence: Specific F1-scores and accuracy metrics are reliable but limited by small dataset size
- Medium confidence: Linguistic insights and temporal trend analyses are methodologically sound but constrained by Reddit-specific corpus

## Next Checks
1. Conduct cross-platform validation by testing NepEMO-trained models on Nepali social media data from Facebook, Twitter, or other platforms to assess generalizability
2. Perform inter-annotator agreement analysis and report Cohen's kappa scores to quantify annotation consistency and reliability
3. Expand the dataset through active learning or semi-supervised approaches to improve representation of underrepresented emotion categories and increase statistical power for temporal trend analyses