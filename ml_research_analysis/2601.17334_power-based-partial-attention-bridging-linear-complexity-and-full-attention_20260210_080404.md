---
ver: rpa2
title: 'Power-based Partial Attention: Bridging Linear-Complexity and Full Attention'
arxiv_id: '2601.17334'
source_url: https://arxiv.org/abs/2601.17334
tags:
- attention
- arxiv
- performance
- full
- window
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces power-based partial attention (PPA), a parameterized\
  \ attention mechanism that bridges the gap between linear-complexity sliding window\
  \ attention (O(L)) and full quadratic attention (O(L\xB2)). The PPA mechanism scales\
  \ as O(L^(1+p)) where p \u2208 [0,1], with p=0 corresponding to sliding window attention\
  \ and p=1 to full attention."
---

# Power-based Partial Attention: Bridging Linear-Complexity and Full Attention

## Quick Facts
- arXiv ID: 2601.17334
- Source URL: https://arxiv.org/abs/2601.17334
- Reference count: 32
- Primary result: PPA achieves near-full-attention performance with sub-quadratic complexity on math benchmarks

## Executive Summary
Power-based partial attention (PPA) introduces a parameterized attention mechanism that scales as O(L^(1+p)) where p ∈ [0,1], bridging the gap between linear-complexity sliding window attention and full quadratic attention. The key innovation uses incrementally increasing stride lengths to create a causal attention mask that allows each token to attend to O(√i) previous tokens while maintaining a regular strided pattern suitable for efficient computation. Experiments on NVIDIA-Nemotron-Nano-9B-v2 models demonstrate an S-shaped performance curve, with sub-quadratic attention (p≈0.75-0.875) achieving near-full-attention performance on MATH500 and GSM8k benchmarks.

## Method Summary
PPA modifies the causal attention mask by combining sliding window attention (constant W tokens) with a power-based incremental stride pattern. For position j, attended if floor(j^p) - floor((j-1)^p) = 1. The method uses PyTorch's flex-attention with a custom mask that unions the sliding window and PPA patterns. The approach is fine-tuned on 200k examples from the math subset of nvidia/Nemotron-Post-Training-Dataset-v2, preprocessed into 4-shot format matching MATH500 evaluation. Nine variants with p ∈ {0, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0} are trained to evaluate performance across the complexity spectrum.

## Key Results
- PPA with p=0.875 achieves 82.4% accuracy on MATH500 compared to 80.8% for full attention
- Performance follows an S-shaped curve with rapid gains in the range p≈0.75-0.875
- Optimal p value depends on training data quantity and prompt format
- Neither sliding window (p=0) nor full attention (p=1) is necessarily optimal for downstream performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incrementally increasing stride lengths achieve sub-quadratic complexity while preserving regular patterns suitable for efficient computation.
- Mechanism: Each token at position i attends to O(i^p) previous tokens using stride lengths that grow as O(L^(1-p)). For p=0.5, positions follow squared values (1,4,9,16,25,36...). The mask is computed via ⌊j^p⌋ - ⌊(j-1)^p⌋, yielding 0 or 1 for each position.
- Core assumption: Key information for downstream tasks can be captured through progressively sparser sampling of distant tokens.
- Evidence anchors:
  - [abstract] "allowing each token to attend to O(√i) previous tokens (for p=0.5) while maintaining a regular strided pattern suitable for efficient computation"
  - [section 2] "A strategy to overcome this issue is to use incrementally increasing stride lengths that preserve both the strided pattern and the √L scaling for each token"
  - [corpus] Sparse attention patterns with structured sparsity are explored in related work, though corpus lacks direct comparison to incremental stride approaches.
- Break condition: If task requires dense token-to-token comparisons (e.g., pairwise classification), sparse striding may miss critical interactions.

### Mechanism 2
- Claim: Combining power-based attention with sliding window ensures local context visibility without affecting asymptotic complexity.
- Mechanism: Union of two masks—sliding window (constant W tokens) plus incremental stride pattern. Since sliding window is O(L), total complexity remains O(L^(1+p)).
- Core assumption: Recent tokens carry disproportionate importance for coherent generation, while distant tokens can be sampled sparsely.
- Evidence anchors:
  - [section 3] "Since sliding window attention has O(L) complexity, it does not affect the overall asymptotic scaling, but it is crucial for generating coherent text by making recent tokens visible to the model"
  - [section 2, Figure 1d] Visualizes combined pattern with blue (sliding window) and red (incremental stride) cells
  - [corpus] Sliding Window Attention Adaptation (SWAA) paper corroborates SWA's linear complexity but notes performance gaps without additional mechanisms.
- Break condition: If critical information lies at intermediate distances (between window and stride points), performance degrades.

### Mechanism 3
- Claim: Performance transitions in an S-curve with rapid gains in narrow p range (≈0.75–0.875), suggesting sub-quadratic attention suffices for near-full performance.
- Mechanism: Not a formal mechanism but empirical finding—model capacity saturates before full attention. Optimal p depends on training data quantity and prompt format.
- Core assumption: Observed S-curve on math benchmarks (MATH500, GSM8k) generalizes to other reasoning tasks.
- Evidence anchors:
  - [abstract] "Experiments on NVIDIA-Nemotron-Nano-9B-v2 models show an S-shaped performance curve... PPA with p=0.875 achieves 82.4% accuracy on MATH500 compared to 80.8% for full attention"
  - [section 4, Table 1] Shows p=0.875 outperforming p=1.0 on MATH500 (0.824 vs 0.808) but not GSM8k (0.907 vs 0.922)
  - [corpus] Related linear/sparse attention papers show varied task-dependent results; no direct S-curve corroboration found.
- Break condition: Tasks requiring dense global reasoning (e.g., long-document QA, code analysis) may require higher p or full attention.

## Foundational Learning

- Concept: **Asymptotic complexity in attention (O(L) vs O(L²) vs O(L^(1+p)))**
  - Why needed here: PPA's core contribution is parameterized complexity; understanding the scaling curves is essential for hardware planning.
  - Quick check question: For p=0.75 and L=100K tokens, what is the approximate attention operation count relative to full attention?

- Concept: **Causal masking and autoregressive constraints**
  - Why needed here: PPA modifies the causal mask; you must understand how query-key visibility is controlled during generation.
  - Quick check question: Why can't encoder-style dynamic stride patterns (using global L) be directly applied to decoder-only models?

- Concept: **Stride patterns and token coverage density**
  - Why needed here: PPA's incremental stride determines which distant tokens are visible; understanding coverage gaps is critical for debugging.
  - Quick check question: For p=0.5, what is the maximum gap between consecutive attended positions at sequence position 100?

## Architecture Onboarding

- Component map:
  - Attention mask generator -> PPA pattern calculator -> Sliding window unioner -> Flex-attention kernel

- Critical path:
  1. Determine target complexity budget (choose p)
  2. Initialize sliding window size for local coherence
  3. Construct incremental stride positions per query token
  4. Compute union mask and pass to flex-attention
  5. Fine-tune pretrained model on task-specific data to adapt to new attention pattern

- Design tradeoffs:
  - Lower p (0.25–0.5): Faster inference, sparser coverage, larger performance gap
  - Higher p (0.75–0.875): Near-full performance, but less speedup vs optimized FlashAttention
  - Training data vs p: More training data may enable lower optimal p (paper's GSM8k underfitting required higher p than MATH500)

- Failure signatures:
  - Incoherent generation: Sliding window too small or missing
  - Missing distant dependencies: p too low for task's context requirements
  - Poor transfer: Model pretrained on full attention but not fine-tuned on PPA mask
  - Kernel inefficiency: Custom mask in flex-attention slower than expected—verify sparse pattern optimization

- First 3 experiments:
  1. p-sweep baseline: Evaluate pretrained model (without PPA fine-tuning) across p∈{0, 0.25, 0.5, 0.75, 0.875, 1.0} to establish zero-shot degradation curve.
  2. Fine-tuning adaptation: Fine-tune each p-variant on task data (as paper does with 200k math examples) and measure recovery toward full-attention performance.
  3. Prompt format sensitivity test: Evaluate fine-tuned models on held-out prompt formats to probe whether optimal p shifts with distribution (replicating paper's MATH500 vs GSM8k finding).

## Open Questions the Paper Calls Out
- Can large-scale pre-training (from scratch) achieve competitive performance with lower p values than the p≈0.75–0.875 observed with limited fine-tuning?
- Does the S-curve transition point generalize across diverse tasks, model architectures, and scale?
- Can hardware-optimized PPA kernels achieve practical wall-clock speedups over FlashAttention despite irregular memory access patterns?
- What are the theoretical approximation guarantees for PPA relative to full attention?

## Limitations
- Empirical generalizability uncertain: S-curve may be model-specific or task-dependent
- Optimal p varies with training data quantity and prompt format
- Mechanism explaining sub-quadratic sufficiency remains speculative without theoretical bounds
- Implementation dependent on flex-attention kernel performance characteristics

## Confidence

**High Confidence**: The basic mechanism of PPA (incremental stride pattern + sliding window union) is clearly specified and implementable. The O(L^(1+p)) complexity analysis follows directly from the construction.

**Medium Confidence**: The empirical findings on NVIDIA-Nemotron-Nano-9B-v2 show consistent S-curves, but the specific p values for optimal performance may not generalize across different model architectures, datasets, or training regimes.

**Low Confidence**: The theoretical explanation for why the S-curve appears and why p≈0.75-0.875 represents a "sweet spot" remains incomplete. The paper does not provide bounds on information preservation or explain the rapid performance transition in narrow p ranges.

## Next Checks
1. **Architecture Transfer Validation**: Implement PPA on a different base model (e.g., LLaMA or Mistral) and evaluate whether the same S-shaped curve appears on MATH500. Compare the optimal p value and performance gap to the NVIDIA-Nemotron results to assess model dependence.

2. **Task Diversity Test**: Apply PPA to non-math reasoning tasks (e.g., long-document QA, code completion, or commonsense reasoning) to determine if the performance transition pattern holds. This would validate whether the mechanism generalizes beyond the specific mathematical reasoning context.

3. **Theoretical Bounds Analysis**: Derive information-theoretic bounds on the fraction of token interactions preserved by PPA with different p values. Compare these bounds to empirical performance drops to determine whether the S-curve reflects fundamental information constraints or model capacity limitations.