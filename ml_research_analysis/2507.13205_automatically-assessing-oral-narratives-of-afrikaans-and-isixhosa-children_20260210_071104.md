---
ver: rpa2
title: Automatically assessing oral narratives of Afrikaans and isiXhosa children
arxiv_id: '2507.13205'
source_url: https://arxiv.org/abs/2507.13205
tags:
- children
- isixhosa
- system
- linear
- afrikaans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We develop an automated system for assessing oral narratives of
  preschool children in Afrikaans and isiXhosa. The system uses automatic speech recognition
  followed by either a linear model or a large language model (LLM) to predict narrative
  and comprehension scores.
---

# Automatically assessing oral narratives of Afrikaans and isiXhosa children

## Quick Facts
- arXiv ID: 2507.13205
- Source URL: https://arxiv.org/abs/2507.13205
- Reference count: 0
- We develop an automated system for assessing oral narratives of preschool children in Afrikaans and isiXhosa using automatic speech recognition followed by either a linear model or a large language model (LLM) to predict narrative and comprehension scores.

## Executive Summary
This paper presents an automated system for assessing oral narratives of preschool children in Afrikaans and isiXhosa. The system uses automatic speech recognition followed by either a linear model or a large language model (LLM) to predict narrative and comprehension scores. The LLM-based system outperforms the linear model in most cases, achieving more than 80% accuracy for identifying Afrikaans children requiring intervention and 64% for isiXhosa. Despite high ASR error rates, the LLM approach performs comparably to a human expert. Translation of ASR transcripts to English further improves LLM performance. The linear model provides interpretable insights into linguistic patterns associated with intervention needs. These results demonstrate the potential of automated assessment to assist teachers in identifying children requiring support, even in low-resource language settings.

## Method Summary
The method uses a two-stage pipeline: first, Whisper-based ASR fine-tuned on 5 minutes of child speech per language transcribes the audio recordings; second, either a lasso-regularized linear model or Gemini-1.5-Flash-8B with in-context learning scores the transcripts. For the LLM approach, transcripts are translated to English before prompting. The linear model uses features like word counts, duration metrics, and Flesch-Kincaid scores, while the LLM uses in-context learning with the full training set as examples. The system predicts four scores: Narrative Skill (0-16), Comprehension Questions (0-10), Structural Complexity (6 ordinal classes), and Requires Intervention (binary).

## Key Results
- LLM-based system achieves 89% accuracy for identifying Afrikaans children requiring intervention and 64% for isiXhosa
- Despite 28-34% ASR error rates, LLM performance remains comparable to human expert scoring
- Translation of ASR transcripts to English consistently improves LLM performance across all metrics
- Linear model provides interpretable features but underperforms LLM (R² 0.20 vs 0.52 for Afrikaans NS)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Noisy ASR outputs retain sufficient signal for downstream scoring even at 28-34% character error rates.
- Mechanism: Whisper-based ASR transcribes child speech → transcripts feed into scoring models → models extract semantic and structural patterns that correlate with expert assessments. The scoring models appear robust to word-level errors because they aggregate over longer narrative sequences.
- Core assumption: The semantic content relevant to narrative assessment (e.g., goal-directed language, emotional terms) survives ASR noise at recoverable rates.
- Evidence anchors:
  - [abstract] "The LLM-based system is comparable to a human expert in flagging children who require intervention, achieving more than 80% accuracy for Afrikaans and 64% for isiXhosa."
  - [Section 5, "The effect of imperfect ASR"] "Despite relatively high error rates, the expected drop in ASR scores is not substantial. Notably, in some cases, ASR scores exceed oracle scores."
  - [corpus] Limited direct corpus evidence; related work [11896] confirms high ASR error rates on same data but does not validate downstream robustness independently.
- Break condition: If ASR errors systematically distort specific linguistic markers (e.g., goal-related vocabulary), scoring accuracy will degrade unpredictably.

### Mechanism 2
- Claim: Machine translation to English before LLM scoring improves performance for low-resource languages.
- Mechanism: ASR output (Afrikaans/isiXhosa) → Google Translate API → English text → LLM prompting. Translation normalizes noisy ASR output and maps it to the LLM's dominant training language, improving pattern recognition.
- Core assumption: The translation model handles child speech disfluencies and ASR errors gracefully, and meaning is preserved sufficiently for scoring.
- Evidence anchors:
  - [Section 3.3] "Despite being multilingual and understanding Afrikaans and isiXhosa, LLMs are primarily trained on English text. To address this, we first translate the Afrikaans and isiXhosa ASR output to English."
  - [Section 5, "The effect of translation"] Table 3 shows consistent improvement with translation across all metrics for both languages.
  - [corpus] No direct corpus validation of translation-then-LLM pipeline for child speech scoring.
- Break condition: If translation introduces systematic semantic drift—especially for culturally specific narrative elements—LLM scoring will inherit compounded errors.

### Mechanism 3
- Claim: In-context learning with full training set examples enables LLMs to approximate expert scoring without task-specific fine-tuning.
- Mechanism: Prompt = (training examples with transcripts + expert scores) + instruction + unseen transcript → LLM outputs predicted scores. The LLM infers scoring rubrics implicitly from examples.
- Core assumption: The LLM's pre-trained linguistic knowledge combines with few-shot pattern matching to generalize to new narratives; the training set is representative of test distribution.
- Evidence anchors:
  - [Section 3.3] "Performance consistently improved with more examples, so we use the entire training set in each prompt for the final system."
  - [Section 5, "LLM vs linear"] LLM achieves R² of 0.52 for Afrikaans NS vs 0.20 for linear model; RI accuracy 89% vs 61%.
  - [corpus] Weak corpus evidence; related work [33, 34] uses BERT-based classifiers rather than in-context LLM scoring.
- Break condition: If test narratives diverge in structure, vocabulary, or error patterns from training examples, in-context learning may fail to generalize.

## Foundational Learning

- **Concept: Whisper ASR fine-tuning on limited child speech**
  - Why needed here: Standard ASR models trained on adult speech perform poorly on child speech; fine-tuning with even 5 minutes of domain data adapts the model to child acoustic patterns.
  - Quick check question: Can you explain why character error rate (CER) is preferred over word error rate (WER) for agglutinative languages like isiXhosa?

- **Concept: Lasso (L1) regularization for feature selection in small data regimes**
  - Why needed here: With ~200 training samples and 1000+ features (word counts, duration metrics), overfitting is likely; Lasso shrinks irrelevant feature weights to zero.
  - Quick check question: What happens to a linear model's interpretability and variance when you increase the L1 penalty α?

- **Concept: In-context learning (few-shot prompting) for LLMs**
  - Why needed here: No gradients are updated; the LLM uses its pre-trained knowledge plus task demonstrations to generalize. Critical when labeled data is too scarce for fine-tuning.
  - Quick check question: How does increasing the number of in-context examples typically affect performance, and what is the limiting factor?

## Architecture Onboarding

- **Component map:**
  [Child Speech Recording] → [Whisper-based ASR Model] → [Text Transcript in Afrikaans/isiXhosa] → [Option A: Linear Model] or [Option B: LLM-based Scoring]

- **Critical path:** ASR accuracy → translation quality (for LLM path) → scoring model. The LLM path is currently superior but depends on external APIs; the linear path is interpretable and deployable offline.

- **Design tradeoffs:**
  - **LLM vs Linear:** LLM achieves higher accuracy but requires cloud API, introduces latency, and is less interpretable. Linear model is fast, interpretable, and offline-capable but underperforms on complex scoring.
  - **With vs without translation:** Translation improves LLM performance but adds pipeline complexity and potential error propagation.
  - **Full training set in prompt vs subset:** More examples improve performance but increase token costs and latency.

- **Failure signatures:**
  - isiXhosa consistently underperforms Afrikaans (RI: 64% vs 89%)—likely due to higher ASR CER and weaker LLM/translation support for morphologically rich languages.
  - Large dev-test performance gaps (e.g., CQ R² drops from 0.41 to 0.20) suggest sensitivity to specific children in each split.
  - Linear model features like type-token ratio and morphological parsing showed no improvement—possibly due to noise sensitivity.

- **First 3 experiments:**
  1. **Ablate translation:** Score isiXhosa/Afrikaans transcripts directly with a multilingual LLM (e.g., GPT-4o) without translation to isolate translation error contribution.
  2. **Feature audit on linear model:** Extract the top 20 non-zero Lasso features per score type and manually inspect whether they align with known narrative assessment criteria (e.g., goal-directed vocabulary).
  3. **Error analysis on RI false negatives:** Identify children the LLM failed to flag for intervention; examine whether ASR or translation corrupted key narrative markers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can end-to-end models trained directly on speech improve performance compared to the cascaded ASR-to-LLM pipeline?
- Basis in paper: [explicit] The conclusion states that "Future work will look at... end-to-end training methods that go directly from speech to a score."
- Why unresolved: The current study relies on a two-step process where ASR errors propagate to the scoring model. An end-to-end approach might bypass transcription errors but was not implemented or tested in this work.
- What evidence would resolve it: A comparative study evaluating the accuracy of an end-to-end model against the cascaded LLM baseline using the same Afrikaans and isiXhosa datasets.

### Open Question 2
- Question: How does the removal of the "idealised" pre-processing step affect system performance in realistic classroom settings?
- Basis in paper: [explicit] The Data section notes that assessor speech was manually removed, creating an "idealised scenario, equivalent to using a perfect diarisation system," and states future work will look at a "complete system."
- Why unresolved: The current results are based on clean audio containing only the child's voice. Real-world deployment requires the system to distinguish the target child's speech from a teacher's prompts and background noise automatically.
- What evidence would resolve it: Evaluating the system on raw, unsegmented audio recordings where speaker diarisation must be performed automatically rather than manually.

### Open Question 3
- Question: Does the performance gap between isiXhosa and Afrikaans stem from inherent data properties or the machine translation process?
- Basis in paper: [explicit] The authors ask whether "lower isiXhosa performance stems from inherent properties of the data itself, or from limitations in the automatic English-to-isiXhosa translation process."
- Why unresolved: The study observed lower intervention classification accuracy for isiXhosa (64%) compared to Afrikaans (89%), but the error analysis could not definitively isolate the root cause of this discrepancy.
- What evidence would resolve it: An ablation study comparing model performance when using human-translated references versus machine-translated outputs, specifically analyzing error rates relative to isiXhosa's agglutinative morphology.

## Limitations
- High ASR error rates (28-34% CER) remain a fundamental bottleneck despite system robustness
- Reliance on Google Translate and Gemini-1.5-Flash-8B introduces dependency on external APIs and potential translation artifacts
- Modest test set size (n=28) for each language constrains statistical confidence in performance claims

## Confidence

- **High confidence**: Linear model interpretability and feature selection patterns; translation consistently improves LLM performance across all metrics.
- **Medium confidence**: LLM vs human expert comparability for intervention detection (especially for Afrikaans at 89% accuracy); general trend of LLM outperforming linear model.
- **Low confidence**: Exact contribution of translation quality to LLM gains; stability of LLM scoring across prompt variations; whether the 80% Afrikaans accuracy generalizes to broader populations.

## Next Checks

1. **Ablate translation pipeline**: Score isiXhosa/Afrikaans transcripts directly with a multilingual LLM (e.g., GPT-4o) without translation to isolate translation error contribution and test whether translation artifacts explain the isiXhosa performance gap.

2. **Error analysis on RI false negatives**: Identify children the LLM failed to flag for intervention; examine whether ASR or translation corrupted key narrative markers (e.g., goal-directed vocabulary, emotional terms) that would have triggered correct classification.

3. **Confidence interval estimation**: Using bootstrap resampling on the test set, compute 95% confidence intervals for all reported metrics (R², κ, accuracy) to quantify statistical uncertainty given the small sample size.