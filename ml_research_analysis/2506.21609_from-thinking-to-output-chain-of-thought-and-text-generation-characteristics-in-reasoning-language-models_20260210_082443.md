---
ver: rpa2
title: 'From Thinking to Output: Chain-of-Thought and Text Generation Characteristics
  in Reasoning Language Models'
arxiv_id: '2506.21609'
source_url: https://arxiv.org/abs/2506.21609
tags:
- reasoning
- consistency
- logical
- score
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares the reasoning processes and outputs of four
  leading reasoning language models (GPT-o1, DeepSeek-R1, Kimi-k1.5, and Grok-3) using
  a multi-domain evaluation dataset. A novel framework analyzes self-reflection patterns
  through keywords and LLM-as-a-judge paradigm, linking internal thinking processes
  to final outputs.
---

# From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models

## Quick Facts
- arXiv ID: 2506.21609
- Source URL: https://arxiv.org/abs/2506.21609
- Reference count: 21
- Four leading reasoning models (GPT-o1, DeepSeek-R1, Kimi-k1.5, Grok-3) show significant differences in reflection depth, breadth, and consistency across domains

## Executive Summary
This study analyzes the reasoning processes and outputs of four leading reasoning language models using a multi-domain evaluation dataset. The authors introduce a novel framework that examines self-reflection patterns through keyword statistics and employs an LLM-as-a-judge paradigm to link internal thinking processes to final outputs. Results reveal that models exhibit domain-specific reasoning behaviors, with coding and math tasks eliciting the most extensive internal reasoning, while adversarial and sensitive domains show suppressed reflection depth. The findings highlight trade-offs between computational efficiency and reasoning robustness, offering insights into model design and training strategies.

## Method Summary
The study compares reasoning processes across four models using an 80-item multi-domain dataset spanning eight categories. The authors collect reasoning traces from each model and analyze them using three metrics: Total Reflection Count (TRC) for cumulative reflection keyword occurrences, Reflection Data Count (RDC) for texts containing reflection keywords, and Consistency Score (CS) for alignment with GPT-o1 using an LLM-as-a-judge paradigm. The framework employs keyword extraction and automated evaluation to assess both reasoning processes and final outputs, with results organized by domain and model.

## Key Results
- Coding and math tasks elicit the most extensive internal reasoning across all models, while medical and adversarial domains show significantly lower reflection counts
- DeepSeek-R1 demonstrates strong alignment with GPT-o1 in humanities and physics but weaker consistency in coding tasks
- Models exhibit varying patterns of reasoning depth and breadth, with some excelling in structured tasks while others show higher consistency in abstract reasoning domains
- Safety alignment and adversarial training appear to suppress reasoning depth in sensitive contexts, creating "reasoning vacuums"

## Why This Works (Mechanism)

### Mechanism 1
Extended internal reasoning is selectively activated by domains requiring multi-step derivation (e.g., Coding, Math) rather than knowledge retrieval (e.g., Medicine), suggesting a resource allocation strategy. The model dynamically adjusts compute based on task classification; structured, verifiable problems trigger "Total Reflection Count" (TRC) spikes, whereas factual tasks trigger retrieval. Core assumption: High reflection counts in the "thinking" phase directly correlate with computational effort for logical derivation rather than simple verbosity.

### Mechanism 2
Alignment with a benchmark model's *reasoning process* (GPT-o1) does not guarantee alignment in *final output*, indicating decoupled strategy and execution phases. Models may arrive at similar conclusions via divergent "reasoning paths" (low process consistency) or divergent conclusions via similar paths. The Consistency Score (CS) evaluates structural alignment, not just result accuracy. Core assumption: The "LLM-as-a-judge" (Doubao-1.5-Pro) can reliably discern structural logic over semantic similarity.

### Mechanism 3
Safety alignment and adversarial training suppress the depth of reasoning (TRC) in sensitive domains, creating "reasoning vacuums." In adversarial or sensitive contexts, models prioritize refusal or direct retrieval over exploratory "self-reflection" to minimize the risk of generating harmful intermediate steps. Core assumption: Lower TRC in Advbench is a feature of safety refusal, not a bug of incompetence.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) as an Intermediate State**
  - Why needed here: The paper analyzes the "thinking" text separately from the "output." You must distinguish between the model's internal monologue (reasoning tokens) and the final generation.
  - Quick check question: Can you identify the separator tags (e.g., `<think()>`) or structural shift in the API response that distinguishes reasoning from the final answer?

- **Concept: LLM-as-a-Judge Paradigm**
  - Why needed here: The Consistency Score (CS) relies on an automated judge (Doubao-1.5-Pro) rather than human labeling. Understanding the bias and prompt sensitivity of this judge is critical.
  - Quick check question: Does the judge evaluate based on factual accuracy or structural similarity to the reference outline (GPT-o1)? (The paper prioritizes structure/outline).

- **Concept: Exploration vs. Exploitation in Reasoning**
  - Why needed here: The paper frames high TRC/RDC as "exploration" (trying different paths) versus "exploitation" (direct retrieval).
  - Quick check question: In the context of this paper, does a low Reflection Data Count (RDC) imply the model failed to reason or that it efficiently retrieved the answer?

## Architecture Onboarding

- **Component map:** Multi-domain Dataset (80 items) -> Target Models (DeepSeek-R1, Grok-3, Kimi-k1.5) vs. GPT-o1 (Control) -> Keyword Extraction + LLM-Judge -> Comparative analysis of "Thinking" vs. "Output" consistency
- **Critical path:**
  1. Define the set of "reflection keywords" (e.g., "let me check," "makes sense") used for TRC calculation
  2. Establish the "Judge System Prompt" (Appendix A) which enforces the 1-5 scoring rubric based on the GPT-o1 outline
- **Design tradeoffs:**
  - Keyword vs. Semantic Analysis: The authors use keyword statistics (TRC) for depth. *Assumption:* This is computationally cheaper but may miss implicit reasoning that doesn't use specific trigger words.
  - Reference Bias: Using GPT-o1 as the "Ground Truth" outline penalizes models that use valid but distinct reasoning strategies.
- **Failure signatures:**
  - The "Refusal" Drop: A sudden drop in TRC/RDC in the Advbench or Medical domains likely indicates safety refusals rather than lack of capability.
  - The "Hallucination" Loop: High TRC (many "wait," "think again" keywords) combined with low Output Consistency suggests the model is "spinning" without resolution.
- **First 3 experiments:**
  1. **Validation Run:** Take 5 samples from the "Coding" domain. Run them through the pipeline and manually verify if the extracted keywords actually correlate with logical pivots in the text.
  2. **Judge Consistency Check:** Feed the same Model Output + GPT-o1 Outline pair into the Judge (Doubao-1.5-Pro) 3 times to check variance in the Consistency Score.
  3. **Domain Stress Test:** Compare a "Riddle" (high ambiguity) vs. "Math" (high structure) prompt on a single target model to observe the delta in Reasoning Consistency (CS) vs. Output Consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed evaluation framework perform when applied to a broader range of reasoning models and task domains beyond the four specific models and eight domains tested?
- Basis in paper: [explicit] The Conclusion states, "Future work can extend this framework to more models and tasks, further advancing the development of robust and interpretable LLMs."
- Why unresolved: The current study is limited to a comparative analysis of GPT-o1, DeepSeek-R1, Kimi-k1.5, and Grok-3, leaving the framework's generalizability to other architectures or domains unproven.
- What evidence would resolve it: Successful application of the TRC, RDC, and Consistency Score metrics to emerging models (e.g., Llama or Claude variants) and different reasoning modalities, yielding consistent evaluation results.

### Open Question 2
- Question: Do keyword-based metrics (TRC and RDC) accurately capture reasoning depth in models that utilize implicit reflection or non-standard phrasing?
- Basis in paper: [inferred] The methodology relies on "keywords statistic" to measure reflection frequency, assuming specific phrases (e.g., "let me think") correlate directly with reasoning depth.
- Why unresolved: Models may perform complex logical deductions without using the specific reflection keywords defined in the study, or conversely, use keywords superficially without deep reasoning.
- What evidence would resolve it: A correlation analysis comparing keyword frequency scores with semantic-based evaluations (e.g., human grading or embedding similarity) of reasoning quality across diverse model outputs.

### Open Question 3
- Question: Are the observed patterns of reasoning consistency and reflection depth robust when evaluated on a dataset significantly larger than the 80 curated items used in this study?
- Basis in paper: [inferred] The evaluation dataset consists of only 10 "hand-selected" data points per domain, which may not capture the full variance of model behavior.
- Why unresolved: Small sample sizes can be heavily influenced by outliers or specific selection criteria, potentially masking true performance distributions or domain-specific instabilities.
- What evidence would resolve it: A replication of the study's findings using an automated, large-scale benchmark (e.g., 500+ items per domain) to confirm that the variance and mean scores remain stable.

## Limitations

- The reflection keyword list appears incomplete (only examples provided), which may undercount reasoning depth in models using implicit reasoning without trigger words
- The LLM-as-a-judge paradigm introduces unknown variance - no reported inter-judge reliability or consistency checks across multiple runs
- Domain coverage may be insufficient for generalization, with only 10 samples per domain from potentially narrow benchmark sources

## Confidence

- High confidence: Domain-specific performance patterns (Math/Coding showing higher reflection counts) - well-supported by data patterns
- Medium confidence: Cross-model consistency findings - dependent on judge reliability and keyword completeness
- Low confidence: Causal claims about reasoning depth vs. output quality - correlation observed but mechanistic proof lacking

## Next Checks

1. Replicate judge consistency by running 20 random samples through Doubao-1.5-Pro three times each to measure scoring variance and establish reliability thresholds
2. Conduct blind human evaluation on 15 samples comparing model outputs to judge scores to assess alignment between automated and human consistency ratings
3. Test keyword detection robustness by manually annotating reasoning traces for 10 samples to verify automatic counting captures all reflection instances, then expand keyword list based on findings