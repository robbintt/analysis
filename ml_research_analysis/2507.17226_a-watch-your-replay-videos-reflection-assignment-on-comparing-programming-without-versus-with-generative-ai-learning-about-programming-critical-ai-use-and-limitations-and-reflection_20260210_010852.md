---
ver: rpa2
title: 'A "watch your replay videos" reflection assignment on comparing programming
  without versus with generative AI: learning about programming, critical AI use and
  limitations, and reflection'
arxiv_id: '2507.17226'
source_url: https://arxiv.org/abs/2507.17226
tags:
- students
- learning
- reflection
- programming
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduced a novel video-based reflection assignment
  for computing students, designed using the DEAL framework, to compare their programming
  processes with and without generative AI. Students recorded two programming sessions,
  then reflected on differences in planning, debugging, help-seeking, and AI use.
---

# A "watch your replay videos" reflection assignment on comparing programming without versus with generative AI: learning about programming, critical AI use and limitations, and reflection

## Quick Facts
- arXiv ID: 2507.17226
- Source URL: https://arxiv.org/abs/2507.17226
- Authors: Sarah "Magz" Fernandez; Greg L Nelson
- Reference count: 39
- One-line primary result: Video-based DEAL reflection assignment helped computing students develop critical AI literacy and metacognitive awareness about programming processes.

## Executive Summary
This study introduced a novel video-based reflection assignment for computing students, designed using the DEAL framework, to compare their programming processes with and without generative AI. Students recorded two programming sessions, then reflected on differences in planning, debugging, help-seeking, and AI use. Analysis of 30 student reflections revealed that the assignment deepened metacognitive awareness: students learned about AI's limitations, benefits, and critical use, as well as general programming skills like planning and collaboration. They also set concrete goals to improve their coding and reflective practices. The approach fostered critical AI literacy and lifelong learning habits. The authors recommend further refinement for novice students and suggest scaling the method through research-practitioner partnerships to integrate AI ethically and effectively in computing education.

## Method Summary
The study deployed a DEAL framework-based assignment where students recorded two screen-captured programming sessions (one without AI, one with AI) during team projects. Students then watched their videos at 2x speed and completed a structured 4-column template reflecting on challenges, time distribution across activities, and comparing sessions. Analysis used thematic analysis with affinity diagramming on three specific questions about learning and goals. The assignment was implemented in an upper-level software engineering course with 30 students working on React/Firebase web apps and a Discord bot.

## Key Results
- Students reported learning to slow down and understand before writing or generating code
- Students learned and reflected on AI limits and downsides, and strategies to use AI more critically
- Students set concrete goals to improve their coding and reflective practices

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Video-stimulated recall may enable pattern recognition in one's own programming behaviors that in-the-moment reflection cannot achieve.
- **Mechanism**: Temporal distance from the cognitive load of programming allows students to observe their own behaviors as an external observer would, revealing patterns in help-seeking, planning, and debugging that remain invisible during task execution.
- **Core assumption**: Students can accurately observe and interpret their own behaviors when given structured guidance.
- **Evidence anchors**:
  - [abstract]: "Students reported learning to slow down and understand before writing or generating code, recognized patterns in their problem-solving approaches"
  - [section]: "For the Describe step, we had students record and then later review a screen recording of each programming session"
  - [corpus]: Weak/no direct corpus evidence—the paper explicitly states "having students review videos of their work as an assignment is rare in computing education"
- **Break condition**: If students watch videos superficially (skimming without engagement) or lack scaffolding questions, pattern recognition likely will not occur.

### Mechanism 2
- **Claim**: Comparing sessions with versus without GenAI may create cognitive contrast that prompts deeper reflection on tool dependencies and trade-offs.
- **Mechanism**: The "compare the differences" column forces explicit articulation of what changed when GenAI was introduced, surfacing trade-offs that single-session reflection would miss.
- **Core assumption**: Students have sufficient baseline programming experience and task similarity to meaningfully compare sessions.
- **Evidence anchors**:
  - [abstract]: "Students learned and reflected on AI limits and downsides, and strategies to use AI more critically"
  - [section]: "Students also compared their two programming sessions... For each reflection question from the prior steps, students answered 'Compare and Evaluate: write about 3-5 bullet points... on the differences between the two videos / programming sessions, [and] what caused those differences'"
  - [corpus]: Limited support—"Demystify, Use, Reflect" shows structured LLM reflection emerging, but not comparative video approaches
- **Break condition**: If the two sessions involve fundamentally different tasks or difficulty levels, comparison may confound task complexity with tool effects.

### Mechanism 3
- **Claim**: The DEAL framework's sequential structure (Describe→Examine→Articulate) may guide students from surface observations to transferable, actionable insights.
- **Mechanism**: The scripted question sequence prevents premature conclusions, while the "Articulate Learning" phase explicitly prompts transfer by asking how learning will be applied.
- **Core assumption**: Students engage genuinely with scaffolded questions rather than producing performative responses.
- **Evidence anchors**:
  - [abstract]: "They also set concrete goals to improve their coding and reflective practices"
  - [section]: "In the Articulate Learning step, students answered... 'In what ways will I use this learning?' 'What goals might I set...'"
  - [corpus]: No DEAL-specific evidence in computing education—paper states "we have found no research on computing students reflecting using an assignment following the DEAL framework"
- **Break condition**: If scaffolding is overly prescriptive, it may constrain authentic reflection; if too loose, students may produce superficial responses.

## Foundational Learning

- **Concept: Metacognitive awareness**
  - Why needed here: Students must recognize their own cognitive processes to identify what to improve.
  - Quick check question: Can the student articulate what strategy they used the last time they got stuck debugging?

- **Concept: Comparative analysis reasoning**
  - Why needed here: Students need to isolate variables and attribute causation when comparing two sessions.
  - Quick check question: Given two scenarios differing in one variable, can the student identify what caused observed differences?

- **Concept: Technical video recording setup**
  - Why needed here: Students must successfully capture screen recordings without technical failures or privacy leaks.
  - Quick check question: Has the student recorded and played back a screen recording, including adjusting playback speed?

## Architecture Onboarding

- **Component map**:
  1. Recording phase: Screen capture → local video file (privacy guardrails: notifications off, edit sensitive content)
  2. Review phase: Video player with 2x speed capability → 4-column structured template (Question, Session 1, Session 2, Compare)
  3. Reflection phase: DEAL-aligned question sequence (Describe→Examine→Articulate)
  4. Submission phase: Written reflection with explicit length norms (3-5 bullets per question)

- **Critical path**:
  1. Adapt reflection questions to course context (categories: planning, writing, debugging, help-seeking, AI use)
  2. Provide recording instructions with privacy guidance
  3. Schedule two recording sessions with sufficient spacing
  4. Deploy scaffolded template with time guidelines
  5. Collect and review for themes

- **Design tradeoffs**:
  - Scaffold specificity vs. autonomy: Tighter scaffolding yields comparable responses but may limit authentic discovery
  - Video length vs. review burden: Longer sessions capture more context but increase review time
  - Privacy vs. authenticity: Allowing edits increases comfort but may reduce observational completeness

- **Failure signatures**:
  - Generic reflections without specific evidence cited from videos
  - Comparison column focuses only on "AI was faster" without examining trade-offs
  - No concrete goals in Articulate Learning phase
  - Students admit or imply they skipped video review

- **First 3 experiments**:
  1. Pilot the question template with 3-5 students to validate comprehensibility and time estimates before full deployment
  2. Run a single-session reflection (without comparison) to isolate whether video review alone produces metacognitive gains
  3. Deploy a 4-6 week follow-up survey to check whether students acted on their stated goals

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How must the DEAL-based video reflection scaffolding be adapted for novice computing students to ensure effective metacognitive insights?
- Basis in paper: [explicit] The authors state that "different or more scaffolding might be required particularly for more novice computing students" and explicitly suggest future work explore "lower level courses."
- Why unresolved: The study was conducted in an upper-level software engineering course where students had prior experience; it is unknown if novices possess the domain knowledge required to critically compare programming processes.
- What evidence would resolve it: A deployment of the assignment in an introductory CS course (CS1/CS2) with an analysis of reflection quality and student struggles.

### Open Question 2
- Question: To what extent do students retain and act upon the learning and goals generated during the reflection assignment in their long-term programming practice?
- Basis in paper: [explicit] The authors "recommend follow-up with students with a survey and interviews to see how well they retain and act on their learning and goals they set from reflection."
- Why unresolved: The current study analyzes reflection submissions immediately after the task, capturing student intent rather than longitudinal behavioral changes.
- What evidence would resolve it: Longitudinal data, such as follow-up interviews or analysis of subsequent coding sessions, showing sustained changes in AI use or help-seeking behavior.

### Open Question 3
- Question: Does integrating scaffolded video reflection earlier and repeatedly in a course result in better learning outcomes than a single end-of-term intervention?
- Basis in paper: [explicit] The authors "propose randomized quantitative evaluation of scaffolded comparative video reflection, such as during instead of just the end of a course."
- Why unresolved: This study utilized a single assignment at the end of the semester without a control group, making it difficult to quantify the specific impact on programming performance or skill acquisition.
- What evidence would resolve it: A randomized controlled trial comparing student performance and metacognitive awareness between groups receiving the intervention at different frequencies.

## Limitations
- The assignment was implemented in a single course with upper-level undergraduates in a team project context
- The DEAL framework's effectiveness in computing education remains untested beyond this single implementation
- There is no comparison group to determine whether video-based reflection produces greater metacognitive gains than traditional written reflection alone

## Confidence
- **High confidence**: Findings about students' reported learning regarding AI limitations and benefits
- **Medium confidence**: Claims about the DEAL framework's effectiveness as this represents the first known computing education application
- **Low confidence**: Claims about the assignment's impact on programming skills beyond AI use, as these improvements could be attributed to the project-based course itself

## Next Checks
1. Implement a control condition comparing video-based reflection against traditional written reflection in the same course to isolate the effect of video-stimulated recall on metacognitive awareness.

2. Conduct longitudinal follow-up with the same students 4-6 weeks post-assignment to determine whether they actually implemented their stated goals and sustained new practices in subsequent programming tasks.

3. Test the DEAL framework in multiple computing education contexts (different course levels, individual vs. team projects, different programming languages) to establish whether the sequential Describe→Examine→Articulate structure produces consistent benefits across varied implementations.