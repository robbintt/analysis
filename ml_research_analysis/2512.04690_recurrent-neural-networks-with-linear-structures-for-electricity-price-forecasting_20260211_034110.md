---
ver: rpa2
title: Recurrent Neural Networks with Linear Structures for Electricity Price Forecasting
arxiv_id: '2512.04690'
source_url: https://arxiv.org/abs/2512.04690
tags:
- forecasting
- electricity
- price
- linear
- prices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel recurrent neural network architecture
  specifically designed for day-ahead electricity price forecasting. The proposed
  model combines linear structures, such as expert models and Kalman filters, with
  recurrent networks to improve forecasting accuracy and interpretability.
---

# Recurrent Neural Networks with Linear Structures for Electricity Price Forecasting

## Quick Facts
- arXiv ID: 2512.04690
- Source URL: https://arxiv.org/abs/2512.04690
- Reference count: 22
- Primary result: Proposed model achieves ~11% RMSE improvement over state-of-the-art benchmarks for day-ahead electricity price forecasting

## Executive Summary
This paper introduces a novel recurrent neural network architecture that combines linear structures (expert models and Kalman filters) with non-linear networks for day-ahead electricity price forecasting. The model is designed to capture both linear relationships from fundamental drivers and non-linear patterns in electricity prices while maintaining interpretability. Tested on hourly data from the German electricity market (2018-2025), the proposed architecture achieves approximately 11% higher accuracy compared to high-dimensional linear models and neural network baselines.

## Method Summary
The architecture employs a three-branch parallel design: a Linear Expert Model (LEM) initialized with OLS estimates, a Kalman Filter (KF) branch using identity activation, and a non-linear RNN branch with ReLU activation. These branches are summed to produce the final 24-hour ahead forecast. The model is trained jointly using rolling window evaluation with Optuna hyperparameter optimization. Key inputs include load, wind, solar, fuel prices, and calendar effects, with training on 2018-2021 data and testing on 2022-2025.

## Key Results
- Achieves ~11% RMSE improvement over state-of-the-art benchmarks
- Demonstrates superior performance compared to both pure linear models and neural networks
- Shows that interpretable LEM coefficients remain stable after joint training
- Validates the effectiveness of combining linear and non-linear structures for electricity price forecasting

## Why This Works (Mechanism)

### Mechanism 1
Decomposing the forecasting task into explicit linear (structural) and non-linear (residual) paths allows the model to learn complex patterns without sacrificing interpretability or stability. The architecture employs skip connections to sum LEM and RNN outputs, with LEM capturing first-order economic relationships and RNN fitting non-linear residuals. This works under the assumption that electricity price formation involves both stable linear drivers and volatile non-linear interactions. The model may fail if relationships become purely non-linear or if the linear component is misspecified.

### Mechanism 2
A linear recurrent layer (identity activation) functions as a trainable Kalman Filter, smoothing temporal dynamics and capturing state-space trends more effectively than static linear models. By setting RNN activation to identity, the network mimics Kalman filter structure where hidden state evolves linearly, capturing smooth temporal dependencies. This assumes underlying temporal dynamics contain linear state components that can be modeled as dynamical systems. The approach may fail with purely chaotic or discontinuous time series lacking linear autocorrelation structure.

### Mechanism 3
Initializing the linear branch with OLS estimates (warm-starting) stabilizes the joint optimization process and preserves economic interpretability. Before gradient descent, LEM weights are set to closed-form OLS solution, providing sensible starting point in loss landscape and preventing RNN from ignoring interpretable branch. This assumes OLS solution provides reasonable approximation of linear component that gradient descent can fine-tune. The approach may fail if learning rate is too high or OLS initialization is severely misaligned with hybrid loss function's local optimum.

## Foundational Learning

- **Concept: Linear Expert Models (LEM)**
  - Why needed here: Serves as the "interpretable backbone" of the architecture, mapping physical fundamentals to price for decision-making interpretation
  - Quick check question: If the coefficient for `Wind_DA` is negative in the LEM branch, what does that imply about the merit-order effect in this market?

- **Concept: Elman Recurrent Neural Networks (RNN)**
  - Why needed here: Non-linear processing unit capturing temporal dependencies without complex gating
  - Quick check question: Why might a standard Elman RNN with ReLU activation be preferred over an LSTM for this specific short-term forecasting task?

- **Concept: State-Space Models & Kalman Filters**
  - Why needed here: Treats Kalman Filter as specific instance of linear RNN, distinguishing KF branch's smoothing function from RNN branch's non-linear pattern recognition
  - Quick check question: How does "identity activation" in the KF branch mathematically distinguish it from the non-linear RNN branch?

## Architecture Onboarding

- **Component map:**
  - Inputs: $X_t$ (Fundamentals: Load, Wind, Solar, Fuels, Calendar)
  - Branch 1 (LEM): Linear layer (OLS-initialized) → Direct prediction of baseline price
  - Branch 2 (KF): Linear Recurrent Layer (Identity activation) → Temporal smoothing/State estimation
  - Branch 3 (RNN): Non-linear Recurrent Layer (ReLU activation) → Complex pattern/Residual learning
  - Integration: Summation of three branch outputs → Final Price Forecast

- **Critical path:**
  1. Data Prep: Align hourly data and handle Daylight Saving Time (DST) to ensure temporal consistency
  2. Initialization: Calculate OLS weights for LEM branch and set them (do not randomize)
  3. Forward Pass: Pass inputs through all three branches in parallel
  4. Loss Calculation: Compute MSE on combined output; add L1/L2 regularization
  5. Backprop: Update all branches simultaneously (Joint Optimization)

- **Design tradeoffs:**
  - Interpretability vs. Accuracy: LEM branch highly interpretable (econometric coefficients), RNN branch is "black box" boosting accuracy
  - Stability vs. Complexity: Joint training computationally efficient compared to ensembles, requires careful hyperparameter tuning to prevent RNN from overfitting and ignoring LEM
  - Sequence Length: Paper finds optimal sequence length L=1, suggesting long-term memory is less critical than fundamental/feature richness

- **Failure signatures:**
  - Coefficient Drift: If LEM coefficients shift wildly or reverse sign after training, RNN may have overpowered linear branch, destroying interpretability
  - Exploding Gradients: Without gradient clipping (set to ≤5), recurrent branches may destabilize, especially with Identity activation in KF branch
  - Overfitting to Crisis: If validation only on high-volatility periods (e.g., 2022), hyperparameters may overfit to volatility and fail in stable periods

- **First 3 experiments:**
  1. Ablation Study: Run model three times: (1) LEM only, (2) RNN only, (3) LEM-RNN combined. Compare RMSE to verify "synergy" claim
  2. Coefficient Stability Check: Train hybrid model and extract LEM coefficients before (Pre-OLS) and after (Post-GD) training. Plot them to ensure they retain economic meaning
  3. Input Sensitivity: Remove "Fuel Prices" from input features and measure degradation in RMSE to quantify importance of exogenous market variables versus autoregressive history

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would replacing the linear summation of component forecasts with a learned, adaptive weighting mechanism improve forecasting accuracy?
- Basis in paper: [explicit] Authors state in Section 7: "rather than employing a linear summing of the component forecasts in the neural network output layer, the model could dynamically and adaptively ascertain the contribution of each branch, identify the most effective components based on forecast accuracy, and assign appropriate weights."
- Why unresolved: Current architecture uses fixed linear combination without mechanism to adjust branch contributions dynamically based on market conditions
- What evidence would resolve it: Implementing attention-based or gating mechanisms for branch weighting and comparing RMSE against baseline LEM-KF-RNN model

### Open Question 2
- Question: Can more advanced state-space models like Mamba provide better temporal dependency modeling than current Kalman filter implementation?
- Basis in paper: [explicit] Section 7 explicitly proposes "investigating more advanced state-space models, such as the Mamba model (Gu & Dao 2024), as this model allows more expressive and adaptive (dynamic) temporal dependencies."
- Why unresolved: KF branch implemented as deterministic linear state-space layer without full probabilistic covariance propagation
- What evidence would resolve it: Comparative study substituting KF branch with Mamba-style selective state-space layers and evaluating on same test period

### Open Question 3
- Question: Does LEM-KF-RNN architecture generalize to electricity markets with different structural characteristics (e.g., high renewable penetration, different regulatory frameworks)?
- Basis in paper: [explicit] Section 7 states methodology "could be further tested on other electricity markets with different characteristics (e.g., markets with high renewable penetration or different regulatory frameworks) to assess its generalisability."
- Why unresolved: All empirical results derive from single market (German-Luxembourg) over 2018-2025
- What evidence would resolve it: Applying identical architecture to diverse markets (e.g., Nordic, US PJM, Australian NEM) and comparing relative RMSE improvements

### Open Question 4
- Question: Would enforcing orthogonality among component forecast errors improve ensemble diversity and accuracy?
- Basis in paper: [inferred] Section 6.4.1 finds LEM, KF, and RNN error components are positively correlated with low correlation distances, indicating components "operate in a coherent system" rather than as independent experts
- Why unresolved: Paper notes this alignment but does not investigate whether decorrelation mechanisms could extract additional performance gains
- What evidence would resolve it: Adding regularization penalties to minimize inter-component error correlations and measuring impact on combined forecast RMSE

## Limitations
- Primary uncertainty in generalizability beyond German day-ahead market
- Limited testing on truly out-of-sample crisis periods
- Linear component may introduce bias if fundamental drivers differ significantly across markets
- Does not address computational overhead compared to simpler ensemble methods

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Architectural design and empirical RMSE improvement (~11%) | High |
| KF branch functioning as trainable Kalman Filter | Medium |
| Interpretable coefficients retaining economic meaning after joint training | Low |

## Next Checks
1. **Cross-Market Validation:** Apply trained model to electricity price data from different European market (e.g., France or Spain) to test generalizability of linear coefficients and overall forecasting performance
2. **Crisis Period Stress Test:** Evaluate model's performance during 2022 energy crisis, comparing accuracy and interpretability to purely non-linear model (e.g., LSTM) under extreme volatility
3. **Component Ablation Under Noise:** Systematically remove key input features (e.g., fuel prices, renewable generation) and introduce synthetic noise to assess model's robustness and relative importance of interpretable LEM branch in noisy environments