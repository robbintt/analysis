---
ver: rpa2
title: Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations
arxiv_id: '2510.02352'
source_url: https://arxiv.org/abs/2510.02352
tags:
- bias
- arxiv
- biases
- gender
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic evaluation of biases in
  spoken dialogue large language models (LLMs) with audio input and output. The authors
  introduce FairDialogue, a dataset with controlled paralinguistic attributes (gender,
  age, accent) for decision-making and recommendation tasks, synthesized using TTS
  systems.
---

# Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations

## Quick Facts
- **arXiv ID**: 2510.02352
- **Source URL**: https://arxiv.org/abs/2510.02352
- **Reference count**: 0
- **Primary result**: First systematic evaluation of bias in spoken dialogue LLMs with audio input/output using FairDialogue dataset and fairness metrics

## Executive Summary
This paper presents the first systematic evaluation of biases in spoken dialogue large language models (LLMs) with audio input and output. The authors introduce FairDialogue, a dataset with controlled paralinguistic attributes (gender, age, accent) for decision-making and recommendation tasks, synthesized using TTS systems. They propose two fairness metrics: Group Unfairness Score (GUS) for decision tasks and SNSR/SNSV for recommendation tasks. Experiments across open-source (Qwen2.5-Omni, GLM-4-Voice) and closed-source (GPT-4o Audio, Gemini-2.5-Flash) models reveal that closed-source models exhibit lower bias overall, while open-source models show higher sensitivity to age and gender. Multi-turn conversations can amplify or persist biases, with some groups requiring more corrective feedback to achieve fair outcomes. The study highlights the importance of fairness evaluation for audio-based interactive systems and provides resources for further research.

## Method Summary
The study evaluates bias in spoken dialogue LLMs using the FairDialogue dataset (~7200 samples, ~1700 minutes) with controlled paralinguistic attributes (gender, age, accent). Text prompts are generated using GPT-4o with paralinguistic constraints, then synthesized into audio using Index-TTS (for gender/age control) and ElevenLabs (for accent control). The evaluation pipeline runs inference on target SDMs with fixed settings (beam=1, sampling disabled), transcribes audio outputs with Whisper, and computes GUS for decision tasks and SNSR/SNSV for recommendations. Multi-turn evaluation involves applying corrective feedback over 4 additional turns after initial negative decisions, tracking revision success rate (RST) and average rounds to transform (ANR).

## Key Results
- Closed-source models (GPT-4o Audio, Gemini-2.5-Flash) exhibit lower overall bias (GUS 0.12-0.14) compared to open-source models (GUS 0.17-0.21)
- Multi-turn conversations can amplify or persist biases, with some demographic groups requiring more corrective feedback (higher ANR) to achieve fair outcomes
- Recommendation tasks tend to amplify cross-group disparities more than binary decision tasks, with accent-related bias being particularly pronounced

## Why This Works (Mechanism)

### Mechanism 1: Paralinguistic-to-Decision Leakage
- Claim: Audio encodings carry demographic signal that influences downstream decisions when not explicitly decoupled
- Mechanism: Speech encoders tokenize audio with implicit speaker characteristics; the LLM backbone conditions on these tokens, enabling correlations between paralinguistic attributes and output distributions
- Core assumption: Model weights do not separate task-relevant semantic content from speaker identity cues during inference
- Evidence anchors:
  - [abstract]: "Paralinguistic features, such as age, gender, and accent, can affect model outputs"
  - [section 3.3.1]: Open-source models show GUS 0.17–0.21 vs closed-source 0.12–0.14 on decision tasks
  - [corpus]: MULTI-Bench confirms multi-turn SDMs remain underexplored for bias, supporting the novelty of this leakage analysis
- Break condition: If audio encoders used adversarial disentanglement or speaker-invariant representations, the correlation would weaken or disappear

### Mechanism 2: Multi-Turn Feedback Asymmetry
- Claim: Corrective feedback reduces biased outputs unevenly across demographic groups
- Mechanism: Conversation history accumulates context tokens; models revise prior outputs based on negation tokens, but revision dynamics differ by group—likely due to priors encoded during pretraining
- Core assumption: The model's prior over "credible" or "correctable" user inputs is demographically conditioned
- Evidence anchors:
  - [abstract]: "some groups requiring more corrective feedback to achieve fair outcomes"
  - [section 3.3.2, Table 3]: Elder Male RST 88–95% vs Young Female 69–84%; Qwen2.5 needs more rounds for Elder Male (ANR 2.73 vs 2.63)
  - [corpus]: No direct corpus evidence for feedback asymmetry in SDMs; this is an emergent finding requiring replication
- Break condition: If feedback were uniformly applied via explicit state resets or instruction re-initialization, group-based asymmetry would likely diminish

### Mechanism 3: Task-Complexity Disparity Amplification
- Claim: Recommendation tasks exhibit higher cross-group variance than binary decision tasks due to ranking-space exposure
- Mechanism: Decision tasks collapse to binary outputs; recommendation tasks generate ranked lists with K items, exposing more surface area for preference divergence to manifest
- Core assumption: Training data underrepresents certain group-preference mappings, causing models to default to majority-group patterns
- Evidence anchors:
  - [abstract]: "recommendation tasks tend to amplify cross-group disparities"
  - [section 3.3.1]: SNSR for GLM recommendation tasks 0.66–0.68 vs decision GUS 0.14–0.21; accent-related bias larger in recommendations
  - [corpus]: "Unequal Opportunities" paper shows LLM geographical recommendations also exhibit bias under data imbalance
- Break condition: If recommendations were constrained to group-calibrated score thresholds rather than open ranking, disparity metrics would compress

## Foundational Learning

- Concept: **Group Unfairness Score (GUS)**
  - Why needed here: Quantifies decision-level bias as max disparity in positive outcome rates across attribute groups
  - Quick check question: If a model approves 80% of Group A applicants and 60% of Group B with identical qualifications, what is the minimum GUS?

- Concept: **PRAG*@K and SNSR/SNSV**
  - Why needed here: Captures ranking consistency between groups for recommendation tasks; variance reveals stability across group pairs
  - Quick check question: Two groups have PRAG*@10 = 0.7 and 0.4. Is SNSR = 0.3 or 0.35?

- Concept: **Paralinguistic Features (age, gender, accent)**
  - Why needed here: These are the controlled variables in FairDialogue; understanding their independence is critical for experimental design
  - Quick check question: Can you synthetically vary accent while holding prosody constant in your TTS pipeline?

## Architecture Onboarding

- Component map:
  - FairDialogue dataset -> TTS synthesis -> Inference on SDM -> Whisper transcription -> Metric computation
  - FairDialogue dataset (7200 samples, 1700 min audio) -> Index-TTS/ElevenLabs -> Qwen2.5-Omni, GLM-4-Voice, GPT-4o Audio, Gemini-2.5-Flash -> Whisper -> GUS/SNSR/SNSV

- Critical path:
  1. Generate neutral text prompts via GPT-4o with paralinguistic constraints
  2. Synthesize audio with controlled attribute variation
  3. Run inference on target SDM
  4. Transcribe outputs with Whisper
  5. Compute GUS (decision) or SNSR/SNSV (recommendation)

- Design tradeoffs:
  - Two TTS systems required to cover all attributes (Index-TTS lacks accent control; ElevenLabs lacks fine-grained age/gender editability)
  - ASR transcription adds noise; direct audio-to-audio evaluation would avoid this but lacks standard metrics
  - Multi-turn evaluation limited to decision tasks; recommendation multi-turn not analyzed

- Failure signatures:
  - High SNSV (>0.15) with low SNSR: inconsistency rather than systematic bias
  - RST >90% but ANR >3: model is compliant but inefficient, suggesting weak priors rather than strong bias
  - Accent GUS >0.15 in decision tasks: indicates accent leakage into semantic understanding

- First 3 experiments:
  1. **Baseline single-turn audit**: Run FairDialogue decision tasks through your SDM; compute GUS by attribute. If any GUS >0.20, flag for mitigation.
  2. **Multi-turn correction test**: Take samples with unanimous negative decisions; apply 4 rounds of negation feedback. Compute RST and ANR by group. If group ANR delta >0.5, investigate feedback asymmetry.
  3. **Recommendation disparity check**: Generate top-10 recommendations across groups for Occupation task. Compute SNSR. If SNSR >0.6, audit training data for occupation-gender or occupation-accent co-occurrence skew.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What bias mitigation techniques are effective for spoken dialogue LLMs, and can text-based debiasing methods transfer to the audio modality?
- Basis in paper: [explicit] The conclusion states: "Future research should investigate bias mitigation techniques... to support the responsible deployment of spoken dialogue LLMs."
- Why unresolved: This study focused solely on bias measurement and characterization; no mitigation strategies were proposed or evaluated
- What evidence would resolve it: Experiments applying debiasing methods (e.g., counterfactual data augmentation, reinforcement learning from fairness feedback) to spoken dialogue models, with measured reductions in GUS and SNSR/SNSV metrics

### Open Question 2
- Question: Do bias patterns observed with TTS-synthesized speech generalize to natural human speech with authentic paralinguistic variation?
- Basis in paper: [inferred] The FairDialogue dataset uses entirely synthesized speech from Index-TTS and ElevenLabs; natural speech may contain richer prosodic cues that models could exploit differently
- Why unresolved: TTS systems produce cleaner, more controlled audio than real-world speech, potentially underestimating or mischaracterizing bias pathways
- What evidence would resolve it: Comparative evaluation using recordings from human speakers across demographic groups, testing whether GUS/SNSR patterns replicate

### Open Question 3
- Question: How do biases evolve in extended multi-turn conversations beyond five total turns, and do certain groups face compounding disadvantage?
- Basis in paper: [inferred] Multi-turn experiments only examined four additional turns after initial negative decisions; the paper notes "biased decisions may persist" but does not explore longer interactions
- Why unresolved: Real-world voice assistants often engage in much longer dialogues; cumulative bias effects remain unknown
- What evidence would resolve it: Longitudinal experiments tracking revision success rates and fairness metrics across 10-50 turn conversations for each demographic group

### Open Question 4
- Question: To what extent do biases generalize across languages and cultural contexts beyond English?
- Basis in paper: [inferred] All experiments used English utterances and accents (US, UK, India, Australia, African); no non-English languages were evaluated
- Why unresolved: Bias manifestations may differ substantially in languages with different social markers or grammatical gender systems
- What evidence would resolve it: Multilingual FairDialogue equivalents with native speakers/TTS in languages such as Mandarin, Spanish, or Arabic, comparing cross-linguistic GUS and SNSR patterns

## Limitations

- The study relies entirely on synthetic speech data, which may not capture the full complexity of natural human speech patterns and could introduce artifacts that affect model behavior
- The evaluation pipeline uses Whisper for transcription, adding an additional layer of potential error that could compound or mask the original biases
- The dataset construction relies on GPT-4o for prompt generation, creating potential circularity where biases present in the prompting LLM could influence evaluation outcomes

## Confidence

- **High Confidence**: The finding that closed-source models exhibit lower overall bias than open-source models (GUS 0.12-0.14 vs 0.17-0.21) is well-supported by consistent experimental results across multiple decision tasks and attribute groups
- **Medium Confidence**: The multi-turn feedback asymmetry results show promising patterns (e.g., Elder Male requiring more rounds than Young Female) but are based on a limited set of 4 corrective turns and may not generalize to longer conversations or different feedback strategies
- **Low Confidence**: The claim that recommendation tasks amplify cross-group disparities more than decision tasks is partially supported but requires additional validation, as the study only evaluates SNSR/SNSV at K=10 and doesn't explore how these metrics behave across different ranking depths or task complexities

## Next Checks

1. **Cross-validation with human speech**: Replicate the core bias experiments using real human speech recordings with controlled demographic attributes to verify whether synthetic speech artifacts are influencing the observed patterns
2. **ASR-independent evaluation**: Implement direct audio-to-audio evaluation where possible, comparing results against the Whisper-based pipeline to quantify the impact of transcription errors on bias measurements
3. **Extended multi-turn analysis**: Expand the multi-turn evaluation to include 8-10 corrective feedback rounds and test different feedback strategies (e.g., explicit instructions vs. example-based correction) to better understand the persistence and amplification of biases in extended conversations