---
ver: rpa2
title: A Second-Order Perspective on Pruning at Initialization and Knowledge Transfer
arxiv_id: '2509.24066'
source_url: https://arxiv.org/abs/2509.24066
tags:
- pruning
- task
- transfer
- loss
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study explores pruning-at-initialization (PaI) on pre-trained
  vision models to compress models before fine-tuning, without requiring task-specific
  data. It investigates whether pruning on one task retains performance on unseen
  tasks and how data influences pruning.
---

# A Second-Order Perspective on Pruning at Initialization and Knowledge Transfer

## Quick Facts
- arXiv ID: 2509.24066
- Source URL: https://arxiv.org/abs/2509.24066
- Reference count: 36
- Primary result: Pruning pre-trained vision models on one task preserves zero-shot transfer to unseen tasks, with Hessian-based methods (block-diagonal for ResNet, diagonal for ViT) outperforming others.

## Executive Summary
This study investigates pruning-at-initialization (PaI) on pre-trained vision models to compress models before fine-tuning, without requiring task-specific data. It explores whether pruning on one task retains performance on unseen tasks and how data influences pruning decisions. The core method involves applying PaI techniques—SNIP, GraSP, PX, and Hessian-based approximations (isotropic, diagonal, block-diagonal)—on pre-trained models like ResNet-50 and ViT-B/16, then evaluating feature quality via linear classifiers. Results show that pruning on one task preserves zero-shot performance on unseen tasks, and fine-tuning further recovers performance. Hessian-based scores (especially block-diagonal for ResNet and diagonal for ViT) outperform others, suggesting task-aligned loss landscapes. The study confirms that extensive pre-training enables effective compression with minimal data, supporting robust transfer learning.

## Method Summary
The study applies pruning-at-initialization to pre-trained vision models, computing saliency scores using only source task data. It evaluates four pruning methods: magnitude pruning (magnitude of weights), SNIP (connection sensitivity), GraSP (gradient signal preservation), and Hessian-based approximations (isotropic, diagonal, block-diagonal). The block-diagonal approximation uses Kronecker factorization for tractability. Models are pruned at specified sparsity levels, then fine-tuned on source task data using SGD with momentum. Feature quality is assessed using frozen linear classifiers trained via ridge regression on all 10 tasks. The evaluation tests zero-shot transfer performance and fine-tuning recovery across different architectures (ResNet-50, ViT-B/16) and pre-training methods (MoCoV2, DINO, CLIP).

## Key Results
- Pruning on source task data preserves zero-shot transfer performance to held-out tasks across all sparsity levels tested
- Fine-tuning on source task never hurts and often improves transfer performance, recovering any lost accuracy
- Block-diagonal Hessian approximations work best for ResNet-50, while diagonal approximations work best for ViT-B/16
- PX pruning is ineffective for ViT due to mismatch with neural tangent kernel regime assumptions

## Why This Works (Mechanism)

### Mechanism 1: Cross-Task Loss Landscape Alignment from Large-Scale Pre-training
Pruning on source task data preserves zero-shot transfer because extensive pre-training on large-scale datasets creates favorable loss landscapes that align across tasks. When source and transfer tasks share similar data support to pre-training, their Hessian curvature structures align, making parameters unimportant for one task similarly unimportant for others. This alignment fails when transfer tasks have labeling functions orthogonal to pre-training distribution.

### Mechanism 2: Hessian Approximation Fidelity Determines Pruning Quality
Block-diagonal Hessian approximations more accurately identify truly redundant weights than diagonal or isotropic approximations by capturing within-layer interactions via Kronecker structure. The optimal pruning score requires full Hessian inversion, but approximations trade accuracy for tractability. ResNet benefits from block-diagonal capturing cross-parameter curvature, while ViT performs better with diagonal due to architecture-specific curvature structures.

### Mechanism 3: Source-Only Fine-tuning Recovers Transfer Performance via Riemann Manifold Proximity
Fine-tuning pruned models on source task data improves or maintains transfer task performance because the fine-tuning trajectory follows gradient descent in the shared convex basin, moving closer to the transfer task's optimum in the Riemann manifold induced by the aligned Hessian. The second-order Taylor approximation holds locally, and the pre-trained point is near-optimal for both tasks.

## Foundational Learning

- **Second-Order Taylor Expansion for Loss Approximation**
  - Why needed: The entire theoretical framework rests on approximating L(D,θ) ≈ L(D,θ₀) + (θ-θ₀)ᵀH(θ-θ₀) to derive optimal pruning scores and weight updates
  - Quick check: If the Hessian H has large off-diagonal entries, which approximation (isotropic, diagonal, block-diagonal) would be most accurate and why?

- **Ridge Regression for Linear Probing**
  - Why needed: The evaluation protocol uses frozen linear classifiers trained via ridge regression to isolate feature quality degradation from classifier retraining effects
  - Quick check: Why add regularization term α∥ξ∥²₂ instead of solving ordinary least squares Φξ = Y?

- **Kronecker-Factored Approximate Curvature (K-FAC)**
  - Why needed: Block-diagonal Hessian approximation uses Kronecker product structure H[l] ≈ A[l] ⊗ B[l] to reduce memory requirements from O(p²l) to O(m²l + h²l), enabling practical computation
  - Quick check: For a layer with 512 input features and 256 output features, compare memory requirements for full Hessian vs. Kronecker-factored approximation

## Architecture Onboarding

- Component map: Pre-trained Encoder (W₀) -> Pruning Score Computation (S(θ₀[j])) -> Mask Generation (top-k selection) -> Pruned Encoder (W₀ ⊙ c) -> Fine-tuning on Source (SGD on Ds) -> Evaluation: Frozen Linear Probes (ξ*s, ξ*t)

- Critical path: 1) Select appropriate Hessian approximation based on architecture (magnitude for ResNet, diagonal/block for ViT) 2) Compute saliency scores using only source task data Ds 3) Generate binary mask c keeping top-k weights 4) Fine-tune remaining weights on Ds using MSE loss 5) Evaluate on held-out tasks Dt using pre-computed frozen classifiers

- Design tradeoffs: Magnitude pruning (O(p) time, works well for ResNets, fails for ViTs) vs Diagonal Hessian (O(p) time, captures per-parameter curvature, architecture-dependent effectiveness) vs Block Hessian (O(m³l + h³l) per layer, most accurate, best overall performance). Sparsity vs transfer: Higher sparsity (>75%) consistently degrades transfer more than source.

- Failure signatures: ViT with magnitude pruning shows sharp transfer accuracy drops at moderate sparsity. Diagonal Hessian on ResNet prunes important directions, creating unfavorable loss landscapes. Cross-task landscape misalignment causes fine-tuning to fail recovering transfer performance.

- First 3 experiments: 1) Baseline probe training: Fit frozen linear classifiers ξ*k on unpruned model for all 10 tasks using ridge regression (α=1) 2) Architecture-appropriate pruning test: Apply magnitude pruning to ResNet-50/ImageNet-1k at 66% sparsity using Task 1 as source; evaluate zero-shot accuracy on Tasks 2-10 3) Fine-tuning recovery validation: Take pruned model from experiment 2, fine-tune on Task 1 for 90 epochs, then re-evaluate on all tasks

## Open Questions the Paper Calls Out

- Can formal theoretical guarantees be derived to define safe pruning thresholds that preserve transferability? The conclusion explicitly states that "theoretical guarantees for safe pruning thresholds remain unexplored" and calls for linking perturbations to loss landscape geometry. A theoretical derivation quantifying maximum allowable perturbation based on local curvature would resolve this.

- Does the observed robustness of Pruning-at-Initialization (PaI) persist under dynamic or heterogeneous deployment conditions? The authors note their controlled setup omits real-world complexities and suggest future research should test "PaI under dynamic or heterogeneous conditions." Experiments evaluating transfer performance in continual learning settings or on non-i.i.d. data streams would resolve this.

- Do the cross-task transferability findings generalize to structured pruning methods that enforce hardware-friendly sparsity? The methodology explicitly restricts the investigation to "unstructured neural network pruning," leaving the impact of structured removal unknown. A comparative study showing structured pruning masks derived from source task maintain zero-shot transfer performance would resolve this.

## Limitations

- Cross-task alignment assumes downstream tasks resemble pre-training data; performance degrades when tasks are orthogonal to pre-training distribution
- Hessian approximations trade accuracy for tractability; block-diagonal assumes layer-wise parameter independence may miss cross-layer interactions
- The study focuses on vision tasks; applicability to NLP or multimodal domains requires validation

## Confidence

- High confidence: Cross-task zero-shot transfer preservation - directly validated across 10 tasks and multiple architectures
- High confidence: Fine-tuning on source recovers transfer performance - empirically observed across all experiments
- Medium confidence: Hessian approximation superiority - architecture-dependent effectiveness suggests need for task-aware selection
- Low confidence: Theoretical landscape alignment mechanism - while consistent with observations, direct landscape visualization would strengthen claims

## Next Checks

1. **Orthogonal task validation**: Test pruning transfer between tasks with labeling functions orthogonal to pre-training (e.g., artistic style classification on ImageNet-pretrained models) to verify landscape alignment assumptions

2. **Architecture-specific curvature analysis**: Systematically compare magnitude, diagonal, and block-diagonal effectiveness across diverse architectures (ConvNeXt, Swin Transformer) to identify universal vs architecture-specific patterns

3. **Multi-task pruning transfer**: Extend from single-source to multi-source pruning (prune on combinations of tasks) to test whether richer pre-training signals improve cross-task alignment and transfer robustness