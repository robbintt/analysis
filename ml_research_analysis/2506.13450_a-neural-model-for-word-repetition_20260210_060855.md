---
ver: rpa2
title: A Neural Model for Word Repetition
arxiv_id: '2506.13450'
source_url: https://arxiv.org/abs/2506.13450
tags:
- word
- errors
- words
- neural
- effect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study bridges cognitive and neural models of word repetition
  by training deep encoder-decoder neural networks on the full English lexicon. The
  trained model successfully mimics human-like processing effects: length sensitivity,
  primacy/recency patterns, and adherence to the sonority sequencing principle.'
---

# A Neural Model for Word Repetition

## Quick Facts
- arXiv ID: 2506.13450
- Source URL: https://arxiv.org/abs/2506.13450
- Reference count: 19
- This study trains neural encoder-decoder networks on the full English lexicon to model human word repetition behavior

## Executive Summary
This study bridges cognitive and neural models of word repetition by training deep encoder-decoder neural networks on the full English lexicon. The trained model successfully mimics human-like processing effects including length sensitivity, primacy/recency patterns, and adherence to the sonority sequencing principle. Single-phoneme representations show structured organization into vowels and consonants, with manner-of-articulation features most predictive of neural distances. While the model captures several human behavioral patterns, it does not fully replicate dual-route processing, suggesting lexical and sublexical processes may be more entangled than distinct.

## Method Summary
The researchers trained LSTM encoder-decoder models on 30,000 most frequent English words from WordFreq with phonetic transcriptions from CMU dictionary. The model used shared input/output embeddings, batch size 2048, dropout=0, learning rate 0.001, and Adam optimizer for 100 epochs with early stopping at epoch 75. Training involved 10 million frequency-weighted samples. The model was evaluated on a factorial dataset with 12 conditions plus sonority sequences, testing six behavioral effects: lexicality, frequency, length, morphological complexity, primacy/recency, and sonority gradient.

## Key Results
- Model achieves zero error rate on training set and exhibits length sensitivity with primacy/recency patterns
- Single-unit ablations primarily affect sublexical (pseudoword) processing rather than lexical processing
- Phoneme representations self-organize by linguistic features (especially manner of articulation) from distributional statistics alone

## Why This Works (Mechanism)

### Mechanism 1: Emergent Working Memory via Recurrent Decay
The model simulates working memory constraints through natural dynamics of recurrent processing limits. As the LSTM encoder processes sequential phonemes, the hidden state must maintain sequence history. Longer sequences increase load on fixed-dimensionality state, leading to higher error rates for later or middle items, mimicking phonological output buffer capacity limitations.

### Mechanism 2: Differential Vulnerability to Sublexical Processing
Ablating single neurons disproportionately impacts pseudoword repetition compared to real words. Real words are processed via optimized lookup paths strengthened by frequency-weighted training, making them robust to minor neuron loss. Pseudowords require online assembly via sublexical rules, which relies on distributed features making this process more fragile to ablation.

### Mechanism 3: Statistical Self-Organization of Phonetic Features
The network spontaneously organizes internal representations according to linguistic features (e.g., manner of articulation) purely from distributional statistics. Since certain phonemes co-occur in similar contexts more frequently than others, embedding layers and hidden states naturally cluster these tokens to minimize prediction error, reflecting the Sonority Sequencing Principle.

## Foundational Learning

- **Concept: Encoder-Decoder (Seq2Seq) Architecture**
  - Why needed: Structural backbone where Encoder compresses auditory sequence into hidden state and Decoder unrolls it into production
  - Quick check: Where might the "Phonological Output Buffer" cognitively reside within the Encoder-Decoder flow?

- **Concept: Cognitive Dual-Route Model (Lexical vs. Sublexical)**
  - Why needed: Entire behavioral evaluation derived from hypothesis that humans have two processing paths
  - Quick check: If a patient has damage to the "lexical route," should they perform better or worse on "boot" (real word) vs. "quab" (pseudoword)?

- **Concept: Ablation Studies**
  - Why needed: Primary method to link artificial network to clinical "brain damage" profiles
  - Quick check: Does ablating a neuron completely erase a word from memory, or distort processing capacity?

## Architecture Onboarding

- **Component map:** Input Phoneme Sequence -> Embedding Layer -> Encoder LSTM -> Context Vector -> Decoder LSTM -> Softmax Output
- **Critical path:** Input Processing -> Transfer final Encoder hidden state to Decoder -> Auto-regressive Generation until EOS
- **Design tradeoffs:** RNNs chosen over Transformers for biological plausibility (sequential processing) rather than pure performance; Vocabulary size 30k required larger hidden states (128 vs 64) to achieve zero error
- **Failure signatures:** Premature Termination (ablating critical units causes EOS emission too early), Serial Position Loss (errors cluster in middle if hidden state capacity overwhelmed)
- **First 3 experiments:** 1) Baseline Training on 30k lexicon to verify 0% error rate, 2) Behavioral Probe on factorial dataset to plot Length Effect and confirm real word vs pseudoword discrimination, 3) Targeted Ablation to locate high-impact unit and verify premature stopping behavior

## Open Questions the Paper Calls Out

### Open Question 1
Can dual-route processing with clear double dissociation be induced in neural models of word repetition? The authors note future research should investigate whether dual-route processing can be more explicitly induced by incorporating working-memory dynamics or adding architectural constraints inspired by human neuroanatomy. This remains unresolved because single-unit ablation studies showed only a single dissociation rather than the double dissociation pattern seen in human patients.

### Open Question 2
Why does the model fail to exhibit a morphological-complexity effect despite being trained on a large lexicon with multi-morphemic words? The cognitive model predicts morphemes are stored as discrete units reducing effective word length, but the neural model did not show this expected effect despite learning from training statistics.

### Open Question 3
What are the neural mechanisms underlying dramatic length-related errors caused by ablating specific units? The authors document that ablating Unit 49 causes error rates up to 80% with premature sequence termination, but note no pattern was easily identifiable for why this specific unit controls sequence completion.

## Limitations

- Model does not fully capture dual-route processing characteristic of human word repetition, showing only single dissociation
- Ablation study methodology provides correlational rather than causal evidence linking neural damage to cognitive processing deficits
- The mechanism by which single units control global sequence production behaviors during training remains unexplained

## Confidence

**High Confidence:**
- Model successfully trains to zero error on 30K English words using specified LSTM architecture
- Model exhibits length sensitivity and position-based errors consistent with working memory constraints
- Phoneme representations show clustering by manner of articulation from distributional statistics alone

**Medium Confidence:**
- Mapping between working memory effects and recurrent dynamics is plausible but not directly measured
- Claim that single dissociation implies route entanglement is reasonable but requires additional evidence
- Emergence of sonority sequencing reflects underlying feature structure, though alternative explanations exist

**Low Confidence:**
- Model captures full complexity of human word repetition behavior
- Findings generalize to other linguistic tasks or architectures
- Representational structure directly maps to cognitive feature systems

## Next Checks

1. **Quantify Behavioral Coverage Gap**: Systematically compare model's performance across all 12 factorial conditions against human data to determine which specific effects remain unexplained and measure magnitude of discrepancies.

2. **Alternative Architecture Test**: Train a Transformer model on identical data with context window constraints to test whether length and primacy/recency effects require recurrence or could emerge from other architectural constraints.

3. **Feature Ablation Analysis**: Beyond single-unit ablation, perform targeted feature-level ablation (e.g., remove all manner-of-articulation information) to test whether observed representational structure is necessary for behavioral performance.