---
ver: rpa2
title: LVLMs and Humans Ground Differently in Referential Communication
arxiv_id: '2601.19792'
source_url: https://arxiv.org/abs/2601.19792
tags:
- human
- matcher
- basket
- round
- rounds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated whether large vision-language models (LVLMs)
  can engage in referential communication by collaborating with humans or other LVLMs
  to identify and order baskets through multi-turn dialogue. A factorial design tested
  all combinations of human/AI director-matcher pairs across four rounds.
---

# LVLMs and Humans Ground Differently in Referential Communication

## Quick Facts
- arXiv ID: 2601.19792
- Source URL: https://arxiv.org/abs/2601.19792
- Reference count: 29
- Large vision-language models (LVLMs) fail to form conceptual pacts and exploit common ground in referential communication tasks, unlike humans.

## Executive Summary
This study investigated whether large vision-language models (LVLMs) can engage in referential communication by collaborating with humans or other LVLMs to identify and order baskets through multi-turn dialogue. A factorial design tested all combinations of human/AI director-matcher pairs across four rounds. Results showed that only human-human pairs achieved high accuracy and improved efficiency over rounds by forming compact referring expressions, while AI-AI and mixed pairs showed declining accuracy, persistent verbosity, and failure to form conceptual pacts. LVLMs did not adapt their communication or exploit common ground, regardless of partner type. Human participants could identify AI partners, suggesting LVLMs fail to model grounding—a key skill underlying effective human language use.

## Method Summary
The experiment used a referential communication task where director-matcher pairs collaborated to identify and order 12 baskets across four rounds via text chat. A factorial 2×2 design tested human/AI director × human/AI matcher combinations (HH: 32 pairs; HA: 17; AH: 22; AA: 18). GPT-5.2 with "none" reasoning served as AI participants, interacting through a web-based oTree platform with real-time chat. Both partners viewed basket images (director: 12 targets; matcher: 12 targets + 4 distractors), and the director described baskets for the matcher to identify and place in sequence. Communication was structured using JSON outputs with zero-shot chain-of-thought prompting.

## Key Results
- Human-human pairs achieved 94% accuracy in later rounds with decreasing word/turn counts and lexical compression.
- AI-AI and mixed pairs showed declining accuracy (AI-Human: 85%→~65%) and persistent verbosity across rounds.
- AI pairs failed to form conceptual pacts, showing high lexical overlap without compression (repetition, not entrainment).
- Human participants could reliably identify AI partners (77% vs. 12% perceived human-likeness).

## Why This Works (Mechanism)

### Mechanism 1: Conceptual Pact Formation
- Human interlocutors establish shared referential representations through collaborative negotiation, enabling more efficient communication over repeated interactions. Partners propose referring expressions → matcher accepts/modifies → both ratify a "conceptual pact" that becomes part of their common ground. This shared representation reduces subsequent cognitive load and enables lexical compression.

### Mechanism 2: Grounding Criterion Optimization
- Humans dynamically adjust communication effort based on an implicit cost-benefit analysis of grounding depth. Partners seek/provide evidence of understanding until a "grounding criterion" sufficient for current purposes is met. In high-stakes or novel contexts, more evidence is required; with established common ground, less effort suffices.

### Mechanism 3: LVLM Common Ground Blindness
- LVLMs process dialogue history as context but do not form stable, exploitable representations of shared knowledge. The model generates responses conditioned on visible history tokens but lacks an architecture that maintains stateful beliefs about mutual knowledge. High lexical overlap in AI-AI dialogues reflects template-like repetition, not adaptive compression.

## Foundational Learning

- **Concept: Common Ground**
  - Why needed here: The core theoretical construct explaining why human communication becomes more efficient over time—and what LVLMs lack.
  - Quick check question: Can you explain why two strangers discussing the same objects use longer descriptions than partners who have discussed those objects before?

- **Concept: Lexical Entrainment**
  - Why needed here: The measurable behavioral signature of successful grounding; operationalized as overlap + compression of referring expressions.
  - Quick check question: If you observe two partners using the same unusual phrase to describe an object by round 3, what cognitive process has occurred?

- **Concept: Director-Matcher Paradigm**
  - Why needed here: The experimental structure that creates asymmetric information and forces collaborative grounding.
  - Quick check question: In this paradigm, which partner has more initiative, and why does that matter for evaluating AI collaboration?

## Architecture Onboarding

- **Component map:**
  - oTree platform -> GPT-5.2 API -> JSON-structured prompt system -> Director/Matcher interface

- **Critical path:**
  1. Both partners view basket images (director: 12 targets; matcher: 12 targets + 4 distractors)
  2. Director describes one basket → Matcher asks clarifying questions or selects
  3. Matcher places basket in sequence
  4. Repeat for all 12 baskets across 4 rounds
  5. Evaluate: accuracy, word/turn counts, lexical overlap

- **Design tradeoffs:**
  - JSON-structured output enforces state tracking but may constrain natural dialogue
  - "None" reasoning level prioritizes responsiveness over deliberation
  - Pragmatic prompting (conciseness, turn-taking) explicitly encodes norms humans acquire implicitly
  - Trade-off: more scaffolding = more control but less naturalistic failure modes

- **Failure signatures:**
  - Accuracy decline over rounds (especially AI-Human: 85%→~65%)
  - Flat or increasing word/turn counts across rounds
  - High lexical overlap without compression (verbose repetition, not entrainment)
  - Excessive confirmations after every turn
  - Director "hallucinating" wrong basket descriptions in later rounds

- **First 3 experiments:**
  1. Baseline replication: Run Human-Human and AI-AI conditions with the provided pipeline to confirm the accuracy/efficiency gap.
  2. Ablate pragmatic prompting: Remove the communication norms from the system prompt; measure whether verbosity increases and accuracy drops further.
  3. Partner-type detection: Present human raters with anonymized transcripts; test whether they can classify partner type above chance (the paper reports 77% vs. 12% perceived human-likeness).

## Open Questions the Paper Calls Out

- How do dialogue act sequences and repair strategies differ between human and AI dyads in multi-turn grounding tasks? The authors intend to perform deeper analyses of the transcripts, for example using dialogue act analysis and analyzing attempts at repairs.

- Do open-weight LVLMs exhibit the same inability to ground as proprietary frontier models like GPT-5.2? The paper notes that open-weight models were not considered and suggests this may affect reproducibility of results.

- Can interactive post-training or fine-tuning methods successfully induce lexical entrainment where prompt engineering fails? The paper concludes that prompt engineering is not likely to be the solution and suggests learning-based adaptation is a necessary next step.

## Limitations
- Exact basket images used are only visible as thumbnails in Figure 3; visual details matter for distinguishing similar baskets.
- GPT-5.2 API details and reasoning effort configuration are unspecified despite being central to the study.
- Image encoding format for visual context wrapper is not specified, making it unclear how composite grids were constructed and passed to LVLMs.

## Confidence
- High confidence in the core behavioral finding: human-human pairs showed sustained accuracy and communicative efficiency gains, while AI-involved pairs did not.
- Medium confidence in the theoretical interpretation that LVLMs lack common ground awareness, since the paper's evidence is correlational.
- Low confidence in the generalizability of the results beyond the specific basket stimuli and experimental setup, given the lack of detail on stimulus design and model configuration.

## Next Checks
1. Recreate the basket stimulus set from Figure 3 and rerun the experiment with GPT-5.2 (or similar model) using the provided prompts; verify that accuracy declines and verbosity persists for AI-AI and mixed pairs.
2. Conduct a post-hoc human rater study on anonymized transcripts to confirm that humans can reliably distinguish AI from human partners (as reported: 77% vs. 12% perceived human-likeness).
3. Systematically ablate pragmatic prompting from the AI system prompt; measure whether this increases verbosity and further reduces accuracy, isolating the effect of explicit communication norms.