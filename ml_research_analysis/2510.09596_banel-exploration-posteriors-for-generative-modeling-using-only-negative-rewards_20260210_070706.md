---
ver: rpa2
title: 'BaNEL: Exploration Posteriors for Generative Modeling Using Only Negative
  Rewards'
arxiv_id: '2510.09596'
source_url: https://arxiv.org/abs/2510.09596
tags:
- reward
- success
- rate
- banel
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of post-training generative models
  when the initial model achieves near-zero success rates and reward evaluations are
  expensive. This setting requires learning from exclusively negative (failure) samples
  while minimizing reward function evaluations (NREs).
---

# BaNEL: Exploration Posteriors for Generative Modeling Using Only Negative Rewards

## Quick Facts
- **arXiv ID:** 2510.09596
- **Source URL:** https://arxiv.org/abs/2510.09596
- **Reference count:** 40
- **Primary result:** Achieves 13,500× improvement on MNIST digit generation (8e-26 to 1.08e-21 success rate) using only negative samples

## Executive Summary
The paper addresses post-training generative models in extremely sparse reward settings where initial success rates are near-zero and reward evaluations are expensive. BaNEL (Bayesian Negative Evidence Learning) trains a separate negative model on failure samples to identify unsuccessful patterns, then uses Bayesian updates to steer the main model away from likely failures. The approach is evaluated on three tasks: MNIST digit generation, adversarial attacks on a math problem solver, and GSM8K reasoning tasks. Results show dramatic improvements in success rates while using only negative-reward samples, with performance scaling with computation.

## Method Summary
BaNEL operates by first sampling examples from the current generative model, then training a separate negative model (p_φ) on all sampled examples using maximum likelihood estimation. A rejection region is formed by computing the likelihood ratio p_θ(x)/p_φ(x) with length normalization, and the model is distilled on samples that pass this filter. This process repeats for multiple rounds with a fixed budget of reward function evaluations. The key innovation is using the negative model to identify regions of failure without requiring any positive samples, enabling exploration in settings where traditional policy gradient methods fail catastrophically.

## Key Results
- MNIST 0→6 generation: Improves success rate from 8e-26 to 1.08e-21 (13,500× improvement)
- Adversarial attack on math solver: Success rate increases from 0.0004 to 0.1112 (278× improvement)
- GSM8K-Hard: Outperforms RND baselines on 4 of 6 problems
- Performance scales with computation, discovering failure modes that enable targeted attacks

## Why This Works (Mechanism)
BaNEL works by constructing an implicit exploration distribution through rejection sampling based on the negative model. The negative model learns to identify patterns associated with failure, creating a rejection region that the main model is steered away from. This Bayesian updating mechanism allows the model to explore the space efficiently without requiring any positive samples, which is critical in settings where success is extremely rare and reward evaluations are expensive.

## Foundational Learning
- **Rejection sampling**: Used to filter out likely failures based on the negative model's predictions; needed to create exploration distribution without positive samples
- **Likelihood ratio computation**: Core mechanism for determining which samples to keep; requires careful normalization for variable-length sequences
- **Maximum likelihood training on failures**: The negative model learns failure patterns through standard supervised training on negative samples
- **Bayesian posterior updates**: Steering mechanism that updates the main model's distribution based on rejection region; enables exploration without explicit rewards
- **Autoregressive transformers for sequence generation**: Used for MNIST digit generation; requires understanding of sequence modeling and length normalization
- **Distillation from filtered posterior**: Transfers knowledge from exploration back to the main model; critical for maintaining diversity

## Architecture Onboarding

**Component Map:**
Base Generator -> Negative Model -> Rejection Region -> Filtered Samples -> Distilled Generator

**Critical Path:**
1. Sample m examples from current generator
2. Train negative model on all samples (MLE)
3. Compute likelihood ratios with length normalization
4. Keep top m/f samples passing threshold
5. Distill filtered samples back to generator

**Design Tradeoffs:**
- Separate negative model avoids coupling issues but increases computational cost
- Fixed NRE budget balances exploration vs. evaluation expense
- Length normalization prevents bias toward shorter sequences
- Resetting to base model each round prevents catastrophic forgetting

**Failure Signatures:**
- Model collapse if negative model trained insufficiently (epochs <50 for MNIST)
- Success rate peaks then declines due to distillation errors
- Length bias in likelihood ratios causing poor sample selection

**First Experiments:**
1. Implement MNIST 0→6 task with exact transformer architecture and verify 13,500× improvement
2. Test BaNEL's sensitivity to negative model training epochs on sample diversity
3. Analyze likelihood ratio computation with and without length normalization

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** Can Low-Rank Adaptation (LoRA) efficiently parameterize the negative model $p_\phi$ without inducing harmful coupling with the generator?
- **Basis in paper:** [explicit] Section 6 notes that maintaining a separate model is expensive and suggests LoRA as a way to mitigate costs while avoiding the coupling issues found in prompt-conditioned approaches
- **Why unresolved:** The authors attempted negative prompting but found it altered the original policy; it is unclear if LoRA avoids this specific failure mode
- **What evidence would resolve it:** Experiments comparing the success rates and stability of a LoRA-adapted shared model against the current separate-model architecture

**Open Question 2**
- **Question:** Does a decaying update schedule prevent the decline in success rates observed during extended training?
- **Basis in paper:** [explicit] Section 6 attributes the non-monotonic success rate (peaking and then declining) to the accumulation of distillation errors and misclassification in the rejection region
- **Why unresolved:** The authors suggest a decaying schedule as a remedy but do not implement or validate this mechanism
- **What evidence would resolve it:** Empirical results showing sustained or monotonic improvement in success rates when a decay factor is applied to the Bayesian posterior updates

**Open Question 3**
- **Question:** How can the optimal stopping point be identified in a truly zero-reward setting?
- **Basis in paper:** [explicit] Section 6 states that "success rate cannot be reliably estimated until we discover high-reward samples, making it difficult to determine when training should be stopped"
- **Why unresolved:** Without positive samples, standard validation metrics fail, leaving the model vulnerable to performance collapse before success is detected
- **What evidence would resolve it:** A proxy metric for exploration saturation that correlates with the peak success rate without requiring explicit positive reward evaluations

## Limitations
- Reliance on expensive reward oracle calls limits practical applicability
- Potential for model collapse during training if negative model is undertrained
- GSM8K results show modest improvement with only 4 of 6 problems outperforming baselines
- Success rate cannot be reliably estimated until positive samples are discovered, making early stopping difficult

## Confidence
- **MNIST results:** High confidence - well-specified task with clear metrics and dramatic improvement
- **Adversarial attack results:** Medium confidence - practical significance but architecture details unclear
- **GSM8K results:** Low confidence - limited sample size (6 problems) and modest overall performance

## Next Checks
1. Implement MNIST 0→6 task with the exact autoregressive transformer architecture (layers, heads, hidden dim) and verify success rate improvement from 8e-26 to 1.08e-21 using the specified 30-round budget with m=250 samples per round
2. Reproduce the adversarial attack results on the math problem solver, confirming the 278× improvement (0.0004 to 0.1112 success rate) and analyzing failure modes discovered by BaNEL
3. Test BaNEL's robustness to distillation errors by implementing early stopping criteria and tracking success rate trajectories across training rounds to identify potential collapse points