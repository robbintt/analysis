---
ver: rpa2
title: 'CMV-Fuse: Cross Modal-View Fusion of AMR, Syntax, and Knowledge Representations
  for Aspect Based Sentiment Analysis'
arxiv_id: '2512.06679'
source_url: https://arxiv.org/abs/2512.06679
tags:
- semantic
- syntactic
- sentiment
- graph
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CMV-Fuse, a cross-modal view fusion framework
  for aspect-based sentiment analysis (ABSA) that integrates multiple complementary
  linguistic perspectives including AMR semantics, constituency and dependency syntax,
  semantic attention, and external knowledge. The key innovation is a hierarchical
  gated fusion mechanism that combines these views across local syntactic, intermediate
  semantic, and global knowledge levels, supported by a structure-aware multi-view
  contrastive learning objective.
---

# CMV-Fuse: Cross Modal-View Fusion of AMR, Syntax, and Knowledge Representations for Aspect Based Sentiment Analysis

## Quick Facts
- arXiv ID: 2512.06679
- Source URL: https://arxiv.org/abs/2512.06679
- Authors: Smitha Muthya Sudheendra; Mani Deep Cherukuri; Jaideep Srivastava
- Reference count: 18
- Primary result: CMV-Fuse achieves 87.76% accuracy and 81.99% F1 on Restaurant14, outperforming BiSyn-GAT (87.49%) and Dual GCN (87.13%).

## Executive Summary
CMV-Fuse introduces a hierarchical gated fusion framework that integrates multiple linguistic perspectives—AMR semantics, constituency and dependency syntax, semantic attention, and external knowledge—for aspect-based sentiment analysis. The architecture fuses these views across three levels (local syntactic, intermediate semantic, global knowledge) using learnable gates and cross-attention, supported by structure-aware multi-view contrastive learning. Experimental results on benchmark datasets demonstrate superior performance over strong baselines, with ablation studies confirming the importance of both multi-view fusion and contrastive learning.

## Method Summary
CMV-Fuse processes input sentences through a BERT encoder and four separate GCNs (for constituency, dependency, semantic, and AMR graphs), then hierarchically fuses these representations across three levels: local syntactic fusion, intermediate semantic fusion, and global knowledge fusion. The model employs learnable gates at each fusion level and cross-attention mechanisms to enable structured information flow. A novel structure-aware multi-view contrastive learning objective aligns representations across views during training. The framework uses off-the-shelf parsers (AMR-BART, LEAMR, SuPar) to generate linguistic structures, builds adjacency matrices for each view, and trains with a combined cross-entropy and contrastive loss.

## Key Results
- Achieves 87.76% accuracy and 81.99% F1 on Restaurant14 dataset
- Outperforms BiSyn-GAT (87.49% accuracy) and Dual GCN (87.13% accuracy)
- Ablation studies show each linguistic view contributes to performance, with contrastive learning providing additional gains
- Knowledge integration benefits Restaurant14 while syntactic patterns dominate Laptop14

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Gated Fusion
The architecture uses three fusion levels—local syntactic (constituency + dependency), intermediate semantic (AMR + semantic attention), and global knowledge (KG)—with learnable gates to balance contributions at each level. Cross-attention between levels enables structured information flow. This hierarchical approach captures complementary signals from different linguistic levels that flat fusion or single-view models miss.

### Mechanism 2: Structure-Aware Multi-View Contrastive Learning
Three contrastive losses—syntactic-semantic alignment, AMR consistency, and KG alignment—are applied to semantically salient nodes using importance-based sampling. Margin-based objectives pull structurally relevant tokens closer and push unrelated tokens apart, regularizing representations and improving cross-modal consistency during training.

### Mechanism 3: AMR as Semantic Abstraction
AMR graphs provide semantic role information that complements syntactic views, especially for aspect-opinion disambiguation. AMR captures sentiment-bearing relationships better than surface syntax alone, such as resolving "small" as portion size versus "delicious" as taste through semantic role labeling.

## Foundational Learning

- **Graph Convolutional Networks (GCNs):** Four separate GCNs process constituency, dependency, semantic, and AMR graphs. Understanding message passing and adjacency-based aggregation is essential.
  - Quick check question: Given an adjacency matrix A and node features H, how does a GCN layer update node representations?

- **Attention Mechanisms:** Multi-head self-attention builds the semantic adjacency matrix; cross-attention fuses levels; gated attention balances views.
  - Quick check question: Explain how scaled dot-product attention computes weights between query and key vectors.

- **Contrastive Learning:** The structure-aware contrastive objective aligns multi-view representations using positive/negative pairs based on graph connectivity.
  - Quick check question: In a contrastive loss, what defines a positive pair vs. a negative pair for this architecture?

## Architecture Onboarding

- **Component map:** BERT encoder → Multi-View GCNs (ConGCN, DepGCN, SemGCN, AMR-GCN) → Hierarchical Fusion (Local Syntactic → Intermediate Semantic → Global Knowledge) → Classification Head. Contrastive loss runs alongside cross-entropy.

- **Critical path:** Correct adjacency matrix construction for each view (dependency, constituency, AMR, semantic) is essential. Errors in parsing or alignment (e.g., AMR token alignment via LEAMR) propagate through the model.

- **Design tradeoffs:** More GCN layers capture longer-range dependencies but increase computational cost. Sparse sampling in contrastive learning reduces overhead but may miss important nodes. Off-the-shelf parsers introduce domain mismatch errors.

- **Failure signatures:**
  - Flat accuracy around 85-86% may indicate a single view dominating (check gate weights).
  - No improvement from contrastive loss may indicate incorrect positive/negative pair definition or margin settings.
  - Parsing errors in AMR or dependency trees cause noisy adjacency matrices, degrading GCN outputs.

- **First 3 experiments:**
  1. Run ablation with each view disabled (H_amr, H_syn, H_kg) to confirm contribution per dataset (Table 3).
  2. Vary contrastive loss weights (λ_syn, λ_amr, λ_kg) to find optimal balance; monitor impact on F1.
  3. Test parser sensitivity: swap AMR-BART for a different AMR parser or use gold parses if available, measuring accuracy delta.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the hierarchical fusion mechanism adaptively weight linguistic representations based on input characteristics, rather than using fixed learnable weights?
- Basis in paper: The limitations section states "determining which representations meaningfully contribute to a specific dataset remains a challenge," noting that Restaurant14 benefits from knowledge integration while Laptop14 relies more on syntactic patterns.
- Why unresolved: Current α weights are learnable but static during inference; no mechanism exists to dynamically adjust view importance based on sentence length, domain, or structural complexity.
- What evidence would resolve it: Experiments showing performance gains when fusion weights are conditioned on input features (sentence length, parse tree depth, AMR graph density) compared to static weighting.

### Open Question 2
- Question: How does CMV-Fuse perform on languages with morphologically rich syntax or limited AMR parsing resources?
- Basis in paper: The conclusion explicitly calls for "multilingual adaptation" while the limitations note "the current evaluation is limited to English benchmark datasets."
- Why unresolved: AMR parsing tools are primarily trained on English; the framework's reliance on four language-specific structural representations creates challenges for cross-lingual transfer.
- What evidence would resolve it: Evaluation on multilingual ABSA benchmarks with analysis of how parser quality affects downstream performance.

### Open Question 3
- Question: What is the performance degradation when parser errors propagate through the multi-view architecture?
- Basis in paper: The limitations acknowledge "reliance on off-the-shelf parsers" whose "inherent parsing errors or domain mismatches can propagate through the model," yet ablation studies only assess clean-view combinations.
- Why unresolved: No analysis examines robustness when AMR, dependency, or constituency parses contain errors; real-world deployment would face noisy parser outputs.
- What evidence would resolve it: Systematic experiments injecting synthetic parser errors or evaluating on out-of-domain text where parser accuracy drops.

### Open Question 4
- Question: What interpretability mechanisms can disentangle each linguistic view's contribution to specific sentiment predictions?
- Basis in paper: The limitations state the need for "deeper interpretability analyses to better understand how each view contributes to the final prediction."
- Why unresolved: While ablation shows aggregate contributions, current architecture provides no mechanism to explain why particular views dominate for specific aspect-sentiment pairs in individual examples.
- What evidence would resolve it: Attention visualization across fusion levels, gradient-based attribution to specific graph edges, or probing tasks measuring which linguistic phenomena each view captures.

## Limitations

- The hierarchical fusion mechanism relies on learnable gates, but gate weight analysis is absent, making it unclear whether views contribute equally or if one dominates.
- External knowledge integration is underspecified, with no clear source or alignment mechanism defined, creating a critical gap for reproducibility.
- Parser dependency creates hidden fragility, as AMR-BART, LEAMR, and SuPar are all off-the-shelf and no error analysis or robustness tests are provided.

## Confidence

- **High confidence:** The hierarchical fusion architecture (three levels with gated attention) is clearly specified and validated via ablation (Tables 3 and 4). The contrastive learning objective is also well-defined and its impact is measurable.
- **Medium confidence:** The mechanism of AMR abstraction and its role in aspect-opinion disambiguation is plausible but only weakly supported by external literature. No direct corpus evidence or error analysis is provided.
- **Low confidence:** The external knowledge integration and its actual contribution to performance are unclear due to lack of specification and absence of ablation for this component.

## Next Checks

1. **Gate weight analysis:** Extract and analyze the learned α fusion weights across datasets and runs to confirm balanced contribution from syntactic, semantic, and knowledge views, or identify dominant components.

2. **Knowledge ablation and specification:** Clarify and implement the external knowledge source (e.g., ConceptNet), run an ablation to measure its impact, and analyze how missing or misaligned KG entities are handled.

3. **Parser error sensitivity:** Systematically corrupt or remove AMR/dependency parses for a subset of sentences and measure performance drop to quantify the model's robustness to parser errors.