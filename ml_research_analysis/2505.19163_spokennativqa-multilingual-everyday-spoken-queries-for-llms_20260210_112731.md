---
ver: rpa2
title: 'SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs'
arxiv_id: '2505.19163'
source_url: https://arxiv.org/abs/2505.19163
tags:
- performance
- english
- arabic
- spoken
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SpokenNativQA, the first multilingual spoken
  question-answering dataset designed to evaluate large language models (LLMs) on
  real-world conversational settings. The dataset contains approximately 33,000 naturally
  spoken questions and answers across Arabic and English, featuring speech variability,
  accents, and linguistic diversity.
---

# SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs

## Quick Facts
- arXiv ID: 2505.19163
- Source URL: https://arxiv.org/abs/2505.19163
- Reference count: 0
- First multilingual spoken question-answering dataset for real-world conversational evaluation of LLMs

## Executive Summary
This paper introduces SpokenNativQA, the first multilingual spoken question-answering dataset designed to evaluate large language models on real-world conversational settings. The dataset contains approximately 33,000 naturally spoken questions and answers across Arabic and English, featuring speech variability, accents, and linguistic diversity. The authors benchmarked various ASR systems (Google, Azure, Whisper, Fanar) and evaluated LLMs including GPT-4o, Fanar, and ALLaM using both cascaded and end-to-end approaches. Results showed that ASR errors significantly impact SQA performance, with Whisper achieving the best results among ASR systems. GPT-4o-audio outperformed all other models, highlighting the importance of end-to-end audio-specific systems. English SQA performance was consistently better than Arabic, even with Arabic-centric models. The study demonstrates the need for cascade-less systems, especially for critical applications like health-related queries.

## Method Summary
The study evaluates spoken question answering through cascaded ASR+LLM systems and end-to-end audio models on the SpokenNativQA dataset. The dataset contains 988 Arabic and 2,322 English test samples with audio recorded at 48 kHz mono (~3 seconds average). Four ASR systems (Google, Azure, Whisper, Fanar) transcribe audio, then LLMs (GPT-4o, Fanar, ALLaM) answer questions in zero-shot mode using language-specific prompts with answer length constraints. GPT-4o-audio-preview provides direct audio-to-answer evaluation. Primary metric is BERTScore F1 (AraBERT v2 for Arabic, bert-base-uncased for English), with ASR quality measured by WER.

## Key Results
- ASR errors significantly impact SQA performance, with cascaded systems showing consistent F1 degradation proportional to transcription quality
- GPT-4o-audio-preview outperformed all cascaded systems, achieving F1=0.55 (Arabic) and 0.62 (English)
- English SQA performance was consistently better than Arabic, even with Arabic-centric models like ALLaM
- Whisper achieved the best ASR results among tested systems (5.85 WER for Arabic, 10.58 WER for English)

## Why This Works (Mechanism)

### Mechanism 1: ASR Error Propagation Degrades QA Performance
Cascaded ASR+LLM systems suffer compounding errors that reduce QA accuracy proportionally to transcription quality. ASR transcription errors → incorrect/degraded text input → LLM processes noisy signal → reduced answer quality or outright failures (refusals, hallucinations). Core assumption: Error magnitude in ASR output correlates with downstream QA performance degradation. Evidence: Table 4 shows No-ASR F1=0.536 (Arabic) dropping to 0.522-0.531 with ASR; English drops from 0.619 to 0.536-0.594. Break condition: When ASR WER approaches ~5-6% (e.g., Google Arabic at 5.85 WER), cascaded performance approximates No-ASR baseline.

### Mechanism 2: End-to-End Audio Models Bypass Transcription Bottleneck
Direct audio-to-answer models outperform cascaded systems by eliminating intermediate transcription errors. Unified audio encoder extracts semantic representations without forcing discrete text tokens, preserving prosodic and acoustic cues that aid comprehension. Core assumption: The end-to-end model learns to map audio directly to semantic understanding without an explicit transcription step. Evidence: GPT-4o-audio achieves F1=0.55 (Arabic) and 0.62 (English), exceeding No-ASR for Arabic and matching for English. Break condition: If the "end-to-end" model internally transcribes (paper notes: "It is unclear whether this model operates as a cascaded system"), the advantage may derive from better transcription rather than true audio understanding.

### Mechanism 3: Speaker Native-Language Status Affects ASR Performance Asymmetrically
L2 (non-native) speakers experience higher ASR error rates than L1 speakers due to accent/prosody mismatch with training data. ASR systems trained predominantly on native speech → accent and pronunciation variations in L2 speech produce higher WER → downstream QA suffers. Core assumption: ASR training data underrepresents L2 accented speech for the target languages. Evidence: English WER ranges 10.58-33.80% vs Arabic WER 5.85-12.50% across systems; authors note ASR performance is affected for English because all speakers are L2 speakers. Break condition: When ASR is explicitly fine-tuned on accented or L2 speech data for the target population.

## Foundational Learning

- **Word Error Rate (WER)**: Primary ASR quality metric; directly reported in Table 3 and correlated with downstream QA performance degradation. Quick check: If ASR outputs "What Doha known for foods" for "What is Doha known for food", how many errors occurred (substitutions, deletions, insertions)?

- **Cascaded vs End-to-End Architecture**: Central experimental comparison; different failure modes, interpretability tradeoffs, and performance ceilings. Quick check: In cascaded SQA, which component's errors propagate downstream—and which component never sees the original audio?

- **Semantic Similarity Metrics (BERTScore F1)**: Paper uses BERTScore F1 rather than BLEU/ROUGE, noting limitations of exact-match metrics for long-form QA. Quick check: Why might semantic embedding similarity better capture QA quality than token-level overlap for open-ended answers?

## Architecture Onboarding

- **Component map**: Audio → ASR (Google/Azure/Whisper/Fanar) → Text → LLM (GPT-4o/Fanar/ALLaM) → Answer (cascaded); Audio → GPT-4o-audio-preview → Answer (end-to-end); Evaluation: BERTScore with language-specific encoders (AraBERT for Arabic, bert-base-uncased for English)

- **Critical path**: 1) Data: SpokenNativQA test subset (988 Arabic, 2322 English samples, ~3s average duration) 2) ASR selection: Match system to language/speaker profile (Google-Qatar for Arabic L1, Whisper for English L2) 3) Prompting: Zero-shot with language-specific prompts; include length constraint from dataset answer length 4) Evaluation: BERTScore F1; compare against No-ASR gold-transcript baseline

- **Design tradeoffs**: Region-specific ASR (lower WER, limited geographic scope) vs general multilingual ASR (higher WER, broader applicability); Cascaded (interpretable intermediate transcription, debuggable) vs end-to-end (higher performance, black-box); Length-constrained prompting (controlled output) vs open generation (risk of verbose/irrelevant responses)

- **Failure signatures**: Language confusion: ASR transcribing English audio as Arabic script (Azure example in paper caused GPT-4o refusal); LLM refusal: Transcription errors triggering "wrong language" detection; Hallucination: Garbled transcription leading to fabricated answers (Fanar example in paper)

- **First 3 experiments**: 1) Establish baseline: Run No-ASR setup with gold transcripts; record per-model F1 for Arabic and English 2) ASR benchmark: Evaluate Google, Azure, Whisper, Fanar on both languages; compute WER and downstream F1 delta from baseline 3) End-to-end comparison: Run GPT-4o-audio-preview on raw audio; compare against best cascaded configuration to quantify gap

## Open Questions the Paper Calls Out

### Open Question 1
Can open-source, cascade-less Large Language Models (LLMs) achieve performance parity with proprietary end-to-end systems like GPT-4o-audio on spoken query tasks? Basis: The conclusion explicitly calls for "moving towards the development of a cascade-less LLM supporting everyday queries," noting that GPT-4o-audio's superior performance highlights the need for such systems with open models. Unresolved because while the authors demonstrated the effectiveness of the proprietary GPT-4o-audio, there is currently no comparable open-source end-to-end model evaluated in the study to serve as a baseline for the community. Evidence: Benchmarking an open-source end-to-end audio-text model on SpokenNativQA that matches or exceeds the F1 scores of GPT-4o-audio (0.55 for Arabic, 0.62 for English).

### Open Question 2
Does the SpokenNativQA evaluation framework generalize to other low-resource, dialect-rich languages and cultural contexts beyond Qatar? Basis: The "Future Work" section explicitly lists "extending the dataset for other regions and locations" as a primary goal for subsequent studies. Unresolved because the current study is limited to Arabic and English (Qatar context); it is undetermined if the ASR error propagation and LLM performance trends observed are specific to this dataset's linguistic nuances or universal. Evidence: Replicating the data collection and benchmarking methodology for a different region (e.g., South Asia or Southeast Asia) and comparing the ASR-to-LLM error correlations.

### Open Question 3
To what extent does the Second Language (L2) speaker status of the English cohort influence ASR error rates compared to Native (L1) Arabic speakers? Basis: The authors note that ASR performance was lower for English, "possibly because all speakers are L2 speakers," whereas Arabic speakers were L1. This suggests the dataset contains an imbalance in speaker nativity that may skew cross-linguistic comparisons. Unresolved because the paper reports the difference in performance but does not isolate "accent/non-native speech" as an independent variable from the "language" variable (English vs. Arabic). Evidence: A controlled experiment evaluating ASR systems on SpokenNativQA using L1 English speakers compared to the current L2 English corpus to isolate the impact of accent versus language complexity.

## Limitations
- GPT-4o-audio-preview's internal processing remains opaque - it's unclear whether it truly operates as end-to-end or internally performs transcription
- English speaker population consists entirely of L2 speakers, creating a confounding factor where English WER is inherently higher regardless of model quality
- Study lacks cross-domain validation - all speakers recorded in Qatar using identical equipment and conditions, limiting generalizability

## Confidence

- **High confidence**: ASR error propagation degrades QA performance (empirically demonstrated through consistent F1 drops across systems and languages)
- **Medium confidence**: End-to-end models outperform cascaded systems (supported by GPT-4o-audio results, but mechanism attribution uncertain)
- **Medium confidence**: Language asymmetry reflects speaker nativeness (plausible given WER patterns, but not definitively proven due to single recording context)

## Next Checks

1. **Mechanism validation**: Instrument GPT-4o-audio-preview to determine whether it performs internal transcription or truly processes audio directly, resolving the uncertainty about claimed "bypass" mechanism.

2. **Cross-domain robustness**: Evaluate the same models on SpokenNativQA samples recorded in varied acoustic environments (different rooms, equipment, noise levels) to test generalizability beyond controlled Qatar recording conditions.

3. **L2 accent analysis**: Test the hypothesis about nativeness by evaluating English ASR performance on native English speakers and comparing error patterns to the L2 speaker results, controlling for acoustic environment.