---
ver: rpa2
title: 'FRAG: Frame Selection Augmented Generation for Long Video and Long Document
  Understanding'
arxiv_id: '2504.17447'
source_url: https://arxiv.org/abs/2504.17447
tags:
- long
- frames
- frag
- video
- lmms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of processing long video and
  long document inputs using Large Multimodal Models (LMMs) without requiring long
  context models. The core method, Frame Selection Augmented Generation (FRAG), first
  scores each frame/page independently for its relevance to the query, then selects
  the top-K scoring frames/pages for final generation.
---

# FRAG: Frame Selection Augmented Generation for Long Video and Long Document Understanding

## Quick Facts
- arXiv ID: 2504.17447
- Source URL: https://arxiv.org/abs/2504.17447
- Authors: De-An Huang; Subhashree Radhakrishnan; Zhiding Yu; Jan Kautz
- Reference count: 40
- Primary result: Achieves state-of-the-art performance on long video and long document benchmarks by selecting top-K relevant frames/pages without requiring long context models

## Executive Summary
FRAG (Frame Selection Augmented Generation) addresses the challenge of processing long videos and documents using existing Large Multimodal Models (LMMs) without fine-tuning or long-context architectures. The method scores each frame/page independently for relevance to a query using a binary "sufficiency" prompt, then selects the top-K scoring frames for final answer generation. This approach significantly improves performance on five long video benchmarks and three multi-page document benchmarks while being computationally more affordable than processing all frames jointly.

## Method Summary
FRAG processes long inputs by first uniformly sampling frames (256 for videos, all pages for documents), then scoring each frame/page independently using an LMM with a binary relevance prompt ("Does this frame contain sufficient information to answer the question?"). The top-K scoring frames/pages are selected and fed to an answering LMM in their original temporal/page order. This zero-shot approach requires no fine-tuning and works across different LMM families (LLaVA-OneVision and InternVL2) and model sizes, with optimal performance typically at K=32 frames and N=256 sampled frames.

## Key Results
- Improves InternVL2-76B by 5.8% on MLVU and 3.7% on Video-MME for long videos
- Outperforms proprietary models like GPT-4o on LongVideoBench
- Achieves over 20% improvements on MP-DocVQA compared to specialized models
- Doubles F1 scores on SlideVQA for long documents
- State-of-the-art results while being computationally more affordable than processing all frames jointly

## Why This Works (Mechanism)

### Mechanism 1: Zero-Shot Relevance Scoring
Existing LMMs can act as effective relevance filters without fine-tuning by evaluating the probability of a binary "sufficiency" prompt. The system prompts the LMM with a specific query and single frame, asking if it contains sufficient information to answer the question, then extracts the probability of the "yes" token as a relevance score. This works because the LMM's internal representation of "sufficiency" correlates with actual utility for answering queries.

### Mechanism 2: Noise Filtration for Fixed Context Windows
Selecting Top-K frames significantly improves signal-to-noise ratio compared to uniform sampling, allowing standard context windows to handle long inputs effectively. By filtering out frames with low relevance scores, the answering LMM receives a dense "highlight reel" of the video or document, preventing attention mechanisms from being diluted by irrelevant temporal noise.

### Mechanism 3: Decoupled Parallel Inference
Decoupling the scoring process from generation eliminates quadratic complexity of processing long contexts in a single forward pass. Frames are scored independently (batch size 1 or parallel batches), allowing the system to scale input length linearly rather than quadratically since no self-attention is calculated across the entire video/document at once.

## Foundational Learning

- **Logit-based Probability Extraction**: Understanding how to access raw logits from an LMM head is essential since FRAG relies on the probability the model assigns to the "Yes" token rather than the text output. Quick check: Can you modify a standard HuggingFace generate call to return the softmax probability of a specific token ID rather than the decoded string?

- **Multi-Image Context Packing**: After selection, multiple frames must be fed into the answering LMM, requiring understanding how specific LMM architectures handle multiple image embeddings (e.g., concatenating visual tokens, special separators). Quick check: Does your target LMM support an arbitrary number of image tokens in a single prompt, or is it restricted to single-image interleaved text?

- **Temporal Redundancy vs. Diversity**: The paper identifies a failure mode where "Top-K" selects temporally adjacent frames if sampling rate is too high. Quick check: If you sample 1000 frames from a 10-second video, why might a simple Top-K selection provide a worse answer than sampling 100 frames?

## Architecture Onboarding

- **Component map**: Sampler (uniform downsampling) -> Scoring LMM (Query, Frame -> Probability("Yes")) -> Selector (sorts scores, picks Top-K) -> Answering LMM (Query, Top-K Frames -> Final Answer)
- **Critical path**: The Scoring LMM quality is the primary bottleneck. Table 7 shows that using a weak scorer (SigLIP or 1B models) hurts performance, while a strong scorer (76B model) improves results even when the Answering model is smaller (40B).
- **Design tradeoffs**: It's often better to use a larger, more capable model for scoring (relevance) and a smaller model for answering, or the same model. Performance peaks around 32 frames; more frames can introduce noise or exceed context limits. Increasing sampled frames (N) can hurt performance because Top-K frames become too temporally concentrated.
- **Failure signatures**: Temporal Concentration (model selects 32 frames all from same 2 seconds, missing "what happened next" context); Uniform Baseline Match (if FRAG performs identically to Uniform sampling, Scoring LMM is failing to distinguish relevance).
- **First 3 experiments**: 1) Sanity Check: Run scoring prompt on video with clear visual query and plot scores over time to ensure they peak at correct frames. 2) Ablation on K: Keep N fixed (256), vary K (8, 16, 32, 64) to find saturation point where additional frames add noise. 3) Cross-Model Scoring: Use high-performance model (e.g., InternVL2-76B) solely for scoring, lightweight model for answering, to assess latency/quality trade-offs.

## Open Questions the Paper Calls Out

### Open Question 1
Can incorporating explicit temporal diversity constraints into the selection mechanism mitigate the performance regression observed when sampling very large numbers of frames (e.g., 1024)? The authors identify that Top-K selection with excessive sampling causes frames to be "too concentrated in a short time period," limiting diversity and hurting performance. A comparative study with Top-K augmented with non-maximum suppression or temporal spacing constraints on the 1024-frame setting would resolve this.

### Open Question 2
How can the computational efficiency of the scoring phase be improved to handle extreme-length inputs without sacrificing the zero-shot nature of the method? The current method requires processing every sampled frame through an LMM to generate a score, which is parallelizable but computationally heavy. Implementation of a sparse scanning method or lightweight preliminary filter would resolve this.

### Open Question 3
Does the binary "sufficient information" scoring prompt fail to retrieve necessary context for questions requiring synthesis of information distributed across many non-standalone frames? The method relies on the probability of a "Yes" answer to a binary prompt, assuming relevance correlates with standalone sufficiency, which might not hold for multi-hop reasoning. An ablation study comparing the current binary prompt with a "partial relevance" prompt on multi-hop reasoning benchmarks would resolve this.

## Limitations

- Effectiveness may be limited for global summarization queries requiring integration across all frames, as the method assumes queries can be localized to specific frames/pages
- Temporal diversity degradation occurs when oversampling frames (e.g., 1024), causing Top-K selection to pick temporally adjacent frames and reducing content diversity
- Performance heavily depends on scoring LMM quality, with weak scorers (1B, SigLIP) potentially degrading performance compared to uniform sampling baselines

## Confidence

**High confidence**: Computational efficiency claim (quadratic complexity reduction) is well-supported by architecture design where scoring is decoupled and performed independently on frames. Empirical improvements across multiple benchmarks provide strong evidence for core approach's effectiveness.

**Medium confidence**: Generalizability across different LMM families (LLaVA-OneVision and InternVL2) and model sizes is supported by experiments, but limits of this transferability aren't fully explored. Optimal parameters (K=32, N=256) work well but may not be universally optimal.

**Low confidence**: Claim that FRAG "doubles" F1 scores on SlideVQA compared to specialized models is impressive but relies on comparisons with potentially outdated baselines without independent replication.

## Next Checks

1. **Temporal diversity analysis**: For sample videos, plot temporal distribution of selected frames when varying N (100, 256, 1024) with fixed K to verify high N causes temporal clustering and reduced performance.

2. **Scoring model ablation study**: Implement scoring using increasingly weak models (1B, 8B, 34B, 76B) on subset of benchmarks to measure degradation curve and identify minimum effective scorer size.

3. **Global query benchmarking**: Design new test set with queries requiring full-video context (total counts, comprehensive summaries) and measure FRAG performance against uniform sampling to validate localizability assumption.