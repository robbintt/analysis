---
ver: rpa2
title: 'Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based
  Bias Benchmarks'
arxiv_id: '2507.16989'
source_url: https://arxiv.org/abs/2507.16989
tags:
- source
- link
- bias
- nationality
- names
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates nationality bias in LLMs by replacing explicit
  nationality labels with culturally indicative names, using an extended version of
  the BBQ dataset. The approach tests whether bias persists when overt demographic
  markers are removed.
---

# Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks

## Quick Facts
- **arXiv ID**: 2507.16989
- **Source URL**: https://arxiv.org/abs/2507.16989
- **Reference count**: 19
- **Primary result**: Nationality bias in LLMs persists even when explicit nationality labels are replaced with culturally indicative names, with smaller models showing higher bias and error retention.

## Executive Summary
This study evaluates nationality bias in large language models by replacing explicit nationality labels with culturally indicative names, using an extended version of the BBQ dataset. The research tests whether bias persists when overt demographic markers are removed, addressing concerns about explicit bias detection in traditional benchmarks. Results demonstrate that while substituting names for nationalities reduces bias in some cases, it does not eliminate it—models still display stereotypical associations under ambiguous conditions. Smaller models (Claude Haiku, GPT-4o-mini) exhibit higher bias and lower accuracy than larger models (GPT-4o, Claude Sonnet), with smaller models retaining a larger portion of errors after name substitution.

## Method Summary
The researchers developed a name-based bias benchmark by extending the BBQ dataset to replace explicit nationality labels with culturally indicative names. They evaluated multiple LLMs including GPT-4o, Claude Haiku, Claude Sonnet, and Gemini-1.5-flash across various tasks. The methodology involved testing whether models would still exhibit nationality-based biases when direct nationality cues were obscured. They measured both bias persistence and accuracy differences between name-based and explicit nationality-based evaluations, focusing particularly on error retention rates when names were substituted for nationalities.

## Key Results
- Smaller models (Claude Haiku, GPT-4o-mini) show higher bias and lower accuracy than larger models (GPT-4o, Claude Sonnet)
- Name substitution reduces bias in some cases but does not eliminate it—models retain stereotypical associations under ambiguous conditions
- Claude Haiku and Gemini-1.5-flash show the highest error retention after name substitution, with smaller models retaining a larger portion of errors overall

## Why This Works (Mechanism)
The approach works because names carry cultural and ethnic associations that can trigger stereotypical associations even without explicit nationality labels. Models trained on web data have learned these implicit associations between names and nationalities through patterns in their training corpus. When explicit nationality cues are removed, the models must rely on these learned name-based associations, which can still activate stereotypical biases. This demonstrates that bias is embedded in the models' understanding of cultural indicators beyond just explicit demographic information.

## Foundational Learning

**Cultural Name Associations**
Why needed: Names carry implicit cultural and ethnic information that models learn during training
Quick check: Verify that name substitution preserves cultural context while removing explicit nationality markers

**BBQ Dataset Extension**
Why needed: Standard bias benchmarks often use explicit demographic labels that can trigger overt bias detection
Quick check: Confirm that name-based evaluation captures bias without providing explicit demographic cues

**Error Retention Analysis**
Why needed: Understanding how bias persists when explicit markers are removed reveals the depth of embedded stereotypes
Quick check: Compare error rates between name-based and explicit nationality-based evaluations

## Architecture Onboarding

**Component Map**
BBQ dataset -> Name substitution module -> LLM evaluation -> Bias measurement -> Error retention analysis

**Critical Path**
Name selection → Dataset extension → Model evaluation → Bias quantification → Error retention comparison

**Design Tradeoffs**
The name-based approach reduces explicit bias detection but may not eliminate implicit cultural associations. Smaller models trade off accuracy for efficiency, resulting in higher bias retention. The methodology balances ecological validity with controlled evaluation conditions.

**Failure Signatures**
High error retention in smaller models indicates persistent bias when explicit cues are removed. Models showing consistent bias patterns across name substitutions reveal deeply embedded stereotypes. Discrepancies between name-based and explicit evaluations suggest incomplete bias mitigation.

**3 First Experiments**
1. Evaluate bias retention rates across different model sizes using the name-based benchmark
2. Compare name-based bias detection with traditional explicit nationality benchmarks
3. Test cross-cultural generalizability by expanding name sets beyond the initial dataset

## Open Questions the Paper Calls Out
None

## Limitations
- The methodology assumes names serve as accurate cultural indicators, which may not fully capture the complexity of nationality identity
- Focus on limited model set and tasks may not generalize to broader applications or other cultural contexts
- Small-scale datasets and lack of diverse cultural perspectives constrain robustness of conclusions

## Confidence

**High**: Models exhibit bias when explicit nationality labels are replaced with names, and smaller models show higher bias and error retention than larger models

**Medium**: Name substitution reduces bias in some cases but does not eliminate it, and models retain stereotypical associations under ambiguous conditions

**Low**: The generalizability of findings to other cultural contexts, tasks, or model architectures, as the study is limited in scope and diversity of evaluation

## Next Checks
1. Expand cultural diversity by testing the methodology with names and cultural indicators from a broader range of nationalities and regions to assess generalizability
2. Evaluate larger tasks by applying the name-based bias benchmark to more complex tasks (e.g., decision-making, content generation) to determine if bias persists in real-world applications
3. Compare with alternative methods by validating the name-based approach against other bias detection methods (e.g., sentiment analysis, human evaluation) to ensure robustness and consistency