---
ver: rpa2
title: 'Beyond Neural Networks: Symbolic Reasoning over Wavelet Logic Graph Signals'
arxiv_id: '2507.21190'
source_url: https://arxiv.org/abs/2507.21190
tags:
- graph
- symbolic
- spectral
- wavelet
- signal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a fully non-neural learning framework for
  graph-structured data based on Graph Laplacian Wavelet Transforms (GLWT). Unlike
  traditional neural architectures, the model operates entirely in the spectral domain
  using structured multiscale filtering, interpretable nonlinear shrinkage, and symbolic
  reasoning over wavelet coefficients.
---

# Beyond Neural Networks: Symbolic Reasoning over Wavelet Logic Graph Signals

## Quick Facts
- arXiv ID: 2507.21190
- Source URL: https://arxiv.org/abs/2507.21190
- Authors: Andrew Kiruluta; Andreas Lemos; Priscilla Burity
- Reference count: 15
- Key outcome: Introduces a fully non-neural learning framework for graph-structured data based on Graph Laplacian Wavelet Transforms (GLWT), achieving 83.1% accuracy on Cora and 72.5% on Citeseer without neural networks.

## Executive Summary
This paper presents a novel non-neural framework for graph learning that operates entirely in the spectral domain using Graph Laplacian Wavelet Transforms. Unlike traditional neural architectures, the model combines structured multiscale filtering, interpretable nonlinear shrinkage, and symbolic reasoning over wavelet coefficients to achieve transparent and efficient graph learning. The framework demonstrates competitive performance on both synthetic graph denoising tasks and real-world citation networks while offering greater interpretability and robustness compared to lightweight GNNs and classical denoising methods.

## Method Summary
The framework introduces Graph Laplacian Wavelet Transforms (GLWT) as a spectral decomposition technique that enables multiscale filtering of graph signals without neural network components. The method processes graph-structured data through three main stages: spectral decomposition using GLWT, nonlinear shrinkage of wavelet coefficients through modulation functions, and rule-based aggregation for final predictions. This approach operates entirely in the frequency domain, leveraging the graph Laplacian's spectral properties to capture multiscale patterns while maintaining interpretability through symbolic reasoning over the wavelet coefficients.

## Key Results
- Achieved 83.1% classification accuracy on Cora and 72.5% on Citeseer citation networks
- Demonstrated lower mean squared reconstruction error compared to classical denoising methods on synthetic signals
- Outperformed lightweight GNNs while maintaining full interpretability through symbolic reasoning

## Why This Works (Mechanism)
The framework leverages the mathematical properties of graph Laplacians and wavelet transforms to decompose graph signals into interpretable frequency components. By operating in the spectral domain, the model captures multiscale patterns inherent in graph-structured data while avoiding the opacity of neural networks. The nonlinear shrinkage operation selectively amplifies or suppresses wavelet coefficients based on their magnitude and position, effectively filtering noise while preserving meaningful signal components. The symbolic reasoning component then interprets these processed coefficients through rule-based logic, creating transparent decision pathways that connect spectral features to final predictions.

## Foundational Learning
- **Graph Laplacian Spectral Theory**: Understanding the relationship between graph structure and its spectral decomposition is crucial for interpreting how GLWT captures multiscale patterns. Quick check: Verify that the eigenvalues of the graph Laplacian correspond to meaningful frequency components in your data.
- **Wavelet Transform Principles**: Knowledge of continuous and discrete wavelet transforms helps explain how multiscale filtering preserves both local and global signal features. Quick check: Confirm that wavelet coefficients at different scales capture complementary aspects of the graph signal.
- **Nonlinear Shrinkage Functions**: Understanding various shrinkage operators (hard, soft, adaptive) is essential for optimizing the trade-off between noise reduction and signal preservation. Quick check: Test different shrinkage functions to find the optimal balance for your specific noise characteristics.
- **Symbolic Logic Systems**: Familiarity with rule-based reasoning systems is necessary to interpret how symbolic aggregation translates wavelet coefficients into interpretable predictions. Quick check: Validate that the symbolic rules capture domain-relevant patterns rather than spurious correlations.

## Architecture Onboarding
- **Component Map**: Graph Signal -> GLWT Spectral Decomposition -> Nonlinear Shrinkage -> Symbolic Aggregation -> Predictions
- **Critical Path**: The spectral decomposition stage is the computational bottleneck, as it requires eigenvalue decomposition of the graph Laplacian matrix. The symbolic aggregation stage is the interpretability bottleneck, as rule quality depends on the expressiveness of the wavelet coefficient representation.
- **Design Tradeoffs**: The framework trades computational efficiency for interpretability, with the full GLWT pipeline being more expensive than simple spectral filtering but less resource-intensive than deep neural networks. The symbolic reasoning component adds interpretability but may limit the model's capacity to capture highly complex, non-linear relationships.
- **Failure Signatures**: Poor performance on highly irregular graph structures where the graph Laplacian's spectral properties don't align with meaningful signal patterns. Over-smoothing of wavelet coefficients leading to loss of important local features. Symbolic rules that become overly complex or fail to generalize across different graph instances.
- **First Experiments**: 1) Apply GLWT to synthetic graph signals with known frequency components to verify multiscale decomposition accuracy. 2) Test different shrinkage functions on noisy graph signals to identify optimal noise reduction characteristics. 3) Evaluate symbolic rule generation on simple graph classification tasks to assess interpretability quality.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance comparison with lightweight GNNs may be influenced by architectural choices and hyperparameter tuning that are not fully detailed
- Symbolic reasoning component's robustness across diverse graph structures and noise levels needs empirical verification
- Computational complexity of the full GLWT pipeline compared to established methods is not explicitly quantified

## Confidence
- High confidence in the technical formulation of GLWT-based spectral filtering and shrinkage operations, as these build on established wavelet and graph signal processing theory
- Medium confidence in the experimental results, given that performance metrics are reported but detailed ablation studies and hyperparameter sensitivity analyses are lacking
- Medium confidence in the interpretability claims, as the symbolic reasoning framework is described but not thoroughly validated against alternative interpretability methods

## Next Checks
1. Conduct ablation studies to isolate the contributions of multiscale filtering, shrinkage modulation, and symbolic aggregation components to overall performance
2. Perform scalability and runtime analysis comparing the GLWT-based model to both lightweight GNNs and classical denoising methods across graphs of varying sizes and densities
3. Design experiments to test the symbolic reasoning component's interpretability and robustness by perturbing graph structures and noise patterns, then assessing the consistency and meaningfulness of the derived rules