---
ver: rpa2
title: Multi-fidelity Batch Active Learning for Gaussian Process Classifiers
arxiv_id: '2510.08865'
source_url: https://arxiv.org/abs/2510.08865
tags:
- learning
- function
- active
- gaussian
- bpmi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficient simulation budget
  allocation in multi-fidelity settings for binary classification using Gaussian Process
  models. The core contribution is a new acquisition function, Bernoulli Parameter
  Mutual Information (BPMI), which approximates mutual information between Bernoulli
  parameters by linearizing the probit link function.
---

# Multi-fidelity Batch Active Learning for Gaussian Process Classifiers

## Quick Facts
- arXiv ID: 2510.08865
- Source URL: https://arxiv.org/abs/2510.08865
- Reference count: 39
- Primary result: BPMI consistently achieves higher ELPP and lower MSE than baseline methods on synthetic and real-world problems

## Executive Summary
This paper introduces Bernoulli Parameter Mutual Information (BPMI), a novel acquisition function for multi-fidelity batch active learning with Gaussian Process classifiers. BPMI addresses the intractability of mutual information calculation in probability space by using a first-order Taylor expansion of the probit link function, enabling efficient analytical approximations. The method focuses sampling on uncertain regions near decision boundaries by weighting the MI calculation with the derivative of the probit link function, and adaptively determines sampling frequency based on estimated aleatoric uncertainty. Experiments demonstrate BPMI outperforms random sampling and latent function MI approaches across multiple synthetic benchmarks and a real-world laser-ignited rocket combustor application.

## Method Summary
The method uses a bi-fidelity Gaussian Process classifier with an autoregressive structure (f_H = ρf_L + δ) trained via variational inference in gpytorch. BPMI approximates mutual information between Bernoulli parameters using a first-order Taylor expansion of the probit link function, transforming the problem into a tractable Gaussian domain. Batch selection employs a greedy submodular optimization strategy that maximizes marginal information gain per unit cost until the computational budget is exhausted. The approach includes a heuristic for determining sampling frequency based on predicted probability variance and adds small random jitter to repeated sample coordinates to maintain positive-definite kernels.

## Key Results
- BPMI achieved ELPP of -0.20 versus -0.22 for random sampling after three active learning iterations on the rocket combustor problem
- On synthetic test cases, BPMI consistently achieved higher ELPP and lower MSE than baseline methods
- BPMI samples concentrated near the transition boundary while random sampling dispersed samples across the domain
- The method achieved 8,192 GPU hours of computation in the rocket combustor application

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A first-order Taylor expansion of the probit link function enables an efficient, analytical approximation of Mutual Information (MI) in probability space for Bernoulli-distributed outputs.
- Mechanism: The standard MI calculation is intractable for the non-Gaussian probability distribution produced by the probit link function. By linearizing the link function around the posterior mean (p_joint ≈ Φ(μ_f) + (f_joint - μ_f) ⊙ Φ'(μ_f)), the problem is transformed back into a Gaussian domain. This permits the use of analytical MI formulas based on log-determinants of covariance matrices, which are computationally efficient.
- Core assumption: The first-order Taylor expansion provides a sufficiently accurate approximation of the probit function in the regions of interest (i.e., near the posterior mean) for the purpose of calculating mutual information.
- Evidence anchors:
  - [abstract] "BPMI circumvents the intractability of calculating mutual information in the probability space by employing a first-order Taylor expansion of the link function."
  - [Section 2.2.3] Describes the linearization procedure in detail, leading to an approximated Gaussian distribution for the Bernoulli parameters.
  - [corpus] Corpus signals are weak for this specific Taylor-expansion-for-MI mechanism in classification contexts. Neighboring papers discuss multi-fidelity Bayesian optimization broadly, but not this specific workaround for GP classifiers.
- Break condition: The Taylor approximation will degrade if the latent function's posterior has high variance or is highly non-Gaussian, causing the linearized MI estimate to be inaccurate. The method assumes the latent posterior is well-approximated by its mean for the purpose of the expansion.

### Mechanism 2
- Claim: Weighting the MI calculation by the derivative of the probit link function (φ(μ_f)) focuses sampling on the uncertain decision boundary, improving data efficiency.
- Mechanism: The derivative of the probit link function, the standard normal PDF (φ(·)), approaches zero for large absolute values of the latent function. When the transformation matrix D (containing these derivatives) is applied to the covariance matrix (Σ_p ≈ DΣ_f D^T), it effectively nullifies the contribution of regions where the latent function is far from zero (i.e., where the classification is already certain). This prevents the acquisition function from wasting budget on points where reducing latent uncertainty does not reduce probability uncertainty.
- Core assumption: Regions of high latent variance but saturated probability (near 0 or 1) provide negligible value and should be ignored by the active learning strategy.
- Evidence anchors:
  - [Section 2.2.3] "The key advantage of this formulation is that the derivative term φ(μ_f) approaches zero as |μ_f| becomes large... This naturally focuses the acquisition function on uncertain regions near the decision boundary."
  - [Section 3.2, Figure 4] Shows that BPMI concentrates samples near the transition boundary, whereas random sampling disperses them.
  - [corpus] No direct evidence in the provided corpus for this specific derivative-weighting mechanism.
- Break Condition: If the decision boundary is ill-defined or non-existent in certain regions, the method's focus on high-uncertainty areas might be misled by noise.

### Mechanism 3
- Claim: A submodular greedy optimization strategy efficiently solves the NP-hard problem of batch selection for multi-fidelity queries under a budget constraint.
- Mechanism: Selecting the optimal batch of points is a combinatorial problem. Mutual information is a submodular function, meaning it exhibits diminishing returns. This property guarantees that a simple greedy algorithm (iteratively adding the query that maximizes marginal information gain per unit cost) achieves a solution within a (1-1/e) factor of the optimal solution, making the selection process computationally tractable.
- Core assumption: Mutual information is a monotonic and submodular set function in this context, and the greedy approximation is sufficient for practical performance.
- Evidence anchors:
  - [Section 2.2.1] "However, mutual information is known to be a submodular function... a simple greedy algorithm is guaranteed to find a solution that is within a constant factor (1-1/e) of the optimal solution."
  - [Section 2.2.1, Equation 17] Defines the greedy selection rule: selecting the query that maximizes marginal information gain per unit cost.
  - [corpus] Corpus neighbors like "Holistic Bioprocess Development Across Scales Using Multi-Fidelity Batch Bayesian Optimization" and "Efficient Learning of Vehicle Controller Parameters..." use similar greedy or Bayesian optimization strategies for multi-fidelity problems, supporting the general approach.
- Break Condition: The submodularity guarantee holds for a fixed model. If re-training the model after each greedy selection significantly changes the information landscape, the theoretical guarantee may be weakened, though the paper operates without re-training during batch construction.

## Foundational Learning

- Concept: **Gaussian Process Classification (GPC)**
  - Why needed here: The entire method is built on a multi-fidelity GPC model. Understanding how a GP models a latent function and passes it through a link function (like probit) to get binary class probabilities is essential to understand what BPMI is trying to optimize.
  - Quick check question: What is the role of the latent function f(x) in a GPC, and how is it converted to a class probability p(y=1|x)?

- Concept: **Variational Inference**
  - Why needed here: The paper uses a variational inference framework to train the model because exact inference is intractable due to the non-Gaussian likelihood. Understanding that the model learns an approximate posterior q(u) is key to understanding the source of the uncertainty that BPMI uses.
  - Quick check question: Why is exact inference impossible for Gaussian Process Classification, and what does variational inference optimize?

- Concept: **Multi-fidelity Modeling (Auto-regressive)**
  - Why needed here: The method relies on an auto-regressive scheme (f_H(x) = ρ f_L(x) + δ(x)) to link low- and high-fidelity data sources. Understanding this relationship is critical to understanding how information from cheap simulations transfers to the expensive ones.
  - Quick check question: In the bi-fidelity model, what are the roles of the correlation parameter ρ and the discrepancy function δ(x)?

## Architecture Onboarding

- Component map: Model -> Variational Trainer -> BPMI Acquisition Function -> Greedy Batch Selector
- Critical path:
  1. Model Initialization: Train the bi-fidelity GPC on any available initial data using variational inference.
  2. Candidate Generation: Define the search space (e.g., a grid or random samples).
  3. Batch Construction:
      a. Calculate BPMI score for all candidates.
      b. Select the point/fidelity pair with the highest marginal gain per cost.
      c. Add to batch and mark as selected.
      d. Repeat steps a-c until budget B is reached.
  4. Execution: Run the selected simulations (low and high fidelity).
  5. Retraining: Add new data to the training set and re-train the GPC model.
  6. Iteration: Repeat from step 3 for the next active learning round.

- Design tradeoffs:
  - **Taylor Approximation vs. Accuracy**: The first-order Taylor expansion makes MI calculation tractable and fast. The tradeoff is that it's an approximation that may be less accurate if the latent function's posterior is highly non-Gaussian or has large variance.
  - **Greedy Selection vs. Optimal Batch**: The greedy approach is computationally efficient and has a provable near-optimality guarantee. A full combinatorial search might find a better batch but would be computationally prohibitive.
  - **Heuristic Repeats**: The method uses a heuristic for the number of times to sample a point (N), based on predicted probability variance. This adds efficiency but is an ad-hoc rule and may not be optimal for all problems.

- Failure signatures:
  - **Sampling in Saturated Regions**: If BPMI is not correctly implemented (e.g., missing the derivative weighting), the model may waste budget sampling where the outcome is already certain.
  - **Poor Fidelity Correlation**: If the low- and high-fidelity data sources are not correlated (ρ is near zero), the multi-fidelity model provides little benefit, and the algorithm may struggle to decide where to allocate the budget.
  - **Model Mismatch**: If the Gaussian Process with an RBF kernel is a poor fit for the true underlying function, the model's uncertainty estimates will be unreliable, degrading the performance of BPMI.

- First 3 experiments:
  1. Replicate a Synthetic Problem: Re-create one of the paper's toy problems (e.g., Linear Transformation) to validate the implementation of the bi-fidelity model and the BPMI acquisition function. Compare ELPP and MSE against the paper's reported values.
  2. Ablation on Derivative Weighting: Run the active learning loop with a baseline acquisition function that does not include the derivative weighting term (φ(μ_f)) in the MI calculation. Compare its performance against BPMI to confirm that focusing on the decision boundary is the key to its efficiency.
  3. Ablation on Fidelity Cost Ratio: Test BPMI with different cost ratios between low- and high-fidelity simulations (e.g., 5:1, 10:1, 20:1) to understand how the algorithm adapts its selection of fidelity levels and how robust it is to varying cost structures.

## Open Questions the Paper Calls Out

- Does BPMI maintain superior performance in higher-dimensional input spaces or across a broader set of benchmark problems?
  - Basis in paper: [explicit] Page 7 states, "in future work we plan to expand our evaluation problem set, which at present is limited."
  - Why unresolved: Current experiments are restricted to low-dimensional (2D) parameter spaces and specific synthetic cases.
  - What evidence would resolve it: Evaluation of BPMI on benchmarks with dimensionality d > 5 and diverse synthetic geometries.

- Is the performance of BPMI on the laser-ignited rocket combustor statistically significant given the lack of repeated runs?
  - Basis in paper: [explicit] Page 10 notes, "Due to computational limitations the BPMI strategy was only run once."
  - Why unresolved: A single run cannot establish statistical significance or robustness against stochasticity compared to the baseline which was run three times.
  - What evidence would resolve it: Confidence intervals generated from running the BPMI strategy multiple times on the combustor application.

- Can the bi-fidelity BPMI framework be extended to handle multiple fidelities (three or more)?
  - Basis in paper: [inferred] The paper explicitly scopes the problem to a "bi-fidelity setting" (Section 1) and defines the autoregressive model for only two levels (f_H = ρ f_L + δ).
  - Why unresolved: The mathematical formulation for covariance matrices (Eq 9) and the Taylor expansion implementation are described specifically for two levels.
  - What evidence would resolve it: Derivation of the acquisition function for M fidelities and successful application on a chain of multi-fidelity simulations.

## Limitations

- The Taylor expansion approximation introduces uncertainty in regions where the latent function has high variance, potentially degrading MI estimates
- The method assumes monotonic submodularity throughout batch construction, though this guarantee technically applies to a fixed model rather than an iteratively retrained one
- The heuristic for determining sampling frequency is problem-dependent and may not generalize optimally across diverse applications

## Confidence

- Mechanism 1 (Taylor expansion approximation): **Medium** - The linearization is mathematically sound, but its accuracy depends on posterior characteristics that vary by problem
- Mechanism 2 (Derivative weighting for decision boundary focus): **High** - The mathematical relationship is explicit and the experimental results strongly support this claim
- Mechanism 3 (Greedy submodular optimization): **High** - This follows established theory for submodular maximization

## Next Checks

1. Test BPMI performance when the posterior variance is artificially increased to assess Taylor approximation breakdown conditions
2. Compare BPMI against a version without the derivative weighting term to quantify the contribution of decision boundary focus
3. Evaluate BPMI under varying fidelity cost ratios (e.g., 5:1, 20:1) to assess robustness to different budget structures