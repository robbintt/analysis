---
ver: rpa2
title: Constrained Auto-Regressive Decoding Constrains Generative Retrieval
arxiv_id: '2504.09935'
source_url: https://arxiv.org/abs/2504.09935
tags:
- retrieval
- distribution
- corpus
- relevant
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the theoretical limitations of generative
  retrieval (GR) models that use constrained auto-regressive decoding. The study focuses
  on two key aspects: (1) the impact of constraints imposed by downstream corpora,
  and (2) the use of beam search during generation.'
---

# Constrained Auto-Regressive Decoding Constrains Generative Retrieval

## Quick Facts
- arXiv ID: 2504.09935
- Source URL: https://arxiv.org/abs/2504.09935
- Reference count: 40
- Primary result: Theoretical analysis showing that corpus constraints and beam search introduce inherent limitations in generative retrieval models

## Executive Summary
This paper presents a theoretical analysis of generative retrieval (GR) models using constrained auto-regressive decoding, identifying fundamental limitations that constrain their performance. The authors examine an idealized Bayes-optimal GR model to demonstrate how two key factors—corpus-specific constraints and beam search decoding—create inherent mismatches between predicted and ground-truth distributions. Through rigorous mathematical analysis and empirical validation on both synthetic and real-world datasets (MS MARCO V1), the paper reveals that these constraints lead to KL divergence errors and poor recall performance, particularly in sparse relevance distributions. The findings suggest that achieving high recall while maintaining precision requires careful consideration of relevance distribution concentration during model design and training.

## Method Summary
The paper analyzes generative retrieval through the lens of an idealized Bayes-optimal model, focusing on two main limitations: marginal distribution mismatch caused by corpus constraints and poor recall resulting from beam search. The authors derive theoretical bounds on KL divergence errors when corpus-specific constraints create a mismatch between predicted and ground-truth distributions, with the error quantified by Simpson diversity index. For beam search limitations, they prove that while top-1 precision can be perfect, recall performance degrades significantly in sparse distributions. The analysis is validated through synthetic experiments with controlled relevance distributions and real-world experiments using SLIM++ relevance scores and semantic docIDs generated via Zeng et al.'s method on the MS MARCO V1 corpus.

## Key Results
- Corpus constraints create a theoretical lower bound on KL divergence between predicted and ground-truth marginal distributions, with error magnitude dependent on Simpson diversity index
- Beam search using marginal distributions achieves perfect top-1 precision but suffers from poor recall, especially when relevance distributions are sparse
- Real-world experiments on MS MARCO V1 confirm theoretical predictions, showing KL divergence increases as downstream corpus sampling rate decreases

## Why This Works (Mechanism)
None provided in the input.

## Foundational Learning
- **Bayes-optimal generative retrieval**: A theoretical model that makes optimal predictions given perfect knowledge of relevance distributions; needed to establish baseline performance limits and prove impossibility results
- **Simpson diversity index**: A measure of diversity in probability distributions; needed to quantify how concentration of relevance affects KL divergence bounds
- **Marginal distribution in auto-regressive decoding**: The probability distribution over tokens at each generation step; needed to understand how constraints affect prediction accuracy
- **Beam search pruning**: A heuristic search strategy that keeps only top-k candidates at each step; needed to explain why recall degrades in sparse distributions
- **KL divergence**: A measure of difference between probability distributions; needed to quantify the mismatch between predicted and ground-truth distributions

## Architecture Onboarding

**Component map:** Relevance distribution → DocID generation → Constrained auto-regressive decoding → Marginal distribution prediction → KL divergence/error calculation

**Critical path:** The generation of docIDs (defining the search space) → application of corpus constraints (defining valid branches) → auto-regressive decoding (making predictions) → beam search (pruning candidates) → final retrieval results

**Design tradeoffs:** The paper shows a fundamental tradeoff between precision and recall when using beam search in sparse relevance distributions. Corpus constraints improve precision by focusing search but increase KL divergence error. The theoretical analysis reveals that perfect top-1 precision comes at the cost of poor top-k recall in sparse settings.

**Failure signatures:** High KL divergence values indicate severe marginal distribution mismatch; poor recall@50 values indicate beam search failure to find relevant documents in sparse distributions; mismatch between theoretical bounds and empirical results suggests implementation issues in docID generation or relevance scoring

**3 first experiments:**
1. Reproduce synthetic KL divergence curves from Figure 1 using controlled uniform and non-uniform relevance distributions
2. Implement docID generation from Zeng et al. [61] and verify branching structure on small MS MARCO subset
3. Calculate first-step KL divergence and Recall@50 on sampled downstream corpora as described in Table 2

## Open Questions the Paper Calls Out

### Open Question 1
How do corpus constraints and beam search algorithms interact when applied simultaneously in generative retrieval?
Basis in paper: The authors state in the Limitations section: "We have not studied how the two factors affect each other when using constrained beam search."
Why unresolved: The theoretical analysis deliberately disentangled the two factors to derive individual error bounds, leaving the compound effect of their interaction on generalization unproven.
What evidence would resolve it: A unified theoretical bound or empirical study quantifying the compounding error when marginal distribution mismatch and pruning occur together.

### Open Question 2
How can the theoretical results regarding Bayes-optimal models be utilized to analyze the training properties of corpus-specific GR models?
Basis in paper: The Conclusion explicitly lists this as a direction: "As to future work, we will continue to study how these results can be used for analyzing training properties of corpus-specific GR models."
Why unresolved: The paper focused on the inference behavior of an idealized model to establish theoretical limits, rather than the optimization dynamics of practical, trained models.
What evidence would resolve it: Deriving specific training loss bounds or convergence rates for models like DSI or NCI based on the proposed inference-time error constraints.

### Open Question 3
Can learnable decoding strategies be effectively incorporated during the training of a differentiable search index?
Basis in paper: The Conclusion suggests that "incorporating learnable decoding strategies during the training of a differentiable search index may also be of interest in this field."
Why unresolved: The paper proves standard beam search leads to poor top-k recall but does not propose or evaluate adaptive alternatives to overcome this inherent limitation.
What evidence would resolve it: A new training objective that co-optimizes the index structure and decoding strategy, demonstrating improved recall on sparse distributions compared to standard constrained beam search.

## Limitations
- The theoretical analysis focuses on an idealized Bayes-optimal model rather than practical trained models
- Real-world validation relies on proxy relevance scores (SLIM++) which may not perfectly represent ground truth
- The compound effect of corpus constraints and beam search interaction is not studied
- Score extrapolation method for tail distributions is loosely specified

## Confidence

**High Confidence:** The theoretical derivations of KL divergence bounds and the asymptotic behavior of recall failures under beam search. These follow from mathematical analysis and are independent of experimental implementation details.

**Medium Confidence:** The synthetic experiments demonstrating the theoretical bounds. While the methodology is clear, small discrepancies may arise due to finite vocabulary sizes and implementation choices in the sampling process.

**Medium Confidence:** The real-world experimental results, primarily due to uncertainties in the ground-truth relevance distribution estimation and the docID construction methodology from the referenced work.

## Next Checks

1. **DocID Construction Validation:** Implement the hierarchical docID generation method from Zeng et al. [61] and verify that it produces the expected branching structure for a small subset of MS MARCO passages.

2. **KL Divergence Sensitivity Analysis:** Conduct ablation studies on the score extrapolation method by varying the Gaussian noise parameters and observing the impact on KL divergence values across different corpus sampling rates.

3. **Recall Failure Mode Verification:** Create controlled synthetic datasets with known relevance distributions (both concentrated and sparse) and systematically demonstrate the beam search recall failures predicted by the theoretical analysis.