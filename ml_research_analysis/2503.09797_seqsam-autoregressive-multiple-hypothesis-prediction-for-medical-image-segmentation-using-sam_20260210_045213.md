---
ver: rpa2
title: 'SeqSAM: Autoregressive Multiple Hypothesis Prediction for Medical Image Segmentation
  using SAM'
arxiv_id: '2503.09797'
source_url: https://arxiv.org/abs/2503.09797
tags:
- masks
- image
- multiple
- each
- mask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SEQSAM, a method for generating multiple
  plausible segmentation masks for medical images, addressing the challenge of inherent
  uncertainty in medical image annotation. SEQSAM builds on the Segment Anything Model
  (SAM) by introducing a recurrent module that sequentially generates masks using
  a single prediction head, rather than multiple parallel heads as in standard Multiple
  Choice Learning.
---

# SeqSAM: Autoregressive Multiple Hypothesis Prediction for Medical Image Segmentation using SAM

## Quick Facts
- arXiv ID: 2503.09797
- Source URL: https://arxiv.org/abs/2503.09797
- Authors: Benjamin Towle; Xin Chen; Ke Zhou
- Reference count: 0
- Primary result: Sequential generation with bipartite matching improves Dice score from 82.4 to 83.8 on LIDC-IDRI

## Executive Summary
SeqSAM introduces an autoregressive approach for generating multiple plausible segmentation masks for medical images, addressing inherent uncertainty in medical image annotation. The method builds on Segment Anything Model (SAM) by using a recurrent module to sequentially generate masks through a single prediction head, rather than multiple parallel heads. A key innovation is bipartite matching for credit assignment during training, ensuring each generated mask captures clinically relevant variations. Evaluated on LIDC-IDRI and QUBIQ Kidney datasets, SeqSAM shows notable improvements in both accuracy and distribution metrics compared to SAM-based baselines while demonstrating the ability to produce up to 10 masks without loss in quality.

## Method Summary
SeqSAM extends SAM with a recurrent module that sequentially generates masks using a single prediction head. At each timestep, the decoder produces logits which are passed through a recurrent module with hidden state H(m) to generate mask m. The hidden state tracks which masks have been generated to encourage diversity. Training uses bipartite matching to assign predictions to labels without requiring canonical ordering, combined with randomized strided pooling to enable arbitrary mask counts at inference. The model is evaluated on two multi-annotator medical imaging datasets using Dice score and Generalized Energy Distance metrics.

## Key Results
- Dice score improvement from 82.4 (SAM MCL) to 83.8 (SeqSAM) on LIDC-IDRI
- Generalized Energy Distance improves from 0.250 to 0.242 on LIDC-IDRI
- Maintains quality when generating up to 10 masks, compared to baseline degradation
- Bipartite matching provides more effective credit assignment than winner-takes-all approaches

## Why This Works (Mechanism)

### Mechanism 1: Sequential Generation with Memory Tracking
A hidden state H(m) is updated at each timestep via convolution over the concatenation of the previous hidden state and current logits mask (H(m+1) = conv-2([H(m), Z(m)])). This state maintains information about which masks have already been generated, preventing collapse to similar outputs. The sequential structure encourages the model to canonicalize its generated masks, distributing them across the modes present in the label distribution.

### Mechanism 2: Bipartite Matching for Credit Assignment
Set-based optimization via Hungarian matching ensures each predicted mask receives gradient signal from an appropriate label, preventing mode collapse. The loss function solves a linear assignment problem: L(Ŷ, Y) = min_π Σ_m L(ŷ(m), y_π(m)). Each prediction is assigned to exactly one label, enforcing coverage of all annotation modes. Dice loss combined with bipartite matching provides sufficient gradient diversity to train distinct outputs without requiring explicit ordering.

### Mechanism 3: Randomized Strided Pooling for Scalable Output Generation
When M > K, predictions are divided into K chunks (C(1)...C(K)). During training, one mask is randomly sampled from each chunk for loss computation; during inference, all M masks are retained. This encourages each chunk to specialize for a particular role. Chunking creates a canonical structure that survives the training-inference mismatch in sequence length, enabling inference with arbitrary mask counts beyond the training configuration.

## Foundational Learning

- **Concept: Winner-Takes-All (WTA) Loss in Multiple Choice Learning**
  - Why needed here: Understanding why standard MCL fails—sparse gradients cause one head to dominate, making other outputs clinically irrelevant.
  - Quick check question: Given 4 prediction heads with WTA loss, if head 1 achieves 85% Dice on all samples while others get 70%, what happens to heads 2-4 during training?

- **Concept: Bipartite Matching / Hungarian Algorithm**
  - Why needed here: Core to SeqSAM's credit assignment; must understand O(N³) assignment solves the label-permutation problem.
  - Quick check question: If you have 3 predictions and 3 labels with pairwise Dice losses in a 3×3 matrix, how does Hungarian matching differ from greedy assignment?

- **Concept: Backpropagation Through Time (BPTT)**
  - Why needed here: The recurrent module requires gradients to flow across timesteps; ablation shows removing BPTT from logits degrades performance.
  - Quick check question: What happens to gradient magnitude as it backpropagates through 10 timesteps without architectural mitigation?

## Architecture Onboarding

- **Component map:** Image → SAMenc (frozen) → Embedding E → SAMdec ← Prompt Encoder → Logits Z(m) → Recurrent Module H(m+1) = conv-2([H(m), Z(m)]) → Sequential masks {ŷ(1)...ŷ(M)}

- **Critical path:** Image encoding (once, expensive) → Decoder + Recurrent Module (M times, lightweight) → Bipartite matching loss during training

- **Design tradeoffs:**
  - Single head vs. multiple heads: Sequential generation adds latency proportional to M but enables arbitrary output counts
  - Frozen encoder vs. fine-tuning: Freezing limits adaptation but reduces parameter count and training time
  - Chunk-based sampling vs. direct training: Required when M > K; adds complexity but enables flexibility

- **Failure signatures:**
  - Mode collapse: All masks converge to similar outputs → check recurrent module gradient flow
  - Dominant early masks: Later masks degrade in quality → verify chunking strategy during training
  - Out-of-distribution outputs at inference: Generating M=10 when trained on K=3 without strided pooling → ensure training matches Section 2.5 protocol

- **First 3 experiments:**
  1. Reproduce baseline comparison: Run SAM MCL and SeqSAM with M=K=3 on LIDC-IDRI, verify Davg improvement (target: ~83.8 vs 82.4)
  2. Ablate recurrent memory: Disable hidden state updates (use zero H(m) for all timesteps), measure performance drop vs. full model
  3. Test arbitrary mask generation: Train with K=4 labels, inference with M=10, compare Davg and GED against M=4 to verify minimal degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SEQSAM be effectively adapted to datasets with only a single annotation per image, given the scarcity of multi-annotator medical data?
- Basis in paper: [explicit] The authors explicitly state, "one limitation of our approach is that it requires a dataset with multiple annotations per image, which are much rarer than single annotator datasets."
- Why unresolved: The current methodology relies on multi-annotator ground truth to perform the bipartite matching loss optimization, and it is unclear how the model would learn the distribution of plausible masks without multiple labels.
- What evidence would resolve it: A study evaluating SEQSAM's performance when trained on synthetic multi-label data generated from single-annotator datasets, or an adaptation of the loss function for single-label supervision.

### Open Question 2
- Question: Does the sequential generation process suffer from error accumulation or quality degradation when generating a significantly larger number of masks (e.g., M > 10)?
- Basis in paper: [inferred] The paper demonstrates success with M=10, but the autoregressive nature (RNN-inspired) suggests potential vulnerability to drift or collapse over longer sequences, which was not tested.
- Why unresolved: The empirical evaluation caps at M=10, leaving the stability of the "arbitrary number" claim for larger hypothesis sets unverified.
- What evidence would resolve it: Quantitative results (Dice/GED) and qualitative analysis of mask diversity for sequence lengths extending significantly beyond 10 (e.g., 20, 50).

### Open Question 3
- Question: How does SEQSAM perform when prompted with sparser inputs, such as single point clicks, rather than bounding boxes?
- Basis in paper: [inferred] The authors mention using bounding boxes because previous studies found them most effective, but SAM's utility lies in flexible prompting; the sequential module's stability under high-uncertainty (sparse prompt) conditions remains uncharacterized.
- Why unresolved: The experimental setup relies exclusively on bounding box prompts, leaving the model's robustness to weaker forms of interaction untested.
- What evidence would resolve it: Ablation studies comparing model performance and convergence stability when trained and inferred with point-based prompts versus box-based prompts.

## Limitations
- Critical dependence on SAM architecture, inheriting limitations with small structures and overfitting to high-frequency details
- Computational scalability concerns as inference time scales linearly with M for applications requiring many high-quality masks
- Generalization beyond multi-annotator datasets remains unproven for single-annotator or limited annotation diversity scenarios
- Bipartite matching scalability concerns with O(N³) Hungarian algorithm complexity for very large M

## Confidence
- High confidence: Sequential generation with recurrent memory tracking and bipartite matching credit assignment mechanisms are well-supported by ablation studies
- Medium confidence: Randomized strided pooling technique for enabling arbitrary mask generation is demonstrated empirically but lacks theoretical justification
- Low confidence: Clinical relevance and inter-observer variability capture claims are primarily supported by quantitative metrics rather than qualitative medical expert validation

## Next Checks
1. **Cross-dataset generalization test:** Evaluate SeqSAM on single-annotator medical datasets (e.g., BraTS with single ground truth per case) to verify the method still produces meaningful uncertainty estimates when label diversity is limited.

2. **Clinical expert validation:** Conduct a blinded study where radiologists assess whether SeqSAM-generated masks capture clinically relevant variations and whether they improve diagnostic decision-making compared to single SAM predictions.

3. **Large-scale inference benchmark:** Systematically evaluate performance degradation as M increases beyond training configuration (e.g., train with K=4, test with M=20, 50, 100) to establish practical limits of the arbitrary mask generation capability.