---
ver: rpa2
title: Addressing Imbalanced Domain-Incremental Learning through Dual-Balance Collaborative
  Experts
arxiv_id: '2507.07100'
source_url: https://arxiv.org/abs/2507.07100
tags:
- learning
- uni00000013
- class
- uni00000044
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of class imbalance in Domain-Incremental
  Learning (DIL), where models must adapt to evolving domains while preserving knowledge
  from previous tasks. The proposed Dual-Balance Collaborative Experts (DCE) framework
  tackles two critical issues: intra-domain class imbalance and cross-domain class
  distribution shifts.'
---

# Addressing Imbalanced Domain-Incremental Learning through Dual-Balance Collaborative Experts

## Quick Facts
- arXiv ID: 2507.07100
- Source URL: https://arxiv.org/abs/2507.07100
- Authors: Lan Li; Da-Wei Zhou; Han-Jia Ye; De-Chuan Zhan
- Reference count: 30
- Primary result: Achieves state-of-the-art performance on four benchmark datasets for class-imbalanced domain-incremental learning, particularly improving few-shot class accuracy

## Executive Summary
This paper addresses the critical challenge of class imbalance in Domain-Incremental Learning (DIL), where models must adapt to evolving domains while preserving knowledge from previous tasks. The proposed Dual-Balance Collaborative Experts (DCE) framework tackles two fundamental issues: intra-domain class imbalance and cross-domain distribution shifts. By employing frequency-aware expert networks with specialized loss functions and a dynamic expert selector trained on synthetic features, DCE achieves significant improvements over existing methods, particularly for few-shot classes.

## Method Summary
DCE employs a two-stage approach per domain: (1) Training three specialized experts using different loss functions—standard cross-entropy, balanced softmax, and inverse distribution loss—to address class imbalance within each domain, and (2) Training a dynamic expert selector using synthetic features generated from Gaussian sampling of historical class statistics. The framework uses a frozen ViT-B/16 backbone with VPT prompts trained only on the first task, and implements Oracle Approximating Shrinkage for stable covariance estimation in few-shot classes.

## Key Results
- Achieves state-of-the-art performance across four benchmark datasets (Office-Home, DomainNet, CORe50, CDDB-Hard)
- Demonstrates significant improvements on few-shot classes compared to existing DIL methods
- Shows effective mitigation of catastrophic forgetting while enabling knowledge transfer across domains
- Outperforms domain-specific prompt methods by providing more balanced outcomes across class frequencies

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Aware Loss Specialization
- **Claim:** Decoupling optimization via specialized experts reduces underfitting on rare classes when intra-domain class imbalance causes gradient conflicts
- **Mechanism:** Three parallel experts per domain use distinct loss functions: Standard Cross-Entropy for many-shot classes, Balanced Softmax to adjust for class priors, and Inverse Distribution Loss to emphasize few-shot classes
- **Core assumption:** Optimization trajectories for many-shot and few-shot classes are sufficiently distinct that monolithic training fails to converge on a shared optimal minima
- **Evidence anchors:** Section 4.1 describes the three loss functions and their purposes; Abstract confirms frequency-aware expert networks balance representation

### Mechanism 2: Probabilistic Feature Replay (Synthetic Rehearsal)
- **Claim:** Training the expert selector on synthetic features sampled from Gaussian distributions preserves historical decision boundaries without storing raw exemplars
- **Mechanism:** Class-wise means and covariances are computed using the frozen PTM, OAS is applied to stabilize covariance estimation, and pseudo-features are sampled from $\mathcal{N}(\mu, \Sigma)$ for balanced selector training
- **Core assumption:** PTM features are unimodal and Gaussian-like per class
- **Evidence anchors:** Section 4.2 describes the Gaussian sampling approach and OAS regularization; relies on cited assumption about PTM feature distributions

### Mechanism 3: Dynamic Collaborative Routing
- **Claim:** A soft-weighted selector allows new domain experts to correct old few-shot errors without overwriting old many-shot knowledge
- **Mechanism:** An MLP selector learns to output weights for all existing experts based on input features, trained on balanced synthetic dataset to route difficult samples to later domains while preserving many-shot knowledge
- **Core assumption:** New domains contain transferable semantic information that benefits few-shot classes from old domains
- **Evidence anchors:** Section 3.2 and 4.2 describe the shared prompt approach and the trade-off between preserving old knowledge and leveraging new data

## Foundational Learning

- **Concept:** Class-Balanced Losses (e.g., Balanced Softmax / Logit Adjustment)
  - **Why needed here:** Standard Cross-Entropy is biased; adjusting logits by log-priors mathematically negates data imbalance
  - **Quick check question:** Can you derive why adding $\log(p_y)$ to the logits allows the decision boundary to shift away from the minority class?

- **Concept:** Covariance Shrinkage Estimators (OAS)
  - **Why needed here:** OAS stabilizes covariance estimation for classes with very few samples; without it, synthetic features may be degenerate
  - **Quick check question:** Why is the empirical covariance matrix unreliable when the number of samples $n$ is less than the feature dimension $d$?

- **Concept:** Mixture of Experts (MoE) Routing
  - **Why needed here:** The dynamic expert selector is a soft router; understanding how to train a router to weigh multiple outputs is distinct from standard ensembling
  - **Quick check question:** In standard MoE, routers are often trained jointly with experts. Here, the router is trained after experts on synthetic data. Why does this order matter?

## Architecture Onboarding

- **Component map:** Input features -> Frozen ViT-B/16 backbone -> Three domain-specific experts (MLPs) -> Dynamic expert selector (MLP) -> Weighted expert logits
- **Critical path:** Task 1 Init: Train Prompts + Experts jointly → Subsequent Tasks: Freeze Prompt/Backbone → Train 3 new Experts → Update Global Stats → Generate Synthetic Data → Retrain Selector on Synthetic Data → Inference: Extract feature → Selector generates weights → Weighted sum of expert logits
- **Design tradeoffs:** Storage vs. Accuracy (domain-level covariance averaging saves space but may reduce class-specific fidelity); Shared vs. Isolated (soft shared space through router vs. hard task assignment)
- **Failure signatures:** Covariance Collapse (ill-conditioned covariance matrices); Selector Bias (favoring majority domain experts); Negative Transfer (rapid accuracy drop on old many-shot classes)
- **First 3 experiments:** 1) Sanity Check: Visualize t-SNE of frozen PTM features and compare with synthetic samples; 2) Ablation: Compare Universal expert vs. 3-expert setup on few-shot class performance; 3) Router Stress Test: Feed OOD data to selector and check weight distribution

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the approximation of averaging class-specific covariances into a single domain-level matrix affect the fidelity of synthetic features for few-shot classes?
- **Basis in paper:** Appendix B states that domain-level covariance averaging using OAS is used to reduce storage costs
- **Why unresolved:** Averaging statistics may smooth over distinct feature geometry of minority classes, potentially reducing synthetic feature effectiveness
- **What evidence would resolve it:** Comparative ablation study measuring selector accuracy and final classification performance with vs. without per-class covariance matrices

### Open Question 2
- **Question:** Does the dynamic expert selector scale effectively to long-sequence scenarios where output dimension ($3b$) grows linearly with tasks?
- **Basis in paper:** Section 4.2 defines selector output as $R^{3b}$ where $b$ is task index; experiments limited to 11 domains
- **Why unresolved:** Optimizing selector weights using only synthetic data may become unstable with expanding output space
- **What evidence would resolve it:** Performance evaluation on datasets with >50 domains, monitoring selector convergence speed and inference latency

### Open Question 3
- **Question:** How sensitive is the Gaussian sampling strategy to violations of unimodal feature distribution assumption?
- **Basis in paper:** Section 4.2 relies on observation that PTMs yield unimodal Gaussian feature distributions per class
- **Why unresolved:** Multi-modal or highly skewed distributions will produce unrepresentative synthetic features
- **What evidence would resolve it:** Analysis comparing performance with Gaussian Mixture Models vs. single Gaussian sampling

## Limitations
- The core assumptions about Gaussian feature distributions and inverse distribution loss effectiveness lack strong empirical validation
- Synthetic feature generation is vulnerable to covariance estimation errors for few-shot classes
- The paper does not specify the exact number of synthetic samples (K) per class, which could significantly impact performance
- The dynamic selector mechanism may be sensitive to domain ordering and could exhibit negative transfer in adversarial sequences

## Confidence
- **High confidence**: Framework's architectural design and general approach to handling class imbalance through specialized experts
- **Medium confidence**: Synthetic feature generation approach using OAS-regularized covariances (methodologically sound but relies on unverified assumptions)
- **Low confidence**: Specific implementation details of dynamic expert selector and its ability to prevent catastrophic forgetting across arbitrary domain sequences

## Next Checks
1. **Gaussian Assumption Verification**: Visualize t-SNE plots of frozen PTM features for multiple classes and compute statistical divergence between real and synthetic samples
2. **Synthetic Data Quality Test**: Perform ablation studies varying K (number of synthetic samples) and measure selector performance to determine sensitivity to synthetic data volume
3. **Catastrophic Forgetting Analysis**: Design experiments with adversarial domain sequences and monitor per-class accuracy drift to quantify selector's ability to prevent forgetting