---
ver: rpa2
title: 'Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise
  Semantic Similarity'
arxiv_id: '2506.00245'
source_url: https://arxiv.org/abs/2506.00245
tags:
- semantic
- arxiv
- similarity
- entropy
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Semantic Nearest Neighbor Entropy (SNNE),
  a novel uncertainty quantification method for large language models that addresses
  limitations in existing semantic entropy approaches. The key insight is that current
  methods fail to account for both intra-cluster similarity (spread within clusters)
  and inter-cluster similarity (distance between clusters) when generating longer
  one-sentence outputs, which is common in tasks like summarization and translation.
---

# Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise Semantic Similarity

## Quick Facts
- **arXiv ID:** 2506.00245
- **Source URL:** https://arxiv.org/abs/2506.00245
- **Reference count:** 30
- **Primary result:** SNNE achieves 0.83-0.84 AUROC on QA vs 0.79-0.80 for semantic entropy

## Executive Summary
This paper introduces Semantic Nearest Neighbor Entropy (SNNE), a novel uncertainty quantification method for large language models that addresses limitations in existing semantic entropy approaches. The key insight is that current methods fail to account for both intra-cluster similarity and inter-cluster similarity when generating longer one-sentence outputs, which is common in tasks like summarization and translation. SNNE computes uncertainty by measuring semantic similarities between all pairs of generated responses without requiring explicit clustering, using a LogSumExp aggregation to reduce outlier sensitivity. Extensive experiments across nine datasets, two recent LLMs (Phi3 and Llama3), and three text generation tasks demonstrate SNNE's superiority over existing methods, achieving AUROC scores of 0.83-0.84 compared to 0.79-0.80 for semantic entropy on QA tasks.

## Method Summary
SNNE computes uncertainty by generating n responses to a query, computing pairwise semantic similarities between all responses, then aggregating these similarities using LogSumExp to calculate entropy. The method avoids explicit clustering by directly measuring all pairwise relationships, capturing both how tightly responses cluster together and how different clusters relate. A white-box extension (WSNNE) incorporates token probabilities to weight responses by model confidence. The method uses ROUGE-L similarity by default, with τ=1.0 scale parameter found to be robust across tasks.

## Key Results
- SNNE achieves AUROC scores of 0.83-0.84 on QA tasks compared to 0.79-0.80 for semantic entropy
- For text summarization and translation, SNNE achieves PRR scores of 0.63-0.64 compared to 0.58-0.62 for semantic entropy
- The method is not sensitive to hyperparameter tuning and maintains consistent performance across different settings
- WSNNE (white-box version) consistently outperforms black-box SNNE by ~0.02-0.03 AUROC

## Why This Works (Mechanism)

### Mechanism 1
LogSumExp aggregation reduces sensitivity to outlier responses compared to summation-based approaches. Rather than summing pairwise similarities directly, LogSumExp acts as a smooth approximation to the maximum function, bounding each response's contribution by its strongest semantic neighbor and preventing low-quality samples from corrupting the uncertainty signal.

### Mechanism 2
Explicit pairwise similarity captures both intra-cluster spread and inter-cluster distance without requiring discrete clustering. Traditional semantic entropy loses information about how tightly packed responses are within clusters and how semantically distant different clusters are from each other. SNNE's full n×n similarity matrix naturally weights same-cluster pairs higher while still accounting for cross-cluster relationships.

### Mechanism 3
Token probability weighting in WSNNE adds model confidence signal to semantic similarity. The white-box extension weights each response's contribution by its normalized sequence probability, creating a hybrid signal that combines semantic diversity with the model's own confidence. High-probability responses that are semantically isolated contribute more to uncertainty, while low-probability outliers are downweighted.

## Foundational Learning

- **Concept:** Semantic Entropy (SE) and its clustering approach
  - **Why needed here:** SNNE is explicitly positioned as addressing SE's limitations. Understanding how SE clusters responses via bidirectional entailment and computes entropy over cluster probabilities is necessary to grasp what SNNE improves upon.
  - **Quick check question:** Given 10 responses that cluster into 3 groups with sizes [5, 3, 2], how would SE compute uncertainty vs. a scenario where all 10 fall in one cluster?

- **Concept:** Length-normalized sequence probability
  - **Why needed here:** WSNNE uses ˜P(a|q) = P(a|q)/len(a) to weight responses. Without understanding why raw probabilities favor shorter outputs, the normalization rationale is unclear.
  - **Quick check question:** Why does unnormalized sequence probability systematically disadvantage longer responses, and what does length normalization achieve?

- **Concept:** Similarity functions for text (ROUGE-L, NLI entailment, embeddings)
  - **Why needed here:** SNNE requires choosing f(aᵢ, aⱼ|q). The paper empirically compares ROUGE-L, NLI-based entailment scores, and cosine similarity of embeddings. Understanding their tradeoffs is essential for implementation.
  - **Quick check question:** For two paraphrases with different vocabulary but identical meaning, which similarity function would score highest: ROUGE-L, NLI entailment, or embedding cosine similarity?

## Architecture Onboarding

- **Component map:** Input: Question q, LLM, n (sample count), τ (scale), similarity function f → [Generator] → Sample n responses {a₁...aₙ} at temperature T=1.0 → [Similarity Matrix] → Compute n×n matrix S where Sᵢⱼ = f(aᵢ, aⱼ|q) → [LogSumExp Aggregation] → For each i: LSEᵢ = log(Σⱼ exp(Sᵢⱼ/τ)) → [Entropy Computation] → SNNE = -(1/n) Σᵢ LSEᵢ → [Optional: Probability Weighting] → WSNNE = -Σᵢ P̄(aᵢ|q) × LSEᵢ

- **Critical path:** The similarity function f is the single most impactful choice. Paper experiments show ROUGE-L outperforms NLI entailment and embeddings on summarization/translation tasks. Start with ROUGE-L as default.

- **Design tradeoffs:**
  - τ (scale factor): Low τ (→0) emphasizes maximum similarity only, approaching nearest-neighbor behavior. High τ smooths toward average pairwise similarity. Paper finds τ=1 robust across tasks.
  - Sample count n: More samples improve AUROC but linearly increase compute. Diminishing returns after n=10 at T=1.0.
  - Black-box vs. white-box: WSNNE requires logit access; SNNE needs only generated text. Paper shows WSNNE consistently higher AUROC (~0.02-0.03 improvement).
  - Similarity function choice: ROUGE-L is lexical, fast, no external model needed. NLI entailment captures semantics but requires DeBERTa inference. Embeddings require sentence transformer inference.

- **Failure signatures:**
  - Constant SNNE values: Check if all pairwise similarities are identical (e.g., very short responses, broken similarity function).
  - SNNE < 0 consistently: Verify similarity function f returns appropriate scale; ROUGE-L ∈ [0,1], embeddings may need normalization.
  - No correlation with correctness: If AUROC ≈ 0.5, the LLM may already be well-calibrated for the task, or correctness metric is misaligned with semantic equivalence.
  - Extreme sensitivity to τ: May indicate similarity values are on wrong scale; normalize f output.

- **First 3 experiments:**
  1. Reproduce SQuAD baseline: Generate 10 responses per question with Llama-3.1-8B at T=1.0, compute SNNE with ROUGE-L and τ=1, measure AUROC against F1 correctness. Target: ~0.83 per Table 2.
  2. Ablate similarity function: Same setup, compare ROUGE-L vs. NLI entailment (DeBERTa) vs. embedding cosine similarity (sentence transformer). Expect ROUGE-L ≈ best on longer outputs per Table 5.
  3. Validate τ sensitivity: Sweep τ ∈ {0.1, 1, 10, 100} on held-out subset. Confirm paper finding that τ=1 is robust; if not, check similarity function normalization.

## Open Questions the Paper Calls Out

### Open Question 1
How can SNNE be extended to multi-sentence or paragraph-length generations where uncertainty must be aggregated across sentences? The authors state in the Limitations section that they did not investigate uncertainty estimation in cases where the model generates multiple sentences or an entire paragraph, proposing that a naive approach would be to compute the uncertainty for each sentence independently and then aggregate these values into a single scalar.

### Open Question 2
What similarity functions are appropriate for non-natural-language outputs such as mathematical expressions, LaTeX equations, or code? The Limitations section states that for different data formats such as mathematical expressions, LaTeX equations, or code, the method requires further considerations and designing an appropriate similarity function could help generalize the approach to these types of data.

### Open Question 3
Can SNNE's pairwise similarity approach be integrated into LUQ to improve atomic sentence-level uncertainty scores? The paper notes that the method can be integrated into LUQ to provide more reliable atomic scores, offering a promising direction for extending the approach to multi-sentence settings, which they leave for future work.

## Limitations
- The method has not been tested on multi-sentence or paragraph-length generations, limiting its applicability to tasks requiring longer outputs
- SNNE's effectiveness on non-English languages and specialized domains (e.g., legal, medical) remains untested
- The method requires generating multiple responses per query, which increases computational cost linearly with sample count

## Confidence

- **High Confidence:** SNNE outperforms semantic entropy on the tested datasets and tasks, with statistically significant improvements in AUROC and PRR metrics. The LogSumExp aggregation mechanism is sound and well-supported by theoretical and empirical evidence.
- **Medium Confidence:** SNNE's superiority generalizes beyond the tested LLMs and tasks. The method is not sensitive to hyperparameter tuning (τ=1 is robust). Token probability weighting meaningfully improves uncertainty estimates across diverse scenarios.
- **Low Confidence:** SNNE maintains effectiveness for extremely short responses (1-2 words) where discrete clustering might suffice. The method's performance on non-English languages and specialized domains matches English general-domain results.

## Next Checks

1. **Cross-model generalization test:** Apply SNNE to three additional LLMs with varying architectures (e.g., Mistral, Gemma, and a smaller model like DistilBERT) on the same QA, summarization, and translation tasks. Measure whether the ~0.83 AUROC on QA and ~0.63 PRR on TS/MT are replicated, or if performance degrades significantly with model size or architecture changes.

2. **Temperature robustness analysis:** Systematically evaluate SNNE across the full temperature range (T=0.1 to T=2.0) on a held-out dataset. Plot AUROC vs. temperature for both SNNE and semantic entropy to identify if SNNE maintains its advantage at deterministic generation (T→0) or becomes sensitive to temperature extremes.

3. **Domain and language extension:** Test SNNE on non-English datasets (e.g., MLQA for multilingual QA, or WMT datasets for additional language pairs) and specialized domains (e.g., PubMed abstracts for medical summarization). Compare performance against semantic entropy to determine if the method's effectiveness is language-agnostic and domain-invariant, or if it requires adaptation for different linguistic structures.