---
ver: rpa2
title: Enhancing Multi-Label Emotion Analysis and Corresponding Intensities for Ethiopian
  Languages
arxiv_id: '2503.18253'
source_url: https://arxiv.org/abs/2503.18253
tags:
- emotion
- intensity
- language
- languages
- multi-label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the EthioEmo dataset by annotating emotion intensity
  for four Ethiopian languages (Amharic, Oromo, Somali, Tigrinya), enabling more nuanced
  emotion analysis. Emotion intensity is annotated on a 4-level scale (0-3) by native
  speakers, and cross-lingual experiments are conducted to explore knowledge transfer.
---

# Enhancing Multi-Label Emotion Analysis and Corresponding Intensities for Ethiopian Languages

## Quick Facts
- arXiv ID: 2503.18253
- Source URL: https://arxiv.org/abs/2503.18253
- Reference count: 23
- Primary result: African language-centric PLMs outperform LLMs for Ethiopian emotion analysis, with AfroXLM-R achieving state-of-the-art results

## Executive Summary
This paper extends the EthioEmo dataset by annotating emotion intensity for four Ethiopian languages (Amharic, Oromo, Somali, Tigrinya), enabling more nuanced emotion analysis. Emotion intensity is annotated on a 4-level scale (0-3) by native speakers, and cross-lingual experiments are conducted to explore knowledge transfer. Benchmarking is performed using multilingual PLMs (LaBSE, RemBERT, mBERT, mDeBERTa, XLM-RoBERTa, AfriBERTa, AfroLM, AfroXLM-R, EthioLLM) and open-source LLMs (Qwen2.5-72B, Dolly-v2-12B, Llama-3-70B, Mistral-8x7B, DeepSeek-R1-70B). PLMs, especially African language-centric models, outperform LLMs in both multi-label emotion classification and intensity prediction. Cross-lingual transfer is effective, particularly between Amharic and Tigrinya (same script), with AfroXLM-R achieving the best results. Intensity annotation improves decision-making in downstream tasks like mental health monitoring. Limitations include subjective annotation and the need for more annotators. The enriched dataset will be publicly released.

## Method Summary
The authors extend the EthioEmo dataset by adding emotion intensity annotations on a 4-level scale (0-3) for four Ethiopian languages. Native speakers annotated each emotion label with intensity values, which were aggregated using threshold-based averaging formulas. They benchmarked multilingual PLMs (AfroXLM-R, AfriBERTa, EthioLLM, mBERT, mDeBERTa, XLM-RoBERTa) and open-source LLMs (Qwen2.5-72B, Llama-3-70B, Mistral-8x7B, DeepSeek-R1-70B) using fine-tuning for PLMs and zero-shot inference for LLMs. Cross-lingual experiments evaluated knowledge transfer between languages, particularly between script-related languages (Amharic and Tigrinya). Evaluation metrics included Macro-F1 for emotion classification and Pearson correlation for intensity prediction.

## Key Results
- African language-centric PLMs (especially AfroXLM-R) significantly outperform general multilingual models and LLMs for Ethiopian emotion analysis
- Cross-lingual transfer is most effective between languages sharing the same script system (Amharic and Tigrinya both use Ge'ez script)
- Intensity annotation improves decision-making for downstream applications like mental health monitoring
- PLMs achieve substantially higher Pearson correlation scores for intensity prediction compared to LLMs

## Why This Works (Mechanism)

### Mechanism 1
African language-centric pre-trained models outperform general multilingual models and LLMs for Ethiopian emotion tasks because they develop representations that better capture linguistic patterns, morphological structures, and cultural expressions of emotion that differ from high-resource languages. Fine-tuning adapts these representations to the subjective task of emotion and intensity prediction.

### Mechanism 2
Cross-lingual transfer is more effective between languages sharing the same script system because shared orthography reduces the representation gap in embedding space, allowing models to transfer learned emotion patterns more effectively across languages. Amharic and Tigrinya both use the Ge'ez script, creating structural alignment during fine-tuning.

### Mechanism 3
Intensity annotation granularity improves downstream decision-making for sensitive applications because four-level intensity scales (0-3) capture graded emotional strength, enabling models to distinguish between subtle distress signals and severe expressions—critical for prioritization in mental health or crisis detection systems.

## Foundational Learning

- **Multi-label classification with intensity**: Needed because texts can express multiple emotions simultaneously (e.g., joy + sadness), each with different intensity levels. Quick check: Given "I'm thrilled about the promotion but terrified of the responsibility," which emotions are present and what intensity would you assign each?

- **Cross-lingual transfer learning**: Needed to evaluate training on some languages and testing on held-out languages to measure knowledge transfer, particularly between script-related languages. Quick check: Why might Amharic → Tigrinya transfer work better than Amharic → Oromo transfer based on the paper's findings?

- **Encoder-only PLMs vs. Decoder-only LLMs for low-resource languages**: Needed because the paper shows fine-tuned PLMs (AfroXLM-R) outperform zero-shot LLMs (Llama-3-70B, Qwen2.5) on Ethiopian languages. Quick check: What advantage does fine-tuning provide over zero-shot prompting for subjective tasks in low-resource languages?

## Architecture Onboarding

- **Component map**: EthioEmo dataset → native annotators → intensity labels → African-centric PLM (AfroXLM-R) → fine-tuning → multi-label output + intensity regression → Macro-F1 and Pearson correlation evaluation

- **Critical path**: 1) Obtain native speaker annotators (minimum 3 per language; 5 for Amharic) 2) Annotate intensity for each emotion label using 4-level scale 3) Aggregate labels using threshold-based averaging formulas 4) Fine-tune African-centric PLM (recommend AfroXLM-R-76L) 5) Evaluate with held-out test sets; for cross-lingual, exclude target language from training

- **Design tradeoffs**: Annotation quality vs. cost (more annotators improve reliability but increase expense); model selection (AfroXLM-R best performance vs. EthioLLM smaller vs. general multilingual models worse); task formulation (joint multi-label + intensity vs. staged detection)

- **Failure signatures**: Zero or near-zero Pearson correlation on intensity (model failing to capture graded emotion strength); large gap between monolingual and cross-lingual performance (transfer not working; check script alignment); LLM outputting generic or misaligned emotions (zero-shot prompts not providing sufficient cultural context)

- **First 3 experiments**: 1) Baseline replication: Fine-tune AfroXLM-R-76L on Amharic multi-label emotion + intensity; report Macro-F1 and Pearson correlation to confirm SOTA (~68% F1) 2) Cross-lingual probe: Train on Amharic + Oromo + Somali, test on Tigrinya; compare against training on all languages except Tigrinya to isolate script effect 3) Ablation on annotation granularity: Train models using only binary emotion labels vs. full intensity annotations; measure impact on downstream task simulation

## Open Questions the Paper Calls Out

1. **Does modeling annotator-level disagreement directly improve emotion classification and intensity prediction compared to majority-vote aggregation for Ethiopian languages?**
   - Basis: The authors recommend modeling annotator-level data instead of majority vote but acknowledge this hasn't been evaluated yet
   - Why unresolved: The paper uses majority vote to determine final labels but annotator-level data will be released for future modeling
   - What evidence would resolve it: Experiments comparing performance trained on majority-vote labels versus methods incorporating annotator disagreement

2. **Can explicit emotion intensity features measurably improve performance on downstream tasks such as mental health monitoring or prioritized crisis detection?**
   - Basis: The paper claims intensity is significant for decision-making in healthcare but provides no empirical validation
   - Why unresolved: Intensity prediction is benchmarked as standalone task without evaluation on actual downstream applications
   - What evidence would resolve it: Comparative study showing models with intensity predictions achieve higher precision/recall in triaging high-risk content

3. **What architectural or prompting improvements would enable open-source LLMs to match PLM performance on emotion intensity prediction for low-resource Ethiopian languages?**
   - Basis: LLMs (including 70B parameter models) perform substantially worse than PLMs on intensity prediction (33.93% vs 60.24% for Amharic)
   - Why unresolved: The paper benchmarks existing models but doesn't investigate causes of LLM underperformance
   - What evidence would resolve it: Ablation studies on prompting strategies, few-shot examples, or fine-tuning approaches that narrow performance gap

## Limitations
- Annotation subjectivity and reliability concerns due to the subjective nature of emotion perception, with no explicit reporting of inter-annotator agreement metrics
- Data split transparency issues as exact train/dev/test split ratios and random seeds are not specified
- Architectural ambiguity regarding whether intensity is modeled as regression or ordinal classification despite using Pearson correlation metric

## Confidence

**High Confidence**:
- African language-centric PLMs outperform general multilingual models and LLMs (directly supported by experimental results)

**Medium Confidence**:
- Cross-lingual transfer is more effective between languages sharing the same script (correlational evidence, not fully controlled for other factors)

**Low Confidence**:
- Intensity annotation granularity improves downstream decision-making for sensitive applications (not empirically validated within the study)

## Next Checks

1. **Inter-Annotator Agreement Analysis**: Calculate and report inter-annotator agreement metrics (Krippendorff's alpha, average pairwise Cohen's kappa) for intensity annotations across all four languages to quantify label reliability.

2. **Controlled Cross-Lingual Transfer Experiment**: Design experiment isolating script similarity effect by comparing Amharic→Tigrinya (same script) against Amharic→Oromo (different script) while controlling for training data size and linguistic similarity.

3. **Downstream Task Simulation with Intensity Labels**: Implement triage system for mental health monitoring using high-intensity negative emotions and compare performance against baseline system using only binary emotion labels, measuring precision, recall, and F1-score.