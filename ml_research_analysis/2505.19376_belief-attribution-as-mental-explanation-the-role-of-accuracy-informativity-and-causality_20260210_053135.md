---
ver: rpa2
title: 'Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity,
  and Causality'
arxiv_id: '2505.19376'
source_url: https://arxiv.org/abs/2505.19376
tags:
- causal
- belief
- beliefs
- statement
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how people selectively attribute beliefs
  to others as explanations for behavior, a fundamental aspect of human theory-of-mind.
  The authors develop a computational model that evaluates the explanatory strength
  of natural language belief statements using three factors: accuracy, informativity,
  and causal relevance.'
---

# Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity, and Causality

## Quick Facts
- arXiv ID: 2505.19376
- Source URL: https://arxiv.org/abs/2505.19376
- Reference count: 6
- Primary result: Causal relevance is the single best predictor of human belief attribution (r = 0.81), outperforming accuracy-informativity combinations (r = 0.68)

## Executive Summary
This paper investigates how people selectively attribute beliefs to explain others' behavior, developing a computational model that evaluates explanatory strength using accuracy, informativity, and causal relevance. The model uses a probabilistic generative framework to assess how likely belief statements are true, how informative they are to listeners, and how causally relevant they are to observed actions. When tested against human judgments in gridworld scenarios, causal relevance alone best predicted belief attribution (r = 0.81), while accuracy and informativity combined provided reasonable but weaker predictions (r = 0.68). The findings highlight that people naturally prefer beliefs that causally explain actions, especially when not explicitly prompted to communicate with a listener.

## Method Summary
The method uses a language-augmented Bayesian Theory of Mind (LaBToM) framework to rank natural language belief statements about an agent's knowledge. The model computes three explanatory factors: accuracy (posterior probability of the belief being true given actions and observations), informativity (KL divergence measuring how much the belief reduces listener uncertainty), and causal relevance (combination of causal necessity, sufficiency, and normality under hypothetical interventions). These factors are combined using log-linear scoring with fitted coefficients, and rankings are generated using Luce choice probabilities. The approach was tested on 18 gridworld scenarios where participants watched an agent navigate puzzles and ranked belief statements about the agent's knowledge.

## Key Results
- Causal relevance alone achieved r = 0.81 correlation with human belief attribution rankings, outperforming all other factor combinations
- Accuracy and informativity combined achieved r = 0.68 correlation, with higher accuracy-informativity correlations when listener visibility was explicitly considered
- Human split-half correlation was r = 0.94, establishing a high benchmark for model performance
- Causal relevance was particularly dominant when participants weren't explicitly prompted to communicate with a listener

## Why This Works (Mechanism)

### Mechanism 1: Causal Relevance Dominates Belief Attribution
People preferentially attribute beliefs that are causally relevant to observed actions. Causal relevance combines causal necessity (actions wouldn't occur if belief were false), causal sufficiency (actions would occur if belief were true), and normality (prior probability). The formula `Causal(φ,t) = [(1-CNorm) × CNecc]^α × [CNorm × CSuff]^α` captures how abnormal causes matter when multiple are necessary, while normal causes matter when multiple are sufficient. This works because agents are approximately Boltzmann-rational, enabling counterfactual predictions under belief interventions. Evidence: causal alone achieves r=0.81 (Table 1), vs. accuracy r=0.36 and informativity r=0.43. Break condition: highly boundedly-rational behavior violates the Boltzmann assumption.

### Mechanism 2: Accuracy-Informativity Trade-off Captures Communicative Intent
Combined accuracy and informativity predict belief attribution reasonably well (r=0.68), suggesting people partly attribute beliefs as if communicating to an informed listener. Accuracy is posterior probability P(φt = T | A1:t, O1:t), while informativity is KL divergence measuring belief statement helpfulness. Combined via log-linear scoring. Evidence: Acc+Info r=0.68; when listener has no visibility (Info*), correlation drops to r=0.54. Break condition: strong accuracy-informativity conflicts where the model struggles to match human preferences.

### Mechanism 3: Probabilistic Ranking via Explanatory Score
Belief attribution is modeled as probabilistic choice over candidate statements using explanatory strength scores. Each statement receives a composite score from weighted explanatory factors, with attribution probability following Luce choice: `P_attr(φ; Φ) = exp(Score(φ)) / Σ exp(Score(φ'))`. Rankings are computed as products of sequential choice probabilities. Evidence: model-human correlation plots show clustering at high-ranked statements. Break condition: disjunctive statements with multiple readings break the single-reading assumption.

## Foundational Learning

- **Concept: Bayesian Theory of Mind (BToM)**
  - Why needed here: The entire model builds on BToM's generative model of how beliefs drive actions. Without this, you can't compute posteriors, counterfactuals, or KL divergences.
  - Quick check question: Given an agent who walks toward box A instead of box B, what belief update does BToM predict?

- **Concept: Causal Necessity vs. Sufficiency**
  - Why needed here: The causal relevance metric combines these asymmetrically. Understanding when each matters (multiple necessary causes → prefer abnormal; multiple sufficient → prefer normal) is essential for the mechanism.
  - Quick check question: If an agent would reach the goal regardless of which box they check first, is belief about box contents causally necessary, sufficient, both, or neither?

- **Concept: KL Divergence / Information Gain**
  - Why needed here: Informativity is formalized as KL divergence between listener's prior and posterior over agent beliefs. This captures "helpfulness" of a belief statement.
  - Quick check question: A statement that's always true (tautology) has what informativity value?

## Architecture Onboarding

- **Component map:** Generative Model (BToM core) → Epistemic Language-of-Thought Parser → Explanatory Factor Compute → Ranking Engine
- **Critical path:** Action observations → BToM posterior inference → For each candidate statement: compute Acc, Info, Causal → Combine scores → Generate ranking distribution
- **Design tradeoffs:** Boltzmann rationality vs. bounded rationality (current assumes optimality; human-like errors require additional modeling); single vs. multiple statement readings (disjunctions are ambiguous); listener visibility assumptions (wrong assumption → worse fit)
- **Failure signatures:** Low correlation on disjunctive statements (ambiguity); divergence when agents pursue visibly suboptimal subgoals (bounded rationality violation); high uncertainty on mid-ranked statements
- **First 3 experiments:**
  1. Implement BToM generative model for gridworld, compute three factors for paper's 18 scenarios, verify r≈0.81 for Causal model
  2. Test factor combinations on held-out scenarios; confirm Acc+Info captures different variance than Causal (their r=0.81 correlation suggests partial overlap)
  3. Design scenarios where accuracy-informativity strongly conflict with causal relevance; measure where each model fails

## Open Questions the Paper Calls Out

- **Open Question 1:** Which formalization of causal relevance best captures people's intuitions about mental state causation—necessity-sufficiency models or counterfactual effect-size models?
  - Basis: Unclear which formulation best captures people's causal intuitions; more careful experimental design might distinguish formulations
  - Why unresolved: Current experiment not designed to distinguish competing causal relevance formalizations
  - What evidence would resolve it: Targeted experiments with scenarios producing divergent predictions under different formalizations

- **Open Question 2:** Do belief attribution preferences generalize across different types of mental states (e.g., beliefs vs. desires) and different social contexts?
  - Basis: Future research could investigate preferences across mental state types or social contexts with different pragmatic demands
  - Why unresolved: Current study only examined belief attribution in puzzle-solving context
  - What evidence would resolve it: Experiments using analogous paradigms for desire attribution and manipulating social context

- **Open Question 3:** Can integrating ambiguity-aware parsing and boundedly-rational planning improve model predictions for human belief attribution?
  - Basis: Future work could integrate ambiguity-aware parsing and boundedly-rational planning to potentially improve model fit
  - Why unresolved: Current model assumes single valid reading and Boltzmann-rational agents, but Scenario D showed prediction failures
  - What evidence would resolve it: Model comparison showing whether incorporating multiple readings and sub-optimal planning yields higher correlations

## Limitations
- Controlled gridworld environment may not capture richer real-world belief attribution contexts
- Reliance on Boltzmann-rational agent assumptions breaks down with boundedly-rational behavior
- ELoT parsing of natural language may not scale to more complex belief statements
- Small sample of 18 scenarios limits generalizability claims

## Confidence

- **High confidence:** Causal relevance mechanism is the dominant factor in human belief attribution
- **Medium confidence:** Accuracy-informativity mechanism captures communicative aspects but performs less robustly
- **Low confidence:** Model's handling of disjunctive statements and ambiguous readings

## Next Checks

1. Test the model on scenarios with visibly suboptimal agent behavior to measure breakdown under bounded rationality
2. Design scenarios where accuracy-informativity conflict strongly with causal relevance to identify model failure modes
3. Validate the ELoT parsing mechanism independently on belief statement interpretation tasks