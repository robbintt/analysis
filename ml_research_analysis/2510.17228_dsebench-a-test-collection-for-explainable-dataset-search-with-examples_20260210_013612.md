---
ver: rpa2
title: 'DSEBench: A Test Collection for Explainable Dataset Search with Examples'
arxiv_id: '2510.17228'
source_url: https://arxiv.org/abs/2510.17228
tags:
- https
- dataset
- datasets
- test
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DSEBench, the first test collection for Explainable
  Dataset Search with Examples (ExDSE). The authors address the limitation of current
  dataset search approaches that cannot handle queries combining keywords and target
  datasets as examples.
---

# DSEBench: A Test Collection for Explainable Dataset Search with Examples

## Quick Facts
- arXiv ID: 2510.17228
- Source URL: https://arxiv.org/abs/2510.17228
- Reference count: 40
- Primary result: First test collection for Explainable Dataset Search with Examples (ExDSE)

## Executive Summary
This paper introduces DSEBench, the first test collection for Explainable Dataset Search with Examples (ExDSE). The authors address the limitation of current dataset search approaches that cannot handle queries combining keywords and target datasets as examples. They construct DSEBench by extending the NTCIR dataset collection with human annotations for query relevance and target similarity at the dataset level, plus field-level annotations for explainability. To address data scarcity, they also generate synthetic training cases using a fine-tuned T5 model and LLM annotations. Comprehensive baselines are established by adapting retrieval, reranking, and explanation methods.

## Method Summary
DSEBench extends the NTCIR-15 English dataset collection with human and LLM annotations for query relevance and target similarity. Synthetic queries are generated using a fine-tuned T5-base model, and annotations are produced at scale using GLM-3-Turbo with heuristic filtering. The dataset includes 141 human-annotated test cases and 5,699 LLM-annotated training cases. Retrieval baselines include lexical (BM25, TF-IDF) and dense (BGE, GTE) models, with fine-tuned ColBERTv2 and coCondenser showing substantial gains. Reranking is performed using text-based cross-encoders, structure-based GNNs, and multi-layer LLM methods. Explainability is evaluated by identifying indicator fields using post-hoc methods like SHAP and LIME.

## Key Results
- Dense retrieval models (BGE, GTE) outperform lexical models on DSEBench
- Fine-tuning dense retrievers yields substantial performance gains
- Multi-layer LLM reranking achieves highest scores (MAP@10: 0.2398, NDCG@10: 0.4451)
- SHAP and few-shot LLM methods perform best for identifying indicator fields
- Description field is most frequently selected as indicator field

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic query generation combined with LLM annotations can create effective training data for the DSE task when human annotations are scarce.
- Mechanism: A fine-tuned T5-base model generates queries from dataset titles and descriptions; GLM-3-Turbo annotates relevance, similarity, and indicator fields at scale; heuristic filtering removes low-quality annotations (trivial, verbose, or vague queries; inconsistent relevance/similarity labels). This produces 5,737 training cases from an original 672 human-labeled cases.
- Core assumption: LLM annotations approximate human judgment sufficiently for weak supervision, and synthetic queries capture realistic information needs.
- Evidence anchors:
  - [abstract] "To address data scarcity, they also generate synthetic training cases using a fine-tuned T5 model and LLM annotations."
  - [section 3.2] "We fine-tuned a T5-base model... to generate a query q from the title and description of each dataset."
  - [section 4.3] "The performance difference between the five-fold and annotator splits is minimal, suggesting that the large-scale LLM annotations in DSEBench are almost as effective as human annotations for training."
- Break condition: If target datasets involve domain-specific vocabulary or nuanced similarity concepts not captured by GLM-3-Turbo's training, annotation quality may degrade significantly.

### Mechanism 2
- Claim: Multiplicative combination of relevance and similarity scores enforces a stricter joint constraint than additive or independent evaluation.
- Mechanism: Relevance (0-2) and similarity (0-2) are multiplied to produce a combined score (0, 1, 2, or 4). A candidate must be highly relevant (2) AND highly similar (2) to achieve the maximum score (4); any low score in either dimension disproportionately penalizes the overall score.
- Core assumption: Query relevance and target similarity are equally important and independent dimensions; their product is a meaningful proxy for joint utility.
- Evidence anchors:
  - [section 4.2] "Since query relevance and target similarity are indispensable, their product was taken as the gold-standard graded label... to ensure that dc receives a high overall score only if it is highly relevant to q and highly similar to Dt."
- Break condition: If users in practice weight relevance and similarity differently, the multiplicative formulation may over-penalize candidates with one strong dimension.

### Mechanism 3
- Claim: Multi-layer LLM reranking with ensemble voting reduces context-length constraints and improves robustness over single-pass reranking.
- Mechanism: Datasets are repeatedly (20 times) divided into groups of 5; each group is reranked; datasets appearing frequently in the top half of reranked groups receive higher final scores. This limits context length per inference and aggregates across multiple orderings.
- Core assumption: Group-wise relative comparisons are more reliable than whole-corpus ranking; frequency of top-half appearances correlates with true relevance.
- Evidence anchors:
  - [section 4.4] "Multi-layer [47] which repeatedly (20 times) divides the datasets into five groups and reranks each group, and then reranks all datasets by their frequency of appearing in the top half of a reranked group."
  - [table 5] LLM (multi-layer) achieves MAP@10: 0.2398, NDCG@10: 0.4451—highest across all reranking configurations.
- Break condition: Computational cost scales linearly with iterations (20× single-pass cost); for very large candidate sets or real-time systems, latency may become prohibitive.

## Foundational Learning

- **Graded relevance and similarity judgments (0/1/2 scales)**
  - Why needed here: DSEBench extends binary relevance to a 3-level scale for both query relevance and target similarity; evaluation metrics (NDCG@k) require understanding graded relevance.
  - Quick check question: If a candidate is "partially relevant" (1) and "highly similar" (2), what is its combined score under DSEBench's formulation?

- **Post-hoc feature attribution (SHAP, LIME)**
  - Why needed here: ExDSE requires identifying which fields (title, description,