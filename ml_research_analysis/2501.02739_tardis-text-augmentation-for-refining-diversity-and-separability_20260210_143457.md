---
ver: rpa2
title: 'TARDiS : Text Augmentation for Refining Diversity and Separability'
arxiv_id: '2501.02739'
source_url: https://arxiv.org/abs/2501.02739
tags:
- class
- data
- examples
- target
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# TARDiS : Text Augmentation for Refining Diversity and Separability

## Quick Facts
- arXiv ID: 2501.02739
- Source URL: https://arxiv.org/abs/2501.02739
- Authors: Kyungmin Kim; SangHun Im; GiBaeg Kim; Heung-Seon Oh
- Reference count: 12
- Primary result: Proposed TARDiS framework improves few-shot text classification accuracy via LLM-based augmentation and alignment

## Executive Summary
TARDiS introduces a two-stage text augmentation framework for few-shot text classification that combines generative diversity with alignment-based separability refinement. The method uses a large language model (Llama2-13b-chat) to generate augmented examples, followed by three alignment modules—Spark Examples Generation (SEG), Class-Edge Generation (CEG), and Clarification Augmentation (CA)—to improve class distinctiveness and reduce ambiguity. Experimental results show consistent accuracy improvements over baseline few-shot methods across multiple benchmark datasets.

## Method Summary
TARDiS operates in two stages: generation and alignment. First, seed examples are augmented using Llama2-13b-chat with a repetition penalty of 1.15, generating 50 examples per class for SEG and CEG (25 for 2-shot TREC6). The alignment stage includes SEG (generating spark thoughts and examples to diversify), CEG (identifying ambiguous class pairs and generating discriminative text), and CA (verifying and modifying misaligned examples via retrieval). A sentence-transformers/all-mpnet-base-v2 model provides embeddings for similarity and retrieval. Classification is performed by fine-tuning RoBERTa-base/large or BERT-base for 4,000 steps (5/10-shot) or 1,000 steps (2-shot), using the [CLS] token.

## Key Results
- TARDiS consistently outperforms baseline few-shot methods (e.g., PromptMix, CPFT) on BANKING77, CLINC150, HWU64, and TREC6 datasets.
- Accuracy gains are attributed to improved class separability and diversity via LLM-based augmentation and alignment modules.
- The method demonstrates robust performance across varying shot settings (2, 5, and 10 shots).

## Why This Works (Mechanism)
TARDiS improves few-shot text classification by addressing the core challenge of limited and ambiguous training data. The LLM-based augmentation generates diverse examples, while the alignment modules (SEG, CEG, CA) refine these examples to enhance class separability and reduce ambiguity. SEG diversifies by generating multiple perspectives (spark thoughts), CEG targets ambiguous class pairs to generate discriminative text, and CA verifies and corrects misaligned examples via retrieval and LLM-based modification. This two-stage process ensures that augmented data is both diverse and class-distinctive, leading to better generalization.

## Foundational Learning
- **Few-shot text classification**: Learning from very limited labeled examples per class; needed to evaluate TARDiS in low-data regimes; quick check: confirm seed splits match DialoGLUE.
- **LLM-based text augmentation**: Using large language models to generate synthetic training examples; needed for scalable augmentation; quick check: verify Llama2-13b-chat with repetition penalty is used.
- **Sentence transformer embeddings**: Dense vector representations for semantic similarity and retrieval; needed for CEG and CA alignment; quick check: ensure all-mpnet-base-v2 is used for embeddings.
- **Fine-tuning classifiers**: Adapting pre-trained models (RoBERTa/BERT) on augmented data; needed for final classification; quick check: confirm training steps (4,000 or 1,000) and [CLS] token usage.
- **Class ambiguity detection**: Identifying similar or overlapping classes to improve separability; needed for CEG; quick check: verify top-5 ambiguous class selection.
- **Retrieval-augmented verification**: Using nearest-neighbor retrieval to find and correct misaligned examples; needed for CA; quick check: ensure SBERT retrieval is used for verification.

## Architecture Onboarding

**Component Map:**
Seed Examples -> LLM Augmentation (SEG, CEG, CA) -> Augmented Dataset -> Classifier Fine-tuning (RoBERTa/BERT) -> Test Accuracy

**Critical Path:**
Seed examples → SEG (spark thoughts + examples) → CEG (ambiguous class detection + discriminative text) → CA (retrieval + verification/modification) → fine-tuning classifier → evaluation

**Design Tradeoffs:**
- Using LLM (Llama2-13b-chat) enables high-quality, diverse augmentation but introduces API dependency and cost.
- SBERT embeddings facilitate fast similarity and retrieval but may not capture all semantic nuances.
- Fine-tuning pre-trained classifiers (RoBERTa/BERT) leverages strong representations but requires careful hyperparameter selection.

**Failure Signatures:**
- Low diversity in augmented examples: may indicate insufficient spark thoughts or overly restrictive generation parameters.
- High misalignment rate after CA: could result from poor SBERT retrieval or ineffective discriminative text generation.
- Suboptimal accuracy: may stem from missing or incorrect hyperparameters for classifier fine-tuning.

**Exactly 3 First Experiments:**
1. Generate augmented examples using SEG and CEG with provided prompts; inspect diversity and class separability.
2. Implement CA to retrieve and verify examples; check alignment accuracy and modification effectiveness.
3. Fine-tune RoBERTa/BERT on augmented data for 4,000 (5/10-shot) or 1,000 (2-shot) steps; evaluate test accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- Missing critical hyperparameters for classifier fine-tuning (learning rate, batch size, optimizer) referenced to CPFT/PromptMix but not specified.
- Exact number of spark thoughts per seed example in SEG and value of m (retrieved examples) for CA verification are unspecified.
- Dependence on proprietary LLM APIs (Llama2-13b-chat) and specific embedding models (sentence-transformers/all-mpnet-base-v2) may introduce variability in results.

## Confidence
- **High confidence**: The overall two-stage augmentation framework (generation + alignment) and its motivation for improving class separability in few-shot settings are well-defined and reproducible.
- **Medium confidence**: The general workflow for SEG, CEG, and CA is clear from prompts and equations, but exact generation dynamics (e.g., spark thought count, m) and classifier hyperparameters are unspecified.
- **Low confidence**: Exact reproduction of reported accuracy numbers is not feasible without the missing hyperparameters and generation settings.

## Next Checks
1. Confirm learning rate, batch size, and optimizer settings used for RoBERTa/BERT fine-tuning (referenced to CPFT/PromptMix but not specified).
2. Validate the number of spark thoughts generated per seed example in SEG and the value of m (retrieved examples) for CA verification prompts.
3. Test the effect of different generation temperatures and sampling parameters on diversity and alignment quality in augmented examples.