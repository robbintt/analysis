---
ver: rpa2
title: 'On the origin of neural scaling laws: from random graphs to natural language'
arxiv_id: '2601.10684'
source_url: https://arxiv.org/abs/2601.10684
tags:
- scaling
- laws
- language
- power
- fits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates the origin of neural scaling laws in transformers\
  \ by studying simplified models of sequence prediction. The authors train transformers\
  \ on random walks on graphs (Erd\u0151s-R\xE9nyi, Barab\xE1si-Albert, and bigram\
  \ graphs) and find that power-law scaling laws emerge even when the input data has\
  \ no inherent power-law structure."
---

# On the origin of neural scaling laws: from random graphs to natural language

## Quick Facts
- arXiv ID: 2601.10684
- Source URL: https://arxiv.org/abs/2601.10684
- Reference count: 16
- Key outcome: Power-law scaling laws emerge from transformers trained on random walks over graphs without inherent power-law structure

## Executive Summary
This paper investigates the origin of neural scaling laws by training transformers on synthetic datasets generated from random walks over graphs (Erdős-Rényi, Barabási-Albert, and bigram graphs). The authors demonstrate that power-law scaling laws emerge even when the input data lacks inherent power-law structure, challenging existing theories about the data-origin hypothesis. Through systematic experiments varying dataset complexity using sequences from increasingly complex language models, they reveal how scaling exponents evolve. The paper also critically examines conventional scaling law results, showing that 2-layer transformers with short context lengths can reproduce key findings, and proposes using neural network regression instead of 2D parametric fits for more accurate compute-optimal scaling predictions.

## Method Summary
The authors generate synthetic datasets by sampling unbiased random walks on graphs with 1K-50K nodes and 5K-2M edges. They train decoder-only GPT-style transformers with 2-4 layers, varying embedding dimensions (128-4096), using rotary positional encoding and context lengths of 50-100 tokens. Models are trained for 1 epoch with AdamW optimizer, using either maximal update parameterization (µP) or standard parameterization (SP). The study systematically increases dataset complexity by training on sequences generated from progressively more complex language models, from random token generation to real text. Scaling laws are analyzed using 1D power law fits L(N)_D = E + A·N^{-α} and L(D)_N = E + B·D^{-β}, and compute-optimal curves are obtained via 3-layer FCN regression.

## Key Results
- Power-law scaling laws emerge from transformers trained on random walks over Erdős-Rényi graphs without inherent power-law structure
- Scaling exponents evolve monotonically as dataset complexity increases from random tokens to real language
- 2-layer transformers with short context lengths (50-100 tokens) can reproduce conventional scaling law results
- Maximal update parameterization may be more parameter-efficient than standard parameterization
- Neural network regression provides more accurate compute-optimal scaling predictions than 2D parametric fits

## Why This Works (Mechanism)
The paper challenges the prevailing theory that scaling laws arise from data with inherent power-law structure. Instead, it demonstrates that the transformer architecture itself can induce power-law correlations in its internal representations, even when processing data without such structure. The authors propose that the optimization dynamics and the architecture's inductive biases create these scaling relationships, suggesting that scaling laws are more fundamental to the learning process than previously thought.

## Foundational Learning
- Power-law scaling laws: Why needed - Fundamental to understanding model performance trends; Quick check - Verify that loss decreases as L(N)_D ∝ N^{-α} with α > 0
- Compute-optimal scaling: Why needed - Determines how to allocate resources between model size and training data; Quick check - Confirm that compute-optimal curves minimize total compute for target performance
- Maximal update parameterization: Why needed - Affects training stability and parameter efficiency; Quick check - Compare training stability between µP and SP at various learning rates
- Graph-based sequence generation: Why needed - Provides controlled synthetic datasets without natural language structure; Quick check - Verify that random walks sample edges uniformly in unbiased case
- 1D vs 2D scaling law fits: Why needed - Determines accuracy of extrapolation and compute-optimal predictions; Quick check - Compare MSE of 1D fits vs 2D parametric fits on held-out data

## Architecture Onboarding

Component map: Graph generation -> Sequence sampling -> Tokenization -> Transformer training -> Loss computation -> Scaling law fitting

Critical path: Graph generation → Random walk sampling → Transformer training → Loss evaluation → Scaling law analysis

Design tradeoffs:
- Context length: Short (50-100) for efficiency vs long for capturing dependencies
- Model depth: 2-4 layers for controlled experiments vs deeper for capacity
- Parameterization: µP for stability vs SP for standard comparison

Failure signatures:
- Training instabilities with SP at large learning rates
- Poor fit quality when excluding irreducible loss term E
- Loss plateaus indicating saturation of model capacity

First experiments:
1. Train 2-layer transformer on ER graph random walks with varying embedding sizes
2. Compare µP vs SP training stability across learning rate sweeps
3. Test scaling law emergence on BA graphs vs ER graphs

## Open Questions the Paper Calls Out
1. Do power laws emerge in the internal activations or representations of transformers even when the input data lacks power-law structure? (Explicit) The authors state in the discussion, "It would be interesting to further study these internal activations and see to what extent signatures of power laws arise," specifically to explain why scaling laws appear in Erdős-Rényi graphs without data power laws.

2. How do the scaling exponents α_D and β_N evolve as the bias exponent κ of the random walk is continuously tuned between 0 and 1? (Explicit) The authors note regarding biased random walks, "It would be interesting to continuously tune the exponent κ and see the evolution in the scaling exponents... which we leave for future work."

3. Can datasets or architectures be constructed where scaling is limited by architectural capacity (β_N ≫ α_D) rather than data size? (Explicit) In the Discussion, the authors state, "It would be interesting to find settings that are more limited by architectural capacity than by dataset size, so that β_N ≫ α_D."

4. How robust are the derived scaling law parameters to the choice of optimization algorithm and the specific fitting window used for extrapolation? (Inferred) The authors note that optimization algorithms affect the loss L(N, D) and that "In nearly all cases, we found that the parameters of the fits for the scaling laws are sensitive to the window chosen," explicitly leaving a sensitivity analysis for future work.

## Limitations
- Results primarily based on 2-layer transformers with relatively short context lengths (50-100 tokens)
- Language model experiments use simplified architectures (up to 4 layers) that may not capture state-of-the-art LLM behavior
- The study relies on specific fitting windows and optimization settings without extensive sensitivity analysis
- Claims about 2-layer transformers reproducing conventional scaling law results need more thorough validation across different benchmark tasks

## Confidence
- High confidence: Power-law scaling from random graph sequences, methodological approach, 2-layer transformers reproducing scaling law patterns
- Medium confidence: Maximal update parameterization efficiency, stability of compute-optimal scaling exponents
- Medium confidence: Inadequacy of 2D parametric fits for compute-optimal predictions

## Next Checks
1. Verify the power law fitting methodology by explicitly testing whether including the irreducible loss term E significantly improves fit quality and extrapolation accuracy compared to standard 2-parameter fits

2. Test whether the observed scaling behavior persists when using longer context lengths (e.g., 512-2048 tokens) and deeper architectures (e.g., 6-12 layers) to assess scalability

3. Conduct ablation studies on the graph generation parameters (edge density, degree distribution) to determine which structural properties are essential for producing the observed scaling laws