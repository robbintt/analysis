---
ver: rpa2
title: 'Beyond the Failures: Rethinking Foundation Models in Pathology'
arxiv_id: '2510.23807'
source_url: https://arxiv.org/abs/2510.23807
tags:
- pathology
- foundation
- tissue
- clinical
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Despite successes in general vision and language, foundation models\
  \ (FMs) in pathology exhibit low accuracy, instability, and heavy computational\
  \ demands due to conceptual mismatches with tissue data. Dense embeddings fail to\
  \ capture tissue morphology\u2019s combinatorial richness, and models inherit flaws\
  \ from self-supervision, patch design, and noise-sensitive pretraining."
---

# Beyond the Failures: Rethinking Foundation Models in Pathology

## Quick Facts
- arXiv ID: 2510.23807
- Source URL: https://arxiv.org/abs/2510.23807
- Reference count: 16
- Foundation models in pathology show low accuracy (40-42% F1) and heavy computational demands compared to task-specific alternatives

## Executive Summary
Despite success in general vision and language, foundation models (FMs) fail in pathology due to fundamental mismatches between their design and tissue data. Dense single-vector embeddings cannot capture the combinatorial richness of tissue morphology, while self-supervised pretraining objectives designed for natural images instead learn superficial stain textures. Empirical evidence shows these models are unstable, computationally expensive (up to 35× more energy than task-specific alternatives), and vulnerable to site-specific biases and adversarial perturbations.

## Method Summary
The paper synthesizes findings from multiple empirical studies rather than presenting new experimental results. It analyzes TCGA dataset (11,444 WSIs, 23 organs, 117 cancer subtypes) and various pathology models through evaluation protocols including zero-shot retrieval via Yottixel patch-embedding, Robustness Index computation, rotation invariance tests, and adversarial attack analysis. The work compares dense foundation models against task-specific alternatives and examines failure modes through theoretical analysis of embedding dimensionality limits and self-supervised learning assumptions.

## Key Results
- Foundation models achieve only 40-42% macro-averaged F1 in retrieval tasks compared to specialized models
- Models exhibit severe site-specific bias (Robustness Index < 1) and rotation sensitivity
- Dense embeddings cannot encode the combinatorial complexity of tissue morphology due to theoretical limits
- Models require up to 35× more energy than task-specific alternatives while relying on linear probing rather than full fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
Dense single-vector embeddings face theoretical limits that cannot encode the combinatorial complexity of tissue morphology, regardless of model scale or training quality. Communication complexity and sign-rank theory show that embedding dimension scales polynomially with retrievable patterns, while real-world pathology retrieval demands grow combinatorially. A 768-1024 dimensional vector can encode only approximately 4M-250M distinct patterns—insufficient for heterogeneous tissue structures.

### Mechanism 2
Self-supervised pretraining objectives designed for natural images cause models to learn stain texture statistics rather than biologically meaningful morphology. SSL frameworks like contrastive learning assume "local-global crop" coherence—where patches from the same image share semantic content. In tissue, patches contain heterogeneous structures with no single dominant object, so the pretext task reward collapses to superficial texture matching.

### Mechanism 3
Small amounts of label noise during pretraining cause irreversible collapse of representation geometry (skewness/kurtosis), degrading downstream clinical performance even when upstream accuracy appears normal. Noise flattens the statistical distribution of embeddings—skewness and kurtosis shrink toward zero, making the representation space symmetric and less discriminative.

## Foundational Learning

### Concept: Dense vs. Sparse/Multi-Vector Embeddings
Why needed here: The paper argues single-vector embeddings are theoretically insufficient for pathology. Understanding sparse retrieval, multi-vector representations (e.g., ColBERT-style late interaction), and cross-encoders is essential to evaluate alternatives.
Quick check question: Can you explain why increasing embedding dimension from 768 to 4096 only polynomially increases representable patterns, not exponentially?

### Concept: Self-Supervised Learning Assumptions
Why needed here: The critique centers on pretext-task mismatch. You need to understand what invariances contrastive learning (SimCLR, MoCo), masked autoencoders (MAE), and self-distillation (DINO) enforce—and why these fail for tissue.
Quick check question: What semantic assumption does the "local-global crop" heuristic make, and why does it break in whole-slide images?

### Concept: Transfer Learning Instability (Catastrophic Forgetting vs. Inheritance)
Why needed here: Pathology FMs rely on frozen backbones with linear probing. You must distinguish between forgetting (downstream fine-tuning degrades pretrained knowledge) and inheritance (pretraining flaws propagate downstream).
Quick check question: If a model achieves 95% upstream accuracy but 40% downstream F1, is this more likely catastrophic forgetting or catastrophic inheritance?

## Architecture Onboarding

### Component map
WSI Input -> Patch Extraction (224×224) -> ViT Encoder (frozen FM) -> Dense Embeddings [d=768-1024] -> Aggregation (mean/max/attention) -> Downstream Head (linear probe or task-specific classifier)

### Critical path
1. Input representation: 224×224 patches vs. diagnostic FOV (~2048×1536)
2. Pretraining source: ImageNet vs. from-scratch pathology
3. Encoder choice: General ViT vs. domain-specific topology
4. Adaptation: Linear probing (frozen) vs. fine-tuning (unstable)

### Design tradeoffs
- Larger patches capture context but increase compute quadratically
- From-scratch training avoids inheritance but requires massive labeled pathology data (unavailable)
- Multi-scale attention adds complexity but addresses FOV mismatch
- Task-specific models outperform FMs when data sufficient, but lack transferability

### Failure signatures
- Robustness Index (RI) < 1: embeddings cluster by site, not biology
- Rotation sensitivity: m-kNN drops >0.2 under 90° rotation
- Cross-organ F1 variance >30 points (e.g., kidney 68% vs. lung 21%)
- Linear probing outperforms fine-tuning by >5% (signals frozen-backbone dependency)
- Adversarial perturbations collapse accuracy from 97%→12%

### First 3 experiments
1. Robustness Index audit: Compute within-class vs. within-center embedding similarity on your dataset. If RI < 1, site bias dominates—do not deploy cross-institutionally.
2. Rotation invariance test: Rotate patches in 15° increments and measure m-kNN stability. If variance >0.1, explicit rotation augmentation is required during training.
3. Patch size ablation: Compare 224×224 vs. 512×512 vs. 1024×1024 patches on a held-out retrieval task. If larger patches improve F1 >5%, FOV mismatch is limiting performance.

## Open Questions the Paper Calls Out

### Open Question 1
Can multi-vector, sparse, or hybrid embedding architectures overcome the theoretical representational limits of dense single-vector embeddings for pathology retrieval tasks? The theoretical work is recent, and no pathology-specific multi-vector architectures have been empirically validated against dense baselines. Comparative benchmarks showing multi-vector or hybrid retrieval architectures achieving significantly higher F1 scores on large-scale pathology retrieval would resolve this.

### Open Question 2
What patch sizes or hierarchical patching strategies are optimal for encoding diagnostically meaningful tissue morphology? Larger patches increase computational costs quadratically, and no systematic study has validated optimal patch dimensions across organs and magnifications. Controlled experiments comparing diagnostic accuracy across varying patch sizes with matched compute budgets would resolve this.

### Open Question 3
Does training pathology models from scratch eliminate catastrophic inheritance from noisy ImageNet pretraining? Training from scratch requires substantially more pathology data and compute, and no large-scale comparison of scratch-trained vs. ImageNet-initialized pathology FMs exists. Head-to-head comparison on robustness metrics and downstream clinical tasks would resolve this.

### Open Question 4
What self-supervised pretext tasks can capture biological morphology rather than superficial stain texture in histopathology images? Standard SSL objectives assume single coherent objects; histopathology lacks discrete objects and requires context-dependent interpretation. Novel SSL objectives evaluated on morphology-aware metrics and validated by pathologist assessment would resolve this.

## Limitations
- Paper is a position piece synthesizing findings from multiple studies rather than presenting new experimental results
- No direct experimental replication of theoretical claims about embedding dimensionality limits
- Absence of ablation studies isolating SSL pretext-task effects from other factors
- 40-42% F1 figure aggregates results across heterogeneous retrieval tasks without baseline comparators

## Confidence

**High confidence:** Claims about FM instability (RI < 1, rotation sensitivity, adversarial vulnerability) are directly supported by cited empirical studies with specific metrics.

**Medium confidence:** Theoretical arguments about communication complexity and sign-rank limits are logically sound but not experimentally validated on pathology-specific retrieval tasks.

**Low confidence:** The catastrophic inheritance hypothesis (noise flattening skewness/kurtosis) relies on single cited works without independent replication in the corpus.

## Next Checks
1. Replicate the rotation invariance test (15° increments) on TCGA-KIRC using 2-3 public pathology FMs; verify m-kNN variance exceeds 0.2 as claimed.
2. Compute Robustness Index on a multi-institutional pathology dataset; confirm RI < 1 indicates site clustering over biological similarity.
3. Implement a small ablation comparing dense (768-dim) vs. sparse (ColBERT-style) embeddings on a held-out TCGA retrieval task; measure if sparse retrieval improves F1 by >5%.