---
ver: rpa2
title: Bootstrapping LLMs via Preference-Based Policy Optimization
arxiv_id: '2511.12867'
source_url: https://arxiv.org/abs/2511.12867
tags:
- preference
- policy
- reward
- have
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of aligning large language models
  (LLMs) with human preferences by introducing a novel preference-based policy optimization
  (PbPO) framework. The core method formulates the learning process as a min-max game
  between the LLM policy and a reward model (RM), where the RM is constrained within
  a confidence set derived from collected preference data to ensure reliable exploitation.
---

# Bootstrapping LLMs via Preference-Based Policy Optimization

## Quick Facts
- arXiv ID: 2511.12867
- Source URL: https://arxiv.org/abs/2511.12867
- Reference count: 40
- The paper introduces PbPO, a framework that formulates LLM alignment as a min-max game between policy and constrained reward model, achieving superior performance on five benchmarks.

## Executive Summary
This paper addresses the challenge of aligning large language models with human preferences by introducing a novel preference-based policy optimization (PbPO) framework. The core method formulates the learning process as a min-max game between the LLM policy and a reward model (RM), where the RM is constrained within a confidence set derived from collected preference data to ensure reliable exploitation. The framework integrates reward-agnostic and reward-aware exploration, actively collecting new preference data through guided exploration of the evolving policy to enable continual self-improvement. Theoretical guarantees are provided, establishing high-probability regret bounds for both sequence-level and token-level RMs. Extensive experiments on five benchmark datasets demonstrate that PbPO consistently outperforms state-of-the-art preference optimization techniques.

## Method Summary
PbPO operates through a constrained min-max game where a main policy is optimized against a reward model that is constrained to a confidence set derived from preference data. The method interleaves reward-agnostic exploration (using an enhancer policy to maximize feature-space uncertainty and collect informative preference pairs) with reward-aware optimization (policy ascent against a conservative RM). A Lagrangian relaxation converts the constrained problem into tractable bi-level optimization. The framework provides theoretical regret guarantees under linear reward realizability assumptions and demonstrates practical improvements across multiple benchmarks.

## Key Results
- PbPO achieves 64.7% zero-shot accuracy on AGIEval versus 58.2% for best prior method
- Consistent improvements across all five benchmarks: BBH (76.2% vs 70.3%), ARC-C (55.6% vs 50.9%), MMLU (54.1% vs 51.3%), GSM8K (60.2% vs 58.1%)
- Ablation studies show both reward-agnostic and reward-aware exploration are critical for performance
- Theoretical regret bounds of Õ(d√K) for sequence-level and Õ(dH^(3/2)√K) for token-level settings

## Why This Works (Mechanism)

### Mechanism 1: Min-Max Adversarial Game for Robust Policy Improvement
- Claim: PbPO achieves conservative but guaranteed policy improvement by optimizing against the worst-case reward model consistent with observed preferences.
- Mechanism: The policy (leader) maximizes performance gap against a reference policy, while the reward model (follower) minimizes this gap. This adversarial formulation prevents overconfident updates when reward uncertainty is high. The confidence set R(D^pref_k) constrains the RM to reward functions that are statistically plausible given collected data.
- Core assumption: The ground-truth reward function lies within the linear function class G_r (Assumption 2/4: realizability).
- Evidence anchors:
  - [abstract] "formulates the learning process as a min-max game between the main policy and a reward model (RM). The RM is constrained within a confidence set derived from preference data to ensure reliable exploitation."
  - [Algorithm 1, Lines 13-14] "π^k = arg max_{π∈Π} min_{r∈R(D^pref_k)} J(π, r) - J(π^k_ref, r)"
  - [corpus] Related work "Adversarial Policy Optimization for Offline Preference-based RL" shows similar adversarial formulations improve conservatism but in offline settings.
- Break condition: If the RM function class cannot represent the true reward, the confidence set will never contain r*, and the min-max objective may optimize against systematically incorrect reward models.

### Mechanism 2: Uncertainty-Guided Enhancer Policy for Data Diversity
- Claim: An enhancer policy deliberately explores underexplored regions of the feature space, reducing covariance-based uncertainty and improving reward model estimation.
- Mechanism: The enhancer policy \hat{π}^k is selected to maximize the expected feature difference norm weighted by the inverse empirical covariance matrix. This targets trajectory pairs where the RM has high uncertainty (low coverage in feature space), yielding more informative preference labels per sample.
- Core assumption: Features are linearly related to rewards; uncertainty in feature coverage correlates with reward estimation error.
- Evidence anchors:
  - [Algorithm 1, Lines 5-7] "\hat{π}^k = arg max_{π∈Π} ||E[ϕ(τ^0) - ϕ(τ^1)]||_{\hat{Σ}^{-1}_k}"
  - [Figure 2(a-d)] Ablation shows removing reward-agnostic exploration degrades final performance across all tasks.
  - [corpus] Weak direct corpus evidence on enhancer policy mechanisms specifically for PbRL; related work focuses on online RLHF without explicit uncertainty-based exploration.
- Break condition: If features do not capture reward-relevant structure, maximizing feature uncertainty may not improve reward estimation. High stochasticity in preferences could also make uncertainty estimates unreliable.

### Mechanism 3: Lagrangian Relaxation for Tractable Confidence Constraints
- Claim: Converting hard confidence-set constraints into a regularized objective via Lagrangian relaxation enables practical gradient-based optimization while approximately preserving theoretical guarantees.
- Mechanism: The constrained min-max problem is reformulated with a Lagrange multiplier β controlling trade-off between adversarial loss (policy improvement) and preference likelihood (reward accuracy). The RM is trained via MLE on preference data while simultaneously serving as adversary.
- Core assumption: The penalty coefficient β ≥ 0 sufficiently approximates the confidence-set boundary; β = 0 removes the preference constraint entirely.
- Evidence anchors:
  - [Section: Approximate Policy Optimization] "we apply a Lagrangian relaxation... convert the constrained min-max problem into an unconstrained bi-level optimization problem"
  - [Figure 2(e)] Sensitivity analysis shows β > 0 consistently outperforms β = 0; β ∈ {0.04, 0.1} achieve similar top performance, indicating robustness.
  - [corpus] No direct corpus evidence on Lagrangian relaxation for PbRL confidence sets.
- Break condition: If β is set too low, the RM may diverge from preference-consistent solutions. If β is too high, the adversarial component becomes weak, reducing robustness to uncertainty.

## Foundational Learning

- Concept: **Bradley-Terry-Luce (BTL) Preference Model**
  - Why needed here: Defines how reward differences translate to binary preference probabilities; this is the observation model for all preference data collection.
  - Quick check question: Can you derive why P(τ^0 ≻ τ^1) = σ(r(τ^0) - r(τ^1)) implies a sigmoid link function?

- Concept: **Regret Analysis in Online Learning**
  - Why needed here: The theoretical guarantees are expressed as cumulative regret bounds; understanding these helps assess sample efficiency claims.
  - Quick check question: Why does an upper bound of Õ(d√K) indicate near-optimal performance for a d-dimensional linear bandit?

- Concept: **Elliptical Potential Lemma**
  - Why needed here: Used to bound cumulative exploration uncertainty; fundamental to the regret proof for both sequence-level and token-level settings.
  - Quick check question: How does log det(V_K) relate to the information gained from exploring in feature space?

## Architecture Onboarding

- Component map:
  - Enhancer Policy -> Reference Policy -> Main Policy -> Reward Model
  - Covariance Tracker -> Feature Extractor -> Preference Oracle

- Critical path:
  1. Initialize π^0 with SFT model; D^pref = ∅
  2. For each episode k: (a) Select enhancer \hat{π}^k via uncertainty maximization; (b) Sample trajectory pairs from (\hat{π}^k, π^k_ref); (c) Collect preference label; (d) Update D^pref and Σ_k; (e) Run T_out outer steps of policy ascent with T_in inner steps of RM descent
  3. Output final policy π^K

- Design tradeoffs:
  - **Sequence-level vs. Token-level RM**: Token-level (Theorem 3) has higher regret bound (Õ(dH^(3/2)√K)) but empirically outperforms on long-horizon reasoning tasks (Table 2: GSM8K 60.2% vs 58.1%).
  - **RM capacity vs. convergence speed**: Larger RMs (70B) achieve better final performance; smaller RMs (3B) converge faster (Figure 2f).
  - **Episode budget**: 100 episodes often sufficient for moderate gains; 300 episodes yield diminishing returns on some benchmarks.

- Failure signatures:
  - Performance plateaus early: Check if enhancer policy is actually exploring (inspect covariance matrix conditioning); may indicate β too high.
  - RM loss diverges: Preference likelihood term overwhelmed by adversarial term; increase β.
  - Degraded performance vs. SFT: Exploration may be collecting harmful preference data; verify preference oracle quality.

- First 3 experiments:
  1. **Ablation on exploration**: Compare full PbPO vs. without enhancer policy vs. without reward-aware exploration on a single benchmark (e.g., GSM8K) to reproduce Figure 2 findings.
  2. **β sensitivity sweep**: Test β ∈ {0, 0.01, 0.02, 0.04, 0.1} with fixed episode budget (100) to validate robustness claim and identify task-specific optimum.
  3. **RM scaling**: Train with 3B, 7B, and 13B reward models to confirm the capacity-convergence tradeoff and determine viable minimum for target task complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the theoretical guarantees of PbPO be extended to non-linear reward model function classes, such as general deep neural networks?
- **Basis in paper:** [Explicit] The theoretical analysis strictly relies on the **linearity assumption** for the Reward Model (Assumption 1 on Page 3 and Assumption 3 on Page 4) to derive covariance matrices and regret bounds.
- **Why unresolved:** The proofs utilize the properties of linear feature vectors to bound the elliptical potential and define confidence sets, techniques which do not directly transfer to non-linear function approximators without significant modifications (e.g., Neural Tangent Kernel assumptions).
- **What evidence would resolve it:** A derivation of high-probability regret bounds for PbPO under a general function class $G_r$ using tools for non-convex optimization or neural network analysis.

### Open Question 2
- **Question:** Is the regret bound for the token-level RM setting ($\tilde{O}(\kappa d H^{3/2}\sqrt{K})$) tight, or can the dependency on the horizon $H$ be improved?
- **Basis in paper:** [Inferred] Table 1 (Page 2) highlights a gap between the derived **upper bound** ($\tilde{O}(dH^{3/2})$) and the information-theoretic **lower bound** ($\Omega(dH)$).
- **Why unresolved:** While Remark 2 (Page 4) notes the result is "nearly optimal," the difference in the power of $H$ ($3/2$ vs $1$) suggests the current analysis may not fully capture the efficiency of the algorithm in long-horizon tasks.
- **What evidence would resolve it:** A refined upper bound matching the linear dependency on $H$, or a new lower bound construction that proves the $H^{3/2}$ scaling is unavoidable for preference-based optimization.

### Open Question 3
- **Question:** To what extent does the gradient-based Stackelberg game approximation (Algorithm 2) preserve the theoretical properties of the exact min-max formulation (Algorithm 1)?
- **Basis in paper:** [Explicit] Page 5 states that solving the constrained min-max problem is **"generally intractable"** for flexible function approximators, necessitating a **reformulation** into an unconstrained bi-level optimization problem for the practical implementation.
- **Why unresolved:** The paper's theoretical guarantees (Theorems 1-4) are derived for Algorithm 1. The performance gap or convergence error introduced by the Lagrangian relaxation and gradient-based adversarial training in Algorithm 2 remains unquantified.
- **What evidence would resolve it:** A convergence analysis for the gradient-based bi-level optimization or an empirical study quantifying the error introduced by the relaxation parameter $\beta$ relative to the theoretical regret bounds.

## Limitations

- The paper assumes a linear reward model function class, which may not capture complex reward structures in real-world applications.
- Performance depends critically on the quality of the preference oracle (GPT-4), with no validation of oracle reliability through human studies.
- The feature extractor ϕ(·) is never explicitly defined, making reproduction difficult and potentially impacting the effectiveness of uncertainty-guided exploration.

## Confidence

- **High confidence**: The min-max formulation with confidence-set constrained RM provides robust policy improvement against uncertainty. This is supported by theoretical guarantees and consistent ablation results showing β > 0 improves performance.
- **Medium confidence**: The uncertainty-guided enhancer policy meaningfully improves exploration efficiency. While Figure 2(a-d) shows clear gains, the mechanism's effectiveness depends heavily on the unspecified feature extractor, and direct corpus evidence is weak.
- **Low confidence**: The theoretical regret bounds accurately predict empirical sample efficiency. The bounds are asymptotic and assume linear realizability, but empirical results are reported only in terms of final accuracy, not learning curves or regret convergence rates.

## Next Checks

1. **Feature extractor ablation**: Implement multiple feature extraction methods (last hidden state, mean-pooled states, learned projection) and compare PbPO performance across methods to identify the most critical design choice.

2. **Oracle robustness test**: Replace GPT-4 with a weaker open model (e.g., GPT-3.5 or Llama-3-8B) or introduce synthetic noise in preferences, then measure degradation in final performance and learning stability.

3. **Regret convergence analysis**: Track cumulative regret and feature uncertainty norm during training on a single benchmark, then compare empirical regret growth to the theoretical O(d√K) or O(dH^(3/2)√K) bounds to validate sample efficiency claims.