---
ver: rpa2
title: 'Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making'
arxiv_id: '2601.05529'
source_url: https://arxiv.org/abs/2601.05529
tags:
- task
- reasoning
- spatial
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates LLM and VLM performance in safety-critical
  robotic navigation tasks. Seven diagnostic tasks were designed across complete,
  incomplete, and Safety-Oriented Spatial Reasoning (SOSR) categories, using ASCII
  maps, sequence reasoning, and natural-language scenarios.
---

# Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making

## Quick Facts
- arXiv ID: 2601.05529
- Source URL: https://arxiv.org/abs/2601.05529
- Reference count: 40
- Primary result: LLMs fail catastrophically in safety-critical robotic navigation despite high aggregate accuracy on standard benchmarks

## Executive Summary
This paper evaluates LLM and VLM performance in safety-critical robotic navigation tasks, revealing that high accuracy rates in standard benchmarks mask dangerous failure modes. Seven diagnostic tasks across complete, incomplete, and Safety-Oriented Spatial Reasoning (SOSR) categories expose catastrophic failures: models like LLaMA-3-8b collapsed map structures entirely, while Gemini-2.0 Flash and GPT-4o dropped to 0% success on complex maps. Even top performer GPT-5 showed occasional safety violations under uncertainty. The analysis demonstrates that current LLMs are not ready for direct deployment in safety-critical systems, as rare errors can cause catastrophic harm.

## Method Summary
The evaluation framework consists of three task categories: Complete Information (ASCII maps), Incomplete Information (sequence masking/uncertain terrain), and SOSR (natural language scenarios). Models were tested zero-shot via API for proprietary models (GPT-4o, GPT-5, Gemini-2.0/2.5 Flash) and local inference for open-source VLMs (LLaVA, Qwen, InternVL). Success rates were measured across 30-100 trials per model with failure-mode classification (map collapse, path discontinuity, obstacle violation, constraint breach, safety violation).

## Key Results
- LLaMA-3-8b collapsed all map structures (100% failure rate)
- Gemini-2.0 Flash and GPT-4o dropped to 0% success on complex maps
- Gemini-2.5 Flash directed robots toward server rooms instead of exits in fire scenarios (33% safety violations)
- GPT-5 achieved 100% on deterministic maps but showed 6.7% constraint violations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASCII map representations isolate spatial reasoning failures from visual processing errors
- Mechanism: By abstracting navigation tasks into textual grid formats (S, G, #, ?, .), the framework eliminates multimodal interpretation ambiguity, enabling direct assessment of topological continuity, obstacle constraints, and coordinate consistency
- Core assumption: ASCII representations are sufficiently isomorphic to real-world spatial planning that failures in this controlled format predict embodied deployment failures
- Evidence anchors: Abstract states "Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing"

### Mechanism 2
- Claim: Incomplete information tasks expose hallucination tendencies by forcing inference over missing spatial context
- Mechanism: By masking frames in trajectory sequences or introducing unknown terrain cells (?), the framework tests whether models maintain spatial continuity and trajectory consistency or fabricate plausible but unsupported explanations
- Core assumption: Hallucination in controlled incomplete-information scenarios correlates with unsafe inference under real-world uncertainty
- Evidence anchors: Abstract states "Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations"

### Mechanism 3
- Claim: Safety-Oriented Spatial Reasoning (SOSR) tasks reveal catastrophic risk-prioritization failures through natural language scenario testing
- Mechanism: By presenting life-threatening scenarios (fire evacuation) with competing objectives (thesis retrieval vs. exit), SOSR tasks test whether models override goal-oriented reasoning with human-safety-first decisions
- Core assumption: Natural language scenario responses reflect likely robotic decision-making in equivalent physical deployments
- Evidence anchors: Abstract states "In SOSR tasks, Gemini-2.5 Flash directed robots toward server rooms instead of exits in fire scenarios"

## Foundational Learning

- **Concept: Spatial continuity in embodied systems**
  - Why needed here: The paper demonstrates that models like GPT-4o and Gemini-2.0 Flash exhibit pathline truncation (0% success on complex maps), indicating inability to maintain continuous spatial representations across multi-step planning
  - Quick check question: Given a 10x10 ASCII grid with 5 obstacles, can you trace a valid 4-connected path from start to goal while preserving grid structure?

- **Concept: Constraint adherence under uncertainty**
  - Why needed here: Uncertain terrain tasks show models frequently violate explicit prohibitions (diagonal movement, obstacle traversal) even when instructed otherwise, with GPT-5 showing 6.7% constraint violations despite high overall success
  - Quick check question: If an unknown cell (?) may be passable but you assume not, how does your path change? What if the assumption is wrong?

- **Concept: Safety-utility tradeoff in goal-oriented reasoning**
  - Why needed here: SOSR fire scenarios reveal models sometimes prioritize stated objectives (thesis retrieval) over unstated safety imperatives, with 33% of Gemini-2.5 Flash responses directing users toward hazardous locations
  - Quick check question: In a scenario with conflicting goals (save document vs. evacuate), what implicit hierarchy should govern decision-making?

## Architecture Onboarding

- **Component map:** Complete Information ASCII maps → Incomplete Information sequence masking/uncertain terrain → SOSR natural language scenarios → success-rate metrics across 30-100 trials per model → failure mode taxonomy → deployment-readiness conclusion

- **Critical path:** ASCII map tasks → spatial continuity assessment → uncertain terrain extension → SOSR natural language scenarios → failure mode taxonomy → deployment-readiness conclusion. GPT-5 achieved 100% on deterministic maps but showed 6.7% constraint violations; LLaMA-3-8b collapsed all map structures; Gemini-2.5 Flash showed 33% safety violations in fire scenarios.

- **Design tradeoffs:** ASCII abstraction isolates reasoning but may not transfer to visual navigation; 30-trial evaluation detects catastrophic failures (0% success) but lacks statistical power for rare-event quantification (1% failure rate implications are argued theoretically, not empirically validated at scale); natural language scenarios enable safety testing but introduce prompt-sensitivity variability.

- **Failure signatures:**
  - Map collapse: Random symbol generation, grid distortion (LLaMA-3-8b: 100% failure)
  - Pathline truncation: Mid-route termination under complexity (Gemini-2.0 Flash, GPT-4o: 0% on Normal/Hard maps)
  - Constraint violation: Diagonal movement despite explicit prohibition (GPT-5: 6.7% on Uncertain Ver. 2)
  - Safety violation: Hazardous location selection in emergency scenarios (Gemini-2.5 Flash: 33% toward professor's room/server room in fire)

- **First 3 experiments:**
  1. Replicate ASCII deterministic map evaluation with GPT-4o and Gemini-2.0 Flash on Normal map across 30 trials to confirm binary collapse pattern (pathline truncation frequency, grid integrity)
  2. Extend uncertain terrain evaluation to include probabilistic unknown-cell handling (50% passable assumption) and measure constraint violation rates under mixed uncertainty
  3. Design controlled SOSR variant with explicit safety priming ("Human safety is the highest priority") vs. no priming to quantify prompt-sensitivity of safety violations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the catastrophic failure modes observed in smaller models (e.g., map collapse, spatial hallucination) persist in state-of-the-art models with significantly larger parameter counts?
- Basis in paper: The "Future Works" section notes the evaluation was constrained by GPU memory to smaller models, and scaling experiments to larger SOTA models is a necessary next step
- Why unresolved: It is unclear if the observed lack of spatial continuity and constraint consistency is a fundamental limitation of current architectures or a capacity issue resolved by scale
- What evidence would resolve it: Benchmarking results of models with significantly larger parameter counts (e.g., GPT-4 class) on the ASCII map and SOSR tasks

### Open Question 2
- Question: How should future safety benchmarks quantify the trade-off between a model that refuses to act due to safety policies versus one that confidently provides hazardous instructions?
- Basis in paper: The Appendix highlights a discrepancy where GPT-4o refuses to answer fire scenarios (safe refusal) while Gemini-2.5 Flash provides confident but dangerous directions
- Why unresolved: Current benchmarks rely on success rates, failing to distinguish between "safe abstention" and "catastrophic error"
- What evidence would resolve it: A new evaluation framework that separately penalizes hazardous hallucinations versus safe refusals to establish a normative standard for "safe" behavior

### Open Question 3
- Question: Can the spatial reasoning failures identified in the "Back of the Building" task be mitigated by deploying the models on physical robotic platforms rather than in simulation?
- Basis in paper: The authors propose extending the "Back of the Building" scenario to a physical robot to gain insights into practical challenges and dynamic environments
- Why unresolved: The study currently relies on static images and ASCII maps; it is unknown if the "grounding" provided by physical sensorimotor data would correct the topological distortions observed in the models
- What evidence would resolve it: Comparative performance data between simulation-based and real-world robotic execution of the navigation tasks

## Limitations

- The diagnostic framework's ecological validity remains unconfirmed - ASCII map success rates may not transfer to real-world visual navigation where perception errors dominate
- The 30-trial evaluation provides binary failure detection (0% vs >0%) but insufficient statistical power to quantify rare-event risks (1-5% failure rates) that could be catastrophic in deployment
- SOSR scenarios rely on natural language interpretation without confirming whether verbal responses correlate with actual embodied action selection

## Confidence

- **High:** LLMs exhibit catastrophic failures in deterministic spatial reasoning (map collapse, path truncation) - empirically validated across multiple models with 0% success rates
- **Medium:** Hallucination tendencies in incomplete information tasks correlate with unsafe inference under uncertainty - mechanism plausible but transfer to real-world uncertainty unconfirmed
- **Low:** Natural language SOSR responses predict embodied safety violations - scenario-based testing shows safety violations but lacks validation that verbal responses reflect actual robotic decision-making

## Next Checks

1. Test model performance on equivalent visual navigation tasks (real images or synthetic scenes) to confirm ASCII-to-visual transfer validity and quantify perception-augmented reasoning failures
2. Extend SOSR evaluation with explicit safety priming variations and measure safety violation reduction rates to quantify prompt-sensitivity and potential mitigation strategies
3. Implement adversarial stress-testing on the complete framework - introduce out-of-distribution map configurations, ambiguous scenario framings, and conflicting multi-objective prompts to map failure boundary conditions