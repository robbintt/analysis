---
ver: rpa2
title: 'ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for
  Efficient Reinforcement Learning'
arxiv_id: '2511.21005'
source_url: https://arxiv.org/abs/2511.21005
tags:
- icpo
- reward
- responses
- training
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ICPO addresses challenges in RLVR such as sparse rewards, reward
  noise, and entropy collapse by leveraging the intrinsic probabilities of model-generated
  responses as a self-assessment signal. The method computes a preference advantage
  score for each response by comparing relative generation probabilities within a
  group, then integrates these scores with external rewards through multi-stage weight
  adjustment.
---

# ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.21005
- Source URL: https://arxiv.org/abs/2511.21005
- Reference count: 40
- Primary result: ICPO improves RLVR performance by 1.5-4.2% across 7 benchmarks by leveraging intrinsic model probabilities as self-assessment signals

## Executive Summary
ICPO addresses fundamental challenges in reinforcement learning from verifiable rewards (RLVR) including sparse rewards, reward noise, and entropy collapse. The method computes a preference advantage score for each response by comparing relative generation probabilities within a group, then integrates these scores with external rewards through a multi-stage weight adjustment. This design suppresses overconfident errors, enhances undervalued high-quality responses, and sustains exploration. Comprehensive experiments on seven benchmarks across multiple model architectures demonstrate that ICPO consistently outperforms baselines like GRPO, with notable improvements in general and mathematical reasoning tasks.

## Method Summary
ICPO modifies GRPO by computing preference advantage scores from within-group generation probabilities. For G=5 responses per prompt: (1) sort responses by ascending log-probability (length-normalized), (2) compute Sp_k = δ·Σ(logπ(oj)/logπ(ok)) - δ·logπ(oG), (3) final reward R_final = R_verif + ω·min(Sp_k, |R_verif|/τ), (4) apply multi-stage weight adjustment (warmup then decay). The method leverages the model's intrinsic probabilities as a self-assessment signal, using the relative probabilities within a group to identify high-value but low-confidence responses while suppressing overconfident errors. Base GRPO hyperparameters include lr=1e-6, KL=0.001, batch=768, mini-batch=192, clip=0.2, temp=1.0, steps=500.

## Key Results
- ICPO outperforms GRPO by 1.5-4.2% average accuracy across 7 benchmarks (MMLU-Pro, GPQA, TheoremQA, WebInstruct, MATH-500, Minerva, AIME24)
- Notable improvements in mathematical reasoning tasks (MATH-500 +3.2%, AIME24 +4.1%)
- Maintains higher policy entropy throughout training, preventing collapse to repetitive patterns
- Robust to 40% reward noise injection while GRPO shows significant degradation

## Why This Works (Mechanism)

### Mechanism 1
Generation probabilities encode self-assessment that differentiates high-value low-confidence responses from overconfident errors. For a group of G responses, responses are sorted by sequence-level probability, and a preference advantage score is computed via pairwise log-probability ratios, accumulating how much each response is "preferred" over others with higher probability. This gives larger scores to correct responses the model is underconfident about, while penalizing incorrect responses the model is overconfident about.

### Mechanism 2
Clipping intrinsic rewards by verifiable reward magnitude prevents reward inversion from propagating. Final reward combines verifiable reward with clipped preference score using a min operation that caps intrinsic bonuses to be proportional to the verifiable signal, ensuring incorrect low-probability responses don't receive outsized encouragement.

### Mechanism 3
Warmup-decay weight schedule aligns intrinsic signal injection with the emergence of learnable low-probability correct responses. Weight follows inverse cosine growth to peak at 40% training progress, then linear annealing to minimum. Early training has high entropy (hard to identify valuable responses); mid-training reveals novel correct responses; late-training low-probability responses are likely noise.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: ICPO is a modification of GRPO. Must understand that GRPO samples G responses per prompt, computes advantages by normalizing rewards within each group, and uses PPO-style clipped objectives—but without a value network.
  - Quick check: Can you explain why GRPO uses intra-group normalization instead of a learned value function baseline?

- **Entropy Collapse**: ICPO explicitly aims to prevent entropy collapse, where policy rapidly collapses to repetitive high-reward patterns. Understanding this failure mode clarifies why intrinsic exploration signals matter.
  - Quick check: What happens to policy diversity if entropy drops to near-zero during RL training, and how would you detect this?

- **Verifiable Rewards (RLVR)**: ICPO augments RLVR's rule-based rewards. Must understand that RLVR uses deterministic verifiers (e.g., exact match for math) rather than learned reward models.
  - Quick check: Why would binary outcome-based rewards create "zero advantage" situations, and how does this affect gradient signals?

## Architecture Onboarding

- **Component map**: Rollout Generator -> Probability Scorer -> Preference Ranker -> Advantage Calculator -> Weight Scheduler -> Policy Updater
- **Critical path**: 1) Generate G=5 responses per prompt with sample_temperature=1.0, 2) Compute log-prob for each response using current policy, 3) Sort responses, compute preference advantage scores (δ scales by model size: 0.5 for <4B, 0.4 for 4-8B, 0.3 for >8B), 4) Merge with verifiable rewards using current ω from scheduler, 5) Normalize within group, compute PPO-style loss
- **Design tradeoffs**: Higher G (responses per prompt) gives better preference signal estimation but higher compute; higher δ gives more aggressive preference differentiation but may over-penalize legitimate uncertainty; higher τ gives tighter clipping, safer but weaker intrinsic signal; warmup vs. no warmup critical for base models, optional for already-tuned models
- **Failure signatures**: Flat accuracy curve early in training (likely warmup weight too aggressive); entropy spikes then crashes (intrinsic signal may be amplifying noise); performance degradation on instruction-tuned models (check if advantages are near-zero); large models underperforming (δ may be too high for overconfident models)
- **First 3 experiments**: 1) Reproduce GRPO baseline on Qwen2.5-7B-Base with 7 benchmark setup, measure entropy trajectory to confirm collapse pattern, 2) ICPO ablation on δ using δ∈{0.3, 0.4, 0.5, 0.6} on Qwen2.5-7B-Base, verify δ=0.4 produces best average accuracy, 3) Noisy reward stress test: inject 40% random noise into verifiable rewards, compare ICPO vs. GRPO degradation curves

## Open Questions the Paper Calls Out

- **Question:** How can ICPO be adapted to maintain stability when external rewards systematically deviate from true task objectives, causing the intrinsic encouragement-suppression mechanism to reverse?
- **Question:** Can the preference advantage score mechanism be extended to effectively mitigate noise for high-confidence correct responses and low-confidence erroneous responses?
- **Question:** Does the non-linear relationship between model scale and confidence calibration necessitate adaptive hyperparameter strategies for ICPO to maintain performance gains on models significantly larger than 14B?

## Limitations
- Performance improvements vary significantly with model scale; smaller models (<4B) show minimal gains due to flat probability distributions, while larger models (>14B) show diminishing returns due to overconfidence
- The method assumes verifiable rewards provide reliable direction; if verifiers themselves are noisy or misaligned, the clipping mechanism could amplify errors rather than prevent them
- Warmup-decay schedule lacks theoretical grounding; the 40% turning point appears somewhat arbitrary without systematic ablation studies

## Confidence

- **High confidence**: Claims about entropy collapse prevention and general performance improvements over GRPO baselines
- **Medium confidence**: Claims about intrinsic probability encoding epistemic uncertainty
- **Low confidence**: Claims about optimal warmup-decay schedule and model-size-specific δ parameters

## Next Checks

1. **Calibration analysis**: Run ICPO with temperature sweep (0.8-1.2) on a held-out set and measure the correlation between predicted probability gaps and actual correctness gaps

2. **Adversarial reward testing**: Replace 50% of verifiable rewards with random values and measure whether ICPO's clipping mechanism prevents performance degradation compared to GRPO

3. **Mechanism dissection**: Implement a variant that uses sequence length (not probability) for ranking, keeping all other ICPO components, to test whether probability-based uncertainty is essential to performance gains