---
ver: rpa2
title: 'HAMMER: Hamiltonian Curiosity Augmented Large Language Model Reinforcement'
arxiv_id: '2509.25240'
source_url: https://arxiv.org/abs/2509.25240
tags:
- zhang
- wang
- learning
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HAMMER, a curriculum learning method that
  improves large language model reinforcement learning by reordering training samples
  to maximize semantic diversity early in training. Instead of using difficulty-based
  ordering, HAMMER uses sentence embeddings from the backbone LLM to construct a minimum-semantic
  Hamiltonian path, exposing the model to more diverse samples at the start to encourage
  exploration and prevent early overfitting to easy problems.
---

# HAMMER: Hamiltonian Curiosity Augmented Large Language Model Reinforcement

## Quick Facts
- arXiv ID: 2509.25240
- Source URL: https://arxiv.org/abs/2509.25240
- Reference count: 29
- Improves LLM RL by curriculum reordering based on semantic diversity via minimum-semantic Hamiltonian paths

## Executive Summary
HAMMER introduces a curriculum learning approach that enhances large language model reinforcement learning by prioritizing semantic diversity in training sample ordering. Unlike traditional difficulty-based curricula, HAMMER uses sentence embeddings from the backbone LLM to construct a minimum-semantic Hamiltonian path, exposing models to diverse samples early in training to encourage exploration and prevent overfitting to easy problems. The method demonstrates consistent 3-4% accuracy gains across multiple math benchmarks when integrated with RLVR algorithms like DAPO and GRPO, while also improving answer consistency and pass@k metrics.

## Method Summary
HAMMER constructs a curriculum by computing sentence embeddings for training samples using the backbone LLM, then solving a minimum-semantic Hamiltonian path problem to order samples by maximal semantic diversity. This approach prioritizes diverse samples early in training to encourage exploration and prevent premature convergence to easy solutions. The method is theoretically grounded in claims that diverse subsets preserve optimal policies and tighten generalization bounds. HAMMER is integrated with existing RLVR algorithms like DAPO and GRPO, maintaining compatibility while improving sample efficiency and final performance metrics.

## Key Results
- Consistent 3-4% accuracy gains over baselines across multiple math benchmarks (AIME 2024/2025, AMC 2023, Olympiad)
- Improves both pass@k and answer consistency metrics
- Maintains gains across different batch sizes and model scales

## Why This Works (Mechanism)
HAMMER works by maximizing semantic diversity in early training stages, which encourages broader exploration of the solution space and prevents the model from overfitting to easy or similar problems. By exposing the model to diverse samples first, it develops more robust representations that generalize better to varied problem types. The minimum-semantic Hamiltonian path construction ensures that each training sample contributes unique information early in the learning process, while the theoretical foundation suggests this approach preserves optimal policies and tightens generalization bounds compared to random or difficulty-based ordering.

## Foundational Learning
- Curriculum Learning: why needed - enables staged skill acquisition; quick check - compare learning curves with/without curriculum
- Semantic Diversity: why needed - prevents early overfitting to similar samples; quick check - measure embedding similarity distributions
- Hamiltonian Path Optimization: why needed - ensures maximal diversity coverage; quick check - validate path construction algorithms
- Reinforcement Learning Theory: why needed - provides theoretical guarantees; quick check - verify policy preservation under diverse subsets
- Sentence Embeddings: why needed - captures semantic relationships between samples; quick check - evaluate embedding quality with downstream tasks

## Architecture Onboarding

**Component Map**
Backbone LLM -> Sentence Embedding Generator -> Hamiltonian Path Solver -> Ordered Dataset -> RLVR Algorithm -> Model

**Critical Path**
Embedding generation → path optimization → sample ordering → RL training loop

**Design Tradeoffs**
HAMMER trades computational overhead for improved sample efficiency and final accuracy. The Hamiltonian path computation adds preprocessing time but reduces the number of training iterations needed. Semantic diversity prioritization may initially slow convergence on easy problems but improves long-term generalization.

**Failure Signatures**
- Excessive computational overhead during preprocessing
- Poor embedding quality leading to suboptimal path construction
- Over-prioritization of diversity causing slow progress on core skills
- Path optimization failures resulting in duplicate or poorly ordered samples

**3 First Experiments**
1. Compare learning curves with HAMMER vs random ordering on a simple math benchmark
2. Measure semantic diversity metrics (embedding similarity distributions) before/after HAMMER ordering
3. Validate Hamiltonian path construction by checking coverage and diversity metrics

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Computationally expensive Hamiltonian path computation may limit scalability
- Semantic embeddings from backbone LLM may not capture all relevant features for non-mathematical domains
- Theoretical guarantees assume conditions that may not hold in practical scenarios
- Results are primarily validated on math problems, raising questions about generalizability

## Confidence

| Claim | Confidence |
|-------|------------|
| 3-4% accuracy improvements on math benchmarks | Medium |
| Theoretical policy preservation guarantees | High |
| Improved sample efficiency and consistency | Medium |
| Broader applicability beyond math domains | Low |

## Next Checks
1. Test HAMMER on non-mathematical domains like code generation or natural language understanding to assess generalizability
2. Conduct ablation studies to isolate the impact of semantic diversity from other curriculum factors
3. Evaluate computational efficiency at scale to determine practical deployment feasibility