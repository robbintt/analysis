---
ver: rpa2
title: What's in Common? Multimodal Models Hallucinate When Reasoning Across Scenes
arxiv_id: '2511.03768'
source_url: https://arxiv.org/abs/2511.03768
tags:
- objects
- reasoning
- arxiv
- dataset
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Common-O Bench, a new multi-image benchmark
  designed to test multimodal models' ability to reason across scenes. While current
  models excel at single-image perception tasks, they struggle when asked to identify
  common objects across multiple scenes.
---

# What's in Common? Multimodal Models Hallucinate When Reasoning Across Scenes

## Quick Facts
- **arXiv ID:** 2511.03768
- **Source URL:** https://arxiv.org/abs/2511.03768
- **Reference count:** 35
- **Primary result:** Best model (GPT-4o) achieves only 35% accuracy on multi-image reasoning benchmark

## Executive Summary
This paper introduces Common-O Bench, a new benchmark testing multimodal models' ability to identify common objects across multiple scenes. While current models excel at single-image tasks, they struggle significantly with cross-scene reasoning, with the best model achieving only 35% accuracy and performance dropping below 1% on complex scenes. The study reveals that models frequently hallucinate objects when reasoning across scenes, particularly when objects are similar, suggesting they rely on object co-occurrence patterns rather than genuine reasoning. Models trained with multi-image inputs show substantially better performance, indicating this as a promising research direction.

## Method Summary
The authors created Common-O Bench with 10.5k examples combining real and synthetic images, carefully curated to avoid contamination from web training data. They tested 13 state-of-the-art multimodal models, including those specifically trained for chain-of-thought reasoning. The benchmark evaluates models' ability to identify common objects across multiple scenes, with varying complexity levels. Synthetic images were generated with human verification to ensure quality, while real images were selected to represent diverse scenarios. The study systematically compares model performance across different architectural approaches and training paradigms.

## Key Results
- GPT-4o achieves 35% accuracy on Common-O Bench, with performance dropping below 1% on complex scenes
- Models frequently hallucinate objects when reasoning across scenes, especially for similar objects
- Models trained with multi-image inputs show significantly better performance than single-image models
- Performance gaps suggest architectural limitations rather than simple training data issues

## Why This Works (Mechanism)
The benchmark works by systematically evaluating whether models can identify common objects across multiple scenes rather than just processing individual images. The design exposes a fundamental limitation in current multimodal models: their inability to perform genuine cross-scene reasoning. By including both real and synthetic images and carefully controlling for contamination, the benchmark isolates the reasoning capability itself. The multi-level difficulty structure reveals that performance degradation is not linear but catastrophic for complex scenes, suggesting models lack robust mechanisms for scene comparison and object correspondence.

## Foundational Learning

**Multimodal reasoning:** Understanding how models process and integrate visual and textual information across multiple inputs. *Why needed:* The study's core contribution is exposing reasoning limitations that aren't visible in single-image benchmarks. *Quick check:* Test whether models can correctly identify objects in individual images before testing cross-scene reasoning.

**Object co-occurrence patterns:** Statistical relationships between objects that frequently appear together in training data. *Why needed:* The hallucination findings suggest models may be exploiting these patterns rather than genuine reasoning. *Quick check:* Analyze whether hallucinated objects are those that commonly co-occur with presented objects in training data.

**Cross-attention mechanisms:** How models attend to relevant features across multiple input modalities and images. *Why needed:* Understanding these mechanisms is crucial for diagnosing why models fail at multi-image reasoning. *Quick check:* Examine attention weights to see if models are focusing on relevant regions across scenes.

## Architecture Onboarding

**Component map:** Input images → Visual feature extractor → Cross-attention module → Reasoning layer → Output prediction

**Critical path:** Visual feature extraction → Cross-image comparison → Common object identification → Confidence scoring

**Design tradeoffs:** Single-image models vs. multi-image models (architectural modifications vs. training data), real vs. synthetic images (quality vs. control), object-level vs. scene-level reasoning (granularity vs. complexity)

**Failure signatures:** Hallucination of similar objects, performance collapse on complex scenes, reliance on co-occurrence patterns rather than semantic correspondence

**First experiments:**
1. Test single-image performance baseline to establish individual image processing capability
2. Evaluate cross-image reasoning with identical objects to test basic correspondence ability
3. Measure performance degradation across increasing scene complexity levels

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic image generation quality may introduce artifacts that bias model responses
- Benchmark focuses exclusively on object-level reasoning, not higher-level scene understanding
- Study doesn't isolate whether performance improvements stem from architectural changes or more training data

## Confidence
- Confidence in findings about single-image model limitations: **High**
- Confidence in explanations for failure modes: **Medium**
- Confidence in broader implications for multimodal reasoning: **Low**

## Next Checks
1. Replicate benchmark results using exclusively real images to isolate synthetic data effects
2. Test model responses with systematically manipulated object co-occurrence patterns to validate hallucination hypothesis
3. Evaluate performance on multi-object scenes requiring sequential reasoning rather than single common object identification