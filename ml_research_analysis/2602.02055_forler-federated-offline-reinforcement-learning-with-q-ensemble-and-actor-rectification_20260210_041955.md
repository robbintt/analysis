---
ver: rpa2
title: 'FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor
  Rectification'
arxiv_id: '2602.02055'
source_url: https://arxiv.org/abs/2602.02055
tags:
- policy
- offline
- learning
- local
- forler
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the challenge of offline federated reinforcement\
  \ learning (FRL) where heterogeneous, low-quality data can degrade both local policy\
  \ updates and global aggregation, a problem termed policy pollution. To address\
  \ this, the authors propose FORLER, which combines two key innovations: (1) Q-ensemble\
  \ aggregation on the server that robustly merges device Q-functions using a pessimistic\
  \ Bellman operator to mitigate pollution, and (2) \U0001D6FF-periodic actor rectification\
  \ on devices that enriches policy gradients with a zeroth-order search for high-Q\
  \ actions plus a regularizer that anchors the policy to the received global model."
---

# FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification

## Quick Facts
- arXiv ID: 2602.02055
- Source URL: https://arxiv.org/abs/2602.02055
- Reference count: 21
- Primary result: FORLER achieves robust performance in offline federated RL under heterogeneous data quality by combining Q-ensemble aggregation and δ-periodic actor rectification.

## Executive Summary
This paper tackles offline federated reinforcement learning where heterogeneous, low-quality data can degrade both local policy updates and global aggregation—a problem termed policy pollution. FORLER introduces a two-pronged solution: (1) Q-ensemble aggregation on the server that robustly merges device Q-functions using a pessimistic Bellman operator to mitigate pollution, and (2) δ-periodic actor rectification on devices that enriches policy gradients with a zeroth-order search for high-Q actions plus a regularizer anchoring to the received global model. Theoretical analysis provides safe policy improvement guarantees, and extensive experiments on D4RL MuJoCo tasks show FORLER consistently outperforms strong baselines in terms of final performance and convergence speed.

## Method Summary
FORLER operates through server-device communication where the server maintains a global Q-ensemble and policy, while devices perform local CQL updates with periodic actor rectification. The server aggregates 2K critic heads from devices using a pessimistic minimum operator and updates on an auxiliary dataset D_o. Devices perform local CQL updates (Eq. 3) plus δ-periodic actor rectification (Eq. 4-5) that samples actions, evaluates Q-values, and regularizes toward high-Q actions. The δ-periodic strategy reduces computation by limiting the frequency of the expensive rectification search. This design ensures policy pollution resistance while maintaining computational efficiency.

## Key Results
- FORLER consistently outperforms Fed-CQL, Fed-TD3BC, and FEDORA baselines on Walker2d, Hopper, HalfCheetah, and Ant tasks in final performance and convergence speed
- Policy pollution analysis demonstrates FORLER's resilience when low-quality devices join, maintaining high scores while baselines degrade
- Ablation studies confirm actor rectification contributes ~10-20% score improvement and δ-periodic strategy provides stable performance across reasonable δ values
- Theoretical analysis provides safe policy improvement guarantees under the method's aggregation and rectification framework

## Why This Works (Mechanism)

### Mechanism 1: Q-Ensemble Aggregation for Pollution Resistance
- Claim: Aggregating Q-functions (not policies) via ensemble with pessimistic targets reduces policy pollution from low-quality devices
- Mechanism: Server collects 2K critic heads, computes pointwise minimum across them for Bellman targets, and applies conservative regularization to suppress overestimation from low-quality contributors
- Core assumption: Low-quality device Q-functions exhibit higher variance/overestimation that ensemble minimum can suppress
- Evidence anchors: Abstract states "server robustly merges device Q-functions to curb policy pollution"; Section III.C Eq. (6) shows min over 2K critics plus entropy and conservative regularizer
- Break condition: If all devices have similarly poor Q-functions, ensemble minimum provides no benefit; requires at least some high-quality contributors

### Mechanism 2: Zeroth-Order Actor Rectification for Local Optima Escape
- Claim: Derivative-free search for high-Q actions, combined with regularization toward them, helps local policies escape poor optima under low-quality data
- Mechanism: For each state, sample N actions from Gaussian, evaluate Q-values, update distribution toward high-Q samples via weighted averaging, then regularize policy toward best found action with anchoring to global policy
- Core assumption: Conservative critic identifies genuinely better actions within data-supported region, not just overestimated ones
- Evidence anchors: Abstract mentions "actor rectification enriches policy gradients via a zeroth-order search for high-Q actions"; Section III.B details sampling, Q-evaluation, and regularization procedure
- Break condition: If critic is severely miscalibrated or data coverage is extremely sparse, sampled actions may not find meaningful improvements; O(I·N) Q-evaluations per rectification step may be prohibitive on constrained devices

### Mechanism 3: δ-Periodic Rectification for Computational Efficiency
- Claim: Performing actor rectification only every δ local steps reduces compute while retaining most benefits
- Mechanism: Full zeroth-order search runs at iterations τ ∈ {0, δ, 2δ, ...}; at other steps, rectified action is simply reused; amortizes search cost by factor ~δ
- Core assumption: Rectified actions remain useful for multiple consecutive policy updates without recomputation
- Evidence anchors: Abstract states "δ-periodic strategy further reduces local computation"; Fig. 6(a) shows stable performance across δ values
- Break condition: If policy changes rapidly between rectification points, stale rectified actions may misguide updates; δ must be tuned to local update intensity

## Foundational Learning

- **Conservative Q-Learning (CQL)**
  - Why needed here: FORLER builds on CQL for both local and server-side updates; understanding how CQL penalizes out-of-distribution actions is essential to grasp robustness to distribution shift
  - Quick check question: Can you explain why CQL subtracts the expectation under the dataset from the expectation under a random/reference policy?

- **Federated Averaging (FedAvg)**
  - Why needed here: FORLER departs from standard policy-weight aggregation; instead, it aggregates Q-parameters; knowing how FedAvg works clarifies what's being changed and why dataset-size weighting may be inadequate under data quality heterogeneity
  - Quick check question: In FedAvg, what are the weights for aggregation, and what assumption does this make about data quality?

- **Actor-Critic Offline RL (e.g., TD3BC)**
  - Why needed here: Local update builds on actor-critic structure with behavior cloning terms; Eq. (1) shows TD3BC's λQ − (π−a)² objective; understanding this baseline clarifies how actor rectification replaces BC term with searched action
  - Quick check question: In TD3BC, what does the (π(s)−a)² term encourage, and what happens when dataset actions are far from optimal?

## Architecture Onboarding

- **Component map:** Server (global Q-ensemble, policy π_o, auxiliary dataset D_o) -> Devices (local Q_k, π_k) -> Server (Q-ensemble aggregation)
- **Critical path:** 1) Server broadcasts Q_o^t, π_o^t to devices 2) Each device runs Γ local steps: sample minibatch, (optionally) run rectification search, update critic via Eq. (3), update actor via Eq. (4) 3) Devices upload Q_k^τ to server 4) Server runs ensemble offline RL on D_o via Eq. (6-7) to produce Q_o^(t+1), π_o^(t+1) 5) Repeat
- **Design tradeoffs:** Server computation vs. device burden (moving ensemble to server reduces device load but requires server-side dataset); Rectification frequency δ (lower = more accurate but higher compute); Ensemble size 2K (more critics = more robust pessimism but higher server memory); α₁ vs. α₂ balance (overweighting α₁ may cause local overfitting; overweighting α₂ may stall improvement)
- **Failure signatures:** Performance collapses when low-quality devices join (check if policy pollution occurring; verify ensemble aggregation using min-over-critics correctly); Local policy diverges from global (α₂ may be too small; increase anchoring strength); Excessive device latency (δ may be too small; increase periodicity or reduce N); No improvement despite training (critic may be overly conservative; check ω_c or ω_s)
- **First 3 experiments:** 1) Replicate policy pollution test: Configure 4 high-quality + 2 random-data devices on Walker2d; compare FORLER vs. FEDORA vs. Fed-CQL; expect FORLER to maintain high scores 2) Ablate actor rectification: Run FORLER with/without rectification on Hopper medium-expert; expect ~10-20% score drop without rectification 3) Sweep δ values: Test δ ∈ {1, 2, 3, 5, 10} on HalfCheetah; verify performance stable across reasonable δ, degrading only at very high values

## Open Questions the Paper Calls Out
- **Real-world IoT testbed validation:** All results are simulation-based on D4RL MuJoCo; authors explicitly state they leave real-world testbed validation regarding latency, energy consumption, and stability for future work
- **Communication-aware optimizations:** Authors list "communication-aware optimizations" as future work, noting bandwidth costs of transmitting 2K Q-function heads compared to aggregating only policy parameters is unexamined
- **Server dataset quality sensitivity:** Section II assumes server possesses auxiliary dataset D_o for aggregation, but experiments only vary device dataset quality, leaving impact of server's data properties untested

## Limitations
- Zeroth-order actor rectification lacks external validation beyond this work and may not generalize to non-MuJoCo tasks
- Reliance on public auxiliary dataset D_o for server updates may limit applicability in truly data-scarce settings
- Method's robustness to severe heterogeneity (all devices low-quality) remains unproven as ensemble aggregation only helps when some high-quality contributors exist

## Confidence
- Q-ensemble aggregation mechanism: Medium (ensemble minimum approach novel but related to existing pessimistic methods)
- Zeroth-order actor rectification: Medium (mechanism described but lacks external validation)
- Theoretical guarantees: Low-Medium (derived but tightness under realistic data heterogeneity unverified)
- Overall empirical claims: Medium (strong D4RL results but simulation-only evidence)

## Next Checks
1. Test FORLER under extreme heterogeneity where all devices have low-quality data to check if ensemble aggregation still provides benefit
2. Validate zeroth-order actor rectification on a non-MuJoCo task (e.g., Atari or a gridworld) to assess generalizability
3. Conduct ablation on ensemble size (e.g., 1K vs. 4K critics) to quantify marginal value of pessimism as ensemble grows