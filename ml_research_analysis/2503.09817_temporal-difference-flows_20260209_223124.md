---
ver: rpa2
title: Temporal Difference Flows
arxiv_id: '2503.09817'
source_url: https://arxiv.org/abs/2503.09817
tags:
- td-cfm
- td2-cfm
- learning
- td2-dd
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning long-horizon predictive
  models in reinforcement learning, specifically geometric horizon models (GHMs) that
  directly predict future states to avoid compounding errors from step-by-step unrolling.
  The authors introduce Temporal Difference Flows (TD-Flow), which leverages the temporal
  difference structure of successor measures alongside flow-matching techniques.
---

# Temporal Difference Flows

## Quick Facts
- arXiv ID: 2503.09817
- Source URL: https://arxiv.org/abs/2503.09817
- Reference count: 40
- Primary result: TD-Flow achieves 5x longer effective horizons and 4 orders of magnitude better value function MSE at horizon 100 compared to prior methods

## Executive Summary
This paper addresses the challenge of learning long-horizon predictive models in reinforcement learning, specifically geometric horizon models (GHMs) that directly predict future states to avoid compounding errors from step-by-step unrolling. The authors introduce Temporal Difference Flows (TD-Flow), which leverages the temporal difference structure of successor measures alongside flow-matching techniques. TD-Flow introduces two key variants: td2-cfm and td2-dd, which reduce gradient variance during training by using bootstrapped predictions both for sampling and as regression targets. Theoretically, the authors establish a new convergence result showing TD-Flow methods follow a Bellman-like update that converges to the successor measure, with td2-cfm having lower gradient variance compared to naive implementations.

## Method Summary
TD-Flow combines flow matching with temporal difference learning to train geometric horizon models that predict successor measures (discounted future state distributions). The method learns a vector field that transforms noise into future state distributions, using both one-step flow matching and bootstrapped targets during training. Two main variants are introduced: td-cfm and td2-cfm (flow matching), and td-dd and td2-dd (diffusion). The td2 variants reduce gradient variance by using bootstrapped predictions as both sampling targets and regression targets, creating more stable gradient estimates. The algorithm operates on batches of transitions, generating training targets by mixing real successor states with samples from a target network, then computing losses against predicted velocities.

## Key Results
- Achieves over 5x the horizon length of prior methods
- Demonstrates nearly four orders of magnitude better value function MSE at horizon 100 compared to baselines
- Shows consistent performance across four domains (maze, walker, cheetah, quadruped) and tasks
- td2-cfm achieves 10x reduction in value function MSE compared to td-cfm
- Generalized policy improvement applications show approximately 30% improvement over foundation models

## Why This Works (Mechanism)

### Mechanism 1
TD-Flow reduces gradient variance during training compared to naive bootstrapping approaches. The td2-cfm variant uses bootstrapped predictions both as sampling targets and as regression targets in the objective function, creating a more stable gradient estimate by reducing computational variance that arises from the sampling process. The variance reduction is theoretically quantified as being discounted by γ².

### Mechanism 2
The method achieves convergence to the successor measure via a Bellman-like contraction on probability paths. The algorithm defines a Bellman operator over entire probability paths rather than just single-step distributions, with each iteration updating the probability path by applying this operator. Theorem 1 proves this operator is a γ-contraction in 1-Wasserstein distance, ensuring convergence to a unique fixed point corresponding to the Monte-Carlo flow matching solution.

### Mechanism 3
Direct prediction of geometric-horizon states avoids compounding inference errors inherent in step-by-step world model unrolling. Instead of predicting the next state and unrolling for k steps (where errors compound), GHMs directly learn the distribution of states sampled from a geometric distribution over future time steps, learning to transform noise into this distribution in a single pass.

## Foundational Learning

**Concept: Successor Measures / Successor Features**
- Why needed here: This is the core object being learned, representing discounted future state occupancy rather than single-step transitions, enabling value estimation for any reward function via expectation
- Quick check question: Given a learned successor measure mπ(·|s,a) and a reward function r(s), how would you compute the value Qπ(s,a)?

**Concept: Flow Matching (Conditional Flow Matching)**
- Why needed here: The paper's generative modeling framework, learning a vector field that transforms samples from noise to future states via an ODE, with conditional version making training tractable
- Quick check question: In conditional flow matching, what is the role of the conditional vector field ut|Z versus the learned vector field vt?

**Concept: Temporal Difference (TD) Learning & Bootstrapping**
- Why needed here: The paper combines flow matching with TD principles, understanding the trade-offs of bootstrapping (reduced variance, potential bias, and instability) versus Monte Carlo returns
- Quick check question: What are the potential downsides of using a bootstrapped estimate as a regression target, and how does td2-cfm attempt to mitigate them?

## Architecture Onboarding

**Component map:**
Vector Field Network (U-Net) -> ODE Solver -> Target Network (EMA) -> Loss Function -> Sampling Procedure

**Critical path:**
1. Sample batch of transitions (S, A, S')
2. Generate training targets X1: with probability (1-γ) use S', otherwise use target network's ODE solver
3. Sample interpolation time t and corresponding intermediate point Xt
4. Compute loss comparing main network's predicted velocity vt(Xt | S, A) against combined target velocity
5. Update main network via gradient descent and target network via Polyak averaging

**Design tradeoffs:**
- Straight vs. Curved Paths: td-cfm(c) has higher variance with curved paths, while td2-cfm is robust
- Solver Steps: Fewer ODE solver steps are faster but may introduce discretization error
- Flow vs. Diffusion: Flow matching showed better empirical results, potentially due to less inherent noise

**Failure signatures:**
- Divergence at Long Horizons: td-cfm may fail to converge at high γ, producing nonsensical samples
- Mode Collapse: Insufficient variance reduction may cause model to predict only high-probability states
- Instability during GPI: Inaccurate GHM can lead to worse performance than baseline

**First 3 experiments:**
1. Baseline Comparison on Single-Policy Tasks: Train td-cfm and td2-cfm, compare value function MSE and EMD against ground truth
2. Ablation on Effective Horizon (γ): Vary γ across range, plot value function MSE against effective horizon to confirm td2-cfm robustness
3. Simple GPI Test: Implement GPI in simple environment, train policy-conditioned GHM using td2-cfm, compare against baseline policy

## Open Questions the Paper Calls Out
The paper identifies several open questions for future work: integrating consistency models or self-distillation techniques to reduce computational burden of neural ODE sampling during inference; extending convergence guarantees to account for function approximation errors; and testing the method's effectiveness for complex downstream tasks like imitation learning and off-policy evaluation beyond the planning applications demonstrated.

## Limitations
- Computational burden of sampling due to reliance on multi-step solvers, making inference more expensive than one-step generators
- Theoretical convergence results assume idealized updates where flow-matching loss is minimized exactly, not accounting for function approximation errors
- Empirical validation focuses on planning and policy evaluation, leaving effectiveness for imitation learning and off-policy evaluation untested

## Confidence

**High Confidence**: The core claim that direct prediction of geometrically-distributed future states (GHMs) avoids compounding errors compared to step-by-step unrolling is well-supported by theoretical framework and empirical results showing 5x improvement in horizon length and orders of magnitude better MSE at long horizons.

**Medium Confidence**: The specific claim that td2-cfm achieves superior performance primarily due to reduced gradient variance is supported by theory but could benefit from additional ablation studies isolating the variance component.

**Medium Confidence**: The convergence result (Theorem 1) is mathematically sound under idealized assumptions, but its practical implications for training stability and generalization remain uncertain without further empirical validation.

## Next Checks

1. **Variance Reduction Validation**: Measure and compare actual gradient variance during training between td-cfm and td2-cfm implementations using Taylor expansion analysis or control variate techniques to quantify reduction and correlation with performance.

2. **Ablation on Target Network Frequency**: Systematically vary target network EMA coefficient and mixing ratio in td2-cfm loss to empirically determine optimal configuration and validate experimental choices.

3. **Generalization Across Domains**: Test TD-Flow on environments with different characteristics (sparse rewards, high-dimensional observations, non-Markovian dynamics) to assess robustness beyond DeepMind Control Suite domains.