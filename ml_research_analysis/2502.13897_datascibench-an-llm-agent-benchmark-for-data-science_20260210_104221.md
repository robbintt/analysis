---
ver: rpa2
title: 'DataSciBench: An LLM Agent Benchmark for Data Science'
arxiv_id: '2502.13897'
source_url: https://arxiv.org/abs/2502.13897
tags:
- data
- output
- task
- file
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DataSciBench is a comprehensive benchmark designed to evaluate
  large language models on data science tasks. It addresses limitations of existing
  benchmarks by incorporating challenging, real-world prompts with uncertain ground
  truth and complex evaluation metrics.
---

# DataSciBench: An LLM Agent Benchmark for Data Science

## Quick Facts
- arXiv ID: 2502.13897
- Source URL: https://arxiv.org/abs/2502.13897
- Authors: Dan Zhang; Sining Zhoubian; Min Cai; Fengzu Li; Lekang Yang; Wei Wang; Tianjiao Dong; Ziniu Hu; Jie Tang; Yisong Yue
- Reference count: 40
- Key outcome: DataSciBench evaluates 23 LLMs on 222 data science prompts using a Task-Function-Code framework that enables automated evaluation of complex multi-step tasks

## Executive Summary
DataSciBench addresses critical limitations in existing LLM benchmarks by creating a comprehensive evaluation framework for data science tasks. The benchmark uses a novel Task-Function-Code (TFC) framework that decomposes complex prompts into evaluable components, enabling automated assessment where direct output comparison would fail. By combining LLM self-consistency with human verification, the authors generate reliable ground truth for tasks with uncertain canonical answers. The benchmark evaluates 23 models across six task types, revealing significant performance gaps between API-based models like GPT-4o and open-source alternatives, while highlighting persistent challenges in instruction-following and tool utilization.

## Method Summary
DataSciBench uses a semi-automated pipeline that first collects 222 data science prompts from multiple sources, then employs GPT-4o-mini to identify task types and generate evaluation code. The framework decomposes each prompt into Task-Function-Code tuples, where tasks specify the data science operation, functions provide programmatic validation rules, and code executes the validation. For ground truth generation, multiple LLM samples are generated and self-consistency filtering selects the most frequent outputs, which human experts then verify. The benchmark includes 25 aggregate evaluation functions covering data quality, visualization validity, and model accuracy, with a final score weighting completion rate (65%) more heavily than success rate to account for partial progress in multi-step pipelines.

## Key Results
- GPT-4o achieves significantly higher scores than open-source models, with CodeLlama-34B-Instruct showing particularly poor performance on instruction-following tasks
- All models demonstrate substantial room for improvement, with success rates remaining low even on easy prompts
- Larger models paradoxically show worse performance on some tasks, potentially due to training data biases against specific output formats
- Visualization generation remains particularly challenging, with models often producing plots that lack key components or misinterpret data relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing data science prompts into Task-Function-Code tuples enables systematic automated evaluation where direct output comparison would fail.
- Mechanism: Each prompt is parsed into a list R = {(Ti, Fi, Ci)} where Ti identifies the task type (e.g., data cleaning), Fi specifies the evaluation function (e.g., "Data Completeness"), and Ci provides executable code that programmatically validates outputs. This allows comparing structured intermediate results rather than unstructured final outputs.
- Core assumption: Complex data science tasks can be meaningfully decomposed into discrete, independently-evaluable subtasks with deterministic validation rules.
- Evidence anchors:
  - [abstract] "We propose an innovative Task-Function-Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules."
  - [section 3.3] "We aggregate all generated functions to the top-K function category, select top-K functions for each task type, and finally obtain 25 functions."
  - [corpus] WebCoderBench (arxiv 2601.02430) similarly uses comprehensive evaluation metrics for LLM-generated web apps, suggesting decomposition patterns generalize across code generation domains.
- Break condition: Tasks that require subjective judgment (e.g., "generate an insightful visualization") may resist programmatic validation.

### Mechanism 2
- Claim: LLM self-consistency sampling combined with human verification produces reliable ground truth for tasks where canonical answers are ambiguous or computationally expensive to derive.
- Mechanism: Multiple LLM samples generate candidate solutions; self-consistency (taking the most frequent output) filters noise, then human experts verify task type, evaluation function, code, and final outputs. For BCB-sourced prompts, existing test cases provide additional validation.
- Core assumption: Convergent LLM outputs across multiple samples indicate higher likelihood of correctness, and human verification catches systematic errors.
- Evidence anchors:
  - [abstract] "This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT."
  - [section 3.3] "We initially adopt a self-consistency strategy to obtain outputs and then ensure their reliability and precision by having six authors of the paper verify the default assigned prompts."
  - [corpus] Human-in-the-Loop (arxiv 2509.07010) demonstrates similar hybrid evaluation for 3D model generation, suggesting human-LLM collaboration patterns for complex outputs.
- Break condition: Systematic LLM biases that produce consistently wrong but convergent answers would evade detection.

### Mechanism 3
- Claim: Weighting completion rate (65%) more heavily than success rate and individual metrics creates evaluation that rewards partial progress while still penalizing total failure.
- Mechanism: Final Score = 0.05×(F1+F2+F3+F4+F5+VLM) + 0.05×SR + 0.65×CR. This accounts for data science tasks being multi-step pipelines where partial completion has value.
- Core assumption: In real data science workflows, code that completes most subtasks correctly is more useful than code that either fully succeeds or catastrophically fails.
- Evidence anchors:
  - [section 4.2] "The completion rate is then calculated as follows: Completion Rate (CR) = Σ(P to T) s_t / (T × s_max)" with scores of 0 (missing/fail), 1 (non-compliant), 2 (compliant).
  - [section 5.4] "The model often forgets to export required execution outcomes or just outputs undesired data" — explaining why partial completion tracking matters.
  - [corpus] MedAgentGym (arxiv 2506.04405) uses similar stepwise evaluation for biomedical code tasks.
- Break condition: Tasks where any single step failure invalidates all subsequent work (e.g., data corruption propagating through pipeline).

## Foundational Learning

- Concept: **Self-consistency prompting**
  - Why needed here: Understanding how DataSciBench generates ground truth without expensive human annotation for every prompt.
  - Quick check question: If 10 samples yield 7 identical answers and 3 different ones, what does self-consistency select?

- Concept: **Aggregate functions vs. test cases**
  - Why needed here: DataSciBench uses programmatic evaluation functions (e.g., "Data Completeness" checking null ratios) rather than exact output matching.
  - Quick check question: Why would checking "number of nulls == 0" be preferable to comparing full CSVs for data cleaning tasks?

- Concept: **Coarse-grained vs. fine-grained metrics**
  - Why needed here: The benchmark distinguishes success rate (binary: did all TFCs pass?) from completion rate (partial credit per step).
  - Quick check question: If a model successfully preprocesses data but fails visualization, what happens to SR vs. CR?

## Architecture Onboarding

- Component map: Prompt Collection -> TFC Generator -> Data Interpreter -> Evaluation Engine -> VLM Judge -> Final Score
- Critical path:
  1. Prompt → TFC extraction → Ground truth generation (self-consistency + human verify)
  2. Model generates code → Execute in sandbox → Collect outputs
  3. Run evaluation functions → Apply thresholds → Compute SR/CR/F1-F5/VLM → Weighted score
- Design tradeoffs:
  - **Precision vs. coverage**: 25 aggregated functions improve scalability but may miss edge cases specific to individual prompts
  - **Automation vs. reliability**: VLM-as-judge enables visualization evaluation but acknowledged as potentially imprecise (Section 7)
  - **Difficulty calibration**: Easy/medium/hard classification uses source-based heuristics rather than empirical difficulty
- Failure signatures:
  - **Instruction non-compliance**: Model completes subtasks but forgets to export required files (o1-mini example in Section 5.4)
  - **Tool hallucination**: Model calls non-existent library functions or invents CSV column names
  - **Format mismatch**: Larger models (CodeLlama-34B) fail to follow prompt-specified output formats, possibly due to training data biases
- First 3 experiments:
  1. Run GPT-4o and Deepseek-Coder-33B on 10 prompts from each difficulty level to establish baseline performance gaps
  2. Ablate the VLM judge by comparing human scores vs. GPT-4o-mini scores on visualization outputs to quantify evaluation noise
  3. Test instruction-following by varying output format specificity in prompts (e.g., "save to file X" vs. implicit) to identify compliance failure modes

## Open Questions the Paper Calls Out
None

## Limitations
- VLM Evaluation Reliability: The use of GPT-4o-mini as a VLM-as-a-judge for visualization scoring is acknowledged as potentially imprecise, with no comparison to human expert ratings provided.
- Ground Truth Generation Bias: While self-consistency and human verification are employed, the reliance on LLM-generated solutions as starting points may embed systematic biases.
- Evaluation Function Coverage: The 25 aggregated functions, while comprehensive, may miss edge cases specific to individual prompts or task types.

## Confidence
- Task-Function-Code Framework Design: High - The decomposition approach is well-specified and logically sound for enabling automated evaluation.
- Performance Rankings: Medium - The relative performance between API-based and open-source models is clear, but absolute scores depend on evaluation reliability.
- Generalizability Claims: Low - Limited cross-task analysis and no ablation studies on the TFC framework's effectiveness across different domains.

## Next Checks
1. Human vs. VLM Scoring Comparison: Have human experts score visualization outputs and compare distributions with GPT-4o-mini VLM scores to quantify evaluation noise.
2. Ablation on TFC Aggregation: Test the 25 aggregate functions on a subset of prompts with custom evaluation criteria to identify gaps or false positives.
3. Instruction-Following Stress Test: Systematically vary output format specificity in prompts to quantify compliance failure rates across model sizes.