---
ver: rpa2
title: 'GenFusion: Closing the Loop between Reconstruction and Generation via Videos'
arxiv_id: '2503.21219'
source_url: https://arxiv.org/abs/2503.21219
tags:
- video
- reconstruction
- view
- diffusion
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GenFusion addresses the conditioning gap between 3D reconstruction
  (requiring dense views) and 3D generation (working from single/no views) by proposing
  a reconstruction-driven video diffusion model. The method trains a video diffusion
  model using artifact-prone RGB-D renderings from masked 3D reconstruction, then
  iteratively fuses generative outputs back into reconstruction optimization.
---

# GenFusion: Closing the Loop between Reconstruction and Generation via Videos

## Quick Facts
- **arXiv ID**: 2503.21219
- **Source URL**: https://arxiv.org/abs/2503.21219
- **Reference count**: 40
- **Primary result**: GenFusion achieves PSNR of 15.29 (vs 13.07–14.17 baseline) on 3-view sparse reconstruction and 20.47 PSNR on masked input extrapolation, outperforming state-of-the-art methods.

## Executive Summary
GenFusion bridges the gap between 3D reconstruction (requiring dense views) and 3D generation (working from sparse/no views) by introducing a reconstruction-driven video diffusion model. The method trains a video diffusion model on artifact-prone RGB-D renderings from masked 3D reconstruction, then iteratively fuses generative outputs back into reconstruction optimization. This cyclical fusion enables artifact-free view synthesis and content expansion for sparse and masked inputs. Experiments demonstrate significant improvements over state-of-the-art methods on Mip-NeRF360, DL3DV, and Tanks and Temples datasets, achieving up to 2.22 PSNR improvement on sparse reconstruction and 20.47 PSNR on masked input extrapolation.

## Method Summary
GenFusion operates in two stages: first, it trains a video diffusion model using artifact-prone RGB-D renderings from masked 2D Gaussian Splatting (2DGS) reconstruction, where 75% of pixels are masked during training data generation. Second, it performs cyclic fusion by initializing 2DGS from COLMAP point clouds, warming up with input views only, then every K iterations rendering the current reconstruction along novel trajectories, denoising via diffusion, and adding generated frames to supervision. The sparsity-aware densification updates the Gaussian representation using both original and generated views via photometric loss, creating a positive feedback loop that progressively densifies primitives in previously under-observed areas.

## Key Results
- PSNR of 15.29 (vs 13.07–14.17 baseline) on Mip-NeRF360 3-view sparse reconstruction
- PSNR of 20.47 on masked input extrapolation for DL3DV
- Scene-level completion demonstrated on Tanks and Temples
- FID of 22.55 achieved with RGB-D VAE vs 26.16 with RGB VAE

## Why This Works (Mechanism)

### Mechanism 1
Masked 3D reconstruction generates realistic training data for video diffusion by simulating far-field artifacts in controlled settings. By masking 75% of pixels during reconstruction, unconstrained regions show artifact patterns (floaters, black backgrounds) that naturally occur when rendering distant viewpoints from sparse captures. The video diffusion model learns to map these artifact-prone renderings to clean ground-truth video, creating a repair model that generalizes to real sparse-view scenarios.

### Mechanism 2
Cyclical fusion between reconstruction and generation creates a positive feedback loop where each iteration improves both the 3D representation and the generated content. Every K iterations, the current 3D reconstruction is rendered along novel trajectories to produce artifact-prone videos. The video diffusion model repairs these and generates plausible content for unobserved regions, which are then added to the supervision set. The Gaussian representation is updated using both original input views and generated views via photometric loss, enabling densification of primitives in previously under-observed areas.

### Mechanism 3
Depth-conditioned video diffusion maintains 3D consistency that RGB-only models cannot achieve. By replacing the VAE with an RGB-D VAE, depth is encoded alongside RGB into a shared latent space. During training, the model learns to denoise both appearance and geometry jointly. During inference, the model generates depth maps that are geometrically plausible alongside RGB frames, providing more reliable supervision for the depth loss term in the cyclic fusion.

## Foundational Learning

- **3D Gaussian Splatting (3DGS)**: Why needed - GenFusion uses 2DGS as its 3D representation; understanding Gaussian primitives, opacity, and rasterization is essential for modifying the fusion pipeline. Quick check - Can you explain how 2DGS differs from 3DGS in representing scene geometry?
- **Video Diffusion Models (SVD, DynamiCrafter)**: Why needed - The core generator is a fine-tuned DynamiCrafter; understanding latent diffusion, temporal attention, and DDIM sampling is necessary for debugging generation quality. Quick check - When fine-tuning a video diffusion model from RGB to RGB-D, what components must be retrained vs. kept frozen?
- **Variational Autoencoders (VAE) in Latent Diffusion**: Why needed - The switch from RGB VAE to RGB-D VAE is a key architectural change; understanding latent space dimension and encoding/decoding is critical. Quick check - If the RGB-D VAE compresses 4-channel input to the same latent dimension as 3-channel RGB, what information bottleneck risks arise?

## Architecture Onboarding

- **Component map**: DL3DV videos → masked 2DGS reconstruction → artifact-prone RGB-D renderings → video diffusion training → cyclic fusion (COLMAP → 2DGS → warm-up → every K iterations: render → denoise → add to supervision)
- **Critical path**: Masked reconstruction quality → training data distribution → diffusion model generalization → novel trajectory sampling → coverage of 3D space
- **Design tradeoffs**: Resolution vs. speed (higher resolution improves FID but requires more steps), λ scheduling (aggressive generation loss early causes instability), densification strategy (gradient-based vs. sparsity-aware)
- **Failure signatures**: Needle-like artifacts (diffusion model not generating plausible depth), blurry outputs in large unobserved regions (temporal inconsistency), training divergence (λ too high or depth poorly aligned)
- **First 3 experiments**: 1) Mask ratio ablation (50%, 75%, 90% masking on DL3DV subset), 2) Cyclical fusion frequency (K = 500, 1000, 2000 iterations on TnT), 3) Depth consistency validation (RGB-D VAE vs. RGB VAE + separate depth estimator)

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the fusion module be explicitly designed to model and resolve temporal/geometric inconsistencies between generated video fragments to prevent blurriness in large unobserved regions? [explicit] The authors state: "Modeling and addressing this inconsistency in the fusion module will be a key step toward achieving the next level of quality."
- **Open Question 2**: Can the computational overhead of the cyclic fusion pipeline be reduced below the current ~40-minute per-scene cost without compromising reconstruction fidelity? [explicit] The limitations section notes the method requires "additional denoising steps and slightly increasing training time (about 40 minutes per scene)."
- **Open Question 3**: Does the rigid, 4-corner patch masking strategy used for training data generation limit the model's generalization to irregularly distributed artifacts or sparse inputs? [inferred] The method divides input captures into 4 regular patches for training, potentially creating spatial bias.

## Limitations
- Computational overhead: Cyclic fusion requires ~40 minutes per scene due to iterative denoising steps
- Generalization uncertainty: Masked reconstruction artifacts may not fully generalize to all sparse-view scenarios
- Temporal consistency: Current fusion tends to average conflicting outputs rather than maintaining sharp, consistent geometry across video frames

## Confidence

- **High confidence**: Core pipeline architecture and quantitative improvements on benchmark datasets
- **Medium confidence**: Mechanism explanations for cyclical fusion improvements and generalization claims
- **Low confidence**: Long-term stability of feedback loop and behavior on highly irregular scenes

## Next Checks
1. **Generalization robustness**: Test GenFusion on scenes with highly irregular geometry (random point clouds, fractal structures) to verify generative priors don't fail catastrophically
2. **Ablation on masking strategies**: Compare 50%, 75%, 90% masking ratios systematically to determine optimal artifact diversity vs. training stability tradeoff
3. **Long-term stability analysis**: Run cyclic fusion for extended iterations (>10K) on several scenes to monitor for convergence metrics and content quality degradation