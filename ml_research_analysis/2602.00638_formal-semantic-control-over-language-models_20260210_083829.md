---
ver: rpa2
title: Formal Semantic Control over Language Models
arxiv_id: '2602.00638'
source_url: https://arxiv.org/abs/2602.00638
tags:
- latent
- semantic
- space
- inference
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis advances semantic representation learning to render
  language representations or models more semantically and geometrically interpretable,
  and to enable localised, quasi-symbolic, compositional control through deliberate
  shaping of their latent space geometry. Within a VAE framework, it explores two
  complementary research directions: (i) sentence-level learning and control, disentangling
  and manipulating specific semantic features in the latent space to guide sentence
  generation using explanatory text; and (ii) reasoning-level learning and control,
  isolating and steering inference behaviours in the latent space to control natural
  language inference.'
---

# Formal Semantic Control over Language Models

## Quick Facts
- **arXiv ID:** 2602.00638
- **Source URL:** https://arxiv.org/abs/2602.00638
- **Authors:** Yingji Zhang
- **Reference count:** 0
- **One-line primary result:** Advances semantic representation learning to enable quasi-symbolic, compositional control over language models through geometric shaping of latent spaces

## Executive Summary
This thesis develops frameworks for formal semantic control over language models by manipulating latent space geometry. It proposes treating semantic features as convex cones in latent space, enabling localized control through vector operations. The work introduces methods for disentangling semantic features using supervised invertible neural networks and controlling natural language inference by encoding reasoning patterns in parametric space. Experiments demonstrate improved interpretability and controllability of latent representations for both sentence generation and reasoning tasks.

## Method Summary
The approach centers on a VAE framework where semantic features are formalized as convex cones in latent space. The method uses Optimus (BERT encoder + GPT-2 decoder) integrated with an Invertible Neural Network (INN) to disentangle semantic features. Training occurs in two stages: first training the Optimus autoencoder, then freezing it and training the INN with cluster supervision based on semantic role labeling annotations. The model uses ELBO loss for the VAE and supervised INN loss mapping embeddings to cluster centers. Geometrical data augmentation is applied by averaging vectors sharing role-content tags.

## Key Results
- Formalization of semantic features as convex cones enables localized semantic control through vector operations
- Cluster-supervised INNs achieve better separation of semantic features compared to unsupervised approaches
- Dual-encoder architecture improves syntactic-semantic disentanglement
- Encoding reasoning patterns in parametric space enables controllable natural language inference

## Why This Works (Mechanism)

### Mechanism 1: Semantic Geometry as Convex Cones
Treating formal semantic features as convex cones allows sentence meaning to be localized at cone intersections. Manipulating vectors between cone intersections effectively swaps semantic attributes. This relies on the latent space being continuous and structured enough for linear separability of high-level concepts.

### Mechanism 2: Supervised Invertible Disentanglement
Integrating flow-based INNs with cluster supervision improves separation of entangled semantic features. The INN learns a bijective mapping from entangled space to Gaussian space, clustering similar role-content pairs together. This increases density and separation of semantic regions when conditioned on semantic role clusters.

### Mechanism 3: Gradient Alignment via Symbolic Prefixes
Prefixing inputs with symbolic inference types aligns gradient updates into separable functional subspaces. This causes gradients for different reasoning operations to become approximately orthogonal in parameter space, reducing interference between reasoning patterns during training.

## Foundational Learning

- **Variational Autoencoders (VAEs) & Latent Geometry**: The thesis relies on VAEs to create a "latent sentence space" where geometry can be manipulated. Understanding the KL divergence trade-off is critical for preventing posterior collapse.
  - Quick check: Can you explain how the KL divergence term in a VAE forces the latent space to resemble a Gaussian distribution, and why this facilitates interpolation?

- **Semantic Role Labeling (SRL) & Abstract Meaning Representation (AMR)**: These formal semantic frameworks define the "ground truth" structure that neural models try to learn. Without these, "quasi-symbolic" control lacks a definition.
  - Quick check: How does SRL decompose a sentence into a "predicate-argument" structure, and why is this preferred over simple constituency parsing for semantic control?

- **Neural Tangent Kernel (NTK) Theory**: Provides theoretical justification for why adding "inference type" prefixes might work by predicting that different functions can be learned in orthogonal subspaces if gradients are properly aligned.
  - Quick check: In the context of NTK, what does it mean for two distinct functions to have "orthogonal" gradient updates, and how does that prevent interference during training?

## Architecture Onboarding

- **Component map:** Input Sentence → SRL/Graph Annotation → Encoder → Latent Vector z → (Control Point) → Decoder → Controlled Output
- **Critical path:** The control point involves either INN-based disentanglement or prefix-based gradient alignment
- **Design tradeoffs:**
  - Continuous vs. Discrete: Continuous spaces allow smooth interpolation but may suffer from KL vanishing; discrete spaces offer harder boundaries but may lose semantic nuance
  - Supervision vs. Unsupervised: Supervision improves disentanglement but requires labeled data
- **Failure signatures:**
  - Posterior Collapse: Decoder ignores latent vector z, resulting in loss of control
  - Gradient Overlap: Reasoning type gradients are not orthogonal, leading to interference
  - Entangled Cones: Convex cones overlap significantly, producing incoherent outputs from arithmetic operations
- **First 3 experiments:**
  1. Train standard Optimus VAE and perform latent traversal to observe semantic attribute consistency
  2. Implement INN with and without cluster supervision, measuring cluster separation via t-SNE and KNN accuracy
  3. Fine-tune T5 with symbolic inference prefixes, measuring gradient cosine similarities to verify subspace separation

## Open Questions the Paper Calls Out

- How can semantic representation learning be leveraged to improve compositional generalisation in NLI tasks?
- Can autoregressive LLMs exhibit human-interpretive compositional behaviours in their latent spaces for NLI?
- How can structured semantic representations be effectively integrated into diffusion-based language models to enhance sampling efficiency and controllability?

## Limitations

- The convex cone formalization assumes linear separability in latent space, which may not hold for complex semantic relationships
- Cluster-supervised INN approach depends heavily on the quality of semantic role labeling annotations
- Gradient alignment mechanism via symbolic prefixes relies on NTK theory approximations that may not fully translate to finite-width practical models

## Confidence

- **High Confidence:** Geometric formalization of semantic features as convex cones, basic reconstruction metrics (BLEU, perplexity)
- **Medium Confidence:** Supervised disentanglement improvements, syntactic-semantic separation via dual encoders
- **Low Confidence:** Gradient alignment through symbolic prefixes, reasoning pattern encoding in parametric space

## Next Checks

1. **Latent Space Linearity Test:** Systematically test linear vs. non-linear traversals between semantic concepts in the Optimus VAE latent space. Measure semantic coherence along both paths to validate the convex cone assumption.

2. **Annotation Dependency Analysis:** Train models with varying levels of SRL annotation noise (0-30%) and measure the degradation in disentanglement quality. This quantifies the impact of annotation quality on the control framework.

3. **Finite-Width Gradient Analysis:** Implement the symbolic prefix training but measure actual gradient cosine similarities (not just NTK predictions) between different reasoning types. Compare against a baseline without prefixes to empirically validate the interference reduction claim.