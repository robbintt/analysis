---
ver: rpa2
title: 'From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation Framework
  for Autonomous Vehicles'
arxiv_id: '2601.12358'
source_url: https://arxiv.org/abs/2601.12358
tags:
- agent
- behavior
- pipeline
- vehicle
- agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes an agentic framework that uses large language
  models (LLMs) and vision models to generate behavior trees (BTs) for autonomous
  vehicles. The system includes three specialized agents: a Descriptor that assesses
  scene criticality, a Planner that constructs sub-goals, and a Generator that synthesizes
  executable BTs in XML.'
---

# From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation Framework for Autonomous Vehicles

## Quick Facts
- arXiv ID: 2601.12358
- Source URL: https://arxiv.org/abs/2601.12358
- Reference count: 29
- Primary result: Agentic framework using LLMs and vision models generates adaptive behavior trees for autonomous vehicles when baseline execution fails, successfully navigating around unexpected obstacles in CARLA+Nav2 simulation

## Executive Summary
This paper proposes an agentic framework that uses large language models (LLMs) and vision models to generate behavior trees (BTs) for autonomous vehicles. The system includes three specialized agents: a Descriptor that assesses scene criticality, a Planner that constructs sub-goals, and a Generator that synthesizes executable BTs in XML. The framework triggers only when a baseline BT fails, such as when the vehicle is blocked by an obstacle. In a CARLA+Nav2 simulation, the system successfully generated and executed a BT to navigate around unexpected obstacles (e.g., fire trucks blocking the road) without human intervention. The Descriptor agent achieved a mean absolute error of 0.5 on criticality labels, and the entire pipeline averaged 21.36 seconds generation time and 45,111 tokens per scene. The generated BTs were valid, executable, and aligned with the planner's output, demonstrating the framework's adaptability and viability for autonomous driving scenarios.

## Method Summary
The framework employs a three-agent sequential pipeline triggered by baseline BT failure. First, a Descriptor agent uses Chain-of-Symbols prompting with gpt-4o-mini to analyze the scene and output JSON with criticality assessment, issue explanation, and scene description. Second, a Planner agent receives this JSON and generates ordered sub-goals via in-context learning. Third, a Generator agent converts each sub-goal into XML BT sub-trees using few-shot examples constrained to Nav2's available leaf nodes. The system integrates with CARLA simulator, ROS2 Humble, Nav2 stack, and MPPI controller, using specified sensors (RGB camera, LiDAR, odometry) and Ackermann steering parameters.

## Key Results
- Descriptor agent achieved mean absolute error of 0.5 on criticality labels (0-1 scale)
- Complete pipeline averaged 21.36 seconds generation time and 45,111 tokens per scene
- Generated BTs were valid, executable, and successfully navigated around unexpected obstacles in CARLA+Nav2 simulation
- Framework demonstrated adaptability by handling scenarios where baseline BT failed due to blocked road conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Failure-triggered activation may reduce unnecessary LLM invocations by only engaging the agentic pipeline when baseline BT execution fails.
- Mechanism: The system continuously monitors the root node status of the static Nav2 behavior tree. When `B_base(e) = Failure`, this signal initiates the three-agent pipeline. This gating mechanism prevents token expenditure on routine scenarios the baseline BT can handle.
- Core assumption: Baseline BT failures reliably correlate with scenarios requiring novel behavior generation, and failure detection latency is acceptable for safety-critical timelines.
- Evidence anchors:
  - [abstract]: "system triggers only upon baseline BT failure, demonstrating successful navigation around unexpected obstacles"
  - [Section III-A4]: "In the condition that the root node of this baseline tree returns Failure, the entire agentic pipeline initiates execution."
  - [corpus]: Related work (VLM-driven Behavior Tree) uses similar reactive generation but lacks explicit failure-gating; this paper's contribution is the conditional trigger.
- Break condition: If baseline BT fails silently without returning explicit Failure status, or if failure detection introduces unsafe latency (>2-3 seconds in dynamic scenarios), the gating mechanism becomes unreliable.

### Mechanism 2
- Claim: Sequential agent specialization with structured JSON handoffs enables modular reasoning decomposition across scene understanding, planning, and code generation.
- Mechanism: The Descriptor (LMM with Chain-of-Symbols prompting) outputs `isCritical`, `issueExplanation`, and `sceneDescription` as JSON. The Planner receives this, generates ordered sub-goals in natural language. The Generator converts each sub-goal to XML BT sub-trees using few-shot examples constrained to Nav2's available leaf nodes.
- Core assumption: Each agent's output quality is sufficient for downstream consumption; errors do not compound catastrophically across the pipeline.
- Evidence anchors:
  - [abstract]: "Descriptor agent applies chain-of-symbols prompting to assess scene criticality, a Planner agent constructs high-level sub-goals via in-context learning, and a Generator agent synthesizes executable BT sub-trees in XML format"
  - [Section III-B1]: "The first key, isCritical, is just a boolean value... If the value is False, the entire pipeline halts."
  - [corpus]: LLM-HBT paper similarly decomposes BT construction but uses different agent roles; this paper's Descriptor-Planner-Generator split is novel for AV domain.
- Break condition: If Descriptor hallucinates scene elements (e.g., non-existent obstacles), downstream agents propagate errors. Current 0.5 MAE on criticality suggests non-trivial error rates.

### Mechanism 3
- Claim: Chain-of-Symbols (CoS) prompting may improve spatial reasoning efficiency by representing objects and relations as compact symbols rather than verbose natural language.
- Mechanism: Instead of describing "the fire truck is 15 meters ahead blocking the left lane," CoS assigns symbols (e.g., "A = fire_truck, B = ego_vehicle, rel(A,B) = blocking_front"). This reduces token count while preserving spatial relationships for the LMM's reasoning process.
- Core assumption: LMMs can reliably map visual features to symbolic representations and perform spatial inference on compressed symbol sequences.
- Evidence anchors:
  - [Section III-B1]: "The flavor of CoT used was Chain of Symbols (CoS), where objects that are detected are given arbitrary symbols, and the physical relations constructed between each object are also given a symbol. This streamlines the token footprint and boosts the accuracy of the model in spatial related tasks."
  - [Section IV-B]: Descriptor consumes 38,488 of 45,111 total tokens (85%), suggesting spatial reasoning is the dominant cost.
  - [corpus]: No direct comparison in corpus; CoS for AV scene understanding appears novel.
- Break condition: If symbol grounding fails (wrong object assigned to symbol), entire reasoning chain corrupts. No quantitative CoS accuracy reported in paper—this is an assumption.

## Foundational Learning

- **Concept: Behavior Trees (BTs)**
  - Why needed here: The entire framework generates BTs; you must understand hierarchical task decomposition, tick signals, and node types (Sequence, Selector, Fallback, Recovery) to debug generated outputs.
  - Quick check question: Given a Sequence node with children [A, B, C], what happens if B returns Failure?

- **Concept: In-Context Learning (ICL) with Few-Shot Prompting**
  - Why needed here: All three agents use ICL to adapt without fine-tuning. Understanding how examples in prompts steer output is critical for improving agent performance.
  - Quick check question: Why might ICL fail on scenarios semantically distant from few-shot examples?

- **Concept: ROS2 Publisher-Subscriber Architecture**
  - Why needed here: The system integrates CARLA simulator, Nav2 stack, and custom agents via ROS2 topics. Debugging requires tracing message flows across nodes.
  - Quick check question: What ROS2 tool would you use to monitor the /failure topic that triggers the agentic pipeline?

## Architecture Onboarding

- **Component map:**
  [CARLA Simulator] → sensors → [SLAM Layer (slam_toolbox)]
                              ↓
                        [Nav2 Stack] ←→ [Baseline BT]
                              ↓
                        [Failure Signal] → [Agentic Pipeline]
                              ↓                    ↓
                   [Descriptor (GPT-4o-mini)] → JSON scene analysis
                              ↓
                   [Planner (reasoning LLM)] → sub-goal sequence
                              ↓
                   [Generator (reasoning LLM)] → XML BT
                              ↓
                        [Human Review] → [Nav2 BT Update]

- **Critical path:** Descriptor → Planner → Generator. The Descriptor's 10.24s generation time dominates latency (48% of total). Optimizing or bypassing Descriptor (e.g., using traditional CV for criticality) yields largest speedup potential.

- **Design tradeoffs:**
  - Remote API vs. local LLM: Current 21.36s latency includes network overhead; local deployment could reduce this but may sacrifice reasoning quality with smaller models.
  - Human-in-loop vs. fully autonomous: Paper requires manual review of generated BTs before execution. Removing this speeds deployment but risks malformed XML or unsafe behaviors.
  - Static baseline vs. always-on agentic: Failure-triggered design reduces cost/latency but assumes baseline covers all routine cases.

- **Failure signatures:**
  - Baseline BT never fails → agentic pipeline never activates (could indicate over-conservative baseline or incorrect failure detection)
  - Descriptor returns `isCritical: false` for actual hazards → pipeline halts prematurely (0.5 MAE suggests ~25% misclassification rate)
  - Generator produces malformed XML → runtime parsing error; currently caught by human review
  - Generated BT parameters wrong (e.g., turn angle) → execution fails; paper notes manual tuning was required

- **First 3 experiments:**
  1. **Latency profiling:** Instrument each agent with timestamps to identify if API network latency or inference time dominates the 10.24s Descriptor cost. This informs whether local deployment is worth pursuing.
  2. **Criticality threshold sweep:** Vary the `isCritical` decision threshold and measure false positive/negative rates on held-out BDD-X scenes. Determine if a traditional CNN classifier would outperform the LMM approach.
  3. **Failure mode injection:** Deliberately provide the Planner with incorrect scene descriptions (simulating Descriptor hallucination) and measure downstream BT quality degradation. Quantify error propagation sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the 21.36-second generation latency be reduced to sub-second levels suitable for real-time safety-critical decision-making while maintaining BT quality?
- Basis in paper: [explicit] Authors state "The average GT of the pipeline presents a challenge for deployment" and suggest "future iterations could significantly reduce latency by using locally deployed LLMs," but note potential "trade-off in terms of generation quality."
- Why unresolved: Current proof-of-concept uses remote API calls; the feasibility of locally-deployed models achieving both speed and quality remains untested.
- What evidence would resolve it: Benchmark results comparing local model variants (quantized, distilled) against cloud APIs on both latency and BT correctness metrics.

### Open Question 2
- Question: How can generated BTs be formally verified to provide safety guarantees before execution?
- Basis in paper: [explicit] Authors state "the generated BTs are not formally verified... there is no guarantee that it will always function correctly" and currently rely on manual human review.
- Why unresolved: The paper provides no verification mechanism; formal methods for LLM-generated control structures are not explored.
- What evidence would resolve it: Integration of a formal verifier (e.g., model checking) that validates generated BTs against safety properties with provable coverage.

### Open Question 3
- Question: What architectural improvements would enable the system to handle fundamentally different problem structures rather than only surface-level variations?
- Basis in paper: [explicit] Authors note the pipeline "struggles with complex patterns" and hypothesize this stems from "the current simplicity of the agentic architecture, which may lack the capacity for deeper contextual reasoning required in novel scenarios."
- Why unresolved: The three-agent sequential architecture has not been tested against diverse scenario types (static vs. dynamic obstacles, multi-agent interactions).
- What evidence would resolve it: Evaluation across systematically varied scenario categories showing improved generalization with enhanced agent architectures (e.g., hierarchical planning, memory mechanisms).

### Open Question 4
- Question: How well does the framework transfer from CARLA simulation to real-world driving conditions?
- Basis in paper: [explicit] Authors acknowledge "the current evaluation framework relies heavily on a simulated environment" and "it may not fully capture the complexities and unpredictability of real-world scenarios."
- Why unresolved: No real-world testing has been conducted; sim-to-real gap for LMM-based scene understanding and BT generation is unquantified.
- What evidence would resolve it: Real-vehicle trials comparing success rates and failure modes between simulated and physical deployments across matched scenarios.

## Limitations

- The 21.36-second generation latency, while acceptable for demonstration, remains problematic for real-time safety-critical scenarios where reaction time should ideally be under 2-3 seconds
- The system requires human review of generated BTs before execution, preventing fully autonomous operation and introducing potential bottlenecks
- The 0.5 MAE on criticality labels translates to approximately 25% misclassification on a binary task, indicating non-trivial false positive/negative rates

## Confidence

- **High confidence**: The sequential agent pipeline architecture is sound and the BT generation produces syntactically valid XML. The failure-triggered activation mechanism correctly identifies when baseline execution fails.
- **Medium confidence**: The integration with CARLA+Nav2 works for demonstrated scenarios, and the BDD-X dataset evaluation provides reasonable proxy for real-world performance. However, real-world validation remains untested.
- **Low confidence**: The claimed 0.5 MAE accuracy for criticality assessment and the effectiveness of Chain-of-Symbols prompting over alternative approaches lack direct comparative validation.

## Next Checks

1. **Error Propagation Analysis**: Systematically inject synthetic errors at each agent stage (Descriptor hallucination, Planner confusion, Generator syntax errors) and measure downstream BT quality degradation to quantify system robustness.

2. **Latency Reduction Validation**: Profile each agent's actual contribution to the 21.36s latency and test local LLM deployment versus remote API to quantify achievable speed improvements without sacrificing accuracy.

3. **Generalization Testing**: Evaluate the framework on held-out BDD-X scenes and additional CARLA scenarios with varying obstacle types, weather conditions, and traffic densities to assess robustness beyond the single fire truck demonstration.