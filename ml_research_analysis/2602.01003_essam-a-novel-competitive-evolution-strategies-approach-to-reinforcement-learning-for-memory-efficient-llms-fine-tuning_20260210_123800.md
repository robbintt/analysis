---
ver: rpa2
title: 'ESSAM: A Novel Competitive Evolution Strategies Approach to Reinforcement
  Learning for Memory Efficient LLMs Fine-Tuning'
arxiv_id: '2602.01003'
source_url: https://arxiv.org/abs/2602.01003
tags:
- essam
- training
- memory
- reward
- b-instruct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes ESSAM, a zero-order fine-tuning method that
  combines Evolution Strategies with Sharpness-Aware Maximization to improve mathematical
  reasoning in LLMs while drastically reducing GPU memory usage. It addresses the
  high memory cost of RL fine-tuning by introducing neighborhood probing and a two-stage
  evaluation update that guide optimization toward flatter, more robust solutions.
---

# ESSAM: A Novel Competitive Evolution Strategies Approach to Reinforcement Learning for Memory Efficient LLMs Fine-Tuning

## Quick Facts
- **arXiv ID:** 2602.01003
- **Source URL:** https://arxiv.org/abs/2602.01003
- **Reference count:** 40
- **Key outcome:** Zero-order fine-tuning method achieving RL-comparable GSM8K accuracy while reducing GPU memory usage by 10-18× through seed replay and in-place updates.

## Executive Summary
ESSAM addresses the high memory cost of RL fine-tuning for LLMs by introducing a zero-order optimization method that combines Evolution Strategies with Sharpness-Aware Maximization. It achieves comparable mathematical reasoning performance to PPO and GRPO while using only forward passes and maintaining inference-level memory consumption. The method uses neighborhood probing to guide optimization toward flatter, more robust solutions and implements memory-saving techniques including seed replay evaluation and decomposed in-place updates.

## Method Summary
ESSAM adapts Evolution Strategies for LLM fine-tuning by introducing a two-stage neighborhood probing mechanism inspired by Sharpness-Aware Maximization. In Stage 1, it computes a SAM neighborhood point by moving parameters opposite to the reward gradient estimate, then in Stage 2 evaluates perturbations around this point to update parameters. The method eliminates gradient computation entirely, relying only on forward passes and reward evaluation. Memory efficiency is achieved through seed-based perturbation replay and in-place parameter updates that regenerate noise layer-by-layer rather than storing full perturbed copies.

## Key Results
- Achieves 78.27% average accuracy on GSM8K, comparable to PPO (77.72%) and GRPO (78.34%)
- Reduces GPU memory usage by 18× compared to PPO and 10× compared to GRPO
- Maintains inference-level memory consumption (2.99-21.45 GiB) across 0.5B-8B models
- Demonstrates 2-3.5× longer runtime on smaller models due to double evaluation requirement

## Why This Works (Mechanism)

### Mechanism 1: Zero-order gradient estimation
ESSAM achieves RL-comparable performance using only forward passes, eliminating gradient-related memory overhead. Evolution Strategies samples N Gaussian perturbations, applies them to parameters, evaluates rewards via forward generation, then updates parameters using reward-weighted noise aggregation. No backpropagation means no activation storage or optimizer states. The method assumes the reward landscape is smooth enough that local random perturbations provide sufficient gradient-like signal for optimization.

### Mechanism 2: Two-stage neighborhood probing
The two-stage neighborhood probing guides optimization toward flatter minima, improving generalization over standard ES. Stage 1 computes a "SAM neighborhood point" by moving parameters opposite to the reward gradient estimate. Stage 2 samples new perturbations at this neighborhood point, evaluates rewards, and uses those for the actual update. This probes the loss landscape curvature before committing to updates, biasing toward flat regions where reward remains stable under perturbation.

### Mechanism 3: Memory-efficient evaluation
Seed Replay Eval and Decomposed In-place Update enable inference-level memory usage regardless of population size. Instead of storing all perturbed parameters, ESSAM stores only random seeds. For each evaluation, it regenerates noise layer-by-layer using the seed, applies perturbation in-place, evaluates, then restores. Updates also proceed layer-by-layer without materializing full parameter copies. This trades computation for memory by regenerating noise rather than storing population.

## Foundational Learning

- **Zero-order optimization:** ESSAM's entire approach relies on gradient-free updates; understanding why random perturbations can approximate gradients is essential for debugging convergence issues.
  - Quick check question: If you double the population size N, would you expect the update variance to increase or decrease, and why?

- **Sharpness-Aware Minimization (SAM):** The paper adapts SAM from gradient-based to zero-order settings; understanding the original SAM intuition (flat vs. sharp minima) clarifies why the two-stage procedure helps generalization.
  - Quick check question: Why might a parameter setting that achieves high training reward but has high sharpness (rapidly changing loss nearby) perform poorly on test data?

- **In-place operations and seed-based reproducibility:** ESSAM's memory efficiency depends critically on careful seed management and layer-by-layer perturbation/restoration; implementation errors here cause subtle bugs.
  - Quick check question: If two parallel processes use the same random seed simultaneously, what would happen to the perturbation vectors they generate?

## Architecture Onboarding

- **Component map:** Main process -> Worker processes (P total) -> Reward function -> Parameter broadcast
- **Critical path:** Sample N seeds → distribute to workers → Workers: regenerate noise, perturb in-place, forward pass, compute reward, restore parameters → Aggregate rewards → Stage 1: Compute θSAMt via DIPU with reverse update → Broadcast θSAMt → Repeat steps at neighborhood point → Stage 2: Update θt+1 via DIPU with forward update → Broadcast new parameters
- **Design tradeoffs:** Population size N vs. update quality (larger N reduces variance but increases compute); noise scale σ vs. exploration (too small insufficient exploration, too large unstable updates); ρ (SAM step) vs. generalization benefit (larger ρ explores wider neighborhood but may overshoot); parallelism P vs. memory (more workers = more memory for model copies, but faster iteration)
- **Failure signatures:** Rewards not improving (check noise scale appropriateness); memory higher than expected (verify in-place operations aren't creating copies); divergent behavior across workers (ensure seeds are unique and properly distributed); training reward increases but test accuracy flat (may be overfitting despite SAM)
- **First 3 experiments:** 1) Baseline reproduction on Qwen-2.5-0.5B with paper hyperparameters, target ~54% GSM8K accuracy within 500-800 iterations; 2) Ablation of SAM comparing standard ES vs ESSAM on same model/dataset, expect faster convergence and ~2-3% higher final test accuracy; 3) Population size scaling testing N∈{20, 40, 80} on Qwen-2.5-1.5B, plot iterations-to-convergence and final accuracy vs. N

## Open Questions the Paper Calls Out

### Open Question 1
Can ESSAM be effectively adapted for standard Reinforcement Learning from Human Feedback (RLHF) settings where rewards are derived from a learned neural network rather than a rule-based function? The paper exclusively evaluates ESSAM on GSM8K using a rule-based reward function and does not test the method on tasks requiring a learned reward model. It is unclear if the zero-order gradient estimation remains stable or efficient when the reward signal itself is noisy or non-stationary.

### Open Question 2
Does the performance and memory advantage of ESSAM scale effectively to models with parameter counts significantly larger than 8B? The experiments are limited to models between 0.5B and 8B parameters, and the estimated gradient and variance depend on the number of parameters and population size, suggesting potential scaling challenges typical of Evolution Strategies.

### Open Question 3
Can the computational overhead of the two-stage evaluation be reduced to improve time efficiency on smaller models where memory is not the primary bottleneck? Table 2 indicates that ESSAM takes significantly longer (2x to 3.5x) than GRPO on smaller models due to the double evaluation requirement per iteration.

## Limitations
- Experimental base limited to GSM8K only, 4-5 model scales, 2-3 seeds per condition, limiting generalization claims
- SAM adaptation mechanism lacks independent theoretical validation beyond the paper's Proposition 3.1
- Computational overhead of double evaluation significantly impacts runtime on smaller models where memory is sufficient

## Confidence
- **High confidence:** Memory efficiency claims (18× reduction vs PPO/GRPO) - directly measured and clearly demonstrated
- **Medium confidence:** GSM8K performance parity - results align with expectations but test on additional datasets needed
- **Low confidence:** SAM mechanism effectiveness - limited ablation studies, no theoretical grounding beyond self-proof

## Next Checks
1. Test ESSAM on MATH benchmark and/or AIME 2024 problems to verify reasoning capabilities transfer beyond GSM8K's elementary school math
2. Systematically compare standard ES vs ESSAM on models 0.5B-8B with multiple seeds to quantify generalization benefit magnitude and consistency
3. Measure both memory and wall-clock time as population size N varies from 20 to 120 on 3B-8B models to identify optimal tradeoffs between convergence speed and computational cost