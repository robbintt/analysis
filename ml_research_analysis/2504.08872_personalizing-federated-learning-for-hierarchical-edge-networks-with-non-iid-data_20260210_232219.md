---
ver: rpa2
title: Personalizing Federated Learning for Hierarchical Edge Networks with Non-IID
  Data
arxiv_id: '2504.08872'
source_url: https://arxiv.org/abs/2504.08872
tags:
- edge
- data
- each
- test
- phe-fl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of data heterogeneity in hierarchical\
  \ federated learning (HFL) for edge networks, where devices connected to the same\
  \ edge often share geographic or contextual similarities, leading to varying edge-level\
  \ data heterogeneity with different subsets of labels per edge, on top of device-level\
  \ heterogeneity. The proposed Personalized Hierarchical Edge-enabled Federated Learning\
  \ (PHE-FL) personalizes each edge model to perform well on the unique class distributions\
  \ specific to each edge by creating different cloud-aggregated models for each edge\
  \ and using a dynamic parameter \u03B1 to balance contributions from the edge-aggregated\
  \ and cloud-aggregated models."
---

# Personalizing Federated Learning for Hierarchical Edge Networks with Non-IID Data

## Quick Facts
- arXiv ID: 2504.08872
- Source URL: https://arxiv.org/abs/2504.08872
- Reference count: 40
- Primary result: PHE-FL achieves up to 83% higher accuracy than baseline federated learning methods in hierarchical edge networks with non-IID data.

## Executive Summary
This paper proposes Personalized Hierarchical Edge-enabled Federated Learning (PHE-FL) to address data heterogeneity in hierarchical federated learning (HFL) for edge networks. Traditional HFL approaches struggle with varying edge-level data distributions, where devices connected to the same edge often share geographic or contextual similarities. PHE-FL creates edge-specific cloud-aggregated models and uses a dynamic parameter α to balance contributions from edge-aggregated and cloud-aggregated models, achieving significantly higher accuracy and stability compared to existing approaches.

## Method Summary
PHE-FL operates in a three-level architecture (Client → Edge → Cloud) and personalizes each edge model to perform well on the unique class distributions specific to each edge. The cloud generates K distinct Cloud-Aggregated Models (CAM_i), each excluding the target edge's data to create an unbiased external knowledge baseline. Each edge then calculates a dynamic weight α based on the accuracy ratio between its local Edge-Aggregated Model (EAM) and the specific CAM on a small Personalization Test Set (PTD). The final Personalized Edge-Aggregated Model (PEAM) is a weighted combination of EAM and CAM using this dynamic α, allowing the system to adapt to varying levels of edge-level non-IIDness while maintaining stability.

## Key Results
- PHE-FL achieves up to 83% higher accuracy compared to existing federated learning approaches that incorporate edge networks under the same training conditions
- The method exhibits improved stability with reduced accuracy fluctuations compared to FedAvg with two-level aggregation
- PHE-FL is evaluated across 4 scenarios with varying levels of edge-level non-IIDness, including extreme IoT device-level non-IIDness where each device has only one label

## Why This Works (Mechanism)

### Mechanism 1
Excluding the target edge from cloud aggregation creates relevant "external knowledge" by isolating knowledge transfer potential from other edges. Instead of a single global model, the cloud generates K Cloud-Aggregated Models (CAM_i) per round, where for edge i, CAM_i is the weighted average of all models from edges j ≠ i. This allows fair evaluation of cross-edge collaboration without contamination from the target's own data.

### Mechanism 2
A dynamic parameter α balances local relevance versus global generalization by reacting to real-time accuracy signals. Each edge evaluates its local EAM and specific CAM against a small Personalization Test Set (PTD). The dynamic weight α is calculated as the normalized accuracy ratio: α = Acc(EAM) / (Acc(EAM) + Acc(CAM)), linearly interpolating the final PEAM toward the more performant source.

### Mechanism 3
Shifting the optimization target from a single global distribution to distinct edge-level distributions ensures the model optimizes for specific label subsets present at each location. The paper formalizes the objective function to minimize loss on D_k (data at edge k) rather than D (total data), operationalized by locating test sets on edge servers rather than the cloud, forcing evaluation metrics to reflect edge-specific optimization goals.

## Foundational Learning

- **Concept: Hierarchical Federated Learning (HFL)** - Understanding the 3-level architecture (Client → Edge → Cloud) is essential to see where PHE-FL injects personalization logic at the Edge → Client distribution step. Quick check: In standard HFL, does the cloud see raw device updates or only aggregated edge models? (Answer: Only aggregated edge models).

- **Concept: Non-IID Data (Label Skew)** - The core friction is "Hierarchical Non-IIDness," requiring distinction between Device-level skew (a phone sees only one class) and Edge-level skew (a region sees only 5 out of 10 classes). Quick check: If Edge A sees only cats and Edge B sees only dogs, would a global average model be robust? (Answer: No, it would likely suffer from catastrophic forgetting or weight divergence).

- **Concept: Model Interpolation / Weighted Averaging** - PHE-FL fundamentally uses a weighted sum of two models (α · Local + (1-α) · Global). Understanding that neural network weights can be mathematically combined to create functional "middle ground" models is essential. Quick check: Does interpolation require retraining the model? (Answer: No, it is a post-processing step on the weights).

## Architecture Onboarding

- **Component map:** IoT Device → Edge Server → Cloud Server → Edge Server → IoT Device
- **Critical path:**
  1. **Upward Pass:** Device Train → Edge Agg (EAM) → Cloud
  2. **Processing:** Cloud generates K unique CAM models
  3. **Downward Pass:** Cloud sends specific CAM_k to Edge k
  4. **Personalization:** Edge k tests EAM_k vs CAM_k on PTD_k → computes α → creates PEAM_k → broadcasts PEAM_k to devices

- **Design tradeoffs:**
  - **Compute vs. Fit:** Cloud creates K models per round instead of 1, trading O(K) aggregation overhead for local accuracy gains
  - **Privacy vs. Personalization:** PHE-FL assumes Edge Server can hold a PTD (15% test set). If strict privacy forbids storing even validation labels at the edge, this mechanism requires modification

- **Failure signatures:**
  - **Alpha Oscillation:** If α fluctuates wildly between rounds (e.g., 0.2 → 0.9 → 0.1), indicates PTD is too small or CAM is inconsistent
  - **Global Collapse:** If CAM consistently outperforms EAM everywhere, α trends to 0, effectively reducing to standard global FL
  - **Memory Bottleneck:** Cloud server OOM when scaling to many edges (K > 100), as it must store K versions of the model in memory

- **First 3 experiments:**
  1. **Sanity Check (Alpha Extremes):** Set α=0 (Global) and α=1 (Local) manually on MNIST D1 (Extreme Non-IID) scenario to verify PHE-FL performs better than either extreme
  2. **Scaling Test:** Increase number of edges (K) to 50 or 100 while keeping cloud instance fixed to measure O(K) aggregation overhead latency
  3. **Data Ablation:** Run PHE-FL on CIFAR-10 using "Balanced" vs. "Imbalanced" PTD to see if α calculation is sensitive to validation data distribution

## Open Questions the Paper Calls Out
1. **Physical Deployment Performance:** How does PHE-FL perform in physical, real-world edge deployments characterized by unstable network connectivity and device dropouts? The current study relies on simulations using AWS instances and static datasets that may not capture communication latency and system heterogeneity of physical IoT environments.

2. **Generalization to Other Domains:** Can the dynamic parameter α mechanism effectively generalize to non-image domains such as Natural Language Processing (NLP) or time-series forecasting? The evaluation is restricted to image classification datasets, and it's unproven whether accuracy-based weighting translates effectively to high-dimensional embedding spaces or sequential data distributions.

3. **Sensitivity to Test Set Quality:** How sensitive is the dynamic weighting parameter α to noise or distribution shift in the edge-held personalization test set (PTD_k)? The method updates based on accuracy of CAM and EAM on PTD_k, and if PTD_k is unrepresentative or corrupted, personalization will weight models incorrectly.

## Limitations
- The computational overhead of generating K cloud-aggregated models per round is not explicitly analyzed for scalability beyond the tested 10-edge configuration
- The evaluation focuses heavily on image classification tasks, which may not generalize to other data modalities or federated settings with stricter privacy constraints
- The claim of "improved stability" relies on a single metric (accuracy fluctuations) without exploring other failure modes

## Confidence
- **High Confidence**: The core mechanism of using edge-excluded cloud aggregation and dynamic α interpolation is clearly defined and mathematically sound, with directly reported empirical results showing performance gains
- **Medium Confidence**: The assumption that edge-level data distributions are sufficiently distinct to benefit from personalization is well-motivated but not rigorously tested across diverse datasets or network topologies
- **Low Confidence**: The generalizability of the 15% personalization test set size and the robustness of the dynamic α calculation to noisy or biased validation data are not addressed, nor are potential privacy risks from storing local test data at the edge

## Next Checks
1. **Ablation Study**: Test PHE-FL with varying personalization test set sizes (e.g., 5%, 10%, 20%) to quantify the impact on α stability and final accuracy
2. **Scalability Test**: Scale the number of edges from 10 to 50 or 100 to measure the computational overhead and latency of the O(K) cloud aggregation loop
3. **Robustness Check**: Evaluate PHE-FL on a non-image dataset (e.g., healthcare or sensor data) to assess its effectiveness across different data modalities and federated learning contexts