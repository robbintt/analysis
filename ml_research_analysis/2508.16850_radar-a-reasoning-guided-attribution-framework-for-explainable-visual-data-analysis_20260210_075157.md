---
ver: rpa2
title: 'RADAR: A Reasoning-Guided Attribution Framework for Explainable Visual Data
  Analysis'
arxiv_id: '2508.16850'
source_url: https://arxiv.org/abs/2508.16850
tags:
- reasoning
- attribution
- chart
- answer
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RADAR, a framework for attributing mathematical
  reasoning in chart-based question answering by linking generated answers and intermediate
  reasoning steps to specific chart regions through bounding boxes. The authors develop
  a semi-automatic approach to curate a benchmark dataset of 17,819 samples spanning
  line and bar charts, questions, reasoning steps, and attribution annotations.
---

# RADAR: A Reasoning-Guided Attribution Framework for Explainable Visual Data Analysis

## Quick Facts
- arXiv ID: 2508.16850
- Source URL: https://arxiv.org/abs/2508.16850
- Authors: Anku Rani; Aparna Garimella; Apoorv Saxena; Balaji Vasan Srinivasan; Paul Pu Liang
- Reference count: 27
- Key outcome: 15% attribution accuracy improvement over baselines through reasoning-guided visual grounding

## Executive Summary
RADAR introduces a novel framework for attributing mathematical reasoning in chart-based question answering by linking generated answers and intermediate reasoning steps to specific chart regions through bounding boxes. The framework employs a semi-automatic approach to curate a benchmark dataset of 17,819 samples spanning line and bar charts, questions, reasoning steps, and attribution annotations. By leveraging reasoning steps to improve attribution accuracy, RADAR demonstrates strong performance across different chart types and shows that attribution quality correlates with answer generation quality.

## Method Summary
RADAR is a reasoning-guided attribution framework that addresses the challenge of explainable visual data analysis in chart-based question answering. The framework employs a semi-automatic approach to curate a benchmark dataset of 17,819 samples spanning line and bar charts, questions, reasoning steps, and attribution annotations. The method links generated answers and intermediate reasoning steps to specific chart regions through bounding boxes, leveraging the reasoning process to improve attribution accuracy. The framework demonstrates scalability by successfully processing 1,068 pie charts while maintaining robust performance metrics across different chart types.

## Key Results
- 15% attribution accuracy improvement over baseline methods
- BERTScore correlation of ~0.90 between attribution quality and answer generation
- Successful processing of 1,068 pie charts with similar performance metrics
- Strong performance across line, bar, and pie chart types

## Why This Works (Mechanism)
RADAR works by establishing a direct link between reasoning steps and visual chart regions through bounding box annotations. The framework uses reasoning steps as intermediate guidance to improve attribution accuracy, creating a feedback loop where visual grounding supports reasoning and reasoning improves visual attribution. The semi-automatic curation approach enables efficient dataset creation while maintaining quality through iterative validation.

## Foundational Learning

**Visual grounding** - Linking reasoning steps to specific chart regions through bounding boxes. Why needed: Enables explainable AI by connecting abstract reasoning to concrete visual evidence. Quick check: Verify bounding boxes accurately capture relevant data points.

**Semi-automatic annotation** - Hybrid approach combining automated processes with human validation. Why needed: Scales dataset creation while maintaining annotation quality. Quick check: Measure inter-annotator agreement on validation samples.

**Multi-chart generalization** - Framework adaptation across different chart types. Why needed: Ensures robustness across varied visual representations. Quick check: Compare performance metrics across chart types.

## Architecture Onboarding

**Component map**: Input Charts -> Reasoning Module -> Attribution Module -> Output Answers + Bounding Boxes

**Critical path**: Chart preprocessing -> Question understanding -> Step-by-step reasoning generation -> Visual attribution assignment -> Final answer synthesis

**Design tradeoffs**: 
- Semi-automatic vs fully manual annotation (efficiency vs perfect accuracy)
- Granularity of bounding boxes (precision vs computational overhead)
- Reasoning step granularity (detailed vs concise explanations)

**Failure signatures**: 
- Incorrect bounding boxes leading to wrong reasoning steps
- Mismatch between reasoning logic and visual evidence
- Overfitting to specific chart layouts or question patterns

**First experiments to run**:
1. Cross-validation on different chart types to verify generalization
2. Ablation study removing reasoning steps to quantify their impact
3. Human evaluation of attribution quality and explanation clarity

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Semi-automatic annotation approach may introduce label noise affecting performance metrics
- Dataset coverage limited to specific chart types (line, bar, pie charts)
- Generalization to more complex chart types and real-world scenarios remains untested

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Attribution accuracy improvements | Medium |
| Cross-chart-type generalization claims | Medium |
| Benchmark dataset creation methodology | High |

## Next Checks
1. Conduct human evaluation studies to validate the semi-automatic annotation quality and assess potential label noise impact on reported metrics
2. Test framework performance on charts with overlapping data points and complex visual encodings beyond the current dataset scope
3. Evaluate temporal generalization by testing on charts with time-series data spanning multiple years or irregular time intervals