---
ver: rpa2
title: 'FairSAM: Fair Classification on Corrupted Data Through Sharpness-Aware Minimization'
arxiv_id: '2503.22934'
source_url: https://arxiv.org/abs/2503.22934
tags:
- fairness
- corrupted
- data
- across
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses fairness and robustness in image classification
  under data corruption, proposing FairSAM to mitigate performance disparities across
  demographic subgroups. The authors introduce a novel metric, Corrupted Degradation
  Disparity, to measure fairness in accuracy degradation under noise.
---

# FairSAM: Fair Classification on Corrupted Data Through Sharpness-Aware Minimization

## Quick Facts
- **arXiv ID:** 2503.22934
- **Source URL:** https://arxiv.org/abs/2503.22934
- **Reference count:** 25
- **One-line primary result:** FairSAM reduces Corrupted Degradation Disparity while maintaining accuracy across demographic subgroups under data corruption.

## Executive Summary
This paper addresses the challenge of fairness and robustness in image classification when models trained on clean data face corrupted test data. The authors introduce FairSAM, a novel optimization method that integrates instance reweighing strategies into Sharpness-Aware Minimization (SAM) to ensure equitable robustness across demographic subgroups. FairSAM achieves lower Corrupted Degradation Disparity compared to baseline methods, demonstrating that it can simultaneously improve fairness and maintain competitive accuracy under various corruption types.

## Method Summary
FairSAM modifies the standard SAM optimizer by incorporating instance reweighing to achieve fairness-aware robustness. The method assigns higher weights to samples from disadvantaged subgroups based on their group size and gradient norms, then computes per-batch perturbations that consider these weighted gradients. During training on clean data, FairSAM uses these weights to guide the optimization process toward solutions that maintain equitable performance across subgroups when tested on corrupted data. The approach is evaluated on datasets like CelebA and FairFace with various corruption types from ImageNet-C.

## Key Results
- FairSAM achieves significantly lower Corrupted Degradation Disparity (∆p) compared to Vanilla, SAM, GroupSAM, and fairness-aware techniques across multiple corruption types
- The method maintains competitive overall accuracy while improving fairness metrics, achieving higher accuracy and lower ∆p across varying corruption levels
- FairSAM demonstrates superior performance on out-of-distribution data (LFW) compared to other methods in fairness-accuracy trade-offs

## Why This Works (Mechanism)
FairSAM works by modifying the SAM optimization process to account for subgroup performance disparities. By assigning higher weights to disadvantaged groups and incorporating these weights into the perturbation calculation, the optimizer is steered toward solutions that are robust to corruption while maintaining fairness across demographic subgroups. The instance reweighing ensures that the model doesn't optimize primarily for the majority group, resulting in more equitable accuracy degradation under corrupted conditions.

## Foundational Learning
- **Concept: Sharpness-Aware Minimization (SAM)**
  - **Why needed here:** SAM is the robustness optimization technique that FairSAM modifies. Understanding its goal of finding "flat minima" in the loss landscape to improve generalization is a prerequisite for grasping how FairSAM extends it.
  - **Quick check question:** How does seeking a "flat minimum" improve a model's resilience to corrupted data?

- **Concept: Algorithmic Fairness & Subgroup Performance**
  - **Why needed here:** The paper's core motivation is to correct for unfair performance degradation across demographic subgroups. Foundational knowledge of fairness concepts like performance disparity is required.
  - **Quick check question:** What is a "performance disparity" between demographic subgroups in a classification task?

- **Concept: Instance Reweighing**
  - **Why needed here:** This is the primary mechanism FairSAM integrates into SAM to achieve fairness. Understanding how adjusting sample weights can balance a model's focus is critical.
  - **Quick check question:** How does increasing the weight of samples from a disadvantaged subgroup alter the model's training dynamics?

## Architecture Onboarding
- **Component map:** Data Loader -> Weighting Module -> FairSAM Optimizer -> Evaluator
- **Critical path:**
  1. Initialization: Load dataset and define the sensitive attribute for subgroups
  2. Weight Assignment: The Weighting Module assigns a weight to each sample, initializing the fairness mechanism
  3. FairSAM Update: The model is trained via the FairSAM Optimizer, which uses the assigned weights to bias the perturbation direction and gradient update toward equitable robustness
  4. Evaluation: After training on clean data, the model is evaluated on both clean and corrupted test sets. The Evaluator calculates the final accuracy and the ∆p metric to measure success

- **Design tradeoffs:**
  - Fairness vs. Overall Accuracy: The reweighing mechanism may sacrifice some overall accuracy to achieve a lower ∆p. The strength of this tradeoff is controlled by the reweighing scale
  - Computational Cost: FairSAM inherits the computational overhead of standard SAM (two backward passes per step) and adds the cost of weight management, making it more expensive per iteration than a vanilla optimizer
  - Hessian Approximation: FairSAM relies on a low-rank Hessian approximation for its per-batch perturbation calculation. If this assumption fails for a particular model, the fairness-aware perturbation direction may be suboptimal

- **Failure signatures:**
  - Persistent High Disparity: A high final ∆p value suggests the reweighing strategy is insufficient to overcome subgroup imbalances or model biases
  - Accuracy Collapse: A significant drop in overall accuracy indicates the fairness constraints are too strong or the model is overfitting to the weighted reweighed samples
  - Divergence: Training loss failing to decrease could signal issues with the perturbation calculation or learning rate hyperparameters

- **First 3 experiments:**
  1. Baseline Comparison: Train a standard SAM model and a Vanilla model on a chosen dataset. Compare their overall accuracy and ∆p on a corrupted test set to quantify the fairness-robustness gap
  2. FairSAM Validation: Train the FairSAM model on the same data. Confirm that it achieves a significantly lower ∆p than the baselines while maintaining competitive overall accuracy
  3. Ablation Study: Vary the corruption type (e.g., snow vs. Gaussian noise) and severity. Measure how FairSAM's performance and ∆p change relative to baselines across these different conditions

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** How does FairSAM perform across a broader range of data corruption scenarios beyond the noise types (Gaussian, snow, blur) tested in this paper?
- **Basis in paper:** [explicit] "In future work, we aim to explore more data corruption scenarios to further expand FairSAM's potential as a foundational framework for equitable and robust image classification."
- **Why unresolved:** The paper only tests a limited set of corruption types following ImageNet-C settings. Other real-world corruptions such as compression artifacts, sensor malfunctions, or domain-specific corruptions remain unexplored.
- **What evidence would resolve it:** Comprehensive experiments across the full ImageNet-C corruption benchmark (15+ corruption types) and additional domain-specific corruption scenarios, showing consistent fairness-robustness trade-offs.

### Open Question 2
- **Question:** Can FairSAM effectively handle settings where models are trained on corrupted data rather than clean data?
- **Basis in paper:** [inferred] The paper specifically focuses on "a setting where a model trained on a clean and noise-free dataset and tested on a corrupted dataset" but doesn't explore the reverse or combined scenarios.
- **Why unresolved:** Real-world applications may involve training on already corrupted data or data with varying levels of corruption. The paper's framework and metrics assume clean training data as the baseline.
- **What evidence would resolve it:** Experiments comparing FairSAM performance when trained on clean vs. corrupted data vs. mixed-corruption datasets, with analysis of how the Corrupted Degradation Disparity metric would be adapted.

### Open Question 3
- **Question:** How does FairSAM extend to scenarios with multiple sensitive attributes or non-binary demographic groups?
- **Basis in paper:** [inferred] The paper's formulation defines sensitive attributes as binary (s ∈ S = {s+, s-}) and experiments only test single sensitive attributes at a time (gender or age).
- **Why unresolved:** Real-world fairness concerns often involve intersectionality across multiple attributes and more nuanced demographic categories than binary divisions.
- **What evidence would resolve it:** Experiments with datasets containing multiple sensitive attributes (e.g., gender, race, and age simultaneously) and analysis of how the reweighing mechanism and fairness metrics scale to these settings.

## Limitations
- The paper lacks specific hyperparameter details for FairSAM implementation, making exact reproduction challenging
- Comparison with GroupSAM is limited as it only applies SAM to the disadvantaged subgroup, which may not fully represent state-of-the-art fairness techniques
- Claims about generalization to out-of-distribution data are based on limited LFW experiments without extensive validation across diverse OOD scenarios

## Confidence
- **High confidence:** The conceptual framework of integrating instance reweighing into SAM is sound and well-explained. The Corrupted Degradation Disparity metric is clearly defined and appropriate for measuring fairness under corruption.
- **Medium confidence:** The experimental results showing FairSAM's superiority over baselines are compelling, but the lack of detailed hyperparameter information and limited baseline comparisons reduce reproducibility confidence.
- **Low confidence:** The paper's claims about generalization to out-of-distribution data are based on LFW experiments but lack extensive validation across diverse OOD scenarios.

## Next Checks
1. Reproduce the baseline SAM and GroupSAM results on CelebA with Gaussian noise corruption to establish performance gaps before implementing FairSAM
2. Conduct an ablation study varying the reweighing scale and perturbation magnitude to identify optimal hyperparameter settings for different corruption types
3. Extend experiments to include additional fairness-aware baselines like FairBatch and Reweighting to provide a more comprehensive comparison