---
ver: rpa2
title: 'GUI-explorer: Autonomous Exploration and Mining of Transition-aware Knowledge
  for GUI Agent'
arxiv_id: '2505.16827'
source_url: https://arxiv.org/abs/2505.16827
tags:
- knowledge
- element
- task
- exploration
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GUI-explorer addresses challenges in GUI automation by introducing
  a training-free agent that autonomously explores app functionalities and mines transition-aware
  knowledge without manual intervention. The system combines function-aware trajectory
  exploration using structural priors with unsupervised knowledge extraction from
  state transitions, eliminating the need for fine-tuning when apps update.
---

# GUI-explorer: Autonomous Exploration and Mining of Transition-aware Knowledge for GUI Agent

## Quick Facts
- **arXiv ID:** 2505.16827
- **Source URL:** https://arxiv.org/abs/2505.16827
- **Reference count:** 40
- **Primary result:** Training-free GUI agent achieving 53.7% task success rate on SPA-Bench without fine-tuning

## Executive Summary
GUI-explorer introduces a training-free agent that autonomously explores mobile app functionalities and mines transition-aware knowledge to improve task completion. The system combines function-aware trajectory exploration using structural priors with unsupervised knowledge extraction from state transitions. By eliminating the need for manual intervention and fine-tuning when apps update, it achieves 53.7% task success rate on SPA-Bench and 47.4% on AndroidWorld, outperforming state-of-the-art methods by 2.6%-11.7%.

## Method Summary
GUI-explorer operates through three phases: autonomous exploration, knowledge extraction, and dynamic guidance. During exploration, it uses structural priors from app manifests to generate exploration goals via DFS traversal with state restoration. The knowledge extraction phase analyzes state transitions from interaction triples, filtering out ineffective actions using perceptual hashing. Finally, during execution, it retrieves and ranks relevant knowledge using visual-semantic embeddings and MLLM-based pairwise comparison, injecting ranked knowledge into reasoning prompts as "Expert Techniques."

## Key Results
- Achieves 53.7% task success rate on SPA-Bench and 47.4% on AndroidWorld
- Outperforms state-of-the-art methods by 2.6%-11.7% on both benchmarks
- Reduces prior knowledge errors by 16.0% through instruction-aware knowledge ranking
- Cross-environment knowledge transfer shows 4.3% improvement when applying Android-mined knowledge to web tasks

## Why This Works (Mechanism)

### Mechanism 1: Exploration Anchors Constrain MLLM Hallucination
- **Core assumption:** Manifest-declared activities accurately reflect actual app functionality; visual screenshots provide sufficient additional context for goal generation when manifests are incomplete.
- **Evidence:** Screenshot-only exploration achieves 22.2% success vs. 33.3% with full metadata—structural priors help but aren't strictly required.

### Mechanism 2: Transition Filtering Yields Valid Operation Logic
- **Core assumption:** Perceptual hashing reliably detects meaningful state changes; GPT-4o's 86.6% accuracy on Dynamic Comprehension task validates extraction quality.
- **Evidence:** Discard transitions where o_i ≈ o_{i+1} measured via perceptual hashing.

### Mechanism 3: Instruction-Aware Knowledge Ranking Improves Relevance
- **Core assumption:** MLLMs can reliably judge relative utility of knowledge entries; merge-sort parallelization compensates for per-comparison latency (~28.5s ranking overhead per step).
- **Evidence:** Ranking reduces Prior Knowledge error rate from 9.8% to 6.8% (3% absolute improvement).

## Foundational Learning

- **State Transition Modeling (observation, action, outcome)**
  - Why needed here: The entire knowledge extraction pipeline depends on understanding GUI interactions as state transitions rather than isolated actions.
  - Quick check question: Can you explain why discarding transitions where o_i ≈ o_{i+1} improves knowledge quality?

- **Visual-Semantic Embedding Retrieval**
  - Why needed here: Knowledge retrieval uses SigLIP embeddings to match current UI elements against stored knowledge entries.
  - Quick check question: How would you handle a UI element with no visually similar precedent in the knowledge store?

- **Depth-First Search with State Restoration**
  - Why needed here: Exploration efficiency depends on DFS branch inheritance—each branch inherits parent terminal state, avoiding BFS reset overhead.
  - Quick check question: Why does DFS outperform BFS for this exploration scenario?

## Architecture Onboarding

- **Component map:**
  - Exploration phase: Environment E → Anchor Extraction → Task Generator (MLLM) → DFS Explorer → Trajectory Store
  - Knowledge phase: Trajectories → Transition Filter → Knowledge Extractor (MLLM) → Vector Store (SigLIP embeddings)
  - Execution phase: Observation → UI Element Extraction → Knowledge Retrieval → Ranker (MLLM) → Guidance Prompt → Action

- **Critical path:** Knowledge extraction quality depends on exploration coverage; ranking effectiveness depends on knowledge store diversity. The 1,300+ knowledge items across 46 apps (Appendix H) suggest minimum viable scale.

- **Design tradeoffs:**
  - Ranking latency (28.5s/step) vs. relevance quality—merge-sort parallelizable but MLLM calls dominate
  - Manifest dependency vs. screenshot-only robustness—5.5% absolute gap suggests manifests worth acquiring when available
  - GPT-4o base model selection vs. cost—$0.064/step total cost documented in Table 2

- **Failure signatures:**
  - Perceptual errors: Agent misreads icon states (e.g., filled vs. hollow "saved" indicator)
  - Reasoning errors: Appends instead of replaces text—task decomposition failure
  - Missing knowledge errors: Exhausts step limit searching wrong UI sections

- **First 3 experiments:**
  1. Reproduce SPA-Bench Level 3 single-app results with GPT-4o base to validate setup
  2. Ablate ranking module to confirm ~3% error rate increase matches paper
  3. Test cross-environment generalization: apply AndroidWorld-mined knowledge to SPA-Bench tasks to measure 4.3% improvement claim

## Open Questions the Paper Calls Out

- **Cross-platform structural priors:** How can exploration anchors be adapted for web and desktop environments lacking mobile-specific structural metadata?
- **Ranking optimization:** Can the MLLM-based pairwise ranking process be optimized to reduce the 28.5-second per-step latency while maintaining knowledge error reduction?
- **Cross-platform knowledge transfer:** Can transition-aware knowledge mined from one platform (Android) be generalized to improve performance on another (Web) without platform-specific exploration?

## Limitations

- **Structural dependency:** Current reliance on Android manifest declarations limits direct applicability to web and desktop environments without equivalent structural metadata.
- **Latency concerns:** The O(n log n) ranking mechanism with 28.5s/step latency may become prohibitive as knowledge stores scale beyond current sizes.
- **Perceptual limitations:** The approach may fail when perceptual hashing cannot distinguish functionally different states with similar visual appearances.

## Confidence

- **High Confidence:** Core transition filtering mechanism validated at 86.6% accuracy; 16.0% reduction in prior knowledge errors is measurable and significant.
- **Medium Confidence:** Task success rates (53.7% SPA-Bench, 47.4% AndroidWorld) are impressive but may not generalize to all app types.
- **Low Confidence:** Claims about ranking latency and cost efficiency assume current knowledge store sizes and may not scale linearly.

## Next Checks

1. **Cross-platform scalability test:** Apply knowledge extraction pipeline to desktop/web applications without manifest metadata and measure performance degradation.
2. **Longitudinal stability evaluation:** Track knowledge validity across 3+ app version updates to quantify degradation rates and determine re-exploration frequency.
3. **Ranking complexity scaling:** Systematically increase knowledge store size (100, 1,000, 10,000 entries) and measure ranking latency growth against theoretical O(n log n) predictions.