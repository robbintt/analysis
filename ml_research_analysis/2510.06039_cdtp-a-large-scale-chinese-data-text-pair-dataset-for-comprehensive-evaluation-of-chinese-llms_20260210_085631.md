---
ver: rpa2
title: 'CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive Evaluation
  of Chinese LLMs'
arxiv_id: '2510.06039'
source_url: https://arxiv.org/abs/2510.06039
tags:
- tasks
- chinese
- performance
- cdtp
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Chinese Data-Text Pair (CDTP) dataset
  and a comprehensive benchmark for evaluating Chinese large language models (LLMs).
  CDTP includes over 7 million text samples paired with 15 million triples across
  four domains, addressing the scarcity of structured data for Chinese NLP.
---

# CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive Evaluation of Chinese LLMs

## Quick Facts
- **arXiv ID**: 2510.06039
- **Source URL**: https://arxiv.org/abs/2510.06039
- **Reference count**: 28
- **Primary result**: Introduces CDTP dataset with 7M+ text samples and 15M+ triples across 4 domains, enabling comprehensive evaluation of Chinese LLMs through three benchmark tasks.

## Executive Summary
This paper introduces the Chinese Data-Text Pair (CDTP) dataset, a large-scale resource containing over 7 million text samples paired with 15 million triples across four knowledge domains. The dataset addresses the critical need for structured Chinese language data for evaluating large language models. The authors develop a comprehensive benchmark that evaluates LLMs on Knowledge Graph Completion, Triple-to-Text Generation, and Question Answering tasks, revealing significant variations in model performance across different domains and tasks. The study demonstrates that supervised fine-tuning with CDTP consistently improves performance, particularly for structured knowledge tasks, while also showing that larger models generally perform better but fine-tuning can narrow performance gaps across model sizes.

## Method Summary
The researchers constructed the CDTP dataset by first collecting diverse knowledge triples from multiple sources across four domains: Medicine, Chemistry, News, and Culture & History. They then developed an automated pipeline to generate high-quality Chinese text samples corresponding to each triple, ensuring semantic consistency and linguistic quality. For the benchmark evaluation, they designed three distinct tasks: Knowledge Graph Completion (predicting missing elements in triples), Triple-to-Text Generation (generating coherent text from triples), and Question Answering (answering questions based on triple information). The evaluation employed automated metrics including exact match (EM) for knowledge completion, ROUGE and BLEU scores for text generation, and standard QA metrics. They tested multiple Chinese LLMs with varying parameter sizes (1.3B to 33B) both with and without CDTP fine-tuning to assess the dataset's impact on model performance.

## Key Results
- CDTP dataset contains 7.2 million text samples paired with 15.3 million triples across four knowledge domains
- Model performance varies significantly across tasks, with knowledge graph completion showing larger improvements from fine-tuning compared to text generation
- Supervised fine-tuning with CDTP consistently improves results across all model sizes, with larger models showing better absolute performance but smaller relative improvements

## Why This Works (Mechanism)
The CDTP dataset provides structured knowledge representation in Chinese that bridges the gap between unstructured text and knowledge graph formats. By pairing triples with high-quality text samples, the dataset enables models to learn both structured reasoning and natural language generation simultaneously. The multi-domain coverage ensures models encounter diverse knowledge patterns and linguistic expressions, improving generalization. The three-task benchmark design captures different aspects of model capability - from structured reasoning in knowledge completion to natural language fluency in generation and comprehension in QA tasks.

## Foundational Learning
- **Knowledge Graph Completion**: Understanding structured knowledge representation and reasoning over triples is essential for evaluating models' ability to handle structured data. Quick check: Verify models can predict missing elements in triples with accuracy above baseline random performance.
- **Triple-to-Text Generation**: This requires models to synthesize structured information into coherent natural language, testing their ability to bridge structured and unstructured knowledge representations. Quick check: Evaluate generated text for semantic fidelity to source triples using automated metrics.
- **Multi-domain Knowledge Integration**: Models must handle diverse knowledge domains with different terminologies and structures, testing their adaptability and generalization. Quick check: Compare cross-domain performance to identify domain-specific strengths and weaknesses.

## Architecture Onboarding

**Component Map**: CDTP Dataset Construction -> Benchmark Task Design -> Model Evaluation -> Fine-tuning Impact Analysis

**Critical Path**: Data Collection → Triple-Text Pairing → Task Formulation → Model Testing → Performance Analysis

**Design Tradeoffs**: The study balances dataset comprehensiveness against quality control, opting for automated generation of text samples to achieve scale while maintaining semantic consistency through careful validation.

**Failure Signatures**: Models may show strong performance in one task but weak performance in others, indicating task-specific weaknesses. Poor cross-domain generalization suggests overfitting to specific knowledge patterns.

**First Experiments**:
1. Baseline evaluation of models without CDTP fine-tuning to establish performance without structured knowledge training
2. Knowledge graph completion task to assess structured reasoning capabilities
3. Cross-domain performance comparison to identify domain-specific model strengths and weaknesses

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Potential domain bias in dataset construction, as domains were selected based on resource availability rather than comprehensive coverage
- Reliance on automated metrics for text quality evaluation, which may not fully capture semantic quality and factual accuracy in Chinese text
- Confounding factors from varying pre-training data amounts across different model sizes when comparing fine-tuning effects

## Confidence
- **Dataset Representativeness**: Medium - substantial size but limited to four selected domains
- **Evaluation Metric Validity**: Medium - automated metrics used but may not capture full semantic quality
- **Fine-tuning Effect Isolation**: Medium - model size variations may confound CDTP fine-tuning benefits

## Next Checks
1. Conduct ablation studies using subsets of the CDTP dataset across different domains to quantify domain-specific performance variations and identify potential biases.
2. Implement human evaluation protocols for a subset of generated text and question-answering outputs to validate automated metric scores, particularly for Chinese language nuances.
3. Test the CDTP benchmark with additional Chinese LLMs that have similar pre-training but different fine-tuning approaches to isolate the effect of CDTP training from other factors.