---
ver: rpa2
title: "An explicit formulation of the learned noise predictor $\u03B5_\u03B8({\\\
  bf x}_t, t)$ via the forward-process noise $\u03B5_{t}$ in denoising diffusion probabilistic\
  \ models (DDPMs)"
arxiv_id: '2507.04203'
source_url: https://arxiv.org/abs/2507.04203
tags:
- noise
- diffusion
- function
- forward-process
- learned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper derives an explicit formulation of the learned noise\
  \ predictor \u03B5\u03B8(xt, t) in terms of the forward-process noise \u03B5t in\
  \ denoising diffusion probabilistic models (DDPMs). The author shows that \u03B5\
  \u03B8(xt, t) can be expressed as the conditional expectation of \u03B5t under the\
  \ posterior distribution q(x0|xt), providing a clear mathematical connection between\
  \ the forward diffusion noise and the learned predictor."
---

# An explicit formulation of the learned noise predictor $ε_θ({\bf x}_t, t)$ via the forward-process noise $ε_{t}$ in denoising diffusion probabilistic models (DDPMs)

## Quick Facts
- arXiv ID: 2507.04203
- Source URL: https://arxiv.org/abs/2507.04203
- Reference count: 6
- Shows that the learned noise predictor can be expressed as the conditional expectation of forward-process noise under the posterior distribution

## Executive Summary
This paper derives an explicit mathematical formulation showing that the learned noise predictor $\epsilon_\theta(x_t, t)$ in denoising diffusion probabilistic models (DDPMs) is the conditional expectation of the forward-process noise $\epsilon_t$ under the posterior distribution $q(x_0|x_t)$. This connection clarifies the relationship between the noise prediction task and the underlying score function of the data distribution. The author provides a novel and rigorous proof of the fundamental identity linking the score function to the learned noise predictor, offering new theoretical insight into the structure of diffusion models.

## Method Summary
The paper employs variational calculus to derive the optimal noise predictor by minimizing the mean squared error between the true forward-process noise and the predictor output. The key insight is that perturbing the optimal function and computing the derivative at zero reveals that the minimizer must satisfy an integral equation equivalent to the conditional expectation formula. This approach provides a mathematically rigorous foundation for understanding why noise prediction in DDPMs implicitly captures the score function of the marginal distribution.

## Key Results
- The learned noise predictor $\epsilon_\theta(x_t, t)$ is the conditional expectation $\mathbb{E}_{q(x_0|x_t)}[\epsilon_t(x_t|x_0)]$
- The fundamental identity $\nabla_{x_t} \log q(x_t) = -\epsilon_\theta(x_t, t)/\sqrt{1-\bar{\alpha}_t}$ is rigorously proven
- The noise prediction objective implicitly captures the score of the marginal distribution through conditional averaging

## Why This Works (Mechanism)

### Mechanism 1: Conditional Expectation as the Optimal Predictor
The optimal learned noise predictor minimizes the mean squared error, and by variational calculus, this requires the predictor to equal the conditional expectation of the forward-process noise given the noisy observation. This establishes the explicit mathematical connection between the noise prediction task and the posterior distribution over clean data.

### Mechanism 2: Gradient-Score Consistency via Posterior Averaging
The proof shows that the noise predictor implicitly estimates the score of the marginal distribution because the conditional averaging of noise terms, when combined with the identity linking noise to score gradients, collapses the joint distribution gradient into the marginal gradient through marginalization.

### Mechanism 3: Deterministic Noise Inversion
The forward-process noise $\epsilon_t$ can be treated as a deterministic function of $x_t$ and $x_0$ using the algebraic relationship $\epsilon_t(x_t|x_0) = (x_t - \sqrt{\bar{\alpha}_t}x_0)/\sqrt{1-\bar{\alpha}_t}$, which converts the random noise variable into a determined residual conditioned on the clean data.

## Foundational Learning

- **Concept: Tweedie's Formula / Score Function**
  - Why needed here: The paper relies on the identity $\nabla_{x_t} \log q(x_t|x_0) \propto -(x_t - \sqrt{\bar{\alpha}_t}x_0)$ to bridge the gap between the "noise" term and the "gradient" term.
  - Quick check question: Given a Gaussian distribution $q(x_t|x_0) = \mathcal{N}(x_t; \mu, \sigma^2 I)$, what is the analytical form of $\nabla_{x_t} \log q(x_t|x_0)$?

- **Concept: Variational Calculus (Functional Optimization)**
  - Why needed here: The proof uses a variational argument (perturbing the optimal function $f^*$ by $sh$) to derive the integral form.
  - Quick check question: If $F(f) = \int L(x, f(x)) dx$, what condition must hold for $F(f)$ to be at a minimum with respect to a small perturbation $h(x)$?

- **Concept: Posterior Distribution $q(x_0|x_t)$**
  - Why needed here: The core result defines the predictor as an expectation over this posterior, representing the belief state of the clean image given the noisy one.
  - Quick check question: In a diffusion context, why is $q(x_0|x_t)$ generally intractable to calculate directly, necessitating a neural network approximation?

## Architecture Onboarding

- **Component map:** Input Layer (noisy state $x_t$ and time $t$) -> Latent State (encodes implicit representation of posterior $q(x_0|x_t)$) -> Output Head (predicts $\epsilon_\theta(x_t, t)$) -> Theoretical Map (Output Head maps to conditional expectation node $\int \epsilon_t(x_t|x_0) q(x_0|x_t) dx_0$)

- **Critical path:**
  1. Draw data pair $(x_0, \epsilon_t)$
  2. Construct $x_t$ using deterministic formula
  3. Calculate target $\epsilon_t(x_t|x_0)$ (which is just drawn $\epsilon_t$)
  4. Minimize $\|\epsilon_\theta(x_t, t) - \epsilon_t\|^2$
  5. At convergence, network output approximates average noise over all possible $x_0$ explanations for that $x_t$

- **Design tradeoffs:**
  - Explicit vs. Implicit Modeling: The paper mathematically defines the target as a conditional expectation, but standard architectures (U-Nets) do not explicitly compute the integral; they learn a function that hopefully converges to it.
  - Assumption: The paper assumes perfect convergence to $f^*$. In practice, the equality $\epsilon_\theta = \mathbb{E}[\dots]$ holds only approximately due to early stopping or capacity limits.

- **Failure signatures:**
  - Misinterpreting Diversity: Believing $\epsilon_\theta(x_t, t)$ predicts the exact forward noise $\epsilon_t$ for a specific sample rather than the conditional expectation leads to misunderstanding why diffusion models produce diverse outputs.
  - Determinism Fallacy: Believing the learned predictor is a deterministic reversal of the specific noise realization, rather than an average "drift" term derived from the posterior.

- **First 3 experiments:**
  1. Numerical Verification of Expectation: Discretize a simple 2D mixture of Gaussians, numerically compute the integral, and compare to a trained small neural network.
  2. Score Matching Check: Use the derived identity to visualize the vector field of a trained model and check if arrows point towards high-density regions of the data manifold.
  3. Ablation on Capacity: Test if smaller networks fail to approximate the conditional expectation (high variance in predictions for the same $x_t$).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the derivation of Theorem 1.1 be fully formalized by explicitly specifying the necessary function spaces and regularity conditions?
- Basis in paper: [explicit] The author states that "a fully rigorous proof would require explicitly specifying the appropriate function space," but opts for a concise derivation for accessibility.
- Why unresolved: The current proof assumes general properties of the function class without formally defining the constraints or spaces required for the variational calculus arguments to hold strictly.
- What evidence would resolve it: A formal proof supplement detailing the specific functional analytic setting and conditions under which the minimizer $f^*$ is guaranteed to exist and satisfy the integral equation.

### Open Question 2
- Question: Does the explicit formulation of $\epsilon_\theta$ as a conditional expectation suggest specific architectural constraints or inductive biases for neural networks?
- Basis in paper: [inferred] The paper concludes that the result "reinforces the interpretation of noise prediction as a form of conditional averaging," implying a mathematical structure that current neural network architectures might approximate but not explicitly enforce.
- Why unresolved: While the math shows the predictor *is* an expectation, standard networks are trained via regression without explicitly modeling the posterior distribution $q(x_0|x_t)$ or the averaging process.
- What evidence would resolve it: Studies analyzing whether architectures explicitly designed to compute conditional expectations (e.g., via attention mechanisms over prototypes) converge faster or achieve lower error than standard MLP/UNet architectures.

### Open Question 3
- Question: How does the derived formulation of the noise predictor generalize to the continuous-time SDE framework or non-Gaussian diffusion processes?
- Basis in paper: [inferred] The introduction notes that the identity (1.1) is often derived via reverse-time SDE frameworks, yet this paper's explicit formulation relies on discrete-time definitions and Gaussian integrals.
- Why unresolved: The proof relies heavily on the specific Gaussian transition kernel $q(x_t|x_0)$ and the discrete noise schedule $\bar{\alpha}_t$; it is not immediately clear if the conditional expectation form holds identically in the continuous limit or under different noise distributions.
- What evidence would resolve it: A derivation showing that the explicit integral form $\int \epsilon_t(x_t|x_0)q(x_0|x_t)dx_0$ remains valid as $t$ becomes a continuous variable or when the noise assumption is changed.

## Limitations
- The framework is specific to Gaussian linear forward processes and does not extend to non-Gaussian or non-linear diffusion processes without significant modification
- The practical implications for training dynamics and generalization are not addressed
- The proof assumes smoothness conditions allowing interchange of gradient and integral operations that may fail for certain noise schedules or data distributions

## Confidence

- **High Confidence:** The variational calculus proof establishing $\epsilon_\theta(x_t, t)$ as conditional expectation is mathematically sound given the stated assumptions
- **Medium Confidence:** The derivation of the score function identity follows logically from the conditional expectation result but depends on regularity conditions for differentiation under the integral sign
- **Low Confidence:** The practical implications for training dynamics and generalization are not addressed

## Next Checks
1. **Regularity Verification:** Test the interchange of gradient and integral operations numerically on synthetic data with analytically tractable posteriors to confirm dominated convergence conditions hold in practice
2. **Capacity Sensitivity Analysis:** Systematically vary network depth/width in $\epsilon_\theta$ and measure prediction variance for fixed $x_t$ values to empirically quantify the gap between theoretical conditional expectation and learned approximation
3. **Non-Gaussian Extension:** Attempt to derive analogous formulations for a non-Gaussian diffusion process (e.g., Student's t-distribution) to test the framework's generalizability beyond the standard Gaussian case