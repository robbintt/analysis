---
ver: rpa2
title: 'ImagineBench: Evaluating Reinforcement Learning with Large Language Model
  Rollouts'
arxiv_id: '2505.10010'
source_url: https://arxiv.org/abs/2505.10010
tags:
- rollouts
- learning
- tasks
- offline
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ImagineBench, the first comprehensive benchmark
  for evaluating offline reinforcement learning algorithms that leverage both real-world
  and LLM-generated imaginary rollouts. The benchmark provides datasets spanning locomotion,
  manipulation, and navigation tasks, with natural language instructions at varying
  complexity levels.
---

# ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts

## Quick Facts
- arXiv ID: 2505.10010
- Source URL: https://arxiv.org/abs/2505.10010
- Authors: Jing-Cheng Pang; Kaiyuan Li; Yidi Wang; Si-Hang Yang; Shengyi Jiang; Yang Yu
- Reference count: 5
- Primary result: Hybrid real-imaginary rollout training achieves 35.44% success rate on hard tasks versus 64.37% with real rollouts alone

## Executive Summary
This paper introduces ImagineBench, the first comprehensive benchmark for evaluating offline reinforcement learning algorithms that leverage both real-world and LLM-generated imaginary rollouts. The benchmark provides datasets spanning locomotion, manipulation, and navigation tasks with natural language instructions at varying complexity levels. Through systematic evaluation of state-of-the-art offline RL algorithms, the authors find that simply combining real and imaginary rollouts achieves suboptimal performance on unseen tasks compared to training on real rollouts alone, highlighting the need for algorithmic advancements to better utilize LLM-generated rollouts.

## Method Summary
The benchmark framework combines real expert demonstrations with LLM-generated imaginary rollouts for novel tasks. LLMs are fine-tuned on environment-specific data to predict dynamics, explain rollouts, and generate new rollouts. Offline RL policies are then trained on hybrid datasets combining real and imaginary experiences. The evaluation spans four hierarchical task levels (Training, Rephrasing, Easy, Hard) across multiple environments including MuJoCo, Meta-world, LIBERO, CLEVR-Robot, and BabyAI, measuring success rates for generalization to unseen tasks.

## Key Results
- Simple combination of real and imaginary rollouts yields 35.44% success rate on hard tasks versus 64.37% with real rollouts alone
- Imaginary rollout quality degrades significantly with task complexity: consistency drops from 88.0% (Rephrasing) to 25.8% (Hard)
- LIBERO environment shows near-zero performance across all algorithms, indicating environment-specific challenges
- Current offline RL methods cannot effectively leverage imaginary rollouts despite their coverage of novel tasks

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning LLMs on environment-specific data enables them to generate plausible task execution trajectories for novel goals. Supervised fine-tuning trains the LLM on dynamics prediction, rollout explanation, and rollout generation, grounding it in environment dynamics, state/action semantics, and goal-behavior mapping. At inference, goal-oriented prompts elicit trajectory generation for unseen tasks. Core assumption: knowledge from training tasks transfers sufficiently to generate useful rollouts for novel task compositions. Evidence: [abstract] states LLMs can generate synthetic experience for mastering novel tasks; [Section 4.2] details SFT paradigms. Break condition: when novel tasks require dynamics or state configurations out-of-distribution from fine-tuning data, rollout consistency drops sharply (25.8% on Hard vs 88.0% on Rephrasing).

### Mechanism 2
Training offline RL policies on hybrid real-imaginary datasets improves generalization to unseen tasks compared to real data alone, but with substantial performance gaps. Real rollouts provide grounded environmental dynamics while imaginary rollouts provide coverage of novel goal-conditioned behaviors. Offline RL algorithms learn from this combined static dataset without online interaction. Core assumption: imaginary rollouts contain sufficient signal about goal-relevant behaviors that offline RL can extract despite distributional mismatch and noise. Evidence: [abstract] shows 35.44% success rate on hard tasks versus 64.37% with real rollouts; [Section 5.3] shows policies augmented with imaginary rollouts exhibit higher performance on novel tasks. Break condition: current methods treat imaginary and real data equivalently; performance degrades when imaginary rollouts contain dynamics violations that standard algorithms cannot filter.

### Mechanism 3
Hierarchical task categorization (Training → Rephrasing → Easy → Hard) reveals graduated generalization difficulty tied to compositional complexity. Training tasks test retention; Rephrasing tests language invariance; Easy tasks test single-step behavioral transfer; Hard tasks test multi-step compositional generalization. Success rates decline with complexity (BabyAI: 88.0% consistency on Rephrasing vs 25.8% on Hard for imaginary rollouts). Core assumption: task difficulty can be meaningfully stratified by required behavioral composition rather than environmental factors alone. Evidence: [Section 4.3] defines four hierarchical levels with examples; [Table 1] shows task categorization across environments. Break condition: when Hard tasks require physical or logical constraints not captured in fine-tuning data, imaginary rollouts violate constraints (Fig. 4b shows simultaneous instead of sequential picking).

## Foundational Learning

- **Concept: Offline RL and Distributional Shift**
  - Why needed: The benchmark evaluates offline RL algorithms that must learn from static datasets without exploration. Understanding conservatism, policy constraints, and value overestimation is essential to interpret benchmark results.
  - Quick check: Can you explain why standard Q-learning fails in offline settings when the policy deviates from the behavior policy distribution?

- **Concept: LLM Fine-Tuning for Structured Sequential Data**
  - Why needed: The paper modifies LLMs to handle numerical state-action sequences rather than pure text. Understanding how tokenization, additional neural heads, and supervised objectives adapt LLMs to decision-making domains is critical.
  - Quick check: What modifications would be needed to adapt a text-only LLM to process (s_t, a_t) numerical tuples?

- **Concept: Goal-Conditioned / Language-Conditioned RL**
  - Why needed: Policies are conditioned on natural language instructions (encoded via BERT). Understanding goal-augmented MDPs and instruction following is necessary for interpreting the architecture.
  - Quick check: How does concatenating language embeddings with state observations enable policy generalization across tasks?

## Architecture Onboarding

- **Component map**: Environments (MuJoCo, Meta-world, LIBERO, CLEVR-Robot, BabyAI) -> Real rollouts (expert policies, behavior cloning, rule-based) + Imaginary rollouts (fine-tuned Llama-2-7b) -> LLM pipeline (SFT on dynamics prediction, rollout explanation, rollout generation) -> Offline RL algorithms (BC, CQL, BCQ, TD3+BC, PRDC, COMBO, SAC) with BERT language encoding -> Evaluation (success rate across four task levels)

- **Critical path**: 1) Load environment and collect/obtain real rollouts with language annotations; 2) Fine-tune LLM on real rollout-instruction pairs (three SFT objectives); 3) Generate imaginary rollouts for novel task instructions; 4) Train offline RL policy on combined dataset (50/50 sampling ratio); 5) Evaluate on held-out task levels using success rate metrics

- **Design tradeoffs**: Data quality vs. coverage (real rollouts accurate but limited; imaginary rollouts cover novel tasks but contain violations at 72.9% transition correctness on Hard); LLM size vs. compute (Llama-2-7b used; larger models may improve quality but increase costs); Conservative vs. expressive offline RL (conservative methods may underutilize novel behaviors in imaginary rollouts)

- **Failure signatures**: Imaginary rollout quality degradation (consistency drops from 88.0% to 25.8% across task levels); Hard task collapse (success rates below 10% on Hard tasks for Meta-world, CLEVR-Robot, BabyAI); LIBERO failure (near-zero performance across all algorithms); Sequential reasoning failures (LLM generates simultaneous instead of sequential actions)

- **First 3 experiments**: 1) Baseline replication: Train BC and CQL on real rollouts only, evaluate across all four task levels; 2) Imaginary rollout quality audit: Sample 100 imaginary rollouts per task level, manually annotate consistency, transition correctness, and dynamics legality; 3) Hybrid ablation: Train policies with varying real/imaginary ratios (100/0, 75/25, 50/50, 25/75, 0/100) on BabyAI to characterize performance-complexity tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
How can offline RL algorithms be specifically designed to better account for the unique uncertainties, distributional shifts, and quality variations inherent in LLM-imaginary rollouts compared to real rollouts? Basis: [explicit] The paper states current algorithms may not fully exploit unique features of imaginary rollouts. Why unresolved: Existing methods achieve only 35.44% success on hard tasks with imaginary data versus 64.37% with real data. What evidence would resolve it: An algorithm that narrows or closes the ~29 percentage point gap between imaginary-rollout and real-rollout performance on hard tasks.

### Open Question 2
How can low-quality or physically implausible imaginary rollouts be effectively detected and filtered prior to policy training? Basis: [explicit] The paper identifies effectively filtering low-quality or unreal rollouts as an important direction. Why unresolved: Quality analysis shows consistency drops from 88.0% on rephrasing to 25.8% on hard tasks. What evidence would resolve it: A filtering method that improves downstream policy success rates by selectively retaining high-quality imaginary rollouts.

### Open Question 3
What scaling laws govern LLM imagination quality—how do model size, prompt engineering, and domain-specific pretraining affect rollout consistency, transition correctness, and dynamics legality? Basis: [explicit] The paper states scaling laws for LLM imagination warrant systematic investigation. Why unresolved: ImagineBench uses only Llama-2-7b-chat-hf, leaving effects of model scale and training strategies unexplored. What evidence would resolve it: A systematic study correlating LLM scale/configuration with the three quality metrics across task difficulty levels.

## Limitations

- Substantial performance gap (35.44% vs 64.37% success rate) indicates current algorithms cannot effectively leverage imaginary rollouts despite their coverage of novel tasks
- Quality degradation of imaginary rollouts with task complexity limits their utility for hard compositional tasks
- Benchmark focuses on single-modal tasks, with multi-modal extension claims remaining largely theoretical

## Confidence

- **High confidence**: Hierarchical task categorization framework and empirical validation across multiple environments; systematic quality degradation of imaginary rollouts with task complexity
- **Medium confidence**: Conclusion that simple combination of real and imaginary rollouts is suboptimal for unseen tasks; need for algorithmic advancements to better utilize imaginary rollouts
- **Low confidence**: Generalizability of findings to more complex, multi-modal tasks beyond evaluated domains

## Next Checks

1. **Algorithm-specific contribution isolation**: Run controlled experiment varying only the offline RL algorithm (keeping data fixed) to determine whether performance gaps stem from algorithm limitations versus data quality issues in imaginary rollouts.

2. **Fine-tuning curriculum ablation**: Systematically evaluate impact of different fine-tuning paradigms (dynamics-only, generation-only, multi-task) on imaginary rollout quality across task complexity levels to identify which components most influence downstream policy performance.

3. **Real-time online adaptation study**: Implement hybrid offline-online training protocol where initial policy training uses combined real-imaginary data, followed by limited online fine-tuning on real environment interactions, to assess whether this bridges the performance gap observed in purely offline settings.