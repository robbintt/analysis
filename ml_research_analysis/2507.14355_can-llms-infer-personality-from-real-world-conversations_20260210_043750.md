---
ver: rpa2
title: Can LLMs Infer Personality from Real World Conversations?
arxiv_id: '2507.14355'
source_url: https://arxiv.org/abs/2507.14355
tags:
- personality
- trait
- five
- mini
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates large language models' ability to infer Big
  Five personality traits from semi-structured interview transcripts, using a real-world
  dataset of 555 participants with validated BFI-10 scores. Three state-of-the-art
  LLMs (GPT-4.1 Mini, Meta-LLaMA, and DeepSeek) were tested using zero-shot and chain-of-thought
  prompting.
---

# Can LLMs Infer Personality from Real World Conversations?

## Quick Facts
- **arXiv ID**: 2507.14355
- **Source URL**: https://arxiv.org/abs/2507.14355
- **Reference count**: 40
- **Primary result**: Current LLMs show high reliability but limited validity for personality inference from conversational text, with weak correlations to ground-truth Big Five scores (maximum Pearson's r = 0.27).

## Executive Summary
This study evaluates whether large language models can reliably infer Big Five personality traits from semi-structured interview transcripts using a real-world dataset of 555 participants with validated BFI-10 scores. Three state-of-the-art LLMs were tested using zero-shot and chain-of-thought prompting across different input context lengths. While models demonstrated high test-retest reliability (ICCs > 0.85), their construct validity was limited, with weak correlations to ground-truth scores and low interrater agreement. The findings suggest that current LLMs lack sufficient psychometric validity for reliable personality assessment, despite showing internal consistency.

## Method Summary
The study used semi-structured interview transcripts from 555 participants with validated BFI-10 scores to evaluate three LLMs (GPT-4.1 Mini, Meta-LLaMA-3.3-70B-Instruct, and DeepSeek-R1-Distill-70B). Models were tested using zero-shot and chain-of-thought prompting across three input context lengths (100 words, ~1000 words, and full transcripts). Evaluation metrics included Pearson correlation, MAE, RMSE, exact match, off-by-one accuracy, Cohen's kappa, and ICC for test-retest reliability. The study compared model predictions against ground-truth BFI-10 scores to assess construct validity.

## Key Results
- All models showed high test-retest reliability (ICCs > 0.85) but weak correlations to ground-truth scores (maximum Pearson's r = 0.27)
- Predictions were systematically biased toward moderate-to-high trait levels
- Chain-of-thought prompting provided only modest improvements in distributional alignment
- Medium-length inputs (~1000 words) yielded optimal performance but still produced weak correlations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs produce stable but not necessarily valid personality inferences from conversational text.
- Mechanism: Models identify surface-level linguistic patterns (word choice, self-descriptions) that are internally consistent across runs, but these patterns do not reliably map onto validated psychometric constructs.
- Core assumption: Personality manifests in detectable, consistent linguistic signals that self-report instruments also capture.
- Evidence anchors:
  - [abstract] "All models showed high test-retest reliability (ICCs > 0.85), construct validity was limited, with weak correlations to ground-truth scores (maximum Pearson's r = 0.27)."
  - [section] Table 3 shows ICCs of 0.86-0.93 for most BFI-10 items, but Table 2 shows correlations near zero or negative for several items (e.g., BFI-5 r = -0.18 for Meta-LLaMA).
  - [corpus] Related work (arXiv:2511.23101) similarly finds LLM personality predictions show high consistency but variable accuracy across datasets.
- Break condition: When target traits are abstract or require contextual inference (e.g., "is reserved," "is generally trusting"), surface patterns fail and correlations approach zero or invert.

### Mechanism 2
- Claim: Moderate input context length optimizes the signal-to-noise ratio for personality inference.
- Mechanism: Medium-length inputs (~1000 words) provide sufficient behavioral evidence without overwhelming the model with noise, yielding highest correlations. Short inputs lack signal; full contexts introduce variance.
- Core assumption: Personality-relevant signals are distributed across utterances and accumulate with more text, but irrelevant content also accumulates.
- Evidence anchors:
  - [abstract] "Input context length had a modest positive effect on correlation but increased prediction variance."
  - [section] Figure 9 shows medium-length input yields highest Pearson correlations (Agreeableness r = 0.219, Conscientiousness r = 0.214), while short inputs fall below r = 0.06.
  - [corpus] No direct corpus evidence on context length effects for personality inference; this is underexplored.
- Break condition: Beyond optimal length, additional context degrades accuracy by increasing RMSE (Figure 10 shows full-context RMSE highest for most traits).

### Mechanism 3
- Claim: Chain-of-thought prompting improves distributional alignment but not trait-level accuracy.
- Mechanism: CoT guides models to explicitly reason about linguistic cues before scoring, which helps calibrate overall score distributions but does not correct for fundamental mismatches between linguistic patterns and psychological constructs.
- Core assumption: Personality inference benefits from structured reasoning similar to fact-based tasks.
- Evidence anchors:
  - [abstract] "Chain-of-thought prompting and longer input context modestly improved distributional alignment, but not trait-level accuracy."
  - [section] Discussion: "CoT prompting modestly improved categorical coherence for some models... yet failed to enhance correlation strength or reduce prediction error."
  - [corpus] Weak corpus evidence; related papers do not systematically evaluate CoT for personality tasks.
- Break condition: When traits are latent and globally distributed (vs. fact-based), stepwise reasoning offers limited leverage—personality depends more on tone, affect, and implicit patterns.

## Foundational Learning

- Concept: **Construct validity vs. reliability in psychological measurement**
  - Why needed here: The paper shows models are reliable (high ICCs) but not valid (weak correlations). Without understanding this distinction, one might mistake consistency for accuracy.
  - Quick check question: If an LLM produces identical personality scores across three runs but those scores correlate at r = 0.10 with ground truth, is it a valid instrument? (Answer: No—validity requires alignment with the construct being measured.)

- Concept: **Big Five personality structure and BFI-10 measurement**
  - Why needed here: The paper evaluates predictions against BFI-10 scores; understanding what each trait captures (e.g., Conscientiousness = diligence, impulse control) is necessary to interpret why some items are easier to infer than others.
  - Quick check question: Why might "tends to be lazy" (BFI-3) be easier to infer from text than "is generally trusting" (BFI-2)? (Answer: Laziness is often explicitly stated as a behavioral self-description; trust is latent and inferred from absence of doubt or mention of betrayal.)

- Concept: **Distributional alignment vs. pointwise accuracy**
  - Why needed here: The paper reports both correlation metrics (pointwise) and binned distribution comparisons. CoT improved the latter but not the former, indicating different failure modes.
  - Quick check question: If a model overpredicts "High" scores by 300 cases but has 70% off-by-one accuracy, what does this suggest? (Answer: The model captures rough trait tendencies but has systematic bias, likely inflating moderate/high predictions.)

## Architecture Onboarding

- Component map: Preprocess transcripts -> Truncate to target length -> Apply prompt template -> Run inference -> Extract scores -> Compute metrics
- Critical path: 1. Preprocess interview transcripts → 2. Truncate to target length (100/1000/full) → 3. Apply prompt template (zero-shot or CoT) → 4. Run inference (single pass, no fine-tuning) → 5. Extract scores via regex parsing → 6. Compute metrics against BFI-10 ground truth
- Design tradeoffs:
  - **Short vs. long context**: Short inputs lack signal (r < 0.06); full inputs increase variance (RMSE rises). Medium (~1000 words) is optimal but still yields weak correlations.
  - **Zero-shot vs. CoT**: CoT adds inference cost but only marginally improves distributional alignment—not worth it for accuracy gains alone.
  - **Model selection**: GPT-4.1 Mini achieves best correlations but still weak; DeepSeek has lowest errors but near-zero correlations (may be regressing toward mean).
- Failure signatures:
  - **Central tendency bias**: Models overpredict moderate/high scores, underpredict low scores (Figure 6-7).
  - **Negative correlations**: Some items (BFI-5, BFI-6) show consistent negative correlations, suggesting models actively misunderstand certain traits.
  - **Near-zero kappa despite high off-by-one**: Indicates agreement is largely chance-driven (κ < 0.10 across conditions).
- First 3 experiments:
  1. **Baseline replication**: Run zero-shot inference on a held-out subset of interviews with BFI-10 ground truth. Confirm ICC > 0.85 and max r ≈ 0.27 to validate the pipeline.
  2. **Input length ablation**: Test 250, 500, 1000, 2000, and full context on same transcripts. Plot correlation and RMSE curves to find optimal length for each trait.
  3. **Trait-specific prompting**: Design prompts that explicitly ask for behavioral evidence for each trait (e.g., "What behaviors suggest conscientiousness in this transcript?"). Compare to generic zero-shot on BFI-3, BFI-8 (best-performing items) vs. BFI-2, BFI-5 (worst-performing).

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset consists of semi-structured interviews rather than naturalistic conversations, potentially limiting ecological validity
- Only three LLM architectures tested, all relatively large models—smaller or specialized models might perform differently
- Does not address potential demographic biases in either the dataset or model predictions
- Focuses solely on English language data, limiting generalizability to other linguistic contexts

## Confidence
**High confidence**: The finding that all models show high test-retest reliability (ICCs > 0.85) is well-supported by multiple runs and consistent across models. The observation that predictions are systematically biased toward moderate-to-high trait levels is also robust, evident across different input lengths and prompting strategies.

**Medium confidence**: The claim that chain-of-thought prompting provides only modest improvements has moderate support, though the effect sizes are small and inconsistent across traits. The finding that medium-length inputs (~1000 words) optimize performance is supported by correlation data but shows increased variance, suggesting the relationship may be more complex than presented.

**Low confidence**: The assertion that LLMs lack sufficient psychometric validity for reliable personality assessment should be interpreted cautiously, as the study only tested a limited set of models and prompting strategies. The specific correlations reported (maximum r = 0.27) may not generalize to all personality assessment contexts or evaluation metrics.

## Next Checks
1. **Ecological validity test**: Replicate the study using naturalistic conversation data (e.g., Reddit posts, social media interactions) rather than semi-structured interviews to assess whether the findings hold in more representative contexts.

2. **Bias audit**: Conduct a systematic analysis of demographic bias by testing model predictions across different age, gender, and cultural groups represented in the dataset to identify potential systematic disparities.

3. **Alternative prompting strategies**: Evaluate specialized prompting approaches that explicitly instruct models to identify behavioral evidence for each trait, comparing performance against the zero-shot and CoT approaches tested in this study.