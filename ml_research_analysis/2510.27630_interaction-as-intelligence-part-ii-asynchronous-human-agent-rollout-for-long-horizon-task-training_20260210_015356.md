---
ver: rpa2
title: 'Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout for
  Long-Horizon Task Training'
arxiv_id: '2510.27630'
source_url: https://arxiv.org/abs/2510.27630
tags:
- agent
- training
- data
- arxiv
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: APOLLO addresses the challenge of training LLM agents for long-horizon,
  domain-specialized tasks by introducing an asynchronous human-in-the-loop framework.
  Instead of requiring continuous human supervision, it enables annotators to intervene
  only when the agent drifts from promising trajectories, reducing annotation costs
  while maintaining trajectory quality.
---

# Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout for Long-Horizon Task Training

## Quick Facts
- arXiv ID: 2510.27630
- Source URL: https://arxiv.org/abs/2510.27630
- Reference count: 40
- Key outcome: APOLLO achieves 50% improvement over untrained baseline and 28% improvement over variant without human interaction for LLM agents in long-horizon research tasks.

## Executive Summary
APOLLO introduces an asynchronous human-in-the-loop framework for training LLM agents on long-horizon, domain-specialized tasks. Instead of requiring continuous human supervision, it enables annotators to intervene only when the agent drifts from promising trajectories, significantly reducing annotation costs while maintaining trajectory quality. The approach integrates action-level supervision control to filter unreliable actions and prevent error propagation. Evaluated on InnovatorBench using GLM-4.5, APOLLO demonstrates substantial performance gains and the ability to sustain improvements over extended horizons, addressing a critical challenge in long-horizon LLM research tasks.

## Method Summary
APOLLO addresses the challenge of training LLM agents for long-horizon, domain-specialized tasks through an asynchronous human-in-the-loop framework. The method involves running Claude-4-Sonnet in the ResearchGym environment, with human annotators intervening asynchronously via a custom UI when the agent drifts from promising trajectories. The framework incorporates context management that triggers summarization when the context exceeds approximately 100k tokens, and implements action-level supervision control to filter unreliable actions using symbolic rules and an LLM-as-Judge. The training process fine-tunes GLM-4.5 using a modified slime codebase with action masking, employing specific hyperparameters including max tokens of 128k, batch size of 64, and learning rate scheduling from 5e-6 to 1e-6.

## Key Results
- APOLLO achieves a 50% improvement over the untrained baseline on InnovatorBench
- APOLLO shows a 28% improvement over a variant without human interaction
- The approach enables agents to sustain performance gains over extended horizons

## Why This Works (Mechanism)
The asynchronous intervention model reduces the burden on human annotators while maintaining trajectory quality. By intervening only when the agent drifts, rather than requiring continuous supervision, APOLLO makes long-horizon training more practical. The action-level supervision control prevents error propagation by filtering unreliable actions before they influence future decisions. Context summarization ensures the agent can maintain relevant information over extended rollouts without exceeding token limits, preserving critical historical information while managing computational constraints.

## Foundational Learning
- **ResearchGym Environment**: Why needed - Provides standardized environment for LLM research task evaluation; Quick check - Verify environment supports the 20 InnovatorBench tasks
- **Asynchronous Human Intervention**: Why needed - Reduces annotation costs while maintaining quality; Quick check - Confirm intervention triggers only on trajectory drift
- **Action-Level Supervision**: Why needed - Prevents error propagation through the decision chain; Quick check - Validate filtering masks are correctly applied to training data
- **Context Summarization**: Why needed - Manages token limits in long rollouts; Quick check - Monitor that summarization triggers at ~100k tokens
- **ReAct Framework**: Why needed - Combines reasoning and acting for task completion; Quick check - Verify trajectories include both reasoning and action steps
- **LLM-as-Judge**: Why needed - Provides automated quality assessment for action filtering; Quick check - Test judge prompts from Appendix E produce consistent outputs

## Architecture Onboarding
**Component Map:** ResearchGym Environment -> Claude-4-Sonnet Agent -> Custom UI Interface -> Human Annotators -> Action Filter -> Training Pipeline -> GLM-4.5 Model

**Critical Path:** Trajectory generation in ResearchGym → Asynchronous human intervention → Action filtering with LLM-as-Judge → Context summarization → Fine-tuning with masked loss

**Design Tradeoffs:** Asynchronous intervention reduces human cost but may miss early errors; action filtering prevents error propagation but may discard potentially recoverable actions; context summarization manages token limits but risks losing early context

**Failure Signatures:** Context truncation when rollouts exceed 128k tokens; error propagation when bad actions aren't filtered; performance degradation when human intervention frequency is too low or too high

**First Experiments:** 1) Test context summarization triggers correctly at 100k tokens threshold, 2) Validate action filtering correctly masks error observations, 3) Verify ReAct trajectories are properly upsampled 7x during training

## Open Questions the Paper Calls Out
None

## Limitations
- Access to ResearchGym environment and specific training tasks from Table 3 is required but not provided
- Human-AI interaction interface architecture is described but not fully specified for exact replication
- System prompts for ReAct loop and summarization steps are not fully disclosed, only filtering prompts are provided

## Confidence
- **High Confidence**: General framework architecture and training pipeline are clearly specified
- **Medium Confidence**: Filtering mechanism and human intervention strategy described but implementation details unclear
- **Medium Confidence**: Experimental results are well-documented but exact reproduction requires specific training tasks

## Next Checks
1. Verify context summarization triggers correctly when context exceeds 100k tokens during rollout sampling
2. Validate that action-level supervision masks are applied correctly to exclude error observations
3. Confirm that ReAct trajectories are upsampled 7x and summarization trajectories 10x in the training dataset