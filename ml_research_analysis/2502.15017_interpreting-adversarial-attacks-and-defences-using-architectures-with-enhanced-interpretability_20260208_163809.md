---
ver: rpa2
title: Interpreting Adversarial Attacks and Defences using Architectures with Enhanced
  Interpretability
arxiv_id: '2502.15017'
source_url: https://arxiv.org/abs/2502.15017
tags:
- pgd-at
- std-tr
- distance
- hyperplane
- frequency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses Deep Linearly Gated Networks (DLGN) to interpret
  adversarial training (PGD-AT) versus standard training (STD-TR). DLGN's linear feature
  network allows unique analyses of hyperplanes, PCA alignment, and subnetwork overlap.
---

# Interpreting Adversarial Attacks and Defences using Architectures with Enhanced Interpretability

## Quick Facts
- arXiv ID: 2502.15017
- Source URL: https://arxiv.org/abs/2502.15017
- Reference count: 40
- One-line primary result: PGD adversarial training positions hyperplanes farther from data points and creates more diverse, less overlapping active subnetworks, enhancing robustness through spatial separation and reduced adversarial transfer.

## Executive Summary
This paper uses Deep Linearly Gated Networks (DLGN) to interpret adversarial training (PGD-AT) versus standard training (STD-TR). DLGN's linear feature network allows unique analyses of hyperplanes, PCA alignment, and subnetwork overlap. Hyperplane analysis shows PGD-AT hyperplanes are positioned farther from data points than STD-TR, reducing gate flipping under attacks. PCA analysis reveals PGD-AT hyperplanes have lower alignment with principal components, prioritizing robustness over data proximity. Subnetwork overlap analysis via neural path kernel shows PGD-AT creates more diverse, less overlapping active subnetworks across classes, preventing attack-induced overlaps. Gate visualization indicates PGD-AT models maintain sharper, more distinct class features and prevent adversaries from activating unrelated gating patterns. In summary, PGD-AT learns to map inputs to a more robust, diverse path space, enhancing adversarial robustness through spatial separation and reduced subnetwork overlap.

## Method Summary
The paper introduces Deep Linearly Gated Networks (DLGN), a two-network architecture with a linear feature network producing gates and a value network aggregating path contributions. PGD-AT is implemented with ε=0.3 and T=40 steps, compared against STD-TR. Key analyses include hyperplane projection distances, PCA alignment of hyperplanes with principal components, Neural Path Kernel (NPK) overlap (ΨD, ΨS), and IOU of active gates. Experiments use MNIST, Fashion MNIST, and synthetic 2D datasets with both fully-connected and convolutional DLGN variants.

## Key Results
- PGD-AT hyperplanes are positioned farther from data points than STD-TR, reducing gate flipping under attacks.
- PGD-AT hyperplanes have lower alignment with principal components, prioritizing robustness over data proximity.
- PGD-AT creates more diverse, less overlapping active subnetworks across classes, preventing attack-induced overlaps.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** PGD-AT positions decision hyperplanes farther from data points than standard training, reducing gate-flipping sensitivity to small perturbations.
- **Mechanism:** In DLGN's linear feature network, each gate's activation depends on which side of a hyperplane the input lies. Larger median projection distances from hyperplanes require either larger dimension-wise perturbations or coordinated small perturbations across many dimensions to flip a gate. PGD-AT implicitly maximizes these distances during the min-max optimization, creating a buffer zone against perturbations within the ε-ball.
- **Core assumption:** Perturbation magnitude δ is bounded (e.g., L∞ constraint), making spatial separation an effective defense.
- **Evidence anchors:**
  - [abstract]: "Hyperplane analysis shows PGD-AT hyperplanes are positioned farther from data points than STD-TR, reducing gate flipping under attacks."
  - [section]: Figure 3 shows correlation between larger median projection distance and lower flip rates; Figure 4 shows PGD-AT hyperplanes have systematically higher median distances.
  - [corpus]: Weak direct support—neighbor papers focus on attack methods rather than hyperplane geometry. Adjacent work on robustness-interpretability tradeoffs (Paper 29013) suggests the connection is underexplored.
- **Break condition:** If perturbation budget ε exceeds the median hyperplane distance, spatial separation no longer protects; adversarial examples can reliably flip gates.

### Mechanism 2
- **Claim:** PGD-AT learns to map inputs to a more diverse path space where active subnetworks have lower overlap both within and across classes.
- **Mechanism:** The Neural Path Kernel (NPK) measures overlap of active pathways between sample pairs. PGD-AT reduces ΨD (cross-class overlap) and ΨS (within-class overlap) compared to STD-TR. This prevents adversarial perturbations from activating gating patterns associated with incorrect classes, as the active subnetwork for each class occupies a distinct region in path space.
- **Core assumption:** Distinct path-space representations correlate with classification robustness; overlap enables adversarial transfer.
- **Evidence anchors:**
  - [abstract]: "Subnetwork overlap analysis via neural path kernel shows PGD-AT creates more diverse, less overlapping active subnetworks across classes, preventing attack-induced overlaps."
  - [section]: Table 1 shows ΨD_adv and ΨD_adv,org are consistently lower for PGD-AT across all MNIST class pairs.
  - [corpus]: No direct corpus support for path-space diversity as a robustness mechanism; this appears to be a DLGN-specific analysis.
- **Break condition:** If the value network learns similar NPV weights for different path-space regions, reduced overlap may not translate to classification robustness.

### Mechanism 3
- **Claim:** PGD-AT hyperplanes have lower alignment with principal components than STD-TR, reflecting a tradeoff between robustness and data proximity.
- **Mechanism:** PCA minimizes point-to-hyperplane distances by projecting onto high-variance directions. STD-TR hyperplanes align more closely with principal components (higher similarity scores), optimizing for clean accuracy. PGD-AT sacrifices this alignment to maximize spatial separation from decision boundaries, reducing PCA weight similarity. Embedding PCA in the input layer degrades PGD-AT robustness more than STD-TR accuracy.
- **Core assumption:** Principal components capture directions most useful for standard classification but are suboptimal for adversarial robustness.
- **Evidence anchors:**
  - [abstract]: "PCA analysis reveals PGD-AT hyperplanes have lower alignment with principal components, prioritizing robustness over data proximity."
  - [section]: Figure 7 shows embedding PCA reduces PGD-AT robust accuracy more severely than STD-TR clean accuracy; Figure 8 shows higher PCA-weight alignment in STD-TR.
  - [corpus]: No corpus papers directly address PCA-robustness tradeoffs.
- **Break condition:** If data distribution has intrinsically robust features aligned with principal components, PCA may not harm robustness; the tradeoff is dataset-dependent.

## Foundational Learning

- **Concept: Projected Gradient Descent (PGD) Adversarial Training**
  - **Why needed here:** Understanding the min-max optimization objective (inner maximization via PGD, outer minimization via SGD) explains why robust features differ from standard features.
  - **Quick check question:** Given ε = 0.3 and α = 0.01, how many PGD steps are needed to reach the perturbation boundary from a random initialization?

- **Concept: Linear Separability and Hyperplane Geometry**
  - **Why needed here:** DLGN's linear feature network enables hyperplane analysis. Understanding how projection distance relates to gate stability is central to Mechanism 1.
  - **Quick check question:** For a point x and hyperplane defined by w·x + b = 0, what is the signed projection distance? If ||w|| = 2 and w·x + b = 0.6, how far is x from the hyperplane?

- **Concept: Path-Space Representation in Gated Networks**
  - **Why needed here:** Neural Path Features (NPF) and Neural Path Values (NPV) decompose the network output into path contributions. Overlap analysis depends on understanding this representation.
  - **Quick check question:** In a 2-layer network with 2 nodes per layer, how many unique paths exist from input to output? What is the NPF for a path with gates (1, 0, 1)?

## Architecture Onboarding

- **Component map:** Input → Feature Network (linear transformations + gate computation) → NPF vector → Dot product with NPV → Output logit.
- **Critical path:** Input → Feature Network (linear transformations + gate computation) → NPF vector → Dot product with NPV → Output logit. Adversarial attacks can only modify NPF through the feature network.
- **Design tradeoffs:**
  - Higher β → sharper gates (more binary) → cleaner path-space analysis but potentially less smooth gradients.
  - Deeper/wider feature networks → more expressive path space but exponential path count (P = Π m_l).
  - Convolutional variant: Feature network uses Conv layers instead of FC; gating patterns become spatial maps rather than scalar gates.
- **Failure signatures:**
  - Gate flip rate > 30% under ε = 0.3 attacks suggests insufficient hyperplane separation.
  - High cross-class NPK overlap (ΨD > 0.8) indicates brittle class representations.
  - I^ado visualizations producing meaningful class images (rather than noise) suggests adversarial perturbations activate semantically unrelated paths.
- **First 3 experiments:**
  1. **Hyperplane distance sweep:** Train DLGN with varying ε (0.1, 0.2, 0.3) and plot median projection distance vs. robust accuracy to validate Mechanism 1's distance-robustness correlation.
  2. **Gate masking ablation:** Mask top-k gates by median projection distance vs. random masking vs. bottom-k gates. Confirm that top-gate masking degrades robust accuracy most severely (replicating Figure 5).
  3. **Binary classification NPK analysis:** Train DLGN on 3vs8 MNIST binary task with both STD-TR and PGD-AT. Compute ΨD_orig, ΨD_adv, ΨD_adv,org and verify PGD-AT reduces cross-class overlap under attack.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the specific properties identified in PGD-AT (such as increased hyperplane distance and diverse active subnetworks) be formalized into explicit regularizers to enhance robustness in standard neural network architectures?
- **Basis in paper:** [explicit] The authors state in the conclusion: "We believe that leveraging the results of our analysis to develop novel algorithms that account for the properties examined could effectively enhance robustness."
- **Why unresolved:** This work focuses on analyzing and interpreting existing models (PGD-AT vs. STD-TR) rather than using these insights to design new training objectives.
- **What evidence would resolve it:** A new regularization term based on maximizing projection distance or minimizing path overlap that successfully increases robustness in standard networks like ResNets.

### Open Question 2
- **Question:** Do the interpretability findings regarding hyperplane alignment and subnetwork overlap generalize to more complex, non-linear architectures like Vision Transformers (ViT)?
- **Basis in paper:** [explicit] The authors suggest: "Extending this analysis to larger and more complex models, such as transformers or other deep architectures, could provide further insights into the generalizability of our findings."
- **Why unresolved:** The methodology relies on the DLGN architecture, which utilizes a linear feature network to define effective hyperplanes; standard transformers lack this specific linear decomposition, making the analysis non-trivial.
- **What evidence would resolve it:** Adaptation of the path analysis or hyperplane metrics to Transformer attention mechanisms, demonstrating similar separation/diversity in robustly trained ViTs.

### Open Question 3
- **Question:** Is the reduced alignment between hyperplanes and Principal Components (PCA) in PGD-AT models a necessary condition for robustness, or is it merely a side effect of the training process?
- **Basis in paper:** [inferred] The paper notes that embedding PCA layers harms robustness (Figure 7) and that PGD-AT hyperplanes have lower PCA alignment (Figure 8). However, it does not determine if this misalignment is strictly required or if it limits the model's capacity to learn clean features.
- **Why unresolved:** The paper establishes a correlation (PCA alignment drops as robustness rises) but does not isolate the causal mechanism or trade-offs involved in forcing/excluding PCA alignment.
- **What evidence would resolve it:** Ablation studies forcing PCA alignment during adversarial training to see if robustness collapses, or conversely, if starting with PCA components accelerates robustness learning.

## Limitations
- Major uncertainties include the lack of detailed training hyperparameters (optimizer, learning rate, batch size, epochs) and unclear β values for gate sharpness, which directly affect hyperplane interpretation.
- The paper's claims rely heavily on DLGN-specific analyses without strong support from the broader corpus.
- The absence of standard robustness benchmarks and ablation studies on β or network depth limits generalizability.

## Confidence
- **High**: Hyperplane distance correlates with gate flip resistance; gate masking ablates robust accuracy.
- **Medium**: PCA alignment tradeoffs between robustness and clean accuracy; path-space diversity reduces adversarial overlap.
- **Low**: Direct causality between gate visualization and adversarial robustness; NPK-based interpretations are novel but unverified outside DLGN.

## Next Checks
1. **Ablation on β parameter**: Sweep β values to quantify its effect on gate binarization, hyperplane distance, and robust accuracy to confirm Mechanism 1's distance-buffer claim.
2. **Cross-dataset robustness**: Apply DLGN analysis to CIFAR-10 or SVHN to test if PGD-AT's hyperplane separation and NPK diversity generalize beyond MNIST.
3. **Standard network comparison**: Train a ReLU-based ResNet on MNIST with PGD-AT and measure hyperplane distances and NPK overlap to test if DLGN-specific geometry is essential to observed mechanisms.