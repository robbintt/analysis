---
ver: rpa2
title: Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with LLM-based
  Multi-Agent System
arxiv_id: '2509.17240'
source_url: https://arxiv.org/abs/2509.17240
tags:
- system
- reviews
- prisma
- systematic
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-agent system (MAS) that automates
  the evaluation of systematic literature reviews (SLRs) by assessing them against
  PRISMA guidelines. The system uses 27 specialized agents organized into six PRISMA-aligned
  societies, each handling specific evaluation tasks.
---

# Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with LLM-based Multi-Agent System

## Quick Facts
- arXiv ID: 2509.17240
- Source URL: https://arxiv.org/abs/2509.17240
- Reference count: 12
- Primary result: 84% agreement with expert reviewers on PRISMA scores across five SLRs

## Executive Summary
This paper introduces a multi-agent system (MAS) that automates the evaluation of systematic literature reviews (SLRs) by assessing them against PRISMA guidelines. The system uses 27 specialized agents organized into six PRISMA-aligned societies, each handling specific evaluation tasks. Evaluated on five SLRs across diverse domains, the MAS achieved 84% agreement with expert human reviewers on PRISMA scores. The highest alignment occurred in Introduction (97%), Discussion (94%), and Methods (93%) sections, with lower but still strong agreement in Results (84%) and Other Information (81%). This demonstrates the system's potential to streamline SLR evaluation while maintaining consistency with expert judgment, though results are based on a small sample and further validation is needed.

## Method Summary
The system evaluates SLRs against PRISMA guidelines using 27 specialized agents organized into six societies (Abstract & Title: 2, Introduction: 2, Methods: 11, Results: 7, Discussion: 1, Other Information: 4) plus 2 utility agents. A Coordinator Agent and Task Agent decompose the PRISMA checklist into modular tasks, dispatch via few-shot prompts, monitor outputs against thresholds, and reallocate or spawn new agents when scores fall below quality bars. Each agent uses the arXiv Toolkit to retrieve relevant research, assigns a 0–5 score, and provides qualitative feedback. The system was evaluated on five published SLRs from diverse domains (Medical, E-commerce, AI, Metaverse, IoT) with ground-truth PRISMA scores from three expert human reviewers.

## Key Results
- 84% agreement with expert reviewers on PRISMA scores across five SLRs
- Highest agreement in Introduction (97%), Discussion (94%), and Methods (93%) sections
- Lower but strong agreement in Results (84%) and Other Information (81%)
- ICC of 0.924, Krippendorff's α of 0.889, and MAE-derived agreement percentages

## Why This Works (Mechanism)

### Mechanism 1
Single-task agent specialization reduces output instability compared to multi-item agents. Assigning one agent per PRISMA checklist item isolates evaluation scope, preventing attention dilution and cross-item interference. Each agent receives focused context and produces a single 0–5 score with qualitative feedback. This works under the assumption that PRISMA items are sufficiently independent for decomposition. Evidence shows early experiments with multi-item agents resulted in unstable behavior with overloaded agents and degraded performance.

### Mechanism 2
Hierarchical coordination with dynamic reallocation maintains evaluation coverage when individual agents underperform. The Coordinator Agent and Task Agent decompose the PRISMA checklist, dispatch via few-shot prompts, monitor outputs against thresholds, and reallocate or spawn new agents when scores fall below quality bars. This assumes underperforming agents can be detected reliably via threshold metrics. The system integrates protocol validation, methodological assessment, and topic relevance checks using a scholarly database.

### Mechanism 3
External retrieval (arXiv) combined with PRISMA-anchored prompts grounds agent judgments in domain literature. Each agent accesses the arXiv Toolkit to retrieve relevant research, assigns a score, and provides feedback within the PRISMA framework, reducing hallucination risk by tethering outputs to retrieved evidence. This assumes arXiv coverage is sufficient for interdisciplinary SLR evaluation. The system excludes other key databases like PubMed or Scopus, creating potential gaps in coverage.

## Foundational Learning

- Concept: PRISMA Guidelines (Preferred Reporting Items for Systematic Reviews and Meta-Analyses)
  - Why needed here: The entire system architecture is structured around PRISMA checklist societies; understanding item granularity is essential for debugging agent behavior.
  - Quick check question: Can you name at least three PRISMA sections and explain why Methods requires more checklist items than Discussion?

- Concept: Multi-Agent System (MAS) Coordination Patterns
  - Why needed here: The system uses hierarchical decomposition (Coordinator → Task → Specialized Agents); understanding failure modes in task dispatch and reallocation is critical.
  - Quick check question: What happens if two agents produce conflicting scores for semantically related PRISMA items—does the system have a reconciliation mechanism?

- Concept: Inter-Rater Reliability Metrics (ICC, Krippendorff's α, MAE)
  - Why needed here: System evaluation uses ICC=0.924, α=0.889, and MAE-derived agreement percentages; interpreting these correctly determines whether 84% agreement is acceptable.
  - Quick check question: Why is ICC appropriate here instead of simple percent agreement, and what does MAE of 0.05 vs 0.44 tell you about paper-level variance?

## Architecture Onboarding

- Component map: PDF upload → OCR parsing → Checklist decomposition → Parallel agent evaluation (with arXiv retrieval) → Score aggregation → Threshold check → Potential reallocation → Synthesis → User display + follow-up chat
- Critical path: Input Layer (OCR-enabled Vision–Language Model) → Orchestration Layer (Coordinator Agent + Task Agent) → Evaluation Layer (6 PRISMA-aligned societies with 27 specialized agents) → Retrieval Layer (arXiv Toolkit) → Interaction Layer (SLR-GPT Agent) → Output Layer (Unified synthesis → Web UI)
- Design tradeoffs:
  - Specialization vs coherence: One-agent-per-item improves stability but may fragment holistic judgment; no explicit cross-item reconciliation described
  - arXiv-only vs multi-database: Faster integration but coverage gaps for clinical/commercial databases (PubMed, Scopus)
  - Evaluation-only vs full drafting: Current system assesses existing SLRs; does not support real-time collaborative writing
- Failure signatures:
  - High MAE on specific papers (e.g., Paper 2 at 0.44) suggests domain-specific agent weakness
  - Lower agreement in Other Information (81%) may indicate checklist items with ambiguous criteria or sparse arXiv coverage
  - Methods section shows highest inter-expert variability—agents may also struggle here
- First 3 experiments:
  1. Ablation study: Run the same 5 SLRs with multi-item agents (2–3 items per agent) vs single-item agents; compare MAE and agreement stability
  2. Retrieval coverage audit: For each domain (Medical, AI, E-commerce, IoT, Metaverse), log arXiv retrieval success rates and correlate with section-level agreement gaps
  3. Threshold calibration test: Vary the reallocation threshold and measure agent spawn rate, total processing time, and score convergence across 3 reruns per SLR

## Open Questions the Paper Calls Out

- Question: Does the 84% agreement rate with expert reviewers hold when the system is evaluated on a significantly larger corpus of systematic literature reviews?
  - Basis: The authors state in the Limitations section that "the current evaluation spans five SLRs" and that in subsequent studies, they are "looking towards increasing the number of papers."
  - Why unresolved: The small sample size (n=5) limits the statistical power and generalizability of the reported performance across diverse domains.
  - What evidence would resolve it: Evaluation results from a dataset of 50+ SLRs demonstrating consistent agreement metrics across various disciplines.

- Question: How does the system's reliance solely on arXiv impact the accuracy of citation verification and relevance checks in medical domains?
  - Basis: The Limitations section notes that the system "excludes other key databases like PubMed or Scopus, creating potential gaps" in coverage.
  - Why unresolved: Medical and life science reviews rely heavily on PubMed-indexed literature, which the current toolkit cannot access.
  - What evidence would resolve it: A comparative analysis of the system's citation accuracy using integrated PubMed/Scopus toolkits versus the current arXiv-only implementation.

- Question: To what extent does the system improve workflow efficiency and user trust when deployed as an interactive, browser-based co-pilot?
  - Basis: The Future Directions section proposes deploying an interface to collect structured feedback using Likert scales to "evaluate clarity, usefulness, and trust."
  - Why unresolved: The current study evaluates algorithmic agreement with experts, not the tool's usability or its impact on human workflow speed.
  - What evidence would resolve it: Results from user studies involving systematic reviewers measuring time-to-completion and subjective trust ratings.

## Limitations
- Small sample size (5 SLRs) limits generalizability of the 84% agreement claim
- arXiv-only retrieval may create coverage gaps for medical or commercial domains
- Threshold-based reallocation mechanism lacks sensitivity analysis
- No explicit cross-item reconciliation mechanism for semantically related PRISMA items

## Confidence
- **High confidence**: The multi-agent architecture design and its alignment with PRISMA guidelines; the quantitative agreement metrics (ICC, MAE, Krippendorff's α) are appropriately applied.
- **Medium confidence**: The 84% agreement claim, given the small sample size (5 papers) and lack of statistical significance testing.
- **Low confidence**: The effectiveness of dynamic agent reallocation, as no ablation study is provided comparing threshold-based vs static allocation.

## Next Checks
1. Ablation on agent granularity: Run the system with 2-3 item agents vs. 1-item agents on the same 5 SLRs; compare MAE and agreement stability across sections.
2. Retrieval coverage audit: Log arXiv retrieval success rates per domain (Medical, AI, E-commerce, IoT, Metaverse) and correlate with section-level agreement gaps, particularly for Other Information (81% agreement).
3. Threshold sensitivity test: Vary the reallocation threshold across 3 runs per SLR; measure agent spawn rate, total processing time, and convergence of final scores.