---
ver: rpa2
title: 'Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets
  for Indian Languages'
arxiv_id: '2510.07000'
source_url: https://arxiv.org/abs/2510.07000
tags:
- arxiv
- data
- indic
- datasets
- indian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces a human-in-the-loop pipeline that combines
  translations and synthetic expansion to create high-quality, culturally grounded
  post-training datasets for Indian languages. The approach addresses the lack of
  multilingual, culturally inclusive instruction-tuning and preference-tuning data
  by curating two datasets: Pragyaan-IT (22.5K) and Pragyaan-Align (100K) across 10
  Indian languages.'
---

# Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages

## Quick Facts
- **arXiv ID**: 2510.07000
- **Source URL**: https://arxiv.org/abs/2510.07000
- **Reference count**: 18
- **Primary result**: Human-in-the-loop pipeline creates culturally grounded instruction-tuning and preference-tuning datasets for 10 Indian languages, achieving 60-61% win rates on Updesh benchmark

## Executive Summary
This work introduces Pragyaan, a human-in-the-loop pipeline that addresses the critical gap in culturally inclusive post-training datasets for Indian languages. The approach combines LLM-based translations and synthetic expansion with human refinement to create two high-quality datasets: Pragyaan-IT (22.5K) and Pragyaan-Align (100K) across 10 Indic languages. The pipeline systematically addresses translation errors and cultural misalignments through expert annotators who evaluate outputs across five linguistic dimensions, ensuring both grammatical accuracy and cultural appropriateness. The resulting datasets demonstrate measurable improvements in model alignment when fine-tuned, with pilot experiments showing 60-61% win rates on the Updesh benchmark.

## Method Summary
The method employs a two-pronged approach: translating high-quality English instruction datasets into 10 Indic languages using proprietary LLMs, then refining through human annotators who correct both linguistic errors and cultural misalignments. Alternatively, the pipeline uses a modified Self-Instruct framework for synthetic prompt expansion before translation and refinement. Human annotators work with detailed guidelines covering five quality dimensions for both generation and translation tasks, including relevance, grammatical accuracy, and cultural grounding across three tiers (IC-1: generic, IC-2: Indic-grounded, IC-3: explicitly Indian). The final datasets are configured across task complexity, turn counts, safety levels, and thinking requirements before being used for instruction tuning and DPO-based alignment training.

## Key Results
- Pragyaan-IT contains 22.5K instruction-tuning pairs across 10 Indian languages
- Pragyaan-Align contains 100K preference-tuning pairs for DPO alignment
- Models fine-tuned with Pragyaan-Align achieve 60-61% win rates on Updesh benchmark
- Dataset covers 13 broad categories with 56 sub-categories ensuring task diversity
- IC-3 (explicitly Indian) comprises 57.8% of Pragyaan-IT, showing heavy cultural grounding

## Why This Works (Mechanism)

### Mechanism 1
Human-in-the-loop refinement systematically corrects translation errors and cultural misalignments that purely synthetic approaches introduce. Annotators evaluate LLM outputs along 5 dimensions (relevance, grammatical accuracy, cohesion/coherence, rationality, completeness for generation; lexical diversity, coherence, completeness, grammatical accuracy, named entity handling for translation), then either adapt configurations or regenerate pairs. This catches both mechanical errors (wrong case endings, subject-verb disagreement common in morphologically rich Indic languages) and cultural mismatches (Western herbs vs. tulsi/pudina).

### Mechanism 2
Three-tier Indian Cultural Context (IC-1/IC-2/IC-3) taxonomy enables controlled cultural grounding progression in training data. IC-1 provides culturally neutral generic responses (e.g., "pancakes, cereal"), IC-2 provides Indic-grounded responses to generic prompts (e.g., "idli, dosa, poha"), IC-3 uses explicitly Indic prompts eliciting fully Indic responses. Models trained on IC-2/IC-3 learn to surface culturally appropriate defaults without explicit prompting.

### Mechanism 3
Preference dataset construction with human-curated chosen/rejected pairs enables DPO-based alignment for multilingual contexts. Pragyaan-Align provides 100K preference examples where preferred responses demonstrate better cultural grounding, instruction following, or safety. DPO optimization (β=0.3) shifts model probability mass toward preferred outputs without requiring a separate reward model.

## Foundational Learning

**Instruction Tuning (SFT):**
- Why needed here: Pragyaan-IT is fundamentally an instruction-tuning dataset; understanding the objective (next-token prediction on instruction-response pairs) is prerequisite.
- Quick check question: Can you explain why instruction tuning improves zero-shot task generalization compared to pre-training alone?

**Direct Preference Optimization (DPO):**
- Why needed here: The paper uses DPO for alignment experiments; understanding the objective (implicit reward modeling via classification loss) is essential for interpreting results.
- Quick check question: How does DPO differ from PPO-based RLHF in terms of computational requirements and reward model dependency?

**Cross-lingual Transfer:**
- Why needed here: The pipeline translates English prompts/responses; understanding translation artifacts and cross-lingual knowledge transfer helps diagnose failure modes.
- Quick check question: What types of errors commonly arise when translating instruction-tuning data between typologically distant languages?

## Architecture Onboarding

**Component map:** Seed Data Pool (57 datasets → 56 sub-categories) -> Prompt Generator (modified Self-Instruct) -> Translation Layer (Krutrim-2-12B) -> Human Annotation Interface (50 annotators) -> Quality Assessment (5-dimension scoring) -> Configuration Assignment (complexity, turns, IF level, safety, IC tier, thinking) -> Dataset merge -> DPO training

**Critical path:** Seed data → Approach selection (translation vs. synthetic) → LLM generation/translation → Human refinement → Configuration assignment (complexity, turns, IF level, safety, IC tier, thinking) → Dataset merge → DPO training

**Design tradeoffs:**
- Translation fidelity vs. cultural adaptation: Generic translations (IC-1) preserve source semantics but lose cultural relevance; context-adapted translations (IC-2/IC-3) improve grounding but risk semantic drift
- Dataset size vs. annotation depth: Pragyaan-IT (22.5K) is small but richly annotated; scaling to 100K+ may require relaxing annotation guidelines
- Easy vs. hard complexity: 62.3% easy tasks may limit reasoning capability gains; hard tasks (37.7%) are more expensive to curate

**Failure signatures:**
- Translation artifacts: Fixed expressions, idioms, named entities rendered incorrectly (e.g., "thyme" transliterated instead of substituted with "tulsi")
- Cultural hallucination: Model fabricates Indian cultural details when IC-3 prompts lack sufficient context
- Preference annotation bias: Systematic preference for longer/verbose responses regardless of quality
- Language imbalance: Gujarati (17.7%) over-represented vs. Tamil (figure ~8%)—may cause uneven performance

**First 3 experiments:**
1. **Ablate human refinement:** Train on raw synthetic/translated data (no human editing) vs. HITL-refined data; measure delta on Updesh benchmark to quantify annotation value
2. **IC-tier mixture analysis:** Train three model variants (IC-1 only, IC-2 only, IC-3 only); evaluate cultural appropriateness on held-out Indic prompts to isolate grounding effects
3. **Cross-lingual transfer probe:** Fine-tune on Hindi-only subset, evaluate on Tamil/Telugu to assess whether cultural grounding transfers across related Indic languages

## Open Questions the Paper Calls Out

**Open Question 1:** How does model performance scale with increased representation of complex reasoning (CoT, self-thinking) tasks that currently comprise only 0.02% of Pragyaan-IT? The paper explicitly notes that thinking trails constitute only 0.01% each of the dataset, and future iterations will expand complex reasoning capabilities. The near-absence of reasoning examples makes it impossible to assess whether the cultural grounding approach generalizes to tasks requiring explicit multi-step reasoning.

**Open Question 2:** Can the human-in-the-loop pipeline generalize to other multilingual contexts without significant re-validation of translation quality and cultural adaptation guidelines? The paper states that extending the pipeline to new cultural and linguistic contexts will require additional validation, as the generalization of findings to other multilingual settings remain currently unexplored. The annotation guidelines, quality dimensions, and cultural context levels (IC-1 to IC-3) were designed specifically for Indian languages with their particular morphological complexity and cultural features.

**Open Question 3:** To what extent does the heavy imbalance toward single-turn (91.66%) and simple instruction-following (96.95%) tasks limit model performance on complex, multi-constraint user interactions? The dataset analysis reveals severe skewness, yet the evaluation uses only single-turn examples from Updesh, leaving multi-turn and complex instruction-following capabilities unevaluated despite being stated dataset goals.

## Limitations
- Evaluation relies on LLM-as-a-judge scoring which may introduce systematic bias
- The proprietary LLM (Krutrim internal model) used for translation is unspecified, making exact reproduction challenging
- No inter-annotator agreement statistics reported to validate human refinement reliability
- Heavy dataset skew toward single-turn (91.66%) and easy tasks (62.3%) may limit complex reasoning capabilities

## Confidence

**High Confidence:** The core pipeline architecture (seed data → translation/synthetic expansion → human refinement → configuration assignment) is clearly specified with concrete numbers (22.5K IT pairs, 100K Align pairs, 10 languages, 56 sub-categories).

**Medium Confidence:** The three-tier cultural grounding taxonomy (IC-1/IC-2/IC-3) is well-defined conceptually, but the paper provides limited empirical evidence that this specific framework produces measurable improvements in cultural appropriateness beyond anecdotal examples.

**Low Confidence:** The DPO optimization results showing 60-61% win rates are promising but rely on LLM judgment rather than human evaluation, and the baseline comparison conditions are not fully detailed.

## Next Checks

1. **Annotator Agreement Audit:** Measure inter-annotator agreement (Cohen's kappa or similar) across the five quality dimensions to validate the reliability of human refinement. Low agreement would indicate subjective cultural judgments that may not generalize.

2. **Cultural Grounding Ablation:** Train three model variants using only IC-1 (generic), IC-2 (Indic-grounded), or IC-3 (explicitly Indian) data, then evaluate on held-out Indic cultural prompts to isolate the contribution of each grounding level.

3. **Cross-Validation of LLM-as-Judge:** Re-run the Updesh benchmark evaluation with multiple LLM judges (different models, prompting strategies) and compare against a small sample of human judgments to assess potential systematic bias in the automated evaluation.