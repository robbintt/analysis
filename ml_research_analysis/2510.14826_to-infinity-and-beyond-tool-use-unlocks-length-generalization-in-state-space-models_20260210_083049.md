---
ver: rpa2
title: 'To Infinity and Beyond: Tool-Use Unlocks Length Generalization in State Space
  Models'
arxiv_id: '2510.14826'
source_url: https://arxiv.org/abs/2510.14826
tags:
- length
- arxiv
- tool
- memory
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: State Space Models (SSMs) cannot solve long-form generation tasks
  due to their fixed memory size, limiting their performance compared to Transformers.
  This work shows that allowing SSMs interactive access to external tools overcomes
  this limitation, enabling length generalization on tasks like arithmetic, reasoning,
  and coding.
---

# To Infinity and Beyond: Tool-Use Unlocks Length Generalization in State Space Models

## Quick Facts
- arXiv ID: 2510.14826
- Source URL: https://arxiv.org/abs/2510.14826
- Reference count: 40
- Primary result: Interactive tool use enables SSMs to achieve length generalization on arithmetic, reasoning, and coding tasks, matching or exceeding Transformer performance.

## Executive Summary
State Space Models (SSMs) struggle with long-form generation tasks due to their fixed memory size, limiting their ability to generalize beyond training lengths. This work demonstrates that allowing SSMs interactive access to external tools overcomes this limitation, enabling them to solve problems far beyond their training complexity. By training on task-specific tool-use trajectories, SSMs achieve high accuracy on arithmetic, reasoning, and coding tasks where traditional approaches fail to extrapolate. The key insight is that external tools with unbounded memory simulate Turing machine behavior, allowing SSMs to learn algorithms rather than memorize patterns.

## Method Summary
The approach involves training SSMs on algorithm-execution trajectories that include interleaved thoughts, tool commands, and observations. Models are trained with next-token prediction while masking observation tokens (generated by tools, not the model). Training distributions contain full execution traces for tasks like multi-digit arithmetic, Tower of Hanoi, logical graphs, and code fixing. At inference, tool oracles execute commands and return observations. The method uses pointer-based or search-based memory tools depending on task structure, with interactive multi-turn tool use being essential for length generalization.

## Key Results
- SSMs achieve ~100% accuracy on 1000-digit addition trained only on 5-digit examples
- Mamba-130M outperforms Pythia-160M and Mistral-7B on length generalization across all tasks
- Tool-augmented SSMs maintain high accuracy on problems 10-100× longer than training data
- Single-turn tool use fails to provide length generalization (per Theorem 2.1)

## Why This Works (Mechanism)

### Mechanism 1: Bounded State Constraint on Output Diversity
SSMs have fixed-size state spaces that limit the number of distinct outputs they can generate. When output diversity grows with problem complexity (as in truly long-form generation tasks), the model cannot cover the required output distribution, leading to error ≥ 1-α. This limitation applies to CoT-only or single-turn tool-use modes where outputs are deterministic functions of the bounded state.

### Mechanism 2: External Memory Tool as Turing Machine Simulation
Interactive tool-use with pointer-based read/write memory enables SSMs to simulate Turing machines. The model's internal state tracks the finite control while external memory serves as unbounded tape. Training trajectories expose all (state, symbol) transition pairs; once learned, the model generalizes to arbitrary lengths. This works when training data covers sufficient samples of required transition pairs.

### Mechanism 3: Trajectory-Based Training with Localized Computation
Training on algorithm-execution trajectories (not just input-output pairs) enables learning computational procedures rather than memorizing patterns. Training distributions include full trajectories with interleaved thoughts, tool commands, and observations. The next-token prediction objective with masking forces the model to learn algorithm control flow. Each step depends only on local information, allowing learned patterns to transfer to longer inputs.

## Foundational Learning

- **State Space Models (SSMs)**:
  - Why needed here: Central to understanding why bounded memory causes failure
  - Quick check question: Can you explain why an SSM's state at position t has the same dimension regardless of sequence length?

- **Length Generalization**:
  - Why needed here: Formally defined ability to achieve low error on complexity n ≥ n₀ after training only on complexities up to n₀
  - Quick check question: If a model achieves 100% accuracy on 5-digit addition but 0% on 6-digit addition, does it have length generalization?

- **ReAct Agent Framework**:
  - Why needed here: Models tool-augmented agents using interleaved thoughts, actions, observations
  - Quick check question: What is the difference between a "thought" token and an "action" token in the ReAct framework?

## Architecture Onboarding

- **Component map**:
  Input x → [SSM Core: fixed state h_t] → Token generation loop
                    ↑                           ↓
                    └── [Tool Oracle: unbounded memory M_t] ← Tool commands

- **Critical path**:
  1. Define memory tool interface (read, write, move_left, move_right) as special tokens
  2. Generate training trajectories that execute target algorithm using the tool
  3. Train SSM with next-token prediction, masking observation tokens
  4. At inference, tool oracle executes commands and returns observations

- **Design tradeoffs**:
  - **Pointer-based vs. search-based memory**: Pointer-based works for arithmetic; search-based works for graph reasoning
  - **Interactive vs. single-turn**: Only interactive multi-turn use solves bounded memory problem
  - **SSM vs. Transformer**: SSMs excel at parsing long tool outputs efficiently; Transformers may outperform on small inputs but fail to extrapolate

- **Failure signatures**:
  - Perfect in-distribution accuracy but rapid degradation on longer inputs → bounded memory limitation without proper tool use
  - High token accuracy but low exact-match accuracy (e.g., Tower of Hanoi) → task requires exponential output length
  - Model generates invalid tool commands → training trajectories didn't cover required tool usage patterns

- **First 3 experiments**:
  1. Replicate 5-digit → 1000-digit addition: Train Mamba-130M on pointer-based addition trajectories (≤5 digits), evaluate on up to 1000 digits
  2. Ablate tool interactivity: Train same task with single-turn tool (calculator that returns full answer), expect perfect in-distribution but no length generalization
  3. Test search-based tool on logical graphs: Train on graphs with ≤10 nodes using search tool, evaluate on 100-1000 nodes, expect Mamba maintains high accuracy while Transformers degrade

## Open Questions the Paper Calls Out

- Can the theoretical guarantee of length generalization be formally extended from the simplified string-matching learner to standard gradient descent on neural networks? (Footnote 6, Page 6)
- Why do tool-augmented SSMs struggle with length generalization on recursive algorithm implementations compared to iterative ones? (Appendix D.4, Page 22)
- To what extent does reliance on synthetically curated, problem-specific training data limit applicability to general-purpose pre-training? (Introduction)

## Limitations

- Theoretical results rely on simplified string-matching learners rather than actual gradient descent optimization
- Limited evaluation on real-world code fixing beyond small codebases (16 functions)
- Performance on complex, multi-step reasoning tasks requiring nested tool calls remains unexplored
- Recursive algorithm implementations show weaker length generalization than iterative approaches

## Confidence

- **High**: SSMs with interactive tools can achieve length generalization on synthetic arithmetic/reasoning tasks
- **Medium**: SSMs outperform Transformers on tool-augmented tasks beyond training length
- **Medium**: Tool interactivity is necessary (not sufficient with single-turn use)
- **Low**: Performance transfers directly to complex real-world code fixing

## Next Checks

1. Test SSMs on multi-step reasoning tasks requiring variable-length tool interactions (e.g., planning problems with nested tool calls)
2. Evaluate model performance on code fixing with progressively larger codebases (10→100→1000 functions)
3. Implement ablation studies removing trajectory-based training to confirm learning algorithm execution is essential