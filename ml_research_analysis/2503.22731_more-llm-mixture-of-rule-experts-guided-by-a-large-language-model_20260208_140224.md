---
ver: rpa2
title: 'MoRE-LLM: Mixture of Rule Experts Guided by a Large Language Model'
arxiv_id: '2503.22731'
source_url: https://arxiv.org/abs/2503.22731
tags:
- rule
- rules
- knowledge
- which
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MoRE-LLM, a framework that integrates large
  language models (LLMs) into the training process of a small, interpretable classifier
  to enhance both accuracy and interpretability. The method combines a black-box model
  with a rule-based classifier through a mixture-of-experts architecture, where an
  LLM refines the rules for better domain alignment.
---

# MoRE-LLM: Mixture of Rule Experts Guided by a Large Language Model

## Quick Facts
- **arXiv ID**: 2503.22731
- **Source URL**: https://arxiv.org/abs/2503.22731
- **Reference count**: 24
- **Primary result**: Matches black-box accuracy while significantly increasing interpretable rule utilization (up to 200%) and reducing rule count (up to 60%) through LLM-guided pruning

## Executive Summary
MoRE-LLM introduces a framework that integrates large language models into the training process of small, interpretable classifiers to enhance both accuracy and interpretability. The method combines a black-box model with a rule-based classifier through a mixture-of-experts architecture, where an LLM refines the rules for better domain alignment. The key innovation is using the LLM to correct and contextualize rules during training while a gating model prevents confabulations by assigning instances to either the rule-based or black-box model based on performance. Experiments on three tabular datasets show that MoRE-LLM matches the performance of non-interpretable baselines while significantly increasing the use of interpretable rules.

## Method Summary
MoRE-LLM trains a small, interpretable classifier alongside a black-box model using a mixture-of-experts architecture. The method iteratively generates rules for regions currently assigned to the black-box model, using Anchors to create local surrogate rules that approximate the black-box decision boundary. An LLM then refines these rules by adjusting thresholds, removing predicates, and pruning contradictory or over-specific rules based on domain knowledge. A gating model, trained via constrained optimization with Dynamic Barrier Gradient Descent, assigns instances to either the rule-based classifier or black-box model while maintaining performance within a tolerance threshold. The LLM access is only required during training, with the final interpretable model deployable without LLM dependencies.

## Key Results
- Matches black-box model accuracy while significantly increasing interpretable rule utilization (up to 200% increase in rule usage compared to baseline)
- Reduces rule count by up to 60% through LLM-based pruning while maintaining coverage
- Provides human-understandable explanations for predictions without requiring LLM access at test time
- Successfully applied to three tabular datasets (adult, g-credit, diabetes) with performance gains over existing interpretable baselines

## Why This Works (Mechanism)

### Mechanism 1: Performance-Constrained Gate Routing
A learned gating model maximizes interpretable rule utilization while maintaining black-box performance within a tolerance threshold. The gating model g_ω assigns instances to either the rule-based classifier r_R or black-box model f_θ using a constrained optimization objective. An auxiliary interpretability loss l_int(x) = -log(g2_ω(x)) encourages rule assignment, while a hard constraint L_task(g_ω, f_θ, r_R) ≤ (1 + ε)L_task(f_θ*) bounds performance degradation. Dynamic Barrier Gradient Descent (DBGD) handles the non-convex optimization by computing an adaptive coefficient λ_t that weighs the interpretability gradient against the task gradient at each step.

### Mechanism 2: LLM-Guided Rule Regularization via Domain Knowledge Injection
An LLM regularizes discovered rules toward domain alignment by pruning irrelevant predicates and adjusting thresholds, improving rule quality without requiring LLM access at inference. After Anchors generates local surrogate rules, the LLM receives the full rule set with feature descriptions and performs rule adaptation—removing predicates, adjusting numerical thresholds, swapping categorical values, or correcting output classes to align with domain knowledge—and rule pruning—removing contradictory, over-specific, or under-complex rules. The LLM outputs structured responses with reasoning that are stored for deployment-time explanation augmentation.

### Mechanism 3: Iterative Rule Discovery with Gate-Guided Exploration
Iteratively generating rules for regions currently assigned to the black-box model efficiently expands rule coverage while respecting the performance constraint. Each iteration samples B instances from regions where g_ω assigns to f_θ (not r_R), using mixed strategy: exploitation—low-entropy samples where f_θ is confident, trusting its predictions; exploration—high-entropy samples where f_θ is uncertain, deferring rule quality assessment to the gate and LLM. Anchors generates local surrogate rules approximating f_θ's local decision boundary with precision threshold τ. Duplicate rules (same predicates) are removed post-generation.

## Foundational Learning

- **Concept: Mixture of Experts (MoE) with Gating**
  - Why needed here: MoRE-LLM's core architecture routes inputs dynamically between experts (rule-based classifier vs. black-box model) based on a learned gating function; understanding soft/hard routing and load balancing is prerequisite.
  - Quick check question: Given a 2-expert MoE with gate outputs [0.3, 0.7] and expert predictions [class_0, class_1], what is the final prediction if using soft routing? What if using hard routing (argmax)?

- **Concept: Local Surrogate Explanation Methods (Anchors/LIME)**
  - Why needed here: MoRE-LLM uses Anchors to generate rule-based local surrogates of the black-box model; understanding fidelity, coverage, and precision thresholds is essential for diagnosing rule quality.
  - Quick check question: Anchors generates a rule with precision 0.95 and coverage 0.12 on a local neighborhood. What does this tell you about the rule's reliability vs. generality?

- **Concept: Constrained Optimization with Barrier Methods**
  - Why needed here: The gate optimization uses DBGD to maximize interpretability subject to a performance constraint; understanding barrier functions, feasibility, and gradient projection clarifies why simple weighted loss combinations fail.
  - Quick check question: Why does a fixed λ in min(L_int + λ·L_task) fail to guarantee constraint satisfaction L_task ≤ (1+ε)L_task*? How does an adaptive λ_t address this?

## Architecture Onboarding

- **Component map:**
Training-time components:
Black-box classifier f_θ (MLP or LR)
Gating model g_ω (MLP or LR, 2 outputs: g1→f, g2→r_R)
Rule-based classifier r_R (rule set R = {R_1, ..., R_U})
Explainer module E (Anchors, generates local surrogates)
LLM Q (GPT-4, receives rules + feature descriptions, outputs adapted/pruned rules + reasoning)

Test-time components:
f_θ
g_ω
r_R (with stored rules)
Stored LLM-generated justifications (per-rule)

- **Critical path:**
1. Initialize: Train f_θ* unconstrained on full dataset D
2. Iterate (typically 3 iterations):
- Sample D_S ⊂ D from regions where gate assigns to f_θ (mixed explore/exploit)
- Generate rules via Anchors: R_u = E(x_u, f_θ) for each x_u ∈ D_S
- Query LLM to adapt rules (predicate removal, threshold adjustment) and prune (remove contradictions)
- Update rule set R, remove duplicates
- Constrained optimization: update g_ω via DBGD to maximize rule usage while L_task ≤ (1+ε)L_task(f_θ*); update f_θ via L_task for all samples
3. Deploy: Store final R with LLM justifications; no LLM access needed at inference

- **Design tradeoffs:**
- ε (slack): Lower ε = stricter performance constraint → less rule usage but higher accuracy guarantee; ε=0.1 used in experiments
- Explore vs. exploit sampling ratio: Paper uses 4 explore + 4 exploit per iteration; more exploration increases rule diversity but may generate lower-quality rules
- Rule budget (B per iteration): More rules = higher coverage but increased complexity; duplicates automatically removed
- LLM choice: GPT-4 used; smaller models may lack domain knowledge for effective refinement

- **Failure signatures:**
- Gate collapse: g_ω always outputs [1, 0] (all black-box) → check if ε too low or rules too inaccurate
- Rule explosion: R grows without coverage increase → check duplicate removal, LLM pruning effectiveness
- LLM hallucination: Rules contain features not in training data → strengthen prompt constraints on valid features/operators
- Accuracy drop on test set: Constraint satisfied on training but not test → may indicate overfitting; consider regularization

- **First 3 experiments:**
1. Baseline replication (LR architecture): Run MoRE-LLM on a single tabular dataset (e.g., diabetes) with ε=0.1, 3 iterations; compare rule coverage/usage vs. MoRE-without-LLM to isolate LLM contribution
2. Ablation on ε: Run with ε ∈ {0.05, 0.1, 0.2} on the same dataset; plot coverage vs. test accuracy to find the Pareto frontier
3. Rule quality inspection: After training, manually inspect 10 rules with LLM justifications; verify that threshold adjustments align with domain knowledge and pruning decisions are sensible

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but raises several important considerations about scalability, LLM dependency, and optimization tradeoffs that warrant further investigation.

## Limitations
- LLM's domain knowledge is assumed sufficient for effective rule refinement but may fail on specialized domains where pre-training coverage is sparse
- Rule fidelity depends entirely on Anchors' ability to generate high-quality local surrogates, which may be limited in high-dimensional or noisy regions
- The performance constraint (ε=0.1) was chosen heuristically and may not be optimal across different dataset complexities

## Confidence
- **High confidence**: The mixture-of-experts architecture with performance-constrained gating is well-established and the experimental results (accuracy maintenance, rule usage increase) are reproducible given the implementation details provided.
- **Medium confidence**: The LLM-guided rule refinement mechanism is novel and the reported 60% rule reduction through pruning is plausible, but effectiveness depends on prompt quality and LLM domain knowledge alignment, which varies by dataset.
- **Medium confidence**: The iterative rule discovery approach is theoretically sound, but the explore-exploit sampling strategy (4+4 per iteration) may not be optimal for all dataset sizes and complexities; coverage vs. quality tradeoffs need further study.

## Next Checks
1. **Rule quality validation**: Manually inspect 10 rules from each dataset with LLM justifications to verify threshold adjustments align with domain knowledge and pruning decisions are sensible, not arbitrary.
2. **Constraint sensitivity analysis**: Run experiments with ε ∈ {0.05, 0.1, 0.2} on diabetes dataset to quantify the Pareto frontier between coverage increase and accuracy maintenance.
3. **LLM knowledge dependency**: Test MoRE-LLM on a dataset with specialized domain knowledge (e.g., molecular properties) where LLM pre-training coverage is limited to assess when LLM-guided refinement fails.