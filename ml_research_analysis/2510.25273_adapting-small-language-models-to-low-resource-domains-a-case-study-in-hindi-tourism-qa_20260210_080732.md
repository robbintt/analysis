---
ver: rpa2
title: 'Adapting Small Language Models to Low-Resource Domains: A Case Study in Hindi
  Tourism QA'
arxiv_id: '2510.25273'
source_url: https://arxiv.org/abs/2510.25273
tags:
- data
- synthetic
- language
- question
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of domain-specific question answering
  in low-resource languages, particularly Hindi, by proposing a multi-stage fine-tuning
  strategy that leverages both original and synthetic data. Large language models
  (LLaMA-70B and Phi-14B) are used to generate synthetic question-answer pairs, which
  are then combined with the limited original dataset to fine-tune a smaller model
  (LLaMA-3.1-8B).
---

# Adapting Small Language Models to Low-Resource Domains: A Case Study in Hindi Tourism QA

## Quick Facts
- arXiv ID: 2510.25273
- Source URL: https://arxiv.org/abs/2510.25273
- Reference count: 25
- Primary result: Staged fine-tuning (original data → synthetic data) improves robustness on unseen test distributions compared to single-stage or multi-source mixing

## Executive Summary
This paper addresses the challenge of domain-specific question answering in low-resource languages, specifically Hindi tourism. The authors propose a multi-stage fine-tuning strategy that leverages both original and synthetic data. Large language models (LLaMA-70B and Phi-14B) generate synthetic question-answer pairs from the limited original dataset, which are then combined with the original data to fine-tune a smaller model (LLaMA-3.1-8B). The experimental results demonstrate that continued fine-tuning—first on original data, then on synthetic data—improves model robustness and performance on unseen data. The model achieved top rankings in BLEU-2 and QA-F1 metrics on a held-out test set, validating the effectiveness of synthetic data augmentation. However, indiscriminate mixing of multi-source synthetic data can degrade performance, highlighting the need for careful curation.

## Method Summary
The authors employ a three-stage approach: first, they generate synthetic question-answer pairs using large language models (LLaMA-70B and Phi-14B) via few-shot prompting on the original training contexts. Second, they fine-tune a small model (LLaMA-3.1-8B) using three different strategies: baseline training on original data, continued fine-tuning on original followed by synthetic data, and multi-source training combining all data. Finally, they evaluate the models using ROUGE-L, BLEU, and BERTScore metrics across multiple test splits. The staged approach (original → synthetic) proved most effective for improving robustness on held-out test distributions, while indiscriminate mixing of synthetic sources degraded precision metrics.

## Key Results
- Continued fine-tuning (original data → synthetic data) achieved superior BLEU-2 and QA-F1 scores on held-out test sets
- The staged approach improved robustness to distributional shifts compared to single-stage training
- Multi-source synthetic data mixing degraded BLEU performance due to conflicting answers for similar questions
- The best model (M2) ranked first in BLEU-2 and QA-F1 metrics on the held-out test set

## Why This Works (Mechanism)

### Mechanism 1: Staged Fine-Tuning for Distributional Robustness
- Claim: Continued fine-tuning on synthetic data, following initial training on original data, improves robustness to held-out, unseen test distributions compared to single-stage or multi-source mixing strategies
- Mechanism: The staged exposure allows the model to first learn the fundamental characteristics and noise profile of the high-quality, authentic domain data, establishing a strong prior. Subsequent, controlled exposure to a larger volume of synthetic data then expands the model's knowledge and linguistic variations without completely overwriting the grounded representations learned from the original data
- Core assumption: The synthetic data contains distributional artifacts or noise that differ from the authentic domain, and the original dataset provides a cleaner grounding
- Evidence anchors: [abstract] "...best results were achieved with continued fine-tuning—first on original data, then on synthetic data—which improved robustness on held-out test sets"; [Page 4, Section 6] "A crucial insight comes from the two-stage trained model (M2[5]). Despite its moderate performance on Test Data-1, it achieved superior BLEU-2 and QA-F1 scores on the held-out Test Data-2... This suggests that late exposure to synthetic data is effective for building distribution robustness."
- Break condition: The benefit would diminish or reverse if the original dataset is extremely small or of poor quality, or if the synthetic data is of exceptionally high fidelity

### Mechanism 2: Knowledge Distillation from Large to Small Models
- Claim: Large language models can effectively transfer broad, pre-trained knowledge into a specialized, structured format (synthetic QA pairs) that smaller models can then efficiently learn from
- Mechanism: The large LLM acts as a knowledge distiller, using its vast pre-training corpus to extract information relevant to the low-resource domain and format it into task-specific training instances. The smaller model's task is simplified from learning world knowledge and the QA format to primarily learning the task format and domain-specific patterns from a much larger synthetic dataset
- Core assumption: The large LLM's pre-training data contained a non-trivial amount of information about the target domain (Hindi tourism), and this knowledge can be reliably surfaced with simple prompts
- Evidence anchors: [abstract] "...large models can efficiently generate synthetic data, while small models can effectively adapt to it..."; [Page 6, Table 5] Shows examples of synthetic QA pairs generated by LLaMA-70B and Phi-4-14B that are relevant and correctly formatted
- Break condition: This mechanism would fail if the large model's training data had virtually no exposure to the target domain, resulting in hallucinated or nonsensical synthetic data

### Mechanism 3: Synthetic Data Quality and Multi-Source Conflicts
- Claim: Indiscriminately combining synthetic data from multiple, diverse sources can degrade performance on certain metrics due to conflicting signals
- Mechanism: Merging data from multiple synthetic generators without curation can introduce conflicting answers for similar questions, as each model may have its own biases and generation patterns. This ambiguity can confuse the fine-tuning process, leading to a degradation in precision-oriented metrics like BLEU
- Core assumption: Different LLMs will generate meaningfully different, and sometimes conflicting, responses for the same input context
- Evidence anchors: [Page 4, Section 6] "However, the combined multi-source model (M3[5]) underperformed in BLEU-2, which is a precision based metric. A potential reason is that combining multi-source data may introduce conflicting answers for similar questions, leading to ambiguity and performance degradation."
- Break condition: The negative impact would be minimal if the synthetic generators produce highly consistent answers or if a robust data curation pipeline is used

## Foundational Learning

### Concept: Curriculum Learning / Staged Fine-Tuning
- Why needed here: The paper's core finding is that the order of training data presentation matters. Understanding that simply having the right data is insufficient; the training sequence (original → synthetic) functions as a curriculum that influences the model's final capabilities and robustness
- Quick check question: If you have a small, high-quality dataset (A) and a large, noisier dataset (B), which training strategy—train on (A+B) together, or train on A then fine-tune on B—would this paper suggest for better held-out performance?

### Concept: Synthetic Data Distillation
- Why needed here: This work relies on using a larger "teacher" model to generate training data for a smaller "student" model. Understanding the role of each—the teacher provides broad knowledge in a structured format, while the student learns to mimic this expertise efficiently
- Quick check question: What is the primary risk of using synthetic data from a large LLM to train a smaller model for a specialized domain, and how does the paper suggest mitigating it (hint: see Mechanism 1)?

### Concept: Distributional Robustness & Shift
- Why needed here: The paper evaluates models on multiple test sets (Validation, Test-1, Test-2) and finds that the best model varies by set. Understanding that performance is not monolithic; models can overfit to specific data distributions and must generalize to unseen, potentially shifted distributions
- Quick check question: Why did the model trained solely on the smaller, original dataset (M1) perform best on the validation set, but the continued fine-tuning model (M2) performed best on the held-out Test-2 set? What does this imply about their respective strengths?

## Architecture Onboarding

### Component map:
1. **Synthetic Data Generator (Teacher):** Large LLMs (LLaMA-70B, Phi-14B). Function: Takes training set contexts as input and generates new question-answer pairs using few-shot prompting
2. **Data Augmentation Pipeline:** Function: Combines original and synthetic datasets. This includes strategies for mixing (baseline, continued, multi-source) and potential quality filtering
3. **Task-Specific Model (Student):** Small LLM (LLaMA-3.1-8B). Function: The model to be fine-tuned and deployed. It learns from the augmented dataset
4. **Evaluation Harness:** Function: Assesses model performance using metrics like ROUGE-L, BLEU, and BERTScore across multiple test splits to measure both in-distribution and out-of-distribution robustness

### Critical path:
The most critical sequence for achieving the paper's best results is: Generate synthetic QA pairs from training contexts using a large LLM → Stage 1 Fine-Tuning: Train the small model (LLaMA-8B) exclusively on the original, high-quality training data to establish a grounded prior → Stage 2 Fine-Tuning: Continue training the same model on the large, synthetic dataset

### Design tradeoffs:
- **Precision vs. Robustness:** The multi-source model (M3) achieved the highest ROUGE-L on Test-1 but had lower BLEU scores, suggesting it learned broader, more diverse responses at the cost of precision. The staged model (M2) offered the best balance for held-out robustness
- **Data Volume vs. Quality:** The paper argues against indiscriminately mixing all available data (M3) in favor of a more controlled integration (M2). The tradeoff is between the sheer volume of data (which can help) and the potential for introducing noise and conflicting signals (which can hurt)
- **Computational Cost:** Generating synthetic data with a 70B model is costly, but it's a one-time cost. Fine-tuning the 8B model is cheap. The overall system trades the high inference cost of a large model for the one-time generation cost and the low inference cost of a small, fine-tuned model

### Failure signatures:
- **Catastrophic Forgetting:** If Stage 1 training is too long, or if Stage 2 uses a high learning rate, the model might forget the foundational knowledge learned from the original data. This would manifest as poor performance on the original validation set
- **Synthetic Artifact Overfitting:** If the synthetic data contains repetitive patterns or hallucinations, the fine-tuned model will generate outputs that reflect these artifacts. This might look like plausible-sounding but factually incorrect or nonsensical answers
- **Distribution Mismatch:** If the synthetic data is too different from the real-world test distribution, any model trained on it (M2 or M3) will fail to generalize, performing no better than the baseline (M1)

### First 3 experiments:
1. **Reproduce the Key Comparison:** Implement the baseline (M1) and the continued fine-tuning (M2) setups. Compare their performance on the provided validation and Test-1 sets to validate the paper's core claim about robustness
2. **Ablation on Synthetic Data Sources:** Instead of mixing both LLaMA and Phi synthetic data (as in M3), create two separate continued fine-tuning experiments: M2-LLaMA (train on original, then LLaMA-synthetic) and M2-Phi (train on original, then Phi-synthetic). This isolates the contribution of each synthetic generator and tests the "multi-source conflict" hypothesis
3. **Varying the Synthetic Ratio:** In the continued fine-tuning stage (M2), experiment with different ratios of synthetic data (e.g., 10%, 25%, 50%, 100% of the synthetic corpus). This will help determine if there's a point of diminishing returns where more synthetic data begins to introduce more noise than signal

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the multi-stage fine-tuning strategy be effectively generalized to other low-resource languages?
- Basis in paper: [explicit] The Conclusion states the approach "can be extended to other low-resource languages to test its generalizability."
- Why unresolved: The current study is limited to a case study in Hindi tourism
- What evidence would resolve it: Successful replication of the continued fine-tuning method across diverse linguistic families with similar data constraints

### Open Question 2
- Question: How effective are LLM-based filtering methods for the systematic evaluation of synthetic data quality?
- Basis in paper: [explicit] The Conclusion lists "systematic evaluation of synthetic data quality, potentially using LLM-based filtering methods" as future work
- Why unresolved: The authors note considerable overlap and redundancy in synthetic outputs but lack a rigorous, automated selection stage to filter for quality
- What evidence would resolve it: A benchmark showing that LLM-curated synthetic datasets yield higher downstream performance than unfiltered synthetic datasets

### Open Question 3
- Question: Does the joint evaluation of question and answer pairs improve data curation compared to evaluating them separately?
- Basis in paper: [explicit] Section 6 suggests "joint evaluation of both question and answer within each synthetic pair" as a potential direction for future research
- Why unresolved: Current analysis highlights redundancy, but it is unclear if checking the Q&A together better captures semantic alignment than independent checks
- What evidence would resolve it: Development of a joint metric that correlates more strongly with human judgments of QA relevance than independent metrics

## Limitations

- The exact prompt templates for synthetic data generation are not specified, making exact reproduction difficult
- Performance gains on held-out Test-2 are significant but based on a single test set
- The study focuses on one specific domain (Hindi tourism) and language pair, limiting generalizability
- Multi-source mixing (M3) degradation suggests the need for sophisticated data selection, but the paper doesn't provide a clear selection methodology

## Confidence

- **High Confidence:** The core finding that continued fine-tuning (original → synthetic) improves robustness compared to single-stage training
- **Medium Confidence:** The claim that multi-source synthetic data mixing can degrade performance
- **Medium Confidence:** The general applicability of the staged fine-tuning approach to other low-resource domains

## Next Checks

1. **Prompt Template Validation:** Reconstruct and test the few-shot prompt templates used for synthetic data generation to verify that generated data matches the quality and characteristics shown in Table 5
2. **Cross-Domain Transfer Test:** Apply the M2 (continued fine-tuning) approach to a different low-resource domain (e.g., medical or legal Hindi QA) to assess generalizability beyond tourism
3. **Synthetic Data Ratio Sensitivity:** Systematically vary the proportion of synthetic data in the continued fine-tuning stage (M2) to identify the optimal ratio and test whether performance degrades with excessive synthetic exposure