---
ver: rpa2
title: Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology
  Texts using BERT Embeddings
arxiv_id: '2510.17437'
source_url: https://arxiv.org/abs/2510.17437
tags:
- recognition
- clinical
- spanish
- medications
- biomedical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study developed deep contextual embedding models for clinical
  named entity recognition (NER) in the cardiology domain across three languages:
  Spanish, English, and Italian. The research focused on extracting disease and medication
  mentions from clinical case reports using BERT-based models fine-tuned on general
  domain text.'
---

# Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings

## Quick Facts
- arXiv ID: 2510.17437
- Source URL: https://arxiv.org/abs/2510.17437
- Reference count: 40
- Best F1 scores: SDR 77.88%, SMR 92.09%, EMR 91.74%, IMR 88.9%

## Executive Summary
This study developed deep contextual embedding models for clinical named entity recognition (NER) in the cardiology domain across three languages: Spanish, English, and Italian. The research focused on extracting disease and medication mentions from clinical case reports using BERT-based models fine-tuned on general domain text. Four monolingual models were created for Spanish Diseases Recognition (SDR), Spanish Medications Recognition (SMR), English Medications Recognition (EMR), and Italian Medications Recognition (IMR), along with two multilingual models for SDR and medications recognition across all three languages. The models were fine-tuned using the MultiCardioNER dataset, which includes clinical case reports annotated with diseases and medications. The best-performing models achieved F1-scores of 77.88% for SDR, 92.09% for SMR, 91.74% for EMR, and 88.9% for IMR, outperforming the mean and median F1 scores in the test leaderboard across all subtasks. These results demonstrate the effectiveness of BERT-based models in clinical NER tasks across multiple languages, particularly for medications recognition, while highlighting the need for further improvements in disease recognition.

## Method Summary
The study employed BERT-based models fine-tuned on general domain text for clinical NER tasks. The approach used a two-stage fine-tuning process: first adapting BERT models on DisTEMIST/DrugTEMIST corpora, then further fine-tuning on CardioCCC development data. The BIO tagging schema was used to frame NER as token classification. Four monolingual models (Spanish Diseases, Spanish Medications, English Medications, Italian Medications) and two multilingual models were trained. The models processed clinical case reports up to 256 tokens, with preprocessing involving sentence splitting and word-level tokenization preserving character offsets. Fine-tuning used cross-entropy loss over BIO labels with hyperparameters including 10 epochs, batch size 8, and learning rate 9e-6.

## Key Results
- Medications recognition achieved strong performance: SMR 92.09%, EMR 91.74%, IMR 88.9% F1 scores
- Diseases recognition performed notably lower: SDR achieved 77.88% F1 score
- Multilingual models showed modest improvements: 0.44% increase for SMR, 0.28% for EMR
- Spanish Diseases Recognition showed severe overfitting: 18.35% gap between development (96.23%) and test (77.88%) performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: General-domain BERT models can be adapted to clinical NER through fine-tuning on domain-specific data.
- Mechan: Pre-trained BERT models capture general linguistic patterns (syntax, context, subword relationships). Fine-tuning on annotated clinical texts adjusts the token classification head while preserving learned contextual embeddings, enabling recognition of domain-specific entities like "myocardial infarction" or "aspirin" that follow similar syntactic patterns to general entities.
- Core assumption: The syntactic and contextual patterns in clinical texts share sufficient similarity with general text for transfer learning to be effective.
- Evidence anchors: [abstract] "We explore the effectiveness of different monolingual and multilingual BERT-based models, trained on general domain text, for extracting disease and medication mentions from clinical case reports"; [section 3.2] "We experimented with the following BERT-based models, specifically trained to perform NER... and further fine-tuned them for the biomedical domain using the MultiCardioNER dataset"; [corpus] Related work (OpenMed NER) confirms domain-adapted transformers achieve state-of-the-art performance across diverse biomedical entity types.

### Mechanism 2
- Claim: BIO tagging schema enables NER to be framed as multi-label token classification.
- Mechan: The Beginning-Inside-Outside (BIO) format converts span-level entity recognition into per-token classification. B-ENFERMEDAD marks the first token of a disease, I-ENFERMEDAD marks continuations, and O marks non-entity tokens. This simplifies training by using standard classification loss rather than complex span prediction.
- Core assumption: Entity boundaries align cleanly with word-level tokens produced by the tokenizer.
- Evidence anchors: [section 3.2] "All these BERT-based models utilize the standard Beginning-Inside-Outside (BIO) format for tagging entities. This format is crucial as it allows NER to be approached as a multi-label classification task"; [section 3.2.1] "Following the standard BIO format, we defined our label list as follows: B-ENFERMEDAD, I-ENFERMEDAD, O, [CLS], and [SEP]"; [corpus] No direct corpus evidence on BIO-specific limitations in clinical NER; weak support.

### Mechanism 3
- Claim: Multilingual models benefit from cross-lingual transfer when training data is limited per language.
- Mechan: Multilingual BERT shares parameters across languages, allowing the model to leverage patterns learned from higher-resource languages (English) to improve performance on lower-resource languages (Spanish, Italian). Medication names often share Latin/Greek roots across languages, facilitating transfer.
- Core assumption: Clinical entity patterns share cross-lingual regularities that a shared model can exploit.
- Evidence anchors: [section 4] "employing a multilingual model proves beneficial in certain substasks, such as Spanish Medications Recognition (SMR) and English Medications Recognition (EMR), resulting in an improved F1-score from 91.65% to 92.09%, and from 91.46% to 91.74%"; [section 3.2.2] "the multilingual model was trained on an aggregated dataset encompassing all three languages, but separately evaluated for each language"; [corpus] FewTopNER framework (related work) confirms language-specific calibration with shared multilingual encoders improves low-resource NER.

## Foundational Learning

- Concept: **Transfer Learning in Transformers**
  - Why needed here: The entire approach relies on fine-tuning pre-trained BERT models. Understanding how gradient updates affect the classification head versus the encoder is critical for debugging performance gaps.
  - Quick check question: If fine-tuning on a small dataset causes overfitting, which layers would you freeze first?

- Concept: **BIO Tagging Schema**
  - Why needed here: The paper treats NER as token classification using BIO labels. Understanding B-/I-/O transitions is necessary to interpret model outputs and post-process predictions.
  - Quick check question: Given the sequence "B-FARMACO I-FARMACO O B-FARMACO," how many distinct medication entities are predicted?

- Concept: **Micro-averaged F1 for Entity Recognition**
  - Why needed here: The evaluation uses micro-averaged precision, recall, and F1 based on exact span matches. This differs from macro-averaging and from token-level metrics.
  - Quick check question: If a model correctly identifies "aspirin" but predicts the span as tokens [5,6] when ground truth is [5,7], does this count as a true positive?

## Architecture Onboarding

- Component map:
  - Preprocessing: Sentence splitting (max 256 tokens) → Word-level tokenization with offset preservation → BIO label encoding
  - Model backbone: BERT-based encoder (monolingual or multilingual) with token classification head
  - Training: Fine-tuning on MultiCardioNER with cross-entropy loss over BIO labels
  - Postprocessing: BIO output → BRAT format conversion (character-level spans)

- Critical path:
  1. Tokenizer alignment: Ensure subword tokenization preserves character offsets for BRAT output. Misalignment here cascades to evaluation failures.
  2. Label consistency: Verify BIO label lists match between training and inference (B-ENFERMEDAD vs B-DISEASE).
  3. Data split validation: Development vs. test F1 gaps indicate overfitting (observed: 18.35% gap for Spanish Diseases).

- Design tradeoffs:
  - Monolingual vs. Multilingual: Monolingual models may capture language-specific patterns better; multilingual models enable cross-lingual transfer but may dilute language-specific signals.
  - Fine-tuning depth: Training on development set improves test F1 (e.g., Cardio-SMR: 88.52% → 91.65%) but risks overfitting on small cardiology sets (258 dev samples).
  - Sequence length: 256-token limit handles most clinical sentences but may truncate long reports.

- Failure signatures:
  - Large dev-test F1 gap (>15%): Overfitting to limited domain data (observed in Spanish Diseases: 96.23% dev vs. 77.88% test).
  - Incomplete entity predictions: Model predicts "cardiac" but misses "cardiac arrest" (visible in Figure 2).
  - Span misalignment: BRAT output offsets don't match original text due to tokenizer mismatch.

- First 3 experiments:
  1. Baseline monolingual fine-tuning: Train bert-base-NER on DrugTEMIST-English, evaluate on CardioCCC-English. Measure F1 gap between dev and test.
  2. Multilingual aggregation: Train bert-base-multilingual-cased-ner-hrl on concatenated DrugTEMIST (ES/EN/IT). Compare per-language F1 to monolingual baselines.
  3. Overfitting diagnosis: For Spanish Diseases (high dev-test gap), run ablation with frozen encoder layers (freeze all but last 2 transformer layers) to assess whether reducing model capacity narrows the gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can recent Large Language Models (LLMs) effectively resolve the issues of incomplete or incorrect predictions currently observed in BERT-based models for Spanish Disease Recognition?
- Basis in paper: [explicit] The authors explicitly state in the Conclusion: "To address these issues, we aim to explore the capabilities of recent large language models (LLMs)."
- Why unresolved: The current study utilized fine-tuned BERT-based models, which, while successful overall, struggled specifically with the Spanish Diseases Recognition (SDR) subtask, occasionally producing incomplete entity spans.
- What evidence would resolve it: A comparative evaluation of generative LLMs (e.g., GPT-4, Med-PaLM) against the current Cardio-SDR model on the MultiCardioNER test set, specifically analyzing the reduction in incomplete entity predictions.

### Open Question 2
- Question: Is the severe overfitting observed in Spanish Diseases Recognition (SDR) models caused primarily by excessive model complexity relative to the limited diversity of cardiology-specific entities?
- Basis in paper: [inferred] The authors note a large performance gap between development and test sets for SDR (~18%) and hypothesize in the Results section: "the model is too complex for the limited diversity of cardiology-specific entities present in the development set."
- Why unresolved: The paper identifies the overfitting and offers a plausible explanation regarding model complexity versus data diversity, but does not experimentally validate this hypothesis.
- What evidence would resolve it: Ablation studies using smaller model architectures or data augmentation techniques to increase entity diversity, followed by an analysis of the resulting reduction in the dev-test performance gap.

### Open Question 3
- Question: Does the reliance on machine-translated training data for English and Italian medication recognition limit the models' ability to generalize to natively written clinical texts?
- Basis in paper: [inferred] Section 3.1 notes that the multilingual DrugTEMIST corpus was "transferred into English and Italian using machine translation and lexical annotation projection."
- Why unresolved: While the models achieved high F1-scores, the study assumes that training on machine-translated text is sufficient for clinical NER without evaluating if translation artifacts negatively impact the recognition of natural clinical language nuances.
- What evidence would resolve it: Evaluation of the fine-tuned English and Italian models on a held-out dataset of natively written clinical case reports (rather than translated ones) to assess any performance degradation.

## Limitations

- **Entity type generalization gap**: Models perform significantly better on medications (88.9-92.09% F1) than diseases (77.88% F1), suggesting limited cross-entity generalization.
- **Development-test overfitting**: Spanish Diseases Recognition shows a concerning 18.35% F1 gap between development and test sets, indicating overfitting to limited training data.
- **Machine translation uncertainty**: English and Italian training data were machine-translated from Spanish without quantified validation, raising questions about translation quality affecting model performance.

## Confidence

- **High Confidence (9/10)**: Medications recognition results are robust with consistent F1 scores above 88% across all three languages and measurable improvements from multilingual approach.
- **Medium Confidence (6/10)**: Diseases recognition results show concerning overfitting with 77.88% F1 score and 18.35% dev-test gap, making generalization uncertain.
- **Low Confidence (4/10)**: Cross-lingual transfer benefits are difficult to quantify precisely with only modest gains (1-2 percentage points) and lack of detailed analysis.

## Next Checks

1. **Overfitting Diagnosis**: Re-run the Spanish Diseases Recognition training with frozen encoder layers (freeze all but last 2 transformer layers) to determine if reducing model capacity narrows the 18.35% development-test F1 gap. Monitor training curves to identify optimal early stopping points.

2. **Cross-lingual Transfer Analysis**: Conduct an ablation study where the multilingual model is trained on each language individually versus combined, then compare performance gains. Additionally, test the model's ability to recognize medication names in clinical texts from a fourth language (e.g., French) to assess genuine cross-lingual capabilities.

3. **Dataset Quality Validation**: Quantify the machine translation validation process by sampling and manually evaluating 100 translated medication and disease mentions from the English/Italian DrugTEMIST datasets. Compare against the original Spanish to measure translation accuracy and identify systematic errors affecting model training.