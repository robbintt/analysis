---
ver: rpa2
title: Thompson Sampling for Multi-Objective Linear Contextual Bandit
arxiv_id: '2512.00930'
source_url: https://arxiv.org/abs/2512.00930
tags:
- pareto
- regret
- round
- reward
- objective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the first Thompson Sampling algorithm with
  Pareto regret guarantees for multi-objective linear contextual bandits. The key
  idea is an optimistic sampling strategy that samples parameters for each objective
  and selects arms from an "effective Pareto front" that accounts for cumulative rewards
  under repeated selections.
---

# Thompson Sampling for Multi-Objective Linear Contextual Bandit

## Quick Facts
- arXiv ID: 2512.00930
- Source URL: https://arxiv.org/abs/2512.00930
- Authors: Somangchan Park; Heesang Ann; Min-hwan Oh
- Reference count: 40
- Primary result: First Thompson Sampling algorithm with Pareto regret guarantees for multi-objective linear contextual bandits, achieving Õ(d^{3/2}√T) worst-case bound

## Executive Summary
This paper introduces MOL-TS, the first Thompson Sampling algorithm with provable Pareto regret guarantees for multi-objective linear contextual bandits. The key innovation is an optimistic sampling strategy that combines multiple parameter samples per objective with selection from an "effective Pareto front" that accounts for cumulative rewards under repeated selections. The algorithm achieves a worst-case Pareto regret bound of Õ(d^{3/2}√T), matching the best known order for randomized linear bandit algorithms for single objectives. Empirical results demonstrate improved regret minimization and strong multi-objective performance compared to UCB and ε-greedy baselines.

## Method Summary
The MOL-TS algorithm uses regularized least squares (RLS) estimation to track the unknown parameter θ̂^{(ℓ)}_t for each objective ℓ. For each round, it samples M parameters per objective from Gaussian distributions centered at the RLS estimates with covariance c²V_t^{-1}, where V_t is the regularized feature covariance matrix. The algorithm then computes optimistic estimates μ̃^{(ℓ)}_{t,a} as the maximum over M samples for each arm-objective pair. Arms are selected uniformly from the empirical effective Pareto front C̃_t, which contains arms not dominated by any convex combination of others under the optimistic estimates. The algorithm requires M = ⌈-log L / log(1-p)⌉ samples per objective with p = 0.15 to ensure sufficient optimism probability.

## Key Results
- Achieves worst-case Pareto regret bound of Õ(d^{3/2}√T), matching best known randomized linear bandit algorithms
- Outperforms MOL-UCB and MOL-ε-Greedy baselines in both Pareto regret and effective Pareto regret metrics
- Demonstrates superior multi-objective performance with cumulative rewards closer to the Pareto-optimal frontier
- Maintains strong performance across different numbers of objectives (L = 2 to 4) and arms (K = 50 to 200)

## Why This Works (Mechanism)
The algorithm's success stems from combining Thompson Sampling's exploration-exploitation balance with optimistic sampling to ensure sufficient exploration of the Pareto front. By sampling multiple parameters per objective and taking maxima, the algorithm creates an optimistic view that encourages exploration of potentially optimal arms. The effective Pareto front selection mechanism ensures that chosen arms remain competitive across all objectives when considering cumulative rewards,