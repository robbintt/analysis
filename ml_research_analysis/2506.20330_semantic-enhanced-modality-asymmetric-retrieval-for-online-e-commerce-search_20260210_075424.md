---
ver: rpa2
title: Semantic-enhanced Modality-asymmetric Retrieval for Online E-commerce Search
arxiv_id: '2506.20330'
source_url: https://arxiv.org/abs/2506.20330
tags:
- retrieval
- item
- which
- image
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving semantic retrieval
  in e-commerce search by leveraging multimodal data, specifically incorporating image
  information alongside text. The authors propose a novel model, SMAR (Semantic-enhanced
  Modality-Asymmetric Retrieval), to tackle the unique problem of asymmetric modality
  retrieval, where queries are unimodal (text) but items are multimodal (text + image).
---

# Semantic-enhanced Modality-asymmetric Retrieval for Online E-commerce Search

## Quick Facts
- arXiv ID: 2506.20330
- Source URL: https://arxiv.org/abs/2506.20330
- Reference count: 30
- One-line primary result: Proposed SMAR model achieves 4.90% improvement in recall and 1.50% in precision over baseline models in e-commerce semantic retrieval.

## Executive Summary
This paper addresses the challenge of improving semantic retrieval in e-commerce search by leveraging multimodal data, specifically incorporating image information alongside text. The authors propose a novel model, SMAR (Semantic-enhanced Modality-Asymmetric Retrieval), to tackle the unique problem of asymmetric modality retrieval, where queries are unimodal (text) but items are multimodal (text + image). SMAR employs a two-stage training strategy: a pre-training stage using multi-task learning to address modality fusion, alignment, and contributions, and a fine-tuning stage with adaptive embedding learning to dynamically determine the importance of image modality for different product categories. Extensive offline experiments on a large industrial dataset demonstrate significant improvements over baseline models, with a 4.90% improvement in recall and 1.50% in precision. Online A/B tests further validate the model's effectiveness, showing notable improvements in gross merchandise value (GMV) and user conversation rate (UCVR), especially for fashion categories. The authors also open-sourced their industrial dataset to facilitate reproducibility and future research.

## Method Summary
SMAR employs a two-stage training strategy to improve semantic retrieval in e-commerce search. The first stage is a multi-task pre-training phase that jointly optimizes three objectives: semantic projection (text-to-text), modality alignment (text-to-image), and asymmetric alignment (text-to-multimodal). This pre-training uses synthetic queries generated to be more than 100 times the number of true queries, along with batch negatives. The second stage is a fine-tuning phase with adaptive embedding learning, where a prediction header P(q) learns to dynamically decide whether to use multimodal or text-only item embeddings based on query characteristics. The model architecture consists of a query tower (text embedding), an item text tower, an item image tower, and an item multimodal tower with cross-attention. The loss function is a weighted combination of the three pre-training objectives (L = αL_t + βL_i + γL_m), and the fine-tuning loss is adjusted based on the prediction header output.

## Key Results
- SMAR achieves a 4.90% improvement in recall and 1.50% in precision over baseline models in offline experiments.
- Online A/B tests show significant improvements in gross merchandise value (GMV) and user conversation rate (UCVR), especially for fashion categories (+1.112% GMV vs +0.285% overall).
- Ablation studies confirm the importance of cross-attention fusion, modality alignment, and adaptive gating mechanisms.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-attention fusion captures supplementary visual signals without overwhelming text semantics.
- Mechanism: The Item Multimodal Tower uses cross-attention where text embeddings guide visual feature selection, allowing images to fill attribute gaps (e.g., color not mentioned in title) while suppressing irrelevant visual noise.
- Core assumption: Image features contain recoverable product attributes that text lacks or obscures.
- Evidence anchors:
  - [abstract] "visual information (e.g, image) of item is leveraged as supplementary of textual information"
  - [section 3.2.1] SMAR vs SMAR-nc ablation shows cross-attention removal degrades all metrics
  - [corpus] Neighbor paper "Factorized Transport Alignment" confirms multi-view image signals improve e-commerce representations, but does not validate cross-attention specifically
- Break condition: When product images are generic, stock photos, or visually ambiguous (e.g., electronics with similar appearances), cross-attention may introduce noise rather than signal.

### Mechanism 2
- Claim: Multi-task pre-training creates shared embedding space for asymmetric query-item alignment.
- Mechanism: Three parallel tasks—text-to-text (semantic projection), text-to-image (modality alignment), and text-to-multimodal (asymmetric alignment)—jointly optimize query and item towers, enforcing that query embeddings can match items via any modality path.
- Core assumption: Synthetic queries from the 100× augmentation strategy provide sufficient coverage of real query distribution.
- Evidence anchors:
  - [section 2.3.2] "the number of synthetic queries is more than 100 times of true queries"
  - [section 3.2.1] SMAR outperforms DPSR by 4.90% R@50; ablations (SMAR-ni, SMAR-nt, SMAR-nm) each degrade performance
  - [corpus] Weak direct evidence—neighbor papers focus on dense retrieval but not multi-task asymmetric pre-training
- Break condition: If synthetic queries drift from real user intent (e.g., niche or ambiguous searches), pre-training may optimize for incorrect alignment patterns.

### Mechanism 3
- Claim: Adaptive modality gating prevents image noise injection for low-visual-relevance categories.
- Mechanism: A prediction header P(q) learns to route queries: if P=1, use multimodal item embedding; if P=0, fall back to text-only. This is supervised by a fashion-category dataset, enabling category-aware routing.
- Core assumption: Visual utility is category-correlated (high for fashion, low for books/medicine).
- Evidence anchors:
  - [section 2.4.1] "When P(q)=1, we introduce image modality... otherwise, learn item text embedding"
  - [section 3.4] Fashion categories show +1.112% GMV vs +0.285% overall, indicating disproportionate visual benefit
  - [section 3.3] Weight sensitivity shows α and γ (text and multimodal) correlate; β (image-only) has distinct trend
  - [corpus] No direct corpus validation for adaptive gating; neighbor papers assume fixed multimodal fusion
- Break condition: If prediction header misclassifies (e.g., fashion queries with P=0), visual benefits are lost; conversely, non-fashion with P=1 may introduce noise.

## Foundational Learning

- Concept: Two-tower dense retrieval architecture
  - Why needed here: SMAR builds on the standard query/item dual-encoder pattern; understanding embedding separation and late fusion is prerequisite.
  - Quick check question: Can you explain why two-tower models enable efficient approximate nearest neighbor search at scale?

- Concept: Transformer cross-attention (Q, K, V formulation)
  - Why needed here: The Item Multimodal Tower uses cross-attention with H_t (text) as queries and H_i (image) as keys/values.
  - Quick check question: In cross-attention, what happens if the key and value matrices come from a different modality than the query?

- Concept: Multi-task learning with weighted loss
  - Why needed here: Pre-training combines three losses (L_t, L_i, L_m) with hyperparameters α, β, γ that require tuning per dataset.
  - Quick check question: If one task's gradient dominates, what happens to the shared representation?

## Architecture Onboarding

- Component map:
  - Query Tower: Text embedding → 4-layer transformer (12 heads) → L2 norm → Q(q)
  - Item Text Tower: Title/category/brand → 1-layer transformer → S_t(s)
  - Item Image Tower: Patch embeddings → transformer → S_i(s)
  - Item Multimodal Tower: Cross-attention with H_t as Q, H_i as K and V → S_m(s)
  - Prediction Header: Binary classifier (fashion vs non-fashion routing)

- Critical path:
  1. Pre-training: Multi-task loss on synthetic + click data → initialize all towers
  2. Fine-tuning: Click logs + adaptive gating → freeze or fine-tune towers with P(q) routing
  3. Inference: Query → Q(q); Item → conditional S_m(s) or S_t(s) based on P(q); dot-product scoring

- Design tradeoffs:
  - Cross-attention vs early concatenation: Cross-attention is more expressive but adds compute; ablation shows it's necessary for fusion quality.
  - Fixed vs adaptive modality: Adaptive adds a prediction step but prevents noise injection for non-visual categories.
  - Synthetic query ratio (100×): Higher coverage vs potential distribution drift; not ablated in paper.

- Failure signatures:
  - Recall improves but precision drops: Check if image modality introduces false positives in non-fashion categories.
  - Fashion gains absent: Verify prediction header accuracy; P(q) may be misclassifying.
  - Training instability: Check α/β/γ balance; Fig. 3 shows sensitivity (optimal varies by dataset).

- First 3 experiments:
  1. Reproduce DPSR baseline on open-sourced dataset, then add SMAR-ni variant to isolate modality alignment contribution.
  2. Ablate cross-attention (SMAR-nc) to confirm fusion mechanism on your own product catalog.
  3. Tune α/β/γ on a held-out validation split following Fig. 3 protocol; compare fixed weights vs category-specific weights.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the adaptive embedding mechanism be refined to make instance-level decisions rather than relying on binary category-level classification (fashion vs. non-fashion)?
- Basis in paper: [inferred] Section 2.4.1 describes the fine-tuning model using a prediction header $P$ supervised by a "fashion dataset," effectively creating a hardcoded switch for visual features based on category rather than the specific utility of the image to the query.
- Why unresolved: The current design forces the inclusion of image features for entire categories, which may introduce noise for items within fashion categories where the image is uninformative or for non-fashion items where visual attributes are actually critical.
- What evidence would resolve it: A modification of the gating mechanism to be a continuous function of query-image relevance, demonstrating improved Recall@k over the binary category-based method.

### Open Question 2
- Question: How can the modality contribution weights ($\alpha, \beta, \gamma$) be learned dynamically per query-item pair rather than tuned as static global hyperparameters?
- Basis in paper: [inferred] Section 3.3 (Modality Contributions) analyzes the sensitivity of these weights and notes that text and image contribute differently, suggesting that fixed weights might be suboptimal for the heterogeneous query distribution found in e-commerce.
- Why unresolved: The paper establishes the importance of weighting but relies on manual tuning (grid search) for specific datasets (Overall, Fashion, Not-fashion), leaving the automatic, context-aware adjustment of these weights unexplored.
- What evidence would resolve it: A comparative analysis showing that a learned, dynamic weighting strategy (e.g., using attention scores) outperforms the best statically tuned hyperparameters on the same test set.

### Open Question 3
- Question: Does the introduction of video modality into the item representation yield significant retrieval gains over the image-text fusion proposed in SMAR?
- Basis in paper: [inferred] The Introduction explicitly lists "images and videos" as supplementary resources available in e-commerce systems, but the proposed methodology and experiments restrict the multimodal tower to static images only.
- Why unresolved: While video is mentioned as a resource, the technical challenge of fusing temporal visual data with static text in the "item multimodal tower" (Section 2.2) remains unaddressed by the current architecture.
- What evidence would resolve it: Experimental results extending the $S_m(s)$ tower to accept video embeddings, demonstrating whether temporal features provide statistically significant improvements over static image features for relevant categories.

## Limitations
- The synthetic query generation process (referenced as [14] Qiu et al. 2022) is critical for the 100× augmentation but not fully specified, creating a potential reproducibility gap.
- The exact hyperparameter values for α, β, γ weighting in the multi-task loss are not provided despite sensitivity analysis being shown.
- The prediction header P's architecture and training procedure for adaptive gating are not detailed, particularly how the fashion vocabulary is constructed and labeled.
- The cross-attention mechanism's effectiveness depends on high-quality, informative product images—the paper does not address failure modes when images are generic or visually ambiguous (e.g., electronics).

## Confidence
- **High Confidence:** The two-stage training framework with multi-task pre-training and adaptive fine-tuning is well-articulated and supported by offline ablation results (SMAR vs SMAR-nc, SMAR-ni, etc.).
- **Medium Confidence:** Category-aware adaptive gating is plausible given the fashion category GMV improvement, but the prediction header's accuracy and generalization to other categories remain unverified.
- **Medium Confidence:** Cross-attention fusion captures supplementary visual signals, but the paper lacks ablation studies isolating cross-attention from other modality fusion components.

## Next Checks
1. Reproduce SMAR-nc (no cross-attention) ablation on the open-sourced dataset to confirm that modality fusion degrades without explicit attention mechanisms.
2. Conduct category-specific offline evaluation beyond fashion/not-fashion split to validate adaptive gating generalization.
3. Test synthetic query distribution drift by comparing SMAR performance on real vs. synthetic query subsets.