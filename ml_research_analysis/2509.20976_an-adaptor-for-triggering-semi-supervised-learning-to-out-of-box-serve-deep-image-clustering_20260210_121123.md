---
ver: rpa2
title: An Adaptor for Triggering Semi-Supervised Learning to Out-of-Box Serve Deep
  Image Clustering
arxiv_id: '2509.20976'
source_url: https://arxiv.org/abs/2509.20976
tags:
- clustering
- data
- learning
- image
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ASD, an adaptor that enables semi-supervised
  learning (SSL) methods to perform deep image clustering without requiring any pretraining
  or clustering models as prerequisites. The key idea is to sample pseudo-labeled
  data, train an instance-level classifier to provide discriminative features, track
  class transitions to extract semantic similarities, and map these to stable cluster-level
  labels for SSL training.
---

# An Adaptor for Triggering Semi-Supervised Learning to Out-of-Box Serve Deep Image Clustering

## Quick Facts
- arXiv ID: 2509.20976
- Source URL: https://arxiv.org/abs/2509.20976
- Authors: Yue Duan; Lei Qi; Yinghuan Shi; Yang Gao
- Reference count: 40
- This paper introduces ASD, an adaptor that enables semi-supervised learning (SSL) methods to perform deep image clustering without requiring any pretraining or clustering models as prerequisites.

## Executive Summary
This paper introduces ASD, an adaptor that enables semi-supervised learning (SSL) methods to perform deep image clustering without requiring any pretraining or clustering models as prerequisites. The key idea is to sample pseudo-labeled data, train an instance-level classifier to provide discriminative features, track class transitions to extract semantic similarities, and map these to stable cluster-level labels for SSL training. ASD addresses the cold-start problem by allowing SSL learners to be directly applied to clustering tasks. Experiments across five benchmark datasets (CIFAR-10, CIFAR-100, STL-10, ImageNet-10/-Dogs) show that ASD achieves superior performance compared to state-of-the-art deep clustering methods, with only a 1.33% accuracy gap versus SSL methods using ground-truth labels on CIFAR-10. ASD is also compatible with existing SSL-embedded deep clustering frameworks, further boosting their performance.

## Method Summary
ASD enables SSL methods to perform deep image clustering through a novel adaptor framework. The method samples pseudo-labeled instances, trains an instance-level classifier for discriminative features, tracks prediction transitions to extract semantic similarities, and maps these to cluster-level labels for SSL training. This approach eliminates the need for pretraining or separate clustering models, addressing the cold-start problem in deep clustering. The framework uses Optimal Transport to align instance labels across iterations and k-Medoids clustering on transition matrices to establish stable cluster assignments.

## Key Results
- ASD achieves superior performance compared to state-of-the-art deep clustering methods on CIFAR-10, CIFAR-100, STL-10, ImageNet-10, and ImageNet-Dogs
- ASD shows only a 1.33% accuracy gap versus SSL methods using ground-truth labels on CIFAR-10
- ASD is compatible with existing SSL-embedded deep clustering frameworks, further boosting their performance

## Why This Works (Mechanism)
ASD works by creating a bridge between instance-level supervision and cluster-level learning. By sampling pseudo-labeled data and training an instance-level classifier, ASD provides discriminative features that capture semantic similarities between samples. The tracking of class transitions over time allows the system to identify stable cluster structures without requiring ground-truth labels. The Optimal Transport alignment ensures consistent instance labeling across training iterations, while k-Medoids clustering on transition matrices maps instance-level semantics to cluster-level assignments. This approach enables SSL learners to operate effectively on clustering tasks by providing stable pseudo-supervision derived from the data's inherent structure.

## Foundational Learning
- **Optimal Transport (Sinkhorn-Knopp)**: Used to align instance labels across iterations by computing the cost-minimizing transport plan between feature distributions. Why needed: Ensures consistent instance labeling when the model's feature space evolves during training. Quick check: Verify transport plan stability across consecutive iterations on a small dataset.
- **k-Medoids Clustering**: Applied to transition matrices to map instance-level labels to cluster-level assignments. Why needed: Extracts stable cluster structure from accumulated prediction transitions without requiring true labels. Quick check: Confirm cluster assignments remain stable when increasing transition accumulation iterations.
- **Instance-level Pseudo-labeling**: Assigns unique labels to sampled pseudo-labeled instances for training discriminative features. Why needed: Provides fine-grained supervision that captures semantic similarities for later clustering. Quick check: Monitor instance classifier accuracy on held-out pseudo-labeled samples.
- **Class Transition Tracking (CTT)**: Accumulates prediction transitions over multiple batches to build a similarity matrix. Why needed: Captures temporal stability of semantic relationships between samples. Quick check: Verify transition matrix reflects expected semantic groupings in simple datasets.
- **Prototype-based Sampling (PS)**: Selects pseudo-labeled samples based on feature prototypes and neighbors. Why needed: Ensures initial semantic coverage across different classes. Quick check: Compare cluster quality with random vs. prototype-based initial sampling.
- **Two-headed Architecture**: Separate instance-level and cluster-level classifiers share backbone features. Why needed: Decouples fine-grained instance discrimination from coarse-grained clustering tasks. Quick check: Monitor training dynamics of both heads independently.

## Architecture Onboarding

**Component Map**: Input Images -> Backbone ResNet-18 -> Instance Classifier (nl outputs) & Cluster Classifier (k outputs) -> Optimal Transport Alignment -> Class Transition Tracking -> k-Medoids Mapping -> SSL Loss

**Critical Path**: Image input → Backbone feature extraction → Instance classifier training → Optimal Transport alignment → CTT accumulation → k-Medoids mapping → Cluster classifier SSL training

**Design Tradeoffs**: Fixed vs. adaptive pseudo-label sample count (nl=4k) balances coverage vs. noise; separate instance/cluster heads enable specialized learning but increase parameters; CTT accumulation (Nb=1000) trades memory for stability

**Failure Signatures**: Mode collapse (all samples assigned to single cluster); semantic drift (instance labels lose consistency across iterations); poor cluster separation (low NMI/ARI scores despite reasonable ACC)

**Three First Experiments**:
1. Verify Optimal Transport alignment by testing instance label consistency across consecutive iterations on a synthetic dataset with known structure
2. Implement CTT module and test transition matrix quality by running on a simple dataset where semantic relationships are known
3. Compare random sampling vs. prototype-based sampling impact on initial cluster quality using k-means on backbone features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the performance gap between ASD and fully supervised SSL methods be further narrowed by developing mechanisms to strictly guarantee the correctness of pseudo-labels as training converges?
- Basis in paper: [explicit] The Conclusion states that narrowing the gap with baseline SSL learners (e.g., the 1.33% gap on CIFAR-10) is a future goal.
- Why unresolved: The current method relies on self-constructed dynamic supervision which inherently contains noise, capping performance below the theoretical maximum of the SSL learner using ground truth.
- What evidence would resolve it: Empirical results where ASD matches the accuracy of the base SSL learner (e.g., FreeMatch) using ground-truth labels, or theoretical proof that the noise rate of assigned labels approaches zero.

### Open Question 2
- Question: How can the sampling strategy be improved to guarantee semantic coverage in fine-grained or long-tailed datasets where prototype-based sampling tends to collapse similar classes?
- Basis in paper: [explicit] Appendix B explicitly identifies failure cases on ImageNet-Dogs due to visual similarity and suggests exploring "long-tailed-aware reweighting" or "dynamic sampling adjustments" in future work.
- Why unresolved: The current Prototypes accompanied by Neighbors (PS) strategy relies on feature distances that may not distinguish fine-grained semantics effectively without prior knowledge.
- What evidence would resolve it: An adaptive sampling method that maintains high accuracy on datasets with high intra-class similarity or severe class imbalance (e.g., ImageNet-LT).

### Open Question 3
- Question: Is there a dynamic strategy for determining the optimal number of pseudo-labeled samples (nl) per iteration to automatically balance the trade-off between class coverage and noise introduction?
- Basis in paper: [inferred] Section IV-C1 analyzes the trade-off for nl, noting that while small values miss classes, large values introduce noise that damages performance, yet the method relies on a fixed heuristic (nl=4k).
- Why unresolved: The optimal number of samples likely shifts as the model's discriminative capability improves, but the current implementation uses a static value.
- What evidence would resolve it: An adaptive algorithm that adjusts nl during training and outperforms the fixed constant baselines on diverse benchmarks.

## Limitations
- Exact optimizer hyperparameters and learning rate schedules are not explicitly specified, requiring additional hyperparameter tuning for faithful reproduction
- Architectural details of the instance-level classifier implementation and feature dimensions for Optimal Transport are not fully clarified
- Direct comparison to specific SSL methods with ground-truth labels cannot be independently verified without exact implementation details

## Confidence
- **High Confidence**: The core conceptual framework of using instance-level pseudo-labels with semantic alignment for clustering is well-articulated and logically sound. The reported performance improvements over existing deep clustering methods are statistically significant and consistent across multiple benchmarks.
- **Medium Confidence**: The experimental methodology and ablation studies are comprehensive, but exact reproduction requires additional hyperparameter tuning that may affect absolute performance numbers.
- **Low Confidence**: Direct comparison to specific SSL methods with ground-truth labels (reporting only 1.33% accuracy gap on CIFAR-10) cannot be independently verified without access to the exact implementation details and hyperparameters.

## Next Checks
1. Verify the Sinkhorn-Knopp implementation for Optimal Transport by testing on a small synthetic dataset to ensure instance label consistency across iterations.
2. Implement the k-Medoids clustering on the transition matrix with varying Nb and Nt values to confirm stability of cluster assignments.
3. Conduct controlled experiments comparing random sampling vs. prototype-based sampling (PS) to validate the reported impact on clustering performance.