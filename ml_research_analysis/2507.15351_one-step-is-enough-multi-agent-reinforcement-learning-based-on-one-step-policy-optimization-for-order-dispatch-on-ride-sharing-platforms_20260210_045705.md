---
ver: rpa2
title: 'One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy
  Optimization for Order Dispatch on Ride-Sharing Platforms'
arxiv_id: '2507.15351'
source_url: https://arxiv.org/abs/2507.15351
tags:
- grpo
- order
- time
- policy
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the order dispatch problem in ride-sharing\
  \ platforms using Autonomous Vehicles (AVs), where the goal is to efficiently assign\
  \ incoming orders to a fleet of homogeneous AVs in real-time. The authors propose\
  \ two Multi-Agent Reinforcement Learning (MARL) methods\u2014GRPO (Group Relative\
  \ Policy Optimization) and OSPO (One-Step Policy Optimization)\u2014that bypass\
  \ value function estimation, leveraging the homogeneous property of AV fleets."
---

# One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms

## Quick Facts
- **arXiv ID**: 2507.15351
- **Source URL**: https://arxiv.org/abs/2507.15351
- **Reference count**: 31
- **Primary result**: Two MARL methods (GRPO, OSPO) outperform existing methods on real-world ride-hailing data, with OSPO achieving lowest GPU utilization while maintaining high efficiency.

## Executive Summary
This paper addresses the order dispatch problem in ride-sharing platforms using homogeneous Autonomous Vehicles (AVs). The authors propose two Multi-Agent Reinforcement Learning methods—Group Relative Policy Optimization (GRPO) and One-Step Policy Optimization (OSPO)—that eliminate the need for value function estimation by leveraging the homogeneous property of AV fleets. GRPO replaces the V-value baseline in PPO with group average reward-to-go, while OSPO further simplifies to one-step rewards under strict homogeneity assumptions. Experiments on Manhattan ride-hailing data show both methods outperform baselines, with OSPO achieving the best efficiency and eliminating bounded horizon bias.

## Method Summary
The paper proposes two MARL methods for order dispatch in homogeneous AV fleets. GRPO modifies PPO by replacing the V-value baseline with the group average reward-to-go, eliminating critic estimation errors. OSPO further simplifies this to one-step rewards by demonstrating that under strict homogeneity assumptions, the advantage function reduces to a normalized single-step reward. Both methods use shared policy networks across agents, bipartite matching for assignment, and avoid centralized critics that suffer from curse of dimensionality. The policy is trained using PPO-style clipped objectives with KL regularization against the best checkpoint.

## Key Results
- GRPO and OSPO outperform existing methods in optimizing pickup times and number of served orders on real-world Manhattan ride-hailing data
- OSPO achieves the lowest GPU utilization (~3.8GB) among all methods while maintaining or improving performance
- OSPO outperforms GRPO due to elimination of bounded time horizon bias in reward-to-go computation
- Simple MLP architecture (20K params) outperforms more complex GAT-based methods (117K params), demonstrating efficiency of homogeneity-based approaches

## Why This Works (Mechanism)

### Mechanism 1: Group Average Reward-to-Go as V-value Replacement
Under homogeneous AV fleets, the group average reward-to-go converges to an approximation of the V-value with negligible estimation error. This allows replacing the Qπ(s,a) - Vπ(s) advantage with [Στ∈Kt γτ R(si,t+τ, ui,t+τ) - μt:] / σt:, eliminating critic estimation errors while maintaining valid advantage estimation.

### Mechanism 2: One-Step Reward Simplification
When agent homogeneity holds strictly, the advantage function reduces to a one-step normalized reward. The derivation shows Aπ(si,t, ui,t) = r(si,t, ui,t) - Ej[r(sj,t, uj,t)] + γ(εi,t+1 - εi,t), which under homogeneity and normalization becomes Ãπ(si,t, ui,t) = [r(si,t, ui,t) - μt] / σt - αΔδt, eliminating the need for multi-step reward accumulation.

### Mechanism 3: Bounded Horizon Bias Elimination
GRPO computes reward-to-go over finite episodes (30 minutes), causing systematic bias for agents receiving orders near episode end. OSPO's per-step normalization avoids this by computing advantages independently at each timestep rather than cumulatively, providing more stable training signals.

## Foundational Learning

- **Advantage Functions in Policy Gradient Methods**: Both GRPO and OSPO are fundamentally advantage-based methods; understanding A(s,a) = Q(s,a) - V(s) is essential to grasp why replacing V(s) with group averages works. Quick check: Can you explain why subtracting a baseline (V-value or group average) from the Q-value reduces variance without introducing bias in policy gradient estimation?

- **Bipartite Matching for Assignment**: The action selection mechanism uses bipartite matching to enforce mutual exclusivity—each order assigned to at most one agent, each agent receiving at most one order per timestep. Quick check: Given probabilities pij for agent i matching order j, how does the Hungarian algorithm or similar ensure globally optimal assignment under the mutual exclusivity constraints?

- **Parameter Sharing in Homogeneous MARL**: The paper's efficiency gains rely on all agents sharing a single policy network πθ; understanding why this is valid under homogeneity is critical. Quick check: Under what conditions does parameter sharing across agents preserve optimal policy learning, and when would it fail?

## Architecture Onboarding

- **Component map**: Observations {[si,t, sot]} → Policy Network → Probabilities {pi,t} → Bipartite Matching → Joint Action Ut → Environment → Rewards → Advantage Computation → Policy Gradient Update → Updated θ

- **Critical path**: 1) Observations to Policy Network → 2) Probabilities to Bipartite Matching → 3) Joint Action to Environment → 4) Rewards to Advantage Computation → 5) Advantages to Policy Gradient Update → 6) Periodic checkpoint update

- **Design tradeoffs**: GRPO provides richer temporal signal but introduces horizon bias; OSPO is simpler and more robust but may struggle with highly non-stationary reward distributions. MLP (20K params) vs. GAT (117K params) shows architectural simplicity benefits under homogeneity. KL reference policy using best checkpoint vs. initial policy trades stability against premature convergence.

- **Failure signatures**: Homogeneity collapse (Δδt grows), no convergence with OSPO (σt → 0), GRPO underperformance at episode boundaries, centralized critic failure due to curse of dimensionality.

- **First 3 experiments**: 1) Baseline reproduction: Train both GRPO and OSPO on Manhattan 19:00-19:30 training split with 1,000 vehicles; verify OSPO achieves lower GPU utilization and comparable/better reward. 2) Homogeneity validation: Plot cumulative reward distribution across agents at multiple timesteps; compute standard deviation δt and verify it remains small and stable. 3) Horizon bias ablation: Train GRPO with varying episode lengths (15, 30, 60 minutes); measure if performance gap between GRPO and OSPO decreases as horizon extends.

## Open Questions the Paper Calls Out
- **Can OSPO benefit human drivers?** The authors express interest in exploring whether their approach could benefit human drivers, though the core assumption of homogeneity may be violated by diverse driver behaviors and preferences.
- **Heterogeneous fleet adaptation?** The methods' performance on fleets with varying vehicle capacities or types remains unexplored, as the homogeneous property may not hold.
- **Complex spatial representations?** The paper uses a simple MLP and ignores order relationships; it's unclear if OSPO's one-step signal provides sufficient gradient information for training high-capacity networks needed for complex spatial dependencies.

## Limitations
- Homogeneity assumption may not hold in real-world scenarios with geographic clustering or capacity constraints
- Performance may degrade on non-stationary demand patterns or heterogeneous fleets
- Generalizability to regions with different traffic patterns or demand distributions remains untested

## Confidence
- **High confidence**: Group average reward-to-go effectively eliminates critic estimation errors; OSPO's one-step optimization avoids bounded horizon bias
- **Medium confidence**: Homogeneity assumption holds across real-world ride-sharing fleets
- **Low confidence**: Methods generalize to heterogeneous fleets or different reward structures

## Next Checks
1. **Heterogeneity stress test**: Train GRPO/OSPO on mixed fleet (varying capacities/reward functions) and measure performance degradation to quantify homogeneity assumption robustness
2. **Non-stationary demand validation**: Apply methods to datasets with sudden demand spikes/shifts and evaluate if OSPO's per-step normalization maintains advantage signal quality versus GRPO's cumulative approach
3. **Architectural complexity ablation**: Compare MLP (20K params) against GAT/transformers on heterogeneous fleets to determine if simplicity benefits are universal or specific to homogeneity