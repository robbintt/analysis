---
ver: rpa2
title: Personalized Author Obfuscation with Large Language Models
arxiv_id: '2505.12090'
source_url: https://arxiv.org/abs/2505.12090
tags:
- obfuscation
- user
- author
- successful
- personalized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores using large language models for author obfuscation\
  \ through paraphrasing, focusing on user-level performance variations. The authors\
  \ train authorship verification models and find that LLMs can reduce detection accuracy,\
  \ but effectiveness varies significantly across authors\u2014a bimodal distribution."
---

# Personalized Author Obfuscation with Large Language Models

## Quick Facts
- **arXiv ID**: 2505.12090
- **Source URL**: https://arxiv.org/abs/2505.12090
- **Reference count**: 10
- **Primary result**: LLM-based paraphrasing can reduce authorship verification accuracy, but effectiveness varies significantly across authors in a bimodal distribution

## Executive Summary
This paper investigates using large language models for authorship obfuscation through paraphrasing, addressing the critical challenge that obfuscation effectiveness varies dramatically across different authors. The authors find a bimodal distribution in obfuscation performance, where some authors achieve strong obfuscation while others see little improvement. To address this, they propose a personalized prompting approach that uses SHAP values to identify the most predictive stylistic features for each author, then generates targeted prompts to modify those features. This personalization significantly improves obfuscation performance and reduces the bimodal distribution, making it harder for verification models to attribute texts to their original authors. The study demonstrates that LLaMA-3.1 outperforms GPT-4 in obfuscation effectiveness, though this may come at the expense of text quality and semantic preservation.

## Method Summary
The study trains author verification models using Writeprints features with Logistic Regression and XGBoost, plus BERT-large-uncased embeddings. Three datasets are used: IMDb62 (10 users, ~1,350 reviews/user), Yelp (10 users, ~500 reviews/user), and Blog (5 users, 700-3,000 posts/user). The method involves three key steps: (1) training per-user binary verifiers, (2) zero-shot paraphrasing with GPT-4-turbo and LLaMA-3.1-8B-Instruct using generic prompts, and (3) computing SHAP values on validation sets to identify the most predictive feature per author, then generating personalized prompts targeting that feature's modification. The approach addresses the observed bimodal distribution in obfuscation effectiveness by tailoring prompts to each author's unique stylistic patterns.

## Key Results
- LLMs can reduce authorship verification accuracy, but effectiveness varies significantly across authors in a bimodal distribution
- Personalized prompting using SHAP-identified features improves obfuscation performance and reduces bimodality
- LLaMA-3.1 outperforms GPT-4 in obfuscation effectiveness, though semantic preservation was not evaluated
- LLMs show strong ability to modify punctuation, uppercase letters, and adverbs, but struggle with noun frequency and whitespace adjustments

## Why This Works (Mechanism)
The personalized prompting approach works by leveraging SHAP values to identify the most predictive stylistic features for each individual author, then targeting those specific features for modification. This addresses the fundamental challenge that different authors have distinct writing patterns, so a one-size-fits-all obfuscation approach cannot effectively mask all authorial styles. By tailoring prompts to modify the features that are most distinctive for each author, the method achieves more consistent obfuscation performance across the user population.

## Foundational Learning
- **Authorship attribution vs. verification**: Attribution assigns text to one of N authors, while verification determines if text belongs to a specific author (needed for binary classification setup; check by verifying binary output format)
- **SHAP values**: Shapley Additive Explanations provide feature importance scores based on game theory principles (needed to identify predictive features per author; check by verifying SHAP library usage and aggregation)
- **Zero-shot paraphrasing**: Generating paraphrases without task-specific fine-tuning, relying solely on prompt engineering (needed for LLM-based obfuscation; check by verifying no additional training on paraphrasing datasets)
- **Writeprints features**: Stylometric features capturing lexical, syntactic, and structural writing patterns (needed for author verification model inputs; check by verifying feature extraction implementation matches standard definitions)
- **Hartigan's dip test**: Statistical test for unimodality vs. multimodality in distributions (needed to quantify bimodality in obfuscation results; check by verifying test implementation and p-value interpretation)
- **Binary author verification**: Determining whether a text was written by a specific author rather than among multiple candidates (needed for per-user classification setup; check by verifying label encoding as 0/1)

## Architecture Onboarding
- **Component map**: Datasets (IMDb62/Yelp/Blog) -> Author Verifiers (Logistic Regression/BERT) -> Zero-shot Paraphrasing (GPT-4/LLaMA-3.1) -> SHAP Analysis -> Personalized Prompts -> Obfuscated Texts -> Evaluation (F1/Hartigan's test)
- **Critical path**: Train verifiers → Generate generic paraphrases → Compute SHAP values → Create personalized prompts → Re-paraphrase → Measure F1 reduction
- **Design tradeoffs**: Generic prompts provide baseline but show bimodal performance; personalized prompts improve consistency but require additional SHAP computation; LLaMA-3.1 offers better obfuscation but potential quality tradeoffs
- **Failure signatures**: Bimodal F1 distribution persisting after personalization; SHAP values not correlating with feature modification success; LLMs failing to modify requested features (especially noun frequency/whitespace)
- **First experiments**: 1) Verify dataset splits and preprocessing consistency across users; 2) Confirm author verifier training converges and achieves reasonable baseline F1 scores; 3) Test generic paraphrasing with both LLMs and verify feature modification success rates

## Open Questions the Paper Calls Out
The paper explicitly identifies three key areas for future work: (1) expanding prompt design to combine multiple stylistic features rather than single-feature modification, which could provide deeper insights into successful obfuscation patterns; (2) evaluating the tradeoff between obfuscation effectiveness and semantic preservation across different LLMs, since the current study did not measure semantic similarity; and (3) understanding why LLMs struggle to modify certain stylistic features like noun frequency and whitespace while succeeding with others like punctuation and uppercase letters. The paper also suggests investigating personalized obfuscation performance against more sophisticated ensemble or adversarially-trained authorship verification systems, as the current evaluation only tested individual model types.

## Limitations
- No evaluation of semantic preservation or text quality after paraphrasing, leaving the tradeoff between obfuscation and meaning unclear
- Limited to two LLM models (GPT-4 and LLaMA-3.1) without comparison to other architectures or fine-tuned models
- Single-feature personalization may miss synergistic effects from combining multiple stylistic modifications
- Tested only against standard author verification models, not ensemble or adversarially robust detection systems

## Confidence
- **Dataset selection and preprocessing**: High - clearly specified user counts, splits, and basic statistics
- **Author verifier training**: Medium - hyperparameters provided but implementation details for Writeprints features unknown
- **LLM paraphrasing**: Medium - prompt templates provided but inference parameters unspecified
- **SHAP analysis**: Medium - methodology described but implementation details unclear
- **Personalization effectiveness**: High - clear before/after comparison with statistical tests
- **Cross-LLM comparison**: Medium - results reported but semantic quality not evaluated

## Next Checks
1. Verify feature modification success rates by computing before/after values for key stylistic features (punctuation, uppercase, nouns, whitespace) to confirm LLMs follow personalized prompts
2. Replicate Hartigan's dip test results on F1 distributions to confirm bimodality reduction after personalization
3. Test semantic preservation using BERTScore or BLEU metrics on paraphrased texts to quantify quality tradeoffs across LLMs