---
ver: rpa2
title: Evaluating Hydro-Science and Engineering Knowledge of Large Language Models
arxiv_id: '2512.03672'
source_url: https://arxiv.org/abs/2512.03672
tags:
- llms
- hydro-se
- knowledge
- questions
- engineering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Hydro-SE Bench, a comprehensive evaluation
  benchmark for large language models (LLMs) in the Hydro-Science and Engineering
  (Hydro-SE) domain, featuring 4,000 multiple-choice questions across nine subfields
  and three cognitive categories. The benchmark was used to assess ten commercial
  LLMs and six small-parameter open-source models, revealing that commercial models
  achieve accuracy values between 0.74 and 0.80, while smaller models range from 0.41
  to 0.68.
---

# Evaluating Hydro-Science and Engineering Knowledge of Large Language Models

## Quick Facts
- **arXiv ID**: 2512.03672
- **Source URL**: https://arxiv.org/abs/2512.03672
- **Reference count**: 40
- **Primary result**: Hydro-SE Bench benchmark reveals LLMs excel at physics-grounded reasoning but struggle with domain-specific knowledge and industry standards in Hydro-SE applications

## Executive Summary
This paper introduces Hydro-SE Bench, a comprehensive Chinese-language benchmark for evaluating large language models in Hydro-Science and Engineering domains. The benchmark features 4,000 multiple-choice questions across nine subfields and three cognitive categories, enabling systematic assessment of both commercial and open-source models. The study reveals that while commercial models achieve 74-80% accuracy, smaller models range from 41-68%, with performance varying significantly by domain type. Physics-grounded subfields show the highest performance while industry standards and domain-specific knowledge lag behind, demonstrating that model scaling primarily improves reasoning abilities rather than specialized domain knowledge.

## Method Summary
The evaluation uses Hydro-SE Bench, a Chinese-language benchmark with 4,000 multiple-choice questions (2,700 single-choice, 1,300 multi-choice) across 9 subfields and 3 cognitive categories (A: conceptual knowledge, B: engineering applications, C: reasoning/calculation). Questions are sourced from 985 books, 600 industry standards, 57 laws, and 1,113 statistical yearbooks. Models are evaluated using temperature=0 for deterministic outputs, with a two-stage prompting approach: generate response, then extract choice letters using DeepSeek-V3.2-Exp as extractor. Confidence estimation uses verbalized 1-5 scales. The evaluation workflow includes response generation, answer extraction, accuracy scoring, and calibration curve analysis.

## Key Results
- Commercial LLMs achieve 74-80% accuracy while smaller models range from 41-68% on Hydro-SE Bench
- Parameter scaling yields 56.0% improvement in reasoning/calculation tasks (Type C) but only 23.4-32.7% gains in conceptual knowledge and engineering applications
- Physics-grounded subfields (Power Systems, Meteorology, Hydraulics) achieve 79-83% accuracy while Industry Standards only reach 70%
- Smaller models show severe confidence miscalibration, with GLM-4-32B assigning 81.4% of questions highest confidence while achieving only 74.4% accuracy on those questions

## Why This Works (Mechanism)

### Mechanism 1: Parameter Scaling Preferentially Enhances Reasoning Over Domain Knowledge
- Claim: Increasing model parameters from small (7B-32B) to large (70B+) scales yields a 56.0% improvement in reasoning/calculation tasks (Type C) but only 23.4-32.7% gains in conceptual knowledge and engineering applications.
- Mechanism: Scaling laws improve general reasoning capacity and pattern-matching for multi-step physical inference, numerical approximation, and stability judgment. However, without domain-specific corpus expansion, larger models cannot acquire specialized terminology, codified rules, or procedural criteria that remain underrepresented in training data.
- Core assumption: The performance differential reflects corpus coverage gaps rather than architectural limitations; domain-adaptive pre-training could close the gap.
- Evidence anchors: [abstract]: "Model scaling primarily improves reasoning and calculation abilities, but gains in conceptual knowledge and engineering applications are more limited." [section 3.3]: "Without incorporating domain-specific knowledge into the training corpus, or performing dedicated continuous pre-training and fine-tuning within specialized Hydro-SE contexts, model performance in conceptual knowledge and engineering applications tasks remains limited."

### Mechanism 2: Physics-Grounded Subfields Leverage Generalizable Scientific Knowledge
- Claim: LLMs achieve higher accuracy (0.79-0.83) in subfields like Power Systems, Meteorology, and Hydraulics/River Dynamics because these domains build on universal physical principles with consistent mathematical foundations.
- Mechanism: Physics-based domains share reasoning patterns with general scientific training data (fluid mechanics, thermodynamics, atmospheric science). Models transfer learned physical reasoning to Hydro-SE contexts where mechanisms are "relatively universal and less dependent on regional or institutional variations."
- Core assumption: The training corpora of general LLMs contain sufficient physics and natural science content to enable cross-domain transfer.
- Evidence anchors: [section 3.2]: "Subfields such as 'Power Systems' (PS), 'Meteorology' (M), and 'Hydraulics and River Dynamics' (HRD) yield relatively high accuracies... This divergence suggests that LLMs tend to perform better in subfields that are physically grounded and more closely aligned with the natural sciences."

### Mechanism 3: Confidence Miscalibration Creates Deployment Risk in Safety-Critical Engineering
- Claim: Smaller models (GLM-4-32B) show severe overconfidence—assigning 81.4% of questions the highest confidence score while achieving only 74.4% accuracy on those questions—creating asymmetric calibration that is "risky in real-world engineering scenarios."
- Mechanism: Verbalized confidence estimation relies on the model's internal uncertainty representations, which are poorly calibrated for domain boundaries the model cannot recognize. Models cannot distinguish between "I know this" and "I don't know that I don't know this" in specialized domains.
- Core assumption: Verbalized confidence (1-5 scale) approximates true epistemic uncertainty; calibration correlates with hallucination rates.
- Evidence anchors: [section 3.4]: "This mismatch between subjective confidence and objective correctness highlights a notable miscalibration problem, which can be risky in real-world engineering scenarios where misplaced confidence may lead to misleading or unsafe recommendations."

## Foundational Learning

- **Concept: Benchmark Cognitive Stratification (Knowledge vs. Application vs. Reasoning)**
  - Why needed here: Hydro-SE Bench separates Type A (conceptual recall), Type B (scenario-based engineering application), and Type C (multi-step reasoning/calculation). Understanding this distinction is critical because scaling improves C >> A > B, revealing that "knowing" ≠ "applying" ≠ "calculating."
  - Quick check question: If a model scores 0.80 on Type A but 0.65 on Type B, what does this imply about its deployment readiness for engineering decision support?

- **Concept: Domain Coverage vs. Model Scale Independence**
  - Why needed here: The paper demonstrates that "scaling alone cannot compensate for insufficient domain coverage." A 70B model without Hydro-SE corpus expansion will underperform a 32B model with targeted domain pre-training on industry standards and regulatory knowledge.
  - Quick check question: Why does the 56% improvement in Type C tasks from scaling not translate to equivalent gains in Industry Standards (IS) subfields (only 15% relative improvement)?

- **Concept: Calibration Asymmetry in Specialized Domains**
  - Why needed here: The paper reveals that models are overconfident in wrong answers and underconfident in some correct answers. In engineering safety contexts, an overconfident wrong answer is more dangerous than an underconfident correct answer.
  - Quick check question: If a model assigns confidence 5 to an answer that is actually incorrect, what type of calibration failure does this represent, and how would you detect this pattern systematically?

## Architecture Onboarding

- **Component map:**
Knowledge Corpus Sources (985 books + 600 standards + 57 laws + 1,113 yearbooks) -> Semi-automatic Question Generation Pipeline (expert few-shot + LLM expansion) -> Expert Review (≥3 rounds per question) -> Hydro-SE Bench (4,000 questions × 9 subfields × 3 types) -> Evaluation Workflow (response generation → answer extraction → accuracy scoring) -> Confidence Estimation (verbalized 1-5 scale → calibration curves)

- **Critical path:**
  1. **Question quality assurance**—each question must be traceable to authoritative sources and verified by ≥3 domain experts
  2. **Prompt consistency**—use identical templates across all models (Table B5) to avoid instruction-following bias
  3. **Answer extraction decoupling**—separate reasoning generation from choice extraction to capture genuine problem-solving processes

- **Design tradeoffs:**
  - **Multiple-choice vs. open-ended:** MCQs enable scalable automated evaluation but cannot capture "open-ended and dynamic decision-making processes required in real engineering applications" (Section 5.2)
  - **Full benchmark (4,000 questions) vs. mini-subset (30%+ sampling):** Full benchmark requires 83.9M tokens for SOTA models; 30% sampling maintains <5% accuracy variance while reducing cost by ~70%
  - **Verbalized vs. logit confidence:** Verbalized approach works for closed-source models but may not capture true token-probability uncertainty

- **Failure signatures:**
  - **Domain hallucination:** Model generates plausible-sounding but non-existent industry standards or outdated regulations (especially in IS and BK subfields)
  - **Asymmetric difficulty sensitivity:** Type C questions show higher variance in small sampling ratios (<30%), indicating inconsistent reasoning chains
  - **Confidence-accuracy inversion:** Smaller models show steeper calibration decay (confidence 5 → 74.4% accuracy; confidence 4 → 34.8% accuracy)

- **First 3 experiments:**
  1. **Baseline calibration check:** Run DeepSeek-V3.2-Exp and GLM-4-32B on 30% subset (1,200 questions), plot calibration curves, verify replication of paper's confidence distributions
  2. **Subfield error analysis:** For Industry Standards (IS) subfield where models achieve only 0.70 accuracy, categorize failure modes: outdated knowledge, terminology confusion, or regulation misapplication
  3. **Domain-adaptive intervention test:** Select 100 lowest-performing IS questions, provide relevant standard excerpts in context, measure accuracy delta to quantify retrieval-augmented potential vs. pre-training gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do current LLMs perform on multimodal Hydro-SE tasks involving satellite imagery and sensor time-series data compared to text-based reasoning?
- Basis in paper: [explicit] The authors state in the Limitations section that future benchmarks should integrate multimodal information such as remote sensing imagery and video streams, as the current study is text-only.
- Why unresolved: Hydro-SE Bench is restricted to text-based multiple-choice questions and lacks visual or time-series data components.
- What evidence would resolve it: Performance metrics from a new benchmark that includes image-based and sensor-data inputs alongside text questions.

### Open Question 2
- Question: To what extent can LLMs execute open-ended, dynamic decision-making processes in scenario-based agentic Hydro-SE tasks?
- Basis in paper: [explicit] The authors note in the Discussion that the current benchmark cannot fully reflect the open-ended, dynamic decision-making or scenario-based agentic tasks required in real engineering.
- Why unresolved: The evaluation relies on static multiple-choice questions rather than interactive, continuous simulations.
- What evidence would resolve it: Evaluation results from a benchmark incorporating interactive, scenario-driven tasks with open-ended response formats.

### Open Question 3
- Question: What specific domain-adaptive pre-training or supervised fine-tuning strategies are most effective for improving conceptual knowledge and engineering application accuracy?
- Basis in paper: [inferred] The paper concludes that scaling alone is insufficient for conceptual knowledge and engineering applications, but it does not test specific fine-tuning methods to resolve this gap.
- Why unresolved: The study evaluates pre-trained models but does not experiment with the targeted training interventions it suggests for future work.
- What evidence would resolve it: Comparative accuracy scores of models before and after specific domain-specific fine-tuning regimes.

## Limitations

- **Language restriction**: The benchmark is Chinese-language only, limiting cross-linguistic generalizability
- **Evaluation format**: Multiple-choice questions cannot capture open-ended reasoning and dynamic decision-making required in real engineering scenarios
- **Model availability**: Several evaluated models (GPT-5, Grok-4, Llama-4 Maverick) appear to be future versions, creating uncertainty about actual performance comparisons

## Confidence

- **High confidence**: Parameter scaling preferentially enhances reasoning over domain knowledge (supported by quantitative improvements: 56% vs 23.4-32.7% gains); Physics-grounded subfields leveraging generalizable scientific knowledge (consistent accuracy differentials: 0.79-0.83 vs 0.70-0.73); Overall benchmark design and methodology (clearly specified procedures, expert review process)
- **Medium confidence**: Confidence miscalibration creates deployment risk (calibration curves shown but verbalized confidence validity uncertain); Domain coverage vs. model scale independence (mechanism assumes corpus coverage gaps rather than architectural limits); Sampling ratio recommendations (30% subset maintains <5% accuracy variance but not independently verified)
- **Low confidence**: Cross-linguistic generalizability (benchmarks limited to Chinese); Real-world engineering applicability (MCQs cannot capture dynamic decision-making); Long-term deployment safety (calibration asymmetry important but not validated in actual engineering contexts)

## Next Checks

1. **Replication calibration analysis**: Run the same 30% subset evaluation on two additional SOTA models (Claude-3.5-Sonnet and Qwen2.5-72B-Chat) to verify whether confidence miscalibration patterns hold across model families and whether verbalized confidence correlates with actual accuracy.

2. **Domain-adaptive intervention test**: Select the 100 lowest-performing questions from Industry Standards subfield, provide relevant standard excerpts in context during prompting, and measure accuracy improvement to quantify whether retrieval-augmentation can close the gap or if this requires dedicated pre-training.

3. **Cross-linguistic transfer validation**: Translate 200 representative questions from Hydro-SE Bench into English and evaluate the same models on both versions to determine whether performance differentials stem from language-specific knowledge gaps or fundamental domain knowledge limitations.