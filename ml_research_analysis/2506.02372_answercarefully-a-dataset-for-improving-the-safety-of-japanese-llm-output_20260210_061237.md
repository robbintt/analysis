---
ver: rpa2
title: 'AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output'
arxiv_id: '2506.02372'
source_url: https://arxiv.org/abs/2506.02372
tags:
- safety
- evaluation
- japanese
- dataset
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AnswerCarefully is a dataset of 1,800 Japanese question-answer
  pairs designed to improve the safety of LLM outputs. It covers a wide range of harm
  categories, including discrimination, adult content, mental health, privacy, and
  misinformation, based on the English Do-Not-Answer taxonomy but manually created
  to reflect Japanese socio-cultural context.
---

# AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output

## Quick Facts
- arXiv ID: 2506.02372
- Source URL: https://arxiv.org/abs/2506.02372
- Reference count: 26
- Primary result: Dataset of 1,800 Japanese Q&A pairs for LLM safety alignment, reducing harmful outputs without over-refusal

## Executive Summary
AnswerCarefully is a manually curated dataset of 1,800 Japanese question-answer pairs designed to improve the safety of LLM outputs. It adapts the English Do-Not-Answer taxonomy to Japanese socio-cultural context, covering harm categories like discrimination, adult content, mental health, privacy, and misinformation. The dataset serves dual purposes: instruction tuning for safety alignment and benchmarking 12 Japanese LLMs. Experiments show that fine-tuning a Japanese LLM with AnswerCarefully significantly reduces harmful outputs while maintaining general response quality. Recent updates include English translations and annotations to support cross-lingual dataset creation.

## Method Summary
The dataset construction involved adapting the English Do-Not-Answer taxonomy to Japanese context, then manually creating question-answer pairs for each harm category. For experiments, AnswerCarefully v2 (336 test samples) was mixed with general instruction datasets (OpenAssistant, Dolly, ichikara-004-001-single) at a 16:1 ratio favoring AC samples. Supervised Fine-Tuning was performed on LLM-jp-13B-v2.0 using standard SFT hyperparameters (assumed LR 2e-5, 1-3 epochs). Evaluation employed GPT-4-as-a-judge with reference answers provided, using a 5-point rubric measuring Violation Rate and Acceptable Response Rate.

## Key Results
- Fine-tuning on AnswerCarefully significantly reduces harmful outputs across all evaluated harm categories
- No significant degradation in general response quality (Acceptable Response Rate maintained)
- Large performance differences observed across 12 Japanese LLMs evaluated on the dataset
- Automatic evaluation with GPT-4 shows high correlation (0.66-0.74) with human judgment when reference answers are provided

## Why This Works (Mechanism)
Safety alignment through instruction tuning with carefully curated harm-focused data enables LLMs to recognize and appropriately handle sensitive topics. The 16x duplication of safety data during training ensures it competes effectively with larger general instruction datasets. Providing reference answers to the LLM-as-a-judge evaluator creates consistent evaluation criteria and high correlation with human judgment.

## Foundational Learning
- **Japanese socio-cultural context**: Essential for adapting harm categories appropriately - quick check: verify domain experts reviewed taxonomy adaptation
- **Supervised Fine-Tuning (SFT)**: Core technique for instruction tuning - quick check: understand standard SFT hyperparameters and procedures
- **LLM-as-a-judge evaluation**: Automated safety assessment method - quick check: validate correlation with human evaluation on small sample
- **Harm taxonomy adaptation**: Process of localizing safety categories - quick check: compare original and adapted category definitions
- **Dataset mixing ratios**: Critical for balancing safety and general instruction - quick check: verify 16:1 ratio implementation

## Architecture Onboarding
- **Component map**: AC dataset → SFT training → LLM model → GPT-4 evaluation → performance metrics
- **Critical path**: Dataset creation → SFT training → evaluation → safety improvement
- **Design tradeoffs**: Manual curation ensures cultural relevance but limits scalability; 16x data duplication ensures safety learning but may affect general capability
- **Failure signatures**: Over-refusal (catastrophic forgetting), low evaluation correlation, cultural misalignment
- **First experiments**: 1) Train with varying data mixing ratios to find optimal balance, 2) Evaluate on general capability benchmarks to detect over-refusal, 3) Test automatic vs human evaluation correlation on subset

## Open Questions the Paper Calls Out
- How does the inclusion of "borderline data" impact the trade-off between reducing harmful outputs and minimizing false refusals (over-safety) in Japanese LLMs? The paper plans to add borderline data where questions are similar to harmful ones but can be answered straightforwardly, but performance on these edge cases remains unmeasured.
- How should safety alignment techniques handle "regionally sensitive" categories where appropriate answers differ by geography? Future versions will add this category, but cultural relativity introduces complexity beyond current universal safety standards.
- Does fine-tuning on AnswerCarefully generalize to protect against adversarial "Jailbreak" attacks, or is it limited to explicit harmful queries? The current evaluation focuses on everyday safety, not malicious jailbreak attempts designed to circumvent safety filters.

## Limitations
- Specific SFT hyperparameters and exact LLM-as-a-judge system prompt are not provided, requiring assumptions for reproduction
- Cross-lingual utility depends on translation quality and cultural relevance verification, which is not independently assessed
- Evaluation uses GPT-4-as-a-judge which may introduce its own biases and limitations in safety assessment

## Confidence
- **High confidence**: Dataset construction methodology and taxonomy adaptation are clearly described and reproducible
- **Medium confidence**: Experimental results showing reduced harmful outputs are likely reproducible given described methodology, but exact metrics depend on unknown training hyperparameters
- **Low confidence**: Claims about cross-lingual dataset creation utility are difficult to validate without independent verification of translation quality and cultural relevance

## Next Checks
1. Verify correlation between automatic and human evaluation by running a small-scale human evaluation study on the same test set
2. Test for over-refusal by evaluating model performance on general capability benchmarks (e.g., MT-Bench) before and after fine-tuning
3. Assess cultural relevance of translated English pairs by having bilingual experts evaluate whether safety concerns are preserved across languages