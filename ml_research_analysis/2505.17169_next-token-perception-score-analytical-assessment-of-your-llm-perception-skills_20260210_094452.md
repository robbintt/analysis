---
ver: rpa2
title: 'Next Token Perception Score: Analytical Assessment of your LLM Perception
  Skills'
arxiv_id: '2505.17169'
source_url: https://arxiv.org/abs/2505.17169
tags:
- ntps
- downstream
- linear
- perception
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Next Token Perception Score (NTPS), a
  metric to measure the alignment between autoregressive pretraining and downstream
  perception tasks. The core idea is that representations learned for next-token prediction
  may not align well with the subspaces needed for perception tasks, and NTPS quantifies
  this misalignment by measuring the overlap between feature subspaces.
---

# Next Token Perception Score: Analytical Assessment of your LLM Perception Skills

## Quick Facts
- arXiv ID: 2505.17169
- Source URL: https://arxiv.org/abs/2505.17169
- Reference count: 40
- Primary result: NTPS metric quantifies alignment between autoregressive pretraining and downstream perception, correlating strongly with linear probe accuracy and predicting LoRA fine-tuning gains

## Executive Summary
The Next Token Perception Score (NTPS) is a novel metric that quantifies the alignment between representations learned through autoregressive pretraining and those needed for downstream perception tasks. The authors prove that NTPS both upper- and lower-bounds the excess loss when using autoregressive features for perception tasks under a linear regime. Empirically, NTPS correlates strongly with linear probe accuracy across 12 NLP datasets and eight pretrained models ranging from 270M to 8B parameters, and reliably predicts accuracy gains from LoRA fine-tuning. The metric is computed in closed form from pretrained representations and labeled data, making it a lightweight tool for assessing LLM perception alignment and guiding adaptation decisions.

## Method Summary
NTPS measures the alignment between autoregressive and perception subspaces by computing the normalized Frobenius norm of the projection of the perception subspace onto the autoregressive subspace. The method involves extracting token-level hidden states from a pretrained transformer, constructing covariance matrices for both autoregressive (cov0, cov1) and perception (meanXX, meanXY) objectives, solving generalized eigenvalue problems to obtain subspaces V and U, and computing the overlap via orthogonal projection. The metric is evaluated across different layers and subspace dimensions, with the optimal configuration selected based on correlation with downstream performance. Linear probes and LoRA fine-tuning are used to validate NTPS's predictive power for downstream task performance.

## Key Results
- NTPS correlates strongly with linear probe accuracy (Spearman r ≈ 0.9) across 12 NLP datasets and eight models
- NTPS both upper- and lower-bounds the excess loss when using autoregressive features for perception tasks under linear assumptions
- NTPS increases after LoRA fine-tuning (71/96 runs) and inversely predicts accuracy gains from LoRA adaptation
- Larger models show more dramatic NTPS improvements after LoRA, while smaller models may sacrifice NTPS for downstream performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Misalignment between autoregressive pretraining and perception tasks arises from solving distinct generalized eigenvalue problems with different covariance structures.
- Mechanism: Autoregressive training learns features capturing sequential dependencies (covariance between X L₁ and X L₂), while perception tasks learn features capturing label-relevant covariance (between X and Y). These diverge because V prioritizes syntactic/predictive structure while U prioritizes semantic/task-relevant structure, as visually demonstrated in Figure 1 where V-space clusters by syntactic category while U-space separates by emotion class.
- Core assumption: Linear regime approximation is sufficiently informative to capture the fundamental misalignment between nonlinear autoregressive training and perception tasks.
- Evidence anchors:
  - [abstract]: "features optimized for next-token prediction do not consistently transfer well to downstream perception tasks... may lie outside the subspaces most informative for perception"
  - [section 3.2, Theorem 1]: "U and V capture distinct co-variability structures; hence, the autoregressively derived V may not generalize well to downstream tasks that depend on U"
  - [corpus]: Related work (arXiv:2511.17129) notes that "LLMs are inherently causal and optimized for next-token prediction," supporting the premise that autoregressive objectives shape representations differently.
- Break condition: If non-linear interactions (e.g., attention mechanisms) dominate representation learning such that linear subspace analysis fails to predict downstream performance, the mechanism loses predictive power.

### Mechanism 2
- Claim: NTPS quantifies alignment between autoregressive and perception subspaces, providing theoretically grounded bounds on excess task loss.
- Mechanism: NTPS measures the fraction of the perception subspace (U) that lies within the autoregressive subspace (V) via orthogonal projector P = VV†. Theoretical analysis proves that excess loss ΔL is bounded by (1 - NTPS) scaled by task-dependent constants Cmin and Cmax. When NTPS → 1 (perfect alignment), excess loss vanishes; when NTPS → 0 (orthogonal subspaces), loss increases linearly.
- Core assumption: The bounds derived in the linear regime (Theorem 2) extend meaningfully to nonlinear, large-scale LLMs despite architectural complexity.
- Evidence anchors:
  - [abstract]: "proven to both upper- and lower-bound the excess loss"
  - [section 3.3, Theorem 2]: "Cmin(1 − NTPS(U,V)) ≤ ΔL ≤ Cmax(1 − NTPS(U,V))"
  - [corpus]: No directly comparable metrics found in corpus; this appears novel.
- Break condition: If task-dependent constants Cmin/Cmax vary unpredictably across tasks or models such that the bounds become too loose for practical guidance, NTPS loses utility.

### Mechanism 3
- Claim: LoRA fine-tuning improves downstream perception performance by increasing subspace overlap (NTPS), and the magnitude of initial NTPS inversely predicts the accuracy gains from fine-tuning.
- Mechanism: LoRA adapts representations by learning low-rank updates to QKV projections, which systematically shifts the autoregressive subspace (V) closer to the perception subspace (U). NTPS increases in 71/96 runs after LoRA (Table 2), especially in larger models. Lower initial NTPS predicts larger accuracy gains from LoRA (Figure 3, Spearman r = 0.40–0.90), because poorly-aligned representations have more "headroom" for improvement.
- Core assumption: The low-rank updates learned by LoRA preferentially adjust representations toward the perception subspace rather than arbitrarily.
- Evidence anchors:
  - [abstract]: "NTPS increases following low-rank adaptation (LoRA) fine-tuning, especially in large models... inversely predicts the accuracy gains from LoRA"
  - [section 4.2, Table 2]: NTPS increases post-LoA in 71/96 runs, with dramatic increases in large models (e.g., Qwen2-7B: 53–135% improvement across datasets)
  - [corpus]: "LoRA Users Beware" (arXiv:2506.11402) warns of shortcut vulnerabilities, suggesting the mechanism may have unexplored failure modes.
- Break condition: If LoRA learns shortcuts or spurious correlations rather than genuine subspace alignment, the observed NTPS increase may not correlate with robust performance gains.

## Foundational Learning

- **Concept: Generalized Eigenvalue Problems**
  - Why needed here: The core theoretical result (Theorem 1) derives optimal feature maps for both autoregressive (V) and perception (U) objectives as the top eigenvectors of distinct generalized eigenvalue problems. Understanding this is essential to grasp why the objectives diverge.
  - Quick check question: Given covariance matrices A and B, what does solving Av = λBv for the top eigenvalues/vectors find?

- **Concept: Linear Probing vs. Fine-Tuning**
  - Why needed here: The paper's empirical validation compares linear probing (frozen pretrained weights + trained linear head) against full training from scratch (Table 1) and LoRA fine-tuning (Section 4). Understanding this distinction is critical to interpret NTPS's practical utility.
  - Quick check question: Why might a linear probe on frozen pretrained representations underperform training a small model from scratch on a specific task?

- **Concept: Subspace Overlap Metrics**
  - Why needed here: NTPS is defined as the normalized Frobenius norm of the projection of the perception subspace onto the autoregressive subspace (Section 3.3). This quantifies alignment as a scalar [0,1].
  - Quick check question: If two k-dimensional subspaces in R^d are orthogonal, what is their NTPS score?

## Architecture Onboarding

- **Component map:**
  Input dataset -> Extract hidden states X^l -> Construct L₁, L₂ matrices -> Accumulate covariance matrices (meanXX, meanXY, cov0, cov1) -> Solve generalized eigenvalue problems -> Extract subspaces Uₖ, Vₖ -> Compute orthogonal projector Pₖ -> Calculate NTPS

- **Critical path:**
  1. Ensure correct construction of L₁ (upper-triangular) and L₂ (lower-shifted identity) matrices from token representations (Appendix A.2, Eqs. 21–22).
  2. Verify generalized eigenvalue solvers for both perception (Eqs. 9, 27–28) and autoregressive (Eq. 8, 25) subspaces.
  3. Correctly compute the orthogonal projector P and normalize by ||U||²_F to get NTPS (Algorithm 1, line 17).

- **Design tradeoffs:**
  - **Layer selection l:** NTPS varies across layers; paper evaluates all layers and reports best. Tradeoff: earlier layers capture more task-agnostic features, later layers more task-specific. No principled selection method provided.
  - **Subspace dimension k:** Paper evaluates k ∈ [0.05d, 0.95d]. Tradeoff: smaller k captures dominant directions but may miss relevant variance; larger k includes noise.
  - **Pretrained model selection:** Smaller models (OpenELM-270/450M) show NTPS *decrease* after LoRA (Table 2), possibly due to limited capacity. Tradeoff: larger models more stable but costlier to evaluate.

- **Failure signatures:**
  - NTPS ≈ 1 but downstream performance is poor: Perception subspace lies within autoregressive subspace, but autoregressive representations themselves are uninformative for the task.
  - NTPS increases after LoRA but accuracy decreases: LoRA may have learned shortcuts or overfit; verify on held-out data.
  - Small models show NTPS decrease after LoRA: Limited capacity forces tradeoff; model sacrifices autoregressive alignment for downstream performance.

- **First 3 experiments:**
  1. **Reproduce Table 1 on a new dataset:** Compare linear probe vs. full training on a held-out downstream task to confirm variability of pretrained representation effectiveness.
  2. **Sweep layers and k for NTPS computation:** Implement Algorithm 1 and reproduce Figure 2 for one model (e.g., OpenELM-450M) to verify the monotonic relationship between NTPS and downstream performance.
  3. **Test LoRA prediction on a new model:** Apply LoRA to a model not in the paper's evaluation and verify that low initial NTPS predicts larger accuracy gains.

## Open Questions the Paper Calls Out

- **How can the theoretical formulation of NTPS be extended to explicitly account for the role of attention mechanisms in Transformer architectures?**
  - Basis: The authors state in the Limitations section (Section 5) that "NTPS is derived under a simplified linear setting and does not currently account for the role of attention mechanisms," suggesting linear attention models as a potential starting point.
  - Why unresolved: The current derivation relies on linear mapping assumptions ($V^\top X$), which ignores the complex, data-dependent interactions modeled by self-attention layers that define modern LLM representations.
  - What evidence would resolve it: A reformulation of the NTPS metric within a linear attention framework that maintains the theoretical bounds on excess loss while accounting for context mixing.

- **Is there a principled, a priori method to determine the optimal subspace dimension $k$ and layer selection $l$ for computing NTPS without requiring an exhaustive search?**
  - Basis: Section 5 notes that the authors "have not yet explored how to select optimal configurations... the choice of $k$ may depend on the model's compression rate," and acknowledges the need to eliminate the exhaustive search over all layers and $k$ values currently used in the validation.
  - Why unresolved: The empirical validation relies on a grid search over "every $k$ proportion value" and "all layers" to find the highest rank correlation, making the metric computationally inefficient for rapid screening.
  - What evidence would resolve it: A theoretical or heuristic rule linking model architecture (e.g., hidden dimension, depth) to the optimal $k$ that achieves comparable correlation to the search-based method.

- **Does the observed decrease in NTPS in smaller models after LoRA fine-tuning reflect a hard capacity limit where models must sacrifice next-token prediction geometry to accommodate perception tasks?**
  - Basis: Section 4.2 shows NTPS decreases in small models (OpenELM-270M/450M) post-LoRA. The authors hypothesize this is because "these models sacrifice the next-token prediction for higher downstream performance due to its limited capability," but do not verify the mechanism.
  - Why unresolved: It is unclear if this represents a zero-sum game in the subspace capacity of smaller models or if it is an artifact of the specific LoRA configuration used.
  - What evidence would resolve it: An analysis measuring the degradation in perplexity (next-token prediction loss) alongside NTPS changes in small models to confirm if subspace overlap shifts are directly correlated with a loss of generative capability.

## Limitations

- The theoretical framework relies on linear subspace analysis that may not fully capture the complex, non-linear interactions in transformer architectures, particularly attention mechanisms
- The mechanism may not generalize to encoder-decoder models, retrieval-augmented systems, or models trained with alternative objectives (masked language modeling, contrastive learning)
- While NTPS correlates strongly with downstream performance, the paper does not establish causal mechanisms, and alternative explanations (shortcut learning, overfitting) remain possible

## Confidence

- **High confidence**: The empirical correlation between NTPS and linear probe accuracy (Spearman r ≈ 0.9) across multiple models and datasets. The reproducibility of NTPS computation methodology and its monotonic relationship with downstream performance metrics.
- **Medium confidence**: The theoretical bounds on excess loss (Theorem 2) and their practical relevance for guiding adaptation decisions. While mathematically proven under linear assumptions, the extension to non-linear, large-scale LLMs requires empirical validation rather than theoretical guarantees.
- **Low confidence**: The causal interpretation that LoRA improves performance specifically by increasing subspace alignment (NTPS). Alternative explanations exist, including potential shortcut learning or overfitting that coincidentally increases NTPS without improving genuine task understanding.

## Next Checks

1. **Ablation on attention mechanisms**: Modify Algorithm 1 to compute NTPS on attention weight matrices or attention-based representations rather than token-level hidden states. Compare correlation with downstream performance to assess whether NTPS captures attention-level alignment that better explains non-linear effects.

2. **Cross-architecture generalization**: Compute NTPS for BERT-style encoder models, encoder-decoder models, and models trained with contrastive objectives. Test whether NTPS maintains predictive power across architectures or whether the mechanism is specific to decoder-only next-token prediction.

3. **Longitudinal study of LoRA learning**: Track NTPS and validation accuracy throughout LoRA training epochs. Determine whether NTPS increases monotonically with performance or whether initial NTPS increase precedes later performance gains, helping distinguish genuine alignment from shortcut learning.