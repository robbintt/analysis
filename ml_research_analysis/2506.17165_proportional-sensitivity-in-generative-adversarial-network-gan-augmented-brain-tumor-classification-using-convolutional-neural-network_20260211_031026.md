---
ver: rpa2
title: Proportional Sensitivity in Generative Adversarial Network (GAN)-Augmented
  Brain Tumor Classification Using Convolutional Neural Network
arxiv_id: '2506.17165'
source_url: https://arxiv.org/abs/2506.17165
tags:
- data
- real
- images
- brain
- tumor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluated how varying ratios of real
  and GAN-generated brain MRI images affect CNN-based tumor classification performance.
  A DCGAN was used to synthesize 1,000 synthetic brain MRI images, which were blended
  with real images in 11 different proportions to train a custom CNN.
---

# Proportional Sensitivity in Generative Adversarial Network (GAN)-Augmented Brain Tumor Classification Using Convolutional Neural Network

## Quick Facts
- arXiv ID: 2506.17165
- Source URL: https://arxiv.org/abs/2506.17165
- Reference count: 19
- Primary result: Small GAN data proportions (≤10%) improve CNN-based brain tumor classification performance, but performance degrades as synthetic ratio increases

## Executive Summary
This study systematically evaluated how varying ratios of real and GAN-generated brain MRI images affect CNN-based tumor classification performance. A DCGAN was used to synthesize 1,000 synthetic brain MRI images, which were blended with real images in 11 different proportions to train a custom CNN. The model achieved peak performance (95.20% accuracy, 95.25% F1-score, AUC 0.98) when trained on 90% real and 10% GAN data, indicating that small amounts of synthetic data can enhance generalization. However, performance steadily declined as the proportion of GAN data increased, with accuracy dropping to 65.40% when trained entirely on synthetic images (AUC 0.71). The results highlight that while GANs are useful for augmenting limited datasets, over-reliance on synthetic data can impair model reliability, emphasizing the need for a balanced integration of real and synthetic images in medical AI applications.

## Method Summary
The study used the BR35H dataset (3,000 images: 1,500 tumor, 1,500 healthy) to train a DCGAN for 1,000 epochs, generating 1,000 synthetic images (500 tumor, 500 healthy). A custom CNN with 3 convolutional layers and a dense layer was trained across 11 configurations with varying real-to-synthetic ratios (100:0, 90:10, ..., 0:100). Each configuration used 1,000 total training images with 800:200 train/validation split. Performance was evaluated on a fixed real-world test set of 1,000 images (500 per class) using accuracy, precision, recall, F1-score, and AUC-ROC metrics.

## Key Results
- Peak performance achieved at 90% real, 10% GAN: 95.20% accuracy, 95.25% F1-score, AUC 0.98
- Performance degradation begins beyond 50% GAN data: 90.40% accuracy at 80% real/20% GAN
- At 100% GAN: accuracy drops to 65.40%, AUC to 0.71, recall to 48.80%, precision to 73.05%

## Why This Works (Mechanism)

### Mechanism 1
Small proportions of GAN-generated data (≤10%) can improve generalization by acting as a regularizer that introduces controlled variability into the training distribution. The synthetic images expose the CNN to minor distributional variations that are consistent with real MRI characteristics but not identical to any specific real sample. This prevents overfitting to idiosyncratic features of the limited real dataset while maintaining fidelity to the underlying pathology patterns. The core assumption is that the DCGAN has learned to generate images that preserve diagnostically relevant tumor features rather than merely superficial visual similarity.

### Mechanism 2
Performance degradation at high synthetic ratios stems from cumulative domain shift caused by systematic differences between GAN-generated and real MRI distributions. As synthetic proportion increases, the CNN increasingly learns features specific to GAN artifacts (texture patterns, edge artifacts from the generation process) that do not transfer to real MRIs. The discriminator in DCGAN training only ensures the generator produces plausible images, not that they match the full statistical distribution of real scans. The core assumption is that the DCGAN's discriminator does not enforce preservation of all diagnostically relevant features—only visual plausibility at the pixel/patch level.

### Mechanism 3
The asymmetric degradation pattern (recall dropping more sharply than precision at high synthetic ratios) indicates the model loses sensitivity to true tumor features while retaining conservative decision boundaries. At 100% GAN training, recall drops to 48.80% while precision remains at 73.05%. This suggests the CNN learns to recognize "obvious" tumor patterns but misses subtle or atypical presentations that the GAN failed to capture in its synthetic distribution. The core assumption is that GAN-generated tumor images represent a subset of the real tumor phenotype distribution—specifically, more visually distinctive cases.

## Foundational Learning

- **Concept: Adversarial Training Dynamics (GAN Loss Landscape)**
  - Why needed: Understanding why DCGAN-generated images contain artifacts requires grasping the equilibrium-seeking nature of generator-discriminator training and its failure modes
  - Quick check: Can you explain why a perfectly converged GAN might still produce images that differ systematically from real data in diagnostically relevant ways?

- **Concept: Domain Shift in Transfer Learning**
  - Why needed: The core finding is that synthetic-to-real transfer degrades proportionally—this is a domain shift problem where the source (synthetic) and target (real) distributions diverge
  - Quick check: If you trained on 50% GAN data from dataset A and tested on real data from dataset B, what additional confounding factor would you need to isolate?

- **Concept: Regularization vs. Noise Injection in Data Augmentation**
  - Why needed: The study's central claim is that small synthetic ratios regularize while large ratios inject noise—understanding this threshold is critical for practical application
  - Quick check: What metric would you monitor during training to detect when synthetic data transitions from regularization to harmful noise?

## Architecture Onboarding

- **Component map:**
  Real MRI Dataset (BR35H, 3000 images) -> Preprocessing Pipeline (resize 64×64, center crop, normalize to [-1,1], RGB conversion) -> DCGAN Training (1000 epochs, 1000 synthetic images) and CNN Training (3 conv layers, 128 dense, sigmoid output) -> Ratio Blending (11 configurations) -> Independent Real Test Set (1000 images)

- **Critical path:** Preprocessing consistency → DCGAN convergence quality → Ratio blending → Real-world evaluation. Breaks occur if preprocessing differs between real and synthetic, or if DCGAN mode-collapses.

- **Design tradeoffs:**
  - 64×64 resolution trades spatial detail for GAN training stability and CNN speed
  - Binary classification simplifies the problem but masks potential issues with tumor subtype generalization
  - Fixed 1000-image training sets enable ratio comparison but limit absolute performance ceiling
  - Assumption: Standard DCGAN loss without task-specific perceptual losses

- **Failure signatures:**
  - Generator/discriminator loss divergence during GAN training → mode collapse
  - High training accuracy with low test accuracy → synthetic overfitting
  - Recall << precision at high synthetic ratios → feature distribution mismatch
  - AUC below 0.8 → synthetic data dominant, real features not learned

- **First 3 experiments:**
  1. **Baseline replication:** Train the 11-ratio sweep on your own hardware using the published preprocessing pipeline to validate reproducibility before modifying architecture
  2. **Ablation on GAN quality:** Vary DCGAN training epochs (100, 500, 1000, 2000) at a fixed 50% ratio to isolate whether degradation is from ratio or GAN quality
  3. **Per-class analysis:** Evaluate tumor vs. non-tumor synthetic image quality separately using SSIM/FID against real samples to determine if one class degrades faster—guiding targeted GAN improvement

## Open Questions the Paper Calls Out

### Open Question 1
How do diffusion models or domain-adapted GANs compare to DCGANs in maintaining classification performance at higher synthetic data ratios? This study only evaluated DCGAN; newer generative architectures may produce higher-quality synthetic images that allow for greater synthetic data proportions without performance degradation. A comparative study using identical experimental protocols with StyleGAN, diffusion models, or domain-adapted variants, measuring performance across the same real-to-synthetic ratios would resolve this.

### Open Question 2
What specific artifacts or structural inconsistencies in GAN-generated MRIs cause the observed performance degradation as synthetic proportions increase? The paper attributes declining accuracy to artifacts affecting generalization but does not characterize what these artifacts are or how they differ from real MRI features. Systematic feature-level analysis comparing real and synthetic images, potentially with radiologist evaluation or interpretability methods (e.g., Grad-CAM) to identify inconsistent anatomical regions would resolve this.

### Open Question 3
Does the optimal 90:10 real-to-GAN ratio generalize across different brain MRI datasets, tumor types, and classification architectures? The study used a single dataset (BR35H), a single GAN type (DCGAN), and a single CNN architecture, leaving the generalizability of the 10% synthetic optimum untested. The optimal ratio may depend on dataset size, image quality, tumor heterogeneity, or model capacity; different contexts could yield different optima. Replication of the proportional sensitivity experiment across multiple public datasets (e.g., BraTS, Figshare) and with alternative CNN or transformer-based classifiers would resolve this.

### Open Question 4
How does GAN-based augmentation affect multi-class brain tumor classification compared to binary tumor vs. non-tumor detection? The study only performed binary classification, whereas clinical practice requires distinguishing tumor subtypes (glioma, meningioma, pituitary), which may be differentially sensitive to synthetic data quality. Multi-class tasks impose stricter feature discrimination requirements; synthetic data that suffices for binary detection may introduce class-specific biases or confusion. Extending the experimental framework to multi-class datasets and analyzing per-class performance trends across synthetic data ratios would resolve this.

## Limitations
- DCGAN architecture details are unspecified, making exact reproduction difficult without architectural assumptions
- The specific threshold where synthetic data transitions from regularization to harmful noise remains unclear and likely dataset-dependent
- No ablation study isolating whether degradation stems from image quality, label noise, or distributional mismatch
- Limited analysis of which tumor features the GAN fails to capture, making targeted improvements difficult

## Confidence
- **High confidence:** The empirical finding that 10% GAN augmentation improves performance over pure real data is well-supported by the systematic ratio sweep and clear peak at 90:10
- **Medium confidence:** The mechanism explaining degradation at higher synthetic ratios is plausible but requires additional validation through feature-space analysis
- **Low confidence:** The claim that recall drops more than precision at 100% GAN is based on a single data point and needs broader testing across different GAN architectures

## Next Checks
1. Conduct ablation experiments varying DCGAN training epochs (100, 500, 1000, 2000) at fixed 50% ratio to separate GAN quality effects from ratio effects
2. Perform feature attribution analysis (e.g., Grad-CAM) comparing models trained at different synthetic ratios to identify which tumor features degrade first
3. Test alternative GAN architectures with perceptual losses or domain adaptation objectives to determine if degradation can be mitigated while maintaining augmentation benefits