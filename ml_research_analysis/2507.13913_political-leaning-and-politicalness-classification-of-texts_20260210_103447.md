---
ver: rpa2
title: Political Leaning and Politicalness Classification of Texts
arxiv_id: '2507.13913'
source_url: https://arxiv.org/abs/2507.13913
tags:
- political
- have
- datasets
- dataset
- leaning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automatically classifying
  text by political leaning and politicalness using transformer models. The authors
  found that existing approaches perform poorly on out-of-distribution texts, so they
  compiled a diverse dataset combining 12 political leaning datasets and created a
  new politicalness dataset by extending 18 existing datasets.
---

# Political Leaning and Politicalness Classification of Texts

## Quick Facts
- arXiv ID: 2507.13913
- Source URL: https://arxiv.org/abs/2507.13913
- Authors: Matous Volf; Jakub Simko
- Reference count: 18
- Primary result: Models achieve 83.7-87.2% F1 for political leaning classification, but performance drops significantly on out-of-distribution data.

## Executive Summary
This paper addresses the challenge of automatically classifying text by political leaning and politicalness using transformer models. The authors found that existing approaches perform poorly on out-of-distribution texts, so they compiled a diverse dataset combining 12 political leaning datasets and created a new politicalness dataset by extending 18 existing datasets. They evaluated existing models and trained new ones with enhanced generalization capabilities. The best existing model achieved 90.4% F1 score for politicalness classification, while their new models trained on all available datasets established state-of-the-art performance with 83.7-87.2% F1 scores for political leaning classification. The study reveals that all models perform consistently worse on unseen datasets, confirming the difficulty of teaching models to classify political texts universally across different domains.

## Method Summary
The authors compiled a diverse dataset by combining 12 political leaning datasets and extending 18 existing datasets to create a new politicalness dataset. They fine-tuned transformer models (POLITICS and DeBERTa V3 large) with optimized hyperparameters, using a 3-class schema for political leaning (left/center/right) and binary classification for politicalness. The methodology included systematic sampling, label harmonization, and filtering non-political texts using a high-confidence threshold. Models were evaluated using leave-one-out benchmarks on F1 score, accuracy, precision, and recall.

## Key Results
- Best existing model achieved 90.4% F1 score for politicalness classification
- New models trained on all available datasets achieved state-of-the-art performance with 83.7-87.2% F1 scores for political leaning classification
- All models performed consistently worse on unseen datasets, confirming OOD generalization challenges
- POLITICS model showed improved left-out dataset F1 scores (e.g., 63.1% on Article bias prediction) compared to base models

## Why This Works (Mechanism)

### Mechanism 1: Domain Diversity Improves OOD Generalization
- Claim: Training transformer models on a unified, multi-domain dataset of political texts improves out-of-distribution (OOD) generalization compared to single-domain training.
- Mechanism: Aggregating 12 diverse datasets (news, social media, synthetic statements) exposes the model to varied linguistic styles, topics, and ideological contexts, encouraging it to learn more generalizable political language representations rather than domain-specific artifacts.
- Core assumption: The learned features transfer across domains and the improved OOD performance is not solely due to increased training data volume but to the diversity of sources.
- Evidence anchors:
  - [abstract] "...compile a diverse dataset by combining 12 datasets... train new ones with enhanced generalization capabilities."
  - [section 4.4.1] The POLITICS model trained on all available datasets shows improved left-out dataset F1 scores (e.g., 63.1% on Article bias prediction) compared to base models trained on less diverse data (e.g., BERT left-out average 49.4%).
  - [corpus] Evidence in related work is weak or missing for this exact multi-domain aggregation strategy, though related work notes the impact of training data on model bias.

### Mechanism 2: Label Aggregation and Filtering Define Task Scope
- Claim: Political leaning classification (3-class: left, center, right) is a prerequisite to defining a meaningful politicalness (binary) task, and data quality for both is bounded by label source reliability.
- Mechanism: The authors map diverse labels (e.g., 5-class spectra, media outlet ratings) to a unified 3-class schema and use an external politicalness model to filter non-political content from a leaning dataset. This preprocessing defines the distribution the model learns.
- Core assumption: The 3-class schema is a sufficient and consistent approximation of political ideology across all text types, and the filter model's high-confidence threshold (logit > 0.99) accurately removes non-political texts without excessive false negatives.
- Evidence anchors:
  - [section 3.1.1] "...labels have been reduced to 3 classes by mapping both of these levels to the single left or right class..."
  - [section 3.1.1] "We have utilized the model as a filter and removed just the non-political texts... with the logit score above 0.99."
  - [corpus] Not directly addressed in neighbors.

### Mechanism 3: Transformer Architecture and Hyperparameter Tuning Optimize Performance
- Claim: Fine-tuning a domain-adapted transformer (POLITICS) with optimized hyperparameters yields state-of-the-art performance on political text classification.
- Mechanism: POLITICS, a RoBERTa model further pre-trained on political news, provides a stronger inductive bias for political language. Hyperparameter optimization (learning rate, dropout, warmup) then refines the model's capacity to generalize from the diverse training data.
- Core assumption: The hyperparameter search space and optimization objective (F1 score on a left-out validation set) capture the configuration that maximizes OOD performance.
- Evidence anchors:
  - [abstract] "...train new ones with enhanced generalization capabilities."
  - [section 3.5.2] "POLITICS has proven to be the best model for further fine-tuning, consistently scoring the highest in all previous benchmarks."
  - [section 3.5.2] Hyperparameter search found specific values (e.g., learning rate 5.97e-5) that improved performance.
  - [corpus] Weak or missing. Neighbor papers focus on LLM biases, not fine-tuning strategies for smaller, specialized models.

## Foundational Learning

- Concept: **Fine-tuning Transfer Learning**.
  - Why needed here: The paper builds on pre-trained transformers (BERT, RoBERTa, DeBERTa, POLITICS). Understanding how knowledge transfers from general corpora to a specific, complex task like political ideology classification is essential to interpret the results and improvements.
  - Quick check question: How does the performance of a fine-tuned POLITICS model compare to a base RoBERTa model on the same out-of-distribution political leaning task?

- Concept: **Out-of-Distribution (OOD) Generalization**.
  - Why needed here: The core problem the paper addresses is models failing on data different from their training set. Grasping OOD concepts (domain shift, distribution shift) is necessary to understand the leave-one-out benchmark and the authors' goals.
  - Quick check question: If a model trained on news articles is evaluated on social media posts, is this an in-distribution or out-of-distribution task?

- Concept: **Dataset and Label Bias**.
  - Why needed here: Political data is inherently biased. The paper's methodology (harmonizing labels, filtering non-political text) is a direct attempt to manage these biases, which is critical for model reliability and fairness.
  - Quick check question: Why might using media outlet bias ratings as labels for individual articles introduce noise or error into a political leaning dataset?

## Architecture Onboarding

- Component map:
  - Data Pipeline: 12 political leaning datasets + 18 politicalness datasets -> preprocessing (downsampling, label harmonization, filtering) -> unified training/validation/test sets
  - Model Core: Pre-trained Transformer (POLITICS or DeBERTa V3) with a classification head (3-way for leaning, 2-way for politicalness)
  - Training & Evaluation: Fine-tuning loop with hyperparameter search (Optuna), evaluated using leave-one-in/leave-one-out benchmarks on F1 score, accuracy, precision, and recall

- Critical path: Data aggregation and labeling (3.1) -> Model selection and hyperparameter optimization (3.5) -> Training on full dataset (3.6) -> OOD evaluation (4)

- Design tradeoffs:
  - **Model Size vs. Resources**: A larger model (DeBERTa V3 large) may offer better performance but requires significantly more GPU memory and training time, limiting the ability to run full hyperparameter searches.
  - **Label Granularity vs. Data Availability**: A simpler 3-class leaning schema was chosen over more nuanced options (e.g., 5-class or regression) due to data availability and labeling difficulty, potentially sacrificing detail for feasibility.
  - **Politicalness Filter Threshold**: A high confidence threshold (0.99) for filtering non-political text may produce a cleaner leaning dataset but risks removing some genuine political content (false negatives).

- Failure signatures:
  - **Siloed Performance**: High accuracy on in-distribution data but a significant drop (>10% F1) on left-out datasets.
  - **Class Imbalance Collapse**: A model failing to predict the "center" class or any minority class due to under-representation in the training data.
  - **Filter Over-Pruning**: A very low volume of texts being classified as "political" by the politicalness filter, suggesting it may be too aggressive.

- First 3 experiments:
  1. Establish a baseline by evaluating an existing model (e.g., Political bias prediction AllSides DeBERTa) on both an in-distribution and an out-of-distribution dataset to confirm the OOD performance drop.
  2. Run a "leave-one-out" experiment by training a base model (e.g., RoBERTa) on all but one of the political leaning datasets and evaluating on the held-out dataset to measure initial OOD capability.
  3. Conduct a focused hyperparameter search (e.g., varying learning rate and batch size) on the POLITICS model to find a configuration that improves performance on the held-out dataset from the previous experiment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a unified architecture that incorporates politicalness as a "non-political" class within the political leaning classifier outperform the current separate pipeline?
- Basis in paper: [explicit] The authors suggest in Section 7 that a unified approach could simplify the pipeline and help the model better understand the relationship between political and non-political content.
- Why unresolved: The current methodology treats politicalness as a binary pre-filter task, entirely distinct from the 3-class political leaning classification.
- What evidence would resolve it: A comparative study evaluating a 4-class unified model (left, center, right, non-political) against the sequential application of the separate models on the same diverse datasets.

### Open Question 2
- Question: How does replacing the discrete 3-class system with a continuous numerical scale or a "mixed" class impact the model's ability to capture nuanced political positions?
- Basis in paper: [explicit] Section 7 proposes implementing a more granular spectrum (5+ levels) or a "mixed" class to acknowledge texts supporting multiple positions simultaneously.
- Why unresolved: The current classification schema forces a simplification of the political spectrum into three rigid categories, potentially mislabeling complex or centrist viewpoints.
- What evidence would resolve it: Training regression-based models or multi-label classifiers on the datasets and measuring the correlation with human judgments for texts identified as having "mixed" or nuanced ideology.

### Open Question 3
- Question: To what extent do the trained models rely on unintended stylistic artifacts or media source identities rather than semantic content to predict political leaning?
- Basis in paper: [inferred] Section 6 notes the lack of extensive testing to ensure models have not learned arbitrary features like specific media outlet characteristics, and Section 7 calls for improved interpretability.
- Why unresolved: High performance on benchmark datasets may be inflated by the model learning the writing style of specific news outlets rather than the actual political arguments.
- What evidence would resolve it: An interpretability analysis (e.g., attention visualization) and robustness testing using handcrafted examples where political content is decoupled from the stylistic markers of the source outlet.

## Limitations

- The label harmonization process may have collapsed meaningful ideological distinctions into the 3-class schema, potentially limiting the model's ability to capture nuanced political perspectives.
- The high confidence threshold (0.99) for the politicalness filter may have removed legitimate political content, creating an artificially "pure" training set that doesn't reflect real-world data distributions.
- The improved OOD generalization claim is plausible but not definitively proven, as the study doesn't isolate the effect of dataset diversity from overall data size.

## Confidence

- **High Confidence**: The finding that existing models perform poorly on out-of-distribution texts is well-supported by the leave-one-out benchmark results showing consistent performance drops across multiple models and datasets.
- **Medium Confidence**: The claim that the POLITICS model achieves state-of-the-art performance on political leaning classification (83.7-87.2% F1) is supported by the reported scores but depends on the specific datasets and evaluation methodology used.
- **Low Confidence**: The assertion that the improved OOD generalization is primarily due to domain diversity rather than increased training data volume is plausible but not definitively proven.

## Next Checks

1. **Domain Diversity Isolation Test**: Train a model with the same total number of examples but from a single domain (e.g., only news articles) versus the multi-domain aggregated dataset, then compare OOD performance to isolate the effect of diversity versus volume.

2. **Filter Threshold Sensitivity Analysis**: Evaluate the political leaning classifier performance using different politicalness filter thresholds (0.95, 0.97, 0.99, 0.995) to determine if the high threshold is optimal or if it's removing too much valid political content.

3. **Nuanced Label Schema Validation**: Train and evaluate models using more granular label schemes (5-class or regression) on datasets where such labels are available, to assess whether the 3-class simplification is limiting model performance or if the simpler schema is sufficient for practical applications.