---
ver: rpa2
title: Causal Estimation of Tokenisation Bias
arxiv_id: '2506.03149'
source_url: https://arxiv.org/abs/2506.03149
tags:
- tokenisation
- bias
- effect
- subwords
- vocabulary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study identifies tokenisation bias as a persistent influence
  of vocabulary choice on language model predictions. It proposes a causal framework
  using regression discontinuity design to estimate the effect of including or excluding
  a subword from the vocabulary.
---

# Causal Estimation of Tokenisation Bias

## Quick Facts
- arXiv ID: 2506.03149
- Source URL: https://arxiv.org/abs/2506.03149
- Authors: Pietro Lesci; Clara Meister; Thomas Hofmann; Andreas Vlachos; Tiago Pimentel
- Reference count: 40
- Primary result: Tokenisation bias is a persistent influence of vocabulary choice on language model predictions, quantified using regression discontinuity design to compare subwords near tokeniser cutoff points

## Executive Summary
This paper identifies and quantifies "tokenisation bias"—the causal effect of including or excluding a subword from a model's vocabulary on the probability assigned to that subword's characters. Using regression discontinuity design (RDD) around tokeniser cutoff points, the authors estimate this bias across different model scales, vocabulary sizes, and tokenisers. They find that bias is largest for small models (up to 17× probability increase for included subwords) and decreases with scale but doesn't vanish, stabilizing around 1 nat for large models. The bias grows during training and is stronger for smaller vocabularies, while also reducing output stability for out-of-vocabulary character strings.

## Method Summary
The paper applies regression discontinuity design to estimate tokenisation bias by exploiting the exogenous ranking of subwords in BPE and WordPiece tokenisers. Models are trained with truncated vocabularies (K) while preserving the full ranked subword list (K⁺ = 320k) to identify subwords near the cutoff. The RDD estimator compares outcomes (log-probabilities) for subwords just above and below K, treating the merge index as the running variable. The causal effect ψ is estimated via regression controlling for the running variable and its interactions. Experiments span vocabulary sizes (8k, 32k, 128k) and model scales (57M-850M parameters) using Llama-style transformers trained on MiniPile and Fineweb-Edu.

## Key Results
- Tokenisation bias is consistent across scales, vocabularies, and tokenisers: small models show up to 17× probability increases for in-vocabulary subwords
- Bias stabilizes around 1 nat for larger models (~850M parameters) rather than vanishing completely
- Bias grows during training, dropping sharply by step 2k then slowly increasing again
- Smaller vocabularies show stronger bias effects, with 32k and 128k vocabularies showing similar magnitudes
- Bias reduces output stability for out-of-vocabulary character strings, with negative effects on standard deviation and IQR

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Including a subword in the vocabulary causally increases the probability its characters receive, with effects up to 17× in small models.
- Mechanism: Single-token representations bypass sequential prediction steps. When v exists in vocabulary, p(cv|c<t) requires predicting one token; when absent, it decomposes into multiple predictions whose joint probability is the product of lower individual probabilities. This creates a systematic advantage for in-vocabulary character strings.
- Core assumption: Models do not perfectly compensate for tokenisation differences during training (Theorem 1 shows perfect models would have zero bias, but trained models are imperfect).
- Evidence anchors: [abstract] "a subword's presence in a small model's vocabulary may increase its characters' probability by up to 17 times"; [section 6] "we estimate this bias to be 2.88 nats, with treated character-strings cv having a log-probability of −7.72 and untreated ones −10.60"

### Mechanism 2
- Claim: Regression discontinuity design enables causal estimation without retraining models under different tokenisers.
- Mechanism: Bottom-up tokenisers (BPE, WordPiece) rank subwords by frequency or PMI and include the top K. The cutoff K is arbitrary and exogenous. Subwords just above and below the cutoff have similar properties (frequency, length) but different treatment status. Comparing their outcomes isolates the vocabulary-inclusion effect.
- Core assumption: Assumption 1 (Continuity)—the relationship between the running variable (merge index) and potential outcomes is smooth at the cutoff. Any discontinuity is treatment-induced, not pre-existing.
- Evidence anchors: [abstract] "we exploit the fact that tokenisation algorithms rank subwords and add the first K to a tokeniser's vocabulary"; [section 4.2] "In RD design, γv is termed a running variable and K is the cutoff of this running variable"

### Mechanism 3
- Claim: Tokenisation bias grows during training and decreases with model scale, stabilising around 1 nat for large models.
- Mechanism: Small models have limited capacity to smooth over tokenisation differences; each token prediction carries higher variance. Larger models can better approximate the underlying character distribution, reducing—but not eliminating—bias. Training increases bias because models learn to exploit vocabulary structure (e.g., memorising that certain tokens are more predictive).
- Core assumption: The relationship between model capacity and bias-mitigation is monotonic but asymptotic (beyond ~850M parameters, further scaling shows diminishing returns in bias reduction).
- Evidence anchors: [section 6] "bias is smaller on larger (e.g., 340M parameters) than in small models...Among our largest models, however (with either 340M or 850M parameters), we see little difference"; [section 6] "this effect is large at the start of training. Notably, it drops sharply by step 2k and then slowly grows again"

## Foundational Learning

- **Concept: Regression Discontinuity Design (RDD)**
  - Why needed here: The paper's core contribution applies RDD to tokenisation—understanding the running variable, cutoff, and continuity assumption is essential to interpreting all results.
  - Quick check question: If the vocabulary cutoff K were chosen based on validation perplexity, would RDD still be valid? Why or why not?

- **Concept: Subword Tokenisation Algorithms (BPE, WordPiece)**
  - Why needed here: The mechanism relies on the iterative, greedy construction of vocabularies. You need to understand why subwords are ranked and how merges create dependencies.
  - Quick check question: In BPE, if subwords A and B are merged at iteration k, what happens to all occurrences of (A, B) in the corpus for subsequent iterations?

- **Concept: Potential Outcomes Framework (Rubin Causal Model)**
  - Why needed here: The paper defines tokenisation bias as Y₁(v) − Y₀(v)—the difference between observed and counterfactual outcomes. Without this framing, the RD estimator's logic is opaque.
  - Quick check question: For a given subword v, can we ever observe both Y₁(v) and Y₀(v)? How does the paper address this?

## Architecture Onboarding

- **Component map:** Train tokeniser with oversized vocabulary (K⁺ >> K) -> Train model with truncated vocabulary (size K) -> Collect log-probabilities for subwords in window around K -> Fit RD regression Y_obs(v) = α + β·γ_v + ψ_RD·W_v + ε_v -> Compute mean, std, median, IQR of log p(cv|c<t) across contexts

- **Critical path:**
  1. Train tokeniser with oversized vocabulary (K⁺ >> K) to preserve ranking information beyond cutoff
  2. Train model with truncated vocabulary (size K)
  3. At evaluation, extract subwords in window [K − w, K + w] where w ≈ 5k
  4. For each subword v, collect log-probabilities across all validation contexts where its characters cv appear
  5. Fit RD regression to estimate ψ_RD (local average treatment effect at cutoff)

- **Design tradeoffs:**
  - Window size: Larger w → more samples, better statistical power, but stronger dependence on correct f(γ_v) specification. Paper finds estimates stabilise at w ≥ 1000 (Figure 5).
  - Functional form for f: Linear is simple but may underfit; LOESS is flexible but risks overfitting noise (Figure 9 shows similar estimates across forms).
  - Subword exclusion: Must exclude nested subwords (e.g., if ⟨hello⟩ and ⟨he⟩ are both in vocabulary) to avoid SUTVA violations—this reduces sample size.

- **Failure signatures:**
  - Instability across window sizes: If estimates vary wildly for w < 1000, the RD assumption may be violated (discontinuity not locally random)
  - Negative bias estimates: Possible for large vocabularies where marginal subwords are rare and potentially harmful (Figure 6, 8k vocab shows smaller effects)
  - High variance in outcome measures: Std/IQR effects are negative (Figure 2), indicating out-of-vocabulary subwords have unstable predictions—this is a feature, not a bug

- **First 3 experiments:**
  1. **Reproduce the 32k BPE baseline:** Train tokeniser to 320k, truncate to 32k, train 57M Llama model for 50k steps, estimate ψ_RD with w = 5k. Verify ~2.8 nat bias.
  2. **Vocabulary size sweep:** Repeat with K ∈ {8k, 32k, 128k} on same model size. Confirm that larger vocabularies do not necessarily reduce bias (32k and 128k show similar effects).
  3. **Model scale test:** Train 57M, 340M, 850M variants with identical tokeniser (32k BPE). Verify that bias decreases from ~2.8 to ~1 nat but does not vanish, and that 340M vs 850M show minimal difference (Figure 4, right panel).

## Open Questions the Paper Calls Out

- **Cross-linguistic validation:** Does the observed tokenisation bias persist across different model architectures, training procedures, and multilingual settings? The current study restricts its scope to Llama-style architectures and English text (MiniPile/Fineweb-Edu) to control costs.

- **Morphological generalisation:** How does tokenisation bias impact a model's lexical generalisation across morphological variants? The current outcome variable Y_W(v) measures average log-probability, not the model's ability to generalize semantic or orthographic rules to unseen variants.

- **Vocabulary optimization:** Can the causal estimator ψ be effectively utilised as a metric to optimize vocabulary construction? The paper quantifies the bias but does not implement an optimization loop or demonstrate that minimizing/maximizing this specific metric leads to a superior downstream model.

## Limitations

- The continuity assumption (Assumption 1) may be violated if subwords near the vocabulary cutoff differ systematically in ways not captured by frequency or merge order
- All experiments use English text from MiniPile and Fineweb-Edu; generalization to morphologically rich languages or code remains uncertain
- The RDD window selection (5k subwords) appears arbitrary, and sensitivity to functional form specifications is only briefly explored

## Confidence

**High Confidence**: The causal identification strategy using RDD is methodologically sound and well-executed. The empirical finding that tokenisation bias decreases with model scale but stabilizes around 1 nat is robust across multiple experimental conditions. The observation that bias grows during training and is stronger for smaller vocabularies is consistently replicated.

**Medium Confidence**: The quantitative estimates of bias magnitude (e.g., 17× effect for small models, 2.88 nats for 32k vocab) depend on specific experimental conditions and may vary with different corpora, tokenisers, or model architectures.

**Low Confidence**: Generalisability to non-English text, extreme vocabulary sizes (<4k or >128k), or different model families (e.g., decoder-only vs encoder-decoder) remains untested. The mechanism explaining why bias grows during training is hypothesised but not conclusively proven.

## Next Checks

1. **Cross-linguistic validation**: Apply the RD framework to morphologically rich languages (e.g., Turkish, Finnish) where subword tokenisation decisions have more dramatic effects on character string representations. Test whether bias magnitudes and scaling relationships hold.

2. **Alternative tokenisers and architectures**: Validate the findings using SentencePiece (which doesn't use frequency-based ranking) and different model architectures (e.g., GPT-2 style vs Llama). This would test whether the bias is intrinsic to subword tokenisation or specific to BPE/WP ranking schemes.

3. **Extended training and regularisation experiments**: Train models beyond 50k steps to test whether bias eventually plateaus or continues growing. Experiment with character-level auxiliary losses or vocabulary dropout during training to assess whether these techniques can reduce bias without changing the tokeniser.