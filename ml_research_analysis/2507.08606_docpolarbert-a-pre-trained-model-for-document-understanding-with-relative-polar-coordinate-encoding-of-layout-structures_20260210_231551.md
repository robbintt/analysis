---
ver: rpa2
title: 'DocPolarBERT: A Pre-trained Model for Document Understanding with Relative
  Polar Coordinate Encoding of Layout Structures'
arxiv_id: '2507.08606'
source_url: https://arxiv.org/abs/2507.08606
tags:
- document
- relative
- docpolarbert
- attention
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DocPolarBERT, a novel BERT-like model for
  document understanding that encodes layout information using relative polar coordinates
  instead of absolute Cartesian positions. The key innovation is replacing absolute
  positional embeddings with a relative attention mechanism that uses distance and
  angle between text elements in polar coordinates.
---

# DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures

## Quick Facts
- arXiv ID: 2507.08606
- Source URL: https://arxiv.org/abs/2507.08606
- Authors: Benno Uthayasooriyar; Antoine Ly; Franck Vermet; Caio Corro
- Reference count: 15
- Outperforms LayoutLM on PAYSLIPS and DOCILE while pre-trained on dataset more than six times smaller

## Executive Summary
DocPolarBERT introduces a BERT-like model for document understanding that replaces absolute positional embeddings with relative attention based on polar coordinates. The key innovation is encoding spatial relationships as distance and angle between text elements, which the model learns through two separate attention bias terms. Despite being pre-trained on a dataset (OCR-IDL + DOCILE) more than six times smaller than the widely used IIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results on multiple document understanding tasks including FUNSD, SROIE, CORD, PAYSLIPS, and DOCILE. The model particularly excels at handling structured documents with tables and forms, demonstrating better length generalization and improved attention patterns focused on semantically relevant elements.

## Method Summary
DocPolarBERT modifies standard BERT architecture by removing absolute 2D positional embeddings and replacing them with relative polar coordinate attention. The model computes distance (ρ) and angle (θ) between all token pairs using their bounding box centers, discretizes these into buckets (4 quantile-based distance buckets, 8 fixed angle buckets), and adds learnable embeddings as attention biases. Pre-training uses Masked Language Modeling (30% masking) and optionally a 1-LOP loss on OCR-IDL + DOCILE corpus (1.8M documents). The model is initialized from RoBERTa and fine-tuned on five downstream NER datasets. The polar attention mechanism computes softmax(QK + diag(QS^T) + diag(QT^T)) where S and T are distance and angle bias matrices.

## Key Results
- Achieves state-of-the-art F1 scores on PAYSLIPS (75.40) and DOCILE datasets
- Outperforms LayoutLM on average across all five datasets (81.34 vs 80.23 F1)
- Demonstrates superior length generalization, especially on long documents with tables
- Ablation confirms removing absolute positional embeddings improves performance by 1.11 F1 points
- Attention analysis reveals more coherent attention patterns focused on semantically relevant elements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relative polar coordinate encoding improves generalization on structured documents by decoupling angular and distance relationships.
- Mechanism: The attention mechanism computes two independent biases—$S_j = E_{dist.}(\rho(c_i, c_j))$ and $T_j = E_{angle}(\theta(c_i, c_j))$—added to the query-key similarity. This explicitly separates "direction" (e.g., "is above") from "proximity" (e.g., "is nearby"), allowing the model to learn distinct patterns for column alignment vs. field grouping.
- Core assumption: Documents have semantically meaningful spatial relationships that are better captured by angle/distance separation than Cartesian (x,y) offsets.
- Evidence anchors:
  - [abstract]: "relative polar coordinate system rather than the Cartesian one"
  - [section 1]: "Angular information: column headers are often in first row... Distance information: related fields in a form are close"
  - [corpus]: Neighbor paper "Decoupling the 'What' and 'Where' With Polar Coordinate Positional Embeddings" reports similar benefits in separating positional from content information (FMR=0.56, h-index=89), providing independent support for polar encoding's general utility.
- Break condition: If documents have no consistent spatial structure (random layouts), the angular/distance decomposition provides no advantage over Cartesian or sequential encodings.

### Mechanism 2
- Claim: Eliminating absolute 2D positional embeddings improves performance by enforcing translation invariance.
- Mechanism: Without absolute position embeddings, the model cannot overfit to specific document regions. All spatial reasoning must occur through relative relationships, forcing the model to learn layout patterns that transfer across document positions.
- Core assumption: The semantic meaning of layout patterns (e.g., header-value pairs in tables) should not depend on absolute page location.
- Evidence anchors:
  - [section 1]: "absolute positional embeddings are not invariant to translation, which is a key characteristic of many documents of interest"
  - [section 7.1, Table 7]: Ablation shows removing 2D-pos embeddings improves average F1 from 80.23 to 81.34 (FUNSD: 76.22→78.26, PAYSLIPS: 72.52→75.40)
  - [corpus]: No direct corpus evidence on translation invariance in document models; this is a paper-specific claim requiring replication.
- Break condition: If downstream tasks require absolute location awareness (e.g., "extract the footer"), removing absolute embeddings would harm performance.

### Mechanism 3
- Claim: Quantile-based distance discretization per-document improves handling of variable-length documents compared to fixed bucketing.
- Mechanism: Rather than using fixed distance thresholds, quantiles adapt to each document's distance distribution. This ensures balanced bucket occupancy regardless of document size, preventing sparsity in long documents or granularity loss in short ones.
- Core assumption: The relevant spatial scale varies across documents, and adaptive bucketing captures this better than fixed thresholds.
- Evidence anchors:
  - [section 4.3]: "quantiles are computed from the distribution of distances within each structured document"
  - [section 6.4, Table 6]: Length generalization experiments show DocPolarBERT achieves best average F1 (74.86) when trained on short documents and tested on long ones, particularly on PAYSLIPS (85.10 vs. 77.80 for LayoutLM)
  - [corpus]: No corpus evidence on quantile-based discretization; this is a paper-specific design choice.
- Break condition: If all documents have similar size distributions, quantile-based bucketing offers minimal advantage over fixed bucketing.

## Foundational Learning

- Concept: **Self-attention with relative position biases**
  - Why needed here: DocPolarBERT modifies standard attention by adding learnable bias terms based on spatial relationships. Understanding Shaw et al. (2018)'s relative attention formulation is prerequisite.
  - Quick check question: Given query $Q$, key $k$, and relative position matrix $R$, how does adding $\text{diag}(QR^\top)$ change attention distribution?

- Concept: **Polar coordinate systems**
  - Why needed here: The model encodes spatial relationships as $(\rho, \theta)$ pairs rather than $(x, y)$ offsets. Intuition for how angle/distance separate spatial concepts is essential.
  - Quick check question: In a table, why might "angle = π/2" (directly above) be more semantically useful than "y-offset = -100 pixels"?

- Concept: **OCR-based document representation**
  - Why needed here: Input is a sequence of tokens with bounding boxes from OCR. Understanding that multiple tokens can share a bounding box, and that linearization order matters, is critical.
  - Quick check question: Why does the paper preserve OCR linearization order even though documents are 2D?

## Architecture Onboarding

- Component map:
  - Input layer: Token embeddings + 1D positional embeddings (NO 2D absolute embeddings)
  - Polar attention module: Computes $\rho(c_i, c_j)$ and $\theta(c_i, c_j)$ for all token pairs, discretizes into buckets, retrieves $E_{dist.}$ and $E_{angle}$ embeddings, adds as biases to attention scores
  - Standard BERT encoder: 12 layers, 12 heads, 768 dimensions (inherited from RoBERTa initialization)
  - Pre-training heads: MLM (30% masking) + optional 1-LOP loss

- Critical path:
  1. OCR extraction → (tokens, bounding boxes)
  2. Bounding box centers → pairwise polar coordinates
  3. Discretization: distance → 4 quantile buckets, angle → 8 fixed buckets
  4. Embedding lookup → bias matrices $S$ and $T$
  5. Modified attention: $\text{softmax}(Qk + \text{diag}(QS^\top) + \text{diag}(QT^\top))$

- Design tradeoffs:
  - **Fewer distance buckets (4) vs. more (32)**: Paper finds 4 quantile buckets optimal on average (81.34 F1), but dataset-specific tuning helps (SROIE prefers 32 buckets at 97.74 F1—see Table 8)
  - **With vs. without 1-LOP loss**: Adds +0.39 average F1 but requires additional pre-training complexity
  - **Single-GPU vs. multi-GPU**: Per-document inference is slower than baselines (25.9ms vs. 9.5ms for LayoutLM), but 4-GPU scaling is 1.82× better (Table 4)

- Failure signatures:
  - **Dense receipt confusion**: On CORD, model confuses semantically similar labels in dense regions—suggests polar encoding struggles with fine-grained structures (section 6.1)
  - **Simple layout underperformance**: On FUNSD/SROIE with clear horizontal/vertical alignments, Cartesian baselines are competitive—polar abstraction may be unnecessary overhead
  - **Attention scatter**: If attention heatmaps show uniform distribution rather than focused patterns, check that bounding boxes are correctly extracted and polar coordinates are computed

- First 3 experiments:
  1. **Sanity check**: Fine-tune on a small subset of PAYSLIPS (strong results in paper: 75.40 F1). Verify attention focuses on semantically related tokens, not random regions. Compare attention heatmaps against Figure 3.
  2. **Ablation: bucket count**: Test distance bucket counts [4, 8, 16, 32] on your target dataset. Paper shows optimal varies by dataset (Table 8). If your documents resemble invoices, start with 4 quantile buckets.
  3. **Length generalization test**: Split your data by token length, train on shortest 50%, test on longest 50%. Compare against LayoutLM baseline. Expect DocPolarBERT advantage to emerge on documents with long tables (section 6.4).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the polar-relative encoding scheme be extended to better handle dense document regions with multiple semantically similar or hierarchically related elements?
- Basis in paper: [explicit] The authors observe that on CORD, "our model often confuses semantically similar or hierarchically related labels (e.g. subtypes within the 'menu' category). This is likely due to limitations of polar-relative encoding in capturing fine-grained structures in dense regions with multiple mentions."
- Why unresolved: The current polar coordinate discretization (8 angle buckets, 4 distance buckets) may lack sufficient granularity for distinguishing nearby elements in cluttered layouts.
- What evidence would resolve it: Systematic evaluation on documents with controlled density variations, or ablation studies with finer discretization buckets in dense regions.

### Open Question 2
- Question: How does DocPolarBERT perform on document understanding tasks beyond named-entity recognition, such as document classification, visual question answering, or layout analysis?
- Basis in paper: [explicit] "We evaluate our approach only on named-entity recognition, which is the most common benchmark for layout models, but it may not cover all use cases."
- Why unresolved: Different tasks may require different spatial reasoning capabilities; NER emphasizes local relationships while classification may need global structure understanding.
- What evidence would resolve it: Benchmarking on datasets like DocVQA, RVL-CDIP, or PubLayNet to assess task transferability.

## Limitations

- Pre-training corpus size (1.8M documents) is significantly smaller than competitors like LayoutLM (11M documents), potentially limiting generalization
- Underperforms on FUNSD and CORD datasets, suggesting polar encoding may not universally improve document understanding
- Implementation complexity of quantile-based distance discretization may affect reproducibility across different document types

## Confidence

- High confidence: The ablation results showing that absolute positional embeddings degrade performance (80.23 → 81.34 F1 average), and the consistent outperformance on PAYSLIPS and DOCILE datasets
- Medium confidence: Claims about improved attention patterns being more coherent, as this is based on qualitative visualization rather than quantitative metrics
- Low confidence: Generalization claims across all document types, given the model underperforms on FUNSD and CORD

## Next Checks

1. Replicate the ablation study on absolute positional embeddings by training a version of DocPolarBERT with and without 2D positional embeddings on a held-out validation set to verify the 1.11 F1 improvement is reproducible
2. Conduct attention pattern analysis using quantitative metrics (entropy, attention focus scores) rather than qualitative visualization to statistically validate claims about improved attention coherence
3. Test the model's performance on a diverse set of document types beyond the five evaluated datasets to assess whether the polar encoding advantage generalizes to documents with varying layout complexity