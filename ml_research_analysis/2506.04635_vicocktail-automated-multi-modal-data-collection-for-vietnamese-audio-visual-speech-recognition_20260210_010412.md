---
ver: rpa2
title: 'ViCocktail: Automated Multi-Modal Data Collection for Vietnamese Audio-Visual
  Speech Recognition'
arxiv_id: '2506.04635'
source_url: https://arxiv.org/abs/2506.04635
tags:
- speech
- recognition
- audio
- attention
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a fully automated pipeline for generating audio-visual
  speech recognition (AVSR) datasets from raw video, demonstrating its effectiveness
  by creating the first Vietnamese AVSR dataset (ViCocktail) with 269 hours of training
  data. The pipeline extracts lip regions, audio, and transcriptions using automated
  tools, including an ASR model for Vietnamese text generation.
---

# ViCocktail: Automated Multi-Modal Data Collection for Vietnamese Audio-Visual Speech Recognition

## Quick Facts
- arXiv ID: 2506.04635
- Source URL: https://arxiv.org/abs/2506.04635
- Reference count: 0
- Primary result: 9.40% WER in clean conditions, 18.39% WER in severe cocktail-party noise (-5 dB SNR) with 2 interferers

## Executive Summary
This paper presents a fully automated pipeline for generating audio-visual speech recognition (AVSR) datasets from raw video, demonstrating its effectiveness by creating the first Vietnamese AVSR dataset (ViCocktail) with 269 hours of training data. The pipeline extracts lip regions, audio, and transcriptions using automated tools, including an ASR model for Vietnamese text generation. Experiments show that AVSR models significantly outperform audio-only models in noisy conditions, with the best model achieving 9.40% WER in clean conditions and only 18.39% WER in severe cocktail-party noise (-5 dB SNR), compared to 70.01% for audio-only models. The work demonstrates AVSR's robustness to noise and provides an open-source pipeline and dataset for expanding AVSR to low-resource languages.

## Method Summary
The method involves a multi-stage automated pipeline that processes raw YouTube videos through shot detection, face tracking, Active Speaker Detection (ASD), and SyncNet verification to filter and extract suitable audio-visual segments. Lip regions are extracted using Dlib landmarks and cropped to 96×96 resolution. Two model architectures are explored: AV-HuBERT CTC/Attention (initialized from English MuAViC checkpoint) and Conformer CTC/Attention (initialized from AutoAVSR English checkpoint). Both use a 6-layer Transformer decoder with SentencePiece tokenization (2057 vocab). Training incorporates noise augmentation with 1-2 interfering speakers at various SNR levels (-5/0/5/10 dB).

## Key Results
- AVSR model achieves 9.40% WER in clean conditions, significantly outperforming audio-only model (53.83%)
- In severe cocktail-party noise (-5 dB SNR with 2 interferers), AVSR model reaches 18.39% WER compared to 70.01% for audio-only model
- English pre-trained AV-HuBERT checkpoint outperforms multilingual checkpoint (9.40% vs 14.28% WER) and training from scratch (18.60% WER)

## Why This Works (Mechanism)

### Mechanism 1
Automated pipeline can generate viable AVSR training data from raw video without manual labeling. Multi-stage filtering cascade progressively removes unsuitable content: shot detection isolates continuous segments, face tracking ensures temporal consistency, Active Speaker Detection confirms the visible person is speaking, and SyncNet verifies audio-visual synchronization and removes dubbed/mismatched content. Each stage acts as a quality gate, yielding ~51% retention (269h from 528.8h). Core assumption: High-quality ASR transcriptions are sufficient proxies for human labels during training. Break condition: If source videos have frequent shot changes, multiple speakers, or poor audio-visual sync, retention drops below usable thresholds.

### Mechanism 2
Cross-lingual transfer from English pre-trained AVSR models improves Vietnamese AVSR more than training from scratch or multilingual checkpoints. Visual features (lip movements, mouth shapes) are largely language-agnostic—phoneme-viseme mappings share cross-lingual structure. English pre-trained AV-HuBERT has learned robust audio-visual correspondences through self-supervised clustering on 1,759 hours. Fine-tuning on Vietnamese adapts language-specific audio patterns while retaining general visual representations. Core assumption: Visual speech features transfer across languages more readily than audio features alone. Break condition: If target language has fundamentally different viseme structure (e.g., tonal languages with visual distinctions English lacks), transfer may degrade.

### Mechanism 3
Audio-visual fusion provides dramatic noise robustness with minimal clean-condition performance loss. Visual modality is inherently immune to acoustic noise—lip movements remain identical regardless of SNR. At severe noise (-5dB SNR, 2 interferers), audio-only WER degrades 8.3× (7.53% → 70.01%) while AV degrades only 2× (9.4% → 18.39%). The model learns to weight visual features more heavily when audio is unreliable. Core assumption: The fusion mechanism can dynamically adjust modality weighting based on input quality.

## Foundational Learning

- **Concept: Audio-Visual Speech Recognition (AVSR)**
  - Why needed here: Core task—understanding why adding video helps is essential before implementing the pipeline
  - Quick check question: In a cocktail party scenario, why would lip movements help when the audio is corrupted by overlapping speech?

- **Concept: Self-Supervised Pre-training (AV-HuBERT)**
  - Why needed here: The best-performing model uses AV-HuBERT, which learns audio-visual representations by predicting cluster assignments without labels
  - Quick check question: How does predicting "which cluster does this frame belong to" create useful representations for downstream speech recognition?

- **Concept: CTC/Attention Hybrid Decoding**
  - Why needed here: Both architectures use this decoder; understanding the trade-off between monotonic alignment (CTC) and conditional dependencies (Attention) is critical
  - Quick check question: Why would you want both CTC (assumes conditional independence) and Attention (models dependencies) in the same decoder?

## Architecture Onboarding

- **Component map:**
  Raw Video → Shot Detection → Face Tracking → ASD + SyncNet → Lip Extraction → ViCocktail Dataset
                                                                                            ↓
  Audio + Lip Frames → 3D-ResNet-18 (visual) + FFN/1D-ResNet-18 (audio) → Fusion MLP → Transformer Encoder → CTC/Attention Decoder → Transcript

- **Critical path:**
  1. SyncNet confidence filtering—removes misaligned samples that would poison training
  2. Pre-trained encoder initialization—English AV-HuBERT checkpoint determines convergence speed and final performance
  3. Noise augmentation during training—enables robustness to cocktail-party conditions at inference

- **Design tradeoffs:**
  - English vs. multilingual pretraining: English checkpoint outperforms multilingual (WER 9.4% vs 14.28% clean), possibly because multilingual dilutes visual feature quality
  - From-scratch vs. transfer: Training from scratch fails catastrophically for Conformer; AV-HuBERT from scratch achieves 18.6% WER vs. 9.4% with English pretraining
  - Automation quality vs. scale: Automated labels enable 269 hours but require manual annotation for reliable test evaluation

- **Failure signatures:**
  - Training loss plateaus early → checkpoint initialization failed or learning rate too high for fine-tuning
  - Clean WER good but noisy WER degrades sharply → insufficient noise augmentation during training
  - Visual-only WER >60% → lip extraction pipeline failing (wrong landmarks, low resolution)
  - Sync between audio and video drifts → SyncNet filtering threshold too permissive

- **First 3 experiments:**
  1. Pipeline validation: Run pipeline on 10 hours of raw video; manually inspect 100 random samples for lip extraction quality and transcription accuracy before scaling to full dataset
  2. Transfer vs. scratch ablation: Train AV-HuBERT on Vietnamese with (a) English checkpoint, (b) multilingual checkpoint, (c) random initialization—confirm paper's 9.4% / 14.28% / 18.6% pattern
  3. Noise robustness stress test: Evaluate on progressively harder conditions (clean → 10dB → 0dB → -5dB, 0→1→2 interferers); verify graceful degradation curve rather than cliff-like failure

## Open Questions the Paper Calls Out

- **Open Question 1**: Does self-supervised pre-training on the Vietnamese ViCocktail dataset yield better performance and efficiency than the current approach of transferring English-pretrained checkpoints? The authors state in the conclusion: "In the future, we plan to scale up data collection and explore self-supervised pre-training to develop more efficient models."

- **Open Question 2**: Why does adaptation from an English-only checkpoint result in lower WER for Vietnamese AVSR than adaptation from a multilingual checkpoint? Table 2 shows that model AV1 (English pre-trained) consistently outperforms model AV2 (Multilingual pre-trained) across all noise conditions (e.g., 9.40% vs. 14.28% WER in clean settings), contradicting the common intuition that multilingual models transfer better.

- **Open Question 3**: To what extent do the visual features in the proposed model specifically capture and utilize Vietnamese tonal information? The introduction posits that "Visual cues... can help distinguish tonal differences" in Vietnamese, but the results report general WER without isolating the model's ability to recognize tones versus segmental phonemes.

## Limitations

- The automated pipeline relies on ASR-generated transcriptions, which may contain errors that propagate to model training, though manual annotation was only performed for the test set
- Cross-lingual transfer assumptions may not generalize to languages with very different viseme-phoneme mappings beyond Vietnamese
- The 51% pipeline retention rate (269h from 528.8h) indicates substantial content filtering, but the characteristics of discarded data remain unexplored

## Confidence

- **High confidence**: AVSR significantly outperforms audio-only models in noisy conditions (18.39% vs 70.01% WER at -5dB/2 interferers); cross-lingual transfer from English checkpoints improves Vietnamese AVSR; automated pipeline successfully generates usable training data
- **Medium confidence**: Visual features are "more language-agnostic" than audio features—supported by results but not rigorously tested across diverse languages
- **Medium confidence**: English checkpoint consistently outperforms multilingual checkpoint—results show this pattern but underlying reasons (e.g., data quality differences in MuAViC) aren't fully explored

## Next Checks

1. Pipeline quality audit: Manually evaluate 100 random samples from automated pipeline output to quantify ASR transcription error rates and SyncNet filtering accuracy
2. Cross-lingual transfer stress test: Compare English vs. multilingual pretraining on additional low-resource languages (e.g., Thai, Hindi) to validate generalization of transfer superiority
3. Noise robustness boundary analysis: Systematically evaluate model performance across SNR range (-10dB to 20dB) to identify exact degradation thresholds and determine if visual modality provides consistent benefits across all noise levels