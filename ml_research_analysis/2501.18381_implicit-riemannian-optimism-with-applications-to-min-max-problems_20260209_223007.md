---
ver: rpa2
title: Implicit Riemannian Optimism with Applications to Min-Max Problems
arxiv_id: '2501.18381'
source_url: https://arxiv.org/abs/2501.18381
tags:
- riemannian
- g-convex
- where
- holds
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RIOD, an implicit Riemannian optimistic online
  learning algorithm for Hadamard manifolds, and RIODA, a corresponding min-max optimization
  method. RIOD improves upon prior work by handling in-manifold constraints without
  requiring strong assumptions and achieving regret bounds independent of geometric
  constants like the minimum curvature.
---

# Implicit Riemannian Optimism with Applications to Min-Max Problems

## Quick Facts
- arXiv ID: 2501.18381
- Source URL: https://arxiv.org/abs/2501.18381
- Reference count: 40
- Key outcome: Introduces RIOD algorithm for Riemannian online learning and RIODA for min-max optimization, achieving curvature-independent regret bounds and near-optimal gradient complexity for smooth g-convex problems.

## Executive Summary
This paper introduces RIOD, an implicit Riemannian optimistic online learning algorithm for Hadamard manifolds, and RIODA, a corresponding min-max optimization method. RIOD improves upon prior work by handling in-manifold constraints without requiring strong assumptions and achieving regret bounds independent of geometric constants like the minimum curvature. The method uses inexact implicit updates, making it computationally efficient for smooth losses. RIODA builds on RIOD to solve g-convex, g-concave smooth min-max problems, achieving near-optimal gradient oracle complexity (O(LR²/ε) for ε duality gap) that matches Euclidean algorithms up to logarithmic factors. This is the first Riemannian min-max algorithm to remove dependence on geometric terms ζ, except for log factors. Experiments on robust Karcher mean problems in symmetric positive definite manifolds and hyperbolic spaces validate the theoretical results, showing linear convergence. The work addresses key challenges in Riemannian optimization, such as enforcing constraints and reducing complexity, advancing the field significantly.

## Method Summary
RIOD is an implicit Riemannian optimistic online learning algorithm that operates on Hadamard manifolds. It maintains two iterates: a primary optimistic iterate updated using a hint function, and a secondary conservative iterate updated using the true loss. The algorithm solves an inexact implicit update by minimizing the regularized loss function ℓ_t(x) + (1/2η)d(x, x_t)², preserving geodesic convexity of subproblems. For min-max optimization, RIODA extends this framework by alternating between solving primal and dual subproblems using RIOD as a subroutine. The algorithm achieves regret bounds independent of geometric constants like minimum curvature and gradient oracle complexity of O(LR²/ε) for achieving ε duality gap.

## Key Results
- RIOD achieves regret bounds independent of geometric constants (like minimum curvature ζ), matching best known Euclidean rates up to logarithmic factors
- RIODA achieves near-optimal gradient oracle complexity of O(LR²/ε) for smooth g-convex min-max problems, the first Riemannian algorithm to remove dependence on geometric terms ζ except for log factors
- Experimental validation shows linear convergence on robust Karcher mean problems in SPD manifolds and hyperbolic spaces
- Algorithm handles in-manifold constraints without requiring strong assumptions about projection operators

## Why This Works (Mechanism)

### Mechanism 1: Preservation of Geodesic Convexity via Implicit Updates
The algorithm guarantees regret bounds and convergence by avoiding the linearization of loss functions, which preserves the geodesic convexity (g-convexity) of subproblems. Unlike standard Riemannian gradient methods that linearize losses (resulting in non-g-convex approximations), RIOD solves an inexact implicit update by minimizing the full regularized loss function ℓ_t(x) + (1/2η)d(x, x_t)². This ensures the subproblem remains g-convex and solvable within manifold constraints. The core assumption is that loss functions ℓ_t and hint functions ẽ_t must be g-convex and L-smooth on the constraint set X.

### Mechanism 2: Two-Step Optimism for Error Correction
The algorithm decouples the conservative "secondary" update from the optimistic "primary" action to prevent the accumulation of prediction errors common in single-step optimistic methods. RIOD maintains two iterates: x_t (secondary, updated using the true loss ℓ_{t-1}) and ẽ_x_t (primary, updated using the hint ẽ_ℓ_t). This structure allows the agent to leverage the optimistic hint for better actions while anchoring the update sequence to the actual loss history, ensuring the secondary iterates remain independent of prediction errors. The core assumption is that the hint function ẽ_ℓ_t provides a reasonable approximation of the next loss ℓ_t.

### Mechanism 3: Curvature-Independent Regularization
The algorithm achieves regret bounds independent of geometric constants (like minimum curvature ζ) by leveraging the strong convexity of the squared distance function on Hadamard manifolds. The regularizer (1/2η)d(x, x_t)² is (1/η)-strongly g-convex. By solving the implicit update exactly (or to sufficient precision), the analysis relies on the intrinsic contraction properties of the proximal step rather than comparing tangent spaces across potentially large distances where curvature terms usually scale. The core assumption is that the manifold M is a Hadamard manifold (complete, simply-connected, non-positive curvature).

## Foundational Learning

- **Hadamard Manifolds**: Why needed here - The theoretical guarantees rely heavily on the geometric properties of non-positive curvature (unique geodesics, non-expansive projection). Quick check question - Does the target manifold (e.g., Hyperbolic space, SPD matrices) have strictly non-positive sectional curvature everywhere?

- **Implicit Online Learning**: Why needed here - RIOD differs from standard online gradient descent by solving a proximal minimization problem rather than taking a gradient step. Quick check question - Can you define the difference between an "explicit" gradient step and an "implicit" proximal step in Euclidean space?

- **Geodesic Convexity (g-convexity)**: Why needed here - The mechanism requires the loss landscape to be convex along geodesics, not just straight lines, to guarantee the subproblems are solvable. Quick check question - Is the function f(x) = -log(det(X)) g-convex on the SPD manifold?

## Architecture Onboarding

- **Component map**: Inputs (compact g-convex constraint set X, loss oracle ℓ_t, hint oracle ẽ_ℓ_t) -> RIOD Core (Sub-solver: CRGD/PRGD, Precision Controller) -> Outputs (action ẽ_x_t played at round t)

- **Critical path**: The execution of the Sub-solver (CRGD/PRGD) at every iteration. The theoretical validity depends on achieving the specific precision ε_t defined in Algorithm 1 efficiently.

- **Design tradeoffs**:
  - CRGD vs. PRGD for Subproblems: Using Composite RGD (CRGD) is computationally heavier per step but theoretically guarantees curvature-independent complexity (Õ(1) oracle calls). Using Projected RGD (PRGD) is simpler to implement but introduces a dependency on geometric constants (Õ((Lη + ζD)ζR̃)).
  - Hint Choice: A "perfect" hint (ẽ_ℓ_t = ℓ_t) drives regret to zero (static case), while a poor hint degrades performance to standard online learning rates.

- **Failure signatures**:
  - Divergence of Sub-solver: If the sub-solver fails to find an ε_t-minimizer (Line 4/6) because the condition number is too high or the function is not sufficiently smooth, the regret bounds break.
  - Constraint Violation: If the implementation relies on unconstrained RGD (RIODA-RGD) but the iterates drift beyond the radius where g-convexity holds (B(x*, 8R)), convergence is not guaranteed.

- **First 3 experiments**:
  1. Robust Karcher Mean (SPD/Hyperbolic): Implement RIODA on the symmetric positive definite (SPD) manifold or hyperbolic space for the robust averaging problem (Eq. 59) to verify linear convergence.
  2. Sub-solver Comparison: Compare the wall-clock time and iteration count of using CRGD vs. PRGD as the subroutine for the implicit update to validate the trade-off between complexity and geometric dependence.
  3. Precision Sensitivity: Test the algorithm's stability by varying the precision ε_t of the sub-solver to see if looser tolerances degrade the final duality gap (validating Corollary 2).

## Open Questions the Paper Calls Out

### Open Question 1
Can RIOD and RIODA be extended to general Riemannian manifolds, particularly those with positive curvature? The paper states its contributions are specific to "Hadamard manifolds" and notes in the related work that "For manifolds with positive curvature, the metric projection is not a non-expansive operator," which complicates the analysis of enforcing constraints. This remains unresolved because the theoretical analysis relies heavily on the geometric properties of non-positive curvature (uniquely geodesic, non-expansive projections) to ensure regret bounds and constraint satisfaction.

### Open Question 2
Is it possible to design a Riemannian optimistic algorithm that effectively utilizes linearized hints instead of full-function implicit updates? The text notes that unlike the Euclidean setting, "a function linearized at a point x̄ ∈ M... is not g-convex," and therefore the authors "chose a two-step variant" requiring the minimization of the full loss function rather than a linearized approximation. This remains unresolved because linearized hints are computationally cheaper but break the geodesic convexity required for the theoretical guarantees in the current framework.

### Open Question 3
Can the Composite Riemannian Gradient Descent (CRGD) subproblems required by the optimal RIODA implementation be solved efficiently in practice? The authors state, "Note that implementing the CRGD update steps might be a hard computational problem," despite it offering the best theoretical gradient oracle complexity. This remains unresolved because there is a gap between the theoretical oracle complexity and the practical computational cost of solving the composite minimization subproblem at each step.

## Limitations
- Algorithm performance heavily depends on availability of good hint functions; poor hints degrade method to standard rates
- Theoretical regret bounds rely on achieving specific precision in sub-solver, but practical implementations may use fixed tolerances
- While method handles in-manifold constraints, projection operator P_X is assumed available but can be expensive or complex to compute in some manifolds

## Confidence

- **High Confidence**: Curvature-independent regret bounds for g-convex losses on Hadamard manifolds (Theorem 1, Corollary 2)
- **Medium Confidence**: Near-optimal gradient complexity for Riemannian min-max problems (Theorem 3, Corollary 4). The proof structure is sound, but constants and log factors require careful tracking.
- **Medium Confidence**: Experimental validation on SPD and hyperbolic spaces. The linear convergence plots are compelling, but the robustness to hyperparameter choices (e.g., PRGD step size) is not fully characterized.

## Next Checks

1. **Sensitivity Analysis**: Test RIODA's convergence with deliberately poor hint functions to quantify the degradation from the optimal O(LR²/ε) rate.

2. **Solver Precision Study**: Systematically vary the sub-solver's precision ε_t and measure its impact on the final duality gap to validate the theoretical tolerance requirements.

3. **Constraint Set Scaling**: Evaluate the algorithm's performance as the constraint set radius R changes, particularly testing the assumption that the optimal solution lies within B(x*, 8R).