---
ver: rpa2
title: Feeding Two Birds or Favoring One? Adequacy-Fluency Tradeoffs in Evaluation
  and Meta-Evaluation of Machine Translation
arxiv_id: '2509.20287'
source_url: https://arxiv.org/abs/2509.20287
tags:
- adequacy
- fluency
- translation
- metrics
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the tradeoff between adequacy and fluency
  in machine translation evaluation and meta-evaluation. The authors find that current
  evaluation metrics tend to lean toward adequacy, showing stronger correlation with
  adequacy scores than fluency scores.
---

# Feeding Two Birds or Favoring One? Adequacy-Fluency Tradeoffs in Evaluation and Meta-Evaluation of Machine Translation

## Quick Facts
- arXiv ID: 2509.20287
- Source URL: https://arxiv.org/abs/2509.20287
- Reference count: 17
- This work investigates the tradeoff between adequacy and fluency in machine translation evaluation and meta-evaluation

## Executive Summary
This paper addresses a critical issue in machine translation evaluation: the tendency of metrics to favor adequacy over fluency, and how this bias propagates to meta-evaluation. The authors demonstrate that current evaluation metrics correlate more strongly with adequacy than fluency scores, and more importantly, reveal that meta-evaluation datasets are similarly skewed due to their composition of translation systems. To address this problem, they propose a novel method for synthesizing translation systems with controlled variance in adequacy and fluency, enabling more balanced meta-evaluation. Using both pairwise accuracy (PA) and soft pairwise accuracy (SPA) metrics, the study analyzes several contemporary translation metrics and finds that most lean toward adequacy, with MetricX variants showing relatively more balanced behavior compared to Comet variants.

## Method Summary
The authors propose a method to synthesize translation systems with desired variance in adequacy and fluency scores for meta-evaluation. This involves creating synthetic systems through targeted editing that specifically manipulate either adequacy or fluency while controlling for the other dimension. The meta-evaluation is conducted using both pairwise accuracy (PA) and soft pairwise accuracy (SPA) metrics, which compare how well different evaluation metrics align with human judgments of translation quality. The study analyzes several contemporary translation metrics, including Comet and MetricX variants, to assess their balance between adequacy and fluency correlation.

## Key Results
- Current evaluation metrics show stronger correlation with adequacy scores than fluency scores
- Meta-evaluation datasets are skewed toward adequacy due to the composition of systems in these datasets
- Most contemporary metrics lean toward adequacy, with MetricX variants showing relatively more balanced behavior compared to Comet variants

## Why This Works (Mechanism)
The paper's approach works by recognizing that the tradeoff between adequacy and fluency is not just a property of evaluation metrics but is embedded in the meta-evaluation process itself. By creating synthetic systems with controlled variance in adequacy and fluency, the authors can isolate and measure how different metrics balance these two aspects of translation quality. This synthetic approach allows for more targeted meta-evaluation that can reveal biases that would be difficult to detect using traditional methods based on real translation systems alone.

## Foundational Learning
- **Adequacy**: The degree to which a translation preserves the meaning of the source text. Why needed: Fundamental dimension of translation quality that must be measured separately from fluency. Quick check: Can be evaluated by comparing semantic content preservation between source and translation.
- **Fluency**: The degree to which a translation reads naturally in the target language. Why needed: Essential for user experience and comprehension, independent of semantic accuracy. Quick check: Can be assessed by native speakers for grammatical correctness and naturalness.
- **Meta-evaluation**: The process of evaluating how well evaluation metrics correlate with human judgments. Why needed: Ensures that automatic metrics actually measure what they claim to measure. Quick check: Correlation between metric scores and human quality assessments.
- **Pairwise accuracy (PA)**: A meta-evaluation metric that measures how often an evaluation metric correctly ranks two translations relative to human preferences. Why needed: Provides a direct measure of metric quality in distinguishing better from worse translations. Quick check: Percentage of correctly ordered translation pairs.
- **Synthetic systems**: Artificially generated translation systems with controlled properties. Why needed: Enables systematic study of metric behavior under controlled conditions. Quick check: Correlation with human judgments in controlled experiments.

## Architecture Onboarding
The architecture involves a pipeline from source text through synthetic system generation to meta-evaluation. Source text is processed through targeted editing rules that create variants emphasizing either adequacy or fluency. These synthetic translations are then evaluated by different metrics (Comet, MetricX variants, etc.) and compared against human judgments using PA and SPA metrics. The critical path involves: Source Text -> Targeted Editing (Adequacy/Fluency variants) -> Synthetic Systems -> Metric Evaluation -> Human Judgment Correlation (PA/SPA). Design tradeoffs include the complexity of targeted editing rules versus the need for controllable variance. Failure signatures would include poor correlation between synthetic and real systems, or metrics showing extreme bias toward one dimension. Three first experiments would be: 1) Testing targeted editing rules on a small set of sentences, 2) Comparing correlation of synthetic vs real systems with human judgments, 3) Running initial PA/SPA calculations on a subset of metrics.

## Open Questions the Paper Calls Out
None

## Limitations
- The assumption that synthetic systems accurately represent real translation behavior may not hold across all contexts
- The study focuses primarily on English-to-other-language translation directions, potentially limiting generalizability
- The targeted editing approach may not capture the full complexity of adequacy and fluency as judged by human evaluators

## Confidence
- High confidence: The observation that current evaluation metrics show stronger correlation with adequacy than fluency scores
- Medium confidence: The finding that meta-evaluation datasets are skewed toward adequacy due to system composition
- Medium confidence: The proposed synthetic system generation method's effectiveness in creating balanced meta-evaluation conditions

## Next Checks
1. Validate the synthetic system approach by applying it to additional language pairs and translation directions not covered in the original study, comparing results with traditional meta-evaluation methods
2. Test the robustness of the proposed meta-evaluation method by applying it to metrics beyond those studied in the paper, particularly newer metrics developed after the study period
3. Conduct human evaluation studies where professional translators assess the synthetic systems to verify that the targeted editing approach produces systems that match the intended adequacy-fluency characteristics