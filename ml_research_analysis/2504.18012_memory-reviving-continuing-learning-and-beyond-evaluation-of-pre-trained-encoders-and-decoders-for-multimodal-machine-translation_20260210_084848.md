---
ver: rpa2
title: 'Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained
  Encoders and Decoders for Multimodal Machine Translation'
arxiv_id: '2504.18012'
source_url: https://arxiv.org/abs/2504.18012
tags:
- translation
- pre-trained
- visual
- language
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates the role of pre-trained
  encoders and decoders in multimodal machine translation (MMT). The authors conduct
  experiments across English-German and English-French translation tasks using Multi30K
  and CoMMuTE datasets, testing various combinations of pre-trained components under
  different training strategies (freezing, fine-tuning, or training from scratch).
---

# Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained Encoders and Decoders for Multimodal Machine Translation

## Quick Facts
- **arXiv ID**: 2504.18012
- **Source URL**: https://arxiv.org/abs/2504.18012
- **Reference count**: 12
- **Primary result**: Decoder-side pre-training consistently improves multimodal machine translation, while encoder-side benefits depend on visual-text alignment quality.

## Executive Summary
This study systematically evaluates how pre-trained encoders and decoders affect multimodal machine translation (MMT) performance. Through controlled experiments on Multi30K and CoMMuTE datasets across English-German and English-French translation tasks, the authors demonstrate that pre-trained decoders consistently improve translation quality regardless of architecture, while pre-trained encoders show variable benefits depending on visual-text alignment quality. The research reveals that decoder-only models like Qwen2.5 and LLaMA3.2 are particularly robust at incorporating visual information as contextual cues, even when visual-text pairs are mismatched. These findings suggest that decoder-side pre-training is essential for MMT performance, while effective visual grounding depends more on cross-modal alignment quality than architectural capacity.

## Method Summary
The study uses a unified MMT framework with CLIP-ViT-Large/14 as the frozen visual encoder and tests various text encoder/decoder combinations including Transformer variants, T5, mBART, Qwen2.5, and LLaMA3.2. Experiments are conducted on Multi30K (En-De, En-Fr; 29K train, 1,014 val, Test2016/Test2017/MSCOCO test sets) and CoMMuTE datasets (155 ambiguous sentences). Models are trained using full fine-tuning on 2x RTX 4090 GPUs until convergence, with performance evaluated using BLEU, METEOR, and COMET metrics. The study systematically varies training strategies including freezing, fine-tuning, and training from scratch to isolate the contribution of pre-trained components.

## Key Results
- Pre-trained decoders consistently improve translation quality across all metrics (BLEU, METEOR, COMET) regardless of architecture
- Pre-trained encoders provide variable benefits that depend on the quality of visual-text alignment
- Decoder-only models (Qwen2.5, LLaMA3.2) demonstrate particular robustness in handling visual information as contextual cues
- Mismatched visual-text pairs degrade performance more severely for encoder-pretrained models (2-6 point COMET drops) than decoder-only models

## Why This Works (Mechanism)
The study reveals that pre-trained decoders are crucial for MMT because they provide rich language understanding and generation capabilities that effectively integrate visual context. Decoder-only models show robustness to visual-text misalignment because they can treat visual information as supplementary context rather than relying on precise cross-modal alignment. The effectiveness of pre-trained components varies based on how well the model can leverage visual information, with decoder-side knowledge proving more universally beneficial than encoder-side representations for the translation task.

## Foundational Learning
- **Cross-modal alignment**: Understanding how visual and textual features are integrated in multimodal systems
  - Why needed: Critical for MMT performance as misalignment degrades results
  - Quick check: Compare performance when shuffling visual-text pairs
- **Pre-trained component utilization**: How frozen vs. fine-tuned pre-trained models contribute to downstream tasks
  - Why needed: Determines optimal training strategy for different model components
  - Quick check: Ablate pre-trained components to measure performance impact
- **Modality fusion mechanisms**: Methods for combining visual and textual features in neural networks
  - Why needed: Affects how effectively visual information enhances translation
  - Quick check: Test different fusion approaches (attention, gating, concatenation)

## Architecture Onboarding
- **Component map**: CLIP-ViT-Large/14 (frozen) -> Text Encoder/Decoder -> Cross-Attention Fusion -> Translation Output
- **Critical path**: Visual features → cross-attention layers → decoder → translated text
- **Design tradeoffs**: 
  - Encoder-pretrained models offer strong language understanding but are sensitive to alignment quality
  - Decoder-only models provide robust contextual processing but may lack explicit visual grounding
  - Full fine-tuning vs. parameter-efficient training strategies
- **Failure signatures**: 
  - 2-6 point COMET drops when visual-text pairs are mismatched for encoder-pretrained models
  - Visual modality degrades performance when cross-modal alignment is poor
  - Decoder-only models maintain performance better under misalignment
- **First experiments**:
  1. Compare full fine-tuning vs. freezing pre-trained components on translation quality
  2. Test different modality fusion mechanisms (attention depth, gating strategies)
  3. Systematically degrade visual-text alignment quality to measure sensitivity

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The study lacks detail on the exact modality fusion mechanism and training hyperparameters, making exact reproduction difficult
- The underlying reasons for asymmetric benefits between encoder and decoder pre-training remain incompletely explained
- Alternative grounding strategies for improving cross-modal alignment were not explored
- The prompt engineering approach for decoder-only models incorporating visual features is unclear

## Confidence
- **High**: Pre-trained decoders consistently improve MMT performance
- **Medium**: Encoder pre-training benefits depend on alignment quality
- **Medium**: Decoder-only models are more robust to visual-text misalignment

## Next Checks
1. Conduct ablation studies varying the modality fusion mechanism (cross-attention depth, gating strategies) to isolate its impact on performance
2. Test the alignment sensitivity hypothesis by systematically degrading visual-text alignment quality and measuring performance degradation across different model configurations
3. Implement and evaluate alternative prompt engineering approaches for decoder-only models to optimize visual feature integration