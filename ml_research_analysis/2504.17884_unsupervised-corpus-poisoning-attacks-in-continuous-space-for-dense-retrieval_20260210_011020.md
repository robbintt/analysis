---
ver: rpa2
title: Unsupervised Corpus Poisoning Attacks in Continuous Space for Dense Retrieval
arxiv_id: '2504.17884'
source_url: https://arxiv.org/abs/2504.17884
tags:
- adversarial
- attack
- retrieval
- document
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses unsupervised corpus poisoning attacks in dense
  retrieval, where an adversary injects malicious documents to compromise ranking
  performance without prior knowledge of query distribution. The authors propose an
  optimization method that operates directly in the continuous embedding space, using
  a reconstruction model to recover documents from token embeddings and a perturbation
  model that maintains geometric distance between original and adversarial embeddings
  while maximizing token-level dissimilarity.
---

# Unsupervised Corpus Poisoning Attacks in Continuous Space for Dense Retrieval

## Quick Facts
- **arXiv ID**: 2504.17884
- **Source URL**: https://arxiv.org/abs/2504.17884
- **Reference count**: 40
- **Key outcome**: Continuous-space corpus poisoning generates adversarial documents 4x faster than HotFlip with significantly lower perplexity (188.9 vs 6032.6) while achieving comparable attack success rates up to 0.984 in white-box settings and superior transferability in black-box attacks across multiple datasets.

## Executive Summary
This paper introduces an unsupervised corpus poisoning method for dense retrieval that operates directly in continuous embedding space, achieving faster and more coherent adversarial document generation compared to token-level approaches like HotFlip. The method uses a reconstruction model to recover documents from embeddings and a perturbation model that maintains geometric distance while maximizing token-level dissimilarity. Without prior knowledge of query distribution, the approach achieves competitive attack success rates by targeting corpus-central documents through clustering. The method demonstrates superior transferability across multiple retrieval architectures while producing adversarial examples that are harder to detect through perplexity-based filtering.

## Method Summary
The method trains a reconstruction model to map token-level embeddings back to text, then optimizes a perturbation model in continuous space to generate adversarial documents. The reconstruction model (BERT-large decoder) is trained once on MS MARCO corpus to achieve high accuracy. For each target document, a document-specific 3-layer MLP perturbation model is trained to minimize MSE between original and adversarial embeddings while maximizing token-level dissimilarity via cross-entropy loss, with a clipping threshold λ=5. The adversarial document is decoded via argmax from the reconstruction model and injected into the corpus to evaluate attack success rates.

## Key Results
- **Speed improvement**: 119.9s per document vs 512.1s for HotFlip (4x faster)
- **Perplexity reduction**: 188.9 vs 6032.6, making attacks harder to detect
- **Attack success**: ASR up to 0.984 in white-box settings with superior black-box transferability across SimLM, DPR, ColBERTv2

## Why This Works (Mechanism)

### Mechanism 1
Operating in continuous embedding space enables faster, more coherent adversarial document generation by allowing gradient-based optimization over entire document embeddings simultaneously rather than iterative token substitution. The reconstruction model maps contextual token-level embeddings back to text, enabling this continuous-space optimization. The core assumption is that the reconstruction model generalizes sufficiently to map perturbed embeddings to coherent text, validated out-of-domain on NQ with 0.93+ F1. Break condition: if reconstruction accuracy degrades significantly on out-of-distribution corpora or longer documents, adversarial outputs may become incoherent.

### Mechanism 2
A dual-objective loss produces adversarial documents that are embedding-similar but token-dissimilar to targets. The perturbation model minimizes MSE between original and adversarial embeddings while maximizing cross-entropy loss, with combined loss L_Attack = L_MSE - min(L_CE, λ) where λ caps semantic similarity. The core assumption is that these two objectives can be simultaneously optimized without collapse, with λ=5 empirically balancing tradeoffs. Break condition: if λ is too low, adversarial documents may share too many tokens with targets; if too high, embedding similarity may degrade, reducing attack success.

### Mechanism 3
Unsupervised attacks succeed by targeting corpus-central documents identified through K-means clustering, hypothesizing that centrality in embedding space correlates with retrieval frequency across unknown queries. All documents in each cluster can be considered a ranking sorted by distance to cluster centroid. Break condition: if query distribution is highly skewed or corpus clusters don't align with query topics, centrality-based targeting may underperform.

## Foundational Learning

- **Dense Retrieval Bi-Encoders**: Needed because the attack targets architectures that encode queries and documents independently into shared embedding space via dot product or cosine similarity. Quick check: Can you explain why attacking the [CLS] token embedding affects retrieval ranking?

- **Vec2Text / Embedding Inversion**: Needed because the reconstruction model is a Vec2Text-style inverter that predicts tokens from embeddings, enabling continuous-space optimization. Quick check: Given a document embedding e_d ∈ R^{|d|×768}, what does the reconstruction model output?

- **Adversarial Transferability**: Needed because black-box attacks rely on transfer—adversarial documents generated against SimLM must fool unknown models. Quick check: Why would lower perplexity improve transferability across retrieval models?

## Architecture Onboarding

- **Component map**: Frozen retrieval encoder E(·) -> Reconstruction model R(·) -> Perturbation model φ(·) -> Loss combiner -> Adversarial document
- **Critical path**: 1) Train reconstruction model R on MS MARCO (5 epochs, lr=1e-5) 2) For each target document: initialize φ, optimize L_Attack for 3000 epochs 3) Decode adversarial document via argmax on P(d̃|e_d; φ) 4) Inject into corpus, evaluate ASR/Top@k
- **Design tradeoffs**: λ controls token dissimilarity vs embedding similarity balance; document-specific φ is lightweight but requires training per target; reconstruction model trained once per retriever but accuracy drops on out-of-domain corpora
- **Failure signatures**: High BLEU (>0.3) indicates adversarial document too similar to target—increase λ; Low ASR (<0.5) indicates embedding drifted too far—reduce λ or increase training epochs; High perplexity (>1000) indicates reconstruction failed—verify R was trained on correct retriever
- **First 3 experiments**: 1) Validate reconstruction model on NQ corpus: confirm F1 > 0.93 before attacking 2) Top-1 white-box attack on TREC DL 19: compare ASR, perplexity, and time vs HotFlip baseline 3) Black-box transfer to DPR/ColBERTv2: verify ASR exceeds HotFlip on at least 3 datasets

## Open Questions the Paper Calls Out

### Open Question 1
Can adversarial training using the proposed method's generated documents systematically improve dense retrieval robustness without degrading retrieval effectiveness? The authors briefly discuss that computational efficiency enables adversarial training possibility but present limited experimentation showing only 7.9% relative reduction in ASR. What evidence would resolve it: Large-scale adversarial training experiments across multiple retrieval architectures measuring both attack success rates and standard retrieval metrics on diverse benchmarks.

### Open Question 2
Why do different retrieval model architectures exhibit substantially different robustness profiles against corpus poisoning attacks? Figure 5 shows DPR and RankLLaMA demonstrate higher robustness than SimLM re-ranker and ColBERTv2 when attacked via transfer, but provides no theoretical or empirical explanation. What evidence would resolve it: Systematic ablation studies correlating specific architectural components with attack susceptibility using probing tasks to identify vulnerability-inducing representations.

### Open Question 3
Can detection methods beyond perplexity-based filtering identify the low-perplexity adversarial documents generated by this approach? The paper claims their method's low perplexity makes attacks more difficult to detect by perplexity-based filtering but doesn't evaluate against other detection strategies. What evidence would resolve it: Evaluation against diverse detection methods including embedding distribution analysis, statistical stylometry, neural classifiers trained to distinguish adversarial from natural text, and human inspection studies.

## Limitations

- Reconstruction model generalization to out-of-domain corpora and specialized domains remains untested beyond NQ corpus
- Computational cost analysis doesn't amortize one-time reconstruction model training across all targets
- Transferability assumptions may not hold for retrievers with different sensitivity to token-level changes
- Limited evaluation of performance on extremely long documents or specialized domain corpora

## Confidence

**High Confidence**: 
- Continuous-space optimization approach significantly outperforms HotFlip in generation speed (4x faster) and perplexity reduction (6032.6 → 188.9)
- Reconstruction model generalizes well to out-of-domain corpora (NQ: 0.99 accuracy, 0.93 F1)
- Dual-objective loss formulation effectively balances embedding similarity and token dissimilarity

**Medium Confidence**:
- Unsupervised attacks targeting cluster centroids achieve competitive ASR without query knowledge
- Method maintains semantic irrelevance (BLEU scores) while preserving attack success
- Transferability performance exceeds HotFlip baselines across multiple retrievers

**Low Confidence**:
- Performance on retrievers other than SimLM, Contriever, DPR, and ColBERTv2
- Behavior on extremely long documents or specialized domain corpora
- Impact of varying λ values beyond the reported λ=5 across all datasets

## Next Checks

1. **Cross-Retriever Validation**: Test reconstruction model accuracy and attack success rates on all five additional retrievers (E5-base-v2, TAS-B, DRAGON+, RetroMAE) across all five attack datasets to verify consistent performance claims.

2. **Robustness to Document Length**: Generate adversarial documents at varying lengths (50, 128, 256 tokens) and measure how reconstruction accuracy, perplexity, and ASR degrade with increasing document size to identify operational limits.

3. **Sensitivity Analysis for λ**: Systematically vary λ from 1 to 10 across all datasets and retrievers, measuring the tradeoff between BLEU scores (semantic relevance) and ASR (attack success) to identify optimal parameter settings for different use cases.