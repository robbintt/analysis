---
ver: rpa2
title: 'Rectify Evaluation Preference: Improving LLMs'' Critique on Math Reasoning
  via Perplexity-aware Reinforcement Learning'
arxiv_id: '2511.10303'
source_url: https://arxiv.org/abs/2511.10303
tags:
- llms
- correct
- perplexity
- preference
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of imbalanced evaluation preference
  in Large Language Models (LLMs) critiquing mathematical reasoning, where LLMs tend
  to judge solutions with lower perplexity as correct. The authors propose a novel
  perplexity-aware reinforcement learning algorithm (Group Relative Policy Optimization)
  that rectifies this preference by using perplexity as a "baton" to rescale advantage
  distributions during training, encouraging the model to explore counter-preference
  trajectories.
---

# Rectify Evaluation Preference: Improving LLMs' Critique on Math Reasoning via Perplexity-aware Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.10303
- Source URL: https://arxiv.org/abs/2511.10303
- Reference count: 9
- Key outcome: Proposes perplexity-aware reinforcement learning algorithm to address imbalanced evaluation preference in LLMs critiquing mathematical reasoning, achieving state-of-the-art performance on self-constructed OPS benchmark and existing critic benchmarks.

## Executive Summary
This paper addresses a critical issue in Large Language Models (LLMs) critiquing mathematical reasoning: the tendency to favor solutions with lower perplexity as correct, leading to imbalanced evaluation preference. The authors introduce a novel perplexity-aware reinforcement learning approach that uses perplexity as a "baton" to rescale advantage distributions during training. By constructing a One-to-many Problem-Solution benchmark and implementing class-level loss aggregation, their method encourages exploration of counter-preference trajectories and achieves balanced optimization. Experimental results demonstrate significant improvements in critiquing mathematical reasoning while alleviating the identified bias.

## Method Summary
The authors propose a perplexity-aware reinforcement learning algorithm called Group Relative Policy Optimization that addresses imbalanced evaluation preference in LLM critique. The method introduces a "perplexity baton" mechanism that rescales advantage distributions during training, encouraging the model to explore solutions that deviate from the low-perplexity preference. A One-to-many Problem-Solution benchmark is constructed to quantify this bias, and class-level loss aggregation ensures balanced optimization across different solution types. The approach combines RL-based training with perplexity-aware reward shaping to systematically rectify the evaluation preference while maintaining or improving critique performance.

## Key Results
- Successfully demonstrates existence of imbalanced evaluation preference where LLMs favor low-perplexity solutions in mathematical reasoning critique
- Achieves state-of-the-art performance on both self-constructed OPS benchmark and existing critic benchmarks
- Shows effective alleviation of evaluation bias through perplexity-aware RL approach with class-level loss aggregation

## Why This Works (Mechanism)
The proposed method works by fundamentally restructuring how advantage distributions are calculated during reinforcement learning. By using perplexity as a "baton" to rescale advantages, the algorithm explicitly counters the model's natural preference for low-perplexity solutions. This mechanism forces the model to explore and value higher-perplexity solutions that may actually be correct, addressing the core bias. The class-level loss aggregation ensures balanced optimization across different solution categories, preventing the model from overfitting to any particular preference pattern. Together, these components create a training framework that systematically rectifies evaluation bias while maintaining strong critique performance.

## Foundational Learning
- Perplexity-based evaluation bias: Understanding how LLMs naturally favor low-perplexity outputs is crucial for identifying the core problem. Quick check: Measure perplexity distribution across correct vs incorrect solutions in existing datasets.
- Reinforcement learning with advantage rescaling: The perplexity baton mechanism relies on modifying standard RL advantage calculations. Quick check: Compare standard vs rescaled advantage distributions during training.
- Class-level loss aggregation: Ensures balanced optimization across different solution types and prevents overfitting to preference patterns. Quick check: Monitor loss distribution across classes during training.

## Architecture Onboarding

Component Map:
Problem Input -> Perplexity Calculation Module -> Advantage Rescaling (Baton) -> Group Relative Policy Optimization -> Critique Output

Critical Path:
The critical path flows from problem input through perplexity calculation, to advantage rescaling using the perplexity baton, through the policy optimization step, and finally to the critique output. The perplexity baton mechanism represents the key innovation that distinguishes this approach from standard RL methods.

Design Tradeoffs:
- Computational cost vs bias correction effectiveness: The perplexity-aware approach adds computational overhead but provides significant bias reduction
- Model complexity vs training stability: The rescaling mechanism adds complexity but helps prevent preference overfitting
- Benchmark specificity vs generalizability: The OPS benchmark is tailored but may limit real-world applicability

Failure Signatures:
- If the perplexity baton is too aggressive, the model may start favoring high-perplexity solutions indiscriminately
- Insufficient class-level aggregation may lead to continued preference imbalance in certain solution categories
- Poor scaling of the advantage rescaling may cause training instability or convergence issues

3 First Experiments:
1. Baseline comparison: Evaluate standard RL critique method vs perplexity-aware approach on OPS benchmark
2. Ablation study: Test performance with and without the perplexity baton mechanism
3. Class distribution analysis: Compare solution preference distributions across different problem categories

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation relies primarily on self-constructed datasets, potentially limiting generalizability to real-world mathematical reasoning scenarios
- Limited direct comparisons with other perplexity-aware methods and lack of detailed ablation studies on the baton mechanism's effectiveness
- Computational costs and training efficiency implications of the perplexity-aware RL approach are not discussed

## Confidence
High confidence: The existence of imbalanced evaluation preference in LLMs critiquing mathematical reasoning, as demonstrated through empirical observations on the OPS benchmark.

Medium confidence: The effectiveness of the proposed perplexity-aware RL approach in alleviating the identified bias, based on experimental results on constructed datasets. The generalizability of these findings to broader contexts requires further validation.

Low confidence: Claims about achieving state-of-the-art performance in critiquing mathematical reasoning, given the limited scope of comparisons and potential dataset-specific optimizations.

## Next Checks
1. Conduct extensive testing of the proposed method on diverse, real-world mathematical reasoning datasets beyond the self-constructed OPS benchmark to evaluate generalizability and robustness.

2. Perform detailed ablation studies to isolate the impact of the perplexity baton mechanism and other components of the proposed approach on overall performance. Compare against other perplexity-aware methods in the literature.

3. Investigate the computational efficiency and scalability of the perplexity-aware RL approach compared to standard critique methods. Assess training times, memory requirements, and performance on larger models or more complex problem domains.