---
ver: rpa2
title: A Probabilistic Inference Scaling Theory for LLM Self-Correction
arxiv_id: '2508.16456'
source_url: https://arxiv.org/abs/2508.16456
tags:
- self-correction
- accuracy
- curve
- theory
- round
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes a probabilistic theory to model accuracy evolution\
  \ in multi-round LLM self-correction, deriving the formula $Acct = Upp - \u03B1\
  ^t(Upp - Acc0)$ to describe how accuracy changes across rounds. The theory decomposes\
  \ self-correction into confidence and critique capabilities, quantified by CL and\
  \ CS metrics."
---

# A Probabilistic Inference Scaling Theory for LLM Self-Correction

## Quick Facts
- arXiv ID: 2508.16456
- Source URL: https://arxiv.org/abs/2508.16456
- Reference count: 40
- Primary result: Derived formula Acc_t = Upp - α^t(Upp - Acc_0) accurately predicts accuracy evolution in multi-round LLM self-correction across 8 models and 8 datasets

## Executive Summary
This paper introduces a probabilistic theory that models how accuracy evolves during multi-round LLM self-correction. The theory decomposes self-correction into confidence (CL) and critique (CS) capabilities, deriving a closed-form formula that predicts accuracy at any round based on single-round estimates. Experiments validate the theory's predictions across diverse models and benchmarks, establishing an accuracy upper bound determined by the CL-CS balance. The work provides a theoretical foundation for understanding self-correction dynamics and offers practical insights for optimizing this inference-scaling technique.

## Method Summary
The method estimates accuracy evolution by sampling M responses per question and computing transition probabilities between rounds. For classification tasks, softmax probabilities over candidate labels are used; for generation tasks, response frequencies serve as estimates. The self-correction loop uses a "reask" prompt strategy, re-presenting the original question as the correction prompt. From round 1 accuracy data, CL (weighted average of P(correct|was_correct)) and CS (weighted average of P(correct|was_wrong)) are computed. These metrics feed into the theoretical formula Acc_t = Upp - α^t(Upp - Acc_0) to predict accuracy curves, which are compared against empirical multi-round results.

## Key Results
- Theoretical accuracy curve matches empirical results across 8 models and 8 datasets with high fidelity
- Accuracy upper bound Upp = CS/(1-CL+CS) is determined solely by CL-CS balance, independent of initial accuracy
- External verification simulations show potential for achieving 100% accuracy by increasing CL toward 1
- Self-correction failures occur when poor prompts cause accuracy to converge downward rather than upward

## Why This Works (Mechanism)

### Mechanism 1: Markovian State Transition Structure
- Claim: Multi-round self-correction follows a recursive accuracy formula derived from probabilistic state transitions between correct and incorrect answers.
- Mechanism: Each round, correct answers stay correct with probability CL (confidence), while incorrect answers become correct with probability CS (critique). This yields the recurrence: Acc_t = Acc_{t-1}·CL + (1-Acc_{t-1})·CS = (CL-CS)·Acc_{t-1} + CS, which solves to Acc_t = Upp - α^t(Upp - Acc_0).
- Core assumption: CL and CS are approximately constant across rounds for a given model-dataset-prompt combination (empirically validated in Appendix C, Figure 2).
- Evidence anchors:
  - [section 2.2-2.3] The derivation explicitly treats P(ai,t|ai,t-1) and P(ai,t|¬ai,t-1) as round-independent, denoted P^con_i and P^cri_i.
  - [section 3.2] Empirical curves across 8 datasets closely match theoretical predictions using single-round estimated parameters.
  - [corpus] Limited direct corpus support; related work (Boosting LLM Reasoning via Spontaneous Self-Correction) addresses self-correction empirically but lacks this theoretical framing.
- Break condition: If CL or CS drift substantially across rounds (non-stationary behavior), the predicted curve will diverge from empirical results.

### Mechanism 2: Confidence-Critique Trade-off Governs Convergence Ceiling
- Claim: The accuracy upper bound Upp = CS/(1-CL+CS) is determined solely by the balance between confidence and critique capabilities.
- Mechanism: High CL (retaining correct answers) alone is insufficient; low CS (weak error correction) caps the ceiling. Conversely, high CS with low CL causes correct-to-incorrect flips that prevent convergence to 100%.
- Core assumption: CL > CS typically holds (models more likely to retain correct answers than fix incorrect ones), ensuring 0 < α < 1 and bounded convergence.
- Evidence anchors:
  - [section 2.4, Corollary 1] Final converged accuracy is independent of initial accuracy Acc_0.
  - [Appendix E.1, Figure 3] Experiments manipulating Acc_0 from 0% to 100% all converge to the same Upp.
  - [corpus] No direct corpus evidence on CL-CS decomposition; this builds on Yang et al. (2024b).
- Break condition: If prompts cause CL ≤ CS (e.g., over-critiquing destroys correct answers), α ≥ 0 could cause non-convergence or accuracy decline.

### Mechanism 3: Oracle Verification as Theoretical Limit Case
- Claim: External verification that achieves CL=1 eliminates the confidence loss pathway, allowing convergence to 100% accuracy.
- Mechanism: When CL=1, the formula simplifies to Acc_t = 1 - (1-CS)^t(1-Acc_0), and as t→∞, Acc_t→1. This represents the best-case scenario where correct answers are never flipped.
- Core assumption: External feedback can approximate oracle verification by reducing false negatives (correct→incorrect transitions).
- Evidence anchors:
  - [Appendix E.3, Figure 5] Simulated CL=1 conditions show theoretical curves matching empirical data and substantially outperforming intrinsic self-correction.
  - [corpus] Rollout Roulette (2502.01618) explores inference-time scaling with reward models but does not explicitly model CL=1 limits.
- Break condition: Real external verifiers are imperfect; partial CL<1 means the 100% ceiling remains unreachable.

## Foundational Learning

- Concept: **Conditional Probability and Law of Total Probability**
  - Why needed here: The core derivation partitions accuracy based on whether prior answers were correct/incorrect (Equation 1).
  - Quick check question: Given P(correct|was_correct)=0.8 and P(correct|was_wrong)=0.3, what is P(correct) if prior accuracy was 0.6?

- Concept: **Geometric Sequence Convergence**
  - Why needed here: The term α^t decays geometrically, determining how quickly accuracy approaches Upp.
  - Quick check question: If α=0.7, after how many rounds does α^t fall below 0.1?

- Concept: **Markov Chain Stationary Distribution**
  - Why needed here: The two-state transition (correct↔incorrect) under constant CL/CS is a Markov chain; Upp is the stationary probability of "correct."
  - Quick check question: In a 2-state chain with P(stay in correct)=0.85 and P(flip to correct)=0.25, what is the long-run probability of being correct?

## Architecture Onboarding

- Component map:
  - Probability Estimator -> Metric Calculator -> Curve Predictor -> Self-Correction Loop

- Critical path:
  1. Run 1 round of self-correction on dataset with M≥5 samples per question.
  2. Estimate CL and CS from transition frequencies.
  3. Compute predicted curve and Upp; compare against multi-round empirical data to validate.

- Design tradeoffs:
  - More samples (M) improves probability estimates but increases cost.
  - Prompt choice strongly affects CL/CS balance; "reask" prompts used here, but "Are you sure?" can reduce CL (Section F, Figure 6).
  - Intrinsic self-correction has bounded Upp < 100%; external feedback can raise CL toward 1.

- Failure signatures:
  - Accuracy decreases across rounds → check if Upp < Acc_0 (poor prompt disrupting CL/CS balance).
  - Theoretical curve diverges from empirical → check CL/CS stability across rounds (non-stationarity).
  - High variance in estimates → increase M or use longer context for more stable sampling.

- First 3 experiments:
  1. **Baseline validation**: Run 5-round self-correction on GSM8K with Llama3-8B; estimate CL/CS from round 1 only; plot predicted vs. empirical curve.
  2. **Prompt sensitivity**: Compare "reask" vs. "Are you sure?" prompts; measure impact on CL, CS, and Upp.
  3. **Initial accuracy manipulation**: Force Acc_0 to 0%, 50%, 100% (per Appendix E.1) and verify all converge to same Upp.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the probabilistic theory be extended to model accuracy dynamics in other inference scaling settings such as long Chain-of-Thought reasoning and Monte Carlo Tree Search?
- Basis in paper: [explicit] The Limitations section states: "how performance improves in other inference scaling settings (e.g. long COT, MCTS) is still unknown, and we leave it to future work."
- Why unresolved: The current theory specifically models iterative self-correction where only the previous answer informs the next round. Long CoT and MCTS involve different computational structures and decision processes that may not follow the same recursive accuracy relationship.
- What evidence would resolve it: Successful derivation of analogous theoretical accuracy curves for CoT length scaling and MCTS depth/width scaling that match empirical observations across models and tasks.

### Open Question 2
- Question: How can prompts be systematically optimized to maximize the theoretical upper bound (Upp) and prevent self-correction failure?
- Basis in paper: [explicit] The Discussion section notes: "A simple approach inspired by our theory could be testing various prompts and selecting the one with the highest Upp, and we leave further explorations in avoiding this failure to future work."
- Why unresolved: While the theory identifies Upp as determined by CL and CS, the relationship between prompt design and these metrics remains unexplored. Poor prompts can cause accuracy to converge downward.
- What evidence would resolve it: Development of prompt selection/optimization methods that consistently achieve higher Upp values, validated across multiple models and datasets.

### Open Question 3
- Question: Does the theory generalize to multi-step reasoning tasks and external self-correction settings with diverse feedback sources?
- Basis in paper: [explicit] The Limitations section states: "We only experimentally validate our theory on 8 models and 8 datasets in the intrinsic self-correction setting, leaving more verification experiments on more datasets (e.g. multi-step reasoning tasks) and setting (e.g. external self-correction) for future work."
- Why unresolved: Multi-step reasoning may involve interdependent corrections across reasoning steps, and external feedback sources vary in quality and information content, potentially violating the constant CL/CS assumption.
- What evidence would resolve it: Empirical validation showing theoretical curves fit multi-step reasoning benchmarks (e.g., GSM8K with step-wise correction) and external correction with various feedback types (code execution, retrieval, human feedback).

## Limitations

- The theory assumes constant CL and CS across rounds, which may not hold for complex reasoning tasks or extended self-correction sessions.
- The formula's predictions are validated primarily on short-to-medium round counts; long-horizon extrapolation may accumulate drift errors.
- The theoretical 100% accuracy ceiling via external verification (CL=1) is practically unattainable with real-world imperfect feedback mechanisms.
- The theory has been tested mainly on text-only benchmarks and may not generalize to multi-modal or non-English tasks.

## Confidence

- Markovian assumption validity: **High** confidence for short-to-medium rounds, **Medium** for long-horizon extrapolation
- Accuracy curve predictions: **High** confidence when CL/CS stability holds, **Medium** when drift occurs
- 100% accuracy via external verification: **Low** confidence in practice due to imperfect feedback
- CL/CS decomposition insights: **High** confidence in the theoretical framework, **Medium** in practical estimation variance

## Next Checks

1. Test CL/CS stability across 10+ rounds on complex reasoning tasks (e.g., GPQA, AIME) to verify the Markovian assumption holds under extended self-correction.
2. Implement external verifier with varying CL values (simulated oracle, reward model, human feedback) to empirically measure the accuracy ceiling improvements predicted by CL=1 theory.
3. Evaluate the theory's predictions on non-English languages and multi-modal tasks where self-correction dynamics may differ substantially from text-only benchmarks.