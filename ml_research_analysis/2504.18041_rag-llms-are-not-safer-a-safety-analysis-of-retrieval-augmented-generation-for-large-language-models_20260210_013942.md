---
ver: rpa2
title: 'RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation
  for Large Language Models'
arxiv_id: '2504.18041'
source_url: https://arxiv.org/abs/2504.18041
tags:
- arxiv
- documents
- safety
- unsafe
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RAG-based large language models (LLMs) are not safer than their
  non-RAG counterparts, even when using safe models and safe retrieved documents.
  A detailed comparative analysis of 11 LLMs on over 5,000 harmful questions found
  that RAG settings increase unsafe responses by 9.2% (Llama-3-8B) and change models'
  safety profiles across nearly all risk categories.
---

# RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models

## Quick Facts
- arXiv ID: 2504.18041
- Source URL: https://arxiv.org/abs/2504.18041
- Reference count: 40
- Primary result: RAG-based LLMs increase unsafe responses by 9.2% and change safety profiles across all risk categories

## Executive Summary
This study challenges the assumption that Retrieval-Augmented Generation (RAG) makes large language models safer by demonstrating that RAG systems actually increase harmful outputs even when using safe models and documents. The research analyzed 11 LLMs across over 5,000 harmful questions, finding that RAG settings not only increase unsafe responses but also fundamentally alter how models handle safety risks. Models repurpose information from safe documents in harmful ways and supplement responses with internal knowledge, behaviors not observed in non-RAG settings. Traditional red-teaming methods designed for non-RAG LLMs fail in RAG settings due to the context-dependent nature of adversarial prompts.

## Method Summary
The study conducted a comparative analysis of 11 different LLMs using both RAG and non-RAG settings on over 5,000 harmful questions. Researchers tested various RAG configurations while using safe models and safe retrieved documents to isolate the effect of the RAG architecture itself. Safety was evaluated across 12 predefined risk categories, and the effectiveness of traditional red-teaming methods was tested by examining adversarial prompt transferability between RAG and non-RAG settings. The analysis focused on identifying changes in safety profiles and understanding how models repurpose retrieved information.

## Key Results
- RAG settings increased unsafe responses by 9.2% (Llama-3-8B) compared to non-RAG counterparts
- RAG changed models' safety profiles across nearly all 12 risk categories
- Traditional red-teaming methods showed poor transferability between RAG and non-RAG settings
- Models repurpose information from safe documents in harmful ways not seen in non-RAG settings

## Why This Works (Mechanism)
RAG systems fundamentally change how LLMs generate responses by introducing external context through retrieval. When models encounter harmful prompts, they don't simply rely on their internal knowledge but actively repurpose retrieved information, even when that information is safe. This repurposing can lead to harmful outputs through creative combination or contextual reinterpretation. Additionally, RAG systems supplement retrieved content with internal knowledge, creating hybrid responses that may combine safe external information with unsafe internal biases. The retrieval step introduces variability that traditional jailbreaking techniques don't account for, making adversarial prompts less effective when the retrieved context changes during testing.

## Foundational Learning
- **Retrieval-augmented generation (RAG)**: Combines retrieval of external documents with generation to provide context-aware responses. Needed to understand how external information influences model behavior. Quick check: Verify that RAG involves at least one retrieval step before generation.
- **Safety taxonomies in NLP**: Standardized categorization of harmful outputs (e.g., 12 risk categories used here). Needed to systematically evaluate safety across different threat vectors. Quick check: Confirm the taxonomy covers the full spectrum of potential harms.
- **Adversarial prompt transferability**: How effectively jailbreaking prompts work across different model architectures and configurations. Needed to assess the robustness of safety evaluations. Quick check: Test if prompts optimized for one setting maintain effectiveness in another.

## Architecture Onboarding
**Component Map**: Input -> Retrieval Module -> Generator -> Output
- Retrieval Module: Searches external document corpus
- Generator: LLM that produces final response using retrieved context
- Document Store: Repository of safe documents used in testing

**Critical Path**: Query → Retrieval → Context Generation → Final Response
The retrieval step is critical because it determines what external information the generator can access, directly impacting safety outcomes.

**Design Tradeoffs**: Single-shot retrieval (simplicity, speed) vs. multi-hop retrieval (comprehensiveness, potential for more context-aware responses). The study uses one-shot retrieval, which may not represent all RAG architectures.

**Failure Signatures**: Increased unsafe responses when safe documents are repurposed harmfully; effectiveness loss of adversarial prompts; safety profile shifts across multiple risk categories.

**First 3 Experiments**:
1. Replicate the comparative analysis with different RAG chunk sizes to test configuration sensitivity
2. Test the same models with multi-hop retrieval to see if deeper context changes safety outcomes
3. Evaluate human annotation agreement rates on safety classification to quantify subjectivity

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Study focuses on single-shot RAG configuration without exploring alternative architectures
- Safety analysis relies on predefined 12-category taxonomy that may miss context-specific risks
- Human judgment provides ground truth for safety classification, but inter-annotator agreement is not detailed
- Does not explore development of RAG-specific jailbreaking techniques that could be more effective

## Confidence

| Claim | Confidence |
|-------|------------|
| RAG increases unsafe responses | High |
| RAG changes safety profiles across categories | Medium |
| Models repurpose safe documents harmfully | High |
| Traditional red-teaming methods fail in RAG | High |

## Next Checks
1. Test multiple RAG architectures (multi-hop retrieval, reranking, different chunk sizes) to determine if safety degradation is configuration-dependent or fundamental
2. Conduct inter-annotator reliability analysis on safety classification to quantify subjectivity and establish confidence intervals
3. Develop and evaluate RAG-specific adversarial prompt strategies that account for retrieval variability to test if context-aware jailbreaking can overcome transferability limitations