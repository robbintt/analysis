---
ver: rpa2
title: A Dataset and Benchmark for Consumer Healthcare Question Summarization
arxiv_id: '2512.23637'
source_url: https://arxiv.org/abs/2512.23637
tags:
- question
- questions
- summary
- original
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CHQ-Summ, a dataset for consumer healthcare
  question summarization. The dataset contains 1,507 domain-expert annotated question-summary
  pairs derived from the Yahoo!
---

# A Dataset and Benchmark for Consumer Healthcare Question Summarization

## Quick Facts
- arXiv ID: 2512.23637
- Source URL: https://arxiv.org/abs/2512.23637
- Reference count: 40
- Primary result: Introduced CHQ-Summ dataset with 1,507 expert-annotated question-summary pairs; DeepSeek-7B achieved highest nDCG@4 (0.804) on healthcare answer retrieval

## Executive Summary
This paper introduces CHQ-Summ, a dataset for consumer healthcare question summarization designed to address the challenge of condensing verbose, informal patient queries into concise medical questions. The dataset contains 1,507 expert-annotated question-summary pairs derived from Yahoo! Answers L6 corpus, featuring unique annotations for question focus and type. The authors benchmark the dataset using both fine-tuned encoder-decoder models (BART, PEGASUS, ProphetNet, T5) and instruction-tuned LLMs (Qwen2-7B-Instruct, Mistral-7B-Instruct-v0.3, Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, Gemma-7B-IT, DeepSeek-7B-Chat) with various prompting strategies, demonstrating significant improvements in downstream retrieval performance.

## Method Summary
The CHQ-Summ dataset was constructed by filtering the Yahoo! Answers L6 corpus using Stanza NER for medical entity identification and length thresholds (â‰¥10 words). Expert annotators created summaries, identified question focus (primary entity), and classified question types (treatment, diagnosis, medication, general health). The dataset was split into 1,000 training, 400 validation, and 107 test samples. Fine-tuned models were trained using AdamW optimizer with validation-based selection, while instruction-tuned LLMs were evaluated using four prompting strategies (Standard, Element-Aware, Hierarchical, Chain-of-Density) in 0/2/5-shot settings with greedy decoding and repetition penalty (1.02-1.03). Evaluation included reference-based metrics (ROUGE-LSum, METEOR, BERTScore-F1), reference-free metrics (Semantic Coherence, Entailment consistency), human evaluation (Factual Correctness, Informativeness, Fluency), and downstream retrieval performance on LiveQA corpus.

## Key Results
- DeepSeek-7B achieved the highest top-1 average score across evaluation metrics
- Element-Aware prompting strategy achieved highest Entailment score (0.8623) with Qwen2-7B-Instruct
- Hierarchical prompting strategy achieved highest ROUGE-LSum (0.3628) with Llama-3.2-3B-Instruct
- Generated summaries improved retrieval performance from 0.678 to 0.804 nDCG@4 on LiveQA dataset

## Why This Works (Mechanism)

### Mechanism 1
Explicit extraction of medical entities prior to generation improves factual consistency by constraining the model to ground summaries in source text entities, reducing hallucination. The Element-Aware strategy forces identification of diseases, drugs, symptoms before generation, blocking ungrounded statements. This relies on the model's ability to accurately extract entities in the first step. Evidence shows Qwen2-7B-Instruct achieves highest Entailment score (0.8623) using this strategy. Break condition: ambiguous or technical entities may cause extraction errors that propagate to summaries.

### Mechanism 2
Decomposing long, noisy consumer questions into hierarchical phases improves lexical alignment for smaller models. The Hierarchical strategy breaks summarization into splitting text, extracting key phrases, and synthesizing final question, reducing cognitive load. This assumes medical meaning is distributed such that segmentation doesn't sever critical context. Evidence shows Llama-3.2-3B-Instruct achieves highest ROUGE-LSum (0.3628) with this strategy. Break condition: narrative threads spanning entire text may be severed, causing disjointed summaries.

### Mechanism 3
Abstractive summarization acts as query refinement step that improves downstream answer retrieval performance. Consumer questions are verbose and colloquial; summarization distills key information into format better aligned with retrieval algorithms. This assumes summaries preserve specific medical concepts while removing noise. Evidence shows DeepSeek-7B summaries achieve 0.804 nDCG@4 vs 0.678 for original questions. Break condition: over-compression or hallucination of symptoms/drugs leads retrieval to fetch incorrect answers.

## Foundational Learning

- **Concept: Abstractive Summarization**
  - Why needed here: Requires generating new text (paraphrasing) to condense verbose user posts into concise questions
  - Quick check question: Can you distinguish between extracting a key sentence from a patient post versus rewriting their 200-word narrative into a single 15-word medical question?

- **Concept: Hallucination vs. Faithfulness**
  - Why needed here: Healthcare generation must avoid factually unsupported text; understanding Entailment scores is critical for safety evaluation
  - Quick check question: If a user asks about "headaches" and the summary adds "caused by stress" when stress wasn't mentioned, is this an abstraction or a hallucination?

- **Concept: Zero-shot vs. Few-shot Prompting**
  - Why needed here: Benchmark uses these strategies; understanding how examples influence output format adherence is key to reproducing results
  - Quick check question: How does providing 5 examples of (Question, Summary) pairs in the prompt context change the model's behavior compared to a standalone instruction?

## Architecture Onboarding

- **Component map**: Input Layer (Yahoo! Answers L6 Corpus) -> Filtering Module (Stanza NER + length checks) -> Annotation Module (Human Experts + MetaMap) -> Model Layer (Fine-tuned Encoders or LLMs with Prompting) -> Evaluation Module (Metrics + Retrieval)

- **Critical path**: The "Filtering Module" determines dataset quality by removing false positives; the "Prompting Strategy" in the Model Layer is the primary lever for trading off lexical similarity (ROUGE) versus factual safety (Entailment)

- **Design tradeoffs**: Safety vs. Fluency (Element-Aware maximizes safety while Hierarchical maximizes lexical alignment); Model Size vs. Complexity (smaller models with complex prompts can match larger models on specific metrics but require more complex inference pipelines)

- **Failure signatures**: High ROUGE, Low Entailment (model copies phrases but misunderstands intent); Hallucinated Entities (model inserts specific drugs/conditions not present); Over-compression (reduces complex query to generic term)

- **First 3 experiments**: 1) Baseline Reproduction: Fine-tune BART-Large on train split and evaluate ROUGE-LSum to verify pipeline matches Table 5; 2) Safety Evaluation: Run Zero-shot Element-Aware prompting with Qwen2-7B-Instruct and measure Entailment score; 3) Retrieval Loopback: Generate summaries for LiveQA questions using DeepSeek-7B, feed into BM25 retriever, and check if nDCG@4 improves over raw questions

## Open Questions the Paper Calls Out

- **Open Question 1**: Does explicitly training models to utilize "question focus" and "question type" annotations as auxiliary tasks improve factual consistency of summaries? The paper introduces these unique annotations and cites prior work suggesting question-type information guides factually correct summaries, but benchmark focuses on standard fine-tuning without multi-task learning. Evidence would come from comparing baseline models against multi-task architectures that jointly predict these attributes during training, specifically measuring Entailment consistency improvements.

- **Open Question 2**: Can a hybrid prompting strategy simultaneously maximize both lexical overlap (style) and medical faithfulness (safety)? The paper identifies a trade-off where Hierarchical yields best ROUGE but Element-Aware yields best Entailment, but doesn't explore combining entity extraction with hierarchical structuring. Evidence would come from composite prompting methods that enforce entity grounding within coarse-to-fine generation framework, optimizing both ROUGE-L and Entailment metrics.

- **Open Question 3**: Does question summarization improve retrieval performance in dense embedding spaces as effectively as for the BM25 lexical baseline? The paper validates via LiveQA retrieval restricted to BM25, leaving impact on dense retrieval unexplored. Consumer questions suffer vocabulary mismatch; while summarization helps BM25, it's unclear if information loss harms dense semantic matching in RAG pipelines. Evidence would come from benchmarking generated summaries with medical-domain dense encoders compared to raw consumer questions.

## Limitations
- Dataset size (1,507 samples) is relatively small for training large-scale models, potentially limiting generalization
- Evaluation heavily relies on automated metrics with only 50 samples evaluated by human raters, which may not fully capture nuanced quality
- Annotation schema defined by single expert team without cross-validation from multiple medical institutions
- Downstream retrieval evaluation uses specific LiveQA corpus that may not represent all healthcare information-seeking scenarios

## Confidence

**High Confidence**: Dataset construction methodology is sound with clear filtering criteria and expert annotation; results showing DeepSeek-7B achieving highest nDCG@4 (0.804) are reproducible given clear evaluation protocol

**Medium Confidence**: Effectiveness of prompting strategies is demonstrated but impact may vary with different model architectures or domain shifts; trade-off between ROUGE scores and Entailment consistency is empirically observed but requires broader validation

**Low Confidence**: Claim that summarization directly improves retrieval performance (0.678 to 0.804 nDCG@4) is promising but based on single retrieval pipeline (BM25 with MedQuAD); real-world impact on diverse retrieval systems remains uncertain

## Next Checks

1. **Cross-Institutional Validation**: Replicate annotation process with second expert team from different medical institution to verify inter-annotator agreement and schema robustness

2. **Retrieval System Generalization**: Test summarization-retrieval pipeline with multiple retrieval approaches (dense retrieval with DPR, hybrid BM25+neural) and different answer corpora (PubMed, ClinicalTrials.gov) to confirm 0.804 nDCG@4 improvement is not pipeline-specific

3. **Longitudinal Safety Audit**: Conduct temporal analysis where summaries generated from archived consumer questions are evaluated by current medical experts to assess whether Element-Aware prompting strategy maintains factual consistency over time and across evolving medical knowledge