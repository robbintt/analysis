---
ver: rpa2
title: A simple mean field model of feature learning
arxiv_id: '2510.15174'
source_url: https://arxiv.org/abs/2510.15174
tags:
- networks
- learning
- equation
- kernel
- theory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a mean-field (MF) theory for Bayesian inference
  in two-layer neural networks trained with stochastic gradient Langevin dynamics
  (SGLD), focusing on the emergence of feature learning (FL) in finite-width networks.
  The authors derive a self-consistent MF theory that predicts a symmetry-breaking
  phase transition marking the onset of FL, driven by a competition between isotropic
  prior regularization and data-driven alignment.
---

# A simple mean field model of feature learning

## Quick Facts
- arXiv ID: 2510.15174
- Source URL: https://arxiv.org/abs/2510.15174
- Reference count: 40
- Primary result: Mean-field theory predicts symmetry-breaking phase transition marking feature learning onset in finite-width networks, with Automatic Relevance Determination capturing post-transition generalization improvements

## Executive Summary
This paper develops a mean-field (MF) theory for Bayesian inference in two-layer neural networks trained with stochastic gradient Langevin dynamics (SGLD), focusing on how finite-width networks learn meaningful features from data. The authors identify a symmetry-breaking phase transition that marks the onset of feature learning, driven by competition between isotropic prior regularization and data-driven alignment. The basic MF model underestimates post-transition generalization improvements, which the authors attribute to a missing mechanism: self-reinforcing input feature selection (IFS). By incorporating coordinate-dependent weight variances via Automatic Relevance Determination (ARD), they develop an MF-ARD model that captures this mechanism while preserving tractability. The resulting model quantitatively matches SGLD-trained network performance across varying dataset sizes and noise levels.

## Method Summary
The authors derive a self-consistent mean-field theory for Bayesian inference in two-layer neural networks trained with SGLD. They start with a fully factorized Gaussian weight distribution ansatz and derive self-consistent equations for the mean-field parameters. The theory predicts a symmetry-breaking phase transition marking the onset of feature learning, where the network transitions from a kernel-like regime to one where it actively reshapes its feature space. To capture the post-transition phase where the basic MF model fails, they incorporate Automatic Relevance Determination by allowing coordinate-dependent weight variances, leading to the MF-ARD model. The models are validated against numerical experiments on synthetic datasets with known ground truth features.

## Key Results
- MF theory predicts a symmetry-breaking phase transition marking feature learning onset in finite-width networks
- Basic MF model significantly underestimates post-transition generalization improvements
- MF-ARD model captures self-reinforcing input feature selection mechanism and matches SGLD-trained network performance
- Phase boundary scales with intrinsic problem dimension rather than ambient dimension, overcoming curse of dimensionality

## Why This Works (Mechanism)
The mechanism works through a competition between two forces: isotropic prior regularization that encourages uniform weight distributions, and data-driven alignment that favors weights aligned with informative input features. At low signal-to-noise ratios, the prior dominates and weights remain isotropic (kernel regime). As signal strength increases past a critical threshold, data-driven alignment wins, causing weights to concentrate on informative features - this is the symmetry-breaking transition marking feature learning onset. The post-transition phase involves self-reinforcing input feature selection, where concentrating weights on informative features further amplifies their signal, creating a positive feedback loop that the basic MF model misses but MF-ARD captures through coordinate-dependent weight variances.

## Foundational Learning

### Bayesian inference in neural networks
**Why needed**: Provides probabilistic framework for understanding generalization and feature learning
**Quick check**: Verify posterior concentration on solutions with good generalization properties

### Mean-field approximations
**Why needed**: Makes Bayesian inference tractable in high-dimensional weight spaces
**Quick check**: Ensure factorization assumption doesn't break down in post-transition phase

### Stochastic gradient Langevin dynamics
**Why needed**: Provides practical algorithm for approximately sampling from Bayesian posterior
**Quick check**: Confirm SGLD samples represent true posterior for validation

### Automatic Relevance Determination
**Why needed**: Captures coordinate-dependent weight variances crucial for post-transition feature selection
**Quick check**: Verify ARD parameters converge to meaningful values during training

## Architecture Onboarding

**Component map**: Prior regularization -> Data alignment -> Weight concentration -> Feature selection -> Generalization improvement

**Critical path**: The phase transition from kernel regime to feature learning regime is the critical path. This occurs when data-driven alignment overcomes prior regularization, triggering weight concentration on informative features.

**Design tradeoffs**: The basic MF model trades accuracy for tractability through weight factorization assumption. MF-ARD adds complexity (coordinate-dependent variances) to capture post-transition behavior while maintaining analytical tractability.

**Failure signatures**: 
- If signal-to-noise ratio too low: remains in kernel regime, no feature learning
- If prior too strong: suppresses feature learning despite informative data
- If MF factorization breaks down: correlations between weights become important

**First experiments**:
1. Validate symmetry-breaking transition on simple synthetic dataset with known ground truth features
2. Compare MF-ARD predictions against SGLD-trained network on varying dataset sizes
3. Test phase boundary scaling with intrinsic vs ambient dimension

## Open Questions the Paper Calls Out
None

## Limitations
- MF model assumes fully factorized weight distributions, which may break down in post-transition phase
- MF-ARD model requires validation against more comprehensive numerical experiments
- Limited validation to simple synthetic datasets, generalizability to real-world problems uncertain

## Confidence
- Phase transition prediction: Medium-High (supported by theory and numerical validation)
- MF-ARD quantitative predictions: Medium (simplifying assumptions, limited validation scope)

## Next Checks
1. Extend MF-ARD model to deeper networks and validate against numerical experiments
2. Test model's predictions on real-world datasets with known ground truth feature structures
3. Investigate role of weight correlations in post-transition phase through more sophisticated mean-field approximations