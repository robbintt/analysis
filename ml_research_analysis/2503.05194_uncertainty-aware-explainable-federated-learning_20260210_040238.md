---
ver: rpa2
title: Uncertainty-Aware Explainable Federated Learning
arxiv_id: '2503.05194'
source_url: https://arxiv.org/abs/2503.05194
tags:
- uncertainty
- rule
- rules
- uncertainxfl
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UncertainXFL, the first explainable federated
  learning framework that explicitly incorporates uncertainty information into the
  explanation generation process. The framework generates logical rules as explanations
  while quantifying their uncertainty, which arises from the labeller's confidence
  in identifying features.
---

# Uncertainty-Aware Explainable Federated Learning

## Quick Facts
- arXiv ID: 2503.05194
- Source URL: https://arxiv.org/abs/2503.05194
- Reference count: 10
- Achieves 90.34% model accuracy and 90.84% rule accuracy on CUB dataset

## Executive Summary
This paper introduces UncertainXFL, the first explainable federated learning framework that explicitly incorporates uncertainty information into the explanation generation process. The framework generates logical rules as explanations while quantifying their uncertainty, which arises from the labeller's confidence in identifying features. During FL training, clients upload both their model updates and rule sets to the server, which aggregates them in a conflict-free manner while weighting clients based on the reliability of their explanations (measured by both accuracy and uncertainty). The uncertainty information guides the FL aggregation process, prioritizing more reliable clients.

## Method Summary
UncertainXFL integrates a concept bottleneck model with uncertainty quantification into federated learning. Each client trains a local model that generates logical rules from feature-concept relationships while incorporating labeler uncertainty through element-wise multiplication with feature vectors. These rules, along with their accuracy and uncertainty scores, are uploaded to a server that performs greedy conflict-free aggregation. The server ranks rules by a score combining accuracy and uncertainty, then aggregates them while resolving conflicts (using AND for non-conflicting features, OR for same-group conflicts). Clients are reweighted in the FL aggregation based on how frequently their rules are selected, creating a feedback loop where better explainers contribute more to the global model.

## Key Results
- Achieves 90.34% model accuracy and 90.84% rule accuracy on CUB dataset
- Outperforms state-of-the-art baseline without uncertainty consideration by 2.71% in model accuracy and 1.77% in rule accuracy
- Ablation studies show 1.53% accuracy drop when removing uncertainty integration and 3.09% improvement from uncertainty-weighted aggregation over standard FedAvg

## Why This Works (Mechanism)
The framework works by creating a tight coupling between explanation quality and model contribution. By quantifying uncertainty from labeler confidence and incorporating it into both rule generation and aggregation, the system can prioritize more reliable clients. The greedy conflict-free aggregation ensures that only the most accurate and certain rules survive, while the reweighting mechanism ensures that clients who provide better explanations have greater influence on the global model. This creates a virtuous cycle where explanation quality directly improves model performance.

## Foundational Learning

- Concept: **Federated Learning (FL) & Aggregation**
  - Why needed here: This is the core setting. You must understand how clients train locally and how a server aggregates updates (e.g., FedAvg) to see how UncertainXFL modifies this with explanation-based weighting.
  - Quick check question: How does FedAvg aggregate client model updates?

- Concept: **Aleatoric vs. Epistemic Uncertainty**
  - Why needed here: The paper focuses on incorporating aleatoric uncertainty (from data/labelers). Distinguishing this from model uncertainty is key to understanding what the framework measures and its stated limitations.
  - Quick check question: Which type of uncertainty stems from noise in the data and cannot be reduced with more data?

- Concept: **Concept Bottleneck Models & Logic Rules**
  - Why needed here: The explainability component relies on a concept-based network to generate human-readable logical rules (e.g., "has wing AND beak"). Understanding this architecture is essential for implementing the rule extraction and conflict resolution modules.
  - Quick check question: In a concept bottleneck model, what is the intermediate layer designed to predict?

## Architecture Onboarding

- Component map: ResNet feature extractor -> Concept-based Network -> Uncertainty Integrator -> Rule Extractor -> Server-side Aggregator -> Reweighting Module

- Critical path: Data annotation with uncertainty -> Client training with uncertainty-modulated features -> Local rule extraction & uncertainty calculation -> Server-side rule ranking & greedy aggregation -> Client re-weighting based on rule contribution -> Weighted aggregation of model parameters

- Design tradeoffs:
  - Greedy vs. Beam Search: The authors chose a greedy aggregation method for efficiency over beam search, trading off a potentially better global rule for lower computational cost
  - Positive-only Rules: The framework excludes negation (¬fᵢ) in rules for better human interpretability, potentially sacrificing some logical expressiveness
  - Proxy Weighting: Using explanation quality (rules) to weight model updates assumes that good explainers are good model trainers. This decouples the two, which could be a risk

- Failure signatures:
  - Simulated vs. Real Uncertainty: On MNIST with artificial uncertainty, the model underperformed. If uncertainty labels are not genuinely reflective of ambiguity, performance may degrade
  - Conflicting Local Rules: If client rules are highly contradictory and of similar score, the greedy aggregation may struggle to form a coherent global rule
  - Low-Quality Rules, High-Accuracy Models: A client could have a high-accuracy model but generate low-quality or high-uncertainty rules, leading to their beneficial model updates being underweighted

- First 3 experiments:
  1. Baseline Comparison: Implement UncertainXFL against the LR-XFL baseline on the CUB dataset, comparing both model accuracy and rule accuracy to validate the 2.71% and 1.77% gains cited
  2. Ablation on Uncertainty Integration: Run an experiment comparing the full framework against a variant that removes the element-wise multiplication of the uncertainty vector (UncertainXFL w/o Uncertainty). The paper shows a 1.53% accuracy drop on CUB from this removal
  3. Ablation on Aggregation Method: Compare the uncertainty-weighted aggregation against standard FedAvg. The paper reports a 3.09% accuracy improvement on CUB when using the proposed weighting scheme, directly testing the value of explanation-guided aggregation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be extended to incorporate epistemic (model) uncertainty alongside the current aleatoric (data) uncertainty?
- Basis in paper: [explicit] The conclusion states, "In the future, we plan to incorporate model uncertainty alongside the aleatoric uncertainty currently used."
- Why unresolved: The current design only quantifies uncertainty from label noise, ignoring uncertainty arising from the model's limitations or lack of knowledge
- Evidence: A modified framework that quantifies both uncertainty types and demonstrates improved robustness in low-data regimes

### Open Question 2
- Question: Does the method generalize to datasets with naturally occurring uncertainty, given the performance drop on the artificially simulated MNIST dataset?
- Basis in paper: [inferred] The authors note that UncertainXFL trailed baselines on MNIST, attributing it to the artificial nature of the introduced uncertainty which might not reflect "practical scenarios."
- Why unresolved: It is unclear if the lower performance is an artifact of the simulation method or a fundamental limitation when uncertainty patterns differ from natural noise
- Evidence: Evaluation on datasets with intrinsic human-label uncertainty (e.g., medical imaging) without artificial overlay techniques

### Open Question 3
- Question: Is the framework feasible in scenarios where the server lacks a representative validation dataset for rule aggregation?
- Basis in paper: [inferred] Algorithm 1 requires the server to hold a validation set to "assess the performance of each new rule" and guide aggregation
- Why unresolved: In strict FL settings, the server often possesses no data; the dependency on server-side data for conflict resolution limits applicability
- Evidence: Experiments showing rule accuracy and model convergence when using heuristic-based aggregation without server-side validation data

## Limitations

- Architectural Specification Uncertainty: Missing details about concept bottleneck model architecture (number of concept features, threshold values, ResNet variant) prevent exact replication
- Uncertainty Quantification Limitations: Only accounts for aleatoric uncertainty from labeler confidence, explicitly excluding epistemic uncertainty from the model
- Simulated Uncertainty Concerns: Performance degradation on MNIST with artificially generated uncertainty suggests framework may not generalize to datasets with naturally occurring uncertainty

## Confidence

- Framework Design and Innovation: High
- Performance Claims: Medium
- Explainability Benefits: High

## Next Checks

1. Implementation Verification: Build and train the concept bottleneck model with varying numbers of concept features (F) and thresholds (t) to identify parameter combinations that achieve results comparable to the reported 90.34% model accuracy on CUB

2. Uncertainty Integration Test: Implement both the full uncertainty-augmented feature approach and the ablation variant (without element-wise multiplication) to verify the reported 1.53% accuracy difference on CUB

3. Aggregation Method Comparison: Run experiments comparing the uncertainty-weighted aggregation against standard FedAvg on CUB to validate the claimed 3.09% accuracy improvement, testing whether explanation quality reliably indicates model quality