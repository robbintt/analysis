---
ver: rpa2
title: Revealing Language Model Trajectories via Kullback-Leibler Divergence
arxiv_id: '2505.15353'
source_url: https://arxiv.org/abs/2505.15353
tags:
- divergence
- arxiv
- preprint
- language
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a consistent scale for measuring Kullback-Leibler
  divergence across diverse language models and settings using log-likelihood vectors.
  By representing model probabilities in a shared coordinate system, the authors enable
  unified comparisons across pretraining checkpoints, quantized models, fine-tuned
  models, and intermediate layers.
---

# Revealing Language Model Trajectories via Kullback-Leibler Divergence

## Quick Facts
- **arXiv ID**: 2505.15353
- **Source URL**: https://arxiv.org/abs/2505.15353
- **Reference count**: 40
- **Key outcome**: Establishes consistent KL divergence scale (0.01-0.1 bits/byte) across language models by embedding in log-likelihood space, revealing early functional stabilization despite continued weight changes.

## Executive Summary
This paper introduces a method to measure Kullback-Leibler divergence between language models using log-likelihood vectors over a fixed text set. By double-centering these vectors, the authors create a shared coordinate system where Euclidean distance approximates KL divergence, enabling unified comparisons across pretraining checkpoints, quantized models, fine-tuned models, and intermediate layers. Experiments on Pythia models reveal that while weight trajectories exhibit Brownian diffusion during training, log-likelihood trajectories are strongly subdiffusive (exponent ≈0.2), indicating that model behavior stabilizes early despite continued parameter changes.

## Method Summary
The method computes log-likelihood vectors for each model over a fixed set of 10,000 texts, then applies double-centering to create coordinates where squared Euclidean distance approximates KL divergence. Models are scaled by average text length and converted to bits/byte units. Diffusion exponents are estimated by measuring how squared distances scale with time intervals. The approach enables cross-architecture comparison by mapping diverse models into a shared geometric space.

## Key Results
- Pretraining checkpoints show KL divergence decreasing from ~0.01-0.1 bits/byte during later training
- Quantization induces median KL shifts of 0.44 bits/byte (8-bit) and 0.49 bits/byte (4-bit)
- Fine-tuning shifts KL by ~0.40 bits/byte between pre-trained and fine-tuned models
- Layer-wise KL shows median 3.0 bits/byte across intermediate layers
- Log-likelihood trajectories are strongly subdiffusive (c_q ≈ 0.2) while weights remain Brownian (c_w ≈ 1)

## Why This Works (Mechanism)

### Mechanism 1: Log-Likelihood Vector as Common Coordinate System
Embedding models via log-likelihood vectors over a fixed text set creates a shared space where squared Euclidean distance approximates KL divergence, enabling cross-architecture comparison. For models p₁, p₂, compute log-likelihoods over N texts to form vectors ℓ₁, ℓ₂ ∈ ℝᴺ. Double-centering yields coordinates q₁, q₂ where ∥q₁ - q₂∥²/N ≈ 2·KL(p₁, p₂). This bypasses weight-space incompatibilities (permutation symmetries, architectural differences).

### Mechanism 2: Subdiffusive Stabilization in Function Space
During later training, model outputs stabilize (subdiffusion, c_q ≈ 0.2) while weights continue drifting (Brownian, c_w ≈ 1), indicating early functional convergence despite parameter movement. Diffusion exponent c characterizes how squared distance scales with time: ∥q_t - q_t₀∥² ∝ |t - t₀|^c. c = 1 is Brownian; c < 1 is subdiffusive (confined). The mapping from weight to log-likelihood space is α-Hölder continuous with α ≈ 0.2, meaning weight-space trajectories fold into a narrower function-space region.

### Mechanism 3: Structured Perturbation from Quantization
Quantization shifts models in log-likelihood space along consistent directions within model families, suggesting it acts as structured (not random) perturbation. 8-bit and 4-bit quantization induce median KL divergences of 0.44 and 0.49 bits/byte respectively, with low variance across models. In t-SNE visualizations, quantization shifts appear directionally aligned within model types.

## Foundational Learning

- **Concept: Kullback-Leibler Divergence**
  - **Why needed here:** The entire metric framework is built on KL divergence as the measure of model difference; interpreting bits/byte values requires intuition for KL's scale.
  - **Quick check question:** If KL(A, B) = 0.1 bits/byte and the reference text entropy is 0.71 bits/byte, approximately what fraction of the text's information content does this divergence represent?

- **Concept: Double-Centering in Distance Geometry**
  - **Why needed here:** The transformation from log-likelihood vectors L to coordinate matrix Q via double-centering is what enables Euclidean distance to approximate KL divergence.
  - **Quick check question:** What would happen to the KL approximation if you skipped row-wise centering (over texts)?

- **Concept: Anomalous Diffusion and Scaling Exponents**
  - **Why needed here:** Interpreting c_q ≈ 0.2 vs. c_w ≈ 1 requires understanding diffusion exponents as measures of trajectory confinement vs. free wandering.
  - **Quick check question:** If a trajectory has diffusion exponent c = 0.5, how does displacement scale with time compared to Brownian motion (c = 1)?

## Architecture Onboarding

- **Component map:**
  1. Text Set Loader → Log-Likelihood Computer → Log-Likelihood Matrix L → Double-Centering Module → Scaling Layer → Distance/KL Estimator

- **Critical path:**
  Text set selection → Log-likelihood computation (most expensive; ~10 min for 7B model on RTX 6000 Ada) → Double-centering → Distance computation. Memory bottleneck is storing L for many models; compute bottleneck is inference over N texts.

- **Design tradeoffs:**
  - Text set size (N): Larger N reduces variance but increases compute; N=10,000 balanced well in experiments (SE estimates provided in Appendix C)
  - Text set domain: Must match target distribution; cross-domain effects unexplored
  - Checkpoint granularity: Pythia checkpoints at 1k steps miss finer dynamics; sub-1k behavior not analyzed
  - Layer analysis method: Logit lens used (noisy in shallow layers); tuned lens would be better but limited availability

- **Failure signatures:**
  - Outlier texts: Repetitive/meaningless sequences (e.g., "028a28a00...") cause irregular log-likelihood jumps; paper removes top 300 outlier-scored texts
  - Anomalous checkpoints: Pythia-1B step 116k had abnormal weight distances; excluded
  - Training loss spikes: Seeds 3 and 4 of Pythia-410M showed abnormally high inter-seed KL; flagged as outliers
  - Shallow layer noise: Logit lens on early layers yields high-variance KL estimates

- **First 3 experiments:**
  1. Reproduce KL scale baseline: Compute KL between consecutive Pythia checkpoints (post-warmup) to verify your pipeline produces ~0.01-0.1 bits/byte range
  2. Quantization perturbation test: Quantize 5-10 models (8-bit and 4-bit), compute KL from base models, verify median ~0.44-0.49 bits/byte and directional consistency via t-SNE
  3. Cross-seed stability check: For models with multiple training runs available, compute final-checkpoint inter-seed KL; values should cluster tightly (~0.1 bits/byte for same size) unless training anomalies exist

## Open Questions the Paper Calls Out

- How does the estimated KL divergence between language models change when comparing models across different text domains or corpora?
  - The authors state: "the impact of domain shifts across text sets has not been examined" in their Limitations section.

- What are the mechanistic causes of the subdiffusive behavior (c_q ≈ 0.2) observed in log-likelihood space trajectories during later training stages?
  - The authors state their analysis "of their underlying causes and implications for learning dynamics remains preliminary and has not yet been quantitatively established."

- How does the diffusion exponent c_q in log-likelihood space scale with model size, and does this relationship generalize beyond the Pythia model family?
  - The authors note "the estimated diffusion exponent c_q exhibits systematic variation across model sizes" and that "a more detailed analysis of model-size dependence is beyond the scope of this paper."

## Limitations
- Cross-domain robustness untested: KL divergence estimates may not generalize across different text corpora
- Diffusion exponent interpretation: Subdiffusive behavior is observed but mechanistic causes remain unquantified
- Quantization alignment: Directional consistency is visually suggested but lacks statistical validation

## Confidence

- **High confidence**: KL scale measurements (0.01-0.1 bits/byte for pretraining, 0.44-0.49 for quantization) - directly measured with clear error bars
- **Medium confidence**: Subdiffusive stabilization claim (c_q ≈ 0.2) - measurement is solid but functional convergence interpretation extends beyond data
- **Medium confidence**: Structured quantization hypothesis - directional alignment is visually suggestive but not statistically validated

## Next Checks

1. Cross-domain robustness test: Apply the method to a different corpus (e.g., C4 or Wikipedia) and verify KL scales remain consistent for the same models
2. Quantization alignment quantification: Measure angular distribution of quantization vectors in log-likelihood space to statistically test the directional alignment hypothesis
3. Temporal scaling verification: Extend diffusion analysis to longer timescales (multiple training runs) to confirm power-law behavior holds beyond the observed window