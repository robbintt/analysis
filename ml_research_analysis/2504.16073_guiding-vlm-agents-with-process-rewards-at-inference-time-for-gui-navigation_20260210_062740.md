---
ver: rpa2
title: Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation
arxiv_id: '2504.16073'
source_url: https://arxiv.org/abs/2504.16073
tags:
- action
- reward
- actions
- arxiv
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of guiding visual language model
  (VLM) agents in GUI navigation tasks, where existing frameworks often struggle to
  generate correct actions in complex environments. The proposed method, GuidNav,
  introduces process supervision via a reward model during inference time, allowing
  the VLM agent to optimize actions at each step rather than relying on delayed, trajectory-level
  feedback.
---

# Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation

## Quick Facts
- arXiv ID: 2504.16073
- Source URL: https://arxiv.org/abs/2504.16073
- Reference count: 8
- Primary result: Achieves 3.4% improvement in single-step action accuracy for static tasks and 33% increase in task success rate in dynamic settings

## Executive Summary
This paper addresses the challenge of guiding visual language model (VLM) agents in GUI navigation tasks, where existing frameworks often struggle to generate correct actions in complex environments. The proposed method, GuidNav, introduces process supervision via a reward model during inference time, allowing the VLM agent to optimize actions at each step rather than relying on delayed, trajectory-level feedback. The reward model is trained using human demonstrations and self-play data, providing step-level guidance to the VLM agent. This approach improves performance in both static and dynamic environments, achieving a 3.4% improvement in single-step action accuracy for static tasks and a 33% increase in task success rate in dynamic settings. Integration with trajectory reflection and retry mechanisms further enhances task success. The method is evaluated across three benchmarks (AitW, GUI Odyssey, and Mind2Web), demonstrating consistent improvements and generalization across diverse GUI tasks. GuidNav offers a more efficient and precise alternative to existing trajectory-level refinement techniques, reducing computational costs while improving task completion rates.

## Method Summary
The method involves fine-tuning a CogVLM2 model as a process reward model with MSE loss using human demonstrations (reward=1 for correct actions) and self-play data (rewards based on action effectiveness). At inference, a policy VLM (GPT-4o, Gemini, or Qwen) generates k=3 action candidates given the instruction, history summary, and labeled screenshot. The reward model scores each candidate, and the highest-scoring action is executed. The GUI is processed using Set-of-Mark (SoM) segmentation with SAM to assign numeric labels to UI elements, converting screenshots into labeled images. History summarization via VLM compresses context to fit token limits. For dynamic evaluation, the method integrates with trajectory reflection modules for retry mechanisms when tasks fail.

## Key Results
- Achieves 3.4% improvement in single-step action accuracy for static tasks
- Increases task success rate by 33% in dynamic settings
- Integration with reflection and retry mechanisms achieves 71.6% success rate vs 43.7% for reflection alone

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Step-level process rewards provide more precise feedback than trajectory-level evaluation, enabling immediate error correction and preventing error accumulation.
- **Mechanism:** A separately trained reward model R receives (instruction x, history summary h_t, current state s_t, action candidate a_t) and outputs a scalar reward r. The model is trained via MSE loss against annotated rewards from human demonstrations (r=1 for correct actions) and self-play data (r based on effectiveness).
- **Core assumption:** The reward model can generalize from limited demonstration data to accurately score unseen action candidates in diverse GUI states.
- **Evidence anchors:**
  - [abstract] "guidance allows the VLM agent to optimize actions at each inference step...achieving a 3.4% improvement in single step action accuracy"
  - [section 5.1] "We evaluate the step-wise reward model on 500 AitW steps, achieving 78.8% accuracy"
  - [corpus] GUI-Shepherd paper (FMR 0.59) corroborates process reward models for GUI tasks, suggesting this approach has emerging validation
- **Break condition:** If reward model accuracy drops below ~70% on held-out tasks, or if reward scores become poorly calibrated (all candidates receiving similar scores), the selection mechanism degrades to random choice among top-k.

### Mechanism 2
- **Claim:** VLMs frequently generate correct actions but fail to rank them first; a reward model recovers the correct action from the candidate set.
- **Mechanism:** The policy VLM generates k action candidates. Instead of selecting the highest-probability candidate (VLM's ranking), the reward model independently scores each and selects argmax. This decouples action generation from action selection.
- **Core assumption:** The correct action appears within the top-k candidates generated by the VLM.
- **Evidence anchors:**
  - [section 5.1, Table 2] "Topk w/ Oracle Eval" achieves 53.7% vs TopK baseline 34.0%, demonstrating that correct actions exist in candidates but aren't ranked first
  - [section 1] "process-based rewards can help reduce these deviations, leading to a more efficient action trajectory"
  - [corpus] Weak corpus evidence on this specific mechanism; no directly comparable best-of-k selection validated in neighbors
- **Break condition:** If k is too small or VLM's action distribution doesn't cover correct actions, reward model cannot recover what doesn't exist. Table 2 suggests current k=3 setting leaves substantial gap vs oracle (38.9% vs 53.7%).

### Mechanism 3
- **Claim:** Process reward guidance and trajectory-level reflection are complementary: process rewards optimize individual steps while reflection provides task-level course correction.
- **Mechanism:** GuidNav guides each step during execution. If the trajectory fails, AR's reflection module generates insights for retry. Process rewards reduce intra-trajectory errors; reflection addresses inter-trajectory strategic errors.
- **Core assumption:** Reflection generates actionable insights rather than hallucinated explanations, and retry trajectories don't repeat identical mistakes.
- **Evidence anchors:**
  - [section 5.1, Table 3] Integration achieves 71.6% vs GuidNav alone 58.1% vs AR alone 43.7% in dynamic settings
  - [section 6.1, Figure 2] "performance curve of the integrated method with the process reward consistently remains above that of the AR alone"
  - [corpus] No corpus papers validate this specific integration pattern
- **Break condition:** If reflection produces low-quality or repetitive insights, additional retries yield diminishing returns (Figure 2 shows plateauing after 2-3 retries).

## Foundational Learning

- **Process Reward Models vs Outcome Rewards:**
  - Why needed here: The paper's core innovation is shifting from trajectory-level (sparse) to step-level (dense) feedback. Understanding this distinction is essential for grasping why GuidNav outperforms AR/DigiRL.
  - Quick check question: Given a 10-step GUI task where only step 7 is wrong, which approach identifies the error faster—trajectory-level evaluation or process rewards?

- **Set-of-Mark (SoM) for GUI Grounding:**
  - Why needed here: The method uses SoM to convert GUI screenshots into labeled elements that VLMs can reference. Without this, the action space would require coordinate prediction.
  - Quick check question: How does SoM change the action representation from continuous coordinates to discrete labels?

- **History Summarization in Multi-Step Tasks:**
  - Why needed here: Equation 1 shows history compression via VLM summarization. This addresses context length limits but introduces potential information loss.
  - Quick check question: What critical information might be lost when compressing 20 previous state-action pairs into a brief summary?

## Architecture Onboarding

- **Component map:**
  Policy VLM -> Reward Model -> Environment Executor, where Policy VLM generates action candidates, Reward Model scores them, and Environment Executor converts selected action to device interaction

- **Critical path:**
  1. Screenshot → SoM labeling → labeled image + element coordinates
  2. Labeled image + instruction + history summary → Policy VLM → k action candidates
  3. Each candidate → Reward Model → k reward scores
  4. argmax(rewards) → Execute action → Update history
  5. (If trajectory fails) → Reflection → Retry with insights

- **Design tradeoffs:**
  - **k value:** Higher k increases oracle upper bound but multiplies reward model calls (cost). Paper uses k=3.
  - **History summarization:** Reduces tokens but risks losing critical context. Paper uses brief natural language summaries.
  - **Reward model training data mix:** Human demos are high-quality but scarce; self-play scales but may propagate errors. Paper uses both.

- **Failure signatures:**
  - **Low reward variance:** All candidates score similarly → reward model not discriminating (check calibration on validation set)
  - **Stuck loops:** Agent repeats actions → history summarization may be losing context about what was already tried
  - **Wrong element selection:** Correct action type but wrong target → SoM segmentation may have merged/split elements incorrectly
  - **Reflection hallucination:** Retry makes identical mistake → reflection quality check needed

- **First 3 experiments:**
  1. **Static validation on AitW subset (75 tasks):** Measure action accuracy without environment execution. Compare TopK vs GuidNav to validate reward model discriminability. Target: reproduce ~4-5% improvement over baseline.
  2. **Reward model calibration check:** On held-out 100 steps, plot predicted reward vs ground-truth correctness. Assess whether high-reward actions are actually correct. Target: 75%+ accuracy at threshold 0.5.
  3. **Ablation on history summarization:** Run same tasks with full history vs. summarized history. Measure accuracy degradation. If >10% drop, summarization is too aggressive.

## Open Questions the Paper Calls Out
- Can GuidNav generalize effectively to desktop operating systems and specialized professional software?
- How robust is the policy model to errors or noise in the process reward model's guidance?
- Does a capability gap between the policy model and the reward model limit the effectiveness of guidance?

## Limitations
- The method's performance relies heavily on the reward model's generalization ability from limited demonstration data
- Self-play data generation process and its impact on reward calibration remain underspecified
- Dynamic evaluation protocol depends on human annotators without reporting inter-annotator agreement metrics

## Confidence
- **High confidence**: The core mechanism of step-level reward guidance (Mechanism 1) and its demonstrated improvement over trajectory-level methods
- **Medium confidence**: The integration benefits with reflection and retry mechanisms (Mechanism 3), as this pattern lacks corpus validation
- **Medium confidence**: The action space coverage assumption (Mechanism 2), given the substantial gap between oracle and achieved performance in Table 2

## Next Checks
1. Test reward model generalization by evaluating on held-out task types (e.g., Mind2Web tasks during AitW training) to assess calibration stability
2. Perform ablation studies varying k (number of candidates) to quantify the trade-off between reward model accuracy and oracle upper bound
3. Measure reflection module quality by analyzing whether retry trajectories make different errors or repeat the same mistakes