---
ver: rpa2
title: 'IndicGEC: Powerful Models, or a Measurement Mirage?'
arxiv_id: '2511.15260'
source_url: https://arxiv.org/abs/2511.15260
tags:
- task
- languages
- output
- language
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the effectiveness of large language models
  (LLMs) for grammatical error correction (GEC) in five Indian languages: Hindi, Telugu,
  Tamil, Malayalam, and Bangla. The study evaluates zero-shot and few-shot prompting
  of models ranging from 4B to large proprietary models, focusing on understanding
  their capabilities without task-specific fine-tuning.'
---

# IndicGEC: Powerful Models, or a Measurement Mirage?

## Quick Facts
- arXiv ID: 2511.15260
- Source URL: https://arxiv.org/abs/2511.15260
- Reference count: 7
- Models achieve high GLEU scores, but qualitative analysis reveals significant data quality issues and metric limitations

## Executive Summary
This paper evaluates large language models for grammatical error correction across five Indian languages using zero-shot and few-shot prompting without task-specific fine-tuning. While models achieve impressive GLEU scores (up to 90 for Malayalam), qualitative analysis uncovers data quality issues including incorrect gold references, context-dependent corrections, and formatting errors. The study finds GLEU unreliable for Indian scripts due to n-gram overlap sensitivity to minor character variations. The research highlights the potential of small multilingual models but emphasizes the need for better datasets and evaluation metrics tailored to Indic languages.

## Method Summary
The study uses DSPy with ReAct prompting to evaluate Gemma3 (4B/12B/27B) and Gemini-2.5-Flash models for GEC across five Indian languages. Zero-shot and few-shot (k=5,10,15) prompting are applied without fine-tuning, with random sampling from training data. Evaluation uses the official shared task GLEU script. The approach achieved rank 2 in Hindi and rank 4 in Telugu in the BHASHA-Task 1 shared task, with experiments extended to Tamil, Malayalam, and Bangla.

## Key Results
- Small multilingual models (4B) achieve near-perfect GLEU scores (up to 90) for Malayalam and Bangla
- Zero-shot prompting outperforms few-shot prompting for most languages and model sizes
- GLEU metric shows high variance for minor character-level variations in alpha-syllabic scripts
- Qualitative analysis reveals ~30% error rate in Telugu gold references and context-dependent corrections

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReAct prompting enables models to reason about grammatical validity before producing corrections.
- Mechanism: The ReAct framework prompts LLMs to generate intermediate reasoning traces that explicitly assess whether input sentences contain errors, are incomplete, or require contextual information. This explicit reasoning step appears to reduce over-correction of grammatically valid sentences.
- Core assumption: The model's internal grammatical knowledge for Indian languages is sufficient to support accurate error detection when explicitly prompted to reason.
- Evidence anchors:
  - [abstract] "using ReAct prompting" across varying model sizes (4B to proprietary models)
  - [section 4.1] Model reasoning trace example: "The input sentence...is incomplete and lacks a proper ending...Without further context...it's impossible to correct it grammatically. Therefore, I will return the original input as is."
  - [corpus] Weak direct evidence; related papers focus on dataset creation rather than prompting mechanisms.

### Mechanism 2
- Claim: Smaller multilingual models (4B-12B parameters) retain sufficient grammatical knowledge for Indian language GEC without task-specific fine-tuning.
- Mechanism: Pre-training on multilingual corpora embeds grammatical patterns that can be accessed via zero-shot prompting. The paper observes that performance differences between 4B and 27B models were "less dramatic than expected," suggesting diminishing returns for scale on this specific task.
- Core assumption: The pre-training data for Gemma3 models contained adequate representation of the five target Indian languages.
- Evidence anchors:
  - [section 3] "the differences are less dramatic than expected considering the large variation in the model parameter sizes"
  - [section 3] 4B model "reaches a GLEU score of almost 90 for Malayalam"
  - [corpus] No direct validation; Pragyaan paper (neighbor) highlights data quality issues for Indian language post-training, which may limit generalization.

### Mechanism 3
- Claim: GLEU metric exhibits high variance for equivalent edit distances in alpha-syllabic scripts, making it unreliable for Indian language GEC evaluation.
- Mechanism: GLEU computes n-gram overlap at the character sequence level. For alpha-syllabic scripts (where single unicode modifications can represent morphologically significant changes), minor character-level variations produce inconsistent scores that do not reflect correction quality.
- Core assumption: The observed score variance is intrinsic to the metric-script interaction, not an artifact of the specific test sentences.
- Evidence anchors:
  - [section 4.2, Table 1] Seven manually created variations with 1-2 unicode character differences produced GLEU scores ranging from 85.33 to 100.0
  - [section 4.2, Table 2] Returning input unchanged yields GLEU scores of 95.79 (Bangla), 94.42 (Malayalam), 92.17 (Tamil)
  - [corpus] Crosslingual Optimized Metric paper (neighbor) critiques string-based metrics for Indian languages, supporting this concern.

## Foundational Learning

- Concept: **In-context learning with few-shot prompting**
  - Why needed here: The entire experimental approach relies on zero-shot and k-shot prompting without fine-tuning. Understanding how example selection affects performance is critical for interpreting the mixed few-shot results.
  - Quick check question: Can you explain why adding randomly sampled few-shot examples might harm rather than help performance on this task?

- Concept: **Alpha-syllabic script structure and Unicode handling**
  - Why needed here: The GLEU metric failure mode is specific to how Indian language scripts encode information at the character level. Consonant-vowel combinations (aksharas) behave differently from alphabetic scripts.
  - Quick check question: What is the difference between a unicode code point and a grapheme cluster, and why does this matter for string-matching metrics?

- Concept: **Grammatical Error Correction evaluation paradigms**
  - Why needed here: The paper's central critique targets evaluation methodology. Understanding reference-based vs. reference-free metrics, and the history of GLEU/BLEU variants, contextualizes the proposed alternatives.
  - Quick check question: Why might a metric that works well for English GEC fail for morphologically rich languages?

## Architecture Onboarding

- Component map:
  - OpenRouter API -> DSPy library with ReAct module -> Gemma3/Gemini models -> GLEU evaluation script

- Critical path:
  1. Load test sentences for target language
  2. Configure DSPy with model via OpenRouter
  3. Define GEC task signature (source_text → target_text)
  4. (Optional) Sample k training examples for few-shot bootstrapping
  5. Execute ReAct inference, extract target_text and reasoning
  6. Compute GLEU against gold references

- Design tradeoffs:
  - **Random vs. strategic few-shot sampling**: Paper used random sampling; notes that selecting examples with "more character transformations" might improve few-shot performance. Current approach trades optimization for simplicity.
  - **Zero-shot vs. few-shot**: Few-shot helped only for Malayalam and Bangla; harmed performance for other languages. Decision to prioritize zero-shot for larger models was resource-driven ("time constraints").
  - **Model size vs. inference speed**: Gemma3 chosen over Qwen3/Sarvam-M2 due to "faster and consistent" inference despite potential capability differences.

- Failure signatures:
  - **Few-shot degradation**: If few-shot GLEU < zero-shot GLEU, check whether training examples contain errors (Section 4 identifies ~30% error rate in Telugu gold outputs).
  - **Reasoning trace detection**: Models may return input unchanged with reasoning indicating "no errors found"—this is expected behavior, not a failure, but should be logged for analysis.
  - **API instability**: Server access errors mentioned for alternative models; implement retry logic and fallback model selection.

- First 3 experiments:
  1. **Baseline establishment**: For each language, compute "return input as-is" GLEU score to understand the floor/ceiling problem before drawing conclusions about model performance.
  2. **Few-shot sample quality audit**: Manually inspect 20 random training examples per language for gold-output errors before using them for few-shot prompting; filter or re-annotate as needed.
  3. **Metric comparison**: Replicate evaluation using at least one alternative metric (e.g., from gec-metrics library cited in Section 4.2) to validate whether GLEU-specific artifacts drive the observed patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM reasoning traces be reliably used to semi-automate the identification of data quality issues in GEC datasets?
- Basis in paper: [explicit] The authors note that reasoning traces successfully identified incomplete sentences and gold-standard errors, stating this "could be a potential source... which we leave as an exploration for the future."
- Why unresolved: The paper only conducted a brief manual analysis; the scalability and accuracy of using LLMs for automated quality assurance remain untested.
- What evidence would resolve it: A study comparing LLM-generated quality flags against a human-annotated ground truth for dataset errors.

### Open Question 2
- Question: Which evaluation metrics provide a more reliable assessment of GEC performance for Indic scripts than GLEU?
- Basis in paper: [explicit] Section 4.2 states that extending evaluation to cover more metrics "may help in choosing a better metric for IndicGEC in future."
- Why unresolved: The study found GLEU scores vary substantially for minor unicode changes and yield high scores for null corrections, but did not test alternatives.
- What evidence would resolve it: A comparative benchmark of metrics (e.g., using the referenced *gec-metrics* library) correlated with human judgment for Indian languages.

### Open Question 3
- Question: Should spelling errors in proper names be classified as grammatical errors within GEC task definitions?
- Basis in paper: [explicit] The authors highlight ambiguity in the Telugu dataset regarding proper names (e.g., "Dommetti" vs. "Dommaraju") and ask "Whether spelling errors in proper names come under GEC or not is a separate question to address."
- Why unresolved: Current datasets include these errors inconsistently, confusing models that identify the sentences as grammatically correct or ambiguous.
- What evidence would resolve it: Distinct evaluation runs where proper noun corrections are excluded/included in the gold standard to measure impact on model scoring.

## Limitations

- Data quality issues affect ~30% of test examples, making it impossible to distinguish true model errors from correct rejections of flawed references
- GLEU metric shows high variance for minor character-level variations in alpha-syllabic scripts, exceeding typical performance differences between model sizes
- Prompting methodology varies across languages without systematic ablation, leaving mechanism unclear

## Confidence

- **High Confidence**: Small multilingual models achieve near-perfect GLEU scores on certain languages when evaluated against flawed references
- **Medium Confidence**: Zero-shot prompting outperforms few-shot prompting for larger models
- **Low Confidence**: 4B models are "powerful enough" for GEC across all five languages

## Next Checks

1. **Gold Reference Validation**: Manually audit 50 randomly selected test examples across all five languages to establish ground truth error rates. Compare against model reasoning traces to determine false positive/negative rates independent of GLEU scores.

2. **Metric Robustness Testing**: Recompute all evaluations using alternative metrics (e.g., chrF, BertScore, or metrics from the gec-metrics library cited) to verify whether GLEU-specific artifacts drive the observed patterns. Focus on languages showing high metric variance.

3. **Controlled Prompt Ablation**: Systematically vary prompt wording and few-shot example selection strategy (random vs. error-focused vs. fluency-focused) while holding model and dataset constant. Measure impact on both GLEU scores and qualitative correction quality to isolate prompt sensitivity effects.