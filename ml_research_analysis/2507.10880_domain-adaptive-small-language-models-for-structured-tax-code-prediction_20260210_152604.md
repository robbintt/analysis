---
ver: rpa2
title: Domain-Adaptive Small Language Models for Structured Tax Code Prediction
arxiv_id: '2507.10880'
source_url: https://arxiv.org/abs/2507.10880
tags:
- codes
- product
- data
- such
- structured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurately predicting hierarchical
  tax codes (HSN/SAC) from unstructured product and service descriptions, a critical
  need in tax compliance for multinational firms. The proposed solution employs a
  domain-adaptive small language model (SLM) with an encoder-decoder architecture
  (specifically T5) to treat tax code prediction as a structured sequence generation
  task, decomposing codes into hierarchical components (chapter, heading, sub-heading,
  tariff) and generating them sequentially.
---

# Domain-Adaptive Small Language Models for Structured Tax Code Prediction

## Quick Facts
- **arXiv ID:** 2507.10880
- **Source URL:** https://arxiv.org/abs/2507.10880
- **Authors:** Souvik Nath; Sumit Wadhla; Luis Perez
- **Reference count:** 15
- **Primary result:** T5 encoder-decoder model achieves 70% precision, 61% recall, and 65% F1-score on hierarchical tax code prediction

## Executive Summary
This paper addresses the challenge of accurately predicting hierarchical tax codes (HSN/SAC) from unstructured product and service descriptions for multinational firms. The authors propose a domain-adaptive small language model (SLM) with an encoder-decoder architecture that treats tax code prediction as a structured sequence generation task. By decomposing codes into hierarchical components and generating them sequentially, the model learns conditional probabilities at each level. The approach significantly outperforms flat classifiers, BERT, and DistilGPT2 models, demonstrating the effectiveness of treating this as a cross-domain mapping problem rather than simple classification.

## Method Summary
The method employs a T5 encoder-decoder architecture (60.5M parameters) to predict hierarchical tax codes by decomposing them into sequential components: chapter, heading, sub-heading, and tariff. The model is fine-tuned on domain-specific data with specialized tokenization that adds hierarchical tokens (e.g., `<hsn_ch_12>`) to the vocabulary. During inference, a constrained beam search ensures generated codes adhere to the tax taxonomy by restricting candidate tokens at each step to only those valid for the previously generated hierarchy level. Text preprocessing includes cleaning, brand masking, and enrichment from external databases to improve description quality before prediction.

## Key Results
- T5 encoder-decoder model achieves 70% precision, 61% recall, and 65% F1-score on tax code prediction
- Model demonstrates high agreement with expert labels (Cohen's Kappa of 0.47)
- Significantly outperforms flat classifiers, BERT, and DistilGPT2 on the structured sequence prediction task
- Shows the effectiveness of treating tax code prediction as a sequence generation problem rather than classification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decomposing hierarchical tax codes into sequential components improves prediction accuracy over flat classification.
- **Mechanism**: The model learns conditional probabilities at each hierarchical level (e.g., P(Heading_i | X, Chapter_i)), reducing output space complexity and capturing dependencies within the taxonomy.
- **Core assumption**: Tax codes have meaningful internal hierarchy that aids prediction.
- **Evidence anchors**: Abstract states decomposition helps model differentiate between code segments; hierarchical prediction validated in related work.
- **Break condition**: If tax codes were random numbers without internal hierarchy, sequential dependency learning would fail.

### Mechanism 2
- **Claim**: Encoder-decoder architecture outperforms encoder-only and decoder-only models for mapping text to structured codes.
- **Mechanism**: Encoder processes input description while decoder generates code sequence, providing clear separation of understanding and generation duties.
- **Core assumption**: Tax code determination is analogous to neural machine translation (cross-domain mapping).
- **Evidence anchors**: T5 outperforms BERT and DistilGPT2; encoder-decoder effectiveness in cross-space mapping tasks.
- **Break condition**: For trivial output structures, encoder-decoder overhead might yield diminishing returns.

### Mechanism 3
- **Claim**: Constrained beam search ensures generated codes are valid within the tax taxonomy.
- **Mechanism**: Restricts candidate pool at each decoding step to only those legally allowed by preceding components, preventing impossible code combinations.
- **Core assumption**: Valid tax code combinations are fixed and available as constraint map.
- **Evidence anchors**: Algorithm 1 pseudocode explicitly defines constrained candidate selection; ensures contextually valid output.
- **Break condition**: If taxonomy constraints are incomplete or new codes are introduced, the mechanism may incorrectly suppress correct predictions.

## Foundational Learning

- **Concept**: **Sequence-to-Sequence (Seq2Seq) Modeling**
  - **Why needed here**: Core innovation moves from classification to generation; understanding encoder-decoder differences from BERT/GPT is critical.
  - **Quick check question**: Can you explain why a masked language model (like BERT) struggles with generating coherent sequential output compared to a decoder?

- **Concept**: **Constrained Decoding / Guided Generation**
  - **Why needed here**: Paper relies on forcing model to obey tax hierarchy grammar; understanding logit manipulation and beam search constraints is critical.
  - **Quick check question**: How does "constrained beam search" differ from "nucleus sampling" in terms of guaranteeing output validity?

- **Concept**: **Tokenization & Special Tokens**
  - **Why needed here**: Model creates specific embeddings for code segments; understanding custom token impact on vocabulary and embeddings is necessary for replication.
  - **Quick check question**: Why is it necessary to add specialized tokens like `<DASH>` or `<hsn_ch...>` to the tokenizer's vocabulary before fine-tuning?

## Architecture Onboarding

- **Component map**: Text Preprocessing -> Tokenizer -> T5 Encoder-Decoder -> Constrained Beam Search -> Sequence Reconstruction
- **Critical path**: Text enrichment phase (fetching metadata) and constrained beam search implementation are most distinct features; missing either degrades performance to baseline levels.
- **Design tradeoffs**:
  - SLM vs. LLM: T5 (60.5M params) chosen for cost/latency over massive models, accepting ~70% precision ceiling
  - Recall vs. Precision: Higher Precision (70%) than Recall (61%) suggests conservative model behavior influenced by beam search constraints
- **Failure signatures**:
  - Over-reliance on Placeholder Tokens: Bias toward `<UNK>` tokens if training data is noisy
  - Data Drift: Performance fluctuations over time indicate sensitivity to changes in descriptions or tax laws
  - Hallucinated Hierarchies: Without constrained beam search, model might predict impossible codes
- **First 3 experiments**:
  1. Baseline Validation: Replicate T5 vs. DistilGPT2 vs. BERT comparison to verify encoder-decoder advantage
  2. Constraint Ablation: Compare standard beam search vs. constrained beam search to quantify validity gain
  3. Tokenization Stress Test: Compare specialized tokens vs. standard tokenization impact on performance

## Open Questions the Paper Calls Out

- **Question**: Can the domain-adaptive encoder-decoder architecture effectively generalize to other government-mandated taxonomies with different structural depths, such as UNSPSC or Brazil's NCM?
  - **Basis in paper**: Abstract explicitly states approach "can also be scaled to other government-mandated tax commodity codes" but provides no experimental validation
  - **Why unresolved**: Current results specific to 8-digit HSN and 6-digit SAC structures; uncertain if constrained beam search requires re-engineering for different branching factors
  - **What evidence would resolve it**: Experimental results showing F1-scores and Kappa coefficients for T5 architecture fine-tuned on UNSPSC or NCM datasets

## Limitations
- Evaluation framework doesn't adequately address edge cases like newly introduced tax codes or products spanning multiple categories
- Model shows sensitivity to data drift over time with performance fluctuations, but causes and mitigation strategies aren't analyzed
- Text enrichment relies on proprietary "internal product database" making full impact assessment difficult

## Confidence
- **High Confidence (70-90%)**: Encoder-decoder architecture outperforms alternatives; constrained beam search ensures validity
- **Medium Confidence (40-70%)**: Hierarchical decomposition improves accuracy; specialized tokenization helps performance
- **Low Confidence (0-40%)**: SLM choice represents optimal tradeoff; text enrichment provides substantial gains

## Next Checks
1. **Constraint Ablation Study**: Implement and compare model performance using standard beam search versus constrained beam search to quantify specific contribution of hierarchical constraints.
2. **Tokenization Impact Analysis**: Train two identical models—one with specialized hierarchical tokens and one with standard T5 tokenization—on same dataset to measure isolated impact of custom tokenization.
3. **Cross-Domain Generalization Test**: Evaluate trained model on tax code prediction for product descriptions from different industries or geographic regions not represented in training data.