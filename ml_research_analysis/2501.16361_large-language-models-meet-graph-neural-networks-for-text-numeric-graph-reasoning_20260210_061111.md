---
ver: rpa2
title: Large Language Models Meet Graph Neural Networks for Text-Numeric Graph Reasoning
arxiv_id: '2501.16361'
source_url: https://arxiv.org/abs/2501.16361
tags:
- graph
- gene
- cell
- embedding
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a new Text-Numeric Graph (TNG) structure that
  integrates textual annotations and numeric values for graph reasoning in scientific
  discovery. It proposes a joint Large Language Model (LLM) and Graph Neural Network
  (GNN) approach to analyze TNGs, specifically Text-Numeric Omics Signaling Graphs
  (TOSGs) generated from single-cell RNA sequencing data.
---

# Large Language Models Meet Graph Neural Networks for Text-Numeric Graph Reasoning

## Quick Facts
- arXiv ID: 2501.16361
- Source URL: https://arxiv.org/abs/2501.16361
- Reference count: 0
- Key outcome: 0.88-0.89 accuracy, 0.93-0.95 F-score on cirrhosis, Alzheimer's, PDAC datasets using joint LLM-GNN approach

## Executive Summary
This paper introduces Text-Numeric Graphs (TNGs) that integrate textual annotations with numeric values for graph reasoning in scientific discovery. The authors propose a joint Large Language Model (LLM) and Graph Neural Network (GNN) approach that analyzes TNGs, specifically Text-Numeric Omics Signaling Graphs (TOSGs) generated from single-cell RNA sequencing data. By combining gene expression data with PubMedBert sentence embeddings, the model achieves significant improvements in classification accuracy and F-score compared to baseline models, demonstrating the effectiveness of integrating LLMs with GNNs for biological pathway inference.

## Method Summary
The proposed LLM-GNN model processes scRNA-seq data by first generating sentence embeddings from gene descriptions using PubMedBert (768-dim, mean pooled). An Expander module projects gene expression values through MLPs and concatenates them with text embeddings before reducing to hidden size. The Gene Encoder applies L transformer layers with centrality/spatial/edge encodings (Graphormer-style). A Path Encoder scores predefined biological paths via attention with positional/edge encodings and cross-attention to path sentence embeddings. The Graph Encoder applies trainable global path weights (sigmoid) and combines layer-wise outputs via Jumping Knowledge (concat + max-pool) before classification. The model is trained on three datasets (cirrhosis, Alzheimer's, PDAC) with 0.7/0.1/0.2 train/val/test splits, repeated 3 times.

## Key Results
- Achieved classification accuracy of 0.88-0.89 across three disease datasets
- Reached F-score of 0.93-0.95, significantly outperforming baseline models
- Demonstrated effective pathway inference through integrated text-numeric graph reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint encoding of textual priors with numeric expression data improves graph reasoning over either modality alone
- Mechanism: The Expander module projects scalar gene expression values through MLPs, then concatenates them with PubMedBert sentence embeddings (768-dim) before reducing to model hidden size
- Core assumption: Textual descriptions from literature encode functional relationships that generalize across samples, and expression levels modulate relevance
- Evidence anchors: Model combines gene expression with sentence embeddings through Expander module (Page 7-8); GALAX paper confirms similar gains from combining quantitative features with textual biological knowledge
- Break condition: If textual descriptions lack domain-specific functional information, sentence embeddings provide no signal, and performance degrades to numeric-only baseline

### Mechanism 2
- Claim: Hierarchical path-level attention enables interpretable pathway mining beyond node-level prediction
- Mechanism: Gene embeddings are scattered into predefined path sequences, scored via learned importance weights with positional and edge encodings, then aggregated via ScatterSoftmax within each path
- Core assumption: Predefined biological paths contain meaningful signal, and attention weights reflect path importance rather than spurious correlations
- Evidence anchors: Gene encoder connects to path encoder layer with expression-aware path embedding computation (Page 10-11); PathFinder baseline uses attention values for intra-cell communication networks (Page 4)
- Break condition: If predefined paths are incomplete or noisy, attention may distribute randomly; path importance scores become uninterpretable

### Mechanism 3
- Claim: Trainable global path weights combined with Jumping Knowledge pooling stabilize graph-level predictions across layers
- Mechanism: A single trainable path score vector M (shared across all samples and layers) is passed through sigmoid to weight path embeddings; layer-wise graph embeddings are concatenated and max-pooled via Jumping Knowledge networks
- Core assumption: Path importance is relatively stable across the dataset, and layer-wise features capture different granularity levels that benefit from combination
- Evidence anchors: Trainable path score M introduced to learn robust important score across whole dataset (Page 11); G = MaxPooling(Concat(g_1, g_2, ..., g_L)) (Page 12)
- Break condition: If path importance varies significantly across disease states or cell types, global weights oversmooth distinctions, reducing sensitivity to condition-specific pathways

## Foundational Learning

- Concept: **Graph Neural Networks with Transformer Attention (Graphormer-style)**
  - Why needed here: The gene encoder uses centrality, spatial, and edge encodings atop standard self-attention to incorporate graph topologyâ€”without this, the model is just a transformer ignoring network structure
  - Quick check question: Can you explain why vanilla transformers fail to encode node adjacency without explicit bias terms?

- Concept: **Sentence Embeddings via Mean Pooling**
  - Why needed here: PubMedBert token embeddings are averaged to produce fixed-size gene/path representations
  - Quick check question: What information is lost when mean-pooling token embeddings compared to attention-based pooling?

- Concept: **Jumping Knowledge Networks**
  - Why needed here: Final graph embeddings combine outputs from all encoder layers via max-pooling to address over-smoothing in deep GNNs
  - Quick check question: Why might max-pooling over layer outputs outperform using only the final layer for graph classification?

## Architecture Onboarding

- Component map: Input Layer (scRNA expression + PubMedBert embeddings + predefined paths) -> Gene Encoder (transformer + Graphormer encodings) -> Path Encoder (cross-attention) -> Graph Encoder (trainable weights + Jumping Knowledge) -> Output (classification + path importance)

- Critical path: Sentence embedding quality -> Expander fusion quality -> Predefined path coverage -> Trainable path weight initialization

- Design tradeoffs:
  - Shared vs. sample-specific path weights: Shared improves robustness but may miss condition-specific pathways
  - Mean pooling vs. attention for sentence embeddings: Simpler but potentially loses token-level nuance
  - Max vs. mean pooling in Jumping Knowledge: Max captures strongest signals but is sensitive to outliers

- Failure signatures:
  - Classification accuracy near random (0.5): Check Expander output dimensions, verify sentence embeddings are non-zero
  - Path attention uniformly distributed: Predefined paths may lack overlap with expressed genes; verify path-gene mapping
  - Training instability: Trainable path weights may explode; add L2 regularization or clamp sigmoid outputs

- First 3 experiments:
  1. Ablation: Remove sentence embeddings (set all to zeros) to quantify text contribution. Expect accuracy drop from ~0.88 to ~0.72 (baseline GCN performance)
  2. Path coverage audit: Compute percentage of expressed genes that appear in at least one predefined path. If <50%, expand path database before retraining
  3. Layer-wise probing: Extract graph embeddings g_l from each layer and train linear probes for classification. If layer 1 outperforms layer L, reduce encoder depth or strengthen residual connections

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the joint LLM-GNN model vary when utilizing different Large Language Models or embedding strategies for the textual annotations?
- Basis in paper: The methodology exclusively employs PubMedBert for generating sentence embeddings without evaluating how alternative models might influence the integration of textual and numeric data
- Why unresolved: The paper does not conduct ablation studies regarding the choice of the language model, leaving the sensitivity of the graph reasoning to the specific semantic quality of PubMedBert unknown
- What evidence would resolve it: Comparative analysis of model accuracy and F-scores when PubMedBert is substituted with other text encoders (e.g., BioGPT, SciBERT) on the same TOSG datasets

### Open Question 2
- Question: To what extent does the reliance on pre-defined paths constrain the model's ability to infer novel signaling pathways not present in existing databases?
- Basis in paper: The Path Encoder utilizes a "pre-defined path list" sampled from gene network databases, implying that the model can only reason over relationships that are already biologically cataloged
- Why unresolved: While the model improves classification, its capacity to discover de novo pathways versus simply re-weighting known paths remains unclear
- What evidence would resolve it: Validation on synthetic datasets where ground-truth "novel" paths are intentionally excluded from the pre-defined list to see if the model can identify them

### Open Question 3
- Question: Is the proposed Text-Numeric Graph (TNG) architecture generalizable to other scientific domains beyond single-cell omics?
- Basis in paper: The paper defines the TNG structure broadly for "complex systems" but restricts experimental validation entirely to Text-Omic Signaling Graphs (TOSG) derived from scRNA-seq data
- Why unresolved: The efficacy of combining LLM textual priors with GNN numeric reasoning has not been demonstrated in other text-numeric contexts mentioned in the introduction
- What evidence would resolve it: Application of the TNG framework to non-biological datasets (e.g., material science or financial networks) to verify that the integration strategy improves reasoning beyond the biological domain

## Limitations
- Missing critical hyperparameters (learning rate, batch size, encoder depth, optimizer, training epochs) make exact reproduction challenging
- Unspecified predefined path database source and generation method introduces variability in path coverage and quality
- No implementation code provided; assumptions required for data preprocessing pipeline and architecture details

## Confidence
- **High Confidence**: Classification accuracy improvements (0.88-0.89) on three independent datasets, supported by direct metric comparisons in Table 3
- **Medium Confidence**: Mechanism 1 (joint encoding benefits) based on described architecture and GALAX corpus support
- **Low Confidence**: Mechanism 3 (trainable global path weights) due to weak corpus validation and untested break conditions

## Next Checks
1. Ablation Study: Remove sentence embeddings by setting all to zero vectors, retrain model, and measure accuracy drop. If accuracy falls from ~0.88 to near the GCN baseline of ~0.72, this validates Mechanism 1's contribution.

2. Path Coverage Audit: Compute the percentage of expressed genes (present in >1% of cells) that appear in at least one predefined biological path. If coverage is below 50%, expand the path database from STRING or BioGRID before retraining, as poor coverage would invalidate Mechanism 2.

3. Layer-wise Probing: Extract intermediate graph embeddings g_l from each encoder layer and train linear classifiers. If early layers (l=1) outperform the final layer (l=L), this suggests over-smoothing and warrants reducing encoder depth or strengthening residual connections.