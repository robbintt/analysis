---
ver: rpa2
title: 'Control Illusion: The Failure of Instruction Hierarchies in Large Language
  Models'
arxiv_id: '2502.15851'
source_url: https://arxiv.org/abs/2502.15851
tags:
- constraint
- instruction
- user
- constraints
- priority
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a systematic framework for evaluating how
  well large language models enforce hierarchical instruction priorities. Through
  controlled experiments with six state-of-the-art models, the authors demonstrate
  that system/user prompt separation fails to reliably establish instruction hierarchies,
  even for simple verifiable formatting conflicts.
---

# Control Illusion: The Failure of Instruction Hierarchies in Large Language Models

## Quick Facts
- arXiv ID: 2502.15851
- Source URL: https://arxiv.org/abs/2502.15851
- Reference count: 26
- Key outcome: System/user prompt separation fails to reliably establish instruction hierarchies; social hierarchy framings show stronger influence on model behavior than explicit role separation

## Executive Summary
This paper systematically evaluates how well large language models enforce hierarchical instruction priorities when conflicting constraints are presented. Through controlled experiments with six state-of-the-art models, the authors demonstrate that explicit system/user role separation (system > user) fails to reliably establish instruction hierarchies, even for simple verifiable formatting conflicts. Models rarely acknowledge conflicting instructions and exhibit strong inherent biases toward certain constraint types regardless of priority designation. Notably, societal hierarchy framings (authority, expertise, consensus) show stronger influence on model behavior than explicit system/user roles, suggesting pretraining-derived social structures function as latent behavioral priors with potentially greater impact than post-training guardrails.

## Method Summary
The authors constructed a dataset of 100 base tasks from IFEval, combined with 6 mutually exclusive constraint pairs (language, case, word length, sentence count, keyword usage, keyword frequency), creating 1,200 test points across two versions: simple (15 words avg) and rich-context (120 words avg). They evaluated six models (Qwen2.5-7B, Llama-3.1-8B/70B, Claude-3-5-sonnet, GPT-4o-mini, GPT-4o) across three baseline configurations (Instruction Following, No Priority) and three separation configurations (Pure, Task Repeated, Emphasized), plus three societal hierarchy framings (authority, expertise, consensus). The primary metrics were Priority Adherence Ratio (PAR), Constraint Bias (CB), Primary/Secondary/Non-Compliance Rates (R1/R2/R3), and Explicit Conflict Acknowledgement Rate (ECAR).

## Key Results
- System/user separation yields low PAR values (typically 0.3-0.5) across all tested models and constraint pairs
- Social hierarchy framings produce significantly higher PAR than explicit system/user roles (e.g., Qwen-7B PAR rises from 14.4% to 65.8% for consensus framing)
- Models exhibit strong constraint-type biases (CB often exceeding |0.5|) that override designated priority
- Conflict acknowledgment does not reliably lead to correct priority enforcement, with low R1 conditional on acknowledgment
- Qwen-7B shows strongest hierarchy enforcement while Claude shows weakest, suggesting model architecture differences affect behavior

## Why This Works (Mechanism)

### Mechanism 1
Societal hierarchy framings produce higher instruction priority adherence than explicit system/user role separation. Pretraining on natural language corpora exposes models to recurring social and institutional authority patterns (e.g., CEO vs. intern, expert vs. novice, majority vs. minority). These patterns become latent inductive biases. At inference, social framing cues activate these learned hierarchies and exert stronger control over constraint selection than post-training role tokens that lack deep corpus grounding. Evidence: Table 4 shows PAR improvements from system/user to consensus framing across all models; reasoning-up paper supports generalizability. Break condition: Anti-hierarchy fine-tuning or architectural isolation of social framing tokens.

### Mechanism 2
Models exhibit strong, consistent constraint-type biases that override designated priority, independent of which constraint is marked primary. Constraint types are unevenly distributed in pretraining data. Models internalize frequency and context correlations as default preferences. When conflicting constraints are presented, these learned priors bias selection toward the statistically favored option, regardless of explicit priority signals. Evidence: Figure 4 shows all models favor lowercase over uppercase, prefer more sentences, and tend to avoid keywords with bias magnitudes often exceeding |0.5|; corpus evidence is weak for direct validation. Break condition: Balanced constraint fine-tuning equalizing frequency and reward parity.

### Mechanism 3
Conflict acknowledgment does not reliably lead to correct priority enforcement; models often recognize contradictions but still select the lower-priority constraint. Conflict detection and constraint selection are partially decoupled processes. A model can verbalize conflict awareness without invoking a separate, robust priority-resolution mechanism. When priority-resolution is weak or absent, the model falls back to constraint bias or recency heuristics. Evidence: Table 3 shows Llama-70B acknowledges conflicts 20.3% of the time but achieves only 30.7% R1 when acknowledging; diagnose-localize-align paper supports broader pattern. Break condition: Training linking acknowledgment to priority-correct outputs or architectural enforcement post-acknowledgment.

## Foundational Learning

- Concept: **Instruction Hierarchy**
  - Why needed here: The paper evaluates whether explicit role-based priority (system > user) functions as intended; understanding hierarchy is prerequisite to interpreting all reported metrics.
  - Quick check question: Given a system prompt "Output in English" and a user prompt "Output in French," which output should an ideal hierarchy-enforcing model produce?

- Concept: **Behavioral Priors from Pretraining**
  - Why needed here: The paper's central claim is that pretraining-derived social and constraint biases override post-training role signals; grasping this distinction is essential for mechanism interpretation.
  - Quick check question: If a model's pretraining corpus contains 10x more lowercase text than uppercase, what prior would you expect for a case-conflict task with no priority designation?

- Concept: **Constraint Bias (CB) Metric**
  - Why needed here: CB quantifies model preference for one constraint over another, independent of priority; readers must understand how CB is computed to interpret Figure 4 and cross-model comparisons.
  - Quick check question: If Rc1 = 70 and Rc2 = 30 for a given constraint pair, what is the Constraint Bias value, and which constraint is favored?

## Architecture Onboarding

- Component map: Input layer (system message, user message) -> Conflict detection pathway (generates acknowledgment text) -> Constraint selection pathway (determines which constraint properties appear in output) -> Latent prior activation (social framing tokens influence constraint selection via pretraining associations)

- Critical path: 1) Prompt arrives with conflicting constraints distributed across system/user or within single message; 2) Model encodes both constraints; conflict may or may not trigger acknowledgment; 3) Constraint selection occurs under influence of (a) explicit priority markers, (b) latent constraint-type bias, (c) latent social hierarchy priors; 4) Output is evaluated against programmatically verifiable constraint satisfaction criteria

- Design tradeoffs: Simple vs. rich context (simple isolates prioritization behavior; rich tests robustness); Pure vs. emphasized separation (emphasis trades naturalistic deployment against explicit priority declaration); System/user vs. social framing (social hierarchies yield higher adherence but introduce uncontrolled latent variables and adversarial exploitation vectors)

- Failure signatures: Bias-dominated failure (high |CB|, low PAR; model consistently selects one constraint regardless of priority); Indecisive failure (low PAR, moderate or low |CB|; model fails to enforce priority even without strong bias); Acknowledgment-decoupled failure (moderate-to-high ECAR but low R1 conditional on acknowledgment)

- First 3 experiments: 1) Replicate Pure Separation on language/case constraint pairs to confirm R1 and CB values for target model; 2) Test social framing variants (authority, expertise, consensus) on same constraint pairs to measure PAR uplift; 3) Introduce balanced-constraint fine-tuning on held-out set and re-evaluate CB to assess bias reduction

## Open Questions the Paper Calls Out

- What are the underlying architectural or representational mechanisms that cause the failure of system/user separation in current LLMs? The authors state in Limitations that "the underlying mechanisms behind the observed failures are still unexplored."

- Does the observed failure of instruction hierarchies generalize to complex semantic constraints, such as safety rules or agentic behaviors? The paper notes that "More complex forms of control than formatting constraints, such as safety rules... remain outside our evaluation."

- Can latent hierarchical priors derived from societal training data be effectively audited and attenuated to prevent adversarial misuse? The authors explicitly ask: "can, and should, latent hierarchical priors from pretraining be surfaced, audited, or attenuated?"

## Limitations

- Dataset Scope and Generalization: The study uses 100 tasks from IFEval, which may not represent the full diversity of real-world instruction hierarchies, and mutual exclusivity requirement simplifies conflict detection but may not capture nuanced overlapping constraints.

- Prompt Engineering Gaps: Exact few-shot prompt templates for conflict acknowledgment extraction are not provided, creating uncertainty about whether different acknowledgment extraction methods would yield comparable ECAR values.

- Social Hierarchy Mechanism: The central claim about pretraining-derived social structures requires stronger causal evidence; current design shows correlation but cannot definitively rule out alternative explanations like token frequency effects.

## Confidence

- High Confidence: Fundamental finding that system/user prompt separation fails to reliably establish instruction hierarchies (consistent PAR values below 0.6 across all models); observation that models exhibit strong, consistent constraint-type biases regardless of priority designation (validated by CB metrics and cross-model consistency)

- Medium Confidence: Comparative effectiveness of societal hierarchy framings over explicit system/user roles (PAR uplift is substantial but may be influenced by uncontrolled variables); claim that conflict acknowledgment does not reliably lead to correct priority enforcement (ECAR-R1 relationship is complex and model-dependent)

- Low Confidence: Precise mechanism by which pretraining-derived social structures override post-training guardrails (causal chain is inferred but not directly measured); generalizability of constraint bias findings to domains outside tested 6 pairs

## Next Checks

1. Cross-Corpus Replication: Replicate Pure Separation experiments on different task corpus (e.g., HumanEval or FLASK) to assess whether hierarchy failures persist across task domains and whether constraint bias patterns remain consistent.

2. Causal Mechanism Test: Design ablation study where social hierarchy tokens are removed from pretraining corpus (via controlled fine-tuning) or masked during inference to test whether PAR improvements are specifically due to social framing activation.

3. Acknowledgment-Resolution Superposition: Train small model (or fine-tune subset of tested models) with explicit supervision linking conflict acknowledgment to priority-correct outputs, then measure whether R1 conditional on acknowledgment increases significantly compared to baseline models.