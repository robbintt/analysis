---
ver: rpa2
title: Improving the Accuracy and Efficiency of Legal Document Tagging with Large
  Language Models and Instruction Prompts
arxiv_id: '2504.09309'
source_url: https://arxiv.org/abs/2504.09309
tags:
- legal
- label
- labels
- legal-llm
- classi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Legal-LLM, a novel approach for legal multi-label
  classification that leverages the instruction-following capabilities of Large Language
  Models (LLMs). The method reframes the classification task as a structured generation
  problem, prompting the LLM to directly output relevant legal categories for a given
  document.
---

# Improving the Accuracy and Efficiency of Legal Document Tagging with Large Language Models and Instruction Prompts

## Quick Facts
- **arXiv ID**: 2504.09309
- **Source URL**: https://arxiv.org/abs/2504.09309
- **Reference count**: 30
- **Primary result**: Legal-LLM achieves 0.83 micro-F1 and 0.76 macro-F1 on POSTURE50K, outperforming strong baselines

## Executive Summary
This paper introduces Legal-LLM, a novel approach for legal multi-label classification that reframes the task as structured text generation. By leveraging instruction-following capabilities of Large Language Models (LLMs) through fine-tuning, Legal-LLM directly outputs relevant legal categories for given documents. The method is evaluated on POSTURE50K and EURLEX57K datasets, demonstrating superior performance over traditional and Transformer-based baselines. Ablation studies validate the effectiveness of weighted loss for handling label imbalance and the importance of instruction prompt design.

## Method Summary
Legal-LLM fine-tunes a pre-trained LLM (further pre-trained on legal documents) using instruction prompts that reframe multi-label classification as structured generation. The input format is "Identify all applicable legal categories for the following legal text: [Document Text]" with output as comma-separated legal labels. A weighted cross-entropy loss function, based on inverse label frequency, mitigates class imbalance. The model is trained on instruction-output pairs and evaluated using micro-F1 and macro-F1 metrics.

## Key Results
- Legal-LLM achieves 0.83 micro-F1 and 0.76 macro-F1 on POSTURE50K
- Legal-LLM achieves 0.80 micro-F1 and 0.71 macro-F1 on EURLEX57K
- Weighted loss improves macro-F1 from 0.69 to 0.71 on EURLEX57K for rare labels
- Original prompt marginally outperforms alternative phrasing (0.80 vs 0.79 micro-F1)

## Why This Works (Mechanism)

### Mechanism 1
Reframing multi-label classification as structured text generation improves label prediction accuracy and captures label dependencies. The model generates comma-separated label sequences conditioned on instruction prompts, allowing implicit modeling of label co-occurrence patterns through sequential generation.

### Mechanism 2
Instruction-based fine-tuning with domain-specific prompts guides the LLM to apply its pre-trained legal knowledge more effectively. A carefully crafted prompt concatenated with the document conditions the model to associate the instruction pattern with the label generation task.

### Mechanism 3
Inverse-frequency weighted loss mitigates label imbalance and improves macro-F1 on rare categories. During fine-tuning, each label's loss contribution is weighted by the inverse of its training frequency, increasing the gradient signal for minority labels.

## Foundational Learning

- **Multi-label classification vs. single-label classification**
  - Why needed here: Legal documents can belong to multiple categories simultaneously. Traditional softmax classifiers assume mutual exclusivity.
  - Quick check question: Given a legal case about a patent dispute in an employment contract, should the model predict one label or multiple? Why does this matter for loss function design?

- **Label imbalance and macro-F1 vs. micro-F1**
  - Why needed here: Legal datasets have skewed label distributions. Micro-F1 aggregates across all predictions (favors majority), while macro-F1 averages per-label scores (sensitive to rare labels).
  - Quick check question: If a model achieves 0.83 micro-F1 but 0.76 macro-F1, what does this suggest about its performance on rare categories?

- **Instruction tuning for LLMs**
  - Why needed here: Pre-trained LLMs are not inherently task-aligned. Instruction tuning teaches the model to map natural language prompts to desired outputs.
  - Quick check question: Why might fine-tuning with instruction prompts outperform standard supervised fine-tuning without instructions for this task?

## Architecture Onboarding

- **Component map**: Pre-trained LLM backbone -> Instruction prompt template -> Weighted cross-entropy loss -> Decoder head -> Post-processing
- **Critical path**: 1) Compute label frequency statistics â†’ derive inverse-frequency weights; 2) Construct instruction-tuning dataset; 3) Fine-tune LLM with weighted loss; 4) Generate and parse label sequences at inference
- **Design tradeoffs**: Prompt phrasing sensitivity, document length limits, label format choices, weighted loss aggressiveness
- **Failure signatures**: Hallucinated labels, empty generation, majority-label bias, long-document degradation
- **First 3 experiments**: 1) Baseline replication on EURLEX57K; 2) Ablation on weighted loss comparison; 3) Prompt sensitivity test across 3-5 variations

## Open Questions the Paper Calls Out

- Can hierarchical attention mechanisms or truncation strategies effectively mitigate performance degradation for documents longer than 512 tokens?
- Can more sophisticated techniques beyond weighted loss functions further improve classification performance on legal datasets with extreme label imbalance?
- Do alternative LLM architectures yield superior performance for legal document tagging compared to the specific pre-trained model utilized in this study?

## Limitations

- Base model architecture and training details are not specified, only "a model further pre-trained on legal documents"
- Hyperparameter configuration (learning rate, batch size, epochs) is not disclosed
- Limited exploration of prompt sensitivity range with only two prompt variants tested

## Confidence

- **High Confidence**: The core methodological claim that reframing multi-label classification as structured generation with instruction prompts is effective
- **Medium Confidence**: The weighted loss mechanism specifically improves macro-F1 for rare labels, though the exact weighting formula remains underspecified
- **Medium Confidence**: Superiority over strong baselines could be influenced by base model quality differences rather than the instruction-based approach alone

## Next Checks

1. **Prompt Robustness Test**: Systematically vary the instruction prompt across 5-10 different formulations to quantify performance stability and sensitivity to prompt engineering
2. **Base Model Dependency Analysis**: Reproduce the method using different pre-trained LLM architectures (T5, BART, GPT-2 variants) all fine-tuned on the same legal corpus
3. **Label Format Generalization**: Test alternative structured output formats (JSON, XML, or key-value pairs) instead of comma-separated lists to evaluate whether the generative advantage persists