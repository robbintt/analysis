---
ver: rpa2
title: 'NaturalVoices: A Large-Scale, Spontaneous and Emotional Podcast Dataset for
  Voice Conversion'
arxiv_id: '2511.00256'
source_url: https://arxiv.org/abs/2511.00256
tags:
- speech
- emotional
- speaker
- oices
- naturalv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NaturalVoices, the first large-scale spontaneous
  podcast dataset specifically designed for emotion-aware voice conversion. It contains
  5,049 hours of real-life expressive speech from thousands of speakers across diverse
  topics and settings.
---

# NaturalVoices: A Large-Scale, Spontaneous and Emotional Podcast Dataset for Voice Conversion

## Quick Facts
- arXiv ID: 2511.00256
- Source URL: https://arxiv.org/abs/2511.00256
- Reference count: 40
- NaturalVoices contains 5,049 hours of spontaneous podcast speech with comprehensive automatic annotations for voice conversion research

## Executive Summary
NaturalVoices introduces the first large-scale spontaneous podcast dataset specifically designed for emotion-aware voice conversion (VC). It contains 5,049 hours of real-life expressive speech from thousands of speakers across diverse topics and settings. The dataset includes comprehensive automatic annotations for emotion (categorical and attribute-based), speech quality, transcripts, speaker identity, and sound events. A modular open-source pipeline enables flexible filtering and task-specific subset construction. Experiments show that NaturalVoices supports high-quality, emotionally expressive voice conversion, while revealing limitations of current models when applied to large-scale spontaneous data. It serves both as a valuable resource and a challenging benchmark for advancing voice conversion research.

## Method Summary
NaturalVoices was constructed by collecting 6,790 podcast episodes (5,049 hours total) and applying a comprehensive automatic annotation pipeline. The process involves document-level processing with ASR and speaker diarization, followed by segment-level annotation using multiple models for quality metrics (PESQ, STOI, SNR, DNSMOS), emotion (categorical labels via PEFT-SER, dimensional attributes via WavLM), sound events (AST), and speaker identity mapping. The open-source pipeline produces JSON metadata per segment, enabling flexible filtering by any criteria. The dataset is available through Hugging Face with both raw and filtered subsets.

## Key Results
- NaturalVoices enables training VC models that achieve high intelligibility (WER 0.454 on in-domain test) and speaker similarity while preserving emotional content
- Models trained on NaturalVoices show significant domain gap when evaluated on acted datasets (ESD), with WER dropping from 0.112 to 0.454
- DDDM-VC performance degrades when scaling from 50% to 100% training data, suggesting current architectures cannot fully leverage large-scale spontaneous data
- Emotion-balanced subsets (340 hours, 85 hours per emotion) enable controlled EVC experiments with consistent emotion transfer metrics

## Why This Works (Mechanism)

### Mechanism 1
Spontaneous podcast speech provides authentic emotional expressiveness that acted datasets systematically omit. Natural conversations contain genuine emotional state transitions, prosodic variability (pitch shifts, pauses, breath sounds), and context-dependent affect that performers cannot fully simulate. The dataset captures these properties at scale through in-the-wild recordings. Core assumption: Models trained on spontaneous speech will better generalize to real-world expressive speech than those trained on scripted/acted speech. Evidence anchors: [abstract] "spontaneous podcast recordings... captures expressive emotional variation across thousands of speakers"; [section II.A] "the same emotion can be expressed differently across individuals... meanings shift with context"; [corpus] Related work UniSS confirms "scarcity of paired speech data that retains expressive styles" as a key challenge. Break condition: If podcast speakers are effectively performing (semi-scripted personas), spontaneity advantage diminishes; if annotation models trained on acted data fail on spontaneous speech.

### Mechanism 2
Multi-level automatic annotations with flexible filtering enable task-specific subset construction for diverse VC scenarios. The pipeline produces segment-level metadata (speaker identity, emotion categories/attributes, speech quality metrics, sound events). Users filter by criteria (e.g., single-speaker, DNSMOS≥2.6, SNR≥30, duration 1-20s) to construct tailored datasets. Core assumption: Annotation quality is sufficient for practical filtering; downstream tasks have sufficiently distinct data requirements to justify modular selection. Evidence anchors: [abstract] "open-source pipeline with modular annotation tools and flexible filtering"; [section III.D] Filtering yielded 870.26 hours from 5,049 hours total for VC experiments; [corpus] Limited direct corpus evidence on annotation quality; neighboring papers focus on generation, not data curation. Break condition: If automatic annotations (emotion, speaker) have high error rates that propagate through filtering; if pre-filtered subsets already released reduce need for custom filtering.

### Mechanism 3
Large-scale spontaneous data exposes architectural limitations in current VC models that remain hidden on smaller, controlled datasets. In-the-wild variability (noise, emotion diversity, recording conditions) stresses models optimized for clean studio data. Scaling from 10%→50%→100% data shows inconsistent improvements for some architectures. Core assumption: Performance degradation or plateau at scale reflects model limitations rather than annotation noise or data quality issues. Evidence anchors: [abstract] "revealing limitations of current architectures when applied to large-scale spontaneous data"; [section V.C.5] "DDDM-VC performs the best at 50%, with speaker similarity declining at 100%... current VC architectures are not yet optimized to fully leverage large-scale, in-the-wild data"; [corpus] Maestro-EVC and ClapFM-EVC focus on architectural improvements (controllability, dual control), suggesting ongoing model development needs. Break condition: If degradation is primarily due to noisy annotations increasing with scale; if data quality filtering was insufficient and low-quality samples dominate at 100%.

## Foundational Learning

- **Voice Conversion (VC) Fundamentals**
  - Why needed here: NaturalVoices targets VC/EVC tasks; understanding speaker identity preservation vs. content preservation is essential for using the dataset correctly.
  - Quick check question: Can you explain the difference between "any-to-any" and "any-to-many" voice conversion, and why d-vector embeddings enable the former?

- **Emotion Representation (Categorical vs. Dimensional)**
  - Why needed here: Dataset provides both categorical labels (angry, happy, sad, neutral) and dimensional attributes (arousal, valence, dominance). Different tasks require different representations.
  - Quick check question: Why might dimensional attributes capture emotional nuance that discrete categories miss, and which metric (ECA vs. EECS) evaluates each?

- **Speech Quality Metrics (SNR, MOS, PESQ, STOI, DNSMOS)**
  - Why needed here: Filtering decisions require understanding what each metric measures and appropriate thresholds. The paper uses DNSMOS≥2.6, SNR≥30, ASR confidence≥0.7.
  - Quick check question: Which metric would you prioritize if your downstream application requires high intelligibility but can tolerate moderate background noise?

## Architecture Onboarding

- **Component map:**
  Data Collection -> Document-level processing (ASR + diarization) -> Segment-level annotation (quality metrics, emotion, sound events, speaker identity) -> Global speaker mapping -> Filtering pipeline

- **Critical path:**
  1. Download raw data or filtered subsets from HuggingFace (JHU-SmileLab)
  2. Define filtering criteria based on task (clean VC vs. noisy-robust vs. emotional VC)
  3. Extract subset; verify distributions match expectations
  4. Train baseline model (TriAAN-VC, ConsistencyVC, or DDDM-VC as reference)
  5. Evaluate on both NaturalVoices test set (in-domain) and ESD (out-of-domain)

- **Design tradeoffs:**
  - **Filtering strictness:** Stricter filters → cleaner data but smaller subset. Paper's 870h from 5,049h represents ~17% retention.
  - **Sample rate selection:** Original rates (44.1kHz dominant) preserve fidelity; downsampling to 16kHz reduces compute but loses high-frequency detail.
  - **Emotion-balanced vs. natural distribution:** Balanced subsets (85h per emotion) enable controlled EVC experiments but alter natural emotion frequencies (neutral dominates at 59%).

- **Failure signatures:**
  - Model performance degrades when scaling from 50% to 100% training data (observed with DDDM-VC) → suggests architecture cannot handle increased variability/noise
  - High WER/CER on NaturalVoices test set but low WER/CER on ESD → indicates model overfits to clean conditions
  - Low emotion category accuracy on ESD but high on NaturalVoices (or vice versa) → emotion representation mismatch between training and evaluation domains

- **First 3 experiments:**
  1. **Replicate baseline filtering:** Use paper's criteria (DNSMOS≥2.6, SNR≥30, ASR conf≥0.7, 1-20s, single-speaker, speech-only) to extract 870h subset; train TriAAN-VC and compare reported metrics.
  2. **Ablation on quality thresholds:** Vary DNSMOS (2.0, 2.6, 3.0) and SNR (20, 30, 40) to measure impact on WER, speaker similarity, and training stability.
  3. **Cross-domain generalization test:** Train on emotion-balanced NaturalVoices subset; evaluate emotion transfer accuracy (ECA, EECS) on both NaturalVoices test set and ESD to quantify domain gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can voice conversion architectures be redesigned to effectively leverage large-scale spontaneous data without suffering from performance degradation?
- Basis in paper: [explicit] The authors state, "These findings suggest that current VC architectures are not yet optimized to fully leverage large-scale, in-the-wild data," noting that models like DDDM-VC actually showed degraded speaker similarity when trained on 100% of the data compared to 50%.
- Why unresolved: Current state-of-the-art models (e.g., TriAAN-VC, ConsistencyVC, DDDM-VC) were primarily developed and tuned on smaller, controlled datasets, making them unable to handle the variability and scale of NaturalVoices.
- What evidence would resolve it: Development of a new VC architecture that consistently improves in both intelligibility and speaker similarity as the training data increases from 50% to 100% on the NaturalVoices benchmark.

### Open Question 2
- Question: Can the intelligibility gap between models trained on spontaneous speech versus those trained on acted speech be narrowed?
- Basis in paper: [inferred] The results in Table IX show that the DISSC model achieves a WER of 0.112 on the acted ESD test set but a much higher WER of 0.454 on the NaturalVoices test set, indicating a significant challenge in modeling spontaneous speech.
- Why unresolved: The paper suggests that spontaneous, in-the-wild speech presents challenges not fully addressed by current architectures, which struggle to maintain speech quality (MOS) on NaturalVoices compared to acted datasets.
- What evidence would resolve it: A model trained on NaturalVoices that achieves a WER on the NaturalVoices test set comparable to the WER achieved by models trained on ESD when tested on ESD.

### Open Question 3
- Question: Does training anti-spoofing systems on NaturalVoices improve the detection of emotion-targeted deepfake attacks compared to standard corpora?
- Basis in paper: [explicit] Section VI.B states, "NaturalVoices... enables training and evaluation of anti-spoofing systems under diverse, human-like expressive conditions, paving the way for more robust and prosody-aware defenses."
- Why unresolved: Current anti-spoofing datasets often rely on acted speech, leaving systems vulnerable to expressive attacks; NaturalVoices provides the resource to explore this, but the experiments have not yet been conducted.
- What evidence would resolve it: Experiments demonstrating that anti-spoofing models fine-tuned on NaturalVoices subsets achieve higher detection accuracy against emotion-spoofing benchmarks (e.g., EmoSpoof-TTS) than models trained on existing standard datasets.

## Limitations
- Automatic emotion annotation accuracy for spontaneous speech remains unverified - models trained on acted datasets may not generalize well to in-the-wild emotional expressions
- The filtering pipeline's sensitivity to annotation quality thresholds is unclear - performance drops at 100% scale could stem from annotation noise rather than architectural limitations
- Speaker identity mapping between diarization labels and MSP-Podcast human annotations may introduce errors that propagate through training

## Confidence
- **High Confidence**: Dataset size claims (5,049 hours), basic filtering criteria (DNSMOS≥2.6, SNR≥30), and core architecture descriptions (TriAAN-VC, DDDM-VC, ConsistencyVC)
- **Medium Confidence**: Automatic annotation quality (emotion categories, speaker identity), cross-dataset generalization results (NaturalVoices vs. ESD performance gap), and the interpretation of scale-dependent performance degradation
- **Low Confidence**: Exact hyperparameter settings for all baseline models, specific test set speaker identities used in experiments, and the reproducibility of the 870-hour filtering yield

## Next Checks
1. Verify emotion annotation accuracy by manually labeling 100 random segments and comparing against WavLM regression outputs
2. Replicate the 50%→100% training scale experiment with DDDM-VC using identical hyperparameters to confirm performance degradation pattern
3. Train a baseline VC model on emotion-balanced subset and test on both NaturalVoices and ESD to quantify domain gap in emotion transfer metrics