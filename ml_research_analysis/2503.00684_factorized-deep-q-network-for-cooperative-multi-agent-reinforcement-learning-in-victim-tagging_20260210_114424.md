---
ver: rpa2
title: Factorized Deep Q-Network for Cooperative Multi-Agent Reinforcement Learning
  in Victim Tagging
arxiv_id: '2503.00684'
source_url: https://arxiv.org/abs/2503.00684
tags:
- victim
- victims
- responder
- time
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of minimizing victim tagging time
  during mass casualty incidents (MCIs) by developing distributed heuristic policies
  and a factorized deep Q-network (FDQN) for cooperative multi-agent reinforcement
  learning. The problem is formalized as an integer linear program, and five practical
  heuristics are proposed to handle varying levels of communication uncertainty.
---

# Factorized Deep Q-Network for Cooperative Multi-Agent Reinforcement Learning in Victim Tagging

## Quick Facts
- arXiv ID: 2503.00684
- Source URL: https://arxiv.org/abs/2503.00684
- Reference count: 40
- Primary result: FDQN outperforms heuristics in small-scale MCIs but heuristics excel in large, uncertain environments

## Executive Summary
This paper develops distributed heuristic policies and a factorized deep Q-network (FDQN) for cooperative multi-agent reinforcement learning in victim tagging during mass casualty incidents. The work formalizes the problem as an integer linear program and proposes five practical heuristics to handle communication uncertainty. Extensive simulations compare these heuristics, revealing that local policies like LNVP are most efficient under constraints. FDQN is introduced to enable adaptive, scalable decision-making and is evaluated across diverse MCI scenarios. Results show FDQN outperforms heuristics in simpler cases, while heuristics excel in complex, high-uncertainty environments.

## Method Summary
The method develops distributed heuristic policies and FDQN for cooperative multi-agent reinforcement learning in victim tagging during mass casualty incidents. The problem is formalized as an integer linear program. Five practical heuristics are proposed: Random Victim Policy, Nearest Victim Policy, Local Nearest Victim Policy, Local Critical Victim Policy, and Local Grid Assignment Policy. FDQN uses VDN-style factorization with shared encoder (FC128→FC64 with ReLU) and per-agent heads, action masking via FSM constraints, and binned distance discretization. The approach is evaluated across diverse MCI scenarios with 3-12 responders, 5-24 victims, and grid sizes from 5×5 to 50×30.

## Key Results
- FDQN outperforms all heuristics in small-scale scenarios (R1-R3: 3 agents, 5 victims, 5×5 grid)
- Local policies like LNVP are most efficient under communication constraints
- Heuristics excel in complex, high-uncertainty environments (R7-R8: 12 agents, 24 victims, 50×30 grid)
- FDQN performance degrades in large environments due to fixed bin discretization losing spatial resolution

## Why This Works (Mechanism)

### Mechanism 1: Factorized Q-value summation
Each responder agent learns an individual Q-function Qi(s, ai). These Q-values are summed to produce Qjoint(s, a) = ΣQi(s, ai). This decomposition allows decentralized action selection during execution while providing a shared global reward signal during training, encouraging cooperation without requiring exponential joint action space enumeration.

### Mechanism 2: Action masking via FSM constraints
Each responder follows an FSM with states {idle, select v, move, tag}. Invalid actions for the current FSM state have their Q-values set to −∞, preventing selection. For example, an agent mid-movement can only continue moving; it cannot select a new victim or tag until arrival.

### Mechanism 3: Binned distance discretization
Pairwise responder-victim distances are discretized into B bins using thresholds. Distances < ζ map to bin 0, ζ to 2ζ map to bin 1, etc., up to B−1. This transforms continuous spatial coordinates into a fixed-size state representation regardless of environment scale.

## Foundational Learning

- **Concept: Value Decomposition in MARL**
  - Why needed: FDQN builds directly on VDN principles; understanding how individual Q-functions combine into joint Q is essential for debugging training and interpreting agent behavior
  - Quick check: Given Q1(s, a1) = 5 and Q2(s, a2) = 3, what is Qjoint(s, a)? Can individual agents have negative Q-values while joint Q is positive?

- **Concept: Deep Q-Network Training Stabilization (Replay Buffer + Target Network)**
  - Why needed: The paper uses both mechanisms; understanding why Q-learning diverges without them is critical for diagnosing training instability
  - Quick check: Why does using the same network for both current Q and target Q create a "moving target" problem? How often should target networks be updated?

- **Concept: Credit Assignment in Cooperative MARL**
  - Why needed: All agents share a global reward; understanding how each agent learns its contribution is key to interpreting why FDQN struggles at scale
  - Quick check: If three agents all select the same victim and one succeeds while others idle, how does each agent learn from the shared reward?

## Architecture Onboarding

- **Component map:** Global state s → [FC 128] → [FC 64] → Split into n agent heads → Agent Head i → Qi(s, ai) for all actions in Ai → Sum all Qi → Qjoint(s, a) → TD loss + Adam update

- **Critical path:** State encoding through shared layers → agent-specific Q-heads → action masking → conflict resolution → environment step → buffer storage → mini-batch TD update

- **Design tradeoffs:**
  - Shared vs. independent encoder: Shared reduces parameters but couples all agent learning; failure propagates
  - Fixed bins vs. adaptive discretization: Fixed is simpler but loses granularity at scale (explicitly noted as limitation in Section VI.B)
  - Global state assumption: Enables better training signal but requires communication infrastructure not available in truly decentralized settings
  - Additive factorization vs. mixing network (QMIX): Simpler but cannot represent non-additive value interactions

- **Failure signatures:**
  - High variance in episode steps across runs → unstable policy, likely exploration or binning issues
  - Loss not converging → check learning rate, target update frequency, or replay buffer corruption
  - FDQN approaching RVP performance → agents not learning meaningful coordination; may indicate state representation collapse or insufficient exploration
  - Some agents always idle → conflict resolution too aggressive or initialization bias

- **First 3 experiments:**
  1. Reproduce R1 (3 responders, 5 victims, 5×5 grid): Smallest scenario where FDQN should outperform all heuristics. Use as sanity check for implementation correctness.
  2. Ablation on bin count B ∈ {5, 10, 20} in R3 environment: Test hypothesis that discretization granularity explains performance degradation.
  3. Scale test with fixed ratio (R2 → R4 → R7): Hold responder:victim ratio approximately constant while increasing absolute counts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive binning strategies for state representation improve FDQN policy stability and convergence in large MCI environments compared to the fixed 5- or 10-bin discretization used in this study?
- Basis in paper: The authors state that fixed binning "loses fine-grained distance information" in larger environments and explicitly suggest "Future work could include adaptive binning."
- Why unresolved: The current experiments showed high variance and unstable learning in large spaces (specifically experiments R7 and R8), which the authors attribute to the loss of spatial granularity caused by fixed-size bins.
- What evidence would resolve it: A comparative analysis of FDQN performance in environments exceeding 50x30 units, contrasting the variance and convergence speed of fixed bins versus a dynamic binning mechanism that adjusts to environment scale.

### Open Question 2
- Question: Does adding explicit interaction terms to the factorized Q-function mitigate the bias introduced by the assumption of independent sub-actions in complex, high-agent scenarios?
- Basis in paper: The authors note that their factorization assumes sub-action independence, which "may not hold when sub-actions interact significantly," leading to a "need for explicit interaction terms or more complex methods."
- Why unresolved: The current FDQN architecture sums individual Q-values, which fails to model inter-agent dependencies. This likely contributes to FDQN underperforming against simple heuristics in scenarios with high responder-to-victim ratios.
- What evidence would resolve it: Implementation of a mixing network (such as QMIX) or attention mechanism that successfully captures action dependencies, resulting in FDQN outperforming heuristics in complex scenarios (R4-R8) where it currently fails.

### Open Question 3
- Question: Does dynamic grid partitioning based on real-time victim locations significantly reduce the inefficiencies observed in the static Local Grid Assignment Policy (LGAP)?
- Basis in paper: The authors note that LGAP performance depends heavily on victim locations, causing load imbalances (e.g., idle responders), and suggest "an extension of LGAP where the responder cells consider victim locations could be an improvement."
- Why unresolved: The static nature of LGAP causes responders in empty cells to remain idle while others are overwhelmed, making it less efficient than nearest-victim policies in the current results.
- What evidence would resolve it: Simulation results showing that a dynamic LGAP variant reduces the standard deviation of workload across agents and lowers total tagging time compared to the static version.

### Open Question 4
- Question: Can the Local Critical Victim Policy (LCVP) outperform other heuristics if the objective function is modified from minimizing "time to tag all" to maximizing "lives saved"?
- Basis in paper: The authors state that while LCVP was not the fastest, "If the goal was instead to prioritize critical victims with a performance metric involving lives saved, LCVP could be explored as a potential optimal policy."
- Why unresolved: The current evaluation minimizes total completion time, which inherently penalizes LCVP for prioritizing potentially distant critical victims over closer non-critical ones, failing to capture the value of triage priority.
- What evidence would resolve it: A new simulation framework incorporating a mortality function based on injury severity and time, demonstrating that LCVP results in higher survival rates than LNVP or FDQN despite longer tagging times.

## Limitations
- Performance degrades sharply in large environments (50×30 grids) due to fixed bin discretization losing spatial resolution
- All results derive from simulation; no field validation exists
- FSM-based action masking may miss adaptive strategies requiring opportunistic action reordering
- Conflict resolution is reactive rather than predictive, masking deeper coordination issues

## Confidence

- **High confidence**: VDN factorization approach and FSM-based action masking are correctly implemented and documented; basic FDQN mechanics are sound
- **Medium confidence**: FDQN outperforms heuristics in small, simple scenarios (R1-R3), but this advantage erodes in complex, high-uncertainty settings (R7-R8)
- **Low confidence**: Claims about FDQN's general scalability are overstated; performance degrades sharply beyond small agent counts and simple environments

## Next Checks

1. **Bin discretization sensitivity**: Test FDQN with B ∈ {5, 10, 20} in R3 to isolate whether performance degradation is due to loss of spatial granularity in larger grids
2. **Responder:victim ratio scaling**: Compare R2 → R4 → R7 while holding ratio constant to determine whether performance drops stem from absolute scale or ratio changes
3. **Adaptive binning vs. fixed bins**: Replace fixed discretization with environment-scaled bins to test if spatial resolution loss explains FDQN's instability in large environments