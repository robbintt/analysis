---
ver: rpa2
title: Towards Long Context Hallucination Detection
arxiv_id: '2504.19457'
source_url: https://arxiv.org/abs/2504.19457
tags:
- hallucination
- dataset
- context
- hallucinations
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of detecting contextual hallucinations
  in long text inputs, where models generate information that is either unsubstantiated
  or contradictory to the provided context. To address this, the authors construct
  a new dataset for long-context hallucination detection by injecting hallucinations
  into an existing book summarization dataset.
---

# Towards Long Context Hallucination Detection

## Quick Facts
- arXiv ID: 2504.19457
- Source URL: https://arxiv.org/abs/2504.19457
- Authors: Siyi Liu; Kishaloy Halder; Zheng Qi; Wei Xiao; Nikolaos Pappas; Phu Mon Htut; Neha Anna John; Yassine Benajiba; Dan Roth
- Reference count: 5
- The paper constructs a dataset for long-context hallucination detection by injecting hallucinations into book summarization data and proposes a decomposition-aggregation architecture that significantly outperforms prior approaches on precision, recall, and balanced accuracy.

## Executive Summary
This paper addresses the challenge of detecting contextual hallucinations in long text inputs, where models generate information that contradicts or is unsubstantiated by the provided context. The authors create a novel dataset by injecting hallucinations into existing book summaries and develop a decomposition-aggregation architecture that enables pre-trained encoders like BERT to process long contexts effectively. Their approach decomposes context and response into chunks, processes each through BERT, and aggregates using attention and pooling mechanisms to classify hallucinations.

The proposed method demonstrates substantial improvements over existing approaches including Longformer, HAT, Alignscore, and LLM-based models, achieving a precision of 54.50%, recall of 73.19%, and balanced accuracy of 67.22%. The architecture also provides faster inference speeds while maintaining strong performance, making it a practical solution for long-context hallucination detection in real-world applications.

## Method Summary
The authors tackle long-context hallucination detection by constructing a dataset through synthetic hallucination injection into book summarization data. Their core innovation is a decomposition-aggregation architecture that overcomes BERT's sequence length limitations for long contexts. The model breaks down both context and response into manageable chunks, processes each chunk through a BERT encoder to obtain representations, then aggregates these chunk-level features using learned attention weights and pooling operations. This allows the model to capture long-range dependencies while maintaining the effectiveness of pre-trained encoders. The aggregated representation is then used to classify whether a given response contains hallucinations relative to the provided context.

## Key Results
- Achieves precision of 54.50%, recall of 73.19%, and balanced accuracy of 67.22% on long-context hallucination detection
- Significantly outperforms prior approaches including Longformer, HAT, Alignscore, and LLM-based models across precision, recall, and balanced accuracy metrics
- Provides substantially faster inference speeds compared to competitive architectures while maintaining strong detection performance

## Why This Works (Mechanism)
The decomposition-aggregation architecture works by breaking the inherent limitation of fixed-length transformers through a divide-and-conquer approach. By decomposing long contexts into smaller chunks that fit within BERT's capacity, the model can leverage powerful pre-trained representations for each segment. The aggregation layer then synthesizes information across all chunks through attention mechanisms that learn to weight relevant segments more heavily, while pooling operations capture global patterns. This allows the model to maintain both local detail from individual chunks and global coherence across the entire context, enabling effective hallucination detection even in lengthy documents where hallucinations may appear anywhere.

## Foundational Learning
- **Long-context processing**: Methods to handle input sequences exceeding transformer maximum lengths, needed because standard transformers cannot process very long texts; quick check: verify chunk size selection relative to model capacity
- **Decomposition-aggregation patterns**: Breaking problems into smaller units then recombining results, needed to scale pre-trained models to longer inputs; quick check: ensure aggregation captures cross-chunk dependencies
- **Attention-based pooling**: Using learned weights to combine representations, needed for selective information fusion across chunks; quick check: examine attention weight distributions for interpretability
- **Synthetic data generation**: Creating training data by injecting controlled errors, needed when real hallucination-labeled data is scarce; quick check: validate synthetic distribution matches real patterns
- **Hallucination detection metrics**: Precision, recall, and balanced accuracy for evaluating false positive vs false negative trade-offs; quick check: compare metric distributions across different hallucination types

## Architecture Onboarding

Component map: Context and Response -> Chunk Decomposition -> BERT Encoder (per chunk) -> Chunk Representations -> Attention + Pooling Aggregation -> Classification

Critical path: The critical path flows from decomposing the context and response into chunks, through individual BERT encoding of each chunk, to the attention-weighted pooling aggregation that produces the final classification decision. The chunk decomposition strategy directly impacts both computational efficiency and detection accuracy, as too few chunks may miss local hallucinations while too many may fragment important contextual relationships.

Design tradeoffs: The architecture trades increased model complexity (additional decomposition and aggregation layers) for the ability to process long contexts with standard BERT. This design enables leveraging powerful pre-trained representations but introduces hyperparameters around chunk size and aggregation strategy. The attention mechanism adds interpretability but also computational overhead compared to simple pooling.

Failure signatures: The model may fail when hallucinations span multiple chunks that individually appear consistent, as the decomposition can obscure cross-chunk contradictions. It may also struggle with very subtle hallucinations that require deep semantic understanding across long distances, as chunk-level representations might miss nuanced inconsistencies. Performance degradation is expected when chunk boundaries split semantically coherent passages.

First experiments:
1. Ablation study removing attention aggregation to test pooling-only performance
2. Varying chunk sizes to find optimal trade-off between context coverage and detail preservation
3. Testing on synthetic vs real hallucination data to validate dataset construction validity

## Open Questions the Paper Calls Out
None

## Limitations
- The synthetic dataset construction may not fully capture the complexity and distribution of real-world hallucinations in long-context scenarios
- The precision of 54.50% indicates substantial false positive rates that limit practical deployment
- The decomposition-aggregation mechanism introduces architectural complexity that may impact scalability and transfer to domains with different hallucination patterns

## Confidence
- High confidence in the decomposition-aggregation architecture's effectiveness compared to baseline models on the constructed dataset
- Medium confidence in the practical utility of the approach given the precision limitations and synthetic dataset nature
- Medium confidence in the claimed efficiency improvements without broader architectural comparisons

## Next Checks
1. Validate model performance on naturally occurring hallucinations from real long-document processing tasks rather than synthetic injections
2. Conduct ablation studies to quantify the individual contributions of attention versus pooling in the aggregation layer
3. Benchmark inference speed and memory usage across a broader range of competitive architectures (including more recent long-context models) at varying context lengths beyond the current evaluation