---
ver: rpa2
title: 'UniConv: Unifying Retrieval and Response Generation for Large Language Models
  in Conversations'
arxiv_id: '2507.07030'
source_url: https://arxiv.org/abs/2507.07030
tags:
- retrieval
- conversational
- generation
- response
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces UniConv, a unified LLM-based model that handles
  both dense retrieval and response generation in conversational search systems. Unlike
  existing approaches that use separate models for retrieval and generation, UniConv
  employs joint fine-tuning with three objectives and two mechanisms to improve consistency
  and mitigate data discrepancy.
---

# UniConv: Unifying Retrieval and Response Generation for Large Language Models in Conversations

## Quick Facts
- **arXiv ID:** 2507.07030
- **Source URL:** https://arxiv.org/abs/2507.07030
- **Reference count:** 25
- **Primary result:** UniConv outperforms existing baselines in both retrieval and generation tasks across five conversational search datasets.

## Executive Summary
UniConv is a unified LLM-based model that handles both dense retrieval and response generation in conversational search systems. Unlike existing approaches that use separate models for retrieval and generation, UniConv employs joint fine-tuning with three objectives and two mechanisms to improve consistency and mitigate data discrepancy. Evaluations on five datasets show UniConv outperforms existing baselines in both retrieval and generation tasks, demonstrating superior generalization and seamless integration between retrieval and generation. The unified model also improves response reliability and history-aware capabilities compared to systems with separated models.

## Method Summary
UniConv is a unified conversational search system built on Mistral-2-7B-chat, using LoRA for efficient fine-tuning. The model jointly optimizes three objectives: retrieval (using contrastive learning on embeddings), generation (using Seq2Seq loss), and context identification instruction (CII). The training uses three distinct input formats per batch: query alone for retrieval, query plus history for generation, and query plus passage for CII. The model extracts retrieval representations from the final token embedding while performing standard autoregressive generation. Joint fine-tuning with data discrepancy mitigation aims to prevent the degradation of generative capabilities often observed when tuning solely for retrieval.

## Key Results
- UniConv outperforms existing baselines in both retrieval (NDCG@3, Recall@10) and generation (F1) tasks across five conversational search datasets
- The unified model demonstrates superior generalization and seamless integration between retrieval and generation compared to separated systems
- Joint fine-tuning with context identification instruction improves response reliability and history-aware capabilities in RAG settings

## Why This Works (Mechanism)

### Mechanism 1: Multi-Objective Joint Fine-Tuning
Training a single decoder-only LLM on both retrieval and generation objectives simultaneously prevents degradation of generative capabilities. The model optimizes a combined loss function $L = L_R + L_G + \alpha L_{CII}$, forcing it to maintain both representation quality and language generation capacity. The core assumption is that the model has sufficient capacity to represent both semantic similarity and syntactic complexity without catastrophic interference.

### Mechanism 2: Context Identification Instruction (CII)
Explicitly training the model to discriminate between relevant and irrelevant context when generating responses improves reliability in RAG settings. The model learns to associate concatenated query and positive passage with ground-truth response via contrastive loss against negative responses, teaching it to ground generation in retrieved evidence.

### Mechanism 3: Data Discrepancy Mitigation
Decoupling supervision signals for retrieval (relevant passages) and generation (ground-truth responses) for the same query turn mitigates confusion in the unified model. The model is trained on conversational search data containing explicit pairs for both requirements, allowing it to distinguish between the mode of output required based on input format.

## Foundational Learning

- **Dense Retrieval vs. Generation in Decoder-only LLMs:** Understanding that retrieval is performed by vectorizing the sequence while generation uses token probability distribution is crucial since UniConv uses a generative decoder rather than BERT-based bi-encoders. *Quick check:* How does the model derive a scalar similarity score for retrieval if it is an autoregressive generator?

- **Contrastive Learning (InfoNCE):** This mathematical engine for retrieval and CII losses relies on "in-batch negatives" to teach similarity. *Quick check:* Why is it necessary to have "negative samples" in the same batch as the positive sample?

- **RAG (Retrieval-Augmented Generation):** UniConv is designed to improve RAG setting consistency. Understanding how concatenating retrieved text to the prompt changes the generation task is essential. *Quick check:* In UniConv, do the retriever and generator share weights while processing text inputs separately?

## Architecture Onboarding

- **Component map:** Mistral-2-7B-chat (Decoder-only LLM) -> LoRA (Low-Rank Adaptation) -> Head 1 (Retrieval: EOS token embedding for cosine similarity) -> Head 2 (Generation: Standard autoregressive token prediction)

- **Critical path:** The training loop processes three distinct input formats per batch: (1) Query alone (for $L_R$), (2) Query + History (for $L_G$), and (3) Query + Passage (for $L_{CII}$).

- **Design tradeoffs:** The CII mechanism presents a distinct trade-off. Table 4 shows adding CII slightly hurts pure retrieval performance while significantly boosting generation. If pure search accuracy is the only goal, CII should be removed; if response quality is the goal, CII is mandatory.

- **Failure signatures:**
  - Retrieval Collapse: Model generates text instead of returning an embedding (input formatting error)
  - Generation Hallucination: Model ignores provided passage in RAG mode (likely due to insufficient CII training)
  - Catastrophic Forgetting: Model loses chat capability (happens if $\alpha$ is too low or training focuses exclusively on $L_R$)

- **First 3 experiments:**
  1. **Overfit Test:** Train on a single conversational turn. Verify the model can retrieve the correct passage AND generate the exact response. If it fails, check input masking for $L_G$.
  2. **Ablation on $\alpha$:** Run a sweep on the loss weight $\alpha$ (e.g., 0.1, 0.5, 1.0) for the CII term. Plot the trade-off curve between NDCG@3 and F1 to find the optimal operating point.
  3. **Consistency Check:** Compare "Separated" vs. "Unified" inference. Use a query where the retrieved passage is ambiguous. Check if the Unified model handles the ambiguity better in the generated response than the pipeline model.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does decoupling the fine-tuning of retrieval objectives ($L_R$) and context identification instructions ($L_{CII}$) into a two-stage process mitigate the degradation in retrieval performance?
- **Basis in paper:** The authors note that CII hurts retrieval metrics and suggest exploring two-stage fine-tuning separately for conversational retrieval and context identification instruction.
- **Why unresolved:** The current joint training creates a trade-off where CII improves generation reliability but reduces retrieval NDCG scores (e.g., dropping from 45.5 to 42.6 on TopiOCQA).
- **What evidence would resolve it:** Comparative experiments showing NDCG@3 and F1 scores for a two-stage training approach versus the current joint approach.

### Open Question 2
- **Question:** Can UniConv be effectively distilled into a smaller model to address efficiency concerns without significantly compromising unified capabilities?
- **Basis in paper:** The authors state that exploring distillation into a more efficient, smaller model is desirable given the high hardware requirements of the 7B parameter backbone compared to SLM-based systems.
- **Why unresolved:** While the 7B model outperforms SLMs, its deployment cost is higher, and it remains untested whether the unified knowledge transfers efficiently to a smaller architecture.
- **What evidence would resolve it:** Performance benchmarks of a distilled student model (e.g., with <1B parameters) on the five conversational search datasets used in the paper.

### Open Question 3
- **Question:** Can the unified UniConv framework be successfully adapted to complex conversational scenarios like product search, item recommendation, or proactive retrieval?
- **Basis in paper:** The authors state that developing a unified system for a broader range of complex conversational search scenarios is valuable, including product search, item recommendation, proactive retrieval, etc.
- **Why unresolved:** The current study evaluates the model strictly on information-seeking datasets, leaving its efficacy in decision-making or recommendation contexts unknown.
- **What evidence would resolve it:** Evaluation results on datasets requiring proactive retrieval or product recommendation, measuring both retrieval relevance and response utility.

## Limitations
- Evaluation scope limited to English-language conversational search datasets, leaving unknown whether UniConv generalizes to multilingual or domain-specific settings
- Paper doesn't quantify the computational overhead of joint training versus separate models beyond noting LoRA usage
- Specific impact of data discrepancy mitigation remains partially unclear, as Table 5 shows performance degradation when removed but doesn't isolate which aspect provides the most benefit

## Confidence
- **High Confidence:** The core claim that joint fine-tuning improves both retrieval and generation performance compared to separated models is well-supported by multiple dataset evaluations and ablation studies.
- **Medium Confidence:** The effectiveness of the Context Identification Instruction mechanism is demonstrated through performance improvements in RAG settings, but the trade-off with pure retrieval performance suggests context-dependent effectiveness.
- **Low Confidence:** The claim that data discrepancy mitigation is essential for unified model success lacks strong experimental isolation, as Table 5 shows degradation when DDM is removed but doesn't provide ablation studies isolating DDM's specific contribution.

## Next Checks
1. **Ablation of CII Trade-off:** Systematically vary the CII loss weight Î± across a wider range (0.0 to 2.0) and measure the Pareto frontier between retrieval accuracy (NDCG@3) and generation quality (F1) to identify optimal operating points for different application requirements.

2. **Computational Overhead Analysis:** Measure and compare wall-clock training time, memory usage, and inference latency between UniConv and separate retriever+generator pipelines across different hardware configurations to quantify the practical cost of unification.

3. **Cross-Domain Generalization Test:** Evaluate UniConv on non-conversational retrieval tasks (standard ad-hoc search) and on multilingual datasets to assess whether the unified architecture maintains performance outside its training distribution, particularly for the generation component which may suffer from catastrophic forgetting of general chat capabilities.