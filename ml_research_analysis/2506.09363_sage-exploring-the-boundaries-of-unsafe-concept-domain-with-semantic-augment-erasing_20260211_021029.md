---
ver: rpa2
title: 'SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment
  Erasing'
arxiv_id: '2506.09363'
source_url: https://arxiv.org/abs/2506.09363
tags:
- concept
- prompts
- erasure
- prompt
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SAGE, a method to safely erase undesirable
  concepts from text-to-image diffusion models without harming their general image
  generation capability. Unlike prior work that focuses on erasing fixed words, SAGE
  uses semantic-augment erasing to explore and unlearn entire concept domains via
  cyclic self-check and self-erasure, starting from the concept prompt itself.
---

# SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing

## Quick Facts
- arXiv ID: 2506.09363
- Source URL: https://arxiv.org/abs/2506.09363
- Reference count: 40
- Key outcome: Introduces SAGE, achieving up to 98% RER for nudity erasure while preserving unrelated concepts through semantic-augment erasing and global-local retention

## Executive Summary
This paper presents SAGE, a method for safely erasing undesirable concepts from text-to-image diffusion models without harming their general image generation capability. Unlike prior work that focuses on erasing fixed words, SAGE uses semantic-augment erasing to explore and unlearn entire concept domains via cyclic self-check and self-erasure, starting from the concept prompt itself. It also includes a global-local collaborative retention mechanism to maintain semantic alignment and local image quality for unrelated concepts. SAGE outperforms existing approaches in removing nudity and artistic style concepts while preserving unrelated concepts, achieving up to 98% RER for nudity erasure and robust generalization against red-teaming attacks. Training is also significantly more efficient.

## Method Summary
SAGE explores concept domains by generating attack prompts through inside-out optimization of template embeddings, starting from the target concept prompt rather than perturbed random embeddings. The method alternates between optimizing attack prompts (maximizing similarity to concept domain while minimizing similarity to original model) and erasing target concepts through text encoder fine-tuning. A global-local collaborative retention mechanism preserves semantic alignment for unrelated concepts via semantic relationship graphs and local noise prediction consistency. The approach modifies only the text encoder (CLIP), enabling zero-shot deployment across models sharing the same encoder architecture.

## Key Results
- Achieves up to 98% Relative Erasure Ratio (RER) for nudity concepts on I2P dataset
- Maintains high FID (<25) and CLIP-S scores (>29) for unrelated concepts
- Outperforms existing methods in both erasure efficacy and retention of image quality
- Demonstrates robust generalization against red-teaming attacks with low Attack Success Rate (ASR)
- Shows 62.1× faster attack prompt generation compared to outside-in approaches

## Why This Works (Mechanism)

### Mechanism 1: Inside-Out Attack Prompt Generation
The method initializes attack prompts with the target concept word plus a learnable template, then optimizes the template embedding using competing objectives that maximize similarity to the concept domain while minimizing similarity to the original model. This creates guided exploration of semantically related expressions at concept domain boundaries. The inside-out approach is more efficient than outside-in methods because it starts from known concept anchors rather than random perturbations, avoiding the multi-step denoising required by outside-in approaches.

### Mechanism 2: Global-Local Collaborative Retention
Decouples retention into global semantic graph alignment and local noise preservation. The global component constructs semantic relationship graphs over batches of retain prompts, expanding the effective receptive field to 32 prompts. The local component selects Top-k prompts with lowest semantic similarity for noise prediction consistency, targeting concepts most vulnerable to semantic drift. This prevents over-erasure while maintaining generation quality for unrelated concepts.

### Mechanism 3: Text Encoder-Only Modification with Transferability
Restricts modifications to the text encoder while keeping the UNet denoiser frozen. Since text encoders are often reused across model variants, the purified encoder can be swapped into compatible models without retraining. This enables zero-shot deployment across models sharing the same encoder architecture while achieving comparable erasure efficacy.

## Foundational Learning

- **Concept: Classifier-Free Guidance**
  - Why needed here: SAGE builds on ESD's negative guidance formulation to steer predictions away from target concepts
  - Quick check question: Can you explain why Eq. 11 subtracts the difference between conditional and unconditional noise predictions?

- **Concept: Cross-Attention in Diffusion Models**
  - Why needed here: The paper contrasts its approach with attention re-steering methods that modify key/value projections in cross-attention layers
  - Quick check question: In Stable Diffusion, where do text embeddings interact with the UNet's intermediate representations?

- **Concept: Projected Gradient Descent (PGD) for Adversarial Attacks**
  - Why needed here: The attack prompt optimization uses iterative gradient-based updates constrained to remain within a semantic neighborhood
  - Quick check question: Why does the inside-out approach avoid the multi-step denoising process required by outside-in methods?

## Architecture Onboarding

- **Component map:**
  Original frozen DM (θo) -> Attack Prompt Generator -> Training DM (θn) -> Semantic Graph Processor -> FID/CLIP-S Evaluator

- **Critical path:**
  1. Warm-up: 200 steps using standard ESD-style erasure to create initial concept separation
  2. Cyclic training (steps 201-1000): Alternate between attack prompt optimization (J=30 steps) and erasure training
  3. Retention: Compute Lgraph (bretain=32) → select Top-4 for Limage → aggregate with Lerase

- **Design tradeoffs:**
  - γt (semantic graph weight): Higher values (3.0 for style vs. 0.4 for nudity) prioritize retention over erasure
  - Warm-up necessity: Ablation shows warm-up slightly improves erasure by establishing initial concept separation
  - Attack step count (J=30): Balances exploration depth vs. training time

- **Failure signatures:**
  - Semantic drift: H2-only optimization causes attack prompts to drift outside target domain
  - Over-erasure: Excessive Lerase without retention causes FID degradation and CLIP-S collapse
  - Transfer failure: Incomplete transfer to SDXL indicates dual-encoder architectures require both encoders

- **First 3 experiments:**
  1. Reproduce nudity erasure on I2P dataset with provided hyperparameters; verify RER > 95% and FID < 25
  2. Ablate attack loss components; compare H1-only, H2-only, and full Lattack on ASR metric
  3. Test zero-shot transfer to SD v1.4 variant; measure nudity detection reduction

## Open Questions the Paper Calls Out

### Open Question 1
How can the semantic-augment erasing method be adapted to improve efficacy in diffusion architectures that utilize dual text encoders, such as SDXL? The current method optimizes a single text encoder, but SDXL relies on a dual-encoder architecture (CLIP-ViT/L and OpenCLIP-ViT/G), leaving the interaction with the second encoder unoptimized.

### Open Question 2
To what extent does the reliance on a GPT-4 generated template library restrict the discovery of "boundary representations" for concepts that are underrepresented in the language model's training data? The "inside-out" exploration assumes the template library provides a valid centroid for the concept domain; if the library is biased or sparse for niche concepts, the self-erasure loop may fail to identify key boundary representations.

### Open Question 3
Does the global-local collaborative retention mechanism effectively mitigate interference when erasing multiple distinct concepts simultaneously? While the retention mechanism preserves unrelated concepts for a single target, simultaneous multi-concept erasure could introduce conflicting gradients in the text encoder, potentially degrading the global semantic relationship graph.

### Open Question 4
Is the strategy of freezing the UNet denoiser sufficient for erasing visual concepts that are primarily defined by pixel-level patterns rather than semantic text embeddings? By restricting modifications to the text encoder, the model may fail to unlearn visual artifacts or styles that are deeply embedded in the UNet's convolutional layers.

## Limitations

- Dual-encoder architectures (like SDXL) show significantly reduced transfer capability (49% vs 98% for single-encoder models), suggesting the text-only modification approach may be insufficient for some architectures
- The reliance on GPT-4-generated templates for attack prompt initialization may introduce bias and limit exploration of underrepresented concepts
- The assumption of continuous semantic space topology may not hold for all concepts, particularly abstract or multi-modal concepts with ill-defined boundaries

## Confidence

**High Confidence:** The general framework architecture (cyclic self-check and self-erasure with global-local retention) is well-specified and reproducible. The ablation studies demonstrating the necessity of each component are convincing, and the quantitative improvements over baselines are specific and measurable.

**Medium Confidence:** The mechanism claims about inside-out optimization being more efficient than outside-in approaches are supported by ASR comparisons, but the topological assumptions about semantic space require further validation. The transferability claims to different model variants are well-demonstrated for single-encoder models but less convincing for dual-encoder architectures.

**Low Confidence:** The claim that the method "explores the boundaries" of concept domains is difficult to verify empirically. The paper shows attack prompts work better, but proving they represent true semantic boundaries rather than just effective adversarial examples requires additional analysis of the prompt space topology.

## Next Checks

1. **Topology Verification:** Visualize the semantic space around target concepts using t-SNE or UMAP on text embeddings to empirically verify that optimized attack prompts cluster at the "boundaries" rather than arbitrary points in the space. Compare the distribution of attack prompts from inside-out vs outside-in methods.

2. **Concept Coverage Analysis:** Systematically evaluate SAGE's performance on concepts with different semantic properties - abstract concepts (justice, freedom), multi-modal concepts (red - color vs political), and concepts with unclear boundaries. This would test the assumption that continuous semantic spaces exist for all target concepts.

3. **Dual-Encoder Transfer Study:** Conduct controlled experiments on dual-encoder architectures where both encoders are modified vs only the text encoder. Measure the marginal benefit of UNet modification and quantify exactly where the 49% vs 98% transfer gap originates (encoder architecture differences, tokenization schemes, etc.).