---
ver: rpa2
title: Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought
  Correctness Perception Distillation
arxiv_id: '2509.05602'
source_url: https://arxiv.org/abs/2509.05602
tags:
- answer
- rationale
- reasoning
- student
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation

## Quick Facts
- **arXiv ID:** 2509.05602
- **Source URL:** https://arxiv.org/abs/2509.05602
- **Reference count:** 40
- **Primary result:** CoPeD improves accuracy on BIG-Bench Hard test by +6.4% over standard CoT distillation, with gains on out-of-distribution datasets (ARC-E: +1.2%, ARC-C: +2.3%, AGIEval: +2.9%).

## Executive Summary
This paper addresses spurious correlations in Chain-of-Thought (CoT) distillation where student models learn to map questions directly to answers rather than relying on generated rationales. The proposed CoPeD framework conditions training on rationale correctness, using a dual-task approach where correct rationales lead to answer prediction and incorrect rationales require revision. A correctness-aware weighted loss dynamically adjusts sample importance based on loss alignment. The method demonstrates improved accuracy on both in-distribution (BIG-Bench Hard) and out-of-distribution (ARC, AGIEval) benchmarks compared to standard CoT distillation.

## Method Summary
CoPeD trains a student model using rationales generated by a teacher LLM, with samples partitioned based on whether the generated answer matches the ground truth (r+ for correct, r- for incorrect). Two tasks are trained: answer prediction for correct rationales and rationale correction for incorrect ones, using special status tokens (rst/rstf) to indicate rationale correctness. A joint loss combines rationale generation and answer prediction losses with equal weighting (α=0.5). Starting from epoch n, a correctness-aware weighted loss computes per-sample weights using softmax over a composite term combining rationale loss, answer loss, and their absolute difference, dynamically down-weighting samples with misaligned losses. The method uses LoRA adapters for efficient fine-tuning on 7B parameter models.

## Key Results
- CoPeD achieves 74.1% accuracy on BBH-test compared to 67.7% for standard CoT distillation (+6.4%)
- Out-of-distribution gains: ARC-E (+1.2%), ARC-C (+2.3%), AGIEval (+2.9%)
- Improved faithfulness (+1.8%) and soundness (+2.8%) metrics over standard CoT
- Ablation shows weighted loss provides +2.4% improvement over uniform weighting
- Rationale correction task essential: replacing with empty string target reduces performance

## Why This Works (Mechanism)

### Mechanism 1: Conditional Decoupling of Spurious Correlations
The method forces the student to rely on the causal reasoning path (Q → R → A) rather than direct spurious mapping (Q → A) by conditioning training on rationale validity. Correct rationales trigger answer prediction while incorrect ones require revision, making rationales indispensable for correct samples and explicit correction targets for incorrect ones.

### Mechanism 2: Loss-Based Confidence Reweighting
Dynamically down-weights training samples where rationale loss and answer loss are misaligned, filtering out low-quality distillation data. The confidence weight uses a composite term (Lr + La) and discrepancy term (|Lr - La|), reducing gradient contribution from unreliable samples.

### Mechanism 3: Implicit Data Augmentation via Rationale Correction
Training on the r- → r+ mapping exposes the model to failure modes and their corrections, forcing learning of valid reasoning structural constraints rather than pattern matching correct examples.

## Foundational Learning

- **Concept: Spurious Correlation (Shortcut Learning)**
  - Why needed here: Targets the problem where models answer correctly for wrong reasons via dataset bias rather than generated rationale
  - Quick check: Can you identify a scenario where a model predicts "A" because the question contains "not," regardless of actual logic?

- **Concept: Knowledge Distillation (Teacher-Student)**
  - Why needed here: Situated in transferring reasoning abilities from large (Teacher) to smaller (Student) models
  - Quick check: What is the standard baseline (Std-CoT) and how does CoPeD modify training data?

- **Concept: Chain-of-Thought (CoT) Faithfulness**
  - Why needed here: Differentiates between answer correctness and reasoning supporting that answer being correct
  - Quick check: If a model outputs correct answer but intermediate steps contradict it, is the model "faithful"?

## Architecture Onboarding

- **Component map:** Teacher (LLM) → Data Processor (labels r+/r-) → Student (Transformer) → Loss Aggregator (weighted loss)

- **Critical path:**
  1. Teacher generates rationales
  2. Label as r+ if answer matches ground truth, else r-
  3. Forward pass: Concatenate Q + R + Status Token
  4. Compute Lr, La
  5. If epoch > n, compute dynamic weights wi using Algorithm 1; else uniform weights
  6. Backward pass

- **Design tradeoffs:**
  - Heuristic Labeling vs. Verification: Relies on answer correctness as proxy for rationale correctness (cheap but noisy)
  - Correction vs. Empty Output: Correcting rationale is better than empty string for errors but requires meaningful "wrong" rationales

- **Failure signatures:**
  - Status Token Hallucination: Generates "rationale is right" for clearly flawed rationale
  - Error Propagation: Teacher produces subtle logical errors that look correct
  - Loss Instability: Temperature τ too low focuses on few easy samples, failing to generalize

- **First 3 experiments:**
  1. Baseline Verification (Std-CoT): Establish spurious correlation baseline on BBH-test
  2. Ablation on Task Setting (CoPeD-T): Verify dual-task improves "Faithfulness" (Table 2)
  3. Full System (CoPeD-TL): Enable weighted loss from epoch n=5 to observe OOD generalization delta

## Open Questions the Paper Calls Out

### Open Question 1
Can the student model independently and effectively verify rationale correctness during inference? The paper notes experiments show no significant performance difference between verification settings, likely due to limited model capacity. A modified training objective enabling statistically significant accuracy improvement in "Verify" setting would resolve this.

### Open Question 2
How can the correctness-aware weighting mechanism be refined to better capture subtle inconsistencies between rationales and answers? The current method may fail to detect semantic contradictions that don't result in high loss values. A new weighting function demonstrating stronger correlation with human evaluations would resolve this.

### Open Question 3
Can dependency on heuristic correctness labels (derived from answer matching) be removed to reduce noise? The current method mislabels instances where reasoning is flawed but answer coincidentally correct. An annotation strategy validating rationale logic independently of final answer would resolve this.

## Limitations

- **Heuristic Labeling Noise:** Relies on brittle assumption that correct answer implies correct rationale, which is demonstrably false in practice
- **Teacher Quality Dependency:** Success bottlenecked by teacher's ability to generate meaningful erroneous rationales, not analyzed for quality distribution
- **Narrow OOD Evaluation:** Tests only on multiple-choice QA datasets, effectiveness for open-ended reasoning unvalidated

## Confidence

- **High Confidence:** Existence of spurious correlations in CoT distillation and general intuition that conditioning on rationale correctness helps
- **Medium Confidence:** Empirical improvements reported but lack statistical significance tests or error analysis
- **Low Confidence:** Claim of significant improvement in robustness to spurious correlations given modest OOD gains

## Next Checks

1. **Manual Error Analysis:** Sample 50 predictions from CoPeD and Std-CoT on BBH-test, categorize errors to directly test if spurious correlations are reduced

2. **Teacher Output Quality Audit:** Generate 100 rationales from teacher on BBH-train, have human annotators label logical soundness to quantify heuristic labeling noise

3. **Cross-Dataset Transfer Test:** Fine-tune CoPeD on different reasoning dataset (e.g., GSM8K/MATH) and test on BBH-test to verify genuine spurious correlation mitigation