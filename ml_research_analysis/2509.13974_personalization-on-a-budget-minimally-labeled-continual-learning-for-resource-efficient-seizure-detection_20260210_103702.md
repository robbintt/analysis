---
ver: rpa2
title: 'Personalization on a Budget: Minimally-Labeled Continual Learning for Resource-Efficient
  Seizure Detection'
arxiv_id: '2509.13974'
source_url: https://arxiv.org/abs/2509.13974
tags:
- data
- learning
- seizure
- samples
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of personalized seizure detection
  using deep learning while minimizing computational and labeling costs. The proposed
  method, EpiSMART, uses a continual learning framework that incrementally adapts
  to individual patient EEG patterns while preserving past knowledge through a size-constrained
  replay buffer.
---

# Personalization on a Budget: Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection

## Quick Facts
- arXiv ID: 2509.13974
- Source URL: https://arxiv.org/abs/2509.13974
- Authors: Amirhossein Shahbazinia; Jonathan Dan; Jose A. Miranda; Giovanni Ansaloni; David Atienza
- Reference count: 40
- One-line result: EpiSMART achieves 21% F1 improvement over baseline while requiring only 6.46 minutes of labeled data and 6.28 updates per day

## Executive Summary
This paper addresses the challenge of personalized seizure detection using deep learning while minimizing computational and labeling costs. The proposed method, EpiSMART, uses a continual learning framework that incrementally adapts to individual patient EEG patterns while preserving past knowledge through a size-constrained replay buffer. The method selects informative samples based on entropy and predicted seizure labels, requiring minimal expert annotation and updates. Experiments on the CHB-MIT dataset show that EpiSMART achieves a 21% improvement in F1 score compared to a baseline without updates, while requiring only 6.46 minutes of labeled data and 6.28 updates per day on average.

## Method Summary
EpiSMART employs a continual learning approach where a pre-trained FCN backbone is fine-tuned on streaming EEG data from individual patients. The method uses entropy-based sample selection to identify high-uncertainty windows that require expert labeling, storing these along with predicted seizure samples in a unified 1-hour replay buffer. Updates are triggered when a threshold number of samples accumulate, using the buffer contents for rehearsal to prevent catastrophic forgetting. The framework achieves personalization through incremental adaptation while maintaining efficiency through selective labeling and batch updates, validated on the CHB-MIT dataset with 18-channel EEG data processed into 4-second windows.

## Key Results
- Achieves 21% F1 improvement (69.69 vs 48.34) over baseline without updates
- Requires only 6.46 minutes of labeled data per day on average
- Performs 6.28 updates per day on average, reducing computational overhead
- Outperforms update-every-hour approach while using half the memory

## Why This Works (Mechanism)

### Mechanism 1: Entropy-Based Sample Selection for Label Efficiency
The model computes prediction entropy for each incoming EEG window, selecting samples exceeding threshold τ_E (model uncertain) for expert labeling. This concentrates labeling effort on decision boundary examples where model confidence is low, yielding higher information gain per labeled sample than random selection. The assumption is that high-entropy samples are more informative for model refinement than low-entropy samples. Evidence shows EpiSMART achieves F1=69.69 with FAR=1.65 versus RU achieving F1=48.99 with FAR=20.83 despite similar labeling costs (~6.5 min/day).

### Mechanism 2: Unified Replay Buffer Mitigates Catastrophic Forgetting
A single size-constrained buffer retaining historical seizure and non-seizure samples enables continual adaptation without separate memory banks. The buffer stores high-entropy and predicted-seizure samples with 50/50 seizure/non-seizure allocation, with seizures prioritized for retention. During model update, fine-tuning uses buffer contents alongside newly labeled samples, rehearsing past patterns to stabilize weights. Evidence shows EpiSMART uses 1-hour unified buffer versus Update Every Hour requiring separate new-data memory, halving storage while achieving comparable F1 (69.69 vs 70.76).

### Mechanism 3: Update Threshold Governs Compute/Performance Trade-off
Triggering model updates only when accumulated samples reach threshold τ_U reduces compute from 24 updates/day to ~6 while maintaining F1. Counter C increments per selected sample, with updates occurring when C ≥ τ_U (default=15), ensuring sufficient batch size for stable gradient estimates. This amortizes annotation+compute over more data per update, reducing overhead for wearable deployment. Evidence shows τ_U=15, τ_E=1e-5 yields strong F1/FAR balance with ~6.28 updates/day and 6.46 min labeling/day.

## Foundational Learning

- **Concept: Catastrophic Forgetting in Sequential Data**
  - Why needed here: Continual learning on non-stationary EEG requires understanding why naive fine-tuning degrades past-task performance; buffer replay is the mitigation strategy.
  - Quick check question: Can you explain why updating a model only on new patient data might cause it to "forget" patterns learned from earlier recordings?

- **Concept: Class Imbalance in Rare-Event Detection**
  - Why needed here: Seizures occupy ~3 hours in 982 hours of CHB-MIT data; naive training collapses to all-negative predictions. EpiSMART's seizure-prioritized buffer and augmentation address this.
  - Quick check question: Why would a model trained on this dataset achieve high accuracy by always predicting "non-seizure," and how does F1 score expose this failure?

- **Concept: Subject-Independent vs. Personalized Models**
  - Why needed here: Leave-One-Out training (23 patients → test on 24th) yields F1=48.34, demonstrating that cross-patient generalization is poor; personalization via buffer replay is the solution.
  - Quick check question: What does the 21% F1 improvement from No-Update to EpiSMART suggest about EEG variability across patients?

## Architecture Onboarding

- **Component map:** Pre-trained FCN backbone -> Entropy + seizure-prediction selector -> Unified replay buffer -> Update trigger -> Post-processing
- **Critical path:** Data stream → inference → entropy check (if H>τ_E OR ŷ=1) → buffer store → counter increment → (if C≥τ_U) expert labeling → fine-tune with buffer → deploy updated model
- **Design tradeoffs:**
  - Higher τ_E → fewer samples selected → lower labeling cost but risk missing informative cases
  - Higher τ_U → fewer updates → lower compute but slower adaptation to distribution shift
  - Larger buffer → better forgetting mitigation but more memory (target: wearable-constrained ~1-hour)
  - Seizure-prioritized eviction → ensures rare-event retention but may reduce non-seizure diversity
- **Failure signatures:**
  - Buffer starvation: C never reaches τ_U → no updates → performance drifts (check fill rate metrics)
  - FAR explosion: Excessive false positives trigger frequent updates with low-utility samples (tune τ_E upward)
  - Seizure amnesia: If seizures cluster, buffer may saturate with recent seizures; old patterns lost (audit seizure timestamps in buffer)
- **First 3 experiments:**
  1. Baseline sanity check: Run No-Update (LOO pre-trained model) on held-out patient to confirm F1≈48 and understand failure modes (likely high FN on patient-specific patterns).
  2. Hyperparameter sweep: Grid search τ_E ∈ {1e-6, 1e-5, 1e-4} × τ_U ∈ {10, 15, 20} on validation patient; plot F1 vs. labeling/update cost to select operating point.
  3. Ablation on sample selection: Compare EpiSMART vs. Random Update (same labeling budget) to quantify entropy-selection value; expect ~20-point F1 gap per Table I.

## Open Questions the Paper Calls Out

### Open Question 1
Can an adaptive mechanism be developed to dynamically adjust the entropy threshold (τ_E) and update threshold (τ_U) for individual patients to eliminate the need for manual hyperparameter tuning?
Basis: The authors state that "These parameters vary across patients, and there are no universally optimal values" and that setting values too low or high involves trade-offs between labeling and update costs.
Why unresolved: The current framework relies on fixed, pre-defined thresholds for sample selection and update frequency, which must be tuned to balance costs for specific patients.
Evidence needed: A modified version of EpiSMART where τ_E and τ_U are learned or adjusted online, demonstrating statistically similar or better F1 scores and lower cost variance across all 24 patients without manual tuning.

### Open Question 2
How does the EpiSMART framework perform when deployed on alternative deep learning architectures, such as Transformers or Recurrent Neural Networks, which may handle temporal dependencies differently than the FCN used?
Basis: The authors claim the "methodology is agnostic with respect to the structure of the deep neural network," but they validate it exclusively using a specific Fully Convolutional Network (FCN).
Why unresolved: While the method is theoretically architecture-agnostic, the entropy-based sample selection strategy might interact differently with the confidence calibration of various architectures.
Evidence needed: Experimental results applying EpiSMART to SOTA architectures (e.g., EEG-ConvTransformer) on the CHB-MIT dataset, comparing the stability of the entropy selection strategy and resulting F1 scores against the FCN baseline.

### Open Question 3
Can the sample selection strategy be refined to utilize temporal context or "neighborhood" data efficiently without incurring the high labeling costs identified in the ablation study?
Basis: Section IV-C.2 notes that while "doctors also evaluate samples in the context of their surrounding data," the experiments showed that incorporating neighborhood data increased labeling cost dramatically (from 5.13 to 38.22 minutes) without performance gains.
Why unresolved: The current implementation treats samples as isolated windows for selection purposes, failing to capture the temporal evolution of EEG patterns that clinicians deem important.
Evidence needed: A selection strategy that labels the primary high-entropy window but uses unlabeled surrounding windows for augmentation or contrastive learning during the update phase, showing improved F1 scores without increasing the "Labeling Cost" metric.

### Open Question 4
Is the framework effective for adult populations or low-channel count wearable devices, given that validation was limited to a pediatric dataset with 18 channels?
Basis: The validation is performed exclusively on the CHB-MIT dataset, which comprises "24 pediatric patients" with 18-23 channels. The paper does not address if the 6.46 minutes of labeled data is sufficient for adult EEG patterns or reduced channel setups common in consumer wearables.
Why unresolved: Pediatric EEG patterns can differ significantly from adult patterns in frequency and amplitude, and reducing channel density typically degrades model performance.
Evidence needed: Validation of EpiSMART on an adult epilepsy dataset (e.g., TUH EEG Corpus) and a subsequent experiment simulating a reduced channel montage (e.g., 4-8 channels) to measure the trade-off between channel density and required labeling effort.

## Limitations
- Lack of exact FCN architecture details makes faithful reproduction challenging despite referencing external work
- Buffer eviction strategy for non-seizure samples (random vs. time-based) may impact long-term performance but isn't explicitly validated
- Entropy threshold selection (τ_E = 1e-5) appears arbitrary; sensitivity analysis is limited to grid search rather than principled derivation
- Update frequency (6.28/day) assumes stable seizure patterns; sudden frequency changes could cause detection gaps

## Confidence
- **High confidence:** Performance improvements over baseline (F1: 69.69 vs 48.34), sample selection mechanism, update cost reduction
- **Medium confidence:** Catastrophic forgetting mitigation through buffer replay, entropy-based selection value
- **Low confidence:** Long-term stability with temporally clustered seizures, generalizability beyond CHB-MIT

## Next Checks
1. Reproduce the 21% F1 improvement by implementing the exact LOO baseline and EpiSMART pipeline on held-out patient data
2. Validate buffer class balance dynamics by logging seizure/non-seizure sample ratios during streaming to ensure prioritized retention works as intended
3. Test adaptive τ_U mechanism by simulating seizure frequency spikes to assess detection latency and update timing trade-offs