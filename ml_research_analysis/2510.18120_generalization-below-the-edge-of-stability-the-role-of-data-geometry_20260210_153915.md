---
ver: rpa2
title: 'Generalization Below the Edge of Stability: The Role of Data Geometry'
arxiv_id: '2510.18120'
source_url: https://arxiv.org/abs/2510.18120
tags:
- data
- generalization
- networks
- where
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper theoretically explores how data geometry controls the
  implicit bias of gradient descent in overparameterized two-layer ReLU networks trained
  below the edge of stability. The authors introduce a unifying principle called "data
  shatterability" - the less shatterable the data geometry, the stronger the implicit
  regularization becomes.
---

# Generalization Below the Edge of Stability: The Role of Data Geometry

## Quick Facts
- **arXiv ID:** 2510.18120
- **Source URL:** https://arxiv.org/abs/2510.18120
- **Reference count:** 40
- **Primary result:** Data geometry controls implicit bias in overparameterized ReLU networks; less shatterable geometry yields stronger implicit regularization.

## Executive Summary
This paper establishes a unifying principle—"data shatterability"—that explains how the geometry of training data controls the implicit regularization of gradient descent in overparameterized two-layer ReLU networks. The authors prove that when networks are trained below the edge of stability (BEoS), the generalization error is controlled by a data-dependent weighted path norm, whose complexity adapts to the intrinsic dimension of the data manifold and deteriorates with radial concentration toward the boundary. Through theoretical bounds and empirical validation, they show that low-dimensional structures generalize better than isotropic distributions, and that this effect is modulated by how mass is distributed within the support.

## Method Summary
The study uses two-layer fully connected ReLU networks trained with vanilla gradient descent under a large learning rate that induces the Below-Edge-of-Stability (BEoS) regime. Synthetic datasets are constructed with specific geometries: mixtures of low-dimensional balls (e.g., union of lines) and isotropic Beta-radial distributions with varying radial concentration parameters. The training procedure monitors the Hessian spectral norm to ensure operation near the edge of stability, and generalization is evaluated using "True MSE" against a noiseless teacher function. Empirical results validate the theoretical predictions about intrinsic dimension adaptation and the spectrum of bounds based on shatterability.

## Key Results
- Generalization bounds provably adapt to intrinsic dimension: O(n^{-1/(2m+4)}) for data on m-dimensional subspaces.
- A spectrum of bounds shows rates deteriorate with radial concentration toward the boundary: O(n^{-2α/(d-1+2α)}) for Beta-radial distributions.
- Empirical results validate theoretical findings, demonstrating that low-dimensional structure and radial concentration significantly affect both generalization performance and the representations learned by gradient descent.

## Why This Works (Mechanism)

### Mechanism 1: Intrinsic Dimension Adaptation
- **Claim:** If data is supported on a mixture of low-dimensional subspaces, the generalization error of BEoS solutions scales with the intrinsic dimension $m$ rather than the ambient dimension $d$.
- **Mechanism:** The stability constraint enforces a data-dependent weighted path norm. When data lies on low-dimensional manifolds, the global weight function $g$ dominates local weight functions $g_j$, effectively constraining network complexity relative to the "knots" formed by ReLU intersections with the subspace.
- **Core assumption:** Data distribution is a mixture of uniform distributions on unit discs of $m$-dimensional subspaces (Assumption 3.1); training converges to a BEoS solution.
- **Evidence anchors:** [abstract] Mentions generalization bounds that "provably adapt to the intrinsic dimension." [section] Theorem 3.2 establishes the rate $O(n^{-1/(2m+4)})$ and discusses the "global-to-local weight domination" in Appendix F.
- **Break condition:** If the data contains significant noise or volume in the ambient space orthogonal to the subspaces, the intrinsic dimension assumption fails.

### Mechanism 2: Data Shatterability and Radial Concentration
- **Claim:** If data concentrates strongly toward the boundary of its support, the generalization guarantee deteriorates because the geometry is "highly shatterable."
- **Mechanism:** High boundary concentration allows ReLU neurons to partition the input space into many non-overlapping "caps" with few samples each. Neurons activating on these sparse regions incur a low penalty (small $g_D$), allowing "memorizing neurons" with large unweighted norms to exist stably.
- **Core assumption:** Data follows an isotropic Beta-radial distribution or is supported on a sphere.
- **Evidence anchors:** [abstract] Introduces "data shatterability" and notes rates deteriorate as mass concentrates toward the sphere. [section] Theorem 3.5 quantifies the spectrum of bounds based on concentration parameter $\alpha$; Theorem 3.7 proves flat interpolation is possible on the sphere.
- **Break condition:** If data is concentrated in the "deep" interior (high Tukey depth), the weight function $g$ remains large, enforcing strong regularization.

### Mechanism 3: Stability-Induced Weighted Path Norm
- **Claim:** If a network is trained below the edge of stability (BEoS), its generalization is controlled by a specific data-dependent weighted path norm.
- **Mechanism:** The BEoS condition ($\lambda_{max} \le 2/\eta$) mathematically implies a bound on the path norm weighted by a function $g_D$ derived from the probability and expectation of neuron activations.
- **Core assumption:** The loss landscape is twice differentiable at the solution; gradient descent operates with a learning rate $\eta$ such that $\lambda_{max} \le 2/\eta$.
- **Evidence anchors:** [abstract] "This paper theoretically explores how data geometry controls this implicit bias." [section] Proposition 2.2 and Appendix E derive the inequality $\|f\|_{path,g_D} \le \frac{1}{\eta} - \frac{1}{2} + (R+1)\sqrt{2L(\theta)}$.
- **Break condition:** If training does not reach a BEoS state, the link between Hessian stability and the weighted path norm bound is not guaranteed.

## Foundational Learning

### Concept: Weighted Path Norm
- **Why needed here:** This is the specific complexity measure the paper proves is bounded by stability. Understanding it is necessary to interpret the generalization bounds.
- **Quick check question:** How does the weight function $g_D(u, t)$ change if a neuron activates on only 1% of the data versus 50%?

### Concept: Tukey Depth
- **Why needed here:** The paper operationalizes "shatterability" using Tukey depth to partition the domain into a "deep core" (hard to shatter) and a "shallow" region (easy to shatter).
- **Quick check question:** Does a point on the surface of a sphere have high or low Tukey depth?

### Concept: Edge of Stability (EoS)
- **Why needed here:** The entire analysis is conditional on the network satisfying the "Below-Edge-of-Stability" (BEoS) condition. You must distinguish this regime from standard convex optimization.
- **Quick check question:** If $\eta = 0.4$, what is the maximum allowable Hessian eigenvalue $\lambda_{max}$ for a solution to be considered BEoS?

## Architecture Onboarding

### Component map:
Two-layer fully connected ReLU network: $f(x) = \sum v_k \phi(w_k^T x - b_k) + \beta$

### Critical path:
1. **Synthetic Data Generation:** Create datasets with specific geometries (Lines in $R^d$ vs. Isotropic Spheres).
2. **Training:** Run Vanilla Gradient Descent with large learning rate (e.g., $\eta=0.4$) until the Hessian eigenvalue $\lambda_{max}$ stabilizes near $2/\eta$.
3. **Analysis:** Correlate generalization error with data geometry (intrinsic dimension $m$ vs. concentration $\alpha$) and neuron statistics (activation rates).

### Design tradeoffs:
- **Width:** Upper bounds are width-agnostic, but lower bound constructions require width $K \ge n$. Use sufficient overparameterization.
- **Learning Rate:** Must be "large" enough to enter the EoS regime ($\lambda_{max} \approx 2/\eta$), but not so large as to diverge.

### Failure signatures:
- **Memorization:** Models trained on spherical/concentrated data will interpolate (train loss $\approx 0$) but have high test error.
- **Unstable Training:** If $\lambda_{max}$ does not stabilize near $2/\eta$, the theoretical bounds do not apply.
- **Neuron Specialization:** On "shatterable" data, look for neurons with very low activation rates ($<10\%$) but large weight magnitudes.

### First 3 experiments:
1. **Intrinsic Dimension Verification:** Train on a union of 20 lines ($m=1$) embedded in $R^{500}$. Verify that the generalization slope on a log-log plot is invariant to $d$.
2. **Shatterability Spectrum:** Train on isotropic Beta-radial distributions in $R^5$ with varying $\alpha \in \{1, 5, 10, 50\}$. Plot log(MSE) vs. log(n) to observe varying slopes.
3. **Representation Analysis:** Train on "Sphere" vs. "Low-dim Mixture". Plot histograms of neuron activation rates to confirm the emergence of "memorizing neurons" on spherical data.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is high data shatterability correlated with faster optimization convergence?
- **Basis in paper:** Section 5 posits that "a flip side of being prone to overfitting is often faster optimization."
- **Why unresolved:** The paper focuses on the generalization gap and implicit regularization, not the convergence speed or optimization dynamics.
- **Resolution:** Empirical measurement of convergence rates on distributions with varying shatterability.

### Open Question 2
- **Question:** Can the shatterability principle be extended to deep networks where hidden layers generate new "representation geometry"?
- **Basis in paper:** Section 5 identifies extending the framework to deeper networks as a "key direction."
- **Why unresolved:** The theoretical results are derived strictly for two-layer fully-connected ReLU networks.
- **Resolution:** Theoretical analysis or empirical verification showing layer-wise shatterability in deep architectures.

### Open Question 3
- **Question:** Do normalization techniques like Batch Norm accelerate training by enforcing more shatterable representations?
- **Basis in paper:** Section 5 raises the question of whether normalization techniques enforce "more isotropic, and thus more shatterable, representations."
- **Why unresolved:** The paper analyzes input data distributions, not the effect of normalization layers on internal representations.
- **Resolution:** Analysis of internal activation statistics with and without normalization to measure changes in shatterability.

### Open Question 4
- **Question:** Do specific architectural biases like CNNs generalize well because their local receptive fields inherently reduce shatterability?
- **Basis in paper:** Section 5 asks if CNNs generalize "precisely because their local receptive fields... reduce the model’s ability to shatter the data."
- **Why unresolved:** The paper's analysis is limited to fully-connected networks and specific data geometries.
- **Resolution:** Quantifying shatterability in architectures with constrained receptive fields and linking it to generalization performance.

## Limitations
- The theoretical analysis relies heavily on specific distributional assumptions (mixture of low-dimensional balls, isotropic Beta-radial distributions) that may not generalize to natural data distributions.
- The empirical validation uses synthetic data and limited real-world datasets (MNIST vs. Gaussian noise), raising questions about the robustness of the "data shatterability" principle to complex, high-dimensional natural signals.
- The mechanism is specific to two-layer fully-connected ReLU networks trained with large learning rates; its applicability to deeper architectures, other activation functions, or different optimization algorithms is unknown.

## Confidence
- **High Confidence:** The mechanism linking Below-Edge-of-Stability (BEoS) training to weighted path norm regularization is mathematically rigorous and well-supported by the theoretical framework.
- **Medium Confidence:** The adaptation to intrinsic dimension is strongly supported for the specific synthetic data distributions studied, but its applicability to real-world data with complex manifold structures requires further investigation.
- **Medium Confidence:** The spectrum of generalization bounds based on radial concentration is theoretically sound for the isotropic Beta-radial family, but the operationalization of "shatterability" through Tukey depth and its empirical measurement could be more explicit.

## Next Checks
1. **Natural Data Manifold Analysis:** Apply the theoretical framework to real image datasets (e.g., CIFAR-10) by estimating the intrinsic dimension of data manifolds using methods like nearest neighbor statistics or autoencoders. Validate whether generalization correlates with estimated intrinsic dimension rather than ambient pixel dimension.

2. **Shatterability Metric on Real Data:** Develop and validate a quantitative "shatterability" metric for natural datasets beyond Tukey depth (e.g., based on activation pattern diversity or data partitionability by ReLU neurons). Test whether this metric predicts generalization performance across different architectures and training regimes.

3. **Robustness to Training Variations:** Investigate the sensitivity of the BEoS-induced regularization to training variations: (a) different optimization algorithms (SGD with momentum, Adam), (b) different loss functions (cross-entropy vs. MSE), and (c) varying degrees of label noise. Determine whether the data geometry effects persist across these variations.