---
ver: rpa2
title: Bidirectional Representations Augmented Autoregressive Biological Sequence
  Generation
arxiv_id: '2510.08169'
source_url: https://arxiv.org/abs/2510.08169
tags:
- peptide
- sequence
- should
- decoder
- mass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CROSSNOVO is a hybrid architecture for de novo peptide sequencing
  that combines autoregressive (AR) and non-autoregressive (NAR) transformers. It
  uses a shared spectrum encoder with an AR decoder for sequential peptide generation
  and a NAR decoder for bidirectional context learning.
---

# Bidirectional Representations Augmented Autoregressive Biological Sequence Generation

## Quick Facts
- arXiv ID: 2510.08169
- Source URL: https://arxiv.org/abs/2510.08169
- Reference count: 40
- Key outcome: CROSSNOVO achieves AA precision 0.811 and peptide recall 0.654 on 9-species benchmark, surpassing AR and NAR baselines

## Executive Summary
CROSSNOVO introduces a hybrid transformer architecture for de novo peptide sequencing that combines autoregressive (AR) and non-autoregressive (NAR) decoding. The model uses a shared spectrum encoder with an AR decoder for sequential peptide generation and a NAR decoder for bidirectional context learning. A cross-decoder attention module transfers the NAR's bidirectional knowledge to the AR decoder, while gradient blocking prevents interference. The model employs a multitask training strategy with importance annealing to balance AR and NAR objectives. On a 9-species benchmark, CROSSNOVO achieves amino acid precision of 0.811 and peptide recall of 0.654, surpassing both AR and NAR baselines.

## Method Summary
CROSSNOVO uses a shared spectrum encoder that processes mass spectrometry peaks with sinusoidal positional encoding. The NAR decoder employs bidirectional attention and CTC loss to generate peptide sequences in parallel, while the AR decoder uses causal attention and cross-entropy loss for sequential generation. During training, a cross-decoder attention module allows the AR decoder to query NAR features, with gradient blocking preventing AR updates from corrupting NAR representations. A two-stage training process first jointly optimizes both decoders with importance annealing, then fine-tunes the AR decoder with frozen NAR features.

## Key Results
- Amino acid precision of 0.811 on 9-species-v1 benchmark
- Peptide recall of 0.654 on 9-species-v1 benchmark
- Outperforms both pure AR and pure NAR baselines
- Shows strong performance on antibody data and post-translational modification tasks

## Why This Works (Mechanism)

### Mechanism 1: Cross-Decoder Attention for Bidirectional Knowledge Injection
The AR decoder leverages holistic sequence context by attending to NAR decoder's latent representations. The AR decoder's cross-attention queries an augmented context combining NAT features V(L′) with spectrum features E(b), using distinct positional encodings to differentiate sources. This enables each AR generation step to access globally-informed embeddings encoding downstream token relationships.

### Mechanism 2: Cross-Decoder Gradient Blocking for Representational Stability
Freezing gradient flow from AR loss into NAT features preserves NAR-optimized representations. The NAT features V(L′) are detached from the AR decoder's computation graph, preventing AR gradient updates from perturbing the NAR's CTC-optimized latent space and avoiding "representational drift."

### Mechanism 3: Importance Annealing for Curriculum-Style Multi-Objective Balancing
Gradually shifting loss weight from NAR to AR objectives improves convergence and final performance. The total loss uses λ_AT(i) = i/T_total, linearly increasing AR weight. Early training prioritizes NAR's CTC loss to establish robust spectral representations, with later training refining AR's sequential generation fidelity.

## Foundational Learning

- **Concept: Autoregressive (AR) vs Non-Autoregressive (NAR) Decoding**
  - Why needed: CROSSNOVO's innovation depends on understanding that AR generates tokens sequentially while NAR generates all positions in parallel
  - Quick check: Can you explain why AR decoding enforces unidirectional context, and what architectural change makes NAR decoding bidirectional?

- **Concept: Connectionist Temporal Classification (CTC) Loss**
  - Why needed: The NAR decoder uses CTC to handle variable-length peptide sequences without explicit alignment
  - Quick check: How does CTC enable training when the input-output alignment is unknown, and what role does the blank token play?

- **Concept: Transformer Cross-Attention and Causal Masking**
  - Why needed: Both decoders attend to shared spectrum features via cross-attention; AR decoder additionally uses causal self-attention masks
  - Quick check: What is the difference between self-attention with and without a causal mask, and how does cross-attention differ from self-attention?

## Architecture Onboarding

- **Component map:** Spectrum peaks (m/z, intensity) → Shared Encoder (9-layer transformer) → NAR Decoder (bidirectional + cross-attention) → CTC loss, AR Decoder (causal + cross-attention to augmented context) → CE loss

- **Critical path:** Stage 1 (Joint Training): Encoder + NAT + AT trained together with L_total, importance annealing; Stage 2 (Fine-tuning): Freeze encoder + NAT; fine-tune AR with cross-decoder attention enabled

- **Design tradeoffs:** Beam size vs accuracy (plateauing gains beyond beam=5); NAT length (T_max must accommodate longest peptides); Gradient blocking vs joint fine-tuning (blocking stabilizes but locks NAT features)

- **Failure signatures:** Gradient explosion without blocking; Precision plateau or recall drop with very large beam sizes; NAT producing empty or repetitive sequences

- **First 3 experiments:** 1) Ablation sanity check: Train with and without gradient blocking on small subset; 2) Cross-attention alignment visualization: Visualize AR cross-attention weights over NAT features; 3) Annealing schedule sweep: Compare linear vs step-wise vs fixed λ on validation recall

## Open Questions the Paper Calls Out
None

## Limitations
- Sparse domain precedent for cross-decoder gradient blocking mechanism
- Parameter sensitivity to T_max, annealing schedule, and beam size not extensively validated
- Evaluation metric sensitivity to mass error thresholds and instrument resolution

## Confidence
- Cross-decoder attention mechanism: High
- Gradient blocking necessity: Medium
- Importance annealing benefit: Medium

## Next Checks
1. Gradient blocking ablation with controlled fine-tuning: Test reduced AR learning rate without blocking and monitor NAR feature stability
2. Cross-attention alignment validation: Visualize AR decoder's cross-attention weights over concatenated NAT features and spectrum features
3. Annealing schedule sensitivity sweep: Compare linear, step-wise, and fixed annealing schedules on peptide recall performance