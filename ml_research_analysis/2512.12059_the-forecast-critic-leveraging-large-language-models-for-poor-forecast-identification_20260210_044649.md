---
ver: rpa2
title: 'The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification'
arxiv_id: '2512.12059'
source_url: https://arxiv.org/abs/2512.12059
tags:
- forecast
- time
- historical
- series
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Forecast Critic leverages Large Language Models (LLMs) to automatically
  identify poor forecasts in time series data by evaluating visual plots and incorporating
  contextual information. We systematically assess LLM performance on detecting common
  forecast errors (trend misalignment, vertical shifts, stretched/compressed periodicity,
  and spike errors) using both synthetic and real-world data from the M5 dataset.
---

# The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification

## Quick Facts
- **arXiv ID**: 2512.12059
- **Source URL**: https://arxiv.org/abs/2512.12059
- **Reference count**: 35
- **Primary result**: LLM-based system achieves F1 score of 0.88 on synthetic data for poor forecast identification

## Executive Summary
The Forecast Critic introduces an automated system that leverages Large Language Models (LLMs) to identify poor forecasts in time series data by analyzing visual plots and incorporating contextual information. The system systematically evaluates forecasts for common error types including trend misalignment, vertical shifts, stretched/compressed periodicity, and spike errors. On synthetic data, the best-performing LLM model achieves an F1 score of 0.88, slightly below human expert performance at 0.97. When provided with promotional context, multi-modal LLMs can correctly identify missing or spurious spikes with F1 scores up to 0.84. Real-world validation on M5 dataset forecasts demonstrates that forecasts flagged as unreasonable by LLMs have at least 10% higher sCRPS than those considered reasonable, validating the system's effectiveness as an automated forecast monitoring tool.

## Method Summary
The Forecast Critic system processes time series forecasts through a multi-stage pipeline. First, it generates visual representations of forecast data including observed values, forecast predictions, and uncertainty intervals. These visualizations are then analyzed by LLMs to detect forecast errors across four categories: trend misalignment (incorrect directional patterns), vertical shifts (level offsets), periodicity distortions (stretched or compressed seasonal patterns), and spike errors (incorrect peak detection). The system uses both text-only and multi-modal LLMs, with the latter incorporating contextual information such as promotional events. Synthetic data generation allows systematic testing across controlled error types, while real-world M5 dataset validation provides practical verification. Performance is measured using F1 scores against human expert annotations, with additional evaluation using sCRPS (scaled Continuous Ranked Probability Score) to quantify forecast accuracy differences between flagged and unflagged forecasts.

## Key Results
- Best LLM model achieves F1 score of 0.88 on synthetic data for poor forecast identification, compared to human expert performance of 0.97
- LLMs reliably identify trend modifications and vertical translations but struggle with periodicity distortions
- Multi-modal LLMs with promotional context achieve F1 scores up to 0.84 for spike detection in real-world M5 data
- Forecasts flagged as unreasonable have at least 10% higher sCRPS than reasonable forecasts in real-world validation

## Why This Works (Mechanism)
The system works by leveraging LLMs' pattern recognition capabilities to analyze visual representations of forecast performance. LLMs can detect subtle deviations from expected patterns in time series data that might be missed by traditional statistical metrics alone. By combining visual analysis with contextual information, the system can identify both structural errors (like trend misalignment) and context-dependent errors (like missing promotional spikes). The synthetic data generation allows controlled testing of specific error types, while the multi-modal approach enables incorporation of domain knowledge that improves spike detection accuracy.

## Foundational Learning

**Time series forecasting fundamentals** - Understanding basic forecast components (trend, seasonality, level) is essential for interpreting forecast errors and evaluating LLM performance across different error types.

**sCRPS metric** - The scaled Continuous Ranked Probability Score measures probabilistic forecast accuracy, needed to quantify the practical impact of LLM-identified forecast errors in real-world data.

**LLM multimodal capabilities** - Understanding how models process both visual and textual information is crucial for implementing the contextual analysis that improves spike detection performance.

**Synthetic data generation** - Creating controlled error scenarios allows systematic evaluation of LLM performance across different forecast error types, providing ground truth for model assessment.

**Forecast error taxonomy** - Classifying forecast errors into distinct categories (trend, level, periodicity, spikes) enables targeted evaluation of LLM capabilities and identification of strengths and weaknesses.

## Architecture Onboarding

**Component map**: Time series data -> Visualization generator -> LLM analyzer -> Error classifier -> Performance evaluator

**Critical path**: Forecast data flows through visualization generation, LLM analysis, and error classification to produce final quality assessments. The visualization-LLM interaction is the core of the system.

**Design tradeoffs**: Text-only vs multi-modal LLMs - text-only models are simpler and faster but multi-modal models with contextual information achieve better spike detection accuracy. Synthetic vs real data - synthetic data provides controlled testing but may not capture all real-world complexity.

**Failure signatures**: LLMs struggle most with periodicity distortions and combined error types. Performance degrades when errors are subtle or when multiple error types occur simultaneously. Context-dependent errors require additional information beyond visual analysis.

**First experiments**:
1. Test LLM performance on synthetic data with single error types (trend, level, periodicity, spikes) to establish baseline capabilities
2. Evaluate multi-modal LLM performance with contextual information for spike detection in promotional scenarios
3. Compare synthetic data results against human expert annotations to validate system accuracy

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Performance gap between LLMs and human experts (F1: 0.88 vs 0.97) indicates room for improvement, particularly for periodicity distortion detection
- Real-world validation limited to single M5 dataset, raising questions about generalizability across different forecasting domains
- Visual analysis approach depends on quality and consistency of generated visualizations, introducing potential biases
- Limited testing on complex error scenarios combining multiple error types simultaneously

## Confidence
- **High confidence** in LLM capability to identify clear trend and level errors based on consistent synthetic data results
- **Medium confidence** in multi-modal LLM performance for spike detection with contextual information, given promising but limited real-world validation
- **Low confidence** in LLM generalization to novel forecast error types and domains beyond those explicitly tested

## Next Checks
1. Test the Forecast Critic on additional real-world forecasting datasets from diverse domains (e.g., energy demand, financial markets, weather forecasting) to assess generalizability across different time series patterns and forecasting contexts

2. Conduct blind validation studies where the LLM-generated flags are compared against ground truth forecast quality assessments from multiple independent human experts to verify the reliability of the 10% sCRPS threshold in practice

3. Evaluate the system's performance on forecast errors that combine multiple characteristics (e.g., trend shifts with periodic distortions) to determine whether the current error-specific approach scales to more realistic, complex forecasting scenarios