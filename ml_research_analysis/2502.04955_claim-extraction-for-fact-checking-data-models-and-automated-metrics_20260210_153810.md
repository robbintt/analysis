---
ver: rpa2
title: 'Claim Extraction for Fact-Checking: Data, Models, and Automated Metrics'
arxiv_id: '2502.04955'
source_url: https://arxiv.org/abs/2502.04955
tags:
- claims
- claim
- linguistics
- association
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses claim extraction for fact-checking, comparing
  generative models (LLMs, small summarization models, and a NER-based baseline) using
  a new FEVERFact dataset. The evaluation framework includes reference-free metrics
  (Atomicity, Fluency, Decontextualization, Faithfulness) and reference-based metrics
  (Focus, Coverage, Redundancy).
---

# Claim Extraction for Fact-Checking: Data, Models, and Automated Metrics

## Quick Facts
- arXiv ID: 2502.04955
- Source URL: https://arxiv.org/abs/2502.04955
- Reference count: 28
- Key outcome: T5-based models and Mistral-7B finetuned on FEVERFact achieve highest F_fact scores (harmonic mean of Focus and Coverage) in claim extraction evaluation framework

## Executive Summary
This paper addresses claim extraction for fact-checking by comparing generative models using a new FEVERFact dataset. The study evaluates three approaches: a NER-based baseline (QACG), large language models (GPT-4 few-shot and Mistral-7B QLoRA fine-tuned), and T5-small models fine-tuned on single-sentence summarization. A comprehensive evaluation framework includes both reference-free metrics (Atomicity, Fluency, Decontextualization, Faithfulness) and reference-based metrics (Focus, Coverage, Redundancy). The results show that modern models achieve high scores in reference-free metrics, with Mistral-7B finetuned on FEVERFact performing best in the F_fact metric (0.61), closely followed by T5 with diverse beam search (0.56). The evaluation framework closely approximates human grading with F1 scores above 80% for reference-free metrics and RMSE around 0.22 for Focus and Coverage.

## Method Summary
The study compares three claim extraction approaches on the FEVERFact dataset (4.4K documents, 17K claims). The QACG baseline uses off-the-shelf NER and question generation. LLM approaches include GPT-4-turbo with 3-shot prompting and Mistral-7B with QLoRA fine-tuning (r=64, α=32, 4-bit quantization). T5-small models are fine-tuned on single-sentence summarization (XSum) then on FEVERFact, using either multi-claim concatenation or single-claim prediction with diverse beam search (k=7, diversity penalty=1). Evaluation uses six metrics: four reference-free (Atomicity, Fluency, Decontextualization, Faithfulness) and two reference-based (Focus, Coverage), with F_fact as the harmonic mean of Focus and Coverage.

## Key Results
- Mistral-7B fine-tuned on FEVERFact achieves highest F_fact score (0.61), followed by T5 with diverse beam search (0.56)
- Reference-free metrics show F1 scores above 80% when approximating human judgment
- Focus and Coverage exhibit precision-recall tradeoff behavior, with F_fact effectively balancing the tradeoff
- Diverse beam search with single-sentence summarization models achieves low redundancy (0.59) while maintaining high coverage

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Single-sentence abstractive summarization models transfer effectively to claim extraction when combined with diverse beam search decoding.
- **Mechanism:** T5 models pre-trained on span-corruption objectives, then fine-tuned on single-sentence summarization (XSum), learn to extract salient information into standalone sentences. Diverse beam search with k beam groups and diversity penalty=1 generates multiple non-redundant claims per document without requiring architectural changes.
- **Core assumption:** Claim extraction shares structural properties with extreme summarization—both require identifying key information and expressing it in a self-contained sentence.
- **Evidence anchors:**
  - [abstract] "validates that diverse beam search with single-sentence summarization models is effective for claim extraction"
  - [Section 3] "T5 was pre-trained on various text-to-text tasks using a span-corruption objective, making it adapt well to our task despite a relatively small train size"
- **Break condition:** If source documents contain claims requiring multi-sentence context or complex coreference chains not present in summarization training data, transfer quality degrades.

### Mechanism 2
- **Claim:** Automated claim quality metrics approximate human judgment by reducing each criterion to an existing NLP task with established solvers.
- **Mechanism:** Each metric maps to a task: Atomicity→Relation Extraction (REBEL model, |relations|≤1), Fluency→Grammatical Error Correction+Scoring (CoEdIT+Scribendi), Decontextualization→Text-to-text rewriting (T5-large from Choi et al.), Faithfulness→Alignment scoring (AlignScore), Focus/Coverage→Semantic overlap via NLI-style faithfulness checks on concatenated claims.
- **Core assumption:** Task-specific models capture the relevant linguistic properties and correlate with human quality judgments.
- **Evidence anchors:**
  - [abstract] "evaluation framework closely approximates human grading, with F1 scores above 80% for reference-free metrics and RMSE around 0.22 for Focus and Coverage"
  - [Section 4.1] "For each metric, we implement a scale using a reduction to an already-explored NLP task"
- **Break condition:** If any task-reduction model (e.g., REBEL for atomicity, AlignScore for faithfulness) has systematic biases or blind spots, metric validity degrades for edge cases.

### Mechanism 3
- **Claim:** Focus and Coverage exhibit a precision-recall tradeoff that F_fact (harmonic mean) effectively balances for model comparison.
- **Mechanism:** Focus measures proportion of generated claims consistent with gold claims (precision-like); Coverage measures proportion of gold information captured (recall-like). Naïve models optimizing one metric sacrifice the other (e.g., QACG covers everything but includes irrelevant claims: Coverage=0.67, Focus=0.20). F_fact = 2×Focus×Coverage/(Focus+Coverage) penalizes extreme imbalances.
- **Core assumption:** The gold claim annotations in FEVERFact represent a reasonable approximation of what "should" be extracted.
- **Evidence anchors:**
  - [Section 4.3] "the Foc and Cov do indeed behave like Precision and Recall metrics in the sense of their mutual tradeoff"
  - [Section 5.1] "models with highest coverage focus too little and vice versa"
- **Break condition:** If gold annotations have low recall of check-worthy claims (paper notes only ~67% named entity recall), models may be penalized for extracting valid claims not in gold set.

## Foundational Learning

- **Concept: One-to-many sequence-to-sequence generation**
  - Why needed here: Claim extraction requires producing variable-length sets of claims from single documents, not one output per input.
  - Quick check question: Can you explain why standard seq2seq with single outputs fails for this task and how diverse beam search addresses it?

- **Concept: Decontextualization in NLP**
  - Why needed here: Claims must be interpretable without source document context; requires resolving pronouns, relative terms, and implicit references.
  - Quick check question: Given "She founded the Missionaries of Charity" with context about Mother Teresa, what transformations are needed to make it standalone?

- **Concept: Faithfulness/hallucination detection**
  - Why needed here: Generated claims must not introduce information absent from source; AlignScore and NLI-based methods detect this.
  - Quick check question: Why can't simple token overlap detect all hallucinations, and what semantic phenomena require NLI-style approaches?

## Architecture Onboarding

- **Component map:** Document → Model inference → Claim set generation → Metric computation → F_fact scoring
- **Critical path:** Document → Model inference → Claim set generation → Metric computation → F_fact scoring. For training: FEVERFact train split → fine-tuning → dev set evaluation.
- **Design tradeoffs:**
  - Single-claim training + diverse beam search vs. multi-claim concatenation: Single-claim easier to train, higher redundancy (0.59 vs 0.54); multi-claim lower redundancy but requires claim separation post-processing.
  - LLMs vs. small models: GPT-4 highest coverage (0.81) but lowest focus (0.21); fine-tuned Mistral best F_fact balance (0.64). Small T5 competitive (0.56) with 60x fewer parameters.
- **Failure signatures:**
  - Non-atomic claims: Complex multi-relation outputs failing |RE(c)|≤1 check
  - Context-dependent claims: Pronouns or relative terms unresolved when T5-decontext modifies the claim
  - Low Focus high Coverage: Model extracts from full document context instead of target sentence (QACG signature: Focus=0.19, Coverage=0.60)
  - High Redundancy: Diverse beam search without sufficient diversity penalty produces semantically similar claims
- **First 3 experiments:**
  1. Reproduce baseline on FEVERFact-test using released t5_sm_diverse_7_beams weights; verify automated metrics match reported values (F_fact≈0.56).
  2. Ablate diverse beam search parameters: test k∈{3,5,7,10} and diversity_penalty∈{0.5,1.0,1.5}; measure F_fact and Redundancy tradeoffs.
  3. Human-evaluate 20 random samples comparing Mistral-QLoRA vs. T5-diverse; annotate Focus/Coverage manually to validate RMSE≈0.22 calibration on new data.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation framework depends on existing NLP models that may have systematic biases affecting metric validity
- Dataset size limited to 4.4K documents with gold claims capturing only ~67% of check-worthy information
- Results may not generalize beyond Wikipedia domain or to more complex claim extraction scenarios

## Confidence
- **High Confidence:** T5-small fine-tuned on FEVERFact achieves competitive F_fact scores (0.56) compared to much larger models, demonstrating effective transfer from summarization
- **Medium Confidence:** Automated metrics' ability to approximate human judgment (F1 > 80%, RMSE ~0.22) relies on assumption that FEVERFact annotations are representative
- **Low Confidence:** Generalizability of diverse beam search effectiveness beyond T5 and XSum, and completeness of gold claim annotations

## Next Checks
1. Apply evaluation framework and best-performing models to an independently constructed claim extraction dataset from different domains to test metric reliability and model generalization beyond FEVERFact's Wikipedia domain.

2. Conduct human evaluation on a random sample of documents to estimate recall of check-worthy claims in the gold annotations, testing the assumption that Focus/Coverage metrics fairly evaluate model performance when gold data may be incomplete.

3. Systematically evaluate how each automated metric responds to controlled perturbations of claims (e.g., adding hallucinations, removing context, introducing multi-relation claims) to identify potential blind spots in the task-reduction approach.