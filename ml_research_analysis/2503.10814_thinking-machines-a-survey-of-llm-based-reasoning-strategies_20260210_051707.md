---
ver: rpa2
title: 'Thinking Machines: A Survey of LLM based Reasoning Strategies'
arxiv_id: '2503.10814'
source_url: https://arxiv.org/abs/2503.10814
tags:
- reasoning
- arxiv
- preprint
- language
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive review of reasoning strategies
  in large language models (LLMs). It identifies three main paradigms for achieving
  reasoning: reinforcement learning, test-time computation, and self-training.'
---

# Thinking Machines: A Survey of LLM based Reasoning Strategies

## Quick Facts
- arXiv ID: 2503.10814
- Source URL: https://arxiv.org/abs/2503.10814
- Reference count: 17
- Primary result: Survey organizes LLM reasoning strategies into RL, test-time computation, and self-training paradigms

## Executive Summary
This survey comprehensively reviews reasoning strategies in large language models, identifying three main paradigms for achieving reasoning: reinforcement learning, test-time computation, and self-training. The paper organizes and explains these approaches, highlighting their applications in complex problem-solving and sensitive domains like healthcare and law. A key finding is that scaling pre-training has diminishing returns for reasoning tasks, while test-time techniques and hybrid approaches show promise.

## Method Summary
The paper provides a comprehensive literature survey of LLM reasoning strategies, organizing them into three main paradigms. The authors conducted an extensive review of recent papers on reinforcement learning approaches, test-time computation methods, and self-training techniques. They synthesized findings from various studies to create a structured overview of current reasoning methodologies, their mechanisms, and applications.

## Key Results
- Scaling pre-training has diminishing returns for reasoning tasks
- Test-time techniques and hybrid approaches show promise for improving reasoning
- Three main paradigms identified: reinforcement learning, test-time computation, and self-training

## Why This Works (Mechanism)

### Mechanism 1: Process Supervision Signals Guide Reasoning Chains
Process Reward Models (PRMs) assign scalar values to each reasoning step, enabling credit assignment across the entire reasoning trajectory. During training or search, the policy optimizes toward high-value intermediate states rather than only receiving sparse signals at task completion. This assumes correct intermediate steps causally contribute to correct final answers.

### Mechanism 2: Test-Time Compute Enables Dynamic Reasoning Depth
Techniques like Chain-of-Thought (CoT), Tree-of-Thought (ToT), and Monte Carlo Tree Search (MCTS) generate multiple candidate reasoning traces, score them via verifiers or self-feedback, and select the highest-scoring path. This bypasses the need for retraining by allocating additional computation during inference to explore multiple reasoning paths and self-correct.

### Mechanism 3: Self-Generated Reasoning Traces Bootstrap Model Improvement
Self-training pipelines generate candidate reasoning traces, filter via correctness checks (rejection fine-tuning), and optionally construct preference pairs for DPO-style updates. This creates a curriculum from self-produced data, assuming correct-answer-conditioned traces contain learnable reasoning patterns.

## Foundational Learning

- **Policy Gradient Methods (PPO, GRPO)**: Essential for RL-based reasoning approaches that frame the LLM as a policy π_θ and optimize expected cumulative reward. Quick check: Can you explain why GRPO removes the need for a separate value network compared to PPO?

- **Monte Carlo Tree Search (MCTS)**: Central to search-based reasoning and test-time feedback methods. Quick check: What role does the UCT (Upper Confidence Bound for Trees) play in the selection phase of MCTS?

- **Preference Optimization (DPO and variants)**: Step-DPO extends DPO to step-level preference pairs. Quick check: How does Step-DPO differ from vanilla DPO in terms of what constitutes a preference pair?

## Architecture Onboarding

- **Component map:**
```
                    ┌─────────────────┐
                    │   Base LLM      │
                    │  (frozen or     │
                    │   trainable)    │
                    └────────┬────────┘
                             │
          ┌──────────────────┼──────────────────┐
          │                  │                  │
          ▼                  ▼                  ▼
   ┌─────────────┐   ┌─────────────┐   ┌─────────────┐
   │ Verifier /  │   │  World      │   │  External   │
   │ Reward      │   │  Model      │   │  Feedback   │
   │ Model       │   │  (LLM-based)│   │  (Code/KB)  │
   └──────┬──────┘   └──────┬──────┘   └──────┬──────┘
          │                  │                  │
          └──────────────────┼──────────────────┘
                             │
                    ┌────────▼────────┐
                    │  Search /       │
                    │  Aggregation    │
                    │  (MCTS/Beam/    │
                    │   Voting)       │
                    └────────┬────────┘
                             │
                    ┌────────▼────────┐
                    │  Final Output   │
                    └─────────────────┘
```

- **Critical path:**
  1. Choose supervision type: process vs. outcome
  2. Select compute allocation: training-time (self-training), test-time (TTC), or hybrid
  3. If using search, implement MCTS or beam search with verifier integration
  4. If using self-training, set up rejection filtering and/or preference pair construction

- **Design tradeoffs:**
  - Process supervision vs. outcome supervision: Higher annotation cost vs. sparser rewards
  - MCTS vs. beam search: MCTS provides deeper exploration but faces "vast search space and often overthinks"
  - Self-feedback vs. external verifier: Self-feedback avoids external dependencies but "LLMs may not effectively aid in planning"

- **Failure signatures:**
  - Models generate plausible-sounding but incorrect intermediate steps that pass verifier checks
  - Excessive test-time compute without quality improvement
  - Self-training collapses when rejection filtering eliminates nearly all samples

- **First 3 experiments:**
  1. **Baseline CoT vs. ToT comparison**: Implement on GSM8K subset, measure accuracy vs. compute tokens
  2. **Outcome-only vs. step-feedback verifier**: Train verifier models and compare on multi-step problems
  3. **Self-training loop pilot**: Run STaR-style rejection fine-tuning on small model, monitor acceptance rate and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How can the generation of process supervision signals be fully automated to eliminate the reliance on costly human annotation? Current methods depend on expensive, fine-grained human supervision, which limits the scalability of training models to verify intermediate reasoning steps effectively.

### Open Question 2
How can search-based strategies like MCTS be optimized to prevent "overthinking" and reduce unnecessary computational overhead? There is currently no standardized mechanism to efficiently prune the search space or terminate reasoning traces early enough to save resources without sacrificing solution quality.

### Open Question 3
Can test-time compute strategies be adapted to enhance reasoning in smaller language models (<10B parameters) where they currently prove detrimental? It is unclear if the failure in smaller models is due to fundamental capacity limits or if current test-time strategies are simply mismatched to smaller architectures.

## Limitations
- Presents an organizational framework rather than empirical validation of proposed reasoning strategies
- Limited quantitative analysis of failure modes and specific conditions
- Assessment of hybrid approaches lacks comparative benchmarks showing when each strategy is optimal

## Confidence

**High Confidence**: The categorization of reasoning strategies into three paradigms and basic mechanism descriptions are well-supported by literature.

**Medium Confidence**: Claims about diminishing returns from scaling pre-training and promise of test-time techniques are supported by citations but need primary experimental validation.

**Low Confidence**: Specific assertions about relative performance (e.g., "DeepSeek-R1 challenges consensus") lack sufficient context about experimental conditions or baselines.

## Next Checks
1. **Benchmark Comparative Study**: Implement all three paradigms on identical reasoning tasks (GSM8K, MATH) with controlled compute budgets to empirically determine optimal approaches.

2. **Failure Mode Characterization**: Systematically catalog and measure failure conditions for each reasoning strategy across different reasoning task complexities.

3. **Hybrid Strategy Optimization**: Design experiments testing various combinations of the three paradigms to identify optimal hybrid configurations and their computational trade-offs.