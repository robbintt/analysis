---
ver: rpa2
title: Sculpting [CLS] Features for Pre-Trained Model-Based Class-Incremental Learning
arxiv_id: '2502.14762'
source_url: https://arxiv.org/abs/2502.14762
tags:
- learning
- methods
- pre-trained
- tosca
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TOSCA, a class-incremental learning method
  that addresses catastrophic forgetting in pre-trained models by deploying a single
  lightweight Learn and Calibrate (LuCA) module on the final [CLS] token. The LuCA
  module combines an adapter for task-specific feature transformation with a calibrator
  for discriminative feature enhancement, achieving a balance between stability and
  plasticity.
---

# Sculpting [CLS] Features for Pre-Trained Model-Based Class-Incremental Learning

## Quick Facts
- **arXiv ID:** 2502.14762
- **Source URL:** https://arxiv.org/abs/2502.14762
- **Reference count:** 40
- **Primary result:** A single sparse adapter on the final [CLS] token achieves 4-12% higher accuracy than adapter-based methods and 7-21% higher than prompt-based methods on out-of-distribution datasets.

## Executive Summary
This paper introduces TOSCA, a parameter-efficient method for class-incremental learning (CIL) that addresses catastrophic forgetting by deploying a lightweight Learn and Calibrate (LuCA) module exclusively on the final [CLS] token of a frozen Vision Transformer. The LuCA module combines an adapter for task-specific feature transformation with a calibrator for discriminative feature enhancement, achieving a balance between stability and plasticity. By localizing adaptations to the final semantic aggregation point, TOSCA preserves the pre-trained model's feature hierarchy while enabling efficient task-specific adaptation. Experiments demonstrate superior performance compared to adapter-based and prompt-based CIL methods, with significantly fewer parameters.

## Method Summary
TOSCA addresses class-incremental learning by attaching a task-specific LuCA module to the final [CLS] token of a frozen pre-trained Vision Transformer. For each new task, a new LuCA module is initialized and trained with cross-entropy loss plus L1 regularization to promote orthogonal parameter configurations across tasks. The LuCA module consists of an adapter (bottleneck MLP with residual connection) followed by a calibrator (squeeze-excite gating mechanism). During inference, task identification is performed via entropy minimization over predictions from all task-specific LuCA modules, selecting the output with lowest uncertainty. The method uses prototypical classifiers and trains with SGD for 20 epochs per task.

## Key Results
- Achieves 4-12% higher accuracy than adapter-based methods (EASE, MOS) on out-of-distribution datasets
- Outperforms prompt-based methods (L2P, DualPrompt) by 7-21% on the same datasets
- Uses ~8× fewer parameters than layer-wise adapters in ViT-B/16
- Maintains strong performance across diverse benchmarks including CIFAR-100, CUB-200, ImageNet-R, ImageNet-A, OmniBenchmark, and VTAB

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A single sparse adapter placed exclusively on the final [CLS] token balances stability and plasticity for class-incremental learning.
- **Mechanism:** The frozen pre-trained backbone (PTM) extracts stable general features while the lightweight LuCA module at the final token learns task-specific residuals via bottleneck projections and calibration gating, avoiding modifications to the feature hierarchy in earlier layers.
- **Core assumption:** The [CLS] token of a ViT aggregates global semantic information sufficient for task-specific refinement without needing to alter intermediate representations.
- **Evidence anchors:** [abstract] "To address this challenge, we first introduce a new parameter-efficient fine-tuning module 'Learn and Calibrate', or LuCA... Second, for each learning session, we deploy a sparse LuCA module on top of the last [CLS] token just before the classifier, which we refer to as 'Token-level Sparse Calibration and Adaptation', or TOSCA." [section 4] "First, by localizing adaptations to the final semantic aggregation point (the [CLS] token), TOSCA preserves the model's feature hierarchy where low/mid-level features remain stable while the final high-level abstractions adapt to new tasks."
- **Break condition:** If tasks require fine-grained spatial feature modifications that cannot be captured by global [CLS] token adjustments (e.g., pixel-level localization tasks), performance may degrade.

### Mechanism 2
- **Claim:** L1-regularization on the LuCA module parameters promotes orthogonal specialization across tasks, reducing interference.
- **Mechanism:** The L1 penalty induces sparsity in the adapter and calibrator parameters. This encourages each task-specific module to utilize distinct feature dimensions, minimizing overlap and interference during sequential learning.
- **Core assumption:** Sparse, orthogonal parameter configurations in the LuCA modules correlate with reduced feature interference between tasks.
- **Evidence anchors:** [section 4] "The ℓ1 term induces sparsity in the parameters, encouraging orthogonal configurations across different tasks. This orthogonal specialization enables each module to focus on distinct feature dimensions, preventing interference between successive tasks." [abstract] "...strategic design improves the orthogonality between the modules..."
- **Break condition:** If the projection dimension `r` is too small, excessive sparsity may overly constrain the model's capacity, leading to underfitting.

### Mechanism 3
- **Claim:** Task identification during inference can be performed via entropy minimization over task-specific predictions without external task identifiers.
- **Mechanism:** For a given input, the shared frozen feature is passed through all task-specific LuCA modules. Each module produces a probability distribution. The final prediction is chosen from the distribution that yields the lowest Shannon entropy, based on the premise that the correct task module produces more confident (lower-entropy) predictions.
- **Core assumption:** A well-calibrated, task-specific LuCA module produces significantly lower-entropy predictions for inputs from its task compared to other modules.
- **Evidence anchors:** [section 4] "The final prediction is selected through entropy minimization over the union of all class probabilities... This entropy-based selection criterion leverages the observation that the correct task-specific module produces predictions with lower uncertainty..." [abstract] "...enabling effective adaptation with well-refined feature representations."
- **Break condition:** If LuCA modules are not well-calibrated and produce low-entropy predictions for incorrect tasks, this selection method will fail.

## Foundational Learning

- **Concept: Residual Adapters**
  - **Why needed here:** To understand how TOSCA's LuCA module modifies features. It's a core building block that adds a learned residual to the input feature via a bottleneck structure (`A(z) = σ(zWdown)Wup + z`), allowing small, efficient updates.
  - **Quick check question:** Given an input feature `z` of dimension `d=768` and a bottleneck `r=48`, what are the shapes of `Wdown` and `Wup`?

- **Concept: Vision Transformer (ViT) [CLS] Token**
  - **Why needed here:** To grasp why TOSCA's placement strategy is viable. The [CLS] token's role is to aggregate information from all patch tokens across all layers into a single vector for final classification.
  - **Quick check question:** In a standard ViT, what is the purpose of the [CLS] token and which layer's output is typically used for the final classification head?

- **Concept: Catastrophic Forgetting in Class-Incremental Learning (CIL)**
  - **Why needed here:** To understand the problem TOSCA is designed to solve. CIL requires a model to learn a sequence of tasks with non-overlapping classes. Catastrophic forgetting is the loss of performance on earlier tasks after training on new ones.
  - **Quick check question:** In a CIL setting with 3 sequential tasks, what is the primary challenge when the model is trained on Task 2 and Task 3 data only?

## Architecture Onboarding

- **Component map:**
    Frozen Pre-trained ViT (`ϕ(x)`) -> LuCA Module (`L(z)`) -> Classifier (`W`)

- **Critical path:**
    1.  **Training (for task `b`):**
        - Get data `Db`.
        - Initialize a **new** `LuCA_b` module.
        - Forward pass: `x` -> Frozen ViT `ϕ(x)` -> `LuCA_b(ϕ(x))` -> Classifier.
        - Compute Loss: `CrossEntropy` + `λ * L1_norm(LuCA_b parameters)`.
        - Update `LuCA_b` and `W`.
        - Store `LuCA_b`.
    2.  **Inference (for input `x`):**
        - Extract shared feature: `feat = ϕ(x)`.
        - For all stored `b in {1..B}`: get task-specific feature `feat'_b = LuCA_b(feat)`.
        - For all `b`: get probability distribution `p_b = softmax(W.T @ feat'_b)`.
        - Select final prediction `y` from the `p_b` that minimizes Shannon entropy `H`.

- **Design tradeoffs:**
    - **Placement (Final Layer vs. All Layers):** TOSCA places a module only on the final [CLS] token. This is ~8x more parameter-efficient and faster than layer-wise adapters (like EASE) but potentially less expressive, as it cannot modify intermediate feature hierarchies.
    - **Module Design (LuCA vs. Plain Adapter):** The LuCA module combines an adapter with a calibrator. This is more complex than a simple adapter but aims to produce better "refined" features (Figure 4c shows LuCA outperforming adapter-only). The order matters: Adapter then Calibrator (`L(z) = C(A(z))`).
    - **Selection (Entropy vs. Key-Query):** TOSCA uses entropy on the final prediction for task identification, which is simpler and requires no additional key network, unlike L2P/DualPrompt. This depends heavily on the "calibrated" nature of the modules.

- **Failure signatures:**
    - **Conflicting modules:** If L1 regularization (`λ`) is too weak, modules may not be orthogonal, leading to interference and ambiguous entropy scores during inference.
    - **Underfitting new tasks:** If the bottleneck dimension `r` is too small or `λ` is too strong, the model may fail to adapt to new classes (low plasticity).
    - **Incorrect task selection:** If a module for task A produces a lower-entropy prediction for an input from task B, TOSCA will misclassify it. This suggests poor module calibration or a failure of the orthogonality assumption.

- **First 3 experiments:**
    1.  **Baseline Reproduction:** Reproduce the CIFAR-100 B0 Inc5 experiment from Table 1 to validate the full training and inference pipeline. Compare accuracy against a simple `SimpleCIL` baseline.
    2.  **Ablation of `λ`:** Run the same experiment while varying the L1 regularization strength (`λ` in `{0, 1e-4, 5e-4, 1e-3}`). Plot accuracy to find the optimal sparsity point, confirming the "orthogonal specialization" hypothesis.
    3.  **Module Ablation:** Compare the full LuCA module (`A -> C`) against a reversed version (`C -> A`, termed `LuCA_r`) and an adapter-only version on a smaller dataset (e.g., a 5-task split of CIFAR-100) to confirm the contribution of each component and their ordering as per Figure 4c.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the TOSCA framework be effectively adapted for few-shot class-incremental learning scenarios where data scarcity challenges the current training protocol?
- **Basis in paper:** [explicit] The authors explicitly state in the "Limitations and future works" section: "In future work, we aim to explore further application scenarios, such as few-shot class-incremental learning to further enhance its versatility and impact."
- **Why unresolved:** The current study evaluates standard CIL benchmarks; few-shot settings introduce data scarcity that may cause overfitting in the LuCA module, requiring modifications to the optimization strategy.
- **What evidence would resolve it:** Results from experiments on standard few-shot CIL benchmarks (e.g., miniImageNet or CIFAR-100 few-shot splits) demonstrating the method's stability with limited training samples per class.

### Open Question 2
- **Question:** How does TOSCA's performance degrade when applied to weaker pre-trained models or scenarios with significant domain shifts beyond the pre-training data?
- **Basis in paper:** [explicit] The authors note: "Possible limitations include the utilization of pre-trained models since it highly relies on the generalization strength of the pre-trained models by adapting thorough a single token only."
- **Why unresolved:** The experiments rely on strong ViT-B/16 backbones pre-trained on ImageNet (1K/21K), leaving the method's effectiveness on less robust or smaller PTMs unverified.
- **What evidence would resolve it:** A comparative analysis of accuracy drops when using smaller backbones (e.g., ViT-S) or datasets with extreme domain shifts (e.g., medical imaging) relative to the pre-training data.

### Open Question 3
- **Question:** Does constraining adaptation exclusively to the final [CLS] token limit the model's capacity to learn fine-grained spatial features compared to layer-wise adapter methods?
- **Basis in paper:** [inferred] The paper contrasts TOSCA with adapter-based methods that modify intermediate layers (EASE, MOS). While TOSCA preserves feature hierarchy, it freezes low/mid-level features, potentially limiting plasticity for tasks requiring spatial re-representation.
- **Why unresolved:** The paper demonstrates high accuracy on standard recognition tasks but does not analyze specific failure modes where intermediate feature modulation might be superior to final-token calibration.
- **What evidence would resolve it:** An ablation study on tasks requiring fine-grained spatial reasoning or texture discrimination, comparing the performance of a final-token adapter against adapters inserted at intermediate transformer blocks.

## Limitations
- Generalization to architectures without [CLS] tokens or with different attention mechanisms remains unverified
- Entropy-based task identification reliability depends on well-calibrated modules, with potential failure modes not thoroughly validated
- Performance with weaker pre-trained models or significant domain shifts beyond pre-training data is unknown

## Confidence
- **High Confidence:** The core claim that a single LuCA module on the [CLS] token can achieve good CIL performance is supported by the reported experimental results.
- **Medium Confidence:** The claim that L1 regularization promotes orthogonal specialization is plausible but not definitively proven through direct analysis of parameter sparsity and interference.
- **Medium Confidence:** The entropy-based task identification is a novel approach, but its reliability is a concern without thorough validation of failure modes or comparison to established methods.

## Next Checks
1. **Ablation of the Calibrator Component:** Conduct an experiment comparing the full LuCA module (Adapter -> Calibrator) against an adapter-only version and a reversed version (Calibrator -> Adapter) on a smaller dataset to isolate the calibrator's contribution and confirm ordering importance.

2. **Analysis of Entropy-Based Task Selection:** For a multi-task experiment, analyze the entropy distributions of predictions from each LuCA module on data from all tasks to provide direct evidence for the reliability of the entropy-based selection mechanism and reveal cases of ambiguity or failure.

3. **Robustness to Task Similarity:** Design an experiment where tasks have high semantic overlap (e.g., CIFAR-100 classes split into "natural" vs. "man-made" super-classes) to evaluate TOSCA's performance and test if L1 regularization maintains orthogonal specialization in this challenging scenario.