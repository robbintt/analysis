---
ver: rpa2
title: 'Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental Health
  Conversations'
arxiv_id: '2505.20201'
source_url: https://arxiv.org/abs/2505.20201
tags:
- patient
- sensemaker
- health
- prompt
- stage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the multi-turn mental health conversation capabilities
  of large language models (LLMs), addressing the gap between their single-turn clinical
  performance and the complexities of real-world, iterative mental health sensemaking.
  To address this, the authors introduce MedAgent, a novel framework for synthetically
  generating realistic, multi-turn mental health conversations grounded in clinical
  literature, producing the Mental Health Sensemaking Dialogue (MHSD) dataset with
  over 2,200 patient-LLM dialogues.
---

# Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental Health Conversations

## Quick Facts
- arXiv ID: 2505.20201
- Source URL: https://arxiv.org/abs/2505.20201
- Reference count: 40
- Key outcome: Frontier reasoning models achieve below-par performance in multi-turn mental health conversations, with average patient-centric scores of 2.55-2.77 on 4-point scale and diagnostic accuracy around 31% exact match

## Executive Summary
This study examines the capabilities of large language models (LLMs) in handling multi-turn mental health conversations, revealing significant gaps between their single-turn clinical performance and the demands of real-world mental health sensemaking. The authors introduce MedAgent, a framework for synthetically generating realistic mental health dialogues, and MultiSenseEval, a comprehensive evaluation framework assessing six critical dimensions of patient-centric communication. Their findings demonstrate that even state-of-the-art reasoning models struggle with the complexities of iterative mental health conversations, showing degraded performance across multiple turns and varying effectiveness based on patient personality types.

## Method Summary
The researchers developed a novel synthetic data generation framework called MedAgent that creates realistic multi-turn mental health conversations by combining clinical literature, patient scenarios, and LLM-based dialogue simulation. This process produced the Mental Health Sensemaking Dialogue (MHSD) dataset containing over 2,200 patient-LLM conversations across diverse mental health conditions and patient personas. They designed MultiSenseEval, a human-centric evaluation framework with six axes: perceived susceptibility, perceived severity, perceived benefits, diagnostic accuracy, conversation flow and correctness, and readability. The framework employs human raters to assess LLM responses across these dimensions, enabling comprehensive evaluation of conversational quality beyond simple task completion metrics.

## Key Results
- Frontier reasoning models (OpenAI o1, DeepSeek-R1) achieved average patient-centric scores of 2.55-2.77 on a 4-point scale, indicating below-par performance
- Diagnostic accuracy for advanced models reached only ~31% exact match for mental health conditions
- Model performance varied significantly by patient persona, with agreeable personalities receiving better responses
- Performance consistently degraded with increased conversation turns, highlighting limitations in multi-turn reasoning

## Why This Works (Mechanism)
The mechanism underlying LLM performance degradation in multi-turn mental health conversations appears to stem from the complex interplay between context window limitations, reasoning capabilities, and the nuanced nature of mental health dialogue. While reasoning models excel at step-by-step problem-solving in single-turn tasks, the iterative sensemaking process in mental health requires continuous contextual integration, emotional attunement, and adaptive communication strategies that current architectures struggle to maintain across extended conversations.

## Foundational Learning
The findings suggest that existing LLM architectures, even those with advanced reasoning capabilities, lack the fundamental learning mechanisms needed for effective mental health sensemaking. These models appear to miss critical foundational elements of therapeutic communication, including sustained empathetic engagement, nuanced interpretation of emotional cues, and the ability to maintain coherent therapeutic narratives across multiple conversational turns. The synthetic data generation approach itself represents a foundational learning about the need for realistic training data that captures the complexity of mental health dialogues.

## Architecture Onboarding
The study implies that current LLM architectures require significant modifications to effectively handle multi-turn mental health conversations. This includes enhanced context retention mechanisms, improved emotional intelligence modules, and architectures that can better balance diagnostic reasoning with therapeutic communication. The performance degradation across turns suggests that existing attention mechanisms and context window designs are insufficient for maintaining the quality of therapeutic dialogue over extended interactions.

## Open Questions the Paper Calls Out
- How can LLM architectures be redesigned to better support the iterative sensemaking process inherent in mental health conversations?
- What specific architectural modifications would enable sustained empathetic engagement across multiple conversation turns?
- How can synthetic data generation be improved to better capture the nuances of real-world mental health dialogues?
- What evaluation metrics beyond human rating can capture the safety and therapeutic effectiveness of LLM responses in mental health contexts?

## Limitations
- The synthetic nature of the MHSD dataset raises questions about ecological validity and whether findings generalize to real-world mental health conversations
- Human evaluation relies on raters who may have varying clinical expertise, potentially introducing subjective bias across the six evaluation axes
- The study focuses on conversational quality and diagnostic accuracy without addressing critical safety concerns like model hallucination or inappropriate advice
- Limited exploration of how different cultural contexts might affect the evaluation framework and persona construction

## Confidence
- Multi-turn performance gap claims: High confidence - systematic evaluation across multiple turns and personas provides robust evidence
- Diagnostic accuracy findings: Medium confidence - exact match scores reported but limited diagnostic scope may not represent clinical reality
- Persona-based performance differences: Medium confidence - observed variation consistent but synthetic persona construction limits generalizability
- Framework validity claims: Medium confidence - comprehensive design but synthetic data and human rating limitations affect generalizability

## Next Checks
1. Conduct pilot testing on real-world mental health conversation datasets to assess ecological validity of synthetic data findings
2. Implement safety and harm detection metrics to measure model propensity for inappropriate advice or potentially harmful responses
3. Perform cross-cultural validation by adapting framework and personas to different cultural contexts and languages
4. Develop automated evaluation metrics that can complement human ratings and provide more scalable assessment methods
5. Investigate specific architectural modifications that could improve multi-turn reasoning capabilities in mental health contexts