---
ver: rpa2
title: 'ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?'
arxiv_id: '2509.19070'
source_url: https://arxiv.org/abs/2509.19070
tags:
- color
- vlms
- visual
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ColorBlindnessEval, a benchmark for evaluating
  Vision-Language Models (VLMs) on visually adversarial tasks inspired by the Ishihara
  color blindness test. The dataset contains 500 Ishihara-like images with numbers
  0-99 embedded in complex color patterns, designed to challenge VLM robustness in
  recognizing numerical information under adversarial visual conditions.
---

# ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?

## Quick Facts
- arXiv ID: 2509.19070
- Source URL: https://arxiv.org/abs/2509.19070
- Authors: Zijian Ling; Han Zhang; Yazhuo Zhou; Jiahao Cui
- Reference count: 16
- Primary result: VLMs drop ~40% accuracy on adversarial images while humans drop only ~8%

## Executive Summary
This paper introduces ColorBlindnessEval, a benchmark for evaluating Vision-Language Models (VLMs) on visually adversarial tasks inspired by the Ishihara color blindness test. The dataset contains 500 Ishihara-like images with numbers 0-99 embedded in complex color patterns, designed to challenge VLM robustness in recognizing numerical information under adversarial visual conditions. The benchmark evaluates nine VLMs (including GPT-4o, Claude, and Qwen models) using Yes/No and open-ended prompts, comparing their performance against human participants. Results show that while VLMs perform well on clear images, their accuracy drops significantly when processing adversarial backgrounds, highlighting prevalent hallucination issues.

## Method Summary
The benchmark uses a 3-stage generation pipeline to create 500 Ishihara-like images across 5 color sets, each containing 100 image pairs (standard adversarial + foreground-only clear version). The evaluation tests 9 VLMs using both Yes/No prompts ("Is the number X? Answer 'yes' or 'no'") and open-ended prompts ("What number do you see in the image? Output the number you see only"). Accuracy is computed using a delta function comparing model outputs to ground truth across 4 conditions: Y*/N (correct number), Y/N* (incorrect number), Open (adversarial), and Open-clear (foreground-only). Human performance serves as a baseline for comparison.

## Key Results
- VLMs show ~40% accuracy drop on adversarial images versus only ~8% for humans
- GPT-4o-mini achieves highest overall accuracy despite being smallest model tested
- No correlation found between model scale and adversarial robustness performance
- VLMs perform better on color sets with high foreground-background contrast

## Why This Works (Mechanism)
The benchmark exploits a fundamental weakness in VLMs: their inability to reliably separate foreground numerical information from complex adversarial backgrounds. By embedding numbers in color patterns that work for human color blindness tests, the evaluation reveals that VLMs struggle with tasks requiring robust visual parsing under challenging conditions. This approach provides a standardized way to measure VLM reliability for applications requiring accurate visual interpretation.

## Foundational Learning

**Ishihara color blindness test** - Standard medical test using colored dot patterns to identify numbers
*Why needed:* Provides the conceptual foundation for adversarial visual testing
*Quick check:* Can you identify the embedded number in a sample Ishihara plate?

**Monte Carlo circle packing** - Stochastic algorithm for placing non-overlapping circles in a bounded space
*Why needed:* Core technique for generating realistic dot pattern backgrounds
*Quick check:* Can you explain why Monte Carlo methods work better than deterministic packing for this application?

**Vision-Language Model architecture** - Systems combining visual encoders with language models through cross-attention
*Why needed:* Understanding how VLMs process and fuse visual and textual information
*Quick check:* Can you describe the difference between frozen visual encoders and jointly trained VLM architectures?

## Architecture Onboarding

**Component map:** Ishihara generation pipeline → Image dataset → VLM API calls → Accuracy computation → Human baseline comparison

**Critical path:** Generation of adversarial images → VLM inference → Response parsing → Accuracy calculation → Statistical comparison

**Design tradeoffs:** Synthetic adversarial images provide controlled testing but may not capture all real-world complexities; numerical recognition is well-defined but may not generalize to broader visual reasoning

**Failure signatures:** VLMs defaulting to "No" responses regardless of input, large accuracy gaps between clear and adversarial conditions, performance variation across different color sets

**First experiments:**
1. Test VLMs on clear foreground-only images to establish baseline performance
2. Evaluate on adversarial images with known high-contrast color sets
3. Compare Y/N versus open-ended prompt performance on the same images

## Open Questions the Paper Calls Out

**Open Question 1:** Can specific fine-tuning strategies or data augmentation techniques effectively eliminate font-specific visual biases to achieve font-agnostic robustness in Vision-Language Models?
*Basis:* Section B.3 notes significant performance discrepancies between Arial and DejaVuSans fonts
*Resolution needed:* Study showing consistent VLM performance across multiple distinct fonts following targeted training

**Open Question 2:** Why does standard few-shot learning degrade performance on color-adversarial tasks, and can alternative sample selection strategies reverse this effect?
*Basis:* Section B.2 reports few-shot learning had negative impact
*Resolution needed:* Ablation study identifying few-shot configurations that improve accuracy

**Open Question 3:** Which specific architectural components or fine-tuning methodologies, distinct from model scale, are responsible for robustness against color-adversarial hallucinations?
*Basis:* Section 4.3 notes no correlation between scale and performance
*Resolution needed:* Controlled comparison of models with fixed architectures but varying training sets

## Limitations
- Proprietary API details prevent full transparency into inference parameters
- Benchmark focuses on numerical recognition which may not generalize to broader visual reasoning
- Synthetic adversarial images may not capture all real-world visual complexity

## Confidence

**Major claim clusters:**
- VLM performance degradation on adversarial images: **High confidence**
- No correlation between model scale and performance: **Medium confidence**
- Human performance advantage: **High confidence**

## Next Checks

1. **Replicate the adversarial generation pipeline** - Validate circle packing algorithm parameters and color assignment thresholds
2. **Cross-model parameter sensitivity analysis** - Test VLMs with varied temperature settings and retry logic
3. **Real-world adversarial image validation** - Test benchmark on naturally occurring adversarial examples from real applications