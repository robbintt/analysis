---
ver: rpa2
title: Consistent Zero-Shot Imitation with Contrastive Goal Inference
arxiv_id: '2510.17059'
source_url: https://arxiv.org/abs/2510.17059
tags:
- learning
- goal
- policy
- imitation
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CIRL, a method for zero-shot imitation learning
  that enables agents to infer and reproduce human demonstrations without access to
  expert data during training. The key idea is to reframe imitation learning as a
  goal inference problem, leveraging contrastive reinforcement learning to train agents
  that can explore autonomously and practice reaching self-proposed goals.
---

# Consistent Zero-Shot Imitation with Contrastive Goal Inference

## Quick Facts
- arXiv ID: 2510.17059
- Source URL: https://arxiv.org/abs/2510.17059
- Authors: Kathryn Wantlin; Chongyi Zheng; Benjamin Eysenbach
- Reference count: 40
- Primary result: Introduces CIRL, achieving 100% imitation scores in Reacher and Pusher by inferring goals from single trajectories without expert data during training

## Executive Summary
This paper presents CIRL, a zero-shot imitation learning method that reframes imitation as a goal inference problem. Rather than learning reward functions or directly cloning expert behaviors, CIRL trains agents to explore autonomously and practice reaching self-proposed goals. During evaluation, it infers the demonstrator's goal from a single trajectory and uses a learned goal-conditioned policy to reproduce the behavior. The method proves theoretically consistent - unlike prior approaches such as FB representations - and achieves significantly higher imitation scores on standard RL benchmarks, with up to 100% success in simple environments.

## Method Summary
CIRL consists of three main components: (1) MaxEnt contrastive RL pretraining with soft Q-function optimization, (2) Mean field variational goal inference model q_ξ(g|τ) = ∏q_ξ(g|s_t,a_t), and (3) GoalKDE automatic goal sampling that selects low-probability states under KDE fit to replay buffer. The agent is first trained via contrastive RL to reach self-proposed goals, then learns to infer goals from trajectories using the trained policy, and finally infers demonstrator goals at test time to reproduce behaviors. The mean field approximation enables scalable inference by sharing parameters across timesteps while preserving predictive power.

## Key Results
- Achieves 100% imitation scores in Reacher and Pusher environments
- Outperforms FB representations and other zero-shot imitation baselines
- Extends to non-goal-conditioned tasks by expanding the goal space
- Theoretical proof of consistency unlike FB representations which can incorrectly infer goals

## Why This Works (Mechanism)

### Mechanism 1: Goal Inference as Inverse RL Reduction
CIRL reduces the MaxEnt IRL problem to goal inference, making imitation tractable without reward function learning. The method learns a variational posterior q_ξ(g|τ) that approximates the true goal posterior, then infers the demonstrator's goal from a single trajectory and commands a pre-trained goal-conditioned policy. This bypasses the need to learn general reward functions by assuming expert behaviors can be described as goal-reaching. The approach correctly accounts for goal difficulty through implicit partition function estimation via contrastive sampling.

### Mechanism 2: Mean Field Approximation for Scalable Inference
The mean field approximation q_ξ(g|τ) = ∏_t q_ξ(g|s_t, a_t) preserves predictive power while being computationally tractable. Rather than encoding the full trajectory, each state-action pair independently contributes to the goal distribution, reducing overfitting and enabling inference from partial trajectories. This parameter sharing across timesteps makes training more efficient while maintaining accuracy.

### Mechanism 3: Contrastive RL with Partition Function Implicit Estimation
CIRL correctly accounts for goal difficulty (partition function) during inference, unlike FB representations which conflate visitation frequency with intended goals. By sampling goals g ~ p(g) and trajectories τ ~ p*(τ|g) from the trained policy, the partition function Z_g is implicitly captured in the samples. Forward amortized variational inference then trains the goal inference model without explicit Z_g computation, enabling consistent goal inference where methods ignoring the partition function fail.

## Foundational Learning

- **Maximum Entropy IRL**: Why needed - CIRL builds directly on MaxEnt IRL's trajectory likelihood formulation p*(τ|g) ∝ exp(∑_t r_g(s_t, a_t))/Z_g. Quick check - Can you explain why the partition function Z_g matters for distinguishing between "hard-to-reach goals" vs. "unintended goals"?

- **Contrastive Learning for RL**: Why needed - The critic f_ϕ,ψ(s, a, g) = log[p^π_γ(s_f|s,a,g) / p^β_γ(s_f)] estimates discounted state occupancy via Noise Contrastive Estimation. Quick check - What does the contrastive objective learn to distinguish, and why is the goal treated as the "positive" sample?

- **Variational Inference (Forward KL)**: Why needed - CIRL uses forward amortized VI (minimizing KL[p*||q]) rather than reverse KL. Quick check - Why does forward KL allow us to avoid computing the conditional likelihood p*(τ|g)?

## Architecture Onboarding

- **Component map**: GoalKDE sampler -> Actor (goal-conditioned policy) -> Replay buffer -> Critic networks (f_ϕ,ψ) and Actor update -> Goal inference network q_ξ

- **Critical path**: 
  1. Pre-training: GoalKDE selects goal → Actor generates trajectory → Replay buffer stores transitions → Critic and Actor update via contrastive + entropy losses
  2. Goal inference training: Sample (g, τ) from trained policy → Train q_ξ to maximize log q_ξ(g|τ) via mean field
  3. Evaluation: Receive expert trajectory → q_ξ infers goal → Actor executes goal-conditioned policy

- **Design tradeoffs**: 
  - Mean field vs. full trajectory input: Mean field is faster and empirically better but may miss temporal dependencies
  - GoalKDE vs. oracle goal sampling: GoalKDE enables self-supervised exploration but may sample infeasible goals in high-dimensional spaces
  - Forward KL vs. reverse KL: Forward KL requires sampling ability but avoids likelihood evaluation; reverse KL is more common but requires tractable likelihood

- **Failure signatures**:
  - Low imitation scores on complex environments (>20% gap from oracle): Likely due to GoalKDE exploring different goal distribution than test-time
  - Goal inference collapse (q_ξ predicts same goal regardless of τ): May indicate insufficient exploration or policy not converged to MaxEnt optimality
  - FB baseline outperforming CIRL: Check if goals are truly reached and stayed at (transient goals violate assumptions)

- **First 3 experiments**:
  1. Reproduce Reacher results: Train CIRL from scratch with GoalKDE, verify imitation score approaches 100%
  2. Ablate goal inference architecture: Compare mean field q_ξ(g|s_t,a_t) vs. full-trajectory encoder q_ξ(g|τ) on Pusher
  3. Test consistency claim: Construct 2-state MDP from Lemma 2, train FB and CIRL, verify FB incorrectly infers π₂ as π₁

## Open Questions the Paper Calls Out

- Can the CIRL framework be effectively extended to language or multi-modal goal representations?
- Do more sophisticated goal-sampling strategies improve CIRL's performance in high-dimensional spaces compared to the GoalKDE baseline?
- Can Forward-Backward (FB) representations match CIRL if equipped with comparable self-supervised exploration mechanisms?

## Limitations

- Requires tasks to be expressible as reaching specific state configurations, not suitable for process-oriented tasks
- Performance gap to oracle remains 10-20% in complex environments, indicating exploration limitations
- Unclear scalability to high-dimensional observation spaces like image inputs

## Confidence

- **High**: Theoretical consistency proof, mean field approximation superiority, CRL pretraining methodology
- **Medium**: Practical performance gap to oracle, generalizability to non-goal tasks
- **Low**: Scaling to high-dimensional observation spaces, robustness to suboptimal expert demonstrations

## Next Checks

1. Stress test GoalKDE exploration: Replace GoalKDE with oracle goal sampling in Pusher and measure imitation score difference to isolate exploration bottleneck.

2. Validate consistency claim empirically: Construct the 2-state MDP from Lemma 2, train FB and CIRL, verify FB incorrectly infers π₂ demonstrations as π₁ while CIRL infers correctly.

3. Test temporal dependency: Modify mean field to use a simple LSTM that processes the full trajectory, compare imitation scores to determine if temporal information is being lost.