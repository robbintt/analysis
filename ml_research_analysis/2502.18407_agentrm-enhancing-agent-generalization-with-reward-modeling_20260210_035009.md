---
ver: rpa2
title: 'AgentRM: Enhancing Agent Generalization with Reward Modeling'
arxiv_id: '2502.18407'
source_url: https://arxiv.org/abs/2502.18407
tags:
- reward
- tasks
- agent
- arxiv
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AgentRM, a generalizable reward model that
  enhances LLM-based agents' performance on unseen tasks through test-time search.
  The key idea is to fine-tune a reward model instead of the policy model, as reward
  modeling is more robust to task distribution shifts.
---

# AgentRM: Enhancing Agent Generalization with Reward Modeling

## Quick Facts
- arXiv ID: 2502.18407
- Source URL: https://arxiv.org/abs/2502.18407
- Reference count: 28
- Primary result: AgentRM improves base policy model by 8.8 points on average across nine agent tasks through test-time search

## Executive Summary
AgentRM addresses the challenge of LLM agent generalization to unseen tasks by fine-tuning a reward model rather than the policy model. Unlike traditional approaches that directly fine-tune policy models on held-in tasks, which suffer from overfitting and poor cross-task generalization, AgentRM learns to evaluate trajectories and guide the policy through test-time search. The method achieves weak-to-strong generalization, where a reward model trained on a weaker policy model (LLaMA-3-8B) improves stronger models (LLaMA-3-70B) by 12.6 points on average. Across nine diverse agent tasks including web navigation, embodied planning, and tool usage, AgentRM outperforms the top general agent by 4.0 points and specialized agents by 11.4 points on held-in tasks.

## Method Summary
AgentRM fine-tunes a reward model using process-level supervision from either tree search exploration (explicit RM), implicit outcome rewards (implicit RM), or LLM-based evaluation (LLM-as-a-judge). The policy model generates multiple candidate trajectories through sampling or beam search, which the reward model scores to select the best one. The approach uses held-in tasks (Webshop, Alfworld, Sciworld) for training and held-out tasks (Babyai, Jericho, Pddl, Maze, ToolQuery, ToolOperation) for evaluation. The explicit RM variant employs MCTS-inspired tree search to generate state-value pairs, which are filtered and used to train a regression model with MSE loss. During inference, the trained reward model guides the policy through Best-of-N sampling or step-level beam search to improve task performance.

## Key Results
- AgentRM improves base policy model by 8.8 points on average across nine agent tasks
- Explicit reward modeling consistently outperforms implicit RM and LLM-as-a-judge approaches
- Weak-to-strong generalization yields 12.6 point improvement on larger LLaMA-3-70B model
- Outperforms top general agent by 4.0 points and specialized agents by 11.4 points on held-in tasks

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a reward model to guide the policy model is more robust than directly fine-tuning the policy model. Policy model fine-tuning increases likelihood of seen action tokens while decreasing that of unseen actions, causing overfitting to held-in tasks. Reward model regression training with MSE loss is inherently less sensitive to the specific distribution of action tokens, enabling better generalization to unseen task distributions. This relationship is assumed to be causal, not merely correlational.

### Mechanism 2
Explicit reward modeling via tree search-based value estimation provides more effective guidance than implicit reward modeling or LLM-as-a-judge approaches. MCTS-inspired tree construction generates state-value pairs through exploration-exploitation balance. The UCB selection strategy identifies promising nodes, while simulation estimates Q-values via rollouts. Backpropagation propagates expected future rewards up the tree, creating dense process-level supervision from sparse outcome rewards. Training on these (state, value) pairs teaches the reward model to predict expected cumulative rewards at intermediate steps.

### Mechanism 3
Reward models trained on states sampled by weaker models can enhance stronger policy models through weak-to-strong generalization. The reward model learns task-agnostic value estimation from exploration trajectories. When applied to a stronger policy model, it provides process-level guidance that steers the more capable model away from suboptimal actions. The stronger model's better action generation combined with the reward model's learned value assessment creates synergistic improvement.

## Foundational Learning

- **Concept: Monte Carlo Tree Search (MCTS) and UCB Selection**
  - Why needed here: The explicit reward modeling approach uses MCTS-inspired algorithms to generate process reward annotations. Understanding the balance between exploration (trying new actions) and exploitation (selecting high-value nodes) is essential for debugging reward quality.
  - Quick check question: Given a node with value V(s)=0.6, visit count N(s)=10, parent visit count N(parent)=100, and exploration constant c=0.5, calculate its UCB score.

- **Concept: Process Reward Models vs. Outcome Reward Models**
  - Why needed here: The paper specifically trains process reward models that assign intermediate step rewards, contrasting with outcome-only supervision. This distinction affects data collection, training objectives, and inference-time computation.
  - Quick check question: Why might a process reward model generalize better than an outcome reward model when both are trained on the same trajectories?

- **Concept: Test-Time Scaling via Search Methods**
  - Why needed here: AgentRM enables performance improvement through Best-of-N sampling and beam search at inference time. Understanding the compute-performance tradeoff is critical for practical deployment.
  - Quick check question: In Best-of-N sampling, what is the minimum N required to achieve 90% of oracle performance if the reward model has 80% preference accuracy? (simplified estimation)

## Architecture Onboarding

- **Component map:** Expert trajectories → SFT agent training → Tree search exploration → State-value extraction → Reward model training → Test-time search guidance

- **Critical path:**
  1. Expert trajectory collection → SFT agent training (1/4 data, 3 epochs, lr=2e-5)
  2. SFT agent exploration → Tree construction → State-value extraction (filter by visit count λ≥3)
  3. Reward model training on (state, value) pairs (2 epochs, lr=1e-5 for Explicit, 5e-7 for Implicit)
  4. Inference: Policy generates candidates → Reward model scores → Selection

- **Design tradeoffs:**
  - **Explicit vs. Implicit RM:** Explicit requires tree search overhead but achieves 6.8-point higher performance; Implicit is simpler but may confuse with many candidates (Figure 5)
  - **State representation:** Full (thought+action+observation) achieves 61.5; action-only achieves 61.2 with faster training (Table 5)
  - **Search method:** Best-of-5 is simpler; Beam Search (W1=5, W2=5) achieves 1.8-point improvement but requires more inference compute
  - **Data split:** 1/4 SFT / 3/4 RM training; alternative splits not explored

- **Failure signatures:**
  - **Overfitting to held-in tasks:** Policy model improvements show diagonal-only pattern in Figure 1(b); reward model shows positive transfer—verify you're training the correct component
  - **Poor generalization to different policy models:** If RM trained on LLaMA-3-8B fails to improve other models, check state representation compatibility
  - **Reward model collapse:** If preference accuracy drops below 50%, training may have converged to trivial solutions—increase data diversity or reduce learning rate
  - **LLM-as-judge degradation:** With N>16, token truncation causes performance drops (Section 5.6)—use Explicit RM for larger N

- **First 3 experiments:**
  1. **Validate SFT agent baseline:** Train SFT agent on held-in tasks (Webshop, Alfworld, Sciworld), evaluate on all 9 tasks with greedy decoding. Expected: baseline performance degradation on held-out tasks vs. non-finetuned model (Section 4.3.1 baseline: 52.7 overall)
  2. **Ablate reward modeling approaches:** Train all three RM variants (Explicit, Implicit, LLM-as-judge) using the same state samples, evaluate with Best-of-5. Expected ranking: Explicit > Implicit > LLM-as-a-judge, with ~8.8-point improvement for Explicit
  3. **Test weak-to-strong transfer:** Apply the trained Explicit RM (from LLaMA-3-8B exploration) to a different policy model (e.g., LLaMA-3-70B or another open-source model). Expected: greater improvement on stronger models (12.6-point gain on 70B vs. 8.8 on 8B average)

## Open Questions the Paper Calls Out

### Open Question 1
Can robust test-time scaling laws be established for Implicit Reward Modeling and LLM-as-a-judge in agent tasks, given their current performance instability with increased candidate counts? The authors state in Section 5.6 that "additional research is necessary to establish robust test-time scaling laws with Implicit RM and LLM-as-a-judge," which they leave for future work. Implicit RM shows performance degradation with more candidates (confusion), and LLM-as-a-judge suffers from context length truncation issues.

### Open Question 2
To what extent does increasing MCTS iterations and simulations beyond the current resource-constrained settings improve the precision of process reward estimation? The Limitations section notes that due to resource constraints, the maximum iteration (40) and number of simulations (1) were set low, and increasing them "could lead to more precise process reward estimations."

### Open Question 3
Does combining AgentRM with advanced prompt engineering strategies (e.g., Reflexion) yield superior generalization compared to AgentRM alone? The Limitations section explicitly states the authors "do not explore the potential of equipping our policy model with prompt engineering designed for agent such as Reflexion."

## Limitations
- Computationally expensive tree search (40 iterations) creates significant inference-time overhead
- State representation increases model complexity and may not generalize to all environments
- Three-task training set represents a narrow slice of agent capabilities
- No analysis of reward model calibration or robustness to adversarial states

## Confidence

**High Confidence:** The core mechanism that reward model fine-tuning generalizes better than policy fine-tuning is well-supported by controlled experiments showing positive transfer across held-out tasks (Section 4.3.1). The weak-to-strong generalization finding (Section 5.4) is particularly robust with clear quantitative improvements (12.6 points on LLaMA-3-70B).

**Medium Confidence:** The superiority of explicit reward modeling over implicit and LLM-as-a-judge approaches is demonstrated but could benefit from more ablation studies. The claimed robustness to token distribution shifts (Mechanism 1) is inferred from experimental results rather than directly measured.

**Low Confidence:** The weak-to-strong generalization mechanism assumes transferable state-value mappings but doesn't validate whether the learned rewards capture task structure versus model-specific exploration patterns. The scaling analysis (Figure 3) shows log-linear improvements but extrapolates beyond the tested data range.

## Next Checks

1. **Cross-model state compatibility:** Apply the reward model trained on LLaMA-3-8B to a completely different architecture (e.g., GPT-Neo) to test whether the state-value mapping transfers beyond model families.

2. **Reward model robustness:** Systematically generate adversarial states (invalid actions, out-of-distribution observations) and measure reward model predictions to assess calibration and reliability under stress.

3. **Computational efficiency analysis:** Compare wall-clock time for explicit vs. implicit reward modeling across varying trajectory lengths to quantify the practical tradeoff between performance gain and inference cost.