---
ver: rpa2
title: 'Millions of States: Designing a Scalable MoE Architecture with RWKV-7 Meta-learner'
arxiv_id: '2504.08247'
source_url: https://arxiv.org/abs/2504.08247
tags:
- state
- meta-state
- rwkv-7
- interactions
- state-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Meta-State, a state-based extension to the
  RWKV-7 architecture that replaces the Feed-Forward Network with a novel mechanism
  for token-parameter interactions. The Self-State Encoder module repurposes a portion
  of the Weighted Key-Value state as transformation weights, enabling efficient state-autoregressive
  evolution without introducing new trainable matrices or softmax operations.
---

# Millions of States: Designing a Scalable MoE Architecture with RWKV-7 Meta-learner

## Quick Facts
- arXiv ID: 2504.08247
- Source URL: https://arxiv.org/abs/2504.08247
- Authors: Liu Xiao; Li Zhiyuan; Lin Yueyu
- Reference count: 6
- Primary result: State-based extension to RWKV-7 replacing FFN with Meta-State layer, achieving 7.2%-13.1% relative reduction in cross-entropy loss across 150M-1.5B parameters

## Executive Summary
This paper introduces Meta-State, a state-based extension to the RWKV-7 architecture that replaces the Feed-Forward Network with a novel mechanism for token-parameter interactions. The Self-State Encoder module repurposes a portion of the Weighted Key-Value state as transformation weights, enabling efficient state-autoregressive evolution without introducing new trainable matrices or softmax operations. The approach supports progressive model scaling by expanding the WKV state while reusing existing parameters, eliminating the need for retraining.

Experiments on the Pile dataset with models ranging from 150M to 1.5B parameters demonstrate consistent improvements over Transformer baselines, achieving 7.2% to 13.1% relative reductions in cross-entropy loss across all model sizes, with performance gains increasing as model size grows. The Meta-State framework offers a scalable, efficient alternative for sequence modeling with linear complexity and constant memory usage.

## Method Summary
The Meta-State layer replaces the standard FFN in RWKV-7 by using the Weighted Key-Value (WKV) state as dynamic transformation weights through the Self-State Encoder (SSE). The SSE extracts a portion of wkv_t and uses it directly as W_sse to transform the current input token, computing z_t = ReLU(x'_t · W_sse). The Meta-State evolves autoregressively using decay factors and removal keys, avoiding quadratic attention complexity. The architecture supports progressive scaling by expanding WKV state dimensions and reusing frozen parameters with lightweight adapters.

## Key Results
- 7.2% to 13.1% relative reduction in cross-entropy loss compared to Transformer baselines
- Consistent performance improvements across all model scales (150M to 1.5B parameters)
- Linear complexity and constant memory usage maintained through state-autoregressive design
- Progressive scaling capability demonstrated without requiring retraining of original parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The model achieves token-parameter interactions without introducing new trainable parameters by repurposing the existing WKV state as a dynamic weight matrix.
- **Mechanism:** The Self-State Encoder (SSE) extracts a portion of the RWKV-7 Weighted Key-Value (WKV) state (wkv_t) and uses it directly as the transformation weight (W_sse) for the current input token (x'_t). By computing z_t = ReLU(x'_t W_sse), the transformation weights become context-dependent, adapting the "FFN" calculation based on the historical state rather than static weights.
- **Core assumption:** The WKV state generated by the Time Mixing block contains sufficient information to serve as an effective transformation matrix for the Meta-State layer.
- **Evidence anchors:** [abstract] "...repurposes a portion of the Weighted Key-Value (WKV) state as transformation weights... without introducing new trainable matrices..."; [section 3.2] "Specifically, we select a portion of wkv_t as W_sse... This formulation uses the WKV state directly as a transformation weight, modulating the input tokens x'_t based on the state's current context..."; [corpus] The related "RWKV-7 Goose" paper confirms the expressiveness of dynamic state evolution, providing a plausible basis for why the state might carry sufficient information for this reuse.
- **Break condition:** If the WKV state suffers from rank collapse or information bottleneck during long sequences, W_sse may fail to provide diverse transformations, reducing the Meta-State to a degenerate linear projection.

### Mechanism 2
- **Claim:** The architecture maintains linear complexity and constant memory usage by treating the Meta-State update as a recurrent process similar to the core RWKV mechanism.
- **Mechanism:** The Meta-State (ms_t) evolves using a state-autoregressive rule involving a decay factor (w_t), removal key (κ̂_t), and the outer product of the encoded representation (z_t^T z_t). This avoids the need to cache historical tokens or compute quadratic attention maps relative to sequence length.
- **Core assumption:** The recursive update rule effectively compresses the token-parameter interaction history into a fixed-size state matrix without significant loss of task-critical information.
- **Evidence anchors:** [abstract] "...state-autoregressive evolution... enabling efficient state-autoregressive evolution without... softmax operations."; [section 3.2] "This state-autoregressive evolution ensures that the Meta-State updates depend on its previous state, preserving the autoregressive property..."; [corpus] Related works like "Linear-MoE" and "MoM" support the general feasibility of combining linear sequence modeling with memory/state mechanisms, though specific dynamics for "Meta-States" are less verified in the corpus.
- **Break condition:** If the decay/removal parameters are not learned effectively, the state may either flush information too quickly (instability) or saturate (loss of resolution), leading to performance degradation similar to RNN gradient issues.

### Mechanism 3
- **Claim:** The architecture supports progressive scaling (increasing capacity) without retraining by expanding the state dimensions and reusing existing parameters.
- **Mechanism:** Scaling is achieved by increasing the dimensionality of the WKV state (D'/h') and adding "placeholder" or zero-initialized parameters to the projection matrices (W_oms). The SSE adapts by projecting inputs into the higher-dimensional space using a lightweight adapter (W_in), allowing the model to utilize a larger state while retaining the behavior of the original frozen parameters.
- **Core assumption:** The learned token-parameter interactions in the smaller model generalize directly to a larger state space via simple projection and concatenation.
- **Evidence anchors:** [abstract] "...supports progressive model scaling by expanding the WKV state... eliminating the need for retraining."; [section 3.4] "...append new columns, initialized to zero or small random values... fine-tuning... keeping the original Meta-State parameters... frozen."; [corpus] TokenFormer (cited in text) is the primary evidence for "token-parameter" scaling; this paper extends that logic to the state-domain.
- **Break condition:** If the new dimensions added to the WKV state disrupt the subspace geometry of the original state, the projection W_in may fail to align inputs with the pre-trained "frozen" portions of the state, leading to a degradation of learned capabilities.

## Foundational Learning

- **Concept:** **Weighted Key-Value (WKV) State Mechanics**
  - **Why needed here:** The core innovation relies on treating the WKV state as a weight matrix (W_sse). Understanding how this state accumulates history via decay and gain is essential to debugging why the "dynamic weights" might be failing.
  - **Quick check question:** Can you explain how the WKV state in RWKV-7 differs from the Key-Value cache in a standard Transformer?

- **Concept:** **State-Autoregression vs. Token-Autoregression**
  - **Why needed here:** The paper claims the Meta-State evolves in a "state-autoregressive" manner. You must distinguish between generating the next *token* vs. generating the next *state transition* based on recursive updates.
  - **Quick check question:** Does the calculation of ms_t require access to x_{t+1}, or is it strictly causal based on ms_{t-1} and x_t?

- **Concept:** **Parameter Tokenization & Scaling**
  - **Why needed here:** The scaling mechanism treats model capacity as a function of state size and "parameter tokens." This breaks the standard assumption that model size is fixed by weight matrix shapes.
  - **Quick check question:** In this architecture, if you double the width of the WKV state, do you necessarily double the number of trainable parameters in the SSE?

## Architecture Onboarding

- **Component map:** Input Layer -> Time Mixing Block (RWKV-7) -> Meta-State Layer (SSE + State Evolution + Output Projection) -> Output
- **Critical path:** The dependency chain is strictly linear: `Input -> TimeMix -> WKV State -> SSE (uses WKV as weights) -> Meta-State -> Output`. Any bottleneck in the WKV state propagation directly degrades the SSE's ability to generate dynamic weights.
- **Design tradeoffs:**
  - Flexibility vs. Stability: Reusing the WKV state as weights (W_sse) saves parameters but couples the "attention-like" function (Time Mix) tightly with the "feed-forward-like" function (Meta-State). Instability in one propagates to the other.
  - Speed vs. State Size: The paper claims constant memory/linear time, but expanding the WKV state for scaling increases the computational cost of the outer product (z_t^T z_t) and state evolution, potentially creating a new bottleneck.
- **Failure signatures:**
  - Loss Spikes: If the Meta-State values explode, the ReLU activation in SSE or the outer product may produce massive gradients (or NaNs).
  - Stagnation: If the decay factors (w_t) trend to 1, the model fails to forget irrelevant history, causing the "dynamic weights" (W_sse) to become static averages.
  - Scaling Collapse: If progressive scaling fails, the new zero-initialized parameters in W_oms will remain near zero, meaning the expanded capacity is unused.
- **First 3 experiments:**
  1. Sanity Check (Overfit): Train a tiny Meta-State model (e.g., 1 layer) on a single batch of data to verify the SSE and Meta-State evolution can technically converge and drive loss to near zero.
  2. Ablation (SSE vs. Static FFN): Replace the SSE (where W_sse = wkv_t) with a standard static FFN on a small benchmark (e.g., Wikitext-2) to isolate the performance contribution of the "dynamic state-as-weights" mechanism.
  3. Scaling Verification: Train a 150M model, then attempt the "progressive scaling" to 200M (adding parameters/state dimensions). Verify if the loss decreases without retraining the original parameters (freezing them) to validate the scalability claim.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Meta-State framework be effectively combined with Mixture of Experts (MoE) routing to achieve dynamic capacity allocation while maintaining linear complexity?
- **Basis in paper:** [explicit] The conclusion explicitly states an aim to "investigate the integration of our approach with other scalable architectures, such as Mixture of Experts (MoE)."
- **Why unresolved:** The current paper focuses on scaling by expanding the WKV state dimension and reusing parameters (Progressive scaling), but does not implement or test expert routing mechanisms.
- **What evidence would resolve it:** Experimental results comparing a Meta-State-MoE hybrid against standard MoE Transformers and dense Meta-State models, specifically analyzing training efficiency and downstream task performance.

### Open Question 2
- **Question:** Does the Meta-State model retain its performance advantages over Transformers on generative sequence-to-sequence tasks like machine translation and summarization?
- **Basis in paper:** [explicit] The authors state they "intend to evaluate our model on a wider range of tasks beyond language modeling, such as machine translation and text summarization."
- **Why unresolved:** The current evaluation is restricted to cross-entropy loss on the Pile dataset (language modeling), leaving the model's efficacy on encoder-decoder tasks or distinct generation benchmarks unproven.
- **What evidence would resolve it:** Benchmarks on standard translation (e.g., WMT) or summarization (e.g., CNN/DailyMail) datasets showing metrics such as BLEU or ROUGE scores compared to strong Transformer baselines.

### Open Question 3
- **Question:** What is the specific performance contribution of the Meta-State layer compared to the vanilla RWKV-7 architecture it modifies?
- **Basis in paper:** [inferred] The experimental section compares the proposed model against Transformer baselines but does not report results against the standard RWKV-7 architecture to isolate the improvement gained by replacing the FFN.
- **Why unresolved:** While the model outperforms Transformers, it is unclear if the Meta-State layer acts as a strict upgrade over the base RWKV-7 or if there are trade-offs in convergence speed or state stability.
- **What evidence would resolve it:** An ablation study or direct comparison table showing the cross-entropy loss of RWKV-7 + Meta-State versus RWKV-7 + FFN at identical parameter counts.

### Open Question 4
- **Question:** Does using the dynamic WKV state as transformation weights (W_sse) introduce training instability or gradient issues not present in fixed-parameter models?
- **Basis in paper:** [inferred] The methodology repurposes the WKV state as weights (W_sse = wkvt) without new trainable matrices, a mechanism that relies on non-stationary, autoregressive values for transformations usually handled by static weights.
- **Why unresolved:** The paper reports final loss values but does not analyze the training dynamics, gradient norms, or variance associated with using a rapidly evolving state matrix as the weight matrix for the Self-State Encoder.
- **What evidence would resolve it:** Analysis of training curves, gradient health, and sensitivity to learning rates, specifically checking for exploding or vanishing gradients within the SSE module.

## Limitations
- The untested nature of "state-as-weights" concept creates uncertainty about whether WKV state contains sufficient information for effective weight reuse
- Progressive scaling relies on untested generalization assumptions about new state dimensions aligning with frozen parameters
- Lack of detailed training hyperparameters and RWKV-7 specification creates barriers to faithful reproduction

## Confidence
- **High Confidence Claims:** The architectural framework and mathematical formulation are clearly specified; linear complexity and constant memory claims follow logically; experimental setup is well-defined
- **Medium Confidence Claims:** Performance improvements over Transformer baselines are reported but not independently verified; progressive scaling capability is theoretically sound but relies on untested assumptions; avoiding retraining through parameter reuse is plausible but may encounter practical limitations
- **Low Confidence Claims:** Assertion that WKV state evolution contains sufficient information for effective weight reuse lacks empirical validation; long-term stability of state-autoregressive updates over extended sequences is not demonstrated; interaction between Meta-State and RWKV-7 time mixing dynamics remains poorly characterized

## Next Checks
1. **Dynamic Weight Expressiveness Test**: Conduct an ablation study comparing Meta-State performance against both static FFN and a version where WKV state is randomly initialized rather than learned. This would isolate whether the learned state evolution is genuinely encoding useful transformation patterns or if the mechanism works with any state input.

2. **Scaling Boundary Analysis**: Systematically test progressive scaling beyond the reported 150M→1.5B range by attempting extreme expansions (e.g., 150M→500M→2B) while monitoring both performance and parameter utilization. Track whether new state dimensions remain unused (zero weights in W_oms) or successfully integrate with frozen parameters.

3. **State Evolution Stability**: Implement monitoring for state norm distributions and decay factor statistics across training epochs and sequence lengths. Verify that the state-autoregressive updates maintain bounded values and that decay parameters exhibit meaningful variation rather than saturating to pathological values.