---
ver: rpa2
title: 'RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step
  NLP Task Solving'
arxiv_id: '2505.11893'
source_url: https://arxiv.org/abs/2505.11893
tags:
- task
- each
- rlap
- tasks
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RLAP, a reinforcement learning enhanced adaptive
  planning framework for multi-step NLP task solving. The core idea is to model NLP
  tasks as Markov decision processes (MDPs) and train a lightweight Actor model to
  estimate Q-values for natural language sequences, enabling adaptive planning of
  subtask orders based on linguistic features.
---

# RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving

## Quick Facts
- arXiv ID: 2505.11893
- Source URL: https://arxiv.org/abs/2505.11893
- Reference count: 40
- Primary result: RLAP improves accuracy by up to 2.45% on MRC tasks and F1-score by up to 43.0% on IE tasks through adaptive subtask ordering without LLM fine-tuning

## Executive Summary
RLAP introduces a reinforcement learning enhanced adaptive planning framework that decomposes multi-step NLP tasks into Markov Decision Processes (MDPs). The framework uses a frozen large language model (LLM) as an executor and trains a lightweight Actor model to determine optimal subtask ordering through Q-learning. By decoupling planning from execution, RLAP achieves significant performance improvements on three NLP tasks (MRC, IE, STC) without requiring LLM fine-tuning. The method demonstrates robustness on complex instances and maintains stable performance as task complexity increases.

## Method Summary
RLAP models NLP tasks as MDPs where each task instance is an episode, states contain task definitions and intermediate results, and actions select subtasks. A frozen LLM serves as the environment executor, while a lightweight Actor model (BERT, gte-multilingual-base, or Qwen2.5) learns optimal action selection via Deep Q-Learning with Double Q-learning and experience replay. The Actor encodes state-action pairs into sequences and outputs Q-values through a linear projection. Training uses task-specific reward functions (stepwise for concatenation tasks, episode-level for modification tasks) with ε-greedy exploration. The framework achieves adaptive planning without LLM fine-tuning by learning instance-specific optimal subtask orderings.

## Key Results
- Accuracy improvements up to 2.45% on SQuAD2.0, CMRC18, RACE-H, and C3-mix MRC tasks
- F1-score improvements up to 43.0% on NYT10, HacRED, SKE21, DuEE, and ACE05 IE tasks
- Context-level accuracy improvements up to 9.84% on CMRC19 STC task
- Robust performance on complex instances where baseline methods degrade significantly

## Why This Works (Mechanism)

### Mechanism 1: MDP Formulation with LLM as Environment
RLAP decomposes NLP tasks into MDPs where states contain task definitions and intermediate results, actions select subtasks, and the LLM executes subtasks deterministically. The framework assumes subtasks satisfy Markov property (execution depends only on current state), completeness (all subtasks together complete the task), and homogeneity (subtasks share structure within instances). A lightweight Actor model learns optimal action selection via Q-values without modifying the LLM.

### Mechanism 2: Lightweight Actor Model with Deep Q-Learning
The Actor concatenates state and action into sequences, encodes them with frozen or partially frozen PLMs, and applies linear projection to output Q-values. Training uses ε-greedy exploration with experience replay and Double Q-learning to stabilize training. The small neural network (110M-7B parameters) learns instance-specific optimal ordering that generalizes across linguistic variations.

### Mechanism 3: Reward Signal Design (Stepwise vs. Episode-Level)
RLAP matches reward structure to task type: stepwise rewards for concatenation tasks where final result equals concatenation of intermediate results, and episode-level rewards for modification tasks where final results derive from modifications. This provides effective learning signals by aligning reward timing with task completion patterns.

## Foundational Learning

- **Markov Decision Processes (MDPs)**
  - Why needed here: RLAP formulates each NLP task as an MDP where state transitions are determined by LLM outputs. Understanding states, actions, transition functions, rewards, and discount factors is prerequisite to implementing the framework.
  - Quick check question: Given a text with 5 sentences for MRC, what is the action space after processing 2 sentences? (Answer: 3 remaining sentences)

- **Q-Learning and Deep Q-Networks (DQN)**
  - Why needed here: The Actor model learns Q-values via DQN with Double Q-learning and experience replay. Understanding temporal difference learning, target networks, and Bellman equations is essential for training.
  - Quick check question: Why does DDQN use a target network with delayed parameter updates? (Answer: To stabilize training by reducing oscillation from moving targets)

- **Sentence/Sequence Representations from PLMs**
  - Why needed here: The Actor encodes state-action pairs using pre-trained models (BERT, GTE, Qwen). Understanding [CLS] token representations for encoders vs. last-token representations for decoders determines implementation correctness.
  - Quick check question: For BERT-based Actor, which hidden state is used as sequence representation? (Answer: h₀, the first token's representation)

## Architecture Onboarding

- **Component map:**
  Environment (LLM, unmodified) -> Actor Model (trainable) -> Training Loop (DDQN)

- **Critical path:**
  1. Define task-specific state schema (what goes in the dict)
  2. Define action space (subtask decomposition satisfying Markov/completeness/homogeneity)
  3. Implement reward function (stepwise vs. episode-level)
  4. Build Actor: load PLM → add linear layer → implement forward pass
  5. Implement DDQN training loop with environment interaction
  6. At inference: Actor selects argmax action → LLM executes → update state → repeat

- **Design tradeoffs:**
  - Actor size vs. sample efficiency: Larger PLMs capture richer features but require more memory; smaller models may need more epochs
  - Freezing vs. fine-tuning foundation model: Freezing reduces overfitting and speeds training; full fine-tuning may improve performance but risks catastrophic forgetting
  - Exploration strategy: ε-greedy with decay balances exploration/exploitation; faster decay may miss optimal sequences

- **Failure signatures:**
  - Q-value collapse: All actions receive similar Q-values → check reward signal density, increase exploration
  - Action repetition or invalid selection: Actor selects already-executed subtask → verify action space correctly shrinks
  - LLM timeout or malformed outputs: Environment step fails → add output validation, increase timeout
  - Performance degrades with task complexity: If accuracy drops sharply on complex instances → check if Actor generalizes to longer action sequences

- **First 3 experiments:**
  1. **Sanity check on toy task:** Implement S2P with 4-sentence contexts. Verify random Actor achieves ~50% SOC, trained Actor improves >10%.
  2. **Ablation on Actor model:** Compare RLAP-random, RLAP-sequence, and RLAP-RL on a single IE dataset. Expected: RLAP-random ≈ baseline, RLAP-sequence slightly better, RLAP-RL significantly better.
  3. **Complexity scaling test:** On MRC task, bin test instances by context length (3-5, 6-8, 9+ sentences). Plot accuracy vs. length for IO, ToT-BFS, RLAP.

## Open Questions the Paper Calls Out

- Can the RLAP framework be effectively extended to multimodal tasks where state representations must incorporate non-textual features? (Basis: Conclusion states future work will explore application in multimodal tasks)
- How can the principles of adaptive planning in RLAP be integrated into the pre-training phase of Large Language Models? (Basis: Conclusion notes future work includes exploring application during pre-training phase)
- Does the relative benefit of the RLAP framework diminish as the parameter scale and intrinsic reasoning capabilities of the frozen LLM executor increase? (Basis: Paper argues existing methods fail because they "rely on the intrinsic planning capabilities of LLMs," which are described as "limited")
- Can a single, unified Actor model be trained to generalize across diverse NLP task types without requiring task-specific retraining? (Basis: Section 3.3 states "for all datasets of the same language and task type, we only need to train a single Actor model")

## Limitations
- Framework assumes subtasks satisfy strict MDP properties (Markov, completeness, homogeneity) that may not hold for all NLP tasks
- Performance improvements depend on specific LLM executor outputs and prompt templates not fully specified in paper
- Gains may not generalize to tasks requiring more complex reasoning or where intermediate results are highly interdependent

## Confidence
- **High confidence:** Core RLAP architecture and MDP formulation are clearly specified with statistically significant performance improvements
- **Medium confidence:** Claim that framework maintains stable performance as task complexity increases requires careful analysis of complexity-scaling experiments
- **Low confidence:** Assertion that improvements stem specifically from adaptive planning rather than other factors lacks sufficient ablation studies

## Next Checks
1. **Ablation study on prompt templates:** Compare RLAP performance using different LLM prompt formulations to determine if improvements are due to planning or execution optimization
2. **Cross-task generalization test:** Apply RLAP to a fourth NLP task (e.g., text summarization) with different subtask structures to evaluate framework robustness
3. **Actor model size scaling analysis:** Systematically vary Actor model parameters (from 110M to 7B) on the same tasks to quantify relationship between model capacity and performance gains