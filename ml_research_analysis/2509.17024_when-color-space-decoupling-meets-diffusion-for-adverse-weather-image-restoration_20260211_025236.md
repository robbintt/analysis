---
ver: rpa2
title: When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image Restoration
arxiv_id: '2509.17024'
source_url: https://arxiv.org/abs/2509.17024
tags:
- image
- restoration
- diffusion
- weather
- psnr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified framework for adverse-weather image
  restoration that addresses the limitations of both degradation-agnostic and prompt-learning
  approaches. The core idea is to decompose degraded images into degradation-related
  luminance and degradation-invariant chrominance components in the YCbCr color space,
  then restore them separately.
---

# When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image Restoration

## Quick Facts
- **arXiv ID**: 2509.17024
- **Source URL**: https://arxiv.org/abs/2509.17024
- **Reference count**: 40
- **Primary result**: Achieves highest PSNR/SSIM on multiple adverse-weather benchmarks via luminance-guided diffusion without degradation prompts.

## Executive Summary
This paper proposes a unified framework for adverse-weather image restoration that addresses the limitations of both degradation-agnostic and prompt-learning approaches. The core idea is to decompose degraded images into degradation-related luminance and degradation-invariant chrominance components in the YCbCr color space, then restore them separately. A luminance-guided diffusion model leverages the restored luminance as a conditional input, eliminating the need for explicit degradation prompts. The method incorporates a dynamic time-step loss that balances low- and high-frequency feature recovery. Extensive experiments on multiple benchmarks including a new DriveWeather dataset demonstrate superior performance over state-of-the-art methods, with the proposed LCDiff achieving the highest PSNR and SSIM values across diverse weather conditions.

## Method Summary
The proposed method follows a two-stage pipeline. First, the degraded image is converted from RGB to YCbCr color space, decomposing it into luminance (Y) and chrominance (CbCr) components. The Luminance Restoration Module (LRM) uses a U-Net-like NAFNet backbone to restore the luminance channel. The Frequency Chrominance Restoration Module (FCRM) employs FFT-based frequency separation, spatial gating on low frequencies, channel attention on high frequencies, and channel cross-attention fusion to restore chrominance. The restored luminance guides a diffusion model (LGDM) through conditional denoising, where the luminance serves as input condition. A dynamic time-step loss (DTS) balances low- and high-frequency recovery by applying weighted losses across different time steps. The final restored image is obtained by combining the diffusion output with the chrominance from FCRM.

## Key Results
- LCDiff achieves the highest PSNR and SSIM values across multiple adverse-weather benchmarks
- The method handles complex scenarios like rain-fog mixtures without requiring weather-specific prompts
- Outperforms state-of-the-art methods on the newly introduced DriveWeather dataset (29,750 image pairs)
- Maintains computational efficiency while achieving superior restoration quality

## Why This Works (Mechanism)
The method works by exploiting the inherent separation between degradation-related luminance variations and degradation-invariant chrominance information in the YCbCr color space. By decoupling these components, the model can apply specialized restoration techniques to each: the luminance channel receives focused attention for degradation removal through the diffusion model, while chrominance restoration leverages frequency-aware processing. The luminance-guided diffusion eliminates the need for explicit degradation prompts by using the restored luminance as natural conditioning. The dynamic time-step loss ensures balanced recovery across frequency bands, preventing over-smoothing while maintaining perceptual quality.

## Foundational Learning

**YCbCr Color Space Decomposition**
- Why needed: Separates luminance (brightness) from chrominance (color) information, allowing targeted restoration
- Quick check: Verify Y channel captures grayscale intensity while CbCr encode color differences

**Diffusion Models for Image Restoration**
- Why needed: Provides powerful generative modeling capability for complex degradation removal
- Quick check: Confirm the diffusion model can generate plausible clean samples from noise

**Frequency Separation with FFT**
- Why needed: Enables specialized processing of low and high frequency components in chrominance
- Quick check: Validate that FFT masks correctly separate frequency bands

**Dynamic Time-Step Loss**
- Why needed: Balances low and high frequency recovery during diffusion denoising
- Quick check: Verify that weighting function ω_t properly emphasizes different time steps

## Architecture Onboarding

**Component Map**
RGB Input -> YCbCr Decomposition -> LRM (NAFNet) -> Luminance Restoration -> Luminance Condition -> LGDM -> Diffusion Output -> FCRM (FFT + Attention) -> Chrominance Restoration -> YCbCr to RGB Conversion -> Final Output

**Critical Path**
The most critical path is RGB -> YCbCr -> LRM -> Luminance Condition -> LGDM -> FCRM -> Final Output. Any failure in the luminance restoration or its conditioning will cascade through the diffusion stage.

**Design Tradeoffs**
The YCbCr decomposition trades color space transformation complexity for specialized processing of luminance and chrominance. The luminance-guided approach eliminates prompt engineering but requires accurate luminance restoration. The dynamic time-step loss adds training complexity but improves frequency balance.

**Failure Signatures**
Color bleeding or desaturation indicates improper YCbCr handling. Over-smoothed textures suggest insufficient high-frequency preservation in the DTS loss. Poor luminance restoration will manifest as residual degradation artifacts in the final output.

**First Experiments**
1. Implement YCbCr decomposition and verify that luminance captures degradation patterns while chrominance remains relatively stable
2. Test the luminance restoration module independently to ensure it can remove weather degradation
3. Validate the frequency separation in FCRM by visualizing low and high frequency components

## Open Questions the Paper Calls Out
None

## Limitations
- Architectural details of the denoiser backbone and condition mapping G(·) are underspecified
- Training sequence (LCDN alone vs. end-to-end) is unclear
- Frequency mask shapes and wavelet transform parameters are not provided

## Confidence
**Confidence**: Medium

The paper presents a well-structured, experimentally rich pipeline for unified adverse-weather restoration. The decoupling into luminance/chrominance is clearly defined, and the dynamic time-step loss is novel. However, several critical architectural details (denoiser depth/width, frequency mask definitions, conditioning projection) are only indirectly referenced, making exact replication difficult. The performance claims are compelling but rely on assumed baseline parameters from prior work.

## Next Checks
1. Implement the LCDN stage with placeholder FFT masks and confirm that YCbCr restoration produces plausible intermediate results
2. Build the luminance projection G(·) and verify that conditioning can be fed into the denoiser
3. Run a small ablation on the dynamic time-step loss (k=5) to confirm the effect on PSNR/SSIM and high-frequency preservation