---
ver: rpa2
title: Shapley-Value-Based Graph Sparsification for GNN Inference
arxiv_id: '2507.20460'
source_url: https://arxiv.org/abs/2507.20460
tags:
- macs
- thousands
- graph
- sparsification
- edges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of Shapley values for graph sparsification
  in Graph Neural Networks (GNNs) to improve inference efficiency. Traditional GNN
  explanation methods produce only non-negative scores, limiting their effectiveness
  for edge pruning.
---

# Shapley-Value-Based Graph Sparsification for GNN Inference

## Quick Facts
- arXiv ID: 2507.20460
- Source URL: https://arxiv.org/abs/2507.20460
- Authors: Selahattin Akkas; Ariful Azad
- Reference count: 40
- Achieves up to 80% edge pruning with less than 2% drop in accuracy on Cora and PubMed datasets

## Executive Summary
This paper introduces a novel approach to improve Graph Neural Network (GNN) inference efficiency through Shapley-value-based graph sparsification. Traditional GNN explanation methods assign only non-negative scores to edges, limiting their effectiveness for edge pruning. The authors demonstrate that Shapley values, which can assign both positive and negative importance scores to edges, enable more effective pruning by removing edges that negatively impact predictions. This approach maintains predictive performance while significantly reducing graph complexity and computational costs.

## Method Summary
The authors propose using Shapley values from cooperative game theory to evaluate edge importance in GNNs. Unlike traditional explanation methods that produce only non-negative scores, Shapley values can assign negative scores to edges that harm prediction accuracy. The method involves computing Shapley values for each edge, then pruning edges based on their signed importance scores. This allows the removal of both redundant edges (low positive scores) and detrimental edges (negative scores). The approach is applied during inference without requiring model retraining, making it particularly suitable for memory-constrained devices.

## Key Results
- Achieves up to 80% edge pruning with less than 2% drop in accuracy on Cora and PubMed datasets
- Outperforms existing explainability and lottery ticket-based approaches
- Reduces Multiply-Accumulate operations by 64-65% during inference
- Demonstrates significant computational cost reduction without requiring retraining

## Why This Works (Mechanism)
Shapley values provide a principled way to quantify the marginal contribution of each edge to the final prediction by considering all possible edge coalitions. Unlike traditional explainability methods that only highlight important edges, Shapley values can identify edges that actively harm prediction accuracy by assigning them negative scores. This dual capability (identifying both helpful and harmful edges) enables more aggressive and effective pruning. The method leverages the fact that removing detrimental edges can improve or maintain accuracy while reducing computational complexity.

## Foundational Learning

**Shapley Values**: A game-theoretic concept that fairly distributes credit among players based on their marginal contributions to all possible coalitions. *Why needed*: Provides a mathematically sound way to quantify individual edge contributions. *Quick check*: Can be computed via Monte Carlo sampling for computational efficiency.

**Graph Neural Networks**: Neural networks that operate on graph-structured data by aggregating information from neighboring nodes. *Why needed*: The target models for which we want to improve inference efficiency. *Quick check*: Standard GNN layers follow message-passing architecture.

**Edge Pruning**: The process of removing edges from a graph while preserving essential structural information. *Why needed*: Directly addresses the goal of reducing computational complexity. *Quick check*: Can be evaluated by measuring changes in node representations after pruning.

## Architecture Onboarding

**Component Map**: Graph Data -> GNN Model -> Shapley Value Computation -> Edge Scoring -> Pruning Decision -> Sparse Graph -> Inference Engine

**Critical Path**: The most computationally intensive step is the Shapley value computation, which requires multiple forward passes with different edge subsets. This dominates the preprocessing time but is amortized over many inference queries.

**Design Tradeoffs**: The method trades preprocessing computation (Shapley value calculation) for faster inference. More accurate Shapley estimates require more sampling iterations, increasing preprocessing time but potentially enabling more aggressive pruning.

**Failure Signatures**: Performance degradation occurs when critical edges are mistakenly pruned due to insufficient sampling in Shapley computation, or when the model relies heavily on edges that appear detrimental in isolation but are beneficial in context.

**First Experiments**: 1) Compare accuracy vs. pruning ratio across different sampling budgets for Shapley computation. 2) Test on graphs with varying densities to identify robustness limits. 3) Measure actual inference time improvements on target hardware platforms.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance claims rely heavily on specific datasets (Cora, PubMed) and may not generalize to larger, more complex graphs
- The 80% pruning threshold with <2% accuracy drop may not be achievable across all graph types, particularly those with denser connectivity patterns
- Computational cost reduction figures are theoretical estimates that may vary significantly in real-world deployment scenarios

## Confidence

**High Confidence**: The fundamental advantage of Shapley values in providing signed importance scores for edges is well-established in game theory and the paper demonstrates this effectively for pruning decisions.

**Medium Confidence**: The comparative performance against existing explainability and lottery ticket methods is convincing on the tested datasets, but requires broader validation across diverse graph structures.

**Low Confidence**: The scalability claims for memory-constrained devices need empirical validation beyond the theoretical framework, particularly regarding how pruning affects long-term model stability and performance across different hardware architectures.

## Next Checks

1. Test the pruning methodology on larger, more complex benchmark graphs (e.g., OGB datasets) to assess scalability and performance degradation patterns.

2. Conduct ablation studies varying pruning percentages to identify the optimal trade-off point between efficiency gains and accuracy preservation.

3. Implement the pruned models on actual edge devices (e.g., Raspberry Pi, mobile GPUs) to measure real-world inference time improvements and memory usage.