---
ver: rpa2
title: 'humancompatible.train: Implementing Optimization Algorithms for Stochastically-Constrained
  Stochastic Optimization Problems'
arxiv_id: '2509.21254'
source_url: https://arxiv.org/abs/2509.21254
tags:
- stochastic
- optimization
- algorithms
- constraints
- constrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces humancompatible.train, a PyTorch-based Python
  package for training deep neural networks with stochastic constraints. The package
  implements multiple algorithms for stochastically-constrained stochastic optimization,
  addressing challenges like large-scale objective and constraint functions, inequality
  constraints, and nonconvexity/nonsmoothness inherent to neural networks.
---

# humancompatible.train: Implementing Optimization Algorithms for Stochastically-Constrained Stochastic Optimization Problems

## Quick Facts
- arXiv ID: 2509.21254
- Source URL: https://arxiv.org/abs/2509.21254
- Reference count: 29
- This work introduces humancompatible.train, a PyTorch-based Python package for training deep neural networks with stochastic constraints.

## Executive Summary
This paper introduces humancompatible.train, a PyTorch-based Python package implementing multiple algorithms for stochastically-constrained stochastic optimization problems in deep learning. The authors address the challenge of incorporating fairness and other inequality constraints into neural network training when both the objective function and constraints are stochastic and potentially nonconvex. The package implements previously unimplemented algorithms including SSL-ALM and Stochastic Switching Subgradient, and demonstrates their effectiveness on a fairness-constrained deep learning task using the ACS Income dataset.

## Method Summary
The paper implements two previously unimplemented algorithms for stochastically-constrained stochastic optimization: SSL-ALM (Stochastic Smoothed and Linearized Augmented Lagrangian Method) and SSw (Stochastic Switching Subgradient). SSL-ALM interprets updates as inexact gradient descent on a smoothed proxy (Moreau envelope) whose stationary points match the original problem, combining augmented Lagrangian framework with stochastic sampling. SSw handles nonsmooth, nonconvex constraints by conditionally switching between objective descent and constraint-correction steps based on feasibility checks. Both algorithms are implemented in a PyTorch-based package with support for stratified sampling across constraint groups, particularly useful for fairness constraints defined over demographic subgroups.

## Key Results
- The humancompatible.train package successfully implements SSL-ALM and Stochastic Switching Subgradient algorithms for constrained DNN training
- Both algorithms can maintain constraint values within specified bounds while minimizing the objective function on ACS Income dataset
- SSL-ALM demonstrates faster convergence under the chosen hyperparameters compared to SSw
- The package provides an easily-extendable API for researchers to compare and develop constrained optimization algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SSL-ALM enables constrained optimization by interpreting updates as inexact gradient descent on a smoothed proxy (Moreau envelope) whose stationary points match the original problem.
- Mechanism: The augmented Lagrangian L_ρ(x,y) = F(x) + y^T C(x) + ρ/2 ||C(x)||^2 combines constraint handling with quadratic penalty. A proximal term μ/2 ||x-z||^2 smooths the landscape. In each iteration, sampling ξ ~ P_ξ and ζ_1, ζ_2 ~ P_ζ provides unbiased gradient estimates for primal and dual updates (Eq. 2-3). The dual multiplier y adaptively penalizes constraint violations.
- Core assumption: The Moreau envelope's stationary points coincide with those of the original function; unbiased stochastic estimates of gradients are obtainable.
- Evidence anchors:
  - [Abstract]: "We implement multiple previously unimplemented algorithms for stochastically constrained stochastic optimization."
  - [Section 2.2]: "The SSL-ALM method was originally proposed in [15] where it is interpreted as an inexact gradient descent step on the Moreau envelope. An important property of the Moreau envelope is that its stationary points coincide with those of the original function."
  - [Corpus]: Limited direct corpus validation for this specific algorithm; neighbor papers focus on general constrained DNN training.
- Break condition: If constraint gradients are highly noisy or biased, dual updates may diverge; convergence guarantees require constraint functions to be at least weakly convex (Table 1 notes assumptions).

### Mechanism 2
- Claim: SSw handles nonsmooth, nonconvex constraints by conditionally switching between objective descent and constraint-correction steps based on feasibility checks.
- Mechanism: At iteration k, sample ζ_1,...,ζ_J to estimate constraint c_J(x_k). If c_J(x_k) < ε_k (within tolerance), take a step using stochastic subgradient S_f of the objective. Otherwise, take a step using stochastic subgradient S_c of the constraint. This alternation prioritizes feasibility when constraints are violated.
- Core assumption: The projection onto feasible set X is efficiently computable; subgradients provide meaningful descent directions for weakly convex functions.
- Evidence anchors:
  - [Section 2.3]: "This is why the notion of gradient of F and C is replaced by a more general notion of subgradient, which is an element of a subdifferential."
  - [Section 2.3]: "If c_J(x_k) is smaller than ε_k, we sample ξ ~ P_ξ and an update... is computed using a stochastic estimate S_f(x_k, ξ)."
  - [Corpus]: Weak corpus validation for switching subgradient methods specifically in DNN fairness contexts.
- Break condition: If tolerance schedule ε_k decays too slowly or step sizes η_f^k, η_c^k are mismatched, oscillation between feasible and infeasible regions may occur.

### Mechanism 3
- Claim: Stratified sampling across constraint groups ensures all constraints receive gradient signal even under class imbalance.
- Mechanism: For fairness constraints defined over demographic subgroups, the toolkit samples equal numbers from each group per iteration. This prevents rare groups from being underrepresented in stochastic constraint estimates.
- Core assumption: Constraint functions decompose by group membership; equal sampling does not introduce distribution shift harmful to the objective.
- Evidence anchors:
  - [Section 3]: "Using a utility provided with the humancompatible.train package, we sample equal number of data points from each subgroup at each iteration to ensure that all constraints can be computed."
  - [Section 3]: Constraints defined as |P(f_θ(X)=1|G=g) - P(f_θ(X)=1)| ≤ c for each group g.
  - [Corpus]: No direct corpus validation for this sampling strategy.
- Break condition: If group distributions differ significantly at test time, stratified training may not generalize; constraint evaluation on held-out data is essential.

## Foundational Learning

- Concept: **Augmented Lagrangian Methods**
  - Why needed here: SSL-ALM builds on this framework; understanding the trade-off between Lagrange multipliers (dual variables) and penalty terms is essential for hyperparameter tuning (ρ, η, τ, β).
  - Quick check question: Can you explain why increasing the penalty parameter ρ alone may lead to ill-conditioning?

- Concept: **Subgradients and Weak Convexity**
  - Why needed here: SSw operates in nonsmooth settings where classical gradients don't exist; subgradients generalize descent directions for functions like ReLU networks.
  - Quick check question: For a ReLU activation, what is the subdifferential at x=0?

- Concept: **Demographic Parity Constraints**
  - Why needed here: The demonstration uses fairness constraints; understanding how positive rate differences translate to inequality constraints clarifies what the optimizer is actually enforcing.
  - Quick check question: If a model achieves demographic parity, does it guarantee equal opportunity? Why or why not?

## Architecture Onboarding

- Component map:
  - `humancompatible.train/` — Core package
    - `algorithms/` — SSL-ALM, SSw implementations with `step()` and `dual_step()` methods
    - `constraints/` — Constraint definitions (e.g., via Fairret integration)
    - `samplers/` — Stratified sampling utilities for subgroup-balanced batches
    - `utils/` — Data loading (ACS/Folktables), metric computation
  - External dependencies: PyTorch (optimization backbone), Fairret (differentiable fairness terms), Scikit-learn (preprocessing)

- Critical path:
  1. Define constraint functions C(x) and objective F(x) in PyTorch-compatible form
  2. Initialize algorithm with hyperparameters (SSL-ALM: μ, ρ, τ, η, β, M_y; SSw: η_f, η_c, tolerance schedule)
  3. Per iteration: sample batch → compute constraint estimates → `step()` for primal update → `dual_step()` for dual/multiplier update
  4. Log loss and constraint values; checkpoint when constraints satisfied and loss stabilized

- Design tradeoffs:
  - SSL-ALM: Faster convergence (per paper's hyperparameters) but more hyperparameters to tune; requires constraint Jacobians (may need separate backward passes).
  - SSw: Fewer hyperparameters conceptually, but tolerance schedule ε_k and separate step sizes add complexity; better suited when projection is cheap.
  - Stratified sampling: Ensures constraint signal but may underrepresent majority group, affecting objective optimization quality.

- Failure signatures:
  - Constraint values oscillate or diverge → dual learning rate η too high or tolerance schedule too aggressive.
  - Loss plateaus while constraints remain infeasible → primal step size τ too small relative to dual dynamics.
  - NaN in gradients → numerical instability in constraint Jacobian; check for zero-variance batches or extreme multiplier values.
  - Large gap between train and test constraint satisfaction → overfitting to sampled constraints; increase batch size or use full-batch constraint evaluation periodically.

- First 3 experiments:
  1. **Baseline comparison**: Train same network with Adam (unconstrained), SSL-ALM, and SSw on ACS Income; plot loss vs. constraint satisfaction trade-off curves to confirm both algorithms respect bounds.
  2. **Hyperparameter sweep for SSL-ALM**: Vary ρ ∈ {0.1, 1.0, 10.0} and η ∈ {0.01, 0.1, 1.0}; measure convergence speed and final constraint violation to identify stable regimes.
  3. **Ablation on sampling strategy**: Compare stratified subgroup sampling vs. standard random sampling; quantify variance in constraint estimates and impact on final fairness metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can theoretical convergence guarantees be established for the Stochastic Smoothed and Linearized AL Method (SSL-ALM) when applied to stochastic non-linear constraints?
- Basis in paper: [explicit] The authors note that while SSL-ALM was originally described for stochastic linear constraints, they apply it to non-linear inequality constraints "due to the lack of algorithms in the literature dealing with stochastic non-linear constraints."
- Why unresolved: The theoretical guarantees provided in the original SSL-ALM literature [15] rely on linearity assumptions that do not hold for the general non-linear constraints implemented in this toolkit.
- What evidence would resolve it: A formal convergence proof extending SSL-ALM to non-linear stochastic settings, or empirical studies demonstrating consistent convergence behavior on diverse non-linear constraint problems.

### Open Question 2
- Question: Does the modification of the Stochastic Switching Subgradient (SSw) method to allow distinct stepsizes for objective and constraint updates ($\eta_f^k \neq \eta_c^k$) preserve the algorithm's theoretical convergence guarantees?
- Basis in paper: [inferred] The authors explicitly state their implementation is "slightly more general" than the original method [16] by allowing separate stepsizes, but they do not cite theoretical justification for this deviation.
- Why unresolved: The original convergence proofs for switching subgradient methods typically depend on specific stepsize decay schedules that may assume uniformity across all updates.
- What evidence would resolve it: A theoretical analysis validating the convergence of the generalized stepsize schedule, or empirical comparisons showing performance parity or degradation against the theoretically grounded uniform stepsize.

### Open Question 3
- Question: Is it possible to develop a unified algorithm with convergence guarantees for the general setting of nonconvex, nonsmooth objective and constraint functions using stochastic sampling?
- Basis in paper: [explicit] The paper states that "there exists currently no algorithm with guarantees for such a general setting" and that current methods address these challenges only partially.
- Why unresolved: The intersection of nonconvexity (from DNNs), nonsmoothness (from activations or loss functions), and stochasticity (from sampling) creates a complex optimization landscape currently lacking a unified theoretical framework.
- What evidence would resolve it: The proposal of a new algorithm accompanied by convergence rate analysis that explicitly handles all three properties simultaneously.

## Limitations

- The paper demonstrates algorithm feasibility on a single dataset (ACS Income) with one specific fairness constraint formulation, limiting generalizability claims.
- Empirical comparisons are restricted to a narrow set of hyperparameters without systematic ablation studies on algorithm sensitivity to learning rates or penalty parameters.
- The absence of theoretical convergence guarantees for the specific DNN application, despite citing general stochastic approximation theory, represents another limitation.
- Critical hyperparameters like batch sizes and constraint sample counts (J) are not specified, which are essential for faithful reproduction.

## Confidence

- **High confidence**: The package implements SSL-ALM and SSw algorithms as described, and stratified sampling methodology for subgroup constraints is correctly specified and implemented.
- **Medium confidence**: The empirical demonstration shows both algorithms can satisfy constraints while minimizing loss, but the comparison between SSL-ALM and SSw is limited by the single hyperparameter configuration tested.
- **Low confidence**: Claims about SSL-ALM's faster convergence require broader hyperparameter sweeps to be validated, and the assertion that these algorithms generalize to arbitrary stochastically-constrained problems needs more diverse empirical validation.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary penalty parameter ρ (SSL-ALM) and tolerance schedule εₖ (SSw) across multiple orders of magnitude to identify robust operating regimes and quantify convergence variability.

2. **Cross-dataset generalization test**: Apply both algorithms to at least two additional fairness-constrained tasks (e.g., COMPAS recidivism prediction with equalized odds constraints, or adult income with different demographic attributes) to assess algorithm robustness.

3. **Theoretical convergence verification**: Conduct experiments varying batch sizes and constraint sample counts (J) to empirically validate whether the variance reduction strategies align with the theoretical assumptions required for convergence in stochastic approximation theory.