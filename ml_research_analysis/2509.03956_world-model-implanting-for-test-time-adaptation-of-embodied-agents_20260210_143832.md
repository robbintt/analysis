---
ver: rpa2
title: World Model Implanting for Test-time Adaptation of Embodied Agents
arxiv_id: '2509.03956'
source_url: https://arxiv.org/abs/2509.03956
tags:
- world
- wormi
- reasoning
- adaptation
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WorMI enables embodied agents to adapt to novel domains without
  retraining by implanting multiple domain-specific world models into a large language
  model-based reasoning policy. The framework retrieves relevant world models using
  trajectory-based prototype matching and fuses them through a trainable compound
  attention mechanism, allowing flexible knowledge integration.
---

# World Model Implanting for Test-time Adaptation of Embodied Agents

## Quick Facts
- arXiv ID: 2509.03956
- Source URL: https://arxiv.org/abs/2509.03956
- Reference count: 40
- Primary result: WorMI achieves 20.41% higher success rate than SayCanPay in zero-shot scenarios and 26.58% improvement in few-shot settings.

## Executive Summary
WorMI is a framework for test-time adaptation of LLM-based embodied agents to novel domains without retraining. It implants multiple domain-specific world models into a frozen reasoning model and retrieves relevant models using prototype-based matching. The framework fuses retrieved models through a trainable compound attention mechanism, enabling flexible knowledge integration. Experiments on VirtualHome and ALFWorld demonstrate 20-27% improvement in success rate and 18-20% reduction in action steps compared to state-of-the-art approaches.

## Method Summary
WorMI trains domain-specific world models on task datasets, computes compact prototypes via k-center clustering, and uses Wasserstein distance for efficient retrieval. At test time, it retrieves K relevant world models and fuses them with the frozen reasoning model through hierarchical compound attention. The framework meta-learns attention parameters to generalize to unseen world model combinations, enabling adaptation without retraining the base LLM.

## Key Results
- Achieves 20.41% higher success rate than SayCanPay in zero-shot scenarios
- Improves few-shot performance by 26.58% with 18-20% reduction in action steps
- Demonstrates robust generalization to unseen tasks and environments
- Maintains performance gains even with adversarial or increasing numbers of world models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prototype-based retrieval selects relevant world models with bounded approximation error, reducing computational cost while maintaining domain alignment.
- **Mechanism:** Object-wise state embeddings from each domain are clustered via k-center algorithm to form compact prototypes (~0.1% of embeddings). At test time, Wasserstein distance between the current observation's prototype and stored prototypes selects the top-K world models. The triangle inequality guarantees δ(pi, pj) ≤ δ(Ei, Ej) + 2ρ, bounding the approximation.
- **Core assumption:** Object-wise state embeddings preserve sufficient domain-distinguishing information; the k-center coverage radius ρ remains small across domains.
- **Evidence anchors:** [abstract] "prototype-based world model retrieval approach, utilizing efficient trajectory-based abstract representation matching"; [section 3.2] Equation 3–4; "using pj, which comprises only about 0.1% of the total embeddings"
- **Break condition:** If domains share nearly identical object distributions but differ in dynamics/affordances, prototype similarity may fail to discriminate; if ρ is large, the bound becomes loose and retrieval quality degrades.

### Mechanism 2
- **Claim:** World-wise compound attention fuses multiple world models and aligns them to the frozen reasoning model via hierarchical cross-attention, enabling test-time knowledge composition.
- **Mechanism:** (1) Linear projection aligns dimensions; (2) world-level cross-attention uses reasoning hidden state as query, world model outputs as keys/values, producing a weighted integration; (3) reasoning-level cross-attention aligns the integrated representation back to the reasoning model's query space. This creates a dual-stage fusion: world-to-world integration, then world-to-reasoning alignment.
- **Core assumption:** World model intermediate representations encode complementary domain knowledge; cross-attention can dynamically reweight and align these without fine-tuning the base LLM.
- **Evidence anchors:** [abstract] "world-wise compound attention method that not only integrates... but also aligns their intermediate representations"; [section 3.3] Equations 5–8; Figure 3 shows hierarchical attention structure
- **Break condition:** If world model representations are misaligned in semantic space (e.g., different tokenization or pretraining), projection/attention may fail to integrate meaningfully; excessive K may exceed attention capacity.

### Mechanism 3
- **Claim:** Meta-learning the compound attention parameters enables generalization to unseen world model combinations, including newly added models.
- **Mechanism:** Inner-loop: adapt θ to a subset of world models on their associated datasets. Outer-loop: aggregate adapted parameters to update meta-parameters. This trains θ as a "composer" that can rapidly specialize to any world model combination with few gradient steps.
- **Core assumption:** A shared integration strategy exists across domain combinations; the compound attention module has sufficient capacity to encode this meta-strategy.
- **Evidence anchors:** [abstract] "trainable compound attention mechanism"; [section 3.4] Equation 9; meta-learning algorithm; "enables the compound attention Cθ to learn how to weigh and align features"
- **Break condition:** If world models have fundamentally incompatible architectures or representation spaces, no meta-strategy may generalize; if task distribution is too narrow, meta-learned θ may overfit.

## Foundational Learning

- **Cross-attention (Query-Key-Value)**
  - Why needed here: Core to how compound attention integrates world model representations (keys/values) conditioned on reasoning state (query).
  - Quick check question: Given query Q from the LLM and keys K from world models, what does the attention weight matrix compute?

- **Meta-learning (MAML-style inner/outer loops)**
  - Why needed here: Enables the attention module to generalize across arbitrary world model subsets, critical for extensibility.
  - Quick check question: In the inner loop, what is being adapted? In the outer loop, what is being optimized?

- **World Models (dynamics, affordance, policy)**
  - Why needed here: Each world model encodes domain-specific transition dynamics, action affordances, and behavioral priors.
  - Quick check question: For a kitchen domain, what three types of predictions should the world model support?

## Architecture Onboarding

- **Component map:** Object detection ΦD and embedding ΦE models → Prototype computation (k-center clustering) → Prototype retrieval (Wasserstein distance) → Compound attention Cθ (linear projection → world-level cross-attention → reasoning-level cross-attention) → Frozen reasoning model πR → Action

- **Critical path:**
  1. Train individual world models on domain datasets (dynamics + affordance + BC)
  2. Compute prototypes via k-center clustering on object embeddings
  3. Meta-train Cθ across world model subsets
  4. At test: match observation prototype → retrieve → fuse via Cθ → act

- **Design tradeoffs:**
  - K (retrieved world models): Paper uses K=3. Higher K adds knowledge but increases attention complexity; Table 5 shows K=1 or K=6 underperform.
  - Prototype size k: Paper uses k=15. Larger k improves retrieval precision but increases matching cost.
  - Intermediate connection layers: Paper uses [13,27] for reasoning model, [7,15] for world models. Layer choice affects representation granularity.

- **Failure signatures:**
  - Performance collapses when using all world models (WorMI-E) or random selection (WorMI-R) → retrieval is critical
  - CONCAT/ADD baselines underperform vs. compound attention → hierarchical attention is necessary
  - Performance degrades with adversarial world models at high ratios → attention can filter noise but has limits

- **First 3 experiments:**
  1. **Ablate retrieval:** Compare prototype-based vs. random vs. all-world-models retrieval on unseen tasks. Expect: prototype > random > all (Table 3a pattern).
  2. **Ablate attention:** Compare compound attention vs. simple concatenation vs. summation. Expect: compound attention shows 15-20% SR gain (Table 3b).
  3. **Visualize attention weights:** For a task requiring knowledge from multiple domains (e.g., "place plum in cabinet"), plot world-level attention over time. Expect: attention shifts dynamically as subgoals progress (Figure 4, Figure A.6 pattern).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the computational overhead of attending to multiple domain-specific world models be optimized to enable deployment on resource-constrained robotic hardware?
- **Basis in paper:** [explicit] The authors state in the Limitations section that "inferring over multiple domain-specific world models increases computational overhead, potentially posing challenges in resource-constrained settings."
- **Why unresolved:** While the paper demonstrates efficiency gains in action steps, it acknowledges the raw inference cost of the framework remains a barrier for real-world use without further optimization.
- **Evidence would resolve it:** Demonstration of WorMI operating in real-time on edge devices or a comparative analysis showing reduced FLOPs/memory usage compared to the current baseline without sacrificing success rates.

### Open Question 2
- **Question:** To what extent can the framework's reliance on the underlying LLM's reasoning capabilities be reduced to maintain robustness against weaker base models?
- **Basis in paper:** [explicit] The authors identify the "performance inherently dependent on the strengths and weaknesses of the underlying language model" as a key limitation.
- **Why unresolved:** The current architecture uses the LLM as the central reasoning hub; it is unclear if the implanted world models can compensate for deficiencies in the LLM's logic or contextual understanding.
- **Evidence would resolve it:** Experiments showing that WorMI improves the performance of smaller, less capable LLMs (e.g., 1B parameters) more significantly than larger models, closing the performance gap.

### Open Question 3
- **Question:** How can the compound attention mechanism be modified to handle a significantly larger pool of world models (e.g., N > 10) without suffering the performance degradation observed in Table 5?
- **Basis in paper:** [inferred] Table 5 and the "Scalability for World Models" analysis show that increasing the number of world models from 3 to 6 leads to a marked performance drop (SR decreases from 66.12% to 48.23%), suggesting a capacity limit or noise saturation in the current attention method.
- **Why unresolved:** The paper retrieves a subset (K), but the presence of more irrelevant models in the total pool (N) appears to introduce noise that the current retrieval and attention struggle to filter out effectively.
- **Evidence would resolve it:** A study showing stable or improved Success Rates as the total available world model pool scales up, indicating the mechanism successfully isolates signal from noise at scale.

## Limitations
- Computational overhead of attending to multiple world models may limit deployment on resource-constrained hardware
- Performance is inherently dependent on the strengths and weaknesses of the underlying language model
- Framework requires training domain-specific world models, which may not be feasible for all environments

## Confidence

- Prototype-based retrieval mechanism: **High** - Well-grounded in computational geometry with clear approximation bounds
- Compound attention fusion: **Medium** - The hierarchical design is novel but relies on implicit alignment assumptions
- Meta-learning generalization: **Medium-Low** - Claims of few-shot adaptability are strong but depend heavily on representation compatibility

## Next Checks

1. **Retrieval ablation with adversarial world models:** Gradually increase the ratio of irrelevant world models (0%, 25%, 50%, 75%) to test attention's filtering capacity and identify breaking points
2. **Cross-domain generalization test:** Apply WorMI trained on VirtualHome to ALFWorld (or vice versa) to evaluate transfer without fine-tuning
3. **Prototype sensitivity analysis:** Systematically vary k (prototype count) and ρ (coverage radius) to map the retrieval accuracy vs. computational cost tradeoff