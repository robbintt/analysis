---
ver: rpa2
title: 'Bridging the Gap: In-Context Learning for Modeling Human Disagreement'
arxiv_id: '2506.06113'
source_url: https://arxiv.org/abs/2506.06113
tags:
- multip
- aggr
- hard
- disaggr
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether large language models can effectively
  capture human disagreement in subjective NLP tasks such as hate speech and offensive
  language detection. The study evaluates four open-source LLMs using in-context learning
  across three label modeling strategies: aggregated hard labels, disaggregated hard
  labels, and disaggregated soft labels, in both zero-shot and few-shot settings.'
---

# Bridging the Gap: In-Context Learning for Modeling Human Disagreement

## Quick Facts
- **arXiv ID:** 2506.06113
- **Source URL:** https://arxiv.org/abs/2506.06113
- **Reference count:** 40
- **Primary result:** Multi-perspective generation works in zero-shot but few-shot setups often fail to capture human disagreement in subjective NLP tasks.

## Executive Summary
This paper investigates whether large language models can effectively capture human disagreement in subjective NLP tasks such as hate speech and offensive language detection. The study evaluates four open-source LLMs using in-context learning across three label modeling strategies: aggregated hard labels, disaggregated hard labels, and disaggregated soft labels, in both zero-shot and few-shot settings. Demonstration selection strategies (textual similarity, annotator disagreement, two-stage ranking) and ordering methods (random vs. curriculum-based) are explored in few-shot prompting. Results show that multi-perspective generation is viable in zero-shot settings, but few-shot setups often fail to capture the full spectrum of human judgments. Prompt design and demonstration selection notably affect performance, while example ordering has limited impact. The findings highlight the challenges of modeling subjectivity with LLMs and underscore the importance of building perspective-aware, socially intelligent models.

## Method Summary
The study uses in-context learning (ICL) with four open-source LLMs (Olmo-7b-Instruct, Llama-3-8b-Instruct, Gemma-7b-it, Deepseek-7b-chat) to model human disagreement in subjective NLP classification tasks. Three label spaces are tested: aggregated hard labels (majority vote), disaggregated hard labels (individual annotator votes), and disaggregated soft labels (probability distributions). Zero-shot and few-shot settings are compared using demonstration selection via BM25, PLM-based cosine similarity, entropy-based annotator disagreement, and two-stage ranking. Example ordering uses random selection or curriculum learning based on entropy. Performance is measured using Jensen-Shannon Divergence (JSD) and Cross-Entropy (CE) for soft labels, and macro F1 for hard labels, across three LeWiDi benchmark datasets.

## Key Results
- Multi-perspective prompting significantly improves zero-shot performance on aggregated labels, with JSD scores dropping from 0.138 to 0.058.
- BM25-based retrieval proves more effective than PLM-based semantic similarity for selecting demonstration examples in subjective tasks.
- LLMs struggle to capture nuanced disagreements in disaggregated labels, exhibiting low variability with outputs like [0.9, 0.1] or [1,1,1,1].
- Demonstration selection by annotator disagreement yields inconsistent results, potentially due to introducing too much ambiguity.
- Curriculum-based example ordering has limited impact on performance compared to random ordering.

## Why This Works (Mechanism)

### Mechanism 1: Multi-Perspective Priming
Explicitly instructing a model to adopt specific personas or consider diverse viewpoints may improve performance on aggregated subjective tasks by reducing single-bias convergence. The prompt adds a "context layer" that forces the attention mechanism to weigh conflicting interpretations of a text rather than defaulting to the most statistically probable label based on pre-training data. This works when the model's pre-training data contains sufficient representations of the requested perspectives.

### Mechanism 2: Similarity-Based Retrieval (BM25)
Retrieving demonstration examples based on lexical overlap (BM25) appears more effective than semantic similarity (PLM-based) for subjective tasks because subjectivity relies on specific lexical triggers (slurs, keywords). BM25 prioritizes keyword matching, ensuring few-shot examples share the specific "offensive" lexicon found in the test instance. This anchors the model's prediction in the text's surface features rather than abstract semantic meaning.

### Mechanism 3: Distributional Collapse
LLMs generally fail to model human disagreement distributions (soft labels) because they are optimized via RLHF/Instruction Tuning to converge on a single "correct" answer. When prompted for a distribution or multiple labels, the model exhibits "low variability," outputting extreme probabilities or unanimous hard labels. The instruction tuning suppresses the uncertainty inherent in the base model.

## Foundational Learning

- **Concept: Jensen-Shannon Divergence (JSD)**
  - **Why needed here:** You cannot use Accuracy or F1 to measure "disagreement." JSD measures the distance between two probability distributions (Human vs. Model). A low JSD means the model is uncertain when humans are uncertain.
  - **Quick check question:** If a model predicts [0.5, 0.5] and the human label is [0.5, 0.5], is the JSD high or low? (Answer: Low/Zero).

- **Concept: Aggregated vs. Disaggregated Labels**
  - **Why needed here:** This paper tests if models can predict the *majority vote* (Aggregated) vs. the *individual votes* (Disaggregated). Most production systems only care about Aggregated, but for "socially intelligent" AI, Disaggregated is required.
  - **Quick check question:** Does a dataset with 100% annotator agreement contain "Disaggregated" data? (Answer: Technically yes, but it lacks "Disagreement").

- **Concept: In-Context Curriculum Learning (ICCL)**
  - **Why needed here:** This is an ordering strategy where you show the model "easy" examples (high agreement) before "hard" examples (high disagreement).
  - **Quick check question:** Did ordering examples from easy to hard significantly improve performance in this study? (Answer: No, impact was limited).

## Architecture Onboarding

- **Component map:** Prompt Constructor -> Retrieval Engine -> Evaluator
- **Critical path:** The **Prompt Constructor** is the highest-leverage component. Specifically, the *Task Definition* text (whether it includes the "consider diverse perspectives" instruction) dictates the success of the zero-shot setup.
- **Design tradeoffs:**
  - *Zero-shot vs. Few-shot:* Zero-shot with Multi-Perspective instructions is computationally cheaper and often outperforms Few-shot for aggregated labels. Few-shot adds retrieval complexity and often degrades performance on nuanced disagreement.
  - *Label Space:* Asking for Soft Labels (probabilities) is ideal for modeling disagreement but current models default to "overconfidence." Hard Labels are reliable but lose nuance.
- **Failure signatures:**
  - **Monolithic Output:** The model returns [1, 1, 1, 1] or [0.9, 0.1] consistently, regardless of input ambiguity. This indicates the model is ignoring the "multi-perspective" instruction.
  - **Refusal:** Llama-3 or other safety-tuned models may refuse to classify offensive text, breaking the pipeline.
- **First 3 experiments:**
  1. **Zero-Shot Baseline vs. MP:** Take the HS-Brexit dataset. Run standard prompt ("Is this hate speech?") vs. MP prompt ("Consider diverse perspectives..."). Compare F1 scores to validate the "Priming" mechanism.
  2. **Retrieval Ablation:** Fix the prompt to "MP Aggregated." Run BM25 retrieval vs. Random selection on the MD-Agreement dataset. Verify if BM25 actually lowers Cross-Entropy (CE).
  3. **Soft Label Stress Test:** Prompt the model for soft labels on 10 high-disagreement examples. Manually inspect if the output varies or stays near [0.9, 0.1]. This validates the "Distributional Collapse" hypothesis locally.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why do LLMs struggle to capture the nuanced disagreements in disaggregated labels (both hard and soft) while performing relatively well on aggregated labels?
- **Basis in paper:** [explicit] "LLMs struggle to capture nuanced disagreements in disaggregated labels... LLMs perform poorly when predicting disaggregated labels, both hard and soft, struggling to capture the nuances of human disagreement."
- **Why unresolved:** The paper documents this failure but does not determine whether this stems from prompting limitations, model architecture, or fundamental ICL constraints.
- **What evidence would resolve it:** Ablation studies comparing prompting formats, fine-tuning approaches, and model architectures specifically designed for distribution prediction tasks.

### Open Question 2
- **Question:** Why does demonstration selection based on annotator disagreement yield inconsistent results in subjective tasks?
- **Basis in paper:** [explicit] "Selecting demonstration examples by annotator disagreement yields inconsistent results. We hypothesize that high-entropy examples introduce too much diversity or ambiguity, hindering the model's performance."
- **Why unresolved:** The authors hypothesize that high-entropy examples introduce too much ambiguity, but this remains speculative without empirical validation.
- **What evidence would resolve it:** Controlled experiments varying entropy thresholds systematically and analyzing the relationship between demonstration disagreement levels and model performance.

### Open Question 3
- **Question:** How can ICL approaches for modeling disagreement be extended to multi-class classification scenarios beyond binary tasks?
- **Basis in paper:** [explicit] "These datasets are typically restricted to binary classification which hinders the generalization of our approach to more complex, multi-class scenarios."
- **Why unresolved:** The study only evaluated binary classification tasks; the effectiveness of multi-perspective prompting and disaggregated label modeling remains untested for tasks with more than two classes.
- **What evidence would resolve it:** Replication of the methodology on multi-class subjective tasks with appropriate adaptations to the label space.

### Open Question 4
- **Question:** To what extent do these findings generalize across languages, particularly for low-resource languages?
- **Basis in paper:** [explicit] "Moreover, most datasets are monolingual, primarily in English, which limits the applicability of our findings in multilingual settings."
- **Why unresolved:** All experiments used English datasets; cultural and linguistic factors affecting disagreement patterns and LLM behavior in other languages remain unexplored.
- **What evidence would resolve it:** Cross-lingual experiments using translated or native-language perspectivist datasets, comparing disagreement modeling performance across languages.

## Limitations
- The exact prompt templates and multi-perspective instructions are not fully specified, limiting reproducibility.
- Only four open-source LLMs were tested, which may not be representative of the broader LLM landscape.
- The study focuses solely on in-context learning without exploring fine-tuning approaches that could potentially improve performance on modeling disagreement.

## Confidence
- **High Confidence:** LLMs struggle to capture the full spectrum of human judgments in few-shot settings.
- **Medium Confidence:** Multi-perspective priming is effective in zero-shot settings.
- **Low Confidence:** BM25 is superior to PLM-based retrieval for subjective tasks.

## Next Checks
1. **Prompt Engineering Validation:** Conduct a controlled experiment varying the multi-perspective instruction wording and role-playing prompts to determine the optimal format for eliciting diverse viewpoints from LLMs.
2. **Retrieval Method Comparison:** Design an ablation study comparing BM25, semantic similarity (PLM-based), and a hybrid approach on a broader range of subjective NLP tasks beyond hate speech and offensive language detection.
3. **Model Calibration Analysis:** Investigate whether instruction-tuned LLMs are inherently overconfident in their predictions by comparing their performance on disaggregated soft labels against base models and models specifically fine-tuned for uncertainty estimation.