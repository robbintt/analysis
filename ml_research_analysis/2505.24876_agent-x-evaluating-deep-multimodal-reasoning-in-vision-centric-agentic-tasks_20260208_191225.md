---
ver: rpa2
title: 'Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks'
arxiv_id: '2505.24876'
source_url: https://arxiv.org/abs/2505.24876
tags:
- reasoning
- tool
- query
- answer
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Agent-X, a benchmark designed to evaluate
  deep multimodal reasoning in vision-centric agents across real-world tasks. It features
  828 tasks spanning six environments, including images, videos, and text, requiring
  multi-step reasoning and tool use.
---

# Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks

## Quick Facts
- arXiv ID: 2505.24876
- Source URL: https://arxiv.org/abs/2505.24876
- Reference count: 40
- Primary result: Benchmark reveals state-of-the-art LMMs achieve <50% full-chain success on vision-centric agentic tasks

## Executive Summary
Agent-X is a benchmark designed to evaluate deep multimodal reasoning in vision-centric agents across real-world tasks. It features 828 tasks spanning six environments, including images, videos, and text, requiring multi-step reasoning and tool use. A fine-grained evaluation framework assesses correctness, logical coherence, and tool usage at each reasoning step. Experiments on 10 LMMs show that even top models like GPT-4o and Gemini-2.5 struggle, with full-chain success below 50%, highlighting key bottlenecks in reasoning and tool integration. The benchmark provides a rigorous, scalable testbed for advancing vision-centric agentic systems.

## Method Summary
Agent-X evaluates LMMs as vision-centric agents on 828 multimodal tasks across six environments. Agents receive natural language queries, visual inputs, and tool metadata, then generate JSON-structured reasoning traces with tool calls. The evaluation pipeline uses an LLM-based judge (GPT-4o or Qwen-14B) to score each reasoning step for grounding, tool precision, and logical coherence. Ten metrics are computed across three evaluation modes: Step-by-Step (grounding, tool precision/accuracy), Deep Reasoning (faithfulness, context, factual precision, semantic accuracy), and Outcome (goal accuracy, toolset accuracy). The benchmark requires agents to autonomously plan tool selection without explicit hints, reflecting real-world deployment conditions.

## Key Results
- State-of-the-art LMMs achieve less than 50% full-chain success on Agent-X tasks
- Open-source models (Qwen2.5-VL-7B, InternVL-1.0) show strong performance in tool usage but struggle with JSON formatting compliance
- GPT-4o demonstrates the highest Goal Accuracy but suffers from high "no action" rates due to conservative behavior
- Video tasks present the greatest challenge, with agents frequently exhibiting shallow reasoning by skipping frame-wise analysis

## Why This Works (Mechanism)

### Mechanism 1: Step-Level Decomposition with Tool Grounding
Breaking complex multimodal tasks into discrete, tool-mediated steps enables fine-grained failure diagnosis that end-to-end metrics miss. Each task is formalized as a tuple with explicit reasoning traces containing tool calls. The evaluation framework scores grounding, tool precision, and logical coherence at each step, isolating whether failures stem from visual misinterpretation, incorrect tool selection, or format violations.

### Mechanism 2: Implicit Tool Discovery via Natural Queries
Withholding explicit tool references in queries forces agents to autonomously plan tool selection, better reflecting real-world deployment conditions. Queries are crafted to avoid direct tool hints, requiring agents to map task semantics to available tool capabilities without guidance, revealing planning and tool-grounding limitations.

### Mechanism 3: Multi-Modal Context Integration Across Temporal Dimensions
Video-based tasks with frame-crossing reasoning expose temporal coherence failures that static image benchmarks cannot detect. 112 video tasks require agents to track entities, actions, and state changes across frames using tools iteratively, with error analysis showing shallow reasoning as a dominant failure mode.

## Foundational Learning

- **Tool-augmented agent architecture:** Why needed here - Agent-X assumes familiarity with LMM-as-controller patterns where language models invoke external tools via structured outputs. Quick check question: Can you explain how an LMM decides between calling OCR vs. SceneDescriber for an image query?

- **Chain-of-thought evaluation metrics:** Why needed here - The benchmark uses metrics like Faithfulness, Semantic Accuracy, and Grounding Score that require understanding what makes reasoning traces coherent vs. confabulated. Quick check question: What's the difference between a grounded reasoning step and a hallucinated one?

- **JSON-structured output parsing:** Why needed here - 26-44% of model errors are JSON format violations. Understanding schema validation is essential for implementing the evaluation pipeline. Quick check question: What happens when a model returns multiple tool calls in a single step?

## Architecture Onboarding

- **Component map:** Task Store (828 JSON instances) -> Tool Executor (14 Python functions) -> Evaluation Judges (GPT-4o/Qwen-14B) -> Metric Computer (per-step scoring to aggregate metrics)

- **Critical path:** Query → LMM receives (query + visual + tool_metadata) → generates reasoning_steps JSON → Tool Executor validates/executes → Judge scores each step → Aggregate metrics

- **Design tradeoffs:** Semi-automated generation (cost-efficient) vs. fully manual annotation (higher quality but unscalable); GPT-4o judge (powerful but proprietary) vs. Qwen-14B (reproducible but lower resolution); Excluding tool hints (realistic) vs. including them (isolates reasoning from planning failures)

- **Failure signatures:** Format errors (Invalid JSON arguments, 26-44% of errors); Planning errors (No action/no response, 0.2-17.6%); Reasoning errors (Misinterpreting visual content, 5.5-34.3%, incorrect spatial reasoning, 0.5-30.5%); Tool hallucination (Models invoke non-existent tools)

- **First 3 experiments:**
  1. Run baseline evaluation on Qwen2.5-VL-7B with full metric suite to establish reference scores and identify dominant error categories
  2. Isolate format compliance by forcing valid JSON schema and measure Gacc improvement to quantify "free" performance gain
  3. Ablate tool discovery difficulty by providing tool hints in queries vs. implicit-only, measuring the gap in Tp to quantify planning vs. execution failures

## Open Questions the Paper Calls Out

- How does agent performance on deep reasoning tasks degrade when extended to multilingual contexts beyond the English-only scope of Agent-X? The current dataset does not provide data to test linguistic robustness or cultural bias in reasoning.

- Can novel training methods like JSON-aware RLHF effectively mitigate the high rate of formatting errors observed in open-source models without inducing the "non-response" behavior seen in conservative models? The paper identifies the trade-off between aggressive and conservative models but does not demonstrate a solution that balances reliability and actionability.

- Does explicitly enforcing temporal reasoning in video tasks yield higher gains in Goal Accuracy compared to improvements in static visual grounding? While general reasoning scores correlate with success, the specific contribution of temporal reasoning mechanisms to video task success remains unquantified.

## Limitations
- Reliance on GPT-4o as primary evaluation judge introduces potential bias and limits reproducibility, with different judge models yielding notably different scores
- Step-level evaluation assumes reasoning failures are localized rather than cascading, which may not hold for tightly coupled reasoning chains
- Benchmark lacks substantial human validation, with only 50 samples annotated by human experts for comparison

## Confidence

- **High Confidence:** The empirical finding that even state-of-the-art models (GPT-4o, Gemini-2.5) achieve <50% full-chain success, demonstrating clear performance gaps in vision-centric agentic tasks
- **Medium Confidence:** The effectiveness of step-level decomposition for failure diagnosis, as the methodology is sound but the cascading error assumption requires further validation
- **Medium Confidence:** The implicit tool discovery mechanism's diagnostic value, as forcing autonomous planning may confound tool-use limitations with planning deficiencies

## Next Checks

1. **Judge Model Validation:** Run the full evaluation pipeline using three different judge models (GPT-4o, Qwen-14B, and Claude-3.5-Sonnet) on the same 50 human-annotated samples to quantify judge variability and establish inter-annotator agreement thresholds

2. **Cascading Error Analysis:** Design experiments that artificially introduce errors at different steps in the reasoning chain to measure how error propagation affects the validity of step-level scoring

3. **Tool Hint Ablation Study:** Systematically vary the presence/absence of tool hints across the entire benchmark to quantify the planning-vs-execution tradeoff and establish whether implicit discovery provides actionable diagnostic information beyond basic performance measurement