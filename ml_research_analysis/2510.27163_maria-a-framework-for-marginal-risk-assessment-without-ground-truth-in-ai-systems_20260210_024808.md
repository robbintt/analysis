---
ver: rpa2
title: 'MARIA: A Framework for Marginal Risk Assessment without Ground Truth in AI
  Systems'
arxiv_id: '2510.27163'
source_url: https://arxiv.org/abs/2510.27163
tags:
- risk
- evaluation
- system
- ground
- truth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MARIA, a framework for marginal risk assessment
  without ground truth when deploying AI systems to replace existing processes. The
  framework shifts evaluation from absolute risk to relative risk differences, addressing
  four key challenges: unknowable, delayed, expensive, and withheld ground truth.'
---

# MARIA: A Framework for Marginal Risk Assessment without Ground Truth in AI Systems

## Quick Facts
- arXiv ID: 2510.27163
- Source URL: https://arxiv.org/abs/2510.27163
- Authors: Jieshan Chen; Suyu Ma; Qinghua Lu; Sung Une Lee; Liming Zhu
- Reference count: 11
- Key outcome: Framework shifts evaluation from absolute risk to relative risk differences using three complementary dimensions: predictability, capability, and interaction dominance.

## Executive Summary
MARIA introduces a novel framework for assessing the marginal risk of deploying AI systems to replace existing processes when ground truth is unavailable. The framework addresses four key challenges—unknowable, delayed, expensive, and withheld ground truth—by shifting from absolute risk to relative risk comparison between new AI systems and baseline processes. MARIA uses three complementary evaluation dimensions with quantifiable metrics, enabling actionable, scalable, and assumption-aware risk assessment for responsible AI adoption in real-world workflows.

## Method Summary
MARIA employs a four-phase workflow to assess marginal risk without ground truth: (1) Setup phase defines scope, assumptions, prepares datasets, and selects metrics; (2) Analysis phase generates outputs and applies metric-specific evaluation; (3) Calibration phase validates metric reliability, adjusts for biases, and runs game-based probing; (4) Aggregation phase normalizes scales, aggregates results, and produces relative ordering with transparency reports. The framework demonstrated on document evaluation revealed that AI reviewers showed high self-consistency but lower agreement with human evaluators, indicating moderate performance shifts with mild fairness biases and new security considerations balanced against efficiency gains.

## Key Results
- AI reviewers showed high self-consistency but lower agreement with human evaluators compared to human-human pairs, indicating moderate performance shifts
- Case study identified mild fairness biases and new security considerations (e.g., prompt injection) balanced against efficiency gains
- The framework enables actionable, scalable, and assumption-aware risk assessment for responsible AI adoption in real-world workflows

## Why This Works (Mechanism)

### Mechanism 1: Relative Evaluation via Predictability Metrics
- Claim: A system that behaves more consistently under perturbation and repetition is assessed as lower marginal risk, without needing ground truth correctness labels
- Mechanism: Measures dispersion and invariance through self-consistency (repeated trials), input stability (paraphrase/order perturbations), and uncertainty governance (entropy/abstention behavior)
- Core assumption: Perturbations preserve semantic meaning; randomness is properly sampled; systems are independent
- Evidence anchors: [abstract], [Page 3, Section 3.3.1], related work on uncertainty-aware assessment (SUPER framework)
- Break condition: If the AI system has been trained on evaluation data from the baseline, or perturbations systematically change meaning, predictability metrics may give false confidence

### Mechanism 2: Capability Assessment via Distributional Comparison
- Claim: Functional competence can be assessed by comparing quantitative performance distributions between new and baseline systems on identical tasks
- Mechanism: Uses model-level benchmarks, agreement rates with human reviewers, throughput/latency metrics, and decision consistency measures
- Core assumption: Tasks, data, and resource settings are comparable; benchmarks remain uncontaminated
- Evidence anchors: [abstract], [Page 7, Section 4.5], importance sampling work (arXiv:2508.01203)
- Break condition: If baseline and new systems operate under fundamentally different constraints, capability comparisons become invalid

### Mechanism 3: Interaction Dominance via Game-Based Evaluation
- Claim: Structured symmetric games reveal behavioral robustness and emergent risks not captured by isolated task performance
- Mechanism: Persuasion duels measure belief shifts; prediction-surprise games reward accurate opponent prediction and novel counterarguments; compression-reconstruction games assess clarity under constraints
- Core assumption: Win rules are symmetric, computable, and relevant to assessed risk; systems have not trained on each other's strategies
- Evidence anchors: [abstract], [Page 4, Section 3.3.3], related game-based evaluation (clembench) for conversational agents
- Break condition: If game rules don't reflect actual deployment risks, or if one system has strategic knowledge of the other's behavior patterns, dominance metrics mislead

## Foundational Learning

Concept: **Marginal vs. Absolute Risk**
- Why needed here: The entire framework depends on shifting from "is this system correct?" to "is this system riskier than what we already accept?"
- Quick check question: Can you explain why comparing two systems without ground truth is more tractable than measuring absolute correctness?

Concept: **Ground Truth Failure Modes** (unknowable, delayed, expensive, withheld)
- Why needed here: Recognizing which mode applies determines whether MARIA's assumptions hold and which proxy metrics remain valid
- Quick check question: For a content moderation system, which ground truth failure mode(s) would apply?

Concept: **Assumption-Aware Evaluation**
- Why needed here: Each metric requires specific assumptions (independence, monotonicity, provenance). Violating these silently invalidates conclusions
- Quick check question: If your AI system was fine-tuned on outputs from your baseline human reviewers, which MARIA metrics become unreliable?

## Architecture Onboarding

- Component map: Scope definition -> Dataset preparation -> Metric selection -> Output generation -> Metric evaluation -> Alignment analysis -> Metric validation -> Bias adjustment -> Game probing -> Normalization -> Weighted aggregation -> Relative ordering -> Transparency report
- Critical path: Step 1 (assumptions) -> Step 3 (metric selection) -> Step 5 (evaluation execution) -> Step 14 (dominance declaration)
- Design tradeoffs: More metrics = higher confidence but more computational cost and potential for conflicting signals; Game-based evaluation = richer behavioral data but requires careful game design relevant to actual risks; Automated judges = scalable but may have systematic biases requiring calibration
- Failure signatures: High disagreement between predictability and capability metrics -> check assumption violations; Game outcomes that don't correlate with deployment observations -> game rules may not reflect real risks; Aggregation showing all systems "incomparable" -> need additional tests or relaxed dominance criteria
- First 3 experiments:
  1. Run self-consistency test on baseline and new system with 10 repeated trials each; compare variance distributions before proceeding
  2. Select 100 representative inputs; generate outputs from both systems; measure agreement rate and identify systematic divergence patterns
  3. On 20 high-disagreement cases, run controlled persuasion duel game; document which system's reasoning proves more robust and why

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the calibration and aggregation phases (Phases 3 and 4) impact the final risk assessment outcomes in a live deployment?
- Basis in paper: [explicit] The authors state the case study is "scoped to Phases 1 and 2... Later phases will be implemented in future extensions."
- Why unresolved: The empirical validation provided only covers setup, data preparation, and comparative analysis, leaving the proposed calibration techniques and aggregation methods untested
- What evidence would resolve it: A complete case study applying all four phases, specifically analyzing the stability of results after applying normalization and aggregation (Steps 12–15)

### Open Question 2
- Question: How sensitive are MARIA's agreement-based metrics to violations of the "provenance control" assumption?
- Basis in paper: [inferred] Section 3.2 explicitly assumes "provenance is controlled" and warns that agreement methods fail if systems trained on each other's data
- Why unresolved: In practice, ensuring that a new AI system has not been trained on data from the baseline system (or vice versa) is difficult, and the framework's robustness to this violation is unknown
- What evidence would resolve it: Simulation results showing the divergence of MARIA scores between independent systems versus systems with known data leakage or shared training corpora

### Open Question 3
- Question: Do "Interaction Dominance" metrics (e.g., persuasion duels) correlate with specific operational risks like bias amplification or security exposure?
- Basis in paper: [inferred] The framework introduces "Interaction Dominance" to capture behavioral robustness, but the case study findings (Section 4.5) rely primarily on predictability and capability metrics
- Why unresolved: While theoretically linked to "emergent risks," it remains unverified whether winning abstract games translates to safer or riskier behavior in specific domains like document evaluation
- What evidence would resolve it: Experiments measuring the correlation between game-based win rates and independent security/fairness benchmarks in a controlled environment

## Limitations
- Framework's effectiveness depends heavily on assumption validity (independence, meaning preservation under perturbation) which may not hold in practice
- Game-based dominance metrics lack validation against real-world outcomes
- Framework doesn't address how to handle cases where baseline and AI systems have fundamentally different error patterns

## Confidence

- High confidence: The three-dimensional framework structure and the mathematical validity of relative risk comparison without ground truth
- Medium confidence: The specific metric definitions and their implementation details (embedding choices, threshold values, game designs)
- Low confidence: The aggregation methodology for combining disparate metrics and the framework's generalizability beyond document evaluation

## Next Checks

1. Test framework on a task where ground truth is eventually available to validate whether MARIA's risk assessments align with eventual correctness patterns
2. Implement cross-validation by running MARIA assessments with perturbed assumptions (e.g., trained models, different perturbation types) to measure sensitivity
3. Compare MARIA outcomes against traditional validation approaches in controlled settings where both are feasible to identify systematic differences in risk assessment