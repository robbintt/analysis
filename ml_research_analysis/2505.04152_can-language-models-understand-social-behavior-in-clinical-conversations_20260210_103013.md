---
ver: rpa2
title: Can Language Models Understand Social Behavior in Clinical Conversations?
arxiv_id: '2505.04152'
source_url: https://arxiv.org/abs/2505.04152
tags:
- social
- patient
- signals
- provider
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores whether large language models (LLMs) can track
  social signals in clinical conversations without explicit training. The authors
  introduce a prompting-based pipeline called SocialLM that uses role specification,
  task description, and structured output formatting to guide models in detecting
  20 different social signals from patient-provider transcripts.
---

# Can Language Models Understand Social Behavior in Clinical Conversations?

## Quick Facts
- arXiv ID: 2505.04152
- Source URL: https://arxiv.org/abs/2505.04152
- Reference count: 40
- Language models can track social signals in clinical conversations without explicit training, achieving best balanced accuracy of 0.588 with LLaMA-FS-CoT configuration.

## Executive Summary
This paper investigates whether large language models (LLMs) can detect social signals in clinical conversations without domain-specific training. The authors introduce SocialLM, a prompting-based pipeline that uses role specification, task description, and structured output formatting to guide models in identifying 20 different social signals from patient-provider transcripts. They evaluate three models (FLAN-T5, Gemma2, and LLaMA) across multiple prompting strategies using a small annotated dataset. Results show that LLaMA generally outperforms others, with the Few-Shot with Chain-of-Thought configuration yielding the best accuracy (0.588 balanced accuracy). The study demonstrates that LLMs can capture social behaviors in clinical contexts, but effectiveness depends on model choice, prompt design, and conversation stage.

## Method Summary
The study uses the Establishing Focus (EF) dataset containing 91 visits and 508 three-minute transcript segments, transcribed via Whisper large-v3 and diarized with Pyannote. Labels are binarized from RIAS 1-6 scales (Type-I: threshold 3.5; Type-II: threshold 1.5). Three models are evaluated: FLAN-T5-Base (250M), Gemma2-2B, and LLaMA 3.1 405B (4-bit quantized). Four prompting strategies are tested: Zero-Shot, Few-Shot, Chain-of-Thought, and Few-Shot with Chain-of-Thought. Prompts include role specification, task description, scoring instructions, and output format. FLAN-T5 is excluded from CoT/FS-CoT due to generation failures; Gemma2 is excluded from FS-CoT for similar reasons.

## Key Results
- LLaMA 405B with Few-Shot and Chain-of-Thought configuration achieves the highest balanced accuracy of 0.588
- FLAN-T5-Base outperforms Gemma2-2B despite being 8x smaller, showing training paradigm matters more than size
- Model performance varies significantly by signal type and transcript segment, with Type-II signals showing higher prevalence but lower accuracy
- LLaMA shows demographic bias on 6 signals violating the 4/5ths rule for non-White patients

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Role specification and structured task framing activate latent social knowledge encoded in foundation models during pretraining.
- **Mechanism:** The base prompt assigns an expert role ("behavior analyst") and describes the annotation process, which constrains the model's output space toward clinically-relevant social reasoning rather than generic text completion. This appears to surface embeddings learned from prior exposure to conversational and affective text in training corpora.
- **Core assumption:** LLMs encode transferable representations of social dynamics from general pretraining data that can be redirected via role-based prompting without domain-specific fine-tuning.
- **Evidence anchors:**
  - [abstract]: "LLMs can track social signals in clinical conversations without explicit training... using a prompting-based pipeline called SocialLM that uses role specification, task description, and structured output formatting"
  - [section]: "We found that LLaMA tends to be the best overall, while smaller models like FLAN-T5 often outperform Gemma2... despite the size, the training conditions can largely enhance the abilities of models"
  - [corpus]: Related work shows LLMs can simulate social interactions and exhibit Theory of Mind reasoning (TactfulToM), suggesting encoded social representations are plausibly present but their transferability to clinical contexts is not directly established by corpus.
- **Break condition:** Performance degrades on signals lacking clear linguistic markers (e.g., irritation, sadness) and when transcripts contain minimal affective vocabulary (LIWC tone and positive emotion words significantly lower in "difficult" samples). Performance also drops at visit start/end segments with less reciprocal dialogue.

### Mechanism 2
- **Claim:** Combining few-shot examples with chain-of-thought reasoning yields the highest prediction accuracy by grounding models in task-specific patterns while enabling explicit reasoning traces.
- **Mechanism:** Few-shot examples provide in-context demonstrations of high vs. low signal expression, establishing decision boundaries. Chain-of-thought prompting elicits explicit reasoning, which improves calibration and allows models to synthesize task expectations with transcript-specific evidence.
- **Core assumption:** The model can reliably generate coherent reasoning traces and that these traces correlate with improved classification decisions on subjective social judgments.
- **Evidence anchors:**
  - [abstract]: "Few-Shot with Chain-of-Thought configuration yields the best accuracy (0.588 balanced accuracy)"
  - [section]: Table 3 shows FS-CoT achieves odds ratio of 1.431 vs Zero-Shot; "FS-CoT... outperforming the Zero-Shot baseline"
  - [corpus]: External validation is limited—corpus papers show LLMs struggle with nuanced social reasoning (TactfulToM on white lies), suggesting CoT benefits may not generalize across all social inference tasks.
- **Break condition:** Only LLaMA reliably produced coherent FS-CoT outputs; FLAN-T5 failed to generate combined answer-reasoning responses, and Gemma2 frequently returned transcript text instead of valid predictions under FS-CoT.

### Mechanism 3
- **Claim:** Model selection based on training paradigm and architecture, not just parameter count, determines social signal processing capability.
- **Mechanism:** FLAN-T5's instruction-following training provides stronger inductive bias for affective content (Type-I signals), while LLaMA's broader conversational pretraining enables better overall social reasoning. Gemma2's focus on factual reasoning appears less suited to subjective social inference despite larger size.
- **Core assumption:** Architectural and training differences create systematic variations in how models weight affective vs. factual reasoning pathways.
- **Evidence anchors:**
  - [abstract]: "while LLaMA tends to be the best overall, smaller models like FLAN-T5 often outperform Gemma2"
  - [section]: GLMM analysis shows Gemma2 underperforms FLAN-T5 by 17% (OR=0.831) despite being 8× larger; "performance is more dependent on the family/style of models compared to their size"
  - [corpus]: Weak external evidence—corpus papers discuss LLM social capabilities generally but do not provide comparative architecture-level analysis for clinical SSP.
- **Break condition:** LLaMA shows disparate impact across patient demographics (6 signals violated 4/5ths rule for non-White patients), while FLAN-T5 and Gemma2 did not, indicating architecture-specific bias patterns that may limit deployment in equity-sensitive contexts.

## Foundational Learning

- **Concept: Social Signal Processing (SSP) and RIAS Framework**
  - **Why needed here:** The paper operationalizes 20 social signals using Roter Interaction Analysis System global affect ratings. Understanding this coding scheme is essential for interpreting what the models are predicting and why certain signals are harder to detect.
  - **Quick check question:** Can you explain why Type-I signals (e.g., dominance, warmth) differ from Type-II signals (e.g., irritation, sadness) in terms of annotation methodology and class balance?

- **Concept: Balanced Accuracy for Imbalanced Classification**
  - **Why needed here:** Social signals exhibit severe class imbalance (e.g., provider irritation appears in <1% of samples). Standard accuracy is misleading; balanced accuracy (average of TPR and TNR) is used throughout.
  - **Quick check question:** If a model always predicts "not present" for provider irritation and achieves 95% accuracy, what would its balanced accuracy be?

- **Concept: Generalized Linear Mixed Models (GLMM) for Configuration Analysis**
  - **Why needed here:** The paper uses GLMMs with binomial distribution to isolate the effect of model, prompt, and task on prediction correctness while controlling for visit-level variability. This provides more robust inference than raw accuracy comparisons.
  - **Quick check question:** Why use random effects for "visit" and "task" rather than treating all predictions as independent samples?

## Architecture Onboarding

- **Component map:** Transcript Input → Preprocessing (Whisper + Pyannote diarization) → Segmentation (3-minute thin slices) → Prompt Assembly (Role + Task + Output Format + Examples) → LLM Inference (FLAN-T5 / Gemma2 / LLaMA locally) → Logit Extraction / Response Parsing → Binary Classification Output → Optional: Ensemble via Logistic Regression

- **Critical path:**
  1. Prompt design is the primary lever—role specification + task description must be adapted per model (LLaMA prefers numeric outputs; FLAN-T5 prefers yes/no)
  2. Model selection: Start with FLAN-T5 for resource-constrained settings; use LLaMA with FS-CoT for maximum accuracy
  3. Few-shot example selection: Include high/low signal examples from the same dataset context; for CoT, ensure model can generate coherent reasoning (test on small subset first)

- **Design tradeoffs:**
  - **LLaMA 405B + FS-CoT** → Highest accuracy (0.588), but requires significant compute and shows demographic bias on 6 signals
  - **FLAN-T5 + Zero-Shot** → Competitive on Type-I signals, no demographic disparity, fully local/efficient, but cannot use CoT
  - **Ensemble approach** → Best overall (0.606 balanced accuracy), but requires running multiple configurations and cross-validation

- **Failure signatures:**
  - Transcripts with low affective vocabulary (low LIWC tone, positive emotion, politeness scores) → systematically lower accuracy
  - Physical examination segments with minimal dialogue → model struggles without non-verbal context
  - Visit start/end segments → reduced performance due to less balanced turn-taking

- **First 3 experiments:**
  1. **Baseline validation:** Run FLAN-T5 Zero-Shot on a 50-sample subset; verify output format compliance and compute balanced accuracy per signal. Target: >0.5 on ≥12/20 signals.
  2. **Prompt sensitivity test:** Compare Zero-Shot vs. Few-Shot on 3 high-prevalence signals (provider dominance, patient engagement, provider warmth) using FLAN-T5. Measure if OR > 1.0 per Table 3 patterns.
  3. **Segment difficulty probe:** Sample 10 "hard" and 10 "easy" transcripts based on LIWC tone scores. Compare LLaMA FS-CoT accuracy between groups to validate linguistic difficulty predictors from Table 5.

## Open Questions the Paper Calls Out
None

## Limitations
- The study relies on a single clinical dataset (EF) with 81.3% White patients and 22 providers, limiting external validity and generalizability.
- Certain social signals (irritation, sadness, hostility) show inherently low prevalence (<1% for irritation), making balanced accuracy a better but still noisy metric.
- Prompt engineering fragility is evident as FLAN-T5 cannot generate CoT responses and Gemma2 echoes transcripts instead of predictions, indicating model-specific limitations.

## Confidence
- **High confidence:** LLMs can track social signals in clinical conversations without explicit training (empirical result supported by multiple configurations across three models).
- **Medium confidence:** Prompting-based approaches (role specification + few-shot + CoT) improve accuracy by surfacing latent social knowledge (mechanism supported but transfer assumptions not directly validated).
- **Low confidence:** LLaMA's superior performance stems from architectural advantages for social reasoning (inferred from performance gaps, but training data and architecture details not publicly disclosed).

## Next Checks
1. **Demographic bias validation:** Replicate the 4/5ths rule analysis across all models and signals on an independent clinical dataset with broader patient diversity to confirm architecture-specific bias patterns.
2. **Linguistic difficulty calibration:** Test model accuracy correlation with LIWC feature distributions (tone, positive emotion, politeness) on a held-out subset to validate linguistic difficulty predictors from Table 5.
3. **Cross-domain transfer test:** Apply the best-performing configuration (LLaMA-FS-CoT) to a non-clinical conversational dataset with similar social signal annotations to assess domain transferability.