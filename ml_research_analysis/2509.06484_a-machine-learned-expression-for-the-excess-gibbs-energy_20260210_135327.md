---
ver: rpa2
title: A machine-learned expression for the excess Gibbs energy
arxiv_id: '2509.06484'
source_url: https://arxiv.org/abs/2509.06484
tags:
- data
- hanna
- binary
- unifac
- mixtures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents HANNA, the first machine learning-based excess
  Gibbs energy model capable of predicting thermodynamic properties for liquid mixtures
  with arbitrary numbers of components. HANNA integrates physical laws as hard constraints
  within a flexible neural network architecture, enabling predictions solely from
  molecular structures.
---

# A machine-learned expression for the excess Gibbs energy

## Quick Facts
- arXiv ID: 2509.06484
- Source URL: https://arxiv.org/abs/2509.06484
- Reference count: 40
- Primary result: First ML model predicting excess Gibbs energy and activity coefficients for liquid mixtures with arbitrary components from molecular structures

## Executive Summary
This work introduces HANNA, a machine learning model that predicts excess Gibbs energy and activity coefficients for liquid mixtures directly from molecular structures. Unlike traditional models that rely on group-contribution methods, HANNA uses a neural network architecture that integrates thermodynamic constraints as hard architectural features. The model is trained end-to-end on experimental equilibrium data and demonstrates superior accuracy compared to established methods like modified UNIFAC, particularly for activity coefficients at infinite dilution and liquid-liquid equilibrium predictions.

## Method Summary
HANNA predicts excess Gibbs energy through a three-network architecture: ChemBERTa-2 embeddings are refined by an embedding network, then processed with temperature and composition data through mixture and property networks to produce binary interaction parameters. Activity coefficients are derived via automatic differentiation to ensure thermodynamic consistency. A novel surrogate solver enables training on liquid-liquid equilibrium data without iterative equilibrium calculations. The Muggianu geometric projection method allows parameter-free extrapolation to multi-component mixtures. The model is trained on extensive experimental data from the Dortmund Data Bank using a carefully designed loss function with thermodynamic constraints.

## Key Results
- HANNA halves the median MAE for activity coefficients at infinite dilution (0.1 vs 0.2 for modified UNIFAC)
- Achieves 87-98% accuracy in identifying liquid-liquid equilibrium miscibility gaps versus 80% for UNIFAC
- Successfully predicts complex phase behaviors including heteroazeotropes and systems with upper/lower critical solution temperatures
- Maintains accuracy across binary, ternary, and quaternary systems using only binary training data through geometric projection

## Why This Works (Mechanism)

### Mechanism 1
Embedding thermodynamic constraints as architectural hard constraints produces Gibbs-Duhem consistent predictions without post-hoc correction. Activity coefficients are derived via automatic differentiation of the predicted excess Gibbs energy rather than predicted directly. Permutation invariance is enforced through a Deep Set aggregation (sum over pairwise interactions), and identical components are lumped via a radial basis function similarity score that zeros their pairwise interaction term. Core assumption: The Gibbs-Duhem equation and summation conditions are sufficient thermodynamic priors. Evidence: Abstract states "integrating physical laws as hard constraints" and section 4.1 shows activity coefficients computed via Autograd. Break condition: If non-ideal pressure dependence or strong electrolyte effects become significant, current hard constraints are incomplete.

### Mechanism 2
A geometric projection (Muggianu) enables parameter-free extrapolation from binary to multi-component mixtures with strictly reduced composition space. N-component mole fractions are projected onto N(N-1)/2 binary subsystems via X_i^(ij) = 1 + x̃_i − x̃_j / 2. The multi-component gE is reconstructed as a weighted sum of binary interactions gE_ij evaluated at projected compositions. No new learnable parameters are introduced beyond binary subsystem predictions. Core assumption: Multi-component interactions are adequately approximated by pairwise contributions. Evidence: Abstract states "a geometric projection method... robust extrapolations to multi-component mixtures, without requiring additional parameters." Break condition: Systems with strong ternary interactions may exhibit degraded accuracy not recoverable by pairwise projections.

### Mechanism 3
A differentiable surrogate solver enables end-to-end incorporation of experimental LLE phase compositions during training. The surrogate solver is a feed-forward network trained on synthetic data from mod. UNIFAC to map discretized Δg_mix curves to phase compositions. During HANNA training, it provides gradients for LLE loss without iterative equilibrium solving. A Gibbs loss further penalizes failure to predict a miscibility gap when experimental LLE exists. Core assumption: The surrogate solver, trained on mod. UNIFAC data, generalizes sufficiently to HANNA's Δg_mix curves. Evidence: Abstract states "A novel surrogate solver developed in this work enabled the inclusion of liquid-liquid equilibrium data in the training process." Break condition: If HANNA's predicted Δg_mix curves deviate significantly from the mod. UNIFAC distribution, surrogate predictions may be unreliable.

## Foundational Learning

- **Concept**: Excess Gibbs energy (gE) and activity coefficients (γ)
  - Why needed: HANNA predicts gE, from which all activity coefficients are derived; understanding this hierarchy is essential for interpreting outputs
  - Quick check: Given gE(x,T) for a binary mixture, how would you compute ln(γ_1)?

- **Concept**: Phase equilibrium fundamentals (VLE, LLE, miscibility gaps)
  - Why needed: Training data includes VLE, ACI, and LLE; evaluation requires understanding when phase splits occur and how tie lines relate to Δg_mix curvature
  - Quick check: What thermodynamic condition on Δg_mix indicates the presence of a liquid-liquid miscibility gap?

- **Concept**: Permutation invariance in set-based neural architectures
  - Why needed: HANNA must yield identical predictions regardless of component ordering; Deep Set summation and RBF-based similarity enforce this structurally
  - Quick check: If you swap components i and j in the input, which architectural components guarantee unchanged gE output?

## Architecture Onboarding

- **Component map**: SMILES → ChemBERTa-2 → Embedding_Network → Mixture_Network → Property_Network → gE summation → Autograd for γ → (optional) surrogate solver for LLE loss
- **Critical path**: SMILES → ChemBERTa-2 → Embedding_Network → Mixture_Network → Property_Network → gE summation → Autograd for γ → (optional) surrogate solver for LLE loss
- **Design tradeoffs**:
  - Lipschitz regularization enforces smoothness but can underfit complex composition dependencies (w_Lips tuned via grid search; high values reduce flexibility)
  - Training only on binary data enables parameter-free multi-component extrapolation but limits ability to capture true ternary interactions
  - Surrogate solver avoids iterative equilibrium solving but introduces approximation error; trained on mod. UNIFAC, not HANNA-generated curves
- **Failure signatures**:
  - Implausible LLE predictions (e.g., two separate phase regions for methanol systems) linked to ChemBERTa-2 tokenization (CO → [C, O] conflates methane/water tokens)
  - Small composition-dependent fluctuations in near-ideal mixtures due to Lipschitz regularization tradeoffs
  - Degraded ACI accuracy for ternary systems vs. binary (median MAE increases by 0.08) potentially from projection method limitations
- **First 3 experiments**:
  1. Reproduce binary VLE predictions for 3-5 test systems from Fig. 2b using the open-source model; verify MAE_sys against reported boxplot statistics
  2. Ablate the Gibbs loss (set w_Gibbs = 0) and measure miscibility gap detection accuracy on held-out LLE systems to quantify its contribution
  3. For a ternary system with available experimental data, compare HANNA predictions using (a) Muggianu projection vs. (b) fitting UNIQUAC parameters to HANNA-predicted binary subsystems to isolate projection-specific error

## Open Questions the Paper Calls Out

- **Open Question 1**: Can incorporating excess enthalpy ($h^E$) and excess heat capacity ($c_p^E$) data into the training loss improve the model's temperature dependence and overall accuracy? The authors state these properties are easily accessible via automatic differentiation and "could be used for the training" to extend the model's capabilities.

- **Open Question 2**: Does including ternary experimental data in the training process mitigate the accuracy loss observed when extrapolating from binary to multi-component systems? The authors observe "deterioration of the predictions going from binary to ternary systems" and suggest "it could be considered to include ternary data in the fitting process."

- **Open Question 3**: Can the model's limitation regarding polymer systems be overcome by replacing the ChemBERTa-2 encoder with graph neural networks or alternative embeddings? The paper notes HANNA "should not be used for polymer systems" because the ChemBERTa-2 tokenizer fails on inputs exceeding 512 tokens, but suggests "learnable embeddings from a graph neural network" as a future alternative.

## Limitations

- Surrogate solver generalization: The LLE surrogate is trained on mod. UNIFAC synthetic data, which may not span the full distribution of HANNA's predicted Δg_mix curves, introducing potential systematic errors in phase composition predictions for strongly non-ideal systems.
- Ternary interaction neglect: The Muggianu projection assumes multi-component gE can be decomposed into binary interactions. Systems with strong three-body effects may show degraded accuracy not captured by the current architecture.
- Tokenization artifacts: ChemBERTa-2 tokenization issues (e.g., 'CO' → 'C'+'O') can lead to implausible predictions for certain systems like methanol, remaining a potential source of error for edge cases.

## Confidence

- **Thermodynamic consistency via hard constraints**: High confidence. The derivation of activity coefficients via automatic differentiation from gE, combined with permutation invariance and component similarity, ensures Gibbs-Duhem consistency structurally.
- **Parameter-free multi-component extrapolation**: High confidence. The Muggianu geometric projection method is well-established in CALPHAD, and comparison to UNIQUAC/NRTL extrapolation supports its efficacy.
- **Superior accuracy to modified UNIFAC**: Medium confidence. While HANNA shows improved median MAE for ACI (0.1 vs 0.2) and LLE miscibility gap detection (87-98% vs 80%), performance on ternary/quaternary systems is less explicitly validated.

## Next Checks

1. **Ablation of Gibbs loss**: Set w_Gibbs = 0 during training and measure the resulting change in LLE miscibility gap detection accuracy on a held-out test set to quantify the contribution of this specific constraint.

2. **Surrogate solver distribution check**: Compare the distribution of HANNA-generated Δg_mix curves for LLE systems against the mod. UNIFAC curves used to train the surrogate. Identify and analyze cases where HANNA's curves fall outside the surrogate's training distribution.

3. **Ternary interaction assessment**: For a set of ternary systems with known strong three-body interactions, compare HANNA's predictions using Muggianu projection against predictions from a model that explicitly includes ternary parameters (if available) or against experimental data to isolate the error introduced by the pairwise approximation.