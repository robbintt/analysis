---
ver: rpa2
title: Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language
  Models
arxiv_id: '2501.13428'
source_url: https://arxiv.org/abs/2501.13428
tags:
- attention
- lssar
- softmax
- length
- mechanism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two critical limitations of standard Softmax
  attention in large language models: numerical instability and degraded performance
  on long sequences. The authors propose a two-stage attention mechanism called LSSAR,
  consisting of a Length Scaled Softplus Attention (LSSA) normalisation stage and
  a sharpening re-weighting stage.'
---

# Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models

## Quick Facts
- **arXiv ID**: 2501.13428
- **Source URL**: https://arxiv.org/abs/2501.13428
- **Reference count**: 40
- **Primary result**: Softplus attention with re-weighting maintains constant validation loss at 16× training sequence length

## Executive Summary
This paper addresses two critical limitations of standard Softmax attention in large language models: numerical instability and degraded performance on long sequences. The authors propose a two-stage attention mechanism called LSSAR, consisting of a Length Scaled Softplus Attention (LSSA) normalisation stage and a sharpening re-weighting stage. LSSA replaces the exponential function with numerically stable Softplus and introduces a dynamic length scale factor based on invariance entropy, while the re-weighting stage applies a power transformation to sharpen attention peaks. Experiments demonstrate that LSSAR maintains nearly constant validation loss even at 16× the training sequence length, achieves superior performance on long-context retrieval tasks where standard attention fails completely, and translates to better downstream benchmark results.

## Method Summary
The proposed LSSAR mechanism operates in two stages. First, the Length Scaled Softplus Attention (LSSA) replaces Softmax's exponential function with Softplus for numerical stability and introduces a dynamic length scale factor based on invariance entropy. This length scale adjusts according to sequence length and model context. Second, a sharpening re-weighting stage applies a power transformation to the attention scores to enhance peak prominence. The combination addresses both numerical underflow issues that plague Softmax on long sequences and the fundamental scaling challenges that cause attention mechanisms to degrade as sequence length increases beyond training ranges.

## Key Results
- LSSAR maintains nearly constant validation loss even at 16× the training sequence length
- Achieves superior performance on long-context retrieval tasks where standard attention fails completely
- Enables a 109M parameter model to recover Newton's gravitational law from orbital trajectories through symbolic regression, while standard Softmax attention and trillion-parameter models fail

## Why This Works (Mechanism)
LSSAR addresses the fundamental scaling limitations of Softmax attention by replacing the exponential function with Softplus, which provides numerical stability through bounded growth rates. The length scale factor, derived from invariance entropy, dynamically adjusts attention normalization based on sequence length, preventing the concentration of attention mass on a few tokens that occurs with standard Softmax on long sequences. The re-weighting stage then sharpens attention peaks through a power transformation, allowing the model to maintain selective focus even when attention distributions become more uniform due to length scaling. This combination preserves the ability to identify relevant tokens across vastly extended contexts.

## Foundational Learning

**Softplus Function**
- Why needed: Provides numerically stable alternative to exponential in Softmax
- Quick check: Verify bounded growth rate compared to exponential

**Invariance Entropy**
- Why needed: Basis for dynamic length scale calculation
- Quick check: Confirm entropy remains bounded across sequence lengths

**Attention Sharpening**
- Why needed: Maintains peak prominence in attention distributions
- Quick check: Measure peak-to-average ratio after re-weighting

**Length Extrapolation**
- Why needed: Critical failure mode of standard attention
- Quick check: Track validation loss as sequence length increases beyond training

## Architecture Onboarding

**Component Map**
LSSA normalisation -> Attention sharpening re-weighting -> Output attention weights

**Critical Path**
Input queries/keys/values -> LSSA computation with length scaling -> Power re-weighting -> Final attention matrix

**Design Tradeoffs**
Numerical stability vs computational overhead (Softplus vs Softmax), dynamic adaptation vs fixed parameters (entropy-based scaling vs constant), attention focus vs coverage (sharpening vs uniform distribution)

**Failure Signatures**
Numerical underflow in attention scores, uniform attention distributions, degraded performance on long sequences, inability to identify relevant tokens in extended contexts

**First Experiments**
1. Test numerical stability by comparing attention score ranges for sequences of increasing length
2. Measure attention distribution uniformity before and after re-weighting
3. Evaluate length extrapolation by training on short sequences and testing on progressively longer ones

## Open Questions the Paper Calls Out
None

## Limitations
- Length scale factor determination relies on empirical validation rather than theoretical derivation
- Effectiveness on extremely long sequences (beyond 16× training length) remains untested
- Symbolic regression experiment represents a single case study rather than systematic evaluation

## Confidence

**High confidence**: Claims about numerical stability improvements of Softplus over Softmax (supported by mathematical analysis and controlled experiments)

**Medium confidence**: Claims about length extrapolation performance (demonstrated on specific benchmarks but limited to tested sequence length ranges)

**Medium confidence**: Claims about downstream benchmark improvements (shown on specific tasks but may not generalize to all NLP applications)

**Medium confidence**: Claims about LSSAR's inductive biases for physical world models (based on single symbolic regression example)

## Next Checks
1. Systematically evaluate LSSAR performance on sequence lengths exceeding 16× training length to identify any degradation thresholds or limitations in the scaling behavior

2. Test LSSAR across diverse transformer architectures (decoder-only, encoder-only, and encoder-decoder models) and parameter scales to verify generalizability beyond the tested 109M parameter model

3. Evaluate LSSAR's symbolic regression capabilities on multiple physical laws and scientific domains beyond gravitational law to assess whether the observed inductive bias is architecture-specific or domain-specific