---
ver: rpa2
title: 'Bias-Aware Machine Unlearning: Towards Fairer Vision Models via Controllable
  Forgetting'
arxiv_id: '2509.07456'
source_url: https://arxiv.org/abs/2509.07456
tags:
- bias
- unlearning
- machine
- fairness
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Bias-Aware Machine Unlearning (BAMU), a\
  \ post-hoc framework that selectively removes biased samples or spurious feature\
  \ dependencies to mitigate various forms of bias in vision models without requiring\
  \ full retraining. The approach adapts machine unlearning\u2014originally for privacy\u2014\
  to fairness by leveraging techniques such as Gradient Ascent, LoRA fine-tuning,\
  \ and Teacher-Student distillation to unlearn biased correlations."
---

# Bias-Aware Machine Unlearning: Towards Fairer Vision Models via Controllable Forgetting

## Quick Facts
- arXiv ID: 2509.07456
- Source URL: https://arxiv.org/abs/2509.07456
- Reference count: 29
- One-line primary result: Introduces BAMU, achieving up to 94.86% improvement in demographic parity while maintaining accuracy through controllable forgetting of biased samples.

## Executive Summary
This paper introduces Bias-Aware Machine Unlearning (BAMU), a post-hoc framework that selectively removes biased samples or spurious feature dependencies to mitigate various forms of bias in vision models without requiring full retraining. The approach adapts machine unlearning—originally for privacy—to fairness by leveraging techniques such as Gradient Ascent, LoRA fine-tuning, and Teacher-Student distillation to unlearn biased correlations. Evaluated across three datasets representing pose bias (CUB-200), class-specific bias (CIFAR-10), and attribute bias (CelebA), BAMU demonstrates substantial reductions in demographic disparities—up to 94.86% improvement in demographic parity on CUB-200, 30.28% on CIFAR-10, and 97.37% on CelebA—while maintaining model accuracy and achieving an average Co-BUM score of 0.62. The findings establish machine unlearning as a practical, efficient framework for enhancing fairness in deployed vision systems.

## Method Summary
BAMU adapts machine unlearning techniques to remove biased samples and spurious correlations from vision models. The framework partitions data into Forget Set (D_f) and Retain Set (D_r), then applies five unlearning methods: Hard Unlearning (retrain on D_r), Gradient Ascent (maximize loss on D_f), LoRA (low-rank adapters), SCRUB (teacher-student distillation), and FMD (influence functions). The approach is evaluated on ResNet-18 across three datasets with different bias types, using Co-BUM as a composite metric combining utility, fairness, quality, privacy, and efficiency measures.

## Key Results
- CUB-200: 94.86% improvement in demographic parity for pose bias
- CIFAR-10: 30.28% improvement for synthetic patch bias with 2.6% RA drop
- CelebA: 97.37% improvement for gender-attribute bias with 7.7% RA drop
- Average Co-BUM score of 0.62 across all methods and datasets
- Maintains model accuracy while substantially improving fairness metrics

## Why This Works (Mechanism)

### Mechanism 1: Gradient Ascent
- Claim: Disrupts entrenched spurious correlations by degrading predictive capacity on D_f
- Mechanism: Maximizes loss on biased subset while regularizing on retain set, pushing decision boundary away from spurious features
- Core assumption: Model relies on spurious correlation rather than semantic signal for D_f classification
- Evidence: GA update defined as θ_{t+1} ← θ_t + η∇_{θ_t}(L(D_f;θ_t) - L(D_r;θ_t)), achieved highest Co-BUM (0.77) on CelebA
- Break condition: Loss on D_f maximized until model loses generalization capability on D_r

### Mechanism 2: Low-Rank Adaptation (LoRA)
- Claim: Mitigates localized biases by constraining updates to low-dimensional subspace
- Mechanism: Freezes pre-trained weights, optimizes only injected low-rank matrices to suppress specific cues
- Core assumption: Bias is localized and can be "turned off" via low-rank perturbations without altering semantic features
- Evidence: LoRA highly effective for synthetic patch bias in CIFAR-10 (Co-BUM 0.50) but struggled with diffuse pose bias in CUB-200 (Co-BUM 0.11)
- Break condition: Bias is distributed or semantic rather than localized, requiring high-rank updates LoRA cannot capture

### Mechanism 3: Teacher-Student Distillation (SCRUB)
- Claim: Decouples biased knowledge from task knowledge by maximizing distributional divergence on D_f
- Mechanism: Student mimics teacher on D_r but diverges on D_f, forcing erasure of bias concept
- Core assumption: Teacher possesses stable representation of retain knowledge that can guide student
- Evidence: SCRUB can equalize prediction rates while retaining asymmetric errors if teacher has imbalanced error distributions
- Break condition: Teacher model encodes same bias as student, causing distillation to reinforce or under-correct bias

## Foundational Learning

### Concept: Spurious Correlations vs. Semantic Signals (x = s + b)
- Why needed: Entire framework rests on premise that models learn bias (b) instead of semantic signal (s)
- Quick check: Can you identify which part of the input x represents semantic signal s vs. bias b in CelebA "smiling" task?

### Concept: Demographic Parity (DP) vs. Equalized Odds (EO)
- Why needed: Paper highlights critical divergence where DP improves while EO lags
- Quick check: If model predicts "Smiling" for 50% of males and 50% of females (DP satisfied), can it still be biased according to Equalized Odds?

### Concept: Influence Functions
- Why needed: Required for Fast Model Debiasing (FMD) mechanism using Hessian approximation
- Quick check: In FMD context, what does Hessian vector product ∇_θ ℓ(x_i)^T H^{-1}_θ represent regarding training data?

## Architecture Onboarding

### Component map:
Input -> Bias Proxy Identification -> Partition (D_f, D_r) -> Backbone (ResNet-18) -> Unlearning Head (GA/LoRA/SCRUB/FMD) -> Co-BUM Evaluator

### Critical path:
1. Bias Proxy Identification: Define D_f using bounding box area, synthetic patches, or attributes
2. Unlearning Loop: Apply one of five update rules (GA, LoRA optimization, etc.)
3. Joint Evaluation: Calculate Co-BUM; if RA drops significantly relative to FA improvements, model is over-forgetting

### Design tradeoffs:
- Localized Bias (CIFAR): Use LoRA (precise, low-rank suppression)
- Diffuse Bias (CUB): Use FMD or Boundary-pushing methods (requires global rebalancing)
- Entrenched Bias (CelebA): Use GA (aggressive fairness) or Hard Unlearning (if compute allows)
- Efficiency vs. Fairness: Hard unlearning is gold standard but computationally expensive

### Failure signatures:
- High RA, Low TA, Poor EO: Indicates shortcut learning persists or adapters failed to disentangle features
- Negative DP Improvement: Indicates model learned reverse shortcut (absence of patch predicts class)
- High MIA Score: Unlearning insufficient to remove trace information of forget set

### First 3 experiments:
1. Baseline Verification: Train ResNet-18 on biased CIFAR-10, verify Grad-CAM highlights patch
2. LoRA Unlearning: Apply LoRA (Rank=8) to final block on CIFAR-10, confirm Grad-CAM shifts to object
3. Co-BUM Calculation: Run unlearned model on validation set to compute RA-TA gap and DP/EO deltas

## Open Questions the Paper Calls Out

### Open Question 1: Adaptive Unlearning Frameworks
- Question: Can adaptive unlearning frameworks be developed to automatically detect bias topology and select optimal forgetting strategy?
- Basis: Future Work section calls for developing adaptive methods that adjust to bias topology
- Why unresolved: Current work manually selects methods for specific bias types without automation
- What evidence would resolve: Unified system identifying bias characteristics and dynamically switching methods

### Open Question 2: Multi-bias Disentanglement
- Question: How can machine unlearning techniques be extended to disentangle and remove multiple overlapping biases?
- Basis: Authors list "extending to multi-bias disentanglement" as promising research direction
- Why unresolved: Study isolates single bias types per dataset, whereas real-world models contain compound, intersecting biases
- What evidence would resolve: Experiments on datasets with known multiple spurious correlations demonstrating selective removal

### Open Question 3: Fairness-Aware Teacher Models
- Question: Does utilizing a fairness-aware teacher model in Teacher-Student distillation prevent transfer of asymmetric error rates?
- Basis: Authors observe distillation can retain asymmetric errors and state it "should only be applied with fairness-aware teachers"
- Why unresolved: Paper identifies EO lag failure mode but leaves proposed solution untested
- What evidence would resolve: Empirical validation on CelebA showing debiased teacher eliminates EO lag

## Limitations

- Method assumes bias proxies are reliably identifiable and meaningful, but real-world bias sources are often subtle and interdependent
- Co-BUM metric aggregates potentially conflicting signals (improved DP but degraded EO), which may mask persistent fairness gaps
- Unlearning methods' effectiveness varies significantly across bias types—LoRA excels at localized biases but fails on diffuse ones

## Confidence

**High Confidence**: Core mechanism of BAMU—using unlearning to remove biased correlations—is well-supported by experimental results across three datasets with consistent improvements in demographic parity.

**Medium Confidence**: Claim that BAMU maintains model accuracy while improving fairness is partially supported, with modest average RA drops but significant variation by method and dataset.

**Low Confidence**: Assertion that machine unlearning is "practical and efficient" for deployed systems is questionable given computational costs of some methods and sensitivity to underspecified hyperparameters.

## Next Checks

1. **Multi-source Bias Testing**: Evaluate BAMU on datasets with overlapping or interdependent biases (e.g., gender and pose) to assess whether framework can disentangle multiple bias factors simultaneously.

2. **Long-term Stability Analysis**: Test whether unlearned models maintain fairness and accuracy over extended periods or after incremental retraining, as unlearning may create unstable decision boundaries.

3. **Real-world Deployment Simulation**: Apply BAMU to a pre-trained, deployed model (e.g., commercial face recognition system) to measure practical efficacy, computational overhead, and potential for unintended consequences in production environments.