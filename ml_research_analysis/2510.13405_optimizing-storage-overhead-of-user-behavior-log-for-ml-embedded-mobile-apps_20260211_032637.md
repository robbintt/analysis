---
ver: rpa2
title: Optimizing Storage Overhead of User Behavior Log for ML-embedded Mobile Apps
arxiv_id: '2510.13405'
source_url: https://arxiv.org/abs/2510.13405
tags:
- uni000003ec
- uni0000011e
- uni00000003
- behavior
- uni00000358
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the storage overhead of user behavior logs
  in ML-embedded mobile apps. As mobile apps increasingly integrate ML models, logging
  user behavior data for feature computation leads to significant storage costs, impacting
  app performance and user retention.
---

# Optimizing Storage Overhead of User Behavior Log for ML-embedded Mobile Apps

## Quick Facts
- **arXiv ID:** 2510.13405
- **Source URL:** https://arxiv.org/abs/2510.13405
- **Reference count:** 40
- **Key result:** Proposed AdaLog reduces behavior log storage by 19-44% with minimal overhead (2s latency, 15MB memory)

## Executive Summary
This paper addresses the storage overhead challenge in ML-embedded mobile apps caused by logging user behavior data for on-device feature computation. As mobile apps increasingly integrate ML models, the resulting behavior logs consume significant storage space, degrading app performance and user retention. The authors propose AdaLog, a lightweight system that achieves substantial storage savings by identifying and merging redundant event rows across features (19-44% reduction) while maintaining minimal system overhead and preserving model inference accuracy.

## Method Summary
AdaLog optimizes storage through two complementary techniques: feature-level data merging and behavior-level log splitting. The system identifies redundant event rows derived from the same user behavior but consumed by different ML features, then merges them using a hierarchical algorithm that reduces the NP-hard hypergraph matching problem to iterative 2D-graph solving. For behavior-level optimization, AdaLog employs a Virtually Hashed Attribute Name (VHAN) design that maps physical attribute names to generic virtual IDs, allowing heterogeneous behaviors to share dense log files. An incremental update mechanism minimizes I/O operations when adapting to changing behavior patterns.

## Key Results
- Achieves 19-44% reduction in behavior log storage size
- Maintains minimal system overhead: 2 seconds latency and 15 MB memory usage
- Preserves model inference accuracy and latency
- Reduces required log files from ~250 to ~20 through VHAN design

## Why This Works (Mechanism)

### Mechanism 1: Feature-level Data Merging
- **Claim:** Storage overhead is reduced by identifying and merging redundant event rows derived from the same user behavior but consumed by different ML features, provided the trade-off with index inflation is managed.
- **Mechanism:** The system models features as nodes in a hypergraph where hyperedges represent potential merging groups weighted by storage savings. To solve the NP-hard global optimization problem, the authors propose a hierarchical merging algorithm. This algorithm iteratively reduces the problem to a 2D graph and uses the Blossom algorithm to merge pairs of feature groups that yield the highest storage reduction, stopping when gains become negative.
- **Core assumption:** Assumes that the optimal configuration can be approximated through iterative pairwise merging rather than solving the global hypergraph problem, and that feature redundancy patterns remain relatively stable between optimization runs.
- **Evidence anchors:**
  - [abstract] "formulates the elimination of feature-level redundant data as a maximum weighted matching problem in hypergraphs, and proposes a hierarchical algorithm."
  - [page 10] "deriving the optimal feature-level data merging strategy is equivalent to solving the maximum weighted matching problem... This optimization problem is NP-hard... decompose the solving process into multiple iterations."
- **Break condition:** If the redundancy across features is low (e.g., features require completely disjoint attribute sets), the overhead of the index structure may exceed the storage savings from merging rows.

### Mechanism 2: Behavior-level Log Splitting (VHAN)
- **Claim:** Storage sparsity is minimized by decoupling physical storage formats from the semantic meaning of attributes, enabling heterogeneous behaviors to share dense log files.
- **Mechanism:** The Virtually Hashed Attribute Name (VHAN) design maps physical attribute names (e.g., "Duration", "Genre") to generic virtual IDs via a dictionary. This allows the system to cluster behavior logs based on attribute count rather than attribute type, reducing the number of fragmented log files from ~250 to ~20 while filling null columns.
- **Core assumption:** Assumes that the underlying storage engine (e.g., SQLite) can efficiently handle mixed data types in unified columns via these virtual mappings without significant casting overhead.
- **Evidence anchors:**
  - [abstract] "employs a virtually hashed attribute design to distribute heterogeneous behaviors into a few log files with physically dense storage."
  - [page 12] "decouple the storage of attribute values from their physical names... reduces the required number of log files from the number of behavior types (≈ 250) to the number of possible attribute counts (≈ 20)."
- **Break condition:** If distinct behaviors have vastly different data types for the same virtual attribute slot, serialization overhead or read errors may occur.

### Mechanism 3: Incremental Update Mechanism
- **Claim:** The system maintains scalability by avoiding full log reconstruction during configuration updates, instead incrementally updating only the affected data.
- **Mechanism:** The Incremental Update Mechanism maps the old storage configuration to the new one using maximum weighted matching on a bipartite graph, where edge weights represent data overlap. It uses a shrink-and-expand strategy: reusing existing rows where overlap exists (shrink) and inserting new rows only where necessary (expand).
- **Core assumption:** Assumes that user behavior patterns and optimal configurations evolve slowly (incrementally) rather than chaotically, allowing for high data reuse.
- **Evidence anchors:**
  - [page 13] "designs an incremental update mechanism to minimize the I/O operations needed for adapting outdated behavior log."
  - [page 14] "model the optimal adaption process as a maximum weighted matching problem in a bipartite graph."
- **Break condition:** If configuration changes are drastic (e.g., entirely new feature sets), the incremental update effectively becomes a full reconstruction, negating efficiency benefits.

## Foundational Learning

- **Concept: Hypergraph vs. 2D Graph Matching**
  - **Why needed here:** The paper models feature merging as an NP-hard hypergraph problem but solves it by reducing it to a 2D graph problem. Understanding this reduction is key to grasping why the "Hierarchical Merging Algorithm" is computationally feasible on mobile devices.
  - **Quick check question:** Why does the author reduce the hypergraph to a 2D graph for each iteration instead of solving the hypergraph directly?

- **Concept: Database Sparsity & Null Values**
  - **Why needed here:** The paper identifies "sparse storage caused by storing behaviors with heterogeneous attribute descriptions" as a primary inefficiency. Understanding how nulls consume space in columnar or SQLite storage is essential for evaluating the VHAN solution.
  - **Quick check question:** How does storing events with heterogeneous attributes in a single file lead to wasted space, even if null values are represented efficiently?

- **Concept: Index Inflation**
  - **Why needed here:** Mechanism 1 relies on the trade-off between reducing data rows and increasing index size. The optimization goal is to minimize total storage, not just row count.
  - **Quick check question:** If you merge event rows, why might the size of the index structure increase?

## Architecture Onboarding

- **Component map:**
  Profiler -> Config Generator -> Config Updater -> Shim Layer

- **Critical path:**
  1. Profiling: Monitor user behaviors to generate metadata
  2. Configuration Generation: Run the hierarchical merging algorithm to group features and assign VHAN mappings
  3. Scheduling: Match old config to new config to create the "shrink-and-expand" plan
  4. Execution: Execute the plan to write/update the optimized SQLite database

- **Design tradeoffs:**
  - Redundancy vs. Index Overhead: Merging more rows reduces data size but increases the number of FilterID columns and index entries, potentially inflating storage if not calculated correctly (Eq. 1 on Page 10)
  - Fragmentation vs. Sparsity: Creating separate files for every behavior type minimizes sparsity but maximizes metadata overhead; VHAN balances this by grouping by attribute count

- **Failure signatures:**
  - Negative Compression: If Index_Size(g_i) grows faster than Data_Size(g_i) reduction, the log size will actually increase
  - Configuration Drift: If the periodic update interval is too long (e.g., > 4 days), the storage configuration becomes suboptimal for current user patterns (Page 22)

- **First 3 experiments:**
  1. Redundancy Baseline: Profile a raw dataset to measure the ratio of redundant event rows to unique behavior events (target: ~67% redundancy as per Page 7)
  2. Index Cost Analysis: Implement the storage cost function (Page 10) to verify if a proposed merging of two feature groups yields a net positive storage saving
  3. Incremental Update Benchmark: Measure I/O latency for full reconstruction vs. the bipartite matching update on a dataset simulating a 2-day delta

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the hierarchical merging algorithm be adapted to efficiently identify high-order redundancies (k > 2) to approach global optimality without sacrificing polynomial time complexity?
- **Basis in paper:** [explicit] Section 3.2 acknowledges that the pairwise hierarchical algorithm may be theoretically sub-optimal if the maximum redundancy is achievable only by merging an odd number of groups (e.g., 3, 5), though the authors claim these cases are rare.
- **Why unresolved:** The current algorithm reduces the NP-hard hypergraph matching to a series of 2D-graph matchings (Blossom algorithm) for tractability, effectively ignoring complex multi-way intersections that might yield higher storage savings.
- **What evidence would resolve it:** A theoretical analysis of the approximation ratio or an empirical comparison against an optimal (but computationally expensive) brute-force solver on synthetic datasets designed to maximize high-order groupings.

### Open Question 2
- **Question:** Does the observed redundancy and storage efficiency of AdaLog generalize to mobile applications dominated by Large Language Models (LLMs) which have distinct feature engineering requirements?
- **Basis in paper:** [explicit] Section 4.1 notes that advanced mobile intelligence with LLMs will require "much more user behavior data" and different fine-tuning/inference patterns, potentially altering the redundancy landscape.
- **Why unresolved:** The evaluation relies on traditional recommendation and ranking models. LLMs on mobile devices may utilize features with different granularities or temporal dependencies, changing the "feature-level correlation" AdaLog exploits.
- **What evidence would resolve it:** An evaluation of AdaLog on on-device LLM workloads to measure compression ratios and latency impacts specific to LLM context windows and fine-tuning data.

### Open Question 3
- **Question:** Is clustering behaviors by attribute count the optimal strategy for the Virtually Hashed Attribute Name (VHAN) design when considering query latency alongside storage savings?
- **Basis in paper:** [inferred] Section 3.3 states AdaLog clusters behaviors by "cardinality of attribute sets" (attribute count) to balance file count and sparsity. It does not explore if clustering by access frequency or semantic similarity might yield better I/O performance.
- **Why unresolved:** Grouping by attribute count minimizes empty columns (sparsity), but may store frequently accessed "hot" behaviors in the same files as "cold" behaviors, potentially degrading cache performance during feature computation.
- **What evidence would resolve it:** An ablation study comparing the current count-based splitting against access-frequency-based splitting, measuring both storage overhead and feature computation wall-clock time.

## Limitations
- Evaluation relies on proprietary data from ByteDance apps, limiting external validation
- VHAN approach's efficiency with heterogeneous data types unverified across different storage engines
- Compression ratios and system overhead measured without exploring potential impacts on feature computation accuracy

## Confidence
- **High confidence:** Storage overhead reduction mechanism (19-44% compression confirmed through evaluation metrics)
- **Medium confidence:** Scalability claims (2-second latency, 15MB memory) - based on controlled evaluation
- **Medium confidence:** Incremental update mechanism - theoretical foundation sound but limited empirical validation

## Next Checks
1. **Data Schema Reconstruction:** Generate synthetic datasets with varying levels of feature redundancy (10-80%) to test algorithm performance across different overlap scenarios
2. **Index Overhead Analysis:** Implement the storage cost function (Eq. 1) and validate that merging decisions consistently produce net positive savings across diverse feature groupings
3. **Incremental Update Benchmark:** Compare full reconstruction vs. incremental update performance on datasets simulating 1-day, 2-day, and 7-day configuration deltas to verify scalability claims