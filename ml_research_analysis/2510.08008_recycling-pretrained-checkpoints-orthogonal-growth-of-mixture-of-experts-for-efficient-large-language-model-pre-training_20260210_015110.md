---
ver: rpa2
title: 'Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts
  for Efficient Large Language Model Pre-Training'
arxiv_id: '2510.08008'
source_url: https://arxiv.org/abs/2510.08008
tags:
- growth
- training
- average
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work proposes a method to efficiently scale large language
  models by recycling pretrained checkpoints. It introduces two orthogonal growth
  strategies for MoE models: depth growth via interpositional layer copying and width
  growth via expert duplication with noise injection.'
---

# Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training

## Quick Facts
- arXiv ID: 2510.08008
- Source URL: https://arxiv.org/abs/2510.08008
- Reference count: 40
- Primary result: 10.66% accuracy gain over training from scratch when scaling 17B→70B MoE under same compute budget.

## Executive Summary
This work introduces a method to efficiently scale large language models by recycling pretrained checkpoints through orthogonal growth strategies. It proposes two complementary approaches for Mixture-of-Experts (MoE) models: depth growth via interpositional layer copying and width growth via expert duplication with noise injection. The method demonstrates that interpositional depth growth preserves learned structural properties of converged models, while noise injection improves expert specialization during width expansion. Experiments show that recycling checkpoints achieves a 10.66% accuracy gain over training from scratch under the same compute budget, validating the effectiveness of this approach for sustainable LLM development.

## Method Summary
The method recycles converged MoE checkpoints by applying orthogonal growth strategies: depth growth and width growth. For depth growth, layers are duplicated in-place (interposition) rather than appended (stacking), preserving the characteristic layer-wise weight norm distribution. For width growth, experts and router logits are duplicated with Gaussian noise injection (α≈0.01) to break symmetry and encourage specialization. The approach is validated through a scaling path from 17B (28L, 96E) to 35B (54L) to 70B (192E, top-12) parameters, using AdamW optimization, RMSNorm, and RoPE. The method achieves superior performance compared to training from scratch under matched compute budgets.

## Key Results
- Interpositional depth growth outperforms stacking for converged models, preserving layer-wise weight norm trends.
- Noise injection (α=0.01) during width growth improves expert specialization compared to direct copying.
- Scaling from 17B to 70B parameters achieves 10.66% accuracy gain over training from scratch under same compute budget.
- Final accuracy shows strong positive correlation with the "sunk cost" (FLOPs) of the base checkpoint.

## Why This Works (Mechanism)

### Mechanism 1: Interpositional Depth Growth Preserves Learned Structure
Converged LLMs exhibit a characteristic layer-wise weight norm distribution where norms increase with depth. Interposition ($l_1, l_1, l_2, l_2...$) preserves this gradual increasing trend locally, whereas stacking ($l_1 \dots l_n, l_1 \dots l_n$) places low-norm early layers after high-norm deep layers, disrupting the functional hierarchy. The core assumption is that this structural signature should be maintained during expansion. Evidence shows interposition maintains norm trends and achieves better loss/accuracy than stacking. Break condition: If the base model is not converged or uses post-layer normalization, the structural preservation benefit may diminish or cause instability.

### Mechanism 2: Noise Injection Breaks Expert Symmetry
Duplicating experts with small Gaussian noise ($\alpha \approx 0.01$) yields better downstream performance than direct copying. Direct copying creates identical experts, leading to redundant gradients and poor specialization. Injecting noise breaks this symmetry, forcing the routing mechanism to diverge and assign distinct roles to the new experts without discarding pre-existing knowledge. The core assumption is that small perturbation destabilizes the router's preference just enough to encourage load balancing and specialization. Evidence demonstrates noise std=0.01/0.05 outperforms no noise in downstream accuracy. Break condition: Excessive noise ($\alpha > 0.1$) degrades performance by corrupting pre-trained features.

### Mechanism 3: Sunk Cost Correlation and Growth Timing
The final accuracy of the grown model is strongly positively correlated with the "sunk cost" (FLOPs) of the base checkpoint. A base checkpoint trained for longer provides a better optimization basin, allowing the larger model to start from a sophisticated representation rather than random initialization. The core assumption is that the training budget for the continued phase is sufficient to stabilize the expanded model. Evidence shows a positive correlation between start checkpoint FLOPs and final accuracy. Break condition: If the base model is over-trained into a sharp local minimum and the continued learning rate is not tuned, the correlation may weaken.

## Foundational Learning

- **Concept: Layer-wise Weight Norm Scaling**
  - Why needed: The paper justifies its "interposition" strategy by analyzing norm trends of layers. You must understand that early layers often have smaller norms (embedding proximity) while deeper layers have larger norms (abstract manipulation) to see why "stacking" violates this geometry.
  - Quick check: Why would placing a low-norm "early" layer immediately after a high-norm "late" layer disrupt signal propagation?

- **Concept: Function-Preserving Transformations**
  - Why needed: The paper contrasts "depth growth" (function-altering) with "width growth" (function-preserving). Understanding how Pre-LN architectures stabilize output distributions helps explain why width growth is more stable immediately post-growth.
  - Quick check: Does adding an expert with zero noise change the output of an MoE layer if the router weights are also copied identically?

- **Concept: Expert Load Balancing in MoEs**
  - Why needed: The noise injection mechanism relies on the intuition that perfect symmetry harms specialization. You need to grasp that routers seek to minimize auxiliary loss, which requires experts to diverge.
  - Quick check: If two experts are identical, how does the router decide which to use, and how does noise solve this?

## Architecture Onboarding

- **Component map**: Input: Converged MoE Checkpoint → Depth Growth Module (Interpositional layer duplication) → Width Growth Module (Expert duplication + noise injection) → Continued Training
- **Critical path**: Recycle converged checkpoint → Apply interpositional depth growth → Train to convergence → Apply width growth with noise injection → Train to convergence
- **Design tradeoffs**: Interposition preserves structure but doubles layers at each position; stacking is simpler but disrupts norm patterns. Noise injection requires tuning α to balance specialization vs corruption.
- **Failure signatures**: Using stacking instead of interposition → disrupted weight norm trends and degraded accuracy; skipping noise injection → redundant experts and poor load balancing; growing from over-trained checkpoint without LR adjustment → reduced marginal gains.
- **Three first experiments**: 1) Implement 3B MoE and train to convergence; 2) Compare interpositional vs stacking depth growth on 3B→6B; 3) Test noise injection with varying α values during width growth.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal learning rate schedule for continued training when initiating growth from a base model checkpoint that is already in the learning rate annealing phase?
- Basis: Section 4.1 notes marginal performance gains diminish for late-stage checkpoints with constant learning rate, suggesting need to "carefully tune the learning rate."
- Why unresolved: The study used constant learning rate for controlled comparisons, leaving schedule tuning unexplored.
- What evidence would resolve: Comparative study of growth performance on late-stage checkpoints using various learning rate schedules (e.g., re-warming vs. continued decay).

### Open Question 2
- Question: Is there a precise scaling law or adaptive method for determining the optimal magnitude of Gaussian noise injected during width growth?
- Basis: Section 3.2 states while small noise improves specialization, "excessive noise may be harmful," and authors fixed value (α=0.01) without claiming optimality.
- Why unresolved: Paper tested discrete values (0, 0.01, 0.05, 0.1) but did not derive rule relating noise scale to model size or training progress.
- What evidence would resolve: Systematic sweep of noise injection parameters across varying model widths and training stages to identify consistent optimum.

### Open Question 3
- Question: Does the observed superiority of interposition method over stacking hold for non-converged or early-stage checkpoints, and how does this interact with layer-wise weight norm hypothesis?
- Basis: Authors hypothesize interposition preserves "upward trend" in weight norms seen in converged models, challenging previous works advocating stacking for early-stage models.
- Why unresolved: Paper focuses on well-converged checkpoints to maximize sunk cost utility, leaving boundary conditions less defined.
- What evidence would resolve: Direct comparison of interposition vs stacking across gradient of checkpoint convergence levels (early training to fully converged).

## Limitations

- Data and training configuration gaps: Exact mixture ratios, shuffling strategies, and epoch schedules for 1T-token corpus unspecified, preventing exact replication of convergence points.
- Scale-specific implementation details: 17B→70B scaling involves implicit engineering decisions around pipeline parallelism and memory management not documented.
- Break conditions and generalization: Unclear how interpositional growth behaves with Post-LN architectures or when growing beyond doubling depth.

## Confidence

- **High Confidence**: Positive correlation between base checkpoint FLOPs and final accuracy; superiority of interpositional depth growth for preserving layer-wise weight norm trends; necessity of noise injection for breaking expert symmetry.
- **Medium Confidence**: Exact magnitude of 10.66% accuracy gain dependent on unreported data configurations; specific noise standard deviation (α=0.01) may not be optimal; general robustness across different MoE architectures.
- **Low Confidence**: Performance with non-converged checkpoints; behavior with Post-LN architectures; scaling beyond tested parameter ranges.

## Next Checks

1. **Implement and validate the interpositional vs stacking comparison on a small-scale 3B MoE** trained to convergence. Monitor layer-wise weight norm distributions and downstream accuracy to confirm the structural preservation benefit claimed in the paper.

2. **Test the noise injection mechanism with varying α values (0, 0.01, 0.05, 0.1)** on a width growth experiment (e.g., 3B→6B). Measure downstream accuracy and analyze expert activation histograms to quantify the impact on load balancing and specialization.

3. **Recreate the sunk cost correlation analysis** by growing models from checkpoints saved at different training stages (e.g., early constant LR vs late annealing). Compare final accuracies under the same continued compute budget to validate the dependency on the base model's optimization state.