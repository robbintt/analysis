---
ver: rpa2
title: LLM Analysis of 150+ years of German Parliamentary Debates on Migration Reveals
  Shift from Post-War Solidarity to Anti-Solidarity in the Last Decade
arxiv_id: '2509.07274'
source_url: https://arxiv.org/abs/2509.07274
tags:
- solidarity
- anti-solidarity
- group-based
- compassionate
- exchange-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) were evaluated as tools for annotating
  solidarity and anti-solidarity in German parliamentary speeches. A fine-grained
  annotation scheme based on Thijssen (2012) was applied to classify stances towards
  migrants across 155 years of debate.
---

# LLM Analysis of 150+ years of German Parliamentary Debates on Migration Reveals Shift from Post-War Solidarity to Anti-Solidarity in the Last Decade

## Quick Facts
- **arXiv ID:** 2509.07274
- **Source URL:** https://arxiv.org/abs/2509.07274
- **Reference count:** 40
- **Primary result:** LLM analysis reveals declining solidarity and rising anti-solidarity in German migration discourse since 2015, with sharp shift toward group-based anti-solidarity in mainstream parties after 2017

## Executive Summary
This study evaluates large language models as tools for annotating solidarity and anti-solidarity in German parliamentary speeches across 155 years of debate. Using a fine-grained annotation scheme based on Thijssen (2012), the researchers applied Llama-3.3-70B to classify stances towards migrants, achieving near-human performance in high-level classification and competitive results in fine-grained subtype detection. The temporal analysis reveals high post-war solidarity, declining solidarity and rising anti-solidarity since 2015, and a sharp shift toward group-based anti-solidarity in mainstream parties after 2017. Systematic error analysis shows models sometimes underestimate solidarity, especially for subtle or cited expressions.

## Method Summary
The researchers filtered 63k+ sentences from the DeuParl corpus using keyword lists (32 for migrants, 18 for women) and extracted 3-sentence context windows. They employed Llama-3.3-70B-Instruct via a two-step prompting strategy: first classifying into 4 high-level categories (solidarity, anti-solidarity, mixed, none), then into 8 subtypes based on Thijssen's typology. The pipeline used few-shot examples (1 per class), Q40 quantization, temperature 0.6, and top-p 0.9. Performance was validated against 3.6k human-labeled instances using Macro F1 and Cohen's κ, with comparison to GPT-4 as reference.

## Key Results
- Llama-3.3-70B achieved near-human performance in high-level classification (Macro F1: 0.65 vs human upper bound 0.75)
- High correlation (0.96) between GPT-4 and Llama-3.3-70B trends validates aggregate temporal patterns
- Post-2015 decline in solidarity and rise in anti-solidarity, with 2017 marking sharp shift toward group-based anti-solidarity in mainstream parties
- Models tend to underestimate solidarity, particularly for subtle or cited expressions

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Prompting for Theory-Driven Annotation
The system decomposes the complex sociological classification task into two steps: high-level stance detection followed by subtype classification based on Thijssen's typology. Few-shot examples and detailed definitions constrain the model's output space, enabling it to map text to abstract sociological concepts. This works because the LLM has sufficient internal representation of concepts like solidarity and empathy to apply provided definitions. The mechanism fails on fine-grained tasks (Macro F1 drops to ~0.37) and historical data when conceptual subtlety exceeds the model's reasoning capabilities.

### Mechanism 2: Aggregation for Trend Stability
Individual LLM annotations contain noise, but aggregate trends across thousands of data points are highly reliable due to the law of large numbers. Random classification errors cancel out over the corpus of 63k+ instances. This works because model errors are not systematically biased in ways that distort overall time-series trends. The mechanism breaks if historical language shifts introduce systematic bias that aggregation cannot correct.

### Mechanism 3: Open-Weight Models as Cost-Effective Proxies
Open-weight models like Llama-3.3-70B can serve as viable, cost-effective substitutes for proprietary models when rigorous validation is performed. The study uses quantized versions to run on in-house hardware, benchmarking against GPT-4 and human labels to establish reliability bounds. This works because the hardware supports 70B+ parameter models and quantization doesn't destroy required semantic nuance. The mechanism fails if research requires frontier reasoning capabilities not present in the open-weight checkpoint.

## Foundational Learning

- **Prompt Engineering (Chain-of-Thought & Few-Shot):** Needed because Thijssen's typology is too complex for zero-shot prompting. Models need explicit reasoning steps and examples to distinguish "compassionate solidarity" from "group-based solidarity." Quick check: Can you explain why providing a "definition" alone is insufficient for classifying "Exchange-based Solidarity"?

- **Inter-Annotator Agreement (Cohen's Kappa / F1):** Needed to determine if LLM failures reflect task subjectivity or model limitations. Human upper bounds contextualize LLM performance. Quick check: If human agreement (Kappa) on a subtype is 0.42 (low), what does that imply about the expected performance ceiling for an LLM?

- **Time-Series Analysis / Trend Correlation:** Needed because the core finding relies on comparing data across 155 years. Understanding correlation coefficients helps verify if observed trends are robust across different models. Quick check: Why might a model have low instance-level accuracy but high trend-level correlation?

## Architecture Onboarding

**Component map:** DeuParl Corpus -> Keyword Filter -> ±3 Sentence Context -> Llama-3.3-70B-Instruct -> Two-Step Prompt Pipeline (High-Level → Subtype) -> Validation against Human Labels

**Critical path:** 1) Preprocessing text to extract speaker/metadata (parsing XML) 2) Generating specific prompts (definitions + context) 3) Running inference on 63k filtered instances 4) Validating sample against 3.6k human-labeled set to establish reliability bounds

**Design tradeoffs:**
- Cost vs. Granularity: Using 70B model instead of GPT-4 saves money but slightly lowers fine-grained F1 (0.37 vs 0.42)
- Context Window: Including 3 sentences before/after helps resolve ambiguity but increases token cost and potential for distraction
- Quantization: Q40 reduces VRAM usage but risks subtle loss in semantic nuance for historical texts

**Failure signatures:**
- Citation Confusion: Misclassifies speaker criticizing anti-solidarity stance as expressing anti-solidarity
- Historical Drift: Performance drops on pre-1950s data due to language evolution
- Underestimation: Models tend to classify subtle solidarity as "None"

**First 3 experiments:**
1. Baseline Validation: Run Llama-3.3-70B on 2k test set, compare Macro F1 against human upper bound to ensure model is within acceptable range (>0.8 of human performance)
2. Prompt Sensitivity: Test Zero-shot vs. Few-shot (1 example per label) to verify impact on your specific open-weight model
3. Keyword Stability: Run stability test described in Section 4.1.1—sample subsets of keywords to ensure trend analysis isn't artifact of single word

## Open Questions the Paper Calls Out

**Open Question 1:** What are the primary political or social drivers behind the sharp shift toward group-based anti-solidarity in mainstream German parties (CDU/CSU and FDP) after 2017? The authors explicitly ask this question and state the shift is "insufficiently explained by recent literature," but cannot establish causal mechanisms using annotation data alone. Evidence needed: causal inference study correlating speech trends with electoral data or qualitative discourse analysis of party manifestos.

**Open Question 2:** Does analysis of (anti-)solidarity towards women reveal a polarization trend or "shift to the left" in German parliamentary discourse? The dataset for "Woman" (138k sentences) was collected and models evaluated on it, but longitudinal analysis was excluded due to space constraints. Evidence needed: applying the validated pipeline to full "Woman" dataset to analyze temporal trends 1949-2025.

**Open Question 3:** How can LLM annotation pipelines be improved to distinguish rhetorical citations of anti-solidarity from direct expressions of it? The error analysis notes models misclassify speakers citing anti-solidarity positions as expressing those positions. Evidence needed: integrating "reported speech" detection module into pipeline and re-evaluating performance on "mixed" or "misclassified citations" subset.

## Limitations
- Classification task is inherently subjective with human upper bound Kappa values ranging from 0.15 to 0.75, limiting theoretical maximum LLM performance
- Analysis relies on aggregated trends rather than individual accuracy, potentially masking systematic biases that could distort temporal patterns
- Historical language drift and citation-based anti-solidarity expressions present persistent classification challenges

## Confidence
- **High Confidence:** Post-2015 decline in solidarity and rise in anti-solidarity, supported by near-identical trends across two independent models (GPT-4 and Llama-3.3-70B) with correlation 0.96
- **Medium Confidence:** Attribution of increased "group-based anti-solidarity" in mainstream parties after 2017, as this relies on granular subtype classification where model performance is notably lower (Macro F1 ~0.37)
- **Low Confidence:** Generalizability to other parliamentary contexts or different theoretical frameworks, as validation is specific to German debates and Thijssen's typology

## Next Checks
1. Conduct systematic analysis of model performance across different historical periods to detect whether semantic drift introduces systematic biases in specific decades
2. Implement explicit citation detection in preprocessing pipeline to handle known failure mode where models misclassify speakers citing anti-solidarity positions
3. Perform targeted human validation on 10% of instances with highest model-human disagreement, particularly focusing on subtle solidarity expressions that tend to be underestimated