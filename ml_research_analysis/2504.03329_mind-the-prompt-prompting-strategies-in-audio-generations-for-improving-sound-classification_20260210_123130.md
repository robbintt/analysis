---
ver: rpa2
title: 'Mind the Prompt: Prompting Strategies in Audio Generations for Improving Sound
  Classification'
arxiv_id: '2504.03329'
source_url: https://arxiv.org/abs/2504.03329
tags:
- audio
- prompt
- datasets
- sound
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how prompt strategies impact the effectiveness
  of synthetic audio data generated by Text-to-Audio (TTA) models for sound classification.
  Three prompt strategies are proposed: a basic template-based approach, a structured
  approach using detailed sound attributes, and an exemplar-based approach using human-annotated
  captions.'
---

# Mind the Prompt: Prompting Strategies in Audio Generations for Improving Sound Classification

## Quick Facts
- arXiv ID: 2504.03329
- Source URL: https://arxiv.org/abs/2504.03329
- Authors: Francesca Ronchini; Ho-Hsiang Wu; Wei-Cheng Lin; Fabio Antonacci
- Reference count: 38
- Primary result: Task-specific prompt strategies and multi-model dataset merging improve synthetic audio utility for sound classification

## Executive Summary
This paper investigates how prompt strategies impact the effectiveness of synthetic audio data generated by Text-to-Audio (TTA) models for sound classification. Three prompt strategies are proposed: a basic template-based approach, a structured approach using detailed sound attributes, and an exemplar-based approach using human-annotated captions. These prompts are used to generate synthetic datasets with two TTA models—AudioGen and Stable Audio Open—to replace or augment real data from ESC50 and US8K benchmarks. The study finds that task-specific prompt strategies outperform basic templates in generating useful synthetic data. Merging datasets from different TTA models enhances performance more than simply increasing data volume. Using structured or exemplar-based prompts as data augmentation improves classification accuracy, demonstrating that prompt design and dataset diversity are key to leveraging synthetic data in sound classification.

## Method Summary
The study uses three prompt strategies to generate captions via GPT-4: Basic (template), Structured (5 attributes: pitch, pattern, intensity, acoustic characteristics, location), and Exemplar (few-shot with Clotho dataset). These captions are fed into two TTA models—AudioGen (auto-regressive) and Stable Audio Open (latent diffusion)—to generate synthetic audio datasets matching ESC50 and US8K class distributions. A CNN10 classifier from PANNs is trained from scratch with early stopping, using k-fold cross-validation matching original benchmarks. Experiments compare synthetic-only training, augmentation with real data, and dataset merging across prompt strategies and TTA models.

## Key Results
- Task-specific prompt strategies (Structured, Exemplar) outperform basic templates in synthetic data utility (0.40–0.56 vs. 0.26–0.42 accuracy)
- Merging synthetic datasets from different TTA models improves classification more than increasing volume from a single model
- Synthetic data works better as augmentation than replacement for real data in sound classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Task-specific prompt strategies produce more useful synthetic audio than basic template prompts.
- **Mechanism:** Rich prompts encode multiple sound attributes → TTA models generate more diverse and representative audio samples → classifier receives richer training signal across intra-class variation.
- **Core assumption:** GPT-4 reliably incorporates specified attributes into generated captions, and TTA models translate these detailed descriptions into acoustically distinct outputs.
- **Evidence anchors:**
  - [abstract] "task-specific prompt strategies significantly outperform basic prompt approaches in data generation"
  - [Section IV-A, Table I] Structured (STR) and Exemplar-based (EXE) strategies achieve 0.40–0.56 accuracy vs. Basic (BSC) at 0.26–0.42 across datasets and models when training solely on synthetic data
  - [corpus] Weak direct evidence; related work on test-time adaptation addresses distribution shift but not prompt design mechanisms
- **Break condition:** If TTA models fail to acoustically differentiate prompts despite textual richness, the mechanism degrades.

### Mechanism 2
- **Claim:** Merging synthetic datasets from different TTA models improves classification more than increasing volume from a single model.
- **Mechanism:** Different TTA architectures produce complementary acoustic representations → merged dataset captures broader feature space → classifier learns more robust decision boundaries.
- **Core assumption:** Model diversity translates to meaningful acoustic diversity rather than uncorrelated noise.
- **Evidence anchors:**
  - [abstract] "merging datasets generated using different TTA models proves to enhance classification performance more effectively than merely increasing the training dataset size"
  - [Section IV-E, Table IV] EXE strategy with merged SA+AG achieves 0.52 (ESC50) and 0.60 (US8K), outperforming 2x data multiplication from single models
  - [corpus] No direct corpus evidence on multi-model synthetic data merging for audio
- **Break condition:** If different TTA models generate highly correlated outputs, diversity gains diminish.

### Mechanism 3
- **Claim:** Synthetic data works better as augmentation than as replacement for real data.
- **Mechanism:** Real data provides ground-truth distribution anchor → synthetic data expands coverage of intra-class variation → combined training reduces overfitting while maintaining accuracy on test samples.
- **Core assumption:** Generated audio maintains sufficient acoustic fidelity to extend rather than corrupt the training distribution.
- **Evidence anchors:**
  - [Section IV-B, Table II] STR + original (ORG) achieves 0.72 (ESC50) and 0.79 (US8K) vs. baseline 0.67 and 0.78
  - [Section IV-A] Training solely on synthetic data fails to match baseline (0.26–0.56 vs. 0.67–0.78)
  - [corpus] Indirect support from [114877] on synthetic augmentation for medical audio classification
- **Break condition:** If synthetic data introduces systematic artifacts or distribution mismatch, performance degrades.

## Foundational Learning

- **Concept: Text-to-Audio (TTA) Generation**
  - Why needed here: Understanding how TTA models translate prompts to waveforms is essential for diagnosing why certain prompts fail.
  - Quick check question: Can you explain why an auto-regressive model (AudioGen) might produce different artifacts than a latent diffusion model (Stable Audio Open)?

- **Concept: Data Augmentation vs. Domain Shift**
  - Why needed here: The paper positions synthetic data as augmentation, but distribution mismatch between synthetic training and real test data remains a core limitation.
  - Quick check question: If synthetic data contains keyboard clicks mixed with mouse clicks, what failure mode might this cause at test time?

- **Concept: Prompt Engineering for Generative Models**
  - Why needed here: The three strategies (basic, structured, exemplar-based) form the intervention space; understanding few-shot prompting with GPT-4 is prerequisite to reproducing or extending this work.
  - Quick check question: What is the risk of using GPT-4 for caption generation without enforcing attribute inclusion constraints?

## Architecture Onboarding

- **Component map:** GPT-4 (few-shot) → caption collection per strategy (BSC/STR/EXE) → AudioGen OR Stable Audio Open → synthetic audio → CNN10 classifier → accuracy evaluation
- **Critical path:**
  1. Define prompt strategy and generate captions via GPT-4
  2. Generate audio using selected TTA model(s) at 16kHz mono
  3. Organize synthetic data following original dataset class distribution
  4. Train CNN10 with early stopping (patience=10, max 200 epochs)
  5. Evaluate using k-fold cross-validation matching original benchmarks
- **Design tradeoffs:**
  - Prompt richness vs. controllability: More detailed prompts improve diversity but do not guarantee acoustic accuracy
  - Data volume vs. diversity: Multi-model merging outperforms single-model multiplication, but requires maintaining multiple TTA pipelines
  - Dataset complexity: ESC50 responds better to synthetic augmentation than US8K—prompt strategy must account for target domain
- **Failure signatures:**
  - Class confusion between acoustically similar sounds persists despite prompt improvements
  - US8K shows minimal gains from STR prompts with AudioGen—likely due to multi-source complexity
  - LLM-generated captions may be biased or repetitive; structured prompts do not guarantee all five attributes appear
- **First 3 experiments:**
  1. **Baseline replication:** Train CNN10 on original ESC50/US8K using reported cross-validation splits to establish baseline accuracy
  2. **Prompt strategy ablation:** Generate synthetic ESC50 using BSC, STR, and EXE with a single TTA model; train CNN10 on synthetic-only data; verify STR/EXE > BSC
  3. **Multi-model merge test:** Generate synthetic data using EXE strategy with both AudioGen and Stable Audio Open; merge and train; compare against 2x data multiplication from single model

## Open Questions the Paper Calls Out
- Can fine-tuning Text-to-Audio (TTA) models on specific target domains enhance the utility of synthetic datasets for sound classification compared to using pre-trained models?
- To what extent can domain adaptation methods mitigate the distribution mismatch between synthetic training data and real-world testing data?
- How can prompt generation strategies be engineered to guarantee the consistent inclusion of specific acoustic attributes?

## Limitations
- TTA models occasionally generate co-occurring sounds (e.g., mouse clicks with keyboard typing), introducing label noise
- Improvement gap between synthetic augmentation and real data replacement is substantial (0.11-0.31 accuracy points)
- Multi-source sounds in US8K show minimal synthetic augmentation benefits compared to single-source ESC50
- Training hyperparameters (learning rate, batch size, optimizer specifics) are unspecified, preventing exact reproduction

## Confidence
- **High confidence**: Task-specific prompts (STR/EXE) outperform basic templates; synthetic data works better as augmentation than replacement; CNN10 architecture and training procedures are clearly specified
- **Medium confidence**: Multi-model dataset merging improves performance more than volume increase; the mechanism that detailed prompts produce acoustically diverse outputs through TTA models
- **Low confidence**: Prompt strategies will generalize to multi-source acoustic environments like US8K; GPT-4 reliably produces attribute-compliant captions across all strategies; synthetic data distribution matches real test data for complex sound classification tasks

## Next Checks
1. **Label noise validation**: Generate synthetic data using EXE strategy with both TTA models, then manually inspect 50 random samples per class for co-occurring sounds or attribute violations; calculate precision@1 for prompt-attribute matching
2. **Hyperparameter sensitivity test**: Train CNN10 on ESC50 synthetic data using learning rates {0.001, 0.01, 0.1} and batch sizes {16, 32, 64}; identify optimal configuration that matches reported baseline performance
3. **Domain shift measurement**: Compute Fréchet Audio Distance (FAD) between synthetic ESC50 and real ESC50 test sets, and between synthetic US8K and real US8K test sets; correlate distance metrics with classification accuracy drops to quantify distribution mismatch