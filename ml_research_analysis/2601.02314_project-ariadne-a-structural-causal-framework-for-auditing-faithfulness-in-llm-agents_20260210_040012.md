---
ver: rpa2
title: 'Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in
  LLM Agents'
arxiv_id: '2601.02314'
source_url: https://arxiv.org/abs/2601.02314
tags:
- reasoning
- causal
- ariadne
- faithfulness
- logic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Project Ariadne introduces a novel causal auditing framework for\
  \ LLM agents using Structural Causal Models (SCMs) and do-calculus interventions\
  \ to evaluate the faithfulness of Chain-of-Thought reasoning. By systematically\
  \ inverting logic, negating premises, and reversing factual claims in intermediate\
  \ reasoning steps, the framework quantifies Causal Sensitivity\u2014measuring how\
  \ much the terminal answer depends on the reasoning trace."
---

# Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents

## Quick Facts
- arXiv ID: 2601.02314
- Source URL: https://arxiv.org/abs/2601.02314
- Reference count: 8
- 96% violation density in scientific reasoning domains

## Executive Summary
Project Ariadne introduces a novel causal auditing framework for LLM agents using Structural Causal Models (SCMs) and do-calculus interventions to evaluate the faithfulness of Chain-of-Thought reasoning. By systematically inverting logic, negating premises, and reversing factual claims in intermediate reasoning steps, the framework quantifies Causal Sensitivity—measuring how much the terminal answer depends on the reasoning trace. Across 500 queries in three domains, the method revealed a significant Faithfulness Gap: up to 96% violation density in scientific reasoning where agents produced identical answers despite contradictory internal logic. This demonstrates that current LLM reasoning traces often function as post-hoc rationalizations rather than generative drivers of decisions.

## Method Summary
The framework models the agent's generation process as an SCM M = ⟨U, V, F⟩ where reasoning steps s₁...sₙ and final answer a are endogenous variables. It performs hard interventions using do-calculus on intermediate reasoning nodes—systematically inverting logic, negating premises, and reversing factual claims. The Causal Sensitivity Score ϕ = 1 - S(a, a*) quantifies faithfulness by measuring semantic divergence between original and counterfactual answers. Violation Density ρ = (1/m)ΣV(qᵢ, kᵢ, ιᵢ) measures population-level unfaithfulness, where V flags Causal Decoupling when similarity exceeds thresholds.

## Key Results
- Mathematical Logic domain showed ϕ̄=0.329, ρ=20% (relatively faithful)
- Scientific Reasoning domain showed ϕ̄=0.030, ρ=96% (severely unfaithful)
- General Knowledge domain showed intermediate violation rates
- Models demonstrated "error-correction" behavior, self-correcting after interventions to maintain pre-determined conclusions

## Why This Works (Mechanism)

### Mechanism 1
Treating reasoning traces as causal graphs enables formal auditing of whether intermediate steps causally drive outputs. The framework models the agent's generation process as an SCM where each step depends on the query and preceding steps, creating a directed causal chain. This formalization allows do-calculus interventions that isolate specific reasoning nodes for perturbation. Break condition: If reasoning is distributed across representations rather than stepwise discrete, node-level interventions may not capture true causal structure.

### Mechanism 2
Hard interventions (logic flips, premise negations) on intermediate reasoning nodes reveal whether outputs causally depend on those steps. The intervention operator ι maps a reasoning step to its contradictory counterpart, and the intervened answer a* is generated by re-executing from the intervention point. High similarity despite contradictory reasoning indicates Causal Decoupling. Break condition: If interventions are semantically weak or models interpret them differently than intended, violations may reflect measurement error.

### Mechanism 3
The Causal Sensitivity Score (ϕ) quantifies faithfulness by measuring semantic divergence between original and counterfactual answers. A binary violation indicator V flags Causal Decoupling when similarity exceeds thresholds. Aggregate Violation Density ρ measures population-level unfaithfulness. Break condition: If semantic similarity judges systematically bias toward equivalence, violation density may be inflated.

## Foundational Learning

- **Structural Causal Models (SCMs) and do-calculus**
  - Why needed here: The entire framework formalizes reasoning as an SCM and uses do-calculus notation for interventions. Without this foundation, the distinction between observational correlation and interventional causation will be unclear.
  - Quick check question: Given an SCM with variables X → Y → Z, what does do(Y=y) mean and how does it differ from conditioning on Y=y?

- **Counterfactual reasoning in autoregressive models**
  - Why needed here: The intervention mechanism requires re-sampling from an intermediate point while holding the intervention fixed. Understanding how autoregressive sampling enables this is essential.
  - Quick check question: If an LLM has generated tokens t₁...tₙ and you want to intervene at tₖ, what must you do to generate a counterfactual continuation?

- **Semantic similarity for answer equivalence**
  - Why needed here: The Causal Sensitivity Score depends entirely on S(a, a*). Understanding limitations of embedding-based or LLM-judge similarity is critical for interpreting results.
  - Quick check question: Why might "The answer is Paris" and "The French capital" receive high semantic similarity despite different surface forms? What are failure modes?

## Architecture Onboarding

- Component map: SCM Constructor -> Intervention Generator -> Counterfactual Executor -> Similarity Scorer -> Violation Detector -> Aggregator
- Critical path: 1) Generate original trace and answer (a) for query q, 2) Select intervention target sₖ and modality τ, 3) Generate counterfactual step ι(sₖ), 4) Regenerate from sₖ onward → a*, 5) Compute S(a, a*) and derive ϕ, 6) Apply violation threshold → V
- Design tradeoffs:
  - Single-node vs multi-step interventions: Paper uses single-node (s₀) interventions; multi-step may capture compound effects but increases complexity
  - Intervention modality selection: LogicFlip vs FactReversal may yield different sensitivity patterns; domain-specific choice matters
  - Similarity judge choice: Using same model family may introduce bias; cross-model judging adds cost
  - Threshold values (τ_sim, λ): Paper doesn't specify exact values; these require calibration
- Failure signatures:
  - False positive violations: Semantically weak interventions flagged as decoupling
  - Judge bias: Similarity scorer systematically overestimates equivalence between contradiction-preserving reformulations
  - Intervention interpretation failure: Model ignores or misinterprets forced contradiction
  - Trace position sensitivity: Early interventions may allow more "correction" than late interventions
- First 3 experiments:
  1. Baseline calibration: Run Ariadne on synthetic reasoning chains with known causal structure to validate that ϕ correctly detects faithful reasoning.
  2. Intervention position sweep: Compare ϕ and ρ when intervening at s₀, sₙ/₂, sₙ₋₁ to measure position-dependent "correction" behavior.
  3. Cross-model consistency: Audit the same query set across multiple model families to determine whether Causal Decoupling is architecture-general or model-specific.

## Open Questions the Paper Calls Out

- Does increased test-time compute in "System 2" architectures result in higher causal faithfulness, or does it merely generate more elaborate post-hoc justifications? The current study evaluates standard autoregressive architectures but does not test models specifically designed for iterative reasoning or search.
- Can the Faithfulness Score (ϕ) be effectively utilized as a reinforcement learning signal to train models that are causally grounded? The paper establishes the diagnostic metric but does not demonstrate its efficacy as a training objective.
- What is the "logical threshold" of simultaneous perturbations required to override a model's latent parametric priors? The current framework focuses on single-node interventions, which models may easily bypass to reach pre-determined conclusions.

## Limitations
- Intervention semantics uncertainty: Exact transformation rules for LogicFlip, FactReversal, and PremiseNegation are not fully specified, affecting interpretability of violation density scores.
- Judge dependency: Results depend critically on Claude 3.7 Sonnet's semantic similarity judgments, which may systematically underestimate equivalence between contradictory answers.
- Trace decomposition assumption: The framework assumes reasoning traces decompose into discrete, intervenable causal nodes. If reasoning is distributed or hierarchical, single-step interventions may miss compound causal effects.

## Confidence
- High confidence: The theoretical foundation using SCMs and do-calculus for intervention is sound and well-established in causal inference literature.
- Medium confidence: The domain-specific violation patterns are internally consistent and align with intuition about reasoning complexity, but depend on threshold calibration.
- Low confidence: Without access to exact prompts, intervention rules, and threshold values, exact reproduction of quantitative results is challenging.

## Next Checks
1. Synthetic benchmark validation: Test the framework on artificially constructed reasoning chains where causal relationships are known to verify ϕ correctly detects faithful reasoning.
2. Cross-model consistency audit: Run the same query set across multiple model families to determine whether Causal Decoupling is architecture-general or model-specific.
3. Threshold calibration study: Systematically vary τ_sim and λ parameters across the query set to map how violation density changes with sensitivity settings.