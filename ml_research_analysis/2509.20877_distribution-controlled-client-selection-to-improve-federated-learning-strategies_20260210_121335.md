---
ver: rpa2
title: Distribution-Controlled Client Selection to Improve Federated Learning Strategies
arxiv_id: '2509.20877'
source_url: https://arxiv.org/abs/2509.20877
tags:
- client
- data
- selection
- imbalance
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of data imbalance in federated
  learning (FL), which negatively impacts model performance. The authors propose a
  Distribution-Controlled (DC) client selection method that extends existing FL strategies
  by selecting clients to align the current label distribution with either a balanced
  distribution or the federation's combined label distribution.
---

# Distribution-Controlled Client Selection to Improve Federated Learning Strategies

## Quick Facts
- **arXiv ID:** 2509.20877
- **Source URL:** https://arxiv.org/abs/2509.20877
- **Reference count:** 36
- **Primary result:** Distribution-Controlled (DC) client selection improves F1-scores in FL under data imbalance by aligning training distributions with target distributions.

## Executive Summary
This paper addresses the critical challenge of data imbalance in federated learning, which significantly degrades model performance. The authors propose a Distribution-Controlled (DC) client selection method that extends existing FL strategies by selecting clients to align the current label distribution with either a balanced distribution or the federation's combined label distribution. The method uses a greedy approach to iteratively add clients that minimize the cosine distance to the target distribution. Experiments on MNIST and CovType datasets with three FL strategies (FedAvg, FedAtt, FedProx) demonstrate that DC client selection significantly improves F1-scores compared to baselines, especially under high data imbalance conditions.

## Method Summary
The method partitions data across 100 clients using Dirichlet distribution with varying α parameters to simulate local and global imbalance. For each training round, the server starts with a random set of clients and then greedily adds m_DC additional clients whose local label distributions, when combined with the current active set, minimize the cosine distance to a target distribution (either balanced or real federation distribution). The greedy selection process iteratively evaluates each candidate client's potential contribution to distribution alignment. The approach is tested with FedAvg, FedAtt, and FedProx strategies across 100 training rounds with 3 local epochs per round.

## Key Results
- DC client selection significantly improves F1-scores compared to baseline FL strategies under high data imbalance
- Balanced target distribution performs best for local imbalance scenarios, while real federation distribution excels for global imbalance
- DC selection outperforms random client addition, validating that strategic client choice matters more than simply increasing participation numbers
- The greedy selection approach achieves performance close to exhaustive search while avoiding combinatorial complexity

## Why This Works (Mechanism)

### Mechanism 1
Selecting clients to minimize the cosine distance between the active training distribution and a specific target distribution mitigates model degradation caused by data imbalance. The method employs a greedy algorithm that iteratively selects the client whose local label distribution, when added to the currently active set, minimizes the distance to a target distribution. This actively shapes the training batch to be more representative than random sampling would allow. The core assumption is that clients truthfully report their local label distributions to the server or via secure aggregation.

### Mechanism 2
The choice of target distribution ("Balanced" vs. "Real") determines success depending on whether the federation suffers from local heterogeneity or global class scarcity. Aligning with a "Balanced" distribution forces the model to see all classes equally, combating the "client drift" caused by local heterogeneity. Aligning with the "Real" distribution respects the actual global priors, which is superior when the global dataset itself is fundamentally imbalanced. The core assumption is that the "Real" global distribution is computable and stable.

### Mechanism 3
Greedy selection provides statistically significant performance improvements over random client addition, validating that the choice of client matters more than merely increasing the number of participants. By comparing the DC selection against an ablation where m_DC clients are added randomly, the paper demonstrates that the delta in F1-score comes from the alignment of distributions, not just the increased volume of training data. The core assumption is that the computational overhead of the greedy selection loop is acceptable compared to the cost of training on misaligned data.

## Foundational Learning

- **Concept:** Local vs. Global Label Imbalance (Non-IID Data)
  - **Why needed here:** The paper distinguishes between local imbalance (clients have different distributions) and global imbalance (the total dataset is skewed). You must distinguish these to choose the correct target distribution.
  - **Quick check question:** If Client A has 90% class 1 and Client B has 90% class 2, but the total dataset has 50/50 class 1/2, is this local or global imbalance? (Answer: Local).

- **Concept:** Cosine Similarity/Distance
  - **Why needed here:** This is the metric used to score potential clients. It measures the orientation of the label distribution vectors rather than their magnitude.
  - **Quick check question:** Does minimizing cosine distance prioritize clients with the most data, or clients with the rarest classes relative to the current batch? (Answer: The rarest classes relative to the batch).

- **Concept:** Secure Aggregation
  - **Why needed here:** To compute the "Real" target distribution without violating privacy. The server needs the sum of all client histograms but cannot inspect individual histograms.
  - **Quick check question:** Why is Secure Aggregation necessary for "Real" target distribution but not for "Balanced" target? (Answer: "Balanced" is a public static vector of ones; "Real" depends on private user data).

## Architecture Onboarding

- **Component map:** Server Controller -> Distribution Vector Store -> Client Filter -> Secure Aggregator
- **Critical path:**
  1. **Init:** Server determines V_target (Equation 1 via Secure Aggregation OR static V_balanced).
  2. **Round Start:** Select random set S_t of size m.
  3. **Selection Loop:** Broadcast V_active to inactive clients; clients calculate hypothetical distance; server selects argmin client; repeat m_DC times.
  4. **Training:** Standard FL training on the expanded set S_t.

- **Design tradeoffs:**
  - **Greedy vs. Exhaustive:** The paper proves Greedy is sufficient, avoiding the combinatorial explosion of finding the "perfect" set of clients.
  - **V_balanced vs. V_real:** Default to V_balanced for highly heterogeneous local data; switch to V_real if the global dataset is known to be naturally skewed.
  - **m_DC magnitude:** Increasing added clients improves performance but with diminishing returns after m_DC=4.

- **Failure signatures:**
  - **Stagnant F1-score:** If adding clients does not improve the score, check if the target distribution is actually achievable given the available client pool.
  - **Adversarial Drift:** If a malicious client reports a perfect "balancing" vector to the server to get selected, they can poison the model.

- **First 3 experiments:**
  1. **Baseline Validation:** Run FedAvg vs. FedAvg^DC on MNIST with high local imbalance (α_local=0.1) to verify the "Balanced" target improves F1-score.
  2. **Target Switching:** Simulate a global imbalance scenario (α_global=0.1) and compare F1-scores when using V_real vs. V_balanced to confirm V_real is superior for this condition.
  3. **Ablation Check:** Compare DC selection against a "Random Addition" baseline (adding m_DC clients randomly) to prove the performance gain comes from the selection logic, not just increased participation.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the client selection process be modified to prevent malicious clients from manipulating selection by falsely reporting minimal cosine distances?
- **Open Question 2:** Does the distribution-controlled selection strategy effectively mitigate performance degradation caused by feature imbalance or quantity imbalance?
- **Open Question 3:** Is it feasible to develop a self-adjusting mechanism that dynamically selects the optimal target distribution during training?

## Limitations

- **Limitation 1:** The method requires clients to truthfully announce their distance metrics, making it vulnerable to adversarial manipulation.
- **Limitation 2:** The approach only addresses label imbalance and does not cover feature imbalance or quantity imbalance scenarios.
- **Limitation 3:** The current implementation requires a static choice between "Real" or "Balanced" targets based on known imbalance type prior to training.

## Confidence

- **Dataset and partitioning method:** High confidence (explicitly specified)
- **DC selection algorithm:** High confidence (Algorithm 1 clearly defined)
- **Model architectures:** High confidence (explicitly specified)
- **Hyperparameters (batch size, learning rate):** Low confidence (not specified in paper)
- **Optimizer details:** Low confidence (not specified in paper)

## Next Checks

1. **Verify baseline implementation:** Run FedAvg on MNIST with Dirichlet partitioning to establish baseline F1-scores before implementing DC selection.
2. **Test greedy selection logic:** Implement and validate the greedy client selection algorithm independently on synthetic label distributions to ensure correct cosine distance minimization.
3. **Compare target distributions:** Run experiments with both "Balanced" and "Real" target distributions on datasets with controlled local vs. global imbalance to confirm the paper's findings about which target performs better under which condition.