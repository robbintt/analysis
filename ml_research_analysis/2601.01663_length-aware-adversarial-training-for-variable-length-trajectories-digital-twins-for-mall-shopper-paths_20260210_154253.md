---
ver: rpa2
title: 'Length-Aware Adversarial Training for Variable-Length Trajectories: Digital
  Twins for Mall Shopper Paths'
arxiv_id: '2601.01663'
source_url: https://arxiv.org/abs/2601.01663
tags:
- intra
- time
- inter
- length
- mall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the challenge of generating variable-length\
  \ trajectories\u2014such as mall shopper paths\u2014where standard mini-batch training\
  \ can struggle due to length heterogeneity, leading to poor distribution matching\
  \ of derived variables like total duration and visit counts. The authors propose\
  \ length-aware sampling (LAS), a simple batching strategy that groups trajectories\
  \ by length and samples within single-length buckets, reducing within-batch length\
  \ variation without altering the model architecture."
---

# Length-Aware Adversarial Training for Variable-Length Trajectories: Digital Twins for Mall Shopper Paths

## Quick Facts
- arXiv ID: 2601.01663
- Source URL: https://arxiv.org/abs/2601.01663
- Reference count: 40
- Primary result: LAS improves derived-variable distribution matching for variable-length trajectories, reducing mean KS statistic by up to 65.7% on mall datasets

## Executive Summary
This paper addresses the challenge of generating variable-length trajectories, such as mall shopper paths, where length heterogeneity within mini-batches leads to poor distribution matching of derived variables like total duration and visit counts. The authors propose length-aware sampling (LAS), a simple batching strategy that groups trajectories by length and samples within single-length buckets. LAS is integrated into a conditional trajectory GAN with auxiliary time-alignment losses and is shown to improve matching of derived-variable distributions across mall data and multiple public sequence datasets. Theoretically, the authors provide distribution-level guarantees for derived variables and explain why LAS improves distribution matching by removing length-only shortcut critics. Empirically, LAS consistently outperforms random sampling.

## Method Summary
The paper tackles the problem of generating variable-length trajectories where standard mini-batch training can struggle due to length heterogeneity, leading to poor distribution matching of derived variables like total duration and visit counts. The authors propose length-aware sampling (LAS), a simple batching strategy that groups trajectories by length and samples within single-length buckets, reducing within-batch length variation without altering the model architecture. LAS is integrated into a conditional trajectory GAN with auxiliary time-alignment losses and is shown to improve matching of derived-variable distributions across mall data and multiple public sequence datasets. Theoretically, the authors provide distribution-level guarantees for derived variables and explain why LAS improves distribution matching by removing length-only shortcut critics. Empirically, LAS consistently outperforms random sampling, reducing the mean KS statistic by up to 65.7% on mall datasets and improving derived-variable fidelity on public datasets. The approach is validated for downstream controllability and counterfactual analyses.

## Key Results
- LAS consistently outperforms random sampling, reducing mean KS statistic by up to 65.7% on mall datasets
- LAS improves derived-variable fidelity on multiple public sequence datasets beyond mall data
- Theoretical distribution-level guarantees are provided for derived variables under LAS

## Why This Works (Mechanism)
The paper explains that LAS improves distribution matching by removing length-only shortcut critics. When trajectories of varying lengths are mixed in a batch, discriminators can exploit length as a proxy feature to distinguish real from generated samples, rather than learning the true underlying distribution. By grouping trajectories of similar lengths, LAS forces the critic to focus on content-level features, leading to better overall distribution matching. The theoretical analysis shows that this approach reduces the global Wasserstein distance between the generated and true distributions of derived variables.

## Foundational Learning
1. **Variable-length trajectory generation**: Why needed - to model real-world sequential data where sequence lengths vary naturally. Quick check - does the model handle sequences ranging from 10 to 1000 steps without degradation?
2. **Distribution matching for derived variables**: Why needed - ensuring that statistics computed from generated sequences (e.g., total duration, visit counts) match those of real data. Quick check - compare KS statistics for derived variables between generated and real data.
3. **Adversarial training with length heterogeneity**: Why needed - standard GAN training can fail when sequence lengths vary widely within batches. Quick check - does the model show improved performance when trained with LAS versus random sampling?

## Architecture Onboarding

**Component Map**: Generator -> Discriminator (Critic) -> Length-Aware Sampler -> Data Loader -> Generator

**Critical Path**: The critical path is the training loop where the generator produces sequences, the discriminator evaluates them, and the length-aware sampler ensures balanced representation of different sequence lengths within each batch.

**Design Tradeoffs**: The main tradeoff is between batch diversity and length homogeneity. LAS sacrifices some batch diversity by grouping similar-length sequences, but gains in distribution matching of derived variables. The choice of the number of length buckets (K) is crucial - too few buckets may not sufficiently reduce length heterogeneity, while too many may reduce batch diversity and slow convergence.

**Failure Signatures**: If LAS is not effective, we might observe: (1) discriminator exploiting length as a shortcut feature, (2) poor matching of derived-variable distributions despite good primary trajectory quality, (3) instability in training when sequence lengths vary widely.

**First Experiments**:
1. Train the model with LAS on mall dataset and evaluate derived-variable KS statistics
2. Compare LAS to random sampling on a public sequence dataset (e.g., activity recognition)
3. Conduct ablation study varying the number of length buckets (K) to find optimal setting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does length-aware sampling provide similar benefits for other generative frameworks beyond GANs (e.g., VAEs, diffusion models, autoregressive transformers)?
- Basis in paper: [explicit] The authors state LAS is "integrated into a conditional trajectory GAN" and evaluate only GAN-style objectives across datasets. They note LAS is a "training-time intervention (no model changes)" but do not test non-adversarial frameworks.
- Why unresolved: The theoretical mechanism relies on removing shortcut critics, which is discriminator-specific. Whether LAS helps VAEs or diffusion models with heterogeneous sequences is untested.
- What evidence would resolve it: Systematic comparison of LAS vs. RS on variable-length sequence datasets using VAE and diffusion objectives, reporting derived-variable distribution matching.

### Open Question 2
- Question: How sensitive is LAS to the number and granularity of length buckets (K)?
- Basis in paper: [inferred] The paper states "We partition the training set into K length buckets using length quantiles" but provides no ablation on K. The choice of bucket granularity could trade off within-batch homogeneity against gradient diversity.
- Why unresolved: Too few buckets may retain length heterogeneity; too many may reduce batch diversity and slow convergence. No guidance is given for selecting K.
- What evidence would resolve it: Ablation study varying K (e.g., 2, 5, 10, 20, 50 buckets) on held-out datasets, measuring KS statistics and training dynamics.

### Open Question 3
- Question: Can the empirical bucket weighting scheme (w_k ∝ p_k) be improved to further reduce length-marginal mismatch?
- Basis in paper: [explicit] Footnote 2 states "Uniform bucket sampling is a straightforward alternative; we do not vary this choice in our experiments."
- Why unresolved: Lemma 12 shows global Wasserstein distance depends on TV(w, ŵ), suggesting bucket weighting directly affects distribution matching. Different weighting schemes may better align generator bucket weights with data.
- What evidence would resolve it: Compare empirical, uniform, and length-inverse-frequency weighting schemes across datasets, reporting TV(w, ŵ) and derived-variable KS statistics.

### Open Question 4
- Question: Does LAS generalize to settings where length is correlated with multiple latent factors (e.g., user intent, demographics) rather than being the dominant heterogeneity source?
- Basis in paper: [inferred] The discussion notes LAS is "most effective when the dataset exhibits substantial length heterogeneity" and that controlling length alone "may not fully resolve capacity or representation limits."
- Why unresolved: In complex domains, length may be a proxy for multiple correlated attributes. Batching only by length might not eliminate shortcut signals from these confounders.
- What evidence would resolve it: Construct synthetic datasets with controlled correlations between length and latent attributes, then measure whether LAS improves matching of all correlated derived variables.

## Limitations
- The method is primarily validated on mall shopper trajectories and may not generalize to all domains with variable-length sequences
- The choice of the number of length buckets (K) is not systematically explored, leaving a hyperparameter tuning challenge
- While LAS improves derived-variable matching, it does not necessarily improve overall trajectory quality or realism

## Confidence
- **High**: LAS improves matching of derived-variable distributions (duration, visit counts, etc.) over random sampling; theoretical distribution-level guarantees are valid under stated assumptions
- **Medium**: LAS generalizes across datasets and use cases; improvements in derived-variable fidelity translate to meaningful downstream controllability
- **Low**: LAS improves overall sample quality beyond derived-variable fidelity; no adverse effects on primary trajectory realism

## Next Checks
1. Evaluate LAS on domains with extreme length heterogeneity (e.g., variable-length event sequences with >1000 steps) to test scalability
2. Conduct ablation studies comparing LAS to curriculum learning or length-based curriculum sampling to isolate the effect of length-aware batching
3. Test LAS in a semi-supervised or noisy-label setting to assess robustness when exact trajectory lengths are unknown or approximate