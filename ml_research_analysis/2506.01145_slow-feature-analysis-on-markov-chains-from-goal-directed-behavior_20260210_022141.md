---
ver: rpa2
title: Slow Feature Analysis on Markov Chains from Goal-Directed Behavior
arxiv_id: '2506.01145'
source_url: https://arxiv.org/abs/2506.01145
tags:
- behavior
- features
- correction
- figure
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the effects of goal-directed behavior on
  slow feature analysis (SFA) for representation learning in reinforcement learning
  settings. The authors formulate SFA on ergodic Markov chains and derive optimal
  solutions as generalized eigenvectors, connecting to spectral graph theory.
---

# Slow Feature Analysis on Markov Chains from Goal-Directed Behavior

## Quick Facts
- **arXiv ID:** 2506.01145
- **Source URL:** https://arxiv.org/abs/2506.01145
- **Reference count:** 11
- **Primary result:** Goal-directed behavior causes harmful scaling in slow features due to state occupancy differences; scale correction restores performance.

## Executive Summary
This work investigates how goal-directed behavior in reinforcement learning affects slow feature analysis (SFA) for representation learning. The authors formulate SFA on ergodic Markov chains and derive optimal solutions as generalized eigenvectors, connecting to spectral graph theory. They find that goal-directed behavior causes significant scaling in slow features due to state occupancy differences, which can harm value function approximation performance. Three correction mechanisms are proposed and evaluated: behavior modification (Boltzmann exploration), learning rate adaptation (LRA), and scale correction. Experiments on linear and lattice graph environments show that goal-averse features often outperform goal-directed ones for value function approximation, but scale correction applied to Boltzmann behavior generally yields the best results. LRA correction shows limited benefit. The study highlights misalignment between slowness optimization and goal-directed behavior, suggesting scale correction as a practical remedy.

## Method Summary
The authors formulate SFA on ergodic Markov chains by defining the symmetrized weight matrix M and diagonal matrix D from the stationary distribution μ. The optimal features are computed as the smallest non-trivial eigenvectors of the generalized eigenvalue problem (D-M)Y = DYA. Three corrections are proposed: Boltzmann behavior to smooth occupancy, LRA to weight transitions by inverse probabilities, and scale correction to post-multiply features by √μ. Performance is evaluated by linear regression of V* using the features as basis functions, with log MSE as the metric.

## Key Results
- Goal-directed behavior causes features to flatten in high-occupancy regions and scale up in low-occupancy regions.
- Goal-averse features naturally align with value function shape and often outperform goal-directed features without correction.
- Scale correction (multiplying features by √μ) improves goal-directed feature performance but degrades goal-averse feature performance.
- LRA correction shows limited benefit compared to scale correction.

## Why This Works (Mechanism)

### Mechanism 1: Stationary Distribution Induces Feature Scaling
- **Claim:** SFA features scale inversely with the square root of state occupancy under the behavior policy.
- **Mechanism:** The unit-variance constraint requires Σᵤ μᵤ(yᵤ)² = 1. Since μᵤ(yᵤ)² ≤ 1, features are bounded by yᵤ ≤ ±1/√μᵤ. In goal-directed behavior, states near rewards have high occupancy (large μ), flattening features there; in goal-averse behavior, low occupancy near rewards amplifies features.
- **Core assumption:** The Markov chain is ergodic with a well-defined stationary distribution μ.
- **Evidence anchors:**
  - [Section 5]: "All resulting features are flat in areas of high occupancy, but scaled up in areas of lowest occupancy."
  - [Eq. 24]: Derivation of the scaling bound μᵢyᵢ² ≤ 1.
  - [corpus]: Weak direct corpus support; no SFA-specific scaling analysis found in neighbors.
- **Break condition:** If the chain is not ergodic or μ is undefined (absorbing states), the bound does not hold.

### Mechanism 2: Goal-Averse Features Align with Value Function Shape
- **Claim:** Uncorrected goal-averse SFA features yield lower MSE for value function approximation than goal-directed features.
- **Mechanism:** Value functions V*(s) peak near reward locations. Goal-averse policies produce low occupancy near rewards, causing SFA features to naturally amplify there—matching V*'s shape. Goal-directed policies flatten features where V* peaks, causing approximation error.
- **Core assumption:** Approximation uses a linear basis Ĝ(s) = wᵀg(s) with ordinary least squares; V* is known (ground truth).
- **Evidence anchors:**
  - [Section 7.1]: "Goal-averse behavior leads to features that perform better in value function approximation when compared to uniform features."
  - [Figure 7]: Visual showing flattened approximation near reward for goal-directed features.
  - [corpus]: "Goals and the Structure of Experience" discusses goal-directed vs. exploratory behavior but not SFA scaling specifically.
- **Break condition:** If the value function shape differs substantially (e.g., negative rewards, multiple goals), alignment may not hold.

### Mechanism 3: Scale Correction Redistributes Feature Variance Uniformly
- **Claim:** Post-hoc rescaling by D^(1/2)Y improves goal-directed feature performance but degrades goal-averse feature performance.
- **Mechanism:** Scale correction transforms features from Y^TDY = I to Y^TY = I by multiplying each state's features by √μᵢ. This undoes occupancy-induced scaling, making features more uniform. For goal-directed features, this fixes the flattening problem; for goal-averse, it removes the beneficial alignment with V*.
- **Core assumption:** An estimate of the stationary distribution μ is available for rescaling.
- **Evidence anchors:**
  - [Section 6]: "The feature of a point can be rescaled by multiplication with √μᵢ."
  - [Figure 11]: Shows improvement (red) for goal-directed and degradation (blue) for goal-averse after correction.
  - [corpus]: No corpus papers examine this correction mechanism.
- **Break condition:** If μ is poorly estimated (sparse sampling), correction introduces noise rather than improvement.

## Foundational Learning

- **Concept: Ergodic Markov Chains and Stationary Distributions**
  - **Why needed here:** The entire SFA-on-Markov-chains formulation depends on μ existing and being reachable from any state. Non-ergodic chains break the theoretical guarantees.
  - **Quick check question:** Given a transition matrix P, does lim(t→∞) P^t converge to a unique μ independent of the starting state?

- **Concept: Generalized Eigenvalue Problem (Ax = λBx)**
  - **Why needed here:** Optimal SFA features are the smallest non-trivial eigenvectors of (D−M)y = λDy (Eq. 21). Understanding this connects SFA to spectral graph methods.
  - **Quick check question:** How do you compute the k smallest non-trivial eigenvectors when D is diagonal and M is sparse?

- **Concept: Value Function Approximation in RL**
  - **Why needed here:** The paper evaluates SFA features by how well they linearly approximate V*(s). Understanding V* = E[Σ γ^t R(s_t)] clarifies why peak alignment matters.
  - **Quick check question:** For a 1D chain with reward at state 90 and γ=0.95, sketch V*(s).

## Architecture Onboarding

- **Component map:**
  - Environment -> Policy π -> Transition matrix P^π -> Stationary distribution μ
  - M = (1/2)(μᵤPᵤᵥ + μᵥPᵥᵤ), D = diag(μ)
  - Generalized eigenvalue solve: (D−M)Y = DYA
  - Optional corrections: LRA (modify M), Scale (post-multiply by D^(1/2))
  - Linear regression: w = (Y^T Y)⁻¹ Y^T V*

- **Critical path:**
  1. Define environment topology (1D/2D lattice) and reward locations.
  2. Choose behavior policy (ζ-greedy or Boltzmann with β).
  3. Compute P^π, solve for μ analytically or by power iteration.
  4. Construct M and D, solve (D−M)Y = DYA for k+1 smallest eigenvectors.
  5. Discard trivial eigenvector y₀ = 1, keep k slowest features.
  6. Optionally apply scale correction: Y ← D^(1/2)Y.
  7. Fit linear regression to V* using Y as basis.

- **Design tradeoffs:**
  - **ζ-greedy vs. Boltzmann:** ζ-greedy produces extreme scaling; Boltzmann distributes occupancy more evenly but requires tuning β. Paper finds Boltzmann + scale correction best in 2D.
  - **LRA vs. Scale correction:** LRA modifies the objective before solving (requires knowing Pᵤᵥ); scale correction is post-hoc (requires μ). Paper finds LRA ineffective; scale correction is preferred.
  - **Goal-directed vs. goal-averse:** Without correction, goal-averse is better; with scale correction, near-uniform (β ≈ 0) is best.

- **Failure signatures:**
  - **Numerical instability:** Extreme goal-directed/averse policies cause μ near zero at some states → D has near-zero entries → generalized eigenvalue solve becomes ill-conditioned. Visible as artifacts at grid boundaries in figures.
  - **Degraded approximation after scale correction on goal-averse:** Blue regions in Figure 11 indicate when correction hurts—do not apply blindly.
  - **Wrong eigenvector ordering:** The trivial eigenvector y₀ = 1 must be discarded; otherwise, constant features dominate.

- **First 3 experiments:**
  1. Replicate the 1D birth-death process (N=50, θ∈{0.45, 0.48, 0.50, 0.52, 0.55}), plot features and μ. Verify scaling bound at extreme states.
  2. On a 10×10 lattice with reward at center vs. corner, compare ζ-greedy (ζ=0.3) vs. Boltzmann (β=2) features with and without scale correction. Report MSE for V* using 5, 10, 20 features.
  3. Sensitivity test: Add small uniform noise to μ before scale correction. At what noise level does correction stop helping goal-directed features?

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the detrimental scaling effects of goal-directed behavior and the efficacy of the proposed corrections transfer to SFA implementations restricted to fixed function approximator families (e.g., neural networks)?
- Basis: [explicit] The authors state that SFA is typically bound to specific architectures and "it is unclear to what extent the features in such a setting exhibit the same effects."
- Why unresolved: The paper derives optimal features analytically via a generalized eigenvalue problem, which assumes unrestricted functional mappings, unlike the constrained capacity of standard function approximators.
- What evidence would resolve it: Experiments repeating the lattice environment analysis using Deep SFA (gradient-based) or Graph Neural Networks to verify if the flattening effect persists and if scale correction remains beneficial.

### Open Question 2
- Question: Can "over-correcting" the feature scale to explicitly mimic goal-averse distributions yield superior value function approximation compared to the standard correction toward uniform stationary distributions?
- Basis: [explicit] The authors note that the degree of correction is a hyperparameter and suggest that "over-correction toward generally better performing goal-averse features is possible."
- Why unresolved: The current work only evaluates the standard scale correction (moving from Y^T D Y to Y^T Y), leaving the potential benefits of aggressive scaling adjustments untested.
- What evidence would resolve it: Ablation studies varying the magnitude of the scale correction factor to determine the optimal scaling point relative to the goal-averse baseline.

### Open Question 3
- Question: How sensitive is the scale correction mechanism to errors in the estimated stationary distribution when applied to finite, online reinforcement learning samples?
- Basis: [inferred] The paper utilizes an "infinite-sample" assumption and notes that scale correction "requires at least an estimate of the stationary distribution."
- Why unresolved: The robustness of the correction is unknown in practical settings where the stationary distribution must be approximated from finite trajectories, potentially introducing estimation noise.
- What evidence would resolve it: Analysis of regression performance when the scale correction is derived from a running estimate of state visitation counts rather than the analytical solution.

## Limitations

- The analysis is limited to discrete Markov chains and may not generalize to continuous state spaces.
- The evaluation uses linear function approximation, which may not capture the full utility of SFA features for other downstream tasks like policy learning.
- The study assumes ergodic Markov chains; non-ergodic chains with absorbing states could produce degenerate feature sets.

## Confidence

- **High confidence:** The theoretical derivation of the scaling bound μᵢyᵢ² ≤ 1 and its visualization in Figure 7 are robust and directly supported by the mathematics.
- **Medium confidence:** The claim that scale correction improves goal-directed features but harms goal-averse ones is supported by Figure 11, though the magnitude of improvement could depend on hyperparameter choices not fully explored.
- **Medium confidence:** The conclusion that LRA correction shows limited benefit is based on limited comparison; the mechanism is sound but implementation details (e.g., how to estimate Pᵤᵥ in practice) are not fully addressed.

## Next Checks

1. **Ergodicity test:** Construct a Markov chain with absorbing states or periodic structure and verify that the scaling bound fails or features become degenerate.
2. **Continuous state space:** Apply the same analysis to a continuous 1D environment (e.g., pendulum with goal state) and check if occupancy-based scaling generalizes beyond discrete grids.
3. **Policy learning evaluation:** Instead of value function approximation, train a policy using the corrected vs. uncorrected features and compare learning curves to assess if scale correction aids or hinders control.