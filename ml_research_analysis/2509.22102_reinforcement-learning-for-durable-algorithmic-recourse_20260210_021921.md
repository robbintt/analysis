---
ver: rpa2
title: Reinforcement Learning for Durable Algorithmic Recourse
arxiv_id: '2509.22102'
source_url: https://arxiv.org/abs/2509.22102
tags:
- recourse
- candidates
- time
- recommendations
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of algorithmic recourse in competitive,
  resource-constrained environments, where recommendations can become invalid over
  time due to shifting applicant pools. The authors introduce a reinforcement learning-based
  framework that models the dynamic feedback between recommendations and candidate
  behavior.
---

# Reinforcement Learning for Durable Algorithmic Recourse

## Quick Facts
- **arXiv ID:** 2509.22102
- **Source URL:** https://arxiv.org/abs/2509.22102
- **Reference count:** 40
- **Key outcome:** Introduces RL-based framework for durable algorithmic recourse in competitive, resource-constrained environments; achieves superior balance between feasibility and long-term validity of recommendations.

## Executive Summary
This paper addresses the problem of algorithmic recourse in competitive environments where recommendations can become invalid over time due to shifting applicant pools. The authors introduce a reinforcement learning framework that models dynamic feedback between recommendations and candidate behavior, explicitly accounting for feature modification difficulties. Their approach designs recommendations to remain valid over a predefined time horizon T. Extensive experiments in synthetic simulation environments demonstrate that their method substantially outperforms existing baselines.

## Method Summary
The framework uses hierarchical reinforcement learning with two policies: µ (predictor) selects target scores and ϕ (recommender) generates counterfactuals to reach those scores. ϕ is pre-trained independently to reduce complexity, then frozen while µ learns in a partially observable Markov decision process that models competitive dynamics. The approach incorporates online difficulty estimation for features and trains policies using soft actor-critic (SAC) with rewards balancing reliability and feasibility.

## Key Results
- Achieves superior balance between feasibility and long-term validity compared to baselines
- Hierarchical decomposition reduces action space complexity and stabilizes learning
- Adaptive feature difficulty estimation improves recommendation feasibility without sacrificing accuracy
- Explicitly modeling competitive dynamics enables target scores that remain valid over multi-step horizons

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical decomposition of recourse into goal selection and counterfactual generation reduces action space complexity and stabilizes learning.
- **Core assumption:** Goal score and feature modification are sufficiently decoupled that sequential training does not critically degrade joint optimality.
- **Evidence:** Pre-training ϕ independently allows µ to focus on validity-feasibility trade-off without jointly learning feature modifications.
- **Break condition:** If goal scores and optimal feature paths exhibit strong conditional dependencies, hierarchical decomposition may yield suboptimal policies.

### Mechanism 2
- **Claim:** Adaptive feature difficulty estimation prioritizes easier feature changes, improving feasibility without sacrificing accuracy.
- **Core assumption:** The parametric form linking difficulty to success probability is correctly specified, and β is known or absorbed into d_i estimates.
- **Evidence:** Recommender policy learns feature difficulty estimates online by comparing predicted vs. observed modification success.
- **Break condition:** If feature difficulties vary across individuals rather than being feature-specific, the global estimate will misguide recommendations for subpopulations.

### Mechanism 3
- **Claim:** Explicitly modeling competitive dynamics enables target scores that remain valid over multi-step horizons.
- **Core assumption:** Candidate behavior follows the specified parametric forms for dropout, reapplication, and modification success.
- **Evidence:** Predictor µ is trained in POMDP where rejected candidates reapply after implementing recommendations, and new candidates enter.
- **Break condition:** If real-world candidate behavior deviates significantly from simulation's parametric assumptions, learned policies may not transfer.

## Foundational Learning

**Partially Observable Markov Decision Processes (POMDPs)**
- **Why needed:** The environment is partially observable because candidates' feature modifications and exits are hidden until reapplication.
- **Quick check:** Can you explain why adding historical metadata to the observation restores the Markov property for the predictor?

**Soft Actor-Critic (SAC)**
- **Why needed:** SAC handles continuous action spaces with sample efficiency via off-policy learning and entropy regularization for exploration.
- **Quick check:** What assumption does SAC break that prevents theoretical convergence guarantees in this setting?

**Algorithmic Recourse (Counterfactual Explanations)**
- **Why needed:** Understanding the baseline problem—generating actionable feature changes to flip unfavorable decisions—is prerequisite to grasping why durability and competition matter.
- **Quick check:** Why does recommending the last-seen decision threshold fail in competitive settings?

## Architecture Onboarding

**Component map:**
Historical applicant metadata window -> Predictor policy (µ) -> Target score -> Recommender policy (ϕ) -> Counterfactual feature vector -> Simulation environment -> Reward feedback

**Critical path:**
1. Pre-train ϕ in single-candidate environment (reward = accuracy + cost; two-phase training)
2. Freeze ϕ; train µ in full multi-candidate POMDP (reward = α·log(RR) + τ·log(RF))
3. Deploy: µ selects target scores → ϕ generates counterfactuals → candidates receive recommendations

**Design tradeoffs:**
- **T (validity horizon):** Larger T improves durability but reduces feasibility and slows convergence
- **α vs. τ (reward weights):** Tune to navigate Pareto frontier between reliability (RR) and feasibility (RF)
- **β (global difficulty):** Lower β sharpens the validity-feasibility trade-off, making high-reliability harder

**Failure signatures:**
- High RR but very low RF: Policy recommends overly difficult changes; check difficulty estimates or increase τ
- Low RR with moderate RF: Policy underestimates competition; increase α or extend training
- Slow/unstable convergence for T>1: Normal per Figure 4; consider curriculum over T or more exploration

**First 3 experiments:**
1. Train ϕ with known difficulties; verify prediction error <0.01 and cost lower than Ustun baseline
2. Sweep α, τ for fixed (T=1, β=0.05); confirm Pareto front spans RR∈[0.2, 0.95] and dominates ARR
3. Fix RR≈0.95, vary T∈[1,5]; verify RF degrades gracefully and matches Figure 3 trends

## Open Questions the Paper Calls Out
None

## Limitations
- Hierarchical decomposition may yield suboptimal policies if goal scores and optimal feature paths exhibit strong conditional dependencies
- Global feature difficulty estimates may misguide recommendations for heterogeneous subpopulations with varying individual difficulties
- Parametric assumptions about candidate behavior may not capture real-world strategic responses like collective action or varying urgency levels

## Confidence
- Mechanism 1 (hierarchical decomposition): Medium - No ablation study tests impact of joint vs. sequential training
- Mechanism 2 (difficulty estimation): Medium - Validation limited to synthetic environments with known difficulties
- Mechanism 3 (competitive dynamics): Low - Real-world candidate behavior likely deviates from assumed parametric forms

## Next Checks
1. Conduct an ablation study comparing hierarchical vs. joint training of µ and ϕ to quantify the optimality gap from decomposition
2. Test the framework on real-world datasets with known ground truth feature difficulties to validate the online estimation procedure
3. Design experiments with candidate behavior that violates the parametric assumptions (e.g., collective action, non-stationary modification success rates) to assess robustness