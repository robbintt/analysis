---
ver: rpa2
title: Can large language models assist choice modelling? Insights into prompting
  strategies and current models capabilities
arxiv_id: '2507.21790'
source_url: https://arxiv.org/abs/2507.21790
tags:
- specifications
- llms
- claude
- specification
- choice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the potential of large language models
  (LLMs) to assist in discrete choice modelling, specifically in specifying and estimating
  Multinomial Logit models. A systematic experimental framework was implemented, evaluating
  thirteen versions of six leading LLMs across five configurations varying by prompting
  strategy (Zero-Shot vs.
---

# Can large language models assist choice modelling? Insights into prompting strategies and current models capabilities

## Quick Facts
- arXiv ID: 2507.21790
- Source URL: https://arxiv.org/abs/2507.21790
- Reference count: 10
- Proprietary LLMs (Claude 4 Sonnet, GPT models) can generate valid and behaviorally sound utility specifications for discrete choice models, particularly with structured Chain-of-Thought prompts.

## Executive Summary
This study investigates the potential of large language models to assist in discrete choice modelling by specifying and estimating Multinomial Logit models. Through systematic experimentation with thirteen versions of six leading LLMs across five configurations varying by prompting strategy and information availability, the authors evaluate LLM-generated specifications based on goodness-of-fit, behavioral plausibility, and model complexity. Results demonstrate that proprietary LLMs can produce valid utility specifications, with Claude 4 Sonnet consistently outperforming others. Notably, some LLMs performed better with limited data access, and GPT o3 uniquely estimated its own specifications correctly. The study reveals both the promise and current limitations of LLMs as assistive agents in choice modelling.

## Method Summary
The study implemented a systematic experimental framework using a subset of the Apollo inter-city mode choice synthetic dataset (500 individuals, 1,000 observations) with four alternatives (car, bus, air, rail) and covariates including travel time, cost, access time, gender, income, and business trip status. Thirteen LLM versions across six leading models were tested in five configurations varying prompting strategy (Zero-Shot vs. Chain-of-Thoughts) and information availability (full dataset vs. data dictionary). LLM-generated specifications were implemented, estimated, and assessed based on goodness-of-fit metrics (Log-Likelihood, AIC, BIC), behavioral plausibility, and model complexity. Proprietary models were accessed via browser interfaces with memory disabled, while open-weight models used local deployment or APIs.

## Key Results
- Proprietary LLMs (Claude 4 Sonnet, GPT models) can generate valid and behaviorally sound utility specifications, particularly with structured Chain-of-Thought prompts
- Some LLMs performed better with limited data access, suggesting that reducing raw data may enhance internal reasoning capabilities
- GPT o3 uniquely estimated its own specifications correctly, while other models either mis-estimated or hallucinated results
- Open-weight models struggled to produce meaningful specifications compared to proprietary counterparts

## Why This Works (Mechanism)

### Mechanism 1
Structured Chain-of-Thought prompting improves behavioral plausibility and statistical validity by decomposing the specification task into intermediate steps (descriptive analysis, behavioral constraints checking, syntax definition). This forces the model to retrieve domain-specific heuristics (e.g., "cost coefficients must be negative") before generating the final mathematical form, reducing the probability of generating syntactically correct but theoretically invalid models. The mechanism assumes the LLM possesses latent parametric knowledge of discrete choice theory accessible through step-by-step reasoning traces.

### Mechanism 2
Constraining input information to metadata (data dictionaries) rather than full datasets can enhance specification quality by reducing "attention dilution." Processing large raw datasets consumes context window and computational resources. By providing only the schema (variable names, types), the model reallocates inference capacity toward mapping theoretical constructs to variables (semantic reasoning) rather than parsing rows of data (syntactic processing). The mechanism assumes the bottleneck is cognitive load of context processing rather than lack of statistical estimation capability.

### Mechanism 3
End-to-end reliability (specification + estimation) currently depends on tool use capability (code execution) rather than purely textual inference. Text-only LLMs tend to hallucinate estimation metrics (fabricating Log-Likelihood values). Models capable of generating and executing code (e.g., GPT o3) create a verifiable feedback loop where the specification is mathematically tested before the result is reported, grounding the output in actual arithmetic rather than probabilistic text. The mechanism assumes the LLM has access to a sandboxed environment with necessary libraries and can self-correct code errors.

## Foundational Learning

- **Concept**: Multinomial Logit (MNL) Utility Specification
  - **Why needed here**: The core task is generating valid utility functions ($V$). You must understand Alternative Specific Constants (ASCs), generic vs. alternative-specific parameters, and interaction terms to evaluate if the LLM is "behaving."
  - **Quick check question**: If an LLM suggests a generic cost parameter for all modes but an alternative-specific time parameter for "car," is this a valid structure?

- **Concept**: Prompt Engineering (Zero-Shot vs. Chain-of-Thought)
  - **Why needed here**: The paper frames the interaction strategy as a causal variable for performance. You need to distinguish between asking "Do X" (ZSP) and "Think about A, then B, then do X" (CoT).
  - **Quick check question**: Does CoT improve results by adding more data, or by structuring the latent knowledge retrieval process?

- **Concept**: Behavioral Plausibility vs. Goodness-of-Fit
  - **Why needed here**: The evaluation criteria are dual. A model can have high fit (AIC) but be theoretically wrong (positive price coefficient). You must learn to screen for sign consistency and significance, not just low Log-Likelihood.
  - **Quick check question**: If an LLM generates a model with a positive coefficient for Travel Cost, should it be accepted if it has the lowest AIC?

## Architecture Onboarding

- **Component map**: Data (CSV/Data Dictionary) -> Prompt (ZSP/CoT) -> LLM (Claude/GPT) -> Specification (Code/Math) -> Verification (Independent Estimator) -> Results
- **Critical path**: Define Prompt Strategy (CoT recommended) → LLM generates Specification (Code/Math) → Human/System Verification: Run specification through independent estimator → Filter for behavioral plausibility (sign checks)
- **Design tradeoffs**: Claude vs. GPT: Claude 4 Sonnet tends toward complexity and better AIC/BIC but has occasional convergence issues. GPT variants prioritize robustness and convergence but may be simpler. Data Access: Full data allows distribution checking but risks "distracting" the model. Data dictionaries are safer for theoretical mapping.
- **Failure signatures**: Hallucination: LLM reports specific LL values that cannot be reproduced when re-estimated. Omission: Missing ASCs in labeled choice experiments. Sign Error: Positive cost/time coefficients.
- **First 3 experiments**:
  1. Prompt Sensitivity Test: Run same dataset using ZSP vs. CoT on GPT-4o. Verify if CoT reduces omission of ASCs.
  2. Hallucination Audit: Ask text-only LLM (e.g., Gemini) to "Suggest & Estimate." Re-run estimation independently to calculate error rate of reported LL values.
  3. Data Constriction Test: Provide complex dataset to Claude 3.7 Sonnet. Run two conditions: (A) Full CSV, (B) Data Dictionary only. Compare AIC values to test "less is more" hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
Can LLMs reliably specify and estimate advanced discrete choice models, such as Mixed Logit or Nested Logit, beyond the Multinomial Logit (MNL) models tested? The authors state they limited focus to MNL models as a "proof of concept" and that "extensions to more complex specifications remains an avenue for future work." The current study only evaluated linear-in-parameter utility specifications, leaving capability to handle complex error structures or random taste variation untested.

### Open Question 2
Do the specification capabilities of leading LLMs generalize to real-world, non-synthetic datasets with larger sample sizes and noisier variables? The study relies entirely on a specific "synthetic revealed preference (RP) data" subset distributed with Apollo software. Synthetic data is typically cleaner and may conform more closely to theoretical ideals than observational data, potentially inflating perceived LLM performance.

### Open Question 3
Through what mechanism does limiting raw data access (providing only a data dictionary) improve model specification performance of certain LLMs? The authors observe that "limiting raw data access may enhance internal reasoning capabilities" but do not definitively explain the underlying cause of this counter-intuitive result. The paper identifies the correlation but leaves the "why" as a hypothesis.

## Limitations

- Experimental design is robust but dataset is synthetic and limited to single mode choice scenario, restricting external validity
- Comparison between full data and data dictionary conditions based on qualitative performance patterns rather than controlled statistical tests
- Hallucination detection relies on post-hoc verification which may miss subtle specification errors
- Study does not evaluate long-term stability of LLM-generated specifications or performance on datasets with missing variables or non-standard naming conventions

## Confidence

- **High confidence**: Proprietary LLMs (Claude 4 Sonnet, GPT models) can generate behaviorally plausible utility specifications when guided by structured prompts
- **Medium confidence**: Chain-of-Thought prompting consistently improves specification quality compared to Zero-Shot prompting
- **Medium confidence**: Some LLMs perform better with limited data access
- **Low confidence**: Open-weight models cannot reliably generate meaningful specifications

## Next Checks

1. External dataset validation: Test prompting strategies on real-world mode choice dataset with known economic theory implications to verify if observed performance patterns generalize beyond synthetic data

2. Hallucination robustness test: Systematically evaluate frequency and severity of hallucinated estimates across all tested LLMs using standardized estimation verification protocol to quantify reliability gap

3. Variable naming sensitivity analysis: Create controlled experiments varying variable name clarity (standard vs. ambiguous) while keeping underlying data structure constant to test limits of "less is more" hypothesis