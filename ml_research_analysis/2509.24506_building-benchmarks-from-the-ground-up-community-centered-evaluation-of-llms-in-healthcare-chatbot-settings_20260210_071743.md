---
ver: rpa2
title: 'Building Benchmarks from the Ground Up: Community-Centered Evaluation of LLMs
  in Healthcare Chatbot Settings'
arxiv_id: '2509.24506'
source_url: https://arxiv.org/abs/2509.24506
tags:
- evaluation
- data
- arxiv
- queries
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Samiksha, a community-centered evaluation
  pipeline for large language models (LLMs) in healthcare chatbot settings. The authors
  co-designed the pipeline with civil society organizations (CSOs) and community members
  in India to ensure evaluations reflect real user needs and cultural contexts.
---

# Building Benchmarks from the Ground Up: Community-Centered Evaluation of LLMs in Healthcare Chatbot Settings

## Quick Facts
- arXiv ID: 2509.24506
- Source URL: https://arxiv.org/abs/2509.24506
- Reference count: 40
- Primary result: Community-centered pipeline co-creating healthcare queries with CSOs and workers produces culturally grounded benchmarks; human evaluators detect nuanced errors that LLM judges miss.

## Executive Summary
This paper introduces Samiksha, a community-centered evaluation pipeline for large language models (LLMs) in healthcare chatbot settings. The authors co-designed the pipeline with civil society organizations (CSOs) and community members in India to ensure evaluations reflect real user needs and cultural contexts. They conducted interviews with five CSOs to identify healthcare query themes, then engaged 15 data workers to create and localize 1,590 queries across three Indian languages (Hindi, Kannada, Malayalam). The resulting benchmark was used to evaluate three state-of-the-art multilingual LLMs. Both human annotators and LLM-as-judge methods assessed model responses across four dimensions: clarity, helpfulness, accuracy, and completeness. Human evaluators were more sensitive to nuanced quality differences than LLM judges, who tended to produce compressed, near-ceiling scores. The study highlights the importance of community-driven evaluation in capturing cultural and linguistic nuances, offering a scalable template for contextually grounded and inclusive LLM benchmarking.

## Method Summary
The study employed a three-phase pipeline to create and evaluate healthcare benchmarks. Phase 1 involved CSO interviews to extract healthcare query themes. Phase 2 engaged data workers to create and localize 1,590 queries across three Indian languages, embedding cultural and linguistic diversity. Phase 3 evaluated three LLMs using both human annotators and LLM judges (GPT-4o, Sarvam-M) on four dimensions: clarity, helpfulness, accuracy, and completeness. Responses were generated with a "health expert" persona, and evaluations used 3-point ordinal scales with comparative analysis to assess inter-judge alignment.

## Key Results
- LLM judges showed moderate internal consistency (Pearson ≈ 0.40) but weak alignment with human evaluators (Pearson ≈ 0.13), particularly for factual accuracy and cultural nuance.
- Human evaluators detected subtle errors and contextual issues that LLM judges consistently missed, demonstrating a ceiling effect in LLM scoring.
- The community-centered pipeline produced queries reflecting social determinants of health, including stigma, family dynamics, and traditional remedies, which generic benchmarks overlook.

## Why This Works (Mechanism)

### Mechanism 1: Stakeholder Layering for Grounded Query Generation
- Claim: Combining CSO expertise with community data workers produces queries that surface culturally-embedded health concerns invisible to synthetic or translated benchmarks.
- Mechanism: CSOs identify domain themes (e.g., reproductive health, chronic conditions) and share real log samples. Data workers then create queries in their native languages, drawing on lived experience. This multi-stage process produces queries that embed social determinants (e.g., family authority, traditional remedies, stigma) rather than isolated medical facts.
- Core assumption: Local annotators with lived experience can articulate contextual nuances better than translations of English-origin benchmarks.
- Evidence anchors:
  - [abstract] "The approach co-creates benchmarks with civil-society organizations (CSOs) and community members, ensuring evaluations reflect real-world needs, cultural contexts, and linguistic diversity."
  - [section 3.2] Data workers created 810 queries across three languages and eight topics; queries addressed "social determinants of health—including socio-economic conditions, cultural beliefs and stigma, and gender norms."
  - [corpus] Related work on cultural sensitivity in LLM chatbots (e.g., Deva et al. 2025) supports the need for contextually grounded benchmarks, but corpus lacks direct replication of this specific stakeholder-layering mechanism.
- Break condition: If data workers lack domain familiarity or if CSO themes are too abstract, queries may revert to generic medical questions without cultural context.

### Mechanism 2: LLM-as-Judge Calibration Failure in Multilingual Settings
- Claim: LLM judges show moderate internal consistency but weak alignment with human evaluators, particularly for factual accuracy and cultural nuance.
- Mechanism: LLM judges (GPT-4o, Sarvam-M) apply rubrics similarly, producing compressed, near-ceiling scores. Human annotators give lower, more dispersed scores and penalize missing information or subtle factual errors. This divergence suggests LLM judges share a model-centric quality signal that underweights language-specific and contextual fidelity.
- Core assumption: Human evaluators with lived experience are the ground truth for contextual quality.
- Evidence anchors:
  - [abstract] "While LLM judges showed consistency among themselves, they diverged from human judgments, particularly in detecting factual errors and contextual nuances."
  - [section 4.3] "LLM judges exhibit moderate mutual alignment (Pearson ≈ 0.40), while both LLM judges correlate only weakly with human annotators (Pearson ≈ 0.13)."
  - [corpus] MEDAL (Liu et al. 2025) confirms LLM judges struggle in multilingual dialogue evaluation, but corpus lacks direct comparison of human-LLM agreement gaps in healthcare settings.
- Break condition: If human annotators are not native speakers or lack domain familiarity, human-LLM alignment may appear higher due to shared superficial judgments.

### Mechanism 3: Hybrid Evaluation for Scalable Ground-Truthing
- Claim: A combined human-LLM evaluation workflow—using LLMs for high-throughput triage and humans for calibration on high-stakes cases—can balance scale and fidelity.
- Mechanism: Standalone ratings provide coarse rankings; comparative evaluations reveal judge-dependent preferences. By identifying where LLM judges diverge from humans (e.g., factual errors, cultural tone), targeted human review can focus on the most risky or contentious responses.
- Core assumption: LLM judges are sufficient for relative ranking but not absolute quality assessment in high-stakes domains.
- Evidence anchors:
  - [section 5.3] "LLM-judges cannot be used as replacements for human evaluators... A hybrid evaluation workflow in which LLM-judges perform high-throughput ranking and triage, and targeted human annotation validates and calibrates the most important or risky decisions, may be a solution."
  - [corpus] No direct corpus evidence on hybrid workflows in healthcare; this is an inference from the paper's findings.
- Break condition: If LLM triage misses systematic error categories (e.g., cultural insensitivity), human reviewers may never see critical failures.

## Foundational Learning

- Concept: **LLM-as-Judge Paradigm**
  - Why needed here: The paper uses GPT-4o and Sarvam-M as automated evaluators; understanding their biases is essential to interpret results.
  - Quick check question: Can you explain why LLM judges might produce higher scores than humans for the same response?

- Concept: **Inter-Evaluator Agreement Metrics**
  - Why needed here: The paper reports Pearson correlation between judges; knowing when correlation is meaningful vs. spurious is critical.
  - Quick check question: If two judges agree 80% of the time but both miss the same error class, is that good agreement?

- Concept: **Cultural Grounding in Benchmarks**
  - Why needed here: Queries embed social determinants (stigma, family dynamics, traditional medicine); evaluators must recognize these as valid dimensions.
  - Quick check question: How would you detect if a benchmark query reflects a genuine cultural concern vs. a stereotype?

## Architecture Onboarding

- Component map:
  - Phase 1 (Query Curation): CSO interviews → theme extraction → example queries
  - Phase 2 (Query Generation): Data worker tasks (creation + localization) → quality review → benchmark dataset
  - Phase 3 (Response Evaluation): LLM response generation → human evaluation + LLM-as-judge → comparative analysis
  - Infrastructure: Karya platform (task deployment, worker management), rubrics, data storage

- Critical path:
  1. CSO engagement (theme definition, rubric co-design)
  2. Data worker recruitment and training
  3. Query creation/localization with coordinator feedback
  4. Response generation using target LLMs
  5. Parallel human and LLM evaluation
  6. Agreement analysis and calibration

- Design tradeoffs:
  - **Scale vs. fidelity**: Pure human evaluation is costly; pure LLM evaluation misses cultural nuance.
  - **Specificity vs. generalizability**: Domain-specific CSO insights may not transfer; overly broad themes lose context.
  - **Example bias vs. guidance**: Example queries help data workers but can constrain creativity (e.g., multiple "animal bite" queries after a snakebite example).

- Failure signatures:
  - Queries become generic or repetitive (insufficient context)
  - LLM judges give near-ceiling scores while humans penalize errors (calibration gap)
  - Data workers produce translations rather than culturally adapted queries
  - CSO themes are too narrow, missing cross-cutting concerns

- First 3 experiments:
  1. **Baseline agreement test**: Run 50 queries through human and LLM evaluation; compute correlation. If <0.2, confirm need for hybrid workflow.
  2. **Rubric sensitivity analysis**: Vary rubric granularity (3-point vs. 5-point scale); measure whether LLM judge compression persists.
  3. **Query diversity audit**: After initial query generation, cluster queries by topic and linguistic pattern; flag over-representation from example bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a hybrid evaluation workflow be optimized to leverage LLM judges for scalable triage while relying on human annotators to validate high-stakes decisions involving factual errors and cultural nuance?
- Basis in paper: [explicit] The authors state that "identifying the most effective, efficient ways to incorporate community input into these evaluations is a key research challenge going forward," and suggest a hybrid workflow as a potential solution to the costliness of purely human evaluation.
- Why unresolved: The study found that while LLM judges agree with each other, they diverge significantly from human judgments (Pearson ≈ 0.13) and suffer from a "ceiling effect" that misses subtle factual errors.
- What evidence would resolve it: A comparative study of different triage protocols (e.g., LLM-first filtering vs. random spot-checking) measuring the trade-off between evaluation cost and alignment with ground-truth human assessments.

### Open Question 2
- Question: To what extent does providing topic-specific example queries bias the semantic range of community-generated queries, and can automated de-duplication effectively restore query diversity?
- Basis in paper: [explicit] The authors observe that "these examples introduced some bias," noting that an example about snake bites caused workers to generate queries about rat bites. They propose using domain-level examples and automated steps to mitigate this in future work.
- Why unresolved: The pilot study confirmed the existence of this "exemplar bias," but the proposed mitigation strategies (domain-level examples, automated similarity detection) have not yet been tested or validated against a control group.
- What evidence would resolve it: An ablation study comparing query diversity and semantic distribution across conditions: one with specific examples, one with domain-level descriptions, and one with automated similarity rejection enabled.

### Open Question 3
- Question: Can the Samiksha pipeline maintain its validity and depth of cultural insight when applied to high-stakes domains other than healthcare, such as legal aid or agriculture?
- Basis in paper: [explicit] The Conclusion states, "We see Samiksha as a template that can be extended to additional languages, contexts and domains to create a comprehensive community-driven benchmark."
- Why unresolved: The current study is limited to the healthcare domain in India; the reliance on CSO expertise and specific health-related cultural nuance may not translate directly to domains with different risk profiles or knowledge structures.
- What evidence would resolve it: Replicating the three-phase pipeline in a different domain (e.g., agricultural support for farmers) and analyzing the resulting benchmark for similar levels of cultural groundedness and inter-evaluator agreement.

## Limitations

- Prompt templates for response generation and LLM evaluation are not disclosed, hindering faithful reproduction.
- Human evaluator composition (native speakers, healthcare expertise) is not specified, limiting interpretation of human-LLM agreement gaps.
- No external validation that embedded social determinants reflect genuine community concerns versus researcher assumptions.

## Confidence

- High confidence: The community-centered pipeline design and stakeholder layering mechanism are well-supported by the methodology description and interview process.
- Medium confidence: The LLM-as-judge calibration failure is evidenced but could reflect specific judge selection rather than a universal phenomenon.
- Medium confidence: The hybrid evaluation recommendation is logically derived from results but lacks direct empirical validation in this study.

## Next Checks

1. **Prompt transparency test**: Request and test the exact prompt templates for both response generation and LLM evaluation to verify if prompt structure drives the observed judge behavior.
2. **Cross-cultural replication**: Apply the same pipeline to a different cultural context (e.g., another Indian state or country) to test whether the stakeholder-layering mechanism generalizes beyond the original setting.
3. **Judge calibration experiment**: Conduct a controlled study comparing LLM judges with and without explicit cultural sensitivity training prompts to isolate whether judge divergence stems from inherent limitations or lack of cultural grounding.