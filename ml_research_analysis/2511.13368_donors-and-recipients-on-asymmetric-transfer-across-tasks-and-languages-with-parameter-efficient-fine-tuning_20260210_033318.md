---
ver: rpa2
title: 'Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with
  Parameter-Efficient Fine-Tuning'
arxiv_id: '2511.13368'
source_url: https://arxiv.org/abs/2511.13368
tags:
- b-instruct
- qwen2
- llama-3
- transfer
- truthfulqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how parameter-efficient fine-tuning on one
  task-language pair affects performance across all other task-language combinations.
  Using a controlled grid of four benchmarks and eleven languages, the authors fine-tune
  each model on a single source cell and measure transfer as percentage-point changes
  across target pairs, decomposing results into matched-task cross-language, matched-language
  cross-task, and cross-task cross-language regimes.
---

# Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning

## Quick Facts
- **arXiv ID:** 2511.13368
- **Source URL:** https://arxiv.org/abs/2511.13368
- **Reference count:** 40
- **Primary result:** Matched-task cross-language transfer under LoRA fine-tuning is most reliable, with target language properties dominating variance and forming stable donor-recipient hierarchies.

## Executive Summary
This study systematically examines how single-source LoRA fine-tuning affects performance across all other task-language combinations in a controlled multilingual grid. The results show pronounced asymmetry: matched-task cross-language transfer yields reliable gains with high win rates and low harm rates, while off-task regimes produce smaller improvements and higher degradation risk. A stable donor-recipient hierarchy emerges, with high-resource languages and broad semantic tasks acting as efficient hubs, and low-resource languages as fragile recipients. Transfer magnitudes are driven primarily by target-side properties, and while coarse regularities are stable across model families, fine-grained donor rankings are not. These findings provide risk-aware fine-tuning heuristics that prioritize matched-task sources and flag high-harm donors to maximize downstream gains.

## Method Summary
The authors create a controlled grid of 4 benchmarks × 11 languages (44 cells per model), establishing zero-shot baselines for all combinations. Each cell serves as a single-source for LoRA fine-tuning (r=32, α=64) with 3 epochs max and early stopping. After fine-tuning, models are evaluated on all 43 target cells to measure transfer as percentage-point changes. Results are partitioned into three regimes: matched-task cross-language (MT-CL), matched-language cross-task (ML-CT), and cross-task cross-language (CT-CL). Variance decomposition via mixed-effects models reveals that target properties dominate transfer variance (75.43% in MT-CL), while donor and model effects are smaller. Donor-recipient scores quantify each cell's propensity to export gains or absorb improvements.

## Key Results
- Matched-task cross-language transfer is most reliable: +1.25 pp mean gain, 66.40% win rate, 7.10% harm rate
- Target language properties explain 75.43% of variance in MT-CL regime, source explains only 8.75%
- Stable donor-recipient hierarchy emerges: high-resource languages and broad semantic tasks act as hubs; low-resource languages as fragile recipients
- Coarse-grained regularities are stable across model families, but fine-grained donor rankings are model-specific

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Matched-task cross-language transfer is more reliable and beneficial than cross-task transfer under single-source LoRA fine-tuning.
- **Mechanism:** LoRA's low-rank updates capture task-specific "reasoning templates" that are language-agnostic and can be projected onto linguistically aligned targets. When the task is held constant, fine-tuning modifies representations that encode abstract problem-solving patterns rather than surface-level linguistic features, enabling cross-lingual generalization.
- **Core assumption:** The fine-tuned adapter learns transferable reasoning structures, not just dataset-specific patterns or format cues.
- **Evidence anchors:**
  - [abstract] "Matched-Task (Cross-Language) transfer emerges as the most effective and predictable regime, driven principally by the identity of the target language rather than model architecture."
  - [Section 4.1.1] MT–CL yields mean gain +1.25 pp, win rate 66.40%, harm rate 7.10% vs. ML–CT (+0.81 pp, 56.90% win, 11.40% harm) and CT–CL (+0.78 pp, 57.70% win, 9.50% harm).
  - [Section A.7] Syntactic distance strongly predicts transfer (ρ = -0.605, p < 0.001), while inventory distance shows no significant effect—suggesting structural compatibility drives transfer more than vocabulary overlap.
  - [corpus] Cross-Lingual Transfer of Cultural Knowledge (arXiv:2506.01675) documents similar asymmetric transfer phenomena where resource quality can outweigh linguistic proximity.
- **Break condition:** If the source task requires language-specific knowledge (e.g., culturally grounded reasoning) rather than abstract reasoning, cross-language transfer degrades. Global-MMLU shows higher harm rates (21.5%) vs. machine-translated benchmarks (2.3%), suggesting curated/knowledge-intensive content transfers less reliably.

### Mechanism 2
- **Claim:** Transfer success is determined primarily by the recipient (target) language's receptivity, not by donor language choice or model architecture.
- **Mechanism:** Pre-existing representation quality in the target language determines how effectively transferred patterns can be mapped onto the target space. High-resource languages have better-aligned representations that can absorb cross-lingual features; low-resource languages have sparse or misaligned representations that lose signal during projection.
- **Core assumption:** The quality and coverage of a language's representations in the base model establish a ceiling for incoming transfer.
- **Evidence anchors:**
  - [Section 4.2, Table 2] In MT–CL regime, target language explains 75.43% of variance, source explains 8.75%, model explains 4.22%.
  - [Section 4.1.3, Figure 2] Spanish and Italian are disproportionately strong recipients relative to donor strength; Bengali and Hindi sit well below the diagonal with low recipient scores (0.60, 0.84) despite competitive donor scores.
  - [Section 5.2] "Transfer is often receptivity-limited on the target side: even when a donor exports a beneficial update, part of the signal can be lost when mapped onto targets with weaker coverage or representation mismatch."
  - [corpus] Language Fusion for Parameter-Efficient Cross-lingual Transfer (arXiv:2501.06892) addresses the "under-representation" problem in multilingual settings, aligning with the receptivity-limited mechanism.
- **Break condition:** When the target language has extremely sparse representations (very low-resource languages beyond the studied set), even syntactically close donors may fail to transfer. The paper studied 11 languages; extrapolation to languages with substantially less pre-training data is not tested.

### Mechanism 3
- **Claim:** Transfer forms a directed graph where some tasks/languages are "hub donors" (export gains broadly) and others are "universal recipients" (absorb from many sources), with asymmetry being the norm.
- **Mechanism:** Tasks that train meta-skills (e.g., avoiding imitative falsehoods in TruthfulQA) produce additive updates that benefit other reasoning tasks. Conversely, tasks that require broad knowledge retrieval (Global-MMLU) act as sinks—they absorb structural-reasoning improvements but contribute little in return because their knowledge is distributed and hard to export.
- **Core assumption:** The functional role of a task (training transferable reasoning vs. testing accumulated knowledge) determines its donor/recipient profile.
- **Evidence anchors:**
  - [Section 4.1.3, Table 7] TruthfulQA: donor score 1.17, recipient score -0.10 (strong donor, fragile recipient). Global-MMLU: donor score 0.03, recipient score 2.78 (weak donor, strong recipient).
  - [Section 4.1.4, Figure 3] Task-to-task heatmap shows Global-MMLU receives consistently large positive gains from every other task but provides almost no benefit in return.
  - [Section 4.1.5, Figure 4] Portuguese and Italian act as exceptionally potent donors for Spanish ("Romance amplification" effect), yielding the largest gains in the study.
  - [corpus] Limited direct corpus evidence on task-role asymmetry; related work on cross-lingual knowledge barriers (Chua et al., 2025, cited in paper) discusses asymmetric transfer but focuses on language rather than task axes.
- **Break condition:** This asymmetry pattern is observed on multiple-choice benchmarks (ARC, HellaSwag, TruthfulQA, Global-MMLU). Open-ended generative tasks may exhibit different donor-recipient dynamics—the paper explicitly notes this as a limitation.

## Foundational Learning

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** The entire study uses LoRA with rank r=32 as the fine-tuning mechanism. Understanding that LoRA adds trainable low-rank matrices to frozen weights (rather than updating all parameters) is essential for interpreting why transfer is constrained and structured rather than catastrophic.
  - **Quick check question:** If LoRA rank were increased from 32 to 128, would you expect more or less collateral harm to off-target tasks? (The paper's ablation shows harm rate increases from 31.8% at r=32 to 34.6% at r=128—higher rank broadens update capacity but increases interference risk.)

- **Concept: Transfer Regimes (MT-CL, ML-CT, CT-CL)**
  - **Why needed here:** The paper's core finding is that transfer outcomes differ drastically across these three regimes. MT-CL (same task, different language) is reliable; off-task regimes (ML-CT, CT-CL) have higher variance and harm rates. Practitioners must map their use case to the correct regime to apply the heuristics.
  - **Quick check question:** You fine-tune on French TruthfulQA and want to improve Arabic TruthfulQA performance. Which regime applies, and should you expect reliable gains? (MT-CL regime—same task, different language. Expect positive transfer with ~66% win rate, but monitor Hindi/Arabic as they have elevated harm rates per Table 9.)

- **Concept: Variance Decomposition via Mixed-Effects Models**
  - **Why needed here:** The paper's stability claims rest on REML variance decomposition showing target properties dominate (75%+ variance in MT-CL). Understanding this statistical framing helps practitioners distinguish robust structural findings from model-specific noise.
  - **Quick check question:** If you observe different fine-tuning outcomes across Llama-3 and Gemma-3 for the same source-target pair, does this contradict the paper's stability claims? (Not necessarily—coarse-grained tiers are stable, but fine-grained rankings are model-specific. CI scores are low (τ ≈ 0.03–0.13), so exact recipient ordering is idiosyncratic.)

## Architecture Onboarding

- **Component map:** Base models (Llama 3, Qwen 2.5, Gemma 3 at 0.5B–8B scales) → LoRA adapters (r=32, α=64) applied to attention projections (q/k/v/o_proj) and MLP blocks (gate/up/down_proj) → 4 benchmarks × 11 languages evaluation grid → Transfer ∆ computation → Variance decomposition

- **Critical path:** 1) Establish zero-shot baseline for all 44 task-language cells 2) Fine-tune on single source cell for max 3 epochs with early stopping 3) Re-evaluate on all 43 target cells 4) Partition results into MT-CL, ML-CT, CT-CL regimes 5) Compute donor/recipient scores and variance decomposition

- **Design tradeoffs:**
  - **LoRA rank (r=32):** Lower harm rate (31.8%) vs. r=128 (34.6%), but saturating on-task gains. The paper chooses r=32 as the conservative trade-off point.
  - **Fixed training budget (270 train, 30 val, 400 test):** Ensures cross-cell comparability but limits signal for low-data languages.
  - **Machine-translated vs. curated benchmarks:** Three benchmarks are MT-derived (ARC, HellaSwag, TruthfulQA), one is curated (Global-MMLU). Stratified analysis shows Global-MMLU has higher harm rates—translation artefacts may inflate transfer estimates for MT benchmarks.

- **Failure signatures:**
  - **Elevated harm rate (>10%) in off-task regimes:** Expected for cross-task transfer; if >20%, check for task format conflicts or catastrophic forgetting.
  - **Low recipient score (<0.7) with high donor score (>1.0):** Indicates a "generous but brittle" language (e.g., Hindi: donor 0.98, recipient 0.84). These languages benefit less from incoming transfer—consider multi-source fine-tuning or direct data augmentation instead.
  - **Inconsistent rankings across model families (CI τ < 0.1):** Fine-grained donor selection must be empirical per model; coarse heuristics (Romance > Indic) are portable.

- **First 3 experiments:**
  1. **Replicate MT-CL transfer for your model family:** Fine-tune on English TruthfulQA, evaluate on Spanish/Italian/Portuguese TruthfulQA. Expect +2–3 pp gains with <5% harm rate. If harm rate exceeds 10%, investigate source data quality or rank setting.
  2. **Map donor-recipient profile for your target language:** If targeting Bengali Global-MMLU, consult Table 8 (recipient score 0.60) and Table 13 (English → Bengali = +0.91 pp, Hindi → Bengali = +0.67 pp). Fine-tune on English or Hindi source, not phylogenetically closest but lower-resource languages.
  3. **Stress-test off-task collateral damage:** Fine-tune on HellaSwag (reasoning task) and evaluate on Global-MMLU (knowledge task). Per Table 11, expect +2.55 pp gain. If observing degradation, check for format specialization (HellaSwag trains sentence completion, which may not align with MMLU's multiple-choice structure).

## Open Questions the Paper Calls Out

- **Question:** Does the target-dominance of transfer variance persist in models larger than 8B parameters or across different architectures?
  - **Basis in paper:** [explicit] The Conclusion asks to "extend this framework to larger models... to test whether target-dominated variance persists at larger scales."
  - **Why unresolved:** The study limits coverage to 0.5B–8B scales across three families.
  - **What evidence would resolve it:** Replicating the transfer grid and variance decomposition on 70B+ models or MoE architectures.

- **Question:** Does the observed donor-recipient hierarchy hold for open-ended generative tasks?
  - **Basis in paper:** [explicit] Section 6 proposes broadening the evaluation to "generative and open-ended tasks" to test if the hierarchy carries over.
  - **Why unresolved:** Current findings rely on automated metrics for multiple-choice classification benchmarks.
  - **What evidence would resolve it:** Evaluating the transfer grid on generation tasks using human or preference-based assessment.

- **Question:** Can few-shot or Chain-of-Thought prompting mitigate the collateral harm observed in off-task regimes?
  - **Basis in paper:** [inferred] The Limitations section notes the protocol fixes zero-shot decoding, stating that "few-shot... or alternative decoding strategies could yield different outcomes."
  - **Why unresolved:** It is unknown if inference-time strategies can protect against negative transfer.
  - **What evidence would resolve it:** Re-evaluating the fine-tuned models using CoT prompting to measure if harm rates decrease.

## Limitations

- **Multiple-choice benchmarks only:** Findings may not generalize to open-ended generative tasks, which the authors explicitly acknowledge as a limitation.
- **Language coverage constraints:** The studied language set (11 languages) excludes many low-resource languages where transfer dynamics could differ substantially.
- **Method-specific results:** Asymmetric transfer patterns are specific to LoRA with rank 32; higher ranks and other parameter-efficient methods were not tested.

## Confidence

- **High Confidence:** Matched-task cross-language transfer reliability (MT-CL regime), target-side dominance in variance decomposition, donor-recipient hierarchy stability at coarse levels
- **Medium Confidence:** Off-task regime harm rates and win rates, specific donor rankings within language families, transferability of heuristics to non-studied languages
- **Low Confidence:** Extrapolation to generative tasks, exact rankings for languages outside the studied set, fine-grained donor selection across model families

## Next Checks

1. **Test MT-CL transfer on a generative task:** Fine-tune on English summarization and evaluate cross-lingual transfer to Spanish/Portuguese summaries to verify if matched-task cross-language reliability holds beyond multiple-choice formats.

2. **Validate low-resource language receptivity:** Extend the grid to include a language with substantially less pre-training data than the studied set (e.g., Swahili or Nepali) to test whether the receptivity ceiling hypothesis holds at the extreme low end.

3. **Compare LoRA rank effects systematically:** Run controlled experiments with r=16, 32, 64, 128 on a single source-target pair to map the harm rate/transfer magnitude trade-off curve and identify the optimal rank for minimizing collateral damage.