---
ver: rpa2
title: Improved Object-Centric Diffusion Learning with Registers and Contrastive Alignment
arxiv_id: '2601.01224'
source_url: https://arxiv.org/abs/2601.01224
tags:
- slots
- slot
- image
- coda
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Contrastive Object-centric Diffusion Alignment
  (CODA), a method that improves object-centric learning by combining register slots
  and a contrastive alignment objective. Register slots absorb residual attention
  to reduce slot entanglement, while the contrastive loss explicitly aligns slots
  with image content, acting as a surrogate for maximizing mutual information.
---

# Improved Object-Centric Diffusion Learning with Registers and Contrastive Alignment

## Quick Facts
- arXiv ID: 2601.01224
- Source URL: https://arxiv.org/abs/2601.01224
- Reference count: 40
- Introduces CODA, achieving +6.1% FG-ARI on COCO object discovery

## Executive Summary
This paper addresses object-centric learning (OCL) challenges in complex scenes by introducing Contrastive Object-centric Diffusion Alignment (CODA). The method combines register slots and a contrastive alignment objective to improve object discovery, property prediction, and compositional image generation. Register slots absorb residual attention to reduce slot entanglement, while the contrastive loss explicitly aligns slots with image content, acting as a surrogate for maximizing mutual information. CODA uses a pretrained diffusion model as a decoder with minimal adaptation, achieving significant improvements over strong baselines on synthetic and real-world datasets.

## Method Summary
CODA enhances object-centric learning by integrating register slots into the Slot Attention architecture and adding a contrastive alignment objective. Register slots, initialized from CLIP padding tokens, absorb residual attention during the softmax operation, preventing semantic slots from capturing background or noise. The contrastive loss minimizes reconstruction loss for correct slot-image pairs while maximizing it for mismatched pairs, effectively aligning slots with their corresponding image regions. The method uses a frozen DINOv2 encoder, a Slot Attention module with register augmentation, and a partially finetuned Stable Diffusion v1.5 decoder (only cross-attention projections are trained).

## Key Results
- Achieves state-of-the-art FG-ARI of 71.3 on MOVi-C and 67.5 on MOVi-E
- Improves COCO object discovery by +6.1% FG-ARI over Stable-LSD
- Outperforms baselines in 3D bounding box prediction on CLEVR, GQA, and COCO

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Input-independent register slots reduce slot entanglement by functioning as "attention sinks" for residual activation mass.
- **Mechanism:** Cross-attention softmax normalization forces total attention weights to sum to one. In standard Slot Attention, if no semantic slot strongly matches a query, the model must arbitrarily distribute this "leftover" attention, causing semantic slots to capture noise or background features. Register slots provide a valid, neutral target for this residual attention, preserving the cleanliness of semantic slots.
- **Core assumption:** The paper assumes that the attention distribution in pretrained diffusion decoders contains "residual" mass that does not correspond to primary semantic objects, and that providing a "sink" prevents this mass from polluting object slots.
- **Evidence anchors:**
  - [abstract] "Register slots absorb residual attention to reduce slot entanglement..."
  - [section 4.1] "...softmax operation... causes the attention mass to spread arbitrarily... Register slots serve as placeholders that absorb this residual attention..."
  - [corpus] Evidence for this specific "register slot" mechanism is weak or missing in the provided corpus neighbors, which focus more on general OCL or adapter layers.
- **Break condition:** If the visual encoder (e.g., DINOv2) produces features where every patch strongly matches a semantic concept with near-zero residual, the register slots would provide negligible benefit.

### Mechanism 2
- **Claim:** Contrastive alignment acts as a surrogate for maximizing mutual information (MI) between slots and image content, forcing disentanglement.
- **Mechanism:** Standard reconstruction losses allow slots to be entangled (e.g., one slot holding half of object A and half of object B) while still reconstructing the full image. By minimizing the denoising loss for correct slots and *maximizing* it for "hard negative" slots (mixed slots from different images), the model is penalized if it cannot distinguish valid slot compositions from invalid ones.
- **Core assumption:** This relies on the assumption that the diffusion decoder is sufficiently frozen (or stop-gradiented) such that it cannot "cheat" by learning to decode entangled slots; it must rely on the Slot Attention encoder to produce distinct representations.
- **Evidence anchors:**
  - [abstract] "...contrastive loss... acting as a surrogate for maximizing mutual information..."
  - [section 4.3] "...model should assign high likelihood to the correct slot representations and low likelihood to mismatched (negative) slots."
  - [corpus] Not explicitly supported by the provided corpus summaries.
- **Break condition:** If the negative slot construction strategy (swapping slots from different images) accidentally creates semantically valid compositions, the contrastive signal becomes contradictory and degrades performance.

### Mechanism 3
- **Claim:** Finetuning cross-attention projections (Key, Value, Output) mitigates pretrained text-conditioning bias without losing generative fidelity.
- **Mechanism:** Pretrained Stable Diffusion (SD) models expect text embeddings. Directly feeding slot embeddings creates a domain mismatch. By finetuning only the projection layers of the cross-attention, the model adapts to map slot embeddings into a semantic space the U-Net can process, while keeping the bulk of the U-Net frozen to maintain its generative prior.
- **Core assumption:** The assumption is that the projection layers are sufficient "bottlenecks" to align the slot space with the diffusion latent space without requiring full model finetuning.
- **Evidence anchors:**
  - [section 4.2] "We adopt a lightweight adaptation strategy: finetuning only the key, value, and output projections... mitigating text-conditioning bias..."
  - [corpus] "Slot-Guided Adaptation" (SlotAdapt) in the corpus uses "adapters," whereas CODA proposes direct projection finetuning as a simpler alternative.
- **Break condition:** If the slot representations differ drastically in dimensionality or distribution from the text embeddings the SD model was trained on, simple projection finetuning may fail to align the spaces effectively.

## Foundational Learning

- **Concept: Slot Attention (SA) Competition**
  - **Why needed here:** The core failure mode CODA addresses (entanglement) is a byproduct of the standard SA competition mechanism (softmax over slots). Understanding this iterative competition is required to diagnose why "register slots" are necessary.
  - **Quick check question:** If you remove the iterative refinement in Slot Attention, would you expect more or less entanglement?

- **Concept: Cross-Attention Conditioning**
  - **Why needed here:** CODA modifies how slots condition the diffusion U-Net. You must understand how the U-Net uses cross-attention to inject conditioning (queries from U-Net, keys/values from slots) to debug alignment issues.
  - **Quick check question:** In a diffusion U-Net, does cross-attention typically map image features to conditioning (slots), or conditioning to image features?

- **Concept: Mutual Information (MI) in Representation Learning**
  - **Why needed here:** The paper frames its contrastive loss as a tractable proxy for maximizing MI. Grasping this intuition helps explain why "negative slots" (mismatched pairs) are crucial for the learning objective.
  - **Quick check question:** Does maximizing MI between an image and a slot require the slot to capture *all* information or just the *relevant* information?

## Architecture Onboarding

- **Component map:** Image -> DINOv2 -> Slot Attention -> Cross-Attention Projections -> Stable Diffusion U-Net

- **Critical path:** The path from **Image $\to$ DINOv2 $\to$ Slot Attention $\to$ Cross-Attention Projections** is the critical adaptation path. Errors here (e.g., incorrect slot initialization or frozen projections) will break the alignment.

- **Design tradeoffs:**
  - **Registers vs. Global Token:** SlotAdapt uses a global pooled token; CODA uses input-independent registers. CODA argues that input-independent registers are better for compositional generalization because they don't encode input-specific "global" features that might leak into local slots.
  - **Finetuning vs. Adapters:** CODA finetunes projections directly rather than adding adapter layers. This is simpler and requires fewer parameters but assumes the pre-trained space is close enough to the target space to be bridged by linear maps.

- **Failure signatures:**
  - **Slot Collapse:** Slots all look identical (entropy too low).
  - **Entanglement:** Generating an image from Slot 1 produces a mixture of Object A and Object B (visualized in Fig 1 of the paper).
  - **Background Leakage:** Semantic slots attention maps cover large irrelevant background areas.

- **First 3 experiments:**
  1. **Attention Sink Visualization:** Visualize the attention maps of the register slots vs. semantic slots to confirm registers are absorbing the "background/residual" mass.
  2. **Single-Slot Generation Ablation:** Attempt to generate images using *only one* semantic slot (plus registers). Compare CODA vs. Stable-LSD to qualitatively assess entanglement reduction.
  3. **Projection Layer Sanity Check:** Train with the cross-attention projections *frozen* to quantify the performance drop caused by the text-conditioning bias.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can CODA be extended to dynamically determine the number of object slots rather than requiring a fixed hyperparameter?
- **Basis in paper:** [explicit] The authors state in Section F that "CODA still requires the number of slots to be specified in advance," which restricts flexibility in scenes with a variable or unknown number of objects.
- **Why unresolved:** The method inherits the fixed-slot constraint from the underlying Slot Attention mechanism and the contrastive loss definition assumes a fixed set of semantic slots.
- **What evidence would resolve it:** An extension of CODA that integrates an adaptive slot mechanism (e.g., stopping criteria or slot birth/death) showing improved mBO/mIoU on datasets with high variance in object counts without manual tuning.

### Open Question 2
- **Question:** Does the register-augmented slot architecture maintain its efficiency and alignment properties when scaled to larger diffusion backbones like SDXL or FLUX?
- **Basis in paper:** [explicit] The authors note in Section F that they "do not explore scaling CODA to larger, more recent architectures such as SDXL or FLUX," citing the need for substantial resources and engineering effort.
- **Why unresolved:** The current validation is restricted to SD v1.5; larger architectures introduce different cross-attention mechanisms and text-conditioning pipelines that may behave differently with register slots.
- **What evidence would resolve it:** Benchmark results from a CODA implementation on SDXL or FLUX, reporting both object discovery metrics (FG-ARI) and computational overhead compared to the SD v1.5 baseline.

### Open Question 3
- **Question:** Can the reliance on DINOv2 features be decoupled or improved to capture fine-grained geometric details required for precise 3D localization?
- **Basis in paper:** [inferred] Section 5.2 notes that "lower performance on 3D bounding box prediction is likely due to DINOv2 features, which lack fine-grained geometric details necessary for precise 3D localization."
- **Why unresolved:** The model's object decoder is currently bottlenecked by the semantic richness of the frozen DINOv2 encoder, which prioritizes semantic features over spatial precision.
- **What evidence would resolve it:** Experiments replacing or augmenting DINOv2 with spatially-aware encoders (e.g., DINOv2 with registers or depth-estimation backbones) that demonstrate improved MSE on 3D bounding box prediction tasks.

## Limitations

- **Register Slot Dependency on CLIP:** The method depends on CLIP's padding token embeddings to initialize register slots, creating a hard coupling to CLIP's architecture and training.
- **Contrastive Loss Design:** The paper uses negative slots constructed by swapping slots between different images, but this assumes swapped compositions are always semantically invalid.
- **Synthetic vs. Real-World Generalization:** While CODA shows strong performance on synthetic datasets (MOVi-C/E) and moderate gains on real datasets (VOC, COCO), the performance drop on COCO suggests the method may not scale equally well to real-world visual complexity.

## Confidence

- **High Confidence:** The register slot mechanism reduces slot entanglement. This is supported by ablation studies and qualitative attention visualizations showing register slots absorbing residual attention mass that would otherwise pollute semantic slots.
- **Medium Confidence:** The contrastive alignment objective acts as a surrogate for mutual information maximization. While the paper provides theoretical framing and empirical gains, the connection to MI maximization is indirect.
- **Medium Confidence:** The finetuning strategy (cross-attention projections only) effectively mitigates text-conditioning bias without losing generative fidelity. The claim is supported by performance gains over baselines, but the ablation is limited.

## Next Checks

1. **Register Slot Sensitivity Analysis:** Systematically test register slot initialization using different token sources (e.g., random vectors, CLIP CLS tokens, frozen slots from Slot Attention). Measure the impact on slot entanglement and object discovery performance to isolate the contribution of CLIP padding tokens versus input-independent properties.

2. **Contrastive Loss Ablation with Controlled Negatives:** Replace the random slot-swapping negative construction with negatives designed to be semantically plausible (e.g., swapping slots from images with similar object co-occurrences). Evaluate whether the contrastive signal remains effective and whether it degrades performance on datasets with repeated objects.

3. **Cross-Attention Projection Ablation on Real-World Data:** Conduct a controlled experiment where cross-attention projections are frozen versus finetuned on COCO, measuring both object discovery performance and generation quality. Include an intermediate condition where only key projections are frozen to isolate the contribution of each projection component to bias mitigation.