---
ver: rpa2
title: 'Aria-MIDI: A Dataset of Piano MIDI Files for Symbolic Music Modeling'
arxiv_id: '2504.15071'
source_url: https://arxiv.org/abs/2504.15071
tags:
- piano
- audio
- recordings
- dataset
- files
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce Aria-MIDI, a large-scale dataset of over one million
  piano MIDI files transcribed from audio recordings using a multi-stage pipeline.
  We develop a language model-guided crawler for efficient video discovery and metadata
  extraction, and a source-separation-based audio classifier for precise pruning and
  segmentation of piano content.
---

# Aria-MIDI: A Dataset of Piano MIDI Files for Symbolic Music Modeling

## Quick Facts
- **arXiv ID:** 2504.15071
- **Source URL:** https://arxiv.org/abs/2504.15071
- **Reference count:** 40
- **Primary result:** Over 1 million piano MIDI files transcribed from YouTube audio using source-separation-based classification and LLM-guided crawling

## Executive Summary
Aria-MIDI is a large-scale dataset of over one million piano MIDI files transcribed from YouTube audio recordings. The authors developed a multi-stage pipeline that combines language model-guided crawling for efficient video discovery, a source-separation-based audio classifier for precise piano identification, and a robust transcription model. The dataset contains roughly 100,000 hours of music with metadata for compositional deduplication and MIR tasks. Evaluations demonstrate the classifier achieves over 98% non-piano removal and 95% high-quality piano retention, with metadata extraction accuracy above 90% for major fields.

## Method Summary
The Aria-MIDI pipeline begins with LLM-guided crawling using Llama 3.1 70B to score YouTube metadata and prioritize piano-relevant videos. Audio classification employs a CNN trained on mel-spectrograms using pseudo-labels generated from source separation (MVSep). The model uses sliding-window inference (5-second windows, 1-second stride) to segment recordings into contiguous piano regions based on score thresholds. Valid segments undergo transcription via Aria-AMT (a Whisper-based model), followed by metadata extraction using LLM parsing of video titles and descriptions. The approach balances precision and recall through carefully tuned thresholds for non-piano removal and segment retention.

## Key Results
- Classifier achieves >98% non-piano removal and >95% high-quality piano retention
- Metadata extraction accuracy exceeds 90% for major fields (composer, opus, genre)
- Dataset comprises approximately 100,000 hours of transcribed piano music
- Successfully demonstrates scalable transcription of diverse YouTube piano recordings

## Why This Works (Mechanism)

### Mechanism 1: Source Separation Distillation for Efficient Classification
The paper uses MVSep source separation to generate pseudo-labels by separating piano from "other" audio, computing RMS energy for each component, and labeling clips based on energy thresholds. This process creates training data for a lightweight CNN classifier, enabling scalable piano identification without running source separation at inference time.

### Mechanism 2: LLM-Guided Crawling for Targeted Video Discovery
Llama 3.1 70B scores YouTube video titles and descriptions (1-5 scale) to assess piano likelihood, determining crawl priority. This approach efficiently discovers solo-piano content by leveraging natural language signals in video metadata.

### Mechanism 3: Sliding-Window Segmentation with Score Thresholding
The pipeline uses 5-second classification windows with 1-second stride to score audio segments. Regions are classified as non-piano if all segments in a window score below threshold λ, with contiguous piano segments retained based on minimum length and average score requirements.

## Foundational Learning

- **Knowledge Distillation**
  - Why needed here: Understanding how source separation outputs are used as pseudo-labels to train a faster classifier is central to the paper's efficiency claims
  - Quick check question: Can you explain why pseudo-labeling from source separation is more efficient than running source separation at inference time?

- **Mel-Spectrograms**
  - Why needed here: The classifier uses mel-spectrograms as input; understanding this representation is necessary to interpret the model architecture
  - Quick check question: What is a mel-spectrogram, and why might it be preferred over raw waveforms for audio classification?

- **Sliding-Window Inference**
  - Why needed here: Segmentation uses a sliding-window approach with stride; understanding this pattern is critical for implementing the inference algorithm
  - Quick check question: How does sliding-window inference with overlap differ from processing fixed-length chunks independently?

## Architecture Onboarding

- **Component map**: Crawling Module (LLM → priority queue) → Audio Classification Module (CNN → piano/non-piano scores) → Segmentation Module (sliding-window → piano segments) → Transcription Module (Aria-AMT → MIDI) → Metadata Extraction Module (LLM → structured metadata)

- **Critical path**: Crawling → Audio Classification → Segmentation → Transcription → Metadata Extraction. The classifier and segmentation logic are the core filtering stages that determine data quality.

- **Design tradeoffs**:
  - Pseudo-label thresholds vs. label noise: Sensitive energy thresholds improve pseudo-label accuracy for quiet non-piano content but risk mislabeling noisy piano recordings
  - Segmentation sensitivity vs. continuity: Lower λ increases non-piano removal but may fragment piano segments
  - Classifier score threshold vs. recall: Higher thresholds remove low-quality recordings but reduce dataset size

- **Failure signatures**:
  - Systematic misclassification: If the classifier was trained on unrepresentative pseudo-labels, expect high false positive or negative rates on specific recording types
  - Over-segmentation: If piano segments are frequently interrupted, check if λ is too high or if the classifier is unstable on silent/low-energy passages
  - Metadata hallucination: LLM metadata extraction may produce incorrect labels; the paper reports ~90%+ accuracy but notes missed labels

- **First 3 experiments**:
  1. Classifier Ablation: Train without pseudo-labeled data and compare segmentation overlap and non-piano removal rates
  2. Segmentation Threshold Sweep: Vary λ and minimum segment length, measuring piano overlap vs. non-piano removal tradeoff
  3. Metadata Validation: Manually verify a random sample of extracted metadata against ground truth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed source-separation-based pipeline be effectively adapted for multi-instrument transcription?
- Basis in paper: The conclusion states, "extending our approach to other instruments such as guitar, as well as the multi-instrument case, could be approachable via variations of the source-separation-based approaches"
- Why unresolved: The current pipeline relies on a specific piano source separation model and a solo-piano classifier, which cannot inherently handle polyphonic multi-instrument audio
- What evidence: Successful application of the distilled classifier and segmentation method to guitar or multi-track datasets

### Open Question 2
- Question: How can language models be optimized to reduce errors in compositional entity recognition within music metadata?
- Basis in paper: The authors identify the need for "further study into metadata attribution using language models, especially targeting improvements in compositional entity recognition"
- Why unresolved: The current method relies on Llama 3.1 to parse YouTube titles, which is prone to missing labels or hallucinating information when the source text is ambiguous
- What evidence: A modified prompting strategy or fine-tuning procedure that yields higher accuracy than the reported baseline

### Open Question 3
- Question: What is the efficacy of Aria-MIDI for pretraining foundational music models compared to smaller, curated datasets?
- Basis in paper: The introduction suggests the dataset "may still be useful for research into pretrained music models" despite being smaller than text/vision datasets
- Why unresolved: The paper focuses on data curation and transcription methodology rather than evaluating downstream performance on generative tasks
- What evidence: A benchmark comparison of generative models pretrained on Aria-MIDI versus those trained on MAESTRO or Lakh

## Limitations

- Pseudo-label quality before classifier training is not evaluated, creating uncertainty about how noise propagates through the pipeline
- LLM metadata extraction accuracy is reported but lacks validation on diverse naming conventions and ground truth comparison
- No assessment of how classifier errors compound across pipeline stages or how segmentation logic handles musical context like rests

## Confidence

- **Classifier performance claims**: Medium - Strong internal consistency but limited external validation on held-out YouTube data
- **Distillation mechanism**: Low - Absent ablation studies and no replication on alternative domains
- **Crawling strategy**: Low - Evaluated indirectly via dataset composition but lacks A/B testing against alternatives
- **Overall pipeline**: Medium - Demonstrates systematic methodology but relies on several uncertain mechanisms

## Next Checks

1. **Pseudo-Label Quality Analysis**: Measure source separation accuracy on a subset of YouTube piano recordings and quantify how pseudo-label errors affect classifier performance

2. **Pipeline Error Propagation**: Instrument the crawling → classification → segmentation → transcription pipeline to measure error rates at each stage on a held-out validation set

3. **Metadata Extraction Validation**: Manually verify a stratified sample of extracted metadata against ground truth, measuring per-field accuracy and false positive rates