---
ver: rpa2
title: 'Coevolutionary Continuous Discrete Diffusion: Make Your Diffusion Language
  Model a Latent Reasoner'
arxiv_id: '2510.03206'
source_url: https://arxiv.org/abs/2510.03206
tags:
- diffusion
- continuous
- discrete
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the expressivity-trainability gap in diffusion
  language models, where continuous diffusion models theoretically offer stronger
  expressivity than discrete ones but underperform in practice due to decoding difficulties.
  The authors propose Coevolutionary Continuous Discrete Diffusion (CCDD), a joint
  diffusion process on both continuous representation space and discrete token space,
  leveraging a single model to simultaneously denoise in both modalities.
---

# Coevolutionary Continuous Discrete Diffusion: Make Your Diffusion Language Model a Latent Reasoner

## Quick Facts
- arXiv ID: 2510.03206
- Source URL: https://arxiv.org/abs/2510.03206
- Authors: Cai Zhou; Chenxiao Yang; Yi Hu; Chenyu Wang; Chubin Zhang; Muhan Zhang; Lester Mackey; Tommi Jaakkola; Stephen Bates; Dinghuai Zhang
- Reference count: 40
- Primary result: CCDD reduces validation perplexity by over 25% compared to discrete-only diffusion models of the same scale on LM1B and OpenWebText datasets.

## Executive Summary
This paper addresses the expressivity-trainability gap in diffusion language models, where continuous diffusion models theoretically offer stronger expressivity than discrete ones but underperform in practice due to decoding difficulties. The authors propose Coevolutionary Continuous Discrete Diffusion (CCDD), a joint diffusion process on both continuous representation space and discrete token space, leveraging a single model to simultaneously denoise in both modalities. CCDD combines the strong expressivity of continuous diffusion with the good trainability of discrete diffusion through cross-modal conditioning. Experiments on LM1B and OpenWebText datasets show that CCDD reduces validation perplexity by over 25% compared to discrete-only diffusion models of the same scale, validating the effectiveness of combining continuous representations with discrete tokens.

## Method Summary
CCDD defines a multimodal diffusion process on continuous embedding space (via SDE) and discrete token space (via CTMC/masking), using a single model to denoise both modalities simultaneously. The forward process combines VP schedule for continuous (zt = αt·z0 + σt·ε) and masked diffusion for discrete (xt ~ Cat(ηt·x0 + (1-ηt)·πt)). A single time-conditioned DiT-based model with two heads (ε-prediction for continuous, logits for discrete) is trained jointly with loss L = γ_cont·L_cont + γ_disc·L_disc. Architectures include MDiT, MMDiT, and MoEDiT. Continuous space uses pretrained embeddings (Qwen3-Embedding-0.6B or RoBERTa-base). Training uses AdamW optimizer with 1M steps on LM1B (batch 512, 8×H100/A100).

## Key Results
- CCDD achieves ≥25% perplexity reduction vs. MDLM baselines with same parameters on LM1B and OpenWebText datasets.
- Validation ELBO is strictly lower than discrete-only diffusion, confirming better expressivity.
- The method scales to different model sizes and sequence lengths while maintaining perplexity improvements.
- Asynchronous schedules (continuous ahead of discrete) show better generation quality than synchronous schedules.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Continuous diffusion provides strictly higher expressivity than discrete diffusion by maintaining full uncertainty information across denoising steps.
- **Mechanism:** Discrete diffusion quantizes logits to single tokens at each step, losing geometric information (Lemma 6: I(ℓ_θ; x_{t-}) ≤ log V < H(ℓ_θ)). Continuous diffusion propagates full continuous states without compulsory quantization, preserving trajectory-level information. The Fokker-Planck equation yields absolutely continuous marginals, while discrete diffusion produces finite-supported distributions (Theorem 1).
- **Core assumption:** The denoising network has sufficient capacity to leverage continuous representations.
- **Evidence anchors:**
  - [abstract] "We prove that continuous diffusion models have stronger expressivity than discrete diffusions"
  - [section 3.1] "The discrete scheme imposes a hard finite-support bottleneck with information loss at every step"
  - [corpus] Related work (Reasoning with Latent Thoughts) supports looped architectures improving reasoning, but does not directly validate this expressivity claim
- **Break condition:** If token vocabulary V is sufficiently large or continuous-to-discrete decoding bottleneck dominates, expressivity advantage may not translate to empirical gains.

### Mechanism 2
- **Claim:** Joint continuous-discrete modeling improves trainability by providing discrete supervision to anchor continuous representations to decodable tokens.
- **Mechanism:** Continuous diffusion alone suffers from decoding ambiguity—contextualized embeddings are smooth generation targets but lack explicit token-level grounding. The discrete component provides: (1) explicit token supervision via cross-entropy on masked tokens, (2) reduces input space uncertainty for the continuous head, (3) enables classifier-free guidance by treating continuous representations as implicit conditions.
- **Core assumption:** The pretrained embedding space (e.g., Qwen3-Embedding) provides semantically meaningful representations that the diffusion model can learn to reconstruct.
- **Evidence anchors:**
  - [abstract] "CCDD is expressive with rich semantics in the latent space, as well as good trainability and sample quality with the help of explicit discrete tokens"
  - [section 3.2] "We attribute the insufficient trainability of continuous DLMs to the large decision space, the deficient and low-quality embeddings, and combinatorial complexity in decoding"
  - [corpus] Weak direct validation; related work on latent reasoning exists but doesn't specifically test this co-training hypothesis
- **Break condition:** If continuous and discrete noise schedules are poorly matched, gradient scales may diverge; if pretrained embeddings lack quality, the knowledge distillation signal degrades.

### Mechanism 3
- **Claim:** Asynchronous noise schedules with continuous guidance ahead of discrete decoding improve generation quality by establishing high-level semantic planning before token commitment.
- **Mechanism:** Setting continuous information decay slower than discrete (α_t = √(1-t) vs. η_t = 1-t) causes the reverse process to first generate meaningful latent representations, which then guide token unmasking. This implements implicit "planning" in latent space before "execution" in token space.
- **Core assumption:** The continuous denoiser can capture sequence-level semantics earlier in the reverse trajectory than discrete unmasking.
- **Evidence anchors:**
  - [section 4.2] "We set the information decay rate in continuous space slower than the discrete modality, so that in the reverse process the model would generate the latent representations faster"
  - [section 4.2] "The continuous representations as the guidance for the discrete part, the continuous schedule then should be ahead of the discrete schedule"
  - [corpus] No direct external validation of asynchronous scheduling benefits
- **Break condition:** If schedules are too asynchronous, the discrete head may receive uninformative continuous guidance; if too synchronized, the implicit planning benefit is lost.

## Foundational Learning

- **Concept: Variance Preserving (VP) Diffusion SDE**
  - Why needed here: CCDD uses VP formulation for the continuous component; understanding α_t, σ_t and the forward/reverse relationship is essential.
  - Quick check question: Given z_t = α_t z_0 + σ_t ε, what does the reverse SDE recover?

- **Concept: Continuous-Time Markov Chains (CTMC) for Discrete Diffusion**
  - Why needed here: The discrete component uses CTMC with generator G_t; masked diffusion is a specific case.
  - Quick check question: How does the absorbing state [MASK] change the transition rates versus uniform noise?

- **Concept: Factored Multimodal Reverse Kernels**
  - Why needed here: CCDD factorizes p_θ(x_s, z_s | x_t, z_t) = p^disc_θ(x_s | x_t, z_t) · p^cont_θ(z_s | x_t, z_t) but allows cross-modal conditioning.
  - Quick check question: Does factorization limit expressivity compared to joint kernels? (See Theorem 12)

## Architecture Onboarding

- **Component map:** Input layer (token embeddings + continuous embeddings) -> DiT backbone with adaptive layer norm -> Dual heads (ε-prediction for continuous, logits for discrete) -> Joint loss

- **Critical path:**
  1. Freeze pretrained embedding model; extract contextualized embeddings z_0 = E(x_0)
  2. Initialize DiT backbone with timestep embeddings
  3. Apply forward corruption independently: (x_t, z_t) ~ q_t(x_t|x_0) · q_t(z_t|z_0)
  4. Train both heads simultaneously with loss weighting (default γ_cont = γ_disc = 1)
  5. For inference: initialize x_T ~ [MASK], z_T ~ N(0, I); alternate DDPM/DDIM for z and Bayes posterior for x

- **Design tradeoffs:**
  - **MDiT vs MMDiT vs MoEDiT:** MDiT shares attention over L tokens (O(L²), same params as DiT); MMDiT processes 2L tokens (2× params, O(4L²)); MoEDiT uses shared attention + modality-specialized MLP experts (1× params, O(4L²))
  - **Embedding source:** RoBERTa-base (768-dim) vs Qwen3-Embedding (32-dim configurable)—lower dim easier to generate but may lose semantics
  - **Schedule synchronization:** Synchronized SNRs vs continuous-ahead (default: VP for continuous, linear for discrete)

- **Failure signatures:**
  - **Mode collapse in discrete head:** Continuous loss dominates, discrete predictions become uniform—check gradient magnitudes
  - **Representation leakage during validation:** Continuous embeddings directly encode masked tokens—must apply masking to z_0 during eval (see pr=1 protocol)
  - **OOD loops:** If trained with synchronous schedules but tested with too few steps, continuous guidance may be under-developed

- **First 3 experiments:**
  1. **Sanity check:** Train CCDD-MDiT on LM1B with 32-dim Qwen3-Embeddings; verify ELBO < discrete-only MDLM baseline (target: >25% perplexity reduction)
  2. **Ablation on embedding depth:** Compare 0-th vs 28-th layer embeddings (Figure 2)—expect 28-th to balance MSE and CE losses better
  3. **Schedule validation:** Test synchronous vs asynchronous (continuous-ahead); measure validation perplexity and generative NLL with CFG scale sweep (w ∈ {0.0, 1.0, 1.5})

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does CCDD exhibit stronger reasoning capabilities (e.g., on benchmarks requiring planning, backtracking, or multi-step inference) compared to discrete-only diffusion models, as suggested by the theoretical expressivity arguments?
- **Basis in paper:** [explicit] The paper claims continuous diffusion enables "implicit reasoning" and that CCDD "retains full semantics in previous denoising steps" (Section 4.1, Appendix E.1), but experiments only evaluate perplexity on LM1B and OpenWebText, not reasoning tasks.
- **Why unresolved:** No empirical validation of reasoning advantages despite theoretical motivation about latent reasoning benefits.
- **What evidence would resolve it:** Evaluation on reasoning benchmarks (e.g., Sudoku, coding tasks, mathematical reasoning) comparing CCDD to discrete baselines.

### Open Question 2
- **Question:** What is the optimal coupling strategy and relative schedule between continuous and discrete modalities?
- **Basis in paper:** [explicit] Proposition 15 and Corollary 16 provide a heuristic for "entropy/MI matching" but state it is "(Informal)" and acknowledge this is a "matching guideline" rather than a proven optimum (Appendix B.2.2).
- **Why unresolved:** The paper provides intuition but no theoretical guarantee or empirical optimization over schedule design.
- **What evidence would resolve it:** Systematic ablation over schedule pairings with principled optimization, or theoretical proof of optimal coupling.

### Open Question 3
- **Question:** How does CCDD scale to larger model sizes, datasets, and longer sequence lengths compared to discrete diffusion baselines?
- **Basis in paper:** [inferred] Experiments use relatively small models (≤216M parameters), standard datasets (LM1B, OWT), and sequence lengths 128-512. The architectural designs (MMDiT, MoEDiT) double attention complexity without clear scaling analysis.
- **Why unresolved:** No scaling curves or larger-scale validation provided.
- **What evidence would resolve it:** Training curves across model scales (e.g., 100M to 1B+ parameters) and comparison of compute-quality tradeoffs.

### Open Question 4
- **Question:** What properties make a continuous embedding space "optimal" for CCDD—theoretical sufficiency for token prediction, training dynamics, or semantic richness?
- **Basis in paper:** [explicit] The paper compares layers of Qwen3-Embedding (Figure 2) and notes tradeoffs between representation MSE and decoding cross-entropy, but does not establish criteria for optimal space selection (Section 4.2, Appendix E).
- **Why unresolved:** Layer selection is empirical; the sufficiency condition in Remark 8 is not tested.
- **What evidence would resolve it:** Controlled study varying embedding quality/dimension while measuring both generation quality and decoding accuracy.

## Limitations

- The expressivity advantage of continuous diffusion over discrete diffusion relies on the denoising network's ability to leverage continuous representations effectively, but this is not directly validated.
- The coevolutionary training mechanism assumes that discrete supervision anchors continuous representations to decodable tokens, but empirical evidence for this knowledge distillation effect is weak.
- The asynchronous scheduling claim (continuous-ahead) implements implicit planning, but the evidence is indirect with no ablation showing synchronous schedules failing.
- The method depends heavily on the quality of pretrained continuous embeddings, yet this sensitivity is not characterized.

## Confidence

- **High confidence:** Experimental results showing 25%+ perplexity reduction on LM1B and OpenWebText datasets.
- **Medium confidence:** Theoretical expressivity claims (continuous > discrete) and the information-theoretic bounds.
- **Low confidence:** The coevolutionary training benefits and asynchronous scheduling advantages.

## Next Checks

1. **Ablation on embedding source and quality:** Compare CCDD with continuous components using (a) frozen Qwen3-Embedding, (b) frozen RoBERTa embeddings, (c) randomly initialized embeddings, and (d) no continuous component.

2. **Schedule synchronization analysis:** Train CCDD with synchronous schedules (same SNR decay rates for continuous and discrete) and compare against the asynchronous (continuous-ahead) setting.

3. **Information flow quantification:** During validation, measure mutual information between continuous representations and masked tokens at different reverse process timesteps. Compare this to discrete-only diffusion to quantify how much continuous guidance contributes to reducing uncertainty before token unmasking occurs.