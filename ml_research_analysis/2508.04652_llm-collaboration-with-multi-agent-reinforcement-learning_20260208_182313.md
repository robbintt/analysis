---
ver: rpa2
title: LLM Collaboration With Multi-Agent Reinforcement Learning
arxiv_id: '2508.04652'
source_url: https://arxiv.org/abs/2508.04652
tags:
- agent
- agents
- function
- prime
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of coordinating multiple large
  language models (LLMs) in collaborative tasks by modeling it as a cooperative Multi-Agent
  Reinforcement Learning (MARL) problem. The authors propose MAGRPO, a multi-turn
  algorithm that leverages group-relative advantages for joint optimization while
  preserving decentralized execution for efficiency.
---

# LLM Collaboration With Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2508.04652
- Source URL: https://arxiv.org/abs/2508.04652
- Authors: Shuo Liu; Tianle Chen; Zeyu Liang; Xueguang Lyu; Christopher Amato
- Reference count: 40
- Primary result: MAGRPO achieves higher efficiency and quality than baselines in LLM writing and coding collaboration tasks.

## Executive Summary
This paper addresses the challenge of coordinating multiple large language models (LLMs) in collaborative tasks by modeling it as a cooperative Multi-Agent Reinforcement Learning (MARL) problem. The authors propose MAGRPO, a multi-turn algorithm that leverages group-relative advantages for joint optimization while preserving decentralized execution for efficiency. Experiments on LLM writing and coding collaboration show that MAGRPO significantly improves both response efficiency and quality compared to baselines, including single models, naive concatenation, sequential pipelines, and one-round discussions. On coding tasks, MAGRPO achieves higher pass@k and accuracy@k metrics, while on writing tasks, it generates more coherent and consistent outputs faster than alternatives. The method enables agents to discover diverse cooperation schemes and opens avenues for applying MARL techniques to LLM collaboration at scale.

## Method Summary
MAGRPO models LLM collaboration as a cooperative Multi-Agent Reinforcement Learning problem using a Dec-POMDP framework. The algorithm generates G responses per agent per step, computes centralized advantages using group-relative returns (R - mean(R) for all groups), and updates policies via gradient ascent. It uses centralized training with decentralized execution, where agents train with access to all group returns but execute independently. The method is applied to writing tasks (summarization, expansion) and coding tasks (function generation) using homogeneous agents, with rewards evaluating structural integrity, syntax correctness, test pass rates, and cooperation quality.

## Key Results
- MAGRPO outperforms single models, naive concatenation, sequential pipelines, and one-round discussions in both writing and coding tasks.
- On coding tasks, MAGRPO achieves higher pass@k and accuracy@k metrics compared to baselines.
- On writing tasks, MAGRPO generates more coherent and consistent outputs faster than alternatives.
- The method enables agents to discover diverse cooperation schemes including fallback, decorator, coordinator, and strategy-filter patterns.

## Why This Works (Mechanism)

### Mechanism 1: Group-Relative Advantage for Multi-Agent Coordination
Using group-relative advantages enables joint optimization without requiring individual agent rewards or complex reward engineering. Instead of learning individual value functions for each agent, MAGRPO samples G responses from each agent, computes returns for each group sample, and calculates advantages relative to the group mean. This creates a centralized training signal without needing a large centralized value model. The core assumption is that the group mean provides a reasonable baseline for advantage estimation and the joint return captures cooperative quality without requiring per-agent credit assignment.

### Mechanism 2: Dec-POMDP Formalization Avoids Non-Optimal Nash Equilibria
Modeling LLM collaboration as a Dec-POMDP (cooperative) rather than POSG (potentially competitive) avoids converging to suboptimal Nash equilibria. In POSG formulations with individual rewards, agents optimize their own returns and may converge to Nash equilibria that are not jointly optimal. Dec-POMDP uses a joint reward, ensuring the optimization target is joint return maximization, eliminating the need for reward engineering to align individual incentives. The core assumption is that all agents are cooperative/non-adversarial and there is a meaningful joint objective that can be measured.

### Mechanism 3: Emergent Cooperation Schemes Through Minimal Constraints
Under minimal prompt constraints with a joint reward, diverse cooperation schemes (fallback, decorator, coordinator, strategy-filter) emerge naturally during training. By providing only problem descriptions and role assignments (no explicit coordination instructions), agents must learn coordination strategies that maximize joint returns. The reward signal reinforces whatever coordination patterns improve outcomes. The core assumption is that the reward model adequately captures task quality and the action space permits multiple valid coordination strategies.

## Foundational Learning

- **Concept: Decentralized Partially Observable Markov Decision Process (Dec-POMDP)**
  - Why needed here: This is the mathematical formalization underlying MAGRPO. Each agent has only partial observations (local prompts), cannot observe the full state, and maintains observation-action histories to make decisions.
  - Quick check question: Can you explain why a Dec-POMDP differs from a standard MDP in terms of what information each agent has access to during execution?

- **Concept: Centralized Training with Decentralized Execution (CTDE)**
  - Why needed here: MAGRPO uses centralized information (group returns from all agents) during training to compute advantages, but each agent executes independently using only its local history. This is essential for scalable deployment.
  - Quick check question: What information is available during training that is NOT available during execution in a CTDE paradigm?

- **Concept: Policy Gradient with Group-Relative Advantages**
  - Why needed here: MAGRPO adapts GRPO to multi-agent settings. Understanding how advantages are computed relative to a group baseline (not a learned value function) is essential for debugging training dynamics.
  - Quick check question: In Equation 1, why is the advantage calculated as R^(g)_t minus the mean of all G group returns, rather than using a learned value function?

## Architecture Onboarding

- **Component map:**
  - Dataset D -> Task sampling -> Initial prompts (o_1,0, ..., o_n,0)
  - Agents -> n LLMs with policies π_θ1, ..., π_θn, each generating G responses per turn
  - Environment -> State transition T, external tools (AST, sandbox), user/system state updates
  - Reward Model R -> Joint reward function R(s^acc_t, a_t) evaluating collective output
  - MAGRPO Trainer -> Computes group-relative advantages (Eq 1), updates policies via gradient (Eq 2)
  - External Feedback (multi-turn) -> Claude-Sonnet-4 or static analyzers providing edit suggestions

- **Critical path:**
  1. Data preparation: Construct tasks with clear joint objectives; coding needs test cases, writing needs structure/consistency metrics.
  2. Reward model design: Define joint reward evaluating collective output (not per-agent).
  3. Group sampling: Each agent generates G responses per turn (G typically 4-8 for training).
  4. Advantage computation: Core innovation—use group mean as baseline (Eq 1).
  5. Gradient update: Each agent's policy updated to increase probability of actions with positive group-relative advantages (Eq 2).

- **Design tradeoffs:**
  - Group size G: Larger G -> lower variance but higher compute. Paper uses G=15 for evaluation.
  - KL coefficient (set to 0): No KL penalty allows greater exploration but risks catastrophic forgetting.
  - Single-turn vs. multi-turn: Multi-turn with external feedback improves quality (Figures 3c-3d) but doubles+ inference cost.
  - Dec-POMDP vs. POSG: Dec-POMDP targets joint optimality but assumes cooperation; POSG handles mixed motives but may converge to suboptimal equilibria.

- **Failure signatures:**
  1. Trivial cooperation: One agent dominates, others output empty/useless responses. Check if reward requires meaningful contribution from all agents.
  2. Training instability with multi-turn: Initial reward drops as agents are "overwhelmed by external feedback" (Figure 5b). Start single-turn, add multi-turn gradually.
  3. Reward hacking: Agents optimize simple metrics (Jaccard similarity, transition words) without improving actual quality.
  4. Dataset lacks cooperative structure: If tasks like `strlen` are atomic, training cannot discover coordination (see CHE vs HE difference in Table 2).
  5. Cascading syntax errors: Auxiliary function errors break main function if fallback logic is absent.

- **First 3 experiments:**
  1. Single-agent GRPO baseline: Fine-tune one LLM with standard GRPO to verify reward model produces sensible gradients.
  2. Two-agent naive concatenation: Run agents with no communication, concatenate outputs. Worse than single-agent confirms coordination is needed.
  3. MAGRPO with G=4, single-turn: Minimal viable training. Monitor: (a) both agents generating non-trivial responses? (b) total return increases over 500+ steps? (c) cooperation metrics improve? If yes, scale to multi-turn and larger G.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MAGRPO perform when applied to heterogeneous agents with diverse capabilities and architectures?
- Basis in paper: [explicit] The authors note they focused on homogeneous agents for simplicity and list heterogeneous agent collaboration as a key direction for future research.
- Why unresolved: All experiments utilized pairs of identical models (e.g., Qwen3-1.7B); the impact of asymmetry in agent knowledge or capacity remains untested.
- What evidence would resolve it: Experiments training agents of different sizes or specializations (e.g., a planning agent vs. a coding agent) and measuring convergence and role specialization.

### Open Question 2
- Question: Can the learned cooperation schemes scale effectively to large-scale software development involving multiple files and modules?
- Basis in paper: [explicit] The authors acknowledge that computational constraints limited experiments to small-scale models and datasets, leaving larger-scale project applicability unresolved.
- Why unresolved: Current results are limited to single-function generation; the interaction dynamics for complex, multi-module codebases are unknown.
- What evidence would resolve it: Training runs on repositories with complex dependencies, tracking the maintenance of structural integrity and test pass rates as complexity increases.

### Open Question 3
- Question: Does the use of fine-grained, process-supervised reward models significantly reduce reward hacking compared to the simple metric-based rewards used in this study?
- Basis in paper: [explicit] The authors admit their simple reward models inevitably lead to narrow signals and potential reward hacking, suggesting multi-aspect rewards as a necessary improvement.
- Why unresolved: The paper relies on heuristic metrics (e.g., Jaccard similarity, transition words) which may be gamed without improving actual content quality.
- What evidence would resolve it: Ablation studies comparing the current reward structure against process-supervised models, analyzing failure cases for superficial compliance versus genuine semantic improvement.

## Limitations

- The method's dependence on carefully engineered reward models raises questions about scalability to more complex domains.
- The comparison focuses on relatively simple coordination tasks (summarization, function generation) without addressing real-world multi-turn conversations or longer-term dependencies.
- Claims about "diverse cooperation schemes" emerge from limited examples without systematic analysis of how often each scheme appears or under what conditions they develop.

## Confidence

- **High Confidence:** The core MARL framework (Dec-POMDP + CTDE + group-relative advantages) is technically sound and well-established in reinforcement learning literature.
- **Medium Confidence:** Empirical results showing MAGRPO outperforming baselines are convincing for the specific tasks tested, though the evaluation scope is limited.
- **Low Confidence:** Claims about emergent cooperation schemes being diverse and task-general lack systematic validation across broader problem domains.

## Next Checks

1. **Reward Model Robustness:** Test whether agents exploit reward functions through degenerate strategies (e.g., maximizing transition word counts without improving coherence in writing tasks).
2. **Cross-Domain Generalization:** Apply MAGRPO to a more complex collaborative task beyond summarization/function generation (e.g., multi-turn dialogue planning or multi-document question answering).
3. **Cooperation Scheme Diversity Analysis:** Quantify the distribution of emergent cooperation schemes across different task types and measure whether certain schemes consistently outperform others.