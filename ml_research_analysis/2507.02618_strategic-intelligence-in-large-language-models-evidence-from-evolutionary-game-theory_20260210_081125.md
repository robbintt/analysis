---
ver: rpa2
title: 'Strategic Intelligence in Large Language Models: Evidence from evolutionary
  Game Theory'
arxiv_id: '2507.02618'
source_url: https://arxiv.org/abs/2507.02618
tags:
- gemini
- openai
- opponent
- cooperation
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tested whether Large Language Models (LLMs) can engage
  in strategic decision-making by pitting them against each other and classic game
  theory strategies in Iterated Prisoner's Dilemma (IPD) tournaments. Seven evolutionary
  tournaments with nearly 32,000 moves revealed that LLMs from OpenAI, Google, and
  Anthropic not only survive but sometimes thrive in these competitive environments.
---

# Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory

## Quick Facts
- **arXiv ID:** 2507.02618
- **Source URL:** https://arxiv.org/abs/2507.02618
- **Authors:** Kenneth Payne; Baptiste Alloui-Cros
- **Reference count:** 5
- **Key outcome:** Large Language Models can engage in strategic decision-making in Iterated Prisoner's Dilemma tournaments, with distinct vendor-specific strategic styles emerging

## Executive Summary
This study tests whether Large Language Models (LLMs) can engage in strategic decision-making by pitting them against each other and classic game theory strategies in Iterated Prisoner's Dilemma (IPD) tournaments. Seven evolutionary tournaments with nearly 32,000 moves revealed that LLMs from OpenAI, Google, and Anthropic not only survive but sometimes thrive in these competitive environments. OpenAI's models were highly cooperative but vulnerable in hostile conditions, while Google's Gemini adapted strategically, defecting ruthlessly when advantageous. Anthropic's Claude emerged as the most forgiving reciprocator. Analysis of nearly 32,000 rationales showed that LLMs actively reason about time horizons and opponent strategies, with this reasoning directly influencing their decisions.

## Method Summary
The study employed evolutionary Iterated Prisoner's Dilemma (IPD) tournaments with 24 agents (10 canonical strategies plus LLM variants from OpenAI, Google, and Anthropic). Agents played round-robin matches with probabilistic termination (p ∈ {0.10, 0.25, 0.75}) and a 30-round hard cap. Population dynamics followed evolutionary game theory with fitness-based reproduction (F = Avg Score, N' = N × (F / F̄)²). LLM agents received standardized prompts including game rules, termination probability, and move history (max 20 turns), generating rationales and binary moves (C/D) via API calls.

## Key Results
- LLMs from OpenAI, Google, and Anthropic survived and sometimes thrived in evolutionary IPD tournaments with nearly 32,000 total moves
- Distinct "strategic fingerprints" emerged: OpenAI models were highly cooperative, Gemini adapted ruthlessly, and Claude was most forgiving
- Varying termination probability (shadow of the future) significantly impacted LLM strategies, with Gemini adapting its cooperation rate while OpenAI remained consistently cooperative
- Analysis of nearly 32,000 rationales demonstrated that reasoning about time horizons and opponent strategies was instrumental to LLM decision-making

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Strategic adaptation is driven by sensitivity to the "shadow of the future" (termination probability), though this sensitivity varies significantly by model vendor.
- **Mechanism:** Models process the explicit termination probability provided in the prompt. Those with higher strategic flexibility (e.g., Gemini) reduce cooperation rates as termination probability increases (approaching one-shot rationality), whereas rigidly aligned models (e.g., OpenAI) may maintain high cooperation regardless of reduced future interactions.
- **Core assumption:** LLMs treat the termination probability as a relevant reasoning variable rather than noise, and their "personality" priors can override game-theoretic optimization.
- **Evidence anchors:**
  - [abstract] Varying the termination probability introduces complexity; Gemini adapted while OpenAI remained highly cooperative.
  - [Section 4.2] Gemini's cooperation rate collapsed to 2.2% in the 75% termination run, while OpenAI's skyrocketed to 95.7%.
  - [corpus] "AI Testing Should Account for Sophisticated Strategic Behaviour" supports the need to evaluate reasoning under varied constraints.
- **Break condition:** If models ignore the termination parameter or if all vendors converge on identical behavior patterns regardless of the probability p.

### Mechanism 2
- **Claim:** Prose rationales are instrumental to decision-making (Chain-of-Thought), functioning as causal computation steps rather than post-hoc justification.
- **Mechanism:** The model generates a rationale (reasoning about horizon/opponent) which constrains the probability distribution of the final move token. Removing or altering the requirement for rationale changes the move distribution.
- **Core assumption:** "Thinking is writing" for LLMs; the autoregressive generation of the rationale text effectively conditions the subsequent move.
- **Evidence anchors:**
  - [abstract] "Analysis of nearly 32,000 prose rationales... demonstrates that this reasoning is instrumental to their decisions."
  - [Section 4.5.4] "The rationale constrains the final output; it is the pathway, not the echo."
  - [corpus] "LLMsPark" (arXiv:2509.16610) suggests evaluating dynamics, but direct evidence for the *instrumentality* of rationale text is primarily drawn from this paper's analysis.
- **Break condition:** If models produce high-quality rationales that statistically contradict their actual moves (e.g., justifying defection but cooperating), or if zero-shot prompting without rationale yields identical fitness scores.

### Mechanism 3
- **Claim:** Evolutionary survival depends on "strategic fingerprints" (distinct vendor-specific priors) matching the environmental conditions.
- **Mechanism:** Models possess persistent behavioral priors (e.g., OpenAI's "niceness," Gemini's "Machiavellianism"). Evolutionary tournaments act as fitness landscapes that select for the fingerprint best suited to the specific noise and horizon parameters of that run.
- **Core assumption:** Training data and RLHF create stable, distinct game-theoretic personalities that do not vanish during the match.
- **Evidence anchors:**
  - [abstract] LLMs exhibit distinctive and persistent "strategic fingerprints" (e.g., Google ruthless, Anthropic forgiving).
  - [Section 4.3] "The models are genuinely different in their approach to the game."
  - [corpus] "Do LLMs trust AI regulation?" (arXiv:2504.08640) corroborates that LLM agents exhibit emergent strategic behaviors in game-theoretic frameworks.
- **Break condition:** If models rapidly converge to a single optimal strategy (e.g., pure Tit-for-Tat) regardless of their vendor origin within the first few rounds.

## Foundational Learning

- **Concept: Iterated Prisoner's Dilemma (IPD) & Shadow of the Future**
  - **Why needed here:** The core experiment relies on the theoretical difference between single-shot games (defect is rational) and iterated games (cooperation can emerge). Without understanding how the termination probability ($p$) alters the expected value of future cooperation, the model behaviors appear random.
  - **Quick check question:** If $p = 0.75$ (75% chance the game ends now), should a rational agent prioritize immediate payoff or long-term reputation?

- **Concept: Evolutionary Game Theory (EGT)**
  - **Why needed here:** The paper uses a specific reproduction rule (Equation 3) where population share is based on relative fitness. Understanding EGT is required to interpret the "Results" tables not just as scores, but as survival/extinction events.
  - **Quick check question:** In this system, does a strategy need to maximize its *absolute* score or its score *relative to the population mean* to survive?

- **Concept: Theory of Mind (ToM) in Agents**
  - **Why needed here:** The analysis of rationales (Section 4.5) distinguishes between simple reaction ("they defected, I defect") and modeling ("they seem to be playing Tit-for-Tat"). This distinction is central to the claim that LLMs possess "Strategic Intelligence."
  - **Quick check question:** Does the agent react to the *last move* (reaction) or conjecture about the *algorithm* driving the opponent (modeling)?

## Architecture Onboarding

- **Component map:** Evolutionary Engine -> IPD Arena -> LLM Agent Interface -> Evolutionary Engine
- **Critical path:** The prompt construction is the bottleneck. If the match history (up to 20 moves) is formatted incorrectly or the termination probability $p$ is omitted, the "strategic reasoning" mechanism breaks. The system relies on the LLM correctly parsing its own history to simulate memory.
- **Design tradeoffs:**
  - **Memory:** The system uses *transient match history* (last 20 moves) rather than a persistent external vector database. This limits long-term strategy but isolates reasoning capability.
  - **Sampling:** OpenAI/Claude used temp=0.7, Gemini used API default. This introduces variance in "deterministic" strategies, potentially muddying the comparison of strict strategic logic vs. noise.
- **Failure signatures:**
  - **The "Sucker" Collapse:** An agent that cooperates >90% in a high-termination ($p=0.75$) or hostile environment (Section 4.2, OpenAI 75% run) will see its population drop to 0 immediately.
  - **Hallucination:** Rationales misstating the move history (Section 5, "The Facts" vs Rationale 1568). This confirms the model is reasoning over its *perception* of history, not the ground truth.
- **First 3 experiments:**
  1. **Replicate the 25% Termination Run:** Verify system stability. The paper identifies this as an "Equilibrium Sweet Spot" (Table 25) where populations should remain stable.
  2. **Stress Test with $p=0.75$:** Confirm the "ruthless" adaptation. Run Gemini vs. OpenAI in a high-termination environment to reproduce the population crash of the cooperative agent.
  3. **Silent Mode (No Rationale):** Modify the prompt to force a move *without* requesting a rationale. Compare scores against the baseline to test the "Mechanism 2" claim that rationales are instrumental to performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the evolutionary success of LLMs scale effectively to frontier models (e.g., GPT-4, Claude 3.5 Sonnet) or plateau?
- **Basis in paper:** [explicit] The authors note they used "mini" and "flash" versions, stating: "As scaling continues, we anticipate better performance in IPD, perhaps approaching the sort of outcomes delivered by the classic Bayesian model."
- **Why unresolved:** The study tested efficient, smaller models; it remains unverified if the strategic sophistication of "advanced" models over "basic" ones continues linearly or if larger models develop new, unforeseen strategic pathologies.
- **What evidence would resolve it:** Replicating the evolutionary tournament using the largest available frontier models and comparing their "strategic fingerprints" and evolutionary fitness scores against the smaller variants.

### Open Question 2
- **Question:** How do spatial assortment and continuous strategy spaces alter LLM evolutionary dynamics?
- **Basis in paper:** [explicit] The authors explicitly list methodological limitations: "Our abstraction departs from natural ecosystems in several ways: no spatial assortment, a fixed menu of discrete strategies, and (in the core runs) no mutation..."
- **Why unresolved:** The current study used a round-robin format (well-mixed population). Standard evolutionary game theory predicts different equilibrium outcomes when agents interact only with neighbors (spatial) or can evolve continuous parameters.
- **What evidence would resolve it:** Running simulations on grid-based or small-world networks where agents interact only with local neighbors, and allowing for mutation of strategy parameters rather than discrete replacement.

### Open Question 3
- **Question:** Can LLMs exhibit the same "strategic fingerprints" in games completely absent from their training data?
- **Basis in paper:** [inferred] The paper argues against the "stochastic parrot" hypothesis by introducing noise and varying time horizons to confound memorization, but the Iterated Prisoner's Dilemma is a canonical game likely present in training corpora.
- **Why unresolved:** While the specific moves were novel, the *concept* of IPD was likely known. It is unclear if the models' reasoning is robust enough to handle entirely novel payoff matrices or game rules where memorization is impossible.
- **What evidence would resolve it:** Testing the agents on novel, non-canonical games (e.g., "Traveler's Dilemma" or custom asymmetric payoffs) to see if the distinct strategic styles (e.g., Gemini's ruthlessness vs. Anthropic's forgiveness) persist.

## Limitations

- The exact text of the standardized prompt for LLM agents is not fully specified, only the components (rules, p, history)
- Limited statistical analysis of LLM behavior variability across runs makes it difficult to assess robustness of observed strategic differences
- No ablation study presented on the impact of rationale generation to directly test whether reasoning is truly instrumental to decision-making

## Confidence

- **High Confidence:** The core finding that LLMs can survive in evolutionary IPD tournaments (basic strategic competence) is well-supported by the population dynamics data
- **Medium Confidence:** Claims about vendor-specific "strategic fingerprints" are supported by the data but could be influenced by uncontrolled prompt variations or API differences
- **Low Confidence:** The claim that rationales are *instrumental* to decision-making (Mechanism 2) lacks direct experimental validation through controlled ablation

## Next Checks

1. Conduct an ablation study comparing LLM performance with and without rationale generation requirements to test the instrumental reasoning hypothesis
2. Run multiple independent trials of the same tournament conditions to establish statistical significance of observed strategic differences
3. Test whether model behavior changes when the termination probability is mentioned in the rules but not explicitly referenced in the prompt to isolate true strategic reasoning from pattern matching