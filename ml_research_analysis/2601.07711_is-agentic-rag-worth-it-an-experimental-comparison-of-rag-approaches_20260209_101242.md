---
ver: rpa2
title: Is Agentic RAG worth it? An experimental comparison of RAG approaches
arxiv_id: '2601.07711'
source_url: https://arxiv.org/abs/2601.07711
tags:
- query
- agentic
- enhanced
- retrieval
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compares two paradigms for Retrieval-Augmented Generation
  (RAG) systems: Enhanced RAG, which uses fixed modules like query rewriting and re-ranking,
  and Agentic RAG, where an LLM dynamically orchestrates the retrieval and generation
  process. Through extensive experiments across four datasets (FiQA, NQ, FEVER, CQADupStack),
  the authors evaluate four dimensions: user intent handling, query-document alignment,
  retrieved document adjustment, and impact of underlying LLM quality.'
---

# Is Agentic RAG worth it? An experimental comparison of RAG approaches

## Quick Facts
- arXiv ID: 2601.07711
- Source URL: https://arxiv.org/abs/2601.07711
- Reference count: 13
- Enhanced RAG performs better at routing user intent and document re-ranking, while Agentic RAG excels at query rewriting and adaptive intent detection

## Executive Summary
This study compares two paradigms for Retrieval-Augmented Generation systems: Enhanced RAG with fixed modules and Agentic RAG with dynamic LLM orchestration. Through extensive experiments across four datasets, the authors evaluate performance across four dimensions: user intent handling, query-document alignment, retrieved document adjustment, and LLM quality impact. Results show Enhanced RAG excels at routing and re-ranking while Agentic RAG outperforms in query rewriting and adaptive intent detection. However, Agentic RAG incurs 1.5-3.6× higher computational costs. The performance gap narrows with better underlying LLMs, suggesting the choice depends on specific use case requirements balancing performance needs against computational costs.

## Method Summary
The study compares Enhanced RAG (using fixed modules: semantic-router, Hyde query rewriting, ELECTRA reranker) against Agentic RAG (using PocketFlow framework with GPT-4o orchestrator) across four datasets (FiQA, NQ, FEVER, CQADupStack). Four evaluation dimensions are measured: user intent handling (F1, recall), query rewriting (NDCG@10), document refinement (NDCG@10), and answer quality (LLM-as-a-Judge via Selene-70B). The Enhanced approach uses text-embedding-3-small for retrieval while Agentic uses GPT-4o for orchestration. Both paradigms are tested across different LLM sizes to assess sensitivity to model quality.

## Key Results
- Enhanced RAG shows superior performance in routing user intent (semantic routing) and document re-ranking (ELECTRA cross-encoder)
- Agentic RAG excels at query rewriting (2.8 NDCG@10 points higher on average) and adaptive intent detection
- Agentic RAG is 1.5-3.6× more expensive due to additional reasoning steps and tool calls
- Performance gap narrows as underlying LLM quality increases, with both showing similar sensitivity to model scale

## Why This Works (Mechanism)

### Mechanism 1: Query Rewriting via Adaptive Agent Decision
The agent assesses whether the original query aligns with knowledge base document format and rewrites when misaligned. This dynamic approach outperforms fixed rewriting rules by selecting appropriate transformations for each domain.

### Mechanism 2: Fixed-Module Re-ranking vs Iterative Retrieval
Enhanced RAG applies an ELECTRA-based cross-encoder reranker to top-20 documents, while Agentic RAG attempts improvement through re-issuing queries. The dedicated reranker consistently outperforms iterative retrieval attempts.

### Mechanism 3: Intent Detection via Semantic Routing vs Agent Reasoning
Performance depends on domain specificity - semantic routing excels in broad domains while agent reasoning works better in narrow domains with clear boundaries.

## Foundational Learning

- **Concept:** Retrieval-Augmented Generation (RAG) architecture
  - **Why needed here:** The paper assumes familiarity with the basic retriever-generator pipeline that both paradigms extend
  - **Quick check question:** Can you explain why naive RAG retrieval can return irrelevant documents even when the query seems clear?

- **Concept:** Query-document semantic gap
  - **Why needed here:** Understanding that short queries and long documents differ in structure and vocabulary motivates query rewriting techniques like Hyde
  - **Quick check question:** Why might embedding a 10-word question and a 500-word document yield poor similarity scores even when the document answers the question?

- **Concept:** Cross-encoder reranking
  - **Why needed here:** The paper's Enhanced RAG uses an ELECTRA-based reranker; understanding joint query-document encoding is essential for interpreting results
  - **Quick check question:** How does a cross-encoder's computational cost scale with the number of candidate documents compared to bi-encoder retrieval?

## Architecture Onboarding

- **Component map:**
  - Enhanced RAG: Router (semantic-router) → Query Rewriter (Hyde via LLM) → Retriever (vector search) → Reranker (ELECTRA cross-encoder) → Generator (LLM)
  - Agentic RAG: Orchestrator Agent (LLM with tool access) → RAG Tool (retriever) → Answer Node (LLM), with iteration capability at agent discretion

- **Critical path:**
  1. Define domain boundaries and create valid/invalid query examples for routing
  2. Configure retriever (embedding model, vector store, top-k)
  3. For Enhanced: train/select reranker; for Agentic: design agent prompt with domain context
  4. Establish evaluation dataset with ground-truth query-document pairs

- **Design tradeoffs:**
  - **Latency vs Quality:** Agentic requires 1.5× longer end-to-end time on average
  - **Cost vs Flexibility:** Agentic consumes 2.7-3.9× more input tokens, 1.7-2.0× more output tokens
  - **Determinism vs Adaptation:** Enhanced offers predictable pipeline behavior; Agentic can adapt but may behave inconsistently
  - **Domain Scope:** Agentic excels in narrow domains (finance, grammar); Enhanced better for broad domains (factual verification)

- **Failure signatures:**
  - Low recall on out-of-scope query detection → Agent over-retrieves (FEVER: 49.3% recall)
  - Iterative retrieval doesn't improve results → Agent cannot identify better documents than initial retrieval
  - Performance plateaus despite larger LLM → Underlying architecture, not model size, is the bottleneck

- **First 3 experiments:**
  1. **Baseline comparison:** Run both paradigms on a held-out validation split with identical retriever settings; measure NDCG@10 and F1 for intent handling
  2. **Cost-performance frontier:** Vary the underlying LLM (e.g., Qwen3-0.6B through 32B) and plot token cost vs. answer quality (using LLM-as-a-Judge) for both architectures
  3. **Hybrid exploration:** Add an explicit re-ranking step to the Agentic pipeline to test the paper's suggestion that "integrating an explicit re-ranking step into Agentic pipelines could provide substantial gains"

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would integrating an explicit re-ranking module into Agentic RAG pipelines close the document list refinement performance gap with Enhanced RAG?
- Basis in paper: "Our results suggest that integrating an explicit re-ranking step into Agentic pipelines could provide substantial gains."
- Why unresolved: The study tested Agentic RAG's iterative retrieval as the sole refinement mechanism, finding it ineffective compared to Enhanced RAG's dedicated reranker. No hybrid approach was evaluated.
- What evidence would resolve it: An experiment comparing vanilla Agentic RAG against Agentic RAG augmented with a cross-encoder reranker on NDCG@10 across the same datasets.

### Open Question 2
- Question: How does equipping Agentic RAG with multiple tools (e.g., web search, summarization) affect the cost-performance trade-off compared to single-tool setups?
- Basis in paper: "our agent is equipped with only a single tool, whereas richer agentic setups might exhibit different behaviors."
- Why unresolved: The experimental design deliberately constrained the agent to one RAG tool to ensure comparability; multi-tool orchestration costs and benefits remain unquantified.
- What evidence would resolve it: A controlled study varying tool count (1, 3, 5 tools) while measuring both task performance and token/latency overhead.

### Open Question 3
- Question: Which specific query rewriting strategies (HyDE, query expansion, multi-query) perform best within Agentic vs. Enhanced RAG architectures?
- Basis in paper: "each block of Enhanced RAG is implemented following previous work but lacks comparison of different approaches, which might perform better than the ones we selected."
- Why unresolved: Only HyDE was tested for Enhanced RAG; the agent's freeform rewriting was not systematically compared against alternative rewriting methods.
- What evidence would resolve it: An ablation study testing 3-4 rewriting techniques across both paradigms using the same NDCG@10 evaluation protocol.

### Open Question 4
- Question: Does the Enhanced RAG advantage in document re-ranking persist when using LLM-based rerankers instead of encoder-based models?
- Basis in paper: The Enhanced RAG used an ELECTRA-based cross-encoder (300M parameters), but recent work shows LLMs can serve as effective rerankers; the paradigm comparison may depend on reranker choice.
- Why unresolved: Only one reranker implementation was tested; the relative advantage could shrink if both paradigms used comparable LLM-based components.
- What evidence would resolve it: A comparison using identical LLM-based reranking (e.g., GPT-4 or Qwen) in both Enhanced and Agentic settings.

## Limitations

- Focus on single orchestrator model (GPT-4o) and reranker architecture (ELECTRA) limits generalizability to other model families
- Relatively small datasets may not capture real-world complexity and variability
- Significant performance variability across domains suggests high sensitivity to prompt engineering and domain boundaries

## Confidence

- **High:** Cost-performance trade-off findings (1.5-3.6× cost increase for Agentic RAG)
- **Medium:** Domain-specific performance patterns (narrow domains favor Agentic, broad domains favor Enhanced)
- **Medium:** Query rewriting superiority (Agentic's adaptive approach outperforms fixed rewriting)
- **Low:** Intent detection mechanisms (results show significant variability across datasets)

## Next Checks

1. Test alternative orchestrator models (e.g., Claude-3, Gemini) to assess model family sensitivity in Agentic RAG
2. Implement the suggested hybrid approach (adding explicit reranking to Agentic) to verify the paper's hypothesis about performance gains
3. Scale experiments to larger, more heterogeneous datasets to evaluate real-world generalization