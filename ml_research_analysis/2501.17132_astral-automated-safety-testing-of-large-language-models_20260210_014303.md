---
ver: rpa2
title: 'ASTRAL: Automated Safety Testing of Large Language Models'
arxiv_id: '2501.17132'
source_url: https://arxiv.org/abs/2501.17132
tags:
- test
- safety
- llms
- unsafe
- astral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ASTRAL, a framework for automated safety
  testing of large language models (LLMs) that addresses the limitations of existing
  static and imbalanced safety testing datasets. The core method employs a novel black-box
  coverage criterion that ensures balanced generation of unsafe test inputs across
  diverse safety categories, writing styles, and persuasion techniques, leveraging
  Retrieval Augmented Generation (RAG), few-shot prompting, and web browsing to create
  up-to-date test cases.
---

# ASTRAL: Automated Safety Testing of Large Language Models

## Quick Facts
- arXiv ID: 2501.17132
- Source URL: https://arxiv.org/abs/2501.17132
- Authors: Miriam Ugarte; Pablo Valle; José Antonio Parejo; Sergio Segura; Aitor Arrieta
- Reference count: 38
- Primary result: ASTRAL framework detects nearly twice as many unsafe LLM behaviors compared to static baselines

## Executive Summary
This paper introduces ASTRAL, a framework for automated safety testing of large language models that addresses the limitations of existing static and imbalanced safety testing datasets. The core method employs a novel black-box coverage criterion that ensures balanced generation of unsafe test inputs across diverse safety categories, writing styles, and persuasion techniques, leveraging Retrieval Augmented Generation (RAG), few-shot prompting, and web browsing to create up-to-date test cases. The framework uses LLMs as automated oracles to classify responses as safe or unsafe. Evaluation across 9 well-known LLMs demonstrates that GPT-3.5 outperforms other models as a test oracle, and ASTRAL uncovers nearly twice as many unsafe behaviors compared to static baselines, with the web browsing feature significantly increasing detection of unsafe responses in production scenarios.

## Method Summary
ASTRAL implements a three-phase pipeline for automated safety testing: generation, execution, and evaluation. The test generator uses GPT-3.5 via OpenAI Assistant APIs with RAG enabled on BeaverTails dataset, few-shot prompting, and Tavily web browsing integration. A black-box coverage criterion ensures balanced combinations across 14 safety categories, 6 writing styles, and 5 persuasion techniques, generating 1260 test inputs (14×6×5×3). The oracle uses GPT-3.5 (identified as best performer in RQ0) to classify outputs as safe or unsafe, with 20 independent classifications per output to handle stochasticity. The framework compares against BeaverTails baseline using 90 random samples per category.

## Key Results
- ASTRAL detects nearly twice as many unsafe behaviors compared to static BeaverTails baseline
- GPT-3.5 outperforms other models (GPT-4, Claude-3.5-Sonnet, Llama-3.1-8B) as test oracle
- Web browsing feature significantly increases detection of unsafe responses in production scenarios
- The framework ensures balanced test input generation across all combinations of safety categories, writing styles, and persuasion techniques

## Why This Works (Mechanism)
ASTRAL works by systematically generating diverse, realistic unsafe prompts that cover all combinations of safety categories, writing styles, and persuasion techniques. The black-box coverage criterion ensures no blind spots in testing, while RAG provides up-to-date context from safety datasets. Web browsing enables testing against current events and specific scenarios that static datasets cannot capture. Using LLMs as oracles allows scalable, automated evaluation of model safety responses.

## Foundational Learning

**Black-box coverage criterion** - Why needed: Ensures systematic exploration of all unsafe prompt variations without requiring white-box access to target models. Quick check: Verify coverage matrix completeness before generation.

**Retrieval Augmented Generation (RAG)** - Why needed: Provides relevant context and examples from safety datasets to guide prompt generation. Quick check: Test RAG retrieval accuracy on sample queries.

**Few-shot prompting for writing styles** - Why needed: Teaches the generator to produce prompts in diverse formats and tones that users might employ. Quick check: Validate generated prompts match target writing styles.

**Web browsing integration** - Why needed: Enables creation of prompts based on current events and specific real-world scenarios. Quick check: Confirm Tavily API returns relevant, recent information.

**LLM-based oracle classification** - Why needed: Provides automated, scalable evaluation of safety responses without manual labeling. Quick check: Run 20 classifications per output and measure consistency.

## Architecture Onboarding

**Component map:** BeaverTails dataset -> RAG vector store -> Test Generator (GPT-3.5 + Tavily) -> Coverage Matrix -> Test Execution -> GPT-3.5 Oracle -> Results Analysis

**Critical path:** Test generation (RAG + few-shot + web browsing) -> Coverage matrix validation -> Test execution against target LLMs -> Oracle classification (20 runs per output) -> Results aggregation

**Design tradeoffs:** The framework trades computational cost (RAG + web browsing for 1260+ prompts) for comprehensive safety coverage. Using GPT-3.5 as oracle prioritizes accuracy over cost, accepting higher API usage.

**Failure signatures:** Oracle stochasticity (same output classified differently), unbalanced coverage (missing combinations), API rate limits/costs, incomplete few-shot examples leading to poor writing style generation.

**First experiments:**
1. Test RAG retrieval on sample safety queries to verify BeaverTails integration
2. Validate coverage matrix generation ensures all (category, style, persuasion) combinations are covered
3. Run oracle on 10 sample outputs with 20 classifications each to measure consistency and identify potential bias

## Open Questions the Paper Calls Out

None

## Limitations

- Proprietary nature of safety datasets prevents independent verification of baseline comparisons
- Methodological uncertainties include exact few-shot examples, specific RAG prompts, and API parameter settings
- Oracle evaluation approach may suffer from LLM classification limitations including bias and context dependency
- Focus on English-language prompts and US context reduces generalizability to other languages and cultures

## Confidence

**High confidence:** Overall methodology for automated test generation and core claim of detecting more unsafe behaviors than static baselines

**Medium confidence:** Comparative performance of different LLM oracles, effectiveness of web browsing feature due to limited ablation study details

## Next Checks

1. Replicate oracle evaluation (RQ0) by testing GPT-3.5, GPT-4, Claude-3.5-Sonnet, and Llama-3.1-8B on subset of 50 outputs with 20 independent classifications each to verify consistency and compare accuracy metrics

2. Implement minimal test generator pipeline using only 2 safety categories and verify coverage criterion ensures balanced generation across all combinations

3. Conduct ablation study comparing ASTRAL's full pipeline against versions without web browsing and without RAG to quantify contribution of each component to unsafe behavior detection