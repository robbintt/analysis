---
ver: rpa2
title: 'Survey Response Generation: Generating Closed-Ended Survey Responses In-Silico
  with Large Language Models'
arxiv_id: '2510.11586'
source_url: https://arxiv.org/abs/2510.11586
tags:
- response
- survey
- restricted
- open-ended
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper systematically investigates the impact of various Survey
  Response Generation Methods on closed-ended survey responses produced by large language
  models. Using 8 methods, 4 political attitude datasets, and 10 open-weight language
  models, it evaluates 32 million simulated responses.
---

# Survey Response Generation: Generating Closed-Ended Survey Responses In-Silico with Large Language Models

## Quick Facts
- arXiv ID: 2510.11586
- Source URL: https://arxiv.org/abs/2510.11586
- Reference count: 40
- Primary result: Restricted Generation Methods (especially Restricted Choice) yield the best closed-ended survey response alignment, while reasoning output does not consistently improve performance.

## Executive Summary
This paper systematically evaluates eight Survey Response Generation Methods for producing closed-ended survey responses using large language models. The study tests methods across four political attitude datasets and ten open-weight language models, generating 32 million simulated responses. The key finding is that Restricted Generation Methods, particularly Restricted Choice, consistently outperform token probability-based methods in both individual-level and subpopulation-level alignment with human survey data. Token Probability-Based Methods show poor robustness and calibration, especially for smaller models, while Verbalized Distribution provides the best subpopulation-level distributional alignment.

## Method Summary
The study compares eight Survey Response Generation Methods: First-Token Probabilities (extract probability on first output token), First-Token Restricted (restrict vocabulary to response options), Answer Prefix (extract probabilities after fixed prefix), Restricted Choice (JSON schema enforcing single valid response), Restricted Reasoning (JSON with reasoning then answer), Verbalized Distribution (JSON probability distribution over all options), Open-Ended Classification (open-ended generation then classify), and Open-Ended Distribution (open-ended generation then verbalized distribution). These methods were tested across four political attitude surveys (ANES 2016, GLES 2017, GLES 2025, ATP 2021) using ten open-weight LLMs including Llama 3.2-3B/3.1-8B/3.3-70B, OLMo-2-1B/7B/32B, and Qwen3-8B/32B with/without reasoning capabilities.

## Key Results
- Restricted Generation Methods (especially Restricted Choice) consistently outperformed other methods, achieving significantly better individual-level alignment and subpopulation-level alignment
- Token Probability-Based Methods often produce invalid or poorly calibrated responses, especially for smaller models
- Verbalized Distribution yields the best subpopulation-level distributional alignment by forcing models to explicitly allocate probability mass across all options
- Reasoning output does not consistently improve alignment and increases computational cost by orders of magnitude

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Restricted Generation Methods (especially Restricted Choice) yield better individual-level and subpopulation-level alignment than token probability-based methods.
- **Mechanism:** Constraining the LLM's vocabulary to valid response options via structured output (JSON schema) reduces invalid outputs and forces the model to commit to a survey-compatible format, which aligns with its instruction-following training.
- **Core assumption:** Instruction-tuned LLMs are better at following formatting constraints than at producing well-calibrated token probabilities for arbitrary tokens.
- **Evidence anchors:**
  - [abstract] "Restricted Generation Methods (especially Restricted Choice) consistently outperformed others, achieving significantly better individual-level alignment and subpopulation-level alignment."
  - [Table 3] Restricted Choice shows significant positive regression coefficients for individual-level F1 across all datasets.
  - [Table 6] Verbalized Distribution significantly improves subpopulation-level alignment.
  - [corpus] Related work (Wang et al., 2024) finds first-token probabilities often mismatch open-ended responses.
- **Break condition:** If inference infrastructure cannot enforce vocabulary restriction (e.g., some APIs), the method degrades to less reliable instruction-following only.

### Mechanism 2
- **Claim:** Token Probability-Based Methods show poor robustness and calibration because first-token distributions are sensitive to option-order biases and response prefixes.
- **Mechanism:** Many instruction-tuned models emit prefixes (e.g., "My answer is") before the actual option token. First-token probability extraction misses this, and even prefix-adjusted methods remain vulnerable to positional biases (e.g., A-bias).
- **Core assumption:** The first token's probability or the probability after a fixed prefix reflects the model's true answer preference.
- **Evidence anchors:**
  - [Section 2.1] "Wang et al. (2024) showed that the first-token probability of an LLM might not always align with its open-ended response."
  - [Table 4] Token Probability-Based Methods show low Fleiss's κ (0.23–0.34) across response option scales, indicating poor robustness.
  - [Figure 10] Token Probability-Based Methods have higher Brier scores (worse calibration) than Verbalized Distribution.
  - [corpus] Direct corpus evidence limited; related work (Tjuatja et al., 2024; McIlroy-Young et al., 2024) documents option-order effects in LLM survey responses.
- **Break condition:** If response option tokens are not uniquely identifiable (e.g., multi-token options without clear first-token mapping), the method fails.

### Mechanism 3
- **Claim:** Verbalized Distribution yields the best subpopulation-level distributional alignment because it forces the model to explicitly allocate probability mass across all options.
- **Mechanism:** Instead of sampling a single choice, the model outputs a JSON object with probabilities for each response option, capturing within-individual uncertainty. Aggregating these distributions improves subpopulation-level estimates.
- **Core assumption:** Models can produce meaningfully calibrated verbalized probability distributions when prompted.
- **Evidence anchors:**
  - [Table 6] Verbalized Distribution shows significant negative coefficients for total variation distance (improvement) in ANES 2016 and GLES 2017.
  - [Table 7] Verbalized Distribution has positive coefficients for global alignment (distance correlation).
  - [corpus] Meister et al. (2025) (cited in paper) found Verbalized Distribution works best for response distribution generation.
- **Break condition:** If the model generates malformed JSON or probabilities that do not sum to 1, post-processing is required; invalid response rates may increase for smaller models.

## Foundational Learning

- **Concept:** Closed-ended vs. open-ended survey responses
  - **Why needed here:** The paper addresses methods to force LLMs—designed for open-ended text—to produce categorical/ordinal survey responses (e.g., vote choice, Likert scales).
  - **Quick check question:** Can you explain why extracting a closed-ended response from an open-ended LLM output requires either token probability extraction or structured output constraints?

- **Concept:** Token probability extraction from LLMs
  - **Why needed here:** Token Probability-Based Methods rely on reading the probability distribution over the vocabulary at specific positions. Understanding this is critical for diagnosing why these methods fail (prefix issues, top-k API limits).
  - **Quick check question:** If an API only returns top-k token probabilities (k=20), what happens when none of the response option tokens are in the top 20?

- **Concept:** Structured output / constrained decoding
  - **Why needed here:** Restricted Generation Methods use JSON schema constraints and vocabulary restriction to force valid outputs. Implementation requires libraries like `xgrammar` or `outlines`.
  - **Quick check question:** How does vocabulary restriction differ from post-hoc parsing of unconstrained output in terms of guarantees?

## Architecture Onboarding

- **Component map:** Persona & Question Prompt -> Survey Response Generation Method -> Structured Output Engine -> Evaluation Layer
- **Critical path:**
  1. Define survey question and response options (Full Text or Indexed).
  2. Construct persona prompt (demographics + question).
  3. Select Survey Response Generation Method: **default to Restricted Choice** for individual-level prediction, **Verbalized Distribution** for subpopulation distributional alignment.
  4. Enable vocabulary restriction via structured output engine.
  5. Aggregate responses and compute alignment metrics.

- **Design tradeoffs:**
  - **Restricted Choice vs. Verbalized Distribution:** Restricted Choice is faster and better for individual-level accuracy; Verbalized Distribution is better for subpopulation-level distributional alignment but ~3–5× slower (see Figure 3).
  - **Token Probability-Based vs. Restricted:** Token methods require access to logits (not all APIs support) and are poorly calibrated; Restricted methods are more robust but require inference-time constraints.
  - **Reasoning models:** Do not consistently improve alignment; increase GPU time by orders of magnitude (Figure 3). Use only if task specifically benefits from chain-of-thought (not shown for survey simulation).

- **Failure signatures:**
  - High invalid response rate (>10%): especially for Token Probability-Based Methods with smaller models or reasoning models (Figure 11).
  - Low agreement across response option scales (Fleiss's κ < 0.4): indicates option-order bias, common in Token Probability-Based Methods.
  - High Brier score (>1.0): poor calibration, common in First-Token Probabilities (Figure 10).

- **First 3 experiments:**
  1. **Baseline comparison:** Run Restricted Choice vs. First-Token Probabilities on a single dataset (e.g., GLES 2017) with identical prompts; compare macro avg. F1 and invalid response rate.
  2. **Subpopulation alignment test:** Run Verbalized Distribution vs. Restricted Choice; compare Total Variation Distance across demographic subgroups.
  3. **Reasoning ablation:** For a reasoning-capable model (e.g., Qwen 3 8B), run with and without reasoning enabled using Restricted Reasoning; check if alignment improves or degrades (expect no consistent improvement per Section 5.1).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do Survey Response Generation Methods perform on surveys about attitudes, opinions, and values beyond political attitudes, and in non-Western contexts?
- **Basis in paper:** [explicit] The authors state in the Limitations section: "Still, further evaluation should be performed in non-Western contexts and on surveys of attitudes, opinions, and values that go beyond the topics that we have studied."
- **Why unresolved:** The study only evaluated political attitude surveys (vote choice, social/cultural change) in the U.S. and Germany; generalizability to other domains (health, consumer preferences, social attitudes) and non-Western populations remains untested.
- **What evidence would resolve it:** Systematic evaluation of all 8 Survey Response Generation Methods on diverse survey topics (e.g., health behaviors, consumer preferences) across non-Western countries using comparable methodology.

### Open Question 2
- **Question:** How do alternative persona prompt formats (e.g., interview-style) interact with different Survey Response Generation Methods to affect alignment quality?
- **Basis in paper:** [explicit] The Limitations section notes: "Alternative persona prompts (e.g., interview-style) have been shown to positively influence the LLM response alignment (Lutz et al., 2025) and might interact differently with different Survey Response Generation Methods."
- **Why unresolved:** This study adopted fixed persona prompt templates from prior work and did not vary the prompt format; potential interactions between prompt style and response generation method remain unknown.
- **What evidence would resolve it:** Factorial experiments crossing multiple persona prompt formats with all 8 Survey Response Generation Methods, measuring both individual-level and subpopulation-level alignment.

### Open Question 3
- **Question:** How do response option scale perturbations such as missing midpoint options affect the alignment and robustness of different Survey Response Generation Methods?
- **Basis in paper:** [explicit] The Limitations section states: "Future research should also investigate further perturbations to the response options scales such as a missing midpoint option, which have been found to impact human and LLM survey responses."
- **Why unresolved:** This study only tested Full Text vs. Indexed and original vs. reversed order variants; other ecologically valid perturbations remain unexplored.
- **What evidence would resolve it:** Experiments systematically manipulating midpoint presence, number of options, and option labeling across methods, measuring alignment degradation and response distribution shifts.

## Limitations
- The study only evaluates political attitude surveys in the U.S. and Germany, limiting generalizability to other domains and non-Western contexts
- Analysis of reasoning models is inconclusive, with no clear benefit demonstrated despite significant computational overhead
- The persona construction process and its sufficiency for capturing human survey response complexity is not validated

## Confidence
- **High confidence:** Superiority of Restricted Generation Methods (especially Restricted Choice) for individual-level alignment is well-supported by consistent regression coefficients across all four datasets (Table 3) and robust Fleiss's κ scores (Table 4)
- **Medium confidence:** Advantage of Verbalized Distribution for subpopulation-level alignment is supported by significant improvements in total variation distance and distance correlation (Tables 6-7), but effect sizes are smaller and more variable
- **Low confidence:** Claim that reasoning models do not consistently improve alignment is based on limited comparisons (Section 5.1) and does not explore task-specific prompting or alternative reasoning strategies

## Next Checks
1. **Cross-domain validation:** Test Restricted Choice and Verbalized Distribution on non-political survey datasets (e.g., health attitudes, consumer preferences) to assess generalizability beyond political attitude questions
2. **Proprietary model comparison:** Run the same methods on GPT-4 or Claude to determine if the superiority of Restricted Generation Methods holds for closed-source models with different training objectives and token vocabularies
3. **Reasoning task-specificity test:** For a domain where chain-of-thought is known to help (e.g., economic forecasting), compare reasoning-enabled vs. disabled Restricted Reasoning models to determine if the lack of improvement is task-general or method-specific