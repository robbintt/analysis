---
ver: rpa2
title: Privacy-Aware Continual Self-Supervised Learning on Multi-Window Chest Computed
  Tomography for Domain-Shift Robustness
arxiv_id: '2510.27213'
source_url: https://arxiv.org/abs/2510.27213
tags:
- learning
- data
- pretraining
- dataset
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a privacy-preserving continual self-supervised
  learning (CSSL) framework for chest CT images across multiple domains (window settings).
  The method addresses catastrophic forgetting in domain-shift environments by storing
  intermediate feature representations (rather than raw images) in a memory buffer,
  ensuring privacy compliance.
---

# Privacy-Aware Continual Self-Supervised Learning on Multi-Window Chest Computed Tomography for Domain-Shift Robustness

## Quick Facts
- arXiv ID: 2510.27213
- Source URL: https://arxiv.org/abs/2510.27213
- Reference count: 40
- Primary result: Privacy-preserving CSSL framework achieves 87.3% classification accuracy and 0.953 AUC on COVID-19/lung cancer tasks

## Executive Summary
This paper proposes a privacy-preserving continual self-supervised learning (CSSL) framework for chest CT images across multiple domains (window settings). The method addresses catastrophic forgetting in domain-shift environments by storing intermediate feature representations (rather than raw images) in a memory buffer, ensuring privacy compliance. A novel feature distillation approach integrates Wasserstein distance-based knowledge distillation (WKD) with batch-knowledge ensemble (BKE) to align feature distributions and stabilize knowledge propagation across training stages. Evaluated on J-MID and RICORD datasets, the method achieved superior performance compared to state-of-the-art CSSL and SSL baselines.

## Method Summary
The framework uses a three-stage approach with ViT-B encoder and MAE pretraining. Stage 1 trains on domain D1 with MAE objective. Stage 2 extracts features from the trained encoder, clusters them via k-means, and stores N×α×β feature representations in memory buffer B. Stage 3 trains on domain D2 with joint SSL (MAE) and feature distillation losses, using stored features from B. The feature distillation combines WKD (aligning feature distributions via Wasserstein distance) and BKE (propagating knowledge through batch similarity graphs). Downstream tasks are evaluated by fine-tuning the encoder on labeled datasets.

## Key Results
- Classification accuracy improved to 87.3% and AUC reached 0.953 on COVID-19 and lung cancer tasks
- Ablation study confirmed effectiveness of each component, particularly WKD+BKE integration with latent replay
- Privacy compliance achieved by storing intermediate feature representations instead of raw CT images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Storing intermediate feature representations instead of raw images enables continual learning while preserving data privacy.
- Mechanism: The latent replay approach extracts features from a trained encoder (ϕ_M1), clusters them via k-means, and stores only the cluster-center-proximal representations in memory buffer B. During subsequent training stages, these compressed latent features—not raw CT slices—are replayed, eliminating raw-data exposure while retaining distributional information critical for mitigating catastrophic forgetting.
- Core assumption: Feature representations contain sufficient information to approximate the original data distribution for knowledge retention.
- Evidence anchors:
  - [abstract] "by incorporating a latent replay-based mechanism into CSSL, our method mitigates catastrophic forgetting due to domain shifts during continual pretraining while ensuring data privacy"
  - [section 4.4] "replacing the experience-replay-based approach with LR in the proposed CSSL framework improved classification accuracy, as LR eliminated the dependence on raw image storage by replaying latent representations, thereby reducing noise and redundancy"
  - [corpus] Limited corpus support; neighbor paper "Continual Self-supervised Learning Considering Medical Domain Knowledge in Chest CT Images" appears to be prior work by same authors but does not specifically address privacy-preserving mechanisms.

### Mechanism 2
- Claim: Wasserstein distance-based knowledge distillation aligns feature distributions across domains more robustly than KL-divergence under distributional shift.
- Mechanism: WKD models teacher and student feature distributions as Gaussians parameterized by mean (μ) and covariance (Σ). The Wasserstein distance between these distributions is decomposed into mean distance and covariance distance terms, balanced by hyperparameter γ. Minimizing this loss pulls the student's feature distribution toward the teacher's, reducing domain-specific bias without requiring aligned sample pairs.
- Core assumption: Feature distributions can be reasonably approximated as Gaussian; diagonal covariance suffices for high-dimensional feature spaces.
- Evidence anchors:
  - [abstract] "A novel feature distillation approach integrates Wasserstein distance-based knowledge distillation (WKD) with batch-knowledge ensemble (BKE) to align feature distributions and stabilize knowledge propagation"
  - [section 3.3.1] "By computing L_WKD, we enable the feature distribution of M2 to align with that of M1, thereby mitigating data interference due to inter-stage domain shifts"
  - [corpus] No direct corpus evidence for WKD in medical imaging; paper cites Lv et al. (NeurIPS 2024) for WD-based distillation in general vision.

### Mechanism 3
- Claim: Batch-knowledge ensemble stabilizes continual learning by propagating knowledge through feature similarity graphs within mini-batches.
- Mechanism: BKE constructs a similarity matrix A between replayed features (P_T) and current mini-batch features (P_S), normalizes it, and iteratively propagates information via Q^(t) = ωÂQ^(t-1) + (1-ω)P_T. This smoothed representation Q_T is then used for distillation. The mechanism exploits intra-batch structure to reduce noise and reinforce consistent features across domains.
- Core assumption: Similarity between replayed and current features correlates with transferable knowledge; propagation reduces noisy gradients.
- Evidence anchors:
  - [abstract] "batch-knowledge ensemble (BKE) to align feature distributions and stabilize knowledge propagation across training stages"
  - [section 4.4] "BKE exploits the similarity among feature representations within mini-batches and those replayed from B to facilitate feature-level knowledge propagation as well as stabilize the optimization process"
  - [corpus] No corpus papers specifically validate BKE in CSSL; cites Ge et al. (arXiv 2021) for self-distillation with batch knowledge ensembling in ImageNet classification.

## Foundational Learning

- Concept: **Catastrophic Forgetting in Continual Learning**
  - Why needed here: CSSL sequentially trains on domains D1 → D2; without mitigation, learning D2 overwrites representations learned from D1, degrading performance on D1-related tasks.
  - Quick check question: Can you explain why training on new data degrades performance on previously learned data in neural networks?

- Concept: **Masked Autoencoder (MAE) Self-Supervised Pretraining**
  - Why needed here: The framework uses MAE as the base SSL objective, masking 75% of image patches and training the model to reconstruct them from latent representations.
  - Quick check question: How does masking input patches force a model to learn useful representations rather than trivial identity mappings?

- Concept: **Knowledge Distillation (Teacher-Student)**
  - Why needed here: WKD and BKE both operate as distillation mechanisms, transferring knowledge from a "teacher" (previous stage model or stored features) to a "student" (current model).
  - Quick check question: What is the intuition behind matching teacher and student outputs rather than training the student directly on raw data?

## Architecture Onboarding

- Component map:
  - Encoder (ϕ) -> Tokenizer (TM) -> Decoder (ψ) -> Memory Buffer (B) -> WKD Module -> BKE Module -> Fine-tuning Head

- Critical path:
  1. Stage 1: Train M1 on D1 via MAE (L_SSL only)
  2. Stage 2: Extract features via ϕ_M1, cluster, sample N×α×β features into B
  3. Stage 3: Train M2 on D2 + replayed features from B, with L_SSL + L_FD (WKD+BKE)
  4. Fine-tune: Initialize encoder from M2, train task-specific head on labeled data

- Design tradeoffs:
  - Memory buffer size (α, β): Larger buffer preserves more distribution diversity but increases memory footprint; paper uses α=0.01, β=0.05
  - γ (mean-covariance balance): Higher γ emphasizes mean alignment; optimal γ differs by domain order (2.0 for D2→D1, 3.0 for D1→D2 on J-MID)
  - BKE batch size: 32 optimal; smaller batches reduce propagation stability, larger may dilute signal
  - Privacy vs. reconstruction risk: Storing features reduces raw-data exposure but theoretical reconstruction attacks remain possible

- Failure signatures:
  - Performance drop on domain order reversal: Indicates domain exposure order sensitivity; check if D1 vs. D2 similarity to downstream task is misaligned
  - Ablation shows LR alone insufficient: Confirms WKD+BKE synergy required; verify both losses are active
  - High variance across seeds: Suggests unstable clustering or insufficient buffer coverage; increase α or β

- First 3 experiments:
  1. Baseline reproduction: Train MAE on D1 only, fine-tune on downstream task (SARS-CoV-2 CT-Scan). Expect ACC ~0.729 (J-MID) per Table 1.
  2. Ablation: LR vs. Experience Replay: Compare storing features vs. raw images with MSE distillation only. Expect LR to slightly outperform raw-image replay per Table 7.
  3. Full pipeline with hyperparameter sweep: Run D1→D2 CSSL with γ ∈ {0, 1, 2, 3, 4} and batch sizes {16, 32, 64, 128}. Identify optimal configuration for your dataset; expect performance peak at γ≈2-3, batch=32.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can textual diagnostic information be effectively integrated into the current privacy-aware framework to enhance multi-modal representation learning?
- **Basis in paper:** [Explicit] The authors state, "Future work includes extending to multimodal learning," specifically suggesting that "incorporating text information, such as physician-written diagnostic reports, may further improve generalizability."
- **Why unresolved:** The current architecture is designed exclusively for visual feature representations using masked autoencoders and lacks mechanisms to process, align, or replay text-based features within the latent replay buffer.
- **What evidence would resolve it:** A modified framework that simultaneously processes chest CT images and corresponding reports, demonstrating improved AUC on downstream tasks compared to the image-only model.

### Open Question 2
- **Question:** What adaptive feature compression or dynamic memory-management techniques can be implemented to maintain computational efficiency as the number of training domains increases?
- **Basis in paper:** [Explicit] The conclusion identifies scalable memory management as a limitation, noting that "memory requirements increase inevitably with the increasing number of domains" and suggesting future exploration of "adaptive feature compression."
- **Why unresolved:** The current implementation stores feature representations without compression, which poses a scalability challenge for long-term continual pretraining involving numerous domains or institutions.
- **What evidence would resolve it:** Experiments showing that a compressed or dynamically managed buffer maintains classification accuracy (e.g., >87% on COVID-19 tasks) while significantly reducing the memory footprint relative to the standard buffer.

### Open Question 3
- **Question:** How can the framework be made robust to the sequence of domain exposure, mitigating the performance drop observed when the pretraining order does not align with the target task?
- **Basis in paper:** [Inferred] The discussion highlights a "critical issue" where performance is "sensitive to domain-exposure order," yet the proposed method relies on the user clarifying data relationships rather than internally resolving the order-dependency.
- **Why unresolved:** The method currently suffers from performance variance depending on whether the model trains on $D_1 \rightarrow D_2$ or $D_2 \rightarrow D_1$, indicating the continual learning mechanism does not fully decouple knowledge retention from the recency or nature of the domain.
- **What evidence would resolve it:** A study demonstrating statistically similar performance metrics across all permutations of domain sequences (e.g., $D_1 \rightarrow D_2$ vs. $D_2 \rightarrow D_1$) on the same downstream task.

## Limitations
- Domain order sensitivity remains a critical limitation, with performance varying significantly between D1→D2 and D2→D1 sequences
- Privacy guarantee relies on feature-space replay, but theoretical reconstruction attacks from compressed latent representations remain possible
- Memory buffer parameters appear tuned for this specific dataset size and may not scale well to larger or more diverse domains

## Confidence
- High confidence: The mechanism of storing latent features instead of raw images for privacy preservation is well-supported and technically sound
- Medium confidence: The integration of WKD and BKE shows consistent improvements across ablations, though corpus evidence for these specific combinations is limited
- Medium confidence: Classification performance improvements (up to 87.3% ACC) are well-documented but may be dataset-specific given the sensitivity to domain order

## Next Checks
1. Test domain order invariance by training on multiple permutations of window settings and measuring performance variance across orders
2. Evaluate reconstruction attack feasibility by attempting to recover original CT slices from stored latent representations using decoder inversion techniques
3. Scale buffer parameters (α, β) to 10× larger values and measure impact on performance and memory requirements to assess scalability limits