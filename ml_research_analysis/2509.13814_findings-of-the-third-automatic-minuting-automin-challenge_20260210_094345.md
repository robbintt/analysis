---
ver: rpa2
title: Findings of the Third Automatic Minuting (AutoMin) Challenge
arxiv_id: '2509.13814'
source_url: https://arxiv.org/abs/2509.13814
tags:
- task
- minutes
- meeting
- evaluation
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoMin 2025 introduced a new question-answering task to the existing
  automatic meeting summarization challenge. The minuting task covered English and
  Czech project meetings and European Parliament sessions, while the QA task focused
  on English transcripts with monolingual and cross-lingual settings.
---

# Findings of the Third Automatic Minuting (AutoMin) Challenge

## Quick Facts
- **arXiv ID**: 2509.13814
- **Source URL**: https://arxiv.org/abs/2509.13814
- **Reference count**: 18
- **Primary result**: AutoMin 2025 introduced QA task; GPT-4 (2025) achieved highest minuting scores; LLM-based evaluation showed high self-correlation but weak alignment with manual annotations

## Executive Summary
AutoMin 2025 expanded the automatic meeting summarization challenge to include a question-answering task alongside the existing minuting task. The challenge covered English and Czech project meetings and European Parliament sessions for minuting, while QA focused on English transcripts with monolingual and cross-lingual settings. Participation was limited, with only one team joining the minuting task and two teams participating in QA. To enable comprehensive evaluation, organizers included multiple baseline systems and employed LLM-based evaluation with Chain-of-Thought prompting for the minuting task and LLM-as-a-judge for QA. GPT-4 (2025) achieved the highest minuting scores across both domains, with strong performance in fluency and grammaticality. The HallucinationIndexes team, using BART models with reinforcement learning, showed domain-dependent results, excelling on EuroParlMin but underperforming on project meetings. In QA, baseline LLMs outperformed participant submissions, likely due to extractive limitations, with cross-lingual QA performance dropping notably for open models.

## Method Summary
The AutoMin 2025 challenge introduced a new question-answering task to the existing automatic meeting summarization challenge. The minuting task covered English and Czech project meetings and European Parliament sessions, while the QA task focused on English transcripts with monolingual and cross-lingual settings. The evaluation framework employed LLM-based evaluation with Chain-of-Thought prompting for the minuting task and LLM-as-a-judge for QA. Multiple baseline systems were included to enable comprehensive evaluation given the limited participant submissions. The organizers used GPT-4 (2025) as the primary evaluation model and compared its scores with traditional ROUGE-based metrics and manual annotations.

## Key Results
- GPT-4 (2025) achieved the highest minuting scores across both project meetings and EuroParlMin domains
- HallucinationIndexes team showed domain-dependent results with strong EuroParlMin performance but weak project meeting results
- LLM-based evaluation scores showed high correlation among themselves but weak alignment with manual annotations and classical metrics

## Why This Works (Mechanism)
The challenge demonstrates that advanced LLMs like GPT-4 (2025) can effectively summarize meeting transcripts when provided with Chain-of-Thought prompting, which helps break down complex reasoning tasks. The domain-specific performance variations suggest that model architecture and training data specialization significantly impact results, with EuroParlMin's structured parliamentary format being more amenable to current approaches than unstructured project meetings. The use of LLM-as-a-judge for QA evaluation leverages the same reasoning capabilities that make these models effective at the minuting task, though this creates a circular evaluation dependency that may not accurately reflect human judgment quality.

## Foundational Learning
- **Chain-of-Thought prompting**: Breaks down complex reasoning into intermediate steps to improve reasoning accuracy and output quality
  - Why needed: Meeting summarization requires understanding context, identifying key points, and generating coherent narratives
  - Quick check: Compare outputs with and without CoT prompting on sample transcripts

- **LLM-as-a-judge evaluation**: Uses LLMs to assess summary quality instead of relying solely on traditional metrics
  - Why needed: Traditional ROUGE metrics poorly capture fluency and grammaticality in meeting summaries
  - Quick check: Correlate LLM evaluation scores with human annotations across multiple domains

- **Cross-lingual QA performance evaluation**: Measures model capability to answer questions about non-native language transcripts
  - Why needed: Real-world applications require understanding content across language barriers
  - Quick check: Compare monolingual vs cross-lingual performance on identical question sets

- **Domain adaptation in meeting summarization**: Adjusting models to perform well across different meeting types (project vs parliamentary)
  - Why needed: Meeting structures and content vary significantly across contexts
  - Quick check: Fine-tune on domain-specific data and measure performance gains

## Architecture Onboarding

Component map:
Raw transcript -> Preprocessing pipeline -> Encoder-decoder model (BART/LLM) -> Post-processing -> Final summary
Evaluation pipeline: Generated summary -> LLM judge (with CoT) -> Quality scores (fluency, relevance, etc.)

Critical path:
Preprocessing -> BART/LLM generation -> LLM evaluation with Chain-of-Thought prompting -> Final scoring

Design tradeoffs:
- Traditional ROUGE metrics vs LLM-based evaluation: ROUGE provides consistency but misses fluency; LLM evaluation captures natural language quality but may lack objectivity
- Domain-specific vs general models: Specialized models perform better on target domains but lack flexibility
- Cross-lingual vs monolingual approaches: Monolingual models achieve higher quality but limit application scope

Failure signatures:
- Domain mismatch: Models trained on EuroParl data fail on project meetings due to structural differences
- Evaluation circularity: LLM judges may favor outputs similar to their own generation style
- Cross-lingual degradation: Performance drops indicate either data quality issues or architectural limitations

First experiments:
1. Compare BART model performance on EuroParlMin vs project meetings with identical evaluation metrics
2. Run Chain-of-Thought vs direct prompting comparison on meeting summarization quality
3. Test cross-lingual QA performance across different open model families (LLaMA, Phi, Mistral)

## Open Questions the Paper Calls Out
None

## Limitations
- LLM-based evaluation showed high correlation among itself but poor alignment with manual annotations and classical metrics, questioning its validity as a human judgment proxy
- Domain-specific performance variations suggest current approaches may be overly specialized, with HallucinationIndexes excelling on EuroParlMin but underperforming on project meetings
- Limited participation (one team for minuting, two for QA) constrains generalizability of findings and prevents meaningful comparison across diverse methodological approaches

## Confidence
- **High Confidence**: GPT-4 (2025) achieving highest minuting scores across domains
- **Medium Confidence**: BART with reinforcement learning showing domain-dependent results
- **Low Confidence**: LLM-based evaluation reliability as proxy for human judgment

## Next Checks
1. Conduct head-to-head comparison of LLM-based evaluation scores against expanded manual annotation sets across multiple human raters to establish correlation strength and identify systematic biases
2. Test model performance on additional meeting domains beyond project meetings and EuroParlMin to assess domain generalization capabilities
3. Evaluate cross-lingual QA performance using alternative metrics (e.g., answer semantic similarity, information retrieval metrics) to isolate whether performance drops stem from model limitations or evaluation methodology