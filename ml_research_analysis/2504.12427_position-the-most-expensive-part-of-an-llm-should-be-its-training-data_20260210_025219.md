---
ver: rpa2
title: 'Position: The Most Expensive Part of an LLM should be its Training Data'
arxiv_id: '2504.12427'
source_url: https://arxiv.org/abs/2504.12427
tags:
- training
- data
- costs
- text
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper quantifies the often-overlooked human labor cost of
  creating training data for Large Language Models (LLMs). The authors estimate the
  cost of producing LLM training datasets from scratch, based on conservative assumptions
  about writing speed and wages.
---

# Position: The Most Expensive Part of an LLM should be its Training Data

## Quick Facts
- arXiv ID: 2504.12427
- Source URL: https://arxiv.org/abs/2504.12427
- Authors: Nikhil Kandpal; Colin Raffel
- Reference count: 16
- Primary result: Even with conservative assumptions, training data costs for LLMs exceed model training costs by 10-1000x

## Executive Summary
This paper quantifies the often-overlooked human labor cost of creating training data for Large Language Models (LLMs). The authors estimate the cost of producing LLM training datasets from scratch, based on conservative assumptions about writing speed and wages. They find that even with these low estimates, the cost of training data far exceeds the cost of training the models themselves (by 10-1000x). For example, GPT-4's dataset would cost $300x more to produce than the model's training, and DeepSeek-V3's dataset would cost $6000x more. This high cost represents a significant financial burden, making it impractical for most companies to compensate data creators even at minimum wages. The paper concludes by highlighting research directions for more ethical and sustainable data practices.

## Method Summary
The authors calculate training data costs by estimating the human labor required to produce the content at minimum wage rates. They use conservative assumptions about writing speed (250 words/hour) and apply these to publicly known or estimated dataset sizes for major LLMs. The cost comparison is made against publicly available information about the compute costs of training these models. The analysis focuses on the opportunity cost of creating training data from scratch rather than using existing sources.

## Key Results
- Training data costs exceed model training costs by 10-1000x across major LLMs
- GPT-4's dataset would cost 300x more to produce than the model's training
- DeepSeek-V3's dataset would cost 6000x more to produce than the model's training
- These high costs make it impractical for companies to compensate data creators at even minimum wages

## Why This Works (Mechanism)
The paper demonstrates that the economic bottleneck in LLM development has shifted from compute to data acquisition and creation. By quantifying the human labor costs required to produce high-quality training data, the authors reveal that the true cost of building competitive LLMs lies not in the training infrastructure but in the content that fuels these models. This mechanism explains why companies are increasingly focused on data acquisition strategies, including web scraping and partnerships, rather than the traditional narrative that model training is the primary cost driver.

## Foundational Learning

### LLM Training Economics
**Why needed**: Understanding the cost structure of LLM development is crucial for strategic planning and ethical considerations
**Quick check**: Compare compute costs vs. data acquisition costs for major LLM projects to validate the economic shift

### Data Quality vs. Quantity Tradeoffs
**Why needed**: The relationship between data quality, quantity, and model performance determines optimal data acquisition strategies
**Quick check**: Analyze how different data sources (synthetic, curated, scraped) impact model performance per dollar spent

### Labor Economics in AI
**Why needed**: The human cost of data creation raises important questions about fair compensation and sustainable practices
**Quick check**: Calculate minimum viable compensation rates that would make data creation economically feasible

## Architecture Onboarding

### Component Map
Data Sources -> Data Cleaning/Filtering -> Model Training -> Inference/Deployment

### Critical Path
The critical path for cost optimization runs through Data Sources -> Data Cleaning/Filtering, as these steps determine both the quality of the final model and the total investment required.

### Design Tradeoffs
Companies face tradeoffs between using high-cost, high-quality data (potentially fair compensation to creators) versus low-cost, potentially lower-quality or ethically questionable data sources. This creates tension between model performance, ethical considerations, and financial viability.

### Failure Signatures
Models trained on insufficient or low-quality data show degraded performance on complex reasoning tasks, increased hallucinations, and poor generalization to novel scenarios. Financial failure occurs when data costs exceed revenue potential from model deployment.

### First Experiments
1. Cost-benefit analysis of different data acquisition strategies (licensing vs. scraping vs. synthetic generation)
2. Performance benchmarking of models trained on differently sourced data at equivalent monetary investments
3. Economic modeling of fair compensation models for data creators across different scales

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis assumes all training data is created from scratch at minimum wage rates, which likely overstates true costs
- Specific assumptions about dataset sizes and composition are not fully detailed, making independent verification difficult
- The analysis focuses only on labor costs without considering other factors like bulk licensing agreements or data partnerships
- The economic model doesn't account for the varying value of different data sources based on quality and relevance

## Confidence
- **High confidence**: The general insight that training data costs are substantial and often underestimated
- **Medium confidence**: The specific claim that data costs exceed training costs by orders of magnitude, given the conservative assumptions used
- **Low confidence**: The precise multiplier values (10-1000x) which depend heavily on assumptions about data creation vs. acquisition

## Next Checks
1. Conduct a detailed audit of actual data acquisition costs for major LLM training runs, distinguishing between newly created content, licensed data, and freely available sources
2. Develop a more nuanced cost model that accounts for data quality, relevance, and the varying costs of different data sources (e.g., professional writers vs. crowdsourced content)
3. Compare the total cost of data acquisition (including curation, cleaning, and filtering) against training compute costs across multiple successful LLM projects to validate the relative magnitude claims