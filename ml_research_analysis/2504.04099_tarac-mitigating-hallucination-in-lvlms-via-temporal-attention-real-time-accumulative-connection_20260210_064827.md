---
ver: rpa2
title: 'TARAC: Mitigating Hallucination in LVLMs via Temporal Attention Real-time
  Accumulative Connection'
arxiv_id: '2504.04099'
source_url: https://arxiv.org/abs/2504.04099
tags:
- attention
- tokens
- image
- tarac
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses hallucination in Large Vision-Language Models
  (LVLMs), where models generate incorrect information not supported by input images.
  Hallucinations arise from language model biases, limited visual perception, and
  multimodal data issues.
---

# TARAC: Mitigating Hallucination in LVLMs via Temporal Attention Real-time Accumulative Connection

## Quick Facts
- **arXiv ID**: 2504.04099
- **Source URL**: https://arxiv.org/abs/2504.04099
- **Reference count**: 12
- **Key outcome**: TARAC reduces hallucination by 25.2% CS and 8.7% CI on CHAIR vs VCD, and ~49% Hal on AMBER for LLaVA-1.5-7B, while improving perception and adding ~4% inference overhead

## Executive Summary
TARAC addresses hallucination in Large Vision-Language Models (LVLMs) by dynamically accumulating and injecting visual attention during autoregressive generation. The method targets the problem of decaying visual attention as LVLMs generate longer sequences, where models shift reliance toward language priors and previously generated text tokens. TARAC maintains an exponentially-weighted moving average of image-token attention across generation steps and reinjects this accumulated attention to maintain visual grounding, significantly improving factual consistency without requiring model training.

## Method Summary
TARAC is a training-free method that mitigates hallucination by accumulating image-token attention across generation steps and reinjecting it to counter attention decay. The method extracts attention from specific transformer layers, maintains an exponential moving average with parameter α, injects accumulated attention scaled by β, and renormalizes rows to sum to 1. It operates selectively on middle transformer layers (empirically determined as [10:16] for LLaVA-1.5-7B) to avoid interference with shallow visual-to-text information transfer and deep task-specific processing.

## Key Results
- On LLaVA-1.5-7B: 25.2% reduction in CS and 8.7% reduction in CI on CHAIR benchmark vs VCD
- On AMBER benchmark: ~21.2% reduction in CHAIR and ~49% reduction in Hal scores
- Improves perception scores on MME while adding only ~4% inference overhead
- Generalizes to Qwen2-VL and InternVL2 with consistent improvements

## Why This Works (Mechanism)

### Mechanism 1
Hallucinations correlate with decayed attention to image tokens during autoregressive generation. As LVLMs generate longer sequences, causal attention progressively dilutes focus on image tokens, shifting reliance toward previously generated text tokens and language priors. This increases hallucination risk at later positions, as hallucinated content is more likely when visual attention is low and the model fills gaps with linguistic priors. Evidence from KDE analysis shows hallucinated tokens exhibit lower attention to image tokens and appear later in captions.

### Mechanism 2
Accumulating and reinjecting image-token attention counters decay and maintains visual grounding. TARAC maintains an exponentially-weighted moving average of image-token attention across generation steps using max-pooling across heads to preserve salient patterns, then injects this accumulated attention back with scaling factor β before renormalization. This ensures the most informative visual attention patterns are captured and shared across different heads while preventing excessively distant visual information from interfering with current token generation.

### Mechanism 3
Layer-selective application is necessary because visual attention functions vary across transformer depth. TARAC is applied only to middle layers (e.g., [10:16] for LLaVA-1.5-7B), avoiding shallow layers that transfer visual information to question tokens and deep layers that may be task-specific. Different layers serve different functions in multimodal integration, and intervening at wrong layers introduces noise.

## Foundational Learning

- **Causal Attention in Transformers**: Understanding that each generated token attends to all prior tokens (including image tokens) is prerequisite. Quick check: Why does the attention matrix dimension grow as (N_t × N_t) during generation?
- **Attention Sink Phenomenon**: The paper relates TARAC's effect to inducing attention sinks on image tokens; prior work shows this correlates with reduced hallucination. Quick check: What is an attention sink, and why might directing it toward image tokens reduce hallucination?
- **CHAIR Benchmark Metrics**: CS (sentences with hallucination) and CI (hallucinated objects proportion) are the primary evaluation metrics. Quick check: If a model generates "a cat and a dog" for an image with only a cat, what happens to CI vs. CS?

## Architecture Onboarding

- **Component map**: Attention extraction -> Accumulation buffer -> Injection module -> Renormalization
- **Critical path**: 1) Identify image token indices in KV cache 2) Extract attention to image tokens from target layers 3) Update accumulated attention via EMA 4) Inject accumulated attention scaled by β 5) Renormalize row to sum to 1
- **Design tradeoffs**: Max vs. mean pooling (salient vs. smooth patterns), row-sum vs. softmax normalization (relative weights vs. uniform), α value (short vs. long memory), layer range (noise vs. interference)
- **Failure signatures**: Repetitive generation (β too high), short/incomplete outputs (over-suppression of language priors), no improvement on discriminative tasks (expected behavior)
- **First 3 experiments**: 1) Layer sweep on MME to identify optimal layer range 2) Hyperparameter grid on CHAIR to optimize α and β 3) Cross-model transfer to verify layer range generalizability

## Open Questions the Paper Calls Out
1. Can TARAC be combined with existing contrastive decoding or penalty-based methods to achieve complementary hallucination reduction?
2. How can the scaling factor (β) and memory update factor (α) be adjusted dynamically during inference rather than manually tuned?
3. How robust is the visual attention mechanism in TARAC when evaluated on counterfactual or virtual imagery rather than real-world scenarios?

## Limitations
- Method relies on empirical identification of optimal layer ranges that may not generalize across LVLM architectures
- Assumes accumulated attention patterns remain relevant throughout generation (untested beyond 8-token window)
- "Training-free" claim is misleading as method requires architecture-specific configuration and hyperparameter tuning
- Effectiveness on very long sequences (>50 tokens) or open-ended generation tasks remains unvalidated

## Confidence
**High confidence**: Correlation between hallucinations and reduced attention to image tokens is well-supported by KDE analysis; empirical improvements on CHAIR and AMBER are robust across multiple models.
**Medium confidence**: Mechanism of accumulated attention injection is theoretically sound but relies on empirical choices that may not be optimal or generalizable.
**Low confidence**: "Training-free" characterization and claims about task generalization are limited by evaluation primarily on captioning and perception tasks.

## Next Checks
1. Apply TARAC with LLaVA-1.5-7B optimal parameters directly to InternVL2 and Qwen2-VL without re-tuning to test layer range transferability.
2. Generate captions of 50+ tokens for complex images to verify whether accumulated attention remains relevant or becomes stale.
3. Evaluate TARAC on visual question answering tasks requiring multi-step reasoning (ScienceQA, MMMU) to test cross-task generalization beyond perception and captioning.