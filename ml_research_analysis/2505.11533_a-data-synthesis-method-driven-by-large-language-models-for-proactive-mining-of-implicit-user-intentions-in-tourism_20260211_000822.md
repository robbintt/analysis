---
ver: rpa2
title: A Data Synthesis Method Driven by Large Language Models for Proactive Mining
  of Implicit User Intentions in Tourism
arxiv_id: '2505.11533'
source_url: https://arxiv.org/abs/2505.11533
tags:
- user
- intention
- data
- agent
- implicit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SynPT, an LLM-driven data synthesis method
  to address the challenge of mining implicit user intentions in tourism. SynPT constructs
  user and assistant agents to simulate dialogues based on seed data from Chinese
  tourism websites, generating a dataset with explicit reasoning capabilities.
---

# A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism

## Quick Facts
- **arXiv ID**: 2505.11533
- **Source URL**: https://arxiv.org/abs/2505.11533
- **Reference count**: 39
- **Primary result**: SynPT improves fine-tuned models' ability to mine implicit user intentions in tourism dialogues, achieving 9.48% higher vagueness judgment accuracy, 8.66% higher implicit intention mining accuracy, and 33.49% higher intention summarization accuracy compared to existing methods.

## Executive Summary
This paper addresses the challenge of mining implicit user intentions in tourism dialogues through SynPT, an LLM-driven data synthesis method. SynPT constructs user and assistant agents that simulate dialogues based on seed data from Chinese tourism websites, generating a dataset with explicit reasoning capabilities. The method introduces probabilistic control for initial inquiries, a memory stack to reduce contextual redundancy, and modules for user emotion and intention value thinking. Experimental results demonstrate significant improvements in mining implicit intentions, with SynPT outperforming existing methods across multiple evaluation metrics while using fewer training samples.

## Method Summary
SynPT employs a dual-agent synthesis approach where one LLM simulates user behavior and another simulates assistant responses to generate training data. The method introduces three key innovations: probabilistic control over initial inquiry detail to align training distributions with real user behavior, a memory stack mechanism to compress dialogue history and reduce token consumption during intention tracking, and explicit thinking modules for emotion detection and intention value prediction. The system generates synthetic dialogues with reasoning traces, which are then used to fine-tune smaller models (Qwen2.5-7B) via LoRA. The approach is evaluated on Chinese tourism tasks across multiple cities, demonstrating superior performance in vagueness judgment, implicit intention mining, and intention summarization.

## Key Results
- SynPT achieves 9.48% higher vagueness judgment accuracy compared to baseline methods
- Implicit intention mining accuracy improves by 8.66% over existing approaches
- Intention summarization accuracy increases by 33.49% with SynPT-generated data
- SynPT with 30% training samples outperforms 100% baseline data, indicating quality over quantity
- Memory stack reduces token consumption by 68K tokens while improving summarization accuracy by 9%

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Control for Initial Inquiry Granularity
SynPT samples the number of intentions in initial queries from a normalized discrete distribution derived from Gaussian parameters, preventing the LLM's bias toward verbose queries. This aligns synthetic training distributions with real-world user behavior diversity, where some queries contain zero explicit intentions while others contain all. Evidence shows SynPT produces more diverse query lengths with lower averages and achieves better performance with fewer training samples.

### Mechanism 2: Memory Stack for Contextual Redundancy Reduction
Instead of passing full dialogue history to the intention extraction LLM each round, SynPT maintains a compressed state representation in a stack. This reduces token consumption from 831K to 763K while improving intention summarization accuracy from 40.83% to 49.89%. The compression preserves sufficient information for accurate intention tracking while avoiding the accuracy degradation caused by excessive historical context.

### Mechanism 3: Emotion Thinking and Intention Value Prediction
Both agents evaluate willingness-to-continue at each turn, allowing early termination when users show frustration from over-questioning. The assistant agent also predicts top-3 likely intention values for current implicit intention types. This enables more actionable responses and prevents negative interaction loops. Qwen-PT achieves 91.54% accuracy in detecting user unwillingness, and SynPT scores 1.71 on reference option reasonableness (0-3 scale).

## Foundational Learning

- **Concept: Dual-Agent Dialogue Synthesis**
  - Why needed: SynPT builds on the paradigm where one LLM simulates the user and another the assistant, generating training data through their interaction.
  - Quick check: Can you explain why Single-Agent synthesis (one LLM generating both sides) might produce less realistic dialogue patterns than Dual-Agent?

- **Concept: Implicit vs. Explicit Intention**
  - Why needed: The core task is mining intentions the user hasn't stated. Explicit intentions appear in the query text; implicit intentions must be inferred and proactively queried.
  - Quick check: Given "Recommend Western restaurants," what are two likely implicit intentions the system should mine?

- **Concept: Chain-of-Thought Distillation**
  - Why needed: SynPT generates not just responses but explicit reasoning traces (emotion thinking, intention value thinking). Fine-tuning on reasoning-augmented data transfers deliberative capabilities to smaller models.
  - Quick check: Why might training on reasoning traces improve a 7B model's proactive questioning ability compared to training on final responses only?

## Architecture Onboarding

- **Component map**: Seed Data Pool (6 tasks, 8 cities) → User Agent (Initial Inquiry Generator → Emotion Thinking → Response Generation) → Memory Stack (Intention Thinking) → Assistant Agent (Emotion Thinking → Reference Options Thinking → Response Generation) → Recording Component → SynPT-Dialog dataset → Downstream fine-tuning → Qwen-PT model

- **Critical path**: Seed data quality → Initial inquiry distribution alignment → Memory stack state updates → Emotion-triggered termination → Reference option relevance → Training data quality → Fine-tuned model proactive capability

- **Design tradeoffs**: Probabilistic control adds sampling complexity but reduces overfitting to verbose queries; memory stack saves tokens but may lose conversational nuance (8% token increase when removed); reflection strategies increased tokens by 40% while degrading performance; emotion thinking prevents over-questioning but may terminate conversations prematurely

- **Failure signatures**: Overfitting to specific task phrasing (struggles with out-of-distribution tasks until diverse seed data added); premature intention summarization (before all intentions surface); repetitive questioning (models re-request already-provided intentions)

- **First 3 experiments**: 1) Baseline comparison: Run Single-Agent, Dual-Agent, and SynPT synthesis on identical seed data; fine-tune Qwen2.5-7B on each; evaluate on held-out cities. 2) Ablation study: Disable probabilistic control and memory stack independently and jointly; measure token consumption and accuracy deltas. 3) LLM combination sweep: Vary user agent and assistant agent LLMs with 10% training data; identify combinations maximizing accuracy while minimizing token costs.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can efficient memory stack strategies or component reuse significantly reduce SynPT's high token consumption during data generation without compromising dialogue quality? The current implementation is computationally expensive during generation.

- **Open Question 2**: How can the SynPT architecture be refined to prevent downstream models from repeating queries about user intentions that have already been provided? The memory stack mechanism doesn't completely eliminate redundant inquiries in the fine-tuned model.

- **Open Question 3**: How can reflection strategies be adapted to effectively enhance intention mining accuracy without the token inefficiencies observed in the naive implementation? The current method creates a low proportion of useful correction samples, making it difficult for the model to learn effective error correction.

## Limitations

- **Seed Data Bias**: Relies on Chinese tourism websites (Ctrip, Qunar), potentially encoding cultural and linguistic biases specific to Chinese travel behavior without validation on non-Chinese contexts.

- **LLM Dependency**: Performance heavily depends on synthesis LLM quality, with absolute performance constrained by underlying model capabilities that weren't explored with weaker synthesis models.

- **Memory Stack Fidelity**: Compresses dialogue history into state representations that may lose nuanced contextual information, with no analysis of how state compression errors accumulate over extended dialogues.

## Confidence

- **High Confidence**: Empirical improvements in accuracy metrics (9.48% vagueness judgment, 8.66% implicit intention mining, 33.49% summarization) are well-supported by controlled experiments comparing SynPT against baseline synthesis methods.

- **Medium Confidence**: Claimed mechanism of probabilistic control improving generalization through better training distribution alignment is supported by performance with fewer samples, but direct evidence linking distribution parameters to real user behavior is limited.

- **Low Confidence**: Assertion that emotion thinking and reference option modules significantly improve user experience lacks direct user study evidence, with performance measured through LLM evaluation rather than human preference judgments.

## Next Checks

1. **Cross-Cultural Generalization Test**: Apply SynPT to seed data from non-Chinese tourism sources (e.g., TripAdvisor, Booking.com) and evaluate whether synthesized data maintains performance advantages across different cultural contexts and languages.

2. **Memory Stack Error Analysis**: Conduct detailed error analysis comparing state-compressed versus full-history intention extraction accuracy across different dialogue lengths and intention complexity levels to quantify information loss.

3. **Human Preference Validation**: Conduct user study comparing assistant responses generated by Qwen-PT versus baseline models, measuring user satisfaction, perceived helpfulness, and frustration levels to validate claimed user experience improvements from emotion-aware modules.