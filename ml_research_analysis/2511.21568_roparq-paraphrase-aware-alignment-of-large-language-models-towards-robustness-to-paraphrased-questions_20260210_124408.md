---
ver: rpa2
title: 'RoParQ: Paraphrase-Aware Alignment of Large Language Models Towards Robustness
  to Paraphrased Questions'
arxiv_id: '2511.21568'
source_url: https://arxiv.org/abs/2511.21568
tags:
- question
- questions
- robustness
- each
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models often fail to answer paraphrased questions
  consistently, suggesting a reliance on superficial patterns rather than true semantic
  understanding. To address this, RoParQ was introduced as a benchmark specifically
  designed to evaluate cross-paraphrase consistency in closed-book multiple-choice
  QA.
---

# RoParQ: Paraphrase-Aware Alignment of Large Language Models Towards Robustness to Paraphrased Questions

## Quick Facts
- arXiv ID: 2511.21568
- Source URL: https://arxiv.org/abs/2511.21568
- Authors: Minjoon Choi
- Reference count: 10
- Key outcome: Fine-tuned lightweight models achieved consistency levels comparable to much larger pre-trained models on paraphrase-sensitive QA.

## Executive Summary
Large language models often fail to answer paraphrased questions consistently, suggesting a reliance on superficial patterns rather than true semantic understanding. To address this, RoParQ was introduced as a benchmark specifically designed to evaluate cross-paraphrase consistency in closed-book multiple-choice QA. The benchmark was constructed by paraphrasing questions from standard datasets and selecting examples that elicited inconsistent confidence from a judge model. A novel evaluation metric, XParaCon, was proposed to quantify robustness by measuring the standard deviation of accuracies across question variants. A reasoning-based, paraphrase-aware supervised fine-tuning (SFT) strategy was also implemented to align models toward semantic invariance. Experimental results showed that fine-tuned lightweight models achieved consistency levels comparable to much larger pre-trained models, demonstrating the effectiveness of the approach in mitigating superficial memorization and fostering more robust, reliable LLMs.

## Method Summary
RoParQ was constructed by paraphrasing questions from existing QA datasets using Gemini 2.5 Flash Lite and Claude 3.5 Sonnet. A judge model (Llama-3.1-8B-Instruct) evaluated 8 choice permutations per variant to identify examples showing "inconsistent confidence" - correct on some paraphrases but incorrect on others. These were retained to form the benchmark. A novel metric, XParaCon, measures consistency by computing the standard deviation of accuracies across paraphrase variants and applying a log transform. The reasoning-based SFT approach trains models to explicitly generate paraphrases and verify answer consistency across variants, coupling answer selection with semantic comparison.

## Key Results
- Fine-tuned lightweight models achieved XParaCon scores comparable to much larger pre-trained models
- The SFT approach successfully improved cross-paraphrase consistency while maintaining competitive accuracy
- Models showed measurable improvement in handling paraphrased questions without sacrificing performance on original formulations
- Math reasoning consistently showed lower XParaCon than general knowledge across all models

## Why This Works (Mechanism)

### Mechanism 1: Targeted Data Selection via Inconsistent Confidence Filtering
The filtering process creates a benchmark that specifically targets paraphrase-sensitivity weaknesses rather than knowledge gaps by retaining only examples where a judge model exhibits "inconsistent confidence" - correct on some paraphrase variants but incorrect on others. This isolates cases where knowledge exists but surface patterns cause failures, assuming models exhibiting inconsistent confidence are relying on surface-level cues rather than semantic understanding.

### Mechanism 2: Reasoning-Based Semantic Invariance Training
The SFT approach couples answer selection with explicit reasoning steps: restate the question, produce a meaning-preserving paraphrase, and verify the same option is predicted. This creates a supervised signal for semantic comparison, making paraphrase awareness an explicit learning objective rather than an emergent property, activating latent reasoning capabilities to compare semantics across phrasings.

### Mechanism 3: XParaCon as Variance-Based Robustness Quantifier
XParaCon measures the standard deviation of accuracies across paraphrase variants, providing a direct metric of cross-paraphrase consistency. Higher values indicate better consistency (lower variance across paraphrases), reflecting the degree of surface-pattern reliance versus semantic understanding through accuracy variance across semantically equivalent paraphrases.

## Foundational Learning

- **Concept: Closed-book QA setting** - Why needed: Enforces performance reflects parametric knowledge and reasoning, not retrieval. Questions with passages are excluded to isolate paraphrase sensitivity from context-dependence. Quick check: Can you explain why removing passage-dependent questions helps isolate paraphrase sensitivity from retrieval capability?

- **Concept: Semantic invariance** - Why needed: The core premise is that semantically equivalent questions should yield identical answers. Understanding this principle is essential for interpreting XParaCon scores and the reasoning-based SFT objective. Quick check: If a model answers "What is the capital of France?" correctly but fails on "France's capital city is what?", what does this suggest about its representations?

- **Concept: LoRA (Low-Rank Adaptation)** - Why needed: The paper uses LoRA for efficient fine-tuning. Understanding its parameter-efficient nature helps contextualize how lightweight models achieve strong results without full fine-tuning costs. Quick check: Why might LoRA be preferred over full fine-tuning when training smaller models for paraphrase awareness?

## Architecture Onboarding

- **Component map:** Source questions → Preprocessing → Paraphrase generation → Judge evaluation (filter for inconsistent confidence) → RoParQ benchmark → SFT on train split (70%) → XParaCon evaluation on test split (15%)

- **Critical path:** Source questions → Preprocessing → Paraphrase generation → Judge evaluation (filter for inconsistent confidence) → RoParQ benchmark → SFT on train split (70%) → XParaCon evaluation on test split (15%)

- **Design tradeoffs:** Proprietary paraphrase generators ensure high quality but create external dependency; strict "perfectly correct" definition is conservative and may exclude borderline cases; judge model choice significantly impacts benchmark composition

- **Failure signatures:** Llama-3.1-8B-Instruct showed accuracy-consistency trade-off (XParaCon improvement but accuracy decline on math); domain disparity with math reasoning consistently lower XParaCon; Claude refused 2 examples due to content filters

- **First 3 experiments:** 1) Baseline establishment: Run target model on RoParQ test split with all 8 choice permutations per question variant. Compute per-variant accuracies and aggregate XParaCon score. 2) Reasoning component ablation: Train three SFT variants—(a) paraphrase generation only, (b) verification only, (c) full pipeline—to isolate which reasoning steps drive consistency gains. 3) Cross-domain transfer test: Fine-tune on general knowledge subset, evaluate on math reasoning (and vice versa) to assess whether paraphrase awareness transfers across reasoning domains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the reasoning-based, paraphrase-aware alignment strategy be effectively generalized to open-ended question answering or generative tasks beyond multiple-choice formats?
- Basis in paper: The Limitations section states that the RoParQ benchmark is restricted to multiple-choice questions and that extending it to open-ended questions is necessary to broaden its scope.
- Why unresolved: The current methodology relies on selecting from fixed choices to determine "perfect correctness" and calculate accuracy, a mechanism that does not exist in free-form text generation.
- What evidence would resolve it: Applying the SFT protocol to open-ended QA datasets (e.g., Natural Questions) and evaluating consistency using semantic similarity metrics (like BERTScore) rather than exact match.

### Open Question 2
- Question: Do advanced alignment techniques like Reinforcement Learning from Human Feedback (RLHF) yield superior semantic invariance compared to the Supervised Fine-Tuning (SFT) approach utilized?
- Basis in paper: The Limitations section notes that the study relied on SFT and explicitly suggests exploring RLHF or other alignment techniques in future research.
- Why unresolved: While SFT successfully improved consistency, RLHF optimizes directly against a reward model, which could potentially enforce the "semantic invariance" constraint more rigorously than static supervised data.
- What evidence would resolve it: Training models using RLHF with a reward function penalizing paraphrase inconsistency and comparing the resulting XParaCon scores against the SFT baselines.

### Open Question 3
- Question: Does the observed paraphrase robustness transfer effectively to multilingual contexts, or is it specific to the English syntax evaluated?
- Basis in paper: The Limitations section mentions the evaluation was conducted solely in English and highlights the need to explore robustness in a multilingual context.
- Why unresolved: Paraphrasing involves syntactic restructuring that varies significantly by language; it is unknown if the "semantic invariance" learned via English SFT generalizes to languages with different morphological structures.
- What evidence would resolve it: Evaluating the fine-tuned models on translated versions of the RoParQ benchmark (e.g., MMLU in diverse languages) with generated paraphrases in those target languages.

### Open Question 4
- Question: Is there an inherent trade-off between optimizing for cross-paraphrase consistency and maintaining raw accuracy in complex reasoning domains?
- Basis in paper: The Results section notes that for the Llama-3.1-8B-Instruct model on the math reasoning subset, the significant boost in XParaCon was accompanied by a "minor decline in raw accuracy."
- Why unresolved: It is unclear if this accuracy drop is an anomaly of the specific model size or an indication that enforcing surface-level invariance might inadvertently degrade the model's precision on step-by-step reasoning tasks.
- What evidence would resolve it: A Pareto analysis of accuracy vs. XParaCon scores across different model scales (e.g., 7B vs 70B) specifically on the MathQA subset to see if the trade-off persists or disappears with scale.

## Limitations

- The benchmark is restricted to multiple-choice questions, limiting generalizability to open-ended QA tasks
- The effectiveness depends on the quality and consistency of the judge model's judgments
- The observed accuracy-consistency trade-off on math reasoning suggests potential domain-specific limitations

## Confidence

- **High Confidence:** The empirical observation that models show accuracy drops on paraphrased questions (surface-form brittleness) is well-established and directly observed in the data
- **Medium Confidence:** The claim that selectively retaining "inconsistent confidence" examples creates a targeted benchmark for paraphrase-sensitivity is supported by the filtering methodology but depends on judge model quality
- **Medium Confidence:** The effectiveness of reasoning-based SFT in improving cross-paraphrase consistency is demonstrated on the RoParQ benchmark, but the trade-off with accuracy on math reasoning suggests potential domain-specific limitations

## Next Checks

1. **Judge Model Sensitivity Analysis:** Repeat the benchmark construction using different judge models (e.g., GPT-4, Claude) to assess how sensitive the RoParQ dataset composition is to the choice of judge. Compare the overlap of examples selected by different judges.

2. **Paraphrase Quality Validation:** Conduct human evaluation of a sample of paraphrase pairs to verify semantic equivalence and detect potential subtle semantic drift. Quantify the proportion of paraphrases that are truly meaning-preserving versus those with minor semantic differences.

3. **Long-Tail Paraphrase Coverage:** Test fine-tuned models on a held-out set of paraphrased questions generated by methods not seen during training (e.g., different paraphrasing models, adversarial paraphrases) to assess generalization beyond the specific paraphrase types used in RoParQ construction.