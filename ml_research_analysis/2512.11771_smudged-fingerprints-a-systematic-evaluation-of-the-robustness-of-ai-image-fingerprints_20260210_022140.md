---
ver: rpa2
title: 'Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image
  Fingerprints'
arxiv_id: '2512.11771'
source_url: https://arxiv.org/abs/2512.11771
tags:
- attacks
- removal
- forgery
- attribution
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first systematic security evaluation of
  model fingerprint detection (MFD) techniques for AI image attribution. The authors
  formalize three threat models capturing different attacker knowledge levels and
  implement five attack strategies to evaluate 14 representative MFD methods across
  12 state-of-the-art generative models.
---

# Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints

## Quick Facts
- arXiv ID: 2512.11771
- Source URL: https://arxiv.org/abs/2512.11771
- Reference count: 40
- Primary result: No evaluated MFD method achieves both high attribution accuracy and robustness across all threat models

## Executive Summary
This paper provides the first systematic security evaluation of model fingerprint detection (MFD) techniques for AI image attribution. The authors formalize three threat models capturing different attacker knowledge levels and implement five attack strategies to evaluate 14 representative MFD methods across 12 state-of-the-art generative models. The evaluation reveals that removal attacks are highly effective, achieving success rates above 80% in white-box settings and over 50% under black-box access, while forgery attacks are more challenging but still show significant success. The study identifies a fundamental utility-robustness trade-off: MFD methods with higher attribution accuracy are generally more vulnerable to attacks.

## Method Summary
The evaluation pipeline trains 14 fingerprint extractors (φ) and a unified attribution classifier (h) on 1,000 images per model. Attack strategies include white-box methods (W1: direct gradient differentiation, W2: analytic approximation, W3: neural surrogate) and black-box methods (B1: surrogate transferability, B2: image transformations). All attacks use Projected Gradient Descent with ℓ∞ constraint (ε=0.025) and quality constraints (LPIPS < 0.05, PSNR > 35 dB). The study evaluates 12 generative models (6 GANs, 3 VAEs, 3 diffusion models) across three feature domains: RGB, frequency, and learned features.

## Key Results
- Removal attacks achieve >80% success in white-box settings across all evaluated MFD methods
- Black-box removal attacks exceed 50% success through surrogate transferability
- White-box forgery attacks achieve up to 98% success for some methods
- A clear inverse relationship exists between attribution accuracy and adversarial robustness

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Fingerprint Removal
Adversaries with white-box access can remove model fingerprints by computing gradients through the differentiable MFD pipeline. When fingerprint extractor φ and attribution classifier h are differentiable, attackers optimize perturbations via Projected Gradient Descent (PGD) to minimize attribution confidence toward the true source label, directly leveraging gradient flow through F(x) = h(φ(x)).

### Mechanism 2: Surrogate Transferability for Black-Box Attacks
Even without access to φ or h, adversaries can achieve >50% removal success by training surrogate classifiers on generated images. Knowing the candidate generator set Y, the attacker samples images from each G ∈ Y, trains surrogate h_s on raw images (bypassing φ), crafts adversarial examples against h_s, and exploits transferability—these examples often fool the true MFD system F despite different architectures.

### Mechanism 3: Utility-Robustness Trade-off
MFD methods achieving higher clean attribution accuracy tend to be more vulnerable to adversarial attacks. High-utility methods (e.g., CNN-based Wang20 at 98.47% accuracy) rely on rich learned representations that provide smooth gradients exploitable by attackers; robust methods (e.g., Marra19a's denoising residuals) use less informative but manipulation-resistant features, sacrificing accuracy.

## Foundational Learning

### Concept: Projected Gradient Descent (PGD) with Perceptual Constraints
- Why needed here: All gradient-based attacks use PGD bounded by ℓ∞ ≤ 0.025 to ensure perturbations remain imperceptible (LPIPS < 0.05, PSNR > 35 dB)
- Quick check question: Given image x and classifier F, formulate the PGD update for removal attack minimizing confidence in true label y while enforcing d(x, x') ≤ ε

### Concept: Adversarial Transferability
- Why needed here: Black-box attacks depend on transferability—adversarial examples crafted against surrogate h_s fooling target F despite architectural differences
- Quick check question: Why might gradients from a CNN surrogate transfer to a frequency-domain MFD method?

### Concept: Fingerprint Feature Domains (RGB, Frequency, Learned)
- Why needed here: The 14 MFD methods span three domains with different robustness profiles; understanding feature extraction is critical for selecting attack strategies
- Quick check question: For a co-occurrence matrix feature (non-differentiable histogram), which white-box attack strategy applies—W1, W2, or W3?

## Architecture Onboarding

### Component Map:
- **Generative Models**: 12 SOTA models (6 GANs: StyleGAN2/3, GANformer, StyleSwin, R3GAN, CIPS; 3 VAEs: VDVAE, VQ-VAE, NVAE; 3 Diffusion: NCSN++, LDM, ADM)
- **Fingerprint Extractor φ**: 14 methods across RGB (4), frequency (7), learned features (3); outputs fingerprint vectors of varying dimensions (4 to 2048)
- **Attribution Classifier h**: MLP with [512, 256, 128] hidden layers mapping fingerprints to 12-class generator labels
- **Attack Infrastructure**: PGD optimizer (50 iterations, momentum 0.9), surrogate φ_s (CNN regressor), surrogate h_s (CNN classifier)

### Critical Path:
Image x sampled from generator G → φ(x) extracts fingerprint → h(φ(x)) predicts source → compare to true label. Attack path: Select knowledge level → choose strategy (W1/W2/W3 for white-box, B1/B2 for black-box) → optimize perturbation → verify ASR under quality constraints

### Design Tradeoffs:
- **W1 vs W2 vs W3**: W1 requires full differentiability (most effective), W2 needs analytic approximation (e.g., soft histograms for co-occurrence), W3 trains neural surrogate (most general but highest approximation error)
- **Accuracy vs Robustness**: Wang20 (98.47% accuracy, 100% W1 ASR) vs Marra19a (66.67% accuracy, 40.71% W3 ASR, 9.55% B1 ASR)
- **Removal vs Forgery**: Removal targets any misclassification (easier); forgery requires targeted misclassification to specific G_t (harder but achievable in white-box)

### Failure Signatures:
- **W2/W3 underperforming B1**: Surrogate approximation error exceeds black-box transfer gap; improve surrogate training data or architecture
- **High ASR variance (e.g., McCloskey18 W3 at ±28.81%)**: Sample-specific vulnerability; investigate which images are intrinsically easier to attack
- **Forgery ASR near 0% for specific methods**: Target model's fingerprint region may be geometrically distant; check decision boundary topology

### First 3 Experiments:
1. **Reproduce W1 baseline**: Run W1 removal on Giudice21 (differentiable DCT-based) with ε=0.025; expect 100% ASR, LPIPS < 0.05; validates gradient flow through pipeline
2. **Test B1 transferability**: Train h_s CNN on 1000 images/generator, run B1 attacks on Nataraj19 (non-differentiable co-occurrence); expect ~85% ASR per Table II; confirms surrogate quality
3. **Probe utility-robustness**: Compare Wang20 (high-utility CNN) vs Marra19a (residual-based) under B1 removal; expect Wang20 ~78% ASR vs Marra19a ~9.5% ASR; validates trade-off hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fingerprint suppression or forgery leave detectable traces that a verifier could flag for closer inspection?
- Basis in paper: Authors state future work "should also study whether fingerprint suppression or forgery leaves detectable traces that could be used as a warning signal that a verifier can flag for a more careful inspection."
- Why unresolved: The paper only evaluates attack success rates without analyzing whether attacks themselves introduce secondary artifacts
- What evidence would resolve it: Analysis of attacked images for consistent post-attack patterns; development and validation of a secondary detector trained to identify manipulation artifacts specific to fingerprint attacks

### Open Question 2
- Question: How does the relationship between removal and forgery attack success scale with candidate pool size and model diversity?
- Basis in paper: Authors note "A more detailed analysis of how pool size and model diversity govern this relationship is left to closer examination" after observing that removal-forgery correlation depends on the number of classes
- Why unresolved: Experiments used a fixed 12-model pool; the geometric relationship between decision boundaries as pool complexity increases remains uncharacterized
- What evidence would resolve it: Systematic evaluation of removal and forgery ASR across varying pool sizes (e.g., 2, 5, 10, 25, 50 models) and controlled model diversity metrics

### Open Question 3
- Question: What are the underlying causes of high sample-level variance in attack vulnerability observed for some MFD methods?
- Basis in paper: Authors observe "certain samples are inherently more vulnerable to fingerprint removal. We leave a detailed investigation of the underlying causes for future work."
- Why unresolved: The paper reports variance (e.g., W3 against McCloskey18 showing up to 28.81pp difference) but does not analyze image properties correlated with vulnerability
- What evidence would resolve it: Correlation analysis between image features (texture, frequency content, semantic content) and per-sample ASR; identification of vulnerability predictors

### Open Question 4
- Question: Can hybrid provenance systems combining active watermarking and passive fingerprinting achieve both high utility and adversarial robustness?
- Basis in paper: From discussion section: "It is thus worthwhile exploring hybrid provenance systems that use watermarking as a high-confidence signal when present, and falls back to fingerprinting when it is not, while explicitly accounting for removal, forgery, and uncertainty under adversarial manipulation."
- Why unresolved: The paper demonstrates no single MFD method achieves both accuracy and robustness, and suggests hybrid approaches as a solution without implementing or evaluating any
- What evidence would resolve it: Design and evaluation of a hybrid system combining watermark detection with MFD fallback, benchmarked against the same threat models used in this paper

## Limitations

- The study focuses on 256×256 images from specific generative models, limiting generalizability to other resolutions and domains
- The transferability assumption for black-box attacks shows high variance across methods, suggesting surrogate quality critically impacts results but is not fully characterized
- The utility-robustness trade-off, while observed empirically, lacks theoretical grounding and may not hold across different MFD architectures

## Confidence

**High Confidence:**
- Gradient-based removal attacks achieve >80% success in white-box settings across diverse MFD methods
- Forgery attacks are more challenging than removal but still show significant success in white-box scenarios
- No single MFD method achieves both high attribution accuracy and robust security across all threat models

**Medium Confidence:**
- Black-box removal attacks achieve >50% success rates through surrogate transferability
- The observed inverse relationship between attribution accuracy and adversarial robustness
- The claim that removal attacks are easier than forgery attacks

**Low Confidence:**
- The precise transferability rate between surrogate and target classifiers is method-dependent and highly variable
- The theoretical basis for the utility-robustness trade-off across different feature domains
- Generalizability of results to non-256×256 images or different generative model families

## Next Checks

1. **Transferability Characterization Study**: Systematically vary surrogate model architecture, training data size, and feature representation to quantify the relationship between surrogate quality and black-box attack success rates. Test whether larger surrogates, data augmentation, or architectural alignment improve transferability.

2. **Cross-Domain Generalization Test**: Evaluate the same attack strategies on MFD methods applied to different image domains (e.g., landscapes, medical imaging) and resolutions (e.g., 512×512, 1024×1024) to validate whether the observed utility-robustness trade-off holds across feature types and scales.

3. **Adaptive Defense Evaluation**: Implement defensive strategies that detect or mitigate adversarial fingerprints (e.g., input preprocessing, adversarial training of MFD classifiers) and re-evaluate attack success rates to determine if current vulnerabilities can be addressed through practical defenses.