---
ver: rpa2
title: Text-Trained LLMs Can Zero-Shot Extrapolate PDE Dynamics, Revealing a Three-Stage
  In-Context Learning Mechanism
arxiv_id: '2509.06322'
source_url: https://arxiv.org/abs/2509.06322
tags:
- prediction
- spatial
- llama-3
- llms
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models pretrained on text can accurately extrapolate
  spatiotemporal dynamics from discretized partial differential equation solutions
  without fine-tuning or natural language prompting. Predictive accuracy improves
  with longer temporal context but degrades at finer spatial discretization.
---

# Text-Trained LLMs Can Zero-Shot Extrapolate PDE Dynamics, Revealing a Three-Stage In-Context Learning Mechanism

## Quick Facts
- arXiv ID: 2509.06322
- Source URL: https://arxiv.org/abs/2509.06322
- Reference count: 40
- Primary result: Text-only pretrained LLMs extrapolate PDE spatiotemporal dynamics zero-shot with accuracy improving via three-stage ICL progression.

## Executive Summary
This paper demonstrates that large language models pretrained solely on text can accurately extrapolate discretized partial differential equation solutions without fine-tuning or natural-language prompting. The models progress through a three-stage in-context learning mechanism: initial syntax imitation, exploratory high-entropy behavior, and finally confident, numerically grounded predictions. The work reveals novel connections between LLM scaling behavior and classical numerical analysis, showing that prediction quality follows predictable laws analogous to truncation error in finite-difference methods.

## Method Summary
The approach quantizes PDE solutions to 3-digit integers (150-850), serializes them with comma-separated spatial values and semicolon-delimited time slices, and feeds them to pretrained LLMs for autoregressive prediction. The models generate future time slices either one-step or through multi-step rollouts, with error measured against high-resolution reference solutions. No training or prompting is used—the LLMs learn solely from in-context examples. The method tests multiple PDEs (Allen-Cahn, Fisher-KPP, heat, wave) across different discretization resolutions and model sizes.

## Key Results
- LLM accuracy improves with longer temporal context following O(1/NT) scaling, matching first-order-in-time solvers.
- Finer spatial discretization degrades accuracy approximately as O(NX) due to longer autoregressive output sequences.
- Multi-step rollouts show algebraic (not exponential) error growth, resembling stable numerical solvers.
- Entropy analysis reveals three distinct ICL stages: syntax-only, exploratory, and consolidation.
- LLMs maintain conservation laws (e.g., energy in heat equation) closer to grid-induced references than coarse finite-difference solvers.

## Why This Works (Mechanism)

### Mechanism 1: Three-Stage In-Context Learning Progression
Text-trained LLMs progress through consistent three-stage ICL mechanism when extrapolating PDE dynamics, transitioning from surface-level pattern imitation to numerically grounded predictions. Entropy analysis of token-level softmax distributions reveals: (1) syntax-only stage—separator tokens predicted with near-perfect confidence while spatial value tokens act as generic placeholders; (2) exploratory stage—mean spatial entropy peaks, distributions broaden, and prediction accuracy improves rapidly as model explores plausible continuations; (3) consolidation stage—entropy decreases, distributions sharpen around target values, reflecting confident predictions aligned with PDE dynamics.

### Mechanism 2: In-Context Neural Scaling Laws Analogous to Numerical Truncation Error
Prediction quality varies predictably with context length and output length, following scaling laws reminiscent of classical numerical analysis. Temporal context scaling shows RMSE decreases approximately as O(1/NT) with more observed time steps, matching first-order-in-time solver convergence. Spatial discretization scaling reveals RMSE grows approximately as O(NX) with finer spatial grids due to longer autoregressive output sequences demanding greater ICL capacity. Rollout horizon scaling demonstrates multi-step errors grow algebraically (not exponentially) with prediction steps, resembling stable global error accumulation in finite-difference methods.

### Mechanism 3: Conservation Law Capture Through Joint Spatiotemporal Inference
LLMs internalize structural invariants of PDE dynamics—such as energy conservation—rather than performing naive univariate extrapolation. When rolling out heat equation under Neumann boundaries, LLMs maintain total thermal energy closer to conserved value than coarse-grid finite-difference solvers operating at identical resolution. This requires simultaneously inferring spatial structure (boundary behavior) and temporal evolution (diffusion dynamics) from serialized context.

## Foundational Learning

- **Discretization of PDEs (finite-difference methods)**: Understanding truncation error (local) vs. global error (accumulated) is essential to interpret LLM error scaling as "numerical-like." Quick check: Given a first-order-in-time scheme, would you expect global error to grow linearly or quadratically with number of time steps?
- **Autoregressive sequence modeling**: LLMs generate predictions token-by-token conditioned on all prior context. Error compounds across outputs, and model cannot "look ahead" to correct early mistakes—critical for understanding why longer outputs degrade accuracy. Quick check: If LLM makes error at token position 5 in 100-token sequence, can it use information from tokens 50-100 to correct that error during generation?
- **Shannon entropy as uncertainty quantification**: Paper's three-stage mechanism identified entirely through entropy analysis of softmax distributions. High entropy = exploratory/uncertain; low entropy = confident/converged. Quick check: If model's softmax distribution over 700 tokens has entropy near log(700) ≈ 9.5 nats, what does this imply about prediction confidence?

## Architecture Onboarding

- **Component map**: Quantization layer -> Serialization format -> Tokenizer requirement -> Inference modes -> Evaluation pipeline
- **Critical path**: 1) Generate random initial condition via cubic spline interpolation on coarse grid. 2) Solve PDE with high-resolution reference solver. 3) Downsample to target discretization, quantize, serialize. 4) Feed serialized string to LLM without natural-language prompting. 5) Record generated tokens AND full softmax distributions at each position. 6) Parse, reconstruct, compute error metrics and entropy.
- **Design tradeoffs**: Quantization precision vs. token efficiency (3-digit integers limit quantization error floor but are token-efficient); context length vs. task difficulty (longer temporal context improves accuracy but may hit model context window limits); model size vs. ICL capacity (larger models handle finer spatial discretizations better).
- **Failure signatures**: Syntax-only failure (high error, low entropy on separators but high entropy or deterministic-wrong values on spatial tokens); capacity overflow (1B model errors exceed temporal-repeat baseline at fine spatial discretization); collapse to trivial behavior (constant-value predictions or repeated patterns); malformed outputs (missing delimiters or non-numeric tokens).
- **First 3 experiments**: 1) Reproduce one-step scaling on Allen-Cahn (fix NX=14, vary NT 2-40, plot RMSE vs NT on log-log scale, verify O(1/NT) decay and identify three stages via entropy analysis, compare against FTCS benchmark). 2) Spatial discretization stress test (fix NT=50, vary NX 2-40, confirm O(NX) error growth, compare Llama-3.1-8B vs Llama-3.2-1B to quantify capacity effects). 3) Multi-step rollout with conservation check (run 10-step rollout on heat equation with Neumann boundaries, compute relative energy deviation ΔE at each step, verify LLM tracks grid-induced reference more closely than coarse FTCS/BTCS).

## Open Questions the Paper Calls Out

- **Internal representations**: What specific internal representations and compositional structures within the LLM support generalization of spatiotemporal dynamics in autoregressive token space? The paper characterizes behavior but doesn't perform mechanistic interpretability to identify where/how these dynamics are encoded in transformer layers or attention heads.

- **Higher spatial dimensions**: How does in-context learning mechanism adapt when applied to time-dependent PDEs in higher spatial dimensions? Current methodology restricted to 1D spatial domains; 2D/3D domains would require different serialization strategies and involve significantly more complex spatial correlations.

- **Stationary and complex systems**: Does zero-shot extrapolation capability hold for stationary PDEs, complex-valued systems, and partially observed or noisy dynamics? Experiments focus exclusively on time-dependent, real-valued PDEs with full observation and low noise; unknown if "syntax-then-exploration" mechanism works without temporal evolution or with complex numbers.

- **Physics-aware prompting**: How does inclusion of physics-aware natural language or symbolic descriptions alter inductive biases and predictive accuracy of ICL process? Authors deliberately omitted natural language prompting to isolate intrinsic zero-shot capabilities, leaving interaction between textual physics knowledge and numerical token prediction unexplored.

## Limitations

- **Tokenizer dependency**: The one-to-one mapping assumption between 3-digit integers/delimiters and tokens depends on specific tokenizer implementations, creating uncertainty about reproducibility across different model families.
- **Limited PDE diversity**: All tested PDEs share diffusion-like characteristics or low-order spatial derivatives; mechanism claims may not generalize to hyperbolic systems with shocks or dispersive waves.
- **Single discretization scheme**: Experiments use uniform finite-difference grids with explicit time stepping; apparent understanding of numerical truncation error may be specific to this discretization rather than continuous PDE structure.

## Confidence

- **High confidence** in basic empirical observation that text-trained LLMs can zero-shot extrapolate PDE dynamics with accuracy improving via temporal context. Scaling laws are clearly demonstrated and reproducible.
- **Medium confidence** in three-stage mechanism interpretation. While entropy analysis shows described patterns, causal link between entropy trajectories and genuine internal representation learning versus surface-level sequence modeling is not definitively established.
- **Medium confidence** in conservation law capture claim. Results are promising but only tested for Neumann heat equation; mechanism by which LLMs implicitly learn structural invariants from serialized context remains speculative.

## Next Checks

1. **Tokenizer sensitivity experiment**: Systematically test same task across multiple LLM families (GPT, Llama, Gemma, Phi-4, SmolLM3) with detailed characterization of how tokenization affects one-to-one mapping success rate, accuracy degradation patterns, and entropy trajectory differences.

2. **Hyperbolic PDE stress test**: Extend experiments to nonlinear hyperbolic systems (Burgers' equation with shock formation, shallow water equations) where numerical stability constraints are more stringent, solutions exhibit discontinuities requiring different discretization approaches, and conservation laws involve momentum and energy.

3. **Serialization format ablation**: Repeat three-stage entropy analysis with alternative serialization schemes: space-separated values without delimiters, JSON with explicit time/space metadata, raw token sequences where spatial/temporal structure is implicit in position. Compare whether entropy trajectories and accuracy scaling remain consistent.