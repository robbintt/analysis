---
ver: rpa2
title: 'Executable Counterfactuals: Improving LLMs'' Causal Reasoning Through Code'
arxiv_id: '2510.01539'
source_url: https://arxiv.org/abs/2510.01539
tags:
- reasoning
- counterfactual
- code
- causal
- counterfactuals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study introduces executable counterfactuals, a novel framework
  that operationalizes causal reasoning through code and math problems, explicitly
  requiring all three steps: abduction, intervention, and prediction. This approach
  addresses the overestimation of LLM performance in prior work that conflated counterfactual
  reasoning with simpler interventional reasoning by skipping the abduction step.'
---

# Executable Counterfactuals: Improving LLMs' Causal Reasoning Through Code

## Quick Facts
- arXiv ID: 2510.01539
- Source URL: https://arxiv.org/abs/2510.01539
- Reference count: 40
- Key outcome: RL outperforms SFT for generalizable counterfactual reasoning, yielding 1.5-2x accuracy gains on out-of-distribution code and math problems

## Executive Summary
This paper introduces executable counterfactuals, a novel framework that operationalizes causal reasoning through code and math problems, explicitly requiring abduction, intervention, and prediction. The framework addresses the overestimation of LLM performance in prior work by distinguishing true counterfactual reasoning from simpler interventional reasoning. Experiments reveal a significant performance drop (25-40%) from interventional to counterfactual reasoning across state-of-the-art models. The authors demonstrate that while supervised fine-tuning (SFT) improves in-domain performance, it fails to generalize to out-of-distribution structures. In contrast, reinforcement learning (RL) consistently induces core cognitive behaviors and generalizes to new domains, yielding substantial accuracy gains (1.5-2x) on both code and math problems.

## Method Summary
The method generates Python functions with latent variables using template-based generation, creating many-to-one mappings through modulo operations at return statements. Counterfactual prompts hide latent variable values, requiring models to first abduce valid values, then intervene, then predict outcomes. The framework provides exact ground truth through program execution, enabling verifiable evaluation. Training uses both SFT on reasoning traces from stronger models and RLVR with exact-match rewards. The critical distinction is that true counterfactual reasoning requires the full abduction-intervention-prediction sequence, while interventional tasks reveal latent variables and skip abduction.

## Key Results
- LLMs show 25-40% performance drop from interventional to counterfactual reasoning on if-else templates
- SFT on reasoning traces improves in-distribution accuracy but hurts OOD generalization to while-loops and multi-variable structures
- RLVR consistently generalizes counterfactual reasoning strategies across code structures and GSM-math problems
- RLVR models show better planning scores (abduction skills) but still struggle with computational accuracy on complex problems

## Why This Works (Mechanism)

### Mechanism 1: Latent Variable Abduction Distinguishes Counterfactual from Interventional Reasoning
- Claim: True counterfactual reasoning requires inferring unobserved latent variables from observations before predicting alternative outcomes; without this abduction step, tasks collapse to simpler interventional reasoning.
- Mechanism: The framework introduces hidden random variables whose values are not revealed. Given an observed output, the model must first abduce valid values, then hold them fixed while intervening, then predict the counterfactual outcome. This enforces the full abduction-intervention-prediction sequence.
- Core assumption: Abduction is a separable cognitive skill that can be isolated and measured; failure at this step propagates to overall counterfactual failure.
- Evidence anchors:
  - [abstract] "existing efforts in assessing LLM's counterfactual reasoning capabilities tend to skip the abduction step, effectively reducing to interventional reasoning"
  - [section 2] "Without latent states and the abduction step, counterfactual reasoning reduces to interventional reasoning, corresponding to Level 2 in Pearl's causal ladder"

### Mechanism 2: SFT Memorizes Surface Patterns While RL Discovers Generalizable Strategies
- Claim: Supervised fine-tuning on reasoning traces from stronger models leads to pattern memorization that transfers poorly to out-of-distribution structures; reinforcement learning with verifiable rewards elicits more robust cognitive behaviors.
- Mechanism: SFT models learn to match the distribution of teacher reasoning traces, which embeds domain-specific heuristics. RLVR, using only outcome-based rewards (exact match), allows the model to discover problem-solving strategies that generalize across code structures and domains.
- Core assumption: Verifiable rewards provide sufficient signal for discovering transferable reasoning patterns; the model has sufficient capacity to discover these patterns through exploration.
- Evidence anchors:
  - [abstract] "While supervised finetuning (SFT) on stronger models' reasoning traces improves in-distribution performance of Qwen models, it leads to a decrease in accuracy on out-of-distribution tasks"
  - [section 5] "SFT memorizes shallow abduction patterns that fail to generalize to complex problems"

### Mechanism 3: Code Provides Executable Ground Truth for Counterfactual Verification
- Claim: Code-based counterfactuals enable scalable, verifiable evaluation because programs are computational graphs with deterministic execution, allowing precise ground-truth answers even when multiple latent configurations produce the same observation.
- Mechanism: The template-based generation creates functions with structural placeholders and value placeholders. A modulo at return induces many-to-one mappings from latent values to observed output, yielding multiple valid counterfactual answers. Exact evaluation uses set equality and F1 against the full valid answer set.
- Core assumption: The mapping from code reasoning to natural-language counterfactual reasoning is transferable; skills learned in the executable domain generalize.
- Evidence anchors:
  - [section 3.1] "our code-based framework avoids the potential ambiguity of natural language, and allows rich and controllable complexity"

## Foundational Learning

- Concept: Pearl's Causal Ladder (Associational → Interventional → Counterfactual)
  - Why needed here: The paper's core distinction between interventional (Level 2) and counterfactual (Level 3) reasoning depends on understanding this hierarchy; the abduction step is what elevates a task to Level 3.
  - Quick check question: Given "The sprinkler is on and the grass is wet," is "Would the grass be dry if the sprinkler were off?" an associational, interventional, or counterfactual query?

- Concept: Abduction as Inverse Inference
  - Why needed here: The paper identifies abduction failure as the primary weakness in LLM counterfactual reasoning; understanding that abduction requires inferring latent causes from observed effects is essential for interpreting results.
  - Quick check question: If you observe `y=5` from function `f(r, x=2) = r*x + 1` with unknown `r`, what value(s) of `r` could produce this observation?

- Concept: Verifiable Rewards in RL (RLVR)
  - Why needed here: The paper's RL experiments use exact match as the sole reward signal; understanding how sparse outcome-based rewards can induce complex reasoning behaviors is critical for reproducing results.
  - Quick check question: Why might an exact-match reward be sufficient for learning counterfactual reasoning, while still failing to teach computational accuracy?

## Architecture Onboarding

- Component map: Template Generator -> Counterfactual Prompt Constructor -> Interventional Counterpart Generator -> Evaluation Pipeline -> Training Pipeline
- Critical path:
  1. Generate diverse functions from templates (ensure OOD structures like while-loops, multiple latent variables held out)
  2. Construct counterfactual prompts with observed input/output pairs
  3. For training, use only if-else templates; for evaluation, test on while-loop, multi_r, if-else-long, and GSM-math OOD splits
  4. Train with RLVR (GRPO, exact-match reward) and compare against SFT baselines

- Design tradeoffs:
  - Template diversity vs. evaluation controllability: More templates increase coverage but complicate difficulty calibration
  - Many-to-one mappings (modulo at return) increase ambiguity, better reflecting real-world scenarios but complicating evaluation
  - RL compute cost: GRPO requires rollouts; SFT is cheaper but less generalizable

- Failure signatures:
  - Brute-force enumeration of all possible values (planning score = 1)
  - Arbitrary assumption of a single value when problem appears complex (planning score = 1)
  - Correct strategy but computational errors in forward prediction (high planning, low execution scores in RL models on while/multi_r)

- First 3 experiments:
  1. Replicate the interventional vs. counterfactual accuracy gap on if-else templates using a base Qwen model; verify ~25-40% drop
  2. Train Qwen-7B-Instruct with SFT on if-else traces and evaluate OOD generalization to while-loop and GSM-math; confirm performance degradation
  3. Train the same base model with RLVR (GRPO, exact-match reward) on if-else only; evaluate on all OOD splits and confirm 1.5-2x gains over base model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can computational accuracy be improved alongside counterfactual reasoning strategies to unlock further gains, or do these skills compete for model capacity during RLVR training?
- Basis in paper: [explicit] Section 5 states that "RLVR generalizes counterfactual reasoning strategies, but is still bottlenecked by computational accuracy" and "calls for future efforts into improving both skills simultaneously to build a strong counterfactual reasoning agent."
- Why unresolved: The paper identifies the asynchronism between learning reasoning strategies and computational skills but does not propose or test interventions to address it.
- What evidence would resolve it: Ablation studies combining RLVR with auxiliary computation-focused rewards, or multi-task training regimes that explicitly target both skills.

### Open Question 2
- Question: Does the code-to-math transfer generalize further to high-stakes natural-language domains such as healthcare or scientific reasoning where counterfactual queries are common?
- Basis in paper: [inferred] The abstract motivates the work with "high-stakes domains such as scientific research and healthcare," but experiments only test transfer from code to GSM-style math problems, not to these target domains.
- Why unresolved: The paper demonstrates domain transfer within formal/structured settings (code→math) but does not test whether the learned abduction-intervention-prediction skills apply to less structured, higher-stakes reasoning.
- What evidence would resolve it: Evaluation on domain-specific counterfactual benchmarks (e.g., medical counterfactual QA or scientific hypothesis revision) using models trained only on executable counterfactuals.

### Open Question 3
- Question: Why does scaling model size improve execution but not planning, and can this dissociation be mitigated through targeted training objectives?
- Basis in paper: [explicit] Section 5 finds that "scaling up the size of Qwen2.5-Instruct models leads to consistent improvements in execution ratings, but not in planning," with smaller models sometimes outperforming larger ones on abduction skills.
- Why unresolved: The paper documents this phenomenon but does not explain whether it stems from pre-training data, architecture, or post-training procedures.
- What evidence would resolve it: Controlled experiments varying pre-training data composition or adding explicit abduction-focused auxiliary losses during post-training.

## Limitations
- The study relies on synthetic code-based counterfactuals, and the transferability of these skills to natural language domains remains unproven
- Exact-match rewards in RLVR may not capture all aspects of counterfactual reasoning quality, potentially missing nuanced reasoning patterns
- The framework's complexity increases evaluation difficulty, particularly when multiple valid counterfactual answers exist due to many-to-one mappings

## Confidence
- **High Confidence:** The interventional vs. counterfactual performance gap (25-40%) is robust across multiple models and well-supported by the framework's design
- **Medium Confidence:** The superiority of RL over SFT for generalization is supported by experiments but depends on specific RLVR implementation details not fully disclosed
- **Medium Confidence:** The abductive reasoning mechanism is theoretically sound but the paper provides limited behavioral analysis of how models actually perform abduction

## Next Checks
1. Test RL-trained models on a diverse set of natural language counterfactual problems to verify that code-based reasoning transfers beyond the synthetic domain
2. Compare RLVR performance against alternative reward functions (e.g., step-by-step correctness) to isolate the effect of exact-match rewards on generalization
3. Conduct detailed behavioral analysis of RL-trained models on complex counterfactual problems to identify whether they truly discover abductive strategies or rely on alternative heuristics