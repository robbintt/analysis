---
ver: rpa2
title: Flexible Realignment of Language Models
arxiv_id: '2506.12704'
source_url: https://arxiv.org/abs/2506.12704
tags:
- realignment
- layer
- integer
- which
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a flexible realignment framework for language
  models, addressing the need to correct model behavior when it fails to meet expected
  performance. The core idea involves Training-time Realignment (TrRa), which uses
  a controllable fusion of logits from reference and aligned models for efficient
  post-training realignment, and Inference-time Realignment (InRa), which introduces
  a lightweight layer adapter enabling smooth adjustment of alignment strength even
  during inference.
---

# Flexible Realignment of Language Models

## Quick Facts
- arXiv ID: 2506.12704
- Source URL: https://arxiv.org/abs/2506.12704
- Authors: Wenhong Zhu; Ruobing Xie; Weinan Zhang; Rui Wang
- Reference count: 40
- Primary result: Achieves 54.63% token reduction with Training-time Realignment while maintaining performance

## Executive Summary
This paper introduces a flexible realignment framework for language models that addresses the challenge of correcting model behavior without full retraining. The framework comprises Training-time Realignment (TrRa), which uses controllable logit fusion between reference and aligned models for efficient post-training adjustments, and Inference-time Realignment (InRa), which employs a lightweight layer adapter enabling dynamic adjustment of alignment strength during inference. The approach demonstrates significant efficiency gains and offers practical methods for building user-controllable and adaptable language models across various alignment tasks including reasoning, safety, and instruction following.

## Method Summary
The framework introduces two complementary approaches: Training-time Realignment (TrRa) fuses logits from reference and aligned models using a controllable parameter λ to create intermediate teacher signals for efficient realignment without full reinforcement learning. Inference-time Realignment (InRa) inserts a duplicate bottom layer adapter initialized as an identity mapping, allowing the model to learn new alignment behaviors while preserving original capabilities. During inference, logits from both paths are interpolated using λ, enabling smooth adjustment of alignment strength. The framework supports both positive and negative λ values, with negative extrapolation surprisingly improving reasoning performance by encouraging deeper thought processes.

## Key Results
- TrRa reduces token usage by 54.63% on DeepSeek-R1-Distill-Qwen-1.5B without performance loss
- InRa successfully integrates fast and slow thinking modes, achieving up to 4% improvement on reasoning benchmarks
- Layer selection analysis confirms bottom-layer tuning yields substantial improvements over top-layer adjustments
- Negative λ extrapolation (λ < 0) can enhance reasoning accuracy by encouraging deeper thinking processes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Controllable logit fusion between reference and aligned models creates efficient realignment without full retraining.
- **Mechanism:** TrRa constructs a teacher distribution by linearly interpolating logits from reference and aligned models using weight λ, training the reference model against this moving target via KL divergence to operate at specific alignment spectrum points.
- **Core assumption:** Reward signals can be effectively linearly interpolated in logit space to represent intermediate alignment states without original reward model access.
- **Evidence anchors:** 54.63% token reduction [abstract], innovative approach with Equation 5 serving as teacher distribution [section 3.1], no direct corpus precedents for logit-level fusion during training.
- **Break condition:** Significant divergence between reference and aligned models (different tokenizers/architectures) may produce nonsensical probability distributions, causing training instability.

### Mechanism 2
- **Claim:** Identity-initialized bottom layer adapter enables isolated learning of new alignment behaviors while preserving original capabilities.
- **Mechanism:** InRa duplicates the first layer, zeroing projection weights for identity initialization, fine-tuning only the adapter on target behavior, and merging logits during inference via λ interpolation.
- **Core assumption:** Lower transformer layers are disproportionately influential for learning alignment preferences compared to upper layers.
- **Evidence anchors:** Identity transformation initialization [abstract], bottom layers bring substantial improvements [section 5.1], AlignFreeze supports layer selection importance though InRa specifically targets bottom via insertion.
- **Break condition:** Non-identity initialization corrupts the reference capability, making smooth interpolation impossible and degrading performance.

### Mechanism 3
- **Claim:** Negative λ extrapolation can improve reasoning performance beyond original "slow-thinking" model by amplifying contrast against "fast-thinking" adapter.
- **Mechanism:** Training adapter on fast reasoning and setting λ < 0 during inference pushes model away from fast-thinking distribution, effectively "deepening" reasoning and increasing accuracy on complex benchmarks.
- **Core assumption:** Alignment direction is linear and monotonic in logit space, such that moving opposite to "fast" direction yields valid "slower/deeper" reasoning state.
- **Evidence anchors:** Surpassing original performance [abstract], λ < 0 encourages deeper thinking with 4% improvements [section 4.2], no corpus evidence explains negative extrapolation.
- **Break condition:** High negative λ magnitude may detach distribution from semantic meaning, leading to incoherent or hallucinated outputs.

## Foundational Learning

- **Concept: Residual Connections & Identity Mapping**
  - **Why needed here:** Understanding why zeroing adapter weights allows it to act as pass-through initially, ensuring λ-interpolation at λ=0 yields exact original model.
  - **Quick check question:** If y = x + f(x), what must f(x) output for y to equal x? (Answer: 0). How does this relate to adapter initialization?

- **Concept: Knowledge Distillation (KL Divergence)**
  - **Why needed here:** TrRa relies on training model to match "teacher" distribution; understanding KL divergence is necessary to grasp how model minimizes difference between its output and fused logits.
  - **Quick check question:** In Equation 6 (L=D_KL(...)), which distribution is the "teacher" and which is the "student"?

- **Concept: Logits vs. Probabilities**
  - **Why needed here:** Fusion happens in logit space (pre-softmax), not probability space, preserving unnormalized distribution shape and allowing negative weights (λ < 0) impossible in probability space.
  - **Quick check question:** Why is interpolating in logit space (λh₁ + (1-λ)h₂) mathematically distinct from interpolating probabilities (λp₁ + (1-λ)p₂)?

## Architecture Onboarding

- **Component map:** Input -> Fork (Path A: Layer Adapter, Path B: Original Bottom Layer) -> Both paths propagate through remaining L-1 layers -> Output (logits interpolated: h_final = λ·h_adapter + (1-λ)·h_orig)

- **Critical path:** Identity Initialization (Sec 3.2) is most fragile step; must verify W_out and W_down are strictly zero before training, otherwise original model is immediately corrupted.

- **Design tradeoffs:**
  - InRa vs. DeRa: InRa packs both paths into one model (better for deployment) but requires doubling KV-cache for first layer branching; DeRa requires loading two distinct models.
  - Speed vs. Accuracy: Increasing λ (fast thinking) reduces tokens/cost; decreasing λ (slow thinking) increases tokens/cost but may boost accuracy.

- **Failure signatures:**
  - "!!!!!" Repetition: Merging hidden states before final layer causes immediate repetitive failure (Section 3.2).
  - Loss of Instruction Following: High λ in dialogue settings causes "Alignment Tax" (Section 4.3), prioritizing style/safety over complex multi-turn instructions.

- **First 3 experiments:**
  1. Verify Layer Significance: Replicate Table 3, fine-tune only top-k vs. bottom-k layers on preference dataset, confirm bottom-layer tuning yields higher AlpacaEval scores.
  2. Identity vs. Random Init: Replicate Table 4, train adapter with random weights vs. zero weights, observe random init degrades performance significantly.
  3. Reasoning Extrapolation: Replicate Figure 4, take reasoning model, apply InRa with short-CoT data, plot accuracy vs. token count for λ ∈ [-0.5, 1.5], check if λ < 0 improves accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- Negative λ success is presented without theoretical justification, representing a significant gap in understanding the proposed geometry
- Results primarily demonstrated on small models (1.5B parameters); effectiveness on larger models with different architectures remains untested
- Computational overhead doubles first layer computation, which could be prohibitive for very long sequences or real-time applications

## Confidence

**High Confidence:** Core mechanism of identity-initialized layer adapters (InRa) and controllable logit fusion (TrRa) is well-supported by experimental evidence, including 54.63% token reduction and layer selection findings.

**Medium Confidence:** Extrapolation claim (λ < 0 improving reasoning) is supported by experimental results but lacks theoretical grounding; mechanism is demonstrated but not explained.

**Medium Confidence:** General framework applicability across different alignment tasks is supported by multiple experiments, though each specific domain could have unique failure modes not fully explored.

## Next Checks

1. **Negative Lambda Boundary Analysis:** Systematically test limits of negative λ extrapolation by evaluating model outputs at λ = -0.5, -1.0, -1.5, -2.0 on multiple reasoning benchmarks, measuring not just accuracy but also coherence, logical consistency, and hallucination rates.

2. **Layer-by-Layer Ablation Study:** Extend layer selection analysis with fine-grained ablation where each individual layer (1 through L) is independently tunable, revealing whether there's a specific critical layer for alignment or whether bottom layers work synergistically.

3. **Cross-Domain Transferability Test:** Apply trained InRa adapters from one domain (e.g., reasoning) to a different domain (e.g., safety alignment or instruction following) without additional fine-tuning, measuring performance degradation to determine domain-specific versus general alignment capabilities.