---
ver: rpa2
title: Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating
  Industrial Hazard Scenarios
arxiv_id: '2511.13970'
source_url: https://arxiv.org/abs/2511.13970
tags:
- hazard
- scene
- score
- image
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating and evaluating
  synthetic workplace hazard images for training computer vision models in industrial
  safety. The authors propose a novel scene graph-guided generative AI framework that
  uses GPT-4o to analyze OSHA accident reports, extracting structured hazard rationales
  which are clustered and converted into scene graphs.
---

# Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating Industrial Hazard Scenarios

## Quick Facts
- arXiv ID: 2511.13970
- Source URL: https://arxiv.org/abs/2511.13970
- Reference count: 40
- Authors: Sanjay Acharjee; Abir Khan Ratul; Diego Patino; Md Nazmus Sakib
- Primary result: Novel framework generates compositionally accurate industrial hazard scenes with VQA-based evaluation

## Executive Summary
This paper introduces a novel generative AI framework for synthesizing and evaluating industrial hazard scenarios to support computer vision training for workplace safety applications. The approach combines GPT-4o for hazard rationale extraction from OSHA reports, scene graph generation, and text-to-image diffusion models to create compositionally accurate hazard depictions. A novel visual question answering (VQA) framework is developed to evaluate generated images through a graph-aligned VQA Graph Score, demonstrating superior discriminative sensitivity compared to CLIP and BLIP metrics while providing higher informational content through entropy analysis.

## Method Summary
The framework operates through a multi-stage pipeline: first, GPT-4o analyzes OSHA accident reports to extract structured hazard rationales, which are then clustered and converted into scene graphs representing hazard compositions. These scene graphs guide text-to-image diffusion models to generate synthetic hazard scenarios with compositional accuracy. To evaluate the generated images, the authors introduce a visual question answering framework that creates graph-aligned VQA Graph Scores, measuring both semantic fidelity and compositional accuracy. The approach is validated across four state-of-the-art generative models, showing that the VQA Graph Score outperforms traditional metrics in discriminative sensitivity while providing richer informational content.

## Key Results
- VQA Graph Score demonstrates superior discriminative sensitivity compared to CLIP and BLIP metrics
- Entropy analysis confirms VQA Graph Score provides higher informational content in evaluations
- Framework successfully generates compositionally accurate industrial hazard scenes from OSHA reports

## Why This Works (Mechanism)
The framework leverages the structured reasoning capabilities of GPT-4o to extract meaningful hazard rationales from unstructured accident reports, converting complex safety violations into formal scene graph representations. These graphs provide precise compositional constraints that guide text-to-image diffusion models, ensuring generated scenes accurately reflect real-world hazard configurations. The VQA-based evaluation framework creates an iterative feedback loop where semantic questions are aligned with scene graph structures, enabling nuanced assessment of both visual realism and compositional accuracy. This multi-modal approach bridges the gap between textual safety documentation and visual hazard representation, creating a scalable pipeline for generating training data that captures the complexity of industrial safety scenarios.

## Foundational Learning
- **Scene graphs**: Structured representations encoding objects, relationships, and attributes in visual scenes; needed to provide compositional constraints for image generation and ensure semantic accuracy
- **Visual Question Answering (VQA)**: AI systems that answer natural language questions about images; needed to create interpretable evaluation metrics that assess both visual content and semantic understanding
- **Text-to-image diffusion models**: Generative models that create images from textual descriptions through iterative denoising; needed to translate scene graph constraints into photorealistic hazard scenarios
- **Entropy analysis in evaluation**: Statistical measure of information content; needed to demonstrate that VQA Graph Score provides richer evaluation signals compared to traditional metrics
- **CLIP and BLIP metrics**: Contrastive learning-based image-text similarity measures; needed as baseline comparisons to establish the superiority of the proposed VQA Graph Score
- **OSHA accident report analysis**: Systematic extraction of safety violation information from regulatory documentation; needed to source authentic hazard scenarios for synthetic data generation

## Architecture Onboarding

Component Map:
GPT-4o Hazard Extractor -> Hazard Clustering -> Scene Graph Converter -> Text-to-Image Diffusion Model -> VQA Evaluation Framework

Critical Path:
The critical path flows from OSHA report analysis through GPT-4o extraction, clustering for generalization, scene graph conversion for compositional control, diffusion model generation for visual synthesis, and VQA evaluation for quality assessment. Each stage builds upon the previous one, with scene graph generation serving as the pivotal transformation that enables controlled image synthesis.

Design Tradeoffs:
The framework trades computational complexity for compositional accuracy, using multiple AI models in sequence rather than a single end-to-end solution. GPT-4o extraction provides rich semantic understanding but introduces potential biases from language model interpretations. Scene graph conversion simplifies complex scenarios but may lose nuanced safety details. The VQA evaluation provides interpretable metrics but requires careful question design to align with scene graph structures.

Failure Signatures:
- Poor hazard rationale extraction leading to irrelevant scene content
- Clustering errors causing loss of critical safety context
- Scene graph conversion failures resulting in impossible object configurations
- Diffusion model artifacts producing unrealistic hazard depictions
- VQA misalignment causing evaluation of incorrect semantic aspects

First Experiments:
1. Test GPT-4o extraction accuracy on a subset of manually annotated OSHA reports
2. Validate scene graph conversion by comparing generated graphs against expert-annotated safety scenarios
3. Conduct ablation study removing VQA evaluation to assess impact on generated image quality

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation methodology relies on VQA systems that may introduce their own biases and errors
- Dependence on GPT-4o for hazard rationale extraction creates a single point of failure in the pipeline
- Clustering and scene graph conversion may oversimplify complex hazard scenarios, missing critical safety nuances
- Results may not generalize across different generative model architectures or safety domains

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Technical feasibility of scene graph-guided generation approach | High |
| Superiority of VQA Graph Score metric | Medium |
| Practical effectiveness for actual safety training applications | Low |

## Next Checks
1. Conduct human evaluation study with safety experts to validate realism and educational value of generated hazard scenarios compared to real OSHA incident images
2. Test framework performance on accident reports from multiple safety databases beyond OSHA to assess generalizability across different reporting standards and languages
3. Perform bias analysis on generated datasets to identify and mitigate potential demographic or workplace type imbalances in synthetic hazard representations