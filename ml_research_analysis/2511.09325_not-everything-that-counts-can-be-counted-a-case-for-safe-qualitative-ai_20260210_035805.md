---
ver: rpa2
title: 'Not Everything That Counts Can Be Counted: A Case for Safe Qualitative AI'
arxiv_id: '2511.09325'
source_url: https://arxiv.org/abs/2511.09325
tags:
- qualitative
- research
- https
- systems
- tools
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Qualitative research is essential for understanding meaning and
  context, yet current AI tools fail to support its interpretive nature, relying on
  biased, opaque, and privacy-compromising general-purpose models like ChatGPT. While
  AI has advanced quantitative research, qualitative methods remain underserved, with
  tools unable to handle nuance, reflexivity, and contextual analysis.
---

# Not Everything That Counts Can Be Counted: A Case for Safe Qualitative AI

## Quick Facts
- arXiv ID: 2511.09325
- Source URL: https://arxiv.org/abs/2511.09325
- Reference count: 40
- Primary result: Current AI tools fail qualitative research by prioritizing fluency over context, requiring dedicated systems built for transparency, reproducibility, and privacy

## Executive Summary
Qualitative research depends on understanding meaning and context, yet existing AI tools like ChatGPT are designed for quantitative, decontextualized analysis. These general-purpose models introduce severe limitations around reproducibility, transparency, and privacy that conflict with interpretive research methods. While AI has revolutionized quantitative research, qualitative researchers lack adequate tools to handle nuance, reflexivity, and contextual analysis. This paper argues for developing qualitative AI systems from the ground up with built-in explainability, reproducible workflows, and local processing to preserve the depth and complexity essential to qualitative inquiry.

## Method Summary
The paper synthesizes literature from AI safety, qualitative research methodology, and computational social science to identify gaps between current AI capabilities and qualitative research needs. It analyzes existing tools like NVivo and ATLAS.ti, examines failure modes of general-purpose LLMs, and proposes architectural principles for qualitative AI. The analysis draws on theoretical foundations from interpretive epistemology and practical constraints from researcher experiences.

## Key Results
- General-purpose LLMs fail qualitative research due to design assumptions that conflict with interpretive epistemology
- Current AI systems amplify dominant narratives while erasing marginalized perspectives through training data composition
- Safe qualitative AI requires three technical foundations: built-in explainability, reproducible workflows, and local-only processing

## Why This Works (Mechanism)

### Mechanism 1
General-purpose LLMs fail qualitative research because their design assumptions (scale, decontextualization, determinism) conflict with interpretive epistemology. Commercial LLMs optimize for fluency and generality over situated meaning. They strip context, cannot maintain positionality, and produce outputs that appear authoritative but lack grounding in lived experience. This creates an "epistemic mismatch"—tools flatten complexity that qualitative inquiry treats as essential data.

### Mechanism 2
Current AI systems structurally amplify dominant narratives while erasing marginalized perspectives through training data composition and model behavior. LLMs trained on internet-scale corpora inherit and amplify existing representation imbalances. Groups with less publication presence or oral/local knowledge traditions are underrepresented or mischaracterized. This produces "epistemic loss"—the gradual erasure of situated, subaltern, or contested knowledge.

### Mechanism 3
Safe qualitative AI requires three technical foundations absent in current systems: built-in explainability, reproducible workflows, and local-only processing. Transparency (reasoning trails), reproducibility (deterministic outputs with fixed parameters), and privacy (local infrastructure preventing data exfiltration) are jointly necessary. These enable audit, scientific rigor, and ethical compliance with regulations like GDPR.

## Foundational Learning

- **Constructivist vs. constructionist epistemology**: Qualitative research treats meaning as constructed through experience (constructivist) and shaped by social/cultural context (constructionist). This distinction clarifies why decontextualized pattern recognition fails the methodology. Quick check: Can you explain why a statistical summary of interview responses might violate constructionist principles even if factually accurate?

- **Positionality and reflexivity**: Qualitative research treats researcher perspective as methodologically relevant, not bias to eliminate. Systems that cannot represent or track positionality remove a core analytical dimension. Quick check: How would an AI system make its analytical "standpoint" visible to a researcher using it?

- **Thick description (Geertz)**: Meaning emerges from detailed, contextual accounts, not abstracted variables. Systems that flatten data into codes or themes risk losing what makes the data valuable. Quick check: What information would be lost if interview data were reduced to sentiment scores?

## Architecture Onboarding

- Component map: Context-preserving data ingestion -> Explainable reasoning module with uncertainty quantification -> Human-in-the-loop interface showing reasoning trails -> Local deployment infrastructure

- Critical path:
  1. Define qualitative epistemology requirements (context-sensitivity, non-reductive reasoning)
  2. Select or develop model architecture with built-in explainability
  3. Implement local-first infrastructure
  4. Design audit/provenance logging for every AI suggestion
  5. Build human override and revision workflows

- Design tradeoffs:
  - Local processing limits model scale → may reduce capability vs. cloud LLMs
  - Explainability constraints may restrict architecture choices (black-box deep models harder to interpret)
  - Non-reductive outputs increase cognitive load vs. clean summaries
  - Determinism requirement may conflict with creative/generative tasks

- Failure signatures:
  - System produces fluent outputs that researchers cannot trace to source data
  - Results change between runs with identical inputs
  - Contextual information (speaker identity, timing, setting) is dropped during processing
  - Contradictions in data are "resolved" rather than preserved and surfaced
  - Sensitive data transmitted to external servers

- First 3 experiments:
  1. Reproducibility test: Run identical interview transcripts through system with fixed parameters multiple times; verify output consistency. Compare against ChatGPT's variability.
  2. Context preservation audit: Input interviews with speaker metadata; verify outputs correctly attribute perspectives and maintain speaker distinctions.
  3. Reasoning trace evaluation: Have qualitative researchers assess whether explanation trails are sufficient to evaluate AI suggestions against their analytical judgment.

## Open Questions the Paper Calls Out

- **Evaluation benchmarks**: What evaluation benchmarks grounded in qualitative theory could effectively assess AI performance on interpretive research tasks? The paper states this will require "dedicated efforts, open-source benchmarks grounded in qualitative theory, and closer partnership between computer scientists and interpretivist researchers." Current NLP benchmarks prioritize accuracy metrics ill-suited to qualitative epistemology.

- **Theoretical integration**: How can qualitative AI systems be architected to support deeper theoretical integration beyond first-pass coding? The paper notes current tools "excel at first-pass coding or prompting but rarely support deeper theoretical integration." Theoretical integration requires sustained reasoning across concepts, contradictions, and contexts.

- **Multilingual and oral data**: What technical approaches enable qualitative AI to process multilingual and oral data while preserving contextual meaning? The paper identifies that "Most current systems operate on English-language corpora and struggle with multilingual or oral data." Oral data lacks transcription standardization; multilingual nuance requires cultural knowledge embedded in under-resourced languages.

- **Preserving marginalized perspectives**: How can qualitative AI preserve marginalized perspectives rather than reproducing dominant narratives? The paper asks "whose knowledge is being represented, and whose is being erased" and calls for systems "that preserve rather than overwrite human complexity." Training data imbalances are structural.

## Limitations

- The paper's central claim about epistemic incompatibility rests on philosophical assumptions not empirically validated
- Proposed architectural requirements are presented as jointly necessary without testing whether subsets might suffice
- No controlled comparisons between qualitative-specific tools and general LLMs on actual research tasks

## Confidence

- **High confidence**: General-purpose LLMs exhibit opacity, irreproducibility, and privacy issues when applied to qualitative data
- **Medium confidence**: These issues create practical barriers for qualitative researchers
- **Low confidence**: General-purpose LLMs are epistemically incompatible with qualitative research

## Next Checks

1. **Controlled task comparison**: Compare researcher accuracy and confidence when using ChatGPT versus a transparency-focused system on identical qualitative analysis tasks, measuring both output quality and researcher trust.

2. **Epistemological audit**: Test whether qualitative researchers can distinguish between AI suggestions that preserve context and those that flatten it, establishing whether context-sensitivity is empirically detectable or assumed.

3. **Implementation feasibility study**: Build a minimal prototype incorporating only reproducibility (fixed outputs) and evaluate whether this single property significantly improves qualitative research practice versus full explainability requirements.