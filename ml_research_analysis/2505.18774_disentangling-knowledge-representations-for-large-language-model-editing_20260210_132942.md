---
ver: rpa2
title: Disentangling Knowledge Representations for Large Language Model Editing
arxiv_id: '2505.18774'
source_url: https://arxiv.org/abs/2505.18774
tags:
- knowledge
- editing
- fine-grained
- irrelevant
- dike
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies that knowledge editing in LLMs often unintentionally
  alters fine-grained irrelevant facts that share the same subject but differ in relation
  and object, due to entanglement in the representation space. To address this, the
  authors propose DiKE, a method that disentangles the subject representation into
  target-knowledge-related and -unrelated components, edits only the related part,
  and preserves the unrelated part.
---

# Disentangling Knowledge Representations for Large Language Model Editing

## Quick Facts
- **arXiv ID:** 2505.18774
- **Source URL:** https://arxiv.org/abs/2505.18774
- **Reference count:** 40
- **Key outcome:** DiKE improves preservation of fine-grained irrelevant knowledge (up to 8.3% improvement in Relational Locality) while maintaining competitive efficacy scores (90.8% on COUNTERFACT) across multiple LLMs

## Executive Summary
The paper addresses a critical limitation in knowledge editing of large language models: when updating one fact about a subject, editing methods often unintentionally alter other unrelated facts about the same subject. This occurs because subject representations become entangled with multiple attributes, making it difficult to isolate the specific knowledge to be edited. DiKE solves this by disentangling the subject representation into target-knowledge-related and -unrelated components, then editing only the relevant part while preserving the rest. The method combines a Knowledge Representation Disentanglement module with a Disentanglement-based Knowledge Edit module, achieving significant improvements in preserving fine-grained irrelevant knowledge while maintaining high editing efficacy.

## Method Summary
DiKE introduces a two-stage approach to knowledge editing. First, a Knowledge Representation Disentanglement (KRD) module uses contrastive learning to decompose subject representations into two components: one related to the target knowledge and one unrelated. This is achieved through a two-branch MLP that projects the subject-relation representation to zr_e (target-related) and zu_e (target-unrelated) using InfoNCE loss to force separation. Second, the Disentanglement-based Knowledge Edit (DKE) module edits only the target-related component through closed-form rank-one parameter updates, while preserving the unrelated component through Frobenius norm constraints. The method is trained once on a pretraining dataset and then applied to any editing task without retraining.

## Key Results
- DiKE achieves 90.8% Efficacy Score on COUNTERFACT benchmark, comparable to state-of-the-art methods
- On the new FINE-KED benchmark, DiKE improves Relational Locality by up to 8.3% compared to baselines across Easy, Middle, and Hard difficulty levels
- The method demonstrates consistent performance across three different LLM architectures (GPT2-XL, GPT-J, LLaMA3) with minimal hyperparameter tuning
- DiKE shows significant improvements in preserving fine-grained irrelevant knowledge, particularly on the Hard level where relations are semantically closer

## Why This Works (Mechanism)

### Mechanism 1: Representation Disentanglement via Contrastive Separation
The KRD module learns to separate entangled subject representations into target-knowledge-related and -unrelated components through contrastive learning. By treating (zr_e, hs) and (zu_e, hs) as positive pairs while treating (zr_e, zu_e) as negative pairs, InfoNCE loss forces the two components to capture distinct semantic attributes. This works under the assumption that factual attributes within subject representations can be separated into approximately independent subspaces through learned projections.

### Mechanism 2: Knowledge-Specific Routing via Constraint Losses
During KRD training, the model learns functional specialization by replacing the original subject representation with each disentangled component and optimizing prediction accuracy. When hs is replaced with zr_e, the model predicts the target object; when replaced with zu_e, it predicts fine-grained irrelevant objects. This creates routing where each component specializes in specific factual content without modifying the frozen LLM itself.

### Mechanism 3: Invariance-Preserving Rank-One Update
The DKE module applies a closed-form parameter update that simultaneously injects target knowledge and preserves both coarse-grained and fine-grained irrelevant knowledge. The objective extends MEMIT with disentanglement-weighted constraints, using the disentanglement matrix W₃ to identify which parameter directions affect fine-grained vs. target knowledge. The closed-form solution Ŵ = W + (W₃ᵀW₃ + E)⁻¹Δ_MEMIT weights the update direction by the disentanglement structure.

## Foundational Learning

- **Concept: Rank-One Matrix Updates**
  - **Why needed here:** DiKE builds on MEMIT's least-squares formulation, extending it with additional constraints
  - **Quick check question:** Why does MEMIT's objective ∥Ŵk* - v*∥²_F + ∥ŴK₀ - V₀∥²_F admit a closed-form solution, and what happens if K₀ and k* are nearly collinear?

- **Concept: Contrastive Learning with InfoNCE**
  - **Why needed here:** The disentanglement module relies on InfoNCE to separate zr_e and zu_e without explicit labels defining which features belong to which component
  - **Quick check question:** If temperature τ is set too high in InfoNCE, how does the gradient behavior change, and what might happen to the separation between zr_e and zu_e?

- **Concept: FFN as Key-Value Associative Memory**
  - **Why needed here:** The locate-and-edit paradigm assumes factual knowledge is stored in FFN layers where W_in produces keys and W_out produces values
  - **Quick check question:** How would you determine which FFN layer to edit for a given fact, and why might multi-hop reasoning facts not fit this key-value assumption?

## Architecture Onboarding

- **Component map:** KRD Module (Disentangler: W₁-W₄ + GELU → zr_e, zu_e; Recomposer: W₅, W₆) → DKE Module (δ optimization on zr_e → v* computation → rank-one update Ŵ)

- **Critical path:**
  1. Extract hs from subject's last token at layer l (17 for GPT2-XL, 8 for GPT-J/LLaMA3)
  2. Extract hr from prompt's last token at later layer (37, 18, 23 respectively)
  3. Apply frozen KRD: (hs, hr) → (zr_e, zu_e)
  4. Optimize δ via AdamW on zr_e + δ to predict target object o* (early-stop at loss < 0.05)
  5. Compute value v* = Rec(zr_e + δ, zu_e) - hp_s
  6. Apply Ŵ = W + (W₃ᵀW₃ + E)⁻¹(v* - Wk*)k*ᵀ(K₀K₀ᵀ + k*k*ᵀ)⁻¹

- **Design tradeoffs:**
  - Pre-trained vs. per-edit KRD: Pre-training enables zero-shot application but may not capture subject-specific entanglement
  - Layer selection for hs/hr: Earlier hs layers capture subject identity; later hr layers capture relation semantics
  - Size of fine-grained constraint set N: Paper shows stable performance with |N| = 1-10; larger N marginally improves Hard-level locality but increases training time

- **Failure signatures:**
  - Disentanglement collapse: t-SNE shows zr_e and zu_e forming single cluster instead of distinct clusters
  - Efficiency-locality inversion: High Efficacy (>95%) with Relational Locality below random baseline indicates disentanglement learned wrong routing
  - Batch scaling failure: Efficacy drops >20% going from batch-size 1 to 4 indicates constraint conflicts

- **First 3 experiments:**
  1. Disentanglement quality baseline: Train KRD on 20k samples from WikiData, visualize t-SNE of zr_e vs zu_e for 1k held-out test samples. Success: clear cluster separation (compute silhouette score >0.3).
  2. Constraint loss ablation: Train three KRD variants (w/o L_ctr, w/o L_con, w/o L_recon) and evaluate on FINE-KED Hard level. Hypothesis: L_con contributes most to Hard-level locality.
  3. Subject-consistent batch stress test: Construct 100 batches (size 4) where all edits share same subject with semantically distant relations. Compare DiKE vs. MEMIT vs. ROME on Relational Locality degradation.

## Open Questions the Paper Calls Out

- **Can DiKE be extended to support unstructured or semi-structured knowledge formats beyond relational triples (s, r, o)?**
  - The authors acknowledge this limitation, stating DiKE is best suited for explicitly represented relational triples and may not generalize to unstructured knowledge formats.

- **How sensitive is the KRD module's disentanglement quality to the diversity and domain coverage of its pretraining data?**
  - The authors note that the quality of disentangled representations may be sensitive to the quality and diversity of pretraining data, as well as hyperparameter selection.

- **Why does DiKE show lower Relational Locality on the "Hard" level of FINE-KED, and can additional mechanisms improve preservation of highly semantically-similar fine-grained knowledge?**
  - The paper shows only a 4.9% improvement over baselines on Hard level vs. 9.1% on Easy, suggesting semantically similar relations remain challenging to disentangle.

## Limitations

- The FINE-KED benchmark construction methodology remains unclear, making it difficult to assess generalization to real-world editing scenarios
- The assumption that fine-grained irrelevant facts can be cleanly separated from target knowledge may not hold when editing batches contain semantically related facts
- The closed-form rank-one update's effectiveness depends on the disentanglement matrix W₃ capturing meaningful structure that may not generalize across different subject types

## Confidence

- **High confidence:** The core mechanism of using contrastive learning for representation disentanglement and the basic efficacy improvement over MEMIT (90.8% on COUNTERFACT)
- **Medium confidence:** The preservation of fine-grained irrelevant knowledge (up to 8.3% Relational Locality improvement) depends heavily on benchmark quality and assumption validity
- **Low confidence:** The generalization of the disentanglement approach to LLMs with different architectures and scalability to large-scale editing operations

## Next Checks

1. **Benchmark Generalization Test:** Apply DiKE to a real-world knowledge base (e.g., Freebase or WikiData) where subject entities have 20+ known facts. Measure Relational Locality degradation when editing 4-8 randomly selected facts per subject, comparing against MEMIT and ROME.

2. **Semantic Proximity Stress Test:** Construct test cases where target and irrelevant facts are semantically related (e.g., editing "capital" while preserving "largest city," "population," and "GDP"). Measure whether DiKE's disentanglement correctly identifies and preserves facts that should remain connected.

3. **Batch Scaling Experiment:** Systematically increase batch sizes from 1 to 16 edits per subject with mixed relation types (some related, some unrelated). Track the point at which Efficacy and Relational Locality start to conflict, identifying the practical limit for DiKE's closed-form update approach.