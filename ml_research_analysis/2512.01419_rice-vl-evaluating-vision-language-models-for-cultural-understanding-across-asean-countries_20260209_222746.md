---
ver: rpa2
title: 'Rice-VL: Evaluating Vision-Language Models for Cultural Understanding Across
  ASEAN Countries'
arxiv_id: '2512.01419'
source_url: https://arxiv.org/abs/2512.01419
tags:
- cultural
- visual
- across
- culturally
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RICE-VL is a benchmark designed to evaluate cultural understanding
  in vision-language models across 11 Southeast Asian countries, addressing the Western-centric
  bias in existing datasets. It includes 28,000 curated visual question-answer pairs
  and 1,000 image-bounding box pairs across 14 cultural categories, annotated by regionally
  informed experts over 720 hours.
---

# Rice-VL: Evaluating Vision-Language Models for Cultural Understanding Across ASEAN Countries

## Quick Facts
- **arXiv ID:** 2512.01419
- **Source URL:** https://arxiv.org/abs/2512.01419
- **Reference count:** 25
- **Primary result:** Rice-VL reveals significant cultural understanding gaps in VLMs, especially for low-resource ASEAN countries, with closed-source models outperforming open-source models.

## Executive Summary
Rice-VL is a benchmark designed to evaluate cultural understanding in vision-language models across 11 Southeast Asian countries. It addresses the Western-centric bias in existing datasets by introducing 28,000 curated visual question-answer pairs and 1,000 image-bounding box pairs across 14 cultural categories, annotated by regionally informed experts over 720 hours. The benchmark introduces SEA-LA VE, an evaluation metric assessing textual accuracy, cultural alignment, and country identification. Testing six open- and closed-source VLMs, results show significant performance gaps, with closed-source models outperforming others, especially in high-resource countries, while low-resource nations like Timor-Leste and Brunei see weaker results.

## Method Summary
Rice-VL evaluates VLMs on two tasks: CulturalVQA (True/False, Fill-in-the-Blank, Open-ended questions) and Cultural Visual Grounding (localizing cultural elements via bounding boxes) across 11 ASEAN countries. The benchmark uses 28,000 VQA pairs from 7,000 images and 1,000 image-bounding box pairs across 14 categories. Evaluation employs the SEA-LAVE metric, which combines Text Understanding (TU), Cultural Understanding (CU), and Country Identification (CI) scores, calculated using Qwen2.5-VL 7B as a judge model. Models are tested under both "Global" and "Southeast Asian" context prompts to assess prompt sensitivity.

## Key Results
- Closed-source models (GPT-4o, Claude-3-Opus) significantly outperform open-source models across all metrics and countries
- Performance varies dramatically by country, with high-resource countries (Singapore, Thailand) showing much higher scores than low-resource countries (Timor-Leste, Brunei)
- Visual grounding accuracy varies by cultural category, with abstract or symbolic themes (Religious Practices, Key Figures, Painting) showing lower accuracy than distinct categories (Clothing, Transport)

## Why This Works (Mechanism)

### Mechanism 1: Expert-Mediated Cultural Grounding
The benchmark effectively exposes model limitations because it relies on regionally-informed expert annotation rather than automated web scraping, which filters out superficial visual patterns that models might otherwise exploit. By utilizing 720 hours of annotation by trained cultural experts, the dataset enforces a distinction between visual correlation and cultural semantic validity, forcing models to require specific cultural priors rather than generic visual recognition.

### Mechanism 2: Contextual Priming for Latent Knowledge Retrieval
VLMs store cultural knowledge latently but fail to retrieve it without explicit regional prompting; the benchmark exploits this by testing "Global" vs. "SEA" prompting conditions. The prompting strategy ("This is a Southeast Asian setting") acts as a cognitive switch, shifting the model's probability mass from dominant Western priors to sparsely represented SEA regional priors, quantifying the accessibility of cultural knowledge vs. lack of existence.

### Mechanism 3: Decomposed Semantic-Cultural Evaluation (SEA-LAVE)
Traditional metrics conflate visual correctness with cultural relevance; SEA-LAVE isolates these dimensions to penalize "hallucinated cultural specificity." The metric decomposes the score into Text Understanding, Cultural Understanding, and Country Identification, preventing models from receiving full credit for generic visual descriptions when specific cultural practice identification is required.

## Foundational Learning

- **Concept: Visual Grounding (Referring Expression Comprehension)**
  - Why needed: The paper introduces "Cultural Visual Grounding," requiring models to output bounding boxes, not just text
  - Quick check: If a model correctly identifies a "religious statue" but the bounding box cuts off the base, does IoU penalize it?

- **Concept: Western-Centric Bias in Pre-training**
  - Why needed: This is the core problem the paper attempts to diagnose
  - Quick check: Why might a model mistake a "traditional game" for a generic children's toy?

- **Concept: LLM-as-a-Judge**
  - Why needed: The SEA-LAVE metric relies on an LLM to grade the VLM's answers
  - Quick check: What are the risks of using a general-purpose VLM to judge the "cultural correctness" of another model?

## Architecture Onboarding

- **Component map:** Dataset Layer (7,000 images, 14 categories) -> Annotation Layer (human-in-the-loop for VQA and bounding boxes) -> Evaluation Layer (SEA-LAVE metric using Qwen2.5-VL judge) -> Model Layer (tested VLMs like GPT-4o, Claude, LLaMA)

- **Critical path:** 1. Data Curation: Gathering images for "low-resource" countries is the bottleneck 2. Prompt Engineering: Formatting input to distinguish "Global" vs. "SEA" contexts 3. Metric Calculation: Running inference, then passing model output + ground truth to SEA-LAVE evaluator

- **Design tradeoffs:** Scope vs. Depth (covering 11 countries dilutes specificity), English-only (simplifies evaluation but ignores native linguistic nuance), Model-as-Judge (trades human labor for speed, risking bias propagation)

- **Failure signatures:** The "Tourist" Effect (high TU but zero CU), Visual Similarity Trap (poor grounding for Western lookalikes), Prompt Sensitivity (large variance between Global and SEA contexts)

- **First 3 experiments:** 1. Baseline Reproduction: Run GPT-4o and LLaMA 3.2 on Global vs. SEA prompt split 2. Category Error Analysis: Isolate Religious Practices and Key Figures categories 3. Negative Ablation: Feed non-SEA images with SEA prompt to test hallucination

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does evaluating VLMs in native Southeast Asian languages versus English affect cultural reasoning accuracy and semantic nuance?
- **Basis in paper:** The authors state in the Limitations section that "the benchmark is English-only, which simplifies evaluation but may overlook culturally nuanced meanings in native languages"
- **Why unresolved:** The study deliberately standardized on English to facilitate comparison
- **What evidence would resolve it:** A comparative study using RICE-VL tasks translated into local languages showing performance deviations

### Open Question 2
- **Question:** What specific training interventions can close the performance gap for severely underrepresented nations like Timor-Leste and Brunei?
- **Basis in paper:** The paper notes that "Timor-Leste remains the most challenging for all systems" and suggests that "region alone is insufficient without culturally grounded training data"
- **Why unresolved:** While the benchmark identifies failure points, it does not test whether fine-tuning or few-shot prompting can mitigate these specific deficits
- **What evidence would resolve it:** Experiments applying parameter-efficient fine-tuning on small, targeted datasets from Timor-Leste to observe performance recovery

### Open Question 3
- **Question:** How can VLMs be improved to accurately localize abstract or symbolic cultural elements that lack distinct visual salience?
- **Basis in paper:** The analysis notes that "areas involving smaller or more abstract elements—such as Religious Practices, Key Figures, and Painting—show lower accuracy"
- **Why unresolved:** Current visual grounding relies heavily on geometric alignment and prominent features, struggling with symbolism requiring contextual inference
- **What evidence would resolve it:** Development of models that incorporate metadata or textual context to boost IoU scores for abstract cultural categories

## Limitations
- Inherent challenge of creating objective "cultural understanding" benchmark due to subjective, context-dependent nature of cultural interpretation
- Western-centric nature of evaluation metric (SEA-LAVE using Qwen2.5-VL) may still introduce bias
- English-only benchmark limits applicability to native language contexts
- Dataset construction faces scalability issues for low-resource countries

## Confidence
- **High Confidence:** Closed-source models outperform open-source models across multiple metrics and countries
- **Medium Confidence:** SEA-LAVE metric effectively captures cultural understanding beyond visual accuracy
- **Low Confidence:** Claim that models have "latent knowledge" of SEA cultures that can be triggered by prompts

## Next Checks
1. Cross-Cultural Prompt Transfer Test: Run SEA-specific prompt on completely different cultural contexts to test hallucination
2. Human Expert Validation: Conduct small-scale human evaluation to verify SEA-LAVE metric scores on stratified sample
3. Low-Resource Data Augmentation Study: Test whether targeted fine-tuning on Timor-Leste/Brunei data can close performance gap