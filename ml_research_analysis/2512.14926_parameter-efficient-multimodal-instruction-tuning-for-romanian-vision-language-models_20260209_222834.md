---
ver: rpa2
title: Parameter Efficient Multimodal Instruction Tuning for Romanian Vision Language
  Models
arxiv_id: '2512.14926'
source_url: https://arxiv.org/abs/2512.14926
tags:
- romanian
- language
- llav
- image
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first systematic effort to reduce the Romanian
  multimodal NLP resource gap by creating high-quality vision-language datasets and
  fine-tuning open-source VLMs with parameter-efficient methods. We translate Flickr30k
  captions into Romanian and extend them with synthetic QA pairs to form Flickr30k-RoQA.
---

# Parameter Efficient Multimodal Instruction Tuning for Romanian Vision Language Models

## Quick Facts
- arXiv ID: 2512.14926
- Source URL: https://arxiv.org/abs/2512.14926
- Authors: George-Andrei Dima; Dumitru-Clementin Cercel
- Reference count: 15
- Primary result: Romanian VQA models adapted with LoRA achieve 80.2% BERTScore F1 and 44.3% ROUGE-L

## Executive Summary
This work addresses the Romanian multimodal NLP resource gap by creating high-quality vision-language datasets and fine-tuning open-source VLMs with parameter-efficient methods. The authors translate Flickr30k captions into Romanian and extend them with synthetic QA pairs to form Flickr30k-RoQA. Using LoRA adapters with <1% parameter updates, they adapt three state-of-the-art VLMs to Romanian visual QA, achieving significant performance improvements and demonstrating generalization to image captioning.

## Method Summary
The authors translate Flickr30k captions into Romanian and generate synthetic QA pairs using LLaMA-3.3-70B, creating Flickr30k-RoQA (25,426 train/6,357 test samples). They fine-tune three VLMs (LLaMA 3.2-11B-Vision, Qwen2-VL-7B-Instruct, LLaVA-v1.6-Mistral-7B) using LoRA adapters with r=α=16, targeting all linear projections in the language transformer while freezing the vision encoder. Training uses batch size 16, 500 steps, learning rate 2e-4, and a single A100 GPU. The adapters also generalize to zero-shot image captioning.

## Key Results
- Qwen2-VL-RoVQA achieves 80.2% BERTScore F1 (+6.05% over baseline) and 44.3% ROUGE-L (+14.03%) on Romanian VQA
- LLaMA-3.2 and Qwen2-VL adapters improve BLEU scores by up to +27.99% on image captioning
- Grammatical error analysis shows significant WER reductions across all adapted models
- LoRA updates <1% of parameters while preserving base model capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LoRA adapters on language branches can transfer Romanian linguistic knowledge to VLMs with <1% parameter updates.
- **Mechanism**: Low-rank decomposition creates small trainable matrices that modify attention and MLP projections without touching vision encoders. The frozen visual backbone preserves pretrained representations while adapters reshape language-side processing for Romanian token patterns and morphology.
- **Core assumption**: Visual features are already sufficiently language-agnostic or transferable across languages, so adaptation is primarily needed on the language side.
- **Evidence anchors**:
  - [abstract]: "Using LoRA adapters, we adapt three state-of-the-art VLMs... updating less than 1% of model parameters."
  - [Section 3.3]: "All original vision parameters remained frozen, while language, attention, and MLP blocks were equipped with LoRA modules... this strategy preserves the pretrained visual representations and reduces GPU memory consumption."

### Mechanism 2
- **Claim**: Multilingual pre-training provides a stronger prior for low-resource language adaptation than model scale alone.
- **Mechanism**: Qwen2-VL's pre-training on ~30 languages creates latent Romanian representations that LoRA fine-tuning can activate and align, whereas monolingual English models must learn Romanian patterns from scratch despite larger parameter counts.
- **Core assumption**: The pre-training corpus genuinely included Romanian (not just typologically similar languages) and the model retained this knowledge through instruction tuning.
- **Evidence anchors**:
  - [abstract]: "The seven-billion-parameter Qwen2-VL-RoVQA obtains top scores on both tasks, with improvements of +6.05% and +2.61% in BERTScore F1 over its original version."
  - [Section 4.2]: "Despite having fewer parameters, Qwen2 outperforms the larger LLaMA-3.2 before adaptation, presumably because Qwen 2 line was intentionally trained on a multilingual corpus that covers roughly thirty languages beyond English and Chinese."

### Mechanism 3
- **Claim**: Task-specific Romanian supervision generalizes to unseen multimodal tasks without additional training.
- **Mechanism**: LoRA adapters learned on VQA create Romanian-language pathways that transfer to captioning. The adapter shapes how visual tokens are decoded into Romanian text, independent of the prompt format or task structure.
- **Core assumption**: The cross-task transfer is not just memorizing prompt-response patterns but learning genuine Romanian-visual alignment.
- **Evidence anchors**:
  - [abstract]: "The adapters also generalize to image captioning, improving BLEU by up to +27.99%."
  - [Section 4.3]: "Although the adapters were trained exclusively on the Romanian VQA corpus, two of the three architectures, LLaMA-3.2 and Qwen2-VL, transfer the acquired knowledge to caption generation."

## Foundational Learning

- **Low-Rank Adaptation (LoRA)**:
  - **Why needed here**: Understanding how r=α=16 with no dropout across all linear projections achieves 0.48-0.60% parameter updates while preserving base capabilities.
  - **Quick check question**: If you wanted to double the trainable parameter count, would you increase rank or target more layers—and what are the memory implications?

- **Vision-Language Model Architectures**:
  - **Why needed here**: The paper compares LLaVA-style projection (MLP), LLaMA-Vision's cross-attention adapter, and Qwen2's dynamic resolution approach—knowing when each helps explains performance differences.
  - **Quick check question**: Why might LLaVA-v1.6 with 4-bit quantization fail to benefit from Romanian fine-tuning while full-precision models succeed?

- **Multilingual Evaluation Metrics**:
  - **Why needed here**: BERTScore captures semantic similarity across morphological variations (critical for Romanian inflections), while BLEU/ROUGE-L measure n-gram overlap—interpreting their divergence reveals adaptation quality.
  - **Quick check question**: If BERTScore improves but ROUGE-L drops after fine-tuning, what does this suggest about the model's output style changes?

## Architecture Onboarding

- **Component map**: Vision encoder (ViT/SigLIP/CLIP) -> Projection/Adapter -> LLM backbone (LLaMA/Mistral/Qwen2) -> LoRA injection -> Supervised fine-tuning

- **Critical path**:
  1. Dataset preparation: Human-verified Romanian captions + LLM-generated QA pairs
  2. Model selection: Prioritize multilingual pre-training (Qwen2 > LLaMA for Romanian)
  3. LoRA configuration: r=16, α=16, target all linear layers, no dropout
  4. Training: 500 steps, batch size 16, learning rate 2e-4, single A100 (4.7 hours)

- **Design tradeoffs**:
  - **Full fine-tuning vs. LoRA**: Full tuning may achieve higher task-specific performance but risks catastrophic forgetting and requires 10x+ compute
  - **4-bit quantization**: Reduces memory but LLaVA-v1.6 results show it can prevent effective language adaptation (BLEU rises but BERTScore drops)
  - **Vision encoder freezing**: Preserves general visual features but cannot adapt to language-specific visual patterns (e.g., Romanian text in images)

- **Failure signatures**:
  - **LLaVA-v1.6 pattern**: BLEU increases (+33.79) but BERTScore F1 drops (-5.20) → model produces shorter, lexically precise but semantically incomplete answers
  - **High WER before fine-tuning**: Substitution errors dominated (5.57 for LLaMA, 4.42 for Qwen) → missing definite articles, diacritic loss
  - **Verbose outputs**: BERTScore recall > precision before fine-tuning → model generates correct content amid superfluous tokens

- **First 3 experiments**:
  1. **Baseline comparison**: Evaluate all three base VLMs (LLaMA-3.2-11B, Qwen2-VL-7B, LLaVA-v1.6-7B) on Romanian VQA and captioning to quantify the multilingual prior advantage.
  2. **LoRA rank sweep**: Test r=[8, 16, 32] on Qwen2-VL (best performer) to find the minimum effective adapter size; expect diminishing returns beyond r=16 given <1% parameter constraint.
  3. **Cross-task validation**: Train separate adapters for VQA-only, captioning-only, and mixed tasks; compare zero-shot transfer to assess whether task-specific or general Romanian supervision is more efficient.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the lightweight LoRA adapters trained on general-domain Flickr30k-Ro data generalize effectively to specialized Romanian multimodal tasks such as medical imaging analysis or remote sensing?
- **Basis in paper**: [explicit] The authors state in the Conclusion that "Future research directions include domain adaptation" and plan to extend the corpus to specialized domains to "evaluat[e] the generality of LoRA–based tuning."
- **Why unresolved**: The current study is limited to "everyday photographic scenes" and does not address "scenarios involving complex visual abstractions or domain-specific terminology."
- **What evidence would resolve it**: Performance benchmarks of the current adapters on new, domain-specific Romanian VQA datasets (e.g., medical or satellite imagery) compared against baseline models.

### Open Question 2
- **Question**: Does relying solely on textual descriptions for synthetic QA generation, without visual grounding, introduce specific hallucinations or fail to capture visual nuances critical for Romanian VQA performance?
- **Basis in paper**: [explicit] The Conclusion notes a limitation where the synthetic QA component "relies on a LLaMA-3 generator... conditioned only on text, which may omit visually grounded reasoning patterns specific to Romanian usage."
- **Why unresolved**: The current methodology generates questions based only on the caption text, potentially missing visual details present in the image but absent from the text description.
- **What evidence would resolve it**: A comparative study evaluating models trained on text-only generated QA vs. models trained on QA generated by a multimodal model (e.g., GPT-4V) that processes the image directly.

### Open Question 3
- **Question**: What are the primary causes of the observed performance degradation in the LLaVA-v1.6 adapter, and can it be mitigated by adjusting adapter rank or avoiding aggressive quantization?
- **Basis in paper**: [inferred] The authors note in Section 4.2 that the LLaVA adapter "depresses BERTScore F1" and suggest "a plausible explanation is that the 7-B backbone, already aggressively quantised to 4-bit precision, has limited capacity."
- **Why unresolved**: The paper documents the failure mode (shorter, lexically exact answers that miss context) but does not isolate whether the issue stems from the 4-bit quantization or the underlying model's multilingual architecture.
- **What evidence would resolve it**: Ablation experiments running the same fine-tuning pipeline on LLaVA-v1.6 using higher precision (e.g., 16-bit) or varying LoRA ranks to observe if semantic coverage improves.

## Limitations

- **Dataset quality and scope**: Flickr30k-RoQA combines human-translated captions with LLM-generated QA pairs, creating potential distribution shifts between human and synthetic data.
- **Model architecture differences**: The three VLMs use fundamentally different vision-language integration strategies, making it difficult to isolate the contribution of multilingual pre-training from architectural advantages.
- **Evaluation metric limitations**: BERTScore with multilingual BERT captures semantic similarity but may not reflect Romanian grammatical accuracy or fluency.

## Confidence

- **High confidence**: The LoRA adaptation mechanism works as described (parameter count, training setup, performance improvements on VQA). The Qwen2-VL multilingual advantage is clearly demonstrated through controlled comparisons with LLaMA-3.2.
- **Medium confidence**: The claim that <1% parameter updates achieve substantial improvements is supported but could benefit from rank sensitivity analysis. Cross-task generalization to captioning is observed but not thoroughly validated across multiple tasks.
- **Low confidence**: The exact contribution of multilingual pre-training vs. architecture differences to Qwen2's performance is not isolated. The grammatical error reduction analysis lacks detail on error type distributions and whether corrections follow Romanian linguistic rules.

## Next Checks

1. **Architecture ablation study**: Fine-tune LLaMA-3.2 and Qwen2-VL with identical architectures (e.g., both using LLaVA-style MLP projection) to isolate the multilingual pre-training effect from architectural advantages.

2. **Extended task evaluation**: Test the Romanian-adapted models on spatial reasoning, counting, and text-in-image tasks from benchmarks like VQA-CP or OCR-VQA to validate generalization beyond VQA and captioning.

3. **Error analysis validation**: Conduct human evaluation of model outputs focusing on grammatical correctness, article usage, and diacritic preservation to complement the WER metrics and identify systematic error patterns.