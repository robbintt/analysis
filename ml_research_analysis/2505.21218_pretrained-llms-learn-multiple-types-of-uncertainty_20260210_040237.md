---
ver: rpa2
title: Pretrained LLMs Learn Multiple Types of Uncertainty
arxiv_id: '2505.21218'
source_url: https://arxiv.org/abs/2505.21218
tags:
- uncertainty
- should
- arxiv
- language
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to identify uncertainty representations
  within pretrained large language models (LLMs) by searching for linear directions
  in their latent space that predict generation correctness. The authors propose training
  linear classifiers at each transformer layer to find these uncertainty vectors,
  which they then use to assess how well the model captures uncertainty without further
  training.
---

# Pretrained LLMs Learn Multiple Types of Uncertainty

## Quick Facts
- arXiv ID: 2505.21218
- Source URL: https://arxiv.org/abs/2505.21218
- Authors: Roi Cohen; Omri Fahn; Gerard de Melo
- Reference count: 40
- Primary result: Pretrained LLMs encode multiple distinct, nearly orthogonal types of uncertainty in their latent space, with performance peaking in intermediate layers.

## Executive Summary
This paper introduces a method to identify uncertainty representations within pretrained large language models by searching for linear directions in their latent space that predict generation correctness. The authors train linear classifiers at each transformer layer to find these uncertainty vectors and demonstrate that LLMs encode multiple distinct types of uncertainty, often dataset-specific and nearly orthogonal. Their results show that these uncertainty vectors can significantly outperform random chance in predicting correctness across multiple datasets and models, with the best performance typically occurring in intermediate layers. The study also finds that model size does not substantially improve uncertainty representation, whereas instruction-tuning and [IDK]-tuning greatly enhance the ability to capture uncertainty and generalize across tasks.

## Method Summary
The paper trains linear probes (logistic regression classifiers) on hidden states from each transformer layer to predict whether a generated answer is correct or incorrect. For each dataset, they train a separate probe at every layer, yielding layer-specific uncertainty vectors. These vectors are then used to predict correctness on test data. The method involves extracting hidden states at each layer during inference, training layer-wise classifiers on the training split, and evaluating accuracy on the test split. The approach is applied across multiple models and datasets to identify patterns in where and how uncertainty is represented.

## Key Results
- Uncertainty vectors peak in performance at intermediate transformer layers (around 1/2 to 3/4 of depth)
- Multiple distinct, nearly orthogonal uncertainty representations exist per model, each specialized to different datasets
- Instruction-tuning significantly improves uncertainty representation and cross-dataset generalization compared to base models
- Model size does not substantially improve uncertainty representation

## Why This Works (Mechanism)
The method works because pretrained LLMs have already learned to represent correctness-related information in their hidden states during pretraining. By training linear classifiers on these states, the paper can identify directions in the latent space that correlate with whether outputs are correct. The emergence of distinct uncertainty vectors suggests that models develop specialized representations for different types of knowledge or uncertainty. The peak performance in intermediate layers indicates that uncertainty information is processed and refined as information flows through the network, rather than being concentrated at input or output stages.

## Foundational Learning
- **Linear Probing in Neural Networks**
  - Why needed here: The entire method rests on finding interpretable directions via linear classifiers in high-dimensional hidden states
  - Quick check question: Can you explain why a linear probe succeeding does not guarantee the underlying representation is purely linear, but may only be linearly separable?

- **Transformer Layer Functions (High-Level)**
  - Why needed here: The paper shows uncertainty signals peak in intermediate layers, requiring understanding of how information flows across layers
  - Quick check question: Which parts of a transformer layer (attention, MLP, residual) could plausibly contribute to encoding an abstract property like "certainty," and why might this emerge mid-network?

- **Calibration vs. Uncertainty Estimation**
  - Why needed here: The paper's correctness prediction task is a binary proxy for uncertainty, distinct from probability calibration
  - Quick check question: How does predicting a binary "correct/incorrect" label from a hidden state differ from interpreting output softmax probabilities as a confidence measure?

## Architecture Onboarding
- **Component Map**: Dataset Preparation -> Model Inference & Labeling -> Layer-wise Probe Fitting -> Evaluation
- **Critical Path**:
  1. Dataset Preparation: Curate QA datasets with clear correctness labels, split into train/test
  2. Model Inference & Labeling: For each training question, generate model's answer and label as correct/incorrect
  3. Layer-wise Probe Fitting: For every layer, train a linear probe on hidden states to yield uncertainty vector for the dataset

- **Design Tradeoffs**:
  - Linearity vs. Expressiveness: Restricting to linear classifiers aids interpretability but may miss complex, nonlinear uncertainty signals
  - Dataset-Specific vs. Unified Probes: Per-dataset probes capture diverse uncertainty types but yield many vectors; unified probes are simpler but may average away distinctions

- **Failure Signatures**:
  - Random-Baseline Performance: Probe accuracy hovers near 0.5 across layers and datasets
  - No Layer-wise Pattern: Performance is erratic across layers with no clear peak in intermediate layers
  - Zero Cross-Dataset Generalization: A probe trained on one dataset performs at chance on all others

- **First 3 Experiments**:
  1. Reproduce Key Result on One Model/Dataset: Pick Llama-3.1-8B and TruthfulQA, train probes at each layer, plot accuracy vs. layer number
  2. Test Multi-Type Hypothesis: Train probes on GSM8K and NaturalQuestions, evaluate each on both test sets, compute cosine similarity
  3. Instruction-Tuning Ablation: Compare probe performance between Llama-3.1-8B and Llama-3.1-8B-Instruct

## Open Questions the Paper Calls Out
- Can actively steering generation using identified uncertainty vectors reduce hallucinations in real-time?
- Do nonlinear probes reveal additional uncertainty structure beyond the linear directions identified?
- Does the multiplicity of uncertainty representations causally contribute to hallucinations?
- How does instruction-tuning mechanistically unify uncertainty representations across datasets?

## Limitations
- Reliance on linear probes may underrepresent the true complexity of uncertainty representations
- Dataset-specific nature of uncertainty vectors raises questions about practical applicability
- The orthogonality of uncertainty vectors suggests unified uncertainty measures may be fundamentally challenging

## Confidence
- **High confidence**: The existence of dataset-specific uncertainty vectors and their orthogonality are well-established
- **Medium confidence**: The claim that instruction-tuning enhances uncertainty representation is supported but based on limited comparisons
- **Medium confidence**: The observation that intermediate layers contain strongest uncertainty signals is consistent but could vary with different architectures

## Next Checks
1. Test whether nonlinear probes (e.g., small MLPs) can capture additional uncertainty information beyond what linear probes detect
2. Evaluate whether uncertainty vectors trained on synthetic or controlled datasets show predictable patterns when applied to real-world datasets
3. Investigate whether ensemble methods combining multiple dataset-specific uncertainty vectors improve overall uncertainty estimation