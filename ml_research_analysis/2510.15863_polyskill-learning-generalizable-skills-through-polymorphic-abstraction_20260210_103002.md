---
ver: rpa2
title: 'PolySkill: Learning Generalizable Skills Through Polymorphic Abstraction'
arxiv_id: '2510.15863'
source_url: https://arxiv.org/abs/2510.15863
tags:
- skill
- skills
- agent
- agents
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building web agents that
  can learn and reuse generalizable skills across diverse websites. Current skill
  induction methods often create over-specialized skills that fail to transfer to
  unseen domains.
---

# PolySkill: Learning Generalizable Skills Through Polymorphic Abstraction

## Quick Facts
- **arXiv ID:** 2510.15863
- **Source URL:** https://arxiv.org/abs/2510.15863
- **Reference count:** 40
- **Primary result:** Hierarchical skill abstraction improves web agent generalization across websites by up to 9.4% on unseen domains

## Executive Summary
PolySkill addresses the challenge of building web agents that can learn and reuse generalizable skills across diverse websites. Current skill induction methods often create over-specialized skills that fail to transfer to unseen domains. The proposed framework applies polymorphic abstraction—inspired by software engineering—to separate a skill's abstract goal from its concrete implementation. This hierarchical structure allows agents to learn reusable, compositional skills that adapt across different websites. Experiments on Mind2Web and WebArena benchmarks show PolySkill improves task success rates by up to 9.4% on unseen websites, reduces steps by over 20%, and increases skill reusability by 1.7x compared to prior methods.

## Method Summary
PolySkill introduces a hierarchical skill induction framework that organizes skills as abstract classes defining domain schemas with goal-level method signatures, and concrete implementations that realize site-specific logic. The agent retrieves the abstract schema on new sites to guide exploration and enforce interface consistency. Skills are induced from successful trajectories using an LLM constrained by the abstract interface, then verified by re-execution. The framework supports continual learning by preserving general knowledge in abstract layers while updating concrete implementations, and enables autonomous skill discovery without predefined tasks.

## Key Results
- Improves task success rates by up to 9.4% on unseen websites
- Reduces task steps by over 20% compared to baselines
- Increases skill reusability by 1.7x on seen sites and 31% on unseen sites
- Prevents catastrophic forgetting during continual learning (+4.9% final advantage over ASI)

## Why This Works (Mechanism)

### Mechanism 1: Polymorphic Hierarchy for Cross-Domain Transfer
Organizing skills as a polymorphic hierarchy improves cross-domain transfer and reuse. An abstract class defines a domain schema with goal-level method signatures (e.g., `search_product(query)`), while concrete subclasses implement site-specific logic. The agent retrieves the abstract schema on a new site, which narrows exploration to known skill types and enforces interface consistency. This works because websites within a domain share a latent functional structure that can be captured by a common abstract interface, with UI variations localized to concrete implementations.

### Mechanism 2: Schema-Guided Skill Induction
Guiding skill induction via domain schemas reduces over-specialization and improves skill reusability. The induction prompt includes the abstract class as context, instructing the LLM to implement specific methods in concrete subclasses conforming to the abstract interface. This constrains the solution space toward generalizable, structurally consistent skills. This works because the LLM can correctly interpret the abstract interface and produce implementations that are semantically aligned with the schema across sites.

### Mechanism 3: Hierarchical Abstraction for Continual Learning
Hierarchical abstraction supports positive transfer and mitigates catastrophic forgetting in continual learning. When learning a new site within a known domain, the agent retrieves the existing abstract class, which scaffolds exploration and constrains skill updates to concrete implementations. This isolates changes, preserving general knowledge in the abstract layer. This works because new sites share enough structure with the abstract class that reuse is beneficial, and skill updates are additive without requiring modification of shared abstractions.

## Foundational Learning

- **Concept: Polymorphism and inheritance in object-oriented design**
  - Why needed here: PolySkill's core architecture models skills as abstract classes with concrete subclasses. Understanding how abstract interfaces separate "what" from "how" is essential for reading the skill schemas and implementation logic.
  - Quick check question: Given an abstract class `AbstractEmailClient` with methods `compose()`, `send()`, and `attach_file()`, what would a concrete subclass `GmailClient` need to implement differently than `OutlookClient`?

- **Concept: LLM-based agent policy and POMDP formulation**
  - Why needed here: The paper models web agent interaction as a POMDP and defines the policy as an LLM conditioned on working memory and a dynamic skill library. Understanding this framing clarifies why skill reusability directly impacts action efficiency.
  - Quick check question: In a POMDP, why does the agent rely on observations rather than full state, and how does a dynamic skill library expand the action space at each timestep?

- **Concept: Skill induction and verification pipelines**
  - Why needed here: PolySkill builds on ASI's verification loop, where skills are induced from successful trajectories and validated via re-execution. This loop is the gatekeeper for library quality.
  - Quick check question: If a skill passes verification on one trajectory but fails on a slight variation of the same task, what does that suggest about the skill's generalizability, and how might polymorphic abstraction help address this?

## Architecture Onboarding

- **Component map:** Agent (Policy π_L) -> Skill Library (K_t) -> Induction Module (Distiller) -> Verification Module (Judge) -> Explorer/Annotator

- **Critical path:** 1) Task execution generates trajectory 2) Induction module proposes skills constrained by abstract class 3) Verification re-runs task using candidate skill 4) On new site, agent retrieves abstract schema to guide exploration

- **Design tradeoffs:**
  - Abstract class initialization: Early vs. delayed—early provides structure but risks low-quality schemas from limited data
  - Skill granularity: Fine-grained (e.g., `click_search_button`) vs. coarse-grained (`search_product`)—fine-grained increases reusability but requires more composition
  - Verification strictness: Exact match vs. functional equivalence—strict verification ensures reliability but may reject broadly useful skills

- **Failure signatures:**
  - Low skill reusability on unseen sites: Abstract classes too site-specific or induction not sufficiently constrained
  - Performance degradation after learning new sites: Catastrophic forgetting; check if abstract classes are being modified
  - High verification rejection rate: Over-constrained induction prompts or noisy trajectories

- **First 3 experiments:**
  1. Reproduce baseline comparison on WebArena Shopping subset: Run ASI, SkillWeaver, and PolySkill (static library) on same tasks, measure success rate, skill reusability, and average steps
  2. Ablate abstract class context in induction: Run PolySkill with and without providing abstract class during skill induction, compare skill reusability and cross-site transfer
  3. Continual learning stress test: Start with WebArena Shopping library, sequentially learn Amazon and Target tasks, monitor held-out WebArena task performance after each update

## Open Questions the Paper Calls Out

### Open Question 1
How can agents automatically repair obsolete skills in highly dynamic web environments without requiring costly, full re-induction cycles? The current framework relies on re-running the induction pipeline when UI changes break skills, which is inefficient. Evidence would be a mechanism that performs differential analysis to apply minimal modifications to broken skills, restoring functionality without re-learning from scratch.

### Open Question 2
How can reinforcement learning reward functions be designed to explicitly encourage skill abstraction and generalization rather than mere task-specific specialization? Standard RL rewards maximize task success, often resulting in over-specialized policies. Evidence would be an RL agent that autonomously learns a polymorphic skill hierarchy with higher transfer rates than prompting-based methods.

### Open Question 3
How can polymorphic frameworks effectively cover "long-tail" websites that blend functionalities from multiple domains rather than fitting a single category? The current hierarchical structure assumes a single dominant domain, struggling with multi-faceted platforms. Evidence would be a method allowing skills to inherit from or compose multiple abstract classes to handle cross-domain websites.

## Limitations
- Automatic induction of high-quality abstract classes from limited sites is uncertain, risking error propagation
- Performance on truly novel domains with no matching abstract class is not thoroughly characterized
- Scalability to hundreds of diverse domains and maintenance of growing abstract class hierarchies remains unclear

## Confidence
- **High confidence:** Core claim that polymorphic abstraction improves skill reusability and cross-domain transfer, supported by direct experimental evidence
- **Medium confidence:** Catastrophic forgetting mitigation—limited sample size (2 new sites) and other factors may contribute to stability
- **Medium confidence:** Task-free autonomous skill discovery—experimental validation is relatively shallow compared to task-guided setting

## Next Checks
1. **Abstract Class Quality Test:** Systematically evaluate how quality of initial abstract class (induced from 1, 3, or 5 sites) impacts cross-domain transfer performance
2. **Novel Domain Stress Test:** Evaluate PolySkill on websites from completely new domains with no matching abstract class to assess adaptation capabilities
3. **Library Growth Analysis:** Track skill library size, verification rejection rates, and inference latency as number of concrete implementations per abstract class grows to quantify practical scalability limits