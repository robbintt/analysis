---
ver: rpa2
title: 'Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay Assessment'
arxiv_id: '2509.15926'
source_url: https://arxiv.org/abs/2509.15926
tags:
- prediction
- essay
- conformal
- asap
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work integrates conformal prediction with fine-tuned open-source
  LLMs to produce calibrated essay scores with set-valued outputs. Two models (Llama-3
  8B, Qwen-2.5 3B) are fine-tuned on three corpora and calibrated to meet 90% coverage.
---

# Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay Assessment

## Quick Facts
- arXiv ID: 2509.15926
- Source URL: https://arxiv.org/abs/2509.15926
- Reference count: 6
- This work integrates conformal prediction with fine-tuned open-source LLMs to produce calibrated essay scores with set-valued outputs, achieving 90% coverage with UAcc rewards for both correctness and conciseness.

## Executive Summary
This paper presents the first application of conformal prediction to automated essay scoring (AES), using fine-tuned open-source LLMs to generate prediction sets with formal coverage guarantees. Two models (Llama-3 8B, Qwen-2.5 3B) are trained on three corpora and calibrated to meet 90% coverage, with Llama-3 achieving highest QWK and tightest prediction sets. The approach introduces UAcc, an uncertainty-aware accuracy metric that rewards models for being both correct and concise, reducing the number of essays flagged for human review. This work demonstrates that mid-sized, open-source LLMs can deliver trustworthy, calibrated essay scoring suitable for high-stakes assessment.

## Method Summary
The method fine-tunes Llama-3 8B and Qwen-2.5 3B models with 4-bit quantization and LoRA adapters on essay corpora (ASAP, TOEFL11, FCE) using an instruction prompt to output single scores. Models are trained 8 epochs with AdamW (lr=1e-5), then calibrated using conformal prediction with the LAC score (1 - p̂(y|x)) to compute thresholds ensuring 90% coverage. Prediction sets are constructed from labels meeting the threshold, and performance is evaluated using coverage, average set size, UAcc, QWK, accuracy, and F1.

## Key Results
- Llama-3 8B achieves highest QWK and tightest prediction sets (2.7 labels on ASAP, ~2 on TOEFL11/FCE)
- UAcc rewards models for being both correct and concise, reducing flagged essays for human review
- First AES study to combine conformal prediction and UAcc, showing mid-sized open-source LLMs deliver trustworthy calibrated scoring
- Coverage meets 90% target while maintaining operational efficiency through prediction set size control

## Why This Works (Mechanism)

### Mechanism 1
Conformal prediction converts point predictions into prediction sets with formal coverage guarantees. The LAC (least-ambiguous classifier) score s(x,y) = 1 - p̂(y|x) ranks labels by model confidence. A threshold q_α computed from the (1-α) quantile of calibration scores determines set membership: C_α(x) = {y ∈ Y | s(x,y) ≤ q_α}. Core assumption: Calibration and test data are exchangeable (i.i.d. or near-i.i.d.). Break condition: Topic drift, demographic shift, or language transfer effects between calibration and deployment.

### Mechanism 2
UAcc provides a single metric rewarding models that are both correct and concise. UAcc = Accuracy × √(K / avg.|C_α(x)|) rescales accuracy by prediction set compactness, penalizing overly cautious models. Core assumption: Smaller prediction sets correlate with practical utility in human-in-the-loop systems. Break condition: When set size reduction doesn't translate to reduced human review burden.

### Mechanism 3
Fine-tuned LLMs produce meaningful probability distributions over ordinal score labels. Instruction prompt ("Read the essay and output a single score:") conditions the model; final token softmax over score tokens (integers or band tokens) yields p̂(y|x) for conformal calibration. Core assumption: The LLM's probability mass over score tokens reflects calibrated uncertainty about essay quality. Break condition: Prompt variations, score tokenization changes, or distribution shift degrade probability calibration.

## Foundational Learning

- Concept: Conformal Prediction
  - Why needed here: Core technique enabling the paper's coverage guarantees; requires understanding of quantile-based thresholding
  - Quick check question: Given calibration LAC scores [0.05, 0.12, 0.18, 0.31, 0.44], what threshold q_α ensures 80% coverage (α=0.2)?

- Concept: Exchangeability Assumption
  - Why needed here: Coverage guarantees depend critically on this assumption; real deployment must detect violations
  - Quick check question: If training essays come from US high schoolers but deployment includes ESL writers, is exchangeability likely to hold?

- Concept: Ordinal vs Categorical Classification
  - Why needed here: Essay scores have natural ordering; prediction sets of {3, 4} are more useful than {2, 9} for same cardinality
  - Quick check question: Why does QWK penalize "larger score discrepancies more heavily than near misses"?

## Architecture Onboarding

- Component map: Fine-tuned LLM -> LoRA adapter -> Calibration set processor -> Prediction set constructor -> UAcc evaluator

- Critical path:
  1. Split data 70/15/15 (train/calibration/test)—calibration set must not influence weights
  2. Fine-tune with instruction prompt + score token as single output
  3. Extract p̂(y|x) on calibration set, compute LAC scores, determine qα at (1-α) quantile
  4. At inference: construct Cα(x) from labels with s(x,y) ≤ qα
  5. Evaluate: coverage ≥ 1-α, average set size, UAcc

- Design tradeoffs:
  * Model size vs inference cost: Llama-3 8B yields tighter sets; Qwen-2.5 3B viable under hardware constraints
  * Coverage vs set size: Higher coverage (lower α) yields larger prediction sets
  * Label granularity: 11-way ASAP produces sets ~2.7 labels; 3-way TOEFL11/FCE produces sets ~1.3-1.7 labels

- Failure signatures:
  * Coverage < 90% → exchangeability violated, calibration set too small, or model probability miscalibrated
  * Average set size approaching K → model over-uncertain or threshold computation error
  * Large accuracy-F1 gap → class imbalance exploitation (check per-class performance)
  * FCE QWK unusually low → expected per section 4.6 (40-point scale post-hoc mapping, small corpus)

- First 3 experiments:
  1. Replicate ASAP Prompt 1 with Llama-3 8B: target QWK ≈ 0.80, coverage ≈ 0.91, avg.|C| ≈ 2.7
  2. Ablate α ∈ {0.05, 0.10, 0.15, 0.20}: plot coverage vs average set size tradeoff curve
  3. Test cross-corpus generalization: calibrate on ASAP, test on TOEFL11 subset to probe exchangeability robustness

## Open Questions the Paper Calls Out

- At what model scale do accuracy and UAcc exhibit diminishing returns for uncertainty-calibrated essay scoring?
- Do ordinal-aware conformal scores tighten prediction sets while preserving coverage?
- Can rubric-aware prompting based on essay characteristics reduce prediction set size without sacrificing coverage?
- How robust is conformal coverage to exchangeability violations from topic drift or demographic shifts in live exams?

## Limitations

- Calibration generalization remains unproven across different corpora with varying scoring distributions
- Critical implementation details including LoRA configuration and prompt templates are unspecified
- Limited empirical evidence that prediction set size reduction actually translates to reduced human review burden

## Confidence

- High Confidence: Core conformal prediction mechanism and Llama-3 8B outperforming Qwen-2.5 3B on QWK
- Medium Confidence: Mid-sized open-source LLMs delivering trustworthy calibrated scoring, operational significance of UAcc
- Low Confidence: Claim of being "first AES study to combine conformal prediction and UAcc"

## Next Checks

1. **Cross-Corpus Exchangeability Test**: Calibrate on ASAP Prompt 1 and evaluate on TOEFL11 test set to quantify performance degradation when exchangeability assumptions are violated.

2. **Prompt Sensitivity Analysis**: Systematically vary the instruction prompt across multiple formulations to determine how prompt sensitivity affects probability calibration and set size.

3. **Human Review Burden Validation**: Conduct a user study where human raters evaluate prediction sets of varying sizes and compositions to empirically validate whether UAcc's assumption holds.