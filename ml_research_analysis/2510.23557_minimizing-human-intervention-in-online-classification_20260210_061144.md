---
ver: rpa2
title: Minimizing Human Intervention in Online Classification
arxiv_id: '2510.23557'
source_url: https://arxiv.org/abs/2510.23557
tags:
- regret
- expert
- convex
- logd
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an online classification problem with costly
  expert access, motivated by question-answering systems. The proposed Conservative
  Hull-based Classifier (CHC) maintains convex hulls of expert-labeled queries and
  calls the expert only when new queries fall outside these hulls.
---

# Minimizing Human Intervention in Online Classification

## Quick Facts
- arXiv ID: 2510.23557
- Source URL: https://arxiv.org/abs/2510.23557
- Reference count: 40
- Primary result: GHC reduces expert calls significantly on LLM embeddings while maintaining accuracy.

## Executive Summary
This paper introduces an online classification framework where an agent must classify high-dimensional queries while minimizing costly expert interventions. The key insight is that classification accuracy and expert reliance can be traded off through geometric structure: maintaining convex hulls of known points and calling the expert only when new queries fall outside these hulls. The proposed Generalized Hull-based Classifier (GHC) introduces a tunable threshold parameter that enables aggressive guessing when class regions are well-separated, dramatically reducing expert calls in high-dimensional LLM embedding spaces. Theoretical regret bounds show O(log^d T) scaling for large horizons, with a Center-based Classifier (CC) recommended for shorter horizons.

## Method Summary
The method maintains convex hulls of expert-labeled queries for each class. For each new query, it checks if the point lies within any known hull - if yes, it classifies immediately; if no, it calls the expert. GHC extends this by allowing guesses when a query is "close enough" to one hull relative to others, controlled by threshold τ. CC estimates class centers and classifies based on nearest neighbor to these centers. The framework handles partial observability by never observing feedback when guessing.

## Key Results
- GHC significantly outperforms baselines on real-world datasets (QQG, ComQA, CQADupStack) using LLM embeddings
- Optimal τ found around 0.9 for high-dimensional spaces, balancing expert calls vs. error penalties
- CHC achieves O(log^d T) regret matching minimax lower bounds in dimension one
- CC performs well for short horizons under subgaussian mixture assumptions

## Why This Works (Mechanism)

### Mechanism 1: Conservative Geometry Exploitation (CHC)
- **Claim:** CHC guarantees zero classification error by maintaining strict geometric boundaries around known classes, at the cost of potentially high expert reliance.
- **Mechanism:** The algorithm maintains convex hulls for each class based on expert-labeled points. If a new query falls strictly within a known hull, it is classified immediately. If it falls outside all hulls, the system calls the expert. This leverages the geometric property that if class regions are convex polytopes, a point inside a hull constructed from samples of that class must belong to it.
- **Core assumption:** The true class regions are convex polytopes, and expert labeling is deterministic based on these regions.
- **Break condition:** If class regions are non-convex or intersecting, the geometric inclusion property fails, potentially leading to misclassification or excessive expert calls.

### Mechanism 2: Aggressive Relative Thresholding (GHC)
- **Claim:** In high dimensions where convex hulls are sparse, a tunable threshold enables "aggressive guessing" to reduce expert intervention while maintaining high accuracy, provided classes are sufficiently separated.
- **Mechanism:** GHC extends CHC by guessing a label if the query is "close enough" to one hull relative to others, specifically if the distance to hull i is less than a fraction τ of the distance to the nearest competing hull.
- **Core assumption:** The query distribution is concentrated such that the distance to the correct class hull is significantly smaller than distances to incorrect class hulls.
- **Break condition:** If classes are dense and overlapping, aggressive guessing (τ > 0) leads to frequent misclassification, increasing regret due to the high penalty (γ) for wrong guesses.

### Mechanism 3: Subgaussian Center Estimation (CC)
- **Claim:** For short time horizons where geometric hulls cannot be reliably formed, estimating class centers is effective if the data follows a subgaussian mixture.
- **Mechanism:** CC calls the expert until enough samples are gathered to estimate the center of each class. It then classifies queries by assigning them to the label of the nearest estimated center.
- **Core assumption:** The distribution is a mixture of subgaussian distributions with well-separated means.
- **Break condition:** If the distribution is uniform or does not cluster around distinct centers, CC performs poorly and may incur high regret.

## Foundational Learning

- **Concept: Convex Hulls & Membership Testing**
  - **Why needed here:** The core operation of CHC is determining if a point q lies inside the convex hull of a set of points Q. In high dimensions, this is solved via Linear Programming.
  - **Quick check question:** Can you implement a check to determine if a 3D point is inside a tetrahedron defined by 4 vertices?

- **Concept: Regret Minimization in Bandits**
  - **Why needed here:** The paper frames the problem as minimizing "Voronoi Regret"—the difference in reward compared to an oracle that knows the labels. Understanding the trade-off between exploration (calling the expert) and exploitation (guessing) is essential.
  - **Quick check question:** If an expert call costs -1 and a wrong guess costs -10, what is the maximum probability of error you can accept to justify guessing instead of asking the expert?

- **Concept: The Curse of Dimensionality**
  - **Why needed here:** Theoretical results show CHC regret scales as O(log^d T), and hulls are sparse until T is exponential in d. This explains why CHC fails in high-dim LLM spaces without the GHC modification.
  - **Quick check question:** Why does the volume of a convex hull grow slowly relative to the volume of the embedding space as dimension d increases?

## Architecture Onboarding

- **Component map:**
  Input embedding q_t → Distance calculation to all N hulls/centers → Threshold comparison (τ) → Decision (Guess/Expert/Reject) → State update (if Expert)

- **Critical path:**
  1. Receive embedding q_t
  2. Compute distances/membership for all N classes against current state
  3. Identify the "closest" or "containing" class
  4. Apply threshold logic (τ) to decide: Guess, Call Expert, or Reject
  5. If Expert, update state; if Guess, observe no feedback

- **Design tradeoffs:**
  - Conservative vs. Aggressive: CHC (τ=0) guarantees no classification error but may call the expert 100% of the time in high dimensions. GHC (τ > 0) risks the penalty γ for wrong guesses to reduce expensive expert calls.
  - Computational Cost: Computing exact convex hulls is exponential; the paper uses LP/QP for membership/distance checks, which are polynomial per query.

- **Failure signatures:**
  - Excessive Expert Calls (CHC): If the system queries the expert almost every round, dimension d is likely too high for sample size T to form meaningful hulls.
  - High Misclassification (GHC): If regret spikes due to penalties (γ), threshold τ is too aggressive for the data separation.
  - Center Drift (CC): If accuracy drops over time in CC, initial center estimates were poor or separation assumption was violated.

- **First 3 experiments:**
  1. Dimensionality Stress Test: Run CHC on synthetic data with increasing dimensions d ∈ {2, 10, 50} to observe exponential increase in expert calls.
  2. Threshold Tuning (GHC): Benchmark GHC on a real dataset sweeping τ ∈ [0, 1] to find the "sweet spot" that minimizes total regret.
  3. Algorithm Switching: Implement a hybrid system that uses CC for the first T ≤ e^d rounds and switches to GHC afterward.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can theoretical regret bounds be derived for GHC when the threshold τ > 0?
- Basis in paper: [explicit] The conclusion states it is a "natural next step is to provide a theoretical regret analysis of this latter algorithm."
- Why unresolved: As detailed in Appendix E.6, GHC involves a history-dependent decision process where hulls are formed from non-i.i.d. queries, preventing the application of standard random polytope theory used for CHC.
- What evidence would resolve it: A mathematical proof bounding GHC's regret with respect to T and τ, or a demonstration of specific instances where the regret is unbounded.

### Open Question 2
- Question: Can the sharp, density-free regret bound established for CHC in dimension d=1 (Theorem 4.3) be generalized to dimensions d ≥ 2?
- Basis in paper: [explicit] Section 4.1 asks "Whether this density-free bound can be generalized to dimension d ≥ 2 is unclear," referencing further discussion in Appendix E.4.
- Why unresolved: Appendix E.4 demonstrates that the Rosenblatt transformation, which maps variables to a uniform distribution, fails to preserve the convex hull properties required for the proof in higher dimensions.
- What evidence would resolve it: A proof extending the density-free bound to higher dimensions or a counterexample showing such a generalization is impossible.

### Open Question 3
- Question: Can the dependence on the density ratio C/c in the CHC regret upper bounds be removed?
- Basis in paper: [explicit] In the proof of Theorem 4.1 (Section 4.1), the authors "conjecture that this dependence could be removed with a tighter analysis."
- Why unresolved: The factor arises from a rejection sampling argument used in the current proof technique, which appears to be an artifact rather than a fundamental limitation of the algorithm.
- What evidence would resolve it: A refined analysis of Theorem 4.1 that bounds regret strictly in terms of T and d, independent of the bounds on the query density f_μ.

## Limitations

- Theoretical guarantees rely on strong geometric assumptions about class regions being convex polytopes, which may not hold for real-world embeddings
- Experimental validation is limited to three datasets with no ablation studies on embedding quality or dataset size impact
- Computational cost of LP/QP solvers for distance-to-hull in high dimensions is not quantified
- Subgaussian mixture assumption for CC is particularly restrictive and likely violated in heterogeneous datasets

## Confidence

- **High confidence:** The regret bounds for CHC under convexity assumptions are theoretically sound; the geometric mechanism is correctly described
- **Medium confidence:** GHC's empirical performance gains on LLM embeddings are promising but rely on unstated properties of the embedding space
- **Low confidence:** The applicability of CC to real-world short horizons without further distributional assumptions; the practical impact of dimensionality on hull sparsity without GHC

## Next Checks

1. **Distributional Robustness Test:** Evaluate GHC on datasets where class regions are known to be non-convex (e.g., concentric rings, XOR patterns) to quantify performance degradation
2. **Embedding Sensitivity Analysis:** Compare GHC's regret across different embedding models (Nomic, E5, Mistral_E5) on the same dataset to isolate the impact of embedding quality vs. algorithm design
3. **Computational Scalability Benchmark:** Measure the runtime of LP/QP distance-to-hull calculations as a function of dimension d and sample size |Q_{i,t}| to assess practical limits