---
ver: rpa2
title: 'A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware
  Training and Post-Training Quantization Strategies'
arxiv_id: '2511.03201'
source_url: https://arxiv.org/abs/2511.03201
tags:
- quantization
- detection
- training
- botnet
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of deploying deep learning models
  for IoT botnet detection on resource-constrained devices. It proposes a VAE-MLP
  framework where a variational autoencoder compresses high-dimensional network traffic
  data into 8-dimensional latent vectors, which are then classified by a multi-layer
  perceptron.
---

# A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies

## Quick Facts
- arXiv ID: 2511.03201
- Source URL: https://arxiv.org/abs/2511.03201
- Reference count: 40
- This study systematically compares PTQ and QAT for IoT botnet detection, finding PTQ achieves 6× speedup and 21× compression with minimal accuracy loss.

## Executive Summary
This paper addresses the challenge of deploying deep learning models for IoT botnet detection on resource-constrained devices. It proposes a VAE-MLP framework where a variational autoencoder compresses high-dimensional network traffic data into 8-dimensional latent vectors, which are then classified by a multi-layer perceptron. The research systematically evaluates two quantization strategies—Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ)—on two benchmark IoT botnet datasets (N-BaIoT and CICIoT2022). Results show that PTQ achieves superior speed-accuracy trade-offs compared to QAT for this application.

## Method Summary
The method involves training a variational autoencoder with an 8-dimensional latent space on network traffic features, then using the encoder to transform high-dimensional data into compact latent representations. An MLP classifier is trained on these latent vectors. Two quantization approaches are then applied: PTQ converts the FP32 model to INT8 post-training using calibration, while QAT incorporates fake quantization nodes during training. Both quantized models are converted to TensorFlow Lite format and evaluated for accuracy, model size, and inference latency on test datasets.

## Key Results
- PTQ achieved 6× speedup and 21× reduction in model size with minimal accuracy loss compared to the original FP32 model
- QAT provided higher compression (24×) but suffered greater performance degradation than PTQ
- The VAE-MLP architecture successfully reduced input dimensionality while maintaining discriminative power for botnet detection

## Why This Works (Mechanism)

### Mechanism 1: VAE Latent Space Compression
Reducing input dimensionality via VAE encoding before classification lowers computational load while preserving discriminative features. A pretrained VAE encoder projects high-dimensional network traffic features into an 8-dimensional latent space. The MLP classifier then operates on these compact representations, reducing FLOPs, memory accesses, and parameter count relative to raw-input classifiers.

### Mechanism 2: Post-Training Quantization Preserves Learned Weights
PTQ converts FP32 weights/activations to INT8 after training with minimal accuracy loss because learned weight structure is largely robust to low-bit rounding. A fully-trained FP32 model is quantized via calibration without retraining. The quantization noise is applied to already-converged weights, so decision boundaries shift only marginally if the original model's confidence margins are wide.

### Mechanism 3: Quantization-Aware Training Introduces Training-Phase Noise
QAT achieves higher compression but can degrade accuracy more than PTQ due to quantization noise injected during optimization. Fake quantization nodes simulate INT8 rounding in the forward pass during training. The optimizer adapts weights to noise, but noise can distort feature learning and gradient estimates, especially in small or sensitive models.

## Foundational Learning

- Concept: Variational Autoencoder (VAE) latent representation
  - Why needed here: Understand how the encoder compresses traffic features and what "8-dimensional latent vectors" implies for information preservation
  - Quick check question: Can you explain why a VAE encoder might preserve class-relevant structure even though it is trained for reconstruction, not classification?

- Concept: Integer quantization (FP32 to INT8) with scale and zero-point
  - Why needed here: PTQ and QAT both rely on affine integer mapping; understanding the formula is necessary to debug accuracy or latency regressions
  - Quick check question: Given a weight tensor, can you compute an INT8 quantization using a scale and zero-point, and identify which values would saturate?

- Concept: Trade-off between model size, latency, and accuracy under quantization
  - Why needed here: The paper's core claim is that PTQ gives better speed/accuracy trade-offs than QAT for this task; you must interpret such trade-offs critically
  - Quick check question: If PTQ gives 6× speedup with 0.1% accuracy drop and QAT gives 24× compression with 2% drop, how would you decide which to deploy on a battery-powered gateway?

## Architecture Onboarding

- Component map: Raw traffic features -> VAE encoder -> 8-D latent vectors -> MLP classifier -> Botnet/benign prediction
- Critical path: 1) Train VAE on traffic data → extract encoder 2) Generate latent vectors for training data 3) Train MLP classifier on latent vectors 4) Apply PTQ or QAT 5) Measure accuracy, storage, and latency on held-out test data
- Design tradeoffs: PTQ offers faster pipeline and better accuracy retention but slightly larger model; QAT provides smaller final model but longer training and potential accuracy loss
- Failure signatures: Large accuracy drop after PTQ → likely outlier weights or poor calibration; QAT model diverges → learning rate or quantization parameters may be incompatible; Inference not accelerated → runtime may not support INT8 kernels
- First 3 experiments: 1) Reproduce PTQ vs. QAT accuracy gap on a small subset of N-BaIoT 2) Ablate latent dimension (4, 8, 16, 32) to quantify information vs. compression trade-offs 3) Compare calibration strategies (random subset vs. class-balanced vs. hard examples) for PTQ

## Open Questions the Paper Calls Out

- Why does Quantization-Aware Training cause greater accuracy degradation than Post-Training Quantization in the VAE-MLP architecture? The paper offers hypotheses but provides no ablation studies isolating the mechanism.
- How does quantization performance vary across different bit widths (e.g., INT4, FP16, mixed-precision) beyond INT8? The study restricts evaluation to INT8 without exploring lower bit widths.
- Does the quantized model maintain reported performance gains when deployed on actual resource-constrained IoT hardware? All benchmarks occur in simulated environments without physical IoT device constraints.
- How does VAE latent space dimensionality affect quantization robustness? The methodology fixes the latent dimension at 8 without justification or sensitivity analysis.

## Limitations
- VAE and MLP architectures are underspecified, limiting exact reproducibility
- No analysis of calibration dataset quality for PTQ, which could affect results
- Results are only shown on two IoT botnet datasets, limiting generalizability
- Latent dimension choice (8-D) was not ablated to validate its sufficiency

## Confidence
- High confidence: PTQ achieves better speed/accuracy trade-off than QAT for this VAE-MLP setup
- Medium confidence: VAE-based dimensionality reduction preserves class-relevant features for botnet detection
- Low confidence: Generalizability of PTQ superiority to other tasks or larger architectures

## Next Checks
1. Conduct latent dimension ablation study (4, 8, 16, 32 dimensions) to quantify information loss vs. compression trade-offs
2. Compare PTQ accuracy using different calibration sampling strategies (random, class-balanced, hard examples)
3. Apply the same VAE-MLP + PTQ pipeline to a non-IoT network intrusion detection dataset to assess external validity