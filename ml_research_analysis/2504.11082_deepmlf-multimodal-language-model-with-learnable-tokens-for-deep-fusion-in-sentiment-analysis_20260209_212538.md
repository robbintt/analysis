---
ver: rpa2
title: 'DeepMLF: Multimodal language model with learnable tokens for deep fusion in
  sentiment analysis'
arxiv_id: '2504.11082'
source_url: https://arxiv.org/abs/2504.11082
tags:
- fusion
- multimodal
- language
- deepmlf
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DeepMLF, a novel multimodal language model
  architecture designed for sentiment analysis that achieves state-of-the-art performance
  through deep fusion across multiple layers. The key innovation is the use of learnable
  fusion tokens that progressively accumulate multimodal information, enabling deeper
  fusion than previous approaches while maintaining independent information flow for
  each modality.
---

# DeepMLF: Multimodal language model with learnable tokens for deep fusion in sentiment analysis

## Quick Facts
- arXiv ID: 2504.11082
- Source URL: https://arxiv.org/abs/2504.11082
- Reference count: 40
- This paper introduces DeepMLF, a novel multimodal language model architecture achieving state-of-the-art performance in sentiment analysis through deep fusion across multiple layers.

## Executive Summary
DeepMLF introduces a novel multimodal language model architecture that achieves state-of-the-art performance in sentiment analysis through deep fusion across multiple layers. The key innovation is the use of learnable fusion tokens that progressively accumulate multimodal information, enabling deeper fusion than previous approaches while maintaining independent information flow for each modality. The model was evaluated on three multimodal sentiment analysis benchmarks (MOSEI, MOSI, SIMS) and consistently outperformed previous state-of-the-art methods.

## Method Summary
DeepMLF is a multimodal language model that augments a pretrained language model with audiovisual features processed through a dedicated encoder, integrating them via gated cross-attention mechanisms across multiple decoder layers. The architecture appends learnable fusion tokens to the language model input, which gather linguistic information via causal self-attention and integrate audiovisual features through gated cross-attention in dedicated multimodal blocks. These MM blocks are inserted at specific layers throughout the language model, enabling progressive multimodal fusion at different levels of linguistic abstraction. The model uses a progressive learning curriculum with separate pretraining of the audiovisual encoder and various regularization techniques including SeqAug and language model loss to prevent overfitting and catastrophic forgetting.

## Key Results
- Achieves state-of-the-art performance on MOSEI, MOSI, and SIMS benchmarks with optimal fusion depths of 5-7 layers
- Optimal fusion token count is 8-20, with performance degrading when using too few or too many tokens
- Significant improvements over previous state-of-the-art methods: 1.92% Acc-2 improvement on MOSEI, 12.63% relative improvement on MOSI, and 3.38% Acc-2 improvement on SIMS
- Ablation studies confirm effectiveness of fusion design, loss terms, and language embedding regularization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Learnable fusion tokens provide dedicated capacity for accumulating multimodal information while preserving independent modality flows.
- **Mechanism:** Fusion tokens gather linguistic information via causal self-attention in frozen LM blocks, then integrate audiovisual features through gated cross-attention in MM blocks. This separation prevents language dominance and enables progressive accumulation across layers.
- **Core assumption:** Restricting cross-modal attention to fusion tokens alone (not language tokens) improves fusion quality by maintaining controlled information flow.
- **Evidence anchors:**
  - [abstract] "append learnable tokens to the LM that: 1) capture modality interactions in a controlled fashion and 2) preserve independent information flow for each modality"
  - [section 6.3.1] Table 10 shows (Xf, Z) + FFW achieves 82.75 Acc-2 vs 81.14 when using language tokens instead
  - [corpus] Limited external validation; related works (DashFusion, PSA-MF) address fusion but don't validate the specific token-based mechanism
- **Break condition:** If fusion tokens >20, performance degrades (Figure 3); if tokens removed entirely, performance drops significantly (Table 10, row 3).

### Mechanism 2
- **Claim:** Deeper fusion (5-7 layers) enables more effective multimodal integration than typical 3-layer approaches.
- **Mechanism:** Multiple MM blocks at different LM layers allow fusion tokens to progressively refine multimodal representations at varying levels of linguistic abstraction.
- **Core assumption:** Shallow LM layers encode low-level linguistic features that don't integrate well with audiovisual signals; deeper layers capture more semantic information suitable for fusion.
- **Evidence anchors:**
  - [abstract] "optimal fusion depths (5-7) exceeding those of existing approaches"
  - [section 6.2.1] Table 6 shows 7 MM blocks optimal for SIMS (0.353 MAE vs 0.374 with 5 blocks); Table 7 shows depth decreases (7→6→5) as model size increases
  - [corpus] No direct validation; related fusion works don't systematically examine depth
- **Break condition:** Performance plateaus or degrades beyond optimal depth (Table 6 shows 9 blocks worse than 7).

### Mechanism 3
- **Claim:** Progressive representation learning (fusion curriculum) with regularization prevents overfitting and catastrophic forgetting.
- **Mechanism:** Pretraining AV encoder separately, then fine-tuning within DeepMLF, combined with LM loss and SeqAug embedding augmentation, creates a learning curriculum that builds representations incrementally.
- **Core assumption:** Joint learning from scratch underperforms sequential unimodal→audiovisual→multimodal learning.
- **Evidence anchors:**
  - [section 6.2.3] Table 8: Pre&Tune achieves 87.15 Acc-2 vs 86.59 for Random&Tune on MOSEI; SIMS shows larger gap (82.75 vs 80.43)
  - [section 6.3.2] Table 11: Removing Lf or LLM (Tier-1 losses) causes largest performance drops
  - [section 6.3.3] Table 12: SeqAug outperforms noise injection (82.75 vs 81.05 Acc-2)
  - [corpus] No external validation of this specific curriculum
- **Break condition:** If language distribution differs significantly from pretraining corpus (e.g., MOSI), LM loss provides no benefit (λLM=0.0 in Table 1).

## Foundational Learning

- **Transformer attention mechanisms (self-attention, cross-attention, causal masking)**
  - Why needed here: DeepMLF relies on causal self-attention in LM blocks and cross-attention in MM blocks; understanding query/key/value operations is essential.
  - Quick check question: Can you explain why cross-attention uses fusion tokens as queries and audiovisual features as keys/values?

- **Autoregressive language modeling and decoder-only architectures**
  - Why needed here: The pretrained LM backbone is a decoder model; understanding how it processes sequential input and why it's frozen matters for architecture comprehension.
  - Quick check question: Why would freezing the LM blocks prevent catastrophic forgetting?

- **Gating mechanisms in neural networks**
  - Why needed here: Sigmoid gating in MM blocks controls information flow; understanding initialization (0.5) vs alternatives (tanh at 0.0) explains performance differences.
  - Quick check question: What happens if gating is removed entirely? (Hint: Table 13 shows 86.86 vs 87.15 Acc-2)

## Architecture Onboarding

- **Component map:**
  AV Encoder (modality-specific transformers + Fusion FFW) → LM Blocks (frozen pretrained decoders) → MM Blocks (gated cross-attention + FFW) → Pooling → Task head → prediction

- **Critical path:**
  Input features → AV Encoder → Z (audiovisual representation)
  Language tokens + fusion tokens → LM Block (causal attention) → MM Block (cross-attention with Z) → [repeat for N fusion layers] → Pooling → Task head → prediction

- **Design tradeoffs:**
  - Fusion depth vs compute: More MM blocks = better performance but higher cost
  - Token count vs capacity: Too few tokens limit multimodal capacity; too many degrade performance
  - LM size vs fusion depth: Larger models need fewer fusion layers (Table 7: base=7, large=5)
  - Frozen vs fine-tuned LM: Freezing reduces parameters but limits adaptation

- **Failure signatures:**
  - Performance plateaus at ~80-82 Acc-2: Likely using only 2-3 fusion layers (check MM block count)
  - Catastrophic forgetting (language-only performance drops): Missing LM loss or SeqAug regularization
  - Language dominance (fusion provides minimal improvement over text-only): Check fusion token count (may be too few) or encoder initialization (Random&Tune underperforms)
  - Overfitting on small datasets: Reduce fusion depth, increase regularization, use LoRA for FFW layers

- **First 3 experiments:**
  1. **Baseline validation:** Reproduce DeepMLF on MOSEI with GPT2-base, 5 MM blocks, 12 fusion tokens. Target: ~85.8 Acc-2 (Table 4). If significantly lower, check feature extraction pipeline.
  2. **Ablation sweep:** Systematically remove each component (LM loss, SeqAug, auxiliary losses, gating) to understand sensitivity. Use Table 11/12 as reference for expected degradation magnitudes.
  3. **Depth scaling study:** Test fusion depths 3, 5, 7, 9 on your target dataset to find optimal configuration before committing to full training runs. Use Figure 3 pattern as guide.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DeepMLF architecture be effectively adapted for self-supervised multimodal language modeling?
- Basis: [explicit] The authors identify applying the approach to self-supervised setups as a promising direction to establish a new architectural paradigm.
- Why unresolved: The current study is limited to supervised Multimodal Sentiment Analysis (MSA) tasks.
- What evidence would resolve it: Demonstration of DeepMLF performance on self-supervised objectives using large-scale unlabeled multimodal datasets.

### Open Question 2
- Question: How can the learnable fusion tokens function as multimodal latents within generative frameworks like diffusion models?
- Basis: [explicit] The future work section proposes integrating the method with diffusion models to utilize these tokens as multimodal latents.
- Why unresolved: The current implementation uses fusion tokens for classification tasks, not as latent variables for generative processes.
- What evidence would resolve it: Experiments showing successful conditioning of diffusion models using the DeepMLF learnable token outputs.

### Open Question 3
- Question: How do the learnable tokens behave when utilized directly in the generative process of the language model?
- Basis: [explicit] The authors state a need for "in-depth analysis and utilization of learnable tokens in the generative process."
- Why unresolved: The paper focuses on using the final representation for sentiment regression/classification rather than analyzing the tokens' role during text generation.
- What evidence would resolve it: An analysis of generated text quality and token attention patterns when the model is tasked with open-ended multimodal generation.

## Limitations

- The effectiveness of learnable fusion tokens beyond sentiment analysis tasks and specific audiovisual features needs broader validation
- Optimal fusion depth (5-7 layers) appears dataset-specific and may not generalize across different tasks or domain shifts
- Computational cost scaling with depth hasn't been thoroughly analyzed, limiting practical deployment considerations

## Confidence

**High Confidence** - The effectiveness of gated cross-attention mechanisms for multimodal fusion is well-supported by the ablation studies (Tables 10, 11, 12). The performance improvements over baseline methods are statistically significant and consistent across all three datasets.

**Medium Confidence** - The optimal fusion depth (5-7 layers) is reasonably supported within the tested datasets, but the generalizability across different tasks, model sizes, and domain shifts is uncertain.

**Low Confidence** - The specific mechanism by which learnable fusion tokens provide superior controlled interaction compared to alternative fusion approaches (e.g., late fusion, hierarchical attention) lacks external validation.

## Next Checks

1. **Cross-task generalization study**: Apply DeepMLF architecture to a different multimodal task (e.g., emotion recognition on IEMOCAP or action detection on Charades) using the same audiovisual feature extraction pipeline. Compare performance against the original sentiment analysis benchmarks to assess whether the 5-7 layer fusion depth and 8-20 token count remain optimal.

2. **Alternative fusion mechanism comparison**: Implement and compare DeepMLF against two alternative fusion approaches on MOSEI: (a) traditional late fusion with modality-specific encoders and weighted averaging, and (b) hierarchical attention fusion without learnable tokens. Use identical audiovisual features and training procedures.

3. **Computational cost analysis**: Measure training time, inference latency, and memory usage for DeepMLF configurations with different fusion depths (3, 5, 7, 9 layers) on the same hardware. Compare against a baseline late fusion approach with equivalent parameter counts.