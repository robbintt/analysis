---
ver: rpa2
title: 'Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language
  Models'
arxiv_id: '2503.17142'
source_url: https://arxiv.org/abs/2503.17142
tags:
- compositional
- embeddings
- image
- visual
- decomposable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether visual embeddings from vision-language
  models (VLMs) exhibit compositional structures analogous to those observed in text
  embeddings. The authors propose Geodesically Decomposable Embeddings (GDE), a framework
  that approximates image representations using geometry-aware compositional structures
  in the latent space, addressing challenges of noise and sparsity in visual data.
---

# Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models

## Quick Facts
- arXiv ID: 2503.17142
- Source URL: https://arxiv.org/abs/2503.17142
- Reference count: 40
- Proposes Geodesically Decomposable Embeddings (GDE) framework showing visual embeddings exhibit compositional structures comparable to text embeddings

## Executive Summary
This paper investigates whether visual embeddings from vision-language models exhibit compositional structures analogous to those observed in text embeddings. The authors propose Geodesically Decomposable Embeddings (GDE), a framework that approximates image representations using geometry-aware compositional structures in the latent space, addressing challenges of noise and sparsity in visual data. GDE achieves state-of-the-art performance in compositional classification and group robustness tasks while using minimal labeled data, demonstrating that visual embeddings can be effectively composed using Riemannian geometry operations.

## Method Summary
GDE approximates image representations by computing weighted intrinsic means on spherical manifolds and combining primitive directions via geodesic operations. The framework computes optimal primitive directions representing individual concepts and composes them through logarithmic and exponential maps on the sphere. This approach addresses noise and sparsity in visual data by leveraging the geometry of normalized embeddings, allowing compositional operations that outperform linear decomposition baselines across multiple vision tasks including classification, robustness, and generation.

## Key Results
- GDE achieves AUC ratio 317.9% over zero-shot CLIP on UT-Zappos compositional classification
- State-of-the-art group robustness performance with higher worst-group accuracy than task-specific methods using minimal labeled data
- Successfully generates novel compositional images using diffusion models from decomposed embeddings, validating semantic encoding

## Why This Works (Mechanism)
The method works by leveraging the spherical geometry of L2-normalized visual embeddings, where compositionality emerges through geodesic operations rather than linear combinations. By computing primitive directions in the tangent space and composing them via exponential maps, GDE captures the intrinsic geometry of the embedding manifold, which better represents the compositional relationships in visual concepts compared to linear approaches that ignore the spherical constraint.

## Foundational Learning
- **Riemannian Geometry on Spheres**: Needed for proper distance and composition operations on normalized embeddings; quick check: verify Exp_μ/Log_μ formulas preserve norm constraints
- **Intrinsic Mean Computation**: Required to find optimal center on manifold for noisy data; quick check: ensure convergence criteria (ε=10^-5) are met
- **Tangent Space Mapping**: Essential for linear operations on manifold; quick check: verify Log_μ outputs lie in tangent space
- **Noise Distribution Modeling**: Critical for handling sparse visual data; quick check: validate temperature t produces reasonable probability distributions
- **Geodesic Composition**: Core operation for combining primitive concepts; quick check: ensure composed vectors remain on sphere

## Architecture Onboarding
**Component Map**: CLIP embedding extraction -> Spherical manifold projection -> Intrinsic mean computation -> Tangent space mapping -> Primitive direction computation -> Geodesic composition -> Classification/generation

**Critical Path**: Image embedding → spherical projection → primitive decomposition → composition → task-specific output (classification or generation)

**Design Tradeoffs**: Spherical geometry provides geometric fidelity but limits to normalized embeddings; geodesic composition is more accurate than linear but computationally heavier; noise modeling improves robustness but requires hyperparameter tuning

**Failure Signatures**: LDE outperforming GDE indicates incorrect spherical geometry implementation; numerical instability suggests improper handling of edge cases in exponential maps; poor performance on sparse datasets suggests insufficient data per primitive

**First Experiments**: 1) Verify spherical geometry operations preserve norms and distances; 2) Test intrinsic mean computation on synthetic spherical data; 3) Compare GDE composition vs linear baseline on simple synthetic concepts

## Open Questions the Paper Calls Out
None

## Limitations
- Framework relies on spherical geometry assumptions that may not generalize to unnormalized visual features or other VLMs
- Noise distribution model specific to CLIP's architecture may require adaptation for different models
- Assumes primitive directions are fixed across tasks, which may not hold for all compositional scenarios
- Effectiveness diminishes with insufficient training data per concept, as seen on MIT-States

## Confidence
- **High confidence**: Compositional classification performance and group robustness results
- **Medium confidence**: Generalizability to other VLMs and semantic meaningfulness of generated images
- **Medium confidence**: Comparison to LDE baseline is solid but doesn't explore broader compositional modeling approaches

## Next Checks
1. Evaluate GDE on additional VLMs beyond CLIP (e.g., Flamingo, BLIP) to verify spherical compositionality assumption
2. Systematically vary number of images k per composite concept and noise temperature t to establish sensitivity ranges
3. Implement and compare against state-of-the-art compositional models like VisProg to benchmark relative performance