---
ver: rpa2
title: 'From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language
  Model Pretraining'
arxiv_id: '2510.06548'
source_url: https://arxiv.org/abs/2510.06548
tags:
- scaling
- training
- growth
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how effective it is to reuse an already pretrained
  language model (the "base model") for further pretraining, either by continuing
  training on new data or by making the model bigger. The main finding is that the
  more the base model has already been trained (the larger its first-stage token count
  D1), the less additional benefit comes from further training (second-stage tokens
  D2).
---

# From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining

## Quick Facts
- arXiv ID: 2510.06548
- Source URL: https://arxiv.org/abs/2510.06548
- Reference count: 40
- Key finding: The scaling exponent for second-stage pretraining decreases logarithmically with first-stage token count, indicating diminishing returns from reusing overtrained base models.

## Executive Summary
This paper investigates the scaling behavior of language models when reusing already pretrained base models through either continual pretraining (CPT) on new data or model growth strategies. The authors discover a fundamental saturation effect: as the base model becomes more extensively pretrained (higher D1 token count), the effectiveness of further pretraining (D2 tokens) diminishes. This is captured by a scaling law where the exponent for D2 decreases logarithmically with D1. The findings provide practical guidance for when it's more efficient to train from scratch rather than reuse an overtrained model, with implications for computational resource allocation in large-scale pretraining.

## Method Summary
The study uses LLaMA-style decoder-only architectures with SwiGLU activations and rotary positional embeddings. Base models are pretrained on Slimpajama-DC CommonCrawl corpus (368B tokens) with varying token counts to create different levels of "overtraining." These base models are then reused through two strategies: continual pretraining on domain-specific data (code from Stack/StarCoder or math from OpenWebMath) or model growth via depth stacking (adding duplicate layers) or width expansion (using function-preserving initialization). The scaling behavior is captured by fitting a multiplicative scaling law L(D1,D2) = A·D1^(-α1)·D2^(-α2+α3·log(D1)) + E to the validation loss curves, with experiments varying both D1 and D2 across log-spaced intervals.

## Key Results
- The scaling exponent for second-stage pretraining tokens decreases logarithmically with first-stage pretraining tokens
- Overtrained base models exhibit larger gradient norms during second-stage training, indicating harder optimization
- The saturation effect is consistent across different reuse strategies (CPT vs growth) and model sizes
- Function-preserving initialization enables controlled study of width expansion scaling behavior

## Why This Works (Mechanism)

### Mechanism 1: Interaction Term Reduces Second-Stage Scaling Efficiency
The joint scaling law contains an interaction term α3·log(D1) that modifies the D2 exponent. As D1 increases, the effective exponent α_eff = α2 - α3·log(D1) shrinks, meaning each additional token in stage 2 provides diminishing returns. This emerges from power-law scaling foundations and logarithmic coupling between stages.

### Mechanism 2: Optimization Difficulty via Elevated Gradient Norms
Overtrained base models exhibit larger gradient norms during second-stage training, indicating harder optimization. Highly optimized parameters from extensive first-stage training occupy sharp/sensitive regions of the loss landscape relative to second-stage objectives, making gradient-based optimization less effective.

### Mechanism 3: Feature Space Expansion in Toy Model
First-stage pretraining expands the effective branching factor of feature space, reducing second-stage learning efficiency. In a k-ary tree model, first-stage training uncovers more features, effectively expanding k to k_new ≈ k·D1^δ, which reduces the power-law exponent α_new ≈ α - θ·log(D1).

## Foundational Learning

- **Power-law scaling laws (L = AX^(-α) + E)**: Essential foundation as the entire framework assumes neural network training follows predictable power-law relationships with tokens and parameters.
  - Quick check: Given two models trained on 10B and 100B tokens respectively with exponent α=0.1, what's the expected loss ratio (ignoring irreducible E)?

- **Loss of plasticity / ossification in neural networks**: Explains why overtrained models become less adaptable—their highly optimized parameters resist effective updates for new objectives.
  - Quick check: Why might a model trained to convergence on task A struggle to learn task B, even with the same architecture?

- **Function-preserving initialization (FPI) for model growth**: Width expansion uses FPI to ensure the grown model initially reproduces base model behavior, enabling controlled study of scaling without confounding initialization effects.
  - Quick check: When expanding hidden dimension from d to √2·d, how should output layer weights be adjusted to preserve function?

## Architecture Onboarding

- **Component map**: Slimpajama CommonCrawl → Base model training (varying D1) → CPT/growth training (varying D2) → Validation loss evaluation
- **Critical path**: Choose D1 → train base model with WSD schedule → save stable-phase checkpoints → select reuse strategy (CPT: code/math data; growth: stacking x2/x4 or expansion x2) → train D2 tokens → evaluate on held-out validation set
- **Design tradeoffs**: Higher D1 yields better initial loss but lower α_eff for D2 (diminishing returns). CPT on code/math shows moderate saturation; stacking shows strongest saturation. From-scratch training eventually outperforms growth at large D2. Replay mixing (25%) and stable-phase initialization can mitigate but don't eliminate saturation.
- **Failure signatures**: (1) D2 scaling curves for different D1 converging instead of maintaining separation; (2) Elevated gradient norms in stage 2 correlating with D1; (3) From-scratch loss curve crossing below growth curve at moderate D2 values.
- **First 3 experiments**:
  1. Replicate Figure 1: Train 0.1B base models on 5 log-spaced D1 values, run CPT on code with 5 log-spaced D2 values each, fit power-law exponents per D1, verify log(D1) dependence of fitted α.
  2. Gradient norm diagnostic: For fixed D2 (e.g., 5B tokens), measure gradient norm trajectories during second-stage training for base models with D1 ∈ {1B, 5B, 20B}, confirm correlation with saturation severity.
  3. Compute crossover identification: For target model size (e.g., 1B) and available compute budget, fit both L_growth(D1,D2) and L_scratch(D) scaling laws, solve for D* where training from scratch becomes more efficient than growth (Equation 12).

## Open Questions the Paper Calls Out

- **Does the predicted crossover point hold in the extreme D2 limit?**: The paper predicts a crossover where less-pretrained base models eventually outperform overtrained ones, but this hasn't been verified due to noise floor and computational constraints.

- **What are the precise mechanistic causes of saturation beyond gradient norms?**: While gradient norms showed a clear trend, other potential indicators like parameter norms and Hessian eigenvalues didn't show clear correlations with D1.

- **Can advanced training techniques eliminate the saturation effect?**: Developing methods to completely eliminate saturation effects during model reuse is identified as a potential future direction.

- **How do replay ratio and model growth factor quantitatively influence scaling laws?**: Incorporating these factors into the scaling law framework is left for future work.

## Limitations

- The logarithmic saturation relationship has an unclear theoretical foundation despite empirical robustness
- Computational expense limits grid coverage across D1-D2 space, particularly for larger models
- Focus on decoder-only architectures may limit generalization to encoder-decoder or multimodal models
- Evaluation relies on pretraining loss rather than comprehensive downstream task performance

## Confidence

- **High confidence**: Empirical observation of logarithmic saturation across multiple model sizes, datasets, and reuse strategies; consistent interaction terms in fitted scaling laws; gradient norm correlation providing mechanistic support
- **Medium confidence**: Theoretical explanations for saturation mechanism (gradient norm observation is empirically grounded but connection to optimization difficulty is indirect; toy model is too abstract)
- **Low confidence**: Generalization to other model families and impact on downstream task performance beyond pretraining loss

## Next Checks

1. **Cross-architecture validation**: Test whether logarithmic saturation holds for encoder-decoder models by training base models with varying D1, then performing second-stage pretraining on code/math data with growth via depth stacking. Compare fitted α3 values to decoder-only baselines.

2. **Downstream task performance correlation**: For a fixed target model size, identify D1 values where from-scratch training becomes more efficient than growth. Train models at these D1 checkpoints, then fine-tune on downstream benchmarks (MMLU, BBH) to measure if saturation in pretraining loss translates to performance degradation.

3. **Optimization intervention experiment**: For highly saturated base models (large D1), test whether advanced optimization techniques (Lion optimizer, Sharpness-Aware Minimization, or LoRA) can restore second-stage scaling efficiency. Compare D2 scaling curves with standard AdamW to isolate optimization difficulty as the primary saturation driver.