---
ver: rpa2
title: 'KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning'
arxiv_id: '2509.15676'
source_url: https://arxiv.org/abs/2509.15676
tags:
- arxiv
- in-context
- learning
- examples
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the exemplar selection problem in in-context
  learning (ICL), where a language model must be provided with a small, informative
  set of task-specific examples from a larger bank to maximize performance on a user
  query. The authors propose KITE (Kernelized and Information Theoretic Exemplars),
  a principled approach that frames exemplar selection as a query-specific optimization
  problem.
---

# KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning

## Quick Facts
- **arXiv ID**: 2509.15676
- **Source URL**: https://arxiv.org/abs/2509.15676
- **Reference count**: 38
- **Primary result**: KITE consistently outperforms strong baselines like kNN, DPP, and BM25 across five classification datasets and three language models, achieving up to +4.55% accuracy improvement over BM25 on SST-5 with GPT-Neo-2.7B and +2.24% on HellaSwag with Llama-3B over the state-of-the-art DPP method.

## Executive Summary
This paper tackles the exemplar selection problem in in-context learning (ICL), where a language model must be provided with a small, informative set of task-specific examples from a larger bank to maximize performance on a user query. The authors propose KITE (Kernelized and Information Theoretic Exemplars), a principled approach that frames exemplar selection as a query-specific optimization problem. They model the LLM as a linear function over input embeddings and derive a surrogate objective that is approximately submodular, enabling the use of a greedy algorithm with provable approximation guarantees. KITE further incorporates the kernel trick to operate in high-dimensional feature spaces and an optimal design-based regularizer to encourage diversity among selected examples. Empirically, KITE consistently outperforms strong baselines like kNN, DPP, and BM25 across five classification datasets and three language models, achieving up to +4.55% accuracy improvement over BM25 on SST-5 with GPT-Neo-2.7B and +2.24% on HellaSwag with Llama-3B over the state-of-the-art DPP method. Ablation studies confirm the importance of kernel choice and diversity in exemplar selection, validating the effectiveness of the kernelized, diversity-aware approach.

## Method Summary
KITE is a kernelized and information-theoretic method for selecting exemplars in in-context learning. It frames exemplar selection as a query-specific optimization problem, deriving a surrogate objective that is approximately submodular. The method uses a greedy algorithm with provable approximation guarantees to select a small set of examples from a larger bank. KITE incorporates the kernel trick to operate in high-dimensional feature spaces and adds an optimal design-based regularizer to encourage diversity among selected examples. The method is evaluated on five classification datasets using three different language models, showing consistent improvements over strong baselines.

## Key Results
- KITE consistently outperforms strong baselines like kNN, DPP, and BM25 across five classification datasets and three language models.
- Achieves up to +4.55% accuracy improvement over BM25 on SST-5 with GPT-Neo-2.7B.
- Achieves +2.24% improvement on HellaSwag with Llama-3B over the state-of-the-art DPP method.
- Ablation studies confirm the importance of kernel choice and diversity in exemplar selection.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: An approximately submodular objective function, derived from information-theoretic principles and optimized greedily, can effectively select in-context examples that minimize prediction error on a specific query.
- Mechanism: KITE models the LLM as a linear function over input embeddings. It defines an objective function $f_z(S) = -z^\top V_S^{-1} z$, which is a surrogate for the prediction error bound on a specific query $z$. By maximizing this function, the algorithm selects a subset of examples $S$ that are most informative for that query. The use of a greedy algorithm is justified because the objective function is shown to be approximately submodular, providing a theoretical guarantee that the greedy solution is close to the global optimum.
- Core assumption: The LLM's behavior can be locally approximated by a linear model in the embedding space.
- Evidence anchors:
  - [abstract] "...formulates example selection as a query-specific optimization problem. They derive a surrogate objective that is approximately submodular, enabling a greedy algorithm with provable approximation guarantees."
  - [Theoretical Model] "...the goal of ICL in the context of LLMs translates to algorithmically selecting a small number of datapoints to train the linear model... we derive a surrogate loss objective that quantifies this query-specific prediction error and show that this objective is approximately submodular."
  - [corpus] The corpus includes related work like InSQuAD which also uses submodular functions for ICL, supporting the broader idea of using submodularity for this task.
- Break condition: The objective function is not approximately submodular (i.e., the submodularity ratio is very low). This occurs when the maximum coherence between examples is high, meaning examples are highly redundant.

### Mechanism 2
- Claim: A diversity-promoting regularizer, based on optimal experimental design principles, improves the selected examples' generalizability and reduces redundancy.
- Mechanism: KITE adds a second term to its objective, $g(S) = \log \det(V_S)$, derived from mutual information maximization in a Bayesian linear regression setting. This term encourages the selected examples to span a larger volume in the feature space, which directly translates to diversity. The final selection is a trade-off between query-relevance (Mechanism 1) and this diversity, controlled by a hyperparameter $\lambda$.
- Core assumption: The selected examples should cover the space of possible inputs to provide a more robust context for the LLM.
- Evidence anchors:
  - [abstract] "...introducing an optimal design-based regularizer to encourage diversity in the selected examples."
  - [Selecting Diverse Examples] "This leads to the classical optimal experimental design problem... We incorporate this design objective as a regularizer..."
  - [corpus] Related work like "Exploring the Role of Diversity in Example Selection for In-Context Learning" explicitly studies this factor, confirming its importance.
- Break condition: The optimal balance between relevance and diversity is not found (e.g., $\lambda$ is poorly tuned). If $\lambda$ is too high, the selected examples may be too far from the query, losing relevance. If too low, the selection becomes a simple nearest-neighbor search with low diversity.

### Mechanism 3
- Claim: The "kernel trick" allows the linear-model-based algorithm to capture non-linear relationships in data without explicit, computationally expensive feature mappings.
- Mechanism: KITE generalizes its linear model formulation to a non-linear one by assuming the underlying function lives in a Reproducing Kernel Hilbert Space (RKHS). All inner products in the algorithm's equations (e.g., $x^\top V_S^{-1} x$) are replaced by kernel evaluations $k(x, x')$. This enables the algorithm to operate in a high-dimensional (potentially infinite-dimensional) feature space implicitly, capturing more complex relationships between the query and examples.
- Core assumption: The relationship between the query and the optimal examples is non-linear and can be captured by a chosen kernel function (e.g., Gaussian RBF, Polynomial).
- Evidence anchors:
  - [abstract] "The method, called KITE, is further enhanced by incorporating the kernel trick to operate in high-dimensional feature spaces..."
  - [Moving Beyond Linearity: Kernel Trick] "This trick helps us to perform all computations in the high (possibly infinite) dimensional Hilbert space... without the need for computing the inner product explicitly."
  - [corpus] Corpus evidence for this specific mechanism in the context of KITE is weak or missing. While kernel methods are a standard ML technique, the paper's specific application to ICL example selection is novel.
- Break condition: The chosen kernel is a poor fit for the data's underlying structure. For example, using a linear kernel when the data manifold is highly complex and non-linear would limit the method's effectiveness.

## Foundational Learning

- **Submodular Optimization**
  - Why needed here: The paper's core theoretical contribution is formulating the example selection problem as maximizing an approximately submodular set function. Understanding this concept is necessary to grasp why a greedy algorithm provides a provably good solution to a combinatorial problem.
  - Quick check question: What property of a set function ensures that a greedy selection algorithm provides a near-optimal solution?

- **Reproducing Kernel Hilbert Space (RKHS) & The Kernel Trick**
  - Why needed here: This is the mechanism used to extend the core algorithm beyond simple linear models. It allows KITE to handle non-linear relationships in the data.
  - Quick check question: How does a kernel function allow an algorithm to operate in a high-dimensional feature space without ever explicitly computing the coordinates of the data in that space?

- **Optimal Experimental Design (D-optimal design)**
  - Why needed here: The paper uses a principle from this field (specifically D-optimal design, related to log-determinant) to formulate its diversity-promoting objective. Understanding the goal of experimental design helps clarify why this particular function was chosen.
  - Quick check question: In D-optimal design, what quantity is maximized to ensure the selected data points are most informative for parameter estimation?

## Architecture Onboarding

- **Component map**: Query Embedding -> Selection Loop (relevance/diversity scoring & matrix update) -> Prompt Construction -> LLM Inference
- **Critical path**: Query Embedding -> Selection Loop (relevance/diversity scoring & matrix update) -> Prompt Construction -> LLM Inference
- **Design tradeoffs**:
  - **Kernel Choice**: A Linear kernel (LITE) is faster but may miss non-linear patterns. Non-linear kernels (e.g., Gaussian RBF) are more expressive but require tuning.
  - **Relevance vs. Diversity ($\lambda$)**: A higher $\lambda$ promotes diversity but risks selecting less relevant examples. The optimal $\lambda$ is dataset-dependent.
- **Failure signatures**:
  - **High Coherence**: If examples in the bank are too similar, the submodularity ratio drops, and the greedy solution may be far from optimal.
  - **Poor Kernel Selection**: If the chosen kernel does not align with the data geometry, performance will suffer.
  - **Context Window Overflow**: The number of selected examples $k$ is limited by the LLM's context size.
- **First 3 experiments**:
  1. **Sanity Check on Synthetic Data**: Replicate the synthetic linear model experiment from the appendix. This validates the core LITE algorithm's performance against baselines in a controlled setting where the linear assumption holds.
  2. **Ablation on Kernel Function**: Using a single LLM and dataset, compare the performance of KITE with different kernels (Linear, Polynomial, Gaussian RBF). This directly tests the benefit of kernelization.
  3. **Sensitivity Analysis on Lambda ($\lambda$)**: On a large, diverse dataset, run KITE across a range of $\lambda$ values. This will reveal the optimal trade-off between relevance and diversity for a given task.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the KITE framework be adapted for generative tasks where outputs consist of multiple interdependent tokens rather than single class labels?
  - Basis in paper: [explicit] The Conclusion explicitly states: "Extending KITE to generative tasks, which involve producing multiple dependent output tokens, is an important direction for future work."
  - Why unresolved: The current theoretical formulation (Eq. 1) models the response $y$ as a scalar generated by a linear function plus noise, which maps cleanly to classification but does not account for the sequential dependencies and structure of free-form text generation.
  - What evidence would resolve it: A reformulation of the objective function $f_z(S)$ to handle vector-valued or sequential outputs, accompanied by empirical results showing performance gains on generative benchmarks (e.g., summarization, translation) compared to retrieval baselines.

- **Open Question 2**: Can a mechanism be developed to automatically select or learn the optimal kernel function for KITE based on the specific dataset distribution?
  - Basis in paper: [inferred] The ablation study notes that "no single kernel is universally optimal; the best choice is contingent on the specific dataset," yet the paper relies on reporting the best result across fixed kernels (Linear, Polynomial, Gaussian).
  - Why unresolved: Requiring a validation set to tune the kernel contradicts the "data-scarce" and "frozen retriever" motivations of the paper, as users in few-shot settings may lack the data necessary to determine whether a linear or RBF kernel is appropriate.
  - What evidence would resolve it: A meta-learning algorithm or a heuristic that dynamically selects or weights kernels based on the geometry of the example bank $X$ and query $z$, removing the need for manual hyperparameter tuning.

- **Open Question 3**: How robust is the KITE objective when the underlying assumption that the LLM behaves as a linear function over input embeddings is violated?
  - Basis in paper: [inferred] The Theoretical Model section explicitly assumes "the model response... is linear in its input" to make the optimization tractable, acknowledging this as a simplification of the LLM's actual behavior.
  - Why unresolved: While the kernel trick captures non-linear relationships between *inputs*, the fundamental objective is derived from a linear estimator $\hat{\theta}_S$. If the LLM's implicit inference mechanism is highly non-linear with respect to the embeddings, the surrogate objective may not correlate with actual prediction error.
  - What evidence would resolve it: An analysis measuring the correlation between the KITE selection score and the true prediction error on synthetic datasets where the linearity assumption is progressively broken (e.g., using non-linear ground truth functions).

## Limitations

- The paper's theoretical guarantees rely on the assumption that the LLM can be locally approximated as a linear function over input embeddings, which is not empirically validated across diverse tasks and models.
- The kernel trick's benefits are demonstrated empirically but lack strong theoretical grounding in the ICL context, and the choice of kernel and its hyperparameters are not systematically studied.
- The diversity-promoting regularizer's impact is evaluated through ablation studies, but the optimal balance between relevance and diversity (lambda) is treated as a hyperparameter without a principled method for setting it.
- The computational cost of maintaining and updating the inverse design matrix $V^{-1}$ for large example banks is not addressed, raising concerns about scalability.

## Confidence

- **High Confidence**: The core greedy selection algorithm (LITE) is well-defined and the submodularity approximation is mathematically sound.
- **Medium Confidence**: The empirical improvements over baselines are significant, but the kernel trick's contribution is less clear without a systematic study of kernel choices.
- **Low Confidence**: The theoretical model's assumption about LLM linearity and the practical implications of the submodularity ratio's lower bound are not fully validated.

## Next Checks

1. **Synthetic Data Sanity Check**: Replicate the synthetic linear model experiment from the appendix to validate the core LITE algorithm's performance in a controlled setting where the linear assumption holds.
2. **Kernel Ablation Study**: Systematically compare the performance of KITE with different kernels (Linear, Polynomial, Gaussian RBF) on a single LLM and dataset to quantify the kernel trick's contribution.
3. **Lambda Sensitivity Analysis**: On a large, diverse dataset, run KITE across a range of lambda values to determine the optimal trade-off between relevance and diversity and assess its stability across tasks.