---
ver: rpa2
title: 'DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty
  with LLMs'
arxiv_id: '2511.11576'
source_url: https://arxiv.org/abs/2511.11576
tags:
- optimization
- problem
- data
- should
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes DAOpt, a framework for modeling and evaluating
  data-driven optimization under uncertainty using large language models (LLMs). The
  key idea is to incorporate domain knowledge from stochastic and robust optimization
  into LLMs to enhance their modeling capabilities in uncertain environments.
---

# DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs

## Quick Facts
- arXiv ID: 2511.11576
- Source URL: https://arxiv.org/abs/2511.11576
- Reference count: 40
- Primary result: Framework significantly outperforms direct prompting in out-of-sample feasibility, with DRO models achieving feasibility rates above 70%

## Executive Summary
DAOpt introduces a framework for modeling data-driven optimization under uncertainty using large language models (LLMs). The system addresses the "optimizer's curse" by incorporating domain knowledge from stochastic and robust optimization into LLMs through tool-augmented few-shot learning with the RSOME package. The framework employs a multi-agent architecture that iteratively generates and validates optimization formulations, achieving significantly higher out-of-sample feasibility rates compared to deterministic approaches.

## Method Summary
The DAOpt framework combines a specialized dataset (OptU) with a multi-agent decision-making system. The Identifier agent extracts uncertain parameters from problem descriptions, the Coder agent generates RSOME code using few-shot examples, and the Checker agent validates and iterates on the code. The system evaluates solutions using out-of-sample testing to measure feasibility and over-optimistic rates. The framework is tested on transportation optimization problems, comparing deterministic, stochastic, robust, and distributionally robust optimization approaches.

## Key Results
- DAOpt achieves significantly higher out-of-sample feasibility rates compared to direct prompting and chain-of-thought prompting
- Distributionally robust optimization models achieve feasibility rates above 70%
- The framework attains substantially higher successful rates in obtaining valid solutions, particularly when using GPT-4o
- Deterministic models show feasibility rates below 27% on out-of-sample data, demonstrating the "optimizer's curse"

## Why This Works (Mechanism)

### Mechanism 1: Domain Knowledge Injection via Tool-Augmented Few-Shot Learning
The framework teaches LLMs to use RSOME through few-shot examples, reducing their task from deriving mathematical reformulations to mapping problem descriptions to correct API calls. This bypasses the LLM's lack of intrinsic mathematical reasoning while leveraging its coding capabilities.

### Mechanism 2: Iterative Self-Correction via a Reflexion-Based Checker
A multi-agent architecture with automated feedback loops converts the hard problem of perfect initial code generation into simpler debugging tasks. The Checker executes generated code and provides error feedback, enabling iterative refinement until success or failure limits are reached.

### Mechanism 3: Out-of-Sample Evaluation for Bypassing the "Optimizer's Curse"
The framework uses held-out data to test generated decisions, revealing that deterministic models optimal on mean values often fail in practice. This exposes the fragility of deterministic approaches and validates the robustness of distributionally robust optimization models.

## Foundational Learning

- **Concept: The "Optimizer's Curse"**
  - Why needed: Explains why models optimal on paper often fail in practice
  - Quick check: If you optimize a delivery route using average traffic times, why might your route be slower and risk missed deadlines compared to uncertainty-aware planning?

- **Concept: Stochastic vs. Robust vs. Distributionally Robust Optimization**
  - Why needed: Framework benchmarks all three approaches
  - Quick check: What's the core difference in handling uncertainty? (Stochastic: optimizes average case; Robust: protects against worst-case set; Distributionally Robust: protects against uncertainty in probability distribution)

- **Concept: RSOME (Robust Stochastic Optimization Made Easy)**
  - Why needed: Specialized toolbox the framework relies on
  - Quick check: Why is RSOME necessary instead of using standard LP solvers directly?

## Architecture Onboarding

- **Component map:** OptU Dataset -> Identifier Agent -> Coder Agent -> Checker Agent -> Evaluation Module
- **Critical path:** Problem Text + Data -> Identifier Agent -> Coder Agent (generates RSOME code) -> Checker Agent (executes code) -> If error, feedback to Coder -> Final Decision
- **Design tradeoffs:** Specializes in RSOME for reliability, losing flexibility for other solvers
- **Failure signatures:**
  - Low Success Rate: Coder failing to produce executable RSOME code
  - High Over-Optimistic Rate: Generated model insufficiently robust
  - Low Feasibility Rate: Decisions invalid on out-of-sample data
- **First 3 experiments:**
  1. Baseline comparison: DAOpt vs. direct prompting on out-of-sample Feasibility Rate
  2. Ablation on feedback: Measure Success Rate drop when disabling Checker feedback loop
  3. Model sensitivity: Compare performance using GPT-4o vs. smaller open-source models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-based frameworks effectively model and solve two-stage optimization problems with recourse decisions?
- Basis: Paper plans to explore two-stage problems in future work
- Why unresolved: Current framework handles only single-stage problems; two-stage introduces sequential decision-making complexity
- Evidence needed: Extension to two-stage problems with successful solution rates and out-of-sample feasibility

### Open Question 2
- Question: Can component-wise fine-tuning of individual agents improve overall framework performance?
- Basis: Future work mentions designing multi-agent system for further fine-tuning
- Why unresolved: Current framework uses base LLMs without specialized fine-tuning for agent roles
- Evidence needed: Comparative experiments showing improvements with fine-tuned vs. base models

### Open Question 3
- Question: Can LLMs learn to derive tractable robust reformulations via Lagrangian duality without external toolboxes?
- Basis: Paper notes approaches without domain knowledge often fail to formally derive dual reformulations
- Why unresolved: Current success depends on RSOME's built-in duality operations
- Evidence needed: Experiments comparing solutions from LLMs without RSOME integration against ground-truth dual reformulations

## Limitations

- Limited generalizability beyond transportation domain, as OptU dataset focuses on logistics scenarios
- Reliance on RSOME creates potential adaptability concerns for other optimization paradigms
- Paper lacks ablation studies isolating contribution of individual agent components

## Confidence

- High confidence: Core mechanism of out-of-sample evaluation exposing "optimizer's curse"
- Medium confidence: Claims about multi-agent decision-making superiority without component isolation studies
- Low confidence: Generalizability to optimization problems beyond transportation domain

## Next Checks

1. **Cross-domain validation**: Test DAOpt on optimization problems from finance, manufacturing, and energy domains to assess generalizability

2. **Solver comparison**: Implement framework using alternative optimization toolboxes (Pyomo, CVXPY) to determine if RSOME's features are essential

3. **Human expert comparison**: Benchmark DAOpt's solutions against those generated by human optimization experts on identical problems to establish practical value