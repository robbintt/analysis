---
ver: rpa2
title: Adapting Interleaved Encoders with PPO for Language-Guided Reinforcement Learning
  in BabyAI
arxiv_id: '2510.23148'
source_url: https://arxiv.org/abs/2510.23148
tags:
- policy
- learning
- perception
- pdit
- babyai
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces PDiT, an interleaved transformer architecture
  that alternates between perception and decision layers, integrated with a PPO policy
  and a CLIP-based contrastive loss. Evaluated on the BabyAI GoToLocal environment,
  PDiT demonstrates more stable reward convergence and a 42% reduction in reward variance
  compared to a standard PPO baseline.
---

# Adapting Interleaved Encoders with PPO for Language-Guided Reinforcement Learning in BabyAI

## Quick Facts
- arXiv ID: 2510.23148
- Source URL: https://arxiv.org/abs/2510.23148
- Reference count: 6
- One-line primary result: Interleaved transformer architecture with PPO and CLIP-style alignment reduces reward variance by 42% and improves policy stability ratio by 73% on BabyAI GoToLocal.

## Executive Summary
This paper introduces PDiT, an interleaved transformer architecture that alternates between perception and decision layers, integrated with a PPO policy and a CLIP-based contrastive loss. Evaluated on the BabyAI GoToLocal environment, PDiT demonstrates more stable reward convergence and a 42% reduction in reward variance compared to a standard PPO baseline. It also achieves a 73% improvement in policy stability ratio, indicating smoother learning dynamics. The interleaving enables direct gradient flow between perception and decision modules, while the contrastive loss improves multimodal alignment between visual observations and textual instructions. Ablation studies confirm that both interleaving and contrastive alignment are critical for performance gains. The results suggest that tightly integrating interleaved encoders with multimodal grounding and policy optimization can enhance sample efficiency and stability in language-guided RL tasks.

## Method Summary
PDiT interleaves perception (P_l) and decision (D_l) transformer layers to process concatenated visual tokens, text tokens, and action/reward history. The architecture uses a total loss combining PPO, CLIP-style contrastive alignment (InfoNCE), and supervised imitation learning. Training involves 200k steps with a 7×7 visual observation field, text mission string, and previous action/reward as inputs. The method aims to dynamically refine perceptual features using policy feedback and improve multimodal grounding through contrastive alignment.

## Key Results
- 42% reduction in reward variance compared to standard PPO baseline
- 73% improvement in policy stability ratio (S=1.73)
- More stable reward convergence, with faster learning dynamics when both interleaving and contrastive alignment are active
- Ablation studies confirm interleaving and CLIP alignment are critical for performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interleaving perception and decision layers enables direct gradient flow from policy signals to perceptual representations.
- Mechanism: Rather than encoding state once via a frozen visual encoder, PDiT alternates P (perception) and D (decision) layers so that ∂at/∂st = (∂Dl/∂Pl) × (∂Pl/∂st), forming an implicit bi-level optimization where perception receives reward-shaped gradients.
- Core assumption: The perception module benefits from task-relevant supervision; generic visual features may encode irrelevant information for policy.
- Evidence anchors:
  - [abstract] "This interleaving allows feedback from decision-making to refine perceptual features dynamically."
  - [Section VI.A] "In PDiT: a(l)t = Dl(Pl(st, at-1)), so the gradient of policy w.r.t. raw state becomes: ∂at/∂st = (∂Dl/∂Pl)(∂Pl/∂st), implying direct gradient flow between policy and perception."
  - [corpus] SPRIG (arXiv:2502.14264) proposes game-theoretic coordination between perception and RL, supporting the premise that separated modules are suboptimal.
- Break condition: If perception and decision operate on incompatible timescales or the task requires no visual discrimination, interleaving adds overhead without benefit.

### Mechanism 2
- Claim: CLIP-style contrastive loss bootstraps visual-textual grounding, reducing the burden on policy learning to discover multimodal associations from sparse rewards alone.
- Mechanism: InfoNCE loss pulls corresponding (observation, mission) pairs closer in latent space while pushing non-corresponding pairs apart, ensuring sim(fv(ot), ft(mt)) > sim(fv(ot), ft(mj)) for j ≠ t before policy optimization begins.
- Core assumption: Pre-aligned multimodal embeddings provide a better initialization for policy learning than unaligned random embeddings.
- Evidence anchors:
  - [abstract] "We integrate a contrastive loss inspired by CLIP to align textual mission embeddings with visual scene features."
  - [Section V.B] "Agents trained without CLIP alignment often fixated on distractor objects."
  - [Section VII.B] "Without CLIP alignment: convergence was ~20% slower. This suggests the policy can eventually learn the visual-textual grounding from rewards alone, but it's much harder."
  - [corpus] Multimodal Representation Alignment (arXiv:2502.20172) discusses interleaved control benefits but in image generation; transferability to RL is assumed, not proven.
- Break condition: If visual and textual modalities share no task-relevant structure, or if pre-trained CLIP embeddings are misaligned with domain-specific features, contrastive alignment may harm rather than help.

### Mechanism 3
- Claim: Joint optimization of PPO, contrastive, and supervised losses stabilizes learning dynamics by providing dense auxiliary signals.
- Mechanism: Ltotal = LPPO + λ1·LCLIP + λ2·Lsup provides multiple gradient sources; even when rewards are sparse (binary success/failure), contrastive and supervised losses continue shaping representations.
- Core assumption: Auxiliary losses do not conflict with the primary RL objective; their gradients point in compatible directions.
- Evidence anchors:
  - [Section III.B] "The total loss is: Ltotal = LPPO + λ1·LCLIP + λ2·Lsup, where Lsup denotes imitation learning supervision loss."
  - [Section V.C] "We obtained S = 1.73, indicating a 73% improvement in policy smoothness across episodes."
  - [corpus] MORAL (arXiv:2504.03153) integrates visual and textual inputs for decision-making but does not isolate whether joint loss is the causal factor; evidence is correlational.
- Break condition: If auxiliary losses dominate gradient magnitude (λ values too high), the policy may optimize for alignment or imitation at the expense of reward maximization.

## Foundational Learning

- Concept: **Transformer self-attention and token mixing**
  - Why needed here: PDiT layers process concatenated visual tokens, text tokens, and action/reward history; understanding how attention routes information across modalities is prerequisite to debugging alignment.
  - Quick check question: Given Q from a "red ball" text token and K from visual patch tokens, which patches would receive highest attention weights if alignment is working?

- Concept: **PPO clipping and advantage estimation**
  - Why needed here: The policy backbone uses PPO; understanding why clipping prevents destructive updates helps diagnose when learning destabilizes despite the architecture.
  - Quick check question: If the probability ratio r(θ) = 2.5 and ε = 0.2, what is the clipped objective value when Ât = 0.1?

- Concept: **InfoNCE / contrastive loss mechanics**
  - Why needed here: The CLIP-style alignment uses InfoNCE; misconfigured temperature τ or batch composition can cause collapsed or uninformative embeddings.
  - Quick check question: If all image-text pairs have identical similarity scores, what does the InfoNCE loss gradient push the model to do?

## Architecture Onboarding

- Component map:
Input: (ot, mt, at-1, rt-1)
     ↓
fv(ot) → visual tokens    ft(mt) → text tokens
     ↓
[P1] ←→ [D1]  (interleaved attention across modalities + history)
     ↓
[P2] ←→ [D2]
     ↓
[PL] ←→ [DL]
     ↓
π(at|st) → action logits
     ↓
Losses: LPPO (policy) + λ1·LCLIP (alignment) + λ2·Lsup (imitation)

- Critical path: Visual encoder → token concatenation → P1 attention over (visual + text + history) → D1 policy embedding → repeat through L layers → action head. If any P layer fails to pass task-relevant gradients, D layers receive degraded inputs.

- Design tradeoffs:
  - More interleaved layers (higher L): Better gradient flow but O(Ln²d) compute cost; diminishing returns if task is simple.
  - Higher λ1 (CLIP weight): Better initial grounding but risks overshadowing reward signals; paper used λ1 as hyperparameter but did not report sensitivity.
  - Frozen vs. fine-tuned CLIP encoder: Frozen saves compute but may not adapt to BabyAI's visual style; paper fine-tuned but does not compare against frozen ablation.

- Failure signatures:
  - Agent fixates on distractor objects → likely CLIP alignment weak or absent (Section V.B).
  - Reward variance matches baseline → interleaving may not be functioning; verify gradient flow through P layers.
  - Convergence >200k steps → check λ1 weight; contrastive signal may be insufficient.
  - Trajectory completion drops → supervised loss may be missing or underweighted.

- First 3 experiments:
  1. **Baseline sanity check**: Run standard PPO (CNN encoder + MLP policy) on GoToLocal; confirm reward curve matches paper's baseline before implementing PDiT.
  2. **Ablation sweep**: Train PDiT with (a) no CLIP loss, (b) no interleaving (sequential P-then-D stack), (c) full model; compare convergence speed and variance to isolate each mechanism's contribution.
  3. **Attention visualization**: Log attention weights from P1 and D1 during evaluation; verify that mission tokens (e.g., "red ball") attend to corresponding visual patches. If attention is diffuse, contrastive alignment may need tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can PDiT generalize to entirely new object types or colors without retraining, or is it limited to the specific semantics of the training domain?
- Basis in paper: [explicit] The authors state in the Limitations section that they "suspect PDiT would struggle if introduced to entirely new object types or colors without retraining."
- Why unresolved: The CLIP alignment was only fine-tuned on the limited BabyAI domain, and the evaluation did not test zero-shot transfer to out-of-distribution semantic entities.
- What evidence would resolve it: Zero-shot evaluation results on environments like AI2-THOR containing objects and colors excluded from the BabyAI training set.

### Open Question 2
- Question: How does the interleaved architecture perform when scaled to complex 3D environments such as Habitat?
- Basis in paper: [explicit] The Conclusion lists "Applying PDiT to more complex 3D environments such as Habitat" as a primary future extension.
- Why unresolved: The current study is restricted to the 2D grid-world of BabyAI, leaving the computational and performance dynamics of PDiT in high-dimensional 3D spaces unverified.
- What evidence would resolve it: Benchmarking PDiT's sample efficiency and stability metrics in a photorealistic 3D simulator compared to the 2D baseline.

### Open Question 3
- Question: Does adding self-supervised reward prediction heads to PDiT improve temporal grounding?
- Basis in paper: [explicit] The paper proposes "Integrating reward prediction heads for self-supervised temporal grounding" as a future modification.
- Why unresolved: The current implementation relies solely on PPO gradients and CLIP alignment, and the impact of auxiliary temporal prediction tasks on the interleaved encoder has not been ablated.
- What evidence would resolve it: A comparative analysis of learning stability and policy performance when a reward prediction auxiliary loss is added to the PDiT training objective.

## Limitations
- Architectural transparency: The paper specifies interleaved P and D layers but does not clarify whether these share parameters or are distinct modules, making exact gradient flow dynamics difficult to reproduce.
- Hyperparameter sensitivity: Key hyperparameters (λ₁ for CLIP loss, λ₂ for supervised loss) are not reported, leaving performance robustness unclear.
- Evaluation scope: Results are demonstrated only on BabyAI GoToLocal, with no testing on other language-guided RL tasks or complex 3D environments.

## Confidence
- **High confidence**: The interleaving mechanism improves gradient flow between perception and decision modules, as supported by explicit gradient derivations and ablation showing performance drops without interleaving.
- **Medium confidence**: The CLIP contrastive loss provides multimodal grounding, but evidence is primarily from correlation (faster convergence) rather than controlled ablation isolating its effect.
- **Low confidence**: The combined auxiliary losses (CLIP + supervised) jointly stabilize learning; while performance improves, the paper does not disentangle whether stability comes from interleaving, alignment, or both.

## Next Checks
1. **Gradient flow verification**: Log and visualize gradients through P and D layers during training. Confirm that policy signals directly update perceptual features and that attention weights between mission tokens and visual patches increase over time.
2. **Ablation on auxiliary losses**: Systematically vary λ₁ (CLIP weight) and λ₂ (supervised weight) across [0, 0.1, 0.5, 1.0]. Measure convergence speed and reward variance to identify optimal balance and confirm neither loss dominates.
3. **Cross-task generalization**: Evaluate PDiT on a different language-guided RL benchmark (e.g., ALFWorld or Habitat). Compare reward variance and convergence speed against standard PPO to test whether interleaving+alignment benefits transfer beyond BabyAI.