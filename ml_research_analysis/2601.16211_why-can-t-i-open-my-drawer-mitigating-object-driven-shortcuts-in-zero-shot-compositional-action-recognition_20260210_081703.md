---
ver: rpa2
title: Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot
  Compositional Action Recognition
arxiv_id: '2601.16211'
source_url: https://arxiv.org/abs/2601.16211
tags:
- seen
- unseen
- verb
- learning
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies object-driven verb shortcuts as a critical
  failure mode in Zero-Shot Compositional Action Recognition (ZS-CAR). These shortcuts
  arise from compositional sparsity and the asymmetric learning difficulty between
  verbs (requiring temporal reasoning) and objects (recognizable from single frames).
---

# Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition

## Quick Facts
- **arXiv ID**: 2601.16211
- **Source URL**: https://arxiv.org/abs/2601.16211
- **Authors**: Geo Ahn; Inwoong Lee; Taeoh Kim; Minho Shim; Dongyoon Wee; Jinwoo Choi
- **Reference count**: 40
- **Primary result**: Object-driven shortcuts cause compositional gaps in ZS-CAR; RCORE reduces reliance on objects and improves unseen composition accuracy

## Executive Summary
This paper identifies object-driven verb shortcuts as a fundamental failure mode in Zero-Shot Compositional Action Recognition (ZS-CAR). Existing models increasingly rely on object cues and co-occurrence statistics during training, leading to negative compositional gaps and poor generalization to unseen verb-object combinations. The authors propose RCORE, a framework that includes VOCAMix (composition-aware augmentation) and TORC (temporal order regularization loss) to address these shortcuts. Across two benchmarks, RCORE significantly improves unseen composition accuracy and achieves positive compositional gaps.

## Method Summary
RCORE addresses object-driven shortcuts in ZS-CAR through two main components: VOCAMix, which synthesizes plausible unseen verb-object combinations while preserving temporal structure, and TORC, a temporal order regularization loss that penalizes reliance on static cues. The framework operates by augmenting training data with compositionally diverse samples and introducing a regularization term that encourages models to learn temporal dependencies rather than object-specific patterns. This approach forces the model to reason about actions in a temporally coherent manner rather than relying on object co-occurrence statistics.

## Key Results
- RCORE significantly improves unseen composition accuracy across Sth-com and EK100-com benchmarks
- The framework achieves positive compositional gaps, reversing the typical negative gap observed in ZS-CAR models
- Object-driven shortcuts are identified as the primary obstacle in compositional video understanding, not model capacity limitations

## Why This Works (Mechanism)
The framework works by directly addressing the compositional sparsity problem through synthetic data generation and temporal regularization. VOCAMix creates diverse training examples that expose the model to novel verb-object combinations, while TORC ensures the model learns temporal reasoning rather than static object recognition. This dual approach breaks the correlation between objects and actions that leads to shortcut learning.

## Foundational Learning
- **Compositional sparsity**: Many verb-object combinations rarely or never appear in training data. Needed because real-world video understanding requires reasoning about novel combinations. Quick check: Calculate coverage of verb-object pairs in training vs. test sets.
- **Temporal reasoning**: Understanding actions requires recognizing how objects and subjects change over time. Needed because object recognition alone cannot capture dynamic actions. Quick check: Evaluate model performance on temporally shuffled videos.
- **Co-occurrence bias**: Models learn to associate objects with actions based on statistical frequency rather than causal relationships. Needed because this bias creates negative compositional gaps. Quick check: Measure correlation between object presence and action prediction.

## Architecture Onboarding
- **Component map**: Input Video -> VOCAMix Augmentation -> Temporal Encoder -> TORC Regularization -> Output Classifier
- **Critical path**: Temporal reasoning must occur before object recognition for effective action classification
- **Design tradeoffs**: Synthetic data quality vs. computational cost; regularization strength vs. baseline performance
- **Failure signatures**: High performance on seen compositions but poor generalization to unseen combinations indicates object-driven shortcuts
- **First experiments**: 1) Measure compositional gap on baseline model, 2) Evaluate VOCAMix effectiveness in isolation, 3) Test TORC regularization impact on temporal vs. static features

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical validation restricted to two video benchmarks with limited compositional diversity
- VOCAMix assumes synthetic verb-object combinations are plausible without explicit validation
- TORC's sensitivity to temporal annotation quality is not thoroughly explored

## Confidence
- Identification of object-driven shortcuts: High
- Effectiveness of RCORE framework on benchmark datasets: Medium
- Generalization to diverse real-world video scenarios: Low

## Next Checks
1. Evaluate RCORE on longer-duration videos and datasets with higher compositional diversity to assess scalability beyond controlled benchmarks
2. Conduct human evaluation of VOCAMix-generated verb-object pairs to verify semantic plausibility and identify potential synthetic noise
3. Test TORC's sensitivity to varying levels of temporal annotation quality and explore alternative temporal regularization strategies