---
ver: rpa2
title: Why is Your Language Model a Poor Implicit Reward Model?
arxiv_id: '2507.07981'
source_url: https://arxiv.org/abs/2507.07981
tags:
- reward
- im-rms
- ex-rms
- responses
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses why implicit reward models (IM-RMs) generalize
  worse than explicit reward models (EX-RMs), despite their near-identical structure.
  The authors analyze the learning dynamics of both models, finding that IM-RMs rely
  more heavily on superficial token-level cues, leading to poorer generalization under
  token-level distribution shifts.
---

# Why is Your Language Model a Poor Implicit Reward Model?

## Quick Facts
- **arXiv ID:** 2507.07981
- **Source URL:** https://arxiv.org/abs/2507.07981
- **Reference count:** 40
- **Primary result:** Implicit reward models (IM-RMs) generalize worse than explicit reward models (EX-RMs) due to token-level overfitting, despite near-identical structure.

## Executive Summary
This paper explains why implicit reward models (IM-RMs) used in reinforcement learning from human feedback (RLHF) generalize worse than explicit reward models (EX-RMs), despite their near-identical architecture. Through theoretical analysis and controlled experiments, the authors show that IM-RMs rely heavily on superficial token-level cues rather than semantic similarity encoded in hidden representations. This leads to poor generalization under token-level distribution shifts like paraphrasing or translation, while EX-RMs leverage hidden representation structure to maintain accuracy. The work highlights how minor design choices in reward model architectures significantly impact their generalization behavior and downstream RL optimization.

## Method Summary
The paper compares EX-RMs (using linear heads on hidden representations) and IM-RMs (using log-likelihoods under the language model) trained with Bradley-Terry preference loss on UltraFeedback and RewardMATH datasets. Both models are initialized from the same pretrained LM and trained identically except for their reward computation. Evaluation includes in-distribution test sets, paraphrased/translated responses, and domain shifts (Math vs Code). Theoretical analysis uses gradient-based learning dynamics and max-margin classification theory. The authors prove that IM-RMs can verify without generating, ruling out alternative explanations for the generalization gap.

## Key Results
- IM-RMs achieve near-random accuracy (~2%) on paraphrased responses while EX-RMs maintain 100% accuracy
- EX-RMs induce significantly higher reward margins (0.976 vs 0.763 normalized), beneficial for RL optimization
- IM-RMs outperform EX-RMs on domain shifts (e.g., 0.720 vs 0.621 on Code tasks)
- Both models perform similarly on in-distribution UltraFeedback test (0.665 vs 0.602)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** IM-RMs rely more heavily on superficial token-level cues than EX-RMs, causing them to generalize poorly under token-level distribution shifts (e.g., paraphrasing, translation).
- **Mechanism:** The learning dynamics of IM-RMs (Equation 5) include coefficients ρk,l(y) that depend on whether tokens match between training and evaluation responses. When tokens don't match, ρk,l can become negative, causing the gradient update to decrease the reward of semantically similar responses with different tokens. EX-RMs lack this token-specific dependency—they depend only on hidden representation alignment.
- **Core assumption:** Hidden representations encode semantic similarity (established in prior work, referenced as Zou et al. 2023; Park et al. 2024).
- **Evidence anchors:**
  - [abstract] "IM-RMs rely more heavily on superficial token-level cues. Consequently, they often generalize worse than EX-RMs under token-level distribution shifts"
  - [Section 4.1] Equation 5 shows ρk,l(v) = 1[ȳk = vl] − πθ(ȳk|x, v<l) − πθ(vl|x̄, ȳ<k) + ..., demonstrating token-level dependency
  - [Section 5.1, Figure 4] Controlled experiments show IM-RMs achieve ~2% accuracy on paraphrased responses while EX-RMs achieve 100%

### Mechanism 2
- **Claim:** EX-RMs generalize to unseen tokens via hidden representation structure, while IM-RMs cannot generalize to tokens absent from training data.
- **Mechanism:** Under Theorem 2, with fixed hidden representations and single-token responses, EX-RM's linear head converges to the max-margin separator u* over hidden representations. If u* correctly separates unseen token pairs based on their hidden representations, EX-RMs generalize. IM-RMs cannot generalize to unseen tokens because the gradient with respect to unseen token embeddings is zero—they remain at initialization.
- **Core assumption:** Hidden representations are well-structured enough that a max-margin separator exists and transfers.
- **Evidence anchors:**
  - [Section 4.2, Theorem 2] "The IM-RM fails to generalize to unseen tokens: accDE(rθIM(t)) = 0.5 for all t ≥ 0"
  - [Section 4.2, Theorem 2] "The EX-RM can generalize via hidden representations... accDE(rθEX(t)) ≥ |{(x,y+,y−) ∈ DE: ⟨u*, hx,y+⟩ > ⟨u*, hx,y−⟩}| / |DE|"

### Mechanism 3
- **Claim:** IM-RMs can learn to verify without learning to generate, ruling out the hypothesis that generation-verification gaps cause the generalization gap.
- **Mechanism:** Theorem 1 constructs a distribution π where the induced IM-RM achieves verification margin δ while π(C(x)|x) ≤ πref(C(x)|x) · exp(δ/β)—only a constant multiplicative increase in correct-response probability over reference. Corollary 1 extends this: if πref isn't an efficient generator, neither needs to be π.
- **Core assumption:** Verification requires only that r(x, y+) > r(x, y−) + δ, not that π can generate correct responses efficiently.
- **Evidence anchors:**
  - [Section 3.1, Theorem 1] Formal proof that IM-RM can be a verifier with margin δ while generating correct responses at only slightly higher probability than reference
  - [Section 3.2, Figure 3] Hamiltonian cycle experiments: IM-RMs achieve 0.980-0.993 test accuracy while generating 0 correct cycles

## Foundational Learning

- **Concept: Bradley-Terry preference modeling**
  - Why needed here: Both EX-RMs and IM-RMs are trained with the Bradley-Terry log-likelihood loss (Equation 3); understanding this is prerequisite to analyzing their learning dynamics.
  - Quick check question: Can you derive why σ(r(x,y+) − r(x,y−)) gives the probability that y+ is preferred over y−?

- **Concept: Gradient-based learning dynamics analysis**
  - Why needed here: The paper's core theoretical contribution comes from analyzing how rewards change under gradient updates via Taylor approximation.
  - Quick check question: Why does ∆r ≈ −η⟨∇r, ∇ℓ⟩ capture the effect of a gradient step on a held-out example?

- **Concept: Max-margin classification in overparameterized linear models**
  - Why needed here: Theorem 2 relies on the implicit bias of gradient descent toward max-margin solutions (Soudry et al. 2018).
  - Quick check question: Why does gradient descent on separable logistic regression converge to the max-margin separator?

## Architecture Onboarding

- **Component map:**
  EX-RM: Prompt(x) + Response(y) → LM Backbone → h_{x,y} → Linear Head → r(x,y)
  IM-RM: Prompt(x) + Response(y) → LM Backbone → π_θ(y|x) → log prob (vs. ref) → r(x,y)

- **Critical path:**
  1. Initialize from same pretrained LM
  2. Train both with same preference data and Bradley-Terry loss
  3. The divergence occurs in how rewards respond to unseen responses—EX-RM via hidden representation similarity, IM-RM via token overlap

- **Design tradeoffs:**
  - **EX-RM advantages:** Better token-level robustness (Table 1: 0.665 vs 0.602 accuracy on token-level shift), higher reward margins (beneficial for RL optimization per Razin et al. 2025b)
  - **IM-RM advantages:** Comparable or better performance on domain shifts (Table 1: 0.720 vs 0.621), no architectural changes needed
  - **EX-RM disadvantages:** Requires adding and training a linear head
  - **IM-RM disadvantages:** Fails dramatically on paraphrased/translated responses

- **Failure signatures:**
  - IM-RM accuracy drops to near-random on paraphrased responses (Figure 4: 0.019-0.022)
  - IM-RM achieves trivial 0.5 accuracy on unseen tokens (Theorem 2)
  - IM-RM shows smaller absolute reward margins (Table 1: 0.763 vs 0.976 normalized margin)

- **First 3 experiments:**
  1. **Token-level shift test:** Train both model types on a preference dataset, evaluate on paraphrased versions of test responses. Expect EX-RM >> IM-RM accuracy.
  2. **Domain shift test:** Train on one domain (e.g., general chat), evaluate on different domains (e.g., math, code). Expect comparable or IM-RM > EX-RM accuracy.
  3. **Reward margin analysis:** Compute normalized reward margins for both models. Expect EX-RM >> IM-RM, correlating with RL optimization quality (cite Razin et al. 2025b for the connection).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Under what specific conditions do IM-RMs consistently outperform EX-RMs on domain shifts, and what mechanisms drive this advantage?
- **Basis in paper:** [explicit] In Section 7.1, the authors state, "Investigating whether there are cases in which IM-RMs consistently outperform EX-RMs and why is left to future work," noting that while IM-RMs fail at token-level shifts, they sometimes perform better on domain shifts.
- **Why unresolved:** The paper’s theoretical analysis focuses on explaining IM-RM *underperformance* (token-level overfitting); the comparative success on domain shifts (e.g., Math to Code) is observed empirically but not derived theoretically.
- **What evidence would resolve it:** A theoretical characterization of IM-RM learning dynamics under domain shift, or ablation studies isolating the features (e.g., structural generalization) that allow IM-RMs to succeed where EX-RMs fail.

### Open Question 2
- **Question:** How do IM-RMs and EX-RMs compare across evaluation criteria beyond accuracy, particularly regarding downstream policy optimization?
- **Basis in paper:** [explicit] Section 7.1 notes that while accuracy is the primary measure used, "it is not the only quantity that determines the effectiveness of a reward model" and suggests exploring a broader set of evaluation criteria.
- **Why unresolved:** The paper establishes that EX-RMs induce higher reward margins, which is beneficial for optimization, but it does not comprehensively evaluate the downstream effects on RL training stability or final policy quality across diverse tasks.
- **What evidence would resolve it:** Experiments measuring KL divergence, policy learning curves, and final task performance in an RLHF loop driven by both reward model types to see if the accuracy gap translates to a policy performance gap.

### Open Question 3
- **Question:** Do the implicit biases regarding token-level reliance and generalization persist in process-supervised reward models that evaluate intermediate reasoning steps?
- **Basis in paper:** [explicit] Section 7.1 explicitly lists studying "implicit biases introduced by additional types, e.g., reward models that provide rewards on intermediate steps of a response" as a direction for future work.
- **Why unresolved:** The theoretical and empirical analysis in this paper is restricted to outcome-based rewards (complete responses), leaving the behavior of process-based models unexplored.
- **What evidence would resolve it:** An experimental setup similar to the paper's controlled experiments (Section 5.1), but applied to intermediate reasoning steps in domains like math, comparing EX-PRMs vs. IM-PRMs on step-level paraphrases.

### Open Question 4
- **Question:** Can the theoretical constraints of fixed hidden representations and single-token responses be relaxed to provide a more general theory of generalization?
- **Basis in paper:** [explicit] Section 7.1 lists the simplifications used in Theorem 2 (single-token responses and fixed representations) and states that "alleviating these restrictions may yield further insights."
- **Why unresolved:** The theoretical guarantees for EX-RM generalization rely on these assumptions, while the empirical results suggest the conclusions hold more broadly, creating a gap between theory and practice.
- **What evidence would resolve it:** A formal extension of Theorem 2 that accounts for variable-length sequences and updating representations, showing that the max-margin separator logic holds under these complex dynamics.

## Limitations
- The theoretical analysis relies on simplified assumptions (single-token responses, fixed representations) that don't fully capture practical scenarios
- Experiments focus primarily on accuracy metrics, not comprehensive downstream RLHF performance evaluation
- The analysis doesn't address process-based reward models that evaluate intermediate reasoning steps

## Confidence
- **Theoretical analysis:** High - proofs are rigorous and clearly stated
- **Empirical results:** Medium - controlled experiments are well-designed but limited to specific datasets and model sizes
- **Practical implications:** Medium - conclusions about RL optimization follow from theoretical analysis but require additional validation

## Next Checks
1. Verify that EX-RM achieves significantly higher reward margins than IM-RM on UltraFeedback test set (target: >0.2 normalized difference)
2. Reproduce the near-random accuracy of IM-RMs on paraphrased responses while EX-RMs maintain high accuracy (target: IM-RM <0.1, EX-RM >0.9)
3. Confirm that both models achieve similar training accuracy but diverge on token-level generalization tasks