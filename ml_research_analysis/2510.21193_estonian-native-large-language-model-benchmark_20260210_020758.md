---
ver: rpa2
title: Estonian Native Large Language Model Benchmark
arxiv_id: '2510.21193'
source_url: https://arxiv.org/abs/2510.21193
tags:
- benchmark
- estonian
- evaluation
- tasks
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present a benchmark for evaluating large language models in
  Estonian across seven tasks including grammar, vocabulary, factual knowledge, summarization,
  and information extraction. The benchmark is based on native Estonian data and includes
  6 base and 26 instruction-tuned models.
---

# Estonian Native Large Language Model Benchmark

## Quick Facts
- arXiv ID: 2510.21193
- Source URL: https://arxiv.org/abs/2510.21193
- Reference count: 0
- Primary result: Native Estonian benchmark shows high correlation with human evaluation; LLM-as-a-judge approach validates as scalable alternative

## Executive Summary
We present a comprehensive benchmark for evaluating large language models on Estonian across seven tasks including grammar, vocabulary, factual knowledge, summarization, and information extraction. The benchmark uses native Estonian data sources without machine translation and includes 6 base and 26 instruction-tuned models. We evaluate results using both human judgment and an LLM-as-a-judge approach, with Claude 3.7 Sonnet showing strong alignment with human ratings (Pearson r = 0.94). The benchmark and evaluation framework are publicly released to support further research in Estonian NLP and other low-resource languages.

## Method Summary
The benchmark evaluates 32 models across seven native Estonian tasks using lm-evaluation-harness with task-specific configurations. Base models are evaluated in 5-shot setting while instruction-tuned models use zero-shot. Human evaluation involves pairwise comparison of 57 questions, with LLM-as-a-judge using Claude 3.7 Sonnet to validate results. All datasets are publicly available on HuggingFace under TalTechNLP organization, with task configurations on GitHub.

## Key Results
- LLM-as-a-judge (Claude 3.7 Sonnet) showed strong alignment with human ratings (Pearson r = 0.94)
- Estonian National Exam task showed highest correlation with human evaluation preferences (r = 0.86)
- Language-specific fine-tuning significantly improved performance, with EstLLM 8B outperforming other small models
- ROUGE-L metric showed weakest human correlation (r = 0.44) for news summarization task

## Why This Works (Mechanism)

### Mechanism 1
Native-source benchmark construction yields higher correlation with human preferences than translated benchmarks for low-resource languages. Datasets derived from authentic Estonian sources preserve cultural context and avoid translation artifacts that distort evaluation, reducing measurement noise and improving validity of capability signals.

### Mechanism 2
High-performing commercial LLMs can serve as reliable automated judges for low-resource language evaluation. Strong models like Claude 3.7 Sonnet encode sufficient multilingual competence to evaluate Estonian responses consistently with human raters, enabling scalable evaluation through pairwise comparison.

### Mechanism 3
Language-specific fine-tuning provides outsized gains for small models on low-resource languages, partially closing the gap with larger models. Targeted instruction-tuning on Estonian data injects language-specific patterns that are underrepresented in general pretraining, improving morphological handling and cultural knowledge.

## Foundational Learning

- **Benchmark localization vs. translation**: Understanding why native-source benchmarks outperform translated ones is essential for designing evaluation suites for other low-resource languages. *Quick check: Can you explain why translating an English benchmark to Estonian might yield misleading model rankings?*

- **LLM-as-a-Judge methodology**: The paper relies on this approach for scalable validation; understanding its limitations is critical for interpreting results. *Quick check: What conditions must hold for an LLM judge to be a valid substitute for human evaluators?*

- **Morphological richness in evaluation**: Estonian has 14 noun cases; the Declension benchmark tests this explicitly. Morphological competence is a key differentiator. *Quick check: Why might standard benchmarks underestimate LLM capabilities on morphologically rich languages?*

## Architecture Onboarding

- **Component map**: 7 benchmark tasks (Exams, Trivia, Declension, Word Meanings, Grammar Correction, News Summarization, Speaker Extraction) → lm-evaluation-harness with custom task definitions → Model inference → Metric computation → Human validation → LLM-judge correlation analysis

- **Critical path**: Dataset creation → Task definition in harness → Model inference → Metric computation → Human validation → LLM-judge correlation analysis

- **Design tradeoffs**: Native sourcing requires more effort but avoids translation noise; ROUGE-L for summarization showed weakest human correlation (r=0.44); 5-shot for base models vs. 0-shot for instruction-tuned makes scores non-directly comparable

- **Failure signatures**: Gemma models failed on speaker extraction due to insufficient context length; GPT-OSS-120B scored 0.0 on speaker extraction; large open models showed quality variance across API providers

- **First 3 experiments**: 1) Replicate Exams benchmark and compute correlation with human ratings to validate localization claim. 2) Test different LLM judge (GPT-4o) and measure agreement with Claude 3.7 Sonnet. 3) Fine-tune small model on Estonian data and measure improvement on Declension and Word Meanings tasks.

## Open Questions the Paper Calls Out

- How do current LLMs perform on social, cultural, and gender bias metrics specific to the Estonian language? The benchmark measures capability rather than social alignment, encouraging further study of fairness and bias.

- Can alternative evaluation metrics provide more reliable assessment of Estonian summarization quality than ROUGE-L? News summarization task showed anomalous behavior with ROUGE-L, suggesting metric limitations.

- Does the LLM-as-a-judge paradigm maintain high reliability when evaluating specific error types found in lower-performing or smaller models? High aggregate correlation may mask judge's inability to distinguish between different flavors of poor quality.

## Limitations

- The benchmark relies heavily on a single judge model (Claude 3.7 Sonnet), raising concerns about judge model dependency
- The Estonian-specific validation set of 57 questions is not publicly available, limiting independent verification
- The paper doesn't address potential cultural bias in native Estonian datasets that could affect generalizability

## Confidence

- **High confidence**: Localization advantage - Supported by strong theoretical justification and aligned with prior research
- **High confidence**: LLM judge validity - Direct empirical evidence shows strong correlation (r=0.94) with human evaluation
- **Medium confidence**: Fine-tuning benefits - Evidence is task-specific and limited to Estonian
- **Low confidence**: Cross-lingual generalization - The paper doesn't test findings on other low-resource languages

## Next Checks

1. Replicate the LLM-as-a-judge evaluation using GPT-4o or another commercial model as the judge, then measure correlation with both human evaluation and Claude 3.7 Sonnet

2. Create a parallel benchmark using machine-translated English tasks and compare model rankings to the native Estonian benchmark

3. Apply the same fine-tuning approach to a morphologically rich but distinct language (Finnish or Hungarian) and measure performance gains on declension tasks