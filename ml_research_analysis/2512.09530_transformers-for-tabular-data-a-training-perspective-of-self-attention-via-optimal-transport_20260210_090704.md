---
ver: rpa2
title: 'Transformers for Tabular Data: A Training Perspective of Self-Attention via
  Optimal Transport'
arxiv_id: '2512.09530'
source_url: https://arxiv.org/abs/2512.09530
tags:
- transformer
- data
- training
- optimal
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis investigates self-attention training through the lens
  of Optimal Transport (OT) and develops an OT-based alternative for tabular classification.
  The study tracks intermediate projections of the self-attention layer during training
  and evaluates their evolution using discrete OT metrics.
---

# Transformers for Tabular Data: A Training Perspective of Self-Attention via Optimal Transport

## Quick Facts
- arXiv ID: 2512.09530
- Source URL: https://arxiv.org/abs/2512.09530
- Reference count: 0
- This study introduces an Optimal Transport-based algorithm for tabular classification that achieves 100% accuracy on a biomedical dataset while reducing computational time compared to standard Transformers.

## Executive Summary
This research investigates self-attention mechanisms through the lens of Optimal Transport (OT) theory, developing a novel approach for tabular data classification. The study analyzes how self-attention mappings evolve during training using discrete OT metrics and identifies inefficiencies in the standard training trajectory. Building on these insights, the authors propose an OT-based algorithm that generates class-specific dummy Gaussian distributions and trains a multilayer perceptron to generalize the OT alignment between these distributions and the data. The method demonstrates competitive accuracy with reduced computational costs, particularly excelling on biomedical classification tasks.

## Method Summary
The proposed method introduces an OT-based algorithm that generates class-specific dummy Gaussian distributions and computes an OT alignment with the data. An MLP is then trained to generalize this mapping, effectively replacing the standard self-attention mechanism. The approach is designed to address the inefficiencies identified in self-attention training trajectories while maintaining classification accuracy. The method incorporates synthetic data pretraining for the MLP section, though this shows sensitivity to initialization. The OT-based algorithm scales more efficiently under standardized inputs and demonstrates reduced computational costs compared to traditional Transformers.

## Key Results
- Achieved 100% accuracy on Bangalore EEG dataset for binary classification vs 92.5% for Transformers
- Reduced computational time to 8.62s compared to 20.59s for Transformers on the same task
- Demonstrated comparable accuracy to Transformers while improving computational efficiency on standardized inputs

## Why This Works (Mechanism)
The OT-based approach works by directly optimizing the transport plan between data distributions rather than learning attention weights through gradient descent. By generating class-specific Gaussian distributions and computing the optimal coupling via OT, the method establishes a geometric alignment that an MLP can learn to generalize. This bypasses the inefficient training trajectory of standard self-attention while maintaining the ability to capture complex relationships in tabular data. The method effectively replaces the quadratic complexity of self-attention with OT computation, which scales more favorably for certain input distributions.

## Foundational Learning
- Optimal Transport theory - Understanding the mathematical framework for computing optimal couplings between probability distributions, which forms the theoretical foundation of the proposed method
- Self-attention mechanism - Knowledge of how transformers process tabular data through attention weights and the inefficiencies in their training dynamics
- Gaussian mixture models - Understanding how to generate class-specific dummy distributions that serve as anchors for the OT alignment
- MLP architecture and training - Familiarity with multilayer perceptrons and their ability to generalize geometric mappings learned from OT computations

Quick checks: Verify OT solver implementation, validate Gaussian distribution parameters, test MLP generalization capacity, benchmark against standard attention layers

## Architecture Onboarding

Component map: Input Data -> OT Solver -> Dummy Gaussian Generator -> MLP -> Classification Output

Critical path: The core computational pipeline involves computing the OT coupling between data and dummy distributions, then training an MLP to learn this mapping for classification.

Design tradeoffs: The method trades the adaptive complexity of self-attention for the geometric optimization of OT, sacrificing some flexibility for improved computational efficiency and more direct optimization of the transport plan.

Failure signatures: Performance degradation occurs when dummy Gaussian geometries are poorly designed or when class distributions have complex shapes that cannot be well-approximated by simple Gaussian distributions.

First experiments: 1) Test OT computation on synthetic two-class problems with varying separability, 2) Evaluate MLP generalization from OT coupling on simple tabular benchmarks, 3) Compare computational scaling with increasing feature dimensions

## Open Questions the Paper Calls Out
The study identifies several open questions regarding the robustness of the dummy-geometry design process and the method's generalizability across diverse tabular domains beyond the biomedical dataset tested.

## Limitations
- Performance heavily depends on careful design of dummy Gaussian distributions, requiring domain expertise
- Method focuses exclusively on classification tasks, limiting applicability to regression problems common in tabular analysis
- Results are primarily demonstrated on a single biomedical dataset, raising questions about generalizability to other tabular domains

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| OT theoretical framework application | High |
| Computational efficiency improvements | Medium |
| Accuracy comparisons across datasets | Medium |
| Method robustness across tabular domains | Low |

## Next Checks
1. Evaluate the OT-based method across multiple diverse tabular datasets (including regression tasks) to assess generalizability and identify failure modes
2. Conduct ablation studies on the dummy-geometry design process to quantify how sensitive performance is to different Gaussian configurations and determine if automated methods could be developed
3. Test the method's scalability on larger tabular datasets (thousands of samples, hundreds of features) to verify the claimed computational advantages maintain under real-world data volumes