---
ver: rpa2
title: 'Exploratory Mean-Variance with Jumps: An Equilibrium Approach'
arxiv_id: '2512.09224'
source_url: https://arxiv.org/abs/2512.09224
tags:
- policy
- 'true'
- market
- investment
- portfolio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper extends the exploratory mean-variance portfolio optimization
  problem to include jump-diffusion dynamics and time-inconsistent preferences. It
  formulates the problem as a time-inconsistent control (TIC) problem, derives the
  extended Hamilton-Jacobi-Bellman equations, and solves for an equilibrium investment
  policy that is a Gaussian distribution centered on the classical solution.
---

# Exploratory Mean-Variance with Jumps: An Equilibrium Approach

## Quick Facts
- arXiv ID: 2512.09224
- Source URL: https://arxiv.org/abs/2512.09224
- Authors: Yuling Max Chen; Bin Li; David Saunders
- Reference count: 0
- Key outcome: The paper extends the exploratory mean-variance portfolio optimization problem to include jump-diffusion dynamics and time-inconsistent preferences. It formulates the problem as a time-inconsistent control (TIC) problem, derives the extended Hamilton-Jacobi-Bellman equations, and solves for an equilibrium investment policy that is a Gaussian distribution centered on the classical solution. The authors leverage the martingale properties of their analytical solutions to design a reinforcement learning model and propose an Actor-Critic algorithm that learns the equilibrium policy without knowledge of market dynamics. The RL model parameters converge to true values in simulation studies. On 24 years of real market data, the proposed RL model is profitable in 13 out of 14 tests, demonstrating practical applicability. The key contribution is solving the exploratory mean-variance problem with jumps via the TIC approach, yielding an equilibrium policy that is optimal at any time during the investment horizon.

## Executive Summary
This paper addresses the time-inconsistent nature of mean-variance portfolio optimization by introducing an equilibrium approach based on time-inconsistent control theory. The authors extend the exploratory mean-variance framework to include jump-diffusion market dynamics and derive an equilibrium investment policy that remains optimal at any point during the investment horizon. They develop a reinforcement learning algorithm that learns this equilibrium policy without requiring knowledge of the true market dynamics, using a novel Orthogonality Condition loss based on martingale properties. The approach is validated through simulation studies showing parameter convergence and backtested on 24 years of S&P 500 data with 13 out of 14 profitable test periods.

## Method Summary
The authors formulate the exploratory mean-variance problem with jumps as a time-inconsistent control problem, deriving an equilibrium investment policy through extended Hamilton-Jacobi-Bellman equations. The policy is a Gaussian distribution centered on the classical mean-variance solution, with exploration controlled by an entropy regularization term. A reinforcement learning model is developed that learns the policy parameters using an Actor-Critic algorithm with an Orthogonality Condition loss based on martingale properties of the value function. The method is applied to both simulated Merton jump-diffusion data and real S&P 500 data over 14 rolling 11-year windows (10 years training, 1 year evaluation), using risk aversion γ=5 and exploration parameters λ=5 for training, λ=0.01 for evaluation.

## Key Results
- Simulation study shows all three model parameters (μ, σ, δ) converge to true values within 550 epochs
- Real market backtest yields profitable Sharpe ratios in 13 out of 14 test periods (2000-2023)
- MLE initialization of parameters is effective for the RL learning process
- The equilibrium policy maintains optimality throughout the investment horizon, unlike classical MV solutions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The equilibrium policy derived via time-inconsistent control (TIC) remains optimal at any point during the investment horizon, unlike classical MV solutions that are only optimal at initialization.
- **Mechanism:** The variance term in mean-variance optimization creates preference reversal (future selves deviate from today's optimal plan). The TIC approach models this as an intra-personal game where current and future selves reach a subgame-perfect Nash equilibrium. The extended HJB equations (3.5-3.6) characterize policies where no future self has incentive to deviate.
- **Core assumption:** Investors have time-inconsistent preferences due to the variance term's nonlinear dependence on conditional expectations.
- **Evidence anchors:**
  - [abstract] "our equilibrium policy is the best option an investor can take at any given time during the investment period"
  - [section 3] Definition 3.1 formalizes equilibrium via perturbed policies; Theorem 3.2 provides explicit Gaussian policy
  - [corpus] Related work (Dai et al. 2023) confirms TIC approach yields equilibrium solutions for EMV problems
- **Break condition:** If investors actually have time-consistent preferences (e.g., using exponential utility), the TIC framework is unnecessary.

### Mechanism 2
- **Claim:** Replacing deterministic controls with Gaussian exploratory policies enables RL learning while preserving convergence to classical solutions.
- **Mechanism:** Entropy regularization (λ term in 3.1) prevents policy collapse to deterministic controls and yields Gaussian policies analytically. The mean centers on the classical solution; variance is controlled by λ/γ(σ² + δ²). This creates informed exploration—sampling around the theoretically optimal action rather than randomly.
- **Core assumption:** The true market dynamics (μ, σ, jump parameters) are unknown but the structural form (jump-diffusion) is correct.
- **Evidence anchors:**
  - [abstract] "equilibrium investment policy, which is a Gaussian distribution centered on the equilibrium control of the classical MV problem"
  - [section 3, Theorem 3.2] Equation (3.10) gives explicit Gaussian form; Remark 3.1 explains entropy's role
  - [corpus] Wang & Zhou (2020) and Wang et al. (2020) established exploratory formulations with asymptotic equivalence proofs
- **Break condition:** If λ is too large, exploration dominates and policy becomes too diffuse; if λ → 0, learning fails due to insufficient exploration.

### Mechanism 3
- **Claim:** The Orthogonality Condition (OC) loss leverages martingale properties to drive parameter convergence without knowing true market dynamics.
- **Mechanism:** When θ = θ_true, the process M^θ (4.6) is a martingale (Theorem 4.1). The OC loss tests this by checking orthogonality between M^θ increments and test functions (∂V^θ/∂θ_j). If parameters are wrong, the martingale property is violated, creating a gradient signal. Actor-critic updates drive θ toward θ_true.
- **Core assumption:** The martingale property characterizes the true parameters uniquely (uniqueness of equilibrium not proven—authors acknowledge this limitation).
- **Evidence anchors:**
  - [abstract] "All of our RL model parameters converge to the corresponding true values in a simulation study"
  - [section 4] Theorem 4.1 proves martingale property; Definition 4.1 specifies OC loss; Algorithm 1 details training
  - [corpus] Jia & Zhou (2022a) introduced OC loss for continuous-time RL; corpus lacks direct validation of this specific jump-diffusion extension
- **Break condition:** If multiple parameter sets yield martingales (non-uniqueness), OC loss may converge to wrong values.

## Foundational Learning

- **Concept: Lévy Processes and Jump-Diffusion**
  - Why needed here: Market dynamics include sudden shocks (jumps) that Brownian motion cannot capture. The Poisson random measure N(t,B) and compensator Ñ(dt,dz) are essential for understanding the controlled dynamics.
  - Quick check question: Can you explain why Ñ(dt,dz) is a martingale while N(t,B) is not?

- **Concept: Time-Inconsistent Stochastic Control**
  - Why needed here: Classical dynamic programming fails when the objective includes variance. Understanding why Bellman's principle breaks down (non-separability of variance over time) motivates the equilibrium approach.
  - Quick check question: Why does maximizing E[X_T] - (γ/2)Var(X_T) violate time-consistency, but maximizing E[-exp(-γX_T)] does not?

- **Concept: Martingale Methods in RL**
  - Why needed here: The OC loss relies on the fact that temporal difference residuals under the true value function are martingales. This is the theoretical foundation distinguishing this approach from standard TD learning.
  - Quick check question: If M^θ is a martingale when θ = θ_true, what is E[dM^θ_t | F_t] for incorrect θ?

## Architecture Onboarding

- **Component map:**
  Market Environment (SDE 2.5, 2.8) -> Parameterized Policy π^θ (Gaussian, Eq. 4.4) -> Portfolio Process X^θ (SDE 4.5) -> Value/Auxiliary Functions V^θ, g^θ (Eqs. 3.11, 3.12) -> Martingale Process M^θ (Eq. 4.6) -> OC Loss L_OC(θ_j) (Eq. 4.11) -> Gradient Update θ ← θ + η · L(θ)

- **Critical path:**
  1. Initialize θ^(0) (MLE from training data works well—see Section 5.2)
  2. Simulate portfolio path X^θ using discretized SDE with Brownian increments and Poisson jumps
  3. Compute value function gradients ∂V^θ/∂θ_j using (4.8-4.10)
  4. Compute OC loss via accumulated martingale increments (4.11)
  5. Update parameters; repeat until convergence

- **Design tradeoffs:**
  - **Three parameters (μ, σ, δ) vs. more expressive models:** Authors chose interpretable parameterization to verify convergence; sacrifices flexibility
  - **Gaussian policy assumption:** Analytically tractable but may not capture multi-modal optimal strategies in more complex markets
  - **Aggregated jump parameter δ:** Simplifies learning but loses information about jump frequency vs. size distribution

- **Failure signatures:**
  - Parameters not converging: Check if discretization Δt is too coarse (authors use 1/252)
  - Poor real-market performance: Training period market regime differs from evaluation (2018, 2022 failures in Table 4)
  - Exploding portfolio values: γ too small relative to λ (insufficient risk aversion)

- **First 3 experiments:**
  1. **Sanity check:** Replicate simulation study (Section 5.1) with known θ_true; verify all three parameters converge within 550 epochs. Plot convergence curves like Figure 1.
  2. **Ablation on λ:** Test λ ∈ {0.1, 1, 5, 10} on simulated data. Confirm small λ slows convergence (under-exploration) and large λ increases variance but speeds initial learning.
  3. **Rolling window backtest:** Implement the 14-window test on S&P500 data (2000-2023). Compare Sharpe ratios against MLE baseline (Table 4). Investigate why 2018 and 2022 fail—does retraining with different λ help?

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the derived equilibrium investment policy unique, or do other equilibrium solutions exist for the exploratory mean-variance problem with jumps?
- Basis: [explicit] The authors state in the Conclusion and Section 1 that "the uniqueness of an equilibrium solution to a TIC problem remains as an open research topic," leading them to "disregard the uniqueness argument" in the current work.
- Why unresolved: The paper establishes the existence of an equilibrium policy via the verification theorem and extended HJB equations, but it does not provide a proof of uniqueness for the derived solution.
- What evidence would resolve it: A mathematical proof showing that the solution to the extended HJB equations is unique, or a counter-example demonstrating the existence of multiple distinct equilibrium policies.

### Open Question 2
- Question: How can the exploratory mean-variance with jumps (EMVJ) framework be extended to incorporate regime-switching market dynamics?
- Basis: [explicit] The authors list "Extension to a regime-switching market" as a specific direction for future work, noting that "an exploratory formulation of this problem remains untouched."
- Why unresolved: The current model assumes constant market parameters within a given time window; however, the numerical results on real data (specifically in 2018 and 2022) show performance drops when market trends flip, suggesting a need to model regime shifts explicitly.
- What evidence would resolve it: A formulation of the EMVJ problem where parameters follow a Markov chain, accompanied by a derived equilibrium policy and a modified RL algorithm that can adapt to changing regimes.

### Open Question 3
- Question: Can a Bayesian approach be effectively utilized to learn the model parameters and account for parameter uncertainty within the EMVJ framework?
- Basis: [explicit] The Conclusion proposes a "Bayesian approach to learn the model parameters" as a potential direction, suggesting the use of filtering methods or Bayesian RL.
- Why unresolved: The current Actor-Critic algorithm learns point estimates for the parameters θ = (μ, σ, δ), which does not capture the stochasticity or uncertainty of the parameters themselves.
- What evidence would resolve it: An algorithm that iteratively updates the posterior distributions of the market parameters, demonstrating convergence and potentially improved robustness compared to the point-estimate approach.

## Limitations

- The uniqueness of the equilibrium solution is not proven, which raises questions about whether the OC loss might converge to incorrect parameter values if multiple martingales exist
- The approach fails in years with market trend reversals (2018, 2022), suggesting limited robustness to regime changes
- Fixed hyperparameters (γ=5, λ=5) without systematic sensitivity analysis may artificially inflate real-market performance

## Confidence

- **High Confidence**: The theoretical framework (TIC approach, equilibrium characterization, martingale-based OC loss) is well-grounded in established literature. The simulation study demonstrating parameter convergence provides strong evidence for the learning mechanism.
- **Medium Confidence**: The real-market backtest results (13/14 profitable windows) are promising but require scrutiny of whether the evaluation methodology accounts for transaction costs, estimation error in MLE initialization, and whether the 11-year rolling windows are representative.
- **Low Confidence**: Claims about the policy being "optimal at any time" are theoretically supported but practically unverified beyond the stated backtest. The paper doesn't compare against alternative RL approaches or benchmark trading strategies beyond MLE.

## Next Checks

1. **Robustness to Market Regimes**: Re-run the backtest with additional stress tests: (a) expand 2018-2022 analysis to understand failure modes, (b) test on completely out-of-sample periods not adjacent to training data, (c) implement rolling window retraining to adapt to regime changes.

2. **Ablation on Exploration Parameter λ**: Systematically test λ ∈ {0.1, 1, 5, 10, 50} across both simulation and real data. Measure convergence speed, final Sharpe ratios, and sensitivity to market conditions. This addresses whether λ=5 is optimal or just coincidentally good.

3. **Benchmark Comparison**: Implement and compare against: (a) standard MV optimization with dynamic rebalancing, (b) alternative RL approaches without martingale-based losses, (c) a simple momentum or trend-following strategy. This establishes whether the TIC equilibrium approach provides meaningful advantage over simpler alternatives.