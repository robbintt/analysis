---
ver: rpa2
title: 'NeoDictaBERT: Pushing the Frontier of BERT models for Hebrew'
arxiv_id: '2510.20386'
source_url: https://arxiv.org/abs/2510.20386
tags:
- hebrew
- bert
- trained
- shmidman
- neodictabert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NeoDictaBERT and NeoDictaBERT-bilingual are BERT-style models for
  Hebrew NLP that adopt the NeoBERT architecture with RMSNorm, RoPE position encoding,
  SwiGLU activation, and extended context window (4,096 tokens). NeoDictaBERT was
  trained solely on Hebrew data; NeoDictaBERT-bilingual on a 60-40 English-Hebrew
  mix.
---

# NeoDictaBERT: Pushing the Frontier of BERT models for Hebrew

## Quick Facts
- arXiv ID: 2510.20386
- Source URL: https://arxiv.org/abs/2510.20386
- Authors: Shaltiel Shmidman; Avi Shmidman; Moshe Koppel
- Reference count: 9
- NeoDictaBERT-bilingual achieved 82.30 EM and 87.49 F1 on Hebrew QA benchmarks

## Executive Summary
NeoDictaBERT introduces two BERT-style models for Hebrew NLP: NeoDictaBERT (Hebrew-only) and NeoDictaBERT-bilingual (60-40 English-Hebrew mix). Both adopt the NeoBERT architecture with RMSNorm, RoPE position encoding, SwiGLU activation, and extended 4,096-token context windows. The models were trained using AdamW optimizer with batch size 2048 and NeMo Framework with mixed-FP8 precision. NeoDictaBERT-bilingual achieved 82.30 EM and 87.49 F1 on Hebrew QA, outperforming previous models. For retrieval, NeoDictaBERT-bilingual improved Hebrew retrieval NDCG@20 from 0.397 to 0.404 after fine-tuning on English data, demonstrating strong cross-lingual transfer capabilities.

## Method Summary
The models use a 28-layer encoder with 768 hidden size, RMSNorm normalization, RoPE position encoding, and SwiGLU activation. Training uses MLM-only objective with 20% masking rate and 100% mask token replacement. The process involves two phases: Phase 1 with 1,024-token context and learning rate 6e-3, followed by Phase 2 with 4,096-token context and learning rate 1e-4. NeoDictaBERT uses a 90K Hebrew vocabulary while NeoDictaBERT-bilingual uses a shared 128K vocabulary. Training was conducted on 232B tokens for Hebrew-only and 612B tokens for bilingual, using AdamW optimizer with 500 warmup steps and cosine annealing.

## Key Results
- NeoDictaBERT-bilingual achieved 82.30 EM and 87.49 F1 on Hebrew QA benchmarks
- NeoDictaBERT scored 76.40 EM and 82.83 F1 on Hebrew QA
- Hebrew retrieval NDCG@20 improved from 0.397 to 0.404 after fine-tuning bilingual model on English data only

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modern architectural components from decoder models improve encoder performance on Hebrew NLP tasks.
- Mechanism: RMSNorm, RoPE position encoding, and SwiGLU activation replace legacy LayerNorm, absolute position encodings, and GeLU. These components improve gradient flow and relative position modeling, which may particularly benefit morphologically rich languages like Hebrew.
- Core assumption: Architecture innovations that improved decoder models transfer to encoder-only models for Hebrew.
- Evidence anchors: [abstract] "models incorporate modern transformer enhancements including RMSNorm, RoPE position encoding, SwiGLU activation"; [section 2.1] Lists specific architectural changes vs. original BERT; [corpus] Neighbor paper "HalleluBERT" similarly adopts RoBERTa architecture for Hebrew with reported gains.
- Break condition: If gains disappear when controlling for parameter count and training data scale, architectural contribution is uncertain.

### Mechanism 2
- Claim: Extended 4,096-token context window enables better semantic understanding, particularly for QA tasks.
- Mechanism: Two-phase training (1,024 → 4,096 tokens) combined with RoPE's relative position encoding allows the model to handle longer documents natively, improving performance on tasks requiring cross-sentence reasoning.
- Core assumption: Hebrew QA tasks benefit from longer context; RoPE generalizes beyond training sequence lengths.
- Evidence anchors: [abstract] "extended context windows (4,096 tokens)"; [section 2.1] "train with an initial window of 1,024 tokens, extended to 4,096 tokens in the second phase"; [corpus] No direct corpus evidence comparing 512 vs. 4,096 token windows on Hebrew tasks.
- Break condition: If QA gains persist in ablations with truncated sequences, context length is not the causal factor.

### Mechanism 3
- Claim: Bilingual pretraining (60-40 English-Hebrew) enables cross-lingual transfer for retrieval tasks.
- Mechanism: Shared semantic representations learned from high-resource English data transfer to Hebrew retrieval, even when fine-tuned only on English corpora.
- Core assumption: English retrieval training data quality/quantity exceeds Hebrew; representations align across languages.
- Evidence anchors: [abstract] "improving Hebrew retrieval NDCG@20 from 0.397 to 0.404 despite being fine-tuned only on English data"; [section 4] "even though our model was fine-tuned only on English data, it improves the baseline"; [corpus] No corpus papers validate this cross-lingual transfer mechanism for Hebrew specifically.
- Break condition: If monolingual Hebrew model matches bilingual on retrieval after equal training, bilingual mixing is not necessary.

## Foundational Learning

- Concept: **RoPE (Rotary Position Embeddings)**
  - Why needed here: Replaces absolute position encodings, enabling relative position modeling and better length generalization critical for 4,096-token contexts.
  - Quick check question: Can you explain why RoPE allows extrapolation beyond training sequence lengths better than learned absolute embeddings?

- Concept: **Pre-RMSNorm vs. Post-LayerNorm**
  - Why needed here: NeoBERT uses Pre-RMSNorm which stabilizes training in deeper networks (28 layers vs. BERT's 12).
  - Quick check question: What is the gradient flow difference between normalization before vs. after the residual sub-layer?

- Concept: **SwiGLU Activation**
  - Why needed here: Gated activation function replacing GeLU; paper adopts this from modern decoder architectures without ablation in this work.
  - Quick check question: How does the gating mechanism in SwiGLU differ from a standard feed-forward layer with GeLU?

## Architecture Onboarding

- Component map: Tokenizer -> Encoder (28 layers, 768 width) -> MLM training -> Fine-tuning
- Critical path:
  1. Phase 1: 1,024-token context, max LR 6e-3, 232B tokens (monolingual) or 612B (bilingual)
  2. Phase 2: 4,096-token context, max LR 1e-4, ~1 epoch
  3. Fine-tune per downstream task (QA, NER, sentiment, retrieval)
- Design tradeoffs:
  - Monolingual (NeoDictaBERT) vs. Bilingual: Monolingual for pure Hebrew tasks; bilingual for cross-lingual retrieval
  - Depth vs. Width: 28 layers / 768 width follows Levine et al. optimal ratio; deeper than BERT but same width
  - Vocabulary: Shared 128K vocab for bilingual increases coverage but may dilute Hebrew token efficiency
- Failure signatures:
  - **Retrieval underperformance on Hebrew-only**: If cross-lingual transfer fails, fine-tune on Hebrew retrieval data directly
  - **OOM on long sequences**: 4,096 tokens require ~8x memory vs. 512; gradient checkpointing may be needed
  - **Tokenizer issues**: Hebrew-specific preprocessing from DictaBERT required; standard HuggingFace defaults may degrade performance
- First 3 experiments:
  1. **Baseline comparison**: Run inference on Hebrew sentiment dataset (Amram et al.) comparing NeoDictaBERT vs. DictaBERT to validate reported F1 gains (79.11 → 89.61).
  2. **Context length ablation**: Test QA performance (HeQ dataset) at 512, 1,024, and 4,096 tokens to isolate context window contribution.
  3. **Cross-lingual retrieval probe**: Fine-tune NeoDictaBERT-bilingual on English retrieval only, then evaluate on Hebrew Semantic Retrieval benchmark to reproduce NDCG@20 improvement (0.397 → 0.404).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal English-Hebrew training data ratio for bilingual Hebrew-focused models?
- Basis in paper: [inferred] The 60-40 English-Hebrew split for NeoDictaBERT-bilingual was chosen without ablation against alternative ratios.
- Why unresolved: No experiments compared different language mixture proportions to determine optimal balance.
- What evidence would resolve it: Training identical architectures with varying ratios (e.g., 80-20, 50-50, 30-70) and evaluating across all Hebrew benchmarks.

### Open Question 2
- Question: Why does English-only retrieval fine-tuning transfer to Hebrew retrieval tasks?
- Basis in paper: [inferred] The paper notes Hebrew retrieval NDCG@20 improved from 0.397 to 0.404 "even though our model was fine-tuned only on English data," but offers no mechanistic explanation.
- Why unresolved: Cross-lingual transfer mechanism in bilingual encoders for retrieval remains unanalyzed.
- What evidence would resolve it: Probing studies of embedding space alignment across languages; replication with other bilingual model configurations.

### Open Question 3
- Question: Which specific architectural innovations (RMSNorm, RoPE, SwiGLU, depth increase, context extension) drive the performance gains?
- Basis in paper: [inferred] All NeoBERT modifications were adopted simultaneously without isolating individual contributions.
- Why unresolved: Improvements cannot be attributed to specific components when all are introduced together.
- What evidence would resolve it: Ablation studies training models with individual architectural changes removed.

### Open Question 4
- Question: Why does monolingual NeoDictaBERT underperform the bilingual variant on Hebrew QA (76.40 vs 82.30 EM) despite being trained exclusively on Hebrew?
- Basis in paper: [inferred] Table 1 shows bilingual model substantially outperforms monolingual on Hebrew tasks, suggesting English pretraining data benefits Hebrew comprehension.
- Why unresolved: The advantage of including English data for Hebrew-only tasks is noted but not explained.
- What evidence would resolve it: Analysis of whether English data improves representation learning through shared cross-lingual patterns or larger effective training scale.

## Limitations

- The Hebrew corpus composition remains unspecified beyond being "expanded from DictaBERT corpus," making it impossible to determine whether performance gains stem from architectural innovations, increased data scale, or both.
- The bilingual model shows promising cross-lingual retrieval transfer but lacks direct comparison with Hebrew-only retrieval fine-tuning.
- The NeoBERT architectural components (RMSNorm, RoPE, SwiGLU) are adopted without ablation to isolate their individual contributions on Hebrew specifically.

## Confidence

**High Confidence**: The architectural specifications and training procedures are clearly described and follow established NeoBERT design patterns. The reported benchmark scores are internally consistent and demonstrate clear improvements over baseline Hebrew models.

**Medium Confidence**: The cross-lingual retrieval transfer mechanism is plausible given the high-resource English training data and bidirectional attention, but lacks direct validation through Hebrew retrieval fine-tuning comparisons. The claim that 4,096-token context improves QA performance is reasonable but untested against shorter context lengths on Hebrew benchmarks.

**Low Confidence**: Without corpus details or ablation studies, the relative contribution of each architectural innovation to Hebrew performance remains speculative. The performance improvements could be attributable to architectural changes, increased depth, larger context windows, or expanded training data rather than any single factor.

## Next Checks

1. **Hebrew-only retrieval baseline comparison**: Fine-tune NeoDictaBERT-bilingual on Hebrew retrieval data and compare against NeoDictaBERT-monolingual to determine whether bilingual pretraining provides additional benefit beyond shared architecture and training scale.

2. **Context length ablation on Hebrew QA**: Evaluate NeoDictaBERT performance on the HeQ benchmark using 512, 1,024, and 4,096 token contexts to isolate the contribution of extended context windows to the reported QA improvements.

3. **Architectural component ablation**: Train variant models with individual NeoBERT components (RMSNorm, RoPE, SwiGLU) removed while holding other factors constant to determine which innovations drive performance gains on Hebrew-specific tasks.