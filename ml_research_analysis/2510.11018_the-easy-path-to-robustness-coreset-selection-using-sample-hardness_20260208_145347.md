---
ver: rpa2
title: 'The Easy Path to Robustness: Coreset Selection using Sample Hardness'
arxiv_id: '2510.11018'
source_url: https://arxiv.org/abs/2510.11018
tags:
- training
- adversarial
- coreset
- samples
- aign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EasyCore, a coreset selection method that
  improves adversarial robustness by prioritizing samples with low average input gradient
  norm (AIGN), which are identified as "easy" samples that are learned quickly and
  occupy regions further from the decision boundary. The approach leverages the insight
  that sample hardness, quantified by AIGN, is linked to adversarial vulnerability.
---

# The Easy Path to Robustness: Coreset Selection using Sample Hardness

## Quick Facts
- arXiv ID: 2510.11018
- Source URL: https://arxiv.org/abs/2510.11018
- Reference count: 31
- Primary result: EasyCore achieves up to 7% improvement in adversarial accuracy over existing methods under standard training and up to 5% under TRADES adversarial training.

## Executive Summary
This paper introduces EasyCore, a data-centric coreset selection method that improves adversarial robustness by prioritizing training samples with low Average Input Gradient Norm (AIGN). The key insight is that samples with low AIGN are learned quickly, occupy regions farther from the decision boundary, and are less vulnerable to adversarial attacks. EasyCore is model-agnostic and requires computing AIGN only once per dataset, making it efficient for large-scale applications. Experiments show EasyCore consistently outperforms uniform and other coreset selection baselines across multiple datasets and training schemes.

## Method Summary
EasyCore selects training samples based on their Average Input Gradient Norm (AIGN), which measures how quickly a model learns each sample. The method first trains a proxy model on the full dataset and computes the input gradient norm for each sample at each epoch. These norms are averaged to obtain AIGN scores, which are used to rank and select the easiest samples (lowest AIGN). The selected coreset is then used to train the final model using either standard or adversarial training. AIGN is claimed to be a model-agnostic dataset property, allowing the selection process to be done once and reused across different model architectures.

## Key Results
- EasyCore achieves up to 7% improvement in adversarial accuracy compared to existing methods under standard training across various coreset fractions and datasets.
- Under TRADES adversarial training, EasyCore achieves up to 5% improvement in adversarial accuracy.
- The method demonstrates that training on easier samples results in a smoother decision boundary, enhancing robustness.
- EasyCore outperforms dynamic selection methods like ACS-B, especially on complex datasets like CIFAR-100.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selecting training samples with low Average Input Gradient Norm (AIGN) reduces adversarial vulnerability.
- Mechanism: Samples with low AIGN are learned quickly and occupy regions farther from the decision boundary. Training on these samples encourages a smoother, less curved decision boundary, making it harder for adversarial perturbations to cross class boundaries.
- Core assumption: Low AIGN samples are representative enough to define class features without introducing boundary complexity.
- Evidence anchors:
  - [abstract] "We demonstrate that *easy* samples (with low AIGN) are less vulnerable and occupy regions further from the decision boundary."
  - [section] Figure 6 and Claim 3 show that training on EasyCore improves adversarial accuracy on low-AIGN (easy) samples compared to uniform selection.
  - [corpus] Related work on scaling adversarial training via data selection (arXiv:2512.22069) supports the general idea that not all samples contribute equally to robustness, though the selection criteria differ.
- Break condition: If low-AIGN samples are too few or lack diversity, the resulting model may have poor clean accuracy and fail to generalize to harder test samples.

### Mechanism 2
- Claim: AIGN is a stable, model-agnostic dataset property.
- Mechanism: The ranking of samples by input gradient norm remains consistent across different model architectures. This allows AIGN to be computed once using a proxy model and reused for training other models.
- Core assumption: The input gradient norm ranking is sufficiently invariant across architectures to serve as a universal selection signal.
- Evidence anchors:
  - [abstract] "As AIGN is a model-agnostic dataset property, EasyCore is an efficient and widely applicable data-centric method."
  - [section] Figure 3 shows similar normalized AIGN distributions between VGG-16 and ResNet-18 on CIFAR-10. The method section states AIGN needs to be computed only once.
  - [corpus] Weak direct corpus evidence on model-agnostic stability; most related work focuses on model-specific selection criteria.
- Break condition: If models have fundamentally different feature extractors or inductive biases, AIGN rankings may not transfer effectively.

### Mechanism 3
- Claim: Static coreset selection based on low AIGN outperforms dynamic selection methods for adversarial robustness on complex datasets.
- Mechanism: EasyCore permanently excludes high-AIGN (hard) samples that increase boundary curvature and introduce vulnerability. Dynamic methods like ACS-B may re-select these harmful samples during training, especially as datasets become more complex with longer distribution tails.
- Core assumption: High-AIGN samples consistently harm robustness and provide negligible robustness benefits during later training stages.
- Evidence anchors:
  - [section] Table 3 and the Results section discussion: EasyCore outperforms ACS-B on CIFAR-100, especially at higher coreset fractions, while consuming less time.
  - [section] Claim 2 and Figure 4 visualize increased decision boundary curvature when training with high-AIGN samples.
  - [corpus] Non-Uniform Class-Wise Coreset Selection (arXiv:2504.13234) suggests class-aware selection improves fine-tuning, implicitly supporting the idea that static, informed selection can be effective, though not specifically for robustness.
- Break condition: If some high-AIGN samples encode rare but critical robust features, permanently discarding them could limit generalization on difficult adversarial examples.

## Foundational Learning

- **Concept: Adversarial Robustness and Decision Boundary Geometry**
  - Why needed here: The paper's core thesis links robustness to boundary curvature. You cannot evaluate EasyCore without understanding why a smoother boundary is hypothesized to be more robust.
  - Quick check question: Can you explain why a smoother decision boundary makes it harder for an attacker to find a successful perturbation within a small $\epsilon$-ball?

- **Concept: Coreset Selection**
  - Why needed here: The method is a coreset selection technique. Understanding the goal of coreset selection (efficient training on a subset that preserves performance) is essential to evaluate the tradeoffs introduced by optimizing for robustness instead of clean accuracy.
  - Quick check question: What is the primary objective of traditional coreset selection methods, and how does EasyCore's objective differ?

- **Concept: Input Gradient Norm**
  - Why needed here: AIGN is the single scoring metric used for all sample selection. Understanding that the input gradient points in the direction of steepest ascent of the loss with respect to input features is crucial to interpreting its link to sample hardness and adversarial attacks.
  - Quick check question: If a sample has a high input gradient norm, what does that suggest about the model's loss surface near that sample, and how might an adversary exploit this?

## Architecture Onboarding

- **Component map:**
  - Proxy Model Trainer -> AIGN Calculator -> Coreset Selector -> Target Model Trainer

- **Critical path:**
  1. The accuracy and stability of the AIGN calculation depend directly on the proxy model's training.
  2. The final robustness is contingent upon the coreset containing a sufficient number of diverse, prototypical samples.

- **Design tradeoffs:**
  - **Coreset Fraction (`f`):** A lower `f` increases training efficiency and often adversarial accuracy (up to a point) but may degrade clean accuracy. A higher `f` preserves clean accuracy but risks including more high-AIGN samples.
  - **Class Balancing:** Essential for large, imbalanced datasets like ImageNet but adds complexity. Without it, easy classes may be over-represented.
  - **Proxy Model vs. Target Model:** Using a smaller proxy model for AIGN computation saves time but assumes AIGN rankings transfer to the potentially larger or different target model architecture.

- **Failure signatures:**
  - **Clean Accuracy Collapse:** Adversarial accuracy is high, but clean accuracy is unacceptably low. Indicates the coreset is too small or lacks representational diversity.
  - **Poor Robustness Gain:** Adversarial accuracy is no better than uniform selection. Suggests the AIGN ranking was not computed correctly, the proxy model was insufficient, or the dataset's low-AIGN samples are not the robust-critical ones.
  - **Class Imbalance in Coreset:** For large datasets, some classes may disappear from the coreset entirely if class balancing is not applied, leading to 0% accuracy for those classes.

- **First 3 experiments:**
  1. **AIGN Transferability Check:** Train two different proxy models (e.g., ResNet-18 and VGG-16) on a subset of CIFAR-10. Compute AIGN rankings for both and measure the rank correlation. This validates the model-agnostic assumption before scaling up.
  2. **Sensitivity to Coreset Fraction:** For a single dataset (e.g., CIFAR-100), train target models using EasyCore coresets with fractions $f \in \{0.2, 0.4, 0.6, 0.8\}$. Measure both clean and adversarial accuracy to find the "sweet spot" that balances robustness and generalization.
  3. **Ablation on Proxy Training Length:** Compute AIGN using proxy models trained for different numbers of epochs (e.g., 10, 30, 50). Compare the robustness of the final models trained on the resulting coresets to determine how much proxy training is necessary for stable rankings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the trade-off between improved adversarial robustness and reduced clean accuracy be effectively mitigated?
- Basis in paper: [explicit] The methodology section states that "prioritizing robust-critical samples may come at the cost of clean accuracy," which is reflected in the results (Table 1) where EasyCore often shows lower clean accuracy than uniform baselines.
- Why unresolved: The current method optimizes for robustness by enforcing boundary smoothness, which naturally conflicts with fitting the hard/atypical features often necessary for maximum clean accuracy.
- What evidence would resolve it: A hybrid selection strategy or regularization method that retains high clean accuracy while preserving EasyCore's robustness gains.

### Open Question 2
- Question: How sensitive is the AIGN ranking to the specific proxy model architecture used for computation?
- Basis in paper: [inferred] The paper claims AIGN is a "model-agnostic dataset property" and requires computing AIGN only once, but relies on a single proxy architecture (e.g., ResNet-18) to score datasets.
- Why unresolved: It is unclear if the "hardness order" remains stable across significantly different architectures (e.g., CNNs vs. Transformers) or if the proxy must closely match the target model.
- What evidence would resolve it: A benchmark measuring the rank correlation of AIGN scores and final robustness when the proxy model is swapped for architecturally distinct networks.

### Open Question 3
- Question: Does EasyCore transfer robustness to threat models beyond $\ell_\infty$ bounded attacks?
- Basis in paper: [inferred] The experiments focus exclusively on PGD-$\ell_\infty$ attacks for evaluation.
- Why unresolved: Coresets optimized for a specific attack norm might overfit the decision boundary geometry relevant to that norm, potentially leaving the model vulnerable to others.
- What evidence would resolve it: Evaluation of EasyCore-trained models against diverse attacks such as $\ell_2$, $\ell_0$, or AutoAttack.

## Limitations

- **Unknown regularization parameter:** The TRADES hyperparameter β, critical for the robustness-accuracy trade-off, is omitted from Table 5, making it impossible to reproduce results or determine if EasyCore's gains are due to selection or implicit hyperparameter tuning.
- **Large-scale balancing details:** The ImageNet balancing procedure is only vaguely described. Without implementation details, it's unclear if the coreset truly preserves class coverage or if EasyCore's ImageNet results are biased by selection artifacts.
- **Proxy model consistency:** The paper states AIGN is model-agnostic but does not confirm if the proxy model used for AIGN computation was identical to the model used for final evaluation, leaving ambiguity about the claimed efficiency and transferability.

## Confidence

- **High Confidence:** The core mechanism that low-AIGN samples are less vulnerable and occupy regions farther from the decision boundary is supported by multiple empirical demonstrations (Figures 4, 6, and Claims 1-3).
- **Medium Confidence:** The model-agnostic transferability of AIGN rankings is plausible based on Figure 3's distribution comparisons, but lacks strong direct evidence from the literature or ablation studies.
- **Low Confidence:** The assertion that EasyCore consistently outperforms dynamic methods like ACS-B across all dataset complexities is primarily supported by CIFAR-100 results; generalization to other datasets or more complex data distributions is not fully validated.

## Next Checks

1. **AIGN Transferability Test:** Compute AIGN rankings using two different proxy models (e.g., ResNet-18 and VGG-16) on CIFAR-10. Measure the rank correlation to empirically validate the model-agnostic assumption before large-scale application.
2. **Coreset Fraction Sensitivity Analysis:** Systematically vary the coreset fraction `f` (e.g., 0.2, 0.4, 0.6, 0.8) on CIFAR-100. Measure the trade-off curve between clean accuracy and adversarial accuracy to identify the optimal balance point.
3. **TRADES Ablation Study:** Reproduce EasyCore's main results while varying the β parameter in TRADES (e.g., β ∈ {1, 3, 6, 10}). Determine if EasyCore's gains persist across different robustness-accuracy trade-offs or if they are sensitive to a specific β value.