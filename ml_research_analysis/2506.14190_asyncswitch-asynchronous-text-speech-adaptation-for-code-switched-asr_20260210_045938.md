---
ver: rpa2
title: 'AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR'
arxiv_id: '2506.14190'
source_url: https://arxiv.org/abs/2506.14190
tags:
- text
- speech
- data
- language
- malay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "AsyncSwitch introduces a three-stage asynchronous adaptation framework\
  \ that leverages large-scale unpaired text and limited speech-text data to improve\
  \ ASR performance in low-resource, code-switched settings. The method first adapts\
  \ the decoder\u2019s internal language model on code-switched text, then aligns\
  \ the encoder and decoder via cross-attention using paired data, and finally fine-tunes\
  \ the full model."
---

# AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR

## Quick Facts
- arXiv ID: 2506.14190
- Source URL: https://arxiv.org/abs/2506.14190
- Authors: Tuan Nguyen; Huy-Dat Tran
- Reference count: 37
- Primary result: 9.02% relative WER reduction on Malay-English code-switching vs Whisper-Large-v3

## Executive Summary
AsyncSwitch introduces a three-stage asynchronous adaptation framework that leverages large-scale unpaired text and limited speech-text data to improve ASR performance in low-resource, code-switched settings. The method first adapts the decoder's internal language model on code-switched text, then aligns the encoder and decoder via cross-attention using paired data, and finally fine-tunes the full model. Evaluated on Malay-English code-switching, the approach achieves a 9.02% relative WER reduction compared to Whisper-Large-v3, while also improving monolingual performance in Singlish and Malay. It outperforms commercial systems (I2R A*STAR and Azure) by 7.7% and 11.9% respectively, and shows strong generalization on the OpenASR Leaderboard without catastrophic forgetting. The framework effectively addresses data scarcity in multilingual code-switching by utilizing abundant textual resources.

## Method Summary
AsyncSwitch is a three-stage adaptation framework for improving ASR in low-resource code-switched scenarios. Stage 1 trains the decoder's self-attention, feedforward, and output projection layers on 38M unpaired code-switched text utterances while freezing the encoder and cross-attention layers. Stage 2 unfreezes cross-attention to align encoder representations with the adapted decoder using 5k hours of paired speech-text data. Stage 3 performs full fine-tuning on all parameters. The final model is created by linearly interpolating with the original Whisper-Large-v3 using a 0.4 ratio to prevent catastrophic forgetting. The approach is evaluated on Malay-English code-switching, Singlish, and Malay tasks.

## Key Results
- 9.02% relative WER reduction on Malay-English code-switching vs Whisper-Large-v3
- Outperforms commercial systems (I2R A*STAR and Azure) by 7.7% and 11.9% respectively
- Shows no catastrophic forgetting on OpenASR Leaderboard while maintaining strong monolingual performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Isolating decoder training on text-only data improves internal language model understanding of code-switched patterns without corrupting acoustic alignments.
- Mechanism: By zeroing encoder output (x = 0), the decoder operates as a conditional language model. Only self-attention (θSA), feedforward (θFF), and output projection (θout) are updated while cross-attention (θCA) remains frozen. This decouples language modeling from acoustic grounding.
- Core assumption: Whisper's acoustic encoder is already sufficiently robust; the bottleneck is the decoder's limited exposure to code-switched text distributions.
- Evidence anchors: [abstract] "trains decoder self-attention and feedforward layers on code-switched text"; [Section III-B-1] "We zero out the encoder output (x = 0) so that the decoder functions purely as a conditional language model"; Related work confirms text-only adaptation is an active research direction.

### Mechanism 2
- Claim: Sequential cross-attention realignment preserves Stage 1 text knowledge while re-establishing encoder-decoder coupling.
- Mechanism: After text adaptation, cross-attention layers (θCA) are unfrozen while encoder (θE) and previously adapted decoder components remain frozen. Paired speech-text data optimizes alignment without overwriting learned language patterns.
- Core assumption: Cross-attention parameters can bridge acoustic and linguistic representations without destabilizing frozen components.
- Evidence anchors: [abstract] "aligns the encoder and decoder via cross-attention using limited speech-text data"; [Section III-B-2] "strengthening the model's ability to align encoder representations to the newly adapted decoder LM"; Limited direct corpus evidence on cross-attention freezing strategies.

### Mechanism 3
- Claim: Model merging with original Whisper preserves generalization and mitigates catastrophic forgetting.
- Mechanism: Final fine-tuned model is interpolated with original Whisper-Large-v3 using ratio 0.4. This retains 60% of original weights, balancing domain adaptation with prior knowledge.
- Core assumption: Linear interpolation in parameter space preserves capabilities linearly.
- Evidence anchors: [Section V-B-3] "We choose a merging ratio of 0.4 for best code-switching performance (17.04) while maintaining acceptable Singlish and Malay results"; [Section V-A] "Our method improves English speech recognition by 5.37% compared to the original model, demonstrating successful knowledge retention"; Linear interpolation for fine-tuned models is supported by Rofin et al. (cited as [35]).

## Foundational Learning

- Concept: Encoder-decoder attention architecture (self-attention vs. cross-attention)
  - Why needed here: AsyncSwitch selectively freezes/unfreezes these components; misunderstanding which attends to what will cause implementation errors.
  - Quick check question: Which attention layer connects the encoder's acoustic representations to the decoder's token predictions?

- Concept: Internal language model in encoder-decoder ASR
  - Why needed here: Stage 1 explicitly adapts this internal LM; confusing it with external LM fusion will misguide experimentation.
  - Quick check question: Why does zeroing encoder output force the decoder to behave as a language model?

- Concept: Catastrophic forgetting in transfer learning
  - Why needed here: The paper explicitly claims prevention of forgetting via model merging; this motivates the multi-stage design.
  - Quick check question: What happens if you skip model merging and deploy the fully fine-tuned model directly?

## Architecture Onboarding

- Component map:
```
Whisper-Large-v3 (Frozen base)
├── Encoder θE (frozen Stages 1-2, unfrozen Stage 3)
└── Decoder θD
    ├── Self-Attention θSA (Stage 1: trained, Stage 2: frozen, Stage 3: unfrozen)
    ├── Feedforward θFF (Stage 1: trained, Stage 2: frozen, Stage 3: unfrozen)
    ├── Cross-Attention θCA (Stage 1: frozen, Stage 2: trained, Stage 3: unfrozen)
    └── Output Projection θout (Stage 1: trained, Stage 2: frozen, Stage 3: unfrozen)

Post-Training: Linear merge with original Whisper (ratio 0.4)
```

- Critical path: Stage 1 text preprocessing → Stage 1 decoder training → Stage 2 paired data alignment → Stage 3 full fine-tuning → Model merge → Evaluation
- Design tradeoffs:
  - Text corpus scale (38M vs. 1.7M): Larger corpus improves overall robustness; smaller focused corpus may overfit to specific patterns
  - Merging ratio (0.4 vs. 0.8): 0.4 optimizes code-switching; 0.8 optimizes overall average but degrades English scenarios
  - Stage 2 data: 5k hours used; reducing this may weaken alignment
- Failure signatures:
  - WER increases on OpenASR Leaderboard → catastrophic forgetting; increase original model merge ratio
  - Good monolingual but poor code-switch performance → text corpus may lack code-switch diversity (low CMI)
  - Stage 1 loss converges but Stage 2 destabilizes → paired speech-text distribution mismatch with text corpus
- First 3 experiments:
  1. Replicate Stage 1 on 1M text subset with validation loss tracking; confirm decoder adapts without cross-attention
  2. Ablate Stage 2 (skip to Stage 3 directly) on held-out code-switch test set; quantify alignment contribution
  3. Sweep merge ratios [0.2, 0.4, 0.6, 0.8] and plot WER across code-switch, monolingual, and OpenASR English sets

## Open Questions the Paper Calls Out
None

## Limitations

- Data Dependency Uncertainty: The effectiveness heavily depends on the quality and composition of the unpaired text corpus, with unclear filtering thresholds and CMI analysis.
- Generalization Across Code-Switch Pairs: Only evaluated on Malay-English; no evidence for other language pairs or different code-switching patterns.
- Cross-Attention Realignment Sensitivity: Stage 2's importance is stated but lacks extensive ablation studies on data efficiency or sensitivity to distribution mismatches.

## Confidence

**High Confidence**: The three-stage architecture is well-specified and experimental results on target tasks are clearly presented with measurable WER improvements. Commercial system comparisons and OpenASR Leaderboard results are convincing.

**Medium Confidence**: The 9.02% WER reduction claim is supported, but methodology transparency could be improved. The approach works for low-resource settings, though the 5k hours paired data requirement isn't fully explored.

**Low Confidence**: Broader generalization claims to other language pairs or truly low-resource scenarios lack support. Model merging strategy (ratio 0.4) is shown to work but sensitivity analysis is limited.

## Next Checks

1. Perform detailed CMI and language distribution analysis of the 38M text corpus to validate representativeness and identify potential biases introduced during filtering.

2. Conduct an ablation study varying the amount of paired speech-text data in Stage 2 (1k, 2.5k, 5k, 10k hours) to quantify minimum effective requirements and test data efficiency.

3. Apply AsyncSwitch to a different code-switching language pair (e.g., Spanish-English or Hindi-English) using publicly available datasets to test cross-lingual generalization and evaluate if the same approach works effectively.