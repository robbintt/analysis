---
ver: rpa2
title: 'Adaptive Data Selection for Multi-Layer Perceptron Training: A Sub-linear
  Value-Driven Method'
arxiv_id: '2510.21286'
source_url: https://arxiv.org/abs/2510.21286
tags:
- data
- selection
- learning
- training
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of selecting high-value training
  samples from massive, multi-source, and heterogeneous data for multi-layer perceptron
  (MLP) training under budget constraints. Existing methods suffer from high computational
  costs, scalability issues, and failure to account for MLP-specific complexities
  like layer-wise representations and dynamic parameter evolution.
---

# Adaptive Data Selection for Multi-Layer Perceptron Training: A Sub-linear Value-Driven Method

## Quick Facts
- **arXiv ID**: 2510.21286
- **Source URL**: https://arxiv.org/abs/2510.21286
- **Reference count**: 40
- **Primary result**: Achieves 3.77-6.61× computational speedup while maintaining 96.4-99.4% of full training accuracy

## Executive Summary
This paper addresses the challenge of selecting high-value training samples from massive, multi-source, and heterogeneous data for multi-layer perceptron (MLP) training under budget constraints. The proposed DVC (Data Value Contribution) method decomposes sample value into layer-wise and global contributions, evaluating six complementary metrics: Quality, Relevance, and Distributional Diversity at the layer level, plus Gradient Impact, Conditional Uncertainty, and Training Stability globally. The method employs efficient algorithms including adaptive weight learning via Bayesian optimization, layer gradient caching, locality-sensitive hashing for similarity computation, online statistical estimation, and multi-armed bandit-based source selection. Extensive experiments across six datasets and eight baselines demonstrate DVC consistently outperforms existing approaches, achieving superior accuracy and F1 scores.

## Method Summary
DVC decomposes sample value into Layer Value Contribution (LVC) and Global Value Contribution (GVC). LVC captures local feature learning using Quality, Relevance, and Distributional Diversity metrics per layer, while GVC assesses end-to-end optimization effects through Gradient Impact, Conditional Uncertainty, and Training Stability metrics. The method integrates these assessments with adaptive weight learning via Bayesian optimization, layer gradient caching, locality-sensitive hashing for diversity computation, online statistical estimation, and Upper Confidence Bound (UCB) algorithm for adaptive source selection. This coordinated approach achieves sublinear scaling complexity O(budget·n^ρ log n·d) where ρ<1, enabling efficient selection of high-value training samples while maintaining accuracy close to full training.

## Key Results
- DVC consistently outperforms eight baseline methods across six datasets
- Achieves 96.4-99.4% of full training accuracy with only 10-40% of training samples
- Demonstrates 3.77-6.61× computational speedup compared to full training
- Maintains sublinear scaling complexity O(budget·n^ρ log n·d) where ρ<1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing data value into layer-wise and global contributions enables more accurate sample evaluation for MLP training.
- Mechanism: The DVC method computes Layer Value Contribution (LVC) using Quality, Relevance, and Distributional Diversity metrics at each layer, and Global Value Contribution (GVC) using Gradient Impact, Conditional Uncertainty, and Training Stability metrics. These are combined via learned weights (Equations 5-6, 10). LVC captures local feature learning (e.g., stable activations at shallow layers), while GVC assesses end-to-end optimization effects.
- Core assumption: Layer-wise representations in MLPs provide meaningful, distinct signals for sample value that global metrics alone miss.
- Evidence anchors:
  - [abstract] "DVC method decomposes data contribution into Layer Value Contribution (LVC) and Global Value Contribution (GVC)."
  - [section 4] "We decompose sample value into layer-wise and global contributions... LVC_l(x,y) = α_l·Q_l(x) + β_l·R_l(x,y) + γ_l·D_l(x)" (Eq 6).
  - [corpus] Weak/Missing. No direct corpus evidence supports this specific dual decomposition; related work (e.g., Shapley Value-driven Data Pruning) focuses on value estimation but not layer-wise splits.
- Break condition: If layers do not exhibit meaningful variation in feature abstraction (e.g., very shallow networks or collapsed representations), the layer-wise decomposition provides no benefit over global-only metrics.

### Mechanism 2
- Claim: Multi-armed bandit (UCB) source selection accelerates convergence to high-quality data sources under budget constraints.
- Mechanism: DVC treats each data source as a bandit arm with unknown expected reward (sample quality). The UCB algorithm (Algorithm 5) maintains confidence bounds on source value estimates, prioritizing sources with high estimated value or high uncertainty. This balances exploration (sampling from uncertain sources) and exploitation (focusing on known good sources).
- Core assumption: Source quality is heterogeneous but stationary enough for UCB to learn reliable estimates within the selection budget.
- Evidence anchors:
  - [abstract] "DVC integrates these assessments with an Upper Confidence Bound (UCB) algorithm for adaptive source selection."
  - [section 5.5] "Sources with either high estimated value (exploitation) or high uncertainty (exploration) receive higher selection probabilities."
  - [corpus] Weak/Missing. Corpus contains no evidence directly assessing UCB for data source selection in MLP training.
- Break condition: If source quality drifts rapidly or arms (sources) are highly non-stationary, UCB's confidence intervals become unreliable, leading to suboptimal selection.

### Mechanism 3
- Claim: Sublinear computational scaling is achieved via coordinated algorithmic approximations (gradient caching, LSH, online estimation).
- Mechanism: (1) Layer Gradient Caching (Algorithm 2) reuses previously computed gradients via LRU eviction, exploiting temporal locality to reduce gradient recomputation. (2) LSH-based Similarity Computation (Algorithm 3) reduces diversity metric complexity from O(N²·d) to O(n^ρ log n·d) with ρ<1 via random hyperplane hashing. (3) Online Statistical Estimation (Algorithm 4) incrementally updates activation and gradient statistics using Welford's algorithm and EMA. These combine to yield overall complexity O(budget·n^ρ log n·d).
- Core assumption: (1) Temporal locality holds—recent gradients are frequently reused; (2) LSH preserves enough similarity structure for diversity selection; (3) Online statistics provide sufficient accuracy without batch recomputation.
- Evidence anchors:
  - [abstract] "The method achieves... computational speedup while maintaining... accuracy, with sublinear scaling complexity O(budget·n^ρ log n·d) where ρ<1."
  - [section 5.2-5.4] Algorithms 2-4 describe caching, LSH, and online estimation. Section 6.2 states "Each round processes candidates with DVC computation dominating at O((1-p)·L·d + h·n^ρ log n·d)."
  - [corpus] Weak/Missing. No corpus paper evaluates these specific algorithmic approximations in the context of DVC.
- Break condition: If cache hit rates are low (e.g., highly stochastic or non-repetitive sampling), or LSH yields poor recall for high-dimensional feature spaces, the speedup degrades toward linear complexity.

## Foundational Learning

- Concept: Multi-Armed Bandits (UCB)
  - Why needed here: To understand how DVC balances exploration vs exploitation across heterogeneous data sources.
  - Quick check question: Given 3 sources with unknown quality and 100 total samples to allocate, how does UCB decide which source to sample from next?

- Concept: Locality-Sensitive Hashing (LSH)
  - Why needed here: To grasp how DVC achieves sublinear similarity computation for the Distributional Diversity metric.
  - Quick check question: In LSH with random hyperplane hashing, do similar vectors hash to the same bucket with higher probability than dissimilar ones?

- Concept: Bayesian Optimization
  - Why needed here: To understand how DVC automatically learns adaptive weights for its six metrics.
  - Quick check question: Why is Bayesian optimization suitable for optimizing expensive-to-evaluate black-box functions like validation performance?

## Architecture Onboarding

- Component map:
  - **LVC Evaluator**: Computes Quality (Q_l), Relevance (R_l), Distributional Diversity (D_l) per layer.
  - **GVC Evaluator**: Computes Gradient Impact (GI), Conditional Uncertainty (CU), Training Stability (TS) globally.
  - **Weight Optimizer (Bayesian)**: Learns {λ_l, μ, α_l, β_l, γ_l, ξ, ζ, η} via Gaussian Process surrogate and Expected Improvement acquisition.
  - **Gradient Cache (LRU)**: Hash-indexed storage for layer-wise gradients; evicts oldest entries when capacity C is exceeded.
  - **LSH Index**: h hash tables with k-dimensional random projection matrices; maps samples to buckets for fast similarity queries.
  - **Online Statistics Estimator**: Maintains running means/variances (Welford) and gradient momentum (EMA) for DVC metrics.
  - **UCB Source Selector**: Maintains source value estimates and confidence bounds; computes source probabilities for multinomial sampling.

- Critical path: (1) Initialize caches, LSH tables, statistics, and UCB bandits. (2) For each round under budget: UCB selects sources → sample candidates → compute DVC via LVC/GVC evaluators (using cached gradients and LSH for diversity) → sort and diversify selection → update model, statistics, and bandit rewards. (3) Periodically trigger Bayesian weight optimization via performance evaluation.

- Design tradeoffs:
  - **Cache size C**: Larger cache increases memory but improves hit rate; smaller cache risks thrashing.
  - **LSH parameters (k, h)**: Higher k and h improve recall but increase index size and query time.
  - **Weight update frequency F**: Frequent updates adapt faster but incur more GP fitting cost; infrequent updates may miss distribution shifts.
  - **UCB exploration constant**: Implicit in confidence bound width; higher values increase exploration, potentially wasting budget on poor sources.

- Failure signatures:
  - **Accuracy plateaus early**: Possible weight collapse (one metric dominates) or source selection stuck in suboptimal arm.
  - **Excessive selection time**: Low cache hit rate or LSH returning too many candidates for similarity computation.
  - **Degraded F1 vs accuracy**: Class imbalance not handled; Diversity or Uncertainty metrics may be underweighted.
  - **Memory growth**: Cache or LSH index not evicting properly; statistics accumulating unbounded history.

- First 3 experiments:
  1. **Metric ablation on validation split**: Train with all six metrics vs removing one at a time on a held-out subset; report accuracy and F1 to confirm each metric's contribution as in Table 2.
  2. **Scaling benchmark**: Measure total selection+training time and accuracy for budgets 10%, 20%, 30% on 100K-500K samples; verify speedup factor and proximity criterion per Table 3.
  3. **Cache hit rate analysis**: Log cache hits/misses and LSH query times during a full selection run; correlate hit rate with total time to validate sublinear scaling assumptions.

## Open Questions the Paper Calls Out

- **Future work will include adapting DVC to deeper architectures.**
  - Specifically mentions adapting to deeper architectures like CNNs and Transformers.

- **Future work will include applying DVC to streaming data environments.**
  - Explicitly identifies streaming data as a target for future work.

- **Future work will include extending DVC to federated learning settings.**
  - Identifies federated learning as a direction for future work.

- **Future work will include optimizing the computational overhead of the Bayesian optimization component.**
  - Notes the need to optimize computational overhead in scenarios with rapidly shifting optimal weights.

## Limitations

- Critical architectural details (MLP layer counts, dimensions) are not specified, making faithful reproduction difficult.
- The paper relies on assumptions about layer-wise feature abstraction variation that lack corpus support.
- Sublinear scaling depends on fragile assumptions about cache hit rates and LSH effectiveness that are not fully validated in the corpus.
- The proximity criterion allows 4% absolute accuracy degradation, which may be significant in low-accuracy regimes.

## Confidence

- Layer-wise value decomposition effectiveness: **Medium**
- UCB source selection acceleration: **Medium**
- Sublinear computational scaling: **Medium**
- Overall accuracy/F1 improvements: **High**
- 3.77-6.61× speedup claims: **Low** (hardware and implementation dependent)

## Next Checks

1. **Ablation under controlled conditions**: Implement MLP with specified architecture and train with all six metrics vs removing one at a time on a fixed validation split; measure accuracy/F1 to confirm each metric's contribution.

2. **Scaling benchmark verification**: Measure total selection+training time and accuracy for budgets 10%, 20%, 30% on 100K-500K samples; verify speedup factor and proximity criterion.

3. **Cache hit rate analysis**: Log cache hits/misses and LSH query times during full selection run; correlate hit rate with total time to validate sublinear scaling assumptions.