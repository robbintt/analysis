---
ver: rpa2
title: 'Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers,
  Students and Evaluators'
arxiv_id: '2507.12143'
source_url: https://arxiv.org/abs/2507.12143
tags:
- questions
- systems
- evaluation
- answers
- material
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Sensemaking shared task at ELOQUENT 2025 evaluated how well
  generative language models can create questions, answer them, and evaluate responses
  based on input materials. Four teams participated, using 7 sources of test materials
  spanning English, German, Ukrainian, and Czech languages.
---

# Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators

## Quick Facts
- **arXiv ID**: 2507.12143
- **Source URL**: https://arxiv.org/abs/2507.12143
- **Reference count**: 38
- **Primary result**: LLM-as-a-Judge systems showed significant unreliability, often rating mismatched or garbled question-answer pairs as acceptable

## Executive Summary
The Sensemaking shared task at ELOQUENT 2025 evaluated how well generative language models can create questions, answer them, and evaluate responses based on input materials. Four teams participated, using 7 sources of test materials spanning English, German, Ukrainian, and Czech languages. The task involved three roles: Teacher (question generation), Student (answer generation), and Evaluator (assessment of responses). Automatic evaluation methods showed that Teacher systems performed reasonably well at question generation, with expert-made questions scoring among the best. However, Evaluator systems demonstrated significant unreliability, particularly when dealing with adversarial or mismatched inputs, highlighting the difficulty of achieving consistent results without robust automatic measures for text understanding evaluation.

## Method Summary
The Sensemaking task involved three stages: Teacher systems generated questions from input materials, Student systems answered these questions, and Evaluator systems assessed the quality of answers. Four teams participated, with test materials covering 7 sources across 4 languages. The task used automatic evaluation metrics including ROUGE-L Recall scores to measure performance. Teacher systems were evaluated on their ability to generate relevant questions, Student systems on their answer quality, and Evaluator systems on their assessment reliability. The study included adversarial testing to probe the robustness of LLM-as-a-Judge systems and examine how well automatic evaluation methods could distinguish between semantic similarity and shallow text-level relationships.

## Key Results
- Teacher systems performed reasonably well at question generation, with expert-made questions scoring among the best
- Student systems achieved moderate success in answering questions, with ROUGE-L Recall scores averaging 40-57% on expert-made questions
- Evaluator systems showed significant unreliability, often rating garbled or mismatched question-answer pairs as acceptable
- Automatic evaluation methods proved unreliable for assessing LLM performance as evaluators in adversarial scenarios

## Why This Works (Mechanism)
The study demonstrates that while LLMs can effectively generate and answer questions based on input materials, their reliability as evaluators remains problematic. The mechanism of evaluation breaks down when LLM judges conflate semantic similarity with shallow text-level relationships, leading to inconsistent and often incorrect assessments. This occurs because current automatic evaluation metrics cannot reliably distinguish between meaningful semantic alignment and superficial textual patterns, particularly in adversarial contexts where inputs are designed to test the limits of the evaluation systems.

## Foundational Learning
1. **ROUGE-L Recall metric** - Measures overlap between generated and reference text at character level; needed for automatic evaluation of text generation quality, quick check: compare ROUGE scores across different answer sources
2. **LLM-as-a-Judge paradigm** - Uses language models as automated evaluators; needed for scalable assessment of generated content, quick check: test judge consistency across adversarial examples
3. **Adversarial testing in NLP** - Creates challenging inputs to probe system robustness; needed to identify evaluation system weaknesses, quick check: measure performance drop on adversarial vs standard inputs
4. **Semantic similarity vs. surface similarity** - Distinguishes deep meaning from textual patterns; needed to understand evaluation failures, quick check: analyze where judges prefer surface matches over semantic correctness
5. **Multi-role evaluation framework** - Teacher-Student-Evaluator paradigm; needed for comprehensive assessment of LLM capabilities, quick check: verify role-specific performance metrics
6. **Cross-lingual evaluation** - Testing across multiple languages; needed for generalizability assessment, quick check: compare performance consistency across language pairs

## Architecture Onboarding
**Component Map**: Input Materials -> Teacher (Question Generation) -> Student (Answer Generation) -> Evaluator (Assessment) -> ROUGE-L Score

**Critical Path**: Teacher generates questions → Student answers questions → Evaluator assesses answers → Automatic metrics calculate scores. The most critical failure point occurs at the Evaluator stage, where current LLM-as-a-Judge systems demonstrate significant unreliability, particularly with adversarial or mismatched inputs.

**Design Tradeoffs**: Automatic evaluation offers scalability and efficiency but sacrifices reliability and nuance compared to human evaluation. The study prioritizes practical automatic metrics over labor-intensive human assessment, accepting that this approach may miss subtle semantic distinctions and produce inconsistent results in edge cases.

**Failure Signatures**: Evaluator systems incorrectly accept garbled answers when they contain surface-level matching text, rate mismatched question-answer pairs as acceptable, and conflate semantic similarity with shallow text relationships. These failures are particularly pronounced in adversarial testing scenarios.

**3 First Experiments**:
1. Run standard question-answer pairs through evaluator to establish baseline performance
2. Introduce adversarial examples with semantic mismatches to test evaluator robustness
3. Compare automatic evaluation results with human judgments on a subset of responses

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the reliability of automatic evaluation methods for LLM-as-a-Judge scenarios. Key questions include how to develop more robust automatic metrics that can distinguish between semantic similarity and shallow text relationships, whether human evaluation studies can provide reliable benchmarks for automatic evaluation systems, and how to improve the generalizability of findings across different languages and task types. The study also raises questions about the fundamental limitations of current evaluation paradigms when dealing with complex text understanding tasks.

## Limitations
- Small number of participating teams (four) limits generalizability of findings
- Limited number of test materials (7 sources) across multiple languages constrains robustness assessment
- Automatic evaluation methods proved unreliable for assessing LLM performance as evaluators, particularly with adversarial inputs
- Study primarily demonstrates limitations of current evaluation paradigms rather than providing comprehensive solutions

## Confidence
- **High**: Teacher systems perform reasonably well at question generation; supported by quantitative metrics showing expert-made questions scoring among the best
- **Medium**: Student systems achieve moderate success in answering questions (ROUGE-L Recall 40-57% on expert-made questions); demonstrated through consistent automatic evaluation results
- **Low**: Current automatic evaluation methods are insufficient for LLM-as-a-Judge scenarios; primarily based on specific experimental setup and requires broader validation

## Next Checks
1. Conduct the same evaluation with a larger number of diverse teams and test materials across additional languages to assess generalizability
2. Develop and test alternative automatic evaluation metrics that can better distinguish between semantic similarity and shallow text relationships in LLM-as-a-Judge scenarios
3. Implement human evaluation studies to compare against automatic metrics and establish ground truth reliability benchmarks for the evaluation systems