---
ver: rpa2
title: 'Self-Rewarding PPO: Aligning Large Language Models with Demonstrations Only'
arxiv_id: '2510.21090'
source_url: https://arxiv.org/abs/2510.21090
tags:
- reward
- arxiv
- learning
- fine-tuning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of overfitting and poor out-of-domain
  generalization in supervised fine-tuning (SFT) of large language models (LLMs),
  particularly in limited-data scenarios. The proposed method, Self-Rewarding PPO
  (SRPPO), combines the strengths of SFT and proximal policy optimization (PPO) to
  enhance alignment from demonstration data.
---

# Self-Rewarding PPO: Aligning Large Language Models with Demonstrations Only

## Quick Facts
- arXiv ID: 2510.21090
- Source URL: https://arxiv.org/abs/2510.21090
- Reference count: 39
- The paper proposes SRPPO, a method that combines supervised fine-tuning (SFT) with proximal policy optimization (PPO) to align LLMs using demonstration data only, without human preference annotations.

## Executive Summary
The paper addresses the problem of overfitting and poor out-of-domain generalization in supervised fine-tuning (SFT) of large language models (LLMs), particularly in limited-data scenarios. The proposed method, Self-Rewarding PPO (SRPPO), combines the strengths of SFT and proximal policy optimization (PPO) to enhance alignment from demonstration data. The core idea is a novel coherent reward function defined as the log policy ratio between the SFT model and the pretrained base model, which serves as an implicit reward signal for on-policy fine-tuning without requiring human preference annotations. Empirical results show that SRPPO consistently outperforms traditional SFT methods across various NLP tasks.

## Method Summary
SRPPO operates in two stages: First, supervised fine-tuning (SFT) on demonstration data to create a reference policy. Second, PPO fine-tuning using a novel "coherent reward" function defined as the log probability ratio between the SFT model and the pretrained base model. This reward is assigned only at the [EOS] token to avoid length degeneration. The method uses on-policy sampling to improve generalization, allowing the use of additional prompts beyond the demonstration set. The actor and critic models are initialized from the SFT checkpoint, with the pretrained model serving as a frozen baseline for reward calculation.

## Key Results
- SRPPO achieved 47.60/41.37 L.Acc/S.Acc on Mistral-7B with minimum overlap, compared to SFT's 42.45/40.53
- Significant improvements in instruction following, math reasoning (GSM8k), and scientific question answering (GPQA)
- Consistent performance gains across multiple model scales (Mistral-7B, LLaMA3-8B) and data regimes
- Effective generalization to out-of-domain prompts without requiring human preference annotations

## Why This Works (Mechanism)

### Mechanism 1: Implicit Reward Shaping via Policy Divergence
The method calculates a "Coherent Reward" $\tilde{r}(x,y) = \log(p_{SFT}(y|x) / p_{PT}(y|x))$, quantifying how much the SFT model "prefers" a specific response trajectory compared to the base pre-training distribution. By optimizing this ratio, the model is pushed toward the SFT distribution without needing an external reward model.

### Mechanism 2: Mitigation of Distribution Shift via On-Policy Sampling
Standard SFT suffers from covariate shift; it learns $p(y|x)$ only for $x$ in the training set. PPO samples $y$ from the current model $\pi_\theta$, forcing the model to learn from its own mistakes and handle states reached by its current policy, improving generalization.

### Mechanism 3: Generalization from Demonstrations to Unseen Prompts
Because the reward is calculated analytically from the SFT/PT models, it can evaluate *any* prompt-response pair. This allows the PPO phase to ingest a broader set of prompts than the original demonstration set, provided the SFT policy generalizes enough to score them meaningfully.

## Foundational Learning

**Behavior Cloning vs. Distributional Shift**
- Why needed: The paper frames standard SFT as "behavior cloning" (off-policy). Understanding why BC fails is essential to understand *why* on-policy PPO is proposed as a solution.
- Quick check: Why does maximizing likelihood on a static dataset lead to degradation on out-of-domain tasks?

**Trust Region / Proximal Optimization**
- Why needed: The method uses PPO (Proximal Policy Optimization). Understanding the "Clip" mechanism is vital to prevent the model from destroying its pre-existing capabilities during the aggressive on-policy updates.
- Quick check: What happens to training stability if the KL-divergence constraint or clipping parameter is removed during RL fine-tuning?

**Log-Likelihood Ratios as Rewards**
- Why needed: The core novelty is deriving reward $r \propto \log \pi_{SFT} - \log \pi_{PT}$. This is a standard technique in control as inference (or IRL), but applied here using the models themselves.
- Quick check: Why subtract the pretrained model's log-prob? (Hint: It acts as a baseline to normalize the score).

## Architecture Onboarding

**Component map:** Pretrained Model (Frozen) $\pi_{PT}$ -> SFT Model (Reference Policy) -> Policy Model (Actor) -> Critic Model (Value Function) -> Reward Calculator

**Critical path:** The critical implementation detail is the **reward calculation**. You must compute `log_prob_SFT` and `log_prob_PT` for the *exact same sequence* generated by the policy model. This requires forward passes through both the frozen PT and frozen SFT models for every batch of generated data.

**Design tradeoffs:**
- **Process vs. Token Reward:** The paper notes that calculating the reward at the *token level* encourages length degeneration. Must assign the reward at the EOS token (process level) to force the model to care about sequence quality, not just length.
- **Data Overlap:** Minimal overlap between SFT data and PPO prompts tests generalization; Medium overlap stabilizes training but risks overfitting.

**Failure signatures:**
- **Length Hacking:** If you mistakenly implement token-wise rewards without normalization, the model will generate endless text.
- **Reward Collapse:** If the SFT model is bad, the coherent reward will reinforce bad behavior (e.g., hallucinations).
- **Capability Forgetting:** If the PPO learning rate is too high or KL penalty too low, the model may forget the reasoning capabilities present in the pretrained weights.

**First 3 experiments:**
1. **Sanity Check (Reward Distribution):** Visualize the coherent reward for high-quality vs. random responses on a validation set. Is the score clearly higher for ground-truth data?
2. **Ablation (SFT vs. SRPPO):** Train a model on a small dataset using only SFT, then compare against SRPPO. Look specifically at out-of-domain benchmarks to verify the "generalization" claim.
3. **Token-Level Failure Reproduction:** Implement the token-wise reward (Appendix E) and observe the generation length explode, confirming the necessity of the EOS-sparse reward design.

## Open Questions the Paper Calls Out

**Open Question 1:** Can the coherent reward be reformulated as a token-wise signal without inducing length degeneration in the model outputs? The authors explored this but faced length degeneration and leave this for future exploration.

**Open Question 2:** How does the limited generalization capability of small-scale pretrained models (e.g., Phi-2) impact the effectiveness of the coherent reward? The method may face challenges if the pretrained model has limited coverage for out-of-distribution prompts.

**Open Question 3:** To what extent does the presence of low-quality demonstration data degrade the generalization capability of the derived coherent reward? While theoretically acknowledged, the method's robustness to specific noise levels or imperfect demonstrations was not quantified experimentally.

## Limitations
- The method's performance in extreme data scarcity scenarios beyond tested conditions remains unclear
- The coherent reward depends entirely on the quality of the SFT model, with no analysis of sensitivity to SFT hyperparameters
- The method requires maintaining three separate model instances during training, tripling computational overhead

## Confidence

**High Confidence Claims:**
- The method architecture (SFT + PPO with coherent reward) is technically sound and implementable
- The coherent reward calculation (log-ratio between SFT and pretrained models) is correctly specified
- The on-policy sampling approach is valid and theoretically grounded

**Medium Confidence Claims:**
- The method improves generalization compared to SFT alone (empirical evidence supports this but may depend on specific datasets)
- The coherent reward provides meaningful signal for optimization (functionally correct but quality depends on SFT model)
- The method works across multiple tasks and model scales (supported by results but limited to tested domains)

**Low Confidence Claims:**
- The method's performance in extreme data scarcity scenarios beyond tested conditions
- The robustness of the coherent reward signal across vastly different prompt distributions
- The computational efficiency claims relative to standard SFT implementations

## Next Checks

1. **Reward Signal Quality Analysis:** Visualize the distribution of coherent rewards for high-quality vs. random responses on held-out validation sets to validate whether the reward signal meaningfully distinguishes between good and bad responses.

2. **Extreme Data Scarcity Test:** Evaluate SRPPO performance on datasets with significantly fewer examples than the minimum-overlap setup (e.g., 1-5k examples total) to test the claimed advantage in truly limited-data scenarios.

3. **SFT Quality Sensitivity Analysis:** Systematically vary SFT training duration, learning rates, and data quality to measure impact on downstream PPO performance to quantify how sensitive the method is to the SFT model quality.