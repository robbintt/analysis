---
ver: rpa2
title: 'Context-Selective State Space Models: Feedback is All You Need'
arxiv_id: '2510.14027'
source_url: https://arxiv.org/abs/2510.14027
tags:
- state
- coffee
- sequence
- embedding
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COFFEE, a context-selective state-space model
  that uses state feedback to enable context-based selectivity in sequence modeling.
  Unlike S6, which relies on token-based selectivity, COFFEE modulates its dynamics
  using the internal state, allowing it to adapt behavior based on accumulated context.
---

# Context-Selective State Space Models: Feedback is All You Need
## Quick Facts
- arXiv ID: 2510.14027
- Source URL: https://arxiv.org/abs/2510.14027
- Authors: Riccardo Zattra; Giacomo Baggio; Umberto Casti; Augusto Ferrante; Francesco Ticozzi
- Reference count: 40
- Key outcome: COFFEE achieves near-perfect accuracy on induction head task with two orders of magnitude fewer parameters than S6 and reaches 97% accuracy on MNIST with only 3585 parameters

## Executive Summary
This paper introduces COFFEE, a context-selective state-space model that uses state feedback to enable context-based selectivity in sequence modeling. Unlike S6, which relies on token-based selectivity, COFFEE modulates its dynamics using the internal state, allowing it to adapt behavior based on accumulated context. The model also eliminates parameter redundancy through basis changes, reducing the number of learnable parameters. Experiments demonstrate that COFFEE outperforms S6 on both induction head and MNIST tasks while using significantly fewer parameters.

## Method Summary
COFFEE introduces a state feedback mechanism to state-space models that enables context-selective behavior. The core innovation involves using the internal state of the system to modulate the dynamics matrix, allowing the model to adapt its behavior based on accumulated context rather than just the current token. The authors also introduce basis changes to eliminate parameter redundancy in the state-space representation, reducing the total number of learnable parameters. This combination of state feedback and parameter efficiency enables COFFEE to achieve competitive performance with significantly fewer parameters than existing approaches like S6.

## Key Results
- COFFEE achieves near-perfect accuracy on induction head task with two orders of magnitude fewer parameters than S6
- Reaches 97% accuracy on MNIST task with only 3585 parameters
- Demonstrates superior parameter efficiency compared to token-based selectivity approaches

## Why This Works (Mechanism)
The state feedback mechanism allows COFFEE to modulate its dynamics based on the internal state rather than just the current input token. This creates a form of context awareness where the model's behavior can change based on accumulated information. The basis change optimization eliminates redundant parameters in the state-space representation, making the model more parameter-efficient without sacrificing expressivity.

## Foundational Learning
- State-space models: Why needed - provide a continuous-time framework for sequence modeling; Quick check - understand the canonical form A, B, C, D matrices
- Context selectivity: Why needed - enables adaptive behavior based on sequence context; Quick check - compare token-based vs state-based selectivity
- Parameter redundancy: Why needed - inefficient use of parameters can limit model capacity; Quick check - understand how basis changes can eliminate redundancy
- Feedback control: Why needed - allows system output to influence future behavior; Quick check - trace how state feedback affects dynamics matrix
- Induction heads: Why needed - test pattern recognition in sequences; Quick check - understand the induction head task requirements
- MNIST classification: Why needed - standard benchmark for sequence models; Quick check - verify COFFEE's approach to image classification

## Architecture Onboarding
Component map: Input -> State-space block (with state feedback) -> Output
Critical path: Input sequence → State evolution (modulated by feedback) → Output computation
Design tradeoffs: State feedback provides context awareness but may introduce stability challenges; parameter reduction through basis changes improves efficiency but requires careful implementation
Failure signatures: Poor performance on induction head suggests inadequate context selectivity; low MNIST accuracy indicates issues with state feedback implementation
First experiments:
1. Verify state feedback affects dynamics matrix as expected
2. Test parameter reduction from basis changes
3. Validate induction head task performance with minimal parameters

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental setup for induction head task lacks full architectural specifications
- State feedback mechanism's behavior in longer sequences and more complex tasks remains unclear
- Basis change optimization's impact on model expressivity is not fully characterized

## Confidence
- High confidence: The mathematical framework for state feedback in state-space models is sound and well-defined
- Medium confidence: The MNIST results (97% accuracy with 3585 parameters) are verifiable but depend on the specific implementation details
- Medium confidence: The claim of two orders of magnitude fewer parameters than S6 needs more detailed architectural comparison

## Next Checks
1. Replicate the induction head experiment with controlled hyperparameter sweeps to verify the claimed efficiency gains and establish the exact parameter reduction ratio
2. Test COFFEE on longer sequence tasks (beyond the 10k tokens used in induction head) to evaluate state feedback's scalability
3. Conduct ablation studies comparing COFFEE with and without basis change optimization to quantify the trade-off between parameter efficiency and model performance