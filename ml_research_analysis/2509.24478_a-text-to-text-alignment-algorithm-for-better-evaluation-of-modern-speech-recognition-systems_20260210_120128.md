---
ver: rpa2
title: A Text-To-Text Alignment Algorithm for Better Evaluation of Modern Speech Recognition
  Systems
arxiv_id: '2509.24478'
source_url: https://arxiv.org/abs/2509.24478
tags:
- alignment
- speech
- alignments
- recognition
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurately aligning reference
  and hypothesis transcripts in speech recognition evaluation. Standard Levenshtein-based
  methods often fail to capture plausible alignments, especially for substitutions
  adjacent to insertions or deletions and for languages with high compounding or agglutination.
---

# A Text-To-Text Alignment Algorithm for Better Evaluation of Modern Speech Recognition Systems

## Quick Facts
- **arXiv ID**: 2509.24478
- **Source URL**: https://arxiv.org/abs/2509.24478
- **Reference count**: 0
- **Primary result**: A two-pass constrained beam search algorithm improves speech recognition alignment quality across 9 languages, achieving higher Global-to-Local Edits (GLE) scores than baselines.

## Executive Summary
This paper addresses the challenge of accurately aligning reference and hypothesis transcripts in speech recognition evaluation. Standard Levenshtein-based methods often fail to capture plausible alignments, especially for substitutions adjacent to insertions or deletions and for languages with high compounding or agglutination. To improve alignment quality, the authors propose a two-pass algorithm that couples dynamic programming with beam search scoring. The method penalizes deviations from the backtrace graph, uses character-level features with structured transition costs, and incorporates word-level alignment rules. Evaluated across multiple English datasets and eight non-English languages, the approach consistently outperforms baselines including optimized word-level alignment and phonetic aligners, achieving higher Global-to-Local Edits (GLE) scores—e.g., 58.9% vs. 65.8% for character-level GLE on Common Voice English with Whisper. The algorithm is made available via PyPI to support fine-grained error analysis in speech recognition.

## Method Summary
The algorithm uses a two-pass approach: first constructing a backtrace graph containing all minimum-edit-distance paths (with doubled substitution costs), then performing beam search over the full graph while penalizing deviations from the backtrace. It incorporates character-level features with transition costs based on vowel/consonant classification and adds word-level alignment rules through preprocessing delimiters. The method is evaluated on Common Voice, TED-LIUM, and PriMock57 datasets using Whisper v3, Phi-4-multimodal, and Parakeet TDT/CTC models, measuring performance via Global-to-Local Edits (GLE) score.

## Key Results
- Outperforms word-level Levenshtein alignment (RapidFuzz) by 6.9 percentage points on Common Voice English with Whisper (65.8% vs 58.9% character-level GLE)
- Achieves higher phoneme-level scores than Power aligner across all evaluated datasets despite simpler phonemic classification
- Shows consistent improvement across eight non-English languages with diverse morphological properties
- Ablation studies confirm critical contributions: substitution penalty (-4.3 GLE), backtrace graph anchoring (-2.2 GLE), and phonemic transition costs (-1.3 GLE)

## Why This Works (Mechanism)

### Mechanism 1: Two-Pass Constrained Search (Backtrace Graph + Beam Search)
- Claim: Restricting beam search to paths near the Levenshtein-optimal backtrace graph improves alignment quality while remaining computationally tractable.
- Mechanism: The algorithm first constructs a backtrace graph G_b containing all minimum-edit-distance paths (with doubled substitution costs to expand the graph). It then performs beam search over the full graph, adding a penalty term [+1] when paths deviate from G_b nodes (eq. 4). This focuses computational resources on plausible alignments rather than exhaustively searching all possible character mappings.
- Core assumption: Linguistically plausible alignments lie close to minimum-edit-distance paths, not arbitrarily far from them.
- Evidence anchors:
  - [Section 3.2]: "To prioritize the most promising paths, we introduce a penalty term when a path deviates from the backtrace graph G_b, which substantially improves both the robustness and efficiency of the beam search."
  - [Table 4 ablation]: "Search restricted to G_b" yields -2.2 GLE degradation, confirming the backtrace graph alone is insufficient but provides a strong anchor.
  - [corpus]: No direct corpus comparison; this appears novel to this work.
- Break condition: If optimal alignments systematically require paths far from minimum edit distance (e.g., extreme reordering phenomena), the penalty term would exclude correct solutions.

### Mechanism 2: Substitution Penalty in Open Cost (Non-Optimal Substructure)
- Claim: Doubling the open cost for substitutions prevents linguistically implausible alignments between dissimilar words, even when they minimize edit distance.
- Mechanism: The normalized cost formula (eq. 1) includes an indicator function [(i−k)(j−l)>0] that doubles c_o when both reference and hypothesis positions advance simultaneously (indicating substitution). This accumulated penalty means the problem lacks optimal substructure—preventing a pure dynamic programming solution and necessitating beam search.
- Core assumption: Words with many character substitutions should not be aligned as direct correspondences; separate deletion+insertion is preferred.
- Evidence anchors:
  - [Section 3.3]: "The indicator function imposes a penalty by doubling c_o for substitutions, ensuring that dissimilar words are not aligned."
  - [Table 4 ablation]: "Eq. (1) w/o substitution pen." shows -4.3 degradation—the largest single ablation impact.
  - [corpus]: The Power aligner ( Ruiz & Federico, 2015) uses phonetic similarity for substitution spans, suggesting phonological plausibility is a recognized concern, though implemented differently.
- Break condition: If valid substitutions are systematically penalized (e.g., systematic phonological reductions in casual speech), recall of correct alignments may decrease.

### Mechanism 3: Phonemically-Informed Transition Costs
- Claim: Coarse phonemic classification (vowel/consonant) for transition costs captures linguistic similarity better than unit costs.
- Mechanism: Transition costs (eq. 5, Table 2) differentiate: matches [0], unvoiced insertions/deletions [1], voiced insertions/deletions [2], same-class substitutions (vowel-vowel or consonant-consonant) [2], cross-class substitutions [3]. This encodes that substituting a vowel for a consonant is costlier than within-class substitutions.
- Core assumption: Vowel-consonant classification approximates phonetic similarity sufficiently for alignment purposes.
- Evidence anchors:
  - [Table 2]: Explicit cost structure showing phonemic awareness.
  - [Table 3]: "Although the Power aligner is explicitly optimized for phonetic similarity, our approach achieves higher phoneme-level scores across every dataset and model."
  - [Table 4 ablation]: "Eq. (5) w/ unit-cost" shows -1.3 degradation, confirming non-unit costs help.
  - [corpus]: Power aligner (Ruiz & Federico, 2015) uses full phoneme conversion; this work achieves comparable results with simpler vowel/consonant classification.
- Break condition: For languages where vowel/consonant distinctions poorly predict phonetic similarity (e.g., tonal languages not addressed in evaluation), costs may misalign with actual confusion patterns.

## Foundational Learning

- Concept: **Levenshtein distance and backtrace extraction**
  - Why needed here: The entire method builds on standard edit distance computation; understanding how multiple optimal paths create ambiguity in the backtrace is essential.
  - Quick check question: Given strings "abc" and "adc," can you trace all cells in the DP table that contribute to minimum-distance paths?

- Concept: **Beam search**
  - Why needed here: The algorithm uses beam search (beam size = 100) to explore paths near the backtrace graph; understanding pruning and candidate expansion is necessary for implementation.
  - Quick check question: If beam size is 5 and 8 candidates have costs [0.1, 0.2, 0.2, 0.3, 0.3, 0.4, 0.4, 0.5], which candidates survive?

- Concept: **Agglutinative morphology and compounding**
  - Why needed here: The paper explicitly identifies one-to-one word substitution constraints as problematic for languages like Turkish and German; understanding why word boundaries differ across languages clarifies the motivation.
  - Quick check question: How would you align the German compound "Nahrungsmittelallergie" (food allergy) to English "food allergy"—as one word or two?

## Architecture Onboarding

- Component map:
Input: (reference, hypothesis) transcript pair
    ↓
[Text Preprocessing]
    • Lowercase, deaccent, replace unvoiced chars with '#'
    • Enclose words in <word> delimiters
    ↓
[Backtrace Graph Construction]
    • Modified Levenshtein DP (2× substitution cost)
    • Extract all minimum-cost paths as G_b
    ↓
[Beam Search over Full Graph]
    • Initialize beam at (0,0)
    • Expand candidates with transition costs (eq. 5)
    • Penalize deviations from G_b (eq. 4)
    • Update normalized cost (eq. 1) on segment close (eq. 9)
    ↓
Output: Word-level alignment sequence

- Critical path:
  1. Preprocessing correctness—delimiter placement determines word boundary detection.
  2. Backtrace graph extraction—must capture all optimal paths; doubled substitution costs are critical for graph expansion.
  3. Beam search scoring—cost accumulation and segment closing logic (eq. 1-4, 9) directly determine alignment quality.

- Design tradeoffs:
  - **Beam size vs. speed**: Authors use 100; smaller beams degrade quality, larger beams increase latency (~100× slower than word-level Levenshtein).
  - **Substitution cost doubling**: Expands backtrace graph (more candidates) but avoids 1:1 word constraint; ablating this reduces GLE.
  - **Phonemic granularity**: Simple vowel/consonant classification enables multilingual support; full phoneme conversion (Power aligner) is more accurate but language-specific and slower.

- Failure signatures:
  - **Empty alignments**: Check if preprocessing strips all voiced characters or if delimiters are malformed.
  - **Cross-word hallucination matches**: Character-level matching spans words implausibly—verify segment closing logic (eq. 9) triggers on '>' delimiters.
  - **Excessive insertions/deletions on similar words**: Substitution penalty may be too aggressive; check if c_o doubling is applied correctly.
  - **Slow performance on long transcripts**: Beam search is O(n×m×beam); consider segmenting long inputs or reducing beam size.

- First 3 experiments:
  1. **Baseline reproduction**: Run the provided PyPI package on Common Voice English with Whisper outputs; verify GLE scores match Table 3 (~78.8 character GLE).
  2. **Ablation validation**: Disable substitution penalty (set indicator to 0 in eq. 1) and confirm ~4 point GLE degradation per Table 4.
  3. **Language transfer test**: Apply to a non-Latin script language (e.g., Russian, Arabic) not in Table 4; assess whether preprocessing handles different word boundary conventions and whether vowel/consonant classification generalizes.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends critically on the assumption that plausible alignments lie near minimum-edit-distance paths, which may not hold for extreme ASR errors involving extensive word reordering or hallucination.
- Runtime performance (~100× slower than word-level Levenshtein) limits applicability for large-scale evaluation scenarios.
- Cross-linguistic generalization beyond the eight tested non-English languages may not extend to languages with non-Latin scripts or highly inflectional morphology.

## Confidence
**High confidence** in the two-pass constrained search mechanism and substitution penalty effects, supported by ablation studies showing -2.2 and -4.3 GLE degradation respectively when disabled.

**Medium confidence** in the phonemic-informed transition costs, as the improvement (-1.3 GLE when using unit costs) is smaller than other components and the vowel/consonant classification is relatively coarse compared to full phonetic analysis.

**Low confidence** in cross-linguistic generalization beyond the eight tested non-English languages, particularly for languages with non-Latin scripts or phonological systems significantly different from those evaluated.

## Next Checks
1. **Robustness to extreme ASR errors**: Test alignment quality when hypothesis contains extensive word reordering or hallucination (e.g., substituting entire phrases) to verify the backtrace graph constraint doesn't exclude valid alignments.

2. **Phoneme-level generalization**: Evaluate the algorithm on languages with non-segmental phonology (e.g., tonal languages) or those with complex morphology (e.g., Finnish, Hungarian) not included in the evaluation to assess whether vowel/consonant classification remains effective.

3. **Runtime optimization**: Profile performance on longer transcripts (>30 seconds) and explore whether beam size reduction or hierarchical processing can maintain alignment quality while improving practical deployment speed.