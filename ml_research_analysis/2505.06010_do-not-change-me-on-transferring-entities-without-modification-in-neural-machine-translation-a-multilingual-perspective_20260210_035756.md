---
ver: rpa2
title: 'Do Not Change Me: On Transferring Entities Without Modification in Neural
  Machine Translation -- a Multilingual Perspective'
arxiv_id: '2505.06010'
source_url: https://arxiv.org/abs/2505.06010
tags:
- language
- translation
- entities
- entity
- category
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the ability of popular neural machine\
  \ translation (NMT) models to preserve non-translatable entities\u2014such as URLs,\
  \ emails, and emojis\u2014during translation. A new multilingual dataset of 36,000\
  \ sentences across English, German, Polish, and Ukrainian is introduced, covering\
  \ nine entity categories."
---

# Do Not Change Me: On Transferring Entities Without Modification in Neural Machine Translation -- a Multilingual Perspective

## Quick Facts
- arXiv ID: 2505.06010
- Source URL: https://arxiv.org/abs/2505.06010
- Reference count: 17
- Key outcome: This study evaluates eight NMT models on preserving non-translatable entities (URLs, emails, emojis, etc.) across English, German, Polish, and Ukrainian, finding EuroLLM 9B achieves ~95.9% accuracy, while OPUS models perform worst at ~45.7%.

## Executive Summary
This paper evaluates the ability of popular neural machine translation (NMT) models to preserve non-translatable entities—such as URLs, emails, and emojis—during translation. A new multilingual dataset of 36,000 sentences across four languages is introduced, covering nine entity categories. Eight NMT models are tested for entity transfer accuracy using regular expressions and Levenshtein distance metrics. EuroLLM 9B and Google Translate achieve the highest overall accuracy, while OPUS models perform worst. Emojis are the most challenging category, with low transfer rates for most models except EuroLLM and Google Translate. The study finds no correlation between sentence length and transfer accuracy, and longer entities increase error likelihood in some models.

## Method Summary
The study introduces a synthetic dataset of 36,000 sentences (1,000 per category-language pair) generated using Gemma 2 9B with vLLM. Eight NMT models are evaluated: OPUS (75M params, 12 direction-specific models), mBART (611M), NLLB (3.3B), M2M100 (1.2B), EuroLLM 9B/1.7B, MADLAD 7B/3B, SeamlessM4T (2.3B), and Google Translate. Entity transfer accuracy is measured via exact string match using regex patterns, with Levenshtein distance for error magnitude and CometKiwi for translation quality. EuroLLM is tested with both generic and focused prompts. The dataset and translations are available at: https://github.com/laniqo-public/do-not-change-me.

## Key Results
- EuroLLM 9B achieves the highest overall entity transfer accuracy at ~95.9%, followed by Google Translate at ~93.3%.
- OPUS models perform worst, with accuracy around ~45.7%.
- Emojis are the most challenging category, with low transfer rates for most models except EuroLLM and Google Translate.
- No correlation between sentence length and transfer accuracy; longer entities increase error likelihood in some models.

## Why This Works (Mechanism)
The study identifies that model size and tokenization granularity are key factors in entity preservation. Larger models like EuroLLM 9B and Google Translate, which use byte-level tokenization, perform better at preserving non-translatable entities. Prompt engineering, specifically focused prompts instructing models to leave entities unchanged, significantly improves EuroLLM's performance. The lack of byte-level tokenization in models like OPUS, mBART, and others leads to poor emoji transfer accuracy.

## Foundational Learning
- **Neural Machine Translation (NMT)**: Why needed: To understand the baseline for entity preservation during translation. Quick check: Verify NMT models are trained to translate while preserving input structure.
- **Byte-level Tokenization**: Why needed: Critical for preserving non-translatable entities like emojis. Quick check: Confirm tokenizer vocabulary includes byte-level support for special characters.
- **Regular Expressions (Regex)**: Why needed: Used for exact entity extraction and accuracy measurement. Quick check: Validate regex patterns correctly identify single entities per sentence.
- **Prompt Engineering**: Why needed: Focused prompts improve entity preservation in LLMs. Quick check: Test both generic and focused prompts on EuroLLM to confirm performance boost.
- **Levenshtein Distance**: Why needed: Measures error magnitude for failed entity transfers. Quick check: Calculate distance between source and target entities for failed cases.
- **CometKiwi**: Why needed: Evaluates translation quality alongside entity preservation. Quick check: Run CometKiwi on translations to ensure adequacy.

## Architecture Onboarding
**Component Map**: Synthetic Dataset Generation -> NMT Model Inference -> Entity Extraction (Regex) -> Accuracy/Levenshtein Calculation -> Quality Evaluation (CometKiwi)
**Critical Path**: Dataset generation (Gemma 2 + vLLM) → Model inference (8 NMT models) → Entity extraction (regex) → Accuracy computation → Quality validation
**Design Tradeoffs**: Synthetic dataset ensures balanced entity coverage but may lack real-world complexity; regex-based accuracy is precise but may overestimate semantic equivalence.
**Failure Signatures**: Low emoji accuracy (<5%) for OPUS, mBART, SeamlessM4T, NLLB, M2M100 indicates tokenization limitations; longer entities correlate with higher error rates in some models.
**First Experiments**:
1. Load EuroLLM 9B and MADLAD 7B; translate a sample sentence containing a URL and emoji using both generic and focused prompts.
2. Extract entities from source and target using regex; compute exact match accuracy and Levenshtein distance.
3. Run CometKiwi on translations to measure quality impact of entity preservation.

## Open Questions the Paper Calls Out
**Open Question 1**: Does the "focused" prompt engineering strategy improve transfer accuracy for LLMs other than EuroLLM? The study demonstrates success with EuroLLM but leaves open whether this prompting strategy is universally effective for other LLM-based translation models like MADLAD.

**Open Question 2**: To what extent is superior emoji transfer performance attributable to byte-level tokenization rather than model size or training data composition? The study identifies a correlation between byte-level support and high emoji accuracy but does not isolate tokenization as the causal variable through an ablation study.

**Open Question 3**: Do entity transfer failure rates observed in the synthetic dataset persist in natural, human-authored text containing complex syntactic structures? The controlled generation process may not capture the distributional complexity of naturally occurring text.

## Limitations
- The synthetic dataset may not capture the distributional complexity of naturally occurring text, limiting real-world generalizability.
- Regular expression matching may overestimate accuracy for entities where minor orthographic variations are semantically irrelevant.
- The comparison between proprietary (Google Translate) and open models is complicated by unclear API versioning and batching parameters.

## Confidence
- **High confidence**: Entity transfer accuracy rankings across models (EuroLLM 9B and Google Translate as top performers, OPUS models as worst performers).
- **Medium confidence**: Claims about entity length correlation with errors; effect sizes vary substantially by model and entity type.
- **Low confidence**: Real-world applicability of findings; synthetic dataset's uniformity and focus on single-entity sentences limit external validity.

## Next Checks
1. **Cross-dataset validation**: Test top and bottom-performing models (EuroLLM 9B, Google Translate, OPUS) on a naturally occurring multilingual corpus (e.g., TED talks, Wikipedia) to assess whether synthetic dataset findings generalize to real-world text distributions.
2. **Trade-off analysis**: Measure the correlation between entity transfer accuracy and translation quality (CometKiwi scores) to determine if high entity preservation comes at the cost of translation adequacy, particularly for models like OPUS.
3. **Error type characterization**: Manually annotate a sample of failed transfers to distinguish between tokenization failures (emoji, alphanumeric sequences), morphological errors (case, spacing), and translation-related errors (entity mistranslation when context demands it) to refine model improvement priorities.