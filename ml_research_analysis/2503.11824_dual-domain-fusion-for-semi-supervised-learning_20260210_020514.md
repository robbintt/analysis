---
ver: rpa2
title: Dual-Domain Fusion for Semi-Supervised Learning
arxiv_id: '2503.11824'
source_url: https://arxiv.org/abs/2503.11824
tags:
- fusion
- training
- data
- learning
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Dual-Domain Fusion (DDF), a model-agnostic semi-supervised
  learning framework for time-series signals. DDF addresses the challenge of training
  accurate machine learning models with limited labeled data by combining 1D time-domain
  signals with their 2D time-frequency representations.
---

# Dual-Domain Fusion for Semi-Supervised Learning

## Quick Facts
- arXiv ID: 2503.11824
- Source URL: https://arxiv.org/abs/2503.11824
- Reference count: 40
- Key outcome: 8-46% accuracy improvement over SSL methods on fault diagnosis datasets

## Executive Summary
This paper introduces Dual-Domain Fusion (DDF), a model-agnostic semi-supervised learning framework specifically designed for time-series signals. The method addresses the challenge of training accurate models with limited labeled data by combining 1D time-domain signals with their 2D time-frequency representations through a tri-model architecture. The approach maintains inference-time efficiency by discarding the time-frequency model during testing, achieving substantial performance gains while preserving computational costs comparable to standard time-domain models.

## Method Summary
Dual-Domain Fusion operates by transforming time-series signals into both time-domain and time-frequency (spectrogram) representations. A tri-model architecture is employed where separate models process each domain, with a fusion component combining their outputs during training. The key innovation is that the time-frequency model is discarded at inference time, maintaining the same computational complexity as standard time-domain approaches. The method leverages the complementary information captured in different domains - temporal patterns and frequency characteristics - to improve semi-supervised learning performance when labeled data is scarce.

## Key Results
- Achieved 8-46% accuracy improvements over standard SSL methods (FixMatch, MixMatch, Mean Teacher, Adversarial Training, Self-training)
- Demonstrated effectiveness on two public fault diagnosis datasets
- Maintained inference-time efficiency by discarding the time-frequency model during testing
- Showed consistent improvements across different levels of labeled data availability

## Why This Works (Mechanism)
The method works by exploiting complementary information across different signal representations. Time-domain signals capture temporal patterns and sequential dependencies, while time-frequency representations reveal periodicities and frequency-based characteristics that may be more discriminative for certain fault types. By training models jointly across both domains, DDF can leverage the strengths of each representation. The fusion component learns to optimally combine these complementary views, while the discarding of the time-frequency model at inference ensures practical deployment without computational overhead.

## Foundational Learning
- Time-domain vs. time-frequency representations: Why needed - different signal characteristics are more apparent in different domains; Quick check - verify that spectrogram captures expected frequency patterns
- Semi-supervised learning fundamentals: Why needed - the method builds on SSL techniques; Quick check - understand consistency regularization principles
- Signal transformation methods: Why needed - quality of transformation affects performance; Quick check - verify that key signal features are preserved in spectrogram
- Model fusion strategies: Why needed - combining domain-specific predictions is central to the approach; Quick check - ensure fusion weights are properly normalized

## Architecture Onboarding

Component Map: Raw Signal -> Time-Domain Model & Time-Frequency Model -> Fusion Component -> Final Prediction

Critical Path: Signal Preprocessing -> Dual Domain Processing -> Feature Extraction -> Fusion -> Prediction

Design Tradeoffs: The main tradeoff is increased training complexity (three models) versus inference efficiency (single model). The method accepts higher training computational costs for better final accuracy and practical deployment.

Failure Signatures: Poor time-frequency transformations that lose critical signal information, imbalanced contributions from different domains during fusion, and overfitting to the fusion component due to limited labeled data.

First Experiments:
1. Verify that individual time-domain and time-frequency models perform comparably to baseline approaches
2. Test fusion component with synthetic balanced inputs to validate proper weighting
3. Measure training time overhead compared to single-domain approaches

## Open Questions the Paper Calls Out
The paper acknowledges that the method's effectiveness depends on the quality of time-frequency transformations and that the complementary information assumption may not hold for all signal types. It also notes that the computational overhead during training, while manageable, requires careful consideration for large-scale deployments.

## Limitations
- Performance heavily dependent on quality of time-frequency transformations
- Experimental validation limited to two fault diagnosis datasets, raising generalizability concerns
- Computational overhead during training (three models) not extensively quantified across hardware configurations
- "Model-agnostic" claim requires validation with diverse architectures beyond those tested

## Confidence

| Claim | Confidence |
|-------|------------|
| Basic framework architecture and inference-time efficiency | High |
| Accuracy improvements on tested datasets | Medium |
| "Model-agnostic" characterization | Low |

## Next Checks

1. Test DDF across diverse time-series domains beyond fault diagnosis (e.g., healthcare monitoring, financial time series) to establish broader applicability
2. Systematically quantify training-time computational overhead across different hardware configurations and compare with alternative SSL methods
3. Evaluate the impact of different time-frequency transformation parameters (window size, overlap, frequency resolution) on final performance to understand sensitivity to preprocessing choices