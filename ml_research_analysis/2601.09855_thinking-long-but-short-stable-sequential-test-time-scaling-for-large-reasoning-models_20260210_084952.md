---
ver: rpa2
title: 'Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning
  Models'
arxiv_id: '2601.09855'
source_url: https://arxiv.org/abs/2601.09855
tags:
- reasoning
- accuracy
- reconstruction
- cache
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Sequential test-time scaling of large reasoning models can improve
  accuracy but suffers from instability and accuracy degradation when reasoning length
  is extended too far. This work proposes Min-Seek, a method that stabilizes sequential
  scaling by keeping only the KV cache of the shortest generated thoughts, thereby
  avoiding flawed reasoning paths.
---

# Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning Models

## Quick Facts
- arXiv ID: 2601.09855
- Source URL: https://arxiv.org/abs/2601.09855
- Authors: Michael R. Metel; Yufei Cui; Boxing Chen; Prasanna Parthasarathi
- Reference count: 9
- Key outcome: Min-Seek stabilizes sequential test-time scaling by maintaining only the shortest reconstruction cycles, achieving 29.4-44.0% faster inference than Budget Forcing while preserving accuracy gains across 100+ induced thoughts

## Executive Summary
Sequential test-time scaling extends reasoning in large reasoning models by generating multiple reconstruction cycles after an initial thought, potentially improving accuracy. However, this approach suffers from instability and accuracy degradation when reasoning length is extended too far. This work introduces Min-Seek, a training-free method that stabilizes sequential scaling by keeping only the KV cache of the shortest generated thoughts, thereby avoiding flawed reasoning paths. The method uses a custom KV cache that dynamically applies position embeddings and implements a strict length-based selection rule for reconstruction cycles.

## Method Summary
Min-Seek addresses sequential test-time scaling instability by maintaining only the shortest reconstruction cycle (RC) in the KV cache while generating additional reasoning paths. The method employs a custom KV cache that stores keys without position embeddings and applies contiguous position IDs before each reconstruction cycle. When a new RC is generated, it replaces the current one only if it's strictly shorter. This length-based selection heuristic leverages the observation that shorter thoughts often correlate with higher accuracy. The approach enables unbounded reasoning beyond model context limits with linear computational complexity and includes two variants: a "Wait" trigger for final RC before answer (7B model) and direct answer generation for the 1.5B model.

## Key Results
- Min-Seek achieves normalized accuracy gains of 1.076-1.148 (1.5B) and >1.095 (7B) for 100+ induced thoughts, significantly outperforming Budget Forcing
- The method maintains stable accuracy across extended reasoning lengths where Budget Forcing shows degradation after 4-6 reconstruction cycles
- Min-Seek is 29.4-44.0% faster than Budget Forcing due to reduced KV cache size while preserving accuracy improvements
- The approach successfully handles reasoning beyond model context limits through linear computational complexity

## Why This Works (Mechanism)
Sequential test-time scaling can improve accuracy by allowing models to refine their reasoning through multiple reconstruction cycles. However, extended reasoning chains often degrade performance due to accumulation of errors or exploration of flawed paths. Min-Seek stabilizes this process by maintaining only the shortest reconstruction cycle, based on the observation that shorter thoughts tend to be more accurate. By dynamically applying position embeddings and limiting KV cache growth to the minimum necessary content, the method prevents computational inefficiency while preserving the beneficial aspects of sequential scaling.

## Foundational Learning
- **KV Cache Management**: Storing and updating key-value pairs for efficient autoregressive generation. Why needed: Enables efficient repeated generation without recomputing attention. Quick check: Verify cache size remains bounded during generation.
- **Position Embeddings**: Mathematical encoding of token positions in sequence. Why needed: Critical for transformer attention mechanisms to maintain token order information. Quick check: Confirm position IDs are contiguous after each RC generation.
- **Reconstruction Cycles**: Multiple reasoning passes generated after initial thought. Why needed: Enables iterative refinement of problem-solving approach. Quick check: Validate that only shortest RC is retained in cache.
- **Test-Time Scaling**: Extending inference computation to improve accuracy. Why needed: Leverages additional compute for better reasoning without model retraining. Quick check: Compare accuracy trends across different M values.
- **Token Limit Management**: Handling generation within context window constraints. Why needed: Prevents out-of-memory errors during extended reasoning. Quick check: Monitor KV cache size relative to model limits.

## Architecture Onboarding

Component Map: Prompt -> Initial Thought -> Reconstruction Cycle Generation -> Min-Seek Selection -> Final Answer

Critical Path: The critical path involves generating the initial thought, then iteratively generating reconstruction cycles while maintaining only the shortest one in the KV cache. Each new RC is compared against the current minimum, and the cache is updated only if the new cycle is strictly shorter. Position embeddings are dynamically reapplied before each generation step.

Design Tradeoffs: The method trades potential exploration of diverse reasoning paths (as in Budget Forcing) for stability and efficiency by always selecting the shortest path. This heuristic-based selection may miss valuable longer reasoning chains that could lead to correct answers, but empirically provides more reliable performance across extended reasoning lengths.

Failure Signatures: Common failure modes include incoherent output when approaching context limits (indicating position embedding issues), degradation after 4-6 RCs in Budget Forcing baseline (indicating accumulation of flawed reasoning), and suboptimal accuracy if the shortest path consistently misses correct solutions (indicating the length heuristic may not always correlate with quality).

First Experiments:
1. Verify Budget Forcing baseline shows expected accuracy degradation after 4-6 reconstruction cycles
2. Test Min-Seek's KV cache management by confirming cache size remains bounded at |PT1| + |shortest RC| + |current RC|
3. Validate position embedding re-application by checking that generated output remains coherent when approaching context limits

## Open Questions the Paper Calls Out

### Open Question 1
Does the custom KV cache with dynamic position embeddings independently contribute to accuracy gains, or is improvement solely from the thought selection rule? The authors state dynamically applying position embeddings has further benefits and may contribute to observed accuracy degradation, but no ablation isolating this component is provided.

### Open Question 2
Would importance-based or semantic-based thought selection outperform the length-based heuristic? The authors acknowledge that a reconstruction cycle deemed insignificant initially might become important after further reasoning, but avoid this approach due to complexity.

### Open Question 3
Does Min-Seek generalize to other large reasoning models beyond DeepSeek-R1 distilled variants? The method is presented as architecture-agnostic but validated only on two distilled DeepSeek-R1 models (1.5B and 7B parameters).

### Open Question 4
What is the causal mechanism connecting shorter reconstruction cycles to higher accuracy? The paper hypothesizes shorter thoughts avoid flawed reasoning paths but relies on prior work's correlational evidence rather than mechanistic analysis.

## Limitations
- The method is evaluated only on distilled versions of a single model family (DeepSeek-R1), limiting generalizability
- The analysis focuses primarily on accuracy trends without extensive qualitative investigation of reasoning chain differences
- Key implementation details for answer generation trigger detection and KV cache boundary handling remain underspecified
- The length-based selection heuristic may miss valuable longer reasoning paths that could lead to correct answers

## Confidence
- High confidence: Min-Seek achieves superior stability compared to Budget Forcing across extended reasoning lengths
- Medium confidence: The 29.4-44.0% speedup claim relative to Budget Forcing is accurate
- Low confidence: The method generalizes effectively to reasoning models beyond the DeepSeek-R1 family

## Next Checks
1. Reproduce the Budget Forcing baseline with the "Wait" trigger mechanism and verify it shows expected accuracy degradation after 4-6 reconstruction cycles
2. Implement the custom KV cache with position embedding re-application and test that Min-Seek maintains coherent output when approaching context limits
3. Evaluate Min-Seek's stability on at least one additional reasoning model family (e.g., Qwen2.5 or Llama) to assess generalizability beyond DeepSeek-R1