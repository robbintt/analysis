---
ver: rpa2
title: 'Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge
  2025'
arxiv_id: '2511.20200'
source_url: https://arxiv.org/abs/2511.20200
tags:
- task
- arxiv
- dialogue
- tool
- track
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MSRASC's winning solution to the CPDC 2025
  challenge, which focuses on building intelligent NPC dialogue systems. The team
  ranked 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU
  tracks.
---

# Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025

## Quick Facts
- arXiv ID: 2511.20200
- Source URL: https://arxiv.org/abs/2511.20200
- Reference count: 18
- MSRA_SC team ranked 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU tracks

## Executive Summary
This technical report presents MSRA_SC's winning solution to the CPDC 2025 challenge, which focuses on building intelligent NPC dialogue systems. The team achieved top rankings across multiple tracks by combining Context Engineering with GRPO training. Their approach significantly improves tool call stability and role-playing guidance through dynamic tool pruning, persona clipping, parameter normalization, and function merging. For GPU track optimization, they adopted GRPO training instead of supervised fine-tuning to directly optimize results via reward signals and mitigate small-sample overfitting.

## Method Summary
The solution combines two key components: Context Engineering and GRPO training. Context Engineering applies dynamic tool pruning and persona clipping for input compression, along with post-processing techniques like parameter normalization and function merging. This improves tool call stability and role-playing guidance. For the GPU track, the team adopts GRPO training instead of supervised fine-tuning, directly optimizing results via reward signals to mitigate small-sample overfitting.

## Key Results
- API track using GPT-4o-mini: Task 1 improved from 0.46 to 0.55, Task 2 from 0.58 to 0.62
- GPU track using Qwen3 models: Task 1 improved from 0.36 to 0.57 (Qwen3-8B) and 0.38 to 0.59 (Qwen3-14B)
- GPU track Task 2 improved from 0.55 to 0.60 (Qwen3-8B) and 0.56 to 0.59 (Qwen3-14B)

## Why This Works (Mechanism)
The Context Engineering approach addresses key challenges in NPC dialogue systems by compressing inputs through dynamic tool pruning and persona clipping, which reduces noise and improves focus on relevant information. Post-processing techniques like parameter normalization and function merging enhance tool call stability and role-playing consistency. The GRPO training directly optimizes for desired behaviors through reward signals rather than relying on supervised fine-tuning, which is particularly effective for the GPU track where small sample sizes could lead to overfitting.

## Foundational Learning
- Context Engineering: Why needed - To handle complex NPC dialogue scenarios with multiple tools and personas; Quick check - Verify input compression doesn't lose critical dialogue context
- GRPO Training: Why needed - To optimize for reward-based objectives in resource-constrained GPU environments; Quick check - Confirm reward signal alignment with desired dialogue quality
- Tool Call Stability: Why needed - To ensure consistent NPC behavior across diverse player interactions; Quick check - Measure tool invocation consistency across similar dialogue contexts
- Role-Playing Guidance: Why needed - To maintain character consistency in dynamic conversations; Quick check - Evaluate persona adherence across extended dialogue sequences

## Architecture Onboarding
Component Map: Context Engineering -> Tool Pruning/Persona Clipping -> Parameter Normalization/Function Merging -> GRPO Training
Critical Path: Input compression through Context Engineering enables stable tool calls, which GRPO training optimizes for reward signals
Design Tradeoffs: Context Engineering vs. model capacity - balancing compression benefits against potential information loss; API track (GPT-4o-mini) vs. GPU track (Qwen3) - different optimization strategies for resource constraints
Failure Signatures: Superficial prompt bias exploitation in LLM-as-a-judge evaluation leading to reward hacking; overfitting in small-sample GPU training scenarios
First Experiments:
1. Compare baseline vs. Context Engineering performance on individual tool call accuracy
2. Test different compression ratios in dynamic tool pruning to find optimal balance
3. Validate GRPO training stability across different reward function formulations

## Open Questions the Paper Calls Out
The paper identifies challenges with LLM-as-a-judge evaluation, finding that models can exploit superficial prompt biases rather than genuinely improving interaction quality, leading to reward hacking.

## Limitations
- Evaluation methodology concerns due to potential reward hacking in LLM-as-a-judge systems
- Limited technical details for reproducing specific Context Engineering components
- Uncertainty about generalization to real-world game environments beyond controlled benchmarks

## Confidence
High: Benchmark improvements on CPDC challenge tasks are based on concrete score measurements
Medium: Relative effectiveness of Context Engineering versus GRPO training is supported by data but lacks detailed ablation studies
Low: Claims about practical impact on game dialogue quality rely on potentially biased automated evaluation without human verification

## Next Checks
1. Conduct human evaluation studies comparing the proposed system against baselines in realistic game dialogue scenarios
2. Perform detailed ablation experiments to isolate individual contributions of Context Engineering components
3. Test system robustness across diverse game genres and dialogue contexts beyond the CPDC benchmark