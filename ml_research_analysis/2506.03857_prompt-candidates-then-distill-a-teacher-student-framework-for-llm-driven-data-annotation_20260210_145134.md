---
ver: rpa2
title: 'Prompt Candidates, then Distill: A Teacher-Student Framework for LLM-driven
  Data Annotation'
arxiv_id: '2506.03857'
source_url: https://arxiv.org/abs/2506.03857
tags:
- candidate
- annotations
- labels
- where
- candist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of data annotation using large
  language models (LLMs), which often produce incorrect labels for difficult samples
  due to inherent uncertainty, compromising downstream data quality. To mitigate this,
  the authors propose a novel paradigm that prompts LLMs to output multiple possible
  labels (candidate annotations) when uncertain, inspired by human ambiguity aversion.
---

# Prompt Candidates, then Distill: A Teacher-Student Framework for LLM-driven Data Annotation

## Quick Facts
- arXiv ID: 2506.03857
- Source URL: https://arxiv.org/abs/2506.03857
- Reference count: 40
- Proposes a teacher-student framework that improves LLM-driven data annotation by generating candidate labels and distilling them into high-quality final annotations

## Executive Summary
This paper addresses the fundamental challenge of data annotation using large language models (LLMs), which often produce incorrect labels for difficult samples due to inherent uncertainty. The authors propose a novel paradigm where LLMs are prompted to output multiple possible labels when uncertain, mimicking human ambiguity aversion. These candidate annotations are then distilled into unique labels using a small language model (SLM) through a teacher-student framework called CanDist. The approach is theoretically grounded, showing that distilling from candidate annotations offers superior noise tolerance compared to using single annotations. Extensive experiments on six text classification tasks demonstrate state-of-the-art performance compared to various LLM and SLM baselines.

## Method Summary
The CanDist framework operates in two phases: first, it prompts LLMs to generate multiple candidate annotations for each sample, capturing uncertainty when the model cannot confidently assign a single label. Second, a small language model acts as a student that distills these candidate annotations into a single, refined label through a distribution refinery mechanism. This approach leverages the diversity of candidate labels to improve the final annotation quality. The framework is theoretically analyzed to show that candidate-based distillation provides better noise tolerance than traditional single-annotation approaches, making it particularly effective for handling difficult or ambiguous samples that would otherwise receive incorrect labels.

## Key Results
- CanDist outperforms various LLM and SLM baselines on six text classification tasks
- Theoretical analysis proves candidate annotations provide superior noise tolerance compared to single annotations
- State-of-the-art results achieved in LLM-driven data annotation across tested datasets

## Why This Works (Mechanism)
The framework addresses a fundamental limitation in LLM-driven annotation: models often confidently produce incorrect labels for difficult samples. By prompting LLMs to output multiple candidate labels when uncertain, the approach captures the model's internal ambiguity and preserves useful information that would otherwise be lost. The teacher-student distillation process then leverages this uncertainty information, allowing the SLM to learn from the distribution of candidate labels rather than a single potentially incorrect annotation. This mechanism effectively transforms LLM uncertainty from a liability into an asset for improving annotation quality.

## Foundational Learning
- LLM Uncertainty Modeling: Understanding how LLMs handle ambiguous inputs is crucial because the framework relies on models' ability to recognize and express uncertainty through multiple candidates. Quick check: Verify that LLMs consistently produce diverse candidates for truly ambiguous samples versus those they simply cannot classify correctly.
- Teacher-Student Distillation: This knowledge transfer technique allows smaller models to learn from larger models' behavior. Quick check: Ensure the SLM can effectively capture the distribution of candidate labels without overfitting to noise.
- Noise Tolerance Theory: The mathematical foundation showing why candidate-based approaches are more robust to annotation errors than single-label approaches. Quick check: Validate that the theoretical noise tolerance advantage translates to practical performance gains across different noise levels.

## Architecture Onboarding

Component Map: LLM -> Candidate Generation -> SLM Distillation -> Final Labels

Critical Path: The most critical path is the LLM's ability to generate meaningful candidate annotations when uncertain. If the LLM cannot express uncertainty effectively or produces uninformative candidates, the entire framework's effectiveness is compromised.

Design Tradeoffs: The framework trades increased computational overhead (generating multiple candidates per sample) for improved annotation quality. The choice of SLM architecture and training strategy significantly impacts the final distillation quality, requiring careful hyperparameter tuning.

Failure Signatures: The framework will fail when LLMs consistently produce identical or highly correlated candidate labels (indicating poor uncertainty expression), when the SLM lacks sufficient capacity to learn from the candidate distribution, or when the distribution refinery mechanism cannot effectively separate signal from noise in the candidate annotations.

First Experiments:
1. Test candidate generation quality by measuring diversity metrics (e.g., pairwise label agreement) across different types of uncertain samples
2. Evaluate SLM distillation performance with varying numbers of candidate labels per sample to find the optimal trade-off
3. Compare noise tolerance of candidate-based distillation versus single-label approaches under controlled noise injection experiments

## Open Questions the Paper Calls Out
None

## Limitations
- Relies heavily on LLMs' ability to generate meaningful candidate annotations, which may not hold for all task types or domains
- Introduces significant computational overhead from generating multiple candidate labels per sample
- Framework performance depends on SLM quality and suitability for the specific task domain

## Confidence

High Confidence Claims:
- Theoretical analysis of noise tolerance is mathematically sound
- Experimental results showing CanDist's superiority over baselines are robust

Medium Confidence Claims:
- Generalizability across diverse NLP tasks beyond tested text classification
- Practical utility in real-world annotation scenarios

Low Confidence Claims:
- Consistent meaningful candidate generation across all LLM uncertainty scenarios
- Scalability to large-scale annotation tasks without prohibitive computational costs

## Next Checks

1. Cross-domain validation: Test CanDist on diverse NLP tasks including named entity recognition, sentiment analysis, and domain-specific classification to evaluate generalizability beyond text classification.

2. Scalability analysis: Conduct experiments measuring computational overhead and annotation quality trade-offs when scaling to datasets with millions of samples, including cost analysis and processing time benchmarks.

3. Candidate quality assessment: Implement automated metrics to evaluate informativeness and diversity of candidate annotations across different uncertainty types, and correlate these metrics with downstream task performance to validate core assumptions.