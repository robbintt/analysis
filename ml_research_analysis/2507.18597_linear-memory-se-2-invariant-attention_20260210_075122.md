---
ver: rpa2
title: Linear Memory SE(2) Invariant Attention
arxiv_id: '2507.18597'
source_url: https://arxiv.org/abs/2507.18597
tags:
- attention
- relative
- invariant
- position
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SE(2) Fourier, a linear memory attention mechanism
  for SE(2) invariant transformers. The core idea is to approximate relative SE(2)
  poses using Fourier series, enabling linear memory computation compared to quadratic
  memory in existing approaches.
---

# Linear Memory SE(2) Invariant Attention

## Quick Facts
- arXiv ID: 2507.18597
- Source URL: https://arxiv.org/abs/2507.18597
- Authors: Ethan Pronovost; Neha Boloor; Peter Schleede; Noureldin Hendy; Andres Morales; Nicholas Roy
- Reference count: 25
- Key outcome: This paper proposes SE(2) Fourier, a linear memory attention mechanism for SE(2) invariant transformers. The core idea is to approximate relative SE(2) poses using Fourier series, enabling linear memory computation compared to quadratic memory in existing approaches. The method replaces the relative pose matrix with a Fourier approximation, allowing use of standard attention kernels like Flash Attention. Experiments on an agent simulation task show SE(2) Fourier achieves lower negative log likelihood (0.190 vs 0.191 for SE(2) Representation) and better minimum average displacement error on turning trajectories (2.60 vs 2.69 for 2D RoPE), while maintaining competitive performance on straight trajectories. The approach demonstrates practical implementation and improved performance over both absolute position embeddings and quadratic memory SE(2) invariant attention.

## Executive Summary
This paper introduces SE(2) Fourier, a novel linear memory attention mechanism that achieves SE(2) invariance for transformer architectures in autonomous driving applications. The key innovation is using Fourier series approximations to encode relative SE(2) poses, replacing the quadratic memory requirements of traditional SE(2) invariant attention with linear memory while maintaining exact group invariance. The approach enables transformers to process sequences of tokens with SE(2) poses (position and heading) in a way that is invariant to global translations and rotations, which is critical for multi-agent motion forecasting where the ego-vehicle's coordinate frame is arbitrary. Experiments on an agent simulation task demonstrate that SE(2) Fourier achieves better performance than both absolute position embeddings and existing SE(2) invariant attention methods, particularly on trajectories involving turns.

## Method Summary
The method approximates relative SE(2) poses using Fourier series expansions, enabling linear memory attention computation. The core approach factorizes the relative pose encoding into independent query and key transformations (ϕq and ϕk) using Fourier basis functions, which are then combined through standard attention mechanisms like Flash Attention. For non-abelian SE(2), the paper approximates terms involving mixed position and rotation components (like xm·cos(θn)) using truncated Fourier series with controllable error bounds. The method requires preprocessing queries and keys with these Fourier coefficient matrices, applying standard SDPA, then post-processing outputs. Position magnitudes are downscaled to 2-8 before computing Fourier coefficients, with basis sizes of 12-28 depending on the scale. This achieves the same theoretical guarantees as exact SE(2) invariant attention while reducing memory complexity from quadratic to linear.

## Key Results
- Achieves lower negative log likelihood on agent simulation task (0.190 vs 0.191 for SE(2) Representation)
- Better minimum average displacement error on turning trajectories (2.60 vs 2.69 for 2D RoPE)
- Maintains competitive performance on straight trajectories compared to baseline methods
- Demonstrates linear memory computation instead of quadratic memory for SE(2) invariant attention
- Approximation error remains below 10^-3 with appropriate basis size selection

## Why This Works (Mechanism)

### Mechanism 1: Factorized Relative Pose Encoding
The paper encodes relative poses as a product of per-token transformations (ϕq(pn) and ϕk(pm)) that enable linear memory while preserving exact group invariance. This transforms quadratic memory algorithms into linear memory by preprocessing queries and keys independently, applying standard SDPA, then post-processing outputs. The group structure permits this factorization, though for SE(2) it requires Fourier approximation. This mechanism is equivalent to having a block-diagonal matrix of 2D rotation matrices between query and key vectors.

### Mechanism 2: Fourier Series Approximation for Non-Abelian SE(2)
For SE(2)'s non-abelian structure, the paper approximates relative pose terms involving mixed position and rotation (like xm·cos(θn)) using truncated Fourier series. The relative x and y positions require basis functions gi(θn) with coefficients computed from key positions. With F terms, spectral norm error decreases as position magnitudes decrease or F increases. The approximation error can be reduced to less than 10^-3 with appropriate basis size selection, achieving comparable accuracy to quadratic representations.

### Mechanism 3: SE(2) Invariance Through Group-Relative Computation
The method computes attention solely from relative poses pn→m = p−1n·pm, yielding outputs invariant to global SE(2) transformations. For any z ∈ SE(2), replacing all pn with z−1·pn leaves pn→m unchanged, thus preserving attention outputs. This is critical for autonomous driving where the ego-vehicle's coordinate frame is arbitrary, and only relative geometry between elements is needed for tasks like multi-agent motion forecasting.

## Foundational Learning

- **SE(2) Group and Lie Groups**: SE(2) represents 2D rigid transformations (translation + rotation). Understanding why SE(2) is non-abelian (rotation composition depends on order) explains why exact factorization fails and approximation is required. Quick check: Why does computing (R1, t1) · (R2, t2) yield a different translation component than (R2, t2) · (R1, t1) when rotations differ?

- **Flash Attention and IO-Awareness**: The paper's linear memory claim depends on leveraging existing Flash Attention kernels. Without understanding how Flash Attention avoids materializing the N×M attention matrix, the benefit of the factorization is unclear. Quick check: What memory tensor does standard attention materialize that Flash Attention avoids, and why does this matter for long sequences?

- **Fourier Series and Truncation Error**: The approximation quality directly depends on basis size F and position magnitude. Understanding how higher-frequency components require more terms helps set hyperparameters. Quick check: If the function cos(x·cos(θ)) has "frequency" proportional to |x|, how does doubling |x| affect the number of Fourier terms needed for fixed error?

## Architecture Onboarding

- **Component map**: Input SE(2) poses and features → Fourier coefficient computation (Γ, Λ) → Basis function evaluation (b vectors) → Matrix construction (ϕq, ϕk) → Pre-process: ϕq^T·q, ϕk·k, ϕk·v → Standard SDPA (Flash Attention) → Post-process: ϕq·output → Transformed features with implicit relative pose encoding

- **Critical path**: 1) Position downscaling (magnitude ≤4 recommended), 2) Fourier coefficient computation for keys (only O(M) per-token compute added), 3) Basis function evaluation for queries, 4) Matrix construction and application

- **Design tradeoffs**: Basis size F vs approximation error (F=18 gives ~10−3 error at magnitude 4; F=28 needed at magnitude 8), feature dimension overhead (each SE(2) block uses (4F+2) intermediate dimensions vs 6 input dimensions), position scaling (aggressive downscaling reduces F requirements but may lose spatial resolution)

- **Failure signatures**: Training instability with large position magnitudes (need position downscaling or larger F), poor performance on rotation-heavy trajectories with 2D RoPE baseline (suggests rotation invariance needed), NaN/Inf in Fourier coefficients (check numerical integration stability for extreme positions)

- **First 3 experiments**: 1) Approximation validation: Sample random SE(2) pose pairs, compute spectral norm of ϕ(pn→m) − ϕq(pn)ϕk(pm) across varying position magnitudes and basis sizes, confirm error < 10−3 at target scale. 2) Ablation on rotation invariance: Compare SE(2) Fourier vs 2D RoPE on synthetic task where only rotation varies, expect SE(2) Fourier to generalize across rotations while 2D RoPE fails. 3) Downstream task sanity check: Replace existing attention in small agent simulation model with SE(2) Fourier, monitor NLL and minADE against absolute position and 2D RoPE baselines, expect improvement on turning trajectories specifically.

## Open Questions the Paper Calls Out

### Open Question 1
Can the Fourier approximation strategy be effectively generalized to SE(3) groups for linear memory attention in 3D robotics tasks? The conclusion explicitly states this is an open question. The current method relies on specific trigonometric decompositions for SE(2), and it is unclear if Fourier approximation remains tractable or accurate when extended to higher dimensions of SE(3).

### Open Question 2
Does the linear-memory Fourier approximation achieve performance parity with exact quadratic-memory SE(2) invariant attention mechanisms? The authors note in Limitations that they will include more ablation experiments comparing their method against quadratic memory SE(2) invariant attention. The current experiments only compare against absolute position embeddings and 2D RoPE, not the non-approximated baseline it seeks to replace.

### Open Question 3
How does the architectural invariance of SE(2) Fourier compare to learning invariance via data augmentation? The Limitations section lists "data augmentation" as a comparison approach that needs to be included in future ablation experiments. It is unproven whether explicit hard-coding of invariance via Fourier basis yields better results than a standard transformer trained on augmented data.

### Open Question 4
Does the performance of SE(2) Fourier generalize to other autonomous driving tasks beyond agent simulation? The authors state in Conclusion they will demonstrate benefits on a wider range of driving tasks and datasets. The paper validates on a private agent simulation dataset, but utility for other tasks like motion forecasting or planning remains unverified.

## Limitations

- The paper relies on a critical Fourier approximation whose error bounds are asymptotic rather than tight, with unclear propagation to downstream task performance
- Results are based on a private 33M scenario dataset, preventing independent verification of claimed improvements
- Does not compare against other linear memory attention mechanisms for geometric data, limiting claims about absolute performance
- Basis size selection depends heavily on position magnitude scaling, requiring careful hyperparameter tuning

## Confidence

**High confidence** in the theoretical framework and linear memory mechanism - the factorization approach and its implementation are mathematically sound and well-explained.

**Medium confidence** in the Fourier approximation quality claims - the error bounds are derived but not extensively validated across different position distributions and task-specific error propagation.

**Low confidence** in the absolute performance claims - results are based on a private dataset with no public baseline comparisons, making it difficult to assess practical significance of improvements.

## Next Checks

1. **Approximation Error Propagation**: Create a synthetic dataset with known ground truth trajectories varying in turning complexity. Apply SE(2) Fourier attention with different basis sizes and measure how approximation error translates to minADE degradation. This would establish practical error tolerance for the Fourier approximation.

2. **Rotation Invariance Stress Test**: Generate scenes with varying rotation angles (0° to 360°) and verify that SE(2) Fourier attention produces identical outputs for rotated versions of the same scene, while 2D RoPE shows dependence on absolute rotation. This would validate claimed invariance property beyond single turning trajectory result.

3. **Memory vs Accuracy Tradeoff**: Implement a controlled experiment comparing SE(2) Fourier (linear memory) against a quadratic memory SE(2) invariant attention baseline on a public geometric dataset. Measure memory savings and any accuracy degradation to quantify practical benefit of linear memory approach.