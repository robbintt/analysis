---
ver: rpa2
title: 'DYNAC: Dynamic Vocabulary based Non-Autoregressive Contextualization for Speech
  Recognition'
arxiv_id: '2506.00422'
source_url: https://arxiv.org/abs/2506.00422
tags:
- dynamic
- vocabulary
- speech
- dynac
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses slow inference speed in autoregressive (AR)
  contextual biasing (CB) for speech recognition by applying dynamic vocabulary expansion
  to non-autoregressive (NAR) models. The proposed DYNAC method integrates dynamic
  vocabulary into intermediate layers of a self-conditioned CTC architecture, enabling
  the model to capture dependencies between static and dynamic tokens while reducing
  real-time factor (RTF).
---

# DYNAC: Dynamic Vocabulary based Non-Autoregressive Contextualization for Speech Recognition

## Quick Facts
- arXiv ID: 2506.00422
- Source URL: https://arxiv.org/abs/2506.00422
- Reference count: 0
- Key outcome: DYNAC reduces RTF by 81% with only 0.1-point degradation in word error rate compared to AR-CB methods while substantially improving biased phrase recognition accuracy

## Executive Summary
This paper addresses the slow inference speed of autoregressive contextual biasing in speech recognition by introducing DYNAC, a non-autoregressive approach that integrates dynamic vocabulary into intermediate layers of a self-conditioned CTC architecture. The method uses a parameter-free back-projection mechanism to efficiently handle dynamically-sized vocabularies while maintaining parallel decoding capabilities. Experimental results on LibriSpeech and Japanese datasets demonstrate significant RTF reduction (81%) with minimal WER degradation (0.1 points) compared to autoregressive contextual biasing methods.

## Method Summary
DYNAC extends self-conditioned CTC by integrating dynamic vocabulary at intermediate encoder layers. The architecture computes token-wise probabilities for both static and dynamic vocabularies at layers 3, 6, and 9, then uses a parameter-free back-projection layer to transform these probabilities into hidden representations that are added to encoder outputs via residual connections. Multi-objective training with auxiliary CTC losses at intermediate layers stabilizes learning. The bias encoder is pre-computed and cached, enabling efficient handling of dynamically changing vocabulary during inference.

## Key Results
- RTF reduction of 81% compared to AR-CB methods
- Only 0.1-point WER degradation relative to AR-CB approaches
- Substantial improvement in biased phrase recognition accuracy over conventional NAR approaches
- Effective handling of over 1000 frequently misrecognized bias phrases

## Why This Works (Mechanism)

### Mechanism 1
Integrating dynamic vocabulary into intermediate encoder layers via self-conditioned CTC enables dependency capture between static and dynamic tokens while preserving NAR inference speed. Scoring layers at intermediate layers compute token-wise probabilities for both vocabularies, and back-projection transforms these into hidden representations added to encoder output via residual connection. This feedback allows self-attention layers to refine contextual relationships.

### Mechanism 2
A parameter-free back-projection layer enables efficient handling of dynamically-sized vocabulary by decomposing and recombining probability distributions. The layer splits token scores into static and dynamic components, transforms them using pre-computed bias phrase embeddings, and recombines them without trainable parameters, avoiding re-training when vocabulary changes.

### Mechanism 3
Multi-objective training with auxiliary CTC losses at intermediate layers stabilizes learning of the dynamic vocabulary integration points. The combined loss function includes final CTC loss, intermediate CTC losses, and attention loss, providing regularization and propagating useful gradient signals to earlier integration points.

## Foundational Learning

- **Concept: CTC Conditional Independence**
  - Why needed here: DYNAC's core motivation is that standard CTC cannot model dependencies between static and dynamic tokens; understanding this limitation clarifies what self-conditioning addresses.
  - Quick check question: Why does CTC's factorization P(A|X) = Πt P(at|X) prevent modeling token-to-token influence?

- **Concept: Self-Conditioned CTC**
  - Why needed here: DYNAC extends this architecture by adding dynamic vocabulary to the intermediate conditioning signal.
  - Quick check question: How does feeding intermediate predictions back into encoder layers relax the conditional independence assumption?

- **Concept: Contextual Biasing with Bias Lists**
  - Why needed here: The dynamic vocabulary is constructed from bias phrases; understanding why rare phrases degrade ASR motivates the entire approach.
  - Quick check question: Why does representing "Raphael" as ["Ra", "pha", "el"] hurt recognition compared to [<Raphael>]?

## Architecture Onboarding

- **Component map:**
  - Audio Encoder: 2 Conv layers (stride 2) → 256-dim projection → 12 Conformer layers (1024 units)
  - Bias Encoder: 6 Transformer blocks → mean pooling → phrase representations V ∈ RN×d
  - Scoring Layers: At layers 3, 6, 9 (intermediate) and 12 (final); compute Sstatic and Sdynamic
  - Back-Projection: Split → Linear(Zstatic) + Zdynamic·V → residual add
  - Output: Combined vocabulary of K + N tokens (static + dynamic)

- **Critical path:**
  1. Pre-process bias list once via BiasEnc → cache V for session
  2. Audio through encoder; at layers 3, 6, 9: score → back-project → residual feedback
  3. Final layer outputs joint distribution; apply bias weight μ to dynamic portion during decoding

- **Design tradeoffs:**
  - Layer selection S: Earlier = less context, later = more compute. Paper uses {3, 6, 9} empirically.
  - Bias weight μ (0.1): Lower values underweight dynamic tokens to prevent over-biasing.
  - Loss weight λ (0.15): Balances final CTC, intermediate CTC, and attention losses.

- **Failure signatures:**
  - U-WER spikes with high B-WER improvement → dynamic tokens dominate (see Figure 3a); check back-projection or μ setting
  - No B-WER improvement → intermediate layers may not be receiving gradient; verify Linter is active
  - RTF not improving → ensure bias encoder V is cached, not recomputed per utterance

- **First 3 experiments:**
  1. Reproduce Table 1 comparison: DYNAC vs. vanilla self-conditioned CTC vs. CTC-based CB (no intermediate integration) on LibriSpeech test-clean with N=1000
  2. Ablate layer set S: Compare {6, 9} vs. {3, 6, 9} vs. {4, 8, 12} to validate intermediate layer placement
  3. Sweep bias weight μ: Plot B-WER vs. U-WER tradeoff curve (e.g., μ ∈ {0.05, 0.1, 0.2, 0.5}) to find operating point

## Open Questions the Paper Calls Out

### Open Question 1
Can the bias weight μ be estimated adaptively or learned during training rather than manually tuned? The paper uses a manually tuned weight μ set to 0.15 and 0.1 experimentally, but manual tuning is dataset-specific and may not generalize well to domains with varying densities of bias phrases without extensive validation.

### Open Question 2
How does DYNAC perform in streaming scenarios where the encoder must process audio in chunks? The paper highlights latency-sensitive applications as motivation for avoiding AR models, but the Conformer encoder typically requires full-sequence context, leaving streaming behavior unexplored.

### Open Question 3
How does inference latency scale when the bias list size increases significantly beyond 1,000 phrases? The paper tests up to N=1000, but the computational cost of the scoring layer grows linearly with N, potentially creating a bottleneck for massive contact lists that could offset RTF gains.

## Limitations

- Vocabulary scaling boundary: The parameter-free back-projection mechanism's efficiency claim depends on fixed bias phrase embeddings, with no empirical validation under frequent bias list updates
- Intermediate layer selection sensitivity: The choice of layers {3, 6, 9} appears empirical rather than theoretically justified, without systematic exploration of alternative configurations
- Domain generalization limits: All experiments use clean speech datasets, with no evaluation of performance degradation under noisy conditions or highly confusable bias phrases

## Confidence

**High Confidence** - The core claim that DYNAC reduces RTF by 81% while maintaining comparable WER to AR-CB methods is well-supported by the experimental methodology on standard benchmarks.

**Medium Confidence** - The mechanism claim that self-conditioned CTC integration enables dependency capture between static and dynamic tokens is theoretically plausible but relies on implicit assumptions about attention layer behavior.

**Low Confidence** - The assertion that the parameter-free back-projection layer allows for efficient handling of dynamically changing vocabulary lacks empirical validation for frequent bias list updates or large vocabulary scaling.

## Next Checks

1. **Bias List Update Frequency Analysis** - Measure RTF impact when bias lists update at utterance-level vs. session-level to quantify the claimed efficiency advantage under realistic streaming conditions.

2. **Layer Configuration Ablation Study** - Systematically test alternative intermediate layer sets (e.g., {4, 8, 12}, {2, 5, 8, 11}) and distributions to identify whether the {3, 6, 9} configuration represents an optimal or merely sufficient choice.

3. **Noise Robustness Evaluation** - Evaluate DYNAC performance on noisy speech datasets (e.g., CHiME, noisy LibriSpeech variants) with the same 1000-bias-phrase setup to quantify domain generalization limits and identify failure modes.