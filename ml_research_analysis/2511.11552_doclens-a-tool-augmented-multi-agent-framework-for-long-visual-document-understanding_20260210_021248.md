---
ver: rpa2
title: 'DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document
  Understanding'
arxiv_id: '2511.11552'
source_url: https://arxiv.org/abs/2511.11552
tags:
- page
- pages
- visual
- answer
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DocLens introduces a tool-augmented multi-agent framework that
  solves the evidence localization challenge in long visual document understanding.
  It employs a "Lens Module" that first navigates to relevant pages using OCR-augmented
  retrieval and then pinpoints specific visual elements through layout detection and
  cropping.
---

# DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding

## Quick Facts
- arXiv ID: 2511.11552
- Source URL: https://arxiv.org/abs/2511.11552
- Reference count: 40
- Primary result: Achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing human experts, with particular strength in vision-centric and unanswerable queries.

## Executive Summary
DocLens introduces a tool-augmented multi-agent framework to address the evidence localization challenge in long visual document understanding. It employs a "Lens Module" that first navigates to relevant pages using OCR-augmented retrieval and then pinpoints specific visual elements through layout detection and cropping. This fine-grained approach achieves near-perfect page recall (97.3%) and significantly improves comprehension of charts, tables, and figures. The "Reasoning Module" uses a sampling-adjudication mechanism to generate reliable answers. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing human experts for the first time, with particular strength in vision-centric and unanswerable queries.

## Method Summary
DocLens is a tool-augmented multi-agent framework that solves long visual document understanding through two key stages. The Lens Module first retrieves relevant pages using an OCR-augmented page navigator that samples $T_e=8$ times with temperature $\tau=0.7$ to generate candidate page sets, taking the union of results. It then localizes specific visual elements (charts, tables) using a layout detector (MinerU) to crop these regions. The Reasoning Module employs a sampling-adjudication mechanism where an answer sampler generates $T_a=8$ reasoning-answer pairs, and an adjudicator selects the most consistent conclusion. The framework uses Gemini-2.5-Pro or Claude models with chunking ($K=50$ pages) for context-limited models, and provides full prompt templates for all agents.

## Key Results
- Achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V benchmarks
- Near-perfect page recall of 97.3% on MMLongBench-Doc
- Surpasses human experts for the first time on visual document understanding tasks
- Significant improvements on vision-centric (+4.9% precision, +9.3% recall) and unanswerable queries (+8.2% to +13.8% absolute gains)

## Why This Works (Mechanism)

### Mechanism 1: OCR-Augmented Page Retrieval via Multi-Sampling
The Page Navigator processes the document using interleaved page screenshots and extracted OCR text. It performs $T_e$ sampling iterations with temperature $\tau > 0$ to generate candidate page sets, taking the union of these sets ($E_{pred}$). This "sampling-union" strategy broadens the search space, while OCR converts implicit visual text into explicit tokens that current LLMs reason over more effectively than pure pixel-based features. This achieves 10-40% recall improvements compared to vector-based retrieval.

### Mechanism 2: Fine-Grained Visual Element Isolation (Zooming)
The Element Localizer applies a layout detection tool (MinerU) to identify bounding boxes of visual elements on predicted pages. It crops these regions to create a focused visual input set $V_k$. This reduces noise from surrounding text/layout and increases the effective resolution of visual evidence, improving precision by 4.9% and recall by 9.3% on visual-centric queries.

### Mechanism 3: Sampling-Adjudication for Hallucination Suppression
The Answer Sampler generates $T_a$ reasoning-answer pairs. The Adjudicator selects the most consistent conclusion, allowing the system to cross-validate "Not answerable" verdicts against hallucinated answers. This mechanism reduces hallucination rates, achieving absolute gains of +8.2%, +13.0%, and +13.8% on the Unanswerable subset.

## Foundational Learning

- **Concept: Vision-Language Retrieval vs. Text-Based Retrieval**
  - Why needed here: DocLens relies on the observation that standard vector retrieval underperforms compared to LLM-based selection with OCR
  - Quick check question: Why does a vector-retrieval model (like ColPali) struggle to find a page based on a reasoning query compared to an LLM reading the OCR text?

- **Concept: Layout Analysis and Document Parsing**
  - Why needed here: The system depends on external tools (MinerU) to segment the page. The quality of the "zoom" depends entirely on the bounding box accuracy
  - Quick check question: What happens to the Element Localizer's output if the layout detector merges a chart with its caption into a single bounding box?

- **Concept: Self-Consistency in LLMs**
  - Why needed here: The "Sampling-Adjudication" mechanism is a variant of self-consistency. Aggregating multiple reasoning paths generally improves performance on tasks requiring logic
  - Quick check question: In the DocLens context, why is "Union" used for page retrieval while "Adjudication" is used for the final answer?

## Architecture Onboarding

- **Component map:**
  1. Inputs: Document pages ($P$), Question ($Q$)
  2. Lens Module: Page Navigator (LLM + OCR tool → Set of relevant pages) → Element Localizer (Layout Detector + Crop tool → Set of cropped images ($V$))
  3. Reasoning Module: Answer Sampler (Generates $N$ candidate reasoning traces) → Adjudicator (Selects the single best trace/answer)
  4. Tools: MinerU (OCR/Layout), Gemini/Claude APIs

- **Critical path:**
  1. Pre-processing: Run MinerU to extract OCR text for all pages
  2. Retrieval: Run Page Navigator with $T_e=8$ samples. Merge results to get $E_{pred}$
  3. Localization: Detect layout on $E_{pred}$, crop elements ($V_k$)
  4. Generation: Sample $T_a=8$ answers using cropped images + text
  5. Adjudication: Run final LLM call to pick the best answer

- **Design tradeoffs:**
  - Recall vs. Context Window: Increasing sampling iterations increases recall but retrieves more pages, potentially overflowing context windows
  - Cost vs. Accuracy: Using Gemini-2.5-Pro for the entire pipeline is expensive; hybrid approach (Flash for Nav, Pro for Reasoning) optimizes cost
  - Granularity vs. Context: Cropping every visual element provides detail but removes spatial context; system feeds both full page and crops to mitigate this

- **Failure signatures:**
  - Low Recall: Page Navigator returns empty list or wrong pages. Check: Is OCR text clean? Is temperature $\tau$ too low?
  - Visual Misinterpretation: Correct page retrieved, wrong answer on a chart. Check: Did the Layout Detector crop the chart correctly?
  - Hallucination: Model invents answer. Check: Did the Adjudicator fail to validate against "Not answerable" condition?

- **First 3 experiments:**
  1. Page Navigator Ablation: Run Page Navigator with OCR enabled vs. disabled on MMLongBench-Doc to verify 10-40% recall lift
  2. Visual Element Impact: Evaluate performance on "Text-Only" vs. "Visual-Only" subsets with Element Localizer enabled/disabled to replicate Figure 4 trend
  3. Hybrid Backbone Cost Test: Implement hybrid configuration (Flash-Lite for Nav, Pro for Reasoning) to verify accuracy remains within 2-3% of full Pro pipeline while reducing cost by >50%

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be automatically adapted to construct expert-level agents for specific domains (e.g., legal, medical) that require specialized knowledge for accurate interpretation?
- Basis in paper: The Limitations section states that the current domain-agnostic approach struggles with specialized documents and identifies "Automatically constructing expert-level agents tailored to different document domains" as a promising direction
- Why unresolved: The current framework uses general-purpose VLMs and tools without distinct mechanisms to encode or retrieve domain-specific expert knowledge
- What evidence would resolve it: A comparative evaluation on domain-specific benchmarks (e.g., legal contracts or medical reports) showing improved performance when using automatically tuned or retrieved domain-specific prompts/tools versus the baseline

### Open Question 2
- Question: Can dedicated agentic frameworks be designed to handle specific complex visual elements (e.g., node graphs, heatmaps) where simple "zooming-in" and cropping strategies fail?
- Basis in paper: The Limitations section and Appendix E note that many challenging cases persist that "cannot be adequately addressed through simple 'zooming-in' strategies" and suggest designing "dedicated agentic frameworks tailored to specific visual element types"
- Why unresolved: The Element Localizer currently treats visual elements generically via layout detection and cropping, which fails on complex spatial reasoning tasks
- What evidence would resolve it: A module specifically designed for, e.g., chart reasoning, that outperforms the generic Element Localizer on the "Hard Cases" provided in the appendix

### Open Question 3
- Question: To what extent is the high recall of the Page Navigator dependent on high-quality OCR, and can visual-only retrieval bridge the performance gap observed when OCR is absent?
- Basis in paper: Table 4 shows that removing OCR drops recall significantly (e.g., from 97.3% to 87.3% on MMLongBench-Doc and 94.3% to 53.8% on FinRAGBench-V), implying a heavy reliance on text extraction
- Why unresolved: The paper demonstrates OCR is vital but does not explore if advanced visual embeddings could recover this loss for documents where OCR is noisy or impossible
- What evidence would resolve it: An ablation study using the "w/o OCR" setting but substituting the text input with state-of-the-art visual embeddings (like ColPali) to see if the recall gap closes

## Limitations
- Heavy dependency on external tool (MinerU) with unspecified configuration parameters
- Context window management challenges for models with limited capacity, with incomplete details on chunk merging
- LLM-as-a-judge evaluation for FinRAGBench-V introduces potential subjectivity compared to rule-based evaluation
- Limited empirical validation of claimed cost-benefit analysis for hybrid backbone configuration

## Confidence

- **High Confidence**: The core multi-agent framework design (Lens + Reasoning modules) and the overall performance trends (SOTA on both benchmarks, superiority on vision-centric and unanswerable queries)
- **Medium Confidence**: The specific quantitative impact of OCR augmentation on page recall (10-40% gains) and the sampling-adjudication mechanism's effect on hallucination suppression (+8-13% on UNA subset)
- **Low Confidence**: The exact cost-benefit analysis of the proposed hybrid backbone configuration, as empirical validation is minimal

## Next Checks

1. **Page Navigator OCR Ablation**: Run the Page Navigator with OCR enabled vs. disabled on MMLongBench-Doc to verify the claimed 10-40% recall improvement

2. **Visual Element Impact Analysis**: Evaluate performance on "Text-Only" vs. "Visual-Only" subsets of data with the Element Localizer enabled/disabled to replicate the trend in Figure 4

3. **Hybrid Backbone Cost-Test**: Implement the "Hybrid" configuration (Flash-Lite for Nav, Pro for Reasoning) to verify if accuracy remains within 2-3% of the full Pro pipeline while reducing cost by >50%