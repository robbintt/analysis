---
ver: rpa2
title: Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph Learning
arxiv_id: '2505.10040'
source_url: https://arxiv.org/abs/2505.10040
tags:
- learning
- task
- feature
- tasks
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of catastrophic forgetting in
  Graph Neural Networks (GNNs) under the Non-Exemplar Continual Graph Learning (NECGL)
  setting, where historical raw data is inaccessible. The proposed Instance-Prototype
  Affinity Learning (IPAL) framework combines Prototype Contrastive Learning (PCL)
  with three key innovations: Topology-Integrated Gaussian Prototypes (TIGP) that
  weight node contributions using PageRank to emphasize influential nodes, Instance-Prototype
  Affinity Distillation (IPAD) that aligns instance-prototype relationships to flexibly
  regularize feature space while maintaining compatibility with PCL, and a Decision
  Boundary Perception (DBP) mechanism that leverages high-entropy instances near decision
  boundaries to enhance inter-class separability.'
---

# Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph Learning

## Quick Facts
- **arXiv ID**: 2505.10040
- **Source URL**: https://arxiv.org/abs/2505.10040
- **Reference count**: 40
- **Primary result**: IPAL consistently outperforms state-of-the-art methods in non-exemplar continual graph learning, achieving superior average performance while balancing plasticity and stability.

## Executive Summary
This paper introduces IPAL (Instance-Prototype Affinity Learning), a novel framework for non-exemplar continual graph learning that addresses catastrophic forgetting in Graph Neural Networks when historical raw data is inaccessible. IPAL integrates prototype contrastive learning with three key innovations: Topology-Integrated Gaussian Prototypes that weight node contributions using PageRank, Instance-Prototype Affinity Distillation that aligns instance-prototype relationships, and a Decision Boundary Perception mechanism that enhances inter-class separability. Extensive experiments on four node classification benchmarks demonstrate that IPAL achieves state-of-the-art performance while maintaining better trade-offs between plasticity and stability.

## Method Summary
IPAL addresses catastrophic forgetting in non-exemplar continual graph learning by combining prototype contrastive learning with three key innovations. First, Topology-Integrated Gaussian Prototypes (TIGP) weight node contributions using PageRank to emphasize influential nodes. Second, Instance-Prototype Affinity Distillation (IPAD) aligns instance-prototype relationships to flexibly regularize feature space while maintaining compatibility with PCL. Third, a Decision Boundary Perception (DBP) mechanism leverages high-entropy instances near decision boundaries to enhance inter-class separability. The framework is evaluated on four node classification benchmarks and demonstrates consistent superiority over state-of-the-art methods.

## Key Results
- IPAL consistently outperforms state-of-the-art methods on four node classification benchmarks (CS-CL, CoraFull-CL, Arxiv-CL, Reddit-CL).
- Achieves superior average performance while maintaining better trade-offs between plasticity and stability.
- Addresses limitations of conventional Prototype Replay by reducing feature drift through PCL while providing more flexible knowledge retention compared to feature distillation approaches.

## Why This Works (Mechanism)
The paper proposes IPAL to address catastrophic forgetting in GNNs under non-exemplar continual graph learning by learning instance-prototype affinities. The core mechanism combines prototype contrastive learning with topology-aware prototypes, affinity distillation, and decision boundary perception. This integrated approach reduces feature drift while maintaining flexible knowledge retention, with the topology weighting ensuring influential nodes contribute more to prototypes and the decision boundary mechanism enhancing inter-class separability.

## Foundational Learning
- **Graph Neural Networks (GNNs)**: Deep learning models for graph-structured data that aggregate information from node neighborhoods; needed for understanding the baseline models being improved.
- **Catastrophic Forgetting**: The phenomenon where neural networks forget previously learned information when trained on new tasks; central problem being addressed.
- **Prototype Learning**: A framework where each class is represented by a prototype vector, enabling contrastive learning and knowledge distillation; forms the foundation of IPAL's approach.
- **PageRank Algorithm**: A link analysis algorithm that measures node importance in graphs; used to weight node contributions in prototype construction.
- **Decision Boundaries**: The surfaces separating different classes in feature space; exploited by DBP to enhance separability.

## Architecture Onboarding

**Component Map**: Input Graphs → GNN Backbone → Feature Extraction → Prototype Construction (TIGP) → Instance-Prototype Affinity Learning → Classification Head

**Critical Path**: GNN Feature Extraction → Topology-Integrated Gaussian Prototypes → Instance-Prototype Affinity Distillation → Decision Boundary Perception → Classification

**Design Tradeoffs**: IPAL trades off computational complexity (through PageRank-based weighting) for improved prototype quality and reduced forgetting. The framework balances between flexibility (affinity distillation) and stability (prototype contrast), while the decision boundary mechanism adds overhead but improves separability.

**Failure Signatures**: Performance degradation may occur on heterophilic graphs where high-impact nodes have different labels than their neighbors, or when decision boundaries are poorly defined early in training. The PageRank weighting may also fail when structural importance doesn't correlate with semantic representativeness.

**First 3 Experiments to Run**:
1. Ablation study removing DBP to isolate its contribution to overall performance
2. Comparison of TIGP with uniform prototype construction on heterophilic datasets
3. Evaluation of affinity distillation strength by varying the distillation weight parameter

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the IPAL framework be effectively adapted to online continual graph learning settings where data arrives in a continuous stream rather than discrete offline tasks?
- **Basis in paper**: [explicit] The authors state, "Our future work will seek to adapt IPAL to online settings with streaming data... its applicability to online settings... remains to be further investigated."
- **Why unresolved**: The current implementation relies on distinct task boundaries to compute offline Topology-Integrated Gaussian Prototypes (TIGP) and execute Feature Drift Compensation (Eq. 11), mechanisms that may fail when task identities are blurred or data arrives in mini-batches.
- **What evidence would resolve it**: Evaluation of IPAL on online CGL benchmarks (e.g., within the CGLB framework) using a streaming data protocol, measuring performance degradation as the frequency of prototype updates increases.

### Open Question 2
- **Question**: Does IPAL generalize to heterogeneous cross-domain scenarios where the sequential tasks originate from distinct graphs with divergent feature spaces?
- **Basis in paper**: [explicit] Appendix C notes that "Existing approaches... assume a unified dataset... However, in real-world scenarios involving heterogeneous domains, the cross-domain generalizability remains to be validated."
- **Why unresolved**: The experimental validation is restricted to class-incremental splits within single datasets (e.g., splitting CoraFull classes into tasks), ensuring consistent topological semantics and feature dimensions that may not hold in cross-domain applications.
- **What evidence would resolve it**: Experiments where the model sequentially trains on graphs from different domains (e.g., citation networks followed by social networks) to observe if the prototype alignment mechanisms maintain zero-shot transfer capabilities.

### Open Question 3
- **Question**: Does the PageRank-based weighting in TIGP negatively impact performance on strongly heterophilic graphs where high-impact nodes may possess different labels than their neighbors?
- **Basis in paper**: [inferred] The method constructs Topology-Integrated Gaussian Prototypes (TIGP) using PageRank (Eq. 3) to emphasize "high-impact" nodes. This implicitly assumes that structural importance correlates with label representativeness (homophily), an assumption challenged in heterophilic graphs.
- **Why unresolved**: While the method is tested on Arxiv-CL, the paper does not explicitly analyze performance on datasets specifically designed to be low-homophily, where high-degree nodes might be structural outliers rather than semantic centroids.
- **What evidence would resolve it**: Ablation studies on heterophilic benchmarks (e.g., Chameleon or Squirrel) comparing the performance of PageRank-weighted prototypes against simple mean-based prototypes to isolate the effect of topology-aware weighting.

## Limitations
- Experimental validation is primarily limited to node classification tasks, raising questions about generalizability to other graph learning scenarios.
- Effectiveness of PageRank-based weighting for prototype construction in continual learning settings has not been thoroughly validated through ablation studies.
- Decision Boundary Perception mechanism's sensitivity to entropy thresholds and performance on poorly-defined boundaries is not discussed.

## Confidence
- **High Confidence**: The general problem formulation of non-exemplar continual graph learning and the overall framework design are well-grounded and clearly articulated.
- **Medium Confidence**: Experimental results showing IPAL's superiority over baseline methods are promising but would benefit from additional validation with variance measures and statistical significance testing.
- **Low Confidence**: Specific implementation details of TIGP construction and IPAD loss formulation are not fully transparent, making reproducibility assessment difficult.

## Next Checks
1. **Ablation Studies with Different Prototype Construction Methods**: Conduct experiments comparing TIGP against alternative prototype construction approaches, such as uniform weighting or using other centrality measures beyond PageRank, to quantify the specific contribution of the topology-integrated weighting scheme.

2. **Cross-Task Generalization Evaluation**: Evaluate IPAL on graph classification and link prediction tasks in continual learning settings to assess whether the instance-prototype affinity learning framework generalizes beyond node classification, and test on graphs with different structural properties (heterophilic, small-world, scale-free).

3. **Sensitivity Analysis of Hyperparameters**: Perform a comprehensive sensitivity analysis on critical hyperparameters, including the entropy threshold for DBP, the weighting factors in the overall loss function, and the PageRank damping factor used in TIGP, to understand the robustness of IPAL to hyperparameter choices and identify potential failure modes.