---
ver: rpa2
title: Evaluation-Aware Reinforcement Learning
arxiv_id: '2509.19464'
source_url: https://arxiv.org/abs/2509.19464
tags:
- policy
- evaluation
- value
- eva-rl
- predictor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces evaluation-aware reinforcement learning (EvA-RL),
  a framework that learns policies optimized for both performance and ease of evaluation.
  Unlike standard RL, which learns arbitrary policies then evaluates them afterward,
  EvA-RL explicitly trains policies to minimize evaluation error under a value prediction
  scheme during training.
---

# Evaluation-Aware Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.19464
- Source URL: https://arxiv.org/abs/2509.19464
- Reference count: 40
- Primary result: RL framework that learns policies optimized for both performance and ease of evaluation, reducing evaluation error by 40-90% across discrete and continuous control tasks.

## Executive Summary
This paper introduces evaluation-aware reinforcement learning (EvA-RL), a framework that learns policies optimized for both performance and ease of evaluation. Unlike standard RL, which learns arbitrary policies then evaluates them afterward, EvA-RL explicitly trains policies to minimize evaluation error under a value prediction scheme during training. The authors propose co-learning a transformer-based state-value predictor alongside the policy, enabling accurate policy evaluation from limited assessment rollouts in a proxy environment. Theoretical analysis reveals a tradeoff between performance and evaluation accuracy when using a fixed predictor, which the co-learning approach mitigates. Experiments across discrete (Asterix, Freeway, Space Invaders) and continuous (HalfCheetah, Reacher, Ant) control domains show that EvA-RL achieves competitive returns while substantially reducing evaluation error compared to standard RL plus OPE baselines.

## Method Summary
EvA-RL modifies standard policy gradient by adding a prediction-error penalty that encourages policies to be more predictable. The method co-learns a transformer-based value predictor alongside the policy, using data from a separate assessment environment to generate behavioral encodings. The predictor is trained to estimate deployment returns from assessment rollouts, while the policy is trained to maximize returns while minimizing the squared difference between true and predicted values. This creates a feedback loop where policies evolve to be more predictable by the learned value function. The approach is implemented on top of A2C-style policy gradient with a two-stage training process: initial policy-only training followed by co-learning.

## Key Results
- EvA-RL reduces evaluation error by 40-90% compared to standard RL with OPE baselines across 6 benchmark environments
- Co-learning the predictor eliminates the theoretical tradeoff between performance and evaluation accuracy identified in Proposition 3.1
- The transformer predictor achieves accurate value estimates using only 5 assessment rollouts from fixed start states
- EvA-RL maintains competitive or superior returns compared to standard RL while enabling more reliable policy assessment

## Why This Works (Mechanism)
EvA-RL works by transforming the policy learning objective from pure return maximization to a weighted combination of return maximization and predictability minimization. The key insight is that by training policies to minimize the squared difference between true and predicted values during training, the resulting policies become inherently more predictable and thus easier to evaluate. The transformer-based predictor learns to aggregate information from assessment rollouts to estimate values in the deployment environment, creating a closed-loop system where policies are shaped by their own predictability. The co-learning approach ensures that as the predictor improves, the policy can adapt to maintain both high performance and low evaluation error, avoiding the performance-evaluation tradeoff that occurs with a fixed predictor.

## Foundational Learning

- **Concept**: Policy gradient with regularization terms
  - Why needed here: EvA-RL modifies standard policy gradient by adding a prediction-error penalty; understanding how regularization affects gradient flow is essential for tuning β.
  - Quick check question: How does increasing β affect the relative contribution of the predictability term versus return maximization in the gradient?

- **Concept**: Off-policy evaluation (OPE) methods (FQE, PDIS, DR)
  - Why needed here: The paper positions EvA-RL against OPE baselines; knowing their failure modes (variance, support mismatch) clarifies the problem being solved.
  - Quick check question: Why does importance sampling suffer from exponential variance in long horizons?

- **Concept**: Transformer attention for sequence aggregation
  - Why needed here: The value predictor uses transformer architecture to aggregate assessment returns; understanding attention as learned weighting is necessary for debugging prediction failures.
  - Quick check question: In the predictor, what role does the query state token play versus the assessment state/return tokens?

## Architecture Onboarding

- **Component map**: Deployment environment M_D -> Policy π_θ -> Deployment returns g(h^D) -> Value predictor ψ_ϕ <- Assessment environment M_A -> Assessment returns g(h_i^A)

- **Critical path**:
  1. Collect deployment rollouts → compute g(h^D) for current policy
  2. Collect k assessment rollouts from fixed start-states → compute {g(h_i^A)}
  3. Store (assessment data, deployment states, deployment returns) in B_ψ
  4. Update predictor ψ_ϕ via regression on deployment returns conditioned on assessment data
  5. Update policy π_θ via gradient including both return maximization and prediction error minimization

- **Design tradeoffs**:
  - Higher β → lower evaluation error but potentially lower returns (Proposition 3.1)
  - Larger k (assessment start-states) → richer behavioral encoding but higher assessment cost
  - Longer assessment horizon → more informative returns but increased variance
  - Co-learning vs. fixed predictor: co-learning reduces tradeoff but requires stable predictor training

- **Failure signatures**:
  - Prediction error remains high despite training: assessment start-states may be uninformative; increase k or redesign selection
  - Returns drop sharply when enabling evaluation-awareness: β too high; reduce predictability coefficient
  - Predictor overfits to recent policies: increase replay buffer size m or reduce predictor learning rate
  - Gradient instability during co-learning: predictor not warmed up sufficiently; increase initial policy-only training phase

- **First 3 experiments**:
  1. Baseline verification: Run EvA-RL with β=0 (standard RL) to confirm policy gradient implementation matches expected performance
  2. Tradeoff characterization: Sweep β ∈ {0, 0.01, 0.1} with frozen pre-trained predictor; verify monotonic decrease in both returns and evaluation error per Proposition 3.1
  3. Co-learning validation: Compare evaluation error of co-learned predictor vs. OPE methods (FQE, PDIS, DR) on final EvA-RL policy; confirm substantial reduction as shown in Figure 3

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis relies on strong assumptions about predictor consistency and deployment-return distributions that may not hold in practice
- Limited exploration of how assessment start-state selection affects performance across different environment types
- No analysis of how the method scales to high-dimensional state spaces or long-horizon tasks
- Potential computational overhead from maintaining and updating the value predictor alongside the policy

## Confidence
- Theoretical claims about the performance-evaluation tradeoff: Medium - Proofs assume idealized conditions
- Empirical evaluation showing superior accuracy: High - Results are well-documented with error bars
- Claim that co-learning mitigates the tradeoff: Medium - Limited ablations and parameter sensitivity analysis
- Claim of broad applicability across domains: Low - Only tested on 6 environments with specific hyperparameter choices

## Next Checks
1. Verify Proposition 3.1 tradeoff by sweeping β values with a frozen predictor on a single environment, measuring the monotonic relationship between returns and evaluation error
2. Test robustness to assessment start-state selection by comparing k=1 vs k=5 vs k=10 in a controlled setting to quantify sensitivity to behavioral coverage
3. Validate the necessity of the two-stage training (policy-only warm-up followed by co-learning) by comparing against immediate co-learning from random initialization