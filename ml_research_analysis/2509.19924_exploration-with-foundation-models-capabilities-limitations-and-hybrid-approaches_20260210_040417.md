---
ver: rpa2
title: 'Exploration with Foundation Models: Capabilities, Limitations, and Hybrid
  Approaches'
arxiv_id: '2509.19924'
source_url: https://arxiv.org/abs/2509.19924
tags:
- exploration
- agent
- reward
- llms
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work benchmarks LLMs and VLMs as zero-shot exploration agents
  in classic RL settings, revealing that VLMs can infer high-level objectives but
  fail at precise low-level control, forming a "knowing-doing gap." Across multi-armed
  bandits, gridworlds, and sparse-reward Atari games, VLMs struggle with motor control
  and semantic grounding despite strong visual understanding. In a simple on-policy
  hybrid framework, VLM-guided PPO agents achieve faster early-stage learning in Freeway,
  demonstrating that VLM guidance can enhance sample efficiency when high-level reasoning
  is accurate.
---

# Exploration with Foundation Models: Capabilities, Limitations, and Hybrid Approaches

## Quick Facts
- arXiv ID: 2509.19924
- Source URL: https://arxiv.org/abs/2509.19924
- Reference count: 11
- Key outcome: VLMs can infer high-level objectives but struggle with precise low-level control, forming a "knowing-doing gap" in exploration tasks.

## Executive Summary
This paper explores the use of Large Language Models (LLMs) and Vision-Language Models (VLMs) as zero-shot exploration agents in classic reinforcement learning (RL) settings. The study reveals that while VLMs can understand high-level visual and textual cues to infer goals, they consistently fail at precise motor control tasks. This creates a significant "knowing-doing gap" where models understand what to do but cannot execute it effectively. The research benchmarks these foundation models across multi-armed bandits, gridworlds, and sparse-reward Atari games, demonstrating their limitations in both semantic grounding and motor precision despite strong visual understanding.

## Method Summary
The authors evaluate LLMs and VLMs in three distinct RL environments: multi-armed bandits, gridworlds, and sparse-reward Atari games. They employ zero-shot prompting strategies where models must infer exploration policies without fine-tuning. The evaluation measures both high-level reasoning (goal inference) and low-level control (action selection). A hybrid approach is then introduced where VLM-guided policies are combined with Proximal Policy Optimization (PPO) agents. This hybrid framework uses the VLM to guide early-stage exploration while standard RL handles fine-tuning, particularly in the Freeway environment where it demonstrates faster initial learning compared to pure RL baselines.

## Key Results
- VLMs can successfully infer high-level objectives from visual observations but fail at precise low-level control tasks
- The "knowing-doing gap" manifests across all tested environments, with VLMs understanding goals but struggling to execute actions
- VLM-guided PPO agents achieve faster early-stage learning in Freeway, demonstrating that VLM guidance can enhance sample efficiency when high-level reasoning is accurate

## Why This Works (Mechanism)
The success of VLM-guided exploration stems from the models' ability to understand visual and textual context to infer goals, which provides useful high-level exploration heuristics. However, the fundamental limitation lies in VLMs' inability to translate abstract understanding into precise motor commands. The hybrid approach works by leveraging the VLM's strength in semantic understanding for initial exploration while relying on traditional RL methods for fine-grained control and policy optimization. This division of labor addresses the knowing-doing gap by not requiring VLMs to perform tasks they're ill-suited for.

## Foundational Learning
- **Zero-shot prompting**: Why needed: Enables immediate deployment without costly fine-tuning; Quick check: Verify prompt format consistency across different environments
- **Reinforcement learning fundamentals**: Why needed: Understanding exploration-exploitation trade-offs in sparse-reward settings; Quick check: Confirm baseline RL performance matches literature
- **Vision-language model architecture**: Why needed: Understanding how VLMs process multimodal inputs for task inference; Quick check: Validate VLM's ability to correctly identify objects and goals in test images
- **Proximal Policy Optimization (PPO)**: Why needed: Standard RL algorithm used as baseline and in hybrid approach; Quick check: Verify PPO implementation matches established hyperparameters
- **Exploration strategies**: Why needed: Comparing VLM-guided exploration against established methods; Quick check: Confirm exploration bonus mechanisms are properly implemented
- **Sparse reward environments**: Why needed: Challenging setting where exploration is critical for learning; Quick check: Verify reward sparsity matches experimental design specifications

## Architecture Onboarding

**Component Map**: VLM/LLM -> Policy Generator -> Action Selector -> Environment -> Reward/Observation Feedback -> VLM/LLM

**Critical Path**: VLM reasoning about visual inputs → Policy generation → Action selection → Environment interaction → Reward observation → Policy refinement (in hybrid approach with PPO)

**Design Tradeoffs**: Zero-shot VLM guidance offers no training overhead but limited precision, while fine-tuning VLMs could improve control but requires extensive data and computational resources. The hybrid approach balances immediate deployment benefits with the need for precise control through standard RL fine-tuning.

**Failure Signatures**: VLMs consistently fail at tasks requiring fine motor control (grid navigation, precise action sequences), while excelling at high-level goal identification and semantic reasoning. The knowing-doing gap appears most prominently in environments requiring both visual understanding and precise execution.

**First Experiments**: 1) Test VLM's ability to identify goals in novel visual scenes without action execution, 2) Compare zero-shot VLM exploration against epsilon-greedy and UCB strategies in multi-armed bandits, 3) Implement and test the hybrid VLM-PPO approach on a simplified version of Freeway to verify early-stage learning improvements.

## Open Questions the Paper Calls Out
The paper identifies several open questions including how to better bridge the knowing-doing gap, whether alternative VLM architectures or prompting strategies could improve low-level control, and how the hybrid approach scales to more complex environments beyond the tested Atari games and gridworlds.

## Limitations
- Evaluation primarily confined to simple environments (multi-armed bandits, gridworlds, Atari games) that may not represent real-world complexity
- Focus on a single VLM model (GPT-4V) limits generalizability across different VLM architectures
- Hybrid approach shows early-stage improvements but lacks long-term performance comparison against standard RL methods

## Confidence
- High: Controlled experimental setup with clear methodology
- Medium: Preliminary nature of hybrid approach and limited environmental scope
- Low: Single VLM model evaluation may not capture full spectrum of capabilities

## Next Checks
1) Evaluate the hybrid approach across a broader range of complex environments to assess scalability and robustness
2) Investigate the impact of different VLM architectures and training regimes on exploration performance
3) Conduct ablation studies to quantify the specific contributions of VLM guidance versus other exploration strategies in the hybrid framework