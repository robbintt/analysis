---
ver: rpa2
title: 'Toward Auditable Neuro-Symbolic Reasoning in Pathology: SQL as an Explicit
  Trace of Evidence'
arxiv_id: '2601.01875'
source_url: https://arxiv.org/abs/2601.01875
tags:
- reasoning
- features
- feature
- pathology
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an SQL-centered neuro-symbolic framework for
  pathology image analysis that enables auditable reasoning. The approach extracts
  multi-scale cellular features and uses Feature Reasoning Agents to compose and execute
  SQL queries over structured feature tables, producing quantitative evidence chains.
---

# Toward Auditable Neuro-Symbolic Reasoning in Pathology: SQL as an Explicit Trace of Evidence

## Quick Facts
- arXiv ID: 2601.01875
- Source URL: https://arxiv.org/abs/2601.01875
- Reference count: 0
- Primary result: Neuro-symbolic framework using SQL queries to provide auditable evidence traces in pathology image analysis

## Executive Summary
This paper presents a neuro-symbolic framework for pathology image analysis that combines deep learning with explicit reasoning through SQL queries. The approach extracts multi-scale cellular features and uses Feature Reasoning Agents to compose and execute SQL queries over structured feature tables, creating quantitative evidence chains. A Knowledge Comparison Agent validates these findings against pathological criteria to generate hypotheses with calibrated confidence. The system couples this SQL reasoning branch with a CNN model, fusing their outputs through a Report Agent to produce final diagnoses with traceable reasoning.

## Method Summary
The framework extracts multi-scale cellular features and employs Feature Reasoning Agents to compose and execute SQL queries over structured feature tables, producing quantitative evidence chains. A Knowledge Comparison Agent validates these findings against pathological criteria to generate hypotheses with calibrated confidence. The system couples this SQL reasoning branch with a CNN model, fusing their outputs through a Report Agent to produce final diagnoses with traceable reasoning. Experiments on two pathology visual question answering datasets demonstrate improved interpretability and decision traceability while maintaining competitive diagnostic accuracy.

## Key Results
- 96.5% accuracy on µ-bench pathology dataset
- 83.5% accuracy on GADVR dataset
- SQL traces provide verifiable links between cellular measurements and diagnostic conclusions

## Why This Works (Mechanism)
The framework works by creating explicit evidence traces through SQL queries that bridge cellular measurements with diagnostic conclusions. Multi-scale feature extraction captures relevant cellular patterns, while Feature Reasoning Agents translate these into structured queries. The Knowledge Comparison Agent validates findings against established pathological criteria, ensuring the reasoning follows medical logic. This explicit reasoning process makes the diagnostic pathway auditable and verifiable.

## Foundational Learning
- Multi-scale feature extraction (why needed: captures cellular patterns at different resolutions; quick check: verify feature consistency across scales)
- SQL-based evidence chaining (why needed: creates explicit, auditable reasoning paths; quick check: validate SQL query logic against feature tables)
- Knowledge validation agents (why needed: ensures reasoning follows pathological criteria; quick check: test against known pathological cases)
- CNN-CNN fusion (why needed: combines symbolic reasoning with pattern recognition; quick check: compare single vs. fused model performance)
- Confidence calibration (why needed: provides uncertainty estimates for hypotheses; quick check: evaluate calibration on validation set)
- Evidence traceability (why needed: enables auditability of diagnostic decisions; quick check: verify complete evidence chains for each diagnosis)

## Architecture Onboarding

**Component Map:** Feature Extraction -> Feature Reasoning Agent -> SQL Query Execution -> Knowledge Comparison Agent -> Report Agent -> Diagnosis

**Critical Path:** Image → Multi-scale Features → SQL Query Composition → Evidence Chain → Hypothesis Validation → Final Diagnosis

**Design Tradeoffs:** The framework prioritizes interpretability and auditability over pure performance, accepting some computational overhead for explicit reasoning traces. The SQL-based approach adds complexity but enables verifiable decision-making.

**Failure Signatures:** Poor SQL query composition leading to incomplete evidence chains, mismatched feature scales affecting query accuracy, knowledge base gaps causing validation failures, or fusion weight misalignment between CNN and SQL reasoning branches.

**First Experiments:** 1) Validate feature extraction consistency across different scales, 2) Test SQL query generation accuracy on synthetic feature tables, 3) Evaluate Knowledge Comparison Agent performance on known pathological cases

## Open Questions the Paper Calls Out
None

## Limitations
- Framework complexity may impact scalability to larger, more diverse pathology datasets
- Single-dataset validations may not generalize across different pathology domains
- Integration of SQL-based reasoning with deep learning models introduces computational overhead

## Confidence

**High Confidence:** SQL-based evidence tracing mechanism and its ability to link cellular measurements to diagnostic conclusions; technical architecture combining Feature Reasoning Agents, Knowledge Comparison Agents, and Report Agents

**Medium Confidence:** Clinical utility of auditable reasoning approach; generalizability of neuro-symbolic framework across different pathology tasks

## Next Checks
1. Cross-validation on additional pathology datasets with different imaging modalities and diagnostic tasks to assess framework generalizability
2. Clinical workflow integration testing with pathologists to evaluate the practical utility of SQL-based evidence traces in real diagnostic scenarios
3. Comparative analysis of reasoning efficiency and computational overhead versus purely CNN-based approaches across different feature scales and dataset sizes