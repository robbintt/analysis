---
ver: rpa2
title: 'LUME-DBN: Full Bayesian Learning of DBNs from Incomplete data in Intensive
  Care'
arxiv_id: '2511.04333'
source_url: https://arxiv.org/abs/2511.04333
tags:
- data
- missing
- bayesian
- learning
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of learning Dynamic Bayesian
  Networks (DBNs) from incomplete longitudinal clinical data, a common issue in intensive
  care settings. Traditional methods fail to account for the temporal nature of the
  data, leading to biased results.
---

# LUME-DBN: Full Bayesian Learning of DBNs from Incomplete data in Intensive Care

## Quick Facts
- arXiv ID: 2511.04333
- Source URL: https://arxiv.org/abs/2511.04333
- Reference count: 40
- Primary result: Full Bayesian Dynamic Bayesian Network learning from incomplete longitudinal ICU data using Gibbs sampling for joint inference of structure, parameters, and missing values

## Executive Summary
This paper addresses the challenge of learning Dynamic Bayesian Networks (DBNs) from incomplete longitudinal clinical data in intensive care settings. Traditional methods fail to account for temporal dependencies, leading to biased results. The authors propose LUME-DBN, a novel Gibbs sampling-based method that treats missing values as unknown parameters following a Gaussian distribution. This approach enables principled imputation and uncertainty estimation within a full Bayesian framework, outperforming standard techniques like MICE in reconstruction accuracy and convergence properties.

## Method Summary
LUME-DBN is a Gibbs sampling-based method for learning Dynamic Bayesian Networks from incomplete longitudinal data. The approach treats missing values as unknown parameters and samples them from tractable Gaussian full conditional distributions that incorporate both parent and child information. The method interleaves parameter updates (collapsed Gibbs), structural updates (Metropolis-Hastings), and missing value updates every EM iterations. This joint inference framework allows the model to escape local optima by maintaining uncertainty throughout learning rather than committing to fixed imputations early.

## Key Results
- LUME-DBN demonstrates superior reconstruction accuracy compared to standard model-agnostic techniques like MICE on synthetic datasets
- The method successfully reconstructs clinically meaningful temporal relationships in ICU data with up to 40% missingness
- Convergence properties are significantly better than traditional sequential imputation-then-learning approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Missing values can be imputed by sampling from tractable Gaussian full conditional distributions that incorporate both parent and child information in the DBN.
- **Mechanism:** Each missing value contributes to its own conditional likelihood and all its children's conditional likelihoods. By the local Markov property and conditional Gaussian properties, the FCD simplifies to a univariate Gaussian with closed-form mean (μ*) and variance (σ²*) parameters that combine precisions from the node and its children.
- **Core assumption:** Missing values are Missing At Random (MAR); the DBN correctly captures temporal dependencies; Gaussian distributions are appropriate for continuous clinical variables.
- **Evidence anchors:**
  - [abstract] "Our method treats each missing value as an unknown parameter following a Gaussian distribution... unobserved values are sampled from their full conditional distributions"
  - [Section 3.3] "the FCD of each missing value is a univariate Gaussian distribution... we can efficiently sample the missing values, enabling a robust Gibbs sampling procedure"
  - [corpus] Related work on Bayesian Integration of Nonlinear Incomplete Clinical Data (BIONIC) confirms tractability of Bayesian missing data handling, though for different model classes
- **Break condition:** If missingness mechanism is MNAR (Missing Not At Random), the uniform/Gaussian prior assumption over missing values is violated, biasing imputations.

### Mechanism 2
- **Claim:** Joint inference of structure, parameters, and missing values outperforms sequential imputation-then-learning approaches.
- **Mechanism:** LUME-DBN interleaves parameter moves (collapsed Gibbs), structural moves (Metropolis-Hastings), and missing value updates every EM iterations. This allows the model to escape local optima by maintaining uncertainty throughout learning rather than committing to fixed imputations early.
- **Core assumption:** The MCMC chain converges to the stationary distribution; burn-in period is sufficient; thinning reduces autocorrelation adequately.
- **Evidence anchors:**
  - [abstract] "Compared to standard model-agnostic techniques such as MICE, our Bayesian approach demonstrates superior reconstruction accuracy"
  - [Section 2] "sampling-based MCMC methods... can escape local minima and yield more accurate network reconstructions"
  - [corpus] Grzegorczyk (2023) on being Bayesian about learning Gaussian BNs from incomplete data provides theoretical foundation for joint inference benefits
- **Break condition:** If EM interval is too large relative to convergence speed, or if missingness exceeds ~40%, structure learning becomes unstable (see convergence diagnostics in Appendix D).

### Mechanism 3
- **Claim:** Temporal coherence is preserved by including children's likelihoods in the imputation FCD, unlike static methods that ignore future time points.
- **Mechanism:** The FCD for a missing value x_t^i incorporates: (1) its own conditional likelihood given parents at t-1, and (2) the conditional likelihoods of children at t+1. The β_i^(j) coefficients weight how strongly each child's observed value constrains the imputation.
- **Core assumption:** First-order Markov assumption holds (X_t+1 ⊥ X_t-1 | X_t); no instantaneous effects within time slices.
- **Evidence anchors:**
  - [Section 3.3] "A missing value for a variable X_i at time t contributes to: Its own conditional likelihood... The conditional likelihoods of all its children"
  - [Section 4] "MICE... completely losing effectiveness once missingness exceeds 20% reflecting its inefficiency when applied in the context of temporal data"
  - [corpus] No direct corpus comparison of temporal vs. static imputation in DBNs; this is a gap the paper addresses
- **Break condition:** If higher-order Markov dependencies exist but are not modeled, imputation quality degrades; if time discretization (6-hour bins in ICU data) is too coarse, temporal relationships are obscured.

## Foundational Learning

- **Concept: Bayesian Linear Regression with Conjugate Priors**
  - Why needed here: DBN learning under Gaussian assumptions reduces to k independent BLRs; understanding Normal-Inverse-Gamma conjugacy is essential for deriving collapsed Gibbs updates.
  - Quick check question: Can you derive the posterior distribution for regression coefficients β given a design matrix X and response Y with Normal-Inverse-Gamma priors?

- **Concept: Markov Chain Monte Carlo (Gibbs Sampling and Metropolis-Hastings)**
  - Why needed here: LUME-DBN combines Gibbs sampling (for parameters and missing values) with MH (for discrete structure moves); understanding convergence diagnostics (PSRF) is critical.
  - Quick check question: Explain why Gibbs sampling works when conditional distributions are tractable but the joint posterior is not.

- **Concept: Dynamic Bayesian Networks and First-Order Markov Assumption**
  - Why needed here: The entire framework assumes temporal dependencies are captured by parent sets from time t-1 to t; understanding d-separation and Markov blankets in DBNs is necessary.
  - Quick check question: What is the Markov blanket of a node X_t^i in a DBN, and why does it matter for deriving FCDs?

## Architecture Onboarding

- **Component map:** Data Layer (N, k, T+1) -> Lagged Form (N·T, 2k) -> Parameter Module (β^(i), σ²^(i), δ²^(i)) -> Structure Module (π^(i)) -> Imputation Module (missing values) -> Orchestrator (Algorithm B.4)
- **Critical path:** 1. Initialize: mean imputation, random parent sets 2. Burn-in: 5k epochs, discard early samples 3. Thinning: Keep every 5th sample post-convergence 4. Aggregation: Average inclusion probabilities 5. Thresholding: Apply 0.8 threshold to adjacency matrix
- **Design tradeoffs:** EM interval: Larger values (e.g., 10) speed convergence but may miss fine-grained imputation updates; Fan-in restriction: Limiting max parents to 5 prevents overfitting but may miss true dense structures; Local vs. global standardization: Local preserves group-specific dynamics; global increases comparability but may introduce spurious arcs
- **Failure signatures:** PSRF > 1.1 on arc indicators or missing values: Chain has not converged; AUC-PR close to baseline (MICE): Likely EM interval too large or missingness too high; Spurious arcs under global standardization: Small sample sizes causing shrinkage to global means; Missing value imputation diverging: Check for systematic missingness (MNAR) patterns
- **First 3 experiments:** 1. Reproduction on synthetic data: Generate 10-node DBNs with T=100, introduce 20% MCAR missingness, verify AUC-PR improvement over MICE matches paper results 2. Convergence sensitivity: Vary EM interval (1, 5, 10, 20) and measure epochs to PSRF < 1.1 for both structure and missing values at 30% missingness 3. Ablation on prior strength: Modify Poisson prior parameter λ (controlling expected parent set size) and measure impact on reconstruction accuracy vs. false positive rate

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LUME-DBN be extended to handle Missing Not At Random (MNAR) patterns, where the missingness mechanism depends on unobserved values?
- **Basis in paper:** [explicit] The conclusion explicitly states, "Another extension we have in mind is the integration of methods to manage Missing Not At Random (MNAR) patterns, which are frequent in clinical data due to unmodeled external factors."
- **Why unresolved:** The current methodology assumes uniform priors and effectively treats data as Missing At Random (MAR) or Missing Completely At Random (MCAR), ignoring the possibility that the probability of missingness is correlated with the missing value itself (e.g., a patient being too sick for a measurement).
- **What evidence would resolve it:** A modification of the Gibbs sampling step to include a model of the missingness mechanism (e.g., a selection model) and a demonstration of reconstruction accuracy on simulated datasets with defined MNAR structures.

### Open Question 2
- **Question:** Can the method be generalized to non-homogeneous Dynamic Bayesian Networks (NH-DBNs) to capture non-stationary relationships across time?
- **Basis in paper:** [explicit] The authors state, "We also aim to generalize our approach to non-homogeneous DBNs (NH-DBNs) to capture non-stationary relationships across time and patient groups."
- **Why unresolved:** The current model assumes the network structure and parameters are stationary (constant) across all time slices (first-order Markov property). This limits the ability to model evolving clinical states, such as distinct treatment phases in an ICU stay.
- **What evidence would resolve it:** Deriving the necessary conditional distributions for a model where parameters vary over time and applying it to longitudinal data where structural changes are known to occur.

### Open Question 3
- **Question:** How can the framework be adapted to handle mixed variable types (e.g., discrete interventions alongside continuous vitals) without relying solely on Gaussian assumptions?
- **Basis in paper:** [explicit] The paper notes, "extending these methods to continuous domains poses an additional challenge, as it requires dealing with mixed variable types. To address these complexities, we plan to investigate approaches based on logistic distributions..."
- **Why unresolved:** The current mathematical derivations for the Full Conditional Distributions (FCDs) rely on Gaussian conjugacy (Normal-Normal) and linear regression. Discrete variables would require logistic or multinomial likelihoods, breaking the closed-form solutions used in the current Gibbs sampler.
- **What evidence would resolve it:** An implementation utilizing Metropolis-Hastings steps or Pólya-Gamma data augmentation to handle discrete nodes, validated on datasets containing both continuous and categorical clinical variables.

## Limitations

- The Gaussian assumption for continuous variables may not hold for all clinical measurements, particularly for count or ordinal data common in intensive care
- The method's reliance on the Missing At Random (MAR) assumption is critical - if missingness patterns are systematically related to unobserved values (MNAR), imputation accuracy will degrade significantly
- The computational complexity scales poorly with network size and missingness rate, limiting applicability to very large datasets

## Confidence

- **High Confidence:** The core mechanism of sampling missing values from tractable Gaussian full conditional distributions is mathematically sound and well-supported by conditional Gaussian properties
- **Medium Confidence:** The superiority over MICE methods is demonstrated empirically but requires more extensive validation across diverse missingness patterns and clinical domains
- **Medium Confidence:** The temporal coherence preservation through child-likelihood incorporation is theoretically justified but needs more rigorous comparison against alternative temporal imputation methods

## Next Checks

1. **Robustness to Missingness Mechanisms:** Systematically test LUME-DBN performance under different missingness patterns (MCAR, MAR, MNAR) to identify failure thresholds and bias patterns.
2. **Generalization to Non-Gaussian Data:** Evaluate the method on clinical variables with non-normal distributions (e.g., count data, binary outcomes) to assess Gaussian assumption violations.
3. **Cross-Domain Validation:** Apply LUME-DBN to non-clinical longitudinal datasets with known ground truth structures to evaluate performance outside the ICU domain.