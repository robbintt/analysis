---
ver: rpa2
title: Exploring the Potential of Large Language Models in Fine-Grained Review Comment
  Classification
arxiv_id: '2508.09832'
source_url: https://arxiv.org/abs/2508.09832
tags:
- code
- review
- llms
- comments
- categories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explored the potential of Large Language Models (LLMs)
  for fine-grained classification of code review comments into 17 categories. We designed
  various prompt engineering approaches using different classification strategies
  (flat vs hierarchical) and input contexts (with or without code).
---

# Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification

## Quick Facts
- arXiv ID: 2508.09832
- Source URL: https://arxiv.org/abs/2508.09832
- Reference count: 40
- Key result: LLMs outperform supervised deep learning for code review comment classification, with Llama 3.1-405B achieving 46.2% weighted F1-score

## Executive Summary
This paper investigates the effectiveness of Large Language Models (LLMs) for fine-grained classification of code review comments into 17 categories. The authors compare various prompt engineering approaches, including flat versus hierarchical classification strategies and the inclusion of code context. Their experiments show that LLMs, particularly Llama 3.1-405B, significantly outperform traditional supervised deep learning methods in classifying review comments, especially for the most useful but scarce categories. The study demonstrates that LLMs can address class imbalance issues common in code review datasets while providing a scalable solution for enhancing code review analytics.

## Method Summary
The study employs zero-shot inference with instruction-tuned LLMs (Qwen 2-7B/72B, Llama 3-8B/70B/405B) to classify 1,828 manually annotated code review comments from the OpenStack Nova Project. Two classification strategies are tested: flat (single-step into 17 categories) and hierarchical (two-step: first 5 groups, then subcategories). Two input contexts are compared: comment-only versus code+comment. Multiple-choice prompt format with refined category definitions is used, with response extraction via "$" stopping criterion. Performance is evaluated using weighted average F1-score, precision, recall, and accuracy, with statistical significance assessed via one-sided Wilcoxon signed-rank test. The baseline is CodeBERT+LSTM with 10-fold cross-validation.

## Key Results
- Llama 3.1-405B achieved the highest average F1 score of 46.2% using the flat strategy with code context
- LLMs significantly outperformed CodeBERT+LSTM in classifying the five most useful categories (Request for modification, Positive, Critical, Positive with suggestion, Others)
- Small models (7-8B) showed 14-21% F1 improvement when using hierarchical strategy instead of flat strategy
- LLMs demonstrated balanced performance across high- and low-frequency categories, addressing class imbalance issues

## Why This Works (Mechanism)
The paper does not explicitly explain the mechanism behind why LLMs work for this task, as it focuses on empirical evaluation rather than theoretical analysis.

## Foundational Learning
- **Zero-shot classification**: Using pre-trained LLMs without fine-tuning on task-specific data, enabling quick deployment and reducing data requirements.
- **Prompt engineering**: Designing effective prompts with clear instructions and multiple-choice formats to guide LLM responses.
- **Hierarchical classification**: Breaking complex classification tasks into multiple steps to improve accuracy, especially for smaller models.
- **Context inclusion**: Providing code context alongside comments to improve classification accuracy by giving models more information.
- **Weighted F1-score**: Using weighted metrics to account for class imbalance in performance evaluation.
- **Statistical significance testing**: Applying Wilcoxon signed-rank test to validate that performance differences are not due to chance.

## Architecture Onboarding

**Component Map**
LLM API -> Prompt Template -> Code Context Provider -> Response Parser -> Performance Metrics

**Critical Path**
1. Load dataset with comments and associated code context
2. Construct prompt templates with refined category definitions
3. Execute zero-shot inference on all comments using multiple LLMs
4. Extract and parse model responses using "$" stopping criterion
5. Compute performance metrics (weighted F1-score, precision, recall)

**Design Tradeoffs**
- Flat vs. hierarchical strategies: Flat strategy simpler but challenging for small models; hierarchical improves small model performance at cost of complexity
- Comment-only vs. code+comment: Code context improves accuracy but requires more processing and may not always be available
- Model size selection: Larger models (405B) perform best but are more expensive; smaller models need hierarchical strategy for good performance

**Failure Signatures**
- <5% of responses are non-standard format (not following multiple-choice structure)
- Small models show poor performance with flat strategy due to long context
- Low F1 scores (7-25%) for minority classes despite overall improvement

**First 3 Experiments**
1. Test Llama 3.1-405B with flat strategy and code+comment context on a small subset
2. Compare hierarchical vs. flat strategies for Qwen 2-7B to verify 14-21% improvement
3. Run baseline CodeBERT+LSTM on the same subset for direct comparison

## Open Questions the Paper Calls Out
None

## Limitations
- Missing complete prompt specifications and refined category definitions in paper body, requiring replication package access
- Unspecified LLM inference hyperparameters (temperature, top_p, max_tokens) that could affect results
- Dataset selection bias from excluding 672 comments without code context
- Persistent low performance (7-25% F1) on minority classes despite overall improvement

## Confidence

**High confidence:**
- Sound experimental methodology with appropriate metrics and statistical testing
- Valid comparative framework against established deep learning baseline

**Medium confidence:**
- Reported performance improvements over traditional methods, limited by incomplete prompt specifications
- Claims about addressing class imbalance, given remaining low F1 scores for rare categories

## Next Checks
1. Reconstruct and test different prompt formulations to validate sensitivity to prompt engineering
2. Systematically vary LLM inference parameters to measure impact on classification performance
3. Compare classification performance on full dataset versus filtered dataset to assess selection bias