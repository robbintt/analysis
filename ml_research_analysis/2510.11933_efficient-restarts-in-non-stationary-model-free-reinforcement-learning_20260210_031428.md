---
ver: rpa2
title: Efficient Restarts in Non-Stationary Model-Free Reinforcement Learning
arxiv_id: '2510.11933'
source_url: https://arxiv.org/abs/2510.11933
tags:
- restarts
- reward
- restartq-ucb
- learning
- restart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses inefficiency in non-stationary reinforcement
  learning by improving restart strategies in existing algorithms. It identifies two
  core problems: complete forgetting of learned information after restarts and rigid
  scheduled restart timings.'
---

# Efficient Restarts in Non-Stationary Model-Free Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.11933
- Source URL: https://arxiv.org/abs/2510.11933
- Reference count: 40
- Primary result: Achieves up to 91% reduction in dynamic regret compared to baseline RestartQ-UCB algorithm

## Executive Summary
This paper addresses inefficiency in non-stationary reinforcement learning by improving restart strategies in existing algorithms. It identifies three core problems: complete forgetting of learned information after restarts, rigid scheduled restart timings, and inefficient use of computational resources during restarts. The authors propose three approaches: partial restarts that use tighter bounds based on variation budgets, adaptive restarts that trigger based on reward analysis, and selective restarts that update only specific Q-table entries. They evaluate these methods on RandomMDP and Bidirectional Diabolical Combination Locks environments, demonstrating significant performance improvements while maintaining theoretical guarantees.

## Method Summary
The paper proposes three restart strategies for non-stationary reinforcement learning: partial restarts that reset Q-values to tighter bounds using variation budgets rather than theoretical maximums, adaptive restarts that trigger based on reward monitoring using a sliding window approach, and selective restarts that update only specific Q-table entries along experienced trajectories. These methods are evaluated on RandomMDP and Bidirectional Diabolical Combination Locks environments, with implementations using both RestartQ-UCB and RANDOMIZEDQ base algorithms. The experiments compare dynamic regret and cumulative reward across different non-stationary scenarios including abrupt and gradual changes.

## Key Results
- Partial restarts reduce dynamic regret by approximately 74% compared to baseline RestartQ-UCB algorithm
- Selective restarts achieve near-zero dynamic regret in abrupt BDCL at episode 7,500, with 91% reduction versus baseline
- Adaptive restarts trigger only after abrupt changes, achieving higher cumulative reward than scheduled restarts in tested environments

## Why This Works (Mechanism)

### Mechanism 1: Partial Restarts with Variation Budgets
Resetting Q-values to tighter upper bounds preserves exploitable information while maintaining optimism required for exploration. Partial restarts use known variation budgets (Δr, Δp) to compute a maximum possible change in optimal Q-values between episodes via Lemma 1. Instead of resetting Q(s,a,h) to theoretical maximum H−h+1, the algorithm resets to QK(s,a) + bound, where QK is the learned value at epoch end.

Core assumption: Q-values remain ≥ optimal Q* after restart; learned Q never drops below minimum Q* within an epoch; variation budgets Δr and Δp are known or well-estimated.

Evidence anchors:
- [abstract] "Partial restarts use variation budgets to reset Q-values to tighter bounds rather than theoretical maximums."
- [section 4.1] Figure 1 demonstrates faster recovery after abrupt change at episode 1001 compared to full restart.
- [corpus] Related work (arxiv:2601.01069) notes weighted/restart strategies are common for non-stationarity but doesn't address partial information retention.

Break condition: If variation budgets are unknown and overestimated, partial restarts degrade toward full restarts; if underestimated, Q-values may fall below Q* and optimism is lost.

### Mechanism 2: Adaptive Restart Triggering
Monitoring reward trajectories enables detection of when environmental change warrants a restart, avoiding unnecessary scheduled restarts. A sliding window of length W (estimated during initial learning) tracks cumulative reward. The algorithm computes: expected reward without restart (rC × remaining episodes) vs. with restart (rL + rB × remaining episodes after relearning). Restart triggers when restart-expected reward exceeds continue-expected reward.

Core assumption: Environmental changes that affect policy quality manifest as measurable drops in per-episode reward; the horizon T is known.

Evidence anchors:
- [abstract] "Adaptive restarts detect when restarts are needed by monitoring reward patterns instead of using fixed schedules."
- [section 4.2] Figure 2 shows adaptive restarts trigger only after abrupt changes, achieving higher cumulative reward than scheduled restarts.
- [corpus] Weak direct evidence—related papers focus on sliding-window and weighted strategies but not reward-based restart triggering.

Break condition: Changes that don't reduce reward rate (e.g., alternative optimal paths emerge) will not trigger detection; worst-case environments can be constructed where adaptive underperforms scheduled.

### Mechanism 3: Selective Q-Table Updates
Updating only Q-entries along experienced trajectories, scaled by Bellman update differences, focuses restart computation on policy-relevant regions. When |Uh(s,a)current − Uh(s,a)last| exceeds bound βh(s,a), the algorithm traces the trajectory leading to (s,a,h) and increments Q-values by: ΔQ = sign(difference) × softmax(Q) × (1/H−h′) × βh′. The softmax weighting prioritizes high-Q actions; the step-index scaling reduces updates for early steps that branch frequently.

Core assumption: Variance of Q(s,·) is small enough that selective updates shift the argmax policy; works best paired with agile stationary algorithms (RANDOMIZEDQ) rather than UCB-based staged updates.

Evidence anchors:
- [abstract] "Selective restarts combine these approaches by updating only specific Q-table entries based on Bellman update differences."
- [section 4.3 + Figure 3] Near-zero dynamic regret in abrupt BDCL at episode 7,500; 91% reduction vs. baseline.
- [corpus] No direct corpus evidence for trajectory-selective restart mechanisms.

Break condition: Update rule (Equation 3) is heuristic without theoretical proof; selective restarts accumulate regret after thousands of episodes as confidence intervals widen.

## Foundational Learning

- **Q-learning with optimism (UCB/Q-UCB)**
  - Why needed here: The base algorithms (RestartQ-UCB, RANDOMIZEDQ) maintain optimistic Q-estimates to drive exploration. Partial restarts specifically preserve this optimism at tighter bounds.
  - Quick check question: Can you explain why initializing Q-values above their true maximum encourages exploration?

- **Variation budgets (Δr, Δp) in non-stationary MDPs**
  - Why needed here: Partial restarts directly depend on these quantities to bound Q* changes. Without understanding them, the Lemma 1 derivation is opaque.
  - Quick check question: Given a non-stationary MDP, how would you compute Δr and Δp from Equations 1–2?

- **Dynamic regret vs. cumulative reward**
  - Why needed here: All performance claims (74%, 91% improvements) are measured via dynamic regret—the gap to an optimal policy that may itself change over time.
  - Quick check question: Why is dynamic regret a more appropriate metric than static regret for non-stationary environments?

## Architecture Onboarding

- **Component map**: Base RL algorithm -> Partial restart module -> Adaptive restart module -> Selective restart module -> Performance evaluation

- **Critical path**: Initialize Q-table (H × S × A) with optimistic values → During episodes: execute policy, observe rewards/transitions, update Q-table per base algorithm rules → At each Q-update point: check selective restart trigger (|Uh difference| vs. βh) → At epoch boundaries (scheduled) or adaptive trigger points: execute restart logic (partial, adaptive, or selective) → Return: cumulative reward, dynamic regret vs. optimal policy

- **Design tradeoffs**:
  - Partial restarts: Require known Δr/Δp; tighter bounds accelerate convergence but risk optimism violation if budgets underestimated
  - Adaptive restarts: Avoid unnecessary restarts but fail to detect changes that don't reduce reward rate; adds O(W) storage
  - Selective restarts: Computationally targeted but heuristic; works better with RANDOMIZEDQ than RestartQ-UCB due to agile vs. staged update structure

- **Failure signatures**:
  - Partial: Q-values systematically below optimal after restart → pessimism, poor exploration (check: is Δr + Δp underestimated?)
  - Adaptive: Restarts trigger too frequently or not at all → check window size W and reward statistics
  - Selective: Dynamic regret grows after initial good performance → confidence interval widening; consider hybrid scheduled + selective approach

- **First 3 experiments**:
  1. Replicate RandomMDP (S=5, A=5, H=5, T=50,000) with AdaptivePartial RestartQ-UCB; measure dynamic regret reduction vs. baseline; verify ~74% improvement
  2. Run abrupt BDCL (T=100,000, changes every 1,001 episodes) with SelectiveRestarts + RANDOMIZEDQ; confirm near-optimal early performance and identify when regret begins accumulating
  3. Ablation test: compare partial-only, adaptive-only, and combined partial+adaptive restarts on gradual BDCL to isolate contribution of each component

## Open Questions the Paper Calls Out

### Open Question 1
Can the selective restart update rule (Equation 3) be derived from theoretical principles to guarantee asymptotic performance, rather than relying on its current heuristic formulation? The authors empirically observed strong early performance but eventual divergence from optimal policy, indicating the heuristic lacks theoretical grounding.

### Open Question 2
How can partial restarts be implemented when variation budgets (Δp and Δr) are unknown, while maintaining theoretical performance guarantees? The partial restart approach requires knowing total variation budgets, which are rarely available in real-world scenarios.

### Open Question 3
How can adaptive restarts detect environmental changes that do not immediately reduce the agent's reward rate? The current sliding-window approach compares reward totals, missing changes that affect optimal policy without changing immediate rewards.

### Open Question 4
What causes selective restarts to accumulate dynamic regret after extended episodes, and can this divergence be prevented? The empirical observation of late-horizon performance degradation was noted but not investigated mechanistically.

## Limitations
- Selective restart updates lack theoretical grounding and accumulate regret after thousands of episodes
- Partial restarts critically depend on accurate knowledge of variation budgets (Δr, Δp) which are rarely available in practice
- Adaptive restart mechanism may miss environmental changes that don't reduce reward rates, creating blind spots in detection capability

## Confidence

**High Confidence**: Dynamic regret improvements (74% on RandomMDP, 91% on BDCL) are well-supported by the experimental results and measurement methodology. The core insight that maintaining vs. discarding information during restarts impacts performance is empirically validated.

**Medium Confidence**: The theoretical framework for partial restarts (Lemma 1 bounds) appears sound, but practical effectiveness depends heavily on accurate variation budget estimation, which isn't addressed in the paper.

**Low Confidence**: Selective restart performance claims, particularly the near-zero regret in abrupt BDCL, require caution. The paper notes selective restarts accumulate regret after thousands of episodes due to confidence interval widening, suggesting the initial performance may not be sustainable.

## Next Checks

1. **Variation Budget Sensitivity**: Run partial restarts with overestimated and underestimated Δr/Δp values to quantify performance degradation when budgets are misspecified. This tests the practical robustness of the approach.

2. **Adaptive Restart Blind Spots**: Design non-stationary MDPs where optimal policies change without reducing reward rates (e.g., alternative optimal paths emerge). Measure whether adaptive restarts fail to trigger and quantify the resulting performance gap.

3. **Selective Restart Long-term Stability**: Extend BDCL experiments beyond 100,000 episodes to observe when and how selective restarts accumulate regret. Compare performance degradation against baseline algorithms to assess practical utility windows.