---
ver: rpa2
title: 'City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal
  Incomplete Learning'
arxiv_id: '2507.12795'
source_url: https://arxiv.org/abs/2507.12795
tags:
- scene
- understanding
- city-vlm
- multimodal
- outdoor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of outdoor scene understanding
  across multiple scales, views, and modalities, which existing large vision-language
  models (LVLMs) struggle with due to limited data and inadequate multimodal fusion
  methods. The authors propose a new dataset, SVM-City, comprising 420k images and
  4,811M point clouds with 567k QA pairs collected from vehicles, drones, planes,
  and satellites.
---

# City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning

## Quick Facts
- **arXiv ID:** 2507.12795
- **Source URL:** https://arxiv.org/abs/2507.12795
- **Reference count:** 40
- **Primary result:** City-VLM achieves 18.14% better performance than existing LVLMs on average across outdoor QA tasks, with improvements up to 30% on low-altitude tasks.

## Executive Summary
City-VLM addresses the challenge of outdoor scene understanding across multiple scales, views, and modalities, which existing large vision-language models (LVLMs) struggle with due to limited data and inadequate multimodal fusion methods. The authors propose a new dataset, SVM-City, comprising 420k images and 4.81B point clouds with 567k QA pairs collected from vehicles, drones, planes, and satellites. They also introduce City-VLM, an LVLM using incomplete multimodal learning with an Incomplete Multimodal Fusion Module (IMF Module) that constructs a joint probabilistic distribution space for robust fusion even when one modality is missing. Experiments on three outdoor QA tasks show City-VLM achieves 18.14% better performance than existing LVLMs on average, with improvements up to 30% on low-altitude tasks.

## Method Summary
City-VLM introduces a novel approach to outdoor scene understanding through incomplete multimodal learning. The method combines pre-trained vision encoders (EVA-CLIP-E for 2D images, Uni3D-L for 3D point clouds) with a Vicuna-7B LLM through a specialized Incomplete Multimodal Fusion (IMF) Module. This IMF module uses a VAE-based adapter to construct a joint probabilistic distribution space, allowing robust fusion even when one modality is missing. The model is trained using LoRA fine-tuning on the SVM-City dataset, which contains 420k images, 4.81B point clouds, and 567k QA pairs across terrestrial, drone, and satellite/aerial views. Training uses Adam optimizer with weight decay of 5e-4 and learning rate of 1e-3 on 8 NVIDIA RTX A6000 GPUs.

## Key Results
- City-VLM achieves 18.14% better performance than existing LVLMs on average across three outdoor QA benchmarks.
- The model shows improvements up to 30% on low-altitude tasks compared to baseline approaches.
- City-VLM demonstrates superior performance on EarthVQA, City-3DQA, and Nuscenes-QA benchmarks.

## Why This Works (Mechanism)
The IMF module's key innovation is constructing a joint probabilistic distribution space through a VAE-based adapter. This allows the model to generate latent representations that can handle missing modalities by sampling from the learned distribution rather than requiring all inputs to be present. The probabilistic approach provides regularization and robustness when one modality is absent, using zero-padding for missing inputs while maintaining coherent fusion through the VAE's learned distribution.

## Foundational Learning
- **Probabilistic Fusion with VAE:** Uses variational autoencoder to learn joint distribution of multimodal features, enabling sampling when one modality is missing.
  - *Why needed:* Traditional fusion methods fail when one modality is absent; VAE provides principled way to handle incompleteness.
  - *Quick check:* Verify the VAE learns reasonable $\mu$ and $\sigma$ distributions by inspecting samples from the learned space.

- **Incomplete Multimodal Learning:** Architecture designed to handle cases where either 2D images or 3D point clouds may be missing during inference.
  - *Why needed:* Real-world scenarios often have incomplete sensor data; model must remain functional without both modalities.
  - *Quick check:* Test model performance with only 2D inputs and only 3D inputs separately.

- **Automated QA Generation:** Uses ChatGPT to generate QA pairs from scene graphs created through segmentation and spatial reasoning.
  - *Why needed:* Manual annotation of 567k QA pairs would be prohibitively expensive; automation enables large-scale dataset creation.
  - *Quick check:* Sample generated QA pairs to verify logical consistency and relevance to scene content.

## Architecture Onboarding

**Component Map:** 2D Images (EVA-CLIP-E) -> IMF Module <- 3D Point Clouds (Uni3D-L) -> Vision-Language Projector -> Vicuna-7B LLM

**Critical Path:** Image/Point Cloud Encoder → IMF Module (VAE fusion) → Vision-Language Projector → LLM Head

**Design Tradeoffs:** The use of zero-padding for missing modalities prioritizes computational simplicity over potential semantic richness that generative imputation might provide. The VAE-based approach trades some representational precision for robustness to incomplete data.

**Failure Signatures:** 
- Performance degradation on counting questions due to autoregressive LLM limitations.
- Potential performance drop when one modality is consistently missing if VAE sampling fails to capture sufficient semantic information.
- Errors in QA pairs due to ChatGPT annotation noise affecting training quality.

**3 First Experiments:**
1. Test IMF module with only 2D inputs and only 3D inputs to verify robustness to missing modalities.
2. Evaluate performance on counting-specific questions to quantify the autoregressive limitation.
3. Compare performance when using zero-padding versus generative imputation for missing modalities.

## Open Questions the Paper Calls Out
The paper explicitly identifies limitations with counting-related questions, attributing difficulties to the limitations of autoregressive methods in accurately handling counting tasks. This suggests the current LLM architecture struggles with numerical precision and spatial reasoning for quantitative queries.

## Limitations
- The model encounters difficulties with counting-related questions due to autoregressive method limitations.
- Training duration and exact vision-language projector architecture are unspecified, potentially affecting reproducibility.
- The IMF module's reliance on zero-tensor padding for missing modalities may lose semantic information compared to generative imputation methods.

## Confidence
- **High:** The overall methodology (IMF module, dataset collection approach) and reported benchmark results are well-described and internally consistent.
- **Medium:** The claimed 18.14% average improvement and 30% boost on low-altitude tasks, while supported by the experimental setup, depend on the unspecified training duration and projector architecture.
- **Low:** The effectiveness of the IMF module in handling missing modalities is plausible but not directly validated in the paper.

## Next Checks
1. **Ablation on training duration:** Run experiments with varying epochs/steps (e.g., 5, 10, 15 epochs) to confirm the reported performance is stable and not overfit to a specific schedule.
2. **Single-modality evaluation:** Systematically test City-VLM with only 2D or only 3D inputs to verify the IMF module's ability to handle missing modalities as claimed.
3. **Counting task stress test:** Create a focused test set of counting questions (e.g., "How many trees?" "How many cars?") to diagnose the reported weakness and measure its impact on overall accuracy.