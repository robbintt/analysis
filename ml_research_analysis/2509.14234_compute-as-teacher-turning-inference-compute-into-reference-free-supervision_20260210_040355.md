---
ver: rpa2
title: 'Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision'
arxiv_id: '2509.14234'
source_url: https://arxiv.org/abs/2509.14234
tags:
- synthesis
- rollouts
- arxiv
- reference
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces Compute as Teacher (CaT), a framework that\
  \ uses inference compute to generate reference-free supervision for reinforcement\
  \ learning, especially in non-verifiable domains like healthcare guidance where\
  \ human annotations are scarce. CaT aggregates multiple parallel rollouts from a\
  \ model into a pseudo-reference using a synthesis method and derives rewards via\
  \ self-proposed rubrics\u2014binary, auditable criteria generated from the pseudo-reference\
  \ and scored by an LLM judge."
---

# Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision

## Quick Facts
- **arXiv ID**: 2509.14234
- **Source URL**: https://arxiv.org/abs/2509.14234
- **Reference count**: 36
- **Primary result**: CaT uses inference compute to generate reference-free supervision, achieving up to 30% relative improvement over initial policy on HealthBench without ground-truth labels

## Executive Summary
Compute as Teacher (CaT) introduces a novel reinforcement learning framework that transforms inference compute into reference-free supervision, addressing a critical bottleneck in domains where human annotations are scarce or costly. The method aggregates multiple parallel rollouts from a model to create a pseudo-reference, then uses self-proposed rubrics—binary, auditable criteria generated by an LLM—to derive rewards without requiring ground-truth labels. This approach is particularly valuable for non-verifiable domains like healthcare guidance, where traditional RLHF struggles due to lack of verifiable rewards.

The framework demonstrates substantial performance gains across benchmarks, achieving up to 30% relative improvement over initial policies on HealthBench while using 9× less test-time compute compared to inference-time synthesis. CaT's versatility extends beyond healthcare to verifiable domains like MATH-500, where it matches top baselines. By leveraging multiple rollouts and systematic rubric generation, CaT overcomes limitations of judge-only feedback and provides a practical pathway for post-training in specialized domains without requiring expensive human annotations.

## Method Summary
CaT operates through a two-stage process: synthesis and supervision. During synthesis, the framework generates multiple parallel rollouts from the current policy and aggregates them into a pseudo-reference using a synthesis method. This synthesis reconciles partial reasoning across different rollouts to create a comprehensive reference output. In the supervision stage, CaT generates self-proposed rubrics—binary, auditable criteria derived from the pseudo-reference—which are then scored by an LLM judge to produce rewards for reinforcement learning.

The approach fundamentally differs from traditional RLHF by eliminating the need for ground-truth labels. Instead of comparing against human annotations or verifiable answers, CaT creates its own supervision signal through the synthesis of multiple model-generated responses. The self-proposed rubrics ensure that the evaluation criteria are both comprehensive and interpretable, addressing the common problem of black-box judge feedback. This reference-free mechanism enables RL in domains where verification is inherently difficult, such as medical advice or complex reasoning tasks.

## Key Results
- Achieved up to 30% relative improvement over initial policy on HealthBench while using 9× less test-time compute than inference-time synthesis
- Matches or exceeds inference-time synthesis quality in healthcare domains without requiring ground-truth labels
- Demonstrates versatility by matching top baselines on MATH-500, proving effectiveness across both verifiable and non-verifiable domains
- Outperforms supervised fine-tuning and selection baselines, with synthesis reconciling partial reasoning across multiple rollouts

## Why This Works (Mechanism)
CaT leverages inference-time compute to generate diverse model responses through parallel rollouts, capturing multiple perspectives on the same input. The synthesis mechanism aggregates these responses to create a pseudo-reference that represents a consensus or comprehensive view, effectively turning compute into a supervision signal. The self-proposed rubrics translate this pseudo-reference into actionable, binary criteria that an LLM judge can reliably score, creating a feedback loop that drives policy improvement without human annotations.

## Foundational Learning
- **Reinforcement Learning without Ground-Truth**: CaT eliminates the dependency on human annotations by synthesizing supervision from model-generated responses, enabling RL in domains where verification is impossible or prohibitively expensive.
- **Inference-Time Compute Utilization**: The framework transforms inference compute into a learning resource by generating multiple parallel rollouts, effectively using test-time compute for training purposes.
- **Self-Proposed Rubrics**: Binary, auditable criteria generated from pseudo-references provide interpretable and consistent evaluation standards, addressing the black-box nature of traditional LLM judges.
- **Synthesis of Multiple Responses**: Aggregating diverse model outputs creates a more robust reference than any single response, capturing comprehensive reasoning patterns across different approaches.

## Architecture Onboarding

**Component Map**: Model Policy -> Parallel Rollouts -> Synthesis Module -> Pseudo-Reference -> Rubric Generator -> LLM Judge -> Reward Signal -> Policy Update

**Critical Path**: The core loop flows from generating parallel rollouts, through synthesis to create a pseudo-reference, then rubric generation and LLM judging to produce rewards that update the policy. Each component must function reliably for the system to work.

**Design Tradeoffs**: The framework trades computational cost during training (multiple rollouts) for eliminating human annotation costs. The synthesis method must balance comprehensiveness with efficiency, while rubric generation must ensure criteria are both comprehensive and reliably scorable by the LLM judge.

**Failure Signatures**: Performance plateaus when rollouts converge (reducing diversity), degrades when base model quality is insufficient to generate useful multiple perspectives, and suffers when LLM judges struggle with rubric consistency or when synthesis fails to capture essential information from diverse rollouts.

**3 First Experiments**:
1. Ablation study removing synthesis to test judge-only feedback performance
2. Varying rollout count to map the convergence curve and identify optimal compute allocation
3. Testing different LLM judge configurations to quantify sensitivity to judge architecture

## Open Questions the Paper Calls Out
None

## Limitations
- Diminishing returns occur as rollouts converge, suggesting scalability constraints for very large rollout counts
- High dependency on base model quality, with uncertain performance transfer to smaller or less capable models
- Reliance on LLM judges introduces potential judge-specific biases that are not fully explored across different judge architectures

## Confidence
- **High confidence**: Claims about CaT's effectiveness in healthcare domains with scarce ground-truth labels and the reported 9× computational efficiency gains
- **Medium confidence**: Cross-domain generalization claims for MATH-500, given evaluation focus on a single verifiable benchmark
- **Low confidence**: Scalability claims for significantly larger rollout counts and performance with different judge architectures, due to observed plateauing behavior

## Next Checks
1. Systematic ablation studies varying base model sizes and judge configurations to quantify sensitivity to these critical components
2. Stress testing with substantially larger rollout counts to map the full convergence curve and identify practical limits of the approach
3. Multi-domain evaluation across diverse non-verifiable tasks to establish robustness beyond healthcare guidance scenarios