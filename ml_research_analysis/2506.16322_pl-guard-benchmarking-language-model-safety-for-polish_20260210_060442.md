---
ver: rpa2
title: 'PL-Guard: Benchmarking Language Model Safety for Polish'
arxiv_id: '2506.16322'
source_url: https://arxiv.org/abs/2506.16322
tags:
- safety
- dataset
- polish
- pl-guard
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PL-Guard, a manually annotated benchmark\
  \ dataset for evaluating language model safety in Polish, along with an adversarial\
  \ variant (PL-Guard-adv) to test robustness under noise. The authors fine-tune three\
  \ models\u2014Llama-Guard-3-8B, a HerBERT-based classifier, and PLLuM\u2014using\
  \ combinations of annotated Polish data, machine-translated WildGuard, and PolyGuard\
  \ datasets."
---

# PL-Guard: Benchmarking Language Model Safety for Polish

## Quick Facts
- arXiv ID: 2506.16322
- Source URL: https://arxiv.org/abs/2506.16322
- Reference count: 10
- Key result: HerBERT-based classifier achieves 0.913 F1 on adversarial Polish safety test set

## Executive Summary
PL-Guard introduces a manually annotated benchmark dataset for evaluating language model safety in Polish, including an adversarial variant to test robustness under noise. The authors fine-tune three models—Llama-Guard-3-8B, a HerBERT-based classifier, and PLLuM—using combinations of annotated Polish data, machine-translated WildGuard, and PolyGuard datasets. Evaluation shows that the HerBERT-based classifier achieves the highest overall performance, particularly in adversarial settings, with a safety F1 score of 0.913 on PL-Guard-adv. The study demonstrates that smaller, specialized models fine-tuned on high-quality native-language data can outperform larger, general-purpose models in safety-critical tasks.

## Method Summary
The authors create PL-Guard, a manually annotated dataset of 7,387 Polish text samples (6,487 training, 900 test) labeled for safety and categorized into 15 hazard types following the Llama Guard taxonomy. An adversarial test set (PL-Guard-adv) is generated using character-level perturbations including typos, diacritic alterations, and OCR errors. Three models are fine-tuned: Llama-Guard-3-8B using full fine-tuning with FSDP, HerBERT-base-cased as a sequence classifier, and PLLuM. HerBERT is trained for 5 epochs with learning rate 1e-5, batch size 32, and standard transformer hyperparameters on 2x A100 GPUs. Models are evaluated on both clean and adversarial test sets using macro F1-score for binary safety classification and multiclass hazard categorization.

## Key Results
- HerBERT-based classifier achieves highest performance with 0.913 F1 on PL-Guard-adv adversarial test set
- Llama-Guard-3-8B and PLLuM perform comparably on clean data but drop more under adversarial conditions
- Cross-lingual tests show Polish-trained models struggle with English safety classification (HerBERT: 0.638 vs 0.935 F1)
- Smaller specialized models fine-tuned on native Polish data outperform larger general-purpose models in safety tasks

## Why This Works (Mechanism)
The HerBERT-based classifier's superior performance stems from its native language training on Polish text, allowing it to capture language-specific patterns and cultural context that general-purpose multilingual models miss. The adversarial testing reveals that models trained specifically on Polish can better handle character-level noise common in Polish text, such as diacritic variations. The manual annotation process ensures high-quality labels that reflect real safety concerns in Polish communication contexts, rather than relying on translations that may miss language-specific nuances.

## Foundational Learning
- Binary safety classification (safe/unsafe): Needed to establish basic safety filtering capability; quick check: verify balanced classes in training data
- Multiclass hazard categorization (15 categories): Needed to provide granular safety analysis; quick check: ensure all 15 Llama Guard categories are represented in Polish
- Adversarial perturbation generation: Needed to test real-world robustness; quick check: validate perturbation types cover common Polish text errors
- Cross-lingual evaluation: Needed to assess generalization limits; quick check: compare performance drop between Polish and English test sets
- Model-specific fine-tuning: Needed to optimize for Polish language characteristics; quick check: monitor validation loss for overfitting

## Architecture Onboarding

Component map: PL-Guard dataset -> HerBERT fine-tuning -> adversarial testing -> performance evaluation

Critical path: Data annotation → Model fine-tuning → Adversarial perturbation → Evaluation metrics

Design tradeoffs: Smaller specialized models (HerBERT) vs larger general models (Llama-Guard-3-8B) - HerBERT wins on Polish safety but lacks multilingual capability

Failure signatures: Large performance drops on adversarial data indicate poor robustness; significant cross-lingual performance gaps indicate language-specific training limitations

First experiments:
1. Fine-tune HerBERT on PL-Guard training set and evaluate on clean test set to establish baseline
2. Generate adversarial samples using described perturbation types and test model robustness
3. Compare HerBERT performance against Llama-Guard-3-8B on both Polish and English safety classification

## Open Questions the Paper Calls Out
None

## Limitations
- Exact adversarial perturbation implementation not specified, making exact reproduction challenging
- Study focuses on Polish language safety evaluation, limiting generalizability to other languages
- Cross-lingual experiments limited to English, though results clearly show language-specific limitations

## Confidence
- High confidence: HerBERT-based models outperform larger models on Polish safety classification, particularly under adversarial conditions
- High confidence: Native-language fine-tuning provides significant advantages for safety tasks
- Medium confidence: Cross-lingual generalization results, as English evaluation was limited

## Next Checks
1. Implement and test multiple adversarial perturbation strategies on PL-Guard dataset to verify robustness findings
2. Evaluate HerBERT model on additional Polish safety datasets or through human evaluation for real-world capability
3. Test HerBERT model on safety benchmarks in other languages with similar character systems to assess generalization beyond Polish-specific perturbations