---
ver: rpa2
title: 'JE-IRT: A Geometric Lens on LLM Abilities through Joint Embedding Item Response
  Theory'
arxiv_id: '2509.22888'
source_url: https://arxiv.org/abs/2509.22888
tags:
- questions
- question
- embedding
- llms
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'JE-IRT replaces global LLM rankings with a shared embedding space
  where direction encodes semantic specialization and norm encodes difficulty. Experiments
  show the traditional total-ordering assumption fails: many items have non-monotonic
  relationships between ability and correctness.'
---

# JE-IRT: A Geometric Lens on LLM Abilities through Joint Embedding Item Response Theory

## Quick Facts
- arXiv ID: 2509.22888
- Source URL: https://arxiv.org/abs/2509.22888
- Reference count: 40
- Key outcome: Replaces global LLM rankings with shared embedding space where direction encodes semantic specialization and norm encodes difficulty

## Executive Summary
JE-IRT introduces a geometric reformulation of Item Response Theory that represents both questions and LLMs in a shared embedding space. The framework captures semantic specialization through directional alignment and difficulty through embedding norm, achieving 75.05% accuracy on predicting LLM correctness. Experiments demonstrate that the traditional total-ordering assumption for LLM abilities fails, with many items showing non-monotonic relationships between ability and correctness. The learned geometry enables efficient onboarding of new LLMs and provides interpretable insights into OOD generalization through directional alignment.

## Method Summary
JE-IRT uses a frozen base encoder (ModernBERT-Large or all-mpnet-base-v2) followed by a trainable 2-layer MLP adapter to produce question embeddings. LLM embeddings are maintained in a learnable table. Correctness probability is computed as σ(E_Q · E_M / ||E_Q|| - ||E_Q||), decoupling semantic direction (unit vector) from difficulty (norm). The model is trained using binary cross-entropy loss on the EmbedLLM dataset containing 112 LLMs and 10 benchmarks. New LLMs can be added efficiently by training only their embeddings, reducing sample complexity from O(d) parameters to O(1).

## Key Results
- Achieves 75.05% accuracy, surpassing traditional 2PL IRT (65.11%) and EmbedLLM (74.12%)
- Embedding norms reliably indicate difficulty with AUC 0.73–0.77 for predicting incorrectness
- Directional alignment explains OOD generalization drops, with high-alignment benchmarks showing smaller performance drops
- Adding new LLMs requires only 10% of data to achieve near-joint-training accuracy

## Why This Works (Mechanism)

### Mechanism 1
Embedding norm encodes question difficulty—larger norms correspond to harder questions. The logit formulation subtracts question norm from projected ability: σ(Θ_M,Q - ||E_Q||). Higher norm increases the threshold a model's projected ability must exceed, reducing correctness probability. Evidence shows binned accuracy decreases monotonically as ||E_Q|| increases with AUC 0.73–0.77 using norm alone to predict incorrectness.

### Mechanism 2
Direction encodes semantic specialization, enabling OOD generalization prediction. Unit-normalized question embeddings capture topical similarity. When held-out benchmarks have high cosine similarity with training benchmarks, prediction accuracy drops are smaller. Benchmarks with higher alignment (LogiQA, MathQA: 96–97% cosine similarity) show smaller leave-one-out drops than low-alignment benchmarks (PIQA: 17–71%).

### Mechanism 3
Projected ability replaces scalar ability ordering. Ability on a question is Θ_M,Q = E_Q · E_M / ||E_Q||. Different models can rank higher on different questions without contradiction, formally proven in Proposition 1. Traditional 2PL IRT fails: 49% of items have negative or near-zero discrimination; correct-set inclusion analysis shows nominally stronger models miss items solved by weaker ones.

## Foundational Learning

- **2-Parameter Logistic IRT (2PL)**: Why needed: JE-IRT is explicitly motivated by 2PL's failure on LLM data; understanding the baseline formalism (ability θ, discrimination a, difficulty b) is prerequisite to grasping why the geometric reformulation matters. Quick check: If discrimination parameter a_j < 0, what happens to the monotonic relationship between ability and correctness?

- **Vector projection and cosine similarity**: Why needed: The core operation computes model ability as projection onto question direction; interpreting the geometry requires fluency with ||u||, u·v/||u||, and cos(u,v). Quick check: If two unit vectors have cosine similarity 0.95, what is their Euclidean distance?

- **Logistic regression sample complexity**: Why needed: Adding new LLMs requires fitting only d parameters; understanding O(d/n) generalization error explains the empirical data efficiency (10% data → near-joint-training accuracy). Quick check: For d=256 dimensions, roughly how many samples are needed to estimate parameters reliably?

## Architecture Onboarding

- **Component map**: Text → f_base (frozen) → g_θ (adapter) → E_Q → (norm, unit direction) → dot product with E_M → sigmoid → BCE loss

- **Critical path**: Question text → f_base → g_θ → E_Q → compute norm and unit direction → dot product with E_M → sigmoid → binary cross-entropy loss

- **Design tradeoffs**: Higher embedding dimension d captures more structure but increases sample complexity for new models; paper finds d=256 sufficient, with diminishing returns beyond. Freezing the base encoder limits semantic expressiveness but enables efficient onboarding.

- **Failure signatures**: Accuracy plateaus below baseline: adapter may be insufficiently expressive or base encoder misaligned with task. Norm-difficulty correlation weak (AUC <0.6): inductive bias violated; consider norm regularization or alternative difficulty encodings. New model embeddings fail to converge with <20% data: dimension d may be too high or learning rate poorly tuned.

- **First 3 experiments**: 1) Sanity check: Fit 2PL on small subset; verify negative discrimination items exist. 2) Dimension sweep: Train JE-IRT with d ∈ {16, 64, 256, 512}; plot accuracy vs dimension to confirm d=256 peak. 3) Leave-one-model-out: Exclude one LLM, train on remaining 111, fit held-out model's embedding with 5%, 10%, 20% of its data; verify convergence within 0.5% of joint-training baseline.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can JE-IRT be adapted to measure non-cognitive abilities such as honesty, persuasion, or emotional support? Basis: Conclusion states future work could incorporate other abilities through variants of JE-IRT. Evidence needed: Successful training of JE-IRT variants on datasets annotated for social behaviors revealing interpretable geometric relationships.

- **Open Question 2**: How does replacing binary correctness targets with model confidence probabilities alter the learned embedding geometry? Basis: Limitations note that LLMs are probabilistic and incorporating predicted probabilities into the loss is a seamless but unexplored extension. Evidence needed: Comparative study of embedding spaces trained on binary vs. soft labels, analyzing changes in difficulty norms and OOD generalization.

- **Open Question 3**: To what extent is the LLM-induced subject taxonomy dependent on the choice of the frozen base encoder? Basis: Section 3.4 notes encoder-dependent effects on GPQA generalization, suggesting the geometry is influenced by the specific base encoder used. Evidence needed: Measuring consistency of clustering purity and subject alignment across diverse base encoders with different pre-training objectives.

## Limitations
- Limited benchmark diversity: Experiments rely on fixed set of 10 established benchmarks; generalizability to radically different domains untested
- Interpretability of embedding geometry: Semantic interpretability of embedding space not deeply explored; unclear if directions correspond to human-understandable concepts
- Potential overfitting to dataset specifics: Model trained and evaluated on same dataset; risk of overfitting to idiosyncrasies of specific dataset composition

## Confidence
- **High Confidence**: Core claim of higher predictive accuracy than traditional 2PL IRT and baselines like EmbedLLM, well-supported by experimental results and internal mathematical consistency
- **Medium Confidence**: Claims about embedding norm indicating difficulty (AUC 0.73–0.77) and directional alignment predicting OOD generalization supported by evidence but rely on correlations within specific dataset
- **Low Confidence**: Claim that geometry "replaces" global LLM rankings is more reframing enabled by model's design; practical significance for downstream applications not fully demonstrated

## Next Checks
1. Cross-Domain Generalization Test: Apply JE-IRT to completely different domain (e.g., biomedical question answering) not present in original dataset; measure predictive accuracy and norm-difficulty alignment
2. Interpretability Probe: Visualize embeddings of questions from single benchmark using UMAP/t-SNE; annotate clusters with human-readable topics; verify if learned directions align with human-defined subject categories
3. Ablation on Embedding Dimension: Systematically vary d (64, 128, 256, 512); plot training accuracy and efficiency of adding new LLMs (accuracy with 5% and 10% of new data); confirm peak around d=256 and investigate diminishing returns at high dimensions