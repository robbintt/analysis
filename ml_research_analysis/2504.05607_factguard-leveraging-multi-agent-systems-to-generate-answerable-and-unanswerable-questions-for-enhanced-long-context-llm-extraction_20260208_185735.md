---
ver: rpa2
title: 'FactGuard: Leveraging Multi-Agent Systems to Generate Answerable and Unanswerable
  Questions for Enhanced Long-Context LLM Extraction'
arxiv_id: '2504.05607'
source_url: https://arxiv.org/abs/2504.05607
tags:
- questions
- unanswerable
- arxiv
- question
- answerable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FactGuard is a multi-agent framework that autonomously generates
  answerable and unanswerable questions for long-context reading comprehension, addressing
  the challenge of training LLMs to recognize unanswerable queries. The framework
  uses specialized agents to segment text, score quality, generate question-answer
  pairs, and synthesize unanswerable examples through evidence removal or misleading
  rewrites.
---

# FactGuard: Leveraging Multi-Agent Systems to Generate Answerable and Unanswerable Questions for Enhanced Long-Context LLM Extraction

## Quick Facts
- arXiv ID: 2504.05607
- Source URL: https://arxiv.org/abs/2504.05607
- Authors: Qian-Wen Zhang; Fang Li; Jie Wang; Lingfeng Qiao; Yifei Yu; Di Yin; Xing Sun
- Reference count: 40
- Primary result: Multi-agent framework generates 25,220 answerable/unanswerable QA pairs across 8K-128K token contexts, improving LLM hallucination resistance from 61.79% to 82.39% accuracy

## Executive Summary
FactGuard introduces a multi-agent framework that autonomously generates synthetic answerable and unanswerable questions for long-context reading comprehension, addressing the critical challenge of training language models to recognize unanswerable queries and avoid hallucination. The framework uses specialized agents for text segmentation, quality scoring, topic labeling, QA generation, and negative example synthesis through evidence removal or misleading rewrites. By eliminating the need for costly human annotation, FactGuard produces contextually relevant and challenging examples across 8K-128K token contexts. Evaluation on seven LLMs shows significant performance gaps on unanswerable questions, with fine-tuning on FactGuard-Bench improving accuracy from 61.79% to 82.39%, demonstrating effectiveness in enhancing model robustness for long-context information extraction.

## Method Summary
FactGuard employs a multi-agent pipeline that processes long documents through discrete stages: document segmentation, quality scoring and filtering, topic labeling, QA generation, and negative example synthesis. The framework generates grounded (Fragment, Question, Answer, Evidence) tuples and creates unanswerable variants via two methods—removing evidence spans or applying misleading rewrites with entity substitutions and false assumptions. Post-processing includes RAG-based conflict detection and web-based filtering to exclude context-independent commonsense questions. The resulting FactGuard-Bench dataset contains 25,220 examples (8,829 answerable, 16,391 unanswerable) across 8K-128K token contexts. Training uses supervised fine-tuning followed by direct preference optimization on Llama3.1-8B-Instruct, with evaluation showing substantial improvements in handling unanswerable queries while maintaining performance on answerable questions.

## Key Results
- FactGuard-Bench achieves 61.79% overall accuracy on seven LLMs, with significant degradation on unanswerable questions (63.34% on lack-of-evidence, 63.16% on misleading-evidence subsets)
- Models fine-tuned with FactGuard-Bench reach 82.39% accuracy, demonstrating 20.6 percentage point improvement in hallucination resistance
- Performance degrades with context length, dropping from 88.32% (0-16K answerable) to 83.89% (64-128K answerable) for Qwen2.5-72B-Instruct
- Manual review confirms ~93% data quality in synthetic dataset despite automated generation

## Why This Works (Mechanism)

### Mechanism 1: Multi-Agent Division of Labor for Data Quality
The framework decomposes QA generation into discrete stages—segmentation, quality scoring, topic labeling, QA generation, and negative example synthesis—where each agent specializes in a specific subtask. Quality agents filter low-potential segments early; QA agents generate grounded tuples; rewriting agents create adversarial variants. An agent console aggregates outputs and coordinates handoffs, aiming to reduce cascading errors. This specialization improves quality and consistency compared to monolithic approaches by enabling focused optimization at each stage.

### Mechanism 2: Controlled Negative Example Synthesis via Evidence Removal and Misleading Rewrites
For "lack of evidence" negatives, the framework removes the supporting evidence span from the context, rendering the grounded question unanswerable. For "misleading evidence" negatives, rewriting agents apply entity substitutions, impossible conditions, or false assumptions while preserving surface plausibility. This systematic manipulation produces challenging, linguistically plausible unanswerable questions that better expose model hallucination tendencies than naturally sampled or human-written unanswerable queries.

### Mechanism 3: Long-Context Stress Testing Across Scaled Context Lengths
FactGuard-Bench includes examples across 8K-128K token contexts, exposing performance degradation patterns invisible in short-context benchmarks. Models must maintain evidence tracking and consistent refusal behavior as context grows. The multi-agent pipeline operates on arbitrary-length inputs, enabling benchmark scaling beyond ~32K-token limits of some prior datasets, revealing fundamental limitations in evidence integration and attention.

## Foundational Learning

- **Concept:** Extractive vs. Abstractive Reading Comprehension
  - **Why needed here:** FactGuard focuses on extractive QA where answers are text spans from the context, plus explicit refusal for unanswerable cases. Understanding this distinction clarifies why hallucination—generating plausible but unsupported answers—is a critical failure mode targeted by the benchmark.
  - **Quick check question:** Given a passage about event X, can you identify whether a question expects a span extraction, a boolean judgment, or a refusal?

- **Concept:** Unanswerable Question Detection (Negative Rejection)
  - **Why needed here:** The benchmark's core challenge is distinguishing answerable from unanswerable queries. Prior datasets like SQuAD 2.0 rely on costly human annotation; FactGuard automates this via controlled synthesis.
  - **Quick check question:** For a question asking "Who won the 2025 Best Actor Oscar?" with a context about 2024 awards, should the model answer, refuse, or explain the temporal mismatch?

- **Concept:** Long-Context Attention and Memory
  - **Why needed here:** Performance degrades with length; understanding positional embeddings, attention patterns, and retrieval-augmented approaches helps diagnose failure modes in extended contexts.
  - **Quick check question:** How does a model's ability to recall evidence at token position 80K differ from evidence at position 2K, and which architectural mechanisms most affect this gap?

## Architecture Onboarding

- **Component map:** Document segmenter → Quality scoring agent → Topic labeling agent → QA generation agent → Quality judgment filter → Evidence removal agent OR Question rewriting agent → RAG review → Web commonsense filter → Agent console → FactGuard-Bench

- **Critical path:**
  1. Long document → segment extraction
  2. Quality + topic filtering (prunes low-value segments)
  3. Grounded QA tuple generation (Fragment, Question, Answer, Evidence)
  4. Negative synthesis via evidence removal OR misleading rewrite
  5. Post-processing review (conflict detection, commonsense exclusion)
  6. Final dataset output (FactGuard-Bench)

- **Design tradeoffs:**
  - **Automation vs. noise:** Fully synthetic pipeline reduces annotation cost but introduces non-trivial noise (manual review reports ~93% overall quality; Table 4). Trade lower cost for imperfect labels.
  - **Challenge vs. validity:** Aggressive misleading rewrites may yield questions that are unanswerable even for humans; calibration is needed to maintain validity.
  - **Length coverage vs. evaluation cost:** 128K contexts increase inference time and resource usage, potentially limiting experimental scale.

- **Failure signatures:**
  - High incorrect answer rates on unanswerable questions (>50%) indicate insufficient refusal training.
  - Sharp accuracy drop beyond 32K tokens suggests long-context integration failure.
  - Inconsistent quality scores across segments may indicate quality agent miscalibration.

- **First 3 experiments:**
  1. **Baseline evaluation:** Run FactGuard-Bench test set on the target model; compute accuracy split by answerable, lack-of-evidence, and misleading-evidence categories to establish a reference.
  2. **Length-stratified analysis:** Measure performance degradation across 0-16K, 16-32K, 32-64K, and 64-128K intervals to identify context-length sensitivity.
  3. **SFT + DPO fine-tuning loop:** Train on FactGuard-Bench training split using supervised fine-tuning followed by DPO with low-quality responses as negatives; compare pre/post accuracy and reasoning-ability metrics (Table 7 categories: incorrect answers, direct refusals, reasoned answers).

## Open Questions the Paper Calls Out

- **Does the efficacy of FactGuard fine-tuning scale proportionally to models larger than 8B parameters?**
  - **Basis in paper:** The authors state that due to computational limitations and API policies, training experiments were restricted to Llama-3.1-8B and could not be applied to larger models like Claude 3.5.
  - **Why unresolved:** It is unknown if the significant accuracy boost (to 82.39%) observed in the 8B model transfers to larger models which may have different reasoning capabilities.
  - **What evidence would resolve it:** Evaluation results of 70B+ parameter models fine-tuned on the FactGuard-Bench dataset.

- **Can the multi-agent framework be optimized to eliminate the residual noise in the synthetic dataset?**
  - **Basis in paper:** The authors acknowledge in the Limitations section that the automated process results in a "certain percentage of noise," confirmed by a manual review quality of ~93%.
  - **Why unresolved:** The current pipeline involves a review process that filters bad data but does not prevent its initial generation.
  - **What evidence would resolve it:** An updated framework iteration achieving >99% data quality in manual review without discarding large volumes of generated examples.

- **How robust is the unanswerable question synthesis method when applied to highly technical or scientific domains?**
  - **Basis in paper:** The dataset currently focuses on law and general books, while Future Work suggests expanding to a "wider range of contexts."
  - **Why unresolved:** The "Misleading Negative Example" generation relies on entity substitution and false assumptions, which may fail or produce illogical results in domains requiring deep semantic understanding (e.g., medicine).
  - **What evidence would resolve it:** Successful application of the FactGuard pipeline to generate high-quality QA pairs for specialized scientific corpora.

## Limitations

- **Synthetic data quality concerns:** The automated generation process introduces non-trivial noise, with manual review confirming only ~93% overall quality, potentially affecting downstream model performance.
- **Computational resource constraints:** Training experiments were limited to 8B-parameter models due to computational and API limitations, leaving scalability questions for larger models unanswered.
- **Domain specificity:** The current dataset focuses on legal and general book domains, with effectiveness in highly technical or scientific domains remaining unverified.

## Confidence

- **High confidence:** The multi-agent framework architecture and its division of labor for QA generation and negative example synthesis is well-described and logically sound.
- **Medium confidence:** The effectiveness of controlled negative example synthesis in improving LLM hallucination resistance is supported by benchmark results but lacks direct comparison to alternative synthesis methods.
- **Medium confidence:** Length-stratified performance degradation patterns are observed but could be influenced by model-specific attention mechanisms rather than fundamental reasoning limitations.

## Next Checks

1. **Prompt sensitivity analysis:** Systematically vary agent prompts and quality thresholds to quantify their impact on output quality and downstream model performance.
2. **Cross-dataset generalization:** Evaluate models fine-tuned on FactGuard-Bench on established QA benchmarks (SQuAD 2.0, NaturalQuestions) to assess transfer of unanswerable question detection capabilities.
3. **Negative example validity study:** Conduct human evaluation specifically focused on misleading negative examples to determine whether they are genuinely unanswerable or contain subtle clues that models exploit.