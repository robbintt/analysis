---
ver: rpa2
title: Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces
arxiv_id: '2507.20853'
source_url: https://arxiv.org/abs/2507.20853
tags:
- learning
- policy
- neural
- state
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel geometric perspective on neural reinforcement
  learning in continuous state and action spaces. The key contribution is a theoretical
  proof that locally attainable states under a wide two-layer neural policy form a
  low-dimensional manifold whose dimensionality is linearly bounded by the action
  space dimension (at most 2da + 1), independent of the high-dimensional state space.
---

# Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces

## Quick Facts
- arXiv ID: 2507.20853
- Source URL: https://arxiv.org/abs/2507.20853
- Authors: Saket Tiwari; Omer Gottesman; George Konidaris
- Reference count: 40
- Primary result: Locally attainable states under wide two-layer neural policies form a manifold with dimension ≤ 2dₐ + 1, linearly bounded by action space dimension

## Executive Summary
This paper establishes a fundamental link between the geometry of learned state manifolds and the dimensionality of the action space in continuous control RL. The key theoretical contribution proves that under wide two-layer neural policies, locally attainable states concentrate around a low-dimensional manifold whose dimensionality is linearly bounded by the action space dimension (at most 2dₐ + 1), independent of the high-dimensional state space. This result is derived by analyzing training dynamics through linearisation and Lie series expansions.

Empirically, the authors validate their theoretical bound across four MuJoCo environments using intrinsic dimensionality estimation algorithms, demonstrating that attainable state sets have much lower intrinsic dimensionality than their nominal state space dimension. They also show practical relevance by introducing a local manifold learning layer (sparse rate reduction) into policy/value networks, achieving significant performance improvements in high-dimensional control tasks with minimal computational overhead.

## Method Summary
The paper develops a theoretical framework for analyzing the geometry of attainable states in continuous control RL. The core approach involves linearising wide two-layer neural policies and analyzing their training dynamics using continuous-time semi-gradient updates and Lie series expansions. For empirical validation, the authors implement DDPG with single hidden layer GeLU networks to estimate intrinsic dimensionality using the Facco et al. (2017) two-nearest-neighbor algorithm. For the practical application, they modify SAC by replacing one feedforward layer with a sparsification layer that projects representations toward low-dimensional manifolds. The method is validated across MuJoCo and DeepMind Control Suite environments.

## Key Results
- Theoretical proof that locally attainable states under wide two-layer neural policies form a manifold with dimension ≤ 2dₐ + 1
- Empirical validation showing estimated intrinsic dimensionality of attained states stays below theoretical bound across four MuJoCo environments
- Sparse SAC with manifold learning layer achieves significant performance improvements on high-dimensional control tasks (Ant, Dog, Quadruped) with ~30% step-rate reduction

## Why This Works (Mechanism)

### Mechanism 1: Action Space Constrains Reachable State Geometry
- **Claim:** Locally attainable states concentrate around a manifold with dimension ≤ 2dₐ + 1, linearly bounded by action space dimension dₐ, independent of state space dimension dₛ.
- **Mechanism:** Policy outputs dₐ-dimensional actions that parameterize control-affine dynamics (ġ = g(s) + Σhᵢ(s)·aᵢ). The Lie series expansion shows reachable states span a space determined by action directions {h₁, ..., hₐ} and their Lie bracket derivatives {v₁, ..., vₐ}, plus time-dependent drift (tg + t²g′). This yields 2dₐ + 1 degrees of freedom.
- **Core assumption:** Wide two-layer neural policy with linearised parameterization; deterministic dynamics; GeLU activation (bounded first/second derivatives).
- **Evidence anchors:**
  - [Abstract] "dimensionality of this manifold is of the order of the dimensionality of the action space...first result of its kind, linking the geometry of the state space to the dimensionality of the action space"
  - [Section 4, Theorem 1] "the random variable...converges weakly to...ˆS concentrated around an m-dimensional manifold M with m ≤ 2dₐ + 1"
  - [Corpus] Weak direct evidence; neighbor papers address continuous control but not geometric manifold bounds.
- **Break condition:** Non-wide networks where linearisation fails; stochastic transitions; non-smooth activations with unbounded derivatives.

### Mechanism 2: Infinite-Width Linearisation Truncates Parameter Complexity
- **Claim:** Wide neural policies converge to linear-in-parameter models where sufficient statistics remain bounded despite infinite parameters.
- **Mechanism:** As width n→∞ with learning rate η→0 at rate 1/√n, the feature map Φ(s; W₀) becomes fixed. Parameter updates evolve via stochastic differential equation whose summary statistics (action outputs Aⱼ(s), products AⱼAⱼ′, derivatives) converge to tractable random ODEs (Lemma 8, 9). The push-forward through Lie expansion concentrates around the manifold.
- **Core assumption:** Lipschitz continuous gradient function G (Assumption 5); bounded gradient variance (Assumption 4).
- **Evidence anchors:**
  - [Section 3.1] "linearised approximation of the policy...is linear in the weights W and non-linear, within Φ, in the initial weights W₀"
  - [Section 5.1, Figure 2] Empirical validation shows returns converge between canonical and linearised policies at large widths (log₂n > 15).
  - [Corpus] No direct evidence on linearisation-width relationship in RL.
- **Break condition:** Finite width without learning rate scaling; non-i.i.d. initialization; gradient estimates with unbounded variance.

### Mechanism 3: Sparsity Layer Exploits Low-Dimensional Structure
- **Claim:** Replacing one feedforward layer with sparse rate reduction improves performance in high-DoF control.
- **Mechanism:** The sparsification layer Z^{ℓ+1} = ReLU(Z^ℓ + αW^T(Z^ℓ - WZ^ℓ) - αλ1) projects representations toward union of low-dimensional manifolds. Since attained states already lie on low-dimensional manifold (Theorem 1), explicit sparsification aligns network capacity with intrinsic geometry.
- **Core assumption:** Network width n ≥ Σᵢdᵢ (union of manifold dimensions); appropriate α parameters.
- **Evidence anchors:**
  - [Section 5.4, Figure 5] SAC with sparse layer significantly outperforms vanilla SAC on Ant, Dog Stand/Walk, Quadruped Walk.
  - [Appendix M] Sparsification layer "learns sparse representation...built upon the idea of sparse rate reduction"
  - [Corpus] No comparable sparsity-for-geometry approaches in neighbor papers.
- **Break condition:** Wrong α tuning (see ablation Figure 8); insufficient width; tasks where dₐ ≈ dₛ (no dimensional reduction to exploit).

## Foundational Learning

- **Concept: Lie Brackets and Vector Field Commutators**
  - **Why needed here:** The manifold dimension 2dₐ+1 comes from counting action directions hⱼ plus their Lie bracket derivatives vⱼ = J_{hⱼ}(s)h(s)ḁ. Without understanding vector field compositions, the bound appears unmotivated.
  - **Quick check question:** For dₐ=2 control-affine system, explain why reachable space can have up to 5 dimensions, not just 2.

- **Concept: Neural Tangent Kernel / Infinite-Width Limit**
  - **Why needed here:** Linearisation (Eq. 7) relies on the NTK regime where Φ(s; W₀) stays frozen. This is standard theory but essential for tracing why the proof works.
  - **Quick check question:** Why does the linear approximation f^{lin}(s; W) = f(s; W₀) + ∇_θf|_{W₀}(W - W₀) become exact as width→∞?

- **Concept: Intrinsic Dimensionality Estimation (Facco et al.)**
  - **Why needed here:** Empirical validation (Figure 3) uses two-nearest-neighbor ratios to estimate manifold dimension. The algorithm itself isn't explained in detail.
  - **Quick check question:** Given sorted neighbor distances r₁, r₂, why does log(μ) vs -log(1-i/N) slope estimate intrinsic dimension?

## Architecture Onboarding

- **Component map:**
  [State s] → [Linearised Policy f^{lin}(s; W)] → [Action a ∈ R^{dₐ}]
       ↓                                           ↓
  [Feature Φ(s; W₀) - frozen]              [Control-Affine Dynamics]
       ↓                                           ↓
  [W updated via semi-gradient]            [State s' = e^X_t(s)]
       ↓
  [Sparse Layer (optional): Z^{ℓ+1} = ReLU(Z^ℓ + αW^T(Z^ℓ-WZ^ℓ) - αλ₁)]

- **Critical path:**
  1. Verify linearisation accuracy at your target width (run canonical vs. linearised comparison as in Figure 2)
  2. Estimate intrinsic dimension of collected states using Facco algorithm (verify ≤ 2dₐ+1)
  3. If dₐ ≪ dₛ, add sparsity layer; tune α_π, α_Q separately

- **Design tradeoffs:**
  - **Width vs. linearisation fidelity:** Wider networks → better linear approximation but more compute. Paper shows log₂n > 15 is sufficient.
  - **Sparsity layer overhead:** ~30% step-rate reduction (Figure 9) for ~2-3x return improvement on high-DoF tasks.
  - **Single vs. multi-layer networks:** Theory assumes single hidden layer; empirical validation uses single-layer with GeLU for dimensionality estimation, but sparse SAC works with standard architectures.

- **Failure signatures:**
  - Estimated dimensionality > 2dₐ+1 → check stochastic transitions, non-smooth activations, or insufficient samples
  - Sparse SAC underperforms vanilla → likely α hyperparameter mismatch (run ablation) or task has dₐ ≈ dₛ
  - Linearised policy diverges from canonical → width too low or learning rate not scaled as η ∝ 1/√n

- **First 3 experiments:**
  1. **Replicate width sweep (Figure 2):** Train DDPG with canonical vs. linearised policies at log₂n ∈ {12, 14, 16}. Verify convergence gap → 0 as width increases.
  2. **Estimate manifold dimension (Figure 3):** Collect trajectories from trained policy; apply Facco algorithm; confirm estimate < 2dₐ+1 for your environment.
  3. **Sparsity ablation (Figure 8):** On Ant or Dog, sweep α_Q ∈ {0.1, 0.3, 0.5, 0.7} with fixed α_π; identify stable regime before deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the low-dimensional manifold structure persist when stochastic transition dynamics are introduced?
- **Basis in paper:** [explicit] The Discussion section states, "the impact of stochastic transitions remains unexplored, as our current analysis assumes deterministic transitions."
- **Why unresolved:** The theoretical proof relies on a specific deterministic ODE formulation of the state trajectory; stochasticity would require modeling the state distribution differently.
- **What evidence would resolve it:** A proof extending Theorem 1 to stochastic differential equations or empirical validation showing intrinsic dimensionality remains bounded in stochastic environments.

### Open Question 2
- **Question:** How does the geometry of attainable states change when the agent relies on an approximate, noisy value function rather than an oracle?
- **Basis in paper:** [explicit] The Discussion notes, "We also assume access to the true value function Q, this is not practical and warrants an extension to the setting where this function is noisy."
- **Why unresolved:** The analysis assumes semi-gradient updates derived from a precise gradient signal; function approximation errors could alter the dynamics that bound the manifold.
- **What evidence would resolve it:** Theoretical analysis showing the manifold dimensionality bound holds under bounded approximation error, or empirical dimensionality estimation in standard actor-critic frameworks without oracle access.

### Open Question 3
- **Question:** Can the theoretical bound on manifold dimensionality be extended to deep neural networks with more than two layers?
- **Basis in paper:** [explicit] Appendix P states, "We anticipate that extending our results to a broader set of activations, architectures... would lead to better applications."
- **Why unresolved:** The proof relies on the linearization dynamics of a specific "wide two-layer" model; adding layers changes the feature representation (Φ) and gradient flow.
- **What evidence would resolve it:** Empirical measurement of intrinsic dimensionality in deep policy networks (e.g., 3+ layers) to see if the 2dₐ+1 bound remains valid.

## Limitations
- Theoretical bound holds specifically for wide two-layer networks with linearised parameterization under control-affine dynamics
- Empirical validation uses single-layer GeLU architecture, deviating from standard multi-layer ReLU networks
- Sparsity layer performance improvements depend heavily on hyperparameter tuning and may not generalize across all high-dimensional tasks

## Confidence

- **High confidence**: Manifold dimension bound for linearised two-layer networks (Theorem 1), linearisation accuracy at sufficient width (Figure 2)
- **Medium confidence**: Empirical intrinsic dimensionality estimates using Facco algorithm, sparsity layer performance gains on tested tasks
- **Low confidence**: Generalizability to deeper networks, robustness across different activation functions, sparsity layer effectiveness when dₐ ≈ dₛ

## Next Checks

1. **Replicate linearisation accuracy sweep**: Train DDPG with canonical vs. linearised policies across multiple widths (log₂n ∈ {12, 14, 16}) to verify convergence gap → 0 as width increases
2. **Cross-activation comparison**: Estimate manifold dimension using ReLU, GeLU, and tanh activations to isolate smoothness requirements
3. **Transfer sparsity layer**: Apply sparse SAC architecture to DeepMind Control Suite Walker2D and Cheetah tasks to test generalizability beyond MuJoCo quadruped environments