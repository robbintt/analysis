---
ver: rpa2
title: 'Enhancing Automated Essay Scoring with Three Techniques: Two-Stage Fine-Tuning,
  Score Alignment, and Self-Training'
arxiv_id: '2602.01747'
source_url: https://arxiv.org/abs/2602.01747
tags:
- score
- performance
- essay
- lora
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the challenge of Automated Essay Scoring
  (AES) in low-resource settings, where labeled data scarcity limits the development
  of robust systems. The authors propose three techniques: Two-Stage fine-tuning with
  low-rank adaptation (LoRA), Score Alignment (SA), and Uncertainty-aware Self-Training
  (UST).'
---

# Enhancing Automated Essay Scoring with Three Techniques: Two-Stage Fine-Tuning, Score Alignment, and Self-Training

## Quick Facts
- arXiv ID: 2602.01747
- Source URL: https://arxiv.org/abs/2602.01747
- Reference count: 40
- Primary result: Integrated method achieves 91.2% of full-data performance in 32-data setting and sets new state-of-the-art in full-data setting

## Executive Summary
This study addresses the challenge of Automated Essay Scoring (AES) in low-resource settings where labeled data scarcity limits robust system development. The authors propose three integrated techniques—Two-Stage fine-tuning with LoRA, Score Alignment, and Uncertainty-aware Self-Training—to enhance the DualBERT baseline model for multi-trait scoring. Experiments on the ASAP++ dataset demonstrate significant improvements, with the integrated approach achieving 91.2% of full-data performance using only 32 labeled essays per prompt, while also establishing new state-of-the-art results in the full-data setting.

## Method Summary
The study integrates three techniques into DualBERT, a strong baseline for multi-trait AES. First, a Two-Stage fine-tuning strategy applies LoRA adapters after initial base model training, allowing trait-specific loss weight optimization. Second, Score Alignment performs post-hoc calibration using development set predictions to correct systematic bias in test predictions. Third, Uncertainty-aware Self-Training generates pseudo-labels from unlabeled essays filtered by prediction uncertainty, expanding the training set while mitigating label noise. These techniques are evaluated across K-data (K=32) and full-data settings on the ASAP++ dataset.

## Key Results
- In 32-data setting, integrated method improves QWK scores by 4.6 points over baseline
- Achieves 91.2% of full-data performance using only 32 labeled essays per prompt
- In full-data setting, Score Alignment with DualBERT achieves new state-of-the-art QWK results
- Individual technique analysis shows each component contributes to overall performance gains

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Fine-Tuning with LoRA
The LoRA-based approach captures fine-grained features by sweeping trait-specific loss weight combinations after initial base model fine-tuning. Stage 1 trains the full DualBERT model on labeled essays. Stage 2 inserts LoRA adapters into query/key/value modules while freezing base weights, then fine-tunes repeatedly with different trait loss weights to find optimal development set performance. This modular adaptation prevents catastrophic forgetting while allowing targeted adjustment.

### Mechanism 2: Score Alignment (SA)
SA corrects systematic bias in test predictions through a linear transformation based on development set predictions. Neural regression models tend to shrink predictions toward the mean (e.g., predicting 0.95 instead of 1.0). SA computes bias from top-p% and bottom-p% of dev predictions versus dev ground truth, then applies this correction to test predictions via min-max rescaling, reducing distribution mismatch.

### Mechanism 3: Uncertainty-aware Self-Training (UST)
UST enables effective use of unlabeled essays by filtering pseudo-labeled samples based on prediction uncertainty. A model trained on limited labeled data predicts scores on unlabeled essays, with uncertainty estimated via standard deviation across T dropout-enabled forward passes. Samples are binned by predicted score magnitude, retaining n_s lowest-uncertainty samples per bin as pseudo-labels, reducing label noise propagation.

## Foundational Learning

- **Low-Rank Adaptation (LoRA)**: Enables parameter-efficient fine-tuning by injecting trainable low-rank matrices into frozen pretrained weights, preserving base knowledge while adapting to AES-specific patterns. Quick check: Can you explain why freezing base weights and only training LoRA adapters might prevent overfitting in low-data regimes compared to full fine-tuning?

- **Monte Carlo Dropout Uncertainty**: UST relies on estimating prediction uncertainty via dropout at inference time; understanding how variance across stochastic forward passes proxies model confidence is essential. Quick check: If dropout rate is set to 0.0 at inference, what happens to uncertainty estimates, and how would this affect UST's filtering?

- **Distribution Calibration in Regression**: Score Alignment is a form of calibration; grasping why neural regression models produce systematically biased predictions (regression toward the mean) clarifies why post-hoc correction works. Quick check: Would you expect Score Alignment to help more for traits with wide score ranges or narrow ranges, and why?

## Architecture Onboarding

- **Component map**: Essays → BERT-TransEnc + BERT-CNN → Concatenation → Trait-specific dense heads → Predictions → LoRA adapters (stage 2) → Score Alignment (post-processing) → Uncertainty estimation (UST) → Pseudo-labels → Retraining

- **Critical path**: 1) Train DualBERT on labeled data (stage 1) → 2) Freeze DualBERT, add LoRA, sweep trait loss weights (stage 2) → 3) Apply SA to generate pseudo-labels for unlabeled data → 4) Run UST filtering, augment training set → 5) Retrain DualBERT (with LoRA optional) → 6) Apply final SA at inference

- **Design tradeoffs**: Training time vs. performance (LoRA sweep adds ~trait_count × epochs overhead); pseudo-label quantity vs. quality (UST's n_b × n_s controls tradeoff); SA simplicity vs. assumptions (requires dev/test distribution alignment)

- **Failure signatures**: LoRA overfitting (validation loss diverges with high rank); SA distortion (predictions outside [0,1] or reversed ordering); UST collapse (pseudo-labels cluster around narrow score range)

- **First 3 experiments**: 1) Baseline replication: Implement DualBERT on ASAP++ prompt, confirm QWK within ±0.02 of 0.701 average; 2) LoRA ablation: Add LoRA, sweep 'overall' vs 'balance' vs trait-specific targets, report per-trait QWK delta; 3) SA validation: Compute pre/post SA QWK on held-out dev set, visualize score distributions, confirm shift toward true distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a zero-data learning method be developed to eliminate the reliance on unlabeled essays required for Uncertainty-aware Self-Training (UST)?
- Basis in paper: The authors explicitly state in the Limitations section that collecting unrated essays may not always be feasible and propose developing a "zero-data learning method" as future work.
- Why unresolved: UST fundamentally depends on a pool of unlabeled data to generate pseudo-labels; removing this dependency requires a paradigm shift in how the model generates its own training signals without external data.
- What evidence would resolve it: A demonstration of a model achieving comparable performance to UST on ASAP++ without accessing any external unrated essays.

### Open Question 2
- Question: How can the Score Alignment technique be adapted for Seq2Seq-based models that generate tokenized text rather than continuous regression scores?
- Basis in paper: The discussion notes that while SA works well with DualBERT, applying it to Seq2Seq models like ArTS is difficult because SA is designed for "continuous predicted scores" whereas Seq2Seq models output discrete text.
- Why unresolved: The linear transformation used in SA assumes a continuous numeric output space; applying this to discrete token sequences requires a different mathematical formulation.
- What evidence would resolve it: A modified SA algorithm successfully integrated into a Seq2Seq model (like ArTS or T5) that yields improved QWK scores.

### Open Question 3
- Question: Would an iterative self-training process yield further performance gains compared to the single-iteration approach implemented in this study?
- Basis in paper: The implementation details specify that the authors "conduct the self-training process only one time without iterations," leaving the potential benefits of multiple refinement loops unexplored.
- Why unresolved: Standard self-training often involves iterative refinement where the model is retrained on its own increasingly confident predictions; skipping this step leaves a potential optimization trajectory untested.
- What evidence would resolve it: Ablation experiments comparing single-iteration UST against multi-iteration UST approach on the 32-data setting.

### Open Question 4
- Question: Can the Two-Stage fine-tuning strategy be optimized to reduce the training complexity that currently scales linearly with the number of essay traits?
- Basis in paper: The authors identify in the Limitations section that the training complexity of Two-Stage strategy "increases linearly with the number of traits," resulting in longer computational times.
- Why unresolved: The current algorithm loops through traits to find optimal loss weights via LoRA; a method that optimizes these weights simultaneously or employs a more efficient search strategy is needed to decouple complexity from trait count.
- What evidence would resolve it: A modified training algorithm that maintains Two-Stage fine-tuning performance gains but reduces training time to be independent of the number of traits.

## Limitations
- LoRA-based two-stage fine-tuning requires computationally expensive hyperparameter sweeps (per-trait loss weight combinations), limiting practical adoption
- Score Alignment assumes development and test sets share similar score distributions - this assumption may break down across different prompts or demographic groups
- UST approach depends on having sufficient unlabeled essays; if the unlabeled pool is small or out-of-domain, pseudo-label quality may degrade

## Confidence
- **High confidence**: Baseline DualBERT architecture and ASAP++ dataset description are clearly specified; Score Alignment mechanism is well-defined mathematically
- **Medium confidence**: UST filtering mechanism and uncertainty estimation process are adequately described; LoRA fine-tuning procedure is clear in concept but lacks precise details about which layers receive adapters
- **Low confidence**: Paper does not report statistical significance testing across multiple random seeds; ablation studies showing individual technique contributions are absent

## Next Checks
1. **Statistical validation**: Run 5-fold cross-validation with different random seeds to establish confidence intervals for QWK scores, particularly comparing baseline DualBERT vs. integrated approach in both K-data and full-data settings
2. **Technique isolation**: Implement each technique independently (LoRA, SA, UST) to quantify their individual contributions through ablation experiments on at least two ASAP++ prompts
3. **Distribution assumption test**: Apply Score Alignment to train/dev/test splits with artificially induced distribution shifts to verify conditions under which SA helps vs. harms performance