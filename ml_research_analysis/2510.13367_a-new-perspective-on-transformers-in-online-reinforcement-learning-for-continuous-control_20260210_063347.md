---
ver: rpa2
title: A New Perspective on Transformers in Online Reinforcement Learning for Continuous
  Control
arxiv_id: '2510.13367'
source_url: https://arxiv.org/abs/2510.13367
tags:
- arxiv
- learning
- transformer
- training
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Transformers are rarely used in online model-free RL due to training
  instability and sensitivity to architectural choices. This work systematically evaluates
  how transformers can be effectively applied to continuous control tasks.
---

# A New Perspective on Transformers in Online Reinforcement Learning for Continuous Control

## Quick Facts
- arXiv ID: 2510.13367
- Source URL: https://arxiv.org/abs/2510.13367
- Reference count: 40
- Primary result: Transformers can match or exceed MLP, LSTM, CNN, and GTrXL baselines in online RL for continuous control with appropriate architectural and training strategies

## Executive Summary
This paper systematically evaluates transformer architectures for online model-free reinforcement learning in continuous control tasks. The authors identify three critical factors that enable transformers to perform competitively: combining observations, actions, and rewards into a single input vector for POMDPs; freezing the transformer backbone during critic updates to prevent gradient interference; and using cross-episode data slicing with masking strategies for early-step learning. Through extensive experiments on MuJoCo and ManiSkill3 environments, the study demonstrates that transformers can achieve state-of-the-art performance when these architectural and training considerations are properly addressed.

## Method Summary
The authors evaluate multiple transformer architectures in online RL settings, comparing them against established baselines including MLPs, LSTMs, CNNs, and GTrXL. They systematically test different input representations (concatenating observations, actions, and rewards), actor-critic architectures (shared vs. separate backbones), and data processing strategies (cross-episode vs. within-episode). Key innovations include freezing the transformer backbone during critic updates to prevent gradient interference and implementing cross-episode data slicing with masking strategies for handling long sequences. The experiments span both MDP and POMDP tasks across MuJoCo and ManiSkill3 environments, including image-based settings.

## Key Results
- Transformers with combined observation-action-reward inputs outperform separate inputs in POMDPs
- Sharing transformer backbones between actor and critic causes gradient interference; freezing the backbone during critic updates resolves this
- Cross-episode data slicing with masking strategies (zero-padding, first-observation padding) enables effective early-step learning
- Transformers match or exceed MLP, LSTM, CNN, and GTrXL baselines across both MDP and POMDP tasks
- Performance improvements are consistent across MuJoCo and ManiSkill3 environments, including image-based settings

## Why This Works (Mechanism)
Transformers' self-attention mechanism allows them to capture long-range dependencies and complex temporal relationships in sequential data. In RL contexts, this capability is particularly valuable for POMDPs where agents must integrate observations, actions, and rewards over time to infer hidden states. The self-attention mechanism enables the model to weigh the importance of different time steps dynamically, which is crucial for decision-making in partially observable environments. Additionally, transformers' parallel processing capability makes them more efficient than recurrent architectures for handling long sequences, though this requires careful architectural design to avoid training instability.

## Foundational Learning
- **POMDPs vs MDPs**: Partially Observable Markov Decision Processes require agents to infer hidden states from incomplete observations, necessitating architectures that can integrate information over time. Quick check: Can the agent solve tasks where observation alone is insufficient?
- **Actor-Critic Methods**: RL algorithms that use separate networks for policy (actor) and value function (critic) estimation, requiring careful architectural design to prevent gradient interference. Quick check: Does freezing the backbone improve training stability?
- **Cross-episode Data Slicing**: Technique for handling long sequences by processing data across episode boundaries, essential for transformers to learn from early steps. Quick check: Does this improve performance on long-horizon tasks?
- **Self-attention Mechanism**: Allows transformers to weigh the importance of different positions in a sequence dynamically, crucial for handling temporal dependencies. Quick check: Does attention weight visualization reveal meaningful patterns?
- **Gradient Interference**: Occurs when shared parameters in multi-task learning receive conflicting gradient signals, requiring architectural solutions like parameter freezing. Quick check: Does gradient variance decrease with backbone freezing?

## Architecture Onboarding
- **Component Map**: Observations/Actions/Rewards -> Transformer Backbone -> Actor/Critic Heads
- **Critical Path**: Input preprocessing (concatenation) → Transformer layers (with attention) → Policy/value heads → Output actions/values
- **Design Tradeoffs**: Shared vs. separate backbones (stability vs. parameter efficiency), cross-episode vs. within-episode processing (long-range learning vs. computational cost), masking strategies (effectiveness vs. implementation complexity)
- **Failure Signatures**: Training instability with shared backbones, poor early-step performance without cross-episode slicing, gradient vanishing with improper masking
- **First Experiments**: 1) Test shared vs. separate backbone architectures on a simple POMDP task, 2) Compare zero-padding vs. first-observation padding strategies, 3) Evaluate freezing the backbone during critic updates on a standard MuJoCo task

## Open Questions the Paper Calls Out
The paper identifies several open questions including the theoretical understanding of why freezing the backbone prevents gradient interference, whether the proposed masking strategies are optimal for different task types, and how transformers perform in real-world robotic scenarios beyond benchmark tasks. The authors also note the need for further investigation into computational efficiency and memory requirements for deploying transformers in resource-constrained environments.

## Limitations
- Limited to benchmark tasks (MuJoCo, ManiSkill3) without testing on real-world robotic systems
- Architectural fixes (e.g., freezing backbone) are empirically effective but lack theoretical justification
- Cross-episode data slicing raises scalability concerns for long-horizon tasks with sparse rewards
- Computational overhead and memory constraints are not addressed
- Limited exploration of generalization to diverse task dynamics and reward structures

## Confidence
- High confidence in transformers achieving competitive performance in online RL for continuous control
- Medium confidence in architectural recommendations due to empirical nature without theoretical grounding
- Low confidence in generalizability to real-world robotic systems and diverse task environments

## Next Checks
1. Test the proposed transformer architecture and training strategies on real-world robotic tasks to evaluate scalability and robustness
2. Conduct ablation studies to determine the necessity and optimality of architectural fixes and explore alternative solutions
3. Investigate computational and memory efficiency, particularly for long-horizon tasks or environments with sparse rewards