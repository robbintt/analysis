---
ver: rpa2
title: An Empirical Study of Knowledge Distillation for Code Understanding Tasks
arxiv_id: '2508.15423'
source_url: https://arxiv.org/abs/2508.15423
tags:
- code
- distillation
- teacher
- student
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates knowledge distillation (KD)
  for code understanding tasks, evaluating logit-based and feature-based KD methods
  across eight student models and two teacher PLMs on three downstream tasks. Results
  show KD consistently outperforms standard fine-tuning, enabling student models to
  retain up to 98% of teacher performance while using as little as 5% of parameters.
---

# An Empirical Study of Knowledge Distillation for Code Understanding Tasks

## Quick Facts
- **arXiv ID:** 2508.15423
- **Source URL:** https://arxiv.org/abs/2508.15423
- **Reference count:** 40
- **Primary result:** KD enables student models to retain up to 98% of teacher performance while using as little as 5% of parameters

## Executive Summary
This empirical study systematically investigates knowledge distillation (KD) for code understanding tasks, evaluating both logit-based and feature-based KD methods across eight student models and two teacher PLMs on three downstream tasks. The research demonstrates that KD consistently outperforms standard fine-tuning, enabling student models to achieve near-teacher performance with dramatically reduced parameter counts. Feature-based KD methods, particularly Contrastive Knowledge Distillation (CKD), prove more effective than logit-based approaches. The study reveals that mid-sized student models (7M-30M parameters) offer optimal efficiency trade-offs, and that code-specific PLMs like UniXcoder serve as more effective teachers than general-purpose models despite lower standalone performance.

## Method Summary
The study evaluates knowledge distillation for code understanding tasks by comparing logit-based and feature-based KD methods across eight student models ranging from 5M to 110M parameters, using two teacher PLMs on three downstream tasks: code summarization, translation, and clone detection. The research employs standard KD techniques including temperature scaling for soft label generation and various feature-based methods like contrastive knowledge distillation. Student models are trained using both standard fine-tuning and KD approaches, with performance measured against teacher models to quantify efficiency gains. The experimental design systematically varies student size, teacher choice, and KD method to identify optimal configurations.

## Key Results
- KD consistently outperforms standard fine-tuning, enabling student models to retain up to 98% of teacher performance while using as little as 5% of parameters
- Feature-based KD methods, particularly CKD, demonstrate superior effectiveness over logit-based approaches
- Mid-sized student models (7M-30M parameters) achieve optimal efficiency trade-offs
- Code-specific PLMs like UniXcoder serve as more effective teachers than general-purpose models despite lower standalone performance
- KD introduces training overhead but enables 2-16× inference speedup compared to teacher models

## Why This Works (Mechanism)
Knowledge distillation works by transferring knowledge from larger teacher models to smaller student models through softened probability distributions and intermediate feature representations. In code understanding tasks, this process allows students to learn not just the correct answers but the reasoning patterns and feature hierarchies that lead to those answers. Feature-based methods like CKD are particularly effective because they capture structural similarities in code representations, enabling students to mimic the teacher's understanding of code semantics and patterns rather than just copying outputs.

## Foundational Learning

**Transformer Architecture** - Why needed: Forms the basis of both teacher and student models in modern code understanding systems. Quick check: Verify understanding of self-attention mechanisms and positional encoding.

**Knowledge Distillation Fundamentals** - Why needed: Core technique for transferring knowledge from large to small models. Quick check: Understand temperature scaling and soft label generation.

**Code Representation Learning** - Why needed: Essential for understanding how models process and represent programming constructs. Quick check: Know how tokenizers handle code-specific syntax.

**Contrastive Learning** - Why needed: Underlies the CKD method's effectiveness in feature-based KD. Quick check: Understand how contrastive loss encourages similar representations for similar inputs.

**Evaluation Metrics for Code Tasks** - Why needed: Critical for assessing model performance on code understanding benchmarks. Quick check: Know BLEU, ROUGE, and exact match metrics for different tasks.

## Architecture Onboarding

**Component Map:** Teacher PLM -> Feature Extractor -> Student Model -> Downstream Task

**Critical Path:** The most critical components are the feature extraction layer that bridges teacher and student representations, and the loss function that balances KD objectives with task-specific objectives.

**Design Tradeoffs:** Larger students capture more teacher knowledge but reduce efficiency gains; feature-based KD provides better performance but increases training complexity; code-specific teachers work better but may be less available.

**Failure Signatures:** Poor KD performance manifests as student models failing to converge or achieving only marginal improvements over baselines; feature mismatch between teacher and student representations indicates architectural incompatibility.

**First Experiments:** 1) Compare logit-based vs feature-based KD on a single task-student pair, 2) Test temperature scaling sensitivity on KD effectiveness, 3) Evaluate teacher model size impact on student performance

## Open Questions the Paper Calls Out
None

## Limitations
- Findings primarily applicable to three specific downstream tasks (code summarization, translation, and clone detection), limiting generalizability to other code understanding domains
- Experimental scope focuses on encoder-based transformer architectures, excluding decoder-only or encoder-decoder models that may exhibit different distillation behaviors
- Evaluation relies on proxy metrics rather than measuring actual developer productivity gains or real-world performance in practical coding scenarios

## Confidence
- **KD consistently improves student performance** (High confidence): Multiple student-teacher combinations across three tasks show robust improvements over standard fine-tuning baselines
- **Feature-based KD outperforms logit-based methods** (High confidence): Systematic comparisons demonstrate CKD's superior effectiveness across most model pairs
- **Mid-sized students achieve optimal efficiency** (Medium confidence): The 7M-30M parameter range shows strong results, though broader model size explorations would strengthen this conclusion
- **Code-specific PLMs serve as better teachers** (Medium confidence): UniXcoder's effectiveness as a teacher despite lower standalone performance is compelling but based on limited model comparisons
- **2-16× inference speedup claims** (Low confidence): These estimates depend on specific hardware configurations and inference implementations not fully detailed in the study

## Next Checks
1. Evaluate KD effectiveness on additional code understanding tasks beyond summarization, translation, and clone detection, particularly defect detection and code repair tasks
2. Conduct ablation studies isolating the contributions of different KD components (temperature scaling, loss weighting, etc.) to establish more precise methodological guidance
3. Measure actual inference latency and memory usage across diverse hardware platforms to validate theoretical efficiency claims with real-world deployment metrics