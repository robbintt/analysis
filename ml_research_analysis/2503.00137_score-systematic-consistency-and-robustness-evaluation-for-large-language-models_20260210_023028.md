---
ver: rpa2
title: 'SCORE: Systematic COnsistency and Robustness Evaluation for Large Language
  Models'
arxiv_id: '2503.00137'
source_url: https://arxiv.org/abs/2503.00137
tags:
- answer
- llama-3
- accuracy
- robustness
- letter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models are typically evaluated using a single accuracy
  metric, which overlooks their robustness to input variations. This work introduces
  SCORE, a systematic framework that evaluates LLM consistency and robustness by repeatedly
  testing models across multiple semantically equivalent prompts, random seeds, and
  answer choice orders.
---

# SCORE: Systematic COnsistency and Robustness Evaluation for Large Language Models

## Quick Facts
- arXiv ID: 2503.00137
- Source URL: https://arxiv.org/abs/2503.00137
- Reference count: 5
- One-line primary result: SCORE reveals that accuracy alone is insufficient for LLM evaluation, as robustness to input variations shows accuracy can vary by up to 15% while consistency rates remain low even for high-performing models.

## Executive Summary
Large language models are typically evaluated using a single accuracy metric, which overlooks their robustness to input variations. This work introduces SCORE, a systematic framework that evaluates LLM consistency and robustness by repeatedly testing models across multiple semantically equivalent prompts, random seeds, and answer choice orders. Experiments show that accuracy can vary by up to 15% depending on prompt or choice order, while consistency rates remain low even for high-performing models. The analysis reveals that higher accuracy does not guarantee better consistency, and model size alone is not a reliable predictor of robustness. SCORE enables a more realistic assessment of LLM capabilities, highlighting the need for multi-scenario evaluation in real-world applications.

## Method Summary
The SCORE framework systematically evaluates LLM robustness through three non-adversarial perturbation tasks: testing across 10 semantically equivalent prompts, shuffling answer choice orders (for MCQ datasets), and running inference with 5 different random seeds. Using generative evaluation (0-shot, max_tokens=1024), the framework measures both accuracy ranges and consistency rates (CR) by comparing model predictions across all variations. The consistency rate is calculated as the pairwise similarity between predictions, capturing prediction stability beyond what accuracy metrics reveal.

## Key Results
- Accuracy can vary by up to 15% depending on prompt changes and 6.1% for choice reordering
- Higher accuracy does not guarantee better consistency - Llama-3.1 70B has 9.6% higher accuracy than Llama-3 70B but both achieve 76% consistency rate
- Model size alone is not a reliable predictor of robustness, with smaller models like Qwen-2 7B sometimes outperforming larger models in consistency

## Why This Works (Mechanism)

### Mechanism 1: Multi-dimensional Perturbation Testing Reveals Model Sensitivity
Testing models across prompt variations, choice orderings, and random seeds exposes hidden sensitivity and instability not visible in single-metric evaluations. The SCORE framework systematically varies non-adversarial input factors that should theoretically be semantically neutral. By measuring both accuracy ranges and consistency rates across these variations, it reveals whether a model's internal representations and decision boundaries are robust or brittle to superficial changes. High accuracy variance or low CR indicates the model relies on spurious correlations or formatting artifacts rather than stable knowledge.

### Mechanism 2: Consistency Rate Captures Prediction Stability Independent of Accuracy
Consistency Rate (CR) provides a distinct signal from accuracy, revealing cases where models make confident but unstable predictions. CR is calculated as the pairwise similarity across all predictions for a single data point. This captures prediction flip-flopping between incorrect answers that accuracy metrics miss. A model can have high accuracy but low CR, meaning it's getting lucky with specific inputs rather than having robust knowledge.

### Mechanism 3: Difficulty-Calibrated Analysis Exposes Deeper Model Instabilities
Model consistency degrades predictably with problem difficulty, revealing fundamental limits in reasoning generalization. By analyzing performance across difficulty levels, the framework shows that lower consistency on harder problems reflects a more uniform underlying probability distribution over solutions. This makes the model's sampling path highly sensitive to random seed changes.

## Foundational Learning

### Concept: Consistency vs. Accuracy in Evaluation
Why needed here: SCORE fundamentally argues these are independent axes; high accuracy does not imply reliability. Understanding this separation is critical to interpreting the paper's results and applying the framework correctly.
Quick check question: If two models have 70% accuracy on a task, but one has 90% CR and the other 50% CR, which is more suitable for a high-stakes deployment? (Answer: The 90% CR model, as its behavior is more predictable even when wrong.)

### Concept: Non-Adversarial Robustness
Why needed here: The paper explicitly avoids adversarial attacks, focusing on benign variations like paraphrasing and reordering. This defines the scope of what SCORE measures and what it doesn't (e.g., security against malicious prompts).
Quick check question: Does a high SCORE ranking guarantee a model is secure against carefully crafted attacks designed to elicit harmful outputs? (Answer: No, SCORE evaluates non-adversarial robustness to natural input variations.)

### Concept: Generative vs. Log-Likelihood Evaluation
Why needed here: SCORE uses generative evaluation, forcing the model to produce text, which is shown to be a more accurate assessment for reasoning tasks. This has major implications for computational cost and the type of errors observed.
Quick check question: Why does the paper argue that generative evaluation is superior for the MATH dataset? (Answer: Because it requires the model to perform multiple reasoning steps to arrive at an answer, which log-likelihood evaluation of single tokens doesn't capture.)

## Architecture Onboarding

### Component map:
Dataset Selector -> Perturbation Engine -> Inference Runner -> Metrics Calculator -> Results Aggregator

### Critical path:
The key sequence is `Perturbation Engine -> Inference Runner -> Metrics Calculator`. Errors in how perturbations are generated (e.g., not properly randomizing choice order) or how predictions are parsed will invalidate the final consistency metrics.

### Design tradeoffs:
- Computational cost vs. thoroughness: Running 25+ evaluations per dataset is expensive. Reducing the number of prompts or seeds speeds up evaluation but reduces the reliability of the consistency estimate.
- Generative vs. Log-Likelihood: Generative evaluation is more realistic but far slower and requires robust parsing logic.

### Failure signatures:
- A model shows near-zero accuracy variance but very low CR. This likely indicates a parsing error where the model generates different but semantically equivalent answers that are not being recognized as similar by the symbolic equivalence checker.
- A prompt is identified as an outlier for all models. This suggests a flaw in the prompt design, not a model weakness.

### First 3 experiments:
1. **Pilot Run on One Model:** Pick a single, small model (e.g., Llama-3.1 8B) and run it on one dataset (e.g., AGIEval) through all three robustness tasks to validate the end-to-end pipeline and metric calculations.
2. **Prompt Sensitivity Analysis:** Using the pilot model, test all 10 prompts to confirm that one is not anomalous and causing a systematic drop in accuracy, which would indicate a prompt formulation issue.
3. **Benchmark Consistency Check:** Run the same model with the same prompt and seed multiple times to confirm that the deterministic components of the framework (e.g., data loading, metric computation) are producing identical results, isolating variability to model inference.

## Open Questions the Paper Calls Out

### Open Question 1
How can the SCORE framework be effectively adapted to evaluate consistency in creative or subjective tasks, such as summarization, where ground truth is not objective?
Basis in paper: The authors state in the Limitations section that they "do not explicitly consider creative tasks such as summarization, where consistency is more subjective."
Why unresolved: The current consistency metric relies on exact matches or symbolic equivalence, which cannot handle semantically equivalent but lexically diverse outputs.
What evidence would resolve it: An extension of the SCORE framework incorporating semantic similarity metrics or LLM-as-a-judge scoring to validate consistency on generative tasks.

### Open Question 2
What are the underlying mechanistic causes for specific, non-adversarial prompt formulations triggering catastrophic accuracy drops in otherwise robust models?
Basis in paper: Section 4.1 identifies a specific "outlier prompt" that caused significant accuracy degradation across models, describing it as a "curious phenomenon" that was excluded from analysis to avoid exaggerated claims.
Why unresolved: The paper identifies the sensitivity empirically but does not investigate the attention mechanisms or token distribution shifts that cause this specific failure mode.
What evidence would resolve it: An interpretability study analyzing token probability distributions and internal activations when the model processes the outlier prompt versus standard prompts.

### Open Question 3
Which specific training data characteristics or alignment techniques correlate most strongly with high consistency rates, independent of model size?
Basis in paper: The results demonstrate that "model size alone is not a reliable predictor of accuracy and consistency," noting that smaller models like Qwen-2 7B sometimes outperform larger models in consistency.
Why unresolved: The study evaluates fixed model weights without ablations regarding training data mixture, instruction tuning volume, or alignment processes.
What evidence would resolve it: A longitudinal study applying SCORE checkpoints throughout the training and alignment phases of different model families to isolate variables that improve robustness.

## Limitations
- The framework's reliance on generative evaluation introduces significant computational overhead, with each model requiring up to 25+ evaluations per dataset.
- The assumption of semantic equivalence between prompt variations may not hold if subtle semantic shifts affect model predictions in ways not captured by consistency metrics.
- The focus on factual questions limits generalizability to open-ended or creative tasks where multiple valid answers exist.

## Confidence

- **High Confidence:** The finding that accuracy variance reaches up to 15% across prompt changes and 6.1% for choice reordering is well-supported by empirical results across multiple datasets and models.
- **Medium Confidence:** The claim that higher accuracy doesn't guarantee better consistency is supported by specific examples (Llama-3.1 70B vs Llama-3 70B), but broader statistical analysis across the entire model landscape would strengthen this assertion.
- **Low Confidence:** The assertion that model size alone is not a reliable predictor of robustness is based on comparisons within the Llama family but lacks cross-architecture validation with models like GPT-4 or Claude.

## Next Checks

1. **Cross-Architecture Validation:** Test the SCORE framework on models from different architectural families (transformers vs. other designs) to verify whether the observed accuracy-consistency decoupling holds universally or is architecture-specific.

2. **Adversarial Extension:** Apply SCORE's consistency metrics to adversarial prompts to determine whether non-adversarial robustness correlates with resistance to malicious inputs.

3. **Real-World Deployment Test:** Deploy high-consistency, moderate-accuracy models versus high-accuracy, low-consistency models in a controlled production environment to measure actual performance differences in user-facing applications.