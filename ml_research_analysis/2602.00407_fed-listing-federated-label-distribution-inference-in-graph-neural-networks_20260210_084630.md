---
ver: rpa2
title: 'Fed-Listing: Federated Label Distribution Inference in Graph Neural Networks'
arxiv_id: '2602.00407'
source_url: https://arxiv.org/abs/2602.00407
tags:
- attack
- distribution
- label
- data
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses label distribution inference attacks in federated
  graph neural networks (FedGNNs), where a malicious server attempts to infer the
  label proportions of target clients without access to raw data. The proposed Fed-Listing
  method leverages final-layer gradients exchanged during federated training to uncover
  statistical patterns revealing class proportions.
---

# Fed-Listing: Federated Label Distribution Inference in Graph Neural Networks

## Quick Facts
- arXiv ID: 2602.00407
- Source URL: https://arxiv.org/abs/2602.00407
- Reference count: 40
- Primary result: Proposed Fed-Listing method achieves high label distribution inference accuracy on FedGNNs, significantly outperforming baselines like random guessing and Decaf.

## Executive Summary
This paper presents Fed-Listing, a novel attack that infers the label distributions of target clients in federated graph neural networks using only final-layer gradients exchanged during training. The attack leverages an auxiliary shadow dataset to generate diverse label partitioning strategies, simulating various client distributions to train an attack model. Experiments on four benchmark datasets (Cora, PubMed, Citeseer, Amazon Computers) and three GNN architectures (GCN, GraphSage, GIN) demonstrate that Fed-Listing significantly outperforms existing baselines, even under challenging non-i.i.d. scenarios. The results expose a critical privacy vulnerability in FedGNNs, highlighting the need for more effective defenses against gradient-based information leakage.

## Method Summary
Fed-Listing exploits final-layer gradients exchanged during federated training to infer label distributions of target clients without access to raw data. The method uses an auxiliary shadow dataset to simulate diverse client label distributions through four partitioning strategies: equal, random, single-class, and missing-class. For each strategy, shadow federated learning processes are executed to collect gradient-label pairs. These pairs train an MLP attack model using a composite loss function combining L1 alignment, variance-based L2, and JS-divergence terms. The attack model predicts label distributions from gradient sequences across training rounds.

## Key Results
- Fed-Listing achieves superior performance across various label distribution settings, particularly in single-class and one-class-dominant distributions
- Attack performance significantly degrades when only single-class distributions are present (CS drops to 0.778 on Cora for GCN)
- Defense mechanisms like differential privacy, noisy gradients, and gradient compression provide limited mitigation while severely degrading model utility
- Composite loss function outperforms single-objective training, achieving 0.995 cosine similarity compared to 0.946 for MSE alone

## Why This Works (Mechanism)

### Mechanism 1: Final-Layer Gradient Distribution Encoding
Gradients from the final classification layer encode recoverable statistical signatures of a client's label distribution. During backpropagation, the gradient of the loss with respect to final-layer weights is computed as a function of prediction errors across all training samples. Since each sample contributes proportionally to its class frequency, the aggregated gradient magnitude per output dimension correlates with class prevalence. The attack captures these gradients across multiple communication rounds, forming a temporal fingerprint of the underlying distribution.

### Mechanism 2: Shadow Federated Learning for Attack Training Data Generation
An auxiliary dataset with similar distribution characteristics is systematically partitioned to simulate diverse client label distributions. The auxiliary dataset is divided into subsets and each subset is further partitioned using four strategies (equal, random, single-class, missing-class). For each partition strategy, a complete shadow FL process is executed, recording final-layer gradients from simulated clients alongside their known label distributions. These pairs form a supervised dataset for the attack model.

### Mechanism 3: Distribution-Aligned Composite Loss for Attack Model Training
A composite loss function combining alignment metrics (L1) with distribution matching metrics (JS-divergence, variance L2) produces more accurate distribution inference than single-objective training. The attack model (a 2-layer MLP) is trained to minimize: L = a·L₁(Y, Ŷ) + b·VarL₂(Y, Ŷ) + c·JS(Y, Ŷ). The L1 term provides robustness to outliers, the variance term reduces distribution spread mismatch, and JS-divergence optimizes distributional overlap.

## Foundational Learning

- **Concept: Federated Averaging (FedAvg) and Gradient Computation**
  - Why needed here: Understanding how client gradients are computed and aggregated is essential to see why they leak label information. The attack exploits the mathematical relationship between local loss gradients and class frequencies.
  - Quick check question: Given a 3-class classification problem where client A has 90% class-1 samples and client B has 33% each, would you expect their final-layer bias gradients to differ significantly in magnitude per class dimension?

- **Concept: Message Passing in Graph Neural Networks**
  - Why needed here: FedGNNs inherit GNN's aggregation mechanisms that propagate node features across graph structure. While Fed-Listing targets final-layer gradients, understanding that GNN representations are built from neighborhood aggregations helps explain why graph-specific attacks differ from image-domain FL attacks.
  - Quick check question: If two clients have identical label distributions but different graph topologies (one sparse, one dense), would you expect their GNN gradients to be identical? Why or why not?

- **Concept: Distribution Divergence Metrics (KL, JS-divergence)**
  - Why needed here: The attack model's loss function and evaluation metrics both rely on distribution similarity measures. Understanding what JS-divergence optimizes (overlap vs. point-wise distance) clarifies why the composite loss outperforms simple MSE.
  - Quick check question: If the true distribution is [0.5, 0.3, 0.2] and predictions are [0.5, 0.25, 0.25], would L1 distance or JS-divergence better capture that the model correctly identified the dominant class but misestimated minority proportions?

## Architecture Onboarding

**Component map:**
Auxiliary Dataset → Partition Module → Shadow FL Orchestrator → Attack Dataset Builder ← Gradient Collector → Attack Model (MLP: 256→128→T units) → Label Distribution Output (T-class probabilities)

**Critical path:**
1. Shadow FL setup: Configure number of clients (C=10), rounds (R=50), local epochs (E varies), and partition strategy
2. Gradient extraction: At each round, compute g_r = θ_global^(r-1) - θ_client^r for final layer only (shape: d_out)
3. Sequence flattening: Concatenate gradients across R rounds into feature vector [g_1, ..., g_R] of shape R×d_out
4. Attack model training: Train MLP with composite loss, tuning (a,b,c) weights via grid search on held-out shadow data

**Design tradeoffs:**
- Shadow FL processes vs. attack generalization: More shadow processes improve attack accuracy but increase computational cost and auxiliary data reuse
- Loss function complexity: Composite loss outperforms single objectives but requires per-dataset hyperparameter tuning
- Defense tolerance vs. model utility: Strong DP (ε≤2) or high noise (σ≥2.5) reduces attack success but severely degrades GNN accuracy

**Failure signatures:**
- Single-class-dominant scenarios: Attack CS drops to 0.704 (Cora/GCN, Table II) when one class dominates; insufficient gradient diversity in training data
- Small auxiliary datasets: Paper uses 20% of original data as auxiliary; smaller fractions likely degrade transfer learning
- High-gradient-compression regimes: At α=0.1 compression ratio, model accuracy drops to 0.547 while attack remains partially effective

**First 3 experiments:**
1. Baseline replication on Cora/GCN: Implement 10-client FedGNN with random partition, collect final-layer gradients over 50 rounds, train attack MLP with L1+VarL2+JS loss (weights 0.0, 0.5, 0.5), measure JS-divergence against ground truth. Target: JS-div ≤ 0.002 per Table II.
2. Ablation on shadow FL process count: Repeat experiment with 5, 10, 15, 20 shadow processes. Plot cosine similarity vs. process count. Expect monotonic improvement converging to ~0.99 per Figure 3a.
3. Defense robustness test: Apply gradient compression with thresholds α∈{0.1, 0.3, 0.5, 0.7, 1.0}. Record both attack JS-divergence and GNN classification accuracy. Characterize the privacy-utility tradeoff curve per Figure 2c.

## Open Questions the Paper Calls Out

### Open Question 1
Can Fed-Listing be adapted to function effectively without real auxiliary datasets by using synthetic or mixed-source data? The conclusion states, "One limitation of this work is its reliance on an auxiliary dataset... exploring the use of synthetic or mixed-source auxiliary data to replace real auxiliary samples is an important future direction." This remains unresolved as the current methodology assumes the attacker possesses a dataset with a distribution similar to the target client, which may not hold in restrictive real-world adversarial settings.

### Open Question 2
Is it possible to construct a defense mechanism that effectively mitigates Fed-Listing without catastrophically degrading the model's task utility? The abstract and conclusion note that existing defenses like differential privacy and gradient compression "barely reduce our attack performance, unless the model's utility is severely degraded." The paper identifies a critical vulnerability where standard defenses fail to balance privacy and utility, but it does not propose a specific countermeasure to resolve this trade-off.

### Open Question 3
Does the Fed-Listing approach generalize to Vertical Federated Graph Neural Networks (VFGNNs) where features and labels are distributed across different clients? Section III explicitly states, "The proposed method in this paper is specifically for horizontal FedGNNs," distinguishing it from the vertical setting. Vertical FL splits data by features or labels rather than nodes, fundamentally changing the nature of the gradient updates and label availability, rendering the current horizontal approach potentially inapplicable.

## Limitations

- The attack relies on strong assumptions about gradient signal-to-noise ratios and transferability of auxiliary datasets that were not stress-tested under extreme non-IID conditions or when auxiliary data is unavailable
- Defense evaluations focus on generic techniques (DP, gradient compression, noise addition) without exploring graph-specific defenses like subgraph partitioning or differential privacy tailored to graph structures
- Evaluation metrics measure distribution similarity but not downstream harm from distribution leakage, potentially underestimating the attack's practical impact

## Confidence

- **High confidence:** The gradient-based attack mechanism works as described, supported by strong empirical evidence across four datasets and three GNN architectures. The composite loss function's superiority is well-validated.
- **Medium confidence:** The shadow FL data generation approach is methodologically sound but untested when auxiliary data is mismatched or unavailable. The attack's vulnerability to single-class-dominant scenarios is demonstrated but not fully explained mechanistically.
- **Low confidence:** Defense effectiveness claims lack nuance - while DP and noise addition degrade model utility severely, the paper doesn't explore whether these are the only viable defenses or if graph-specific techniques could provide better privacy-utility tradeoffs.

## Next Checks

1. **Auxiliary Data Sensitivity:** Systematically vary the similarity between auxiliary and target datasets (e.g., Cora vs. PubMed) to measure attack performance degradation when distributions diverge.
2. **Defense Space Exploration:** Implement and evaluate graph-specific defenses like node feature perturbation, subgraph partitioning, or graph structure anonymization alongside the baseline techniques.
3. **Extreme Non-IID Stress Test:** Create federated scenarios with severe class imbalance (e.g., 99% single class, 1% minority classes) and measure whether the attack's gradient-based signatures remain distinguishable from random noise.